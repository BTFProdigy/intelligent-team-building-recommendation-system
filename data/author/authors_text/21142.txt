Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 951?962,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
System Combination for Grammatical Error Correction
Raymond Hendy Susanto Peter Phandi Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive, Singapore 117417
{raymondhs,peter-p,nght}@comp.nus.edu.sg
Abstract
Different approaches to high-quality
grammatical error correction have been
proposed recently, many of which have
their own strengths and weaknesses. Most
of these approaches are based on classi-
fication or statistical machine translation
(SMT). In this paper, we propose to com-
bine the output from a classification-based
system and an SMT-based system to
improve the correction quality. We adopt
the system combination technique of
Heafield and Lavie (2010). We achieve an
F
0.5
score of 39.39% on the test set of the
CoNLL-2014 shared task, outperforming
the best system in the shared task.
1 Introduction
Grammatical error correction (GEC) refers to the
task of detecting and correcting grammatical er-
rors present in a text written by a second language
learner. For example, a GEC system to correct
English promises to benefit millions of learners
around the world, since it functions as a learning
aid by providing instantaneous feedback on ESL
writing.
Research in this area has attracted much interest
recently, with four shared tasks organized in the
past several years: Helping Our Own (HOO) 2011
and 2012 (Dale and Kilgarriff, 2010; Dale et al.,
2012), and the CoNLL 2013 and 2014 shared tasks
(Ng et al., 2013; Ng et al., 2014). Each shared task
comes with an annotated corpus of learner texts
and a benchmark test set, facilitating further re-
search in GEC.
Many approaches have been proposed to de-
tect and correct grammatical errors. The most
dominant approaches are based on classification
(a set of classifier modules where each module ad-
dresses a specific error type) and statistical ma-
chine translation (SMT) (formulated as a transla-
tion task from ?bad? to ?good? English). Other ap-
proaches combine the classification and SMT ap-
proaches, and often have some rule-based compo-
nents.
Each approach has its own strengths and weak-
nesses. Since the classification approach is able to
focus on each individual error type using a sep-
arate classifier, it may perform better on an er-
ror type where it can build a custom-made classi-
fier tailored to the error type, such as subject-verb
agreement errors. The drawback of the classifica-
tion approach is that one classifier must be built
for each error type, so a comprehensive GEC sys-
tem will need to build many classifiers which com-
plicates its design. Furthermore, the classification
approach does not address multiple error types that
may interact.
The SMT approach, on the other hand, natu-
rally takes care of interaction among words in a
sentence as it attempts to find the best overall cor-
rected sentence. It usually has a better coverage
of different error types. The drawback of this ap-
proach is its reliance on error-annotated learner
data, which is expensive to produce. It is not pos-
sible to build a competitive SMT system without a
sufficiently large parallel training corpus, consist-
ing of texts written by ESL learners and the corre-
sponding corrected texts.
In this work, we aim to take advantage of both
the classification and the SMT approaches. By
combining the outputs of both systems, we hope
that the strengths of one approach will offset the
weaknesses of the other approach. We adopt the
system combination technique of (Heafield and
Lavie, 2010), which starts by creating word-level
alignments among multiple outputs. By perform-
ing beam search over these alignments, it tries
to find the best corrected sentence that combines
parts of multiple system outputs.
The main contributions of this paper are as fol-
951
lows:
? It is the first work that makes use of a system
combination strategy to improve grammatical
error correction;
? It gives a detailed description of methods
and experimental setup for building compo-
nent systems using two state-of-the-art ap-
proaches; and
? It provides a detailed analysis of how one ap-
proach can benefit from the other approach
through system combination.
We evaluate our system combination approach
on the CoNLL-2014 shared task. The approach
achieves an F
0.5
score of 39.39%, outperforming
the best participating team in the shared task.
The remainder of this paper is organized as fol-
lows. Section 2 gives the related work. Section 3
describes the individual systems. Section 4 ex-
plains the system combination method. Section 5
presents experimental setup and results. Section 6
provides a discussion and analysis of the results.
Section 7 describes further experiments on system
combination. Finally, Section 8 concludes the pa-
per.
2 Related Work
2.1 Grammatical Error Correction
Early research in grammatical error correction fo-
cused on a single error type in isolation. For ex-
ample, Knight and Chander (1994) built an article
correction system for post-editing machine trans-
lation output.
The classification approach has been used to
deal with the most common grammatical mistakes
made by ESL learners, such as article and prepo-
sition errors (Han et al., 2006; Chodorow et al.,
2007; Tetreault and Chodorow, 2008; Gamon,
2010; Dahlmeier and Ng, 2011; Rozovskaya and
Roth, 2011; Wu and Ng, 2013), and more recently,
verb errors (Rozovskaya et al., 2014b). Statis-
tical classifiers are trained either from learner or
non-learner texts. Features are extracted from the
sentence context. Typically, these are shallow fea-
tures, such as surrounding n-grams, part-of-speech
(POS) tags, chunks, etc. Different sets of fea-
tures are employed depending on the error type
addressed.
The statistical machine translation (SMT) ap-
proach has gained more interest recently. Earlier
work was done by Brockett et al. (2006), where
they used SMT to correct mass noun errors. The
major impediment in using the SMT approach for
GEC is the lack of error-annotated learner (?par-
allel?) corpora. Mizumoto et al. (2011) mined a
learner corpus from the social learning platform
Lang-8 and built an SMT system for correcting
grammatical errors in Japanese. They further tried
their method for English (Mizumoto et al., 2012).
Other approaches combine the advantages of
classification and SMT (Dahlmeier and Ng,
2012a) and sometimes also include rule-based
components. Note that in the hybrid approaches
proposed previously, the output of each compo-
nent system might be only partially corrected for
some subset of error types. This is different from
our system combination approach, where the out-
put of each component system is a complete cor-
rection of the input sentence where all error types
are dealt with.
State-of-the-art performance is achieved by
both the classification (Dahlmeier et al., 2012;
Rozovskaya et al., 2013; Rozovskaya et al.,
2014a) and the SMT approach (Felice et al., 2014;
Junczys-Dowmunt and Grundkiewicz, 2014),
which motivates us to attempt system output com-
bination from both approaches.
2.2 System Combination
System combination is the task of combining the
outputs of multiple systems to produce an out-
put better than each of its individual component
systems. In machine translation (MT), combin-
ing multiple MT outputs has been attempted in
the Workshop on Statistical Machine Translation
(Callison-Burch et al., 2009; Bojar et al., 2011).
One of the common approaches in system com-
bination is the confusion network approach (Rosti
et al., 2007b). In this approach, a confusion net-
work is created by aligning the outputs of multi-
ple systems. The combined output is generated by
choosing the output of one single system as the
?backbone?, and aligning the outputs of all other
systems to this backbone. The word order of the
combined output will then follow the word order
of the backbone. The alignment step is critical in
system combination. If there is an alignment er-
ror, the resulting combined output sentence may
be ungrammatical.
Rosti et al. (2007a) evaluated three system com-
bination methods in their work:
952
? Sentence level This method looks at the com-
bined N-best list of the systems and selects
the best output.
? Phrase level This method creates new hy-
potheses using a new phrase translation ta-
ble, built according to the phrase alignments
of the systems.
? Word level This method creates a graph by
aligning the hypotheses of the systems. The
confidence score of each aligned word is then
calculated according to the votes from the hy-
potheses.
Combining different component sub-systems
was attempted by CUUI (Rozovskaya et al.,
2014a) and CAMB (Felice et al., 2014) in the
CoNLL-2014 shared task. The CUUI system em-
ploys different classifiers to correct various error
types and then merges the results. The CAMB
system uses a pipeline of systems to combine the
outputs of their rule based system and their SMT
system. The combination methods used in those
systems are different from our approach, because
they combine individual sub-system components,
by piping the output from one sub-system to an-
other, whereas we combine the outputs of whole
systems. Moreover, our approach is able to com-
bine the advantages of both the classification and
SMT approaches. In the field of grammatical error
correction, our work is novel as it is the first that
uses system combination to improve grammatical
error correction.
3 The Component Systems
We build four individual error correction systems.
Two systems are pipeline systems based on the
classification approach, whereas the other two are
phrase-based SMT systems. In this section, we
describe how we build each system.
3.1 Pipeline
We build two different pipeline systems. Each sys-
tem consists of a sequence of classifier-based cor-
rection steps. We use two different sequences of
correction steps as shown in Table 1. As shown
by the table, the only difference between the two
pipeline systems is that we swap the noun number
and the article correction step. We do this because
there is an interaction between noun number and
article correction. Swapping them generates sys-
tem outputs that are quite different.
Step Pipeline 1 (P1) Pipeline 2 (P2)
1 Spelling Spelling
2 Noun number Article
3 Preposition Preposition
4 Punctuation Punctuation
5 Article Noun number
6 Verb form, SVA Verb form, SVA
Table 1: The two pipeline systems.
We model each of the article, preposition, and
noun number correction task as a multi-class clas-
sification problem. A separate multi-class confi-
dence weighted classifier (Crammer et al., 2009)
is used for correcting each of these error types. A
correction is only made if the difference between
the scores of the original class and the proposed
class is larger than a threshold tuned on the devel-
opment set. The features of the article and prepo-
sition classifiers follow the features used by the
NUS system from HOO 2012 (Dahlmeier et al.,
2012). For the noun number error type, we use
lexical n-grams, ngram counts, dependency rela-
tions, noun lemma, and countability features.
For article correction, the classes are the arti-
cles a, the, and the null article. The article an
is considered to be the same class as a. A sub-
sequent post-processing step chooses between a
and an based on the following word. For prepo-
sition correction, we choose 36 common English
prepositions as used in (Dahlmeier et al., 2012).
We only deal with preposition replacement but not
preposition insertion or deletion. For noun number
correction, the classes are singular and plural.
Punctuation, subject-verb agreement (SVA),
and verb form errors are corrected using rule-
based classifiers. For SVA errors, we assume that
noun number errors have already been corrected
by classifiers earlier in the pipeline. Hence, only
the verb is corrected when an SVA error is de-
tected. For verb form errors, we change a verb into
its base form if it is preceded by a modal verb, and
we change it into the past participle form if it is
preceded by has, have, or had.
The spelling corrector uses Jazzy, an open
source Java spell-checker
1
. We filter the sugges-
tions given by Jazzy using a language model. We
accept a suggestion from Jazzy only if the sugges-
tion increases the language model score of the sen-
tence.
1
http://jazzy.sourceforge.net/
953
3.2 Statistical Machine Translation
The other two component systems are based
on phrase-based statistical machine translation
(Koehn et al., 2003). It follows the well-
known log-linear model formulation (Och and
Ney, 2002):
e? = argmax
e
P (e|f)
= argmax
e
exp
(
M
?
m=1
?
m
h
m
(e, f)
)
(1)
where f is the input sentence, e is the corrected
output sentence, h
m
is a feature function, and ?
m
is its weight. The feature functions include a trans-
lation model learned from a sentence-aligned par-
allel corpus and a language model learned from a
large English corpus. More feature functions can
be integrated into the log-linear model. A decoder
finds the best correction e? that maximizes Equa-
tion 1 above.
The parallel corpora that we use to train
the translation model come from two different
sources. The first corpus is NUCLE (Dahlmeier et
al., 2013), containing essays written by students at
the National University of Singapore (NUS) which
have been manually corrected by English instruc-
tors at NUS. The other corpus is collected from
the language exchange social networking website
Lang-8. We develop two versions of SMT sys-
tems: one with two phrase tables trained on NU-
CLE and Lang-8 separately (S1), and the other
with a single phrase table trained on the concate-
nation of NUCLE and Lang-8 data (S2). Multiple
phrase tables are used with alternative decoding
paths (Birch et al., 2007). We add a word-level
Levenshtein distance feature in the phrase table
used by S2, similar to (Felice et al., 2014; Junczys-
Dowmunt and Grundkiewicz, 2014). This feature
is not included in S1.
4 System Combination
We use MEMT (Heafield and Lavie, 2010) to
combine the outputs of our systems. MEMT uses
METEOR (Banerjee and Lavie, 2005) to perform
alignment of each pair of outputs from the compo-
nent systems. The METEOR matcher can identify
exact matches, words with identical stems, syn-
onyms, and unigram paraphrases.
MEMT uses an approach similar to the confu-
sion network approach in SMT system combina-
tion. The difference is that it performs alignment
on the outputs of every pair of component systems,
so it does not need to choose a single backbone.
As MEMT does not choose any single system out-
put as its backbone, it can consider the output of
each component system in a symmetrical manner.
This increases word order flexibility, as choosing
a single hypothesis as the backbone will limit the
number of possible word order permutations.
After creating pairwise alignments using ME-
TEOR, the alignments form a confusion network.
MEMT will then perform a beam search over this
graph to find the one-best hypothesis. The search
is carried out from left to right, one word at a time,
creating a partial hypothesis. During beam search,
it can freely switch among the component sys-
tems, combining the outputs together into a sen-
tence. When it adds a word to its hypothesis, all
the words aligned to it in the other systems are also
marked as ?used?. If it switches to another input
sentence, it has to use the first ?unused? word in
that sentence. This is done to make sure that ev-
ery aligned word in the sentences is used. In some
cases, a heuristic could be used to allow skipping
over some words (Heafield et al., 2009).
During beam search, MEMT uses a few features
to score the hypotheses (both partial hypotheses
and full hypotheses):
? Length The number of tokens in a hypoth-
esis. It is useful to normalize the impact of
sentence length.
? Language model Log probability from a lan-
guage model. It is especially useful in main-
taining sentence fluency.
? Backoff The average n-gram length found in
the language model.
? Match The number of n-gram matches be-
tween the outputs of the component systems
and the hypothesis, counted for small order
n-grams.
The weights of these features are tuned using Z-
MERT (Zaidan, 2009) on a development set.
This system combination approach has a few
advantages in grammatical error correction. ME-
TEOR not only can match words with exact
matches, but also words with identical stems, syn-
onyms, and unigram paraphrases. This means that
it can deal with word form, noun number, and verb
form corrections that share identical stems, as well
954
Data set # sentences # source tokens
NUCLE 57,151 1,161,567
Lang-8 1,114,139 12,945,666
CoNLL-2013 1,381 29,207
CoNLL-2014 1,312 30,144
English
Wikipedia
86,992,889 1,778,849,655
Table 2: Statistics of the data sets.
as word choice corrections (with synonyms and
unigram paraphrases). Also, MEMT uses a lan-
guage model feature to maintain sentence fluency,
favoring grammatical output sentences.
In this paper, we combine the pipeline system
P1 (Table 1) with the SMT system S1, and also
combine P2 with S2. The two component sys-
tems in each pair have comparable performance.
For our final system, we also combine all four sys-
tems together.
5 Experiments
Our approach is evaluated in the context of the
CoNLL-2014 shared task on grammatical error
correction. Specific details of the shared task can
be found in the overview paper (Ng et al., 2014),
but we summarize the most important details rele-
vant to our study here.
5.1 Data
We use NUCLE version 3.2 (Dahlmeier et al.,
2013), the official training data of the CoNLL-
2014 shared task, to train our component systems.
The grammatical errors in this corpus are catego-
rized into 28 different error types. We also use the
?Lang-8 Corpus of Learner English v1.0?
2
(Tajiri
et al., 2012) to obtain additional learner data. En-
glish Wikipedia
3
is used for language modeling
and collecting n-gram counts. All systems are
tuned on the CoNLL-2013 test data (which serves
as the development data set) and tested on the
CoNLL-2014 test data. The statistics of the data
sets can be found in Table 2.
5.2 Evaluation
System performance is evaluated based on pre-
cision, recall, and F
0.5
(which weights precision
twice as much as recall). Given a set of n sen-
tences, where g
i
is the set of gold-standard edits
2
http://cl.naist.jp/nldata/lang-8/
3
http://dumps.wikimedia.org/enwiki/20140102/enwiki-
20140102-pages-articles.xml.bz2
for sentence i, and e
i
is the set of system edits for
sentence i, precision, recall, and F
0.5
are defined
as follows:
P =
?
n
i=1
|g
i
? e
i
|
?
n
i=1
|e
i
|
(2)
R =
?
n
i=1
|g
i
? e
i
|
?
n
i=1
|g
i
|
(3)
F
0.5
=
(1 + 0.5
2
)?R? P
R+ 0.5
2
? P
(4)
where the intersection between g
i
and e
i
for sen-
tence i is defined as
g
i
? e
i
= {e ? e
i
|?g ? g
i
,match(g, e)} (5)
The official scorer for the shared task was
the MaxMatch (M
2
) scorer
4
(Dahlmeier and Ng,
2012b). The scorer computes the sequence of sys-
tem edits between a source sentence and a system
hypothesis that achieves the maximal overlap with
the gold-standard edits. Like CoNLL-2014, F
0.5
is used instead of F
1
to emphasize precision. For
statistical significance testing, we use the sign test
with bootstrap re-sampling on 100 samples.
5.3 Pipeline System
We use ClearNLP
5
for POS tagging and depen-
dency parsing, and OpenNLP for chunking
6
. We
use the WordNet (Fellbaum, 1998) morphology
software to generate singular and plural word sur-
face forms.
The article, preposition, and noun number cor-
rectors use the classifier approach to correct errors.
Each classifier is trained using multi-class confi-
dence weighted learning on the NUCLE and Lang-
8 corpora. The classifier threshold is tuned using a
simple grid search on the development data set for
each class of a classifier.
5.4 SMT System
The system is trained using Moses (Koehn et al.,
2007), with Giza++ (Och and Ney, 2003) for word
alignment. The translation table is trained using
the ?parallel? corpora of NUCLE and Lang-8. The
table contains phrase pairs of maximum length
seven. We include five standard parameters in the
translation table: forward and reverse phrase trans-
lations, forward and reverse lexical translations,
4
http://www.comp.nus.edu.sg/?nlp/sw/m2scorer.tar.gz
5
https://code.google.com/p/clearnlp/
6
http://opennlp.apache.org/
955
and phrase penalty. We further add a word-level
Levenshtein distance feature for S2.
We do not use any reordering model in our sys-
tem. The intuition is that most error types do not
involve long-range reordering and local reorder-
ing can be easily captured in the phrase translation
table. The distortion limit is set to 0 to prohibit
reordering during hypothesis generation.
We build two 5-gram language models using the
corrected side of NUCLE and English Wikipedia.
The language models are estimated using the
KenLM toolkit (Heafield et al., 2013) with mod-
ified Kneser-Ney smoothing. These two language
models are used as separate feature functions in
the log-linear model. Finally, they are binarized
into a probing data structure (Heafield, 2011).
Tuning is done on the development data set with
MERT (Och, 2003). We use BLEU (Papineni et
al., 2002) as the tuning metric, which turns out to
work well in our experiment.
5.5 Combined System
We use an open source MEMT implementation
by Heafield and Lavie (2010) to combine the out-
puts of our systems. Parameters are set to the val-
ues recommended by (Heafield and Lavie, 2010):
a beam size of 500, word skipping using length
heuristic with radius 5, and with the length nor-
malization option turned off. We use five match-
ing features for each system: the number of exact
unigram and bigram matches between hypotheses
and the number of matches in terms of stems, syn-
onyms, or paraphrases for unigrams, bigrams, and
trigrams. We use the Wikipedia 5-gram language
model in this experiment.
We tune the combined system on the develop-
ment data set. The test data is input into both
the pipeline and SMT system respectively and the
output from each system is then matched using
METEOR (Banerjee and Lavie, 2005). Feature
weights, based on BLEU, are then tuned using Z-
MERT (Zaidan, 2009). We repeat this process five
times and use the weights that achieve the best
score on the development data set in our final com-
bined system.
5.6 Results
Our experimental results using the CoNLL-2014
test data as the test set are shown in Table 3. Each
system is evaluated against the same gold standard
human annotations. As recommended in Ng et al.
(2014), we do not use the revised gold standard to
System P R F
0.5
Pipeline
P1 40.24 23.99 35.44
P2 39.93 22.77 34.70
SMT
S1 57.90 14.16 35.80
S2 62.11 12.54 34.69
Combined
P1+S1 53.85 17.65 38.19
P2+S2 56.92 16.22 37.90
P1+P2+S1+S2 53.55 19.14 39.39
Top 4 Systems in CoNLL-2014
CAMB 39.71 30.10 37.33
CUUI 41.78 24.88 36.79
AMU 41.62 21.40 35.01
POST 34.51 21.73 30.88
Table 3: Performance of the pipeline, SMT,
and combined systems on the CoNLL-2014 test
set. All improvements of combined systems over
their component systems are statistically signifi-
cant (p < 0.01). The differences between P1 and
S1 and between P2 and S2 are not statistically sig-
nificant.
ensure a fairer evaluation (i.e., without using alter-
native answers).
First, we can see that both the pipeline and
SMT systems individually achieve relatively good
results that are comparable with the third high-
est ranking participant in the CoNLL-2014 shared
task. It is worth noting that the pipeline systems
only target the seven most common error types,
yet still perform well in an all-error-type setting.
In general, the pipeline systems have higher recall
but lower precision than the SMT systems.
The pipeline system is also sensitive to the or-
der in which corrections are applied; for example
applying noun number corrections before article
corrections results in a better score. This means
that there is definitely some interaction between
grammatical errors and, for instance, the phrase a
houses can be corrected to a house or houses de-
pending on the order of correction.
We noticed that the performance of the SMT
system could be improved by using multiple trans-
lation models. This is most likely due to domain
differences between the NUCLE and Lang-8 cor-
pus, e.g., text genres, writing style, topics, etc.
Note also that the Lang-8 corpus is more than
10 times larger than the NUCLE corpus, so there
956
is some benefit from training and weighting two
translation tables separately.
The performance of the pipeline system P1 is
comparable to that of the SMT system S1, and
likewise the performance of P2 is comparable to
that of S2. The differences between them are not
statistically significant, making it appropriate to
combine their respective outputs.
Every combined system achieves a better result
than its component systems. In every combina-
tion, there is some improvement in precision over
the pipeline systems, and some improvement in re-
call over the SMT systems. The combination of
the better component systems (P1+S1) is also sta-
tistically significantly better than the combination
of the other component systems (P2+S2). Com-
bining all four component systems yields an even
better result of 39.39% F
0.5
, which is even better
than the CoNLL-2014 shared task winner. This is
significant because the individual component sys-
tems barely reached the score of the third highest
ranking participant before they were combined.
6 Discussion
In this section, we discuss the strengths and weak-
nesses of the pipeline and SMT systems, and show
how system output combination improves perfor-
mance. Specifically, we compare P1, S1, and
P1+S1, although the discussion also applies to P2,
S2, and P2+S2.
Type performance. We start by computing the
recall for each of the 28 error types achieved by
each system. This computation is straightforward
as each gold standard edit is also annotated with
error type. On the other hand, precision, as men-
tioned in the overview paper (Ng et al., 2014), is
much harder to compute because systems typically
do not categorize their corrections by error type.
Although it may be possible to compute the pre-
cision for each error type in the pipeline system
(since we know which correction was proposed by
which classifier), this is more difficult to do in the
SMT and combined system, where we would need
to rely on heuristics which are more prone to er-
rors. As a result, we decided to analyze a sample
of 200 sentences by hand for a comparatively more
robust comparison. The results can be seen in Ta-
ble 4.
We observe that the pipeline system has a higher
recall than the SMT system for the following er-
ror types: ArtOrDet, Mec, Nn, Prep, SVA, Vform,
and Vt. Conversely, the SMT system generally has
a higher precision than the pipeline system. The
combined system usually has slightly lower pre-
cision than the SMT system, but higher than the
pipeline system, and slightly higher recall than the
SMT system but lower than the pipeline system.
In some cases however, like for Vform correction,
both precision and recall increase.
The combined system can also make use of cor-
rections which are only corrected in one of the
systems. For example, it corrects both Wform
and Pform errors, which are only corrected by the
SMT system, and SVA errors, which are only cor-
rected by the pipeline system.
Error analysis. For illustration on how sys-
tem combination helps, we provide example out-
put from the pipeline system P1, SMT system
S1, and the combined system P1+S1 in Table 5.
We illustrate three common scenarios where sys-
tem combination helps: the first is when P1 per-
forms better than S1, and the combined system
chooses the corrections made by P1, the second is
the opposite where S1 performs better than P1 and
the combined system chooses S1, and the last is
when the combined system combines the correc-
tions made by P1 and S1 to produce output better
than both P1 and S1.
7 Additional System Combination
Experiments
We further evaluate our system combination ap-
proach by making use of the corrected system out-
puts of 12 participating teams in the CoNLL-2014
shared task, which are publicly available on the
shared task website.
7
Specifically, we combined
the system outputs of the top 2, 3, . . . , 12 CoNLL-
2014 shared task teams and computed the results.
In our earlier experiments, the CoNLL-2013
test data was used as the development set. How-
ever, the participants? outputs for this 2013 data
are not available. Therefore, we split the CoNLL-
2014 test data into two parts: the first 500 sen-
tences for the development set and the remaining
812 sentences for the test set. We then tried com-
bining the n best performing systems, for n =
2, 3, . . . , 12. Other than the data, the experimen-
tal setup is the same as that described in Sec-
tion 5.5. Table 6 shows the ranking of the par-
ticipants on the 812 test sentences (without alter-
7
http://www.comp.nus.edu.sg/?nlp/conll14st/
official submissions.tar.gz
957
T
y
p
e
P
i
p
e
l
i
n
e
S
M
T
C
o
m
b
i
n
e
d
T
P
F
N
F
P
P
R
F
0
.
5
T
P
F
N
F
P
P
R
F
0
.
5
T
P
F
N
F
P
P
R
F
0
.
5
A
r
t
O
r
D
e
t
1
3
3
8
5
4
1
9
.
4
0
2
5
.
4
9
2
0
.
3
8
1
1
3
5
7
6
1
.
1
1
2
3
.
9
1
4
6
.
6
1
1
6
3
0
2
1
4
3
.
2
4
3
4
.
7
8
4
1
.
2
4
C
i
t
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
M
e
c
2
7
3
5
4
3
3
8
.
5
7
4
3
.
5
5
3
9
.
4
7
1
8
4
7
8
6
9
.
2
3
2
7
.
6
9
5
3
.
2
5
2
0
4
7
1
0
6
6
.
6
7
2
9
.
8
5
5
3
.
4
8
N
n
2
7
1
5
4
1
3
9
.
7
1
6
4
.
2
9
4
2
.
9
9
5
2
3
3
6
2
.
5
0
1
7
.
8
6
4
1
.
6
7
1
1
2
1
7
6
1
.
1
1
3
4
.
3
8
5
2
.
8
8
N
p
o
s
0
1
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
9
0
0
.
0
0
0
.
0
0
0
.
0
0
0
9
0
0
.
0
0
0
.
0
0
0
.
0
0
O
t
h
e
r
s
0
1
0
0
.
0
0
0
.
0
0
0
.
0
0
0
3
0
0
.
0
0
0
.
0
0
0
.
0
0
0
3
0
0
.
0
0
0
.
0
0
0
.
0
0
P
f
o
r
m
0
7
0
0
.
0
0
0
.
0
0
0
.
0
0
1
5
0
1
0
0
.
0
0
1
6
.
6
7
5
0
.
0
0
1
5
0
1
0
0
.
0
0
1
6
.
6
7
5
0
.
0
0
P
r
e
f
1
1
0
1
1
8
.
3
3
9
.
0
9
8
.
4
7
0
9
0
0
.
0
0
0
.
0
0
0
.
0
0
0
9
0
0
.
0
0
0
.
0
0
0
.
0
0
P
r
e
p
1
2
2
5
3
2
2
7
.
2
7
3
2
.
4
3
2
8
.
1
7
4
2
6
1
8
0
.
0
0
1
3
.
3
3
4
0
.
0
0
4
2
7
3
5
7
.
1
4
1
2
.
9
0
3
3
.
9
0
R
l
o
c
?
4
1
6
1
8
0
.
0
0
2
0
.
0
0
5
0
.
0
0
0
1
6
0
0
.
0
0
0
.
0
0
0
.
0
0
0
1
6
0
0
.
0
0
0
.
0
0
0
.
0
0
S
f
r
a
g
0
1
0
0
.
0
0
0
.
0
0
0
.
0
0
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
m
o
d
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
p
a
r
0
1
0
0
.
0
0
0
.
0
0
0
.
0
0
0
3
0
0
.
0
0
0
.
0
0
0
.
0
0
0
2
0
0
.
0
0
0
.
0
0
0
.
0
0
S
r
u
n
0
2
0
0
.
0
0
0
.
0
0
0
.
0
0
0
1
0
0
.
0
0
0
.
0
0
0
.
0
0
0
1
0
0
.
0
0
0
.
0
0
0
.
0
0
S
s
u
b
0
1
2
0
0
.
0
0
0
.
0
0
0
.
0
0
1
1
2
1
5
0
.
0
0
7
.
6
9
2
3
.
8
1
1
1
2
1
5
0
.
0
0
7
.
6
9
2
3
.
8
1
S
V
A
4
1
1
6
4
0
.
0
0
2
6
.
6
7
3
6
.
3
6
0
1
4
0
0
.
0
0
0
.
0
0
0
.
0
0
1
1
4
0
1
0
0
.
0
0
6
.
6
7
2
6
.
3
2
T
r
a
n
s
1
1
5
0
1
0
0
.
0
0
6
.
2
5
2
5
.
0
0
0
1
5
0
0
.
0
0
0
.
0
0
0
.
0
0
0
1
5
0
0
.
0
0
0
.
0
0
0
.
0
0
U
m
1
4
0
1
0
0
.
0
0
2
0
.
0
0
5
5
.
5
6
0
5
0
0
.
0
0
0
.
0
0
0
.
0
0
0
5
0
0
.
0
0
0
.
0
0
0
.
0
0
V
0
0
3
0
0
.
0
0
0
.
0
0
0
.
0
0
0
3
3
0
.
0
0
0
.
0
0
0
.
0
0
0
3
3
0
.
0
0
0
.
0
0
0
.
0
0
V
f
o
r
m
4
1
2
4
5
0
.
0
0
2
5
.
0
0
4
1
.
6
7
3
1
3
2
6
0
.
0
0
1
8
.
7
5
4
1
.
6
7
5
1
2
2
7
1
.
4
3
2
9
.
4
1
5
5
.
5
6
V
m
0
2
0
0
.
0
0
0
.
0
0
0
.
0
0
0
5
0
0
.
0
0
0
.
0
0
0
.
0
0
0
6
0
0
.
0
0
0
.
0
0
0
.
0
0
V
t
2
1
6
1
6
6
.
6
7
1
1
.
1
1
3
3
.
3
3
0
1
7
0
0
.
0
0
0
.
0
0
0
.
0
0
0
1
7
0
0
.
0
0
0
.
0
0
0
.
0
0
W
a
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
W
c
i
1
6
0
0
1
0
0
.
0
0
1
.
6
4
7
.
6
9
3
5
2
1
7
5
.
0
0
5
.
4
5
2
1
.
1
3
3
5
5
1
7
5
.
0
0
5
.
1
7
2
0
.
2
7
W
f
o
r
m
0
1
1
0
0
.
0
0
0
.
0
0
0
.
0
0
2
1
0
2
5
0
.
0
0
1
6
.
6
7
3
5
.
7
1
2
1
0
2
5
0
.
0
0
1
6
.
6
7
3
5
.
7
1
W
O
a
d
v
0
0
0
0
.
0
0
0
.
0
0
0
.
0
0
1
1
0
1
0
0
.
0
0
5
0
.
0
0
8
3
.
3
3
0
1
0
0
.
0
0
0
.
0
0
0
.
0
0
W
O
i
n
c
0
5
0
0
.
0
0
0
.
0
0
0
.
0
0
0
4
1
0
.
0
0
0
.
0
0
0
.
0
0
0
4
0
0
.
0
0
0
.
0
0
0
.
0
0
W
t
o
n
e
0
6
0
0
.
0
0
0
.
0
0
0
.
0
0
0
2
0
0
.
0
0
0
.
0
0
0
.
0
0
0
2
0
0
.
0
0
0
.
0
0
0
.
0
0
T
a
b
l
e
4
:
T
r
u
e
p
o
s
i
t
i
v
e
s
(
T
P
)
,
f
a
l
s
e
n
e
g
a
t
i
v
e
s
(
F
N
)
,
f
a
l
s
e
p
o
s
i
t
i
v
e
s
(
F
P
)
,
p
r
e
c
i
s
i
o
n
(
P
)
,
r
e
c
a
l
l
(
R
)
,
a
n
d
F
0
.
5
(
i
n
%
)
f
o
r
e
a
c
h
e
r
r
o
r
t
y
p
e
w
i
t
h
o
u
t
a
l
t
e
r
n
a
t
i
v
e
a
n
s
w
e
r
s
,
i
n
d
i
c
a
t
i
n
g
h
o
w
w
e
l
l
e
a
c
h
s
y
s
t
e
m
p
e
r
f
o
r
m
s
a
g
a
i
n
s
t
a
p
a
r
t
i
c
u
l
a
r
e
r
r
o
r
t
y
p
e
.
958
System Example sentence
Source Nowadays , the use of the sociall media platforms is a commonplace in our lives .
P1 Nowadays , the use of social media platforms is a commonplace in our lives .
S1 Nowadays , the use of the sociall media platforms is a commonplace in our lives .
P1+S1 Nowadays , the use of social media platforms is a commonplace in our lives .
Gold Nowadays , the use of social media platforms is commonplace in our lives .
Source Human has their own rights and privacy .
P1 Human has their own rights and privacy .
S1 Humans have their own rights and privacy .
P1+S1 Humans have their own rights and privacy .
Gold Humans have their own rights and privacy .
Source People that living in the modern world really can not live without the social media sites .
P1 People that living in the modern world really can not live without social media sites .
S1 People living in the modern world really can not live without the social media sites .
P1+S1 People living in the modern world really can not live without social media sites .
Gold People living in the modern world really can not live without social media sites .
Table 5: Example output from three systems.
System P R F
0.5
CUUI 44.62 27.54 39.69
CAMB 39.93 31.02 37.76
AMU 40.77 21.31 34.47
POST 38.88 23.06 34.19
NTHU 36.30 20.50 31.45
RAC 32.38 13.62 25.39
PKU 30.14 13.12 23.93
UMC 29.03 12.88 23.21
SJTU 32.04 5.43 16.18
UFC 76.92 2.49 11.04
IPN 11.99 2.88 7.34
IITB 28.12 1.53 6.28
Table 6: Performance of each participant when
evaluated on 812 sentences from CoNLL-2014
test data.
native answers). Note that since we use a subset of
the original CoNLL-2014 test data for testing, the
ranking is different from the official CoNLL-2014
ranking.
Table 7 shows the results of system combina-
tion in terms of increasing numbers of top sys-
tems. We observe consistent improvements in F
0.5
when we combine more system outputs, up to 5
best performing systems. When combining 6 or
more systems, the performance starts to fluctu-
ate and degrade. An important observation is that
when we perform system combination, it is more
effective, in terms of F
0.5
, to combine a handful
of high-quality system outputs than many outputs
# systems P R F
0.5
2 44.72 29.78 40.64
3 56.24 25.04 45.02
4 59.16 23.63 45.48
5 63.41 24.09 47.80
6 65.02 19.54 44.37
7 64.95 18.13 42.83
8 66.09 14.70 38.90
9 70.22 14.81 40.16
10 69.72 13.67 38.31
11 70.23 14.23 39.30
12 69.72 11.82 35.22
Table 7: Performance with different numbers of
combined top systems.
of variable quality. Precision tends to increase as
more systems are combined although recall tends
to decrease. This indicates that combining multi-
ple systems can produce a grammatical error cor-
rection system with high precision, which is useful
in a practical application setting where high preci-
sion is desirable. Figure 1 shows how the perfor-
mance varies as the number of combined systems
increases.
8 Conclusion
We have presented a system combination ap-
proach for grammatical error correction using
MEMT. Our approach combines the outputs from
two of the most common paradigms in GEC: the
pipeline and statistical machine translation ap-
959
2 4 6 8 10 12
20
40
60
Number of combined systems
P
e
r
f
o
r
m
a
n
c
e
P R F
0.5
Figure 1: Performance in terms of precision (P ),
recall (R), and F
0.5
versus the number of com-
bined top systems.
proach. We created two variants of the pipeline
and statistical machine translation approaches and
showed that system combination can be used to
combine their outputs together to yield a superior
system.
Our best combined system achieves an F
0.5
score of 39.39% on the official CoNLL 2014 test
set without alternative answers, higher than the top
participating team in CoNLL 2014 on this data
set. We achieved this by using component systems
which were individually weaker than the top three
systems that participated in the shared task.
Acknowledgments
This research is supported by Singapore Min-
istry of Education Academic Research Fund Tier
2 grant MOE2013-T2-1-150. We would like to
thank Christopher Bryant for his comments on this
paper.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 9?16.
Ond?rej Bojar, Milo?s Ercegov?cevi?c, Martin Popel, and
Omar Zaidan. 2011. A grain of salt for the WMT
manual evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
1?11.
Chris Brockett, William B Dolan, and Michael Ga-
mon. 2006. Correcting ESL errors using phrasal
SMT techniques. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics, pages 249?256.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28.
Martin Chodorow, Joel R Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In Proceedings of the Fourth ACL-
SIGSEM Workshop on Prepositions, pages 25?30.
Koby Crammer, Mark Dredze, and Alex Kulesza.
2009. Multi-class confidence weighted algorithms.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
496?504.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical error correction with alternating structure opti-
mization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 915?923.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
568?578.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 568?572.
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng
Ng. 2012. NUS at the HOO 2012 shared task. In
Proceedings of the Seventh Workshop on the Inno-
vative Use of NLP for Building Educational Appli-
cations, pages 216?224.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English: The NUS Corpus of Learner English. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 22?31.
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text massaging for computational linguistics
as a new shared task. In Proceedings of the 6th Inter-
national Natural Language Generation Conference,
pages 263?267.
960
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Pro-
ceedings of the Seventh Workshop on the Innovative
Use of NLP for Building Educational Applications,
pages 54?62.
Mariano Felice, Zheng Yuan, ?istein E. Andersen, He-
len Yannakoudakis, and Ekaterina Kochmar. 2014.
Grammatical error correction using hybrid systems
and type filtering. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 15?24.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing: A meta-classifier
approach. In Proceedings of the 2010 Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 163?
171.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
Kenneth Heafield and Alon Lavie. 2010. Combining
machine translation output with open source: The
Carnegie Mellon multi-engine machine translation
scheme. The Prague Bulletin of Mathematical Lin-
guistics, 93:27?36.
Kenneth Heafield, Greg Hanneman, and Alon Lavie.
2009. Machine translation system combination with
flexible word ordering. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
56?60.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197.
Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2014. The AMU system in the CoNLL-2014
shared task: Grammatical error correction by data-
intensive and feature-rich statistical machine trans-
lation. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 25?33.
Kevin Knight and Ishwar Chander. 1994. Auto-
mated postediting of documents. In Proceedings of
the Twelfth National Conference on Artificial Intel-
ligence, pages 779?784.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL 2007 Demo and Poster Ses-
sions, pages 177?180.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revi-
sion log of language learning SNS for automated
Japanese error correction of second language learn-
ers. In Proceedings of the Fifth International Joint
Conference on Natural Language Processing, pages
147?155.
Tomoya Mizumoto, Yuta Hayashibe, Mamoru Ko-
machi, Masaaki Nagata, and Yuji Matsumoto. 2012.
The effect of learner corpus size in grammatical er-
ror correction of ESL writings. In Proceedings of
the 24th International Conference on Computational
Linguistics, pages 863?872.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 shared task on grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1?12.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 shared task
on grammatical error correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 1?14.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
961
Antti-Veikko I. Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie J.
Dorr. 2007a. Combining outputs from multiple ma-
chine translation systems. In Proceedings of the
2007 Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 228?235.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system
combination for machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics, pages 312?319.
Alla Rozovskaya and Dan Roth. 2011. Algorithm
selection and model adaptation for ESL correction
tasks. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 924?933.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The University of Illinois
system in the CoNLL-2013 shared task. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 13?19.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
Dan Roth, and Nizar Habash. 2014a. The Illinois-
Columbia system in the CoNLL-2014 shared task.
In Proceedings of the Eighteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 34?42.
Alla Rozovskaya, Dan Roth, and Vivek Srikumar.
2014b. Correcting grammatical verb errors. In Pro-
ceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 358?367.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and aspect error correction
for ESL learners using global context. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Short Papers, pages
198?202.
Joel R Tetreault and Martin Chodorow. 2008. The
ups and downs of preposition error detection in
ESL writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 865?872.
Yuanbin Wu and Hwee Tou Ng. 2013. Grammat-
ical error correction using integer linear program-
ming. In Proceedings of the 51tst Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1456?1465.
Omar Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
962
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1?14,
Baltimore, Maryland, 26-27 July 2014.
c
?2014 Association for Computational Linguistics
The CoNLL-2014 Shared Task on Grammatical Error Correction
Hwee Tou Ng
1
Siew Mei Wu
2
Ted Briscoe
3
Christian Hadiwinoto
1
Raymond Hendy Susanto
1
Christopher Bryant
1
1
Department of Computer Science, National University of Singapore
{nght,chrhad,raymondhs,bryant}@comp.nus.edu.sg
2
Centre for English Language Communication, National University of Singapore
elcwusm@nus.edu.sg
3
Computer Laboratory, University of Cambridge
Ted.Briscoe@cl.cam.ac.uk
Abstract
The CoNLL-2014 shared task was devoted
to grammatical error correction of all error
types. In this paper, we give the task defi-
nition, present the data sets, and describe
the evaluation metric and scorer used in
the shared task. We also give an overview
of the various approaches adopted by the
participating teams, and present the eval-
uation results. Compared to the CoNLL-
2013 shared task, we have introduced the
following changes in CoNLL-2014: (1)
A participating system is expected to de-
tect and correct grammatical errors of all
types, instead of just the five error types
in CoNLL-2013; (2) The evaluation metric
was changed from F
1
to F
0.5
, to empha-
size precision over recall; and (3) We have
two human annotators who independently
annotated the test essays, compared to just
one human annotator in CoNLL-2013.
1 Introduction
Grammatical error correction is the shared task of
the Eighteenth Conference on Computational Nat-
ural Language Learning in 2014 (CoNLL-2014).
In this task, given an English essay written by a
learner of English as a second language, the goal
is to detect and correct the grammatical errors of
all error types present in the essay, and return the
corrected essay.
This task has attracted much recent research in-
terest, with two shared tasks Helping Our Own
(HOO) organized in 2011 and 2012 (Dale and Kil-
garriff, 2011; Dale et al., 2012), and a CoNLL
shared task on grammatical error correction orga-
nized in 2013 (Ng et al., 2013). In contrast to
previous CoNLL shared tasks which focused on
particular subtasks of natural language process-
ing, such as named entity recognition, semantic
role labeling, dependency parsing, or coreference
resolution, grammatical error correction aims at
building a complete end-to-end application. This
task is challenging since for many error types,
current grammatical error correction systems do
not achieve high performance and much research
is still needed. Also, tackling this task has far-
reaching impact, since it is estimated that hun-
dreds of millions of people worldwide are learn-
ing English and they benefit directly from an auto-
mated grammar checker.
The CoNLL-2014 shared task provides a forum
for participating teams to work on the same gram-
matical error correction task, with evaluation on
the same blind test set using the same evaluation
metric and scorer. This overview paper contains a
detailed description of the shared task, and is orga-
nized as follows. Section 2 provides the task def-
inition. Section 3 describes the annotated training
data provided and the blind test data. Section 4 de-
scribes the evaluation metric and the scorer. Sec-
tion 5 lists the participating teams and outlines the
approaches to grammatical error correction used
by the teams. Section 6 presents the results of the
shared task, including a discussion on cross anno-
tator comparison. Section 7 concludes the paper.
2 Task Definition
The goal of the CoNLL-2014 shared task is to
evaluate algorithms and systems for automati-
cally detecting and correcting grammatical errors
1
present in English essays written by second lan-
guage learners of English. Each participating
team is given training data manually annotated
with corrections of grammatical errors. The test
data consists of new, blind test essays. Prepro-
cessed test essays, which have been sentence-
segmented and tokenized, are also made available
to the participating teams. Each team is to submit
its system output consisting of the automatically
corrected essays, in sentence-segmented and tok-
enized form.
Grammatical errors consist of many different
types, including articles or determiners, preposi-
tions, noun form, verb form, subject-verb agree-
ment, pronouns, word choice, sentence structure,
punctuation, capitalization, etc. However, most
prior published research on grammatical error cor-
rection only focuses on a small number of fre-
quently occurring error types, such as article and
preposition errors (Han et al., 2006; Gamon, 2010;
Rozovskaya and Roth, 2010; Tetreault et al., 2010;
Dahlmeier and Ng, 2011b). Article and preposi-
tion errors were also the only error types featured
in the HOO 2012 shared task. Likewise, although
all error types were included in the HOO 2011
shared task, almost all participating teams dealt
with article and preposition errors only (besides
spelling and punctuation errors). In the CoNLL-
2013 shared task, the error types were extended
to include five error types, comprising article or
determiner, preposition, noun number, verb form,
and subject-verb agreement. Other error types
such as word choice errors (Dahlmeier and Ng,
2011a) were not dealt with.
In the CoNLL-2014 shared task, it was felt that
the community is now ready to deal with all er-
ror types. Table 1 shows examples of the 28 error
types in the CoNLL-2014 shared task.
Since there are 28 error types in our shared task
compared to two in HOO 2012 and five in CoNLL-
2013, there is a greater chance of encountering
multiple, interacting errors in a sentence in our
shared task. This increases the complexity of our
shared task. To illustrate, consider the following
sentence:
Social network plays a role in providing
and also filtering information.
The noun number error networks needs to be cor-
rected (network ? networks). This necessitates
the correction of a subject-verb agreement error
(plays ? play). A pipeline system in which cor-
rections for subject-verb agreement errors occur
strictly before corrections for noun number errors
would not be able to arrive at a fully corrected
sentence for this example. The ability to correct
multiple, interacting errors is thus necessary in our
shared task. The recent work of Dahlmeier and Ng
(2012a) and Wu and Ng (2013), for example, is
designed to deal with multiple, interacting errors.
3 Data
This section describes the training and test data
released to each participating team in our shared
task.
3.1 Training Data
The training data provided in our shared task is
the NUCLE corpus, the NUS Corpus of Learner
English (Dahlmeier et al., 2013). As noted by
(Leacock et al., 2010), the lack of a manually an-
notated and corrected corpus of English learner
texts has been an impediment to progress in gram-
matical error correction, since it prevents com-
parative evaluations on a common benchmark test
data set. NUCLE was created precisely to fill this
void. It is a collection of 1,414 essays written
by students at the National University of Singa-
pore (NUS) who are non-native speakers of En-
glish. The essays were written in response to some
prompts, and they cover a wide range of topics,
such as environmental pollution, health care, etc.
The grammatical errors in these essays have been
hand-corrected by professional English instructors
at NUS. For each grammatical error instance, the
start and end character offsets of the erroneous text
span are marked, and the error type and the cor-
rection string are provided. Manual annotation is
carried out using a graphical user interface specif-
ically built for this purpose. The error annotations
are saved as stand-off annotations, in SGML for-
mat.
To illustrate, consider the following sentence at
the start of the sixth paragraph of an essay:
Nothing is absolute right or wrong.
There is a word form error (absolute? absolutely)
in this sentence. The error annotation, also called
correction or edit, in SGML format is shown in
Figure 1. start par (end par) denotes the
paragraph ID of the start (end) of the erroneous
2
Type Description Example
Vt Verb tense Medical technology during that time [is ? was] not advanced enough to
cure him.
Vm Verb modal Although the problem [would ? may] not be serious, people [would ?
might] still be afraid.
V0 Missing verb However, there are also a great number of people [who? who are] against
this technology.
Vform Verb form A study in 2010 [shown? showed] that patients recover faster when sur-
rounded by family members.
SVA Subject-verb agreement The benefits of disclosing genetic risk information [outweighs ? out-
weigh] the costs.
ArtOrDet Article or determiner It is obvious to see that [internet? the internet] saves people time and also
connects people globally.
Nn Noun number A carrier may consider not having any [child ? children] after getting
married.
Npos Noun possessive Someone should tell the [carriers? carrier?s] relatives about the genetic
problem.
Pform Pronoun form A couple should run a few tests to see if [their? they] have any genetic
diseases beforehand.
Pref Pronoun reference It is everyone?s duty to ensure that [he or she ? they] undergo regular
health checks.
Prep Preposition This essay will [discuss about? discuss] whether a carrier should tell his
relatives or not.
Wci Wrong collocation/idiom Early examination is [healthy? advisable] and will cast away unwanted
doubts.
Wa Acronyms After [WOWII ? World War II], the population of China decreased
rapidly.
Wform Word form The sense of [guilty? guilt] can be more than expected.
Wtone Tone (formal/informal) [It?s? It is] our family and relatives that bring us up.
Srun Run-on sentences,
comma splices
The issue is highly [debatable, a? debatable. A] genetic risk could come
from either side of the family.
Smod Dangling modifiers [Undeniable,? It is undeniable that] it becomes addictive when we spend
more time socializing virtually.
Spar Parallelism We must pay attention to this information and [assisting ? assist] those
who are at risk.
Sfrag Sentence fragment However, from the ethical point of view.
Ssub Subordinate clause This is an issue [needs? that needs] to be addressed.
WOinc Incorrect word order [Someone having what kind of disease?What kind of disease someone
has] is a matter of their own privacy.
WOadv Incorrect adjective/
adverb order
In conclusion, [personally I? I personally] feel that it is important to tell
one?s family members.
Trans Linking words/phrases It is sometimes hard to find [out? out if] one has this disease.
Mec Spelling, punctuation,
capitalization, etc.
This knowledge [maybe relavant? may be relevant] to them.
Rloc? Redundancy It is up to the [patient?s own choice? patient] to disclose information.
Cit Citation Poor citation practice.
Others Other errors An error that does not fit into any other category but can still be corrected.
Um Unclear meaning Genetic disease has a close relationship with the born gene. (i.e., no cor-
rection possible without further clarification.)
Table 1: The 28 error types in the shared task.
3
text span (paragraph ID starts from 0 by conven-
tion). start off (end off) denotes the char-
acter offset of the start (end) of the erroneous text
span (again, character offset starts from 0 by con-
vention). The error tag is Wform, and the correc-
tion string is absolutely.
The NUCLE corpus was first used in
(Dahlmeier and Ng, 2011b), and has been
publicly available for research purposes since
June 2011
1
. All instances of grammatical errors
are annotated in NUCLE.
To help participating teams in their prepara-
tion for the shared task, we also performed au-
tomatic preprocessing of the NUCLE corpus and
released the preprocessed form of NUCLE. The
preprocessing operations performed on the NU-
CLE essays include sentence segmentation and
word tokenization using the NLTK toolkit (Bird
et al., 2009), and part-of-speech (POS) tagging,
constituency and dependency tree parsing using
the Stanford parser (Klein and Manning, 2003;
de Marneffe et al., 2006). The error annotations,
which are originally at the character level, are
then mapped to error annotations at the word to-
ken level. Error annotations at the word token
level also facilitate scoring, as we will see in Sec-
tion 4, since our scorer operates by matching to-
kens. Note that although we released our own
preprocessed version of NUCLE, the participating
teams were however free to perform their own pre-
processing if they so preferred.
NUCLE release version 3.2 was used in the
CoNLL-2014 shared task. In this version, 17 es-
says were removed from the first release of NU-
CLE since these essays were duplicates with mul-
tiple annotations. In addition, in order to facilitate
the detection and correction of article/determiner
errors and preposition errors, we performed some
automatic mapping of error types in the original
NUCLE corpus to arrive at release version 3.2. Ng
et al. (2013) gives more details of how the map-
ping was carried out.
The statistics of the NUCLE corpus (release 3.2
version) are shown in Table 2. The distribution of
errors among all error types is shown in Table 3.
While the NUCLE corpus is provided in our
shared task, participating teams are free to not use
NUCLE, or to use additional resources and tools
in building their grammatical error correction sys-
tems, as long as these resources and tools are pub-
1
http://www.comp.nus.edu.sg/?nlp/corpora.html
Training data Test data
(NUCLE)
# essays 1,397 50
# sentences 57,151 1,312
# word tokens 1,161,567 30,144
Table 2: Statistics of training and test data.
licly available and not proprietary. For example,
participating teams are free to use the Cambridge
FCE corpus (Yannakoudakis et al., 2011; Nicholls,
2003) (the training data provided in HOO 2012
(Dale et al., 2012)) as additional training data.
3.2 Test Data
Similar to CoNLL-2013, 25 NUS students, who
are non-native speakers of English, were recruited
to write new essays to be used as blind test data
in the shared task. Each student wrote two essays
in response to the two prompts shown in Table 4,
one essay per prompt. The first prompt was also
used in the NUCLE training data, but the second
prompt is entirely new and not used previously. As
a result, 50 new test essays were collected. The
statistics of the test essays are also shown in Ta-
ble 2.
Error annotation on the test essays was carried
out independently by two native speakers of En-
glish. One of them is a lecturer at the NUS Cen-
tre for English Language Communication, and the
other is a freelance English linguist with exten-
sive prior experience in error annotation of English
learners? essays. The distribution of errors in the
test essays among the error types is shown in Ta-
ble 3. The test essays were then preprocessed in
the same manner as the NUCLE corpus. The pre-
processed test essays were released to the partic-
ipating teams. Similar to CoNLL-2013, the test
essays and their error annotations in the CoNLL-
2014 shared task will be made freely available af-
ter the shared task.
4 Evaluation Metric and Scorer
A grammatical error correction system is evalu-
ated by how well its proposed corrections or edits
match the gold-standard edits. An essay is first
sentence-segmented and tokenized before evalua-
tion is carried out on the essay. To illustrate, con-
sider the following tokenized sentence S written
by an English learner:
4
<MISTAKE start par="5" start off="11" end par="5" end off="19">
<TYPE>Wform</TYPE>
<CORRECTION>absolutely</CORRECTION>
</MISTAKE>
Figure 1: An example error annotation.
Error type Training % Test % Test %
data data data
(NUCLE) (Annotator 1) (Annotator 2)
Vt 3,204 7.1% 133 5.5% 150 4.5%
Vm 431 1.0% 49 2.0% 37 1.1%
V0 414 0.9% 31 1.3% 37 1.1%
Vform 1,443 3.2% 132 5.5% 91 2.7%
SVA 1,524 3.4% 105 4.4% 154 4.6%
ArtOrDet 6,640 14.8% 332 13.9% 444 13.3%
Nn 3,768 8.4% 215 9.0% 228 6.8%
Npos 239 0.5% 19 0.8% 15 0.5%
Pform 186 0.4% 47 2.0% 18 0.5%
Pref 927 2.1% 96 4.0% 153 4.6%
Prep 2,413 5.4% 211 8.8% 390 11.7%
Wci 5,305 11.8% 340 14.2% 479 14.4%
Wa 50 0.1% 0 0.0% 1 0.0%
Wform 2,161 4.8% 77 3.2% 103 3.1%
Wtone 593 1.3% 9 0.4% 15 0.5%
Srun 873 1.9% 7 0.3% 26 0.8%
Smod 51 0.1% 0 0.0% 5 0.2%
Spar 519 1.2% 3 0.1% 24 0.7%
Sfrag 250 0.6% 13 0.5% 5 0.2%
Ssub 362 0.8% 68 2.8% 10 0.3%
WOinc 698 1.6% 22 0.9% 54 1.6%
WOadv 347 0.8% 12 0.5% 27 0.8%
Trans 1,377 3.1% 94 3.9% 79 2.4%
Mec 3,145 7.0% 231 9.6% 496 14.9%
Rloc? 4,703 10.5% 95 4.0% 199 6.0%
Cit 658 1.5% 0 0.0% 0 0.0%
Others 1,467 3.3% 44 1.8% 49 1.5%
Um 1,164 2.6% 12 0.5% 42 1.3%
All types 44,912 100.0% 2,397 100.0% 3,331 100.0%
Table 3: Error type distribution of the training and test data. The test data were annotated independently
by two annotators.
5
ID Prompt
1 ?The decision to undergo genetic testing can only be made by the individual at risk for a disor-
der. Once a test has been conducted and the results are known, however, a new, family-related
ethical dilemma is born: Should a carrier of a known genetic risk be obligated to tell his or her
relatives?? Respond to the question above, supporting your argument with concrete examples.
2 While social media sites such as Twitter and Facebook can connect us closely to people in
many parts of the world, some argue that the reduction in face-to-face human contact affects
interpersonal skills. Explain the advantages and disadvantages of using social media in your
daily life/society.
Table 4: The two prompts used for the test essays.
There is no a doubt , tracking system
has brought many benefits in this infor-
mation age .
The set of gold-standard edits of a human annota-
tor is g = {a doubt ? doubt, system ? systems,
has ? have}. Suppose the tokenized output sen-
tence H of a grammatical error correction system
given the above sentence is:
There is no doubt , tracking system has
brought many benefits in this informa-
tion age .
That is, the set of system edits is e = {a doubt
? doubt}. The performance of the grammatical
error correction system is measured by how well
the two sets g and e match, in the form of recall
R, precision P , and F
0.5
measure: R = 1/3, P =
1/1, F
0.5
= (1 + 0.5
2
)?RP/(R + 0.5
2
? P ) =
5/7.
More generally, given a set of n sentences,
where g
i
is the set of gold-standard edits for sen-
tence i, and e
i
is the set of system edits for sen-
tence i, recall, precision, and F
0.5
are defined as
follows:
R =
?
n
i=1
|g
i
? e
i
|
?
n
i=1
|g
i
|
(1)
P =
?
n
i=1
|g
i
? e
i
|
?
n
i=1
|e
i
|
(2)
F
0.5
=
(1 + 0.5
2
)?R? P
R + 0.5
2
? P
(3)
where the intersection between g
i
and e
i
for sen-
tence i is defined as
g
i
? e
i
= {e ? e
i
|?g ? g
i
,match(g, e)} (4)
Note that we have adopted F
0.5
as the evaluation
metric in the CoNLL-2014 shared task instead of
the standard F
1
used in CoNLL-2013. F
0.5
em-
phasizes precision twice as much as recall, while
F
1
weighs precision and recall equally. When a
grammar checker is put into actual use, it is im-
portant that its proposed corrections are highly ac-
curate in order to gain user acceptance. Neglecting
to propose a correction is not as bad as proposing
an erroneous correction.
Similar to CoNLL-2013, we use the MaxMatch
(M
2
) scorer
2
(Dahlmeier and Ng, 2012b) as the of-
ficial scorer in CoNLL-2014. The M
2
scorer
3
effi-
ciently searches for a set of system edits that max-
imally matches the set of gold-standard edits spec-
ified by an annotator. It overcomes a limitation of
the scorer used in HOO shared tasks, which can
return an erroneous score since the system edits
are computed deterministically by the HOO scorer
without regard to the gold-standard edits.
5 Approaches
45 teams registered to participate in the shared
task, out of which 13 teams submitted the out-
put of their grammatical error correction systems.
These teams are listed in Table 5. Each team is as-
signed a 3 to 4-letter team ID. In the remainder of
this paper, we will use the assigned team ID to re-
fer to a participating team. Every team submitted
a system description paper (the only exception is
the NARA team). Four of the 13 teams submitted
their system output only after the deadline (they
were given up to one week of extension). These
four teams (IITB, IPN, PKU, and UFC) have an
asterisk affixed after their team names in Table 5.
Each participating team in the CoNLL-2014
shared task tackled the error correction problem
in a different way. A full list summarizing each
2
http://www.comp.nus.edu.sg/?nlp/software.html
3
A few minor bugs were fixed in the M
2
scorer before it
was used in the CoNLL-2014 shared task.
6
Team ID Affiliation
AMU Adam Mickiewicz University
CAMB University of Cambridge
CUUI Columbia University and the University of Illinois at Urbana-Champaign
IITB
?
Indian Institute of Technology, Bombay
IPN
?
Instituto Polit?ecnico Nacional
NARA Nara Institute of Science and Technology
NTHU National Tsing Hua University
PKU
?
Peking University
POST Pohang University of Science and Technology
RAC Research Institute for Artificial Intelligence, Romanian Academy
SJTU Shanghai Jiao Tong University
UFC
?
University of Franche-Comt?e
UMC University of Macau
Table 5: The list of 13 participating teams. The teams that submitted their system output after the
deadline have an asterisk affixed after their team names. NARA did not submit any system description
paper.
team?s approach can be found in Table 6. While
machine-learnt classifiers for specific error types
proved popular in last year?s CoNLL-2013 shared
task, since this year?s task required the correction
of all 28 error types, teams tended to prefer meth-
ods that could deal with all error types simultane-
ously. In fact, most teams built hybrid systems that
made use of a combination of different approaches
to identify and correct errors.
One of the most popular approaches to non-
specific error type correction, incorporated to var-
ious extents in many teams? systems, was the Lan-
guage Model (LM) based approach. Specifically,
the probability of a learner n-gram is compared
with the probability of a candidate corrected n-
gram, and if the difference is greater than some
threshold, an error was perceived to have been de-
tected and a higher scoring replacement n-gram
could be suggested. Some teams used this ap-
proach only to detect errors, e.g., IPN (Hernandez
and Calvo, 2014), which could then be corrected
by other methods, whilst other teams used other
methods to detect errors first, and then made cor-
rections based on the alternative highest n-gram
probability score, e.g., RAC (Boros? et al., 2014).
No single team used a uniquely LM-based solution
and the LM approach was always a component in
a hybrid system.
An alternative solution to correcting all er-
rors was to use a phrase-based statistical machine
translation (MT) system to ?translate? learner En-
glish into correct English. Teams that followed the
MT approach mainly differed in terms of their at-
titude toward tuning; CAMB (Felice et al., 2014)
performed no tuning at all, IITB (Kunchukut-
tan et al., 2014) and UMC (Wang et al., 2014b)
tuned F
0.5
using MERT, while AMU (Junczys-
Dowmunt and Grundkiewicz, 2014) explored a va-
riety of tuning options, ultimately tuning F
0.5
us-
ing a combination of kb-MIRA and MERT. No
team used a syntax-based translation model, al-
though UMC did include POS tags and morphol-
ogy in a factored translation model.
With regard to correcting single error types,
rule-based (RB) approaches were also common in
most teams? systems. A possible reason for this
is that some error types are more regular than oth-
ers, and so in order to boost accuracy, simple rules
can be written to make sure that, for example, the
number of a subject agrees with the number of
a verb. In contrast, it is a lot harder to write a
rule to consistently correct Wci (wrong colloca-
tion/idiom) errors. As such, RB methods were of-
ten, but not always, used as a preliminary or sup-
plementary stage in a larger hybrid system.
Finally, although there were fewer machine-
learnt classifier (ML) approaches than last year,
some teams still used various classifiers to correct
specific error types. In fact, CUUI (Rozovskaya
et al., 2014) only built classifiers for specific er-
ror types and did not attempt to tackle the whole
range of errors. SJTU (Wang et al., 2014a) also
preprocessed the training data into more precise
error categories using rules (e.g., verb tense (Vt)
7
errors might be subcategorized into present, past,
or future tense etc.) and then built a single max-
imum entropy classifier to correct all error types.
See Table 6 to find out which teams tackled which
error types.
While every effort has been made to make clear
which team used which approach to correct which
set of error types, as there were more error types
than last year, it was sometimes impractical to fit
all this information into Table 6. For more infor-
mation on the specific methods used to correct a
specific error type, we must refer the reader to that
team?s CoNLL-2014 system description paper.
Table 6 also shows the linguistic features used
by the participating teams, which include lexical
features (i.e., words, collocations, n-grams), parts-
of-speech (POS), constituency parses, and depen-
dency parses.
While all teams in the shared task used the NU-
CLE corpus, they were also allowed to use addi-
tional external resources (both corpora and tools)
so long as they were publicly available and not
proprietary. Three teams also used last year?s
CoNLL-2013 test set as a development set in this
year?s CoNLL-2014 shared task. The external re-
sources used by the teams are also listed in Ta-
ble 6.
6 Results
All submitted system output was evaluated using
the M
2
scorer, based on the error annotations pro-
vided by our annotators. The recall (R), pre-
cision (P ), and F
0.5
measure of all teams are
shown in Table 7. The performance of the teams
varies greatly, from little more than five per cent to
37.33% for the top team.
The nature of grammatical error correction is
such that multiple, different corrections are of-
ten acceptable. In order to allow the participating
teams to raise their disagreement with the origi-
nal gold-standard annotations provided by the an-
notators, and not understate the performance of
the teams, we allow the teams to submit their
proposed alternative answers. This was also the
practice adopted in HOO 2011, HOO 2012, and
CoNLL-2013. Specifically, after the teams sub-
mitted their system output and the error annota-
tions on the test essays were released, we allowed
the teams to propose alternative answers (gold-
standard edits), to be submitted within four days
after the initial error annotations were released.
Team ID Precision Recall F
0.5
CAMB 39.71 30.10 37.33
CUUI 41.78 24.88 36.79
AMU 41.62 21.40 35.01
POST 34.51 21.73 30.88
NTHU 35.08 18.85 29.92
RAC 33.14 14.99 26.68
UMC 31.27 14.46 25.37
PKU
?
32.21 13.65 25.32
NARA 21.57 29.38 22.78
SJTU 30.11 5.10 15.19
UFC
?
70.00 1.72 7.84
IPN
?
11.28 2.85 7.09
IITB
?
30.77 1.39 5.90
Table 7: Scores (in %) without alternative an-
swers. The teams that submitted their system out-
put after the deadline have an asterisk affixed after
their team names.
The same annotators who provided the error an-
notations on the test essays also judged the alter-
native answers proposed by the teams, to ensure
consistency. In all, three teams (CAMB, CUUI,
UMC) submitted alternative answers.
The same submitted system output was then
evaluated using the M
2
scorer, with the original
annotations augmented with the alternative an-
swers. Table 8 shows the recall (R), precision (P ),
and F
0.5
measure of all teams under this new eval-
uation setting.
The F
0.5
measure of every team improves when
evaluated with alternative answers. Not surpris-
ingly, the teams which submitted alternative an-
swers tend to show the greatest improvements in
their F
0.5
measure. Overall, the CUUI team (Ro-
zovskaya et al., 2014) achieves the best F
0.5
mea-
sure when evaluated with alternative answers, and
the CAMB team (Felice et al., 2014) achieves the
best F
0.5
measure when evaluated without alterna-
tive answers.
For future research which uses the test data of
the CoNLL-2014 shared task, we recommend that
evaluation be carried out in the setting that does
not use alternative answers, to ensure a fairer eval-
uation. This is because the scores of the teams
which submitted alternative answers tend to be
higher in a biased way when evaluated with alter-
native answers.
We are also interested in the analysis of the
system performance for each of the error types.
8
T
e
a
m
E
r
r
o
r
A
p
p
r
o
a
c
h
D
e
s
c
r
i
p
t
i
o
n
o
f
A
p
p
r
o
a
c
h
L
i
n
g
u
i
s
t
i
c
F
e
a
t
u
r
e
s
E
x
t
e
r
n
a
l
R
e
s
o
u
r
c
e
s
A
M
U
A
l
l
M
T
P
h
r
a
s
e
-
b
a
s
e
d
t
r
a
n
s
l
a
t
i
o
n
o
p
t
i
m
i
z
e
d
f
o
r
F
-
s
c
o
r
e
u
s
i
n
g
a
c
o
m
b
i
n
a
t
i
o
n
o
f
k
b
-
M
I
R
A
a
n
d
M
E
R
T
w
i
t
h
a
u
g
m
e
n
t
e
d
l
a
n
g
u
a
g
e
m
o
d
e
l
s
a
n
d
t
a
s
k
-
s
p
e
c
i
fi
c
f
e
a
t
u
r
e
s
.
L
e
x
i
c
a
l
W
i
k
i
p
e
d
i
a
,
C
o
m
m
o
n
C
r
a
w
l
,
L
a
n
g
-
8
C
A
M
B
A
l
l
R
B
/
L
M
/
M
T
P
i
p
e
l
i
n
e
:
R
u
l
e
-
b
a
s
e
d
?
L
M
r
a
n
k
i
n
g
?
U
n
t
u
n
e
d
S
M
T
?
L
M
r
a
n
k
i
n
g
?
T
y
p
e
fi
l
t
e
r
i
n
g
L
e
x
i
c
a
l
,
P
O
S
C
a
m
b
r
i
d
g
e
?
W
r
i
t
e
a
n
d
I
m
p
r
o
v
e
?
S
A
T
s
y
s
t
e
m
,
C
a
m
b
r
i
d
g
e
L
e
a
r
n
e
r
C
o
r
p
u
s
,
C
o
N
L
L
-
2
0
1
3
T
e
s
t
S
e
t
,
F
i
r
s
t
C
e
r
t
i
fi
c
a
t
e
i
n
E
n
g
l
i
s
h
c
o
r
p
u
s
,
E
n
g
l
i
s
h
V
o
c
a
b
u
l
a
r
y
P
r
o
fi
l
e
c
o
r
p
u
s
,
M
i
c
r
o
s
o
f
t
W
e
b
L
M
C
U
U
I
A
r
t
O
r
D
e
t
,
M
e
c
,
N
n
,
P
r
e
p
,
S
V
A
,
V
f
o
r
m
,
V
t
,
W
f
o
r
m
,
W
t
o
n
e
M
L
D
i
f
f
e
r
e
n
t
c
o
m
b
i
n
a
t
i
o
n
s
o
f
a
v
e
r
a
g
e
d
p
e
r
c
e
p
t
r
o
n
,
n
a
?
?
v
e
B
a
y
e
s
,
a
n
d
p
a
t
t
e
r
n
-
b
a
s
e
d
l
e
a
r
n
i
n
g
t
r
a
i
n
e
d
o
n
d
i
f
f
e
r
e
n
t
d
a
t
a
s
e
t
s
f
o
r
d
i
f
f
e
r
e
n
t
e
r
r
o
r
t
y
p
e
s
.
L
e
x
i
c
a
l
,
P
O
S
,
l
e
m
m
a
,
s
h
a
l
l
o
w
p
a
r
s
e
,
d
e
p
e
n
-
d
e
n
c
y
p
a
r
s
e
C
o
N
L
L
-
2
0
1
3
T
e
s
t
S
e
t
,
G
o
o
g
l
e
W
e
b
1
T
I
I
T
B
A
l
l
M
T
/
M
L
P
h
r
a
s
e
-
b
a
s
e
d
t
r
a
n
s
l
a
t
i
o
n
o
p
t
i
m
i
z
e
d
f
o
r
F
-
s
c
o
r
e
u
s
i
n
g
M
E
R
T
a
n
d
s
u
p
p
l
e
m
e
n
t
e
d
w
i
t
h
a
d
d
i
t
i
o
n
a
l
R
B
m
o
d
u
l
e
s
f
o
r
S
V
A
e
r
r
o
r
s
a
n
d
M
L
m
o
d
u
l
e
s
f
o
r
N
n
a
n
d
A
r
t
O
r
D
e
t
.
L
e
x
i
c
a
l
,
s
h
a
l
l
o
w
p
a
r
s
e
N
o
n
e
I
P
N
A
l
l
e
x
c
e
p
t
P
r
e
p
L
M
/
R
B
L
o
w
L
M
s
c
o
r
e
t
r
i
g
r
a
m
s
a
r
e
i
d
e
n
t
i
fi
e
d
a
s
e
r
r
o
r
s
w
h
i
c
h
a
r
e
s
u
b
s
e
q
u
e
n
t
l
y
c
o
r
r
e
c
t
e
d
b
y
r
u
l
e
s
.
L
e
x
i
c
a
l
,
l
e
m
m
a
,
d
e
p
e
n
-
d
e
n
c
y
p
a
r
s
e
W
i
k
i
p
e
d
i
a
N
T
H
U
A
r
t
O
r
D
e
t
,
N
n
,
P
r
e
p
,
?
P
r
e
p
+
V
e
r
b
?
,
S
p
e
l
l
i
n
g
a
n
d
C
o
m
m
a
s
,
S
V
A
,
W
f
o
r
m
R
B
/
L
M
/
M
T
E
x
t
e
r
n
a
l
r
e
s
o
u
r
c
e
s
c
o
r
r
e
c
t
s
p
e
l
l
i
n
g
e
r
r
o
r
s
w
h
i
l
e
a
c
o
n
d
i
-
t
i
o
n
a
l
r
a
n
d
o
m
fi
e
l
d
m
o
d
e
l
c
o
r
r
e
c
t
s
c
o
m
m
a
e
r
r
o
r
s
.
S
V
A
e
r
r
o
r
s
c
o
r
r
e
c
t
e
d
u
s
i
n
g
a
R
B
a
p
p
r
o
a
c
h
.
A
l
l
o
t
h
e
r
e
r
r
o
r
s
c
o
r
r
e
c
t
e
d
b
y
m
e
a
n
s
o
f
a
l
a
n
g
u
a
g
e
m
o
d
e
l
.
I
n
t
e
r
a
c
t
i
n
g
e
r
r
o
r
s
c
o
r
r
e
c
t
e
d
u
s
i
n
g
a
n
M
T
s
y
s
t
e
m
.
L
e
x
i
c
a
l
,
P
O
S
,
d
e
p
e
n
-
d
e
n
c
y
p
a
r
s
e
A
s
p
e
l
l
,
G
i
n
g
e
r
I
t
,
A
c
a
d
e
m
i
c
W
o
r
d
L
i
s
t
,
B
r
i
t
i
s
h
N
a
t
i
o
n
a
l
C
o
r
p
u
s
,
G
o
o
g
l
e
W
e
b
1
T
,
G
o
o
g
l
e
B
o
o
k
s
S
y
n
t
a
c
t
i
c
N
-
G
r
a
m
s
,
E
n
g
l
i
s
h
G
i
g
a
w
o
r
d
P
K
U
A
l
l
L
M
/
M
L
A
L
M
i
s
u
s
e
d
t
o
fi
n
d
t
h
e
h
i
g
h
e
s
t
s
c
o
r
i
n
g
v
a
r
i
a
n
t
o
f
a
w
o
r
d
w
i
t
h
a
c
o
m
m
o
n
s
t
e
m
w
h
i
l
e
m
a
x
i
m
u
m
e
n
t
r
o
p
y
c
l
a
s
s
i
fi
e
r
s
d
e
a
l
w
i
t
h
a
r
t
i
c
l
e
s
a
n
d
p
r
e
p
o
s
i
t
i
o
n
s
.
L
e
x
i
c
a
l
,
P
O
S
,
s
t
e
m
G
i
g
a
w
o
r
d
,
A
p
a
c
h
e
L
u
c
e
n
e
S
p
e
l
l
c
h
e
c
k
e
r
P
O
S
T
A
l
l
L
M
/
R
B
N
-
g
r
a
m
-
b
a
s
e
d
a
p
p
r
o
a
c
h
fi
n
d
s
u
n
l
i
k
e
l
y
n
-
g
r
a
m
?
f
r
a
m
e
s
?
w
h
i
c
h
a
r
e
t
h
e
n
c
o
r
r
e
c
t
e
d
v
i
a
h
i
g
h
s
c
o
r
i
n
g
L
M
a
l
t
e
r
n
a
-
t
i
v
e
s
.
R
u
l
e
-
b
a
s
e
d
m
e
t
h
o
d
s
t
h
e
n
i
m
p
r
o
v
e
t
h
e
r
e
s
u
l
t
s
f
o
r
c
e
r
t
a
i
n
e
r
r
o
r
t
y
p
e
s
.
L
e
x
i
c
a
l
,
P
O
S
,
d
e
-
p
e
n
d
e
n
c
y
p
a
r
s
e
,
c
o
n
s
t
i
t
u
e
n
c
y
p
a
r
s
e
G
o
o
g
l
e
W
e
b
1
T
,
C
o
N
L
L
-
2
0
1
3
T
e
s
t
S
e
t
,
P
y
E
n
c
h
a
n
t
S
p
e
l
l
c
h
e
c
k
i
n
g
L
i
b
r
a
r
y
R
A
C
S
e
e
F
o
o
t
n
o
t
e
a
R
B
/
L
M
R
u
l
e
-
b
a
s
e
d
m
e
t
h
o
d
s
a
r
e
u
s
e
d
t
o
d
e
t
e
c
t
e
r
r
o
r
s
w
h
i
c
h
c
a
n
t
h
e
n
b
e
c
o
r
r
e
c
t
e
d
b
a
s
e
d
o
n
L
M
s
c
o
r
e
s
.
L
e
x
i
c
a
l
,
P
O
S
,
l
e
m
m
a
,
s
h
a
l
l
o
w
p
a
r
s
e
G
o
o
g
l
e
W
e
b
1
T
,
N
e
w
s
C
R
A
W
L
(
2
0
0
7
?
2
0
1
2
)
,
E
u
r
o
p
a
r
l
,
U
N
F
r
e
n
c
h
-
E
n
g
l
i
s
h
C
o
r
p
u
s
,
N
e
w
s
C
o
m
m
e
n
t
a
r
y
,
W
i
k
i
p
e
d
i
a
,
L
a
n
g
u
a
g
e
T
o
o
l
.
o
r
g
S
J
T
U
A
l
l
R
B
/
M
L
R
u
l
e
-
b
a
s
e
d
s
y
s
t
e
m
g
e
n
e
r
a
t
e
s
m
o
r
e
d
e
t
a
i
l
e
d
e
r
r
o
r
c
a
t
e
-
g
o
r
i
e
s
w
h
i
c
h
a
r
e
t
h
e
n
u
s
e
d
t
o
t
r
a
i
n
a
s
i
n
g
l
e
m
a
x
i
m
u
m
e
n
t
r
o
p
y
m
o
d
e
l
.
L
e
x
i
c
a
l
,
P
O
S
,
l
e
m
m
a
,
d
e
p
e
n
d
e
n
c
y
p
a
r
s
e
N
o
n
e
U
F
C
S
V
A
,
V
f
o
r
m
,
W
f
o
r
m
R
B
M
i
s
m
a
t
c
h
e
d
P
O
S
t
a
g
s
g
e
n
e
r
a
t
e
d
b
y
t
w
o
d
i
f
f
e
r
e
n
t
t
a
g
-
g
e
r
s
a
r
e
t
r
e
a
t
e
d
a
s
e
r
r
o
r
s
w
h
i
c
h
a
r
e
t
h
e
n
c
o
r
r
e
c
t
e
d
b
y
r
u
l
e
s
.
P
O
S
N
o
d
e
b
o
x
E
n
g
l
i
s
h
L
i
n
g
u
i
s
t
i
c
s
L
i
b
r
a
r
y
U
M
C
A
l
l
M
T
F
a
c
t
o
r
e
d
t
r
a
n
s
l
a
t
i
o
n
m
o
d
e
l
u
s
i
n
g
m
o
d
i
fi
e
d
P
O
S
t
a
g
s
a
n
d
m
o
r
p
h
o
l
o
g
y
a
s
f
e
a
t
u
r
e
s
.
L
e
x
i
c
a
l
,
P
O
S
,
p
r
e
fi
x
,
s
u
f
fi
x
,
s
t
e
m
W
M
T
2
0
1
4
M
o
n
o
l
i
n
g
u
a
l
D
a
t
a
T
a
b
l
e
6
:
P
r
o
fi
l
e
o
f
t
h
e
p
a
r
t
i
c
i
p
a
t
i
n
g
t
e
a
m
s
.
T
h
e
E
r
r
o
r
c
o
l
u
m
n
l
i
s
t
s
t
h
e
e
r
r
o
r
t
y
p
e
s
t
a
c
k
l
e
d
b
y
a
t
e
a
m
i
f
n
o
t
a
l
l
w
e
r
e
c
o
r
r
e
c
t
e
d
.
T
h
e
A
p
p
r
o
a
c
h
c
o
l
u
m
n
l
i
s
t
s
t
h
e
t
y
p
e
o
f
a
p
p
r
o
a
c
h
u
s
e
d
,
w
h
e
r
e
L
M
d
e
n
o
t
e
s
a
L
a
n
g
u
a
g
e
M
o
d
e
l
i
n
g
b
a
s
e
d
a
p
p
r
o
a
c
h
,
M
L
a
M
a
c
h
i
n
e
L
e
a
r
n
i
n
g
c
l
a
s
s
i
fi
e
r
b
a
s
e
d
a
p
p
r
o
a
c
h
,
M
T
a
s
t
a
t
i
s
t
i
c
a
l
M
a
c
h
i
n
e
T
r
a
n
s
l
a
t
i
o
n
a
p
p
r
o
a
c
h
,
a
n
d
R
B
a
R
u
l
e
-
B
a
s
e
d
a
p
p
r
o
a
c
h
.
a
T
h
e
R
A
C
t
e
a
m
u
s
e
s
r
u
l
e
s
t
o
c
o
r
r
e
c
t
e
r
r
o
r
t
y
p
e
s
t
h
a
t
d
i
f
f
e
r
f
r
o
m
t
h
e
2
8
o
f
fi
c
i
a
l
e
r
r
o
r
t
y
p
e
s
.
T
h
e
y
i
n
c
l
u
d
e
:
?
t
h
e
c
o
r
r
e
c
t
i
o
n
o
f
t
h
e
v
e
r
b
t
e
n
s
e
e
s
p
e
c
i
a
l
l
y
i
n
t
i
m
e
c
l
a
u
s
e
s
,
t
h
e
u
s
e
o
f
t
h
e
s
h
o
r
t
i
n
fi
n
i
t
i
v
e
a
f
t
e
r
m
o
d
a
l
s
,
t
h
e
p
o
s
i
t
i
o
n
o
f
f
r
e
q
u
e
n
c
y
a
d
v
e
r
b
s
i
n
a
s
e
n
t
e
n
c
e
,
s
u
b
j
e
c
t
-
v
e
r
b
a
g
r
e
e
m
e
n
t
,
w
o
r
d
o
r
d
e
r
i
n
i
n
t
e
r
r
o
g
a
t
i
v
e
s
e
n
t
e
n
c
e
s
,
p
u
n
c
t
u
a
t
i
o
n
a
c
c
o
m
p
a
n
y
i
n
g
c
e
r
t
a
i
n
l
e
x
i
c
a
l
e
l
e
m
e
n
t
s
,
t
h
e
u
s
e
o
f
a
r
t
i
c
l
e
s
,
o
f
c
o
r
r
e
l
a
t
i
v
e
s
,
e
t
c
.
?
9
T
y
p
e
A
M
U
C
A
M
B
C
U
U
I
I
I
T
B
I
P
N
N
A
R
A
N
T
H
U
P
K
U
P
O
S
T
R
A
C
S
J
T
U
U
F
C
U
M
C
V
t
1
0
.
6
6
1
9
.
1
2
3
.
7
9
1
.
7
4
0
.
8
8
1
4
.
1
8
1
0
.
6
1
1
2
.
3
0
3
.
7
6
2
6
.
1
9
4
.
1
7
0
.
0
0
1
4
.
8
4
V
m
1
0
.
8
1
2
2
.
5
8
0
.
0
0
0
.
0
0
0
.
0
0
2
9
.
0
3
0
.
0
0
3
.
2
3
0
.
0
0
3
5
.
9
0
0
.
0
0
0
.
0
0
6
.
4
5
V
0
1
7
.
8
6
2
5
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
3
6
.
6
7
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
2
5
.
9
3
V
f
o
r
m
2
2
.
7
6
2
4
.
3
7
2
1
.
4
3
1
.
8
5
4
.
6
3
2
7
.
6
2
2
4
.
3
0
2
5
.
6
4
1
.
8
9
2
7
.
3
5
3
.
6
7
0
.
9
5
1
4
.
6
8
S
V
A
2
4
.
3
0
3
1
.
3
6
7
0
.
3
4
1
.
0
6
1
4
.
1
4
2
7
.
5
0
6
2
.
6
7
1
7
.
3
1
2
0
.
5
6
3
0
.
3
6
1
4
.
8
5
2
8
.
7
0
1
4
.
4
1
A
r
t
O
r
D
e
t
1
5
.
5
2
4
9
.
4
8
5
8
.
8
5
0
.
6
8
0
.
3
3
5
0
.
8
9
3
3
.
6
3
8
.
2
0
5
4
.
4
5
0
.
6
3
1
2
.
5
4
0
.
0
0
2
4
.
0
5
N
n
5
8
.
7
4
5
4
.
1
1
5
6
.
1
0
4
.
4
9
1
0
.
3
6
5
7
.
3
2
4
6
.
7
6
4
1
.
7
8
5
5
.
6
0
3
6
.
4
5
1
0
.
1
1
0
.
0
0
1
7
.
0
3
N
p
o
s
1
4
.
2
9
7
.
6
9
4
.
7
6
0
.
0
0
0
.
0
0
2
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
4
.
7
6
4
.
5
5
0
.
0
0
5
.
2
6
P
f
o
r
m
2
2
.
2
2
2
2
.
5
8
7
.
1
4
0
.
0
0
0
.
0
0
1
4
.
8
1
1
6
.
1
3
1
2
.
0
0
0
.
0
0
3
.
7
0
0
.
0
0
0
.
0
0
1
7
.
2
4
P
r
e
f
9
.
3
3
1
9
.
3
5
1
.
3
2
0
.
0
0
0
.
0
0
1
0
.
0
0
1
.
2
0
1
.
3
5
1
.
3
2
0
.
0
0
0
.
0
0
0
.
0
0
1
2
.
0
5
P
r
e
p
1
8
.
4
1
3
8
.
2
6
1
5
.
4
5
2
.
1
2
0
.
0
0
2
9
.
7
2
1
9
.
4
2
0
.
0
0
2
.
2
8
0
.
0
0
7
.
9
2
0
.
0
0
1
4
.
5
5
W
c
i
1
2
.
0
0
9
.
1
7
0
.
9
4
0
.
3
6
0
.
3
5
7
.
5
5
0
.
6
3
1
.
6
5
1
.
2
7
0
.
3
4
0
.
0
0
0
.
0
0
3
.
2
3
W
a
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
W
f
o
r
m
4
5
.
5
6
4
5
.
0
5
1
7
.
2
4
4
.
0
5
2
.
6
0
3
9
.
0
8
1
4
.
8
1
2
5
.
8
8
6
.
4
9
1
1
.
2
5
1
.
3
9
1
.
3
0
1
6
.
4
6
W
t
o
n
e
8
1
.
8
2
3
6
.
3
6
3
6
.
3
6
0
.
0
0
0
.
0
0
1
4
.
2
9
0
.
0
0
0
.
0
0
2
8
.
5
7
0
.
0
0
1
6
.
6
7
0
.
0
0
4
4
.
4
4
S
r
u
n
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
m
o
d
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
p
a
r
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
5
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
f
r
a
g
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
2
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
2
5
.
0
0
0
.
0
0
2
5
.
0
0
S
s
u
b
7
.
8
9
1
4
.
6
3
0
.
0
0
0
.
0
0
2
.
2
7
1
5
.
3
8
0
.
0
0
0
.
0
0
9
.
5
2
2
.
3
8
2
.
2
7
0
.
0
0
6
.
9
8
W
O
i
n
c
0
.
0
0
3
.
0
3
0
.
0
0
3
.
5
7
0
.
0
0
3
.
0
3
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
6
.
6
7
W
O
a
d
v
0
.
0
0
4
7
.
6
2
0
.
0
0
1
2
.
5
0
0
.
0
0
4
3
.
7
5
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
4
4
.
4
4
T
r
a
n
s
1
3
.
4
3
2
1
.
4
3
2
.
8
6
1
.
4
3
0
.
0
0
1
1
.
2
5
1
.
4
1
1
.
5
2
2
.
6
7
0
.
0
0
0
.
0
0
0
.
0
0
1
2
.
1
6
M
e
c
2
9
.
3
5
2
8
.
7
5
1
5
.
7
9
1
.
0
2
4
.
3
3
3
6
.
6
9
6
.
6
7
3
0
.
2
8
3
6
.
6
1
4
3
.
5
1
0
.
5
1
0
.
0
0
1
6
.
8
0
R
l
o
c
?
5
.
4
1
2
0
.
1
6
7
.
7
6
0
.
0
0
5
.
5
6
1
8
.
6
4
9
.
6
8
1
0
.
4
8
9
.
2
6
9
.
0
9
2
.
5
0
0
.
0
0
1
5
.
8
4
C
i
t
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
O
t
h
e
r
s
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
3
.
1
2
0
.
0
0
0
.
0
0
0
.
0
0
U
m
7
.
6
9
9
.
0
9
0
.
0
0
0
.
0
0
0
.
0
0
4
.
0
0
0
.
0
0
1
5
.
7
9
8
.
7
0
8
.
3
3
0
.
0
0
0
.
0
0
0
.
0
0
T
a
b
l
e
9
:
R
e
c
a
l
l
(
i
n
%
)
f
o
r
e
a
c
h
e
r
r
o
r
t
y
p
e
w
i
t
h
o
u
t
a
l
t
e
r
n
a
t
i
v
e
a
n
s
w
e
r
s
,
i
n
d
i
c
a
t
i
n
g
h
o
w
w
e
l
l
e
a
c
h
t
e
a
m
p
e
r
f
o
r
m
s
a
g
a
i
n
s
t
a
p
a
r
t
i
c
u
l
a
r
e
r
r
o
r
t
y
p
e
.
10
T
y
p
e
A
M
U
C
A
M
B
C
U
U
I
I
I
T
B
I
P
N
N
A
R
A
N
T
H
U
P
K
U
P
O
S
T
R
A
C
S
J
T
U
U
F
C
U
M
C
V
t
1
1
.
6
1
2
0
.
0
0
5
.
7
9
1
.
9
0
0
.
9
8
1
6
.
1
8
1
2
.
9
0
1
4
.
1
6
3
.
3
1
2
9
.
1
7
4
.
5
9
0
.
0
0
1
7
.
6
0
V
m
1
1
.
1
1
2
3
.
3
3
0
.
0
0
0
.
0
0
0
.
0
0
2
9
.
0
3
0
.
0
0
3
.
3
3
0
.
0
0
3
9
.
4
7
0
.
0
0
0
.
0
0
7
.
6
9
V
0
1
9
.
2
3
2
9
.
6
3
0
.
0
0
0
.
0
0
0
.
0
0
3
8
.
7
1
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
3
0
.
7
7
V
f
o
r
m
2
3
.
9
3
2
7
.
4
2
2
1
.
0
5
1
.
9
2
4
.
8
5
2
9
.
0
9
2
4
.
0
7
2
6
.
7
9
2
.
8
3
2
6
.
9
6
3
.
7
7
0
.
9
8
1
5
.
3
2
S
V
A
2
5
.
0
0
3
3
.
9
0
7
2
.
4
1
1
.
1
1
1
4
.
7
4
2
8
.
5
7
6
3
.
7
6
1
7
.
8
2
2
2
.
8
6
3
2
.
4
3
1
5
.
4
6
3
0
.
0
9
1
4
.
9
5
A
r
t
O
r
D
e
t
1
8
.
7
5
5
4
.
7
4
6
7
.
3
8
1
.
8
1
0
.
3
6
5
4
.
4
2
3
7
.
9
6
9
.
6
5
5
9
.
4
1
0
.
6
6
1
4
.
6
3
0
.
0
0
3
3
.
4
2
N
n
6
2
.
1
4
6
2
.
0
3
6
5
.
5
3
4
.
9
1
1
2
.
2
9
6
2
.
6
9
5
2
.
8
9
5
1
.
0
1
6
4
.
1
4
4
2
.
6
7
1
1
.
9
3
0
.
0
0
2
2
.
2
2
N
p
o
s
2
3
.
3
3
4
0
.
0
0
4
.
3
5
0
.
0
0
0
.
0
0
2
9
.
1
7
0
.
0
0
0
.
0
0
0
.
0
0
9
.
5
2
4
.
5
5
0
.
0
0
1
3
.
6
4
P
f
o
r
m
2
2
.
2
2
2
3
.
3
3
7
.
6
9
0
.
0
0
0
.
0
0
1
4
.
8
1
1
7
.
8
6
1
2
.
5
0
0
.
0
0
4
.
0
0
0
.
0
0
0
.
0
0
2
2
.
2
2
P
r
e
f
9
.
5
9
1
8
.
5
6
1
.
3
2
0
.
0
0
0
.
0
0
9
.
8
0
1
.
2
5
1
.
3
3
1
.
3
7
0
.
0
0
0
.
0
0
0
.
0
0
1
1
.
1
1
P
r
e
p
1
8
.
4
1
3
8
.
6
3
1
8
.
2
2
2
.
2
1
0
.
0
0
3
0
.
2
8
2
0
.
4
2
0
.
0
0
2
.
2
5
0
.
0
0
8
.
9
5
0
.
0
0
1
6
.
9
8
W
c
i
1
5
.
2
6
1
5
.
1
8
0
.
9
6
0
.
7
9
0
.
3
8
8
.
0
5
1
.
3
3
3
.
1
7
1
.
9
4
0
.
3
6
0
.
0
0
0
.
0
0
9
.
5
7
W
a
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
W
f
o
r
m
4
5
.
4
5
4
6
.
5
9
2
1
.
1
1
2
.
9
0
2
.
6
7
4
0
.
9
1
1
5
.
5
8
2
7
.
3
8
6
.
4
9
1
2
.
5
0
1
.
4
7
1
.
3
7
1
7
.
1
1
W
t
o
n
e
8
8
.
2
4
3
8
.
4
6
5
2
.
6
3
0
.
0
0
0
.
0
0
1
2
.
5
0
0
.
0
0
0
.
0
0
5
0
.
0
0
0
.
0
0
3
3
.
3
3
0
.
0
0
5
5
.
5
6
S
r
u
n
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
m
o
d
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
p
a
r
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
5
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
S
f
r
a
g
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
1
6
.
6
7
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
2
5
.
0
0
0
.
0
0
2
0
.
0
0
S
s
u
b
7
.
8
9
1
4
.
2
9
0
.
0
0
0
.
0
0
2
.
3
3
1
5
.
3
8
0
.
0
0
0
.
0
0
9
.
7
6
2
.
4
4
2
.
3
3
0
.
0
0
6
.
9
8
W
O
i
n
c
0
.
0
0
3
.
4
5
0
.
0
0
4
.
0
0
0
.
0
0
3
.
3
3
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
7
.
1
4
W
O
a
d
v
0
.
0
0
5
0
.
0
0
0
.
0
0
1
6
.
6
7
0
.
0
0
4
4
.
4
4
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
5
0
.
0
0
T
r
a
n
s
1
4
.
5
2
2
2
.
3
9
3
.
0
8
1
.
6
7
0
.
0
0
1
1
.
8
4
1
.
5
6
1
.
6
4
2
.
8
2
0
.
0
0
0
.
0
0
0
.
0
0
2
0
.
7
8
M
e
c
3
1
.
5
6
3
0
.
6
7
1
7
.
4
7
1
.
1
3
4
.
7
9
3
7
.
2
8
7
.
1
7
3
1
.
6
9
3
7
.
8
8
4
5
.
8
2
1
.
1
0
0
.
0
0
2
2
.
3
1
R
l
o
c
?
5
.
4
5
2
6
.
4
7
7
.
3
8
0
.
0
0
5
.
6
2
2
1
.
4
3
1
1
.
3
4
1
2
.
3
8
1
1
.
8
2
1
0
.
0
0
3
.
6
6
0
.
0
0
2
9
.
6
6
C
i
t
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
O
t
h
e
r
s
0
.
0
0
3
.
0
3
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
0
.
0
0
3
.
3
3
0
.
0
0
0
.
0
0
0
.
0
0
U
m
7
.
6
9
9
.
0
9
0
.
0
0
0
.
0
0
0
.
0
0
4
.
3
5
0
.
0
0
1
5
.
0
0
4
.
5
5
8
.
7
0
0
.
0
0
0
.
0
0
0
.
0
0
T
a
b
l
e
1
0
:
R
e
c
a
l
l
(
i
n
%
)
f
o
r
e
a
c
h
e
r
r
o
r
t
y
p
e
w
i
t
h
a
l
t
e
r
n
a
t
i
v
e
a
n
s
w
e
r
s
,
i
n
d
i
c
a
t
i
n
g
h
o
w
w
e
l
l
e
a
c
h
t
e
a
m
p
e
r
f
o
r
m
s
a
g
a
i
n
s
t
a
p
a
r
t
i
c
u
l
a
r
e
r
r
o
r
t
y
p
e
.
11
Team ID Precision Recall F
0.5
CUUI 52.44 29.89 45.57
CAMB 46.70 34.30 43.55
AMU 45.68 23.78 38.58
POST 41.28 25.59 36.77
UMC 43.17 19.72 34.88
NTHU 38.34 21.12 32.97
PKU
?
36.64 15.96 29.10
RAC 35.63 16.73 29.06
NARA 23.83 31.95 25.11
SJTU 32.95 5.95 17.28
UFC
?
72.00 1.90 8.60
IPN
?
11.66 3.17 7.59
IITB
?
34.07 1.66 6.94
Table 8: Scores (in %) with alternative answers.
The teams that submitted their system output af-
ter the deadline have an asterisk affixed after their
team names.
Computing the recall of an error type is straight-
forward as the error type of each gold-standard
edit is provided. Conversely, computing the pre-
cision of each of the 28 error types is difficult as
the error type of each system edit is not available
since the submitted system output only contains
corrected sentences with no indication of the er-
ror type of the system edits. Predicting the error
type out of the 28 types for a particular system
edit not found in gold-standard annotation can be
tricky and error-prone. Therefore, we decided to
compute the per-type performance based on recall.
The recall scores when distinguished by error type
are shown in Tables 9 and 10.
6.1 Cross Annotator Comparison
To measure the agreement between our two an-
notators, we computed Cohen?s Kappa coefficient
(Cohen, 1960) for identification, which measures
the extent to which annotators agreed which words
needed correction and which did not, regardless
of the error type or correction. We obtained a
Kappa coefficient value of 0.43, indicating mod-
erate agreement (since it falls between 0.40 and
0.60). While this may seem low, it is worth point-
ing out that the Kappa coefficient does not take
into account the fact that there is often more than
one valid way to correct a sentence.
In addition to computing the performance of
each team against the gold standard annotations of
both annotators with and without alternative anno-
tations, we also had an opportunity to compare the
performance of each team?s system against each
annotator individually.
A recent concern is that there can be a high
degree of variability between individual annota-
tors which can dramatically affect a system?s out-
put score. For example, in a much simplified er-
ror correction task concerning only the correction
of prepositions, Tetreault and Chodorow (2008)
showed an actual difference of 10% precision and
5% recall between two annotators. Table 11 hence
shows the precision (P ), recall (R), and F
0.5
scores for all error types against the gold standard
annotations of each CoNLL-2014 annotator indi-
vidually.
The results show that there can indeed be a high
amount of disagreement between two annotators,
the most noticeable being precision in the UFC
system: precision was 70% for Annotator 2 but
only 28% for Annotator 1. This 42% difference is,
however, likely to be an extreme case, and most
teams show little more than 10% variation in pre-
cision and 5% variation in F
0.5
. Recall remained
fairly constant between annotators. 10% is still
a large margin however, and these results rein-
force the idea that error correction systems should
be judged against the gold-standard annotations of
multiple annotators.
Table 12 additionally shows how each annotator
compares against each other; i.e., what score An-
notator 1 gets if Annotator 2 was the gold standard
(part (a) of Table 12) and vice versa (part (b)).
The low F
0.5
scores of 45.36% and 38.54% rep-
resent an upper bound for system performance on
this data set and again emphasize the difficulty of
the task. The low human F
0.5
scores imply that
there are many ways to correct a sentence.
7 Conclusions
The CoNLL-2014 shared task saw the participa-
tion of 13 teams worldwide to evaluate their gram-
matical error correction systems on a common test
set, using a common evaluation metric and scorer.
The best systems in the shared task achieve an
F
0.5
score of 37.33% when it is scored without
alternative answers, and 45.57% with alternative
answers. There is still much room for improve-
ment in the accuracy of grammatical error correc-
tion systems. The evaluation data sets and scorer
used in our shared task serve as a benchmark for
12
Team ID Annotator 1 Annotator 2
P R F
0.5
P R F
0.5
AMU 27.30 13.55 22.69 35.49 12.90 26.29
CAMB 24.96 19.62 23.67 35.22 20.29 30.70
CUUI 26.05 15.60 22.97 36.91 16.37 29.51
IITB 23.33 0.88 3.82 24.18 0.66 2.99
IPN 5.80 1.25 3.36 9.62 1.51 4.63
NARA 13.54 19.20 14.38 18.74 19.69 18.92
NTHU 22.19 11.38 18.64 31.48 11.79 23.60
PKU 21.53 8.36 16.37 27.47 7.72 18.17
POST 22.39 13.89 19.94 29.53 13.42 23.81
RAC 19.68 8.28 15.43 28.52 8.80 19.70
SJTU 21.08 3.09 9.75 24.64 2.59 9.12
UFC 28.00 0.59 2.70 70.00 1.06 4.98
UMC 20.41 8.78 16.14 26.63 8.38 18.55
Table 11: Performance (in %) for each team?s output scored against the annotations of a single annotator.
P R F
0.5
50.47 32.29 45.36
(a)
P R F
0.5
37.14 45.38 38.54
(b)
Table 12: Performance (in %) for output of one gold standard annotation scored against the other gold
standard annotation: (a) The score of Annotator 1 if Annotator 2 was the gold standard, (b) The score of
Annotator 2 if Annotator 1 was the gold standard.
future research on grammatical error correction
4
.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
We thank our two annotators Mark Brooke and Di-
ane Nicholls who provided the gold-standard an-
notations.
References
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Tiberiu Boros?, Stefan Daniel Dumitrescu, Adrian
Zafiu, Dan Tufis?, Verginica Mititelu Barbu, and
Paul Ionut? V?aduva. 2014. RACAI GEC ? a hybrid
approach to grammatical error correction. In Pro-
ceedings of the Eighteenth Conference on Computa-
tional Natural Language Learning: Shared Task.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
4
http://www.comp.nus.edu.sg/?nlp/conll14st.html
Daniel Dahlmeier and Hwee Tou Ng. 2011a. Cor-
recting semantic collocation errors with L1-induced
paraphrases. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 107?117.
Daniel Dahlmeier and Hwee Tou Ng. 2011b. Gram-
matical error correction with alternating structure
optimization. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics, pages 915?923.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
568?578.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
568?572.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English: The NUS Corpus of Learner English. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 22?31.
13
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th EuropeanWorkshop on Natural Lan-
guage Generation, pages 242?249.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the 7th Workshop on the Innovative Use of
NLP for Building Educational Applications, pages
54?62.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth Conference on Language
Resources and Evaluation, pages 449?454.
Mariano Felice, Zheng Yuan, ?istein E. Andersen, He-
len Yannakoudakis, and Ekaterina Kochmar. 2014.
Grammatical error correction using hybrid systems
and type filtering. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing: A meta-classifier
approach. In Proceedings of the Annual Meeting of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
S. David Hernandez and Hiram Calvo. 2014. CoNLL
2014 shared task: Grammatical error correction with
a syntactic n-gram language model from a big cor-
pora. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task.
Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2014. The AMU system in the CoNLL-2014
shared task: Grammatical error correction by data-
intensive and feature-rich statistical machine trans-
lation. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Anoop Kunchukuttan, Sriram Chaudhury, and Pushpak
Bhattacharyya. 2014. Tuning a grammar correction
system for increased precision. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan &
Claypool Publishers.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 shared task on grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1?12.
Diane Nicholls. 2003. The Cambridge Learner Cor-
pus: Error coding and analysis for lexicography and
ELT. In Proceedings of the Corpus Linguistics 2003
Conference, pages 572?581.
Alla Rozovskaya and Dan Roth. 2010. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
961?970.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
Dan Roth, and Nizar Habash. 2014. The Illinois-
Columbia system in the CoNLL-2014 shared task.
In Proceedings of the Eighteenth Conference on
Computational Natural Language Learning: Shared
Task.
Joel R. Tetreault and Martin Chodorow. 2008. Na-
tive judgments of non-native usage: Experiments in
preposition error detection. In COLING Workshop
on Human Judgments in Computational Linguistics,
Manchester, UK.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of the ACL 2010
Conference Short Papers, pages 353?358.
Peilu Wang, Zhongye Jia, and Hai Zhao. 2014a.
Grammatical error detection and correction using a
single maximum entropy model. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task.
Yiming Wang, Longyue Wang, Derek F. Wong,
Lidia S. Chao, Xiaodong Zeng, and Yi Lu. 2014b.
Factored statistical machine translation for gram-
matical error correction. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task.
Yuanbin Wu and Hwee Tou Ng. 2013. Grammat-
ical error correction using integer linear program-
ming. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 1456?1465.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 180?189.
14
