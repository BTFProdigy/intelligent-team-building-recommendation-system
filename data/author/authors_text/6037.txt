Auditory-based Acoustic Distinctive Features and Spectral Cues for Robust
Automatic Speech Recognition in Low-SNR Car Environments
Sid-Ahmed Selouani
Universite? de Moncton
218 bvd. J.-D.-Gauthier,
Shippagan, E8S 1P6, Canada
selouani@umcs.ca
Hesham Tolba
INRS-Te?le?communications
800 de la Gauchetie`re Ouest,
Montre?al, H5A 1K6, Canada
{tolba,dougo}@inrs-telecom.uquebec.ca
Douglas O?Shaughnessy
INRS-Te?le?communications
800 de la Gauchetie`re Ouest,
Montre?al, H5A 1K6, Canada
Abstract
In this paper, a multi-stream paradigm is pro-
posed to improve the performance of auto-
matic speech recognition (ASR) systems in the
presence of highly interfering car noise. It
was found that combining the classical MFCCs
with some auditory-based acoustic distinctive
cues and the main formant frequencies of a
speech signal using a multi-stream paradigm
leads to an improvement in the recognition per-
formance in noisy car environments.
1 Introduction
In general, the performance of existing speech recogni-
tion systems, whose designs are predicated on relatively
noise-free conditions, degrades rapidly in the presence of
a high level of adverse conditions. However, a recognizer
can provide good performance even in very noisy back-
ground conditions if the exact testing condition is used
to provide the training material from which the reference
patterns of the vocabulary are obtained, which is practi-
cally not always the case. In order to cope with the ad-
verse conditions, different approaches could be used. The
approaches that have been studied for achieving noise ro-
bustness can be summarized into two fundamentally dif-
ferent approaches. The first approach attempts to prepro-
cess the corrupted speech input signal prior to the pattern
matching in an attempt to enhance the SNR. The second
approach attempts to modify the pattern matching itself
in order to account for the effects of noise. For more de-
tails see (O?Shaughnessy, 2000).
In a previous work, we introduced an auditory-based
multi-stream paradigm for ASR (Tolba et al, 2002).
Within this multi-stream paradigm, we merge different
sources of information about the speech signal that could
be lost when using only the MFCCs to recognize uttered
speech. Our experiments showed that the use of some
auditory-based features and formant cues via a multi-
stream paradigm approach leads to an improvement of the
recognition performance. This proves that the MFCCs
loose some information relevant to the recognition pro-
cess despite the popularity of such coefficients in all cur-
rent ASR systems. In our experiments, we used a 3-
stream feature vector. The First stream vector consists of
the classical MFCCs and their first derivatives, whereas
the second stream vector consists of acoustic cues derived
from hearing phenomena studies. Finally, the magnitudes
of the main resonances of the spectrum of the speech sig-
nal were used as the elements of the third stream vector.
In this paper, we extend our work presented in (Tolba et
al., 2002) to evaluate the robustness of the proposed fea-
tures (the acoustic distinctive cues and the spectral cues)
using a multi- stream paradigm for ASR in noisy car en-
vironments. As mentioned above, the first stream con-
sists of the MFCCs and their first derivatives, whereas
the second stream vector consists of the acoustic cues are
computed from an auditory-based analysis applied to the
speech signal modeled using the Caelen Model (Caelen,
1985). Finally, the values of the main peaks of the spec-
trum of the speech signal were used as the elements of the
third stream vector. The magnitudes of the main peaks
were obtained through an LPC analysis.
The outline of this paper is as follows. In section 2, an
overview on the auditory Caelen Model is given. Next,
we describe briefly in section 3 the statistical framework
of the multi-stream paradigm. Then in section 4, we pro-
ceed with the evaluation of the proposed approach for
ASR. Finally, in section 5 we conclude and discuss our
results.
2 The Auditory-based Processing
It was shown through several studies that the use of
human hearing properties provides insight into defin-
ing a potentially useful front-end speech representa-
tion (O?Shaughnessy, 2000). However, the performance
of current ASR systems is far from the performance
achieved by humans. In an attempt to improve the ASR
performance in noisy environments, we evaluate in this
work the use of the hearing/perception knowledge for
ASR in noisy car environments. This is accomplished
through the use of the auditory-based acoustic distinctive
features and the formant frequencies for robust ASR.
2.1 The Caelen?s Auditory Model
Caelen?s auditory model (Caelen, 1985) consists of three
parts which simulate the behavior of the ear. The exter-
nal and middle ear are modeled using a bandpass filter
that can be adjusted to signal energy to take into account
the various adaptive motions of ossicles. The next part
of the model simulates the behavior of the basilar mem-
brane (BM), the most important part of the inner ear, that
acts substantially as a non-linear filter bank. Due to the
variability of its stiffness, different places along the BM
are sensitive to sounds with different spectral content. In
particular, the BM is stiff and thin at the base, but less
rigid and more sensitive to low frequency signals at the
apex. Each location along the BM has a characteristic fre-
quency, at which it vibrates maximally for a given input
sound. This behavior is simulated in the model by a cas-
cade filter bank. The bigger the number of these filters the
more accurate is the model. In front of these stages there
is another stage that simulates the effects of the outer and
middle ear (pre-emphasis). In our experiments we have
considered 24 filters. This number depends on the sam-
pling rate of the signals (16 kHz) and on other param-
eters of the model such as the overlapping factor of the
bands of the filters, or the quality factor of the resonant
part of the filters. The final part of the model deals with
the electro-mechanical transduction of hair-cells and af-
ferent fibers and the encoding at the level of the synaptic
endings. For more details see (Caelen, 1985).
2.2 Acoustic Distinctive Cues
The acoustic distinctive cues are calculated starting from
the spectral data using linear combinations of the ener-
gies taken in various channels. It was shown in (Jakob-
son et al, 1951) that 12 acoustic cues are sufficient to
characterize acoustically all languages. However, it is
not necessary to use all of these cues to characterize a
specific language. In our study, we choose 7 cues to be
merged in a multi-stream feature vector in an attempt to
improve the performance of ASR. These cues are based
on the Caelen ear model described above, which does
not correspond exactly to Jakobson?s cues. Each cue is
computed based on the output of the 24 channel filters of
the above-mentioned ear model. These seven normalized
acoustic cues are: acute/grave (AG), open/closed (OC),
diffuse/compact (DC), sharp/flat (SF), mat/strident (MS),
continuous/discontinuous (CD) and tense/lax (TL).
3 Multi-stream Statistical Framework
Most recognizers use typically left-to-right HMMs,
which consist of an arbitrary number of states N
(O?Shaughnessy, 2000). The output distribution associ-
ated with each state is dependent on one or more statisti-
cally independent streams. Assuming an observation se-
quence O composed of S input streams Os possibly of
different lengths, representing the utterance to be recog-
nized, the probability of the composite input vector Ot at
a time t in state j can be written as follows:
bj(Ot) =
S?
s=1
[bjs(Ost)]?s , (1)
where Ost is the input observation vector in stream s at
time t and ?s is the stream weight. Each individual stream
probability bjs(Ost) is represented by a multivariate mix-
ture Gaussian. To investigate the multi-stream paradigm
using the proposed features for ASR, we have performed
a number of experiments in which we merged different
sources of information about the speech signal that could
be lost with the cepstral analysis.
4 Experiments & Results
In the following experiments the TIMIT database was
used. The TIMIT corpus contains broadband record-
ings of a total of 6300 sentences, 10 sentences spoken
by each of 630 speakers from 8 major dialect regions
of the United States, each reading 10 phonetically rich
sentences. To simulate a noisy environment, car noise
was added artificially to the clean speech. Throughout
all experiments the HTK-based speech recognition plat-
form system described in (Cambridge University Speech
Group, 1997) has been used. The toolkit was designed to
support continuous-density HMMs with any numbers of
state and mixture components.
In order to evaluate the use of the proposed features
for ASR in noisy car environments, we repeated the same
experiments performed in our previous study (Tolba et
al., 2002) using the subsets dr1 & dr2 of a noisy ver-
sion of the TIMIT database at different values of SNR
which varies from 16 dB to -4 dB. In all our experi-
ments, 12 MFCCs were calculated on a 30-msec Ham-
ming window advanced by 10 msec each frame. More-
over, the normalized log energy is also found, which is
added to the 12 MFCCs to form a 13-dimensional (static)
vector. This static vector is then expanded to produce a
26-dimensional (static+dynamic) vector. This latter was
expanded by adding the seven acoustic distinctive cues
that were computed based on the Caelen model analysis.
This was followed by the computation of the main spec-
tral peak magnitudes, which were added to the MFCCs
and the acoustic cues to form a 37-dimensional vector
16 dB 8dB 4 dB 0 dB -4 dB
MFCCEDA 81.67 58.02 48.02 33.44 22.81
MFCCEDE 87.60 50.83 38.23 27.29 17.29
MFCCEDP 89.69 69.58 60.73 40.31 27.50
MFCCEDEP 89.38 55.31 41.88 28.44 17.40
[a] %CWrd using 1-mixture triphone models.
16 dB 8dB 4 dB 0 dB -4 dB
MFCCEDA 83.85 60.31 49.58 36.56 25.21
MFCCEDE 88.12 51.98 39.58 28.02 16.56
MFCCEDP 90.21 71.35 59.06 42.92 27.19
MFCCEDEP 89.79 55.73 42.92 29.06 18.12
[b] %CWrd using 2-mixture triphone models.
16 dB 8dB 4 dB 0 dB -4 dB
MFCCEDA 84.58 62.40 51.77 35.73 26.25
MFCCEDE 89.06 53.85 42.29 29.38 17.71
MFCCEDP 89.69 71.67 59.79 42.81 27.81
MFCCEDEP 89.27 58.65 43.75 29.27 19.38
[c] %CWrd using 4-mixture triphone models.
16 dB 8dB 4 dB 0 dB -4 dB
MFCCEDA 85.42 63.54 52.60 40.10 28.75
MFCCEDE 89.38 53.33 41.46 29.27 17.92
MFCCEDP 90.62 70.94 58.85 42.19 28.85
MFCCEDEP 91.35 57.92 43.85 28.75 18.33
[d] %CWrd using 8-mixture triphone models.
Table 1: Comparison of the percent word recognition performance (%CWrd) of the MFCCEDA-, MFCCEDE-
MFCCEDP- and MFCCEDEP-based HTK ASR systems to the baseline HTK using (a) 2-mixture, (b) 4-mixture
and (c) 8-mixture triphone models and the dr1 & dr2 subsets of the TIMIT database when contaminated by additive
car noise for different values of SNR.
upon which the hidden Markov models (HMMs), that
model the speech subword units, were trained. The main
spectral peak magnitudes were computed based on an
LPC analysis using 12 poles followed by a peak picking
algorithm. The proposed system used for the recogni-
tion task uses tri-phone Gaussian mixture HMM system.
Three different sets of experiments has been carried out
on the noisy version of the TIMIT database. In the first
set of these experiments, we tested our recognizer using
a 30-dimensional feature vector (MFCCEDP), in which
we combined the magnitudes of the main spectral peaks
to the classical MFCCs and their first derivatives to form
two streams that have been used to perform the recogni-
tion process. We found through experiments that the use
of these two streams leads to an improvement in the ac-
curacy of the word recognition rate compared to the one
obtained when we used the classical MFCCEDA feature
vector, Table 1. These tests were repeated using the 2-
stream feature vector, in which we combined the acous-
tic distinctive cues to the classical MFCCs and their first
derivatives to form two streams (MFCCEDE). Again, us-
ing these two streams, an improvement in the accuracy
of the word recognition rate has been obtained when we
tested our recognizer using N mixture Gaussian HMMs
using triphone models for different values of SNR, Table
1. We repeated these tests using the proposed features
which combines the MFCCs with the acoustic distinctive
cues and the formant frequencies to form a three-stream
feature vector (MFCCEDEP). Again, using these com-
bined features, an improvement in the accuracy of the
word recognition rate was obtained, Table 1.
5 Conclusion
We have proposed in this paper a multi-stream paradigm
to improve the performance of ASR systems in noisy
car environments. Results showed that combining the
classical MFCCs with the main formant frequencies of
a speech signal using a multi- stream paradigm leads to
an improvement in the recognition performance in noisy
car environments for a wide range of SNR values varying
from 16 dB to -4 dB. These results show that the formant
frequencies are relevant for the recognition process not
only for clean speech, but also for noisy speech, even at
very low SNR values. On the other hand, results showed
also that the use of the auditory-based acoustic distinctive
cues improves the performance of the recognition process
in noisy car environments with respect to the use of only
the MFCCs, their first and second derivatives at high SNR
values, but not for low SNR values.
References
Hesham Tolba, Sid-Ahmed Selouani and Douglas
O?Shaughnessy. 2002. Auditory-based Acoustic Dis-
tinctive Features and Spectral Cues for Automatic
Speech Recognition Using a Multi-Stream Paradigm.
IEEE-ICASSP?2002: 837-840.
Jean Caelen. 1985. Space/Time Data-Information in the
ARIAL Project Ear Model. Speech Communication,
4(1&2): 251-267.
Douglas O?Shaughnessy. 2000. Speech Communication:
Human and Machine. IEEE Press.
Roman Jakobson, Gunnar Fant and Morris Halle. 1951.
Preliminaries to Speech Analysis: The Distinctive Fea-
tures and their Correlates. MIT Press, Cambridge.
Cambridge University Speech Group. 1997. The HTK
Book (Version 2.1.1). Cambridge University Group.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1793?1802, Dublin, Ireland, August 23-29 2014.
Interpolated Dirichlet Class Language Model for Speech Recognition
Incorporating Long-distance N-grams
Md. Akmal Haidar and Douglas O?Shaughnessy
INRS-EMT, University of Quebec
6900-800 De la Gauchetier Ouest, H5A 1K6, Montreal (Quebec), Canada
haidar@emt.inrs.ca, dougo@emt.inrs.ca
Abstract
We propose a language modeling (LM) approach incorporating interpolated distanced n-grams in
a Dirichlet class language model (DCLM) (Chien and Chueh, 2011) for speech recognition. The
DCLM relaxes the bag-of-words assumption and documents topic extraction of latent Dirichlet
allocation (LDA). The latent variable of DCLM reflects the class information of an n-gram event
rather than the topic in LDA. The DCLM model uses default background n-grams where class
information is extracted from the (n-1) history words through Dirichlet distribution in calculat-
ing n-gram probabilities. The model does not capture the long-range information from outside
of the n-gram window that can improve the language modeling performance. In this paper, we
present an interpolated DCLM (IDCLM) by using different distanced n-grams. Here, the class
information is exploited from (n-1) history words through the Dirichlet distribution using in-
terpolated distanced n-grams. A variational Bayesian procedure is introduced to estimate the
IDCLM parameters. We carried out experiments on a continuous speech recognition (CSR) task
using the Wall Street Journal (WSJ) corpus. The proposed approach shows significant perplexity
and word error rate (WER) reductions over the other approach.
1 Introduction
Statistical n-gram LMs have been successfully used for speech recognition and many other applications.
They suffer from insufficiencies of training data and long-distance information, which limit the model
generalization (Chien, 2006). The data sparseness problem is usually solved by backoff smoothing using
lower-order language models (Katz, 1987; Kneser and Ney, 1995). The class-based language model
was investigated where the class n-grams were calculated by considering the generation of concatenated
classes rather than words (Brown et al., 1992). By incorporating the multidimensional word classes
and considering the classes from various positions of left and right contextual information (Bai et al.,
1998), the class n-gram can be improved (Yamamoto et al., 2003). A neural network language model
(NNLM) was trained by linearly projecting the history words of an n-gram event into a continuous
space (Bengio et al., 2003; Schwenk, 2007). Later, a recurrent neural network-based LM was investigated
that shows better results than NNLM (Mikolov et al., 2010; Mikolov et al., 2011). Unsupervised class-
based language models such as Random Forest LM (Xu and Jelinek, 2007), Model M (Chen, 2008) have
been investigated that outperform a word-based LM. However, the long-distance information is captured
by using a cache-based LM that takes advantage of the fact that a word observed earlier in a document
could occur again. This helps to increase the probability of the seen words when predicting the next
word (Kuhn and Mori, 1990).
To compensate for the weakness of the n-gram models, latent topic analysis has been used broadly.
Several techniques such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Bellegarda, 2000),
probabilistic LSA (PLSA) (Hofmann, 1999; Gildea and Hofmann, 1999), and Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003) have been studied to extract the latent semantic information from a training
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1793
corpus. The LSA, PLSA and LDA models have been used successfully in recent research work for LM
adaptation (Bellegarda, 2000; Gildea and Hofmann, 1999; Mrva and Woodland, 2004; Tam and Schultz,
2005; Tam and Schultz, 2006; Haidar and O?Shaughnessy, 2011; Haidar and O?Shaughnessy, 2012b;
Haidar and O?Shaughnessy, 2012a). Even so, the extracted topic information is not directly useful for
speech recognition, where the latent topic of n-gram events should be of concern. In (chien and Chueh,
2008), a latent Dirichlet language model (LDLM) was proposed where the latent topic information was
exploited from (n-1) history words through the Dirichlet distribution in calculating the n-gram proba-
bilities. A topic cache language model was proposed where the topic information was obtained from
long-distance history through multinomial distributions (Chueh and Chien, 2010). Topic-dependent-
class-based n-gram LM was proposed where the LSA method was used to reveal latent topic information
from noun-noun relations (Naptali et al., 2012). In (Bassiou and Kotropoulos, 2010), a PLSA technique
enhanced with long-distance bigrams was used to incorporate the long-term word dependencies in de-
termining word clusters. This technique was used in (Haidar and O?Shaughnessy, 2013b) and (Haidar
and O?Shaughnessy, 2013a) for the PLSA and LDLM models respectively where the long-distance in-
formation was captured by using interpolated distanced n-grams and their parameters were estimated
by using an expectation maximization (EM) procedure (Dempster et al., 1977). In (Chien and Chueh,
2011), the DCLM model was proposed to tackle the data sparseness and to extract the large-span infor-
mation for the n-gram model. In this model, the topic structure in LDA is assumed to derive the hidden
classes of histories in calculating the language model. A Bayesian class-based language model was pre-
sented where a variational Bayes-EM procedure was used to compute the model parameters. Also, a
cache DCLM model was proposed to capture the long-distance information beyond the n-gram window.
However, in the DCLM model (Chien and Chueh, 2011), the class information of the history words was
obtained from the n-gram events of the corpus. Here, the long-range information outside the n-gram
window is not captured. In this paper, we present an IDCLM model to capture the long-range informa-
tion in the DCLM using the interpolated distanced n-grams. The n-gram probabilities of the proposed
IDCLM model are computed by mixing the component distanced word probabilities for classes and the
interpolated class information for histories. Similar to the DCLM model, the parameters of the IDCLM
model are computed by using the variational Bayesian-EM procedure.
The rest of this paper is organized as follows. Section 2 is used for reviewing the DCLM model. The
proposed IDCLM model is described in section 3. The comparison of the IDCLM and the DCLM models
is described in section 4. The experimental details are described in section 5. Finally, the conclusions
and future work are described in section 6.
2 DCLM
LDA is used to compute the document probability by using the topic structure at the document level,
which is inconsistent with the language model for speech recognition where the n-gram regularities are
characterized (Chien and Chueh, 2011). The DCLM was developed to model the n-gram events of the
corpus for speech recognition. In the DCLM, the class structure is described by Dirichlet densities and
estimated from n-gram events. The graphical model of the DCLM for a text corpus that comprises n-
gram events {w
i?1
i?n+1
, w
i
} is described in Figure 1. Here, H and N
h
represent the number of history
events w
i?1
i?n+1
and the number of collected words that occur following the history w
i?1
i?n+1
, respectively.
The (n-1) history words w
i?1
i?n+1
are represented by a (n-1)V ? 1 vector h, consisting of n-1 block
subvectors, with the entries of the seen words assigned to ones and those of unseen words assigned
to zeros (Chien and Chueh, 2011). Here, V represents the size of the vocabulary. The vector h is
then projected into a C-dimensional continuous class space using a class-dependent linear discriminant
function:
g
c
(h) = a
T
c
h (1)
where a
T
c
is the c
th
row vector of matrix A = [a
1
, ? ? ? ,a
C
] (Chien and Chueh, 2011). The function
g
c
(h) describes the class posterior probability p(c|h), which is used in predicting the class information
for an unseen history (Chien and Chueh, 2011). The model can be described as:
1794
      Hh
?A
?
wi N h
ci
Figure 1: The graphical model of the DCLM. Shaded circles represent observed variables.
? For each history vector h, the class information c is drawn from a history-dependent Dirichlet prior
?, which is related to a global projection matrix A:
p(?|h,A) ?
C
?
c=1
?
g
c
(h)?1
c
, (2)
? For each predicted word w
i
of the n-gram events from a multinomial distribution with parameter
?, the associated class c
i
is chosen by using a multinomial distribution with parameter ?. The joint
probability of the variable ?, c
i
, and w
i
conditioned on h can be computed as:
p(?, c
i
, w
i
|h,A,?) = p(?|h,A)p(c
i
|?)p(w
i
|c
i
,?) (3)
? The conditional probability in the n-gram language model can thus be obtained as:
p(w
i
|h,A,?) =
?
p(?|h,A)
C
?
c
i
=1
p(c
i
|?)p(w
i
|c
i
,?)d?, (4)
where the integral is computed as:
p(c
i
|h,A) =
?
p(?|h,A)p(c
i
|?)d? =
g
c
i
(h)
?
C
j=1
g
j
(h)
. (5)
which is an expectation of a Dirichlet distribution of latent class c
i
(Chien and Chueh, 2011).
Therefore, the probability of an n-gram event using the DCLM (Equation 4 and 5) can be written
as (Chien and Chueh, 2011):
p(w
i
|h,A,?) =
C
?
c=1
p(w
i
|c,?)
g
c
(h)
?
C
j=1
g
j
(h)
(6)
The parameters (A,?) of the model are computed by using the variational bayesian EM (VB-EM) pro-
cedure (Chien and Chueh, 2011).
1795
      
?I ?1wi N h1ci
wici N h2
N hL
H I
hI=?d=1
L
hd
ci wi
?2
?L
.
.
AI
Figure 2: The graphical model of the IDCLM. Shaded circles represent observed variables.
3 Proposed IDCLM
The DCLM does not capture the long-range information from outside of the n-gram window (Chien
and Chueh, 2011). To incorporate the long-range information into the DCLM, we propose an IDCLM
where the class information is extracted from interpolated distance n-gram histories through a Dirichlet
distribution in calculating the language model probability. In this model, we interpolate the distanced
n-gram events into the original n-gram events of the DCLM. The graphical model of the IDCLM is
described in Figure 2. In Figure 2, H
I
contains the histories of all the distanced d n-grams, d represents
the distance between words in the n-gram events, and L describes the maximum length of distance d.
When d = 1, the n-grams are the default background n-grams. For example, the distanced tri-grams
of the phrase ?Interpolated Dirichlet Class Language Model for Speech Recognition? are described in
Table 1 for the distance d = 1, 2, 3. Here, the (n-1)V dimensional discrete history vector h
I
is projected
d Trigrams
1 Interpolated Dirichlet Class, Dirichlet Class Language, Class Language Model,
Language Model for, Model for Speech, for Speech Recognition
2 Interpolated Class Model, Dirichlet Language for, Class Model Speech, Language for Recognition
3 Interpolated Language Speech, Dirichlet Model Recognition
Table 1: Distanced tri-grams for the phrase ?Interpolated Dirichlet Class Language Model for Speech
Recognition?
into a C-dimensional continuous class space using a class-dependent linear discriminant function:
g
c
(h
I
) = a
T
c,I
h
I
(7)
1796
where h
I
is the combined histories of all the distanced histories h
d
and is defined as h
I
=
?
L
d=1
h
d
.
Here,
?
represents the logical OR operator. a
T
c,I
is the c
th
row vector of the matrix A
I
and g
c
(h
I
)
describes the class posterior probability p(c|h
I
).
The n-gram probability of the IDCLM model is computed as:
p
I
(w
i
|h
I
,A
I
,?
d
) =
C
?
c
i
=1
{
[
?
d
?
d
p
d
(w
i
|c
i
,?
d
)
]
?
?
p(?
I
|h
I
,A
I
)p(c
i
|?
I
)d?
I
}
=
C
?
c=1
[
?
d
?
d
?
d,ic
]
g
c
(h
I
)
?
C
j=1
g
j
(h
I
)
(8)
where ?
d
are the weights for each component probability estimated on the held-out data using the EM
algorithm (Bassiou and Kotropoulos, 2010; Dempster et al., 1977).
The parameters of the IDCLM model are computed using the variational Bayes EM (VB-EM) proce-
dure by maximizing the marginal distribution of the training data that contains a set of n-gram events
D = {w
i?1
i?n+1
, w
i
}:
log p(D|A
I
,?
d
) =
?
(w
i
,h
I
)?D
log p
I
(w
i
|h
I
,A
I
,?
d
)
=
?
h
I
log
{
?
p(?
I
|h
I
,A
I
)?
[
?
d
N
h
d
?
j=1
C
?
c
j
=1
?
d
p
d
(w
j
|c
j
,?
d
)p(c
j
|?
I
)
]
d?
I
}
(9)
where D contains all the distanced n-gram events, N
h
d
represents the number of collected words that
occur following the history h
d
in d-distanced n-grams. In Equation 9, the summation is over all possible
histories in training samples D. However, directly optimizing the Equation 9 is intractable (Chien and
Chueh, 2011). A variational IDCLM is introduced where the marginal likelihood is approximated by
maximizing the lower bound of Equation 9. The VB-EM procedure is required since the parameter
estimation involves the latent variables of {?
I
, c
h
d
= {c
i
}
N
h
d
i=1
}.
The lower bound L(A
I
,?
d
;
?
?
I
,
?
?
d
) is given by:
?
h
I
{
log ?
(
C
?
c=1
g
c
(h
I
)
)
?
C
?
c=1
log ?(g
c
(h
I
)) +
C
?
c=1
(g
c
(h
I
)? 1)?
(
?(?
h
I
,c
)??
(
C
?
j=1
?
h
I
,j
)
)}
+
?
d
?
h
d
N
h
d
?
i=1
C
?
c=1
?
d
?
h
d
,ic
(
?(?
h
I
,c
)??
(
C
?
j=1
?
h
I
,j
)
)
+
?
d
?
h
d
N
h
d
?
i=1
C
?
c=1
V
?
v=1
?
d
?
h
d
,ic
?(w
v
, w
i
) log ?
d,vc
?
?
h
I
{
log ?
(
C
?
c=1
?
h
I
,c
)
?
C
?
c=1
log ?(?
h
I
,c
)
+
C
?
c=1
(?
h
I
,c
? 1)
(
?(?
h
I
,c
)??
(
C
?
j=1
?
h
I
,j
)
)}
?
?
d
?
h
d
N
h
d
?
i=1
C
?
c=1
?
d
?
h
d
,ic
log ?
h
d
,ic
where ?(.) is the derivative of the log gamma function, and is known as a digamma function (Chien
and Chueh, 2011). The history-dependent variational parameters {
?
?
h
I
= ??
h
I
,c
,
?
?
h
d
=
?
?
h
d
,vc
}, corre-
sponding to the latent variables ?
I
, c
h,d
, are then estimated in the VB-E step by setting the differentials
(?L(?))/(??
h
I
,c
) and (?L(?))/(??
h
d
,ic
) to zero respectively (Chien and Chueh, 2011):
??
h
I
,c
= g
c
(h
I
) +
?
d
N
h
d
?
i=1
?
d
?
h
d
,ic
(10)
1797
??
h
d
,ic
=
?
d,ic
exp
[
?(?
h
I
,c
)??(
?
C
j=1
?
h
I
,j
)
]
?
C
l=1
?
d,il
exp
[
?(?
h
I
,l
)??(
?
C
j=1
?
h
I
,j
)
]
(11)
In computing
?
?
h
d
,ic
the corresponding ?
h
d
,c
is used in Equation 11. With the updated
?
?
h
I
,
?
?
h
d
in the
VB-E step, the IDCLM parameters {A
I
,?
d
} are estimated in the VB-M step as (Chien and Chueh,
2011):
?
?
d,vc
=
?
h
d
?
N
h
d
i=1
?
d
?
?
h
d
,ic
?(w
v
, w
i
)
?
V
m=1
?
h
d
?
N
h
d
i=1
?
d
?
?
h
d
,ic
?(w
m
, w
i
)
(12)
where
?
V
v=1
?
d,vc
=1 and ?(w
v
, w
i
) is the Kronecker delta function that equals one when vocabulary
word w
v
is identical to the predicted word w
i
and equals zero otherwise. The gradient ascent algorithm
is used to calculate the parameters
?
A
I
= [a?
1,I
, ? ? ? , a?
C,I
] by updating the gradient 5
a
c,I
as (Chien and
Chueh, 2011):
5
a
c,I
?5
a
c,I
+
?
h
I
[
?
(
C
?
j=1
g
j
(h
I
)
)
??(g
c
(h
I
)) + ?(??
h
I
,c
)??
(
C
?
j=1
??
h
I
,j
)
]
.h
I
(13)
The n-gram probabilities p
t
(w
i
,h
t
,A
I
,?
d
) of the test document t are then computed using Equa-
tion 8. To capture the local lexical regularities, the model p
t
(w
i
|h
t
,A
I
,?
d
) is then interpolated with the
background trigram model as:
p
Interpolated
(w
i
|h) = ?p
Background
(w
i
|h) + (1? ?)p
t
(w
i
|h
t
,A
I
,?
d
) (14)
4 Comparison of DCLM and IDCLM Models
In the DCLM model, the class information for the (n? 1) history words is obtained by using the n-gram
counts in the corpus. The current word is predicted from the history-dependent Dirichlet parameter,
which is controlled by a matrix A and corpus-based histories h (Chien and Chueh, 2011). In contrast,
the IDCLM model captures long-range information by incorporating distanced n-grams. Here, the class
information is exploited for the interpolated (n ? 1) history words h
I
that are obtained from all the
distanced n-gram events. Both the DCLM and IDCLM exploit the word distribution given the history
words. They perform the history clustering of the corpus. For the DCLM model, the number of parame-
ters {A,?} increases linearly with the number of history words and is given by (n? 1)CV + CV . For
the IDCLM model, the number of parameters {A
I
,?
d
} increases linearly with the number of history
words and distance d and is given by ((n? 1)CV +CV d). The time complexity of DCLM and IDCLM
are O(HV C) and O(H
I
V Cd) with H corpus-based histories, H
I
corpus-based interpolated histories,
V vocabulary words, d distances and C classes.
5 Experiments
5.1 Data and experimental setup
The LM approaches are evaluated using the Wall Street Journal (WSJ) corpus (Paul and Baker, 1992).
The SRILM toolkit (Stolcke, 2002) and the HTK toolkit (Young et al., 2013) are used for generating the
LMs and computing the WER respectively. The ?87-89 WSJ corpus is used to train language models.
The background trigrams are trained using the back-off version of the Witten-Bell smoothing; the 5K
non-verbalized punctuation closed vocabulary. We train the trigram IDCLM model using L = 2 and
L = 3. Ten EM iterations in the VB-EM procedure were used. The initial values of the entries in the
matrix ?,?
d
were set to be 1/V and those in A,A
I
were randomly set in the range [0,1]. To update
the variational parameters in the VB-E step, one iteration was used. The VB-M step was executed to
update the parameters A,A
I
by three iterations (Chien and Chueh, 2011). To capture the local lexi-
cal regularity, trigrams of various methods are interpolated with the background trigrams. The acoustic
model from (Vertanen, 2013) is used in our experiments. The acoustic model is trained by using all
WSJ and TIMIT (Garofolo et al., 1993) training data, the 40-phone set of the CMU dictionary (-, 2013),
1798
approximately 10000 tied-states, 32 Gaussians per state and 64 Gaussians per silence state. The acous-
tic waveforms are parameterized into a 39-dimensional feature vector consisting of 12 cepstral coeffi-
cients plus the 0
th
cepstral, delta and delta delta coefficients, normalized using cepstral mean subtraction
(MFCC
0?D?A?Z
). We evaluated the cross-word models. The values of the word insertion penalty,
beam width, and the language model scale factor are -4.0, 350.0, and 15.0 respectively (Vertanen, 2013).
The interpolation weights ?
d
and ? are computed by optimizing on the held-out data according to the
metric of perplexity. The experiments are evaluated on the evaluation test, which is a total of 330 test
utterances from the November 1992 ARPA CSR benchmark test data for vocabularies of 5K words (Paul
and Baker, 1992; Woodland et al., 1994).
5.2 Experimental Results
Due to the higher memory and training time requirements for the IDCLM model, we trained the DCLM
and IDCLM models for class sizes of 10 and 20. The perplexity and WER results are described in Table 2
and Figure 3 respectively.
Language Model 10 Classes 20 Classes
Background (B) 109.41 109.41
B+Class 106.65 106.97
B+DCLM 100.20 100.45
B+IDCLM (L=2) 98.01 97.94
B+IDCLM (L=3) 95.63 95.43
Table 2: Perplexity results of the models
 ? ??
?
 
?
?
?
?
?
?
?
 
?
?
?
?
?
?
?? ?? ?? ???? ? ?? ?
?? ? ?? ???? ?? ?? ???? ?? ?? ??
? ?? ? ??????? ?? ?? ????? ?? ? ???? ? ?? ????? ?? ? ?????? ?????
?? ? ?????? ??? ???? ????? ?? ? ?????? ??? ???? ?????
Figure 3: WER results for different class sizes
From Table 2, we can note the proposed IDCLM model outperforms the other models for all class
sizes. The performance of IDCLM improves with more distances (L = 3).
We evaluated the WER experiments using lattice rescoring. In the first pass decoding, we used the
background trigram for lattice generation. In the second pass, we applied the interpolated model for
lattice rescoring. The WER results are described in Figure 3. From Figure 3, we can note that the
proposed IDCLM (L = 3) model yields a WER reduction of about 34.54% (5.79% to 3.79%), 33.5%
(5.7% to 3.79%), and 9.76% (4.2% to 3.79%) for 10 classes and about 33.85% (5.79% to 3.83%), 32.8%
1799
(5.7% to 3.83%), and 11.34% (4.32% to 3.83%) over the background trigram, class trigram (Brown et al.,
1992), and the DCLM (Chien and Chueh, 2011) approaches respectively. The significance improvement
in WER is done by using a match-pair-test where the misrecognized words in each test utterance are
counted. The p-values are described in Table 3. From Table 3, we can note that the IDCLM (L = 2)
Language Model 10 Classes 20 Classes
B+Class & B+IDCLM (L=2) 3.8E-10 4.3E-10
B+Class & B+IDCLM (L=3) 4.7E-12 4.7E-12
B+DCLM & B+IDCLM (L=2) 0.04 0.01
B+DCLM & B+IDCLM (L=3) 0.004 0.006
Table 3: p-values obtained from the match-pair test on the WER results
is statistically significant to the class-based LM (Brown et al., 1992) and DCLM (Chien and Chueh,
2011) at a significance level of 0.01 and 0.05 respectively. However, the IDCLM (L = 3) model is
statistically significant to the above models at a significance level of 0.01. We have also seen that the
cache DCLM model also gives the same results as DCLM (Chien and Chueh, 2011) for smaller number
of classes (Chien and Chueh, 2011).
6 Conclusions and Future Work
In this paper, we proposed an integration of distanced n-grams into the original DCLM model (Chien
and Chueh, 2011). The DCLM model (Chien and Chueh, 2011) extracted the class information from the
(n-1) history words through a Dirichlet distribution in calculating the n-gram probabilities. However, it
does not capture the long-range semantic information from outside of the n-gram events. The proposed
IDCLM overcomes the shortcomings of DCLM by incorporating the interpolated long-distance n-grams
that capture the long-term word dependencies. Using the IDCLM, the class information for the histories
is trained using the interpolated distanced n-grams. The IDCLM yields better results with including
more distances (L = 3). The model probabilities are computed by weighting the component word
probabilities for classes and the interpolated class information for histories. A variational Bayesian EM
(VB-EM) procedure is presented to estimate the model parameters.
For future work, we will evaluate the proposed approach with neural network-based language mod-
els and exponential class-based language models. Furthermore, we will find out a way to perform the
experiments for higher numbers of classes.
References
-. 2013. The Carnegie Mellon University (CMU) Pronounciation Dictionary. http://www.speech.cs.
cmu.edu/cgi-bin/cmudict.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM
Algorithm. Journal of the Royal Statistical Society, Series B 39(1):1 ? 38.
Andreas Stolcke. 2002. SRILM-an Extensible Language Modeling Toolkit. In Proceedings of ICSLP, pages
901?904.
Chuang-H. Chueh and Jen-T. Chien. 2010. Topic Cache Language Model for Speech Recognition. In Proc. of
ICASSP, pages 5194?5197.
Daniel Gildea and Thomas Hofmann. 1999. Topic-based Language Models using EM. In Proceedings of EU-
ROSPEECH, pages 2167?2170.
David Mrva and Philip C. Woodland. 2004. A PLSA-based Language Model for Conversational Telephone
Speech. In Proc. of ICSLP, pages 2257?2260.
David M. Blei, Andrew Y. Ng., and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3:993?1022.
1800
Dougls B. Paul and Janet M. Baker. 1992. The Design for the Wall Street Journal-based CSR Corpus. In Proc. of
ICSLP, pages 899?902.
Hirofumi Yamamoto, Shuntaro Isogai, and Yoshinori Sagisaka. 2003. Multi-class Composite n-gram Language
Model. Speech Communication, 41:369 ? 379.
Holger Schwenk. 2007. Continuous Space Language Models. Computer Speech and Language, 21:492 ? 518.
Jen-T. Chien and Chuang-H. Chueh. 2008. Latent Dirichlet Language Model for Speech Recognition. In Proc. of
IEEE SLT Workshop, pages 201?204.
Jen-T. Chien and Chuang-H. Chueh. 2011. Dirichlet Class Language Models for Speech Recognition. IEEE
Trans. on Audio, Speech and Language Processing, 19(3):482 ? 495.
Jen-T. Chien. 2006. Association Pattern Language Modeling. IEEE Trans. on Audio, Speech and Language
Processing, 14(5):1719 ? 1728.
Jerome R. Bellegarda. 2000. Exploiting Latent Semantic Information in Statistical Language modeling. IEEE
Transactions on Speech and Audio Processing, 88 (8):1279?1296.
John S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathan G. Fiscus, David S. Pallett, Nancy L. Dahlgren, and
Victor Zue. 1993. TIMIT Acoustic-phonetic Continuous Speech Corpus. Linguistic Data Consortium.
Keith Vertanen. 2013. HTK Wall Street Journal Training Recipe. http://www.keithv.com/software/
htk/us/.
Md. A. Haidar and Douglas O?Shaughnessy. 2011. Unsupervised Language Model Adaptation using N-gram
Weighting. In Proceedings of CCECE, pages 857?860.
Md. A. Haidar and Douglas O?Shaughnessy. 2012a. LDA-based LM Adaptation using Latent Semantic Marginals
and Minimum Discrimination Information. In Proceedings of EUSIPCO, pages 2040?2044.
Md. A. Haidar and Douglas O?Shaughnessy. 2012b. Topic N-gram Count Language Model for Speech Recogni-
tion. In Proceedings of IEEE Spoken Language Technology (SLT) Workshop, pages 165?169.
Md. A. Haidar and Douglas O?Shaughnessy. 2013a. Fitting Long-range Information using Interpolated Distanced
n-grams and Cache Models into a Latent Dirichlet Language Model for Speech Recognition. In Proc. of IN-
TERSPEECH, pages 2678?2682.
Md. A. Haidar and Douglas O?Shaughnessy. 2013b. PLSA Enhance with a Long-distance Bigram Language
Model for Speech Recognition. In Proc. of EUSIPCO.
Nikoletta Bassiou and Constantine Kotropoulos. 2010. Word Clustering PLSA Enhanced with Long Distance
Bigrams. In Proc. of International Conferance on Pattern Recognition, pages 4226?4229.
P.C. Woodland, J. J. Odell, V. Valtchev, and S. J. Young. 1994. Large Vocabulary Continuous Speech Recognition
using HTK. In Proceedings of ICASSP, pages 125?128.
Peng Xu and Frederick Jelinek. 2007. Random Forests and the Data Sparseness Problem in Language Modeling.
Computer Speech and Language, 21 (1):105 ? 152.
Peter F. Brown, Vincent Della Pietra, Peter De Souza, Jenifer Lai, and Robert L. Mercer. 1992. Classbased n-gram
Models of Natural Language. Computational Linguist., 18 (4):467 ? 479.
Reinhard Kneser and Hermann Ney. 1995. Improved Backing-off for m-gram Language Modeling. In Proc. IEEE
Int Conf. Acoust., Speech, Signal Process., pages 181?184.
Roland Kuhn and Renato D. Mori. 1990. A Cache-based Natural Language Model for Speech Recognition. IEEE
Transactions of Pattern Analysis and Machine Intelligence, 12 (6):570?583.
Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391 ?
407.
Shuanghu Bai, Haizhou Li, Zhiwei Lin, and Baosheng Yuan. 1998. Building Class-based Language Models with
Contextual Statistics. In Proc. IEEE Int Conf. Acoust., Speech, Signal Process, pages 173?176.
1801
Slava M. Katz. 1987. Estimation of Probabilities from Sparse Data for the Language Model Component of a
Speech Recognizer. EEE Trans. Acoust., Speech, Signal Process., 35(3):400 ? 401.
Stanley Chen, 2008. Performance Prediction for Exponential Language Models. Tech. Rep. RC 24671, IBM
Research, Tech. Rep.
Steve Young, Phil Woodland, Gunnar Evermann, and Mark Gales. 2013. The HTK Toolkit 3.4.1. http://htk.
eng.cam.ac.uk/.
Thomas Hofmann. 1999. Probabilistic Latent Semantic Analysis. In Proceedings of the Fifteenth Annual Confer-
ence on Uncertainty in Artificial Intelligence (UAI-99), pages 289?296, San Francisco, CA. Morgan Kaufmann.
Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan H. Cernocky, and Sanjeev Khudanpur. 2010. Recurrent
Neural Network Based Language Model. In Proc. of INTERSPEECH, pages 1045?1048.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan H. Cernocky, and Sanjeev Khudanpur. 2011. Extensions
Recurrent Neural Network Language Model. In Proc. of ICASSP, pages 5528?5531.
Welly Naptali, Masatoshi Tsuchiya, and Seiichi Nakagawa. 2012. Topic Dependent Class-based n-gram Language
Model. IEEE Trans. on Audio, Speech and Language Processing, 20:1513 ? 1525.
Yik-Cheung Tam and Tanja Schultz. 2005. Dynamic Language Model Adaptation using Variational Bayes Infer-
ence. In Proceedings of INTERSPEECH, pages 5?8.
Yik-Cheung Tam and Tanja Schultz. 2006. Unsupervised Language Model Adaptation using Latent Semantic
Marginals. In Proceedings of INTERSPEECH, pages 2206?2209.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A Neural Probabilistic Language
Model. Journal of Machine Learning Research, 3:1137 ? 1155.
1802
