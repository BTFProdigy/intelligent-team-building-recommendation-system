Using Gene Expression Programming to Construct Sentence Ranking 
Functions for Text Summarization  
Zhuli Xie, Xin Li, Barbara Di Eugenio, 
Peter C. Nelson 
Department of Computer Science 
University of Illinois at Chicago 
Chicago, IL 60607, U.S.A. 
zxie@cs.uic.edu, xli1@cs.uic.edu,  
bdieugen@cs.uic.edu, nelson@cs.uic.edu 
Weimin Xiao, Thomas M. Tirpak 
Physical Realization Research Center of 
Motorola Labs 
Schaumburg, IL 60196, U.S.A. 
awx003@motorola.com, 
T.Tirpak@motorola.com 
  
Abstract 
In this paper, we consider the automatic text 
summarization as a challenging task of ma-
chine learning. We proposed a novel summari-
zation system architecture which employs 
Gene Expression Programming technique as its 
learning mechanism. The preliminary experi-
mental results have shown that our prototype 
system outperforms the baseline systems. 
1 Introduction 
Automatic text summarization has been studied for 
decades (Edmundson 1969) and is still a very active 
area (Salton et al 1994; Kupiec et al 1995; Bran-
dow et al 1995; Lin 1999; Aone et al 1999; Sekine 
and Nobata 2001; Mani 2001; McKeown et al 2001; 
Radev et al 2003). Only a few have tried using ma-
chine learning to accomplish this difficult task (Lin 
1999; Aone et al 1999; Neto et al 2002). Most re-
search falls into combining statistical methods with 
linguistic analysis. We regard the summarization as 
a problem of empowering a machine to learn from 
human-summarized text documents. We employ an 
evolutionary algorithm, Gene Expression Program-
ming (GEP) (Ferreira 2001), as the learning mecha-
nism in our Adaptive Text Summarization (ATS) 
system to learn sentence ranking functions. Even 
though our system generates extractive summaries, 
the sentence ranking function in use differentiates 
ours from that of (Edmundson 1969; Sekine and 
Nobata. 1999; Goldstein et al 1999) who specified it 
to be a linear function of sentence features.  We used 
GEP to generate a sentence ranking function from 
the training data and applied it to the test data, which 
also differs from (Lin 1999) who used decision tree, 
(Aone et al 1999; Kupiec et al 1995) who used 
Bayes?s rule, and (Neto et al 2002) who imple-
mented both Na?ve Bayes and decision tree C4.5. 
This paper presents our approach, details the sys-
tem architecture, and discusses preliminary experi-
mental results. Conclusions and future work are 
outlined at the end. 
2 Background 
2.1 Gene Expression Programming 
Gene Expression Programming (GEP), first intro-
duced by (Ferreira 2001), is an evolutionary algo-
rithm that evolves computer programs and predicts 
mathematical models from experimental data. The 
algorithm is similar to Genetic Programming (GP), 
but uses fixed-length character strings (called chro-
mosomes) to represent computer programs which are 
afterwards expressed as expression trees (ETs). GEP 
begins with a random population of candidate 
solutions in the form of chromosomes. The 
chromosomes are then mapped into ETs, evaluated 
based on a fitness function and selected by fitness to 
reproduce with modification via genetic operations. 
The new generation of solutions goes through the 
same process until the stop condition is satisfied. 
The fittest individual serves as the final solution. 
GEP has been used to solve symbolic regression, 
sequence induction, and classification problems effi-
ciently (Ferreira 2002; Zhou 2003). We utilized GEP 
to find the explicit form of sentence ranking func-
tions for the automatic text summarization. 
2.2 Sentence Features 
In our current system, every sentence s is repre-
sented by five normalized features: 
? Location of the Paragraph (P): 
MYP /=                               (1) 
where M is the total number of paragraphs in a 
document; Y is the index of the paragraph s belongs 
to. 
? Location of the Sentence (S): 
NXS /=                        (2) 
where N is the total number of sentences in the 
paragraph; X is the index of sentence s.   
? Length of the Sentence (L): 
The length of the sentence is the number of words it 
contained, i.e., l(s), normalized by Sigmoid function: 
   
))((
))(()(
,
1
1
slstd
slsl
e
eL
???
? ?=+
?= ?
?
                               (3) 
Where u(l(s)) is the average length of sentences, and 
std(l(s)) is the standard deviation of the sentence 
lengths. 
? Heading Sentence (H): 
H = 1, if s is a title, subtitle or heading, 0 otherwise.   
? Content-word Frequencies (F): 
))((
))(()(,
1
1)(
sCWstd
sCWsCW
e
esF ???
? ?=+
?= ?
?
         (4) 
?
=
??=
k
i
ii swwFreqsCW
1
.)],(log[)(                         (5)                              
where Freq(wi) is the frequency of wi in that docu-
ment; ?(CW(S)) is the mean of all the sentence 
scores, and std(CW(s)) is the standard deviation.  
2.3 Sentence ranking function 
We assume that for a certain type of documents, the 
mechanism to perform summarization would be the 
same. Therefore, we only need to find one algorithm 
that links a collection of documents and their corre-
sponding summaries. We process the text summari-
zation learning task in two stages: training and 
testing.  In the training stage, a set of training docu-
ments with their summaries are provided, and the 
text features are preprocessed using statistical meth-
ods and natural language processing methods as de-
fined in 2.2, then each sentence in a document is 
scored based on a sentence ranking function con-
structed by GEP.  Fitness value of the summariza-
tion task is the similarity between the summary 
produced by the machine and the summarization text 
of training document. The top n ranked sentences1 
                                                          
1 The number of sentences extracted by the GEP module can be 
a variable, which is decided by the required number of words in 
a summary. Or it can be a specified percentage of the total num-
ber of sentences in the document. 
will be returned as the summary of that document 
and presented in their nature order. In the testing 
stage, a different document set is supplied to test the 
similarity between the machine summarized text and 
the human or other system summarized text.   
3 System Architecture 
In addition to the traditional way of extracting the 
highest ranked sentences in a document to compose 
a summary as in (Edmundson 1969; Lin 1999; 
Kupiec et al 1995; Brandow 1995; Zechner 1996), 
we embedded a machine learning mechanism in our 
system. The system architecture is shown in Figure 1 
where the GEP module is highlighted. In the training 
stage, each of the training documents is passed to the 
GEP module after being preprocessed into a set of 
sentence feature vectors. The GEP runs m genera-
tions, and in each generation a population of p sen-
tence scoring functions in the form of chromosomes 
in GEP is generated. Every candidate scoring func-
tion is then applied to sentence feature vectors from 
every training document and produces a score ac-
cordingly.  Then all sentences in the same training 
document are ranked according to their scores, and n 
sentences with top scores are selected as an extract.  
The next step is to measure how similar the extract 
is to the objective summary. As discussed by 
(McLellan et al 2001; Goldstein et al 1999; McKe-
own et al 2001), evaluating the quality of a sum-
mary often requires involvement of human subjects.  
This is almost impractical in a machine learning 
procedure.  Thus we chose an alternative similarity 
measure as the approximation, i.e. a cosine function 
that is often seen in Information Retrieval to calcu-
late the relevance of two documents, to compute the 
similarity between an extract and the objective 
summary. We compute the similarity values for each 
of the obtained extracts and their objective summa-
ries respectively, and feed the results into the Fitness 
Calculation module to get a fitness measure for the 
current candidate sentence ranking function under 
consideration: 
)),(( ii OESimilarityAvgFitness = ,                   (6) 
where Ei is the extract of the i-th document in the 
training set and Oi is its objective summary.  
After the fitness value for every chromosome in 
the current generation is computed, the GEP popula-
tion undergoes all genetic operators to produce the 
next generation. After the specified number of gen-
erations has been reached, the final best chromo-
some is returned as an optimal sentence ranking 
function for the training set and is ready to use in a 
test document to produce an extractive summary.  
4 Experiments 
We randomly selected 60 documents from the 
CMPLG corpus2 for our experiments. The only re-
striction is that each document has an abstract pro-
vided which will serve as the objective summary. 
Among these 60 documents, 50 are used for training 
and the remaining 10 are used for testing. The func-
tion set for the GEP to evolve sentence ranking func-
tions includes (+, -, *, /, power, sqrt, exp, log, min, 
max, and constant 1, 2, 3, 5, 7). The length of the 
chromosome is 128.  Other GEP control parameters 
are set as follows: population, 256; probability of 
crossover, 0.5; probability of mutation, 0.2; prob-
ability of rotation, 0.2; generations, 10,000-50,000 
(in five runs).  Our system has produced a five-
sentence extractive summary for each of the testing 
documents, and calculated the similarity between the 
produced summary and the abstract coming along 
with the document.  
  Ideally, we would like to compare our system with 
other summarizers. However, due to the 
unavailability of other summarization systems to 
perform the same task, we designed three baseline 
methods, namely lead-based, randomly-selected, and 
random-lead-based, to generate summaries for per-
formance comparison, which were also adopted by 
(Brandow et al 1995; Zechner 1996; Radev et al 
2003).  The baseline methods are detailed as 
 
                                                          
2 CMPLG corpus is composed of 183 documents from the Com-
putation and Language (cmp-lg) collection, which has been 
marked up in XML. The documents are scientific papers which 
appeared in association for Computational Linguistics (ACL) 
sponsored conferences. 
follows:  
o The lead-based method selects the first sen-
tences from the first five paragraphs as the 
summary of each of the testing documents.   
o The randomly-selected method chooses five sen-
tences from a document at random to compose a 
summary.  
o The random-lead-based method chooses five 
sentences among the first sentences from all 
paragraphs in the document at random.   
We performed the random selection 1,000 times, 
and calculated the average similarity of the testing 
documents for each of the random-based methods.  
The experimental results are plotted in Figure 2, 
which have demonstrated that our system outper-
forms all three baseline methods.   
Figure 2. Similarity Comparison
0.442
0.279
0.17 0.181
0
0.1
0.2
0.3
0.4
0.5
ATS Lead-based Rand-
selected
Rand-Lead-
based
Syste m
Si
m
ila
ri
ty
 
  One sample sentence scoring function learned by the 
GEP is as follows: 
)3(
7)( +
?=
PF
Ssscore ,                 (7). 
5 Conclusions and Future Work 
In this paper, we have presented a prototype summa-
rization system which employs GEP as its learning 
mechanism for sentence ranking function.  In the 
preliminary experiments for performance testing, 
our system outperforms the baseline methods by 
58%-160% when generating summaries for 10 
documents. However, the value of the average simi-
larity gained by our system is not as high as we 
would like. The reason most likely lies in the fact 
that the styles of the objective summaries written by 
humans vary a lot or even conflict with each other.  
In other words, they do not possess many common 
features that are a must for high value of similarity 
between two texts. Using content-words and the co-
sine function to measure the similarity may not be an 
ideal evaluation metric, neither is it an ideal fitness 
measure in the GEP learning mechanism.  Our future 
           Preprocess
Objective 
Summaries 
Training 
Documents 
GEP 
Generate Scor-
ing Function 
Fitness 
Calculation
Extracts
Test 
Document 
 Post-
process 
Summarizer 
Sentence Scor-
ing Function 
 Extractive  
 Summary 
Figure 1: System Architecture 
research will further study what kinds of similarity 
measure can be obtained from raw texts without in-
volvement of human subjects.  Moreover, we plan to 
cluster collected documents to make every cluster 
contains articles summarized in a similar style. We 
will also explore other sentence features, such as 
sentence cohesion, semantic meaning, and rhetorical 
relations, for an ideal uniform sentence ranking 
function. 
6 Acknowledgements 
Our thanks go to the Physical Realization Research 
Center of Motorola Labs for their support of this 
research project. 
References  
Aone, C., Gorlinsky, J., Larsen, B., and Okurowski, 
M. E. 1999. A Trainable Summarizer with Knowl-
edge Acquried from Robust NLP Techniques, Ad-
vances in Automatic Text Summarization, pages 
71-80. The MIT Press, Cambridge, Massachusetts. 
Brandow, R., Mitze, K., and Rau, L. F.  1995. 
Automatic condensation of electronic publications 
by sentence selection.  Information Processing 
and Management, 31(5):675-685. 
Edmundson, H. 1969. New methods in automatic 
abstracting. Journal of ACM, 16(2):264-285. 
Ferreira, C. 2001. Gene Expression Programming: A 
New Adaptive Algorithm for solving problems.  
Complex Systems, 13(2):87-129. 
Ferreira, C. 2002. Gene Expression Programming: 
Mathematical Modeling by an Artificial Intelli-
gence. Angra do Heroismo, Portugal 
Goldstein, J., Kantrowitz, M., Mittal, V., and Car-
bonell, J.  1999.  Summarizing Text Documents: 
Sentence Selection and Evaluation Metrics, in 
Proc. SIGIR ?99, pages 121-128.  Berkeley, Cali-
fornia. 
Kupiec, J., Pedersen, J., and Chen, F.  1995. A train-
able document summarizer. In Proc. 18th ACM-
SIGIR Conference, pages 68-73.  Seattle, Wash-
ington. 
Lin, C. 1999. Training a Selection Function for Ex-
traction. In the 8th International Conference on 
Information and Knowledge Management (CIKM 
99), Kansa City, Missouri. 
Mani, I.  2001.  Automatic Summarization, John 
Benjamins Publishing Company, Amster-
dam/Philadelphia. 
McKeown, K. R., Barzilay, R., Evans, D., Hatzivas-
siloglou, V., Kan, M. Y., Schiffman, B., Teufel, S. 
2001.  Columbia Multi-Document Summarization: 
Approach and Evaluation, in Proceedings of the 
Document Understanding Conference (DUC01).  
Edmonton, Canada. 
McLellan, P., Tombros, A., Jose, J., Ounis, I., and 
Whitehead, M. 2001. Evaluating summarisation 
technologies: A task-oriented approach.  In Proc. 
1st International Workshop on New Developments 
in Digital Libraries (NDDL-2001), International 
Conference on Enterprise Information Systems 
(ICEIS 2001), pages 99-112. Setubal, Portugal. 
Neto, J. L., Freitas, A. A., and Kaestner, C. A. A. 
2002. Automatic Text Summarization using a Ma-
chine Learning Approach. In Proc. 16th Brazilian 
Symp. on Artificial Intelligence (SBIA-2002). Lec-
ture Notes in Artificial Intelligence 2507, pp205-
215. Springer-Verlag. 
Radev, D. R., Teufel, S., Saggion, H., Lam, W., 
Blitzer, J., Qi, H., Celebi, A., Liu, D., and Drabek, 
E. 2003. Evaluation challenges in large-scale 
document summarization, in Proc. 41st Annual 
Meeting of the Association for Computational 
Linguistics, pages 375-382.  Sapporo, Japan. 
Salton, G., Allan, J., Buckley, C., and Singhal, A.  
1994. Automatic Analysis, Theme Generation, and 
Summarization of Machine-Readable Texts.  Sci-
ence, 264(3):1421-1426. 
Sekine, S. and Nobata, C. 2001. Sentence Extraction 
with Information Extraction technique. In Proc. of 
ACM SIGIR'01 Workshop on Text Summarization. 
New Orleans. 
Zechner, K. 1996. Fast generation of abstracts from 
general domain text corpora by extracting relevant 
sentences. In Proc. COLING-96, pages 986-989. 
Copenhagen, Denmark. 
Zhou, C., Xiao, W., Tirpak, T. M., and Nelson, P. C.  
2003. Evolving Classification Rules with Gene 
Expression Programming.  IEEE Transactions on 
Evolutionary Computation, 7(6):519 ? 531. 
c? 2004 Association for Computational Linguistics
Squibs and Discussions
The Kappa Statistic: A Second Look
Barbara Di Eugenio? Michael Glass?
University of Illinois at Chicago Valparaiso University
In recent years, the kappa coefficient of agreement has become the de facto standard for evaluating
intercoder agreement for tagging tasks. In this squib, we highlight issues that affect ? and that
the community has largely neglected. First, we discuss the assumptions underlying different
computations of the expected agreement component of ?. Second, we discuss how prevalence and
bias affect the ? measure.
In the last few years, coded corpora have acquired an increasing importance in ev-
ery aspect of human-language technology. Tagging for many phenomena, such as
dialogue acts (Carletta et al 1997; Di Eugenio et al 2000), requires coders to make
subtle distinctions among categories. The objectivity of these decisions can be as-
sessed by evaluating the reliability of the tagging, namely, whether the coders reach
a satisfying level of agreement when they perform the same coding task. Currently,
the de facto standard for assessing intercoder agreement is the ? coefficient, which
factors out expected agreement (Cohen 1960; Krippendorff 1980). ? had long been
used in content analysis and medicine (e.g., in psychiatry to assess how well stu-
dents? diagnoses on a set of test cases agree with expert answers) (Grove et al 1981).
Carletta (1996) deserves the credit for bringing ? to the attention of computational
linguists.
? is computed as P(A)? P(E)1? P(E) , where P(A) is the observed agreement among the
coders, and P(E) is the expected agreement, that is, P(E) represents the probabil-
ity that the coders agree by chance. The values of ? are constrained to the inter-
val [?1, 1]. A ? value of one means perfect agreement, a ? value of zero means
that agreement is equal to chance, and a ? value of negative one means ?perfect?
disagreement.
This squib addresses two issues that have been neglected in the computational
linguistics literature. First, there are two main ways of computing P(E), the expected
agreement, according to whether the distribution of proportions over the categories
is taken to be equal for the coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel
and Castellan 1988) or not (Cohen 1960). Clearly, the two approaches reflect different
conceptualizations of the problem. We believe the distinction between the two is often
glossed over because in practice the two computations of P(E) produce very similar
outcomes in most cases, especially for the highest values of ?. However, first, we
will show that they can indeed result in different values of ?, that we will call ?Co
(Cohen 1960) and ?S&C (Siegel and Castellan 1988). These different values can lead
to contradictory conclusions on intercoder agreement. Moreover, the assumption of
? Computer Science, 1120 SEO (M/C 152), 851 South Morgan Street, Chicago, IL 60607. E-mail:
bdieugen@uic.edu.
? Mathematics and Computer Science, 116 Gellerson Hall, Valparaiso, IN 46383. E-mail: michael.glass@
valpo.edu.
96
Computational Linguistics Volume 30, Number 1
equal distributions over the categories masks the exact source of disagreement among
the coders. Thus, such an assumption is detrimental if such systematic disagreements
are to be used to improve the coding scheme (Wiebe, Bruce, and O?Hara 1999).
Second, ? is affected by skewed distributions of categories (the prevalence prob-
lem) and by the degree to which the coders disagree (the bias problem). That is, for
a fixed P(A), the values of ? vary substantially in the presence of prevalence, bias, or
both.
We will conclude by suggesting that ?Co is a better choice than ?S&C in those studies
in which the assumption of equal distributions underlying ?S&C does not hold: the vast
majority, if not all, of discourse- and dialogue-tagging efforts. However, as ?Co suffers
from the bias problem but ?S&C does not, ?S&C should be reported too, as well as a third
measure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993).
1. The Computation of P(E)
P(E) is the probability of agreement among coders due to chance. The literature de-
scribes two different methods for estimating a probability distribution for random
assignment of categories. In the first, each coder has a personal distribution, based
on that coder?s distribution of categories (Cohen 1960). In the second, there is one
distribution for all coders, derived from the total proportions of categories assigned
by all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988).1
We now illustrate the computation of P(E) according to these two methods. We
will then show that the resulting ?Co and ?S&C may straddle one of the significant
thresholds used to assess the raw ? values.
The assumptions underlying these two methods are made tangible in the way
the data are visualized, in a contingency table for Cohen, and in what we will call
an agreement table for the others. Consider the following situation. Two coders2
code 150 occurrences of Okay and assign to them one of the two labels Accept or
Ack(nowledgement) (Allen and Core 1997). The two coders label 70 occurrences as Ac-
cept, and another 55 as Ack. They disagree on 25 occurrences, which one coder labels
as Ack, and the other as Accept. In Figure 1, this example is encoded by the top contin-
gency table on the left (labeled Example 1) and the agreement table on the right. The
contingency table directly mirrors our description. The agreement table is an N ? m
matrix, where N is the number of items in the data set and m is the number of labels
that can be assigned to each object; in our example, N = 150 and m = 2. Each entry nij
is the number of codings of label j to item i. The agreement table in Figure 1 shows
that occurrences 1 through 70 have been labeled as Accept by both coders, 71 through
125 as Ack by both coders, and 126 to 150 differ in their labels.
1 To be precise, Krippendorff uses a computation very similar to Siegel and Castellan?s to produce a
statistic called alpha. Krippendorff computes P(E) (called 1 ? De in his terminology) with a
sampling-without-replacement methodology. The computations of P(E) and of 1 ? De show that the
difference is negligible:
P(E) =
?
j
(
?
i
nij
Nk
)2
(Siegel and Castellan)
1 ? De =
?
j
(
?
i
nij
Nk
)(
[
?
i
nij
]
?1
Nk?1
)
(Krippendorff)
2 Both ?S&C (Scott 1955) and ?Co (Cohen 1960) were originally devised for two coders. Each has been
extended to more than two coders, for example, respectively Fleiss (1971) and Bartko and Carpenter
(1976). Thus, without loss of generality, our examples involve two coders.
97
Di Eugenio and Glass Kappa: A Second Look
Example 1
Coder 2
Coder 1 Accept Ack
Accept 70 25 95
Ack 0 55 55
70 80 150
Example 2
Coder 2
Coder 1 Accept Ack
Accept 70 15 85
Ack 10 55 65
80 70 150
Accept Ack
Okay1 2 0
...
Okay70 2 0
Okay71 0 2
...
Okay125 0 2
Okay126 1 1
...
Okay150 1 1
165 135
Figure 1
Cohen?s contingency tables (left) and Siegel and Castellan?s agreement table (right).
Agreement tables lose information. When the coders disagree, we cannot recon-
struct which coder picked which category. Consider Example 2 in Figure 1. The two
coders still disagree on 25 occurrences of Okay. However, one coder now labels 10
of those as Accept and the remaining 15 as Ack, whereas the other labels the same
10 as Ack and the same 15 as Accept. The agreement table does not change, but the
contingency table does.
Turning now to computing P(E), Figure 2 shows, for Example 1, Cohen?s com-
putation of P(E) on the left, and Siegel and Castellan?s computation on the right. We
include the computations of ?Co and ?S&C as the last step. For both Cohen and Siegel
and Castellan, P(A) = 125/150 = 0.8333. The observed agreement P(A) is computed
as the proportion of items the coders agree on to the total number of items; N is the
number of items, and k the number of coders (N = 150 and k = 2 in our example).
Both ?Co and ?S&C are highly significant at the p = 0.5 ? 10?5 level (significance is
computed for ?Co and ?S&C according to the formulas in Cohen [1960] and Siegel and
Castellan [1988], respectively).
The difference between ?Co and ?S&C in Figure 2 is just under 1%, however, the
results of the two ? computations straddle the value 0.67, which for better or worse
has been adopted as a cutoff in computational linguistics. This cutoff is based on the
assessment of ? values in Krippendorff (1980), which discounts ? < 0.67 and allows
tentative conclusions when 0.67 ? ? < 0.8 and definite conclusions when ? ? 0.8.
Krippendorff?s scale has been adopted without question, even though Krippendorff
himself considers it only a plausible standard that has emerged from his and his
colleagues? work. In fact, Carletta et al (1997) use words of caution against adopting
Krippendorff?s suggestion as a standard; the first author has also raised the issue of
how to assess ? values in Di Eugenio (2000).
If Krippendorff?s scale is supposed to be our standard, the example just worked
out shows that the different computations of P(E) do affect the assessment of inter-
coder agreement. If less-strict scales are adopted, the discrepancies between the two
? computations play a larger role, as they have a larger effect on smaller values of ?.
For example, Rietveld and van Hout (1993) consider 0.20 < ? ? 0.40 as indicating fair
agreement, and 0.40 < ? ? 0.60 as indicating moderate agreement. Suppose that two
coders are coding 100 occurrences of Okay. The two coders label 40 occurrences as
Accept and 25 as Ack. The remaining 35 are labeled as Ack by one coder and as Accept
by the other (as in Example 6 in Figure 4); ?Co = 0.418, but ?S&C = 0.27. These two
values are really at odds.
98
Computational Linguistics Volume 30, Number 1
Assumption of different distributions among
coders (Cohen)
Step 1. For each category j, compute the overall
proportion pj,l of items assigned to j by each coder
l. In a contingency table, each row and column
total divided by N corresponds to one such pro-
portion for the corresponding coder.
pAccept,1 = 95/150, pAck,1 = 55/150,
pAccept,2 = 70/150, pAck,2 = 80/150
Assumption of equal distributions among coders
(Siegel and Castellan)
Step 1. For each category j, compute pj, the overall
proportion of items assigned to j. In an agreement
table, the column totals give the total counts for
each category j, hence:
pj =
1
Nk ?
?
i nij
pAccept = 165/300 = 0.55, pAck = 135/300 = 0.45
Step 2. For a given item, the likelihood of both
coders? independently agreeing on category j by
chance, is pj,1 ? pj,2.
pAccept,1 ? pAccept,2 = 95/150 ? 70/150 = 0.2956
pAck,1 ? pAck,2 = 55/150 ? 80/150 = 0.1956
Step 2. For a given item, the likelihood of both
coders? independently agreeing on category j by
chance is p2j .
p2Accept = 0.3025
p2Ack = 0.2025
Step 3. P(E), the likelihood of coders? accidentally
assigning the same category to a given item, is
?
j pj,1 ? pj,2 = 0.2956 + 0.1956 = 0.4912
Step 3. P(E), the likelihood of coders? accidentally
assigning the same category to a given item, is
?
j p
2
j = 0.3025 + 0.2025 = 0.5050
Step 4.
?Co= (0.8333 ? 0.4912)/(1 ? 0.4912) =
.3421/.5088=0.6724
Step 4.
?S&C= (0.8333 ? 0.5050)/(1 ? 0.5050) =
.3283/.4950 = 0.6632
Figure 2
The computation of P(E) and ? according to Cohen (left) and to Siegel and Castellan (right).
2. Unpleasant Behaviors of Kappa: Prevalence and Bias
In the computational linguistics literature, ? has been used mostly to validate cod-
ing schemes: Namely, a ?good? value of ? means that the coders agree on the cate-
gories and therefore that those categories are ?real.? We noted previously that assess-
ing what constitutes a ?good? value for ? is problematic in itself and that different
scales have been proposed. The problem is compounded by the following obvious
effect on ? values: If P(A) is kept constant, varying values for P(E) yield vary-
ing values of ?. What can affect P(E) even if P(A) is constant are prevalence and
bias.
The prevalence problem arises because skewing the distribution of categories in
the data increases P(E). The minimum value P(E) = 1/m occurs when the labels are
equally distributed among the m categories (see Example 4 in Figure 3). The maximum
value P(E) = 1 occurs when the labels are all concentrated in a single category. But
for a given value of P(A), the larger the value of P(E), the lower the value of ?.
Example 3 and Example 4 in Figure 3 show two coders agreeing on 90 out of 100
occurrences of Okay, that is, P(A) = 0.9. However, ? ranges from ?0.048 to 0.80, and
from not significant to significant (the values of ?S&C for Examples 3 and 4 are the
same as the values of ?Co).3 The differences in ? are due to the difference in the relative
prevalence of the two categories Accept and Ack. In Example 3, the distribution is
skewed, as there are 190 Accepts but only 10 Acks across the two coders; in Example 4,
the distribution is even, as there are 100 Accepts and 100 Acks, respectively. These
results do not depend on the size of the sample; that is, they are not due to the fact
3 We are not including agreement tables for the sake of brevity.
99
Di Eugenio and Glass Kappa: A Second Look
Example 3
Coder 2
Coder 1 Accept Ack
Accept 90 5 95
Ack 5 0 5
95 5 100
P(A) = 0.90, P(E) = 0.905
?Co = ?S&C = ?0.048, p = 1
Example 4
Coder 2
Coder 1 Accept Ack
Accept 45 5 50
Ack 5 45 50
50 50 100
P(A) = 0.90, P(E) = 0.5
?Co = ?S&C = 0.80, p = 0.5 ? 10?5
Figure 3
Contingency tables illustrating the prevalence effect on ?.
Example 5
Coder 2
Coder 1 Accept Ack
Accept 40 15 55
Ack 20 25 45
60 40 100
P(A) = 0.65, P(E) = 0.52
?Co = 0.27, p = 0.005
Example 6
Coder 2
Coder 1 Accept Ack
Accept 40 35 75
Ack 0 25 25
40 60 100
P(A) = 0.65, P(E) = 0.45
?Co = 0.418, p = 0.5 ? 10?5
Figure 4
Contingency tables illustrating the bias effect on ?Co.
Example 3 and Example 4 are small. As the computations of P(A) and P(E) are based
on proportions, the same distributions of categories in a much larger sample, say,
10,000 items, will result in exactly the same ? values. Although this behavior follows
squarely from ??s definition, it is at odds with using ? to assess a coding scheme.
From both Example 3 and Example 4 we would like to conclude that the two coders
are in substantial agreement, independent of the skewed prevalence of Accept with
respect to Ack in Example 3. The role of prevalence in assessing ? has been subject
to heated discussion in the medical literature (Grove et al 1981; Berry 1992; Goldman
1992).
The bias problem occurs in ?Co but not ?S&C. For ?Co, P(E) is computed from
each coder?s individual probabilities. Thus, the less two coders agree in their overall
behavior, the fewer chance agreements are expected. But for a given value of P(A),
decreasing P(E) will increase ?Co, leading to the paradox that ?Co increases as the
coders become less similar, that is, as the marginal totals diverge in the contingency
table. Consider two coders coding the usual 100 occurrences of Okay, according to
the two tables in Figure 4. In Example 5, the proportions of each category are very
similar among coders, at 55 versus 60 Accept, and 45 versus 40 Ack. However, in
Example 6 coder 1 favors Accept much more than coder 2 (75 versus 40 occurrences)
and conversely chooses Ack much less frequently (25 versus 60 occurrences). In both
cases, P(A) is 0.65 and ?S&C is stable at 0.27, but ?Co goes from 0.27 to 0.418. Our
initial example in Figure 1 is also affected by bias. The distribution in Example 1
yielded ?Co = 0.6724 but ?S&C = 0.6632. If the bias decreases as in Example 2, ?Co
becomes 0.6632, the same as ?S&C.
3. Discussion
The issue that remains open is which computation of ? to choose. Siegel and
Castellan?s ?S&C is not affected by bias, whereas Cohen?s ?Co is. However, it is
100
Computational Linguistics Volume 30, Number 1
questionable whether the assumption of equal distributions underlying ?S&C is ap-
propriate for coding in discourse and dialogue work. In fact, it appears to us that it
holds in few if any of the published discourse- or dialogue-tagging efforts for which
? has been computed. It is, for example, appropriate in situations in which item i may
be tagged by different coders than item j (Fleiss 1971). However, ? assessments for
discourse and dialogue tagging are most often performed on the same portion of the
data, which has been annotated by each of a small number of annotators (between
two and four). In fact, in many cases the analysis of systematic disagreements among
annotators on the same portion of the data (i.e., of bias) can be used to improve the
coding scheme (Wiebe, Bruce, and O?Hara 1999).
To use ?Co but to guard against bias, Cicchetti and Feinstein (1990) suggest that ?Co
be supplemented, for each coding category, by two measures of agreement, positive
and negative, between the coders. This means a total of 2m additional measures, which
we believe are too many to gain a general insight into the meaning of the specific ?Co
value. Alternatively, Byrt, Bishop, and Carlin (1993) suggest that intercoder reliability
be reported as three numbers: ?Co and two adjustments of ?Co, one with bias removed,
the other with prevalence removed. The value of ?Co adjusted for bias turns out to
be . . . ?S&C. Adjusted for prevalence, ?Co yields a measure that is equal to 2P(A) ? 1.
The results for Example 1 should then be reported as ?Co = 0.6724, ?S&C = 0.6632,
2P(A)?1 = 0.6666; those for Example 6 as ?Co = 0.418, ?S&C = 0.27, and 2P(A)?1 = 0.3.
For both Examples 3 and 4, 2P(A)? 1 = 0.8. Collectively, these three numbers appear
to provide a means of better judging the meaning of ? values. Reporting both ?
and 2P(A) ? 1 may seem contradictory, as 2P(A) ? 1 does not correct for expected
agreement. However, when the distribution of categories is skewed, this highlights
the effect of prevalence. Reporting both ?Co and ?S&C does not invalidate our previous
discussion, as we believe ?Co is more appropriate for discourse- and dialogue-tagging
in the majority of cases, especially when exploiting bias to improve coding (Wiebe,
Bruce, and O?Hara 1999).
Acknowledgments
This work is supported by grant
N00014-00-1-0640 from the Office of Naval
Research. Thanks to Janet Cahn and to the
anonymous reviewers for comments on
earlier drafts.
References
Allen, James and Mark Core. 1997. DAMSL:
Dialog act markup in several layers;
Coding scheme developed by the
participants at two discourse tagging
workshops, University of Pennsylvania,
March 1996, and Schlo? Dagstuhl,
February 1997. Draft.
Bartko, John J. and William T. Carpenter.
1976. On the methods and theory of
reliability. Journal of Nervous and Mental
Disease, 163(5):307?317.
Berry, Charles C. 1992. The ? statistic [letter
to the editor]. Journal of the American
Medical Association, 268(18):2513?2514.
Byrt, Ted, Janet Bishop, and John B. Carlin.
1993. Bias, prevalence, and kappa. Journal
of Clinical Epidemiology, 46(5):423?429.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The Kappa statistic.
Computational Linguistics, 22(2):249?254.
Carletta, Jean, Amy Isard, Stephen Isard,
Jacqueline C. Kowtko, Gwyneth
Doherty-Sneddon, and Anne H.
Anderson. 1997. The reliability of a
dialogue structure coding scheme.
Computational Lingustics, 23(1):13?31.
Cicchetti, Domenic V. and Alvan R.
Feinstein. 1990. High agreement but low
kappa: II. Resolving the paradoxes.
Journal of Clinical Epidemiology,
43(6):551?558.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20:37?46.
Di Eugenio, Barbara. 2000. On the usage of
Kappa to evaluate agreement on coding
tasks. In LREC2000: Proceedings of the
Second International Conference on Language
Resources and Evaluation, pages 441?444,
Athens.
Di Eugenio, Barbara, Pamela W. Jordan,
Richmond H. Thomason, and Johanna D.
Moore. 2000. The agreement process: An
101
Di Eugenio and Glass Kappa: A Second Look
empirical investigation of human-human
computer-mediated collaborative
dialogues. International Journal of Human
Computer Studies, 53(6):1017?1076.
Fleiss, Joseph L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Goldman, Ronald L. 1992. The ? statistic
[letter to the editor (in reply)]. Journal of
the American Medical Association,
268(18):2513?2514.
Grove, William M., Nancy C. Andreasen,
Patricia McDonald-Scott, Martin B. Keller,
and Robert W. Shapiro. 1981. Reliability
studies of psychiatric diagnosis: Theory
and practice. Archives of General Psychiatry,
38:408?413.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to Its Methodology. Sage
Publications, Beverly Hills, CA.
Rietveld, Toni and Roeland van Hout. 1993.
Statistical Techniques for the Study of
Language and Language Behaviour. Mouton
de Gruyter, Berlin.
Scott, William A. 1955. Reliability of content
analysis: The case of nominal scale
coding. Public Opinion Quarterly,
19:127?141.
Siegel, Sidney and N. John Castellan, Jr.
1988. Nonparametric statistics for the
behavioral sciences. McGraw Hill, Boston.
Wiebe, Janyce M., Rebecca F. Bruce, and
Thomas P. O?Hara. 1999. Development
and use of a gold-standard data set for
subjectivity classifications. In ACL99:
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics,
pages 246?253, College Park, MD.
c? 2004 Association for Computational Linguistics
Centering: A Parametric Theory and Its
Instantiations
Massimo Poesio? Rosemary Stevenson?
University of Essex University of Durham
Barbara Di Eugenio? Janet Hitzeman?
University of Illinois at Chicago MITRE Corporation
Centering theory is the best-known framework for theorizing about local coherence and
salience; however, its claims are articulated in terms of notions which are only partially specified,
such as ?utterance,? ?realization,? or ?ranking.? A great deal of research has attempted to
arrive at more detailed specifications of these parameters of the theory; as a result, the claims of
centering can be instantiated in many different ways. We investigated in a systematic fashion
the effect on the theory?s claims of these different ways of setting the parameters. Doing this
required, first of all, clarifying what the theory?s claims are (one of our conclusions being that
what has become known as ?Constraint 1? is actually a central claim of the theory). Secondly,
we had to clearly identify these parametric aspects: For example, we argue that the notion of
?pronoun? used in Rule 1 should be considered a parameter. Thirdly, we had to find appropriate
methods for evaluating these claims. We found that while the theory?s main claim about salience
and pronominalization, Rule 1?a preference for pronominalizing the backward-looking center
(CB)?is verified with most instantiations, Constraint 1?a claim about (entity) coherence and
CB uniqueness?is much more instantiation-dependent: It is not verified if the parameters are
instantiated according to very mainstream views (?vanilla instantiation?), it holds only if indirect
realization is allowed, and is violated by between 20% and 25% of utterances in our corpus even
with the most favorable instantiations. We also found a trade-off between Rule 1, on the one hand,
and Constraint 1 and Rule 2, on the other: Setting the parameters to minimize the violations of
local coherence leads to increased violations of salience, and vice versa. Our results suggest that
?entity? coherence?continuous reference to the same entities?must be supplemented at least
by an account of relational coherence.
1. Motivations
Centering theory (Joshi and Weinstein 1981; Grosz, Joshi, and Weinstein 1983, 1995;
Walker, Joshi, and Prince 1998b) is the component of Grosz and Sidner?s overall theory
? Department of Computer Science, Wivenhoe Park, Colchester CO4 35Q, U.K. E-mail:
poesio@essex.ac.uk.
? Department of Psychology, University of Durham, U.K.
? Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607-7053, USA.
E-mail: bdieugen@cs.uic.edu
? MITRE Corporation, 202 Burlington Road, Bedford, MA 01730-1428, USA. E-mail:hitze@mitre.org.
Submission received: 16 April 2002; Revised submission received: 3 September 2003; Accepted for
publication: 11 December 2003
310
Computational Linguistics Volume 30, Number 3
of attention and coherence in discourse (Grosz 1977; Sidner 1979; Grosz and Sidner
1986) concerned with local coherence and salience, that is, coherence and salience
within a discourse segment. A fundamental characteristic of centering is that it is bet-
ter viewed as a linguistic theory than a computational one. By this we mean that its
primary aim is to make cross-linguistically valid claims about which discourses are
easier to process, abstracting away from specific algorithms for anaphora resolution
or anaphora generation (although many such algorithms are based on the theory).
The result is a very different theory from those one usually finds in computational
linguistics. In central papers such as Grosz, Joshi, and Weinstein (1995), no algo-
rithms are provided to compute notions such as ?utterance,? ?previous utterance,?
?ranking,? and ?realization? that play a crucial role in the theory. The researchers
working on centering argue that while these concepts play a central role in any theory
of discourse coherence and salience, their precise characterization is best left for sub-
sequent research, indeed, that some of these concepts (e.g., ranking) might be defined
in a different way for each language (Walker, Iida, and Cote 1994). In other words,
these notions should be viewed as parameters of centering. This feature of the theory
has inspired a great deal of research attempting to specify centering?s parameters for
different languages (Kameyama 1985; Walker, Iida, and Cote 1994; Di Eugenio 1998;
Turan 1998; Strube and Hahn 1999). Competing versions of the central definitions
and claims of the theory have also been proposed: For example, different definitions
of backward-looking center (CB) can be found in Grosz, Joshi, and Weinstein (1983,
1995) and Gordon, Grosz, and Gillion (1993). As a result, a researcher wishing to test
the predictions of centering, or to use it for practical applications, is confronted with
a large number of possible instantiations of the theory.
The main goal of the work reported in this article was to explore the space of
parameter configurations, measuring the impact of different ways of setting the pa-
rameters of centering on the theory?s claims. This required specifying in an explicit
way what centering?s main claims are; clearly identifying the parameters, not all of
which have previously been discussed in the literature; and developing appropriate
methods (and statistical tests) to carry out this evaluation. The comparison between
instantiations was carried out by annotating a corpus of English texts from different
genres with the information needed to test a variety of centering instantiations and
using this corpus to assess the extent to which the theory?s claims are verified once the
parameters are set in a certain way. The proponents of centering have clearly stated
that the aim of the theory is to identify preferences that make discourses easier to pro-
cess; clearly, the best way to test such preferences is through behavioral experiments,
and many aspects of the theory have in fact been tested this way (Hudson, Tanen-
haus, and Dell 1986; Gordon, Grosz, and Gillion 1993; Brennan 1995). But given the
enormous number of possible ways of setting the theory?s parameters, a systematic
comparison can be made only by computational means. A corpus-based evaluation
has other advantages, as well, among which is that it is perhaps the best way to
identify the aspects of the theory that need to be further specified, and the factors
such as temporal coherence or stylistic variation that may interact with the prefer-
ences expressed by centering. (Also, knowing the extent to which real texts conform
to centering preferences is an important goal in its own right.)
In previous corpus-based studies of centering (Walker 1989; Passonneau 1993;
Byron and Stent 1998; Di Eugenio 1998; Kameyama 1998; Strube and Hahn 1999;
Tetreault 2001), only a few instantiations of centering were compared. The present
study is more systematic in that it considers a greater number of parameters, as
well as more parameter instantiations, including ?crossing? instantiations in which
the parameters are set according to proposals due to different researchers. Only re-
311
Poesio et al Centering: A Parametric Theory
liable annotation techniques were used; we produced an annotation manual that
can be used to extend our analysis to other data, as well as a companion Web site
(http://cswww.essex.ac.uk/staff/poesio/cbc/) to allow readers to try out instantia-
tions not discussed in this article. (The Web site also contains the annotation manual
and a technical report with a full discussion of all results.) Last but not least, our
evaluation is arguably more neutral than in most previous studies in that, first of all,
we are not proposing a new instantiation of the theory; and secondly, all parameter
instantiations were tested on the same data.
The article is organized as follows. We first review the basic concepts of the theory,
discussing the three claims on which we focus (Constraint 1, Rule 1, and Rule 2) and the
parameters used in their formulation. We then discuss how our corpus was annotated
and how the annotation was used to compute violations of the three main claims. In
Section 4 we present our main results, which are discussed in Section 5.
2. Centering Theory and Its Parameters
It is not possible to discuss in this article the entire centering literature; we merely
summarize in this section some of this work in enough detail to allow the reader to
follow the discussion in the rest of the article. For more details, we refer the reader
to classic references such as Grosz, Johsi, and Weinstein (1995) and Walker, Joshi, and
Prince (1998b) or the discussion of centering in Poesio and Stevenson (forthcoming).
2.1 Motivations and Main Intuitions
Centering is simultaneously a theory of discourse coherence and of discourse salience.
As a theory of coherence, it attempts to characterize entity-coherent discourses: dis-
courses that are considered coherent because of the way discourse entities are intro-
duced and discussed.1 At the same time, centering is also intended to be a theory of
salience: that is, it attempts to predict which entities will be most salient at any given
time.
The main claim about local coherence made in centering is that discourse segments
in which successive utterances keep mentioning the same discourse entities are ?more
coherent? than discourse segments in which different entities are mentioned. This
hypothesis was formulated by Chafe (1976) and is backed by empirical evidence such
as Kintsch and van Dijk (1978) and Givon (1983). In centering this hypothesis is further
strenghtened by proposing that every utterance has a unique ?main link? with the
previous utterance: the CB. Having a unique CB, it is claimed, considerably simplifies
the complexity of the inferences required to integrate an utterance into the discourse
(Joshi and Kuhn 1979; Joshi and Weinstein 1981).
Centering?s first contention as far as local salience is concerned is that the dis-
course entities realized by an utterance (more on realization below) are ranked: that is,
that in each utterance some discourse entities are more salient than others. This claim,
as well, is a basic tenet of much work on discourse (Sidner 1979; Prince 1981; Givon
1983; Gundel, Hedberg, and Zacharski 1993) and is supported by much psychologi-
cal evidence (Hudson, Tanenhaus, and Dell 1986; Gernsbacher and Hargreaves 1988;
Gordon, Grosz, and Gillion 1993; Stevenson, Crawley, and Kleinman 1994).
1 Entity-based theories of coherence are so-called by contrast with relation-centered theories of
coherence, such as those developed in Hobbs (1979) and Mann and Thompson (1988) and used in Fox
(1987) and Lascarides and Asher (1993). The earliest detailed entity-based theory of coherence we are
aware of is by Kintsch and van Dijk (1978), who also explicitly mention the need to supplement such
theories with a theory of relational coherence (more on this in Section 5)
312
Computational Linguistics Volume 30, Number 3
These claims about coherence and salience are linked by two further hypotheses:
that the identity of the CB is crucially determined by the entities? ranking and that the
CB is most likely to be realized as a pronoun. This assumption that a ?main entity? or
?topic? or ?focus? is the preferred interpretation of pronouns is commonly found in
theories in the psychological (e.g., Sanford and Garrod 1981), computational (Sidner
1979) and linguistic (Gundel, Hedberg, and Zacharski 1993) literature and is motivated
by evidence such as the contrast between examples (1) and (2):
(1) a. Something must be wrong with John.
b. He has been acting quite odd. (He = John)
c. He called up Mike yesterday.
d. John wanted to meet him quite urgently.
(2) a. Something must be wrong with John.
b. He has been acting quite odd. (He = John)
c. He called up Mike yesterday.
d. He wanted to meet him quite urgently.
Discourses (1) and (2) differ only only in their (d) sentences, but according to
Grosz et al, (1d) is not as felicitous as (2d). The reason, they argue, is that after the
(c) utterances, the discourse entity John is more highly ranked than Mike, so it will be
the CB of the next utterance provided that it is realized in it; and given the preference
for pronominalizing the CB, John should be pronominalized if anything else is.
This link between pronominalization and the identity of the CB has been used
by Grosz et al to support the claim discussed above that utterances have a unique
CB (contra, e.g., Sidner [1979] whose theory assumed two foci). Grosz et al note
the contrast between continuations (3c)?(3f) of the discourse initiated by utterances
(3a)?(3b):
(3) a. Susan gave Betsy a pet hamster.
b. She reminded her that such hamsters were quite shy.
c. She asked Betsy whether she liked the gift.
d. Betsy told her that she really liked the gift.
e. Susan asked her whether she liked the gift.
f. She told Susan that she really liked the gift.
Grosz et al argue that continuations (3c)?(3f) are less and less acceptable, whereas if
Susan and Betsy were equally ranked after (3b), all variants should be equally accept-
able.
2.2 Terminology and Definitions
2.2.1 Local Focus, Forward-Looking Centers, and Utterances. A fundamental assump-
tion underlying centering is that processing a discourse involves continuous updates
to the local attentional state, or local focus. The local focus includes a set of forward-
looking centers (CFs), which correspond to Sidner?s (1979) ?potential discourse foci?
and can be viewed as mentions of discourse entities (Karttunen 1976; Webber 1978;
Heim 1982; Kamp and Reyle 1993). The local focus also contains information about the
relative prominence or rank of these CFs. The local focus gets updated after every ut-
terance: In this update, the current CFs are replaced by new ones, and the CB changes,
313
Poesio et al Centering: A Parametric Theory
as well (see below).2 The set of CFs introduced in the local focus by utterance Ui in
a discourse segment (DS) is indicated by CF(Ui,DS), generally abbreviated to CF(Ui).
Brennan, Friedman, and Pollard (1987) formalized the relationship between utterances
and CFs by means of one of their so-called constraints:3
Constraint 2: Every element of CF(U,DS) must be realized in U.
2.2.2 Ranking, Preferred Centers, and Backward-Looking Centers. We already men-
tioned two important claims of the theory: that forward-looking centers are ranked,
and that because of this ranking, some CFs acquire particular prominence. The ranking
function is only required to be partial, but the most highly ranked CF realized by an
utterance (when one exists) is called the Preferred Center (CP). Ranking is also used
to characterize one of the CFs as the CB. The CB is the closest concept in centering
to the traditional notion of ?topic? (Sgall 1967; Chafe 1976; Sanford and Garrod 1981;
Givon 1983; Vallduvi 1990; Gundel, Hedberg, and Zacharski 1993) and plays a cen-
tral role in the theory?s claims about both coherence and salience. Although in Grosz,
Joshi, and Weinstein (1983), the CB was characterized only in intuitive terms, most
subsequent work has been based on the definition below (Grosz, Joshi, and Weinstein
1995), referred to as ?Constraint 3? by Brennan, Friedman, and Pollard (1987):
Constraint 3 CB(Ui), the backward-looking center of utterance Ui, is the highest-
ranked element of CF(Ui?1) that is realized in Ui.
Notice that according to this definition, the computation of the CB depends exclusively
on ranking and ?previous utterance,? making these parameters crucially important for
the framework.4
2.2.3 Transitions. The hypothesis that discourses are easier to process when successive
utterances are perceived as being ?about? a unique discourse entity is formalized in
centering in terms of a classification of utterances according to the type of transition
(update) they induce in the local focus. Many such classifications of transitions have
been proposed; Grosz, Joshi, and Weinstein (1995) distinguish among three types of
transitions, depending on whether the backward-looking center of Ui?1 is maintained
or not in Ui and on whether CB(Ui) is also the most highly ranked entity (CP) of Ui:
Center Continuation (CON): CB(Ui) = CB(Ui?1), and CB(Ui) is the most highly
ranked CF (CP) of Ui (i.e., CP(Ui) = CB(Ui))
Center Retaining (RET): CB(Ui) = CB(Ui?1), but CP(Ui) = CB(Ui)
Center Shifting (SHIFT): CB(Ui?1) = CB(Ui)
We will consider a few alternative classification schemes below, after discussing how
these classifications are used to formulate one of the core claims of centering, Rule 2.
2 The hypothesis that discourse processing involves continuous updates to the discourse model also lies
at the heart of so-called ?dynamic? theories of discourse semantics (Heim 1982; Kamp and Reyle 1993).
3 The order of presentation of constraints and rules followed here differs from that more familiar in the
centering literature. This is because we want to distinguish between definitions and claims, and the
three constraints proposed by Brennan et al do not all have the same status: While Constraint 2 can be
seen as a ?filter? ruling out certain values of CF(Ui), Constraint 3 is a definition, and Constraint 1 an
empirical claim.
4 Other ways of defining the CB have been proposed. We refer the interested reader to the longer report
and to the companion Web site.
314
Computational Linguistics Volume 30, Number 3
2.3 Main Claims
In the words of Grosz et al, the most fundamental claim of centering is that ?to the
extent that discourse adheres to centering constraints, its coherence will increase and
the inference load placed upon the hearer will decrease? (Grosz, Joshi, and Wein-
stein 1995, page 210). Grosz et al list seven such ?costraints,? three of which can be
directly evaluated. Even though we are not following here the distinction between
?constraints? and ?rules? introduced in Brennan, Friedman, and Pollard (1987), we
will use for these three claims the names Brennan et al gave them, by which they are
now best known:
Constraint 1 (Strong): All utterances of a segment except for the first have exactly
one CB.
Rule 1 (GJW95): If any CF is pronominalized, the CB is.
Rule 2 (GJW95): (Sequences of) continuations are preferred over (sequences of)
retains, which are preferred over (sequences of) shifts.
2.3.1 Constraint 1, Topic Uniqueness, and Entity Coherence. If we view the CB as
a formalization of the idea of ?topic? (Vallduvi 1990; Gundel 1998; Hurewitz 1988;
Miltsakaki 1999; Beaver 2004), Constraint 1 expresses, first and foremost, the original
claim from Joshi and Kuhn (1979) and Joshi and Weinstein (1981) that discourses with
exactly one (or no more than one) ?topic? at each point are easier to process. This view
contrasts both with Sidner?s (1979) hypothesis that utterances may have two ?topics?
and with theories such as Givon (1983), Alshawi (1987), Lappin and Leass (1994) and
Arnold (1998), which view ?topichood? as a matter of degree and therefore allow for
an arbitrary number of topics.
In the strong form just presented, Constraint 1 is also a claim about local coher-
ence. It expresses a preference for discourses to be entity coherent: to continue talking
about the same entities. Each utterance in a segment should realize at least one of the
discourse entities realized in the previous utterance. A weaker form of Constraint 1
has also been suggested (e.g., Walker, Joshi, and Prince 1998a, footnote 2, page 3); the
preference for a unique CB is preserved, but not the preference for entity coherence.
Constraint 1 (Weak): All utterances of a segment except for the first have at most
one CB.
2.3.2 Rule 1 and Pronominalization. Rule 1 is the main claim of centering about
pronominalization. In the version presented above, it states a preference for pronomi-
nalizing the CB, if anything is pronominalized at all. We also examined two alternative
formulations. The original form of the claim in Grosz, Joshi, and Weinstein (1983) was
as follows:
Rule 1 (GJW83): If the CB of the current utterance is the same as the CB of the
previous utterance, a pronoun should be used.
Gordon, Grosz, and Gillion (1993) proposed a much stronger form of the claim. They
found that entities realized in certain positions in the sentence were read more slowly
unless pronominalized (repeated-name penalty [RNP]).5 This evidence led them to
5 Gordon et al observed increased reading times when proper names were used instead of pronouns to
realize an entity in subject position referring to an entity realized in first-mentioned or subject position.
315
Poesio et al Centering: A Parametric Theory
propose a more restrictive definition of CB (briefly, that the CB is the entity subject to
the RNP?for discussion, see the longer version of this article available on the Web
site) and a stronger form of Rule 1, requiring the CB (defined in this more restrictive
way) to be always pronominalized:
Rule 1 (Gordon et al): The CB should be pronominalized.
Although we will refer to this version as ?Gordon et al?s? for brevity, readers should
keep in mind that because the definition of CB proposed by Gordon et al is more
restrictive, their version of Rule 1 is properly evaluated only using that definition.
(The results with this instantiation are discussed in the longer version of the article
availabel on the Web site.)
2.3.3 Rule 2 and the Classification of Transitions. Rule 2 is a claim about coherence, as
well: It states a preference for preserving the CB over changing it and for realizing it as
the most salient entity over changing its relative ranking. This aspect of the theory has
received a lot of attention; several variants of this constraint have been proposed, as
well as many ways of classifying transitions. We studied many alternative proposals.
The version of Rule 2 presented in Grosz, Joshi, and Weinstein (1995) expresses
preferences among sequences of transitions (e.g., CON-CON over SHIFT-SHIFT) rather
than preferences for particular transitions. This form of the constraint is in part mo-
tivated by empirical results. Di Eugenio (1998), for example, found that the relative
distribution of null and explicit pronouns in Italian depends on the previous transition
as well: in Center Continuations that follow a CON or a SHIFT, it is much more likely
that a null pronoun will be used, whereas in Center Continuations that follow a RET
transition, both null and explicit pronouns are equally likely. Turan (1998) obtained
similar results for null and explicit pronouns in Turkish.
Other researchers argue instead that the inferential load is evaluated utterance by
utterance (Brennan, Friedman, and Pollard 1987; Walker, Iida, and Cote 1994; Walker,
Joshi, and Prince 1998a). The version of Rule 2 proposed by Brennan et al is as follows:
Rule 2 (Single Transitions): Transition states are ordered. The CON transition is
preferred to the RET transition, which is preferred to the Smooth Shift
transition, which is preferred to the Rough Shift transition.
This formulation of Rule 2 depends on a further distinction between two types of
SHIFT: Smooth Shift (SSH), when CB(Un) = CP(Un), and Rough-Shift (RSH), when
CB(Un) = CP(Un). Transitions can then be classified along two dimensions, as in the
following table:
CB(Un) = CB(Un?1) or CB(Un?1) = NIL CB(Un) = CB(Un?1)
CB(Un) = CP(Un) Continue Smooth Shift
CB(Un) = CP(Un) Retain Rough Shift
Further refinements of these classification schemes have been proposed. Kameyama
(1986) proposed a fourth transition type, Center Establishment (EST), for utterances
E.g., in Bruno was the bully of the neighborhood. Bruno / He often taunted Tommy, the second sentence would
be read more slowly when Bruno was used than when He was used.
316
Computational Linguistics Volume 30, Number 3
that establish a CB after an utterance without one, such as the first utterance of a seg-
ment. Walker, Iida, and Cote (1994) argued that these utterances should be classified
as Center Continuations, the idea being that even the first utterance of a segment does
have a CB, but this CB is initially underspecified and is determined only when the sec-
ond utterance is processed. Notice that according to the strong version of Constraint
1, the first utterance of a discourse segment is the only utterance allowed not to have a
CB in a coherent discourse; hence, none of these classification schemes for transitions
includes classes either for the inverse of Center Establishment, that we might call Ze-
roing (ZERO) transition (a CB less utterance following one which does have a CB) or
for CB-less utterances following other CB-less ones (the NULL transition).
Strube and Hahn (1999), like Grosz, Joshi, and Weinstein (1995), claim that infer-
ential load is evaluated across sequences (pairs, in fact) of transitions but argue for
a different way of evaluating the inferential load of utterances. In their view, classi-
fications of transitions such as those above do not reflect what should be one of the
crucial claims of the theory: that the CP of one utterance predicts the CB of the next.
In order to formalize their view, they propose a different classification scheme, based
on the distinction between cheap and expensive transitions (Strube and Hahn 1999,
page 332):
? A transition pair is cheap if the CB of the current utterance is correctly
predicted by the CP of the previous utterance, that is, if CB(Un) =
CP(Un?1).
? A transition pair is expensive if CB(Un) = CP(Un?1).
Strube and Hahn then propose a new version of Rule 2 based on this distinction:
Rule 2 (Strube and Hahn): Cheap transition pairs are preferred to expensive ones.6
2.4 The Parameters of Centering
Although Grosz et al discussed possible definitions for the concepts used in the claims
above??utterance,? ?previous utterance,? ?ranking,? and ?realization??they didn?t
settle on a specific definition, even for English. Similarly undefined is the notion of
?pronominalization? governed by Rule 1. But without further specification of these
concepts it is impossible to evaluate the claims above, just as it is not possible to
evaluate the predictions of, say, ?government and binding theory? without providing
an explicit definition of ?command? or ?argument?. As a result, a considerable amount
of research has been concerned with establishing the best specification for what are,
essentially, parameters of the theory. We briefly review some of these proposals in this
section.7
6 Kibble (2001) proposed a version of Rule 2 that further develops the ?decompositional? view of Rule 2
introduced by Brennan et al, while simultaneously incorporating Strube and Hahn?s intuition that
?cheap? transitions should be preferred. Kibble formulates his version of Rule 2 as a series of
preferences: for transitions that preserve the CB?that is, those such that CB(Un) = CB(Un?1) (he calls
these transitions cohesive), that identify CB(Un) with CP(Un), and/or that are cheap. Code to test an
earlier version of Kibble?s form of Rule 2 (Kibble 2000) has been incorporated in the scripts discussed
later in the article, and the results can be seen in the longer technical report accompanying this article,
or from the companion Web site. We will, however, omit a discussion of this version here, in part for
reasons of space, in part because the final version of the rule in Kibble (2001) differs substantially from
the original version that we evaluated.
7 For more details, and for a discussion of the motivations behind these proposals, see the extended
version of this article and Poesio and Stevenson (forthcoming).
317
Poesio et al Centering: A Parametric Theory
2.4.1 Utterance and Previous Utterance. In the early centering papers, utterances were
implicitly identified with sentences. Kameyama (1998), however, argued that such
identification makes the number of potential antecedents of anaphoric expressions
much greater than if they were resolved clause by clause. Furthermore, she noted
that this identification leads to problems with multiclausal sentences: for example,
grammatical function ranking becomes difficult to compute, as a sentence may have
more than one subject. Kameyama proposed that the local focus be updated after every
tensed clause, not after every sentence, and classified tensed clauses into (1) utterances
that constitute a ?permanent? update of the local focus, such as coordinated clauses
and adjuncts, and (2) embedded utterances that result in temporary updates that are
then ?popped?, much as the information introduced into discourse by subordinated
discourse segments is popped according to Grosz and Sidner (1986). According to
Kameyama, only a few types of clauses, such as the complements of certain verbs,
are embedded. For example, Kameyama proposes to break up (4) into utterances as
follows, and to treat each of these utterances, including subordinate clauses such as
(U2) or (U5), as an update:
(4) (u1) Her entrance in Scene 2 Act 1 brought some disconcerting applause
(u2) even before she had sung a note. (u3) Thereafter the audience
waxed applause happy (u4) but discriminating operagoers reserved
judgment (u5) as her singing showed signs of strain.
Experiments by Pearson, Stevenson, and Poesio (2000) confirmed that CFs introduced
in main clauses are significantly more likely to be subsequently mentioned than CFs
introduced in complement clauses. However, a semicontrolled study by Suri and Mc-
Coy (1994) suggested that other types of clauses?specifically, adjunct clauses headed
by after and before?are also ?embedded,? not ?permanent updates? as suggested by
Kameyama; these results were subsequently confirmed by Cooreman and Sanford
(1996). The status of other types of clauses is less clear. Kameyama (1998) also pro-
posed a tentative analysis of relative clauses, according to which they are temporarily
treated as utterances and update the local focus but are then merged with the embed-
ding clause; she didn?t, however provide empirical support for this hypothesis. Other
types of subordinate clauses and parentheticals are not discussed in this literature.
Strube (1998) and Miltsakaki (1999) question Kameyama?s identification of utter-
ances with (tensed) clauses. Miltsakaki (1999) argues, on the basis of data from English
and Greek, that the local focus is updated only after every sentence and that only the
CFs in the main clause are considered when establishing the CB.
2.4.2 Realization. Grosz, Joshi, and Weinstein (1995) simply say that what it means for
utterance U to realize center C depends on the particular semantic theory one adopts.
They consider two ways in which a discourse entity may be ?realized? in an utterance
as required by Constraint 2. Direct realization is when a noun phrase in the utterance
refers to that discourse entity. Indirect realization is when one of the noun phrases
in the utterance is an associative reference to that discourse entity in the sense of
Hawkins (1978),8 that is, an anaphoric expression that refers to an object which wasn?t
mentioned before but is somehow related to an object that already has. For example,
in the following discourse:
(5) (u1) John walked toward the house. (u2) The door was open.
8 Associative references are one type of bridging reference (Clark 1997).
318
Computational Linguistics Volume 30, Number 3
John, the house and the door are directly realized in the respective utterances; in addition,
the house can be thought as being indirectly realized in (u2) by virtue of being referred
to by the associative reference the door (see, e.g., the discussion in Grosz, Joshi, and
Weinstein [1995] and Walker, Joshi, and Prince [1998b]). Clearly, the computation of
the CB is affected by which entities are considered to be ?realized? in an utterance: In
(5), for example, (u2) has a CB (the house) only if the house is considered to be realized
in (u2) by virtue of its being associated with the door. To our knowledge, the effect of
these alternative notions of realization on the predictions of the theory has not been
previously studied, even though theories of focusing such as Sidner?s (1979) do allow
the (discourse) focus to be realized in an utterance in these cases, and the issue is often
mentioned in discussions of centering.
A related issue is whether empty realizations, or traces, should count as realiza-
tions of an entity. Many theories of grammar hypothesize that morphologically null
elements occur in the syntactic structure underlying a variety of constructions, includ-
ing control constructions as in (6a), reduced relatives as in (6b), and even coordinated
verb phrases (VP) as in (6c):
(6) a. John wanted (? to buy a house).
b. John bought a house (? abandoned by its previous occupiers).
c. John bought a house and (? promptly demolished it).
If, for example, the coordinated VP in (6c) is considered a separate utterance, whether
or not it contains a realization of John is going to determine whether or not it has a
CB. To our knowledge, morphologically null elements have been considered in the
centering literature only for languages other than English.
An issue that has been raised in the centering literature (e.g., Walker 1993; Di
Eugenio 1998; Byron and Stent 1998) is whether the CF list contains only entities
realized as third-person noun phrases (NPs), or also the entities realized as first- and
second-person NPs. Walker (1993) suggests that deictic entities are beyond the purview
of centering; however, in example (7), neither utterance (u2) nor utterance (u3) would
have a CB if second-person pronoun you is not counted as introducing an entity in
the CF list.9
(7) (u1) You should not use PRODUCT-Z
(u2) if you are pregnant of breast-feeding.
(u3) Whilst you are receiving PRODUCT-Z . . .
2.4.3 Ranking. Perhaps the most discussed parameter of centering, at least in the
versions of the theory that accept the definition of CB specified by Constraint 3, is
the ranking function. Most researchers working on centering, including Grosz et al,
assume that several factors play a role in determining the relative ranking of forward-
looking centers; in fact, Walker, Iida, and Cote (1994) and Walker, Joshi, and Prince
(1998a) claim that the factors affecting ranking may not be the same in all languages.
9 According to Walker, Joshi, and Prince (1998a), in the original version of Grosz, Joshi, and Weinstein
(1995), which appeared in 1986, Grosz et al provided a more explicit definition of realization:
An utterance U realizes a center c if c is an element of the situation described by U, or c is
the semantic interpretation of some subpart of U.
With this definition, all of the cases considered above?the anchors of associative references, traces, and
the entities realized as first- and second-person pronouns?would be considered as realized by an
utterance.
319
Poesio et al Centering: A Parametric Theory
Nevertheless, most versions of the theory developed since Kameyama (1985, 1986) and
Grosz, Joshi, and Weinstein (1986) have assumed that grammatical function plays the
main role in determining the order among forward-looking centers, at least for English.
Specifically, Grosz, Joshi, and Weinstein (1995) claim that subjects are ranked more
highly than objects, which are ranked more highly than other grammatical positions:
SUBJ ? OBJ ? OTHERS (see also Kameyama 1986; Hudson, Tanenhaus, and Dell 1986).
Slightly different ranking functions based on grammatical function were proposed by
Brennan, Friedman, and Pollard (1987) (who made a further distinction between objects
and indirect objects), by Walker, Iida, Cote (1994) for Japanese, and by Turan (1998)
for Turkish. There is quite a lot of psychological support for at least the part of this
claim stating that entities realized as subjects are more salient than entities realized in
other grammatical functions (Hudson, Tanenhaus, and Dell 1986; Gordon, Grosz, and
Gillion 1993; Brennan 1995; Hudson-D?Zmura and Tanenhaus 1998).
Other factors affecting ranking have been considered as well. Rambow (1993)
proposed that a number of facts about scrambling in German could be explained if
ranking in German were to be determined by surface order of realization. The idea
that order of mention affects salience is well supported by psychological evidence;
for example, the results of probe experiments by Gernsbacher and Hargreaves (1998)
suggest that the first-mentioned discourse entity in a sentence is the most salient. The
interaction of order of mention with grammatical function has also been studied. As
mentioned above, Gordon, Grosz and Gillion (1993) observed a repeated name penalty
for CFs in subject position coreferring with an entity previously introduced. This effect
was observed both when the antecedent was in subject position and when it was the
first-mentioned entity in a nonsubject position (as in In Lisa?s opinion, he shouldn?t have
done that), suggesting that first-mentioned CFs are as highly ranked as subjects.
Strube and Hahn (1999) argue that in German, the rank of discourse entities is
determined by the position they hold in Prince?s (1981, 1992) givenness hierarchy.
Specifically, Strube and Hahn argue that hearer-old entities rank higher than mediated
entities, which and in turn rank higher than hearer-new entities: HEARER-OLD ?
MEDIATED ? HEARER-NEW.10 Order of mention also plays a role in their ranking:
Within each category, the entities realized earlier in the sentence are ranked more
highly.
Finally, Sidner?s original (1979) claim that ranking depended on thematic roles,
abandoned in the early versions of centering, was revisited by Cote (1998). This view is
supported by psychological work on ?implicit causality? verbs (Caramazza et al 1997)
as well as work by Stevenson, Crawley, and Kleinman (1994), and Pearson, Stevenson,
and Poesio (2001). In particular, there is evidence that with certain verbs, the normal
preference for subjects to rank higher than their objects is reversed, although these
preferences are modified by other factors such as order of mention, type of connective,
and animacy (Stevenson, Crawley, and Kleinman 1994; Stevenson et al 2000; Pearson,
Stevenson, and Poesio 2001).
2.4.4 R1-Pronouns. Rule 1 states that if any CF is pronominalized, the CB is, but
the theory does not explicitly specify which types of ?pronouns? are covered by this
rule. It seems clear that realization as a third-person singular pronoun does count;
that is, if the choice is between using a third-person singular pronoun to realize a
10 Strube and Hahn?s hearer-old entities include Prince?s evoked (= discourse old) and unused entities,
which are entities such as Margaret Thatcher that are supposed to be part of shared knowledge.
Mediated entities are the entities falling in Prince?s categories inferrable, containing inferrable, and
anchored brand-new.
320
Computational Linguistics Volume 30, Number 3
CB or another CF, the CB should be chosen. We also saw that in languages such as
Italian, Japanese, and Turkish, the preferred realizations of CBs are morphologically
null elements (Kameyama 1986; Walker, Iida, Cote 1994; Turan 1998; Di Eugenio 1998).
But should an utterance in English count as verifying the rule if a CF is realized as a
third-person pronoun, and the CB as a trace? Or if the CB is realized with a full NP,
but a second CF is realized with a demonstrative pronoun? And what about first- and
second-person pronouns? The precise characterization of the (sub)class of pronouns
subject to Rule 1, which we will call R1-pronouns, is clearly an essential aspect of the
theory, yet to the best of our knowledge, no proposals in this regard can be found in
the centering literature.
2.5 Empirical Support for, and Applications of, Centering
Centering has served as the theoretical foundation for a lot of work in linguistics,
natural language processing (NLP), and psychology. This includes annotation stud-
ies testing the claims of the theory for languages including English, German, Hindi,
Italian, Japanese, and Turkish (e.g., Kameyama 1985; Passonneau 1993; Walker, Iida,
and Cote 1994; Di Eugenio 1998; Turan 1998) and several papers in (Walker, Joshi,
and Prince (1998b). The claims about pronominalization made in centering have been
applied to develop algorithms both for anaphora resolution (Brennan, Friedman, and
Pollard 1987; Strube and Hahn 1999; Tetreault 2001) and for sentence planning (Dale
1992; Henschel, Cheng, and Poesio 2000); this work can be viewed as providing an
evaluation of claims such as Rule 1. Ideas from centering, and in particular Rule 2, are
increasingly found useful in text planning (McKeown 1985; Kibble and Power 2000;
Knott et al 2001; Karamanis 2003).
We have already seen that some predictions of the theory have also been tested
with psychological techniques. In many of these experiments, differences in process-
ing pronominal references in a sentence in a sentence to entities with different ranks
(according to a particular instantiation of the theory) were observed: Hudson, for ex-
ample, observed that pronominal references to entities introduced in subject position
in the previous sentence are interpreted more quickly than nonpronominal references
or references to nonsubjects (Hudson, Tanenhaus, and Dell 1986; Hudson-D?Zmura
and Tanenhaus 1998). And we already mentioned that Gordon, Grosz, and Gillion
(1993) identified a processing time slowdown, the RNP, when NPs in subject position
referring to entities introduced in subject or first-mentioned position in the previous
sentence are not pronominalized.
However, the discussion in this section should have made it clear just how many
parameters the theory has and in how many different ways they can be instantiated.
To our knowledge, no previous study has attempted to analyze in a systematic way
how varying the instantiation of more than one of these parameters affects the claims
of the theory, especially for combinations of parameter settings not considered in the
original papers. This analysis is the goal of the work discussed here.
3. A Corpus-Based Comparison of Centering?s Instantiations
Given the many ways in which the parameters of centering can be set, the only feasible
way to make a systematic comparison between the theory?s ?instantiations? is by
computational means. That is, running computer simulations of the process of local
focus update using an annotated corpus and comparing the results obtained under
different instantiations. The evaluation principle we use for this comparison is the
number of ?violations? of the theory?s claims resulting when the parameters are set
in a certain way (e.g., whether pronominalization choices are in accordance with Rule
321
Poesio et al Centering: A Parametric Theory
1). In this section we discuss how we set about performing such comparison, the data
we used, our annotation methods, and how the annotation was used.
3.1 Evaluating the Claims of Centering against a Corpus
A preliminary question we had to address is what are in fact the main claims of the
theory. As discussed in Section 2, of the seven claims mentioned in Grosz, Joshi, and
Weinstein (1995), Constraint 1, Rule 1, and Rule 2 are the ones that can actually be
verified using a corpus; we concentrated on these. Because several variants of these
three claims have been proposed, we evaluated a few of these variants as well.11
The second important question is how these three claims are meant to be inter-
preted and what we can expect a corpus to tell us about them. The proponents of
centering are quite clear that the theory does not state ?hard? facts about language,
that is, the kind of facts whose violation leads to ungrammaticality judgments. Con-
straint 1, Rule 1, and Rule 2 are meant to be preferences which, when followed, lead
to texts that are easier to process.12 The mere presence of a few exceptions to a claim
does not, therefore, count as a falsification. For one thing, we should expect centering
preferences to interact with other constraints (a point not emphasized enough in the
centering literature). And for another, there may be no way of expressing a particular
piece of information without violating some such preferences.13 So at best, we can
expect the three claims to be verified in a statistical sense: that is, that the number
of utterances that verify such claims will be significantly higher than the number of
utterances that violate them?and in fact, we may find that for some claims, even
statistical significance will not be achieved. As a result, we evaluated each claim using
statistical tests; the tests we used are the sign test for Constraint 1 and Rule 1, and the
Page test for Rule 2 (Siegel and Castellan 1988).
It is also important to keep in mind that a corpus cannot tell us whether ?vi-
olations? actually result in processing difficulties: This can be determined only by
behavioral studies such as reading-time experiments. So we should make it absolutely
clear that minimizing violations cannot and should not be the only deciding factor in
theorizing about centering. Nevertheless, the combinatorics of the problem make it im-
possible to compare the parameter instantiations in any other way. Furthermore, this
form of evaluation is also the most systematic way to identify other preferences and
constraints that may interact with centering. We return to these issues in Section 5.
3.2 The Data
The data used in this work are texts from the GNOME corpus, which currently in-
cludes texts from three domains. The museum subcorpus consists of descriptions of
museum objects and brief texts about the artists who produced them.14 The pharma-
ceutical subcorpus is a selection of leaflets providing patients with legally mandatory
11 In this version of the article, we assume that the CB is defined by Constraint 3. For the results with
alternative definitions of CB, see the extended technical report or the companion Web site.
12 Beaver (2004) argues?correctly, in our opinion?that in one of the best-known pronoun resolution
algorithms based on centering, that proposed by Brennan, Friedman, and Pollard (1987), Rule 1 is
effectively used as a hard constraint, a problem fixed by Beaver?s own optimality-theoretic
reformulation of the algorithm. It is nevertheless quite clear that in the theory, Rule 1 has the status of
a preference.
13 This point is especially important from an NLG perspective: see, e.g., Karamanis (2003). We will return
to this issue in Section 5.
14 The museum subcorpus extends the corpus collected to support the ILEX and SOLE projects at the
University of Edinburgh. ILEX generates Web pages describing museum objects on the basis of the
perceived status of its user?s knowledge and of the objects she has previously looked at (Oberlander et
al. 1998) The SOLE project ILEX with concept-to-speech abilities, using linguistic information to control
intonation (Hitzeman et al 1998).
322
Computational Linguistics Volume 30, Number 3
information about their medicines.15 The GNOME corpus also includes tutorial dia-
logues from the Sherlock corpus collected at the University of Pittsburgh (Di Eugenio,
Moore, and Paolucci 1997). Each subcorpus contains about 6,000 NPs. Texts from the
first two domains were used for the main experiments reported here. The third sub-
corpus was used for the segmentation experiments discussed in the extended technical
report available on the Web site.
The data used for this study have two characteristics that make them of particular
interest. First of all, they cover genres not previously considered in studies on centering
and more similar to those that ?real? NLP applications have to contend with. At the
same time, they are strongly entity-centered (see, e.g., Knott et al[2001] for an analysis
of the museum data), so the hypotheses about coherence formulated in centering are
likely to play an important part in the way these texts are constructed.
3.3 Annotation
The previous corpus-based investigations of centering theory we are aware of (Walker
1989; Passonneau 1993, 1998; Byron and Stent 1998; Di Eugenio 1998; Hurewitz 1998;
Kameyama 1998; Strube and Hahn 1999) were all carried out by a single annotator
annotating her or his corpus according to her or his own subjective judgment. One of
our goals was to use for this study only information that could be annotated reliably
(Passonneau and Litman 1993; Carletta 1996), as we believe this will make our results
easier to replicate. The price we paid to achieve replicability is that we couldn?t test
all proposals about the computation of centering parameters proposed in the litera-
ture, especially those about segmentation and about ranking, as discussed below. The
annotation followed a detailed manual, available on the companion Web site. Eight
paid annotators were involved in the reliability studies and the annotation. In the
following we briefly discuss the information that we were able to annotate, what we
didn?t annotate, and the problems we encountered; for more details, we refer readers
to the extended version of the article and the Web site.
A systematic comparison among different ways of setting the parameters would
be prohibitively expensive with traditional psychological methods, but it?s not easy
to do with corpus analysis, either. Obviously, it can?t be done by directly annotating
?utterances? or ?CB? according to one way of fixing the parameters, as has been done
in most previous studies of centering theory (Byron and Stent 1998; Di Eugenio 1998;
Kameyama 1998; Passonneau 1993; Walker 1989). Instead, we annotated our corpus
with the primitive concepts used by different instantiations of the theory, that is, in-
formation that has been claimed by one or the other instantiation of centering to play
a role in the definitions of its basic notions. This includes, for example, how sentences
break up into clauses and subclausal units; grammatical function; and anaphoric re-
lations, including bridging references. An automatic script uses this information to
compute utterances, their CF ranking, and their CB, according to a particular way of
setting the parameters, and to compute statistics relevant to the three claims according
to that instantiation.
3.3.1 Utterances. In order to evaluate the definitions of utterance proposed in the
literature (sentences versus finite clauses), as well as the different proposals concerning
the ?previous utterance? discussed above, we marked all spans of text that might be
claimed to update the local focus. This includes sentences (defined as all units of text
15 The leaflets in the pharmaceutical subcorpus are a subset of the collection of all patient leaflets in the
United Kingdom, which was digitized to support the ICONOCLAST project at the University of
Brighton, developing tools to support multilingual generation (Scott, Power, and Evans 1998).
323
Poesio et al Centering: A Parametric Theory
ending with a period, a question mark, or an exclamation point) as well as what we
called (discourse) units. Units include clauses (defined as sequences of text containing
a verbal complex, all its obligatory arguments, and all postverbal adjuncts) as well as
other sentence subconstituents that might independently update the local focus, such
as parentheticals, preposed prepositional phrases (PPs), and (the second element of)
coordinated VPs.16
Sentences have one attribute, stype, specifying whether the sentence is declarative,
interrogative, imperative, or exclamative. The attributes of units include
? utype: whether the unit is a main clause, a relative clause, an appositive,
a parenthetical, etc.
? verbed: whether the unit contains a verb or not.
? finite: for verbed units, whether the verb is finite or not.
Marking up sentences proved to be quite easy; marking up units, on the other hand,
required extensive annotator training. The agreement on identifying the boundaries
of units, using the kappa statistic discussed in Carletta (1996), was ? = .9 (for two
annotators and 500 units); the agreement on features (two annotators and at least 200
units) was as follows: utype: ? = .76; verbed: ? = .9; finite: ? = .81. In total, the texts
used for the main study contain 505 sentences and more than 1,000 units, including
900 finite clauses
3.3.2 NPs. Our instructions for identifying NP markables derive from those proposed
in the MATE scheme for annotating anaphoric relations (Poesio, Bruneseaux, and Ro-
mary 1999), in turn derived from DRAMA (Passonneau 1997) and MUC-7 (Chinchor
and Sundheim 1995). In the GNOME corpus, NPs are marked by <ne> (?Nominal en-
tity?) elements. In total, the texts used for this study contain 3,345 NPs. These include
586 pronouns, among which are 217 third-person personal and possessive pronouns,
23 demonstratives, and 308 second-person pronouns; 1,290 definite NPs, including 554
the-NPs, 250 possessive NPs, and 391 proper nouns; 1,119 indefinite NPs, including
745 bare NPs and 269 a-NPs; and 350 other NPs, including 117 quantified NPs and
114 coordinated NPs.
We annotated 14 attributes of <ne> elements specifying their syntactic, semantic,
and discourse properties (Poesio 2000). Those relevant to the study discussed here
include the following:
? The np type, cat, with values such as a-np, that-np, the-np, and pers-pro.
? The agreement features num, per, and gen, used to identify contexts in
which the antecedent of a pronoun could be identified unambiguously.
? The grammatical function gf. Our instructions for this feature are derived
from those used in the FRAMENET project (Baker, Fillmore, and Lowe
1998); see also http://www.icsi.berkeley.edu/?framenet/. The values are
subj, obj, predicate (used for postverbal objects in copular sentences, such
as This is (a production watch)), there-obj (for postverbal objects in
there-sentences), comp (for indirect objects), adjunct (for the argument of
PPs modifying VPs), gen (for nps in determiner position in possessive
NPs), np-compl, np-part, np-mod, adj-mod, and no-gf (for nps occurring by
themselves?eg., in titles).
16 Our instructions for marking up such elements benefited from the discussion of clauses in Quirk and
Greenbaum (1973) and from Marcu?s (1999) proposals for discourse units annotation.
324
Computational Linguistics Volume 30, Number 3
The agreement values for these attributes are as follows: cat: .9; gen: .89; gf: .85; num:
.84; per: .9. We encountered problems even with supposedly ?easy? information such
as number and gender, but especially so with semantic attributes (see the annotation
manual available on the Web site). We were, however, able to mark up the attributes
relevant for this study in a reliable fashion. One exception is that we weren?t able
to reach acceptable agreement on a feature of NPs often claimed to affect ranking,
thematic roles (Sidner 1979; Cote 1988; Stevenson, Crawley, and Kleinman 1994); the
agreement value in this case was ? = .35. As a result, we were not able to evaluate
ranking functions based on thematic roles.
3.3.3 Anaphoric Information. In order to determine whether a CF of an utterance is
realized directly or indirectly in the following utterance, it is necessary to annotate
the anaphoric relations that CFs enter into, including both identity relations and, in
order to compute indirect realization, associative relations (Hawkins 1978). This type
of annotation raises, however, a number of difficult (and sometimes unresolved) se-
mantic issues (Poesio 2004). As part of the MATE and GNOME projects, an extensive
analysis of previously existing schemes for so-called ?coreference annotation,? such
as the MUC-7 scheme, was carried out, highlighting a number of problems with such
schemes, ranging from issues with the annotation methodology to semantic issues.
Although some of these schemes, like DRAMA, allow the marking of associative re-
lations, none of them analyze which among such relations can be reliably annotated
(Poesio, Bruneseaux, and Romary 1999; Poesio 2000). The semantic problems with
these schemes include the inappropriate use of the term coreference to cover semantic
relations such as that between an intensional entity like the temperature that may take
different values at different time points and these values (as in the price of aluminum
siding rose from $3.85 to $4.02) or between a quantifier and a variable the quantifier
binds, in which neither may corefer (as in none of the meetings resulted in an agreement
between its participants (van Deemter and Kibble 2000; Poesio 2004). In MATE, a general
scheme was developed which includes a fine-grained repertoire of semantic relations,
such as binding and function value (Poesio, Bruneseaux, and Romary 1999). For the
GNOME corpus, we adopted a simplified version of the MATE scheme, as for our
purposes it?s not essential to mark all semantic relations between entities introduced
by a text, but only those that may establish a ?link? between two utterances. So for
example, it is in general unnecessary in our case to mark a relation between the subject
of a copular sentence and its predicate (e.g., between the price of aluminum siding and
either $3.85 or $4.02 in the example above). Also, our texts do not include any case
of bound anaphora, so it was not necessary to offer this option to our annotators.
In the GNOME corpus, anaphoric information is marked by means of a special
?ante? element; the ?ante? element itself specifies the index of the anaphoric expression
(a ?ne? element) and the type of semantic relation (e.g., identity), whereas one or more
embedded ?anchor? elements indicate possible antecedents.17 (See (8).)
(8) <unit finite=?finite-yes? id=?u227?>
<ne id=?ne546? gf=?subj?> The drawing of
<ne id=?ne547? gf=?np-compl?>the corner cupboard </ne></ne>
<unit finite=?no-finite? id=?u228?>, or more probably
<ne id=?ne548? gf=?no-gf?> an engraving of
<ne id=?ne549? gf=?np-compl?> it </ne></ne>
17 The presence of more than one ?anchor? element indicates that the anaphoric expression is ambiguous.
325
Poesio et al Centering: A Parametric Theory
</unit>,
.
.
.
</unit>
<ante current="ne549" rel="ident"> <anchor ID="ne547"> </ante>
Work such as Sidner (1979) and Strube and Hahn (1999) as well as our own prelimi-
nary analysis, suggested that indirect realization can play a crucial role in maintaining
the CB. However, previous attempts at marking anaphoric information, particularly
in the context of the MUC initiative, suggested that while it?s fairly easy to achieve
agreement on identity relations, marking up associative relations is quite hard; this
was confirmed by studies such as Poesio and Vieira (1998). For these reasons, and to
reduce the annotators? work, we marked only a few types of relations, and we specified
priorities. Besides identity (IDENT), we marked up only three associative relations: set
membership (ELEMENT), subset (SUBSET), and ?generalized possession? (POSS), which
includes part-of relations as well as ownership relations. We marked only relations
between objects realized by noun phrases and not, for example, anaphoric references
to actions, events, or propositions implicitly introduced by clauses or sentences. We
also gave strict instructions to our annotators concerning how much to mark. (See
the annotation manual available on the Web site.) Furthermore, we specified prefer-
ences: For example, in Francois, the Dauphin, the embedding NP would be chosen as
an antecedent, rather than the NP in appositive position.
As expected, we found a reasonable (if not perfect) agreement on identity relations.
In our most recent analysis (two annotators marking the anaphoric relations between
200 NPs) we observed no real disagreements; 79.4% of the relations were marked up
by both annotators; 12.8% by only one of them; and in 7.7% of the cases, one of the
annotators marked up a closer antecedent than the other.18 With associative references,
limiting the relations did curtail the disagreements among annotators (only 4.8% of the
relations are actually marked differently), but only 22% of bridging references were
marked in the same way by both annotators; 73.17% of relations were marked by only
one or the other annotator. So reaching agreement on this information involved several
discussions between annotators and more than one pass over the corpus (Poesio 2000).
3.3.4 Segmentation. According to Grosz and Sidner (1986), centering is meant to cap-
ture preferences only within discourse segments. A proper evaluation of the claims of
the theory would therefore require a corpus in which discourse segments have been
identified. Unfortunately, discourse segments are difficult to identify reliably (Passon-
neau and Litman 1993), and Grosz and Sidner (1986) do not provide a specification
of discourse intentions explicit enough that it can be used to identify the intentional
structure of texts?which, according to Grosz and Sidner, determines their segmenta-
tion. As a result, only preliminary attempts at annotating texts according to Grosz and
Sidner?s theory have been made.
For this reason, most previous corpus-based studies of centering either ignored
segmentation or used heuristics such as those proposed by Walker (1989): Consider
every paragraph as a separate discourse segment, except when its first sentence con-
tains a pronoun in subject position or a pronoun whose agreement features are not
18 In previous work (Poesio and Vieira 1998) we came to the conclusion that kappa, while appropriate
when the number of categories is fixed and relatively small, is problematic for anaphoric reference,
when neither condition applies, and may result in inflated values of agreement.
326
Computational Linguistics Volume 30, Number 3
matched by any other CF in the same sentence. We tested only heuristic methods as
well, using the layout structure of the texts as a rough indicator of discourse structure.
In this article we discuss only the results with the heuristic proposed by Walker. In
the extended technical report available on the Web site, we discuss the results with
other segmentation heuristics, as well as further results with the tutorial dialogues
subdomain of the GNOME corpus, independently annotated according to relational
discourse analysis (Moser and Moore 1996), a technique inspired by Grosz and Sid-
ner?s proposals, from which a Grosz and Sidner?like segmentation was extracted as
proposed in Poesio and Di Eugenio (2001).
3.4 Automatic Computation of Centering Information
The annotated corpus is used by Perl scripts that automatically compute the center-
ing data structures (utterances, CFs, and CBs) according to the particular parameter
instantiation chosen, find violations of Constraint 1, Rule 1, and Rule 2 (according to
several versions of Rule 1 and Rule 2), and evaluate the claims using the statistical
tests. The behavior of the scripts is controlled by a number of parameters, including
CBdef: which definition of CB should be used. (All the results discussed in this
article were computed using the definition in Constraint 3.)
uttdef: identify utterances with sentences, finite clauses, or verbed clauses.
previous utterance: treat adjunct clauses Kameyama-style or Suri and McCoy-
style.
realization: allow only direct realization, or allow indirect realization as well.
CF-filter: treat all NPs as introducing CFs, or exclude certain classes. At the mo-
ment it is possible to omit first- and second-person NPs and/or NPs in
predicative position (e.g., a policeman in John is a policeman).
rank: rank CFs according to grammatical function, linear order, a combination of
the two as in Gordon, Grosz, and Gillion (1993), or information status as
in Strube and Hahn (1999).
prodef: consider as R1-pronouns only third-person personal pronouns (it, they),
or include also demonstrative pronouns (that, these), and/or the second-
person pronoun (you).
segmentation: identify segments using Walker?s heuristics, or with paragraphs,
sections, or whole texts.
Among the many other script parameters whose effect will not be discussed here we
will just mention those that determine whether implicit anaphors in bridging refer-
ences should be treated as CFs, the relative ranking of entities in complex NPs, and
how to handle ?preposed? adjunct clauses. (See the extended technical report on the
Web site.) The algorithm used to compute statistics concerning violations of the claims
is fairly straightforward, and we will therefore omit it here; the interested reader can
find a discussion in the extended technical report. The one additional complication that
we need to mention here is relative pronouns. As it could be argued that the decision
to generate a relative pronoun is primarily controlled by grammatical considerations,
we attempted to ignore them as much as possible, in the following sense. Our scripts
do not count an utterance as a violation/verification of Rule 1 from Grosz, Joshi, and
Weinstein (1995) if the only ?pronoun? realizing a non-CB is a relative pronoun, or
the CB is realized only by a relative pronoun. What this means in practice is that the
327
Poesio et al Centering: A Parametric Theory
Table 1
Number of utterances and CFs with the vanilla instantiation.
Museum Pharmaceutical Total
Number of utterances 430 577 1,007
Number of utterances that are segment boundaries 91 134 225
Number of CFs 1,731 1,308 3,039
number of utterances examined to evaluate Rule 1 is generally less than the number
of utterances with a CB, as we will see shortly.
4. Main Results
Given the number of parameters, it is difficult, if not impossible, to discuss the results
with all instantiations. Instead, we begin by discussing the results with what we call
the ?vanilla instantiation,? based on the settings for the parameters most often used
in discussions of the theory. We then examine the results obtained by varying the
definitions of utterance and realization. After establishing the ?best? values for these
parameters, we consider the effect of alternative ranking functions. Additional results
are discussed in the extended technical report available on the companion Web site.
Readers who want to try out instantiations not discussed here should visit the Web
site.
4.1 The Vanilla Instantiation
What we call the ?vanilla instantiation? is not an instantiation actually proposed in
the literature, but an attempt to come as close as possible to a ?mainstream? instanti-
ation of centering by blending proposals from Grosz, Joshi, and Weinstein (1995) and
Brennan, Friedman, and Pollard (1987) and incorporating additional suggestions from
Kameyama (1998) and Walker, Joshi, and Prince (1998a). The vanilla instantiation is
based on the definition of CB from Grosz, Joshi, and Weinstein (1995) and uses gram-
matical function for ranking, as proposed there and in Brennan, Friedman, Pollard
(1987). Because Grosz et al do not provide a definition of utterance, the vanilla instan-
tiation incorporates the hypothesis from Kameyama (1998) that utterances are finite
clauses and the characterization of ?previous utterance? proposed there.19 Concerning
realization, in the vanilla instantiation only third-person NPs introduce CFs, and a
discourse entity counts as ?realized? in an utterance only if it is explicitly mentioned.
For the purposes of Rule 1, we mainly studied a ?strict? definition of R1-pronoun
including only personal (and possessive) pronouns and relative pronouns and traces
(see Walker, Joshi, and Prince [1998a], page 4), but we also considered a ?broader?
definition including the demonstrative pronouns this, that, these, and those. Relative
clauses are assumed to include a link to the NP they modify, possibly not explicitly
realized. The segmentation heuristic proposed by Walker (1989) is adopted. With the
parameters set in this way, the number of utterances and CFs in our corpus is as
shown in Table 1.
19 We simplified Kameyama?s hypothesis about relative clauses by considering only instantiations in
which they were treated as utterances both ?locally? and ?globally?, and ones in which they weren?t.
328
Computational Linguistics Volume 30, Number 3
Table 2
Utterances and CBs with the vanilla instantiation.
Museum Pharmaceutical Total Percentage
Number of times at least one CF(Un) is realized in Un+1 195 162 357 (35.4%)
Utterances that satisfy Constraint 1 (have exactly one CB) 189 157 346 (34.4%)
Utterances with more than one CB 6 5 11 (1.1%)
Utterances without a CB but are segment boundary 67 96 163 (16.2%)
Utterances without a CB 168 319 487 (48.4%)
4.1.1 Constraint 1. The statistics relevant to Constraint 1 (that utterances have exactly
one/at most one CB) are shown in Table 2. This table clearly indicates that the weak
version of Constraint 1 (Weak C1) is likely to be verified with the vanilla instantiation.
Even without counting segment boundaries, Weak C1 is verified by 833 utterances
out of 1,007 (82.7%) and violated by only 11 (1.1%): The chance that Weak C1 will
not hold with a different sample is p ? 0.001 by the sign test. (We will henceforth
write +833, ?11 to indicate numbers of verifiers and violators.) On the other hand,
the strong version of C1?that every utterance has exactly one CB?is not likely to
hold with this instantiation: In our corpus, only 346 utterances (34.4%) have exactly
one CB, whereas 498 utterances (49.4%) have zero or more than one CB. With +346,
?498, the chance of error in rejecting the null hypothesis that Strong C1 doesn?t hold
is obviously much higher than 10%. The chance of error doesn?t go below 10% even
if we count the 163 utterances that do not contain references to CFs introduced in the
previous utterance but are segment boundaries and therefore are not governed by the
constraint. In other words, if the vanilla instantiation were the ?right? way of setting
the parameters, we would have to conclude that in the genres contained in our corpus,
utterances are very likely to have a unique CB, but entity coherence does not play a
major role in ensuring that a text is coherent: only 35.4% of utterances in our corpus
would be ?entity-coherent,? that is, would contain an explicit mention to an entity
realized in the previous finite clause.
The following example illustrates why there are so many violations of Strong
C1 with the vanilla instantiation. If we identify utterances with finite clauses, the
two sentences in (9) break up into five utterances, and only the last of these can be
considered in any sense to directly refer to the set of egg vases introduced in (u1).20
(9) (u1) These ?egg vases? are of exceptional quality: (u2) basketwork bases
support egg-shaped bodies (u3) and bundles of straw form the handles,
(u4) while small eggs resting in straw nests serve as the finial for
each lid. (u5) Each vase is decorated with inlaid decoration: . . .
Clearly, there are two ways of ?fixing? this problem. One is to claim that utterances
are best identified with sentences, in which case we would have only two utterances
in this example, one for each sentence. The other is to allow for indirect realization:
20 In fact, the anaphoric relation here is not identity; rather, the set of egg vases serves as domain
restriction for the quantifier in (u5). We were not able to mark this distinction reliably.
329
Poesio et al Centering: A Parametric Theory
(u2)?(u4) all contain implicit references to the egg vases, and therefore all will have a
CB if indirect realization is allowed. Both possibilities are considered below.
The fact that 11 utterances (1.1%) in the corpus have more than one CB (i.e., they
violate Weak C1 as well) is also worth noticing. The reason for the violation is that
in ?classic? centering, ranking is only required to be a partial order (see, e.g., Walker,
Joshi, and Prince 1998a page 3),21 so when two CFs with the same rank in Ui are both
realized in Ui+1, both become the CB. This is illustrated in (10), where we show the
XML markup so that the attributes of elements are visible:
(10) <unit finite=?finite-yes? id=?u227?>
<ne id=?ne546? gf=?subj?>The drawing of
<ne id=?ne547? gf=?np-compl?>the corner cupboard</ne></ne>
<unit finite=?no-finite? id=?u228?>, or more probably
<ne id=?ne548? gf=?no-gf?> an engraving of
<ne id=?ne549? gf=?np-compl?> it </ne></ne>
</unit>,
must have caught
<ne id=?ne550? gf=?obj?>
<ne id=?ne551? gf=?gen?>Branicki?s </ne> attention</ne>
</unit>
<unit id="u229" finite="finite-yes">
<ne gf="subj" id="ne552">Dubois</ne> was commissioned through
<ne gf="adjunct" id="ne553"> a Warsaw dealer </ne>
<unit id="u230" finite="finite-no"> to construct
<ne gf="obj" id="ne554"> the cabinet </ne>
for<ne gf="adjunct" id="ne555">the Polish aristocrat</ne>
</unit>
</unit>
<ante current=?ne554? rel=?ident?><anchor antecedent=?ne549?>
</anchor></ante>
<ante current=?ne555? rel=?ident?><anchor antecedent=?ne551?>
</anchor></ante>
In this example, two discourse entities introduced in utterance (u227) are realized in
utterance (u229):22 the corner cupboard (realized in (u227) by (ne547) and (ne549) and in
(u229) by (ne554)) and Branicki (realized in (u227) by (ne551), and in (u229) by (ne555)).
As their grammatical functions are equivalent under the ranking proposed by Grosz
et al, (np-compl, for np-complement, and gen, for genitive?see the annotation manual
available on the Web site), these two CFs have the same rank in (u227), so they are both
CBs of (u229). The same problem occurs with coordinated NPs, both of which have
the same grammatical function. This problem with the vanilla instantiation can also
be ?fixed? by requiring the ranking function to be a total order, which is easily done
by adding a disambiguation factor such as linear order, as done by Strube and Hahn.
On the other hand, the requirement that ranking be total has not been previously
discussed in the centering literature, and one might argue conversely that examples
such as the one above are arguments againts centering?s claim that utterances have
only one CB. We return to this issue in Section 5.
21 It?s not clear to us why ranking is required only to be partial, yet the CB is clearly claimed to be unique.
22 Neither (u228) nor (u230) is treated as an utterance as they are not finite.
330
Computational Linguistics Volume 30, Number 3
Table 3
Transition statistics for the Brennan et al version of Rule 2.
Museum Pharmaceutical Total (Percentages)
Establishment 96 93 189 (18.8%)
Continuation 37 33 70 (7.0%)
Retain 22 16 38 (3.8%)
Smooth Shift 22 15 37 (3.7%)
Rough Shift 18 5 23 (2.3%)
ZERO 87 81 168 (16.7%)
NULL 148 334 482 (47.9%)
Total 430 577 1,007
4.1.2 Rule 2. The statistics relevant for Brennan et al?s version of Rule 2 are shown
in Table 3. The most obvious consideration suggested by this table is that the three
most frequent transitions in our corpus are ones that either have not been previously
discussed in the Centering literature or have been discussed only in a limited way. By
far the most frequent transition (47.9% of the total) is NULL: following up an utterance
without a CB with a second one, also without a CB. (Examples include (u3), (u4), and
(u5) in (9).) We found this transition discussed only in Passonneau (1998). The second
most common transition (18.8%) is Kameyama?s Establishment (the transition between
an utterance without a CB and one with a CB), followed by its reverse, the ZERO
transition (between an utterance with a CB and one without), never mentioned in the
literature. (An example of ZERO is (u2) in (9).) If we ignore NULL, Establishments,
and ZEROs, the preferences are roughly as predicted by Brennan et al: The Page test
for ordered alternatives (Siegal and Castellan 1988, pages 184?188) indicates a chance
of less than .001 that the four transitions are equally likely. But only the differences
between CON and RET/SSH, and between SSH and RSH, are significant; and there
are more Shifts (SSH + RSH) than Retains.23
Grosz et al?s formulation of Rule 2 in terms of sequences also roughly holds,
except that there are too few sequences for the results to be considered significant,
as shown in Table 4. As we?ll see again in Section 5, in our corpus there seems to
be a preference for avoiding repetition; this tendency is confirmed by the numbers in
the table, which indicate a dispreference for maintaining the same CB for too long or
for maintaining it in the most salient position, at least at the level of finite clauses:
EST/CON sequences are twice as common as sequences of Continuations. As for
the claim that Retaining transitions prepare for Shifts, the figures do not lend much
support to the idea: Retains are more frequently followed by Continuations than by
Shifts, and almost as frequently by other Retains.
Of the other formulations of Rule 2, the version based on a preference for cheap
transition pairs over expensive ones proposed by Strube and Hahn is not verified with
the ranking function used in the vanilla instantiation?which is not, we should em-
23 Similar results were obtained by Passonneau (1998).
331
Poesio et al Centering: A Parametric Theory
Table 4
Rule 2 statistics considering sequences of transitions.
Museum Pharmaceutical Total
Continuation Sequences 10 6 16
Continuation/Retain 9 3 12
Establishment/Continuation 17 18 35
Retain Sequences 15 3 8
Retain/Continuation 7 7 14
Retain/Smooth Shift 3 2 5
Retain/Rough Shift 4 1 5
Smooth Shift Sequences 2 1 3
Rough Shift Sequences 2 1 3
Null Sequences 95 228 323
Other 290 312 602
Table 5
Cheap and expensive transitions with the vanilla instantiation.
Museum Pharmaceutical Total
Cheap transitions 76 63 139
Expensive transitions 263 380 643
Cheap transition pairs 21 14 35
Expensive transition pairs 162 234 396
phasize, the one assumed by Strube and Hahn themselves.24 Ignoring the 225 segment
boundary utterances, we find 396 pairs of expensive transitions and 35 pairs of cheap
transitions, as shown in Table 5. These statistics mean that in only 139 cases out of
357 (the total number of entity-coherent utterances with this instantiation; see Table 2),
CB(Ui) is predicted by CP(Ui?1). We do find that 219 utterances, the majority (61.3%)
of entity-coherent ones, are ?salient? in the sense of Kibble (2001)?that is, their CB is
the same as their CP.
4.1.3 Salience and Pronominalization. The statistics for pronominalization are shown
in Table 6. As noted above, our corpus contains 217 uses of third-person pronouns, 23
demonstratives, and 78 complementizers.25 In this instantiation we take R1-pronouns
to include only personal pronouns and complementizers, for a total of 295 R1-pronouns.
If we identify utterances with finite clauses, 61 personal pronouns (28.1%) have their
24 Similar results were found for dialogues by Byron and Stent (1998).
25 We will use the term complementizer to indicate relative pronouns and relative traces.
332
Computational Linguistics Volume 30, Number 3
Table 6
R1-pronouns in the corpus with the vanilla instantiation.
Museum Pharmaceutical Total
Total number of R1-pronouns 200 95 295
Number of personal pronouns 144 73 217
Number of complementizers 56 22 78
Table 7
cbs and pronominalization with the vanilla instantiation.
Museum Pharmaceuticla Total (Percentage)
Total number of realizations of CBs 211 163 374
Total number of CBs realized as R1-pronouns 138 68 206 (55.1%)
CBs realized as personal pronouns 85 48 133 (35.6%)
CBs realized as complementizers 53 20 73 (19.5%)
CBs not realized as R1-pronouns 73 95 168 (44.9%)
Total number of R1-pronouns that do not realize CBs 58 23 81 (27.5%)
Personal pronouns that do not realize CBs 55 21 76 (35.0%)
Complementizers that do not realize CBs 3 2 5 (6.4%)
antecedent in the same utterance, and 28 (13%) are ?long-distance pronouns? (Hitze-
man and Poesio 1998) whose antecedent is in neither the same nor the previous utter-
ance.
Table 7 shows that the relation between pronominalization and CB with the vanilla
instantiation is not straightforward: Only 55.1% of the 374 mentions of CBs26 are
pronominalized. And if relative clause complementizers were not included among
the R1-pronouns (on the grounds that the decision to use a complementizer is primar-
ily dictated by grammatical, rather than discourse, considerations), more CBs would
be realized as non-R1 pronouns (171, 44.9%) than as R1-pronouns (137, 35.9%). On the
other hand, 73% of R1-pronouns do refer to the CB.27
Table 8 analyzes pronominalization in terms of the three versions of Rule 1 we
are considering.28 Given the figures in Table 7, it should already be clear that the
stronger version of Rule 1 we considered, always pronominalize the CB?generalizing
the proposal by Gordon, Grosz, and Gillion (1993) to the less restrictive definition of
CB given by Constraint 3?is not verified: In fact, 55% of utterances violate it. The two
other versions of Rule 1 we are considering, however?Rule 1 (GJW 83), pronominalize
26 Even though only 357 utterances have a CB with this instantiation, a CB may be realized more than
once in an utterance.
27 Earlier versions of these findings led to the development of the pronominalization algorithm in
Henschel, Cheng, and Poesio (2000).
28 As discussed in Section 3, what is counted here are utterances that verify or violate Rule 1. Not all
utterances are considered: of the 346 utterances that have exactly one CB, 72 are ignored by the script
in that the only realization of an R1-pronoun is done via a relative pronoun or trace, so only 274 (27.2%
of the total number of utterances) are considered relevant for Rule 1.
333
Poesio et al Centering: A Parametric Theory
Table 8
Evaluation of the different versions of Rule 1 with the vanilla instantiation.
Museum Pharmaceutical Total (Percentage)
GJW 95?utterances that satisfy 130 135 265 (96.7%)
GJW 95?utterances that violate 7 2 9 (3.3%)
GJW 83?utterances that satisfy 117 105 222 (81%)
GJW 83?utterances that violate 20 32 52 (19.0%)
Gordon?utterances that satisfy 77 45 122 (44.5%)
Gordon?utterances that violate 60 92 152 (55.5%)
the CB if it?s the same as the CB of the previous utterance, and especially Rule 1 (GJW
95), pronominalize the CB if anything else is?are verified by most utterances.
There are two classes of violations of Rule 1 (GJW 95): possessive pronouns and
pronouns referring to ?global topics?. In (11), CB(u3), PRODUCT-X, is realized as a
proper noun, whereas a possessive pronoun is used to refer intrasententially to the
baby.29
(11) (u1) Infants and children must not be treated continuously with
PRODUCT-X for long periods
(u2) because it may reduce the activity of the adrenal glands, and so
lower resistance to disease.
(u3) Similar effects on a baby may occur after extensive use of
PRODUCT-X by its mother during the last weeks of pregnancy
(u4) or when she is breastfeeding the baby.
In the pharmaceutical leaflets, several violations of Rule 1 are found toward the end
of text, when pronouns are sometimes used to realize the product described by the
leaflet. For example, it in the following example refers to the cream discussed by the
leaflet, not mentioned in the previous two utterances.
(12) (u1) A child of 4 years needs about a third of the adult amount. (u2) A
course of treatment for a child should not normally last more than five
days (u3) unless your doctor has told you to use it for longer.
What we seem to observe here is a conflict between the ?global? preference to real-
ize the ?main character? of a discourse as a pronoun, and the ?local? preference to
pronominalize the locally most salient entity, as identified by the CB.30 By the end of
a leaflet, the product discussed in the leaflet has been mentioned a number of times,
so that it is salient enough to justify pronominalization even when it is not in CF list.
We saw in Table 7 that although there are only 9 violations of Rule 1 from Grosz,
Joshi, and Weinstein (1995), 81 R1-pronouns do not realize CBs. The majority of the
29 The problem of intrasentential pronouns in Centering is discussed, e.g., in Walker (1989), Tetreault
(2001), and Poesio and Stevenson (forthcoming).
30 See also Giouli (1996) and Byron and Stent (1998).
334
Computational Linguistics Volume 30, Number 3
72 cases of pronouns that do not refer to the CB but do not violate Rule 1 fall into
two classes: (1) R1-pronouns used in utterances without a CB (the majority) and (2)
R1-pronouns used in utterances in which the CB is pronominalized as well?as in the
following example, in which both the microscope and the amateur scientist are realized
(by a personal pronoun and a relative trace) in the relative clause (u2):
(13) (u1) This microscope belonged to an amateur scientist,
(u2) who would have used it to explore the mysteries of the natural
world.
82.6% of demonstrative pronouns in the corpus do not realize the CB, which is what
one would expect on the basis of, for example, Gundel, Hedberg, and Zacharski (1993),
and Passonneau (1993). This suggests that treating demonstrative pronouns as R1-
pronouns would not lead to improvements with respect to Rule 1. On the other hand,
because there are only 23 demonstrative pronouns in the corpus, such a change would
be unlikely to drastically affect the results. And indeed, with a broader definition of
R1-pronoun that includes demonstrative pronouns, we find a few more violations of
Rule 1 (GJW 95) (11 instead of 9) and a few less violations of Rule 1 (Gordon et al) (148
instead of 152) and of Rule 1 (GJW 83) (50 instead of 52), but none of these differences
is significant. The results reported in the rest of the article are all obtained with the
?narrow? definition of R1-pronoun that does not include demonstratives.31
4.1.4 Differences Among the Domains. The texts in the museum domain seem to be
more in agreement with the predictions of the theory than the texts in the pharmaceu-
tical domain. This is especially the case for Rule 1. There are fewer personal pronouns
in the texts in the pharmaceutical domain (73 of 1,308 CFs, [5%], as opposed to 144
of 1,731 [8%] for the museum domain), and whereas in the museum domain 40.3%
(85/211) of CB realizations are done via personal pronouns (65.4% if we consider all
R1-pronouns), in the pharmaceutical domain only 29.4% (48/163) are (41.7% for R1-
pronouns). The percentage of utterances satisfying the strong version of Constraint
1 is much higher in the museum domain (44.0%, 189/430) than in the pharmaceuti-
cal one (27.2%, 157/577), and the percentage of utterances with no CB that are not
segment boundaries is much higher in this second domain (55.3%, 319/577) than in
the first (39.1%, 168/430). Finally, almost 72% of utterances in the pharmaceutical do-
main are NULL or ZERO transitions (415/577), whereas just 54.6% are in the museum
domain (235/430); the percentage of EST and CON is also higher in the museum
domain (133/430, 31%, versus 126/577 [21.8%]). These differences are in part due to
the large number of second-person pronouns you in the pharmaceutical domain, so
that the statistics for Constraint 1 improve if we treat the entities referred to by these
pronouns as CFs, as we will see below. A second reason for these differences is that
layout plays a much more important role in the pharmaceutical domain, providing a
different way of achieving coherence. (See Section 5.)
4.2 Varying the Utterance Parameters
We now begin to explore alternative parameter settings. As always, space constraints
prevent a full discussion of all the instantiations. In this article, we discuss the results
with most of the variants quite briefly and analyze at some length only the instantiation
31 The relation between demonstrative NPs in general and the CB in our corpus is analyzed in detail in
Poesio and Nygren-Modjeska (2003).
335
Poesio et al Centering: A Parametric Theory
that identifies utterances with sentences; the results are summarized with graphs.
The extended technical report available on the companion Web site contains a more
extensive discussion of some of the variants; interested readers are also encouraged
to try further instantiations on the Web site. In this subsection we consider how the
definition of utterance (parameter uttdef) and the value of the parameter previous
utterance affect the claims.
4.2.1 Treating Coordinated Verb Phrases as Utterances. Many researchers working
on spoken dialogues or NLG assume that each element of a coordinated VP counts
as a separate utterance: that is, that in We should send the engine to Avon and hook it to
the tanker car, the coordinate VP hook it to the tanker car is actually a separate utterance.
Treating coordinated VPs as separate utterances in our corpus would of course result
in more utterances (1,041 vs. 1,007), which of course would lead to worse results unless
these utterances were treated as containing an implicit trace. If we do so, we obtain
slightly (but significantly) better results for Strong C1 (48% violations instead of 49%),
and nonsignificant differences for Rule 1 and Rule 2 (with slightly higher numbers of
Continuations and slightly lower number of Retains).
4.2.2 Using All Verbed Clauses Instead of Just the Finite Ones. A second extension
of the definition of utterance is to treat as utterances all clauses with a verb, including,
for example, the infinitival to-clause in John wants to visit Bill. The results with such an
instantiation, as well, crucially depend on our grammatical assumptions. With such
a definition, we get of course many more utterances (1,267 instead of 1,007), most of
which, like the example infinitival clause just given, do not contain explicit mentions
of the argument in subject position; so again, if we didn?t assume that traces are
present in such clauses, we would find significantly more violations of Strong C1 (685
instead of 498). Using a crude mechanism for tracking traces (adding a trace referring
to the subject of the matrix clause to all nonfinite complement clauses), we still find a
larger number of violations (598) than with the vanilla instantiation, but because the
number of utterances is much greater, these violations represent a significantly lower
percentage of the total (47% instead of 49%). We find no significant differences in the
number of violations of Rule 1. As for Rule 2, this change results in significantly fewer
NULL transitions (45.0% instead of 47.9%) and significantly more EST (22.1% instead
of 18.8%) and SSH (5.6% instead of 3.7%).
4.2.3 Restricting Finite Clauses. In general, the best results for C1 are obtained by
considering larger chunks of text as a single utterance, thus reducing the number of
utterances. In particular, fewer violations are obtained by not considering as utter-
ances finite clauses that occur as parentheticals, as subjects (as in That John could do this
to Mary was a big surprise to me), and as matrix clauses with an empty subject (as in
It is likely that John will arrive tomorrow). This merging only reduces the overall num-
ber of utterances from 1,007 to 972, but the result is a simultaneous reduction in the
violations of Strong C1 from 498 to 469 (48.2%) (which is significant by the binomial-
proportions test, though still not enough for Strong C1 to be verified) and an increase
in the number of utterances that satisfy Rule 1 (GJW 95) to 281. The violations of Rule
1 are also reduced to 8 (2.8%) (not significant). (There are virtually no changes for Rule
2.) Because of these small improvements, in the rest of the article we always exclude
these occurrences when discussing the results with finite clauses as utterances; we
refer to this instantiation as ?vanilla??.
336
Computational Linguistics Volume 30, Number 3
4.2.4 Relative Clauses. Relative clauses turned out to be one of the most complex
problems we had to face. The reader may recall that Kameyama tentatively proposes
(without empirical support) that relative clauses have a ?mixed? status: They should
be locally treated as updating the local focus, but at the global level they should
be merged with the embedding utterance. This proposal, however, seems to assume
that the local focus may be updated with the content of certain utterances some time
after they have been first processed, which is a rather radical change to the basic
assumptions of the framework. In this study, we simply compared an instantiation
in which relative clauses are treated as utterances with one in which they are not. In
addition, we considered treating relative clauses as adjuncts (i.e., as not embedded) and
treating them as complements (embedded).32 The figures reported so far were obtained
by treating relative clauses as utterances and as akin to adjuncts.33 Not treating relative
clauses as separate utterances results in a 6.5% reduction in the number of utterances
with respect to vanilla? (908 instead of 972) and in fewer violations of Strong C1 (452
[439 utterances without a CB, 13 with two CBs] instead of 469 [457 and 12]); however,
the percentage of violations is higher (49.7% vs. 48.2%). The number of violations of
Rule 1 also stays the same (8 [2.7%]). From the point of view of Rule 2, a lot of relative
clauses seem to function as EST, since their number goes down by almost 15%, to
17.3% (from 190 to 157); we also see a 30% reduction in SSH and an increase in NULL,
to 50.6% of the total. Everything else stays the same.
In purely numerical terms, then, not treating relative clauses as separate utterances
would not improve the results. Furthermore, and most important, we feel that not
treating finite relative clauses as separate utterances would make it very difficult to
maintain the principle that utterances are identified with finite clauses. For these
reasons, in the rest of the article we will continue to count relative clauses as finite
clauses.
4.2.5 Suri and McCoy?s Definition of Previous Utterance. As discussed in Section 2,
Suri and McCoy (1994) suggested that after- and before-clauses behave more like em-
bedding elements (i.e., like complements) than like coordinating ones, and Cooreman
and Sanford (1996) found evidence supporting this treatment for when clauses, as well.
The previous utterance parameter of our script can be used to compare this proposal
with Kameyama?s. When this parameter is set ?Kameyama-style,? adjunct clauses are
treated as not embedded, so that, in (14), the previous utterance for (u3) is (u2). (This
was the setting used for the results discussed so far.) When the parameter is set to
(generalized) Suri and McCoy, adjunct clauses are treated as embedded, so that the
previous utterance for (u3) is (u1):
(14) (u1) John woke up (u2) when Bill rang the door.
(u3) He had forgotten the appointment.
Using Suri and McCoy?s definition of previous utterance results in a small but sig-
nificant reduction in the number of violations of Strong C1, in small improvements
concerning R1 (GJW 95), and in small, but not significant, improvements for Rule 2. As
far as Strong C1 is concerned, 20 utterances that violate Strong C1 with Kameyama?s
32 The difference matters when the relative clause occurs at the end of an embedding clause, as in John
wanted a photograph of the man that Bill had seen entering the building at night. He . . .
33 We also remind the reader that our script treats all relative clauses as containing a link referring to the
entity modified by the relative, even when the clause does not contain an explicit relative pronoun or
complementizer, so that they never violate C1.
337
Poesio et al Centering: A Parametric Theory
definition satisfy it under Suri and McCoy?s, but 9 utterances become violations (by
the sign test, +20, ?9, p ? .03). The reduction is not, however, sufficient for Strong
C1 to be verified (+355, ?458). With Rule 1, we find that the number of utterances
that verify the GJW 95 version increases (+287), but the number and percentage of
violations stays the same (8 [about 2%]).
We should note, however, that the differences between Kameyama?s and Suri?s
definition of previous utterance have mostly to do with a type of clause that was
discussed only briefly by Kameyama and not at all by Suri and McCoy: relative clauses,
as in (15):
(15) (u1) This brooch is made of titanium,
(u2) which is one of the refractory metals.
(u3) It was made by Anne-Marie Shillitoe, an Edinburgh jeweller, in 1991.
If the ?generalized Kameyama? definition of previous utterance is adopted, the pre-
vious utterance for (u3) is the relative clause, (u2); this causes a violation of Strong
C1. In the ?generalized Suri and McCoy? instantiation, by contrast, the relative clause
is treated as embedded; this seems to be the better approach. If relative clauses were
not treated as separate utterances or were treated as embedded in both instantiations,
we would find an equal number of violations, although about 20 violations would be
different in each instantiation. One example in which the difference does have to do
with the way adjuncts are handled is (7). PRODUCT-Z is not mentioned in the adjunct
if-clause, and therefore Strong C1 is violated if (u2) is taken as previous utterance for
(u3). In this case, Suri and McCoy?s proposal works better than Kameyama?s.
(7) (u1) You should not use PRODUCT-Z
(u2) if you are pregnant or breast-feeding.
(u3) Whilst you are receiving PRODUCT-Z ....
Conversely, in the following example, the adjunct clause, as you may damage the patch
inside, introduces the entity the patch, which is then referred to in (u3), so treating
the adjunct (u2) as embedded leads to a violation of C1. In this case, Kameyama?s
definition of previous utterance gives the right result:
(16) (u1) Do not use scissors
(u2) as you may damage the patch inside.
(u3) Take out the patch.
Given that these improvements are significant, if small, in the rest of the article, we
will use Suri and McCoy?s definition when uttdef is set to finite clause. However, our
discussion, and especially the contrast between (7) and (16), gives further support to
the idea that utterances may be best identified with sentences. We consider this setting
next.
4.2.6 Sentences. The setting of uttdef with the most dramatic impact on Strong C1
is that which identifies utterances with sentences. The reasons for this have already
been illustrated with (9): If utterances are identified with sentences, there are only two
utterances in that example, both containing references to the egg vases. The reduction
in violations is such that with this instantiation more utterances verify Strong C1
338
Computational Linguistics Volume 30, Number 3
Table 9
Statistics relevant to Constraint 1 when utterances are identified with sentences.
Museum Pharmaceutical Total (Percentage)
Number of times at least one CF(Un) is realized in Un+1 131 147 278 (41.6%)
Utterances that satisfy Constraint 1 (have exactly one CB) 126 138 264 (39.5%)
Utterances with more than one CB 5 9 14 (2.1%)
Utterances without a CB but segment boundary 65 80 145 (21.7%)
Utterances without a CB 75 171 246 (36.8%)
than violate it, although not so many as to ensure verification at the 5% level.34 The
statistics relevant to Constraint 1 with this definition of utterance are shown in Table 9.
Although Strong C1 is still not verified if we consider all 669 segments of text that
contain NPs, the number of utterances that satisfy Strong C1 (264) is slightly larger
than the number of those that don?t (260).
However, identifying utterances with sentences also has several negative (if small)
effects. The main among these is that the number of violations of Rule 1 goes up:
In the case of Rule 1 (GJW 95), by 50%, from 8 to 12. The reason for this increase is
in part simply that more utterances have a CB; but in some cases, the problem could
be viewed as the CB?s not being updated quickly enough. Consider the following
example:
(17) (s1) The engravings for these rooms, showing the wall lights in place,
were reproduced in Diderot?s Encyclopedie, one of the principal works
of the Age of Enlightenment. (s2) An inscription on the Getty Museum?s
drawing for one of these wall lights explains (cl3) that it should hang
above the fireplace.
The pronoun it in sentence (s2) violates Rule 1 if utterances are viewed as sentences,
but not if they are viewed as clauses. This is because in the first case (s2) has a single
CB, the wall lights, whereas with the vanilla? instantiation clause, (cl3) is a separate
utterance, with CB one of these wall lights. Because the number of violations is still quite
small, both Rule 1 (GJW 95) and Rule 1 (GJW 83) are still verified (+252, ?12; and
+209, ?55, respectively, as opposed to +287, ?8 and +243, ?52, with the vanilla?
instantiation, Suri setting),35 although Rule 1 (Gordon et al) still isn?t (+97, ?167).
Note also that with this instantiation, the number of CBs realized by R1-pronouns
(129) is much smaller than the number realized by other types of NPs (209).
34 There is one complication: Many CFs are introduced not in sentences, but in in titles and other layout
elements that do not have a sentential format, such as Chandelier or Side effects. In order not to leave
these CFs ?stranded,? the scripts also treat as an utterance every unit that contains an NP which is not
contained in any sentence, just as we did for the vanilla instantiation. This means, however, that the
number of utterances is much larger than the number of sentences (669 instead of 505), and that Strong
C1 is not verified, even though it would be if only the 505 sentences were considered (the sign test
would then show p ? 0.001 for Strong C1).
35 The number of utterances to be tested of course varies depending on whether utterances are identified
with finite clauses (295) or sentences (264).
339
Poesio et al Centering: A Parametric Theory
Table 10
Rule 2 statistics with sentences as utterances.
Museum Pharmaceutical Total (Percentage)
Establishments 54 68 122 (18.2%)
Continuations 28 33 61 (9.1%)
Retain 22 23 45 (6.7%)
Smooth Shift 7 12 19 (2.8%)
Rough Shift 20 11 31 (4.6%)
ZERO 52 66 118 (17.6%)
NULL 88 185 273 (40.8%)
The results for Rule 2 are not that different from those obtained with finite clauses,
but we do observe more Continuations and fewer NULLs. The statistics are shown
in Table 10. Note the much greater number of Rough Shifts than of Smooth Shifts,
although the ranking suggested by Brennan et al is still verified by the Page test.
There are still too few sequences to truly test the version of Rule 2 proposed
by Grosz et al, but the preferences are roughly verified. As for the version of Rule
2 proposed by Strube and Hahn, there still 10 times as many expensive-expensive
sequences (191) as cheap-cheap ones (18).
4.2.7 Interim Summary. The effect of the changes in the definition of utterance and
previous utterance on Strong C1 and Rule 1 are summarized in Figure 1 and Figure 2,
respectively. As the figures show, most such changes have fairly small effects, even
though the effects are often significant. The one exception is identifying utterances
with sentences; treating all clauses as utterances also has a positive impact, provided
that we assume nonfinite clauses contain an implicit realization of the subject of the
matrix clause.
Even though identifying utterances with sentences leads to much better results for
Strong C1, we will not simply abandon the hypothesis that utterances may coincide
with finite clauses. This is in part for theoretical reasons, such as the fact that in
other theories of discourse in which ?units? are assumed, such as RST, these units
are generally finite clauses. Also, identifying utterances with sentences leads to small,
but significant, increases in the number of violations of Rule 1 (from 8 in the vanilla
instantiation [2.8%] to 12 [4.5%]) and in the number of Rough Shifts (from 2.9% to
4.6%). We will also see in a moment that there are other ways of changing the vanilla
instantiation that satisfy Strong C1, so identifying utterances with sentences is not
strictly necessary.
In the rest of the article we will, therefore, study the effect of changes to other
parameters both on instantiations in which utterances are identified with finite clauses
(henceforth, u = f ) and on instantiations in which they are identified with sentences
(u = s).
4.3 Realization
In this section we discuss the effect of changes in the values of the realization param-
eters: realization and CF-filter.
340
Computational Linguistics Volume 30, Number 3
Figure 1
The effect of utterance parameters on Strong C1: A summary.
Figure 2
The effect of utterance parameters on Rule 1 (GJW 95): A summary.
4.3.1 IF: Indirect Realization + u = f . Examples such as (9) indicate that another
way to reduce the number of violations of Strong Constraint 1 is to allow for indirect
realization: Then the bridging references to the egg vases in (u2), (u3), and (u4) would
make them the CB of these utterances. And indeed, if we modify the ?best? among the
u = f instantiations (vanilla?, using our generalization of Suri and McCoy?s proposal
to determine the previous utterance) to allow for indirect realization, the reduction in
violations to Strong C1 is such that with 525 utterances (54.0%) having exactly one CB
and 325 having zero or more than one (33.4%), Strong C1 is verified by the sign test
(+525, ?325).36
36 The number of utterances is obviously not affected by changes in the realization parameters.
341
Poesio et al Centering: A Parametric Theory
Table 11
Rule 2 statistics with indirect realization, 4u = f .
Museum Pharmaceutical Total (Percentage)
Establishments 75 95 170 (17.5%)
Continuations 49 40 89 (9.2%)
Retain 76 51 127 (13.1%)
Smooth Shift 39 25 64 (6.6%)
Rough Shift 60 37 97 (10%)
ZERO 60 78 138 (14.2%)
NULL 46 241 287 (29.5%)
However, allowing for indirect realization has a negative effect on other claims,
just as the change to u=s does. The first negative effect is that the number of utter-
ances with more than one CB almost doubles, from 13 with the ?generalized Suri and
McCoy? instantiation (1.3%) to 22 (2.3%). This is because by increasing the number
of ?persistent entities,? we increase the chance of their having an equivalent ranking
in the previous utterance. The number of violations of Rule 1 exactly doubles: from
8 with the Suri and McCoy instantiation to 16. But because with indirect realization
more utterances have a CB, the number of utterances that matter for the purposes of
Rule 1 also increases, from 295 to 467, so that the percentage of violations to Rule 1
does not change that much. With indirect realization, 3.4% of utterances violate Rule
1 (GJW 95), as opposed to 2.7% with generalized Suri and direct realization. As a
result, Rule 1 (GJW 95) (+451, ?16) and Rule 1 (GJW 83) (+318, ?149) are still verified,
whereas Rule 1 (Gordon et al) still isn?t (+136, ?331). An example of a pronoun that
becomes a violation of Rule 1 (GJW 95) if we allow CFs to be indirectly realized is
shown in (18). The NP one stand in (u42) realizes a bridging reference to the discourse
entity introduced by the NP the two stands in (u39), which is therefore realized in (u42),
and thus becomes its CB, but it is not pronominalized.
(18) (u39) The two stands are of the same date as the coffers, but were
originally designed to hold rectangular cabinets.
(u42) One stand was adapted in the late 1700s or early 1800s century to
make it the same height as the other.
Finally, the change to indirect realization has a big impact on the statistics for Rule 2,
shown in Table 11. On the positive side, the number of NULL transitions goes down
significantly (to less than 30%), and the percentages of the four ?classic? transitions
go up. However, the greatest increases are in the number of RET (from 3.8% to 13.1%)
and RSH (from 2.6% to 10.0%). The fact that there are many more RET than CON and
many more RSH than SSH means that this is the first instantiation for which Rule 2
(BFP) is not verified by a Page test. The reason for this can be seen in (18): Because
implicit realizations are implicit NP modifiers (i.e., one stand is interpreted as one of the
two stands), they are never CPs of an utterance. (Rule 2 [Strube and Hahn] still isn?t
verified, although the percentage of cheap transitions increases from 154/747, [20.6%]
to 207/747 [27.7%]).
342
Computational Linguistics Volume 30, Number 3
Table 12
Statistics about Strong C1 with u = s and indirect realization
MUSEUM PHARMA TOTAL
Number of times at least one CF(Un) is realized in Un + 1 194 222 416 (62.2%)
Utterances that satisfy Constraint 1 (have exactly one CB) 184 206 390 (58.3%)
Utterances with more than one CB 10 16 26 (3.9%)
Utterances without a CB that are segment boundaries 47 55 102 (15.2%)
Utterances without CB 30 121 151 (22.6%)
Below, we indicate the instantiations with u=f, Suri and McCoy-style treatment of
adjuncts, and direct realization as DF and those with the same settings, but indirect
realization, as IF.
4.3.2 IS: Indirect Realization +u = s. As one might expect, the results for Constraint
1 get even better if indirect realization is combined with the u=s setting. As shown
in Table 12, With this instantiation (henceforth, IS) 390 utterances out of 669 (58.3%)
satisfy Strong C1, and 177 (26.5%) violate it?significantly better than the instantiation
with u=s and direct realization (henceforth, DS). On the other hand, the number of
utterances with more than one CB almost doubles again (and with respect to the DS
instantiation), to 26 (3.9%) from 14 (2.1%).
The number of violations of Rule 1 (GJW 95), as well, doubles again with respect to
the DS instantiation, from 12 (4.5%) to 26 (6.7% of the 390 utterances with a CB and an
R1-pronoun). While this number of violations isn?t enough to invalidate Rule 1 (GJW
95) (+364, ?26), it is three times the number of violations with the vanilla instantiation.
As for what we called Rule 1 (Gordon et al), even with this instantiation more than
75% of utterances violate it: +97, ?293.37
The results with Rule 2 are comparable to those obtained with the IF instantiation.
Just as in that case, Rule 2 (BFP) is not verified according to a Page test, even though
there is a great reduction in the number of NULL transitions (to 23.2%). The percentage
of RET is even greater than with IF (114 [17.0%], almost twice the percentage of CON
[9.4%]) as is that of Rough Shifts (100 [14.9%], almost three times the percentage of
Smooth Shifts [5.2%]). If we ignore segment boundaries, cheap transitions are 136/444,
30.6% of the total (as opposed to 22.0% with DS and 27.7% with IF).
4.3.3 Second-Person CFs. Second-person pronouns (henceforth, PRO2s) are generally
assumed to be used deictically rather than anaphorically (see, e.g., Di Eugenio 1998).
37 Some readers might think that the additional violations of Rule 1 obtained with instantiations IF and IS
(such as the one in example (18)) shouldn?t really count as violations of Rule 1, because bridging
references such as one stand contain an implicit reference to the two stands, that is, are semantically
equivalent to one of the two stands, and it is these implicit anaphors that satisfy Rule 1. One of the
parameters not discussed here, bridges policy, controls whether these implicit anaphoric references are
treated as R1-pronouns. It turns out that doing this actually results in more violations of Rule 1, because
most bridging references do not refer either to the CB of the present utterance or the CB of the
previous one (see Poesio 2003), and every bridging reference not referring to the CB may cause a
violation. In fact, treating these implicit anaphoric references as R1-pronouns (hence, as CFs) also
dramatically increases the number of utterances with more than one CB, as well as the number of
Rough Shifts. For details, see the extended report and the companion Web site.
343
Poesio et al Centering: A Parametric Theory
However, it has been suggested in recent work that especially in dialogue, they may
actually realize CFs (Byron and Stent 1998).38 In our corpus, and especially in the phar-
maceutical domain, PRO2s are very numerous and often seem to play an important
role in maintaining the coherence of the discourse. And in fact, allowing PRO2s to
count as realizations of CFs does reduce the number of violations of Strong C1 with
both the u = f and the u = s instantiations of the theory, both with direct and with
indirect realization. Even with DF (and the Suri-McCoy setting of the previous utter-
ance parameter), allowing PRO2s to count as CFs is sufficient on its own to verify
Strong C1: The museum domain is not affected, but in the pharmaceutical domain,
the number of utterances that satisfy Strong C1 increases from 164 to 273, so that in
total 464 utterances satisfy C1 and 367 violate it, which makes the constraint verified
(by the sign test, p ? .03). With DS, if we treat PRO2s as CFs, 332 utterances verify
Strong C1, and 214 don?t (as opposed to +264, ?260 when PRO2s are not treated as
CFs). Allowing for indirect realization we get even better results: With IF, we get +623,
?242, a significant improvement even over the instantiation with direct realization and
PRO2s; with IS, we get +439, ?145.
The results with Rule 2 are also improved by treating PRO2s as CFs. The percent-
age of NULL transitions is greatly reduced (for DF, down to 35.0% [from 47.6%]; for
DS, to 30.8% [from 40.8%]; for IF, to 18.2% [from 29.5%]; for IS, to 15.1% [from 23.2%]).
As a result, the percentage of ?continuous? transitions (Kibble 2001)?EST, CON, RET,
SSH, and RSH?increases. However, RSH and SSH increase, as well as EST and CON:
In the IF instantiation with PRO2s, EST are the most common transition (20.2%), but
in the IS instantiation, RSH are (18.4%). Because of these increases, treating PRO2s as
realizations of CFs does not fix the problem with rule 2 (BFP) observed above: the
Rule still isn?t verified with IF and IS. There are no significant changes with Rule 2
(Strube and Hahn).
The results with Rule 1 crucially depend on whether we do or do not consider
second-person pronouns as R1-pronouns. In either case, letting PRO2s realize CFs
results in more violations of Rule 1 (GJW 95), both in absolute and in relative terms,
because more utterances have a CB and therefore count as violations or verifications of
the rule. But if we don?t consider PRO2s as R1-pronouns, then the increase in violations
is small: for DF, from 8 (2.7%) to 12 (2.9%); for DS, from 12 (4.5%) to 17 (5.1%); for IF,
from 16 (3.4%) to 20 (3.5%); and for IS, from 26 (6.7%) to 31 (7.1%). If we treat PRO2s as
R1-pronouns, however, the percentage of violations of Rule 1 (GJW 95) almost triples
for the u=f instantiations and doubles for the u=s ones: 31 violations for DF (7.6%), 38
for DS (11.4%), 51 for IF (9.1%), and 67 for is (15.3%). (Of course, in all of these cases,
Rule 1 [GJW 95] remains verified in a statistical sense.) The reason for this increase
in violations is that PRO2s do not seem to be very good indicators of the CB: About
half of PRO2s are not realizations of CBs, in all instantiations. Given these results,
it seems clear to us that it?s not a good idea to treat PRO2s as R1-pronouns; it?s less
clear whether to treat them as realizing cfs. As we find the position that PRO2s play a
deictic function convincing, in the rest of the article, we will not include their referents
among the CFs, but we will indicate where doing so would result in major differences.
The interested reader is advised to try the alternatives on the companion Web site.
4.3.4 Predicative NPs. The two alternative views considered so far about which en-
tities to realize both result in an increase in the number of CFs. What if we were to
38 Walker (personal communication) also observed that in Japanese, zero pronouns?often taken as
referring to the CB?are allowed to refer to second-person entities.
344
Computational Linguistics Volume 30, Number 3
attempt to reduce the number of CFs instead? Prima facie, one would imagine this type
of modification to have a negative impact on C1, but perhaps some of the violations
of Rule 1 might disappear.
Among the NPs that might be thought not to introduce CFs, an obvious candidate
are predicative NPs, that is, NPs like a policeman in John is a policeman that play the role
of predicates in the logical form of an utterance. But in fact, because our annotators
were instructed to mark up John rather than a policeman as antecedent of subsequent
anaphoric expression in these examples, filtering away such NPs does not have any
positive result at all; on the contrary, it does have a significant (if small) negative
impact on Strong C1,39 because in some cases the annotators had been forced to mark
up an NP in predicative position as the antecedent of an anaphoric expression against
the instructions. Two such examples are given below. Especially in the second case, it
is not clear how else the annotators could have marked the antecedent of Bjorg:
(19) a. An important artist in making these links has been Yasuki
Hiramatsu. His knowledge of metalcraft allows him to push and play
against the boundaries of what the material can physically do.
b. Two such jewellers are Toril Bjorg from Norway and Jacqueline Mina
from England. It may be unsurprising that Bjorg, as a Scandinavian,
should choose silver as her material.
In the following we will continue to treat predicative NPs as not introducing CFs.
4.3.5 Interim Summary. The realization parameters have an even greater impact than
the utterance parameters, especially on Strong C1 and Rule 2. Either allowing for
indirect realization or treating second-person pronouns as introducing CFs, is sufficient
for Strong C1 to be verified. When the two settings are combined, a large majority of
utterances verifies the constraint. On the other hand, allowing for indirect realization
also results in significant increases in the number of violations of Rule 1, although
overall the percentage of such violations remains pretty small. In addition, indirect
realization leads to such an increase in the number of RET and RSH that there are
fewer CON than RET and fewer SSH than RSH, and Rule 2 (BFP) is not verified by
any of the instantiations with indirect realization we have discussed. Treating PRO2s
as realizations of CFs, while sufficient to cause Strong C1 to be verified, has less of
an effect on Rule 2, but when we combine this setting with the IS instantiations, we
obtain an instantiation in which RSH is the most common transition. The effects of
the realization choices are summarized in Figures 3 and 4.
4.4 Ranking
4.4.1 Grammatical Function + Linear Disambiguation. We observed above that be-
cause grammatical function does not always specify a unique, most highly ranked CF,
when using that ranking function some utterances end up with more than one CB,
which causes the violations of the weak version of Constraint 1 seen above (up to 5.7%
of the total in the IF instantiation treating PRO2s as CF realizations). We also men-
tioned, however, that this problem can be fixed by requiring the ranking function to be
a total order, which, in turn, is easily done by adding a tie-breaking factor. Given the
results in Gernsbacher and Hargreaves (1988) and Gordon, Grosz, and Gillion (1993),
39 The difference is significantly larger for all the instantiations not treating PRO2s as CFs; larger, but not
significantly so, if PRO2s are treated as CFs.
345
Poesio et al Centering: A Parametric Theory
Figure 3
The effect of the realization parameters on the violations of Strong C1.
Figure 4
The effect of the realization parameters on the percentage of violations of Rule 1.
the most obvious disambiguating factor is linear order: Whenever two CFs are equally
ranked, assign to, say, the leftmost CF a higher rank. And indeed, we saw in Section 2
that linear order is used by Strube and Hahn (1999) to resolve ties, albeit in conjunction
with a ranking function other than grammatical function. In this section we evaluate
the ranking function obtained by adding linear order to grammatical function, which
we call GFTHERELIN.40 The results with this ranking function are summarized in
Table 13.
40 The reason for the ?there? in the function?s name is that the results can be slightly improved by a
further small change: ranking postcopular nps in there-sentences (e.g., someone in There is someone at the
door) as subjects rather than objects. See, e.g., Sidner (1979).
346
Computational Linguistics Volume 30, Number 3
Table 13
Summary of results for Strong C1, Rule 1 (GJW 95), and Rule 2 (BFP) with GFTHERELIN
ranking. ? indicates that a claim is not verified at the .05 level; ? that it?s not verified at the
.01 level.
Instantiation Strong C1 Rule 1 (GJW 95) Rule 2 (BFP) (Page test,
(Percentage violations) (Percentage violations) probability of not
being verified)
DF-predicate +352,?450 (46.3%) ? +291,?11 (3.6%) .001
DF-predicate+pro2 +465,?355 (36.5%) +403,?15 (3.6%) .001
DS-predicate +273,?249 (37.2%) ? +259,?14 (5.1%) .001
DS-predicate+pro2 +347,?197 (29.4%) +325,?22 (6.3%) .001
IF-predicate +529,?310 (31.9%) +463,?18 (3.7%) 1 ?
IF-predicate+pro2 +635,?219 (22.5%) +325,?22 (3.7%) .05?
IS-predicate +408,?157 (23.5%) +378,?30 (7.4%) .05?
IS-predicate+pro2 +469,?113 (16.9%) +432,?37 (7.4%) 1 ?
The table summarizes eight instantiations: DF, DS, IF, and IS, each in two variants
(including PRO2s and without them). For each instantiation, the table lists verifiers
and violations of Strong C1 and Rule 1 (GJW 95) and the percentage of violations, as
well as the results of the Page test for Rule 2.
Adopting GFTHERELIN as a ranking function doesn?t lead to major changes as far
as Strong C1 is concerned. This is because the only change from the results obtained
with simple grammatical function is that the utterances previously classified as having
two CBs get reclassified as having one, and with the instantiations that would benefit
the most from a reduction in Strong C1 violations?those based on DF?the number of
multi-CB sentences is fairly small, typically 1?2%, although this is enough to make the
improvement significant by the sign test with all instantiations. The improvements are
greater with the u=s instantiations, since with sentences it?s more common for more
than one CF to be realized in the same grammatical position; for example, in the IS
instantiations in which PRO2s are considered as realizations of CFs, we find that 5.7%
of utterances (38/669) have more than one CB. However, Strong C1 is already verified
with these instantiations, even with simple grammatical function.
As in all previous cases, better results with Strong C1 are counterbalanced by
worse results for Rule 1?although, again, not so much worse that Rule 1 ends up
not being verified. The results with the DF instantiations aren?t significantly worse:
For example, we find +291, ?11 (3.6%) with the instantiation not including predicative
NPs and second-person pronouns, as opposed to +280, ?9 for the same instantiation
with simple grammatical function ranking. The number of violations of Rule 1 is
significantly greater with the DS instantiation if PRO2s are treated as CF realizations:
+325, -22 (6.3%) vs. +310, -17 (5.2%). In two of the additional five violations of Rule
1, however, the problem is simply that by adding a disambiguation element, we turn
utterances whose CB is undefined (because more than one CF is equally ranked) into
utterances with a CB. One such example is (20):
(20) (s7) Intended to hold jewels or small precious items, the interiors of
this pair of coffers are lined with tortoiseshell and brass or pewter, with
secret compartments in the base.
347
Poesio et al Centering: A Parametric Theory
Table 14
Transition percentages for IS with GFTHERELIN ranking.
Museum Pharmaceutical Total (Percentage)
Establishments 47 60 107 (16.0%)
Continuations 28 44 72 (10.8%)
Retain 56 65 121 (18.1%)
Smooth Shift 8 24 32 (4.8%)
Rough Shift 48 28 76 (11.4%)
ZERO 43 58 101 (15.1%)
NULL 41 119 160 (23.9%)
(s8) The coffers are each decorated using techniques known as premiere
partie marquetry, a pattern of brass and pewter on a tortoiseshell ground,
and its reverse, contrepartie, a tortoiseshell pattern on a background of
pewter and brass.
With simple grammatical function, both the coffers and brass are CBs of (s8), which
is therefore treated by our script as not having a well-defined CB. As a result, the
pronominalization of a non-CB, premiere partie marquetry, is not counted as a violation.
(s8), however, becomes a violation with GFTHERELIN, since the coffers becomes its
only CB.
With the IF instantiation, the percentage of violations of Rule 1 (3.7%) is nonsignif-
icantly greater than the percentage with simple grammatical function (3.5%). The
percentage of violations with the two IS instantiations (7.4%) is significantly worse (at
the .01 level) than with simple grammatical function (6.7% and 7.1%, respectively).
Table 13 shows that using GFTHERELIN has a positive effect on the number
of violations of Rule 2 (BFP). Whereas with simple grammatical function, none of the
instantiations with indirect realization verifies Rule 2 (BFP) by the Page rank test, with
GFTHERELIN the IS instantiation does (although only at the .05 level), as does IF if
PRO2s are treated as CF realizations. (All direct realization instantiations still verify
the rule.) The main reason for this change is a significant reduction in the percentage
of RSH with GFTHERLIN, especially for the IF and IS instantiations: With IF we see
a reduction in RSH from 9.7% to 7.6%; with IS, from 14.6% to 11.4%. With the DS
instantiation the percentage of RET and RSH is about twice what we find with the
DF instantiation, just as we observed with simple grammatical function ranking, but
otherwise the results are pretty similar to those with DF. With the IF and the IS
instantiations, we get small but significant increases in CON and RET and a reduction
in RSH. In Table 14, we report the complete percentages for this instantiation, for
comparison with other ranking functions.
The change to GFTHERELIN hardly affects the relative percentages of cheap and
expensive transitions, so the results concerning Rule 2 (Strube and Hahn) do not
change.
The IS instantiation with GFTHERELIN ranking is the one in which all three claims
are verified without need to treat PRO2s as CF realizations, even though Rule 2 is
348
Computational Linguistics Volume 30, Number 3
verified with this instantiation only at the .05 level. We will therefore concentrate on
this instantiation when making comparisons with the other ranking variants.
4.4.2 Linear Order. Among the ranking functions alternative to grammatical function,
perhaps the simplest is the one that ranks CFs in the order of their occurrence in the
utterance, from left to right. This ranking function was explicitly proposed by Rambow
(1993) to account for facts about scrambling in German, and effects of order of mention
have been observed by, among others, Gernsbacher and Hargreaves (1988), Gordon,
Grosz, and Gillion (1993), and Stevenson, Crawley, and Kleinman (1994).
Using linear order instead of GFTHERELIN has no effect at all on Constraint 1,
as one would expect, since all that matters for the constraint to be verified is whether
discourse entities are mentioned in successive utterances and whether the ranking
function is total. However, no significant differences were observed with Rule 1 (GJW
95), either: with IS, we find +378, ?30, with linear order, as opposed to +377, ?31,
with GFTHERELIN.41 This is because linear order is a very good approximation of
grammatical function in English: Subjects tend to occur in first position, objects in
second position, etc. The one claim for which the differences are significant is Rule 2
(BFP): with IS, enough CON become RET and enough SSH become RSH that Rule 2
is no longer verified even at the .05 level. (The rule is still verified in the DF and the
DS instantiations.)
All in all, these results are not grounds to argue that linear order is a better rank-
ing function than GFTHERELIN;42 however, because the differences are so small, they
also suggest that linear order (which is far easier to compute) might be a good ap-
proximation of grammatical-function ranking for practical applications working with
English.
4.4.3 Information Structure. Replacing GFTHERELIN with the ranking function pro-
posed by Strube and Hahn (1999) (henceforth, STRUBE-HAHN): rank hearer-old en-
tities more highly than inferrables, and these higher than hearer-old entities?cannot
and did not lead to different results for Strong C1, for the reasons already discussed
for linear-order ranking. Less expected was the fact that?again, just as in the case
of linear order?we didn?t find any significant differences with Rule 1 (GJW 95), ei-
ther, although with the IF and IS instantiations, we find one more violation than with
GFTHERELIN.43 This doesn?t mean that the exact same utterances are violations in
both cases, rather, that the differences ?balance out?. We already saw one example in
which STRUBE-HAHN ranking results in a violation of Rule 1, whereas GFTHERELIN
ranking doesn?t: This is the first sentence in (10), illustrating the kind of situations in
which a partial ranking may result in two CBs. We repeat that sentence in (21) and
include the preceding sentence:
(21) (s67) An inventory of Count Branicki?s possessions made at his death
describes both the corner cupboard and the objects displayed on its
shelves: a collection of mounted Chinese porcelain and clocks, some
embellished with porcelain flowers.
41 With IF the difference goes the other way: +463, ?18, for GFTHERELIN, +463, ?19, for linear order.
There are no differences at all with DF and DS.
42 This point is reinforced by a number of results from Gordon and collaborators (e.g., Gordon, Grosz,
and Gillion 1993; Gordon et al 1999) suggesting that hierarchical position in the parse tree is a better
predictor of salience than linear order, as well as by results suggesting that for a range of languages,
linear order is much less effective?see, for example, Prasad and Strube (2000) for Hindi.
43 We discuss the results only with the version of Rule 1 proposed by Grosz, Joshi, and Weinstein (1995).
349
Poesio et al Centering: A Parametric Theory
Table 15
Transition percentages for IS with STRUBE-HAHN ranking.
Museum Pharmaceutical Total (Percentage)
Establishments 47 60 107 (16.0%)
Continuations 39 55 94 (14.1%)
Retain 50 53 103 (15.4%)
Smooth Shift 18 26 44 (6.6%)
Rough Shift 33 27 60 (9.0%)
ZERO 43 58 101 (15.1%)
NULL 41 119 160 (23.9%)
(s68) The drawing of the corner cupboard, or more probably an
engraving of it, must have caught Branicki?s attention.
As the corner cupboard is in object position, it gets higher ranking in (s67) than Count
Branicki, which is in NP-modifier position, which?while not explicitly discussed in
the centering literature?will presumably fall among the ?other? cases. As a result,
the cupboard is the CB of (s68), and its pronominalization is predicted by Rule 1.
With STRUBE-HAHN ranking, Count Branicki is the highest-ranked entity of (s67), and
therefore the CB of (s68); hence the violation. Conversely, (22) is an example in which
GFTHERELIN and ranking results in a violation of Rule 1, while STRUBE-HAHN
ranking doesn?t:
(22) (s88) Christened by his contemporaries as ?the most skillful artisan in
Paris,? Andre`-Charles Boulle?s name is synonymous with the practice of
veneering furniture with marquetry of tortoiseshell, pewter, and brass.
(s89) Although he did not invent the technique, Boulle was its greatest
practitioner and lent his name to its common name: Boulle work.
In this example, Andre`-Charles Boulle?s name, the subject of (s88), is ranked higher than
Andre`-Charles Boulle, and is therefore the CB of (s89), in which, however, it is not
pronominalized, even though both Boulle and the technique he invented are. Notice
that (21) and (22) are almost stereotypical instances of the class of examples that led
Sidner (1979) to argue that two foci are needed, one for animated entities, and one for
the entities acted upon; we return to this issue in Section 5.
The one claim where STRUBE-HAHN ranking makes a clear difference is Rule 2
(BFP). About 20% of RET become CON and about 20% of RSH become SSH. Although
we still find more RET than CON and more RSH than SSH, these changes are sufficient
to cause Rule 2 (BFP) to be verified at the .01 level in all instantiations considered.44 The
transition percentages with IS and STRUBE-HAHN ranking are presented in Table 15.
Even with IS, however?the instantiation closest to the one proposed by Strube
and Hahn?we still find many more expensive transitions (272) than cheap ones (172)
44 With DF and DS the number of RET and RSH goes down drastically, so that we do find more CON
than RET and more SSH than RSH, but we still find more SSH than RET.
350
Computational Linguistics Volume 30, Number 3
and almost three times as many expensive-expensive sequences (137) as cheap-cheap
ones (56), so Rule 2 (Strube and Hahn) is not verified.
4.4.4 Summary. Because Strong C1 is the most problematic claim, it was to be expected
that the most studied parameter of centering, ranking, would have a smaller impact
than the utterance and realization parameters. It is nevertheless interesting that the
results for Rule 1 (GJW 95) are virtually identical under the three versions of ranking
we considered. More differences can be found for Rule 2 (BFP), which is not verified
by any instantiation with linear-order ranking and is verified only by a few instantia-
tions with GFTHERELIN. Adopting STRUBE-HAHN ranking does result in a greater
percentage of utterances being classified into one of the ?continuous? classes and in a
lower probability of Rule 2?s (BFP) being falsified. Finally, not even these last changes
to parameter settings were sufficient to cause either Rule 1 (Gordon et al) or Rule 2
(Strube and Hahn) to be verified.
5. Discussion
In this section we first discuss the effects of different parameter settings on the re-
sults obtained; we then analyze the claims of centering theory, draw a few theoretical
conclusions, and make some suggestions for further work (empirical and theoretical).
5.1 Setting the Parameters
5.1.1 Comparing Instantiations. A central goal of this study was to compare different
ways of instantiating centering?s parameters, and different versions of its claims, on a
single data set, also examining combinations not previously considered (e.g., whether
Brennan et al?s version of Rule 2 would be verified when the parameters were set
as suggested by Strube and Hahn and vice versa. Our first interesting result in this
sense is that if the parameters are set in the most ?mainstream? way (the ?vanilla?
instantiation) only Rule 1 (GJW 95) and Rule 1 (GJW 83) are clearly verified. The results
concerning Constraint 1 are especially negative. As with this instantiation only 35% of
utterances are continuous?that is, CF(Un) ? CF(Un?1) = ? (Kibble 2000; Karamanis
2001)?only the weak version of Constraint 1 is verified. Strong C1, the best-known
formulation, and the one that in our view best captures the idea of ?entity coherence,?
clearly doesn?t hold. Another interesting observation is that if ranking is required
only to be partial, some utterances end up with more than one CB: The percentage
of such utterances is only 1% with the vanilla instantiation but can be as high as 6%
with some instantiations. This is perhaps obvious, but to our knowledge, it had not
been previously discussed.
As for Rule 2, with the vanilla instantiation the version proposed by Brennan et
al. is verified by a Page rank test, but arguably, the most striking fact about transitions
with this instantiation is the prevalence of NULL transitions (47.9%), Establishments
(18.8%), and ZEROs (16.7%). All together, the four types of transitions falling under
the remit of Rule 2 account for only 16% of utterances, and if Smooth Shifts and Rough
Shifts are counted together, with this instantiation there are more Shifts than Retains.
Other classifications and versions of the rule do not correlate much better with the
observed frequencies: For example, only 39% of entity-coherent transitions (139 out
of 357), and 14% of the total, are cheap in the sense of Strube and Hahn (1999) (i.e.,
CP(Un?1) predicts CB(Un)).
These findings concerning the vanilla instantiation should not, however, lead us
to conclude that the theory in general is not verified. Our second major finding is that
parameters do matter: that is, it is possible to set the parameters in such a way as to
351
Poesio et al Centering: A Parametric Theory
make all three claims verified in a statistical sense. However, because Strong C1 is the
claim with the largest percentage of violations, the parameters whose setting matters
the most when one is trying to find an instantiation in which all claims are satisfied
are those controlling utterance definition and CF realization. Considering a center as
realized in an utterance which contains an associative reference to that center is suf-
ficient for Strong C1 to be verified; identifying utterances with sentences instead of
finite clauses also has a strong positive effect. With the resulting instantiations (IF and
IS), Strong C1 is verified, as well as the two ?basic? versions of Rule 1.
We also found, however, that there is a trade-off between Strong C1, on one side,
and Rule 1 and Rule 2, on the other: The changes to the utterance and realization
parameters just mentioned, while reducing the violations of Strong C1, increase those
of Rule 1 and Rule 2 (see, e.g., Table 13). Identifying utterances with sentences, or (to a
lesser extent) allowing indirect realization, results in statistically significant increases
in the number of violations of Rule 1?up to a total of 7.4% in the IS instantiation (see
Figures 2 and 4)?although Rule 1 (GJW 95) and Rule 1 (GJW 83) are so robust that
they are still verified, even in these instantiations.45 These changes to the utterance
and realization parameters have an even greater impact on Rule 2 (BFP), which is
only weakly verified with the vanilla instantiation. With the IF and IS instantiations
and grammatical-function ranking, we find many more RSH than SSH and many
more RET than ?pure? CON (i.e., without counting Establishments); indeed, in the IS
instantiation with GFTHERELIN ranking, RET are the second most common transition.
As a result, Rule 2 (BFP) is verified with is instantiations only at the .05 level, and with
IF instantiations only if second-person pronouns are counted as realizations of CFs.
On the positive side, with these instantiations a much greater percentage of utterances
(45%) are classified as either CON, RET, SSH, or RSH, and a further 16% as EST.
These results can be further strengthened by making one last change to the param-
eters: adopting the ranking function proposed by Strube and Hahn (1999) instead of
GFTHERELIN. With this instantiation, Rule 2 (BFP) is verified at the .01 level, rather
than only at the .05 level. This is because although the STRUBE-HAHN ranking func-
tion has no effect on Strong C1 (obviously) or R1 (more surprisingly), it does result in
some of the RET becoming CON and some of the SSH becoming RSH. Even though
we still find more RET than CON and more RSH than SSH, these changes are enough
to cause Rule 2 (BFP) to be verified at the .01 level with the IS instantiation. Strube
and Hahn?s own version of Rule 2 still isn?t verified, but this version of the rule is not
verified by any of the instantiations we evaluated. In other words, with the IS or IF
instantiation and STRUBE-HAHN ranking, all three claims of the theory are verified
at the .01 level.
The final observation concerning parameter settings is that issues not widely dis-
cussed in the centering literature had a greater impact on the theory?s claims in our
experiments than parameters such as the choice of ranking function or the definition
of previous utterance. Many of these issues, such as the treatment of second-person
pronouns and of empty categories, have to do with the general issue of which entities
should be included in the CF list. Considering second-person pronouns to be realiza-
tions of discourse entities is enough to make Strong C1 satisfied; we also found that
45 Perhaps the most spectacular demonstration of the trade-off between Strong C1 and Rule 1 can be seen
with the versions of the theory that adopt the definitions of CB proposed by Gordon, Grosz, and
Gillion (1993) and Passonneau (1993). (These instantiations are not discussed in this article, but can be
examined on the companion Web site.) By adopting a particularly restrictive definition of CB, these
versions succeed in reducing (indeed, eliminating, in the case of Passonneau) the violations of Rule 1,
but the price is that only a very few utterances have a CB.
352
Computational Linguistics Volume 30, Number 3
a number of extensions to the definition of utterance, such as the inclusion of rela-
tive clauses and nonfinite clauses, led to much worse results unless reduced relative
clauses and nonfinite clauses were taken to include traces linking these clauses to the
clause in which they were embedded.
5.1.2 Minimizing Violations Should Not Be the Overriding Goal. We said in Sec-
tion 3 that we don?t think that minimizing violations should be the only factor taken
into account in deciding how to set parameters. Some violations are best accepted and
explained in terms of the interaction of centering preferences with other preferences.
(See below.)
Special care is needed when alternative definitions are supported by cross-linguistic
evidence or by the results of psychological studies. In the case of ranking, although
we didn?t find any significant differences between grammatical-function ranking and
linear-order ranking for English, one should keep in mind that such differences have
been found for other languages, especially more free-order ones. Prasad and Strube
(2000), for example, found that in Hindi the difference between grammatical-function
and linear-order ranking is significant, and Strube and Hahn (1999) found significant
differences between grammatical function and information structure in German. Con-
versely, before taking the evidence for a slight advantage of STRUBE-HAHN ranking
over grammatical-function ranking as conclusive, one needs to supplement our stud-
ies with psychological experiments reconciling these results with numerous results
indicating the important role played by grammatical function, especially subjecthood
(among others, Hudson, Tanenhaus, and Dell [1986]; Gordon, Grosz, and Gillion [1993];
Brennan [1995]). Information structure has also been found not to be appropriate for
languages including Greek, Hindi, and Turkish (Turan 1998; Prasad and Strube 2000;
Miltsakaki 2002). Similar considerations apply to the definition of previous utterance,
since we saw that a considerable amount of psychological evidence supports treating
adjuncts as embedded, at least when the syntactically embedded clause is at the end
of the sentence (Cooreman and Sanford 1996; Pearson, Stevenson, and Poesio 2000).
In the case of the definition of utterance, our results indicate that identifying utter-
ances with sentences, rather than finite clauses, leads to results much more consistent
with the claimed preference for discourses to be entity-coherent. While this result is
likely to be useful for a number of reasons and for different types of applications (e.g.,
text planners), we believe that further empirical and theoretical work is needed before
conclusions can be reached about when the local focus is updated. For one thing, most
analyses of discourse structure?for example, rhetorical structures theory (Mann and
Thompson 1988)?view clauses as the basic unit of discourse in written text. And in
spoken dialogue one can hardly find any complete sentences; in this case, the update
unit is much more likely to be a prosodic phrase of some sort.
5.2 The Claims of Centering, Revisited
5.2.1 Centering, Pronominalization, and Salience. One clear result of this work is that
centering?s claims about pronominalization?at least, those expressed by the versions
of Rule 1 proposed in Grosz, Joshi, and Weinstein (1995, 1983)?are very robust. Rule 1
(GJW 95) and Rule 1 (GJW 83) are verified with all parameter instantiations, and in a
very convincing way: In the instantiations we considered, the percentage of violations
of Rule 1 (GJW 95) never exceeds 8% of the total number of utterances.
On the other hand, one should keep in mind that these two versions of Rule 1
make very weak claims about pronominalization. All that Rule 1 (GJW 95) says is that
if we decide to pronominalize, then we should pronominalize the CB. This formulation
doesn?t address the real problem for a theory of pronominalization or, more generally,
353
Poesio et al Centering: A Parametric Theory
of NP form decision, which is to decide when a discourse entity should be realized
as a pronoun (Henschel, Cheng, and Poesio 2000). And our results also indicate that
simply strengthening Rule 1 to the form ?pronominalize the CB,? which can be seen
as a generalization of the proposals in Gordon, Grosz, and Gillion (1993), would be
a very bad idea: Between 50% (with u = f ) and 60% (with u = s) of mentions of
the CB are not realized using a pronoun, and conversely, between 30% and 40% of
personal pronouns are not realizations of the CB. Examples like (12) illustrate one
type of situation in which a mismatch between the CB and pronominalization may
occur: By having been mentioned often in a discourse, a discourse entity may become
sufficiently salient (at the global level) to justify pronominalization even when it is
not the CB.46 These observations suggest that the decision to pronominalize does not
depend only on whether a discourse entity is the CB but must involve a number of
further constraints and preferences.47
5.2.2 CT as a Theory of Coherence: Constraint 1. Another result of this work is that
the validity of centering?s claims about local coherence?Constraint 1 and Rule 2?
depends on the choice of the parameters to a much greater extent than is the case for
the claims about pronominalization. Strong C1 does not hold for the vanilla instan-
tiation, although it does hold for any instantiation in which the implicit anaphoric
component of bridging references is treated as an indirect realization and for many
instantiations in which utterances are identified with sentences. But even under the
most favorable parameter instantiations, there are many more exceptions to Strong
C1 (between 20% and 25% of the total number of utterances) than we find even with
the instantiations which are worse for Rule 1 (7?8%). While the weak version of C1,
requiring only that there is at most one most salient entity per utterance, does hold
even with the vanilla instantiation and does capture the claim that utterances with a
unique CB are easier to process, a central aspect of centering since Joshi and Kuhn
(1979) and Joshi and Weinstein (1981), it says nothing about entity coherence?s being
what ensures local coherence.
Further light on entity coherence is shed by recent work on text planning, particu-
larly by Karamanis (2003), which suggests that when all alternative ways of extracting
a text plan from the propositions expressed by texts such as those we are studying
are considered, the actual ordering found in the texts tends to be in greater agreement
with centering?s preferences about entity coherence than with most of its alternatives.
After extracting the propositions48 expressed by texts in the museum subdomain of
our corpus, Karamanis determined that although the sequence actually found in such
texts is not optimal as far as minimizing the violations to entity coherence (with the
instantiation he considers, more than 50% of the utterances violate Strong C1), approx-
imately 70% of the alternative orderings introduce even more violations.
If we accept that the texts in our corpus are coherent, these results suggest that
there must be other ways of achieving local coherence, apart from what we have
been calling here ?entity coherence?. An obvious candidate for an additional, or
46 The role of global focus in the interpretation of pronouns needs further study. A few preliminary
observations can be found in Hitzeman and Poesio (1998).
47 The discrepancy between pronominalization and CB-hood in our corpus is analyzed in more detail by
Henschel, Cheng, and Poesio (2000), who propose an algorithm for pronominalization that takes into
account factors such as the presence of distractors matching the CB?s agreement features that may lead
to the decision not to pronominalize, as well as factors that may result in the pronominalization of a
non-CB. The algorithm achieves an accuracy of 87.8% in the museum domain.
48 More precisely, the lists of CF realized by each utterance with a DF instantiation, representing that
utterance?s arguments.
354
Computational Linguistics Volume 30, Number 3
alternative, coherence-inducing device are rhetorical relations. Indeed, the claim that
?entity? coherence needs to be supplemented by ?relational? coherence can be found
as far back as Kintsch and van Dijk (1978) and Hobbs (1979). This view is supported
by an analysis of our data. With the u=f instantiations, we find in the pharmaceutical
subdomain many examples in which successive utterances do not mention the same
entities, but the connection between clauses is explicitly indicated by connectives, as
in (23):
(23) (u1) This leaflet is a summary of the important information about
Product A.
(u2) If you have any questions or are not sure about anything to do with
your treatment,
(u3) ask your doctor or your pharmacist.
A more complex case are utterances in the museum subdomain that do not refer to
any of the previous CFs because they express generic statements about the class of
objects of which the object under discussion is an instance, or utterances that make
a generic point that will then be illustrated by a specific object. In (24), (u2) gives
background concerning the decoration of a cabinet:
(24) (u1) On the drawer above the door, gilt-bronze military trophies flank a
medallion portrait of Louis XIV. (u2) In the Dutch Wars of 1672?1678,
France fought simultaneously against the Dutch, Spanish, and Imperial
armies, defeating them all. (u3) This cabinet celebrates the Treaty of
Nijmegen, which concluded the war.
While the analysis of such cases in terms of rhetorical relations is more complex, it
seems clear to us that an analysis in terms of underlying semantic connections between
events or propositions is more perspicuous than one in terms of entity coherence.
While it is true that some of these violations could be fixed by adopting a broader
notion of bridging reference?for example, in (24) we might treat France as a bridge to
Louis XIV?this wider notion of bridging reference has proven to be very difficult to
identify in a reliable way.
Now, given that in an RST-style analysis, every discourse unit is connected by at
least one rhetorical link to at least another discourse unit, one might wonder whether
?entity coherence? is still needed once ?relational coherence? is introduced. However,
Knott et al (2001) convincingly argue that in RST, complete connectivity is usually
achieved by introducing relations such as ?Elaboration? that, when looked at closely,
turn out really to be attempts to capture a notion of entity coherence. This work on
rhetorical relations is coming to a position symmetrical to our own: that a purely rela-
tional account is not sufficient, and a separate theory of entity coherence is necessary
(Knott et al 2001).49
5.2.3 Topic Continuity: Rule 2. Rule 2?stating a preference not just to keep talking
about the same objects, but to preserve their relative ranking?also seems much less
robust than Rule 1, irrespective of its formulation and of the instantiation.
49 The respective role of entity coherence, relational coherence, and other forms of coherence in the
examples in our corpus is studied in more detail in Oberlander and Poesio (2002).
355
Poesio et al Centering: A Parametric Theory
As already noted, one of the most interesting observations about this aspect of
the theory concerns the classification of utterances used to formalize it (at least in
the earlier versions of the theory). With pretty much all parameter instantiations that
we tested, two of the most common transitions were the NULL transition (between
two utterances neither of which has a CB), previously considered only in Passonneau
(1998), and the ZERO transition (from an utterance with a CB to one without), which
as far as we can see has never been discussed before. Indeed, with the vanilla in-
stantiation, 84% of all utterances are either NULL, ZERO, or EST and therefore fall
outside the scope of Rule 2 in almost all its formulations. The question raised by this
finding is whether the theory has to be extended to cover such cases or whether they
have to be accounted for by other components of an overall theory of discourse (see
below).
Three versions of Rule 2 were tested in some detail.50 The version of Rule 2 from
Grosz, Joshi, and Weinstein (1995), formulated in terms of sequences and stating a
preference for sequences of CON over sequences of RET over sequences of SHIFT
(which we tested by counting the number of sequence pairs), suffers from the prob-
lem that even with the ?best? instantiations, fewer than one-third of sequence pairs
involve the same transition, and even fewer are sequences of the transitions consid-
ered by Grosz et al Even in the instantiation which yields the best results for Rule 2
(BFP), IS with STRUBE-HAHN ranking, only 13% of sequence pairs are of the form
CON-CON/RET-RET/SH-SH, and altogether only 28% of sequence pairs involve only
transitions considered by Grosz et al Keeping in mind that Rule 2 (GJW 95) applies
only to a minority of sequence pairs, we do find that with IS and STRUBE-HAHN
ranking, the number of CON-CON sequences (37) slightly exceeds the number of RET-
RET (35), which in turn exceeds the number of SH-SH (19, of which 16 are RSH-RSH).
This doesn?t hold with GFTHERELIN ranking, where RET-RET exceeds CON-CON
even if we treat EST as a type of CON; we find no significant difference between the
IF and the IS setting.
Rule 2 (BFP), formulated in terms of single transitions, accounts for larger per-
centages of the data (single utterances) and was found to be verified both with the
vanilla instantiation and with the ?best? instantiations. However, we still observed a
large percentage of NULL transitions with most instantiations; we also found more
RET than CON and more RSH than SSH in most instantiations in which utterances
are identified with sentences or allow for indirect realization.51
Finally, Strube and Hahn?s preference for sequences of cheap transitions over se-
quences of expensive ones isn?t verified by any of the instantiations we tested; in-
deed, in all instantiations we studied we found more expensive transitions than cheap
ones, meaning that the CP of one utterance generally doesn?t predict the CB of the
next.
These mixed results are in line with those of psychological experiments, which
so far haven?t found clear evidence supporting the claim that, say, Continuations
are easier to process than Shifts, let alne Retains (Gordon, Grosz, and Gillion
1993)
50 As noted earlier, an earlier version of Kibble?s proposal was also tested; the results can be viewed on
the companion Web site.
51 CON can be made the most frequent transition by merging EST and CON. We found, however, that
this merging leads to worse results as far as the correlation between the classification of transitions and
two of the linguistic phenomena for which the classification has been used: predicting the form of
subject NPs, and predicting segment boundaries. These results are discussed in the technical
report.
356
Computational Linguistics Volume 30, Number 3
5.3 Theoretical Consequences
While proposing modifications of centering is beyond the scope of this article, we
believe our results do have broad theoretical consequences worthy of further explo-
ration.
5.3.1 Clarification of the Claims and Identification of Further Parameters. Apart from
comparing different ways of setting the parameters already discussed in the literature,
our work had the more fundamental goal of clarifying the claims of centering theory
by identifying aspects that need to be made more precise. Our study raised a number
of questions about the definitions of the concepts used in centering not previously
examined in the literature or only discussed in passing.
Many of these questions have to do with realization, one of the least studied as-
pects of the theory. One such question is the status of entities realized as second-person
pronouns. Our results indicated that if PRO2s are not considered realizations of CF, or
if we treat them as R1-pronouns, we find many more violations of Strong C1 and Rule
1, respectively (although both claims are still verified). We also saw that the results
concerning Constraint 1 and Rule 1 depended on whether reduced relative clauses and
non-finite VPs were assumed to contain traces and whether or not these traces were
assumed to be R1-pronouns. More generally, we identified the need for a clear defi-
nition of ?R1-pronoun?: that is, whether we should include traces in relative clauses,
the implicit anaphoric elements of bridging references, and demonstrative pronouns
among the ?pronouns? to which Rule 1 applies. This question isn?t mentioned in the
literature we know of, yet our results indicate that, for example, treating the implicit
anaphoric elements of bridging references or second-person pronouns as R1-pronouns
is a very bad idea.
Some of the issues raised by this study are relevant only for certain parameter in-
stantiations. One example is the specification of grammatical function ranking beyond
the simplest cases: for example, whether postcopular NPs in there-clauses should be
treated as subjects or objects (our results suggest the former) or how nominal mod-
ifiers should be ranked (we treated them as adjuncts). An issue for instantiations in
which utterances are identified with finite clauses is what the previous utterance is
when an embedded finite clause is in the middle of another finite clause, rather than
at the end, as in the following example, from the Guardian:
(25) But Hutchinson, who appointed Ranieri last season, today said that he
spent 30 minutes with the Italian after the Blackburn match and that
resignation was never an issue.
5.3.2 Separating Entity Coherence from CB Uniqueness. Starting with Brennan, Fried-
man, and Pollard (1987) and, more recently, Beaver (2004) and Kibble (2001) there have
been attempts to ?unpack? some of the original preferences proposed by centering.
We feel this work has greatly helped our understanding of the theory and believe that
it would be similarly useful to unpack Constraint 1 into two separate claims, as well:
one about uniqueness of the CB, and one about entity coherence.
The first function of (both versions of) Constraint 1 is to claim that the CB is
unique. We will call this claim CB uniqueness:
CB Uniqueness: Utterances have at most one CB.
We have argued throughout the article that Strong Constraint 1 has a second function
as well: to express a preference for utterances that do not occur at the beginning of a
357
Poesio et al Centering: A Parametric Theory
segment to mention at least one of the objects included in the previous utterance. Fol-
lowing Kibble (2000) and Karamanis (2001), we will call this second half of Constraint
1 (entity) continuity:
(Entity) Continuity: CF(Ui?1) ? CF(Ui) = ?
Weak C1 is CB uniqueness, whereas Strong C1 is CB uniqueness plus continuity.
5.3.3 A Hybrid View of Coherence. One clear conclusion suggested by our results is
that entity-based accounts of coherence need to be supplemented by accounts of other
factors that induce coherence at the local level. The most direct way to do this would
be to include in continuity a longer list of factors that may link an utterance to its
previous one and claim that in order for an utterance to be ?locally coherent,? at least
one of these factors must be present. The resulting claim would take a form along the
following lines:
Hybrid Continuity For every utterance Ui, at least one of the following must
hold:
1. CF(Ui?1) ? CF(Ui) = ?;
2. There is a rhetorical relation RR such that RR(Ui?1,Ui).52
3. Ui?1 and Ui are temporally coherent in the sense, for
example, of Kameyama, Passonneau, and Poesio (1993).
4. . . . (other factors)
A more sensible approach, especially as we don?t yet know all the factors affecting
coherence, would be to be more explicit about the scope of centering theory, viewing
it not as a comprehensive account of ?local coherence,? but only as an account of the
contribution of entity coherence to local coherence. In other words, we could view
(Entity) Continuity as only one among the preferences holding at the discourse level.
A natural way to formalize this would be to include Entity Continuity among a set of
constraints like those proposed by Beaver, which would also have to include further
constraints specifying preferences for rhetorical and temporal coherence.
5.3.4 CB Uniqueness. We saw in Section 4 that it?s fairly easy to fix the problem of
utterances? violating Weak C1, or CB uniqueness: All that is needed is to strenghten
the requirements on the ranking function and require it to be total, which in turn can
be easily done by adding a disambiguation factor to ranking functions that aren?t total
like grammatical function. Before doing this, however, we should ask whether this is
the conclusion we should draw from the finding that CB uniqueness will be violated
with partial ranking functions?or if instead we should allow for utterances to have
more than one CB.
When multi-CB utterances such as (10) are considered, it is not immediately ob-
vious that one discourse entity (the corner cupboard) is more salient than the other
(Branicki), especially since neither occupies a particularly salient position either in the
previous utterance (u227) or in the current one (u229). Notice also that both entities
have been mentioned before; and furthermore, one of them is animate (Branicki), the
52 This formulation was intentionally designed in such a way as to finesse the issue of whether RR
should be an informational-level relation between the eventualities expressed by the utterances or a
genuine rhetorical relation between the speech acts performed by them.
358
Computational Linguistics Volume 30, Number 3
other inanimate (the cupboard). In these respects, these examples are reminiscent of
the examples that led Sidner (1979) to argue for two foci?sentences with one an-
imate entity (typically in agent position) and an inanimate one (typically in theme
position), like Mortimer sold the book for 10 cents or Mary took a nickel from her toy bank
yesterday. Although the results from studies such as Gordon, Grosz, and Gillion (1993)
suggest that when two animate entities are considered, only one tends to show RNP
effects, we are not aware of any experiment testing materials like those discussed by
Sidner.
The hypothesis that topicality is not restricted to one entity per utterance has been
advanced by a number of researchers, although it is perhaps most clearly associated
with the work of Givon (1983). Within the centering literature, abandoning the claim
that we call ?CB uniqueness? has been suggested by Gundel (1998) and, more radically,
in work such as Strube (1998), Gordon and Hendrick (1999), and Tetreault (2001) in
which the whole notion of CB is abandoned.
As seen in Section 2, the primary motivation for CB uniqueness is complexity-
theoretic arguments: Inference in monadic logics is less expensive than in normal
logics (Joshi and Kuhn 1979; Joshi and Weinstein 1981). Grosz and colleagues?s lin-
guistic evidence for CB uniqueness is contrasts like those in (3), showing that failing to
pronominalize certain entities (Susan, in that example) is a more serious problem than
failing to pronominalize others (Betsy). This claim is further supported by the evidence
concerning the Repeated Name Penalty (Gordon, Grosz, and Gillion 1993). However,
the RNP is observed only in a subset of the cases that would be considered as CB
mentions according to the definition provided by Constraint 3, and in the example we
are discussing (10), neither Branicki nor the cupboard occur in (u229) in a position that
would be subject to RNP effects according to Gordon et al In other words, (some)
evidence used by Grosz et al in support of CB uniqueness cannot be used to argue
that (u229) in (10) has a single CB. This evidence is also consistent with a different
solution to the problem raised by examples like (10): Instead of attempting to preserve
CB uniqueness by requiring the ranking function to be total, one could abandon CB
uniqueness, as suggested in Givon (1983) and Gundel (1998). In both cases, we would
need a separate theoretical account of RNP effects. More empirical evidence is needed
on this issue.53
5.3.5 Variety. The third conclusion suggested by our results is that ensuring variety
seems to be as important a principle in discourse production as maintaining coher-
ence. This is suggested, first of all, by the fact that only slightly over half of CBs are
realized as R1-pronouns. It is also the case that CBs are hardly ever continued for
more than two or three utterances; that the same discourse entity is very unlikely
to be realized using the same type of NP twice in a row (even with pronouns, we
only have 58 pronoun-pronoun sequences?26% of the total); and that two-thirds of
all transition sequences involve two different transitions. In fact, we wonder whether
that the repeated name penalty observed by Gordon et al might not be an instance of
this more general phenomenon.
53 One way to reconcile the different findings would be to use different conceptual tools to characterize
the connection between subsequent utterances. Each utterance satisfying Continuity would have one or
more links to the previous utterance, that we might call centers of coherence; Entity Continuity would
then become a preference for the set of centers of coherence to be nonempty. In particular situations,
may be experimentally identified using the RNP as a test, one of the centers of coherence may acquire
a particular status, leading to a preference for pronominalization. We may call this center the center of
salience, say. It would also be interesting to examine the connection between a solution along these
lines and Sidner?s solution involving two foci.
359
Poesio et al Centering: A Parametric Theory
Acknowledgments
Special thanks to Nikiforos Karamanis,
Alistair Knott, Mark Liberman, Ruslan
Mitkov, Jon Oberlander, Tim Rakow, and
the other members of the GNOME project:
Kees van Deemter, Renate Henschel, Rodger
Kibble, Jamie Pearson, and Donia Scott. We
also wish to thank James Allen, Jennifer
Arnold, Steve Bird, Susan Brennan, Donna
Byron, Herb Clark, George Ferguson,
Jeanette Gundel, Aravind Joshi, Eleni
Miltsakaki, Rashmi Prasad, Ellen Prince,
Len Schubert, Harold Somers, Joel Tetreault,
Lyn Walker, and audiences at the ACL 2000,
the University of Pennsylvania, the
University of Rochester, CLUK, and the
University of Wolverhampton for comments
and suggestions. The corpus presented here
was annotated by Debbie De Jongh, Ben
Donaldson, Marisa Flecha-Garcia, Camilla
Fraser, Michael Green, Shane Montague,
Carol Rennie, and Claire Thomson, together
with the authors. A substantial part of this
work, including the creation of the corpus,
was supported by the EPSRC project
GNOME, GR/L51126/01. Massimo Poesio
was supported during parts of this project
by an EPSRC advanced fellowship. Barbara
Di Eugenio was supported in part by NSF
grant INT 9996195, in part by NATO grant
CRG 9731157. Janet Hitzeman was in part
supported by the EPSRC project SOLE,
GR/L50341.
References
Alshawi, Hiyan. 1987. Memory and Context
for Language Interpretation. Cambridge
University Press, Cambridge.
Arnold, Jennifer E. 1998. Reference Form and
Discourse Patterns. Ph.D. thesis, Stanford
University, Stanford CA.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project, Montreal, Canada. In
Proceedings of the 36th ACL, pages 86?90.
Beaver, David. 2004. The optimization of
discourse anaphora. Linguistics and
Philosophy, 27(1):3?56.
Brennan, Susan E. 1995. Centering attention
in discourse. Language and Cognitive
Processes, 10:137?167.
Brennan, Susan E., Marilyn W. Friedman,
and Charles J. Pollard. 1987. A centering
approach to pronouns. In Proceedings of the
25th ACL, Stanford, CA, pages 155?162, June.
Byron, Donna and Amanda Stent. 1998. A
preliminary model of centering in dialog.
In Proceedings of the 36th ACL, Montreal,
Canada.
Caramazza, Alfonso, Ellen Grober,
Catherine Garvey, and Jack Yates. 1977.
Comprehension of anaphoric pronouns.
Journal of Verbal Learning and Verbal
Behavior, 16:601?609.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Chafe, Wallace. 1976. Givenness,
contrastiveness, definiteness, subjects, and
topics. In C. Li, editor, Subject and Topic.
Academic Press, New York, pages 25?76.
Chinchor, Nancy A. and Beth Sundheim.
1995. Message understanding conference
(MUC) tests of discourse processing. In
Proceedings of the AAAI Spring Symposium
on Empirical Methods in Discourse
Interpretation and Generation, pages 21?26,
Stanford, CA.
Clark, Herbert H. 1977. Bridging. In P. N.
Johnson-Laird and P. C. Wason, editors,
Thinking: Readings in Cognitive Science,
pages 411?420, Cambridge University
Press, London and New York.
Cooreman, Ann and Tony Sanford. 1996.
Focus and syntactic subordination in
discourse. Research Paper no. RP-79,
University of Edinburgh, HCRC.
Cote, Sharon. 1998. Ranking
forward-looking centers. In M. A. Walker,
A. K. Joshi, and E. F. Prince, editors,
Centering Theory in Discourse. Oxford,
University Press, Oxford, chapter 4,
pages 55?70.
Dale, Robert. 1992. Generating Referring
Expressions. MIT Press, Cambridge, MA.
Di Eugenio, Barbara. 1998. Centering in
Italian. In M. A. Walker, A. K. Joshi, and
E. F. Prince, editors, Centering Theory in
Discourse. Oxford University Press,
Oxford, chapter 7, pages 115?138.
Di Eugenio, Barbara, Johanna D. Moore,
and Massimo Paolucci. 1997. Learning
features that predict cue usage. In
Proceedings of the 35th ACL, Madrid.
Fox, Barbara A. 1987. Discourse Structure and
Anaphora. Cambridge University Press,
Cambridge.
Gernsbacher, Morton A. and
David Hargreaves. 1988. Accessing
sentence participants: The advantage of
first mention. Journal of Memory and
Language, 27:699?717.
Giouli, Paraskevi. 1996. Topic chaining and
discourse structure in task-oriented
dialogues. Master?s thesis, Linguistics
Department, University of Edinburgh.
Givon, Talmy, editor. 1983. Topic Continuity
in Discourse: A Quantitative Cross-Language
Study. Benjamins, Amsterdam and
Philadelphia.
360
Computational Linguistics Volume 30, Number 3
Gordon, Peter C., Barbara J. Grosz, and
Laura A. Gillion. 1993. Pronouns, names,
and the centering of attention in
discourse. Cognitive Science, 17:311?348.
Gordon, Peter C. and Randall Hendrick.
1999. The representation and processing
of coreference in discourse. Cognitive
Science, 22:389?424.
Gordon, Peter C., Randall Hendrick,
Kerry Ledoux, and Chin L. Yang. 1999.
Processing of reference and the structure
of language: An analysis of complex noun
phrases. Language and Cognitive Processes,
14(4):353?379.
Grosz, Barbara J. 1977. The Representation and
Use of Focus in Dialogue Understanding.
Ph.D. thesis, Stanford University,
Stanford, CA.
Grosz, Barbara J., Aravind K. Joshi, and
Scott Weinstein. 1983. Providing a unified
account of definite noun phrases in
discourse. In Proceedings of ACL-83,
Cambridge, MA, pages 44?50.
Grosz, Barbara J., Aravind K. Joshi, and
Scott Weinstein. 1986. Towards a
computational theory of discourse
interpretation. Unpublished manuscript.
Grosz, Barbara J., Aravind K. Joshi, and
Scott Weinstein. 1995. Centering: A
framework for modeling the local
coherence of discourse. Computational
Linguistics, 21(2):202?225.
Grosz, Barbara J. and Candace L. Sidner.
1986. Attention, intention, and the
structure of discourse. Computational
Linguistics, 12(3):175?204.
Gundel, Jeanette K. 1998. Centering theory
and the givenness hierarchy: Towards a
synthesis. In M. A. Walker, A. K. Joshi,
and E. F. Prince, editors, Centering Theory
in Discourse. Oxford University Press,
Oxford, chapter 10, pages 183?198.
Gundel, Jeanette K., Nancy Hedberg, and
Ron Zacharski. 1993. Cognitive status and
the form of referring expressions in
discourse. Language, 69(2):274?307.
Hawkins, John A. 1978. Definiteness and
Indefiniteness. Croom Helm, London.
Heim, Irene. 1982. The Semantics of Definite
and Indefinite Noun Phrases. Ph.D. thesis,
University of Massachusetts at Amherst.
Henschel, Renate, Hua Cheng, and
Massimo Poesio. 2000. Pronominalization
revisited. In Proceedings of 18th COLING,
Saarbruecken, August.
Hitzeman, Janet, Alan Black, Paul Taylor,
Chris Mellish, and Jon Oberlander. 1998.
On the use of automatically generated
discourse-level information in a
concept-to-speech synthesis system. In
Proceedings of the International Conference on
Spoken Language Processing (ICSLP98),
Sydney, Australia.
Hitzeman, Janet and Massimo Poesio. 1998.
Long-distance pronominalisation and
global focus. In Proceedings of
ACL/COLING, vol. 1, pages 550?556,
Montreal.
Hobbs, Jerry R. 1979. Coherence and
coreference. Cognitive Science, 3:67?90.
Hudson, Susan B., Michael K. Tanenhaus,
and Gary S. Dell. 1986. The effect of the
discourse center on the local coherence of
a discourse. In Proceedings of the Eight
Annual Meeting of the Cognitive Science
Society, Amherst, MA, pages 96?101.
Hudson-D?Zmura, Susan and Michael K.
Tanenhaus. 1998. Assigning antecedents
to ambiguous pronouns: The role of the
center of attention as the default
assignment. In M. A. Walker, A. K. Joshi,
and E. F. Prince, editors, Centering Theory
in Discourse. Oxford University Press,
Oxford, pages 199?226.
Hurewitz, Felicia. 1998. A quantitative look
at discourse coherence. In M. A. Walker,
A. K. Joshi, and E. F. Prince, editors,
Centering Theory in Discourse. Oxford
University Press, Oxford, pages 273?291.
Joshi, Aravind K. and Steve Kuhn. 1979.
Centered logic: The role of entity centered
sentence representation in natural
language inferencing. In Proceedings of the
IJCAI, Tokyo, pages 435?439.
Joshi, Aravind K. and Scott Weinstein. 1981.
Control of inference: Role of some aspects
of discourse structure?centering. In
Proceedings of the IJCAI, Vancouver, CA,
pages 385?387.
Kameyama, Megumi. 1985. Zero Anaphora:
The Case of Japanese. Ph.D. thesis, Stanford
University, Stanford, CA.
Kameyama, Megumi. 1986. A
property-sharing constraint in centering.
In Proceedings of the ACL-86, New York,
pages 200?206.
Kameyama, Megumi. 1998. Intra-sentential
centering: A case study. In M. A. Walker,
A. K. Joshi, and E. F. Prince, editors,
Centering Theory in Discourse. Oxford
University Press, Oxford, chapter 6,
pages 89?112.
Kameyama, Megumi, Rebecca Passonneau,
and Massimo Poesio. 1993. Temporal
centering. In Proceedings of the 31st ACL,
pages 70?77, Columbus, OH.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic. Reidel, Dordrecht.
Karamanis, Nikiforos. 2001. Exploring
entity-based coherence. In Proceedings of
the Fourth CLUK. University of Sheffield,
pages 18?26.
361
Poesio et al Centering: A Parametric Theory
Karamanis, Nikiforos. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D.
thesis, Informatics, University of
Edinburgh.
Karttunen, Lauri. 1976. Discourse referents.
In J. McCawley, editor, Syntax and
Semantics 7?Notes from the Linguistic
Underground. Academic Press,
New York, pages 363?385.
Kibble, Rodger. 2000. A reformulation of
rule 2 of centering theory. Technical
report, ITRI, University of Brighton.
Kibble, Rodger. 2001. A reformulation of
Rule 2 of centering theory. Computational
Linguistics, 27(4):579?587.
Kibble, Rodger and Richard Power. 2000. An
integrated framework for text planning
and pronominalization. In Proceedings of
INLG, Mitzpe Ramon, Israel, June.
Kintsch, Walter and Teun van Dijk. 1978.
Towards a model of discourse
comprehension and production.
Psychological Review, 85:363?394.
Knott, Alistair, Jon Oberlander,
Mick O?Donnell, and Chris Mellish. 2001.
Beyond elaboration: The interaction of
relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and
W. Spooren, editors, Text Representation:
Linguistic and Psycholinguistic Aspects. John
Benjamins, Amsterdam and Philadelphia,
pages 181-196.
Lappin, Shalom and Herbert J. Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics,
20(4):535?562.
Lascarides, Alex and Nick Asher. 1993.
Temporal interpretation, discourse
relations and commonsense entailment.
Linguistics and Philosophy, 16(5):437?493.
Mann, William C. and Sandra A.
Thompson. 1988. Rhetorical structure
theory: Towards a functional theory of
text organization. Text, 8(3):243?281.
Marcu, Daniel. 1999. Instructions for
manually annotating the discourse
structures of texts. Unpublished
manuscript, USC/ISI.
McKeown, Kathy R. 1985. Discourse
strategies for generating natural-language
text. Artificial Intelligence, 27(1):1?41.
Miltsakaki, Eleni. 1999. Locating topics in
text processing. In Proceedings of CLIN,
Utrecht, pages 127?138.
Miltsakaki, Eleni. 2002. Towards an
aposynthesis of topic continuity and
intrasentential anaphora. Computational
Linguistics, 28(3):319?355.
Moser, Megan and Johanna D. Moore. 1996.
Toward a synthesis of two accounts of
discourse structure. Computational
Linguistics, 22(3):409?419.
Oberlander, Jon, Mick O?Donnell,
Alistair Knott, and Chris Mellish. 1998.
Conversation in the museum:
Experiments in dynamic hypermedia with
the intelligent labelling explorer. New
Review of Hypermedia and Multimedia,
4:11?32.
Oberlander, Jon and Massimo Poesio. 2002.
Entity coherence and relational coherence:
a corpus-based investigation. Paper
presented at the Berlin workshop on
Topics in Discourse, September.
Passonneau, Rebecca J. 1993. Getting and
keeping the center of attention. In
M. Bates and R. M. Weischedel, editors,
Challenges in Natural Language Processing.
Cambridge University Press, Cambridge,
chapter 7, pages 179?227.
Passonneau, Rebecca J. 1997. Instructions
for applying discourse reference
annotation for multiple applications
(DRAMA). Unpublished manuscript.
Passonneau, Rebecca J. 1998. Interaction of
discourse structure with explicitness of
discourse anaphoric noun phrases. In
M. A. Walker, A. K. Joshi, and E. F.
Prince, editors, Centering Theory in
Discourse. Oxford University Press,
Oxford, chapter 17, pages 327?358.
Passonneau, Rebecca J. and Diane Litman.
1993. Feasibility of automated discourse
segmentation. In Proceedings of 31st Annual
Meeting of the ACL, Columbus, OH, pages
148?155.
Pearson, Jamie, Rosemary Stevenson, and
Massimo Poesio. 2000. Pronoun resolution
in complex sentences. In Proceedings of
AMLAP, Leiden.
Pearson, Jamie, Rosemary Stevenson, and
Massimo Poesio. 2001. The effects of
animacy, thematic role, and surface
position on the focusing of entities in
discourse. In M. Poesio, editor, Proceedings
of the First SEMPRO, University of
Edinburgh.
Poesio, Massimo. 2000. Annotating a corpus
to develop and evaluate discourse entity
realization algorithms: Issues and
preliminary results. In Proceedings of the
2nd LREC, pages 211?218, Athens, May.
Poesio, Massimo. 2003. Associative
descriptions and salience. In Proceedings of
the EACL Workshop on Computational
Treatments of Anaphora, Budapest.
Poesio, Massimo. 2004. The
MATE/GNOME scheme for anaphoric
annotation, revisited. In Proceedings of
SIGDIAL, Boston, May.
362
Computational Linguistics Volume 30, Number 3
Poesio, Massimo, Florence Bruneseaux, and
Laurent Romary. 1999. The MATE
meta-scheme for coreference in dialogues
in multiple languages. In M. Walker,
editor, Proceedings of the ACL Workshop on
Standards and Tools for Discourse Tagging,
pages 65?74.
Poesio, Massimo and Barbara Di Eugenio.
2001. Discourse structure and anaphoric
accessibility. In Ivana Kruijff-Korbayova?
and Mark Steedman, editors, Proceedings
of the ESSLLI 2001 Workshop on Information
Structure, Discourse Structure and Discourse
Semantics, Helsinki.
Poesio, Massimo and
Natalia Nygren-Modjeska. 2003. The
THIS-NP hypothesis: A corpus-based
investigation. In Proceedings of DAARC,
Lisbon.
Poesio, Massimo and Rosemary Stevenson.
Forthcoming. Salience: Theoretical Models and
Empirical Evidence. Cambridge University
Press, Cambridge and New York.
Poesio, Massimo and Renata Vieira. 1998. A
corpus-based investigation of definite
description use. Computational Linguistics,
24(2):183?216.
Prasad, Rashmi and Michael Strube. 2000.
Discourse salience and pronoun
resolution in Hindi. In Penn Working
Papers in Linguistics, volume 6, University
of Pennsylvania, Philadelphia,
pages 189?208.
Prince, Ellen F. 1981. Toward a taxonomy of
given-new information. In P. Cole, editor,
Radical Pragmatics. Academic Press, New
York, pages 223?256.
Prince, Ellen F. 1992. The ZPG letter:
Subjects, definiteness, and information
status. In S. Thompson and W. Mann,
editors, Discourse Description: Diverse
Analyses of a Fund-Raising Text. Benjamins,
Amsterdam and Philadelphia,
pages 295?325.
Quirk, Randolph and Sidney Greenbaum.
1973. A University Grammar of English.
Longman, Harlow, England.
Rambow, Owen. 1993. Pragmatics aspects
of scrambling and topicalization in
German. In Proceedings of the Workshop
on Centering Theory in Naturally-
Occurring Discourse. Institute for
Research in Cognitive Science,
Philadelphia.
Sanford, Anthony J. and Simon C. Garrod.
1981. Understanding Written Language.
Wiley, Chichester, England.
Scott, Donia, Richard Power, and
Roger Evans. 1998. Generation as a
solution to its own problem. In
Proceedings of the ninth International
Workshop on Natural Language Generation,
Niagara-on-the-Lake, CA.
Sgall, Petr. 1967. Functional sentence
perspective in a generative description.
Prague Studies in Mathematical Linguistics,
2:203?225.
Sidner, Candace L. 1979. Towards a
Computational Theory of Definite Anaphora
Comprehension in English Discourse. Ph.D.
thesis, Massachusetts Institute of
Technology, Cambridge.
Siegel, Sidney and N. John Castellan. 1988.
Nonparametric Statistics for the Behavioral
Sciences, 2nd edition McGraw-Hill, Boston.
Stevenson, Rosemary J., Rosalind A.
Crawley, and David Kleinman. 1994.
Thematic roles, focus, and the
representation of events. Language and
Cognitive Processes, 9:519?548.
Stevenson, Rosemary, Alistair Knott,
Jon Oberlander, and Scott McDonald.
2000. Interpreting pronouns and
connectives: Interactions between
focusing, thematic roles and coherence
relations. Language and Cognitive Processes,
15, pages 225?262.
Strube, Michael. 1998. Never look back: An
alternative to centering. In Proceedings of
COLING-ACL, pages 1251?1257, Montreal.
Strube, Michael and Udo Hahn. 1999.
Functional centering?Grounding
referential coherence in information
structure. Computational Linguistics,
25(3):309?344.
Suri, Linda Z. and Kathleen F. McCoy. 1994.
RAFT/RAPR and centering: A
comparison and discussion of problems
related to processing complex sentences.
Computational Linguistics, 20(2):301?317.
Tetreault, Joel R. 2001. A corpus-based
evaluation of centering and pronoun
resolution. Computational Linguistics,
27(4):507?520.
Turan, Umit. 1998. Ranking forward-looking
centers in Turkish: Universal and
language-specific properties. In M. A.
Walker, A. K. Joshi, and E. F. Prince,
editors, Centering Theory in Discourse.
Oxford University Press, Oxford,
chapter 8, pages 139?160.
Vallduvi, Enric. 1990. The Informational
Component. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
van Deemter, Kees and Rodger Kibble. 2000.
On coreferring: Coreference in MUC and
related annotation schemes. Computational
Linguistics, 26(4):629?637.
Walker, Marilyn A. 1989. Evaluating
discourse processing algorithms. In
Proceedings ACL-89, pages 251?261,
Vancouver, British Columbia, Canad, June.
363
Poesio et al Centering: A Parametric Theory
Walker, Marilyn A. 1993. Initial contexts and
shifting centers. In Proceedings of the
Workshop on Centering, University of
Pennsylvania, Philadelphia.
Walker, Marilyn A., Masayo Iida, and
Sharon Cote. 1994. Japanese discourse
and the process of centering.
Computational Linguistics, 20(2):193?232.
Walker, Marilyn A., Aravind K. Joshi, and
Ellen F. Prince. 1998a. Centering in
naturally occurring discourse: An
overview. In M. A. Walker, A. K. Joshi,
and E. F. Prince, editors, Centering Theory
in Discourse. Oxford University Press,
Oxford, chapter 1, pages 1?28.
Walker, Marilyn A., Aravind K. Joshi, and
Ellen F. Prince, editors. 1998b. Centering
Theory in Discourse. Oxford University
Press, Oxford.
Webber, Bonnie L. 1978. A formal approach
to discourse anaphora. Report no. 3761,
BBN, Cambridge, MA.
Latent Semantic Analysis for dialogue act classification
Riccardo Serafin
Computer Science
University of Illinois
Chicago, IL, USA
rseraf1@uic.edu
Barbara Di Eugenio
Computer Science
University of Illinois
Chicago, IL, USA
bdieugen@uic.edu
Michael Glass
Mathematics and Computer Science
Valparaiso University
Valparaiso, IN, USA
Michael.Glass@valpo.edu
Abstract
This paper presents our experiments in apply-
ing Latent Semantic Analysis (LSA) to dia-
logue act classification. We employ both LSA
proper and LSA augmented in two ways. We
report results on DIAG, our own corpus of tu-
toring dialogues, and on the CallHome Spanish
corpus. Our work has the theoretical goal of as-
sessing whether LSA, an approach based only
on raw text, can be improved by using addi-
tional features of the text.
1 Introduction
Dialogue systems need to perform dialog act classifica-
tion, in order to understand the role the user?s utterance
plays in the dialog (e.g., a question for information or a
request to perform an action), and to generate an appro-
priate next turn. In recent years, a variety of empirical
techniques have been used to train the dialogue act clas-
sifier (Reithinger and Maier, 1995; Stolcke et al, 2000;
Walker et al, 2001).
In this paper, we propose Latent Semantic Analysis
(LSA) as a method to train the dialogue act classifier.
LSA can be thought as representing the meaning of a
word as a kind of average of the meanings of all the pas-
sages in which it appears, and the meaning of a passage
as a kind of average of the meaning of all the words it
contains (Landauer et al, 1998). LSA learns from co-
occurrence of words in collections of texts. It builds a se-
mantic space where words and passages are represented
as vectors. Their similarity is measured by the cosine of
their contained angle in the semantic space. LSA is based
on Single Value Decomposition (SVD), a mathematical
technique that causes the semantic space to be arranged
so as to reflect the major associative patterns in the data,
and ignores the smaller, less important influences.
LSA has been successfully applied to many tasks: e.g,
to assess the quality of student essays (Foltz et al, 1999)
and to interpret the student?s input in an Intelligent Tutor-
ing system (Graesser et al, 2000). However, there is no
research on applying LSA to dialogue act classification.
LSA is an attractive method because it is relatively
straightforward to train and use. More importantly, al-
though it is a statistical theory, it has been shown to mimic
a number of aspects of human competence / performance
(Landauer et al, 1998). Thus, it appears to somehow cap-
ture and represent important components of meanings.
We also have a theoretical goal in investigating LSA.
A common criticism of LSA is that its ?bag of words? ap-
proach ignores any other linguistic information that may
be available, e.g. order and syntactic information: to
LSA, man bites dog is identical to dog bites man. We
suggest that an LSA semantic space can be built from the
co-occurrence of arbitrary textual features. We propose
to place in the bag of words other features that co-occur
in the same text. We are calling LSA augmented with
features ?FLSA? (for ?feature LSA?). The only relevant
prior work is (Wiemer-Hastings, 2001), that adds part of
speech tags and some syntactic information to LSA.
This paper describes the corpora and the methods we
used, and the results we obtained. To summarize, plain
LSA seems to perform well on large corpora and classi-
fication tasks. Augmented LSA seems to perform better
on smaller corpora and target classifications.
2 Corpora
We report experiments on two corpora, DIAG and Span-
ish CallHome.
DIAG is a corpus of computer mediated tutoring dia-
logues between a tutor and a student who is diagnosing a
fault in a mechanical system with the DIAG tutoring sys-
tem (Towne, 1997). The student?s input is via menu, the
tutor is in a different room and answers via a text window.
The DIAG corpus comprises 23 dialogues for a total of
607 different words and 660 dialogue acts. It has been an-
notated for a variety of features, including four dialogue
acts1 (Glass et al, 2002): problem solving, the tutor gives
problem solving directions; judgement, the tutor evalu-
ates the student?s actions or diagnosis; domain knowl-
edge, the tutor imparts domain knowledge; and other,
when none of the previous three applies.
The Spanish CallHome corpus (Levin et al, 1998;
Ries, 1999) comprises 128 unrestricted phone calls in
Spanish, for a total of 12066 different words and 44628
dialogue acts. The Spanish CallHome annotation aug-
ments a basic tag such as statement along several dimen-
sions, such as whether the statement describes a psycho-
logical state of the speaker. This results in 232 differ-
ent dialogue act tags, many with very low frequencies.
In this sort of situations, tag categories are often col-
lapsed when running experiments so as to get meaningful
frequencies (Stolcke et al, 2000). In CallHome37, we
collapsed statements and backchannels, obtaining 37 dif-
ferent tags. CallHome37 maintains some subcategoriza-
tions, e.g. whether a question is yes/no or rhetorical. In
CallHome10, we further collapse these categories. Call-
Home10 is reduced to 8 dialogue acts proper (eg state-
ment, question, answer) plus the two tags ??%?? for
abandoned sentences and ??x?? for noise.
3 Methods
We have experimented with four methods: LSA proper,
which we call plain LSA; two versions of clustered LSA,
in which we ?cluster? the document dimension in the
Word-Document matrix; FLSA, in which we incorporate
features other than words to train LSA (specifically, we
used the preceding n dialogue acts).
Plain LSA. The input to LSA is a Word-Document ma-
trix with a row for each word, and a column for each
document (for us, a document is a unit such as a sen-
tence or paragraph tagged with a dialogue act). Cell
c(i; j) contains the frequency with which word
i
appears
in document
j
. Clearly, this w*d matrix will be very
sparse. Next, LSA applies SVD to the Word-Document
matrix, obtaining a representation of each document in a
k dimensional space: crucially, k is much smaller than the
dimension of the original space. As a result, words that
did not appear in certain documents now appear, as an
estimate of their correlation to the meaning of those doc-
uments. The number of dimensions k retained by LSA
is an empirical question. The results we report below are
for the best k we experimented with.
To choose the best tag for a document in the test set, we
compare the vector representing the new document with
the vector of each document in the training set. The tag of
1They should be more appropriately termed tutor moves.
the document which has the highest cosine with our test
vector is assigned to the new document.
Clustered LSA. Instead of building the Word-
Document matrix we build a Word-Tag matrix, where the
columns refer to all the possible dialog act types (tags).
The cell c(i; j) will tell us how many times word
i
is
used in documents that have tag
j
. The Word-Tag matrix
is w*t instead of w*d. We then apply Plain LSA to the
Word-Tag matrix.
SemiClustered LSA. In Clustered LSA we lose the
distribution of words in the documents. Moreover, if the
number of tags is small, such as for DIAG, SVD loses its
meaning. SemiClustered LSA tries to remedy these prob-
lems. We still produce the k-dimensional space apply-
ing SVD to the Word-Document matrix. We then reduce
the Word-Tag matrix to the k dimensional space using a
transformation based on the SVD of the Word-Document
matrix. Note that both Clustered and SemiClustered LSA
are much faster at test time than plain LSA, as the test
document needs to be compared only with t tag vectors,
rather than with d document vectors (t << d).
Feature LSA (FLSA). We add extra features to plain
LSA. Specifically, we have experimented with the se-
quence of the previous n dialogue acts. We compute
the input WordTag-Document matrix by computing the
Word-Document matrix, computing the Tag-Document
matrix and then concatenating them vertically to get the
(w+t)*d final matrix. Otherwise, the method is the same
as Plain LSA.
4 Results
Table 1 reports the best results we obtained for each cor-
pus and method. In parentheses, we include the k di-
mension, and, for FLSA, the number of previous tags we
considered.
In all cases, we can see that Plain LSA performs much
better than baseline, where baseline is computed as pick-
ing the most frequent dialogue act in each corpus. As
concerns DIAG, we can also see that SemiClustered LSA
improves on Plain LSA by 3%, but no other method does.
As regards CallHome, first, the results with plain LSA
are comparable to published ones, even if the comparison
is not straightforward, because it is often unclear what
the target classification and features used are. For exam-
ple, (Ries, 1999) reports 76.2% accuracy by using neural
networks augmented with the sequence of the n previous
speech acts. However, (Ries, 1999) does not mention the
target classification; the reported baseline appears com-
patible with both CallHome37 and CallHome10. The
training features in (Ries, 1999) include part-of-speech
(POS) tags for words, which we do not have. This may
Corpus Plain Clustered SemiClustered FLSA
Diag (43.64%) 75.73% (50) 71.91% (3) 78.78% (50) 74.26% (1,150)
CallHome37 (42.69%) 65.36% (50) 22.08% (10) 31.39% (300) 62.59% (1, 50)
CallHome10 (42.69%) 68.91% (25) 61.64% (5) 58.38% (300) 66.57% (1, 100)
Table 1: Result Summary
explain the higher performance. Including POS tags into
our FLSA method is left for future work.
No variation on LSA performs better than plain LSA in
our CallHome experiments. In fact, clustered and semi-
clustered LSA perform vastly worse on the larger clas-
sification problem in CallHome37. It appears that, the
smaller the corpus and target classification are, the better
clustered and semiclustered LSA perform. In fact, semi-
clustered LSA outperforms plain LSA on DIAG.
Our experiments with FLSA do not support the hy-
pothesis that adding features different from words to LSA
helps with classification. (Wiemer-Hastings, 2001) re-
ports mixed results when augmenting LSA: adding POS
tags did not improve performance, but adding some syn-
tactic information did. Note that, in our experiments,
adding more than one previous speech act did not help.
5 Future work
Our experiments show that LSA can be effectively used
to train a dialogue act classifier. On the whole, plain LSA
appears to perform well. Even if our experiments with
extensions to plain LSA were mostly unsuccessful, they
are not sufficient to conclude that plain LSA cannot be
improved. Thus, we will pursue the following directions.
1) Further investigate the correlation of the performance
of (semi)clustered LSA with the size of the corpus and /
or of the target classification. 2) Include other features in
FLSA, e.g. syntactic roles. 3) Redo our experiments on
other corpora, such as Map Task (Carletta et al, 1997).
Map Task is appropriate because besides dialogue acts it
is annotated for syntactic information, while CallHome
is not. 4) Experiment with FLSA on other tasks, such as
assessing text coherence.
Acknowledgements
This work is supported by grant N00014-00-1-0640 from the
Office of Naval Research.
References
J. Carletta, A. Isard, S. Isard, J. C. Kowtko, G. Doherty-
Sneddon, and A. H. Anderson. 1997. The reliability
of a dialogue structure coding scheme. Computational
Linguistics, 23(1):13?31.
P. W. Foltz, D. Laham, and T. K. Landauer. 1999. The
intelligent essay assessor: Applications to educational
technology. Interactive Multimedia Electronic Journal
of Computer-Enhanced Learning, 1(2).
M. Glass, H. Raval, B. Di Eugenio, and M. Traat. 2002.
The DIAG-NLP dialogues: coding manual. Technical
Report UIC-CS 02-03, University of Illinois - Chicago.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, R. Kreuz, and the Tutoring Research Group.
2000. Autotutor: A simulation of a human tutor. Jour-
nal of Cognitive Systems Research.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. Intro-
duction to Latent Semantic Analysis. Discourse Pro-
cesses, 25:259?284.
L. Levin, A. Thyme?-Gobbel, A. Lavie, K. Ries, and K.
Zechner. 1998. A discourse coding scheme for con-
versational Spanish. In Proceedings ICSLP.
N. Reithinger and E. Maier. 1995. Utilizing statistical
dialogue act processing in Verbmobil. In ACL95, Pro-
ceedings of the 33rd Annual Meeting of the Association
for Computational Linguistics.
K. Ries. 1999. HMM and Neural Network Based Speech
Act Detection. In Proceedings of ICASSP 99.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema,
and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
D. M. Towne. 1997. Approximate reasoning techniques
for intelligent diagnostic instruction. International
Journal of Artificial Intelligence in Education,8.
M. A. Walker, R. Passonneau, and J. E. Boland. 2001.
Qualitative and quantitative evaluation of DARPA
communicator dialogue systems. In ACL01, Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics.
P. Wiemer-Hastings. 2001. Rules for syntax, vectors for
semantics. In CogSci01, Proceedings of the Twenty-
Third Annual Meeting of the Cognitive Science Society.
Building lexical semantic representations for Natural Language instructions
Elena Terenzi
Computer Science
Politecnico di Milano
Milano, Italy
elenat@libero.it
Barbara Di Eugenio
Computer Science
University of Illinois
Chicago, IL, USA
bdieugen@cs.uic.edu
Abstract
We report on our work to automatically build a
corpus of instructional text annotated with lex-
ical semantics information. We have coupled
the parser LCFLEX with a lexicon and ontol-
ogy derived from two lexical resources, Verb-
Net for verbs and CoreLex for nouns. We dis-
cuss how we built our lexicon and ontology,
and the parsing results we obtained.
1 Introduction
This paper discusses the lexicon and ontology we built
and coupled with the parser LCFLEX (Rose? and Lavie,
2000), in order to automatically build a corpus of instruc-
tional text annotated with lexical semantics information.
The lexicon and ontology are derived from two lexical
resources: VerbNet (Kipper et al, 2000a) for verbs and
CoreLex (Buitelaar, 1998) for nouns. We also report the
excellent parsing results we obtained.
Our ultimate goal is to develop a (semi)automatic
method to derive domain knowledge from instructional
text, in the form of linguistically motivated action
schemes. To develop this acquisition engine, our ap-
proach calls for an instructional corpus where verbs are
annotated with their semantic representation, and where
relations such as precondition and effect between the ac-
tions denoted by those verbs are marked. Whereas the
action relation annotation will be manual, the semantic
annotation can be done automatically by a parser.
We are interested in decompositional theories of
lexical semantics such as (Levin and Rappaport Hovav,
1992) to account for examples such as the following:
(1a) Wipe the fingerprints from the counter.
(1b) Wipe the counter.
(2a) Remove the groceries from the bag.
(2b) Remove the bag.
As the effect of the two actions (1a) and (2a), it is inferred
that the specified location (counter in (1a), bag in (2a))
has been ?emptied? of the object (fingerprints in (1a),
groceries in (2a)). Thus, a system could map both verbs
wipe and remove onto the same action scheme. How-
ever, the apparently equivalent transformations from (1a)
to (1b) and from (2a) to (2b) show otherwise. (1b) de-
scribes the same action as (1a), however (2b) cannot have
the same meaning as (2a). (Levin and Rappaport Hovav,
1992) defines classes of verbs according to the ability or
inability of a verb to occur in pairs of syntactic frames
that preserve meaning. The location-as-object variant is
possible only with (some) manner/means verbs such as
wipe, and not with result verbs such as remove.
We chose to base our lexicon and ontology on VerbNet
(Kipper et al, 2000a), that operationalizes Levin?s work
and accounts for 960 distinct verbs classified into 72 main
classes. Moreover, given VerbNet strong syntactic com-
ponents, it can be easily coupled with a parser and used to
automatically generate a semantically annotated corpus.
Of course, when building a representation for a sen-
tence, we need semantics not only for verbs, but also
for nouns. Whereas many NL applications use Word-
Net (Fellbaum, 1998), we were in need of a richer lex-
icon. We found CoreLex (Buitelaar, 1998) appropriate
for our needs. CoreLex is based on a different theory
than Levin?s (that of the generative lexicon (Pustejovsky,
1991)), but does provide a compatible decompositional
meaning representation for nouns.
The contribution of our work is to demonstrate that a
meaning representation based on decompositional lexical
semantics can be derived efficiently and effectively. We
believe there is no other work that attaches a semantics
of this type to a parser for a large coverage corpus. Verb-
Net has been coupled with the TAG formalism (Kipper
et al, 2000b), but no parsing results are available. More-
( :morphology position
:syntax (*or*
((cat n) (root position) (agr 3s) (semtag (*or* lap1 lap2)))
((cat vlex) (root position) (vform bare)
(subcat (*or* np np-advp np-pp))(semtag put)))
:semantics (put (<put-9.1> (subj agent) (obj patient) (modifier destination) (pred destination)))
(lap1 (<lap1>))
(lap2 (<lap2>)))
Figure 1: The entry for position in the LCFLEX lexicon
CLASS: put-9.1
PARENT: -
MEMBERS: arrange immerse lodge mount place position put set situate sling
THEMATIC ROLES: Agt Pat Dest
SELECTIONAL RESTRICTIONS: Agt[+animate] Pat[+concrete] Dest[+location -region]
FRAMES:
Transitive with Locative PP Agt V Pat Prep[+loc] Dest cause(Agt, E0) ^ motion(during(E0), Pat) ^
:Located-in(start(E0), Pat, Dest) ^ Located-in(end(E0), Pat, Dest)
Transitive with Locative Adverb Agt V Pat Dest[+adv-loc] cause(Agt, E0) ^ motion(during(E0), Pat) ^
:Located-in(start(E0), Pat, Dest) ^ Located-in(end(E0), Pat, Dest)
Figure 2: The class put-9.1 from VerbNet
over, we also show that two lexical resources that focus
on verbs and nouns can be successfully integrated.
2 Lexicon and ontology
We chose LCFLex (Rose? and Lavie, 2000), a robust left-
corner parser, because it can return portions of analysis
when faced with ungrammaticalities or unknown words
or structures (the latter is likely in a large corpus). We
modified and augmented LCFLEX?s existing lexicon and
built an ontology.
To illustrate our work, we will refer to the lexical en-
try for position, that can be both a noun (n) or a verb
(vlex) ? the format is provided by LCFLEX, but the
:semantics field was originally empty (see Figure 1).
For the verb, different subcategorization frames are listed
under subcat: the verb can have as argument just an np,
or an np and a pp, or an np and an adverbial phrase. Each
part of speech (POS) category is associated to a semtag,
an index that links the POS entry to the corresponding se-
mantic representation. <put-9.1>, <lap1> and <lap2>
are entries in our ontology. Before discussing the ontol-
ogy, we need to discuss the VerbNet and CoreLex for-
malisms.
Figure 2 shows a simplified version of the VerbNet
class to which the verb position belongs. All verbs that
can undergo the same syntactic alternations belong to the
same class. Each frame is labeled with its name, and con-
sists of the syntactic frame itself (e.g., Agt V Pat Prep
Dest), and its semantic interpretation. Agt stands for
Agent, V for Verb, Pat for Patient, Dest for Destination.
A class includes a list of parent classes, empty in this
case (verb classes are arranged in a hierarchy), its the-
matic roles and selectional restrictions on these. Then,
it specifies all the frames associated with that class, and
provides a meaning representation for each frame. In this
case, the two frames are both transitive. In the first the
destination is a prepositional phrase, whereas in the sec-
ond the destination is an adverb.
The semantics portion of a lexical entry links the syn-
tactic roles built by the parser to the thematic roles in the
verb class. In Figure 1, the following mappings are spec-
ified under put: subject to agent, object to patient, mod-
ifier to destination for the first frame (the parser always
maps prepositional phrases to modifier roles), and pred to
destination for the second frame (the parser usually maps
adverbs to the pred role).
As regards nouns, CoreLex defines semantic types
such as artifact or information. Nouns are characterized
by bundles of semantic types. Nouns that share the same
bundle are grouped in the same Systematic Polysemous
Class (SPC). The resulting 126 SPCs cover about 40,000
nouns.
VerbNet classes and CoreLex SPCs are realized as en-
tities in our ontology. Figure 3 shows the entries for
put-9.1 and the SPCs lap2 (we omit lap1 for lack
of space). We do not have room for many details, how-
ever note that the :spec field is the basis for building the
semantic representation while parsing. The subfields of
:spec are structured as (name type-check arg).
arg can be either a variable or a complex argument built
with one or more functions. type-check is a the type
constraint arg must satisfy to be included in the final
representation. For further details, see (Terenzi, 2002).
(:type <put-9.1>
:isa nil
:vars (agent patient destination)
:spec ((agent <animate> agent)
(patient <concr-ent> patient)
(dest <> (<loc-not-reg> destination))
(event <>
(<event>
(<not-located-in> destination patient)
(<in-motion> patient)
(<located-in> destination patient)
nil
event))))
(:type <lap2>
:isa (<loc>)
:instances nil
:vars nil
:spec ((artifact +)
(location +)
(psych-feat +)))
Figure 3: Two entries in our ontology
3 Results
Our lexicon includes 109 verbs and 289 nouns, grouped
under 9 classes and 47 SPCs respectively (classes and
SPCs are the entries in the ontology).
We evaluated LCFLEX augmented as we have de-
scribed on a test set taken from the home repair portion of
a 9Mb written corpus originally collected at the Univer-
sity of Brighton. We collected the 480 sentences that con-
tained at least one of the verbs in our lexicon ? out of 109
verbs, those sentences cover 75. These 480 sentences in-
clude a main clause plus a number of adjunct clauses. Be-
cause we were mostly interested in those specific verbs,
we simplified those sentences so that the clause that con-
tains the verb of interest becomes the main clause, and
the others are discarded.
Correct Partially Wrong Parser
correct error
Only Verbs 87% 4.8% 2.2% 6%
Verbs, Nouns 96% 4% 0 0
Table 1: Parsing Results
Table 1 reports our results. A correct parse means that
the full semantic representation is built with every syntac-
tic role mapped to the correct thematic role. With partial
correctness we mean that e.g. not all the syntactic roles
were mapped to their correct thematic roles. Correctness
was judged parse by parse by one of the two authors. We
conducted two evaluations, one earlier after we had not
yet included nouns, and one after the full implementa-
tion. In the first evaluation (Only Verbs), we preprocessed
the sentences so that the nouns from the corpus would be
mapped to the closest noun in our then small noun lex-
icon of about 40 nouns. The second evaluation (Verbs,
Nouns) was conducted on 228 sentences out of the 480
tested in the first evaluation. The 228 sentences contain
the original nouns, as we now have the full lexicon for
the nouns too. The improvement in the second evaluation
is due to the full noun lexicon, but the absence of parser
errors to improvements in a new release of the parser.
4 Conclusions and future work
We have shown that two rich lexicons such as VerbNet
and CoreLex can be successfully integrated. We have
also shown that a parser which uses such a lexicon and
ontology performs extremely well on instructional text.
We are now poised to systematically run the parser on
the full home repair portion of the corpus (about 6Mb).
This is likely to require additions to the lexicon and the
ontology.
Acknowledgements
This work is supported by award 0133123 from the National
Science Foundation. Thanks to all who shared their resources
with us.
References
Paul Buitelaar. 1998. CoreLex: Systematic Polysemy and Un-
derspecification. Ph.D. thesis, Computer Science, Brandeis
University, February.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical DataBase. MIT Press, Cambridge, MA.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000a.
Class-based construction of a verb lexicon. In AAAI-2000,
Proceedings of the Seventeenth National Conference on Ar-
tificial Intelligence, Austin, TX.
K. Kipper, H. T. Dang, W. Schuler, and M. Palmer. 2000b.
Building a class-based verb lexicon using TAGs. In TAG+5
Fifth International Workshop on Tree Adjoining Grammars
and Related Formalisms.
B. Levin and M. Rappaport Hovav. 1992. Wiping the slate
clean: a lexical semantic exploration. In B. Levin and S.
Pinker, editors, Lexical and Conceptual Semantics. Black-
well Publishers.
James Pustejovsky. 1991. The generative lexicon. Computa-
tional Linguistics, 17(4):409?441.
C. P. Rose? and A. Lavie. 2000. Balancing robustness and
efficiency in unification-augmented context-free parsers for
large practical applications. In J.-C. Junqua and G. van No-
ord, editors, Robustness in Language and Speech Technol-
ogy. Kluwer Academic Press.
Elena Terenzi. 2002. Building lexical semantics representa-
tions for action verbs. Master?s thesis, University of Illinois
- Chicago, December.
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 566?574,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An effective Discourse Parser that uses Rich Linguistic Information
Rajen Subba ?
Display Advertising Sciences
Yahoo! Labs
Sunnyvale, CA, USA
rajen@yahoo-inc.com
Barbara Di Eugenio
Department of Computer Science
University of Illinois
Chicago, IL, USA
bdieugen@cs.uic.edu
Abstract
This paper presents a first-order logic learn-
ing approach to determine rhetorical relations
between discourse segments. Beyond lin-
guistic cues and lexical information, our ap-
proach exploits compositional semantics and
segment discourse structure data. We report
a statistically significant improvement in clas-
sifying relations over attribute-value learn-
ing paradigms such as Decision Trees, RIP-
PER and Naive Bayes. For discourse pars-
ing, our modified shift-reduce parsing model
that uses our relation classifier significantly
outperforms a right-branching majority-class
baseline.
1 Introduction
Many theories postulate a hierarchical structure for
discourse (Mann and Thompson, 1988; Moser et.
al., 1996; Polanyi et. al., 2004). Discourse struc-
ture is most often based on semantic / pragmatic re-
lationships between spans of text and results in a tree
structure, as that shown in Figure 1. Discourse
parsing, namely, deriving such tree structures and
the rhetorical relations labeling their inner nodes is
still a challenging and mostly unsolved problem in
NLP. It is linguistically plausible that such structures
are determined at least in part on the basis of the
meaning of the related chunks of texts, and of the
rhetorical intentions of their authors. However, such
knowledge is extremely difficult to capture. Hence,
previous work on discourse parsing (Wellner et. al.,
2006; Sporleder and Lascarides, 2005; Marcu, 2000;
Polanyi et. al., 2004; Soricut and Marcu, 2003;
?This work was done while the author was a student at the
University of Illinois at Chicago.
Baldridge and Lascarides, 2005) has relied only on
syntactic and lexical information, lexical chains and
shallow semantics.
We present an innovative discourse parser that
uses compositional semantics (when available) and
information on the structure of the segment being
built itself. Our discourse parser, based on a modi-
fied shift-reduce algorithm, crucially uses a rhetori-
cal relation classifier to determine the site of attach-
ment of a new incoming chunk together with the ap-
propriate relation label. Another novel aspect of our
work is the usage of Inductive Logic Programming
(ILP): ILP learns from first-order logic representa-
tions (FOL). The ILP-based relation classifier is
significantly more accurate than relation classifiers
that use competitive propositional ML algorithms
such as decision trees and Naive Bayes. In addi-
tion, it results in FOL rules that are linguistically
perspicuous. Our domain is that of instructional
how-to-do manuals, and we describe our corpus
in Section 2. In Section 3, we discuss the modified
shift-reduce parser we developed. The bulk of the
paper is devoted to the rhetorical relation classifier
in Section 4. Experimental results of both the rela-
tion classifier and the discourse parser in its entirety
are discussed in Section 5. Further details can be
found in (Subba, 2008).
2 Discourse Annotated Instructional
Corpus
Existing corpora annotated with rhetorical relations
(Carlson et. al., 2003; Wolf and Gibson, 2005;
Prasad et. al., 2008) focus primarily on news arti-
cles. However, for us the development of the dis-
course parser is parasitic on our ultimate goal: de-
veloping resources and algorithms for language in-
566
s1e1-s5e2
general:specific
dddd
dddd
dddd
dddd
dddd
dddd
dddd
dddd
dddd
d
ZZ
ZZ
ZZ
ZZ
ZZ
ZZ
ZZ
ZZ
ZZ
Z
s1e1 s2e1-s5e2
preparation:act
ggg
ggg
ggg
ggg
ggg
ggg
ggg
gg
WWW
WWW
WWW
WWW
WWW
WWW
WWW
WW
Another way ..
.. sheets.
s2e1-s4e1
preparation:act
RR
RR
RR
RR
RR
RR
RR
RR
ll
ll
ll
ll
ll
ll
ll
ll
s5e1-s5e2
act:goal
x
x
x
x
x
F
F
F
F
F
F
F
F
F
F
s2e1-s3e2
preparation:act
RR
RR
RR
RR
RR
RR
RR
RR
ll
ll
ll
ll
ll
ll
ll
ll
s4e1 s5e1 s5e2
s2e1-s2e2
reason:act
F
F
F
F
F
F
F
F
F
F
x
x
x
x
x
s3e1-s3e2
disjunction
F
F
F
F
F
F
F
F
F
F
x
x
x
x
x
x
x
x
x
x
Then lay
the sheet
.. the panel.
Using the ..
.. pattern,
mark the panel.
s2e1 s2e2 s3e1 s3e2
Because these
sheets ..
.. panels,
you can
tape one ..
.. panel.
Mark the
opening ..
.. sheet,
or cut it
.. blade.
Figure 1: Discourse Parse Tree of the Text in Example (1)
terfaces to instructional applications. Hence, we are
interested in working with instructional texts. We
worked with a corpus on home-repair that is about
5MB in size and is made up entirely of written En-
glish instructions,1 such as that shown in Exam-
ple (1). The text has been manually segmented
into Elementary Discourse Units (EDUs), the small-
est units of discourse. In total, our corpus contains
176 documents with an average of 32.6 EDUs for a
total of 5744 EDUs and 53,250 words. The structure
for Example (1) is shown in Figure 1.
(1) [Another way to measure and mark panels for
cutting is to make a template from the protec-
tive sheets.(s1e1)] [Because these sheets are the
same size as the panels,(s2e1)] [you can tape
one to the wall as though it were a panel.(s2e2)]
[Mark the opening on the sheet(s3e1)] [or cut
it out with a razor blade.(s3e2)] [Then lay the
sheet on the panel.(s4e1)] [Using the template
as a pattern,(s5e1)] [mark the panel.(s5e2)]
To explore our hypothesis, that rich linguistic in-
formation helps discourse parsing, and that the state
1The raw corpus was originally assembled at the Informa-
tion Technology Research Institute, University of Brighton.
of the art in machine learning supports such an
approach, we needed training data annotated with
both compositional semantics and rhetorical rela-
tions. We performed the first type of annotation al-
most completely automatically, and the second man-
ually, as we turn now to describing.
2.1 Compositional Semantics Derivation
The type of compositional semantics we are inter-
ested in is heavily rooted in verb semantics, which
is particularly appropriate for the instructional text
we are working with. Therefore, we used VerbNet
(Kipper et. al., 2000) as our verb lexicon. VerbNet
groups together verbs that undergo the same syn-
tactic alternations and share similar semantics. It
accounts for 4962 distinct verbs classified into 237
main classes. Each verb class is described by the-
matic roles, selectional restrictions on the arguments
and frames consisting of a syntactic description and
semantic predicates. Such semantic classification of
verbs can be helpful in making generalizations, es-
pecially when data is not abundant. Generalization
can also be achieved by means of the semantic pred-
icates. Although the verb classes of two verb in-
stances may differ, semantic predicates are shared
across verbs. To compositionally build verb based
567
semantic representations of our EDUs, we (Subba
et al, 2006) integrated a robust parser, LCFLEX
(Rose?, 2000), with a lexicon and ontology based
both on VerbNet and, for nouns, on CoreLex (Buite-
laar, 1998). The augmented parser was able to de-
rive complete semantic representations for 3257 of
the 5744 EDUs (56.7%). The only manual step was
to pick the correct parse from a forest of parse trees,
since the output of the parser can be ambiguous.
2.2 Rhetorical relation annotation
The discourse processing community has not yet
reached agreement on an inventory of rhetorical re-
lations. Among the many choices, our coding
scheme is a hybrid of (Moser et. al., 1996) and
(Marcu, 1999). We focused on what we call infor-
mational relations, namely, relations in the domain.
We used 26 relations, divided into 5 broad classes:
12 causal relations (e.g., preparation:act, goal:act,
cause:effect, step1:step2); 6 elaboration relations
(e.g., general:specific, set:member, object:attribute;
3 similarity relations (contrast1:contrast2, com-
parison, restatement); 2 temporal relations (co-
temp1:co-temp2, before:after); and 4 other rela-
tions, including joint and disjunction.
The annotation yielded 5172 relations, with rea-
sonable intercoder agreement. On 26% of the data,
we obtained ? = 0.66; ? rises to 0.78 when the two
most commonly confused relations, preparation:act
and step1:step2, are consolidated. We also anno-
tated the relata as nucleus (more important mem-
ber) and satellite (contributing member(s)) (Mann
and Thompson, 1988), with ? = 0.67.2 The most
frequent relation is preparation:act (24.46%), and in
general, causal relations are more frequently used in
our instructional corpus than in news corpora (Carl-
son et. al., 2003; Wolf and Gibson, 2005).
3 Shift-Reduce Discourse Parsing
Our discourse parser is a modified version of a shift-
reduce parser. The shift operation places the next
segment on top of the stack, TOP. The reduce oper-
ation will attach the text segment at TOP to the text
segment at TOP-1. (Marcu, 2000) also uses a shift-
reduce parser, though our parsing algorithm differs
2We don?t have space to explain why we annotate for nu-
cleus and satellite, even if (Moser et. al., 1996) argue that this
sort of distinction does not apply to informational relations.
in two respects: 1) we do not learn shift operations
and 2) in contrast to (Marcu, 2000), the attachment
of an incoming text segment to the emerging tree
may occur at any node on the right frontier. This al-
lows for the more sophisticated type of adjunction
operations required for discourse parsing as mod-
eled in D-LTAG (Webber, 2004). A reduce op-
eration is determined by the relation identification
component. We check if a relation exists between
the incoming text segment and the attachment points
on the right frontier. If more than one attachment
site exists, then the attachment site for which the rule
with the highest score fired (see below) is chosen for
the reduce operation. A reduce operation can fur-
ther trigger additional reduce operations if there is
more than one tree left in the stack after the first re-
duce operation. When no rules fire, a shift occurs.
In the event that all the segments in the input list
have been processed and a full DPT has not been
obtained, then we reduce TOP and TOP-1 using the
joint relation until a single DPT is built.
4 Classifying Rhetorical Relations
Identifying the informational relations between text
segments is central to our approach for building the
informational tree structure of text. We believe that
the use of a limited knowledge representation for-
malism, essentially propositional logic, is not ad-
equate and that a relational model that can handle
compositional semantics is necessary. We cast the
problem of determining informational relations as a
classification task. We used the ILP system Aleph
that is based on (Muggleton, 1995). Formulation
of any problem within the ILP framework consists
of background knowledge B and the set of exam-
ples E (E+? E?). In our ILP framework, positive
examples are ground clauses describing a relation
and its relata, e.g. relation(s5e1,s5e2,act:goal), or
relation(s2e1-s3e2,s4e1,preparation:act) from Fig-
ure 1. If e is a positive example of a relation r, then
it is also a negative example for all the other rela-
tions.
Background Knowledge (B) can be thought of as
features used by ILP to learn rules, as in traditional
attribute-value learning algorithms. We use the fol-
lowing information to learn rules for classifying re-
lations. Figure 2 shows a sample of the background
568
Verbs + Nouns: verb(?s5e2?,mark). noun(?s5e2?,panel).
Linguistic Cues: firstWordPOS(?s5e2?,?VB?). lastWordPOS(?s5e2?,?.?).
Similarity: segment sim score(?s5e1?,?s5e2?,0.0).
verbclass(?s5e2?,mark,?image impression-25.1?).
agent(?s5e2?,frame(mark),you).
Compositional Semantics: destination(?s5e2?,frame(mark),panel).
cause(?s5e2?,frame(mark),you,?s5e2-mark-e?).
prep(?s5e2?,frame(mark),end(?s5e2-mark-e?),mark,panel).
created image(?s5e2?,frame(mark),result(?s5e2-mark-e?),mark).
Structural Information: same sentence(?s5e1?,?s5e2?).
Figure 2: Example Background Knowledge
knowledge provided for EDU s5e2.
Verbs + Nouns: These features were derived by
tagging all the sentences in the corpus with a POS
tagger (Brill, 1995).
WordNet: For each noun in our data, we also use
information on hypernymy and meronymy relations
using WordNet. In a sense, this captures the domain
relations between objects in our data.
Linguistic Cues: Various cues can facilitate the
inference of informational relations, even if it is well
known that they are based solely on the content of
the text segments, various cues can facilitate the in-
ference of such relations. At the same time, it is
well known that relations are often non signalled:
in our corpus, only 43% of relations are signalled,
consistently with figures from the literature (44%
in (Williams and Reiter, 2003) and 45% in (Prasad
et. al., 2008)). Besides lexical cues such as but,
and and if, we also include modals, tense, compara-
tives and superlatives, and negation. E.g., wrong-act
in relations like prescribe-act:wrong-act is often ex-
pressed using a negation.
Similarity: For the two segments in question, we
compute the cosine similarity of the segments using
only nouns and verbs.
Compositional semantics: the semantic infor-
mation derived by our parser, as described in Sec-
tion 2.1. The semantic representation of segment
s5e2 from Example (1) is shown in Figure 2. Each
semantic predicate is a feature for the classifier.
Structural Information: For relations between
two EDUs, we use knowledge of whether the two
EDUs are intra-sentential or inter-sentential, since
some relations, e.g. criterion:act, are more likely to
be realized intra-sententially than inter-sententially.
For larger segments, we also encode the hierarchi-
cal representation of text segments that contain more
than one nucleus, the distance between the nuclei
of the two segments and any relations that exist be-
tween the smaller inner segments.
At this point, the attentive reader will be wonder-
ing how we encode compositional semantics for re-
lations relating text segments larger than one EDU.
Clearly we cannot just list the semantics of each
EDU that is dominated by the larger segment. We
follow the intuition that nuclei represent the most
important portions of segments (Mann and Thomp-
son, 1988). For segments such as s5e1-s5e2 that
contains a single nucleus, we simply reduce the se-
mantic content of the larger segment to that of its
nucleus:
s5e1-s5e2
verb(?s5e1-s5e2?,mark).
...
verbclass(?s5e1-s5e2?,..).
agent(?s5e1-s5e2?,..).
In this case, the semantics of the complex text seg-
ment is represented by the compositional semantics
of the single most important EDU.
For segments that contain more than one nu-
cleus, such as s3e1-s3e2, the discourse struc-
ture information of the segment is represented with
the additional predicates internal relation and par-
ent segment. These predicates can be used recur-
sively at every level of the tree to specify the relation
between the most important segments. In addition,
they also provide a means to represent the compo-
sitional semantics of the most important EDUs and
569
make them available to the relational learning algo-
rithm.
s3e1-s3e2
internal relation(s3e1,s3e2,?disjunction?).
parent segment(s3e1-s3e2,s3e1).
parent segment(s3e1-s3e2,s3e2).
LL
LL
LL
LL
LL
LL
LL
LL
LL
LL
L
rr
rr
rr
rr
rr
rr
rr
rr
rr
rr
r
verb(?s3e1?,mark).
noun(?s3e1?,opening).
...
verbclass(?s3e1?,..).
theme(?s3e1?,..).
verb(?s3e2?,cut).
noun(?s3e2?,opening).
...
noun(?s3e1?,blade).
4.1 Learning FOL Rules for Discourse Parsing
In Aleph, the hypothesis space is restricted to a set of
rules that conform to a predefined language L. This
is done with the use of mode declarations which, in
other words, introduces a language bias in the learn-
ing process. modeh declarations inform the learning
algorithm about what predicates to use as the head
of the rule and modeb specifies what predicates to
use in the body of the rule. Not all the information
in B needs to be included in the body of the rule.
This makes sense since we often learn definitions of
concepts based on more abstract higher level infor-
mation that is inferred from some other information
that is not part of our final definition. Mode decla-
rations are used by Aleph to build the most specific
clause (?) that can be learned for each example. ?
constrains the search for suitable hypotheses. ?i is
built by taking an example ei ? E+ and adding lit-
erals that are entailed by B and ei. We then have the
following property, whereHi is the hypothesis (rule)
we are trying to learn and is a generality operator:
  Hi  ?i
Finding the most specific clause (?) provides us
with a partially ordered set of clauses from which to
choose the best hypothesis based on some quantifi-
able qualitative criteria. This sub-lattice is bounded
by the most general clause (, the empty clause)
from the top and the most specific clause (?) at the
bottom. We use the heuristic search in Aleph that is
similar to the A*-like search strategy presented by
(Muggleton, 1995) to find the best hypothesis (rule).
A noise threshold on the number of negative exam-
ples that can be covered by a rule can be set. We
learn a model that learns perfect rules first and then
one that allows for at most 5 negative examples. A
backoff model that first uses the model trained with
noise = 0 and then noise = 5 if no classification
has been made is used. We use the evaluation func-
tion in Equation 1 to guide our search through the
tree of possible hypotheses. This evaluation func-
tion is also called the compression function since it
prefers simpler explanations to more complex ones
(Occam?s Razor). fs is the score for clause cs that
is being evaluated, ps is the number of positive ex-
amples, ns is the number of negative examples, ls is
the length of the clause (measured by the number of
clauses).
fs = ps ? (ns + (0.1? ls)) (1)
Classification in most ILP systems, including
Aleph, is restricted to binary classification (positive
vs. negative). In many applications with just two
classes, this is sufficient. However, we are faced
with a multi-classification problem. In order to per-
form multi-class classification, we use a decision
list. First, we build m binary classifiers for each
relation r ? R. Then, we form an ordered list of the
rules based on the following criterion:
1. Given two rules ri and rj , ri ,is ranked higher
than rj if (pi ? ni) > (pj ? nj).
2. if (pi?ni) = (pj ?nj), then ri is ranked higher
than rj if ( pipi+ni ) > (
pj
pj+nj ).
3. if (pi ? ni) = (pj ? nj) and ( pipi+ni ) = (
pj
pj+nj )then ri is ranked higher than rj if (li) > (lj).
4. default: random order
Classifying an unseen example is done by using
the first rule in the ordered list that satisfies it.
5 Experiments and Results
We report our results from experiments on both the
classification task and the discourse parsing task.
5.1 Relation Classification Results
For the classification task, we conducted exper-
iments using the stratified k-fold (k = 5) cross-
validation evaluation technique on our data. Unlike
570
(Wellner et. al., 2006; Sporleder and Lascarides,
2005), we do not assume that we know the order
of the relation in question. Instead we treat reversals
of non-commutative relations (e.g. preparation:act
and act:goal) as separate relations as well. We
compare our ILP model to RIPPER, Naive Bayes
and the Decision Tree algorithm. We should point
out that since attribute-value learning models can-
not handle first-order logic data, they have been pre-
sented with features that lose at least some of this
information. While this may then seem to result in
an unfair comparison, to the contrary, this is pre-
cisely the point: can we do better than very effec-
tive attribute-value approaches that however inher-
ently cannot take richer information into account?
All the statistical significance tests were performed
using the value of F-Score obtained from each of the
folds. We report performance on two sets of data
since we were not able to obtain compositional se-
mantic data for all the EDUs in our corpus:
? Set A: Examples for which semantic data was
available for all the nuclei of the segments
(1789 total). This allows us to have a better
idea of how much impact semantic data has on
the performance, if any.
? Set B: All examples regardless of whether or
not semantic data was available for the nuclei
of the segments (5475 total).
Model Semantics No Semantics
ILP 62.78 60.25
Decision Tree 56.29 55.45
RIPPER 58.02 56.96
Naive Bayes 35.83 34.66
Majority Class 31.63 31.63
Table 1: Classification Performance: Set A (F-Score)
Table 1 shows the results on Set A. ILP outper-
forms all the other models. Via ANOVA, we first
conclude that there is a statistically significant differ-
ence between the 8 models (p < 2.2e?16). To then
pinpoint where the difference precisely lies, pair-
wise comparisons using Student?s t-test show that
the difference between ILP (using semantics) and all
of the other learning models is statistically signifi-
cant at p < 0.05. Additionally, ILP with semantics
is significantly better than ILP without it (p < 0.05).
For Decision Tree, Naive Bayes and RIPPER, the
improvement in using semantics is not statistically
significant.
Model Semantics No Semantics
ILP 59.43 59.22
Decision Tree 53.84 53.69
RIPPER 51.1 51.36
Naive Bayes 49.69 51.62
Majority Class 22.01 22.01
Table 2: Classification Performance: Set B (F-Score)
In Table 2, we list the results on Set B. Once
again, our ILP model outperforms the other three
learning models. Naive Bayes is much more com-
petitive when using all the examples compared to
using only examples with semantic data. In the case
of the attribute-value machine learning models, the
use of semantic data seems to marginally hurt the
performance of the classifiers. However, this is in
contrast to the relational ILP model which always
performs better when using semantics. This result
suggests that the use of semantic data with loss of in-
formation may not be helpful, and in fact, it may ac-
tually hurt performance. Based on ANOVA, the dif-
ferences in these 8 models is statistically significant
with p < 6.95e?12. A pairwise t-test between ILP
(using semantics) and each of the other attribute-
value learning models shows that our results are sta-
tistically significant at p < 0.05.
In Table 3, we report the performance of the two
ILP models on each relation.3 In general, the models
perform better on relations that have the most exam-
ples.
The evaluation of work in discourse parsing is
hindered by the lack of a standard corpus or task.
Hence, our results cannot be directly compared
to (Marcu, 2000; Sporleder and Lascarides, 2005;
Wellner et. al., 2006), but neither can those works
be compared among themselves, because of differ-
ences in underlying corpora, the type and number of
relations used, and various assumptions. However,
we can still draw some general comparisons. Our
ILP-based models provide as much or significantly
3Due to space limitations, only relations with> 10 examples
are shown.
571
relation Semantics No Semantics
preparation:act 74.86 72.05
general:specific 31.74 28.24
joint 55.23 52
act:goal 86.12 83.85
criterion:act 77.37 75.32
goal:act 73.43 68.9
step1:step2 28.75 35.29
co-temp1:co-temp2 48.84 37.84
disjunction 83.33 80.81
act:criterion 54.29 54.79
contrast1:contrast2 22.22 5.0
act:preparation 65.31 70.59
act:reason 0 10.26
cause:effect 19.05 10.53
comparison 22.22 10.53
Table 3: Classification Performance (F-Score) by
Relation: ILP on Set A
more improvement over a majority-class baseline
when compared to these other works. This is the
case even though our work is based on less training
data, relatively more relations, relations both be-
tween just two EDUs and those involving larger text
segments, and we make no assumptions about the
order of the relations. Our results are comparable to
(Marcu, 2000), which reports an accuracy of about
61% for his classifier. His majority class baseline
performs at about 50% accuracy. (Wellner et. al.,
2006) reports an accuracy of up to 81%, with a ma-
jority class baseline performance of 45.7%. How-
ever, our task is more challenging than (Wellner et.
al., 2006). They use only 11 relations compared to
the 26 we use. They also assume the order of the
relation in the examples (i.e. examples for goal:act
would be treated as examples for act:goal by revers-
ing the order of the arguments) whereas we do not
make such assumptions. In addition, their training
data is almost twice as large as ours, based on re-
lation instances. (Sporleder and Lascarides, 2005)
also makes the same assumption on the ordering of
the relations as (Wellner et. al., 2006). They re-
port an accuracy of 57.75%. Their work, though,
was based on only 5 relations. Importantly, neither
(Wellner et. al., 2006; Sporleder and Lascarides,
2005) model examples with complex text segments
with more than one EDU.
5.2 How interesting are the rules?
Given that our ILP models learn first-order logic
rules, we can make some qualitative analysis of the
rules learned, such as those below, learnt by the ILP
model that uses semantics:
(2a) relation(A,B,?act:goal?) :-
firstWordPOS(A,?VBG?),
verbclass(A,D,?use-1?),
firstWordPOS(B,?VB?).
[pos cover = 23 neg cover = 1]
(2b) relation(A,B,?preparation:act?) :-
discourse cue(B,front,and),
cause(A,frame(C),D,E),
theme(B,frame(F),G), theme(A,frame(C),G).
[pos cover = 12 neg cover = 0]
(2c) relation(A,B,?preparation:act?) :-
discourse cue(B,front,then),
parent segment(A,C), parent segment(A,D),
internal relation(C,D,?preparation:act?).
[pos cover = 17 neg cover = 0]
(2a) is learned using examples such as
relation(s5e1,s5e2,?act:goal?) from Example (1).
(2b) uses relational semantic information. This rule
can be read as follows:
IF segment A contains a cause and a
theme, the same object that is the theme
in A is also the theme in segment B, and B
contains the discourse cue and at the front
THEN the relation between A and B is
preparation:act.
(2c) is a rule that makes use of the structural in-
formation about complex text segments. When us-
ing Set A, more than about 60% of the rules in-
duced include at least one semantic predicate in its
body. They occur more frequently in rules for re-
lations like preparation:act while less in rules for
general:specific and act:goal.
5.3 Discourse Parsing Results
In order to test our discourse parser, we used 151
documents for training and 25 for testing. We eval-
uated the performance of our parser on both the
discourse parse trees it builds at the sentence level
and at the document level. The test set contained
572
Sentence Level Document Level
model Semantics span nuclearity relation span nuclearity relation
SR-ILP yes 92.91 71.83 63.06 70.35 49.47 35.44
SR-ILP no 91.98 69.59 58.58 68.95 48.16 33.33
Baseline - 93.66 74.44 34.32 70.26 47.98 22.46
Table 4: Parsing Performance (F-Score): (Baseline = right-branching majority)
341 sentences out of which 180 sentences were seg-
mented into more than one EDU. We ran experi-
ments using our two ILP models for the relation
identifier, namely ILP with semantics and without
semantics. Our ILP based discourse parsing models
are named SR-ILP. We compare the performance of
our models against a right branching majority class
baseline. We used the sign-test to determine statis-
tical significance of the results. Using the automatic
evaluation methodology in (Marcu, 2000), preci-
sion, recall and F-Score measures are computed for
determining the hierarchical spans, nucleus-satellite
assignments and rhetorical relations. The perfor-
mance on labeling relations is the most important
measure since the results on nuclearity and hierar-
chical spans are by-products of the decisions made
to attach segments based on relations.
On labeling relations, the parser that uses all the
features (including compositional semantics) for de-
termining relations performs the best with an F-
Score of 63.06%. The difference of about 4.5% (be-
tween ILP with semantics and without semantics)
in F-Score is statistically significant at p = 0.006.
Our best model, SR-ILP (using semantics) beats the
baseline by about 28% in F-Score. Since the task at
the document level is much more challenging than
building the discourse structure at the sentence level,
we were not surprised to see a considerable drop in
performance. For our best model, the performance
on labeling relations drops to 35.44%. Clearly, the
mistakes made when attaching segments at lower
levels have quite an adverse effect on the overall
performance. A less greedy approach to parsing dis-
course structure is warranted.
While we would have hoped for a better perfor-
mance than 35.44%, to start with, (Forbes et. al.,
2001), (Polanyi et. al., 2004), and (Cristea, 2000) do
not report the performance of their discourse parsers
at all. (Marcu, 2000) reports precision and recall of
up to 63.2% and 59.8% on labeling relations using
manually segmented EDUs on three WSJ articles.
(Baldridge and Lascarides, 2005) reports 43.2% F-
Score on parsing 10 dialogues using a probabilistic
head-driven parsing model.
6 Conclusions
In conclusion, we have presented a relational ap-
proach for classifying informational relations and a
modified shift-reduce parsing algorithm for building
discourse parse trees based on informational rela-
tions. To our knowledge, this is the first attempt
at using a relational learning model for the task of
relation classification, or even discourse parsing in
general. Our approach is linguistically motivated.
Using ILP, we are able to account for rich composi-
tional semantic data of the EDUs based on VerbNet
as well as the structural relational properties of the
text segments. This is not possible using attribute-
value based models like Decision Trees and RIP-
PER and definitely not using probabilistic models
like Naive Bayes. Our experiments have shown that
semantics can be useful in classifying informational
relations. For parsing, our modified shift-reduce al-
gorithm using the ILP relation classifier outperforms
a right-branching baseline model significantly. Us-
ing semantics for parsing also yields a statistically
significant improvement. Our approach is also do-
main independent as the underlying model and data
are not domain specific.
Acknowledgments
This work is supported by the National Science Founda-
tion (IIS-0133123 and ALT-0536968) and the Office of
Naval Research (N000140010640).
573
References
Asher, N., and Lascarides, A.: Logics of Conversation.
Cambridge University Press, 2003.
Baldridge, J. and Lascarides, A.: Probabilistic Head-
Driven Parsing for Discourse Structure In Proceed-
ings of the Ninth Conference on Computational Natu-
ral Language Learning (CoNNL), Ann Arbor, 2005.
Brill, E.: Transformation-based error-driven learning
and natural language processing: A case study in
part-of-speech tagging. Computational Linguistics,
21(4):543565, 1995.
Buitelaar, P.: CoreLex: Systematic Polysemy and Un-
derspecification. Ph.D. Thesis, Brandies University,
1998.
Carlson, L. D. M. and Okurowski., M. E.: Building a
discourse-tagged corpus in the framework of rhetorical
structure theory. In Current Directions in Discourse
and Dialogue pages 85?112, 2003.
Cristea, D.: An Incremental Discourse Parser Architec-
ture. In D. Christodoulakis (Ed.) Proceedings of the
Second International Conference - Natural Language
Processing - Patras, Greece, June 2000.
Forbes, K., Miltsakaki, E., R. P. A. S. A. J. and Web-
ber., B.: D-ltag system - discourse parsing with a lexi-
calized tree adjoining grammar. Information Stucture,
Discourse Structure and Discourse Semantics, ESS-
LLI 2001.
Grosz, B. J. and Sidner, C. L.: Attention, intention and
the structure of discourse. Computational Linguistics
12:175?204, 1988.
Hobbs, J. R.: On the coherence and structure of dis-
course. In Polyani, Livia editor, The Structure of Dis-
course, 1985.
Kipper, K., H. T. D. and Palmer., M.: Class-based con-
struction of a verb lexicon. AAAI-2000, Proceedings
of the Seventeenth National Conference on Artificial
Intelligence, 2000.
Mann, W. and Thompson, S.: Rhetorical structure the-
ory: Toward a functional theory of text organization.
Text, 8(3):243?281, 1988.
Marcu, D.: Instructions for Manually Annotating the
Discourse Structures of Texts. Technical Report, Uni-
versity of Southern California, 1999.
Marcu, D.: The theory and practice of discourse parsing
and summarization. Cambridge, Massachusetts, Lon-
don, England, MIT Press, 2000.
Moser, M. G., Moore, J. D., and Glendening, E.: In-
structions for Coding Explanations: Identifying Seg-
ments, Relations and Minimal Units. University of
Pittsburgh, Department of Computer Science, 1996.
Muggleton, S. H.: Inverse entailment and progol.
In New Generation Computing Journal 13:245?286,
1995.
Polanyi, L., Culy, C., van den Berg, M. H. and Thione,
G. L.: A Rule Based Approach to Discourse Pars-
ing. Proceedings of the 5th SIGdial Workshop in Dis-
course And Dialogue. Cambridge, MA USA pp. 108-
117., May 1, 2004.
Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo,
L., Joshi, A., and Webber, B.: The Penn Discourse
Treebank 2.0. LREC, 2008.
Rose?, C. P.: A Syntactic Framework for Semantic In-
terpretation, Proceedings of the ESSLLI Workshop
on Linguistic Theory and Grammar Implementation,
2000.
Sporleder, C. and Lascarides., A.: Exploiting linguistic
cues to classify rhetorical relations. Recent Advances
in Natural Language Processing, 2005.
Soricut, R. and Marcu., D.: Sentence level discourse
parsing using syntactic and lexical information. Pro-
ceedings of the Human Language Technology and
North American Assiciation for Computational Lin-
guistics Conference, 2003.
Subba, R., Di Eugenio, B., E. T.: Building lexical re-
sources for princpar, a large coverage parser that gen-
erates principled semantic representations. LREC,
2006.
Subba, R.: Discourse Parsing: A Relational Learn-
ing Approach Ph.D. Thesis, University of Illinois
Chicago, December 2008.
Webber, B.: DLTAG: Extending Lexicalized TAG to Dis-
course. Cognitive Science 28:751-779, 2004.
Wellner, B., Pustejovsky, J., C. H. R. S. and Rumshisky.,
A.: Classification of discourse coherence rela-
tions: An exploratory study using multiple knowledge
sources. In Proceedings of the 7th SIGDIAL Work-
shop on Discourse and Dialogue, 2006.
Williams, S. and Reiter, E.: A corpus analysis of dis-
course relations for natural language generation. Pro-
ceedings of Corpus Linguistics, pages 899?908, 2003.
Wolf, F. and Gibson, E.: Representing discourse coher-
ence: A corpus-based analysis. Computational Lin-
guistics 31(2):249?287, 2005.
574
FLSA: Extending Latent Semantic Analysis with features
for dialogue act classification
Riccardo Serafin
CEFRIEL
Via Fucini 2
20133 Milano, Italy
Riccardo.Serafin@students.cefriel.it
Barbara Di Eugenio
Computer Science
University of Illinois
Chicago, IL 60607 USA
bdieugen@cs.uic.edu
Abstract
We discuss Feature Latent Semantic Analysis
(FLSA), an extension to Latent Semantic Analysis
(LSA). LSA is a statistical method that is ordinar-
ily trained on words only; FLSA adds to LSA the
richness of the many other linguistic features that
a corpus may be labeled with. We applied FLSA
to dialogue act classification with excellent results.
We report results on three corpora: CallHome Span-
ish, MapTask, and our own corpus of tutoring dia-
logues.
1 Introduction
In this paper, we propose Feature Latent Semantic
Analysis (FLSA) as an extension to Latent Seman-
tic Analysis (LSA). LSA can be thought as repre-
senting the meaning of a word as a kind of average
of the meanings of all the passages in which it ap-
pears, and the meaning of a passage as a kind of
average of the meaning of all the words it contains
(Landauer and Dumais, 1997). It builds a semantic
space where words and passages are represented as
vectors. LSA is based on Single Value Decompo-
sition (SVD), a mathematical technique that causes
the semantic space to be arranged so as to reflect
the major associative patterns in the data. LSA has
been successfully applied to many tasks, such as as-
sessing the quality of student essays (Foltz et al,
1999) or interpreting the student?s input in an Intel-
ligent Tutoring system (Wiemer-Hastings, 2001).
A common criticism of LSA is that it uses only
words and ignores anything else, e.g. syntactic in-
formation: to LSA, man bites dog is identical to dog
bites man. We suggest that an LSA semantic space
can be built from the co-occurrence of arbitrary tex-
tual features, not just words. We are calling LSA
augmented with features FLSA, for Feature LSA.
Relevant prior work on LSA only includes Struc-
tured Latent Semantic Analysis (Wiemer-Hastings,
2001), and the predication algorithm of (Kintsch,
2001). We will show that for our task, dialogue
act classification, syntactic features do not help, but
most dialogue related features do. Surprisingly, one
dialogue related feature that does not help is the di-
alogue act history.
We applied LSA / FLSA to dialogue act classi-
fication. Dialogue systems need to perform dia-
logue act classification, in order to understand the
role the user?s utterance plays in the dialogue (e.g.,
a question for information or a request to perform
an action). In recent years, a variety of empiri-
cal techniques have been used to train the dialogue
act classifier (Samuel et al, 1998; Stolcke et al,
2000). A second contribution of our work is to
show that FLSA is successful at dialogue act classi-
fication, reaching comparable or better results than
other published methods. With respect to a baseline
of choosing the most frequent dialogue act (DA),
LSA reduces error rates between 33% and 52%, and
FLSA reduces error rates between 60% and 78%.
LSA is an attractive method for this task because
it is straightforward to train and use. More impor-
tantly, although it is a statistical theory, it has been
shown to mimic many aspects of human compe-
tence / performance (Landauer and Dumais, 1997).
Thus, it appears to capture important components
of meaning. Our results suggest that LSA / FLSA
do so also as concerns DA classification. On Map-
Task, our FLSA classifier agrees with human coders
to a satisfactory degree, and makes most of the same
mistakes.
2 Feature Latent Semantic Analysis
We will start by discussing LSA. The input to LSA
is a Word-Document matrix W with a row for each
word, and a column for each document (for us, a
document is a unit, e.g. an utterance, tagged with a
DA). Cell c(i, j) contains the frequency with which
wordi appears in documentj .1 Clearly, this w ? d
matrix W will be very sparse. Next, LSA applies
1Word frequencies are normally weighted according to spe-
cific functions, but we used raw frequencies because we wanted
to assess our extensions to LSA independently from any bias
introduced by the specific weighting technique.
to W Singular Value Decomposition (SVD), to de-
compose it into the product of three other matrices,
W = T0S0DT0 , so that T0 and D0 have orthonormal
columns and S0 is diagonal. SVD then provides
a simple strategy for optimal approximate fit using
smaller matrices. If the singular values in S0 are or-
dered by size, the first k largest may be kept and the
remaining smaller ones set to zero. The product of
the resulting matrices is a matrix W? of rank k which
is approximately equal to W ; it is the matrix of rank
k with the best possible least-squares-fit to W .
The number of dimensions k retained by LSA is
an empirical question. However, crucially k is much
smaller than the dimension of the original space.
The results we will report later are for the best k
we experimented with.
Figure 1 shows a hypothetical dialogue annotated
with MapTask style DAs. Table 1 shows the Word-
Document matrix W that LSA starts with ? note that
as usual stop words such as a, the, you have been
eliminated. 2 Table 2 shows the approximate repre-
sentation of W in a much smaller space.
To choose the best tag for a document in the test
set, we first compute its vector representation in the
semantic space LSA computed, then we compare
the vector representing the new document with the
vector of each document in the training set. The
tag of the document which has the highest similarity
with our test vector is assigned to the new document
? it is customary to use the cosine between the two
vectors as a measure of similarity. In our case, the
new document is a unit (utterance) to be tagged with
a DA, and we assign to it the DA of the document in
the training set to which the new document is most
similar.
Feature LSA. In general, in FLSA we add extra
features to LSA by adding a new ?word? for each
value that the feature of interest can take (in some
cases, e.g. when adding POS tags, we extend the
matrix in a different way ? see Sec. 4). The only
assumption is that there are one or more non word
related features associated with each document that
can take a finite number of values. In the Word?
Document matrix, the word index is increased to in-
clude a new place holder for each possible value the
feature may take. When creating the matrix, a count
of one is placed in the rows related to the new in-
dexes if a particular feature applies to the document
under analysis. For instance, if we wish to include
the speaker identity as a new feature for the dialogue
2We use a very short list of stop words (< 50), as our experi-
ments revealed that for dialogue act annotation LSA is sensitive
to the most common words too. This is why to is included in
Table 1.
in Figure 1, the initial Word?Document matrix will
be modified as in Table 3 (its first 14 rows are as in
Table 1).
This process is easily extended if more than one
non-word feature is desired per document, if more
than one feature value applies to a single document
or if a single feature appears more than once in a
document (Serafin, 2003).
3 Corpora
We report experiments on three corpora, Spanish
CallHome, MapTask, and DIAG-NLP.
The Spanish CallHome corpus (Levin et al,
1998; Ries, 1999) comprises 120 unrestricted phone
calls in Spanish between family members and
friends, for a total of 12066 unique words and 44628
DAs. The Spanish CallHome corpus is annotated at
three levels: DAs, dialogue games and dialogue ac-
tivities. The DA annotation augments a basic tag
such as statement along several dimensions, such
as whether the statement describes a psychologi-
cal state of the speaker. This results in 232 differ-
ent DA tags, many with very low frequencies. In
this sort of situations, tag categories are often col-
lapsed when running experiments so as to get mean-
ingful frequencies (Stolcke et al, 2000). In Call-
Home37, we collapsed different types of statements
and backchannels, obtaining 37 different tags. Call-
Home37 maintains some subcategorizations, e.g.
whether a question is yes/no or rhetorical. In Call-
Home10, we further collapse these categories. Call-
Home10 is reduced to 8 DAs proper (e.g., state-
ment, question, answer) plus the two tags ??%??
for abandoned sentences and ??x?? for noise.
CallHome Spanish is further annotated for dialogue
games and activities. Dialogue game annotation is
based on the MapTask notion of a dialogue game,
a set of utterances starting with an initiation and
encompassing all utterances up until the purpose of
the game has been fulfilled (e.g., the requested infor-
mation has been transferred) or abandoned (Car-
letta et al, 1997). Moves are the components of
games, they correspond to a single or more DAs,
and each is tagged as Initiative, Response or Feed-
back. Each game is also given a label, such as
Info(rmation) or Direct(ive). Finally, activities per-
tain to the main goal of a certain discourse stretch,
such as gossip or argue.
The HCRC MapTask corpus is a collection of di-
alogues regarding a ?Map Task? experiment. Two
participants sit opposite one another and each of
them receives a map, but the two maps differ. The
instruction giver (G)?s map has a route indicated
while instruction follower (F)?s map does not in-
(Doc 1) G: Do you see the lake with the black swan? Query?yn
(Doc 2) F: Yes, I do Reply?y
(Doc 3) G: Ok, Ready
(Doc 4) G: draw a line straight to it Instruct
(Doc 5) F: straight to the lake? Check
(Doc 6) G: yes, that?s right Reply?y
(Doc 7) F: Ok, I?ll do it Acknowledge
Figure 1: A hypothetical dialogue annotated with MapTask tags
(Doc 1) (Doc 2) (Doc 3) (Doc 4) (Doc 5) (Doc 6) (Doc 7)
do 1 1 0 0 0 0 1
see 1 0 0 0 0 0 0
lake 1 0 0 0 1 0 0
black 1 0 0 0 0 0 0
swan 1 0 0 0 0 0 0
yes 0 1 0 0 0 1 0
ok 0 0 1 0 0 0 1
draw 0 0 0 1 0 0 0
line 0 0 0 1 0 0 0
straight 0 0 0 1 1 0 0
to 0 0 0 1 1 0 0
it 0 0 0 1 0 0 1
that 0 0 0 0 0 1 0
right 0 0 0 0 0 1 0
Table 1: The 14-dimensional word-document matrix W
clude the drawing of the route. The task is for G
to give directions to F, so that, at the end, F is able
to reproduce G?s route on her map. The MapTask
corpus is composed of 128 dialogues, for a total of
1,835 unique words and 27,084 DAs. It has been
tagged at various levels, from POS to disfluencies,
from syntax to DAs. The MapTask coding scheme
uses 13 DAs (called moves), that include: Instruct
(a request that the partner carry out an action), Ex-
plain (one of the partners states some information
that was not explicitly elicited by the other), Query-
yn/-w, Acknowledge, Reply-y/-n/-w and others. The
MapTask corpus is also tagged for games as defined
above, but differently from CallHome, 6 DAs are
identified as potential initiators of games (of course
not every initiator DA initiates a game). Finally,
transactions provide the subdialogue structure of a
dialogue; each is built of several dialogue games
and corresponds to one step of the task.
DIAG-NLP is a corpus of computer mediated tu-
toring dialogues between a tutor and a student who
is diagnosing a fault in a mechanical system with a
tutoring system built with the DIAG authoring tool
(Towne, 1997). The student?s input is via menu, the
tutor is in a different room and answers via a text
window. The DIAG-NLP corpus comprises 23 ?dia-
logues? for a total of 607 unique words and 660 DAs
(it is thus much smaller than the other two). It has
been annotated for a variety of features, including
four DAs3 (Glass et al, 2002): problem solving, the
tutor gives problem solving directions; judgment,
the tutor evaluates the student?s actions or diagno-
sis; domain knowledge, the tutor imparts domain
knowledge; and other, when none of the previous
three applies. Other features encode domain objects
and their properties, and Consult Type, the type of
student query.
4 Results
Table 4 reports the results we obtained for each cor-
pus and method (to train and evaluate each method,
we used 5-fold cross-validation). We include the
baseline, computed as picking the most frequent DA
3They should be more appropriately termed tutor moves.
(Doc 1) (Doc 2) (Doc 3) (Doc 4) (Doc 5) (Doc 6) (Doc 7)
Dim. 1 1.3076 0.4717 0.1529 1.6668 1.1737 0.1193 0.9101
Dim. 2 1.5991 0.6797 0.0958 -1.3697 -0.4771 0.2844 0.4205
Table 2: The reduced 2-dimensional matrix W?
(Doc 1) (Doc 2) (Doc 3) (Doc 4) (Doc 5) (Doc 6) (Doc 7)
do 1 1 0 0 0 0 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
right 0 0 0 0 0 1 0
<Giver> 1 0 1 1 0 1 0
<Follower> 0 1 0 0 1 0 1
Table 3: Word-document matrix W augmented with speaker identity
in each corpus;4 the accuracy for LSA; the best ac-
curacy for FLSA, and with what combination of
features it was obtained; the best published result,
taken from (Ries, 1999) and from (Lager and Zi-
novjeva, 1999) respectively for CallHome and for
MapTask. Finally, for both LSA and FLSA, Table 4
includes, in parenthesis, the dimension k of the re-
duced semantic space. For each LSA method and
corpus, we experimented with values of k between
25 and 350. The values of k that give us the best re-
suls for each method were thus selected empirically.
In all cases, we can see that LSA performs
much better than baseline. Moreover, we can see
that FLSA further improves performance, dramati-
cally in the case of MapTask. FLSA reduces error
rates between 60% and 78%, for all corpora other
than DIAG-NLP (all differences in performance be-
tween LSA and FLSA are significant, other than for
DIAG-NLP). DIAG-NLP may be too small a cor-
pus to train FLSA; or Consult Type may not be ef-
fective, but it was the only feature appropriate for
FLSA (Sec. 5 discusses how we chose appropriate
features). Another extension to LSA we developed,
Clustered LSA, did give an improvement in perfor-
mance for DIAG (79.24%) ? please see (Serafin,
2003).
As regards comparable approaches, the perfor-
mance of FLSA is as good or better. For Span-
ish CallHome, (Ries, 1999) reports 76.2% accu-
racy with a hybrid approach that couples Neural
Networks and ngram backoff modeling; the former
uses prosodic features and POS tags, and interest-
ingly works best with unigram backoff modeling,
i.e., without taking into account the DA history ? see
our discussion of the ineffectiveness of the DA his-
tory below. However, (Ries, 1999) does not mention
4The baselines for CallHome37 and CallHome10 are the
same because in both statement is the most frequent DA.
his target classification, and the reported baseline of
picking the most frequent DA appears compatible
with both CallHome37 and CallHome10.5 Thus,
our results with FLSA are slightly worse (- 1.33%)
or better (+ 2.68%) than Ries?, depending on the
target classification. On MapTask, (Lager and Zi-
novjeva, 1999) achieves 62.1% with Transformation
Based Learning using single words, bigrams, word
position within the utterance, previous DA, speaker
and change of speaker. We achieve much better per-
formance on MapTask with a number of our FLSA
models.
As regards results on DA classification for other
corpora, the best performances obtained are up to
75% for task-oriented dialogues such as Verbmobil
(Samuel et al, 1998). (Stolcke et al, 2000) reports
an impressive 71% accuracy on transcribed Switch-
board dialogues, using a tag set of 42 DAs. These
are unrestricted English telephone conversations be-
tween two strangers that discuss a general interest
topic. The DA classification task appears more diffi-
cult for corpora such as Switchboard and CallHome
Spanish, that cannot benefit from the regularities
imposed on the dialogue by a specific task. (Stolcke
et al, 2000) employs a combination of HMM, neu-
ral networks and decision trees trained on all avail-
able features (words, prosody, sequence of DAs and
speaker identity).
Table 5 reports a breakdown of the experimental
results obtained with FLSA for the three tasks for
which it was successful (Table 5 does not include
k, which is always 25 for CallHome37 and Call-
Home10, and varies between 25 and 75 for Map-
Task). For each corpus, under the line we find re-
sults that are significantly better than those obtained
with LSA. For MapTask, the first 4 results that are
5An inquiry to clarify this issue went unanswered.
Corpus Baseline LSA FLSA Features Best known result
CallHome37 42.68% 65.36% (k = 50) 74.87% (k = 25) Game + Initiative 76.20%
CallHome10 42.68% 68.91% (k = 25) 78.88% (k = 25) Game + Initiative 76.20%
MapTask 20.69% 42.77% (k = 75) 73.91% (k = 25) Game + Speaker 62.10%
DIAG-NLP 43.64% 75.73% (k = 50) 74.81% (k = 50) Consult Type n.a.
Table 4: Accuracy for LSA and FLSA
Corpus accuracy Features
CallHome37 62.58% Previous DA
CallHome37 71.08% Initiative
CallHome37 72.69% Game
CallHome37 74.87% Game+Initiative
CallHome10 68.32% Previous DA
CallHome10 73.97% Initiative
CallHome10 76.52% Game
CallHome10 78.88% Game+Initiative
MapTask 41.84% SRule
MapTask 43.28% POS
MapTask 43.59% Duration
MapTask 46.91% Speaker
MapTask 47.09% Previous DA
MapTask 66.00% Game
MapTask 69.37% Game+Prev. DA
MapTask 73.25% Game+Speaker+Prev. DA
MapTask 73.91% Game+Speaker
Table 5: FLSA Accuracy
better than LSA (from POS to Previous DA) are still
pretty low; there is a difference of 19% in perfor-
mance for FLSA when Previous DA is added and
when Game is added.
Analysis. A few general conclusions can be
drawn from Table 5, as they apply in all three cases.
First, using the previous DA does not help, either
at all (CallHome37 and CallHome10), or very lit-
tle (MapTask). Increasing the length of the dialogue
history does not improve performance. In other ex-
periments, we increased the length up to n = 4:
we found that the higher n, the worse the perfor-
mance. As we will see in Section 5, introducing
any new feature results in a larger and sparser initial
matrix, which makes the task harder for FLSA; to
be effective, the amount of information provided by
the new feature must be sufficient to overcome this
handicap. It is clear that, the longer the dialogue his-
tory, the sparser the initial matrix becomes, which
explains why performance decreases. However, this
does not explain why using even only the previous
DA does not help. This implies that the previous
DA does not provide a lot of information, as in fact
is shown numerically in Section 5. This is surpris-
ing because the DA history is usually considered an
important determinant of the current DA (but (Ries,
1999) observed the same).
Second, the notion of Game appears to be really
powerful, as it vastly improves performance on two
very different corpora such as CallHome and Map-
Task.6 We will come back to discussing the usage
of Game in a real dialogue system in Section 6.
Third, the syntactic features we had access to do
not seem to improve performance (they were avail-
able only for MapTask). In MapTask SRule indi-
cates the main structure of the utterance, such as
Declarative or Wh-question. It is not surprising that
SRule did not help, since it is well known that syn-
tactic form is not predictive of DAs, especially those
of indirect speech act flavor (Searle, 1975). POS
tags don?t help LSA either, as has already been ob-
served by (Wiemer-Hastings, 2001; Kanejiya et al,
2003) for other tasks. The likely reason is that it is
necessary to add a different ?word? for each distinct
pair word-POS, e.g., route becomes split as route-
NN and route-VB. This makes the Word-Document
matrix much sparser: for MapTask, the number of
rows increases from 1,835 for plain LSA to 2,324
for FLSA.
These negative results on adding syntactic infor-
mation to LSA may just reinforce one of the claims
of the LSA proponents, that structural information
is irrelevant for determining meaning (Landauer and
Dumais, 1997). Alternatively, syntactic information
may need to be added to LSA in different ways.
(Wiemer-Hastings, 2001) discusses applying LSA
to each syntactic component of the sentence (sub-
ject, verb, rest of sentence), and averaging out those
three measures to obtain a final similarity measure.
The results are better than with plain LSA. (Kintsch,
2001) proposes an algorithm that successfully dif-
ferentiates the senses of predicates on the basis on
their arguments, in which items of the semantic
neighborhood of a predicate that are relevant to an
argument are combined with the [LSA] predicate
vector ... through a spreading activation process.
6Using Game in MapTask does not introduce circularity,
even if a game is identified by its initiating DA. We checked
the matching rates for initiating and non initiating DAs with
the FLSA model which employs Game + Speaker: they are
78.12% and 71.67% respectively. Hence, even if Game makes
initiating moves easier to classify, it is highly beneficial for the
classification of non initiating moves as well.
5 How to select features for FLSA
An important issue is how to select features for
FLSA. One possible answer is to exhaustively train
every FLSA model that corresponds to one possible
feature combination. The problem is that training
LSA models is in general time consuming. For ex-
ample, training each FLSA model on CallHome37
takes about 35 minutes of CPU time, and on Map-
Task 17 minutes, on computers with one Pentium
1.7Ghz processor and 1Gb of memory. Thus, it
would be better to focus only on the most promis-
ing models, especially when the number of features
is high, because of the exponential number of com-
binations. In this work, we trained FLSA on each
individual feature. Then, we trained FLSA on each
feature combinations that we expected to be effec-
tive, either because of the good performances of
each individual feature, or because they include fea-
tures that are deemed predictive of DAs, such as the
previous DA(s), even if they did not perform well
individually.
After we ran our experiments, we performed a
post hoc analysis based on the notion of Informa-
tion Gain (IG) from decision tree learning (Quinlan,
1993). One approach to choosing the next feature
to add to the tree at each iteration is to pick the one
with the highest IG. Suppose the data set S is classi-
fied using n categories v1...vn, each with probabil-
ity pi. S?s entropy H can be seen as an indicator of
how uncertain the outcome of the classification is,
and is given by:
H(S) = ?
n?
i=1
pilog2(pi) (1)
If feature F divides S into k subsets S1...Sk, then
IG is the expected reduction in entropy caused by
partitioning the data according to the values of F :
IG(S, A) = H(S)?
k?
i=1
|Si|
|S|
H(Si) (2)
In our case, we first computed the entropy of the
corpora with respect to the classification induced
by the DA tags (see Table 6, which also includes
the LSA accuracy for convenience). Then, we com-
puted the IG of the features or feature combinations
we used in the FLSA experiments.
Table 7 reports the IG for most of the features
from Table 5; it is ordered by FLSA performance.
On the whole, IG appears to be a reasonably accu-
rate predictor of performance. When a feature or
feature combination has a high IG, e.g. over 1, there
Corpus Entropy LSA
CallHome37 3.004 65.36%
CallHome10 2.51 68.91%
MapTask 3.38 42.77%
Table 6: Entropy measures
Corpus Features IG FLSA
CallHome37 Previous DA 0.21 62.58%
CallHome37 Initiative 0.69 71.08%
CallHome37 Game 0.59 72.69%
CallHome37 Game+Initiative 1.09 74.87%
CallHome10 Previous DA 0.13 68.32%
CallHome10 Initiative 0.53 73.97%
CallHome10 Game 0.53 76.52%
CallHome10 Game+Initiative 1.01 78.88%
MapTask Duration 0.54 43.59%
MapTask Speaker 0.31 46.91%
MapTask Prev. DA 0.58 47.09%
MapTask Game 1.21 66.00%
MapTask Game+Speaker+Prev. DA 2.04 73.25%
MapTask Game+Speaker 1.62 73.91%
Table 7: Information gain for FLSA
is also a high performance improvement. Occasion-
ally, if the IG is small this does not hold. For exam-
ple, using the previous DA reduces the entropy by
0.21 for CallHome37, but performance actually de-
creases. Most likely, the amount of new information
introduced is rather low and it is overcome by hav-
ing a larger and sparser initial matrix, which makes
the task harder for FLSA. Also, when performance
improves it does not necessarily increase linearly
with IG (see e.g. Game + Speaker + Previous DA
and Game + Speaker for MapTask). Nevertheless,
IG can be effectively used to weed out unpromising
features, or to rank feature combinations so that the
most promising FLSA models can be trained first.
6 Discussion and future work
In this paper, we have presented a novel extension
to LSA, that we have called Feature LSA. Our work
is the first to show that FLSA is more effective than
LSA, at least for the specific task we worked on, DA
classification. In parallel, we have shown that FLSA
can be effectively used to train a DA classifier. We
have reached performances comparable to or better
than published results on DA classification, and we
have used an easily trainable method.
FLSA also highlights the effectiveness of other
dialogue related features, such as Game, to classify
DAs. The drawback of features such as Game is that
Corpus FLSA
CallHome37 0.676
CallHome10 0.721
MapTask 0.740
Table 8: ? measures of agreement
a dialogue system may not have them at its disposal
when doing DA classification in real time. How-
ever, this problem may be circumvented. The num-
ber of different games is in general rather low (8 in
CallHome Spanish, 6 in MapTask), and the game
label is constant across DAs belonging to the same
game. Each DA can be classified by augmenting it
with each possible game label, and by choosing the
most accurate match among those returned by each
of these classification attempts. Further, if the sys-
tem can reliably recognize the end of a game, the
method just described needs to be used only for the
first DA of each game. Then, the game label that
gives the best result becomes the game label used
for the next few DAs, until the end of the current
game is detected.
Another reason why we advocate FLSA over
other approaches is that it appears to be close to hu-
man performance for DA classification, in the same
way that LSA approximates well many aspects of
human competence / performance (Landauer and
Dumais, 1997).
To support this claim, first, we used the ? coef-
ficient (Krippendorff, 1980; Carletta, 1996) to as-
sess the agreement between the classification made
by FLSA and the classification from the corpora ?
see Table 8. A general rule of thumb on how to
interpret the values of ? (Krippendorff, 1980) is to
require a value of ? ? 0.8, with 0.67 < ? < 0.8
allowing tentative conclusions to be drawn. As a
whole, Table 8 shows that FLSA achieves a satis-
fying level of agreement with human coders. To
put Table 8 in perspective, note that expert human
coders achieved ? = 0.83 on DA classification for
MapTask, but also had available the speech source
(Carletta et al, 1997).
We also compared the confusion matrix from
(Carletta et al, 1997) with the confusion matrix
we obtained for our best result on MapTask (FLSA
using Game + Speaker). For humans, the largest
sources of confusion are between: check and query-
yn; instruct and clarify; and acknowledge, reply-y
and ready. Likewise, our FLSA method makes the
most mistakes when distinguishing between instruct
and clarify; and acknowledge, reply-y, and ready.
Instead it performs better than humans on distin-
guishing check and query-yn. Thus, most of the
sources of confusion for humans are the same as for
FLSA.
Future work includes further investigating how to
select promising feature combinations, e.g. by using
logical regression.
We are also exploring whether FLSA can be used
as the basis for semi-automatic annotation of dia-
logue acts, to be incorporated into MUP, an annota-
tion tool we have developed (Glass and Di Eugenio,
2002). The problem is that large corpora are nec-
essary to train methods based on LSA. This would
seem to defeat the purpose of using FLSA as the ba-
sis for semi-automatic dialogue annotation, since, to
train FLSA in a new domain, we would need a large
hand annotated corpus to start with. Co-training
(Blum and Mitchell, 1998) may offer a solution to
this problem. In co-training, two different classi-
fiers are initially trained on a small set of annotated
data, by using different features. Afterwards, each
classifier is allowed to label some unlabelled data,
and picks its most confidently predicted positive and
negative examples; this data is added to the anno-
tated data. The process repeats until the desired per-
fomance is achieved. In our scenario, we will ex-
periment with training two different FLSA models,
or one FLSA model and a different classifier, such
as a naive Bayes classifier, on a small portion of an-
notated data that includes features like DAs, Game,
etc. We will then proceed as described on the unla-
belled data.
Finally, we have started applying FLSA to a dif-
ferent problem, that of judging the coherence of
texts. Whereas LSA has been already successfully
applied to this task (Foltz et al, 1998), the issue is
whether FLSA could perform better by also taking
into account those features of a text that enhance
its coherence for humans, such as appropriate cue
words.
Acknowledgments
This work is supported by grant N00014-00-1-0640 from
the Office of Naval Research, and in part, by award
0133123 from the National Science Foundation. Thanks
to Michael Glass for initially suggesting extending LSA
with features and to HCRC (University of Edinburgh) for
sharing their annotated MapTask corpus. The work was
performed while the first author was at the University of
Illinois in Chicago.
References
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training.
In COLT98, Proceedings of the Conference on
Computational Learning Theory.
Jean Carletta, Amy Isard, Stephen Isard, Jacque-
line C. Kowtko, Gwyneth Doherty-Sneddon, and
Anne H. Anderson. 1997. The reliability of a di-
alogue structure coding scheme. Computational
Lingustics, 23(1):13?31.
Jean Carletta. 1996. Assessing agreement on clas-
sification tasks: the Kappa statistic. Computa-
tional Linguistics, 22(2):249?254.
Peter W. Foltz, Walter Kintsch, and Thomas K. Lan-
dauer. 1998. The measurement of textual coher-
ence with Latent Semantic Analysis. Discourse
Processes, 25:285?308.
Peter W. Foltz, Darrell Laham, and Thomas K.
Landauer. 1999. The intelligent essay assessor:
Applications to educational technology. Interac-
tive Multimedia Electronic Journal of Computer-
Enhanced Learning, 1(2).
Michael Glass and Barbara Di Eugenio. 2002.
MUP: The UIC standoff markup tool. In The
Third SigDIAL Workshop on Discourse and Di-
alogue, Philadelphia, PA, July.
Michael Glass, Heena Raval, Barbara Di Eugenio,
and Maarika Traat. 2002. The DIAG-NLP dia-
logues: coding manual. Technical Report UIC-
CS 02-03, University of Illinois - Chicago.
Dharmendra Kanejiya, Arun Kumar, and Surendra
Prasad. 2003. Automatic Evaluation of Students?
Answers using Syntactically Enhanced LSA. In
HLT-NAACL Workshop on Building Educational
Applications using Natural Language Process-
ing, pages 53?60, Edmonton, Canada.
Walter Kintsch. 2001. Predication. Cognitive Sci-
ence, 25:173?202.
Klaus Krippendorff. 1980. Content Analysis: an
Introduction to its Methodology. Sage Publica-
tions, Beverly Hills, CA.
T. Lager and N. Zinovjeva. 1999. Training a dia-
logue act tagger with the ?-TBL system. In The
Third Swedish Symposium on Multimodal Com-
munication, Linko?ping University Natural Lan-
guage Processing Laboratory (NLPLAB).
Thomas K. Landauer and S.T. Dumais. 1997. A
solution to Plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104:211?240.
Lori Levin, Ann Thyme?-Gobbel, Alon Lavie, Klaus
Ries, and Klaus Zechner. 1998. A discourse cod-
ing scheme for conversational Spanish. In Pro-
ceedings ICSLP.
J. Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann.
Klaus Ries. 1999. HMM and Neural Network
Based Speech Act Detection. In Proceedings of
ICASSP 99, Phoenix, Arizona, March.
Ken Samuel, Sandra Carberry, and K. Vijay-
Shanker. 1998. Dialogue act tagging with
transformation-based learning. In ACL/COLING
98, Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics
(joint with the 17th International Conference on
Computational Linguistics), pages 1150?1156.
John R. Searle. 1975. Indirect Speech Acts.
In P. Cole and J.L. Morgan, editors, Syntax
and Semantics 3. Speech Acts. Academic Press.
Reprinted in Pragmatics. A Reader, Steven Davis
editor, Oxford University Press, 1991.
Riccardo Serafin. 2003. Feature Latent Semantic
Analysis for dialogue act interpretation. Master?s
thesis, University of Illinois - Chicago.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg,
R. Bates, D. Jurafsky, P. Taylor, R. Martin, C. Van
Ess-Dykema, and M. Meteer. 2000. Dialogue
act modeling for automatic tagging and recog-
nition of conversational speech. Computational
Linguistics, 26(3):339?373.
Douglas M. Towne. 1997. Approximate reasoning
techniques for intelligent diagnostic instruction.
International Journal of Artificial Intelligence in
Education.
Peter Wiemer-Hastings. 2001. Rules for syntax,
vectors for semantics. In CogSci01, Proceedings
of the Twenty-Third Annual Meeting of the Cog-
nitive Science Society, Edinburgh, Scotland.
Proceedings of the 43rd Annual Meeting of the ACL, pages 50?57,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Aggregation improves learning:
experiments in natural language generation for intelligent tutoring systems
Barbara Di Eugenio and Davide Fossati and Dan Yu
University of Illinois
Chicago, IL, 60607, USA
{bdieugen,dfossa1,dyu6}@uic.edu
Susan Haller
University of Wisconsin - Parkside
Kenosha, WI 53141, USA
haller@cs.uic.edu
Michael Glass
Valparaiso University
Valparaiso, IN, 46383, USA
Michael.Glass@valpo.edu
Abstract
To improve the interaction between students
and an intelligent tutoring system, we devel-
oped two Natural Language generators, that we
systematically evaluated in a three way com-
parison that included the original system as
well. We found that the generator which intu-
itively produces the best language does engen-
der the most learning. Specifically, it appears
that functional aggregation is responsible for
the improvement.
1 Introduction
The work we present in this paper addresses three
issues: evaluation of Natural Language Generation
(NLG) systems, the place of aggregation in NLG,
and NL interfaces for Intelligent Tutoring Systems.
NLG systems have been evaluated in various
ways, such as via task efficacy measures, i.e., mea-
suring how well the users of the system perform on
the task at hand (Young, 1999; Carenini and Moore,
2000; Reiter et al, 2003). We also employed task
efficacy, as we evaluated the learning that occurs
in students interacting with an Intelligent Tutoring
System (ITS) enhanced with NLG capabilities. We
focused on sentence planning, and specifically, on
aggregation. We developed two different feedback
generation engines, that we systematically evaluated
in a three way comparison that included the orig-
inal system as well. Our work is novel for NLG
evaluation in that we focus on one specific com-
ponent of the NLG process, aggregation. Aggrega-
tion pertains to combining two or more of the mes-
sages to be communicated into one sentence (Reiter
and Dale, 2000). Whereas it is considered an es-
sential task of an NLG system, its specific contri-
butions to the effectiveness of the text that is even-
tually produced have rarely been assessed (Harvey
and Carberry, 1998). We found that syntactic aggre-
gation does not improve learning, but that what we
call functional aggregation does. Further, we ran a
controlled data collection in order to provide a more
solid empirical base for aggregation rules than what
is normally found in the literature, e.g. (Dalianis,
1996; Shaw, 2002).
As regards NL interfaces for ITSs, research on the
next generation of ITSs (Evens et al, 1993; Litman
et al, 2004; Graesser et al, 2005) explores NL as
one of the keys to bridging the gap between cur-
rent ITSs and human tutors. However, it is still not
known whether the NL interaction between students
and an ITS does in fact improve learning. We are
among the first to show that this is the case.
We will first discuss DIAG, the ITS shell we are
using, and the two feedback generators that we de-
veloped, DIAG-NLP1and DIAG-NLP2 . Since the
latter is based on a corpus study, we will briefly de-
scribe that as well. We will then discuss the formal
evaluation we conducted and our results.
2 Natural Language Generation for DIAG
DIAG (Towne, 1997) is a shell to build ITSs based
on interactive graphical models that teach students to
troubleshoot complex systems such as home heating
and circuitry. A DIAG application presents a student
with a series of troubleshooting problems of increas-
ing difficulty. The student tests indicators and tries
to infer which faulty part (RU) may cause the abnor-
mal states detected via the indicator readings. RU
stands for replaceable unit, because the only course
of action for the student to fix the problem is to re-
place faulty components in the graphical simulation.
50
Figure 1: The furnace system
Fig. 1 shows the furnace, one subsystem of the home
heating system in our DIAG application. Fig. 1 in-
cludes indicators such as the gauge labeled Water
Temperature, RUs, and complex modules (e.g., the
Oil Burner) that contain indicators and RUs. Com-
plex components are zoomable.
At any point, the student can consult the tutor
via the Consult menu (cf. the Consult button in
Fig. 1). There are two main types of queries: Con-
sultInd(icator) and ConsultRU. ConsultInd queries
are used mainly when an indicator shows an ab-
normal reading, to obtain a hint regarding which
RUs may cause the problem. DIAG discusses the
RUs that should be most suspected given the symp-
toms the student has already observed. ConsultRU
queries are mainly used to obtain feedback on the di-
agnosis that a certain RU is faulty. DIAG responds
with an assessment of that diagnosis and provides
evidence for it in terms of the symptoms that have
been observed relative to that RU.
The original DIAG system (DIAG-orig) uses very
simple templates to assemble the text to present to
the student. The top parts of Figs. 2 and 3 show the
replies provided by DIAG-orig to a ConsultInd on
the Visual Combustion Check, and to a ConsultRu
on the Water Pump.
The highly repetitive feedback by DIAG-orig
screams for improvements based on aggregation
techniques. Our goal in developing DIAG-NLP1
and DIAG-NLP2 was to assess whether simple,
rapidly deployable NLG techniques would lead to
measurable improvements in the student?s learning.
Thus, in both cases it is still DIAG that performs
content determination, and provides to DIAG-NLP1
and DIAG-NLP2 a file in which the facts to be com-
municated are written ? a fact is the basic unit of
information that underlies each of the clauses in a
reply by DIAG-orig . The only way we altered the
interaction between student and system is the ac-
tual language that is presented in the output win-
dow. In DIAG-NLP1 we mostly explored using syn-
tactic aggregation to improve the feedback, whereas
DIAG-NLP2 is corpus-based and focuses on func-
tional aggregation. In both DIAG-NLP1 and DIAG-
NLP2 , we use EXEMPLARS (White and Cald-
well, 1998), an object-oriented, rule-based genera-
tor. The rules (called exemplars) are meant to cap-
ture an exemplary way of achieving a communica-
tive goal in a given context. EXEMPLARS selects
rules by traversing the exemplar specialization hi-
erarchy and evaluating the applicability conditions
associated with each exemplar.
The visual combustion check is igniting which is abnormal
(normal is combusting).
Oil Nozzle always
produces this abnormality when it fails.
Oil Supply Valve always
produces this abnormality when it fails.
Oil pump always
produces this abnormality when it fails.
Oil Filter always
produces this abnormality when it fails.
System Control Module sometimes
produces this abnormality when it fails.
Ignitor Assembly never
produces this abnormality when it fails.
Burner Motor always
produces this abnormality when it fails.
The visual combustion check indicator is igniting.
This is abnormal.
Normal is combusting.
Within the furnace system,
this is sometimes caused if
the System Control Module has failed.
Within the Oil Burner
this is never caused if
the Ignitor Assembly has failed.
In contrast, this is always caused if
the Burner Motor, Oil Filter, Oil Pump,
Oil Supply Valve, or Oil Nozzle has failed.
The combustion is abnormal.
In the oil burner, check the units along the path of the oil and
the burner motor.
Figure 2: Answers to ConsultInd by DIAG-orig ,
DIAG-NLP1and DIAG-NLP2
51
Water pump is a very poor suspect.
Some symptoms you have seen conflict with that theory.
Water pump sound was normal.
This normal indication never results when this unit fails.
Visual combustion check was igniting.
This abnormal indication never results when this unit fails.
Burner Motor RMP Gauge was 525.
This normal indication never results when this unit fails.
The Water pump is a very poor suspect.
Some symptoms you have seen conflict with that theory.
The following indicators never display normally
when this unit fails.
Within the furnace system,
the Burner Motor RMP Gauge is 525.
Within the water pump and safety cutoff valve,
the water pump sound indicator is normal.
The following indicators never display abnormally
when this unit fails.
Within the fire door sight hole,
the visual combustion check indicator is igniting.
The water pump is a poor suspect since the water pump
sound is ok.
You have seen that the combustion is abnormal.
Check the units along the path of the oil and the electrical
devices.
Figure 3: Answers to ConsultRu by DIAG-orig ,
DIAG-NLP1 and DIAG-NLP2
2.1 DIAG-NLP1 : Syntactic aggregation
DIAG-NLP1 1 (i) introduces syntactic aggregation
(Dalianis, 1996; Huang and Fiedler, 1996; Reape
and Mellish, 1998; Shaw, 2002) and what we call
structural aggregation, namely, grouping parts ac-
cording to the structure of the system; (ii) gener-
ates some referring expressions; (iii) models a few
rhetorical relations; and (iv) improves the format of
the output.
The middle parts of Figs. 2 and 3 show the revised
output produced by DIAG-NLP1 . E.g., in Fig. 2 the
RUs of interest are grouped by the system modules
that contain them (Oil Burner and Furnace System),
and by the likelihood that a certain RU causes the
observed symptoms. In contrast to the original an-
swer, the revised answer highlights that the Ignitor
Assembly cannot cause the symptom.
In DIAG-NLP1 , EXEMPLARS accesses the
SNePS Knowledge Representation and Reasoning
System (Shapiro, 2000) for static domain informa-
tion.2 SNePS makes it easy to recognize structural
1DIAG-NLP1 actually augments and refines the first feed-
back generator we created for DIAG, DIAG-NLP0 (Di Eugenio
et al, 2002). DIAG-NLP0 only covered (i) and (iv).
2In DIAG, domain knowledge is hidden and hardly acces-
similarities and use shared structures. Using SNePS,
we can examine the dimensional structure of an ag-
gregation and its values to give preference to aggre-
gations with top-level dimensions that have fewer
values, to give summary statements when a dimen-
sion has many values that are reported on, and to
introduce simple text structuring in terms of rhetor-
ical relations, inserting relations like contrast and
concession to highlight distinctions between dimen-
sional values (see Fig. 2, middle).
DIAG-NLP1 uses the GNOME algorithm (Kib-
ble and Power, 2000) to generate referential expres-
sions. Importantly, using SNePS propositions can
be treated as discourse entities, added to the dis-
course model and referred to (see This is ... caused
if ... in Fig. 2, middle). Information about lexical
realization, and choice of referring expression is en-
coded in the appropriate exemplars.
2.2 DIAG-NLP2 : functional aggregation
In the interest of rapid prototyping, DIAG-NLP1
was implemented without the benefit of a corpus
study. DIAG-NLP2 is the empirically grounded
version of the feedback generator. We collected
23 tutoring interactions between a student using the
DIAG tutor on home heating and two human tutors,
for a total of 272 tutor turns, of which 235 in re-
ply to ConsultRU and 37 in reply to ConsultInd (the
type of student query is automatically logged). The
tutor and the student are in different rooms, sharing
images of the same DIAG tutoring screen. When
the student consults DIAG, the tutor sees, in tabular
form, the information that DIAG would use in gen-
erating its advice ? the same ?fact file? that DIAG
gives to DIAG-NLP1and DIAG-NLP2? and types
a response that substitutes for DIAG?s. The tutor is
presented with this information because we wanted
to uncover empirical evidence for aggregation rules
in our domain. Although we cannot constrain the tu-
tor to mention only the facts that DIAG would have
communicated, we can analyze how the tutor uses
the information provided by DIAG.
We developed a coding scheme (Glass et al,
2002) and annotated the data. As the annotation was
performed by a single coder, we lack measures of
intercoder reliability. Thus, what follows should be
taken as observations rather than as rigorous find-
ings ? useful observations they clearly are, since
sible. Thus, in both DIAG-NLP1 and DIAG-NLP2 we had to
build a small knowledge base that contains domain knowledge.
52
DIAG-NLP2 is based on these observations and its
language fosters the most learning.
Our coding scheme focuses on four areas. Fig. 4
shows examples of some of the tags (the SCM is the
System Control Module). Each tag has from one to
five additional attributes (not shown) that need to be
annotated too.
Domain ontology. We tag objects in the domain
with their class indicator, RU and their states, de-
noted by indication and operationality, respectively.
Tutoring actions. They include (i) Judgment. The
tutor evaluates what the student did. (ii) Problem
solving. The tutor suggests the next course of ac-
tion. (iii) The tutor imparts Domain Knowledge.
Aggregation. Objects may be functional aggre-
gates, such as the oil burner, which is a system com-
ponent that includes other components; linguistic
aggregates, which include plurals and conjunctions;
or a summary over several unspecified indicators or
RUs. Functional/linguistic aggregate and summary
tags often co-occur, as shown in Fig. 4.
Relation to DIAG?s output. Contrary to all other
tags, in this case we annotate the input that DIAG
gave the tutor. We tag its portions as included / ex-
cluded / contradicted, according to how it has been
dealt with by the tutor.
Tutors provide explicit problem solving directions
in 73% of the replies, and evaluate the student?s ac-
tion in 45% of the replies (clearly, they do both in
28% of the replies, as in Fig. 4). As expected, they
are much more concise than DIAG, e.g., they never
mention RUs that cannot or are not as likely to cause
a certain problem, such as, respectively, the ignitor
assembly and the SCM in Fig. 2.
As regards aggregation, 101 out of 551 RUs, i.e.
18%, are labelled as summary; 38 out of 193 indica-
tors, i.e. 20%, are labelled as summary. These per-
centages, though seemingly low, represent a consid-
erable amount of aggregation, since in our domain
some items have very little in common with others,
and hence cannot be aggregated. Further, tutors ag-
gregate parts functionally rather than syntactically.
For example, the same assemblage of parts, i.e., oil
nozzle, supply valve, pump, filter, etc., can be de-
scribed as the other items on the fuel line or as the
path of the oil flow.
Finally, directness ? an attribute on the indica-
tor tag ? encodes whether the tutor explicitly talks
about the indicator (e.g., The water temperature
gauge reading is low), or implicitly via the object
to which the indicator refers (e.g., the water is too
cold). 110 out of 193 indicators, i.e. 57%, are
marked as implicit, 45, i.e. 41%, as explicit, and 2%
are not marked for directness (the coder was free to
leave attributes unmarked). This, and the 137 occur-
rences of indication, prompted us to refer to objects
and their states, rather than to indicators (as imple-
mented by Steps 2 in Fig. 5, and 2(b)i, 3(b)i, 3(c)i in
Fig. 6, which generate The combustion is abnormal
and The water pump sound is OK in Figs. 2 and 3).
2.3 Feedback Generation in DIAG-NLP2
In DIAG-NLP1 the fact file provided by DIAG is
directly processed by EXEMPLARS. In contrast, in
DIAG-NLP2 a planning module manipulates the in-
formation before passing it to EXEMPLARS. This
module decides which information to include ac-
cording to the type of query the system is respond-
ing to, and produces one or more Sentence Structure
objects. These are then passed to EXEMPLARS
that transforms them into Deep Syntactic Structures.
Then, a sentence realizer, RealPro (Lavoie and Ram-
bow, 1997), transforms them into English sentences.
Figs. 5 and 6 show the control flow in DIAG-
NLP2 for feedback generation for ConsultInd and
ConsultRU. Step 3a in Fig. 5 chooses, among all
the RUs that DIAG would talk about, only those
that would definitely result in the observed symp-
tom. Step 2 in the AGGREGATE procedure in Fig. 5
uses a simple heuristic to decide whether and how to
use functional aggregation. For each RU, its possi-
ble aggregators and the number n of units it covers
are listed in a table (e.g., electrical devices covers
4 RUs, ignitor, photoelectric cell, transformer and
burner motor). If a group of REL-RUs contains k
units that a certain aggregator Agg covers, if k < n2 ,
Agg will not be used; if n2 ? k < n, Agg preceded
by some of will be used; if k = n, Agg will be used.
DIAG-NLP2 does not use SNePS, but a relational
database storing relations, such as the ISA hierarchy
(e.g., burner motor IS-A RU), information about ref-
erents of indicators (e.g., room temperature gauge
REFERS-TO room), and correlations between RUs
and the indicators they affect.
3 Evaluation
Our empirical evaluation is a three group, between-
subject study: one group interacts with DIAG-orig ,
53
[judgment [replaceable?unit the ignitor] is a poor suspect] since [indication combustion is working] during startup. The problem is
that the SCM is shutting the system off during heating.
[domain?knowledge The SCM reads [summary [linguistic?aggregate input signals from sensors]] and uses the signals to determine
how to control the system.]
[problem?solving Check the sensors.]
Figure 4: Examples of a coded tutor reply
1. IND? queried indicator
2. Mention the referent of IND and its state
3. IF IND reads abnormal,
(a) REL-RUs? choose relevant RUs
(b) AGGR-RUs? AGGREGATE(REL-RUs)
(c) Suggest to check AGGR-RUs
AGGREGATE(RUs)
1. Partition REL-RUs into subsets by system structure
2. Apply functional aggregation to subsets
Figure 5: DIAG-NLP2 : Feedback generation for
ConsultInd
one with DIAG-NLP1 , one with DIAG-NLP2 . The
75 subjects (25 per group) were all science or engi-
neering majors affiliated with our university. Each
subject read some short material about home heat-
ing, went through one trial problem, then continued
through the curriculum on his/her own. The curricu-
lum consisted of three problems of increasing dif-
ficulty. As there was no time limit, every student
solved every problem. Reading materials and cur-
riculum were identical in the three conditions.
While a subject was interacting with the system,
a log was collected including, for each problem:
whether the problem was solved; total time, and time
spent reading feedback; how many and which in-
dicators and RUs the subject consults DIAG about;
how many, and which RUs the subject replaces. We
will refer to all the measures that were automatically
collected as performance measures.
At the end of the experiment, each subject was ad-
ministered a questionnaire divided into three parts.
The first part (the posttest) consists of three ques-
tions and tests what the student learned about the
domain. The second part concerns whether subjects
remember their actions, specifically, the RUs they
replaced. We quantify the subjects? recollections in
terms of precision and recall with respect to the log
that the system collects. We expect precision and re-
call of the replaced RUs to correlate with transfer,
namely, to predict how well a subject is able to ap-
ply what s/he learnt about diagnosing malfunctions
1. RU? queried RU
REL-IND? indicator associated to RU
2. IF RU warrants suspicion,
(a) state RU is a suspect
(b) IF student knows that REL-IND is abnormal
i. remind him of referent of REL-IND and
its abnormal state
ii. suggest to replace RU
(c) ELSE suggest to check REL-IND
3. ELSE
(a) state RU is not a suspect
(b) IF student knows that REL-IND is normal
i. use referent of REL-IND and its normal state
to justify judgment
(c) IF student knows of abnormal indicators OTHER-INDs
i. remind him of referents of OTHER-INDs
and their abnormal states
ii. FOR each OTHER-IND
A. REL-RUs? RUs associated with OTHER-IND
B. AGGR-RUs? AGGREGATE(REL-RUs)
? AGGR-RUs
iii. Suggest to check AGGR-RUs
Figure 6: DIAG-NLP2 : Feedback generation for
ConsultRU
to new problems. The third part concerns usability,
to be discussed below.
We found that subjects who used DIAG-NLP2
had significantly higher scores on the posttest, and
were significantly more correct (higher precision)
in remembering what they did. As regards perfor-
mance measures, there are no so clear cut results.
As regards usability, subjects prefer DIAG-NLP1 /2
to DIAG-orig , however results are mixed as regards
which of the two they actually prefer.
In the tables that follow, boldface indicates sig-
nificant differences, as determined by an analysis of
variance performed via ANOVA, followed by post-
hoc Tukey tests.
Table 1 reports learning measures, average across
the three problems. DIAG-NLP2 is significantly
better as regards PostTest score (F = 10.359, p =
0.000), and RU Precision (F = 4.719, p =
0.012). Performance on individual questions in the
54
DIAG-orig DIAG-NLP1 DIAG-NLP2
PostTest 0.72 0.69 0.90
RU Precision 0.78 0.70 0.91
RU Recall .53 .47 .40
Table 1: Learning Scores
Figure 7: Scores on PostTest questions
PostTest3 is illustrated in Fig. 7. Scores in DIAG-
NLP2 are always higher, significantly so on ques-
tions 2 and 3 (F = 8.481, p = 0.000, and F =
7.909, p = 0.001), and marginally so on question 1
(F = 2.774, p = 0.069).4
D-Orig D-NLP1 D-NLP2
Total Time 30?17? 28?34? 34?53?
RU Replacements 8.88 11.12 11.36
ConsultInd 22.16 6.92 28.16
Avg. Reading Time 8? 14? 2?
ConsultRU 63.52 45.68 52.12
Avg. Reading Time 5? 4? 5?
Table 2: Performance Measures
Table 2 reports performance measures, cumula-
tive across the three problems, other than average
reading times. Subjects don?t differ significantly in
the time they spend solving the problems, or in the
number of RU replacements they perform. DIAG?s
assumption (known to the subjects) is that there is
only one broken RU per problem, but the simula-
tion allows subjects to replace as many as they want
without any penalty before they come to the correct
solution. The trend on RU replacements is opposite
what we would have hoped for: when repairing a
real system, replacing parts that are working should
clearly be kept to a minimum, and subjects replace
3The three questions are: 1. Describe the main subsystems
of the furnace. 2. What is the purpose of (a) the oil pump (b)
the system control module? 3. Assume the photoelectric cell is
covered with enough soot that it could not detect combustion.
What impact would this have on the system?
4The PostTest was scored by one of the authors, following
written guidelines.
fewer parts in DIAG-orig .
The next four entries in Table 2 report the number
of queries that subjects ask, and the average time it
takes subjects to read the feedback. The subjects
ask significantly fewer ConsultInd in DIAG-NLP1
(F = 8.905, p = 0.000), and take significantly less
time reading ConsultInd feedback in DIAG-NLP2
(F = 15.266, p = 0.000). The latter result is
not surprising, since the feedback in DIAG-NLP2 is
much shorter than in DIAG-orig and DIAG-NLP1 .
Neither the reason not the significance of subjects
asking many fewer ConsultInd of DIAG-NLP1 are
apparent to us ? it happens for ConsultRU as well,
to a lesser, not significant degree.
We also collected usability measures. Although
these are not usually reported in ITS evaluations,
in a real setting students should be more willing to
sit down with a system that they perceive as more
friendly and usable. Subjects rate the system along
four dimensions on a five point scale: clarity, useful-
ness, repetitiveness, and whether it ever misled them
(the scale is appropriately arranged: the highest clar-
ity but the lowest repetitiveness receive 5 points).
There are no significant differences on individual
dimensions. Cumulatively, DIAG-NLP2 (at 15.08)
slightly outperforms the other two (DIAG-orig at
14.68 and DIAG-NLP1 at 14.32), however, the dif-
ference is not significant (highest possible rating is
20 points).
prefer neutral disprefer
DIAG-NLP1 to DIAG-orig 28 5 17
DIAG-NLP2 to DIAG-orig 34 1 15
DIAG-NLP2 to DIAG-NLP1 24 1 25
Table 3: User preferences among the three systems
prefer neutral disprefer
Consult Ind. 8 1 16
Consult RU 16 0 9
Table 4: DIAG-NLP2 versus DIAG-NLP1
natural concise clear contentful
DIAG-NLP1 4 8 10 23
DIAG-NLP2 16 8 11 12
Table 5: Reasons for system preference
Finally,5 on paper, subjects compare two pairs of
versions of feedback: in each pair, the first feedback
5Subjects can also add free-form comments. Only few did
55
is generated by the system they just worked with,
the second is generated by one of the other two sys-
tems. Subjects say which version they prefer, and
why (they can judge the system along one or more
of four dimensions: natural, concise, clear, content-
ful). The first two lines in Table 3 show that subjects
prefer the NLP systems to DIAG-orig (marginally
significant, ?2 = 9.49, p < 0.1). DIAG-NLP1
and DIAG-NLP2 receive the same number of pref-
erences; however, a more detailed analysis (Table 4)
shows that subjects prefer DIAG-NLP1 for feed-
back to ConsultInd, but DIAG-NLP2 for feedback
to ConsultRu (marginally significant, ?2 = 5.6, p <
0.1). Finally, subjects find DIAG-NLP2 more nat-
ural, but DIAG-NLP1 more contentful (Table 5,
?2 = 10.66, p < 0.025).
4 Discussion and conclusions
Our work touches on three issues: aggregation, eval-
uation of NLG systems, and the role of NL inter-
faces for ITSs.
In much work on aggregation (Huang and Fiedler,
1996; Horacek, 2002), aggregation rules and heuris-
tics are shown to be plausible, but are not based on
any hard evidence. Even where corpus work is used
(Dalianis, 1996; Harvey and Carberry, 1998; Shaw,
2002), the results are not completely convincing be-
cause we do not know for certain the content to be
communicated from which these texts supposedly
have been aggregated. Therefore, positing empir-
ically based rules is guesswork at best. Our data
collection attempts at providing a more solid em-
pirical base for aggregation rules; we found that tu-
tors exclude significant amounts of factual informa-
tion, and use high degrees of aggregation based on
functionality. As a consequence, while part of our
rules implement standard types of aggregation, such
as conjunction via shared participants, we also intro-
duced functional aggregation (see conceptual aggre-
gation (Reape and Mellish, 1998)).
As regards evaluation, NLG systems have been
evaluated e.g. by using human judges to assess the
quality of the texts produced (Coch, 1996; Lester
and Porter, 1997; Harvey and Carberry, 1998); by
comparing the system?s performance to that of hu-
mans (Yeh and Mellish, 1997); or through task ef-
ficacy measures, i.e., measuring how well the users
so, and the distribution of topics and of evaluations is too broad
to be telling.
of the system perform on the task at hand (Young,
1999; Carenini and Moore, 2000; Reiter et al,
2003). The latter kind of studies generally contrast
different interventions, i.e. a baseline that does not
use NLG and one or more variations obtained by pa-
rameterizing the NLG system. However, the evalu-
ation does not focus on a specific component of the
NLG process, as we did here for aggregation.
Regarding the role of NL interfaces for ITSs, only
very recently have the first few results become avail-
able, to show that first of all, students do learn when
interacting in NL with an ITS (Litman et al, 2004;
Graesser et al, 2005). However, there are very few
studies like ours, that evaluate specific features of
the NL interaction, e.g. see (Litman et al, 2004). In
our case, we did find that different features of the NL
feedback impact learning. Although we contend that
this effect is due to functional aggregation, the feed-
back in DIAG-NLP2 changed along other dimen-
sions, mainly using referents of indicators instead of
indicators, and being more strongly directive in sug-
gesting what to do next. Of course, we cannot ar-
gue that our best NL generator is equivalent to a hu-
man tutor ? e.g., dividing the number of ConsultRU
and ConsultInd reported in Sec. 2.2 by the number
of dialogues shows that students ask about 10 Con-
sultRus and 1.5 ConsultInd per dialogue when in-
teracting with a human, many fewer than those they
pose to the ITSs (cf. Table 2) (regrettably we did not
administer a PostTest to students in the human data
collection). We further discuss the implications of
our results for NL interfaces for ITSs in a compan-
ion paper (Di Eugenio et al, 2005).
The DIAG project has come to a close. We are
satisfied that we demonstrated that even not overly
sophisticated NL feedback can make a difference;
however, the fact that DIAG-NLP2 has the best lan-
guage and engenders the most learning prompts us
to explore more complex language interactions. We
are pursuing new exciting directions in a new do-
main, that of basic data structures and algorithms.
We are investigating what distinguishes expert from
novice tutors, and we will implement our findings
in an ITS that tutors in this domain.
Acknowledgments. This work is supported by the Office
of Naval Research (awards N00014-99-1-0930 and N00014-00-
1-0640), and in part by the National Science Foundation (award
IIS 0133123). We are grateful to CoGenTex Inc. for making
EXEMPLARS and RealPro available to us.
56
References
Giuseppe Carenini and Johanna D. Moore. 2000. An em-
pirical study of the influence of argument conciseness
on argument effectiveness. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, Hong Kong.
Jose? Coch. 1996. Evaluating and comparing three text-
production techniques. In COLING96, Proceedings of
the Sixteenth International Conference on Computa-
tional Linguistics, pages 249?254
Hercules Dalianis. 1996. Concise Natural Language
Generation from Formal Specifications. Ph.D. thesis,
Department of Computer and Systems Science, Sto-
cholm University. Technical Report 96-008.
Barbara Di Eugenio, Michael Glass, and Michael J. Tro-
lio. 2002. The DIAG experiments: Natural Lan-
guage Generation for Intelligent Tutoring Systems. In
INLG02, The Third International Natural Language
Generation Conference, pages 120?127.
Barbara Di Eugenio, Davide Fossati, Dan Yu, Susan
Haller, and Michael Glass. 2005. Natural language
generation for intelligent tutoring systems: a case
study. In AIED 2005, the 12th International Confer-
ence on Artificial Intelligence in Education.
M. W. Evens, J. Spitkovsky, P. Boyle, J. A. Michael, and
A. A. Rovick. 1993. Synthesizing tutorial dialogues.
In Proceedings of the Fifteenth Annual Conference of
the Cognitive Science Society, pages 137?140.
Michael Glass, Heena Raval, Barbara Di Eugenio, and
Maarika Traat. 2002. The DIAG-NLP dialogues: cod-
ing manual. Technical Report UIC-CS 02-03, Univer-
sity of Illinois - Chicago.
A.C. Graesser, N. Person, Z. Lu, M.G. Jeon, and B. Mc-
Daniel. 2005. Learning while holding a conversation
with a computer. In L. PytlikZillig, M. Bodvarsson,
and R. Brunin, editors, Technology-based education:
Bringing researchers and practitioners together. Infor-
mation Age Publishing.
Terrence Harvey and Sandra Carberry. 1998. Inte-
grating text plans for conciseness and coherence. In
ACL/COLING 98, Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics, pages 512?518.
Helmut Horacek. 2002. Aggregation with strong regu-
larities and alternatives. In International Conference
on Natural Language Generation.
Xiaoron Huang and Armin Fiedler. 1996. Paraphrasing
and aggregating argumentative text using text struc-
ture. In Proceedings of the 8th International Workshop
on Natural Language Generation, pages 21?30.
Rodger Kibble and Richard Power. 2000. Nominal gen-
eration in GNOME and ICONOCLAST. Technical re-
port, Information Technology Research Institute, Uni-
versity of Brighton, Brighton, UK.
Beno??t Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation systems. In Pro-
ceedings of the Fifth Conference on Applied Natural
Language Processing.
James C. Lester and Bruce W. Porter. 1997. Developing
and empirically evaluating robust explanation genera-
tors: the KNIGHT experiments. Computational Lin-
guistics, 23(1):65?102.
D. J. Litman, C. P. Rose?, K. Forbes-Riley, K. VanLehn,
D. Bhembe, and S. Silliman. 2004. Spoken versus
typed human and computer dialogue tutoring. In Pro-
ceedings of the Seventh International Conference on
Intelligent Tutoring Systems, Maceio, Brazil.
Mike Reape and Chris Mellish. 1998. Just what is ag-
gregation anyway? In Proceedings of the European
Workshop on Natural Language Generation.
Ehud Reiter and Robert Dale. 2000. Building Natu-
ral Language Generation Systems. Studies in Natural
Language Processing. Cambridge University Press.
Ehud Reiter, Roma Robertson, and Liesl Osman. 2003.
Lessons from a failure: Generating tailored smoking
cessation letters. Artificial Intelligence, 144:41?58.
S. C. Shapiro. 2000. SNePS: A logic for natural lan-
guage understanding and commonsense reasoning. In
L. M. Iwanska and S. C. Shapiro, editors, Natural
Language Processing and Knowledge Representation.
AAAI Press/MIT Press.
James Shaw. 2002. A corpus-based analysis for the or-
dering of clause aggregation operators. In COLING02,
Proceedings of the 19th International Conference on
Computational Linguistics.
Douglas M. Towne. 1997. Approximate reasoning tech-
niques for intelligent diagnostic instruction. Interna-
tional Journal of Artificial Intelligence in Education.
Michael White and Ted Caldwell. 1998. Exemplars: A
practical, extensible framework for dynamic text gen-
eration. In Proceedings of the Ninth International
Workshop on Natural Language Generation, pages
266?275, Niagara-on-the-Lake, Canada.
Ching-Long Yeh and Chris Mellish. 1997. An empir-
ical study on the generation of anaphora in Chinese.
Computational Linguistics, 23(1):169?190.
R. Michael Young. 1999. Using Grice?s maxim of quan-
tity to select the content of plan descriptions. Artificial
Intelligence, 115:215?256.
57
MUP: The UIC Standoff Markup Tool
Michael Glass and Barbara Di Eugenio
CS. Dept. M/C 152, University of Illinois at Chicago
851. S. Morgan
Chicago, IL, 60607-7053
fmglass|bdieugeng@cs.uic.edu
Abstract
Recently developed markup tools for di-
alogue work are quite sophisticated and
require considerable knowledge and over-
head, but older tools do not support XML
standoff markup, the current annotation
style of choice. For the DIAG-NLP
project we have created a ?lightweight?
but modern markup tool that can be con-
figured and used by the working NLP re-
searcher.
Introduction
Speech and text corpora augmented with linguis-
tic annotations have become essential to everyday
NLP. In the realm of discourse-related annotation,
which we are interested in, linguistic annotation is
still mostly a manual effort. Thus, the availability of
coding tools that facilitate a human coder?s task has
become paramount. In this paper we present MUP,
a coding tool for standoff markup which is sophisti-
cated enough to allow for a variety of different mark-
ings to be applied, but which is also simple enough
to use that it does not require a sizable set up effort.
Other coding tools have been developed, and
some of them do in fact target discourse phenomena.
Tools specifically developed to code for discourse
phenomena include Nb (Flammia and Zue, 1995),
DAT (Allen and Core, 1997), MATE (McKelvie et
al., 2001), and the Alembic Workbench (Day et al,
1997). MUP differs from all of them because it is
standoff (contrary to Nb and DAT), allows tagging
of discontinuous constituents (contrary to Nb), and
is simple to set up and use (contrary to MATE).
We developed MUP within the DIAG-NLP
project (Di Eugenio et al, 2002), which is grafting
an NLG component onto a tutorial program written
in the VIVIDS (Munro, 1994) and DIAG (Towne,
1997) ITS authoring environment. MUP is targeted
to written or transcribed text. Phenomena such as
intonation contours and overlapping speech have no
opportunity to occur in our transcripts. Thus MUP
lacks features that spoken-language phenomena re-
quire of annotation tools, e.g. layers of annotation to
repair disfluencies, the representation of simultane-
ous speakers, and interfaces to speech tools.
Requirements and Alternatives
Our fundamental requirements for a markup tool are
that it 1) use standoff markup, 2) represent source
documents, annotations, and control files in sim-
ple XML, 3) have a simple graphical annotation in-
terface, 4) provide control over element names, at-
tribute names, and attribute values, enforcing consis-
tency in the final markup, and 5) can be configured
and employed by everyday computational linguists
without much effort or training.
In standoff markup (Thompson and McKelvie,
1997) the source text is inviolate and the annota-
tions are kept physically separate, usually in other
files. Annotatable items in the source text contain
labels, while the physically separate annotations re-
fer to these labels. Since annotations are themselves
labeled, complex structures of linked annotated con-
stitutents pointing to each other are representable.
Thompson and McKelvie list three advantages to
standoff markup: 1) the source document might be
read-only or unwieldy, 2) the annotations can devi-
ate from the strictly tree-structured hierarchies that
in-line XML demands, 3) annotation files can be
       Philadelphia, July 2002, pp. 37-41.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
distributed without distributing the source text. We
note a few more advantages of the standoff style: 4)
discontinuous segments of text can be combined in
a single annotation, 5) independent parallel coders
produce independent parallel annotation files, aiding
the determination of inter-coder reliability, 6) dif-
ferent annotation files can contain different layers
of information, 7) when the source text is regener-
ated from primary sources (for example, to incorpo-
rate more information) existing annotations are pre-
served.
Several years before Thompson and McKelvie,
the Tipster project evolved a similar standoff archi-
tecture for similar reasons (Grishman, 1995). Two
notable differences between Tipster and our own ar-
chitecture are that Tipster annotations refer to abso-
lute byte numbers within an unlabeled source docu-
ment file, and the Tipster architecture does not use
XML or SGML but instead supports its own class
library and internal representation. Other markup
projects, for example the Alembic Workbench, have
taken their cue from Tipster and implemented the
same standoff idea.
We specified XML for an annotation language be-
cause it is a lingua franca: the vocabulary is quite
commonly known, there is a host of XML process-
ing software, people can inspect it, and XML pro-
vides a rich ability to add attributes to elements.
The ATLAS.ti (Muhr, 2002) and the NUD*IST
annotation packages (QSR Corp., 2002) have both
been marketed for many years to researchers in
the ?soft? sciences for computer-assisted qualitative
analysis of texts. Their emphasis is on visually il-
lustrating the various codes attached to parts of the
document so the researcher can observe patterns.
Fairly complicated relationships between the anno-
tation tags can be created and visualized. An impres-
sive characteristic of these packages is that ordinary
researchers in non-technical fields can create their
own tags and commence annotating. However the
annotations in these packages point to sections of the
plain-text source document by absolute byte num-
ber. Thus the annotations are not readily available
for inspection or machine processing outside of the
programs? own interfaces and existing XML-tagged
data cannot readily become a source document for
further markup. These packages are not useful for
the analysis usually needed by NLP projects. It is
<xscript id="t12">
...
<tutor-resp id="t12_turn_3">
<w id="t12_t19">to</w>
<w id="t12_t20">see</w>
<w id="t12_t21">if</w>
<w id="t12_t22">the</w>
<w id="t12_t23">oil</w>
...
Figure 1: Source Document: ?to see if the oil...?
interesting to note that the most recent versions of
ATLAS.ti and NUD*IST have been adding the abil-
ity to import and export structured documents and
annotations, in XML and Rich Text Format respec-
tively.
At another extreme are markup tools like DAT,
an annotator for DAMSL (Allen and Core, 1997), a
dense multi-layered annotation scheme rich with at-
tributes. DAT is extremely convenient for the coder,
but it seems to require expert reprogramming when
DAMSL?s tag set changes.
A Taste of MUP
Running MUP requires a source document, a DTD-
like document describing a tag set, a style file con-
trolling how source text and annotations are dis-
played to the user, and optionally an existing annota-
tion file. The coder can then mark up the document
and save the results.
Source Document Figure 1 shows an extract from
a DIAG-NLP project source document. These
source document elements have special meaning:
word elements in line-wrapped text are tagged
<word> or <w>, and formatted lines of text are
tagged with <line> to be displayed without wrap-
ping. These elements must be labeled with XML
ID atributes to be the target of annotations. Other
XML elements may appear in the source document
as desired. All source document elements can be
optionally revealed or hidden for the coder, styled
according to the style file.
Tag Descriptions Each tag is an empty XML el-
ement, described in the tags description file by an
<!ATTLIST> declaration. We omit <!ELEMENT>
declarations as superfluous, so this file is not fully
a DTD. The pop-up dialogue the coder uses for
entering and editing attributes is driven by the
<!ATTLIST> description. Figure 2 illustrates the
description of a tag named indicator. The id,
idrefs, and comment attributes are standard for
all MUP markup tags. This description is inter-
preted, in part, as follows:
 idrefs will contain the IDs of ranges of tar-
get elements for this annotation, selected by the
coder by painting the source text with a cursor.
 The comment attribute, being CDATA, con-
tains arbitrary text typed by the coder.
 The directness, senseref and indi-
cator name attributes, being enumerated
lists, will present a drop-down list of values
to the coder. Notice the indicator name
is specified by entity substitution for conve-
nience.
Snapshot of MUP at Work Figure 3 shows a
snapshot of MUP at work. A control window has
a list of available markup tags plus it shows which
source document elements are displayed or hidden,
while the source document text is in a separate
window. The style file controls the display of the
source document by selecting which elements and
attributes to show/hide, picking colors for highlight-
ing, and inserting some bracketing text before and
after. In the snapshot, we have elected to display
the data from the <date> tag, the <consult>
element with its attributes but not the data, and the
<tutor-resp> element and data with a separa-
tor after it, while suppressing all other elements. We
have shown the text ?the blue box that lets you see if
the oil is flowing? being annotated as an indica-
tor via a pop-up dialogue.
Discussion
We believe a simplified easy-to-configure and run
tool will have wider applicability beyond our own
project. Other projects that manually code large
quantities of typed text, e.g. the RST dialogue
markup project (Marcu et al, 1999), have found
it desirable to create their own markup tools. The
CIRCSIM-Tutor project, with well over a hundred
transcripts of typed dialogue averaging an hour each,
has been coding in SGML (Freedman et al, 1998;
Kim, 1999) with general-purpose text editors.
The MATE workbench (McKelvie et al, 2001)
is a full-featured dialogue markup tool, however we
found it to be complex and difficult to use. We saw
an opportunity to borrow some of the ideas from
MATE and realize them with a simpler annotation
tool. MATE envisions three levels of user: coders,
researchers for whom the coding task is performed
and who need to view and manipulate the results,
and experts who are able to configure the software
(Carletta and Isard, 1999). It is this last group that
can perform the manipulations necessary for adding
new tags to the tag set and controlling how they
are displayed. MATE permits programmatic con-
trol over the coding interface by means of an XSL
style sheet customized for a particular application.
It is possible to split windows, intercept cursor op-
erations, provide linking operations between text in
different windows, and so on. This kind of flexibil-
ity is useful in annotated speech, for example in sep-
arately displaying and linking two speech streams
and having several related windows update simul-
taneously in response to coder actions. In our ex-
perience the MATE style sheets were quite difficult
to write and debug, and for our application we did
not need the flexibility, so we dispensed with them
and created our own, simpler, mechanism to control
the display of text. One consequence of the lessened
flexibility in MUP is that it presents a consistent cod-
ing interface using familiar single-dialog GUI con-
ventions.
Brooks (1975) estimates that the difference be-
tween a working program and a usable system with
ancillary utilities, shell scripts, etc. is three times the
original effort, and producing a distributable prod-
uct requires another factor of three effort. The core
MUP program works well enough that we have been
using it for several months. Our highest priority next
enhancement is to add a utility for inter-rater com-
parison, featuring some control over how parallel
annotations are compared (e.g., by selecting which
of the element?s attributes must match), and auto-
matically computing  statistics. MUP runs on So-
laris and Linux. We will make it available to re-
searchers as it matures.
Acknowledgments
This work is supported by grant N00014-00-1-0640 from the
ONR Cognitive, Neural and Biomolecular S&T Division and
<!ENTITY % indlist "( current-temp-gauge | sight-hole | water_temp_gauge ...)" >
<!ATTLIST indicator
id ID #required
idrefs IDREFS #required
comment CDATA #implied
indicator_name %indlist; ?unspecified?
directness ( explicit | implicit | summary | unspecified ) ?unspecified?
senseref ( sense | reference | unspecified ) ?unspecified? >
Figure 2: Description of indicator tag
Research Infrastructure grant EIA-9802090 from NSF. Thanks
also to Maarika Traat and Heena Raval, who have been most
helpful in the DIAG-NLP markup effort.
References
James Allen and Mark Core. 1997. Draft of
DAMSL: Dialog Act Markup in Several Lay-
ers. http://www.cs.rochester.edu/
research/cisd/resources/damsl/.
Frederick P. Brooks. 1975. The Mythical Man-Month:
Essays on Software Engineering. Addison-Wesley,
Reading, MA.
Jean Carletta and Amy Isard. 1999. The MATE an-
notation workbench: User requirements. In Marilyn
Walker, editor, Towards Standards and Tools for Dis-
course Tagging: Proceedings of the Workshop, Col-
lege Park MD, pages 11?17, New Brunswick, NJ. As-
sociation for Computational Linguistics.
David Day, John Aberdeen, Lynette Hirschmann, Robyn
Kozierok, Patricia Robinson, and Marc Vilain. 1997.
Mixed-initiative development of language processing
systems. In Fifth Conference on Applied Natural
Language Processing ANLP-97, pages 348?355, New
Brunswick, NJ. Association for Computational Lin-
guistics.
Barbara Di Eugenio, Michael Glass, and Michael J. Tro-
lio. 2002. The DIAG experiments: Natural language
generation for intelligent tutoring systems. In Second
International Natural Language Generation Confer-
ence INLG ?02, Harriman, NY. To appear.
Giovanni Flammia and Victor Zue. 1995. Empirical
evaluation of human performance and agreement in
parsing discourse constituents in spoken dialogue. In
Proc. Eurospeech-95, Fourth European Conference on
Speech Communication and Technology, pages 1965?
1968.
Reva Freedman, Yujian Zhou, Jung Hee Kim, Michael
Glass, and Martha W. Evens. 1998. SGML-based
markup as a step toward improving knowledge acqui-
sition for text generation. In AAAI Spring Symposium
on Applying Machine Learning to Discourse Process-
ing, pages 114?117.
Ralph Grishman. 1995. Tipster phase II architecture
design document (Tinman architecture). Technical
report, New York University. http://cs.nyu.
edu/pub/nlp/tipster/152.ps.
Jung Hee Kim. 1999. A manual for SGML markup
of tutoring transcripts. Technical report, CIRCSIM-
Tutor Project, Illinois Institute of Technology. http:
//www.cs.iit.edu/?circsim/.
Daniel Marcu, Estibaliz Amorrortu, and Magdalena
Romera. 1999. Experiments in constructing a corpus
of discourse trees. In Marilyn Walker, editor, Towards
Standards and Tools for Discourse Tagging: Proceed-
ings of the Workshop, College Park MD, pages 48?57,
New Brunswick, NJ. Association for Computational
Linguistics.
Dave McKelvie, Amy Isard, Andreas Mengel,
Morten Braun M?ller, Michael Grosse, and Mar-
ion Klein. 2001. The MATE workbench ?
an annotation tool for XML coded speech cor-
pora. Speech Communication, 33(1?2):97?112.
http://www.cogsci.ed.ac.uk/?dmck/
Papers/speechcomm00.ps.
Thomas Muhr. 2002. ATLAS.ti home page. http:
//www.atlasti.de/atlasneu.html.
Allen Munro. 1994. Authoring interactive graphical
models. In T. de Jong, D. M. Towne, and H. Spada,
editors, The Use of Computer Models for Explication,
Analysis and Experiential Learning. Springer Verlag.
QSR Corp. 2002. NUD*IST home page. http://
www.qsr.com.au/.
Henry Thompson and David McKelvie. 1997. Hy-
perlink semantics for standoff markup of read-only
documents. In SGML Europe 97, Barcelona.
http://www.infoloom.com/gcaconfs/
WEB/TOC/barcelona97toc.HTM.
Douglas Towne. 1997. Approximate reasoning tech-
niques for intelligent diagnostic instruction. Interna-
tional Journal of AI in Education, 8:262?283.
<   >
see if the oil is flowing.  If it
--end resp--
problem.
is, you have solved the
to the oil burner view and click
To see if the oil is flowing, go 
on the blue box that lets you
<consult prob="1" type="RU">
<tutor-resp id="t12_turn_3">
--end resp--
see if the oil is flowing properly
that was clogged.  Check to
You have replaced the oil filter
<tutor-resp id="t12_turn_2">
<consult prob="2" type= ...>
Document
Tags
Markup
Style
./tlogs2001/t12.xml
./styles_master.
./mglass/t12a.xml
./tags.dtd
QUITSAVERUN
File Selections
Markup Tags
indication
indicator
operationality
ru
related
aggregate_object
consult
date
diag-data
log
tutor-resp
Source Document Tags
t12.xmlMUP Standoff Markup
indicator
Nice cross-modal referring exp
the blue box t...e oil is flowing
oil-flow-indicator
explicit
sense
comment
senseref
directness
indicator_name
indicator_20id
OK CANCEL REMOVE
Edit Markup Tag
<   >  
03/23/01 F 10:00
actionlog
Figure 3: MUP in Action: Control Panel, Text Window, and Edit Tag Window
The problem of ontology alignment on the web: a first report
Davide Fossati and Gabriele Ghidoni and Barbara Di Eugenio
and Isabel Cruz and Huiyong Xiao and Rajen Subba
Computer Science
University of Illinois
Chicago, IL, USA
dfossa1@uic.edu, red.one.999@virgilio.it, bdieugen@cs.uic.edu
ifc@cs.uic.edu, hxiao2@uic.edu, rsubba@cs.uic.edu
Abstract
This paper presents a general architec-
ture and four algorithms that use Natu-
ral Language Processing for automatic on-
tology matching. The proposed approach
is purely instance based, i.e., only the
instance documents associated with the
nodes of ontologies are taken into account.
The four algorithms have been evaluated
using real world test data, taken from the
Google and LookSmart online directories.
The results show that NLP techniques ap-
plied to instance documents help the sys-
tem achieve higher performance.
1 Introduction
Many fundamental issues about the viability and
exploitation of the web as a linguistic corpus have
not been tackled yet. The web is a massive reposi-
tory of text and multimedia data. However, there is
not a systematic way of classifying and retrieving
these documents. Computational Linguists are of
course not the only ones looking at these issues;
research on the Semantic Web focuses on pro-
viding a semantic description of all the resources
on the web, resulting into a mesh of information
linked up in such a way as to be easily process-
able by machines, on a global scale. You can think
of it as being an efficient way of representing data
on the World Wide Web, or as a globally linked
database.1 The way the vision of the Semantic
Web will be achieved, is by describing each doc-
ument using languages such as RDF Schema and
OWL, which are capable of explicitly expressing
the meaning of terms in vocabularies and the rela-
tionships between those terms.
1http://infomesh.net/2001/swintro/
The issue we are focusing on in this paper is
that these languages are used to define ontologies
as well. If ultimately a single ontology were used
to describe all the documents on the web, sys-
tems would be able to exchange information in a
transparent way for the end user. The availability
of such a standard ontology would be extremely
helpful to NLP as well, e.g., it would make it far
easier to retrieve all documents on a certain topic.
However, until this vision becomes a reality, a plu-
rality of ontologies are being used to describe doc-
uments and their content. The task of automatic
ontology alignment ormatching (Hughes and Ash-
pole, 2005) then needs to be addressed.
The task of ontology matching has been typi-
cally carried out manually or semi-automatically,
for example through the use of graphical user in-
terfaces (Noy and Musen, 2000). Previous work
has been done to provide automated support to this
time consuming task (Rahm and Bernstein, 2001;
Cruz and Rajendran, 2003; Doan et al, 2003;
Cruz et al, 2004; Subba and Masud, 2004). The
various methods can be classified into two main
categories: schema based and instance based.
Schema based approaches try to infer the seman-
tic mappings by exploiting information related to
the structure of the ontologies to be matched, like
their topological properties, the labels or descrip-
tion of their nodes, and structural constraints de-
fined on the schemas of the ontologies. These
methods do not take into account the actual data
classified by the ontologies. On the other hand,
instance based approaches look at the information
contained in the instances of each element of the
schema. These methods try to infer the relation-
ships between the nodes of the ontologies from
the analysis of their instances. Finally, hybrid
approaches combine schema and instance based
51
methods into integrated systems.
Neither instance level information, nor NLP
techniques have been extensively explored in pre-
vious work on ontology matching. For exam-
ple, (Agirre et al, 2000) exploits documents (in-
stances) on the WWW to enrich WordNet (Miller
et al, 1990), i.e., to compute ?concept signatures,?
collection of words that significantly distinguish
one sense from another, however, not directly for
ontology matching. (Liu et al, 2005) uses doc-
uments retrieved via queries augmented with, for
example, synonyms that WordNet provides to im-
prove the accuracy of the queries themselves, but
not for ontology matching. NLP techniques such
as POS tagging, or parsing, have been used for
ontology matching, but on the names and defini-
tions in the ontology itself, for example, in (Hovy,
2002), hence with a schema based methodology.
In this paper, we describe the results we ob-
tained when using some simple but effective NLP
methods to align web ontologies, using an instance
based approach. As we will see, our results show
that more sophisticated methods do not necessar-
ily lead to better results.
2 General architecture
The instance based approach we propose uses
NLP techniques to compute matching scores
based on the documents classified under the nodes
of ontologies. There is no assumption on the struc-
tural properties of the ontologies to be compared:
they can be any kind of graph representable in
OWL. The instance documents are assumed to be
text documents (plain text or HTML).
The matching process starts from a pair of on-
tologies to be aligned. The two ontologies are
traversed and, for each node having at least one
instance, the system computes a signature based
on the instance documents. Then, the signatures
associated to the nodes of the two ontologies are
compared pairwise, and a similarity score for each
pair is generated. This score could then be used
to estimate the likelihood of a match between a
pair of nodes, under the assumption that the se-
mantics of a node corresponds to the semantics of
the instance documents classified under that node.
Figure 1 shows the architecture of our system.
The two main issues to be addressed are (1)
the representation of signatures and (2) the def-
inition of a suitable comparison metric between
signatures. For a long time, the Information Re-
Ontologiesdescription(OWL)Instancedocuments(HTML orplain text) NodeSignaturesCreation SignaturesComparison SimilarityScores
FileSystem WordNet
Figure 1: Ontology aligment architecture
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion Tokengroupingand counting
Figure 2: Baseline signature creation
trieval community has succesfully adopted a ?bag
of words? approach to effectively represent and
compare text documents. We start from there to
define a general signature structure and a metric to
compare signatures.
A signature is defined as a function S : K ?
R+, mapping a finite set of keys (which can be
complex objects) to positive real values. With a
signature of that form, we can use the cosine sim-
ilarity metric to score the similarity between two
signatures:
simil(S1, S2) =
?
p S1(kp)S2(kp)
?
?
i S1(ki)
2 ?
??
j S2(kj)
2
kp ? K1 ?K2, ki ? K1, kj ? K2
The cosine similarity formula produces a value
in the range [0, 1]. The meaning of that value de-
pends on the algorithm used to build the signa-
ture. In particular, there is no predefined thresh-
old that can be used to discriminate matches from
non-matches. However, such a threshold could be
computed a-posteriori from a statistical analysis of
experimental results.
2.1 Signature generation algorithms
For our experiments, we defined and implemented
four algorithms to generate signatures. The four
algorithms make use of text and language process-
ing techniques of increasing complexity.
2.1.1 Algorithm 1: Baseline signature
The baseline algorithm performs a very simple
sequence of text processing, schematically repre-
sented in Figure 2.
52
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion
Noungroupingand countingPOS tagging
Figure 3: Noun signature creation
HTML tags are first removed from the in-
stance documents. Then, the texts are tokenized
and punctuation is removed. Everything is then
converted to lowercase. Finally, the tokens are
grouped and counted. The final signature has the
form of a mapping table token ? frequency.
The main problem we expected with this
method is the presence of a lot of noise. In fact,
many ?irrelevant? words, like determiners, prepo-
sitions, and so on, are added to the final signature.
2.1.2 Algorithm 2: Noun signature
To cope with the problem of excessive noise,
people in IR often use fixed lists of stop words
to be removed from the texts. Instead, we intro-
duced a syntax based filter in our chain of pro-
cessing. The main assuption is that nouns are the
words that carry most of the meaning for our kind
of document comparison. Thus, we introduced
a part-of-speech tagger right after the tokeniza-
tion module (Figure 3). The results of the tagger
are used to discard everything but nouns from the
input documents. The part-of-speech tagger we
used ?QTAG 3.1 (Tufis and Mason, 1998), readily
available on the web as a Java library? is a Hidden
Markov Model based statistical tagger.
The problems we expected with this approach
are related to the high specialization of words in
natural language. Different nouns can bear simi-
lar meaning, but our system would treat them as if
they were completely unrelated words. For exam-
ple, the words ?apple? and ?orange? are semanti-
cally closer than ?apple? and ?chair,? but a purely
syntactic approach would not make any difference
between these two pairs. Also, the current method
does not include morphological processing, so dif-
ferent inflections of the same word, such as ?ap-
ple? and ?apples,? are treated as distinct words.
In further experiments, we also considered
verbs, another syntactic category of words bearing
a lot of semantics in natural language. We com-
puted signatures with verbs only, and with verbs
and nouns together. In both cases, however, the
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion
Synsetsgroupingand counting
POS tagging
Noun lookupon WordNet SynsetshierarchicalexpansionNoungroupingand counting
Figure 4: WordNet signature creation
performance of the system was worse. Thus, we
will not consider verbs in the rest of the paper.
2.1.3 Algorithm 3: WordNet signature
To address the limitations stated above, we used
the WordNet lexical resource (Miller et al, 1990).
WordNet is a dictionary where words are linked
together by semantic relationships. In Word-
Net, words are grouped into synsets, i.e., sets of
synonyms. Each synset can have links to other
synsets. These links represent semantic relation-
ships like hypernymy, hyponymy, and so on.
In our approach, after the extraction of nouns
and their grouping, each noun is looked up on
WordNet (Figure 4). The synsets to which the
noun belongs are added to the final signature in
place of the noun itself. The signature can also
be enriched with the hypernyms of these synsets,
up to a specified level. The final signature has the
form of a mapping synset ? value, where value is
a weighted sum of all the synsets found.
Two important parameters of this method are
related to the hypernym expansion process men-
tioned above. The first parameter is the maximum
level of hypernyms to be added to the signature
(hypernym level). A hypernym level value of 0
would make the algorithm add only the synsets of
a word, without any hypernym, to the signature. A
value of 1 would cause the algorithm to add also
their parents in the hypernym hierarchy to the sig-
nature. With higher values, all the ancestors up to
the specified level are added. The second parame-
ter, hypernym factor, specifies the damping of the
weight of the hypernyms in the expansion process.
Our algorithm exponentially dampens the hyper-
nyms, i.e., the weigth of a hypernym decreases ex-
ponentially as its level increases. The hypernym
factor is the base of the exponential function.
In general, a noun can have more than one
sense, e.g., ?apple? can be either a fruit or a tree.
This is reflected in WordNet by the fact that a
noun can belong to multiple synsets. With the
current approach, the system cannot decide which
53
HTML tagsremoval Tokenizationpunctuationremoval Lowercaseconversion
Synsetsgroupingand counting
POS tagging
Noun lookupon WordNet SynsetshierarchicalexpansionWord sensedisambigua-tion
Figure 5: Disambiguated signature creation
sense is the most appropriate, so all the senses
of a word are added to the final signature, with
a weight inversely proportional to the number of
possible senses of that word. This fact poten-
tially introduces semantic noise in the signature,
because many irrelevant senses might be added to
the signature itself.
Another limitation is that a portion of the nouns
in the source texts cannot be located in WordNet
(see Figure 6). Thus, we also tried a variation (al-
gorithm 3+2) that falls back on to the bare lexi-
cal form of a noun if it cannot be found in Word-
Net. This variation, however, resulted in a slight
decrease of performance.
2.1.4 Algorithm 4: Disambiguated signature
The problem of having multiple senses for each
word calls for the adoption of word sense dis-
ambiguation techniques. Thus, we implemented
a word sense disambiguator algorithm, and we
inserted it into the signature generation pipeline
(Figure 5). For each noun in the input documents,
the disambiguator takes into account a specified
number of context words, i.e., nouns preceding
and/or following the target word. The algorithm
computes a measure of the semantic distance be-
tween the possible senses of the target word and
the senses of each of its context words, pair-
wise. A sense for the target word is chosen such
that the total distance to its context is minimized.
The semantic distance between two synsets is de-
fined here as the minimum number of hops in
the WordNet hypernym hierarchy connecting the
two synsets. This definition allows for a rela-
tively straightforward computation of the seman-
tic distance using WordNet. Other more sophisti-
cated definitions of semantic distance can be found
in (Patwardhan et al, 2003). The word sense
disambiguation algorithm we implemented is cer-
tainly simpler than others proposed in the litera-
ture, but we used it to see whether a method that is
relatively simple to implement could still help.
The overall parameters for this signature cre-
ation algorithm are the same as the WordNet sig-
nature algorithm, plus two additional parameters
for the word sense disambiguator: left context
length and right context length. They represent re-
spectively how many nouns before and after the
target should be taken into account by the dis-
ambiguator. If those two parameters are both set
to zero, then no context is provided, and the first
possible sense is chosen. Notice that even in this
case the behaviour of this signature generation al-
gorithm is different from the previous one. In
a WordNet signature, every possible sense for a
word is inserted, whereas in a WordNet disam-
biguated signature only one sense is added.
3 Experimental setting
All the algorithms described in the previous sec-
tion have been fully implemented in a coherent
and extensible framework using the Java program-
ming language, and evaluation experiments have
been run. This section describes how the experi-
ments have been conducted.
3.1 Test data
The evaluation of ontology matching approaches
is usually made difficult by the scarceness of test
ontologies readily available in the community.
This problem is even worse for instance based ap-
proaches, because the test ontologies need also to
be ?filled? with instance documents. Also, we
wanted to test our algorithms with ?real world?
data, rather than toy examples.
We were able to collect suitable test data start-
ing from the ontologies published by the Ontology
Alignment Evaluation Initiative 2005 (Euzenat et
al., 2005). A section of their data contained an
OWL representation of fragments of the Google,
Yahoo, and LookSmart web directories. We ?re-
verse engineered? some of this fragments, in or-
der to reconstruct two consistent trees, one rep-
resenting part of the Google directory structure,
the other representing part of the LookSmart hi-
erarchy. The leaf nodes of these trees were filled
with instances downloaded from the web pages
classified by the appropriate directories. With this
method, we were able to fill 7 nodes of each ontol-
ogy with 10 documents per node, for a total of 140
documents. Each document came from a distinct
web page, so there was no overlap in the data to be
compared. A graphical representation of our two
test ontologies, source and target, is shown in Fig-
54
ure 6. The darker outlined nodes are those filled
with instance documents. For the sake of readabil-
ity, the names of the nodes corresponding to real
matches are the same. Of course, this informa-
tion is not used by our algorithms, which adopt a
purely instance based approach. Figure 6 also re-
ports the size of the instance documents associated
to each node: total number of words, noun tokens,
nouns, and nouns covered by WordNet.
3.2 Parameters
The experiments have been run with several com-
binations of the relevant parameters: number of
instance documents per node (5 or 10), algorithm
(1 to 4), extracted parts of speech (nouns, verbs, or
both), hypernym level (an integer value equal or
greater than zero), hypernym factor (a real num-
ber), and context length (an integer number equal
or greater than zero). Not all of the parameters are
applicable to every algorithm. The total number of
runs was 90.
4 Results
Each run of the system with our test ontologies
produced a set of 49 values, representing the
matching score of every pair of nodes containing
instances across the two ontologies. Selected ex-
amples of these results are shown in Tables 1, 2,
3, and 4. In the experiments shown in those ta-
bles, 10 instance documents for each node were
used to compute the signatures. Nodes that ac-
tually match (identified by the same label, e.g.,
?Canada? and ?Canada?) should show high sim-
ilarity scores, whereas nodes that do not match
(e.g., ?Canada? and ?Dendrochronology?), should
have low scores. Better algorithms would have
higher scores for matching nodes, and lower score
for non-matching ones. Notice that the two nodes
?Egypt? and ?Pyramid Theories,? although intu-
itively related, have documents that take different
perspectives on the subject. So, the algorithms
correctly identify the nodes as being different.
Looking at the results in this form makes it dif-
ficult to precisely assess the quality of the algo-
rithms. To do so, a statistical analysis has to be
performed. For each table of results, let us parti-
tion the scores in two distinct sets:
A = {simil(nodei, nodej) | real match = true}
B = {simil(nodei, nodej) | real match = false}
Target node
Canada
Canada 0.95 0.89 0.89 0.91 0.87 0.86 0.92
0.90 0.97 0.91 0.90 0.88 0.87 0.92
Egypt 0.86 0.89 0.91 0.87 0.86 0.88 0.90
Megaliths 0.90 0.91 0.99 0.93 0.95 0.94 0.93
Museums 0.89 0.88 0.90 0.93 0.88 0.87 0.90
0.88 0.88 0.95 0.91 0.99 0.93 0.91
0.87 0.87 0.86 0.88 0.82 0.82 0.96
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 1: Results ? Baseline signature algorithm
Target node
Canada
Canada 0.67 0.20 0.14 0.35 0.08 0.08 0.41
0.22 0.80 0.15 0.22 0.09 0.09 0.25
Egypt 0.13 0.23 0.26 0.22 0.17 0.24 0.25
Megaliths 0.28 0.20 0.85 0.37 0.22 0.27 0.33
Museums 0.30 0.19 0.18 0.58 0.08 0.14 0.27
0.13 0.12 0.26 0.18 0.96 0.14 0.17
0.42 0.20 0.17 0.26 0.09 0.11 0.80
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 2: Results ? Noun signature algorithm
Target node
Canada
Canada 0.79 0.19 0.19 0.38 0.15 0.06 0.56
0.26 0.83 0.18 0.20 0.16 0.07 0.24
Egypt 0.17 0.24 0.32 0.21 0.31 0.30 0.27
Megaliths 0.39 0.21 0.81 0.41 0.40 0.25 0.42
Museums 0.31 0.14 0.17 0.70 0.11 0.11 0.26
0.24 0.20 0.42 0.29 0.91 0.21 0.29
0.56 0.17 0.22 0.25 0.15 0.08 0.84
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 3: Results ? WordNet signature algorithm
(hypernym level=0)
55
TopGoogle
Science
Social_Sciences
Archaeology
Egypt
Alternative
Megaliths South_America
Nazca_Lines
Methodology
Dendrochronology
Museums Publications
Europe
UnitedKingdom
Organizations
NorthAmerica
Canada
TopLookSmart
Science_&_Health
Social_Science
Archaeology
Pyramid_Theories
Topics
Megaliths Nazca_Lines
Science
Dendrochronology
Museums Publications
UnitedKingdom
Associations
Canada
Seven_Wonders_of_the_World
Gyza_Pyramids
Source Target
44307; 16001;3957; 2220 15447; 4400;1660; 1428 18754; 5574; 1725; 1576
7362; 2377; 953; 823
2872; 949;499; 441
9620; 3257; 1233; 1001 3972; 1355;603; 541 3523; 1270;617; 555
23039; 7926; 1762; 1451
13705; 3958;1484; 1303
6171; 2333;943; 844
10721; 3280; 1099; 9887841; 2486; 869; 769
17196; 5529;1792; 1486
Figure 6: Ontologies used in the experiments. The numbers below the leaves indicate the size of instance
documents: # of words; # of noun tokens; # of nouns; # of nouns in WordNet
Target node
Canada
Canada 0.68 0.18 0.13 0.33 0.12 0.05 0.44
0.23 0.79 0.15 0.20 0.14 0.07 0.23
Egypt 0.15 0.23 0.28 0.22 0.27 0.31 0.27
Megaliths 0.30 0.18 0.84 0.37 0.34 0.27 0.33
Museums 0.29 0.16 0.15 0.60 0.11 0.10 0.24
0.20 0.17 0.38 0.26 0.89 0.21 0.26
0.45 0.17 0.18 0.24 0.15 0.08 0.80
Source node Dendro chronology Megaliths Museums Nazca Lines Pyramid Theories United Kingdom
Dendro chronology
Nazca LinesUnited Kingdom
Table 4: Results ? Disambiguated signature al-
gorithm (hypernym level=0, left context=1, right
context=1)
With our test data, we would have 6 values in
set A and 43 values in set B. Then, let us com-
pute average and standard deviation of the values
included in each set. The average of A represents
the expected score that the system would assign
to a match; likewise, the average of B is the ex-
pected score of a non-match. We define the fol-
lowing measure to compare the performance of
our matching algorithms, inspired by ?effect size?
from (VanLehn et al, 2005):
discrimination size =
avg(A) ? avg(B)
stdev(A) + stdev(B)
Higher discrimination values mean that the
scores assigned to matches and non-matches are
more ?far away,? making it possible to use those
scores to make more reliable decisions about the
matching degree of pairs of nodes.
Table 5 shows the values of discrimination size
(last column) out of selected results from our ex-
periments. The algorithm used is reported in the
first column, and the values of the other relevant
parameters are indicated in other columns. We can
make the following observations.
? Algorithms 2, 3, and 4 generally outperform
the baseline (algorithm 1).
? Algorithm 2 (Noun signature), which still
uses a fairly simple and purely syntactical
technique, shows a substantial improvement.
Algorithm 3 (WordNet signature), which in-
troduces some additional level of semantics,
has even better performance.
? In algorithms 3 and 4, hypernym expansion
looks detrimental to performance. In fact, the
best results are obtained with hypernym level
equal to zero (no hypernym expansion).
? The word sense disambiguator implemented
in algorithm 4 does not help. Even though
disambiguating with some limited context
(1 word before and 1 word after) provides
slightly better results than choosing the first
available sense for a word (context length
equal to zero), the overall results are worse
than adding all the possible senses to the sig-
nature (algorithm 3).
? Using only 5 documents per node signifi-
cantly degrades the performance of all the al-
gorithms (see the last 5 lines of the table).
5 Conclusions and future work
The results of our experiments point out several
research questions and directions for future work,
56
Alg Docs POS Hyp lev Hyp fac L cont R cont Avg (A) Stdev (A) Avg (B) Stdev (B) Discrimination size1 10 0.96 0.02 0.89 0.03 1.372 10 noun 0.78 0.13 0.21 0.09 2.552 10 verb 0.64 0.20 0.31 0.11 1.042 10 nn+vb 0.77 0.14 0.21 0.09 2.483 10 noun 0 0.81 0.07 0.25 0.12 3.083 10 noun 1 1 0.85 0.07 0.41 0.12 2.353 10 noun 1 2 0.84 0.07 0.34 0.12 2.643 10 noun 1 3 0.83 0.07 0.31 0.12 2.803 10 noun 2 1 0.90 0.06 0.62 0.11 1.643 10 noun 2 2 0.86 0.07 0.45 0.12 2.183 10 noun 2 3 0.84 0.07 0.36 0.12 2.563 10 noun 3 1 0.95 0.04 0.78 0.08 1.443 10 noun 3 2 0.88 0.07 0.52 0.12 1.913 10 noun 3 3 0.85 0.07 0.38 0.12 2.453+2 10 noun 0 0 0.80 0.09 0.21 0.11 2.943+2 10 noun 1 2 0.83 0.08 0.30 0.11 2.733+2 10 noun 2 2 0.85 0.08 0.39 0.11 2.404 10 noun 0 0 0 0.80 0.12 0.24 0.10 2.644 10 noun 0 1 1 0.77 0.11 0.22 0.10 2.674 10 noun 0 2 2 0.77 0.11 0.23 0.10 2.594 10 noun 1 2 0 0 0.82 0.10 0.29 0.10 2.564 10 noun 1 2 1 1 0.80 0.10 0.34 0.10 2.274 10 noun 1 2 2 2 0.80 0.10 0.35 0.10 2.22
1 5 noun 0.93 0.05 0.86 0.04 0.882 5 noun 0.66 0.23 0.17 0.08 1.613 5 noun 0 0.70 0.17 0.21 0.11 1.764 5 noun 0 0 0 0.69 0.21 0.20 0.09 1.634 5 noun 0 1 1 0.64 0.21 0.18 0.08 1.58
Table 5: Results ? Discrimination size
some more specific and some more general. As
regards the more specific issues,
? Algorithm 2 does not perform morphological
processing, whereas Algorithm 3 does. How
much of the improved effectiveness of Algo-
rithm 3 is due to this fact? To answer this
question, Algorithm 2 could be enhanced to
include a morphological processor.
? The effectiveness of Algorithms 3 and 4 may
be hindered by the fact that many words are
not yet included in theWordNet database (see
Figure 6). Falling back on to Algorithm 2
proved not to be a solution. The impact of the
incompleteness of the lexical resource should
be investigated and assessed more precisely.
Another venue of research may be to exploit
different thesauri, such as the ones automati-
cally derived as in (Curran andMoens, 2002).
? The performance of Algorithm 4 might be
improved by using more sophisticated word
sense disambiguation methods. It would also
be interesting to explore the application of
the unsupervised method described in (Mc-
Carthy et al, 2004).
As regards our long term plans, first, structural
properties of the ontologies could potentially be
exploited for the computation of node signatures.
This kind of enhancement would make our system
move from a purely instance based approach to a
combined hybrid approach based on schema and
instances.
More fundamentally, we need to address the
lack of appropriate, domain specific resources that
can support the training of algorithms and models
appropriate for the task at hand. WordNet is a very
general lexicon that does not support domain spe-
cific vocabulary, such as that used in geosciences
or in medicine or simply that contained in a sub-
ontology that users may define according to their
interests. Of course, we do not want to develop
by hand domain specific resources that we have to
change each time a new domain arises.
The crucial research issue is how to exploit ex-
tremely scarce resources to build efficient and ef-
fective models. The issue of scarce resources
makes it impossible to use methods that are suc-
cesful at discriminating documents based on the
words they contain but that need large corpora
for training, for example Latent Semantic Anal-
ysis (Landauer et al, 1998). The experiments de-
scribed in this paper could be seen as providing
57
a bootstrapped model (Riloff and Jones, 1999; Ng
and Cardie, 2003)?in ML, bootstrapping requires
to seed the classifier with a small number of well
chosen target examples. We could develop a web
spider, based on the work described on this paper,
to automatically retrieve larger amounts of train-
ing and test data, that in turn could be processed
with more sophisticated NLP techniques.
Acknowledgements
This work was partially supported by NSF Awards
IIS?0133123, IIS?0326284, IIS?0513553, and
ONR Grant N00014?00?1?0640.
References
Eneko Agirre, Olatz Ansa, Eduard Hovy, and David
Martinez. 2000. Enriching very large ontologies
using the WWW. In ECAI Workshop on Ontology
Learning, Berlin, August.
Isabel F. Cruz and Afsheen Rajendran. 2003. Ex-
ploring a new approach to the alignment of ontolo-
gies. In Workshop on Semantic Web Technologies
for Searching and Retrieving Scientific Data, in co-
operation with the International Semantic Web Con-
ference.
Isabel F. Cruz, William Sunna, and Anjli Chaudhry.
2004. Semi-automatic ontology alignment for
geospatial data integration. GIScience, pages 51?
66.
James Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Work-
shop on Unsupervised Lexical Acquisition, pages
59?67, Philadelphia, PA, USA.
AnHai Doan, Jayant Madhavan, Robin Dhamankar, Pe-
dro Domingos, and Alon Halevy. 2003. Learning to
match ontologies on the semantic web. VLDB Jour-
nal, 12(4):303?319.
Je?ro?me Euzenat, Heiner Stuckenschmidt, and
Mikalai Yatskevich. 2005. Introduction
to the ontology alignment evaluation 2005.
http://oaei.inrialpes.fr/2005/results/oaei2005.pdf.
Eduard Hovy. 2002. Comparing sets of semantic rela-
tions in ontology. In R. Green, C. A. Bean, and S. H.
Myaeng, editors, Semantics of Relationships: An In-
terdisciplinary Perspective, pages 91?110. Kluwer.
T. C. Hughes and B. C. Ashpole. 2005. The semantics
of ontology alignment. Draft Paper, Lockheed Mar-
tin Advanced Technology Laboratories, Cherry Hill,
NJ. http://www.atl.lmco.com/projects/ontology/ pa-
pers/ SOA.pdf.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to Latent Semantic Analy-
sis. Discourse Processes, 25:259?284.
Shuang Liu, Clement Yu, and Weiyi Meng. 2005.
Word sense disambiguation in queries. In ACM
Conference on Information and Knowledge Man-
agement (CIKM2005), Bremen, Germany.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In 42nd Annual Meeting of the As-
sociation for Computational Linguistics, Barcelona,
Spain.
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. J. Miller. 1990. Introduction to wordnet: an on-
line lexical database. International Journal of Lexi-
cography, 3 (4):235?244.
Vincent Ng and Claire Cardie. 2003. Bootstrapping
coreference classifiers with multiple machine learn-
ing algorithms. In The 2003 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2003).
Natalya Fridman Noy and Mark A. Musen. 2000.
Prompt: Algorithm and tool for automated ontology
merging and alignment. In National Conference on
Artificial Intelligence (AAAI).
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using semantic relatedness for
word sense disambiguation. In Fourth International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CiCLING-03), Mexico City.
Erhard Rahm and Philip A. Bernstein. 2001. A sur-
vey of approaches to automatic schema matching.
VLDB Journal, 10(4):334?350.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI-99, Sixteenth National Con-
ference on Artificial Intelligence.
Rajen Subba and Sadia Masud. 2004. Automatic gen-
eration of a thesaurus using wordnet as a means to
map concepts. Tech report, University of Illinois at
Chicago.
Dan Tufis and Oliver Mason. 1998. Tagging romanian
texts: a case study for qtag, a language independent
probabilistic tagger. In First International Confer-
ence on Language Resources & Evaluation (LREC),
pages 589?596, Granada, Spain.
Kurt VanLehn, Collin Lynch, Kay Schulze, Joel
Shapiro, Robert Shelby, Linwood Taylor, Donald
Treacy, Anders Weinstein, and Mary Wintersgill.
2005. The andes physics tutoring system: Five years
of evaluations. In 12th International Conference on
Artificial Intelligence in Education, Amsterdam.
58
Discourse Parsing: Learning FOL Rules based on Rich Verb Semantic
Representations to automatically label Rhetorical Relations
Rajen Subba
Computer Science
University of Illinois
Chicago, IL, USA
rsubba@cs.uic.edu
Barbara Di Eugenio
Computer Science
University of Illinois
Chicago, IL, USA
bdieugen@cs.uic.edu
Su Nam Kim
Department of CSSE
University of Melbourne
Carlton, VIC, Australia
snkim@csse.unimelb.edu.au
Abstract
We report on our work to build a dis-
course parser (SemDP) that uses seman-
tic features of sentences. We use an In-
ductive Logic Programming (ILP) System
to exploit rich verb semantics of clauses
to induce rules for discourse parsing. We
demonstrate that ILP can be used to learn
from highly structured natural language
data and that the performance of a dis-
course parsing model that only uses se-
mantic information is comparable to that
of the state of the art syntactic discourse
parsers.
1 Introduction
The availability of corpora annotated with syntac-
tic information have facilitated the use of prob-
abilistic models on tasks such as syntactic pars-
ing. Current state of the art syntactic parsers
reach accuracies between 86% and 90%, as mea-
sured by different types of precision and recall
(for more details see (Collins, 2003)). Recent
semantic (Kingsbury and Palmer, 2002) and dis-
course (Carlson et al, 2003) annotation projects
are paving the way for developments in seman-
tic and discourse parsing as well. However unlike
syntactic parsing, significant development in dis-
course parsing remains at large.
Previous work on discourse parsing ((Soricut
and Marcu, 2003) and (Forbes et al, 2001))
have focused on syntactic and lexical features
only. However, discourse relations connect
clauses/sentences, hence, descriptions of events
and states. It makes linguistic sense that the
semantics of the two clauses ?generally built
around the semantics of the verbs, composed with
that of their arguments? affects the discourse re-
lation(s) connecting the clauses. This may be
even more evident in our instructional domain,
where relations derived from planning such as
Precondition-Act may relate clauses.
Of course, since semantic information is hard
to come by, it is not surprising that previous work
on discourse parsing did not use it, or only used
shallow word level ontological semantics as spec-
ified in WordNet (Polanyi et al, 2004). But when
rich sentence level semantics is available, it makes
sense to experiment with it for discourse parsing.
A second major difficulty with using such rich
verb semantic information, is that it is rep-
resented using complex data structures. Tradi-
tional Machine Learning methods cannot han-
dle highly structured data such as First Or-
der Logic (FOL), a representation that is suit-
ably used to represent sentence level seman-
tics. Such FOL representations cannot be reduced
to a vector of attribute/value pairs as the rela-
tions/interdependencies that exist among the pred-
icates would be lost.
Inductive Logic Programming (ILP) can learn
structured descriptions since it learns FOL de-
scriptions. In this paper, we present our first steps
using ILP to learn semantic descriptions of dis-
course relations. Also of relevance to the topic of
this workshop, is that discourse structure is inher-
ently highly structured, since discourse structure
is generally described in hierarchical terms: ba-
sic units of analysis, generally clauses, are related
by discourse relations, resulting in more complex
units, which in turn can be related via discourse re-
lations. At the moment, we do not yet address the
problem of parsing at higher levels of discourse.
We intend to build on the work we present in this
paper to achieve that goal.
The task of discourse parsing can be di-
vided into two disjoint sub-problems ((Soricut and
Marcu, 2003) and (Polanyi et al, 2004)). The two
sub-problems are automatic identification of seg-
ment boundaries and the labeling of rhetorical re-
lations. Though we consider the problem of auto-
matic segmentation to be an important part in dis-
course parsing, we have focused entirely on the
latter problem of automatically labeling rhetorical
33
Figure 1: SemDP System Architecture (Discourse Parser)
relations only. Our approach uses rich verb seman-
tics1 of elementary discourse units (EDUs)2 based
on VerbNet(Kipper et al, 2000) as background
knowledge and manually annotated rhetorical re-
lations as training examples. It is trained on a lot
fewer examples than the state of the art syntax-
based discourse parser (Soricut and Marcu, 2003).
Nevertheless, it achieves a comparable level of
performance with an F-Score of 60.24. Figure 1
shows a block diagram of SemDP?s system archi-
tecture. Segmentation, annotation of rhetorical re-
lations and parsing constitute the data collection
phase of the system. Learning is accomplished
using an ILP based system, Progol (Muggleton,
1995). As can be seen in Figure 1, Progol takes
as input both rich verb semantic information of
pairs of EDUs and the rhetorical relations between
them. The goal was to learn rules using the se-
mantic information from pairs of EDUs as in Ex-
ample 1:
(1) EDU1: ?Sometimes, you can add a liquid to the water
EDU2: ?to hasten the process?
relation(EDU1,EDU2,?Act:goal?).
to automatically label unseen examples with the
correct rhetorical relation.
The rest of the paper is organized as follows.
Section 2 describes our data collection methodol-
ogy. In section 3, Progol, the ILP system that we
1The semantic information we used is composed of Verb-
Net semantic predicates that capture event semantics as well
as thematic roles.
2EDUs are minimal discourse units produced as a result
of discourse segmentation.
used to induce rules for discourse parsing is de-
tailed. Evaluation results are presented in section
4 followed by the conclusion in section 5.
2 Data Collection
The lack of corpora annotated with both rhetorical
relations as well as sentence level semantic rep-
resentation led us to create our own corpus. Re-
sources such as (Kingsbury and Palmer, 2002) and
(Carlson et al, 2003) have been developed man-
ually. Since such efforts are time consuming and
costly, we decided to semi-automatically build our
annotated corpus. We used an existing corpus of
instructional text that is about 9MB in size and is
made up entirely of written English instructions.
The two largest components are home repair man-
uals (5Mb) and cooking recipes (1.7Mb). 3
Segmentation. The segmentation of the corpus
was done manually by a human coder. Our seg-
mentation rules are based on those defined in
(Mann and Thompson, 1988). For example, (as
shown in Example 2) we segment sentences in
which a conjunction is used with a clause at the
conjunction site.
(2) You can copy files (//) as well as cut messages.
(//) is the segmentation marker. Sentences are
segmented into EDUs. Not all the segmentation
3It was collected opportunistically off the internet and
from other sources, and originally assembled at the Informa-
tion Technology Research Institute, University of Brighton.
34
rules from (Mann and Thompson, 1988) are im-
ported into our coding scheme. For example, we
do not segment relative clauses. In total, our seg-
mentation resulted in 10,084 EDUs. The seg-
mented EDUs were then annotated with rhetorical
relations by the human coder4 and also forwarded
to the parser as they had to be annotated with se-
mantic information.
2.1 Parsing of Verb Semantics
We integrated LCFLEX (Rose? and Lavie, 2000),
a robust left-corner parser, with VerbNet (Kipper
et al, 2000) and CoreLex (Buitelaar, 1998). Our
interest in decompositional theories of lexical se-
mantics led us to base our semantic representation
on VerbNet.
VerbNet operationalizes Levin?s work and ac-
counts for 4962 distinct verbs classified into 237
main classes. Moreover, VerbNet?s strong syntac-
tic components allow it to be easily coupled with a
parser in order to automatically generate a seman-
tically annotated corpus.
To provide semantics for nouns, we use
CoreLex (Buitelaar, 1998), in turn based on the
generative lexicon(Pustejovsky, 1991). CoreLex
defines basic types such as art (artifact) or com
(communication). Nouns that share the same bun-
dle of basic types are grouped in the same System-
atic Polysemous Class (SPC). The resulting 126
SPCs cover about 40,000 nouns.
We modified and augmented LCFLEX?s exist-
ing lexicon to incorporate VerbNet and CoreLex.
The lexicon is based on COMLEX (Grishman et
al., 1994). Verb and noun entries in the lexicon
contain a link to a semantic type defined in the on-
tology. VerbNet classes (including subclasses and
frames) and CoreLex SPCs are realized as types in
the ontology. The deep syntactic roles are mapped
to the thematic roles, which are defined as vari-
ables in the ontology types. For more details on
the parser see (Terenzi and Di Eugenio, 2003).
Each of the 10,084 EDUs was parsed using the
parser. The parser generates both a syntactic tree
and the associated semantic representation ? for
the purpose of this paper, we only focus on the
latter. Figure 2 shows the semantic representation
generated for EDU1 from Example 1, ?sometimes,
you can add a liquid to the water?.
The semantic representation in Figure 2 is part
4Double annotation and segmentation is currently being
done to assess inter-annotator agreement using kappa.
(*SEM*
((AGENT YOU)
(VERBCLASS ((VNCLASS MIX-22.1-2))) (EVENT +)
(EVENT0
((END
((ARG1 (LIQUID))
(FRAME *TOGETHER) (ARG0 PHYSICAL)
(ARG2 (WATER)))))))
(EVENTSEM
((FRAME *CAUSE) (ARG1 E) (ARG0 (YOU)))))
(PATIENT1 LIQUID)
(PATIENT2 WATER)
(ROOT-VERB ADD))
Figure 2: Parser Output (Semantic Information)
of the F-Structure produced by the parser. The
verb add is parsed for a transitive frame with a PP
modifier that belongs to the verb class ?MIX-22.1-
2?. The sentence contains two PATIENTs, namely
liquid and water. you is identified as the AGENT
by the parser. *TOGETHER and *CAUSE are the
primitive semantic predicates used by VerbNet.
Verb Semantics in VerbNet are defined as events
that are decomposed into stages, namely start, end,
during and result. The semantic representation in
Figure 2 states that there is an event EVENT0 in
which the two PATIENTs are together at the end.
An independent evaluation on a set of 200 sen-
tences from our instructional corpus was con-
ducted. 5 It was able to generate complete parses
for 72.2% and partial parses for 10.9% of the verb
frames that we expected it to parse, given the re-
sources. The parser cannot parse those sentences
(or EDUs) that contain a verb that is not cov-
ered by VerbNet. This coverage issue, coupled
with parser errors, exacerbates the problem of data
sparseness. This is further worsened by the fact
that we require both the EDUs in a relation set
to be parsed for the Machine Learning part of our
work. Addressing data sparseness is an issue left
for future work.
2.2 Annotation of Rhetorical Relations
The annotation of rhetorical relations was done
manually by a human coder. Our coding scheme
builds on Relational Discourse Analysis (RDA)
(Moser and Moore, 1995), to which we made mi-
5The parser evaluation was not based on EDUs but rather
on unsegmented sentences. A sentence contained one or
more EDUs.
35
nor modifications; in turn, as far as discourse rela-
tions are concerned, RDA was inspired by Rhetor-
ical Structure Theory (RST) (Mann and Thomp-
son, 1988).
Rhetorical relations were categorized as infor-
mational, elaborational, temporal and others. In-
formational relations describe how contents in
two relata are related in the domain. These re-
lations are further subdivided into two groups;
causality and similarity. The former group con-
sists of relations between an action and other ac-
tions or between actions and their conditions or
effects. Relations like ?act:goal?, ?criterion:act?
fall under this group. The latter group con-
sists of relations between two EDUs according
to some notion of similarity such as ?restate-
ment? and ?contrast1:contrast2?. Elaborational
relations are interpropositional relations in which
a proposition(s) provides detail relating to some
aspect of another proposition (Mann and Thomp-
son, 1988). Relations like ?general:specific? and
?circumstance:situation? belong to this category.
Temporal relations like ?before:after? capture time
differences between two EDUs. Lastly, the cate-
gory others includes relations not covered by the
previous three categories such as ?joint? and ?inde-
terminate?.
Based on the modified coding scheme manual,
we segmented and annotated our instructional cor-
pus using the augmented RST tool from (Marcu et
al., 1999). The RST tool was modified to incor-
porate our relation set. Since we were only inter-
ested in rhetorical relations that spanned between
two adjacent EDUs 6, we obtained 3115 sets of
potential relations from the set of all relations that
we could use as training and testing data.
The parser was able to provide complete parses
for both EDUs in 908 of the 3115 relation sets.
These constitute the training and test set for Pro-
gol.
The semantic representation for the EDUs along
with the manually annotated rhetorical relations
were further processed (as shown in Figure 4) and
used by Progol as input.
3 The Inductive Logic Programming
Framework
We chose to use Progol, an Inductive Logic Pro-
gramming system (ILP), to learn rules based on
6At the moment, we are concerned with learning relations
between two EDUs at the base level of a Discourse Parse Tree
(DPT) and not at higher levels of the hierarchy.
the data we collected. ILP is an area of research
at the intersection of Machine Learning (ML) and
Logic Programming. The general problem speci-
fication in ILP is given by the following property:
B ?H |= E (3)
Given the background knowledge B and the ex-
amples E, ILP systems find the simplest consistent
hypothesis H, such that B and H entails E.
While most of the work in NLP that involves
learning has used more traditional ML paradigms
like decision-tree algorithms and SVMs, we did
not find them suitable for our data which is rep-
resented as Horn clauses. The requirement of us-
ing a ML system that could handle first order logic
data led us to explore ILP based systems of which
we found Progol most appropriate.
Progol combines Inverse Entailment with
general-to-specific search through a refinement
graph. A most specific clause is derived using
mode declarations along with Inverse Entailment.
All clauses that subsume the most specific clause
form the hypothesis space. An A*-like search
is used to search for the most probable theory
through the hypothesis space. Progol allows arbi-
trary programs as background knowledge and ar-
bitrary definite clauses as examples.
3.1 Learning from positive data only
One of the features we found appealing about Pro-
gol, besides being able to handle first order logic
data, is that it can learn from positive examples
alone.
Learning in natural language is a universal hu-
man process based on positive data only. How-
ever, the usual traditional learning models do not
work well without negative examples. On the
other hand, negative examples are not easy to ob-
tain. Moreover, we found learning from positive
data only to be a natural way to model the task of
discourse parsing.
To make the learning from positive data only
feasible, Progol uses a Bayesian framework. Pro-
gol learns logic programs with an arbitrarily low
expected error using only positive data. Of course,
we could have synthetically labeled examples of
relation sets (pairs of EDUs), that did not belong
to a particular relation, as negative examples. We
plan to explore this approach in the future.
A key issue in learning from positive data
only using a Bayesian framework is the ability
to learn complex logic programs. Without any
36
negative examples, the simplest rule or logic
program, which in our case would be a single
definite clause, would be assigned the highest
score as it captures the most number of examples.
In order to handle this problem, Progol?s scoring
function exercises a trade-off between the size of
the function and the generality of the hypothesis.
The score for a given hypothesis is calculated
according to formula 4.
ln p(H | E) = m ln
( 1
g(H)
)
?sz(H)+dm (4)
sz(H) and g(H) computes the size of the hy-
pothesis and the its generality respectively. The
size of a hypothesis is measured as the number
of atoms in the hypothesis whereas generality is
measured by the number of positive examples the
hypothesis covers. m is the number of examples
covered by the hypothesis and dm is a normaliz-
ing constant. The function ln p(H|E) decreases
with increases in sz(H) and g(H). As the number
of examples covered (m) grow, the requirements
on g(H) become even stricter. This property fa-
cilitates the ability to learn more complex rules
as they are supported by more positive examples.
For more information on Progol and the computa-
tion of Bayes? posterior estimation, please refer to
(Muggleton, 1995).
3.2 Discourse Parsing with Progol
We model the problem of assigning the correct
rhetorical relation as a classification task within
the ILP framework. The rich verb semantic repre-
sentation of pairs of EDUs, as shown in Figure 3 7,
form the background knowledge and the manually
annotated rhetorical relations between the pairs of
EDUs, as shown in Figure 4, serve as the positive
examples in our learning framework. The num-
bers in the definite clauses are ids used to identify
the EDUs.
Progol constructs logic programs based on the
background knowledge and the examples in Fig-
ures 3 and 4. Mode declarations in the Progol in-
put file determines which clause to be used as the
head (i.e. modeh) and which ones to be used in
the body (i.e. modeb) of the hypotheses. Figure 5
shows an abridged set of our mode declarations.
7The output from the parser was further processed into
definite clauses.
...
agent(97,you).
together(97,event0,end,physical,liquid,water).
cause(97,you,e).
patient1(97,liquid).
patient2(97,water).
theme(98,process).
rushed(98,event0,during,process).
cause(98,AGENT98,e).
...
Figure 3: Background Knowledge for Example 1
...
relation(18,19,?Act:goal?).
relation(97,98,?Act:goal?).
relation(1279,1280,?Step1:step2?).
relation(1300,1301,?Step1:step2?).
relation(1310,1311,?Step1:step2?).
relation(412,413,?Before:after?).
relation(441,442,?Before:after?).
...
Figure 4: Positive Examples
Our mode declarations dictate that the predicate
relation be used as the head and the other pred-
icates (has possession, transfer and visible) form
the body of the hypotheses. ?*? indicates that the
number of hypotheses to learn for a given relation
is unlimited. ?+? and ?-? signs indicate variables
within the predicates of which the former is an in-
put variable and the latter an output variable. ?#?
is used to denote a constant. Each argument of the
predicate is a type, whether a constant or a vari-
able. Types are defined as a single definite clause.
Our goal is to learn rules where the LHS of the
rule contains the relation that we wish to learn and
:- modeh(*,relation(+edu,+edu,#relationtype))?
:- modeb(*,has possession(+edu,#event,
#eventstage,+verbarg,+verbarg))?
:- modeb(*,has possession(+edu,#event,
#eventstage,+verbarg,-verbarg))?
:- modeb(*,transfer(+edu,#event,#eventstage,-verbarg))?
:- modeb(*,visible(+edu,#event,#eventstage,+verbarg))?
:- modeb(*,together(+edu,#event,
#eventstage,+verbarg,+verbarg,+verbarg))?
:- modeb(*,rushed(+edu,#event,#eventstage,+verbarg))?
Figure 5: Mode Declarations
37
RULE1:
relation(EDU1,EDU2,?Act:goal?) :-
degradation material integrity(EDU1,event0,result,C),
allow(EDU2,event0,during,C,D).
RULE2:
relation(EDU1,EDU2,?Act:goal?) :-
cause(EDU1,C,D),
together(EDU1,event0,end,E,F,G),
cause(EDU2,C,D).
RULE3:
relation(EDU1,EDU2,?Step1:step2?) :-
together(EDU2,event0,end,C,D,E),
has possession(EDU1,event0,during,C,F).
RULE4:
relation(EDU1,EDU2,?Before:after?) :-
motion(EDU1,event0,during,C),
location(EDU2,event0,start,C,D).
RULE6:
relation(EDU1,EDU2,?Act:goal?) :-
motion(EDU1,event0,during,C).
Figure 6: Rules Learned
the RHS is a CNF of the semantic predicates de-
fined in VerbNet with their arguments. Given the
amount of training data we have, the nature of the
data itself and the Bayesian framework used, Pro-
gol learns simple rules that contain just one or two
clauses on the RHS. 6 of the 68 rules that Progol
manages to learn are shown in Figure 6. RULE4
states that there is a theme in motion during the
event in EDU A (which is the first EDU) and that
the theme is located in location D at the start of
the event in EDU B (the second EDU). RULE2 is
learned from pairs of EDUs such as in Example
1. The simple rules in Figure 6 may not readily
appeal to our intuitive notion of what such rules
should include. It is not clear at this point as to
how elaborate these rules should be, in order to
correctly identify the relation in question. One
of the reasons why more complex rules are not
learned by Progol is that there aren?t enough train-
ing examples. As we add more training data in the
future, we will see if rules that are more elaborate
than the ones in Figure 6 are learned .
4 Evaluation of the Discourse Parser
Table 1 shows the sets of relations for which we
managed to obtain semantic representations (i.e.
for both the EDUs).
Relations like Preparation:act did not yield any
Relation Total Train Test
Set Set
Step1:step2: 232 188 44
Joint: 190
Goal:act: 170 147 23
General:specific: 77
Criterion:act: 53 46 7
Before:after: 53 42 11
Act:side-effect: 38
Co-temp1:co-temp2: 22
Cause:effect: 19
Prescribe-act:wrong-act: 14
Obstacle:situation: 11
Reason:act: 9
Restatement: 6
Contrast1:contrast2: 6
Circumstance:situation: 3
Act:constraint: 2
Criterion:wrong-act: 2
Set:member: 1
Act:justification: 0
Comparison: 0
Preparation:act: 0
Object:attribute: 0
Part:whole: 0
Same-unit: 0
Indeterminate: 0
908 423 85
Table 1: Relation Set Count (Total Counts include ex-
amples that yielded semantic representations for both EDUs)
examples that could potentially be used. For a
number of relations, the total number of examples
we could use were less than 50. For the time being,
we decided to use only those relation sets that had
more than 50 examples. In addition, we chose not
to use Joint and General:specific relations. They
will be included in the future. Hence, our training
and testing data consisted of the following four re-
lations: Goal:act, Step1:step2, Criterion:act and
Before:after. The total number of examples we
used was 508 of which 423 were used for training
and 85 were used for testing.
Table 2, Table 3 and Table 4 show the results
from running the system on our test data. A total
of 85 positive examples were used for testing the
system.
Table 2 evaluates our SemDP system against a
baseline. Our baseline is the majority function,
which performs at a 51.7 F-Score. SemDP outper-
forms the baseline by almost 10 percentage points
38
Discourse Precision Recall F-Score
Parser
SemDP 61.7 58.8 60.24
Baseline* 51.7 51.7 51.7
Table 2: Evaluation vs Baseline (* our baseline is
the majority function)
Relation Precision Recall F-Score
Goal:act 31.57 26.08 28.57
Step1:step2 75 75 75
Before:after 54.5 54.5 54.5
Criterion:act 71.4 71.4 71.4
Total 61.7 58.8 60.24
Table 3: Test Results for SemDP
with an F-Score of 60.24. To the best of our
knowledge, we are also not aware of any work that
uses rich semantic information for discourse pars-
ing. (Polanyi et al, 2004) do not provide any eval-
uation results at all. (Soricut and Marcu, 2003) re-
port that their SynDP parser achieved up to 63.8 F-
Score on human-segmented test data. Our result of
60.24 F-Score shows that a Discourse Parser based
purely on semantics can perform as well. How-
ever, since the corpus, the size of training data and
the set of rhetorical relations we have used differ
from (Soricut and Marcu, 2003), a direct compar-
ison cannot be made.
Table 3 breaks down the results in detail for
each of the four rhetorical relations we tested on.
Since we are learning from positive data only and
the rules we learn depend heavily on the amount
of training data we have, we expected the system
to be more accurate with the relations that have
more training examples. As expected, SemDP did
very well in labeling Step1:step2 relations. Sur-
prisingly though, it did not perform as well with
Goal:act, even though it had the second highest
number of training examples (147 in total). In fact,
SemDP misclassified more positive test examples
for Goal:act than Before:after or Criterion:act, re-
lations which had almost one third the number of
Relation Goal:act Step1:step2 Before:after Criterion:act
Goal:act 6 8 5 0
Step1:step2 6 33 5 0
Before:after 0 4 6 1
Criterion:act 0 0 2 5
Table 4: Confusion Matrix for SemDP Test Result
training examples. Overall SemDP achieved a pre-
cision of 61.7 and a Recall of 58.8.
In order to find out how the positive test exam-
ples were misclassified, we investigated the dis-
tribution of the relations classified by SemDP. Ta-
ble 4 is the confusion matrix that highlights this
issue. A majority of the actual Goal:act relations
are incorrectly classified as Step1:step1 and Be-
fore:after. Likewise, most of the misclassification
of actual Step1:step1 seems to labeled as Goal:act
or Before:after. Such misclassification occurs be-
cause the simple rules learned by SemDP are not
able to accurately distinguish cases where positive
examples of two different relations share similar
semantic predicates. Moreover, since we are learn-
ing using positive examples only, it is possible that
a positive example may satisfy two or more rules
for different relations. In such cases, the rule that
has the highest score (as calculated by formula 4)
is used to label the unseen example.
5 Conclusions and Future Work
We have shown that it is possible to learn First Or-
der Logic rules from complex semantic data us-
ing an ILP based methodology. These rules can
be used to automatically label rhetorical relations.
Moreover, our results show that a Discourse Parser
that uses only semantic information can perform
as well as the state of the art Discourse Parsers
based on syntactic and lexical information.
Future work will involve the use of syntactic in-
formation as well. We also plan to run a more thor-
ough evaluation on the complete set of relations
that we have used in our coding scheme. It is also
important that the manual segmentation and an-
notation of rhetorical relations be subject to inter-
annotator agreement. A second human annotator
is currently annotating a sample of the annotated
corpus. Upon completion, the annotated corpus
will be checked for reliability.
Data sparseness is a well known problem inMa-
chine Learning. Like most paradigms, our learn-
ing model is also affected by it. We also plan to
explore techniques to deal with this issue.
39
Lastly, we have not tackled the problem of dis-
course parsing at higher levels of the DPT and seg-
mentation in this paper. Our ultimate goal is to
build a Discourse Parser that will automatically
segment a full text as well as annotate it with
rhetorical relations at every level of the DPT using
semantic as well as syntactic information. Much
work needs to be done but we are excited to see
what the aforesaid future work will yield.
Acknowledgments
This work is supported by award 0133123 from the National
Science Foundation. Thanks to C.P. Rose? for LCFLEX, M.
Palmer and K. Kipper for VerbNet, C. Buitelaar for CoreLex,
and Stephen Muggleton for Progol.
References
Paul Buitelaar. 1998. CoreLex: Systematic Polysemy
and Underspecification. Ph.D. thesis, Computer Science,
Brandeis University, February.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the frame-
work of Rhetorical Structure Theory. In Current Direc-
tions in Discourse and Dialogue, pp. 85-112, Jan van Kup-
pevelt and Ronnie Smith eds., Kluwer Academic Publish-
ers.
Michael Collins. 2003. Head-driven statistical methods for
natural language parsing. Computational Linguistics, 29.
Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad, Anoop
Sarkar, Aravind Joshi and Bonnie Webber. 2001. D-
LTAG System - Discourse Parsing with a Lexicalized Tree
Adjoining Grammar. Information Stucture, Discourse
Structure and Discourse Semantics, ESSLLI, 2001.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
1994. COMLEX syntax: Building a computational lex-
icon. In COLING 94, Proceedings of the 15th Interna-
tional Conference on Computational Linguistics, pages
472?477, Kyoto, Japan, August.
Paul Kingsbury and Martha Palmer. 2000. From Treebank
to Propbank. In Third International Conference on Lan-
guage Resources and Evaluation, LREC-02, Las Palmas,
Canary Islands, Spain, May 28 - June 3, 2002.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000.
Class-based construction of a verb lexicon. In AAAI-2000,
Proceedings of the Seventeenth National Conference on
Artificial Intelligence, Austin, TX.
Beth Levin and Malka Rappaport Hovav. 1992. Wiping the
slate clean: a lexical semantic exploration. In Beth Levin
and Steven Pinker, editors, Lexical and Conceptual Se-
mantics, Special Issue of Cognition: International Journal
of Cognitive Science. Blackwell Publishers.
William C. Mann and Sandra Thompson. 1988. Rhetorical
Structure Theory: toward a Functional Theory of Text Or-
ganization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An unsuper-
vised approach to recognizing discourse relations. In Pro-
ceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL-2002), Philadelphia, PA,
July.
Daniel Marcu, Magdalena Romera and Estibaliz Amorrortu.
1999. Experiments in Constructing a Corpus of Discourse
Trees: Problems, Annotation Choices, Issues. In The
Workshop on Levels of Representation in Discourse, pages
71-78, Edinburgh, Scotland, July.
M. G. Moser, and J. D. Moore. 1995. Using Discourse
Analysis and Automatic Text Generation to Study Dis-
course Cue Usage. In AAAI Spring Symposium on Empir-
ical Methods in Discourse Interpretation and Generation,
1995.
Stephen H. Muggleton. 1995. Inverse Entailment and Pro-
gol. In New Generation Computing Journal, Vol. 13, pp.
245-286, 1995.
Martha Palmer, Daniel Gildea and, Paul Kingsbury. 2005.
The Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?105.
Livia Polanyi, Christopher Culy, Martin H. van den Berg,
Gian Lorenzo Thione, and David Ahn. 2004. Senten-
tial Structure and Discourse Parsing. Proceedings of the
ACL2004 Workshop on Discourse Annotation, Barcelona,
Spain, July 25, 2004.
James Pustejovsky. 1991. The generative lexicon. Computa-
tional Linguistics, 17(4):409?441.
Carolyn Penstein Rose? and Alon Lavie. 2000. Balancing ro-
bustness and efficiency in unification-augmented context-
free parsers for large practical applications. In Jean-
Clause Junqua and Gertjan van Noord, editors, Robustness
in Language and Speech Technology. Kluwer Academic
Press.
Radu Soricut and Daniel Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Information.
In Proceedings of the Human Language Technology and
North American Assiciation for Computational Linguis-
tics Conference (HLT/NAACL-2003), Edmonton, Canada,
May-June.
Elena Terenzi and Barbara Di Eugenio. 2003. Building lex-
ical semantic representations for natural language instruc-
tions. In HLT-NAACL03, 2003 Human Language Tech-
nology Conference, pages 100?102, Edmonton, Canada,
May. (Short Paper).
40
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 55?63,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
KSC-PaL: A Peer Learning Agent that Encourages Students to take the
Initiative?
Cynthia Kersey and Barbara Di Eugenio
Department of Computer Science
University of Illinois at Chicago
Chicago, IL 60607 USA
ckerse2@uic.edu
bdieugen@cs.uic.edu
Pamela Jordan and Sandra Katz
Learning Research and Development Center
University of Pittsburgh
Pittsburgh, PA 15260 USA
pjordan+@pitt.edu
katz+@pitt.edu
Abstract
We present an innovative application of dis-
course processing concepts to educational
technology. In our corpus analysis of peer
learning dialogues, we found that initiative
and initiative shifts are indicative of learn-
ing, and of learning-conducive episodes. We
are incorporating this finding in KSC-PaL, the
peer learning agent we have been developing.
KSC-PaL will promote learning by encourag-
ing shifts in task initiative.
1 Introduction
Collaboration in dialogue has long been researched
in computational linguistics (Chu-Carroll and Car-
berry, 1998; Constantino-Gonza?lez and Suthers,
2000; Jordan and Di Eugenio, 1997; Lochbaum and
Sidner, 1990; Soller, 2004; Vizca??no, 2005), how-
ever, the study of peer learning from a computa-
tional perspective is still in the early stages. This
is an important area of study because peer learning
has been shown to be an effective mode of learn-
ing, potentially for all of the participants (Cohen et
al., 1982; Brown and Palincsar, 1989; Birtz et al,
1989; Rekrut, 1992). Additionally, while there has
been a focus on using natural language for intelli-
gent tutoring systems (Evens et al, 1997; Graesser
et al, 2004; VanLehn et al, 2002), peer to peer in-
teractions are notably different from those of expert-
novice pairings, especially with respect to the rich-
ness of the problem-solving deliberations and ne-
gotiations. Using natural language in collaborative
?This work is funded by NSF grants 0536968 and 0536959.
learning could have a profound impact on the way
in which educational applications engage students in
learning.
Previous research has suggested several mecha-
nisms that explain why peer learning is effective for
all participants. Among them are: self-directed ex-
plaining(Chi et al, 1994), other-directed explaining
(Ploetzner et al, 1999; Roscoe and Chi, 2007) and
Knowledge Co-construction ? KCC for short (Haus-
mann et al, 2004). KCC episodes are defined as
portions of the dialogue in which students are jointly
constructing a shared meaning of a concept required
for problem solving. This last mechanism is the
most interesting from a peer learning perspective be-
cause it is a truly collaborative construct and also be-
cause it is consistent with the widely accepted con-
structivist view of learning.
Since KCC is a high-level concept that is not eas-
ily recognized by an artificial agent we collected
peer learning interactions from students and stud-
ied them to identify features that might be useful in
identifying KCC. We found that linguistically based
initiative shifts seem to capture the notion of col-
laborative construction. A more thorough analysis
found a strong relationship between KCC and initia-
tive shifts and moderate correlations between initia-
tive shifts and learning.
The results of this analysis are being incorporated
into KSC-PaL, an artificial agent that can collaborate
with a human student via natural-language dialogue
and actions within a graphical workspace. KSC-PaL
has been developed in the last two years. Dialogue-
wise, its core is TuTalk (Jordan et al, 2007), a dia-
logue management system that supports natural lan-
55
guage dialogue in educational applications. As we
will describe, we have already developed its user
interface and its student model and have extended
TuTalk?s planner to provide KSC-PaL with the abil-
ity to induce initiative shifts. For the version of
KSCPal we will present in this paper, we wanted to
focus on the question of whether this style of inter-
action helps learning; and we were concerned that
its limitations in disambiguating the student?s input
could impact this interaction. Hence, this round of
experiments employs a human ?helper? that is given
a list of concepts the input may match, and chooses
the most appropriate one.
The work presented in this paper is part of a larger
research program: we analyze different paradigms ?
tutoring dialogues and peer-learning dialogues? in
the same basic domain, devise computational mod-
els for both, and implement them in two separate
SW systems, an ITS and the peer-learning system
we present here. For our work on the tutoring dia-
logue corpus and the ITS please see (Fossati et al,
accepted for publication 2009).
Our domain in both cases is problem solving in
basic data structure and algorithms, which is part of
foundations of Computer Science. While in recent
years, interest in CS in the US has dropped dramat-
ically, CS is of enormous strategic interest, and is
projected to foster vast job growth in the next few
years (AA. VV., 2006). We believe that by support-
ing CS education in its core we can have the largest
impact on reversing the trend of students? disinter-
est. Our belief is grounded in the observation that
the rate of attrition is highest at the earliest phases
of undergraduate CS curricula. This is due in part
to students? difficulty with mastering basic concepts
(Katz et al, 2003), which require a deep understand-
ing of static structures and the dynamic procedures
used to manipulate them (AA. VV., 2001). These
concepts also require the ability to move seamlessly
among multiple representations, such as text, pic-
tures, pseudo-code, and real code in a specific pro-
gramming language.
Surprisingly, few educational SW systems ad-
dress CS topics, e.g. teaching a specific program-
ming language like LISP (Corbett and Anderson,
1990) or database concepts (Mitrovic? et al, 2004).
Additionally, basically they are all ITSs, where the
relationship between the system and the student
is one of ?subordination?. Only two or three of
these ITSs address foundations, including: Autotu-
tor (Graesser et al, 2004) addresses basic literacy,
but not data structures or algorithms; ADIS (Waren-
dorf and Tan, 1997) tutors on basic data structures,
but its emphasis is on visualization, and it appears to
have been more of a proof of concept than a work-
ing system; ProPL (Lane and VanLehn, 2003) helps
novices design their programs, by stressing problem
solving and design skills.
In this paper, we will first discuss the collection
and analysis of peer learning interactions. Then, we
discuss the design of our peer agent, and how it is
guided by the results of our analysis. We conclude
by briefly describing the user experiments we are
about to undertake, and whose preliminary results
will be available at the time of the workshop.
2 Data collection
We have collected peer learning interactions from 15
pairs of students solving problems in the domain of
computer science data structures. Students were re-
cruited from introductory courses on data structures
and algorithms. Each problem involved one of three
types of data structures: linked-lists, stacks and bi-
nary search trees. Each problem was either a debug-
ging problem where the students were asked to work
together to identify errors in the code or an explana-
tion problems in which the students jointly created
an explanation of a segment of code.
The students interacted using a computer me-
diated interface1 where they could communicate
via text-based chat, drawing and making changes
to code (see Figure 1). The graphical workspace
(drawing and coding areas) was shared such that
changes made by one student were propagated to
his/her partner?s workspace. Access to this graph-
ical workspace was controlled so that only one stu-
dent was allowed to draw or make changes to code
at any point in time.
Each pair was presented with a total of 5 prob-
lems, although not all pairs completed all prob-
lems due to time limitations. The interactions for
each pair were subdivided into separate dialogues
1Using text to communicate versus face-to-face interactions
should be comfortable for most students given the prevalence
of communication methods such as text messaging and instant
messengers.
56
Figure 1: The data collection / KSC-PaL interface
for each problem. Thus, we collected a corpus con-
sisting of a total of 73 dialogues.
In addition to collecting problem solving data,
we also presented each student with a pre-test prior
to problem solving and an identical post-test at the
conclusion of problem solving in order to measure
learning gains. A paired t-test of pre- and post-test
scores showed that students did learn during collab-
orative problem solving (t(30)=2.83; p=0.007). The
interactions produced an average normalized learn-
ing gain of 17.5 (possible total points are 50).
3 Analysis of Peer Learning Interactions
Next, we undertook an extensive analysis of the cor-
pus of peer learning interactions in order to deter-
mine the behaviors with which to endow KSC-PaL.
3.1 Initiative: Annotation
Given the definition of KCC, it appeared to us that
the concept of initiative from discourse and dialogue
processing should play a role: intuitively, if the stu-
dents are jointly contructing a concept, the initiative
cannot reside only with one, otherwise the partner
would just be passive. Hence, we annotated the dia-
logues for both KCC and initiative.
The KCC annotation involved coding the dia-
logues for KCC episodes. These are defined as a
series of utterances and graphical actions in which
students are jointly constructing a shared meaning of
a concept required for problem solving (Hausmann
et al, 2004). Using this definition, an outside anno-
tator and one of the authors coded 30 dialogues (ap-
proximately 46% of the corpus) for KCC episodes.
This entailed marking the beginning utterance and
the end utterance of such episodes, under the as-
sumption that all intervening utterances do belong to
the same KCC episode (otherwise the coder would
mark an earlier end for the episode). The result-
ing intercoder reliability, measured with the Kappa
statistic(Carletta, 1996), is considered excellent (? =
0.80).
Our annotation of initiative was two fold. Since
there is disagreement in the computational lin-
guistics community as to the precise definition of
57
initiative(Chu-Carroll and Carberry, 1998; Jordan
and Di Eugenio, 1997), we annotated the dialogues
for both dialogue initiative, which tracks who is
leading the conversation and determining the cur-
rent conversational focus, and task initiative, which
tracks the lead in problem solving.
For dialogue initiative annotation, we used the
well-known utterance-based rules for allocation of
control from (Walker and Whittaker, 1990). In
this scheme, each utterance is tagged with one of
four dialogue acts (assertion, command, question or
prompt) and control is then allocated based on a set
of rules. The dialogue act annotation was done au-
tomatically, by marking turns that end in a question
mark as questions, those that start with a verb as
commands, prompts from a list of commonly used
prompts (e.g. ok, yeah) and the remaining turns as
assertions. To verify that the automatic annotation
was good, we manually annotated a sizable portion
of the dialogues with those four dialogue acts. We
then compared the automatic annotation against the
human gold standard, and we found an excellent ac-
curacy: it ranged from 86% for assertions and ques-
tions, to 97% for prompts, to 100% for commands.
Once the dialogue acts had been automatically an-
notated, two coders, one of the authors and an out-
side annotator, coded 24 dialogues (1449 utterances,
approximately 45% of the corpus) for dialogue ini-
tiative, by using the four control rules from (Walker
and Whittaker, 1990):
1. Assertion: Control is allocated to the speaker
unless it is a response to a question.
2. Command: Control is allocated to the speaker.
3. Question: Control is allocated to the speaker,
unless it is a response to a question or a com-
mand.
4. Prompt: Control is allocated to the hearer.
The resulting intercoder reliability on dialogue ini-
tiative was 0.77, a quite acceptable level of agree-
ment. We then experimented with automatically an-
notating dialogue initiative according to those con-
trol rules. Since the accuracy against the gold stan-
dard was 82%, the remaining 55% of the corpus was
also automatically annotated for dialogue initiative,
using those four control rules.
As concerns task initiative, we define it as any ac-
tion by a participant to either achieve a goal directly,
decompose a goal or reformulate a goal (Guinn,
1998; Chu-Carroll and Brown, 1998). Actions in
our domain that show task initiative include:
? Explaining what a section of code does.
? Identifying that a section of code as correct or
incorrect.
? Suggesting a correction to a section of code
? Making a correction to a section of code prior
to discussion with the other participant.
The same two coders annotated for task initiative
the same portion of the corpus already annotated for
dialogue initiative. The resulting intercoder reliabil-
ity for task initiative is 0.68, which is high enough
to support tentative conclusions. The outside coder
then manually coded the remaining 55% of the cor-
pus for task initiative.
3.2 KCC, initiative and learning
In analyzing the annotated dialogues, we used mul-
tiple linear regression to identify correlations of the
annotated features and post-test score. We used pre-
test score as a covariate because of its significant
positive correlations with post-test score. Due to
variations in student ability in the different problem
types, our analysis focused only on a portion of the
collected interactions. In the tree problem there was
a wide variation in experience level of the students
which would inhibit KCC. In the stack problem, the
students had a better understanding of stacks prior
to problem solving and spent less time in discussion
and problem solving. Thus, our analysis focused
only on the linked-list problems.
We started by analyzing the relationship between
KCC and learning. As a measurement of KCC we
used KCC actions which is the number of utter-
ances and graphical actions that occur during KCC
episodes. This analysis showed that KCC does have
a positive correlation with learning in our corpus. In
Table 1, the first row shows the benefit for the dyad
overall by correlating the mean post-test score with
the mean pre-test score and the dyad?s KCC actions.
The second row shows the benefit for individuals by
58
correlating individual post-test scores with individ-
ual pre-test scores and the dyad?s KCC actions. The
difference in the strength of these correlations sug-
gests that members of the dyads are not benefitting
equally from KCC. If the subjects are divided into
two groups, those with a pre-test score below the
mean score ( n=14) and those with a pre-test score
above the mean score ( n=16) , it can be seen that
those with a low pre-test score benefit more from
the KCC episodes than do those with a high pre-test
score (rows 3 and 4 in Table 1).
KCC actions predict ? R2 p
Mean post-test score 0.43 0.14 0.02
Individual post-test score 0.33 0.08 0.03
Individual post-test score 0.61 0.37 0.03
(low pre-test subjects)
Individual post-test score 0.33 0.09 ns
(high pre-test subjects)
Table 1: KCC Actions as Predictor of Post-test Score
Next, we explored the relationship between learn-
ing and the number of times initiative shifted be-
tween the students. Intuitively, we assumed that fre-
quent shifts of initiative would reflect students work-
ing together to solve the problem. We found there
was a significant correlation between post-test score
(after removing the effects of pre-test scores) and the
number of shifts in dialogue initiative and the num-
ber of shifts in task initiative (see Table 2). This
analysis excluded two dyads whose problem solving
collaboration had gone awry.
Predictor of Post-test ? R2 p
Dialogue initiative shifts 0.45 0.20 0.00
Task initiative shifts 0.42 0.20 0.01
Table 2: Initiative Predictors of Post-test Score
We then computed a second measure of KCC that
is meant to reflect the density of the KCC episodes.
KCC initiative shifts is the number of task initiative
shifts that occur during KCC episodes. Many task
initiative shifts reflect more active KCC.
Table 3 uses KCC initiative shifts as the measure
of co-construction. It shows similar results to ta-
ble 1, where KCC actions was used. Note that when
the outlier dyads were removed the correlation with
learning is much stronger for the low pre-test score
subjects when KCC initiative shifts are used as the
measure of KCC (R2 = 0.45, p = 0.02) than when
KCC actions are used.
KCC initiative shifts predict ? R2 p
Mean post-test score 0.46 0.15 0.01
Individual post-test score 0.35 0.09 0.02
Individual post-test score 0.67 0.45 0.02
(low pre-test subjects)
Individual post-test score 0.10 0.01 ns
(high pre-test subjects)
Table 3: KCC Initiative Shifts Predictors of Post-test
Score
Lastly we investigated the hypothesis that KCC
episodes involve frequent shifts in initiative, as both
participants are actively participating in problem
solving. To test this hypothesis, we calculated
the average initiative shifts per line during KCC
episodes and the average initiative shifts per line
during problem solving outside of KCC episodes for
each dyad. A paired t-test was then used to verify
that there is a difference between the two groups.
The t-test showed no significant difference in aver-
age dialogue initiative shifts in KCC episodes com-
pared with non-KCC problem solving. However,
there is a significant difference between average task
initiative shifts in KCC episodes compared with the
rest of the dialogue ( t(57) = 3.32, p = 0.0016). The
effect difference between the two groups (effect size
= 0.65 ) shows that there is a meaningful increase in
the number of task initiative shifts in KCC episodes
compared with problem solving activity outside of
the KCC episodes.
3.3 Indicators of task initiative shifts
Since our results show that task initiative shifts are
conducive to learning, we want to endow our soft-
ware agent with the ability to encourage a shift in
initiative from the agent to the student, when the
student is overly passive. The question is, what are
natural indicators in dialogue that the partner should
take the initiative? We explored two different meth-
ods for encouraging initiative shifts. One is that stu-
dent uncertainty may lead to a shift in initiative. The
other consists of cues for initiative shifts identified
59
in related literature(Chu-Carroll and Brown, 1998;
Walker and Whittaker, 1990).
Intuitively, uncertainty by a peer might lead to his
partner taking the initiative. One possible identi-
fier of student uncertainty is hedging. To validate
this hypothesis, we annotated utterances in the cor-
pus with hedging categories as identified in (Bhatt
et al, 2004). Using these categories we were unable
to reliably annotate for hedging. But, after collaps-
ing the categories into a single binary value of hedg-
ing/not hedging we arrived at an acceptable agree-
ment (? = 0.71).
Another identifier of uncertainty is a student?s re-
quest for feedback from his partner. When uncertain
of his contribution, a student may request an evalua-
tion from his peer. So, we annotated utterances with
?request for feedback? and were able to arrive at an
excellent level of intercoder reliability (? = 0.82).
(Chu-Carroll and Brown, 1998) identifies cues
that may contribute to the shift of task and dialogue
initiative. Since task initiative shifts appear to iden-
tify KCC episodes, we chose to explore the follow-
ing cues that potentially result in the shift of task
initiative.
? Give up task. These are utterances where
the student explicitly gives up the task using
phrases like ?Any other ideas??.
? Pause. A pause may suggest that the speaker
has nothing more to say in the current turn and
intends to give up his initiative.
? Prompts. A prompt is an utterance that has no
propositional content.
? Invalid statements. These are incorrect state-
ments made by a student.
Using hedging, request for feedback and initia-
tive cues, we were able to identify 283 shifts in task
initiative or approximately 67% of all task initiative
shifts in the corpus. The remaining shifts were likely
an explicit take over of initiative without a preceding
predictor.
Since we found several possible ways to predict
and encourage initiative shifts, the next step was to
identify which of these predictors more often re-
sulted in an initiative shift; and, for which predic-
tors the resulting initiative shift more often led to an
increase in the student?s knowledge level. Table 4
shows the percentage of instances of each predictor
that resulted in an initiative shift.
Percent of instances that
Cue/Identifier led to initiative shift
Hedge 23.94%
Request feedback 21.88%
Give-up task 20.00%
Pause 25.27%
Prompt 29.29%
Invalid statement 38.64%
Table 4: Cues for Shifts in Initiative
Along with the likelihood of a predictor leading
to an initiative shift, we also examined the impact
of a shift of task initiative on a student?s level of
knowledge, measured using knowledge score, cal-
culated on the basis of the student model (see Sec-
tion 4). This is an important characteristic since we
want to encourage initiative shifts in an effort to in-
crease learning. First, we analyzed initiative shifts
to determine if they resulted in an increase in knowl-
edge score. We found that in our corpus, an initiative
shift leads to an increase in a student?s knowledge
level in 37.0% of task initiative shifts, a decrease
in knowledge level in 5.2% of shifts and unchanged
in 57.8% of shifts. Even though over one-half of
the time knowledge scores were not impacted, in
only a small minority of instances did a shift have
a negative impact on a student?s level of knowledge.
Therefore, we more closely examined the predictors
to see which more frequently led to an increase in
student knowledge. The results of that analysis is
show in table 5.
Percent of shifts where
Predictor knowledge level increased
Hedge 23.52%
Request feedback 17.65%
Give-up task 0.00%
Prompt 32.93%
Pause 14.22%
Invalid statement 23.53%
Table 5: Task Initiative Shifts/Knowledge Level Change
60
4 KSC-PaL, a software peer
Our peer-learning agent, KSC-PaL, has at its core
the TuTalk System(Jordan et al, 2007), a dialogue
management system that supports natural language
dialogue in educational applications. Since TuTalk
does not include an interface or a student model, we
developed both in previous years. We also needed to
extend the TuTalk planner to recognize and promote
initiative shifts.
The user interface is structured similarly to the
one used in data collection(see Figure 1). How-
ever, we added additional features to allow a stu-
dent to effectively communicate with the KSC-PaL.
First, all drawing and coding actions of the student
are interpreted and passed to the agent as a natural
language utterance. Graphical actions are matched
to a set of known actions and when a student sig-
nals that he/she has finished drawing or coding ei-
ther by ceding control of the graphical workspace or
by starting to communicate through typed text, the
interface will attempt to match what the student has
drawn or coded with its database of known graphi-
cal actions. These graphical actions include not only
correct ones but also anticipated misconceptions that
were collected from the data collection interactions.
The second enhancement to the interface is a spell
corrector for ?chat slang?. We found in the corpus,
that students often used abbreviations that are com-
mon to text messaging. These abbreviations are not
recognized by the English language spell corrector
in the TuTalk system, so a chat slang interpretation
module was added.
KSC-PaL requires a student model to track the
current state of problem solving as well as esti-
mate the student?s knowledge of concepts involved
in solving the problem in order to guide its behav-
ior. Our student model incorporates problem solu-
tion graphs (Conati et al, 2002). Solution graphs
are Bayesian networks where each node represents
either an action required to solve the problem, a
concept required as part of problem solving or an
anticipated misconception. A user?s utterances and
actions are then matched to these nodes. A knowl-
edge score can be calculated at any point in time by
taking a sum of the probabilities of all nodes in the
graph, except the misconception nodes. The sum of
the probabilities of the misconception nodes are sub-
tracted from the total to arrive at a knowledge score.
This score is then normalized by dividing it by the
maximum possible knowledge score for the solution
graph.
4.1 KSC-PaL and initiative
Since our corpus study showed that the level of task
initiative can be used to identify when KCC and
potentially learning is occurring, we have endowed
KSC-PaL with behaviors to manipulate shifts in task
initiative in order to encourage KCC and learning.
This required three enhancements: first, the ability
to recognize the initiative holder in each utterance
or action; second, the ability to encourage the shift
of initiative from the agent to the student; and three,
extending the TuTalk planner so that it can process
task initiative shifts.
As concerns the first step, that the agent recog-
nize the initiative holder in each utterance or action,
we resorted to machine learning. Using the Weka
Toolkit(Witten and Frank, 2005), we explored var-
ious machine learning algorithms and feature sets
that could reliably identify the holder of task initia-
tive. We found that the relevant features of an ac-
tion in the graphical workspace were substantially
different from those of a natural language utterance.
Therefore, we trained and tested separate classifiers
for each type of student action. After examining a
wide variety of machine learning algorithms we se-
lected the following two classifiers: (1) K* (Cleary
and Trigg, 1995), a clustering algorithm, for clas-
sifying natural language utterances which correctly
classified 71.7699% of utterance and (2) JRip (Co-
hen, 1995), a rule-based algorithm, for classifying
drawing and coding actions which correctly classi-
fied 86.971% of the instances.
As concerns the second step, encouraging initia-
tive shifts so that the student assumes the task initia-
tive, we use the results of our analysis of the indica-
tors of task initiative shifts from Section 3.3. KSC-
PaL will use prompts, request feedback and make
invalid statements in order to encourage initiative
shifts and promote learning.
Finally, we augmented the TuTalk planner so that
it selects scripts to manage task initiative shifts. Two
factors will determine whether a script that encour-
ages initiative shifts will be selected: the current
level of initiative shifts and the change in the stu-
61
dent?s knowledge score. Task initiative shifts will be
tracked using the classifier described above. Scripts
will be selected to encourage initiative shifts when
the average level of initiative shifts is less than the
mean initiative shifts in KCC episodes (calculated
from the corpus data) and the student?s knowledge
level has not increased since the last time a script
selection was requested. The scripts are based on
the analysis of methods for encouraging initiative
shifts described above. Specifically, KSC-PaL will
encourage initiative shifts by responding to student
input using prompts, requesting feedback from the
student and encouraging student criticism by inten-
tionally making errors in problem solving.
We are now poised to run user experiments. We
will run subjects in two conditions with KSC-PaL:
in the first condition (control), KSC-PaL will not en-
courage task initiative shifts and act more as a tutor;
in the second condition, KSC-PaL will encourage
task initiative shifts as we just discussed. One final
note: because we do not want our experiments to be
affected by the inability of the agent to interpret an
utterance, given current NLU technology, the inter-
face will ?incorporate? a human interpreter. The in-
terpreter will receive student utterances along with a
list of possible matching concepts from TuTalk. The
interpreter will select the most likely matching con-
cept, thus assisting TuTalk in natural language in-
terpretation. Note that the interpreter has a limited,
predetermined sets of choices, corresponding to the
concepts TuTalk knows about. In this way, his / her
intervention is circumscribed.
5 Conclusions
After an extensive analysis of peer-learning interac-
tions, we have found that task initiative shifts can
be used to determine when students are engaged
in knowledge co-construction. We have embed-
ded this finding in a peer-learning agent, KSC-PaL,
that varies its behavior to encourage initiative shifts
and knowledge co-construction in order to promote
learning. We are poised to run our user experiments,
and we will have preliminary results available by the
workshop time.
References
AA. VV. 2001. Computer Science, Final Report, The
Joint Task Force on Computing Curricula. IEEE Com-
puter Society and Association for Computing Machin-
ery, IEEE Computer Society.
AA. VV. 2006. US bureau of labor statistics
http://www.bls.gov/oco/oco20016.htm.
Khelan Bhatt, Martha Evens, and Shlomo Argamon.
2004. Hedged responses and expressions of affect in
human/human and human computer tutorial interac-
tions. In Proceedings Cognitive Science.
M. W. Birtz, J. Dixon, and T. F. McLaughlin. 1989. The
effects of peer tutoring on mathematics performance:
A recent review. B. C. Journal of Special Education,
13(1):17?33.
A. L. Brown and A. S. Palincsar, 1989. Guided, cooper-
ative learning and individual knowledge acquisition,
pages 307?226. Lawrence Erlbaum Associates, Hills-
dale, NJ.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22(2):249?254.
M.T.H. Chi, N. De Leeuw, M.H. Chiu, and C. LaVancher.
1994. Eliciting self-explanations improves under-
standing. Cognitive Science, 18(3):439?477.
Jennifer Chu-Carroll and Michael K. Brown. 1998. An
evidential model for tracking initiative in collabora-
tive dialogue interactions. User Modeling and User-
Adapted Interaction, 8(3?4):215?253, September.
Jennifer Chu-Carroll and Sandra Carberry. 1998. Col-
laborative response generation in planning dialogues.
Computational Linguistics, 24(3):355?400.
John G. Cleary and Leonard E. Trigg. 1995. K*: An
instance-based learner using an entropic distance mea-
sure. In Proc. of the 12th International Conference on
Machine Learning, pages 108?114.
P.A. Cohen, J.A. Kulik, and C.C. Kulik. 1982. Educa-
tion outcomes of tutoring: A meta-analysis of findings.
American Education Research Journal, 19(2):237?
248.
William W. Cohen. 1995. Fast effective rule induction.
In Machine Learning: Proceedings of the Twelve In-
ternational Conference.
Cristina Conati, Abigail Gertner, and Kurt Vanlehn.
2002. Using bayesian networks to manage uncer-
tainty in student modeling. User Modeling and User-
Adapted Interaction, 12(4):371?417.
Mar??a de los Angeles Constantino-Gonza?lez and
Daniel D. Suthers. 2000. A coached collaborative
learning environment for entity-relationship modeling.
Intelligent Tutoring Systems, pages 324?333.
Albert T. Corbett and John R. Anderson. 1990. The ef-
fect of feedback control on learning to program with
the LISP tutor. In Proceedings of the Twelfth Annual
Conference of the Cognitive Science Society, pages
796?803.
62
Martha W. Evens, Ru-Charn Chang, Yoon Hee Lee,
Leem Seop Shim, Chong Woo Woo, Yuemei Zhang,
Joel A. Michael, and Allen A. Rovick. 1997. Circsim-
tutor: an intelligent tutoring system using natural lan-
guage dialogue. In Proceedings of the fifth conference
on Applied natural language processing, pages 13?14,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Davide Fossati, Barbara Di Eugenio, Christopher Brown,
Stellan Ohlsson, David Cosejo, and Lin Chen. ac-
cepted for publication, 2009. Supporting Computer
Science curriculum: Exploring and learning linked
lists with iList. EEE Transactions on Learning Tech-
nologies, Special Issue on Real-World Applications of
Intelligent Tutoring Systems.
Arthur C. Graesser, Shulan Lu, George Tanner Jackson,
Heather Hite Mitchell, Mathew Ventura, Andrew Ol-
ney, and Max M. Louwerse. 2004. Autotutor: A tutor
with dialogue in natural language. Behavior Research
Methods, Instruments, & Computers, 36:180?192(13),
May.
Curry I. Guinn. 1998. An analysis of initiative selection
in collaborative task-oriented discourse. User Model-
ing and User-Adapted Interaction, 8(3-4):255?314.
Robert G.M. Hausmann, Michelene T.H. Chi, and Mar-
guerite Roy. 2004. Learning from collaborative prob-
lem solving: An analysis of three hypothesized mech-
anisms. In K.D Forbus, D. Gentner, and T. Regier, edi-
tors, 26th Annual Conference of the Cognitive Science
Society, pages 547?552, Mahwah, NJ.
Pamela W. Jordan and Barbara Di Eugenio. 1997. Con-
trol and initiative in collaborative problem solving di-
alogues. In Working Notes of the AAAI Spring Sympo-
sium on Computational Models for Mixed Initiative,
pages 81?84, Menlo Park, CA.
Pamela W Jordan, Brian Hall, Michael A. Ringenberg,
Yui Cue, and Carolyn Penstein Rose?. 2007. Tools for
authoring a dialogue agent that participates in learning
studies. In Artificial Intelligence in Education, AIED
2007, pages 43?50.
S. Katz, J. Aronis, D. Allbritton, C. Wilson, and M.L.
Soffa. 2003. Gender and race in predicting achieve-
ment in computer science. Technology and Society
Magazine, IEEE, 22(3):20?27.
H. Chad Lane and Kurt VanLehn. 2003. Coached pro-
gram planning: dialogue-based support for novice pro-
gram design. SIGCSE Bull., 35(1):148?152.
Karen E. Lochbaum and Candace L Sidner. 1990. Mod-
els of plans to support communication: An initial re-
port. In Proceedings of the Eighth National Confer-
ence on Artificial Intelligence, pages 485?490. AAAI
Press.
A. Mitrovic?, P. Suraweera, B. Martin, and A. Weeras-
inghe. 2004. DB-Suite: Experiences with Three In-
telligent, Web-Based Database Tutors. Journal of In-
teractive Learning Research, 15(4):409?433.
R. Ploetzner, P. Dillenbourg, M. Preier, and D. Traum.
1999. Learning by explaining to oneself and to others.
Collaborative learning: Cognitive and computational
approaches, pages 103?121.
M. D. Rekrut. 1992. Teaching to learn: Cross-age tutor-
ing to enhance strategy instruction. American Educa-
tion Research Association.
Rod D. Roscoe and Michelene T. H. Chi. 2007.
Understanding tutor learning: Knowledge-building
and knowledge-telling in peer tutors? explanations
and questions. Review of Educational Research,
77(4):534?574.
Amy Soller. 2004. Computational modeling and analysis
of knowledge sharing in collaborative distance learn-
ing. User Modeling and User-Adapted Interaction,
Volume 14(4):351?381, January.
Kurt VanLehn, Pamela W. Jordan, Carolyn Penstein
Rose?, Dumisizwe Bhembe, Michael Bo?ttner, Andy
Gaydos, Maxim Makatchev, Umarani Pappuswamy,
Michael A. Ringenberg, Antonio Roque, Stephanie
Siler, and Ramesh Srivastava. 2002. The architec-
ture of why2-atlas: A coach for qualitative physics es-
say writing. In ITS ?02: Proceedings of the 6th Inter-
national Conference on Intelligent Tutoring Systems,
pages 158?167, London, UK. Springer-Verlag.
Aurora Vizca??no. 2005. A simulated student can im-
prove collaborative learning. International Journal of
Artificial Intelligence in Education, 15(1):3?40.
Marilyn Walker and Steve Whittaker. 1990. Mixed ini-
tiative in dialogue: an investigation into discourse seg-
mentation. In Proceedings of the 28th annual meeting
on Association for Computational Linguistics, pages
70?78, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Kai Warendorf and Colin Tan. 1997. Adis-an animated
data structure intelligent tutoring system or putting an
interactive tutor on the www. In Intelligent Educa-
tional Systems on the World Wide Web (Workshop Pro-
ceedings), at the Eight International Conference on
Artficial Intellignece in Education.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, San Francisco.
63
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 17?20,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
KSC-PaL: A Peer Learning Agent that Encourages Students to take the
Initiative?
Cynthia Kersey
Lewis University
Romeoville, IL 60446 USA
kerseycy@lewisu.edu
Barbara Di Eugenio
University of Illinois at Chicago
Chicago, IL 60607 USA
bdieugen@cs.uic.edu
Pamela Jordan and Sandra Katz
University of Pittsburgh
Pittsburgh, PA 15260 USA
pjordan+@pitt.edu
katz+@pitt.edu
Abstract
We present an innovative application of dia-
logue processing concepts to educational tech-
nology. In a previous corpus analysis of peer
learning dialogues, we found that initiative
and initiative shifts are indicative of learning,
and of learning-conducive episodes. We have
incorporated this finding in KSC-PaL, a peer
learning agent. KSC-PaL promotes learning
by encouraging shifts in task initiative.
1 Introduction
Collaborative learning has been shown to be an ef-
fective mode of learning for potentially all partic-
ipants (Brown and Palincsar, 1989; Fisher, 1993;
Tin, 2003). While collaboration in dialogue has long
been researched in computational linguistics (Chu-
Carroll and Carberry, 1998; Constantino-Gonza?lez
and Suthers, 2000; Jordan and Di Eugenio, 1997;
Soller, 2004), the study of peer learning from a com-
putational perspective is still in the early stages.
Previous research has suggested several mecha-
nisms that explain why peer learning is effective.
Among them are: self-directed explaining (Chi et
al., 1994), other-directed explaining (Ploetzner et
al., 1999; Roscoe and Chi, 2007) and Knowledge
Co-construction ? KCC for short (Hausmann et al,
2004). KCC episodes are defined as portions of the
dialogue in which students are jointly constructing
a shared meaning of a concept required for problem
solving. This last mechanism is the most interesting
from a peer learning perspective because it is a truly
?This work is funded by NSF grants 0536968 and 0536959.
collaborative construct and also because it is consis-
tent with the widely accepted constructivist view of
learning.
In our previous work (Kersey et al, 2009) we de-
rived a model of peer interactions that operational-
izes KCC via the notion of initiative shifts in dia-
logue. This model was based on an extensive corpus
analysis in which we found a strong relationship be-
tween initiative shifts and KCC episodes. A paired t-
test showed that there were significantly more initia-
tive shifts in the annotated KCC episodes compared
with the rest of the dialogue ( t(57) = 3.32, p =
0.0016). The moderate effect difference between
the two groups (effect size = 0.49 ) shows that there
is a meaningful increase in the number of initia-
tive shifts in KCC episodes compared with problem
solving activity outside of the KCC episodes. Addi-
tionally, we found moderate correlations of learning
with both KCC (R2 = 0.14, p = 0.02) and with
initiative shifts (R2 = 0.20, p = 0.00).
We have incorporated this model in an innovative
peer learning agent, KSC-PaL, that is designed to
collaborate with a student to solve problems in the
domain of computer science data structures.
2 KSC-PaL
KSC-PaL, has at its core the TuTalk System (Jordan
et al, 2007), a dialogue management system that
supports natural language dialogue in educational
applications. In developing KSC-PaL we extended
TuTalk in three ways.
The first extension is a user interface (see Fig-
ure 1) which manages communication between
TuTalk and the student. Students interact with KSC-
17
Figure 1: The KSC-PaL interface
PaL using natural language and graphical actions.
The student input is processed by the interface and
its related modules into an appropriate format and
passed to TuTalk. Since TuTalk?s interpretation
module is not able to appropriately handle all stu-
dent utterances, a human interpreter assists in this
process. The interpreter receives a student utterance
along with a list of possible matching concepts from
TuTalk (see Figure 4). The interpreter then selects
the most likely matching concepts from TuTalk thus
assisting in natural language interpretation. If the
student utterance doesn?t match any of these con-
cepts, a second list of concepts, containing student
initiative utterances, are presented to the interpreter.
If none of these match then all known concepts are
presented to the interpreter for matching. Note that
the interpreter has a limited, predetermined set of
choices, corresponding to the concepts that TuTalk
is aware of. In this way, his/her intervention is cir-
cumscribed.
The second addition is the incorporation of a stu-
dent model that allows the KSC-PaL to track the
current state of problem solving and the student?s
knowledge in order to guide its behavior. TuTalk?s
student model was replaced with one that incorpo-
rates problem solution graphs (Conati et al, 2002).
Solution graphs are Bayesian networks where each
node represents either an action required to solve the
problem or a concept required as part of problem
solving. A user?s utterances and actions are then
matched to these nodes. This provides KSC-PaL
with information related to the student?s knowledge
of problem solving concepts as well as the current
topic under discussion.
Thirdly, a planning module was added to TuTalk
to make decisions on implementation of problem
solving goals and responses to student initiative in
order to manage shifts in initiative. The planning
module includes an initiative tracker that codes turns
with with either student initiative or agent initiative
using one classifier for natural language utterances
and a separate classifier for drawing and coding ac-
tions. Once a turn is classified, it is determined
whether a shift in initiative has occurred by compar-
18
ing the current classification with the classification
of the previous turn. An average level of initiative
shifts is then computed by dividing the number of
initiative shifts by the total number of turns. Based
on the initiative level, KSC-PaL encourages initia-
tive shifts by using prompts, hedging, requesting
feedback from the student and encouraging student
criticism by intentionally making errors in problem
solving.
Our evaluation of KSC-PaL (Kersey et al, June
2010) found that students learned using the agent
and that KSC-PaL was was successful in encourag-
ing shifts in initiative.
3 Demonstration Outline
We will demonstrate a problem solving episode with
KSC-PaL where a user will interact with the agent as
a student. Specifically we will show how KSC-PaL
attempts to manage the level of initiative and how
KSC-PaL reacts to student initiative.
1. Amy: hi there, are you ready
to start?
2. Mary: sure
3. Amy: where to begin?
4. Mary: i don?t know
5. Amy: maybe we should draw this
6. Amy: do you want to draw it?
7. Mary: ok
8. Amy: so, line 1 ...
9. Mary: how?s that?
10. Amy: hmmm
11. Amy: line 2: first.next=
second.next
(Mary diagrams the code segment)
12. Mary: there i finished it
13. Amy: looks good
14. Amy: i think we?re done drawing
15. Mary: yes
16. Amy: basically, it switches the
first and second elements
17. Mary: right
18. Amy: ill write the explanation
19. Mary: ok
Figure 2: Problem Solving Example
Figure 2 shows an excerpt from a typical prob-
lem solving session. Amy, the agent, attempts to
maintain a balanced level of initiative either by en-
couraging the student (Mary) to take initiative, or by
taking initiative. In lines 10 and 11, the agent uses a
prompt followed by repetition of the problem solv-
ing goal to encourage the student to retain the initia-
tive. However, in line 16 the agent takes initiative by
proposing a solution to the problem.
KSC-PaL is also able to manage the situation
where the student takes initiative in the dialogue in
an unanticipated manner, as shown in Figure 3. In
line 3, Mary asks a question that is not expected
at this point in the problem solving process and the
agent responds appropriately in line 4. KSC-PaL is
able to respond to a set of student initiations derived
from corpus data, such as the one in this excerpt.
Additionally, the agent responds in a generic man-
ner to other unanticipated initiations and directs the
student back to problem solving.
1. Mary: let?s look at the code
2. Amy: so, i think first and
second both equal bat
3. Mary: is second a reference or
a node?
4. Amy: second is a reference
Figure 3: Student Initiative Example
References
A. L. Brown and A. S. Palincsar, 1989. Guided, cooper-
ative learning and individual knowledge acquisition,
pages 307?226. Lawrence Erlbaum Associates, Hills-
dale, NJ.
M.T.H. Chi, N. De Leeuw, M.H. Chiu, and C. LaVancher.
1994. Eliciting self-explanations improves under-
standing. Cognitive Science, 18(3):439?477.
Jennifer Chu-Carroll and Sandra Carberry. 1998. Col-
laborative response generation in planning dialogues.
Computational Linguistics, 24(3):355?400.
Cristina Conati, Abigail Gertner, and Kurt VanLehn.
2002. Using Bayesian networks to manage uncer-
tainty in student modeling. User Modeling and User-
Adapted Interaction, 12(4):371?417.
Mar??a de los Angeles Constantino-Gonza?lez and
Daniel D. Suthers. 2000. A coached collaborative
learning environment for entity-relationship modeling.
Intelligent Tutoring Systems, pages 324?333.
19
Figure 4: The interface for the human interpreter
E. Fisher. 1993. Distinctive features of pupil-pupil class-
room talk and their relationship to learning: How dis-
cursive exploration might be encouraged. Language
and Education, 7:239?257.
Robert G.M. Hausmann, Michelene T.H. Chi, and Mar-
guerite Roy. 2004. Learning from collaborative prob-
lem solving: An analysis of three hypothesized mech-
anisms. In K.D Forbus, D. Gentner, and T. Regier, edi-
tors, 26th Annual Conference of the Cognitive Science
Society, pages 547?552, Mahwah, NJ.
Pamela W. Jordan and Barbara Di Eugenio. 1997. Con-
trol and initiative in collaborative problem solving di-
alogues. In Working Notes of the AAAI Spring Sympo-
sium on Computational Models for Mixed Initiative,
pages 81?84, Menlo Park, CA.
Pamela W Jordan, Brian Hall, Michael A. Ringenberg,
Yui Cue, and Carolyn Penstein Rose?. 2007. Tools for
authoring a dialogue agent that participates in learning
studies. In Artificial Intelligence in Education, AIED
2007, pages 43?50.
Cynthia Kersey, Barbara Di Eugenio, Pamela Jordan, and
Sandra Katz. 2009. KSC-PaL: a peer learning agent
that encourages students to take the initiative. In Pro-
ceedings of the Fourth Workshop on Innovative Use of
NLP for Building Educational Applications, pages 55?
63. Association for Computational Linguistics.
Cynthia Kersey, Barbara Di Eugenio, Pamela Jordan, and
Sandra Katz. June 2010. KSC-PaL: A peer learning
agent. In ITS 2010, The 10th International Conference
on Intelligent Tutoring Systems, Pittsburgh, PA.
R. Ploetzner, P. Dillenbourg, M. Preier, and D. Traum.
1999. Learning by explaining to oneself and to others.
Collaborative learning: Cognitive and computational
approaches, pages 103?121.
Rod D. Roscoe and Michelene T. H. Chi. 2007.
Understanding tutor learning: Knowledge-building
and knowledge-telling in peer tutors? explanations
and questions. Review of Educational Research,
77(4):534?574.
Amy Soller. 2004. Computational modeling and analysis
of knowledge sharing in collaborative distance learn-
ing. User Modeling and User-Adapted Interaction,
Volume 14(4):351?381, January.
Tan Bee Tin. 2003. Does talking with peers help learn-
ing? the role of expertise and talk in convergent group
discussion tasks. Journal of English for Academic
Purposes, 2(1):53?66.
20
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 523?527,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Co-reference via Pointing and Haptics in Multi-Modal Dialogues
Lin Chen, Barbara Di Eugenio
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607, USA
{lchen43,bdieugen}@uic.edu
Abstract
This paper describes our ongoing work on
resolving third person pronouns and deictic
words in a multi-modal corpus. We show that
about two thirds of these referring expressions
have antecedents that are introduced by point-
ing gestures or by haptic-ostensive actions
(actions that involve manipulating an object).
After describing our annotation scheme, we
discuss the co-reference models we learn from
multi-modal features. The usage of haptic-
ostensive actions in a co-reference model is a
novel contribution of our work.
1 Introduction
Co-reference resolution has received a lot of atten-
tion. However, as Eisenstein and Davis (2006)
noted, most research on co-reference resolution has
focused on written text. This task is much more
difficult in dialogue, especially in multi-modal di-
alogue contexts. First, utterances are informal, un-
grammatical and disfluent. Second, people sponta-
neously use gestures and other body language. As
noticed by Kehler (2000), Goldin-Meadow (2003),
and Chen et al (2011), in a multi-modal corpus,
the antecedents of referring expressions are often in-
troduced via gestures. Whereas the role played by
pointing gestures in referring has been studied, the
same is not true for other types of gestures. In this
paper, alongside pointing gestures, we will discuss
the role played by Haptic-Ostensive (H-O) actions,
i.e., referring to an object by manipulating it in the
world (Landragin et al, 2002; Foster et al, 2008).
As far as we know, no computational models of co-
reference have been developed that include H-O ac-
tions: (Landragin et al, 2002) focused on percep-
tual salience and (Foster et al, 2008) on generation
rather than interpretation. We should point out that
at the time of writing we only focus on resolving
third person pronouns and deictics.
The rest of this paper is organized as follows. In
Section 2 we describe our multi-modal annotation
scheme. In Section 3 we present the pronoun/deictic
resolution system. In Section 4, we discuss experi-
ments and results.
2 The Data Set
The dataset we use in this paper is a subset of the
ELDERLY-AT-HOME corpus (Di Eugenio et al,
2010), a multi-modal corpus in the domain of elderly
care. It contains 20 human-human dialogues. In
each dialogue, a helper (HEL) and an elderly person
(ELD) performed Activities of Daily Living (Krapp,
2002), such as getting up from chairs, finding pots,
cooking pastas, in a realistic setting, a studio apart-
ment used for teaching and research. The corpus
contains videos and voice data in avi format, haptics
data collected via instrumented gloves in csv format,
and the transcribed utterances in xml format.
We focused on specific subdialogues in this cor-
pus, that we call Find tasks: a Find task is a con-
tinuous time span during which the two subjects
were collaborating on finding objects. Find tasks
arise naturally while helping perform ADLs such as
preparing dinner. An excerpt from a Find task
is shown below, including annotations for pointing
gestures and for H-O actions (annotations are per-
523
formed via the Anvil tool (Kipp, 2001)).
ELD : Can you get me a pot?
HEL: (opens cabinet, takes out pot, without saying a word)
[Open(HEL,Cabinet1),Take-Out(HEL,Pot1)]
ELD: Not that one, try over there.
[Point(ELD,Cabinet5)]
Because the targets of pointing gestures and H-
O actions are real life objects, we designed a refer-
ring index system to annotate them. The referring
index system consists of compile time indices and
run time indices. We give pre-defined indices to tar-
gets which cannot be moved, like cabinets, draw-
ers, fridge. We assign run time indices to targets
which can be moved, and exist in multiple copies,
like cups, glasses. A referring index consists of a
type and an index; the index increases according to
the order of appearance in the dialogue. For exam-
ple, ?Pot#1? means the first pot referred to in the
dialogue. If a pointing gesture or H-O action in-
volved multiple objects, we used JSON (JavaScript
Object Notation)1 Array to mark it. For example,
[C#1, C#2] means Cabinet#1 and Cabinet#2.
We define a pointing gesture as a hand gesture
without physical contact with the target, whereas
gestures that involve physical contact with an ob-
ject are haptic-obstensive (H-O).2 We use four tracks
in Anvil to mark these gestures, two for pointing
gestures, and two for H-O actions. In each pair of
tracks, one track is used for HEL, one for ELD. For
both types of gestures, we mark the start time, end
time and the target(s) of the gesture using the re-
ferring index system we introduced above. Addi-
tionally we mark the type of an H-O action: Touch,
Hold, Take-Out (as in taking out an object from a
cabinet or the fridge), Close, Open.3
Our co-reference annotation follows an approach
similar to (Eisenstein and Davis, 2006). We mark
the pronouns and deictics which need to be resolved,
their antecedents, and the co-reference links be-
tween them. To mark pronouns, deictics and tex-
tual antecedents, we use the shallow parser from
1http://www.json.org/
2Whereas not all haptic actions are ostensive, in our dia-
logues they all potentially perform an ostensive function.
3Our subjects occasionally hold objects together, e.g. to fill
a pot with water: these actions are not included among the H-O
actions, and are annotated separately.
Find Subtasks 142
Length (Seconds) 5009
Speech Turns 1746
Words 8213
Pointing Gestures 362
H-O Actions 629
Pronouns and Deictics 827
Resolved Ref. Expr. 757
Textual Antecedent 218
Pointing Gesture Antecedent 266
H-O Antecedent 273
Table 1: Annotation Statistics
Apache OpenNLP Tools 4 to chunk the utterances in
each turn. We use heuristics rules to automatically
mark potential textual antecedents and the phrases
we need to resolve. Afterwards we use Anvil to edit
the results of automatic processing. To annotate co-
reference links, we first assign each of the textual
antecedents, the pointing gestures and H-O actions
a unique markable index. Finally, we link referring
expressions to their closest antecedent (if applicable)
using the markable indices.
Table 1 shows corpus and annotation statistics.
We annotated 142 Find subtasks, whose total length
is about 1 hour and 24 minutes. This sub-corpus
comprises 1746 spoken turns, which include 8213
words. 10% of the 8213 words (827 words) are pro-
nouns or deictics. Note that for only 757/827 (92%)
were the annotators able to determine an antecedent.
Interestingly, 71% of those 757 pronouns or deictics
refer to specific antecedents that are introduced ex-
clusively by gestures, either pointing or H-O actions.
In the earlier example, only the type for the referent
of that in No, not that one had been introduced textu-
ally, but not its specific antecedent pot1. Clearly, to
be effective on such data any model of co-reference
must include the targets of pointing gestures and H-
O actions. Our current model does not take into ac-
count the type provided by the de dicto interpretation
of indefinites such as a pot above, but we intend to
address this issue in future work.
In order to verify the reliability of our annotations,
we double coded 15% of the data for pointing ges-
tures and H-O actions, namely the dialogues from
3 pairs of subjects, or 22 Find subtasks. We ob-
4http://incubator.apache.org/opennlp/
524
tained reasonable ? values: for pointing gestures,
?=0.751, for H-O actions, ?=0.703, and for co-
reference, ?=0.70.
3 The Co-reference Model
In this paper we focus on how to use gesture infor-
mation (pointing or H-O) to solve the referring ex-
pressions of interest. Given a pronoun or deictic, we
build co-reference pairs by pairing it with the targets
of pointing gestures and H-O actions in a given time
window. We mark the correct pairs as ?True? and
then we train a classification model to judge if a co-
reference pair is a true pair. The main component
of the resolution system is the co-reference classifi-
cation model. Since our antecedents are not textual,
most of the traditional features for co-reference res-
olution do not apply. Rather, we use the following
multi-modal features - U is the utterance containing
the pronoun / deictic to be solved:
? Time distance between the spans of U and of
the pointing/H-O action. If the two spans over-
lap, the distance is 0.
? Speaker agreement: If the speaker of U and the
actor of the pointing/H-O action are the same.
? Markable type agreement: If the markable type
of the pronoun/deictic and of the targets of
pointing gesture/H-O action are compatible.
? Number agreement: If the number of the pro-
noun/deictic is the same as that of the targets of
the pointing gesture/H-O action.
? Object agreement: If the deictic is contained
in a phrase, such as ?this big blue bowl?,
we will check if the additional object descrip-
tion ?bowl? matches the targets of pointing
gesture/H-O action.
? H-O Action type: for co-reference pairs with
antecedents from H-O actions.
For markable type agreement, we defined two
types of markables: PLC (place) and OBJ (object).
PLC includes all the targets which cannot easily
be moved, OBJ includes all the targets like cups,
pots. We use heuristics rules to assign markable
types to pronouns/deictics and the targets of point-
ing gestures/H-O actions. To determine the number
of the targets, we extract information from the an-
notations; if the target is a JSON array, it means it
is plural. To extract additional object description for
the object agreement feature, we use the Stanford
Typed Dependency parser (De Marneffe and Man-
ning., 2008). We check if the pronoun/deictic is in-
volved in ?det? and ?nsubj? relations, if so, we ex-
tract the ?gov? element of that relation as the object
to compare with the target of gestures/H-O actions.
4 Experiments and Discussions
We have experimented with 3 types of classification
models: Maximum Entropy (MaxEnt), Decision
Tree and Support Vector Machine (SVM), respec-
tively implemented via the following three pack-
ages: MaxEnt, J48 from Weka (Hall et al, 2009),
and LibSVM (Chang and Lin, 2011). All of the
results reported below are calculated using 10 fold
cross validation.
We have run a series of experiments changing the
history length from 0 to 10 seconds for generating
co-reference pairs (history changes in increments of
1 second, hence, there are 11 sets of experiments).
For each history length, we build the 3 models men-
tioned above. An additional baseline model treats a
co-reference pair as ?True? if speaker agreement is
true for the pair, and the time distance is 0. Beside
the specified baseline, J48 can be seen as a more so-
phisticated baseline as well. When we ran the 10
fold experiment with J48 algorithm, 5 out of 10 gen-
erated decision trees only used 3 attributes.
We use two metrics to measure the performance
of the models. One are the standard precision, re-
call and F-Score with respect to the generated co-
reference pairs; the other is the number of pro-
nouns and deictics that are correctly resolved. Given
a pronoun/deictic pi, if the classifier returns more
than one positive co-reference pair for pi, we use a
heuristic resolver to choose the target. We divide
those positive pairs into two subsets, those where
the speaker of pi is the same as the performer of the
gesture (SAME), and those with the other speaker
(OTHER). If SOME is not empty, we will choose
SOME, otherwise OTHER. If the chosen set con-
tains more than one pair, we will choose the target
525
Model Hist. Prec. Rec. F.
Number
Resolved
Baseline 2 .707 .526 .603 359
J48 1 .801 .534 .641 371
SVM 2 .683 .598 .637 369
MaxEnt 0 .738 .756 .747 374
MaxEnt 2 .723 .671 .696 384
Table 2: Gesture&Haptics Co-reference Model Results
of the gesture/H-O action in the most recent pair.
Given the space limit, Table 2 only shows the
results for each model which resolved most pro-
nouns/deictics, and the model which produced the
best F-score. In Table 2, with the change of History
window setting, the gold standard of co-reference
pairs change. When the history window is larger,
there are more co-reference candidate pairs, which
help resolve more pronouns and deictics.
Given we work on a new corpus, it is hard to
compare our results to previous work, additionally
our models currently do not deal with textual an-
tecedents. For example Strube and Mu?ller (2003)
reports their best F-Measure as .4742, while ours
is .747. As concerns accuracy, whereas 384/827
(46%) may appear low, note the task we are per-
forming is harder since we are trying to solve all pro-
nouns/deictics via gestures, not only the ones which
have an antecedent introduced by a pointing or H-O
action (see Table 1). Even if our feature set is lim-
ited, all the classification models perform better than
baseline in all the experiments; the biggest improve-
ment is 14.4% in F-score, and solving 25 more pro-
nouns and deictics. There are no significant differ-
ences in the performances of the 3 different classifi-
cation models. Table 2 shows that the history length
of the best models is less than or equal to 2 seconds,
which is within the standard error range of annota-
tions when we marked the time spans for events.
5 Conclusions
This paper introduced our multi-modal co-reference
annotation scheme that includes pointing gestures
and H-O actions in the corpus ELDERLY-AT-
HOME. Our data shows that 2/3 of antecedents of
pronouns/deictics are introduced by pointing ges-
tures or H-O actions, and not in speech. A co-
reference resolution system has been built to resolve
pronouns and deictics to the antecedents introduced
by pointing gestures and H-O actions. The classi-
fication models show better performance than the
baseline model. In the near future, we will integrate
a module which can resolve pronouns and deictics to
textual antecedents, including type information pro-
vided by indefinite descriptions. This will make the
system fully multi-modal. Additionally we intend
to study issues of timing. Preliminary studies of our
corpus show that the average distance between a pro-
noun/deictic and its antecedent is 8.26? for textual
antecedents, but only 0.66? for gesture antecedents,
consistent with our results that show the best models
include very short histories, at most 2? long.
Acknowledgments
This work is supported by award IIS 0905593 from
the National Science Foundation. Thanks to the
other members of the RoboHelper project, espe-
cially to Anruo Wang, for their many contributions,
especially to the data collection effort. Additionally,
we thank the anonymous reviewers for their valuable
comments.
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Lin Chen, Anruo Wang, and Barbara Di Eugenio. 2011.
Improving pronominal and deictic co-reference resolu-
tion with multi-modal features. In Proceedings of the
SIGDIAL 2011 Conference, pages 307?311, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Marie-Catherine De Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8. Association for Computational
Linguistics.
Barbara Di Eugenio, Milos? Z?efran, Jezekiel Ben-
Arie, Mark Foreman, Lin Chen, Simone Franzini,
Shankaranand Jagadeesan, Maria Javaid, and Kai
Ma. 2010. Towards Effective Communication with
Robotic Assistants for the Elderly: Integrating Speech,
Vision and Haptics. In Dialog with Robots, AAAI 2010
Fall Symposium, Arlington, VA, USA, November.
526
Jacob Eisenstein and Randall Davis. 2006. Gesture
Improves Coreference Resolution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages 37?
40.
Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll. 2008.
The roles of haptic-ostensive referring expressions in
cooperative, task-based human-robot dialogue. In
Proceedings of the 3rd ACM/IEEE international con-
ference on Human Robot Interaction, HRI ?08, pages
295?302. ACM.
S. Goldin-Meadow. 2003. Hearing gesture: How our
hands help us think. Harvard University Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Andrew Kehler. 2000. Cognitive Status and Form of
Reference in Multimodal Human-Computer Interac-
tion. In AAAI 00, The 15th Annual Conference of the
American Association for Artificial Intelligence, pages
685?689.
Michael Kipp. 2001. Anvil-a generic annotation tool
for multimodal dialogue. In Proceedings of the 7th
European Conference on Speech Communication and
Technology, pages 1367?1370.
Kristine M. Krapp. 2002. The Gale Encyclopedia of
Nursing & Allied Health. Gale Group, Inc. Chapter
Activities of Daily Living Evaluation.
F. Landragin, N. Bellalem, and L. Romary. 2002. Refer-
ring to objects with spoken and haptic modalities. In
Proceedings of the Fourth IEEE International Confer-
ence on Multimodal Interfaces (ICMI?02), pages 99?
104, Pittsburgh, PA.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics-Volume 1.
527
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1376?1385,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating Fine-Grained Reviews of Songs From Album Reviews
Swati Tata and Barbara Di Eugenio
Computer Science Department
University of Illinois, Chicago, IL, USA
{stata2 | bdieugen}@uic.edu
Abstract
Music Recommendation Systems often
recommend individual songs, as opposed
to entire albums. The challenge is to gen-
erate reviews for each song, since only full
album reviews are available on-line. We
developed a summarizer that combines in-
formation extraction and generation tech-
niques to produce summaries of reviews of
individual songs. We present an intrinsic
evaluation of the extraction components,
and of the informativeness of the sum-
maries; and a user study of the impact of
the song review summaries on users? de-
cision making processes. Users were able
to make quicker and more informed deci-
sions when presented with the summary as
compared to the full album review.
1 Introduction
In recent years, the personal music collection of
many individuals has significantly grown due to
the availability of portable devices like MP3 play-
ers and of internet services. Music listeners are
now looking for techniques to help them man-
age their music collections and explore songs they
may not even know they have (Clema, 2006).
Currently, most of those electronic devices follow
a Universal Plug and Play (UPNP) protocol (UPN,
2008), and can be used in a simple network, on
which the songs listened to can be monitored. Our
interest is in developing a Music Recommendation
System (Music RS) for such a network.
Commercial web-sites such as Amazon (www.
amazon.com) and Barnes and Nobles (www.
bnn.com) have deployed Product Recommen-
dation Systems (Product RS) to help customers
choose from large catalogues of products. Most
Product RSs include reviews from customers who
bought or tried the product. As the number of
reviews available for each individual product in-
creases, RSs may overwhelm the user if they make
all those reviews available. Additionally, in some
reviews only few sentences actually describe the
recommended product, hence, the interest in opin-
ion mining and in summarizing those reviews.
A Music RS could be developed along the lines
of Product RSs. However, Music RSs recom-
mend individual tracks, not full albums, e.g. see
www.itunes.com. Summarizing reviews be-
comes more complex: available data consists of
album reviews, not individual song reviews (www.
amazon.com, www.epinions.com). Com-
ments about a given song are fragmented all over
an album review. Though some web-sites like
www.last.fm allow users to comment on indi-
vidual songs, the comments are too short (a few
words such as ?awesome song?) to be counted as
a full review.
In this paper, after presenting related work and
contrasting it to our goals in Section 2, we discuss
our prototype Music RS in Section 3. We devote
Section 4 to our summarizer, that extracts com-
ments on individual tracks from album reviews
and produces a summary of those comments for
each individual track recommended to the user.
In Section 5, we report two types of evaluation: an
intrinsic evaluation of the extraction components,
and of the coverage of the summary; an extrinsic
evaluation via a between-subject study. We found
that users make quicker and more informed deci-
sions when presented with the song review sum-
maries as opposed to the full album review.
2 Related Work
Over the last decade, summarization has become
a hot topic for research. Quite a few systems were
developed for different tasks, including multi-
document summarization (Barzilay and McKe-
own, 2005; Soubbotin and Soubbotin, 2005; Nas-
tase, 2008).
1376
What?s not to get? Yes, Maxwell, and Octopus are a
bit silly! ...
.....
?Something? and ?Here Comes The Sun? are two of
George?s best songs ever (and ?Something? may be
the single greatest love song ever). ?Oh Darling? is
a bluesy masterpiece with Paul screaming.....
.......
?Come Together? contains a great riff, but he ended up
getting sued over the lyrics by Chuck Berry......
Figure 1: A sample review for the album ?Abbey Road?
Whereas summarizing customer reviews can
be seen as multi-document summarization, an
added necessary step is to first extract the most
important features customers focus on. Hence,
summarizing customer reviews has mostly been
studied as a combination of machine learning
and NLP techniques (Hu and Liu, 2004; Ga-
mon et al, 2005). For example, (Hu and Liu,
2004) use associative mining techniques to iden-
tify features that frequently occur in reviews
taken from www.epinions.com and www.
amazon.com. Then, features are paired to the
nearest words that express some opinion on that
feature. Most work on product reviews focuses
on identifying sentences and polarity of opinion
terms, not on generating a coherent summary from
the extracted features, which is the main goal
of our research. Exceptions are (Carenini et al,
2006; Higashinaka et al, 2006), whose focus was
on extracting domain specific ontologies in order
to structure summarization of customer reviews.
Summarizing reviews on objects different from
products, such as restaurants (Nguyen et al,
2007), or movies (Zhuang et al, 2006), has also
been tackled, although not as extensively. We
are aware of only one piece of work that focuses
on music reviews (Downie and Hu, 2006). This
study is mainly concerned with identifying de-
scriptive patterns in positive or negative reviews
but not on summarizing the reviews.
2.1 Summarizing song reviews is different
As mentioned earlier, using album reviews for
song summarization poses new challenges:
a) Comments on features of a song are embed-
ded and fragmented within the album reviews, as
shown in Figure 1. It is necessary to correctly map
features to songs.
b) Each song needs to be identified each time it
is referred to in the review. Titles are often ab-
breviated, and in different ways, even in the same
review ? e.g. see Octopus for Octopus?s Garden
in Figure 1. Additionally, song titles need not be
noun phrases and hence NP extraction algorithms
miss many occurrences, as was shown by prelimi-
nary experiments we ran.
c) Reviewers focus on both inherent features such
as lyrics, genre and instruments, but also on people
(artist, lyricist, producer etc.), unlike in product
reviews where manufacturer/designer are rarely
mentioned. This variety of features makes it
harder to generate a coherent summary.
3 SongRecommend: Prototype Music RS
Figure 2 shows the interface of our prototype Mu-
sic RS. It is a simple interface dictated by our fo-
cus on the summarization process (but it was in-
formed by a small pilot study). Moving from win-
dow to window and from top to bottom:
a) The top leftmost window shows different de-
vices on which the user listens to songs. These
devices are monitored with a UPNP control point.
Based on the messages received by the control
point, the user activities, including the metadata
of the song, are logged.
b) Once the user chooses a certain song on one of
the devices (see second window on top), we dis-
play more information about the song (third top
window); we also identify related songs from the
internet, including: other songs from the same al-
bum, popular songs of the artist and popular songs
of related artists, as obtained from Yahoo Music.
c) The top 25 recommendations are shown in the
fourth top window. We use the SimpleKMeans
Clustering (Mitchell, 1997) to identify and rank
the top twenty-five songs which belong to the
same cluster and are closest to the given song.
Closeness between two songs in a cluster is mea-
sured as the number of attributes (album, artist etc)
of the songs that match.
d) When the user clicks on More Info for one of
the recommended songs, the pop-up, bottom win-
dow is displayed, which contains the summary of
the reviews for the specific song.
4 Extraction and Summarization
Our summarization framework consists of the five
tasks illustrated in Figure 3. The first two tasks
pertain to information extraction, the last three to
repackaging the information and generating a co-
1377
Figure 2: SongRecommend Interface
Figure 3: Summarization Pipeline
herent summary. Whereas the techniques we use
for each individual step are state-of-the-art, our ap-
proach is innovative in that it integrates them into
an effective end-to-end system. Its effectiveness is
shown by the promising results obtained both via
the intrinsic evaluation, and the user study. Our
framework can be applied to any domain where
reviews of individual components need to be sum-
marized from reviews of collections, such as re-
views of different hotels and restaurants in a city.
Our corpus was opportunistically col-
lected from www.amazon.com and
www.epinions.com. It consists of 1350
album reviews across 27 albums (50 reviews
per album). 50 randomly chosen reviews were
used for development. Reviews have noise, since
the writing is informal. We did not clean it, for
example we did not correct spelling mistakes.
This corpus was annotated for song titles and song
features. Feature annotation consists of marking
a phrase as a feature and matching it with the song
to which the feature is attributed. Note that we
have no a priori inventory of features; what counts
as features of songs emerged from the annotation,
since annotators were asked to annotate for noun
phrases which contain ?any song related term or
terms spoken in the context of a song?. Further,
they were given about 5 positive and 5 negative
1378
What?s not to get? Yes, <song
id=3>Maxwell</song>, and <song
id=5>Octopus</song> are a bit silly! ...
.........
.........
<song id=2>?Something?</song> and <song
id=7>?Here Comes The Sun?</song> are two of
<feature id=(2,7)>George?s</feature> best songs
ever (and <song id=2>?Something?</song> may be
......
<song id=4>?Oh Darling?</song> is a <feature
id=4>bluesy masterpiece</feature> with <feature
id=4>Paul</feature> screaming......
.....
<song id=1>?Come Together?</song> contains a
great <feature id=1>riff</feature>, but ...
Figure 4: A sample annotated review
examples of features. Figure 4 shows annotations
for the excerpt in Figure 1. For example in
Figure 4, George, Paul, bluesy masterpiece and
riff have been marked as features. Ten randomly
chosen reviews were doubly annotated for song
titles and features. The Kappa co-efficient of
agreement on both was excellent (0.9), hence the
rest of the corpus was annotated by one annotator
only. The two annotators were considered to be in
agreement on a feature if they marked the same
head of phrase and attributed it to the same song.
We will now turn to describing the component
tasks. The algorithms are described in full in (Tata,
2010).
4.1 Title Extraction
Song identification is the first step towards sum-
marization of reviews. We identify a string of
words as the title of a song to be extracted from
an album review if it (1) includes some or all the
words in the title of a track of that album, and (2)
this string occurs in the right context. Constraint
(2) is necessary because the string of words cor-
responding to the title may appear in the lyrics of
the song or anywhere else in the review. The string
Maxwell?s Silver Hammer counts as a title only in
sentence (a) below; the second sentence is a verse
in the lyrics:
a. Then, the wild and weird ?Maxwell?s Silver
Hammer.?
b. Bang, Bang, maxwell?s silver hammer cam
down on her head.
Similar to Named Entity Recognition (Schedl et
al., 2007), our approach to song title extraction
is based on n-grams. We proceed album by al-
bum. Given the reviews for an album and the list
of songs in that album, first, we build a lexicon of
all the words in the song titles. We also segment
the reviews into sentences via sentence boundary
detection. All 1,2,3,4-grams for each sentence (the
upper-bound 4 was determined experimentally) in
the review are generated. First, n-grams that con-
tain at least one word with an edit distance greater
than one from a word in the lexicon are filtered
out. Second, if higher and lower order n-grams
overlap at the same position in the same sentence,
lower order n-grams are filtered out. Third, the
n-grams are merged if they occur sequentially in
a sentence. Fourth, the n-grams are further fil-
tered to include only those where (i) the n-gram is
within quotation marks; and/or (ii) the first char-
acter of each word in the n-gram is upper case.
This filters n-grams such as those shown in sen-
tence (b) above. All the n-grams remaining at this
point are potential song titles. Finally, for each
n-gram, we retrieve the set of IDs for each of its
words and intersect those sets. This intersection
always resulted in one single song ID, since song
titles in each album differ by at least one content
word. Recall that the algorithm is run on reviews
for each album separately.
4.2 Feature Extraction
Once the song titles are identified in the album re-
view, sentences with song titles are used as an-
chors to (1) identify segments of texts that talk
about a specific song, and then (2) extract the fea-
ture(s) that the pertinent text segment discusses.
The first step roughly corresponds to identify-
ing the flow of topics in a review. The second step
corresponds to identifying the properties of each
song. Both steps would greatly benefit from ref-
erence resolution, but current algorithms still have
a low accuracy. We devised an approach that com-
bines text tiling (Hearst, 1994) and domain heuris-
tics. The text tiling algorithm divides the text into
coherent discourse units, to describe the sub-topic
structure of the given text. We found the relatively
coarse segments the text tiling algorithm provides
sufficient to identify different topics.
An album review is first divided into seg-
ments using the text tiling algorithm. Let
[seg1, seg2, ..., segk] be the segments obtained.
The segments that contain potential features of a
song are identified using the following heuristics:
Step 1: Include segi if it contains a song title.
1379
These segments are more likely to contain features
of songs as they are composed of the sentences
surrounding the song title.
Step 2: Include segi+1 if segi is included and
segi+1 contains one or more feature terms.
Since we have no a priori inventory of features
(the feature annotation will be used for evalua-
tion, not for development), we useWordNet (Fell-
baum, 1998) to identify feature terms: i.e., those
nouns whose synonyms, direct hypernym or di-
rect hyponym, or the definitions of any of those,
contain the terms ?music? or ?song?, or any form
of these words like ?musical?, ?songs? etc, for at
least one sense of the noun. Feature terms exclude
the words ?music?, ?song?, the artist/band/album
name as they are likely to occur across album re-
views. All feature terms in the final set of seg-
ments selected by the heuristics are taken to be
features of the song described by that segment.
4.3 Sentence Partitioning and Regeneration
After extracting the sentences containing the fea-
tures, the next step is to divide the sentences into
two or more ?sub-sentences?, if necessary. For
example, ?McCartney?s bouncy bass-line is espe-
cially wonderful, and George comes in with an ex-
cellent, minimal guitar solo.? discusses both fea-
tures bass and guitar. Only a portion of the sen-
tence describes the guitar. This sentence can
thus be divided into two individual sentences. Re-
moving parts of sentences that describe another
feature, will have no effect on the summary as
a whole as the portions that are removed will be
present in the group of sentences that describe the
other feature.
To derive n sentences, each concerning a single
feature f , from the original sentence that covered
n features, we need to:
1. Identify portions of sentences relevant to each
feature f (partitioning)
2. Regenerate each portion as an independent sen-
tence, which we call f -sentence.
To identify portions of the sentence relevant to the
single feature f , we use the Stanford Typed De-
pendency Parser (Klein and Manning, 2002; de
Marnee and Manning, 2008). Typed Dependen-
cies describe grammatical relationships between
pairs of words in a sentence. Starting from the fea-
ture term f in question, we collect all the nouns,
adjectives and verbs that are directly related to it
in the sentence. These nouns, adjectives and verbs
1. ?Maxwell? is a bit silly.
2. ?Octopus? is a bit silly.
3. ?Something? is George?s best song.
4. ?Here Comes The Sun? is George?s best song.
5. ?Something? may be the single greatest love song.
6. ?Oh! Darling? is a bluesy masterpiece.
7. ?Come Together? contains a great riff.
Figure 5: f -sentences corresponding to Figure 1
become the components of the new f -sentence.
Next, we need to adjust their number and forms.
This is a natural language generation task, specifi-
cally, sentence realization.
We use YAG (McRoy et al, 2003), a template
based sentence realizer. clause is the main tem-
plate used to generate a sentence. Slots in a tem-
plate can in turn be templates. The grammati-
cal relationships obtained from the Typed Depen-
dency Parser such as subject and object identify
the slots and the template the slots follows; the
words in the relationship fill the slot. We use a
morphological tool (Minnen et al, 2000) to ob-
tain the base form from the original verb or noun,
so that YAG can generate grammatical sentences.
Figure 5 shows the regenerated review from Fig-
ure 1.
YAG regenerates as many f -sentences from the
original sentence, as many features were contained
in it. By the end of this step, for each feature f
of a certain song si, we have generated a set of
f -sentences. This set alo contains every original
sentence that only covered the single feature f .
4.4 Grouping
f -sentences are further grouped, by sub-feature
and by polarity. As concerns sub-feature group-
ing, consider the following f -sentences for the
feature guitar:
a. George comes in with an excellent, minimal
guitar solo.
b. McCartney laid down the guitar lead for this
track.
c. Identical lead guitar provide the rhythmic
basis for this song.
The first sentence talks about the guitar solo, the
second and the third about the lead guitar. This
step will create two subgroups, with sentence a in
one group and sentences b and c in another. We
1380
Let [fx-s1, fx-s2, ...fx-sn] be the set of sentences for
feature fx and song Sy
Step 1: Find the longest common n-gram (LCN) be-
tween fx-si and fx-sj for all i 6= j: LCN(fx-si, fx-sj)
Step 2: If LCN(fx-si, fx-sj) contains the feature term
and is not the feature term alone, fx-si and fx-sj are
in the same group.
Step 3: For any fx-si, if LCN(fx-si, fx-sj) for all i and
j, is the feature term, then fx-si belongs to the default
group for the feature.
Figure 6: Grouping sentences by sub-features
identify subgroups via common n-grams between
f -sentences, and make sure that only n-grams that
are related to feature f are identified at this stage,
as detailed in Figure 6. When the procedure de-
scribed in Figure 6 is applied to the three sentences
above, it identifies guitar as the longest pertinent
LCN between a and b, and between a and c; and
guitar lead between b and c (we do not take into
account linear order within n-grams, hence gui-
tar lead and lead guitar are considered identical).
Step 2 in Figure 6 will group b and c together since
guitar lead properly contains the feature term gui-
tar. In Step 3, sentence a is sentence fx-si such
that its LCN with all other sentences (b and c) con-
tains only the feature term; hence, sentence a is
left on its own. Note that Steps 2 and 3 ensure
that, among all the possible LNCs between pair of
sentences, we only consider the ones containing
the feature in question.
As concerns polarity grouping, different re-
views may express different opinions regarding a
particular feature. To generate a coherent sum-
mary that mentions conflicting opinions, we need
to subdivide f -sentences according to polarity.
We use SentiWordNet (Esuli and Sebastiani,
2006), an extension of WordNet where each sense
of a word is augmented with the probability of
that sense being positive, negative or neutral. The
overall sentence score is based on the scores of the
adjectives contained in the sentence.
Since there are a number of senses for each
word, an adjective ai in a sentence is scored as the
normalized weighted scores of each sense of the
adjective. For each ai, we compute three scores,
positive, as shown in Formula 1, negative and ob-
Example: The lyrics are the best
Adjectives in the sentence: best
Senti-wordnet Scores of best:
Sense 1 (frequency=2):
positive = 0.625, negative =0 , objective = 0.375
Sense 2 (frequency=1):
positive = 0.75, negative = 0, objective = 0.25
Polarity Scores Calculation:
positive(best) = 2?0.625+1?0.75(2+1) = 0.67
negative(best) = 2?0+1?0(2+1) = 0
objective(best) = 2?0.375+1?0.25(2+1) = 0.33
Since the sentence contains only the adjective best, its
polarity is positive, from:
Max (positive(best), negative(best), objective(best))
Figure 7: Polarity Calculation
jective, which are computed analogously:
pos(ai) =
freq1 ? pos1 + ... + freqn ? posn
(freq1 + .... + freqn)
(1)
ai is the ith adjective, freqj is the frequency of
the jth sense of ai as given by Wordnet, and posj
is the positive score of the jth sense of ai, as given
by SentiWordnet. Figure 7 shows an example of
calculating the polarity of a sentence.
For an f -sentence, three scores will be com-
puted, as the sum of the corresponding scores
(positive, negative, objective) of all the adjectives
in the sentence. The polarity of the sentence is de-
termined by the maximum of these three scores.
4.5 Selection and Ordering
Finally, the generation of a coherent summary in-
volves selection of the sentences to be included,
and ordering them in a coherent fashion. This step
has in input groups of f -sentences, where each
group pertains to the feature f , one of its subfea-
tures, and one polarity type (positive, negative, ob-
jective). We need to select one sentence from each
subgroup to make sure that all essential concepts
are included in the summary. Note that if there are
contrasting opinions on one feature or subfeatures,
one sentence per polarity will be extracted, result-
ing in potentially inconsistent opinions on that fea-
ture to be included in the review (we did not ob-
serve this happening frequently, and even if it did,
it did not appear to confuse our users).
Recall that at this point, most f -sentences have
been regenerated from portions of original sen-
1381
tences (see Section 4.3). Each f -sentence in a
subgroup is assigned a score which is equivalent
to the number of features in the original sentence
from which the f -sentence was obtained. The sen-
tence which has the lowest score in each subgroup
is chosen as the representative for that subgroup.
If multiple sentences have the lowest score, one
sentence is selected randomly. Our assumption is
that among the original sentences, a sentence that
talks about one feature only is likely to express a
stronger opinion about that feature than a sentence
in which other features are present.
We order the sentences by exploiting a music
ontology (Giasson and Raimond, 2007). We have
extended this ontology to include few additional
concepts that correspond to features identified in
our corpus. Also, we extended each of the classes
by adding the domain to which it belongs. We
identified a total of 20 different domains for all
the features. For example, [saxophone,drums] be-
longs to the domain Instrument, and [tone, vocals]
belong to the domain Sound. We also identified
the priority order in which each of these domains
should appear in the final summary. The order-
ing of the domains is such that first we present the
general features of the song (e.g. Song) domain,
then present more specific domains (e.g. Sound,
Instrument). f?sentences of a single domain form
one paragraph in the final summary. However, fea-
tures domains that are considered as sub-domains
of another domain are included in the same para-
graph, but are ordered next to the features of the
parent domain. The complete list of domains is de-
scribed in (Tata, 2010). f -sentences are grouped
and ordered according to the domain of the fea-
tures. Figure 8 shows a sample summary when the
extracted sentences are ordered via this method.
?The Song That Jane Likes? is cute. The song
has some nice riffs by Leroi Moore. ?The Song
That Jane Likes? is also amazing funk number.
The lyrics are sweet and loving.
The song carries a light-hearted tone. It has
a catchy tune. The song features some nice ac-
cents.
?The Song That Jane Likes? is beautiful
song with great rhythm. The funky beat will
surely make a move.
It is a heavily acoustic guitar-based song.
Figure 8: Sample summary
5 Evaluation
In this section we report three evaluations, two
intrinsic and one extrinsic: evaluation of the song
title and feature extraction steps; evaluation of the
informativeness of summaries; and a user study to
judge how summaries affect decision making.
5.1 Song Title and Feature Extraction
The song title extraction and feature extraction al-
gorithms (Sections 4.1 and 4.2) were manually
evaluated on 100 reviews randomly taken from the
corpus (2 or 3 from each album). This relatively
small number is due to the need to conduct the
evaluation manually. The 100 reviews contained
1304 occurrences of song titles and 898 occur-
rences of song features, as previously annotated.
1294 occurrences of song titles were correctly
identified; additionally, 123 spurious occurrences
were also identified. This results in a precision of
91.3%, and recall of 98%. The 10 occurrences that
were not identified contained either abbreviations
likeDr. forDoctor or spelling mistakes (recall that
we don?t clean up mistakes).
Of the 898 occurrences of song features, 853
were correctly identified by our feature extraction
algorithm, with an additional 41 spurious occur-
rences. This results in a precision of 95.4% and a
recall of 94.9%. Note that a feature (NP) is con-
sidered as correctly identified, if its head noun is
annotated in a review for the song with correct ID.
As a baseline comparison, we implemented the
feature extraction algorithm from (Hu and Liu,
2004). We compared their algorithm to ours on 10
randomly chosen reviews from our corpus, for a
total of about 500 sentences. Its accuracy (40.8%
precision, and 64.5% recall) is much lower than
ours, and than their original results on product re-
views (72% precision, and 80% recall).
5.2 Informativeness of the summaries
To evaluate the information captured in the sum-
mary, we randomly selected 5 or 6 songs from 10
albums, and generated the corresponding 52 sum-
maries, one per song ? this corresponds to a test set
of about 500 album reviews (each album has about
50 reviews). Most summary evaluation schemes,
for example the Pyramid method (Harnly et al,
2005), make use of reference summaries writ-
ten by humans. We approximate those gold-
standard reference summaries with 2 or 3 critic re-
views per album taken from www.pitchfork.
1382
com, www.rollingstone.com and www.
allmusic.com.
First, we manually annotated both critic reviews
and the automatically generated summaries for
song titles and song features. 302, i.e., 91.2%
of the features identified in the critic reviews are
also identified in the summaries (recall that a fea-
ture is considered as identified, if the head-noun of
the NP is identified by both the critic review and
the summary, and attributed to the same song). 64
additional features were identified, for a recall of
82%. It is not surprising that additional features
may appear in the summaries: even if only one of
the 50 album reviews talks about that feature, it is
included in the summary. Potentially, a threshold
on frequency of feature mention could increase re-
call, but we found out that even a threshold of two
significantly affects precision.
In a second evaluation, we used our Feature
Extraction algorithm to extract features from the
critic reviews, for each song whose summary
needs to be evaluated. This is an indirect evalu-
ation of that algorithm, in that it shows it is not af-
fected by somewhat different data, since the critic
reviews are more formally written. 375, or 95%
of the features identified in the critic reviews are
also identified in the summaries. 55 additional
features were additionally identified, for a recall
of 87.5%. These values are comparable, even if
slightly higher, to the precision and recall of the
manual annotation described above.
5.3 Between-Subject User Study
Our intrinsic evaluation gives satisfactory results.
However, we believe the ultimate measure of such
a summarization algorithm is an end-to-end eval-
uation to ascertain whether it affects user behav-
ior, and how. We conducted a between-subject
user study, where users were presented with two
different versions of our Music RS. For each of
the recommended songs, the baseline version pro-
vides only whole album reviews, the experimental
version provides the automatically generated song
feature summary, as shown in Figure 2. The in-
terface for the baseline version is similar, but the
summary in the bottom window is replaced by the
corresponding album review. The presented re-
view is the one among the 50 reviews for that al-
bum whose length is closest to the average length
of album reviews in the corpus (478 words).
Each user was presented with 5 songs in suc-
cession, with 3 recommendations each (only the
top 3 recommendations were presented among the
available 25, see Section 3). Users were asked to
select at least one recommendation for each song,
namely, to click on the url where they can listen to
the song. They were also asked to base their selec-
tion on the information provided by the interface.
The first song was a test song for users to get ac-
quainted with the system. We collected compre-
hensive timed logs of the user actions, including
clicks, when windows are open and closed, etc.
After using the system, users were administered a
brief questionnaire which included questions on a
5-point Likert Scale. 18 users interacted with the
baseline version and 21 users with the experimen-
tal version (five additional subjects were run but
their log data was not properly saved). All users
were students at our University, and most of them,
graduate students (no differences were found due
to gender, previous knowledge of music, or educa-
tion level).
Our main measure is time on task, the total time
taken to select the recommendations from song 2
to song 5 ? this excludes the time spent listen-
ing to the songs. A t-test showed that users in
the experimental version take less time to make
their decision when compared to baseline subjects
(p = 0.019, t = 2.510). This is a positive result,
because decreasing time to selection is important,
given that music collections can include millions
of songs. However, time-on-task basically repre-
sents the time it takes users to peruse the review
or summary, and the number of words in the sum-
maries is significantly lower than the number of
words in the reviews (p < 0.001, t = 16.517).
Hence, we also analyzed the influence of sum-
maries on decision making, to see if they have
any effects beyond cutting down on the number
of words to read. Our assumption is that the de-
fault choice is to choose the first recommenda-
tion. Users in the baseline condition picked the
first recommendation as often as the other two rec-
ommendations combined; users in the experimen-
tal condition picked the second and third recom-
mendations more often than the first, and the dif-
ference between the two conditions is significant
(?2 = 8.74, df = 1, p = 0.003). If we examine
behavior song by song, this holds true especially
for song 3 (?2 = 12.3, df = 1, p < 0.001) and
song 4 (?2 = 5.08, df = 1, p = 0.024). We
speculate that users in the experimental condition
1383
are more discriminatory in their choices, because
important features of the recommended songs are
evident in the summaries, but are buried in the al-
bum reviews. For example, for Song 3, only one
of the 20 sentences in the album review is about
the first recommended song, and is not very posi-
tive. Negative opinions are much more evident in
the review summaries.
The questionnaires included three common
questions between the two conditions. The ex-
perimental subjects gave a more positive assess-
ment of the length of the summary than the base-
line subjects (p = 0.003, t = ?3.248, df =
31.928). There were no significant differences
on the other two questions, feeling overwhelmed
by the information provided; and whether the re-
view/summary helped them to quickly make their
selection.
A multiple Linear Regression with, as predic-
tors, the number of words the user read before
making the selection and the questions, and time
on task as dependent variable, revealed only one,
not surprising, correlation: the number of words
the user read correlates with time on task (R2 =
0.277, ? = 0.509, p = 0.004).
Users in the experimental version were also
asked to rate the grammaticality and coherence of
the summary. The average rating was 3.33 for
grammaticality, and 3.14 for coherence. Whereas
these numbers in isolation are not too telling, they
are at least suggestive that users did not find these
summaries badly written. We found no signifi-
cant correlations between grammaticality and co-
herence of summaries, and time on task.
6 Discussion and Conclusions
Most summarization research on customer reviews
focuses on obtaining features of the products, but
not much work has been done on presenting them
as a coherent summary. In this paper, we described
a system that uses information extraction and sum-
marization techniques in order to generate sum-
maries of individual songs from multiple album
reviews. Whereas the techniques we have used
are state-of-the-art, the contribution of our work is
integrating them in an effective end-to-end system.
We first evaluated it intrinsically as concerns infor-
mation extraction, and the informativeness of the
summaries. Perhaps more importantly, we also ran
an extrinsic evaluation in the context of our proto-
type Music RS. Users made quicker decisions and
their choice of recommendations was more varied
when presented with song review summaries than
with album reviews. Our framework can be ap-
plied to any domain where reviews of individual
components need to be summarized from reviews
of collections, such as travel reviews that cover
many cities in a country, or different restaurants
in a city.
References
Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence fusion for multidocument news summariza-
tion. Computational Linguistics, 31(3):297?328.
Giuseppe Carenini, Raymond Ng, and Adam Pauls.
2006. Multi-document summarization of evaluative
text. In Proceedings of EACL.
Oscar Clema. 2006. Interaction Design for Recom-
mender Systems. Ph.D. thesis, Universitat Pompeu
Fabra, Barcelona, July.
Marie-Catherine de Marnee and Christopher D. Man-
ning. 2008. Stanford Typed Dependencies Manual.
http://nlp.stanford.edu/software/dependencies manual.pdf.
J. Stephen Downie and Xiao Hu. 2006. Review min-
ing for music digital libraries: Phase ii. In Proceed-
ings of the 6th ACM/IEEE-CS Joint Conference on
Digital Libraries, pages 196?197, Chapel Hill, NC,
USA.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of LREC-06, the 5th
Conference on Language Resources and Evaluation,
Genova, IT.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Michael Gamon, Anthony Aue, Simon Corston-Oliver,
and Eric Ringger. 2005. Pulse: Mining customer
opinions from free text. In Advances in Intelli-
gent Data Analysis VI, volume 3646/2005 of Lec-
ture Notes in Computer Science, pages 121?132.
Springer Berlin / Heidelberg.
Frederick Giasson and Yves Raimond. 2007. Mu-
sic ontology specification. Working draft, February.
http://pingthesemanticweb.com/ontology/mo/.
Aaron Harnly, Ani Nenkova, Rebecca Passonneau, and
Owen Rambow. 2005. Automation of summary
evaluation by the Pyramid method. In Proceedings
of the Conference on Recent Advances in Natural
Language Processing.
Marti A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proceedings of the 32nd Meet-
ing of the Association for Computational Linguis-
tics, Las Cruces, NM, June.
1384
Ryuichiro Higashinaka, Rashmi Prasad, and Marilyn
Walker. 2006. Learning to Generate Naturalistic
Utterances Using Reviews in Spoken Dialogue Sys-
tems. In COLING-ACL06, Sidney, Australia.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of KDD,
Seattle, Washington, USA, August.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15, pages 3?10.
Susan McRoy, Songsak Ukul, and Syed Ali. 2003. An
augmented template-based approach to text realiza-
tion. In Natural Language Engineering, pages 381?
420. Cambridge Press.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust, applied morphological generation. In Pro-
ceedings of the 1st International Natural Language
Generation Conference.
Tom Mitchell. 1997. Machine Learning. McGraw
Hill.
Vivi Nastase. 2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
spreading activation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Patrick Nguyen, Milind Mahajan, and Geoffrey Zweig.
2007. Summarization of multiple user reviews in
the restaurant domain. Technical Report MSR-TR-
2007-126, Microsoft, September.
Markus Schedl, Gerhard Widmer, Tim Pohle, and
Klaus Seyerlehner. 2007. Web-based detection of
music band members and line-up. In Proceedings of
the Australian Computer Society.
M. Soubbotin and S. Soubbotin. 2005. Trade-Off Be-
tween Factors Influencing Quality of the Summary.
InDocument Understanding Workshop (DUC), Van-
couver, BC, Canada.
Swati Tata. 2010. SongRecommend: a Music Recom-
mendation System with Fine-Grained Song Reviews.
Ph.D. thesis, University of Illinois, Chicago, IL.
2008. UPnP Device Architecture Version 1.0.
(www.upnp.org).
Li Zhuang, Feng Jing, and Xiaoyan Zhu. 2006. Movie
review mining and summarization. In Conference
on Information and Knowledge Management, Ar-
lington, Virginia, USA.
1385
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 270?280,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Translating Italian connectives into Italian Sign Language
Camillo Lugaresi
University of Illinois at Chicago
Politecnico di Milano
clugar2@uic.edu
Barbara Di Eugenio
Department of Computer Science
University of Illinois at Chicago
bdieugen@uic.edu
Abstract
We present a corpus analysis of how Ital-
ian connectives are translated into LIS, the
Italian Sign Language. Since corpus re-
sources are scarce, we propose an align-
ment method between the syntactic trees
of the Italian sentence and of its LIS trans-
lation. This method, and clustering ap-
plied to its outputs, highlight the differ-
ent ways a connective can be rendered in
LIS: with a corresponding sign, by affect-
ing the location or shape of other signs, or
being omitted altogether. We translate
these findings into a computational model
that will be integrated into the pipeline of
an existing Italian-LIS rendering system.
Initial experiments to learn the four possi-
ble translations with Decision Trees give
promising results.
1 Introduction
Automatic translation between a spoken language
and a signed language gives rise to some of the
same difficulties as translation between spoken
languages, but adds unique challenges of its own.
Contrary to what one might expect, sign languages
are not artificial languages, but natural languages
that spontaneously arose within deaf communities;
although they are typically named after the region
where they are used, they are not derived from the
local spoken language and tend to bear no similar-
ity to it. Therefore, translation from any spoken
language into the signed language of that specific
region is at least as complicated as between any
pairs of unrelated languages.
The problem of automatic translation is com-
pounded by the fact that the amount of computa-
tional resources to draw on is much smaller than
is typical for major spoken languages. Moreover,
the fact that sign languages employ a different
transmission modality (gestures and expressions
instead of sounds) means that existing writing sys-
tems are not easily adaptable to them. The result-
ing lack of a shared written form does nothing to
improve the availability of sign language corpora;
bilingual corpora, which are of particular impor-
tance to a translation system, are especially rare.
In fact, various projects around the world are try-
ing to ameliorate this sad state of affairs for spe-
cific Sign Languages (Lu and Huenerfauth, 2010;
Braffort et al, 2010; Morrissey et al, 2010).
In this paper, we describe the work we per-
formed as concerns the translation of connectives
from the Italian language into LIS, the Italian Sign
Language (Lingua Italiana dei Segni). Because the
communities of signers in Italy are relatively small
and fragmented, and the language has a relatively
short history, there is far less existing research and
material to draw on than for, say, ASL (American
Sign Language) or BSL (British Sign Language).
Our work was undertaken within the purview of
the ATLAS project (Bertoldi et al, 2010; Lom-
bardo et al, 2010; Lombardo et al, 2011; Prinetto
et al, 2011; Mazzei, 2012; Ahmad et al, 2012),
which developed a full pipeline for translating Ital-
ian into LIS. ATLAS is part of a recent crop of
projects devoted to developing automatic transla-
tion from language L spoken in geographic area
G into the sign language spoken in G (Dreuw et
al., 2010; Lo?pez-Luden?a et al, 2011; Almohimeed
et al, 2011; Lu and Huenerfauth, 2012). Input is
taken in the form of written Italian text, parsed,
and converted into a semantic representation of its
contents; from this semantic representation, LIS
output is produced, using a custom serialization
format called AEWLIS (which we will describe
later). This representation is then augmented with
space positioning information, and fed into a fi-
nal renderer component that performs the signs
using a virtual actor. ATLAS focused on a lim-
ited domain for which a bilingual Italian/LIS cor-
270
pus was available: weather forecasts, for which
the Italian public broadcasting corporation (RAI)
had long been producing special broadcasts with
a signed translation. This yielded a corpus of 376
LIS sentences with corresponding Italian text: this
corpus, converted into AEWLIS format, was the
main data source for the project. Still, it is a very
small corpus, hence the main project shied away
from statistical NLP techniques, relying instead on
rule-based approaches developed with the help of
a native Italian/LIS bilingual speaker; a similar ap-
proach is taken e.g. in (Almohimeed et al, 2011)
for Arabic.
1.1 Why connectives?
The main semantic-bearing elements of an Italian
sentence, such as nouns or verbs, typically have
a LIS sign as their direct translation. We focus
on a different class of elements, comprising con-
junctions and prepositions, but also some adverbs
and prepositional phrases; collectively, we refer
to them as connectives. Since they are mainly
structural elements, they are more heavily affected
by differences in the syntax and grammar of Ital-
ian and LIS (and, presumably, in those of any
spoken language and the ?corresponding? SL).
Specifically, as we will see later, some connectives
are translated with a sign, some connectives are
dropped, whereas others affect the positioning of
other signs, or just their syntactic proximity.
It should be noted that our usage of the term
?connectives? is somewhat unorthodox. For ex-
ample, while prepositions can be seen as con-
nectives (Ferrari, 2008), only a few adverbs can
work as connectives. From the Italian Treebank,
we extracted all words or phrases that belonged
to a syntactic category that can be a connective
(conjunction, preposition, adverb or prepositional
phrase). We then found that we could better serve
the needs of ATLAS by running our analysis on
the entire resulting list, without filtering it by elim-
inating the entries that are not actual connectives.
In fact, semantic differences re-emerge through
our analysis: e.g., the temporal adverbs ?domani?
and ?dopodomani? are nearly always preserved,
as they do carry key information (especially for
weather forecasting) and are not structural ele-
ments.
In performing our analysis, we pursued a dif-
ferent path from the main project, relying entirely
on the bilingual corpus. Although the use of sta-
tistical techniques was hampered by the small size
of the corpus, at the same time it presented an in-
teresting opportunity to attack the problem from
a different angle. In this paper we describe how
we uncovered the translation distributions of the
different connectives from Italian to LIS via tree
alignment.
2 Corpus Analysis
The corpus consists of 40 weather forecasts in Ital-
ian and LIS. The Italian spoken utterance and LIS
signing were transcribed from the original videos
? one example of an Italian sentence and its LIS
equivalent are shown in Figure 1. An English
word-by-word translation is provided for the Ital-
ian sentence, followed by a more fluent transla-
tion; the LIS glosses are literally translated. Note
that as concerns LIS, this simply includes the gloss
for the corresponding sign. The 40 weather fore-
cast comprise 374 Italian sentences and 376 LIS
sentences, stored in 372 AEWLIS files. In most
cases, a file corresponds to one Italian sentence
and one corresponding LIS sentences; however,
there are 4 files where an Italian sentence is split
into two LIS sentences, and 2 files where two Ital-
ian sentences are merged into one LIS sentence.
AEWLIS is an XML-based format (see Fig-
ure 2) which represents each sign in the LIS sen-
tence as an element, in the order in which they oc-
cur in the sentence. A sign?s lemma is represented
by the Italian word with the same meaning, always
written in uppercase, and with its part of speech
(tipoAG in Figure 2); there are also IDs referenc-
ing the lemma?s position in a few dictionaries, but
these are not always present. The AEWLIS file
also stores several additional attributes, such as:
a parent reference that represents the syntax of
the LIS sentence; the syntactic role ?played? by
the sign in the LIS sentence; the facial expres-
sion accompanying the gesture; the location in the
signing space (which may be an absolute location
or a reference to a previous sign?s: compare HR
(High Right) and atLemma in Figure 2). These
attributes are stored as elements grouped by type,
and reference the corresponding sign element by
its ordinal position in the sentence. The additional
attributes are not always available: morphologi-
cal variations are annotated only when they differ
from an assumed standard form of the sign, while
the syntactic structure was annotated for only 89
sentences.
271
(1) (Ita.) Anche
Also
sulla
on
Sardegna
Sardinia
qualche
a few
annuvolamento
cloud covers
pomeridiano,
afternoon[adj],
possibilita`
chance
di
of
qualche
a few
breve
brief
scroscio
downpour
di
of
pioggia,
rain,
ma
but
tendenza
trend
poi
then
a
towards
schiarite.
sunny spells.
?Also on Sardinia skies will become overcast in the afternoon, chance of a few brief downpours of rain, but then a trend
towards a mix of sun and clouds?.
(2) (LIS) POMERIGGIO
Afternoon
SARDEGNA
Sardinia
AREA
area
NUVOLA
cloud
PURE
also
ACQUAZZONE
downpour
POTERE
can[modal]
MA
but
POI
then
NUVOLA
cloud
DIMINUIRE
decrease
Figure 1: Italian sentence and its LIS translation
<Lemmi>
<NuovoLemma lemma="POMERIGGIO" tipoAG="NOME" ... endTime="2.247" idSign=""/>
<NuovoLemma lemma="sardegna" tipoAG="NOME_PROPRIO" ... endTime="2.795" idSign="2687"/>
<NuovoLemma lemma="area" tipoAG="NOME" ... endTime="4.08" idSign="2642"/>
<NuovoLemma lemma="nuvola" tipoAG="NOME" ... endTime="5.486" idSign="2667"/>
<NuovoLemma lemma="pure" tipoAG="AVVERBIO" ... endTime="6.504" idSign="2681"/>
...
</Lemmi><SentenceAttribute>
<Parent>
<Timestamp time="1" value="ID:3"/> <Timestamp time="2" value="ID:2"/>
<Timestamp time="3" value="ID:3"/> <Timestamp time="4" value="root_1"/>
<Timestamp time="5" value="ID:3"/> ...
</Parent>
...
<Sign_Spatial_Location>
<Timestamp time="1" value=""/> <Timestamp time="2" value="HR"/>
<Timestamp time="3" value="HL"/> <Timestamp time="4" value="atLemma(ID:2, Distant)"/>
<Timestamp time="5" value=""/> ...
</Sign_Spatial_Location>
...
<Facial>
<Timestamp time="1" value=""/> <Timestamp time="2" value="eye brows:raise"/>
<Timestamp time="3" value="eye brows:raise"/> <Timestamp time="4" value="eye brows:-lwrd"/>
<Timestamp time="5" value=""/> ...
</Facial>
</SentenceAttribute>
Figure 2: Example excerpt from an AEWLIS file
2.1 Distributional statistics for connectives
The list of Italian connectives we considered was
extracted from the Italian Treebank developed
at the Institute for Computational Linguistics in
Pisa, Italy (Montemagni et al, 2003) by searching
for conjunctions, prepositions and adverbs. This
yielded a total of 777 potential connectives. Of
those, only 104 occur in our corpus. A simple
count of the occurrences of connectives in the Ital-
ian and LIS versions of the corpus yields the fol-
lowing results:
(a) 78 connectives (2068 occurrences total) only
occur in the Italian version, for example AL-
MENO (at least), CON (with), INFATTI (in-
deed), PER (for) .
(b) 8 connectives (67 occurrences total) only oc-
cur in the LIS version, for example CIRCA
(about), as in ?Here I am?), PURE (also, ad-
ditionally).
(c) 25 connectives (925 occurrences total) occur
in both versions.
For the third category, we have computed the
ratio of the number of occurrences in Italian over
the number of occurrences in LIS; the ratios are
plotted in logarithmic scale in Figure 3. 0 on the
scale corresponds to an ITA/LIS ratio equal to 1;
positive numbers indicate that there are more oc-
currences in ITA, negative numbers that there are
more occurrences in LIS. We can recognize three
clusters by ratio:
(c1) 9 connectives occurring in both languages,
but mainly in Italian, for example POCO (a
little), PIU? (more), SE (if), QUINDI (hence).
(c2) 13 connectives occurring in both languages
with similar frequency, for example SOLO
(only), POI (then), O (or), MA (but).
(c3) 3 connectives occurring in both languages,
but mainly in LIS: MENO (less), ADESSO
(now), INVECE (instead).
272
both
Page 1
abbastanza (enough)
adesso (now)
ancora (still, yet)
chiaro (clear)
domani (tomorrow)
dopodomani (day after tomorrow)
ecco (here)
invece (instead)
ma (but)
meglio (better)
meno (less)
o (or)
oggi (today)
ora (now)
ovunque (everywhere)
pi? (more)
poco (a little)
poi (then)
proprio (just, precisely)
qui (here)
quindi (hence)
se (if)
sicuro (sure)
solo (only)
tanto (much)
-0.5 0 0.5 1 1.5
Figure 3: Ratio of ITA/LIS occurrences in loga-
rithmic scale.
3 The effect of the Italian connectives on
the LIS translation
From this basic frequency analysis we can already
notice that a large number of connectives only ap-
pear in Italian, or have far more occurrences in
Italian than in LIS. This is unsurprising, consider-
ing that LIS sentences tend to be shorter than Ital-
ian sentences in terms of number of signs/words (a
fact which probably correlates with the increased
energy and time requirements intrinsic into artic-
ulating a message using one?s arms rather than
one?s tongue). However, our goal is to predict
when a connective should be dropped and when
it should be preserved. Furthermore, even if the
connective does not appear in the LIS sentence as
a directly corresponding sign, that does not mean
that its presence in the Italian sentence has no ef-
fect on the translation. We hypothesize four dif-
ferent possible realizations for a connective in the
Italian sentence:
? the connective word or phrase may map to a
corresponding connective sign;
? the connective is not present as a sign, but
may affect the morphology of the signs which
translate words syntactically adjacent to the
connective;
? the connective is not present as a sign, but
its presence may be reflected by the fact that
words connected by it map to signs which are
close to each other in the LIS syntax tree;
? the connective is dropped altogether.
The second hypothesis deserves some explana-
tion. The earliest treatments of LIS assumed that
each sign (lemma) could be treated as invariant.
Attempts to represent LIS in writing simply re-
placed each sign with a chosen Italian word (or
phrase, if necessary) with the same meaning. Al-
though this is still a useful way of representing the
basic lemma, more recent studies have noted that
LIS signs can undergo significant morphological
variations which are lost under such a scheme. The
AEWLIS format, in fact, was designed to preserve
them.
Of course, morphological variations in LIS are
not phonetic, like in a spoken language, but ges-
tural (Volterra, 1987; Romeo, 1991). For exam-
ple, the location in which a gesture is performed
may be varied, or its speed, or the facial expres-
sions that accompany it (Geraci et al, 2008). One
particularly interesting axis of morphology is the
positioning of the gesture in the signing space in
front of the signer. This space is implicitly divided
into a grid with a few different positions from left
to right and from top to bottom (see HR ? High
Right, and LH ? High Left, in Figure 2). Two or
more signs can then be placed in different posi-
tions in this virtual space, and by performing other
signs in the same positions the signer can express
a backreference to the previously established en-
tity at that location. One can even have a move-
ment verb where the starting and ending positions
of the gesture are positioned independently to in-
dicate the source and destination of the movement.
In other words, these morphological variations can
perform a similar function to gender and num-
ber agreement in Italian backreferences, but they
can also assume roles that in Italian would be per-
formed by prepositions, which are connectives. In
fact, as we will see later on, Italian prepositions are
never translated as signs, but are often associated
with morphological variations on related signs.
3.1 Tree Alignment
Two of our four translation hypotheses involve a
notion of distance on the syntax tree, and a no-
273
19_f08_2011-06-08_10_04_10.xml
Anche sulla Sardegna qualche annuvolamento pomeridiano, possibilit? di qualche breve scroscio di pioggia, ma tendenza poi a schiarite
pomeriggio Sardegna area nuvola pure acquazzone potere ma poi nuvola diminuire
sulla
Anche
Sardegna annuvolamento
qualchepomeridiano ,
possibilit?
, ma
di
scroscio
qualche breve di
pioggia
tendenza
poi a
schiarite
NUVOLA
POMERIGGIOAREA
PURE
POTERE
SARDEGNA ACQUAZZONEMA
NUVOLA DIMINUIRE
POI
Figure 4: Example of integrated syntax trees.
tion of signs corresponding to words. Therefore,
it is not sufficient to consider the LIS sentence
and the Italian sentence separately. Instead, their
syntax trees must be reconstructed and aligned.
Tree alignment in a variety of forms has been
extensively used in machine translation systems
(Gildea, 2003; Eisner, 2003; May and Knight,
2007). As far as we know, we are the first
to attempt the usage of tree alignment to aid in
the translation between a spoken and a sign lan-
guage, partly because corpora that include sync-
tactic trees for sign language sentences hardly ex-
ist. (Lo?pez-Luden?a et al, 2011) does use align-
ment techniques for translation from Spanish to
Spanish Sign Language (SSL), but it is limited to
alignment between words or phrases in Spanish,
and glosses or sequences of glosses in SSL.
We have developed a pipeline that takes in input
the corpus files, parses the Italian sentence with
an existing parser, and retrieves / builds a parse
tree for the LIS sentence. The two trees are then
aligned by exploiting the word/sign alignment. A
sample output is shown in Figure 4.
Italian sentence parsing. Since the corpus con-
tains the Italian sentences in plain, unstructured
text form, they need to be parsed. We used the
DeSR parser, a dependency parser pre-trained on a
very large Italian corpus (Attardi et al, 2007; Cia-
ramita and Attardi, 2011). This parser produced
the syntax trees and POS tagging that we used for
the Italian part of the corpus.
LIS syntax tree. One of the attributes allowed
by AEWLIS is ?parent?, which points a sign to its
parent in the syntax tree, or marks it as a root (see
Figure 2). These hand-built syntax trees are avail-
able in roughly 1/4 of the AEWLIS files. Because
the size of our corpus is already limited, and be-
274
cause no tools are available to generate LIS syn-
tax trees, for the remaining 3/4 of the corpus we
fell back on a simple linear tree where each sign is
connected to its predecessor. This solution at least
maintains locality in most cases.
Word Alignment. Having obtained syntax trees
for the two sentences, we then needed to align
them. For this purpose we used the Berkeley Word
Aligner (BWA) 1 (Denero, 2007), a general tool
for aligning sentences in bilingual corpora. BWA
takes as input a series of matching sentences in
two different languages, trains multiple unsuper-
vised alignment models, and selects the optimal
result using a testing set of manual alignments.
The output is a list of aligned word indices for
each input sentence pair. On our data set, BWA
performance is as follows: Precision = 0.736364,
Recall = 0.704348, AER = 0.280000.
Integration. The result is an integrated syntax
tree representation of the Italian and LIS ver-
sions of the sentence, with arcs bridging aligned
word/sign pairs. Since some connectives consist
of multi-word phrases, the word nodes which are
part of one are merged into a super-node that in-
herits all connections to other nodes. Figure 4
shows the end result for the Italian and LIS sen-
tences in Figure 1 (the two sentences are repeated
for convenience at the bottom of Figure 4). The
rectangular boxes are words in the Italian sen-
tence, while the rounded boxes are signs in the LIS
sentence. The Italian tree has its root(s) at the bot-
tom, while the LIS tree has its root(s) at the top.
Solid arrows point from children to parent nodes
in the syntax tree. Gray-shaded boxes represent
connectives (words or signs, as indicated by the
border of the box). Bold dashed lines show word
alignment. Edges with round heads show relation-
ships where a sign has a location attribute refer-
encing another sign. Arrows with an empty tri-
angular head trace the paths described in the next
section.
3.2 Subtree alignment and path processing
At this point individual words are aligned, but that
is not sufficient. Our hypotheses on the effect of
connectives on translation requires us to align a
tree fragment surrounding the Italian connective
with the corresponding tree fragment on the LIS
1http://code.google.com/p/
berkeleyaligner/
side - where the connective may be missing. In ef-
fect, since we have hypothesized that the presence
of a connective can affect the translation of the two
subtrees that it connects, we would like to be able
to align each of those subtrees to its translation.
However, given the differences between the two
languages, it is not easy to give a clear definition
of this mapping - let alne to compute it.
Instead, we can take a step back to word-level
alignment. We make the observation that, if two
words belong to two different subtrees linked by a
connective, so that the path between the two words
goes through the connective, then the frontier be-
tween the LIS counterparts of those two subtrees
should also lie along the path between the signs
aligned with those two words. If the connective is
preserved in translation as a sign, we should ex-
pect to find it along that path; if it is not, its ef-
fect should still be seen along that path, either in
the form of morphological variations to the signs
along the path, or in the shortness of the path itself.
The first step, then, is to split the Italian syntax
tree by removing the connective. This yields one
subtree containing the connective?s parent node, if
any, and one subtree for each of the connective?s
children, if any. The parent subtree typically con-
tains most of the rest of the sentence, so only the
direct ancestors of the connective are considered.
Then, each pair of words belonging to different
subtrees is linked by a path that goes through the
connective in the original tree. Of these words, we
select the ones that have aligned signs, and then
we compute the path between each pair of signs
aligned to words belonging to different subtrees.
This gives us a set of paths to consider in the LIS
syntax tree.
For example, let us consider the connective ?di?
between ?possibilita`? and ?scroscio? in Figure 4.
? This node connects two subtrees: a child sub-
tree containing ?qualche, breve, scroscio, di,
pioggia?, and a parent subtree containing the
rest of the sentence.
? From each subtree, a set of paths is gener-
ated: all paths extending from the connective
to the leaves of the child subtree (for exam-
ple ?scroscio, qualche? or ?scroscio, di, pi-
oggia?), and the path of direct ancestors in
the parent tree (?sulla, annuvolamento, pos-
sibilita`?).
? Iterate through the cartesian product of each
275
Table 1: Translation candidates for connectives with more than 10 occurrences
Connective ITA Occurrences Sign Location Close Missing
domani 71 67 (94.37%) 1 (1.41%) 2 (2.82%) 4 (5.63%)
dopodomani 15 14 (93.33%) 0 (0.00%) 0 (0.00%) 1 (6.67%)
mentre 28 26 (92.86%) 5 (17.86%) 0 (0.00%) 1 (3.57%)
o 37 37 (100.00%) 2 (5.41%) 6 (16.22%) 0 (0.00%)
pero` 10 9 (90.00%) 1 (10.00%) 1 (10.00%) 1 (10.00%)
ancora 72 44 (61.11%) 1 (1.39%) 3 (4.17%) 25 (34.72%)
invece 17 9 (52.94%) 1 (5.88%) 2 (11.76%) 6 (35.29%)
ma 51 29 (56.86%) 1 (1.96%) 2 (3.92%) 21 (41.18%)
poi 22 10 (45.45%) 2 (9.09%) 0 (0.00%) 10 (45.45%)
abbastanza 11 4 (36.36%) 1 (9.09%) 0 (0.00%) 6 (54.55%)
anche 89 33 (37.08%) 5 (5.62%) 1 (1.12%) 53 (59.55%)
ora 17 6 (35.29%) 1 (5.88%) 1 (5.88%) 10 (58.82%)
proprio 11 5 (45.45%) 0 (0.00%) 0 (0.00%) 6 (54.55%)
quindi 35 9 (25.71%) 1 (2.86%) 0 (0.00%) 25 (71.43%)
come 16 0 (0.00%) 1 (6.25%) 1 (6.25%) 14 (87.50%)
dove 28 0 (0.00%) 1 (3.57%) 0 (0.00%) 27 (96.43%)
generalmente 13 0 (0.00%) 0 (0.00%) 0 (0.00%) 13 (100.00%)
per quanto riguarda 14 0 (0.00%) 0 (0.00%) 1 (7.14%) 13 (92.86%)
piuttosto 13 0 (0.00%) 0 (0.00%) 0 (0.00%) 13 (100.00%)
piu` 57 0 (0.00%) 3 (5.26%) 2 (3.51%) 52 (91.23%)
poco 63 2 (3.17%) 3 (4.76%) 0 (0.00%) 58 (92.06%)
sempre 13 1 (7.69%) 0 (0.00%) 0 (0.00%) 12 (92.31%)
soprattutto 16 1 (6.25%) 1 (6.25%) 0 (0.00%) 14 (87.50%)
a 111 0 (0.00%) 18 (16.22%) 30 (27.03%) 66 (59.46%)
con 91 0 (0.00%) 20 (21.98%) 11 (12.09%) 62 (68.13%)
da 97 0 (0.00%) 26 (26.80%) 18 (18.56%) 62 (63.92%)
di 510 2 (0.39%) 92 (18.04%) 140 (27.45%) 312 (61.18%)
e 206 17 (8.25%) 34 (16.50%) 25 (12.14%) 140 (67.96%)
in 168 6 (3.57%) 37 (22.02%) 16 (9.52%) 113 (67.26%)
per 120 0 (0.00%) 7 (5.83%) 35 (29.17%) 82 (68.33%)
su 327 4 (1.22%) 121 (37.00%) 38 (11.62%) 190 (58.10%)
verso 18 0 (0.00%) 6 (33.33%) 1 (5.56%) 12 (66.67%)
pair of sets (in this case we have only one
pair), and consider the full path formed by the
two paths connected by the connective node
(for instance, ?sulla, annuvolamento, possib-
lita`, di, scroscio, breve?).
? For each of these paths, take the signs aligned
to words on different sides of the target con-
nective, and find the shortest path between
those signs in the LIS syntax tree; we call this
the aligned path. For example, from ?pos-
sibilita`? and ?scroscio? we find ?POTERE,
ACQUAZZONE?. If this process generates
multiple paths, only the maximal ones are
kept.
By looking at words within a certain distance of
the connective, at their aligned signs, and at the
distance between those signs in the aligned path,
the program then produces one or more ?transla-
tion candidates? for each occurrence of a connec-
tive:
? Sign: if the connective word is aligned to a
connective sign in LIS, that is its direct trans-
lation;
? Location: if morphology variations (cur-
rently limited to the ?location? attribute, see
Figure 2) are present on a sign aligned to an
It. word belonging to one of the examined
paths, and the word is less than 2 steps away
from the connective, that morphological vari-
ation in LIS may capture the function of the
connective;
? Close: if two It. words are connected by
a connective, and they map to signs which
have a very short path between them (up to
3 nodes, including the two signs), the con-
nective may be reflected simply in this close
connection between the translated subtrees in
the LIS syntax tree;
? Missing: if none of the above hypotheses are
possible, we hypothesize that the connective
has been simply dropped.
Table 1 shows the results of this analysis. It in-
cludes only connectives with more than 10 occur-
rences. For each connective and translation hy-
pothesis, the shading of the cell is proportional to
276
the fraction of occurrences where that hypothesis
is possible; this fraction is also given as a percent.
Note that Sign, Location and Close candidates are
not mutually exclusive: for instance, an occur-
rence of a connective might be directly aligned
with a sign, but at the same time it might fit the
criteria for a Location candidate. For this reason,
the sum of the percents in the four columns is not
necessarily 100.
k-means clustering (MacQueen, 1967; Lloyd,
1982) has been applied to the connectives, with
the aforementioned fractions as the features. The
resulting five clusters are represented by the row
groupings in the table.
The first cluster contains words which clearly
have a corresponding sign in LIS, such as ?do-
mani? (tomorrow). ?Domani? and ?dopodomani?
are not actually connectives, while ?mentre?, ?o?
and ?pero`? are. It is interesting to note that, while
a logician might expect ?e? (and) and ?o? (or) to
be treated similarly, they actually work quite dif-
ferently in LIS: there is a specific sign for ?o?, but
there is no sign for ?e?. Instead, signs are sim-
ply juxtaposed in LIS where ?e? would be used in
Italian.
The words in the second cluster also have a di-
rect sign translation, but they are missing in the
LIS translation around half of the time. Several
words represent connections with previous state-
ments or situations, such as ?ancora? (again), ?in-
vece? (instead), ?ma? (but). These appear to be
often dropped in LIS when they reference a previ-
ous sentence, e.g. a sentence-initial ?ma?; or when
they are redundant in Italian, e.g. ?ma? in ?ma an-
che? (?but also?). Therefore, we think can see two
phenomena at play here: a stronger principle of
economy in LIS, and a reduced number of explicit
connections across sentences.
The third cluster is similar to the second cluster,
but with a higher percent of dropped connectives.
This is probably related to the semantics of these
five words. ?Abbastanza? means ?quite, enough?,
and in general indicates a medium quantity, not
particularly large nor particularly small. It is no
surprise that this word is more likely to succumb
to principles of economy in language. ?Anche?
means ?also?, and is either translated as ?PURE?
(also) or dropped. This does not seem to depend
on the specific circumstances of its usage; rather, it
seems to be largely a stylistic choice by the trans-
lator. ?Proprio? (?precisely?, ?just?) has a corre-
sponding sign ?PROPRIO?, but since it does not
convey essential information it is a good candidate
for dropping. ?Quindi?, meaning ?therefore?, has
its own sign ?QUINDI?, but once again the causal
relationship it conveys is usually not essential to
understanding what the weather will be, and thus
it is frequently dropped.
The fourth cluster consists of connectives which
are largely simply dropped. Some of these are el-
ements that just contribute to the discourse flow
in Italian, such as ?per quanto riguarda? (?con-
cerning?); in fact, this connective mainly oc-
curs in sentence-initial position in the Italian sen-
tences in our corpus and denotes a change of
topic from the previous sentence, corroborating
our hypothesis of a reduced use of explicit inter-
sentence connections in LIS. It may seem strange
for comparative and intensity markers such as
?piu`? (more) or ?poco? (a little) to be so consis-
tently dropped, but it turns out that intensity varia-
tions for weather phenomena are often embedded
into a specific sign, for example ?NUVOLOSITA`
AUMENTARE? (increasing cloud cover).
The fifth cluster contains all Italian prepositions
(with 10 or more occurrences in the corpus), none
of which is translated as a sign (the 6 occurrences
for ?in?, the 4 for ?su? and the 2 for ?di? are due
to alignment errors). We can conclude that prepo-
sitions do not exist in LIS as parts of speech; how-
ever, the prepositions in this cluster are often asso-
ciated with morphological variations in the spatial
positioning of related signs, which suggests that
the role associated with these prepositions in Ital-
ian is performed by these variations in LIS. The
conjunction ?e? (and) also ends up in this cluster,
although it has 8 legitimate sign alignments with
?pure? (?too?); the rest are alignment errors. Un-
surprisingly, all connectives in this class also have
high ratings for the ?close? hypothesis.
4 Rule extraction
We trained a classifier to help a LIS generator de-
termine how an Italian connective should be trans-
lated. Because the translation pipeline we plan to
integrate with is rule-based, we chose a Decision
Tree as our classifier: this allows rules to be easily
extracted from the classification model.
In order to identify a single class for each exam-
ple, we ranked the four possible translation can-
didates as follows: Sign is the strongest, then
Location, then Close, and finally Missing is the
277
child1 align = None ? word = Per quanto riguarda ? parent align = None ? Missing
child1 align = None ? word = Per quanto riguarda ? parent align = PREVEDERE ? Close
child1 align = None ? word = o ? Align(O)
child1 align = None ? word = su ? child2 align mykind = location ? child2 align = SICILIA ? Location
Figure 5: Some rules extracted from the decision tree
weakest. Then, each example is labeled with
the strongest translation candidate available for it:
thus, for example, if the connective word appears
to be translated with a connective sign, and the
words it connects are also aligned to signs which
are close to each other syntactically, then the class
is Sign, not Close.
Our training data suffers from large imbalance
between the ?missing? class and the others. A
classifier that simply labels all examples as ?miss-
ing? would have an accuracy above 60%, and in
fact, that is the classifier that we obtain if we at-
tempt to automatically optimize the parameters of
a Decision Tree (DT). We also note that, for con-
nectives where both options are possible, choosing
to translate them can make the sentence more ver-
bose, but choosing to drop them risks losing part of
the sentence?s meaning: the worse risk is the lat-
ter. Following accepted practice with unbalanced
datasets (Chawla et al, 2004), we rebalanced the
classes by duplicating all examples of the Align,
Location and Close classes, but not those of the
Missing class.
On our data set of connectives with at least
10 occurrences, we trained a DT using AdaBoost
(Freund and Schapire, 1997). The features include
the word neighboring the connective in the Ital-
ian syntax tree, their aligned signs if any, part of
speech tags, and semantic categories such as time
or location. The resulting tree is very large, but
we provide a few examples of the rules that can be
extracted from it in Figure 5.
Bootstrap evaluation shows our DT to have an
accuracy of 83.58% ? 1.03%. In contrast, a base-
line approach of taking the most common class for
each connective results in an accuracy of 68.70%
? 0.88%. Furthermore, the baseline classifier has
abysmal recall for the Close and Location classes
(0.00% and 0.85%, respectively), which our DT
greatly improves upon (86.73% and 75.32%).
In order to estimate the impact of the lack of
a LIS syntax tree in most of the corpus, we also
learned and evaluated a DT using only the 1/4 of
the corpus for which LIS syntax trees are avail-
able. The accuracy is 81.44% ? 2.03%, versus a
baseline of 71.55% ? 1.74%. The recall for Close
and Location is 89.22% and 73.58%, vs. 0.00%
and 3.51% for the baseline. These results are com-
parable with the those obtained on the whole cor-
pus, confirming that linear trees are a reasonable
fallback.
Both clustering and classification were per-
formed using RapidMiner 5.3. 2
5 Conclusions and Future Work
The small size of our corpus, with around 375
bilingual sentences, posed a large challenge to
the use of statistical methods; on the other hand,
having no access to a LIS speaker prevented us
from simply relying on a rule-based approach. By
combining syntax tree processing with several ma-
chine learning techniques, we were able to analyze
the corpus and detect patterns that show linguis-
tic substance. We have produced initial results in
terms of rule extraction, and we will be integrat-
ing these rules into the full Italian-LIS translation
system to produce improved translation of connec-
tives.
6 Acknowledgements
This work was supported by the ATLAS project,
funded by Regione Piemonte within the ?CIPE
2007? framework. Partial support to the au-
thors was also provided by awards IIS 0905593
(from the NSF) and NPRP 5-939-1-155 (from the
QNRF). A special thanks to A. Mazzei (ATLAS)
for his willingness to answer our email bursts.
Thanks to other members of ATLAS, in particu-
lar P. Prinetto, N. Bertoldi, C. Geraci, L. Lesmo;
and to C. Soria, who extracted the list of potential
connectives from the Italian Treebank.
References
Nadeem Ahmad, Davide Barberis, Nicola Garazzino,
Paolo Prinetto, Umar Shoaib, and Gabriele Tiotto.
2012. A virtual character based italian sign language
dictionary. In Proceedings of the Conference Uni-
versal Learning Design. Masaryk University.
2http://rapid-i.com/
278
Abdulaziz Almohimeed, Mike Wald, and R.I. Damper.
2011. Arabic Text to Arabic Sign Language Trans-
lation System for the Deaf and Hearing-Impaired
Community. In Proceedings of the Second Work-
shop on Speech and Language Processing for Assis-
tive Technologies, pages 101?109, Edinburgh, Scot-
land, UK, July. Association for Computational Lin-
guistics.
Giuseppe Attardi, Felice Dell?Orletta, Maria Simi,
Atanas Chanev, and Massimiliano Ciaramita. 2007.
Multilingual dependency parsing and domain adap-
tation using DeSR. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL, pages
1112?1118.
N Bertoldi, G Tiotto, P Prinetto, E Piccolo, F Nunnari,
V Lombardo, A Mazzei, R Damiano, L Lesmo, and
A Del Principe. 2010. On the creation and the an-
notation of a large-scale Italian-LIS parallel corpus.
In 4th Workshop on the Representation and Process-
ing of Sign Languages: Corpora and Sign Language
Technologies, CSLT.
Annelies Braffort, Laurence Bolot, E Chtelat-Pel, An-
nick Choisier, Maxime Delorme, Michael Filhol,
Je?re?mie Segouat, Cyril Verrecchia, Flora Badin, and
Nad?ege Devos. 2010. Sign language corpora for
analysis, processing and evaluation. In Proc. of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC?10).
Nitesh V Chawla, Nathalie Japkowicz, and Aleksander
Kotcz. 2004. Editorial: special issue on learning
from imbalanced data sets. ACM SIGKDD Explo-
rations Newsletter, 6(1):1?6.
Massimiliano Ciaramita and Giuseppe Attardi. 2011.
Dependency parsing with second-order feature maps
and annotated semantic information. In H. Bunt,
P. Merlo, and J. Nivre, editors, Trends in Parsing
Technology, volume 43 of Text, Speech and Lan-
guage Technology, pages 87?104. Springer.
John Denero. 2007. Tailoring word alignments to syn-
tactic machine translation. In In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2007, pages 17?24.
Philippe Dreuw, Jens Forster, Yannick Gweth,
Daniel Stein, Hermann Ney, Gregorio Martinez,
Jaume Verges Llahi, Onno Crasborn, Ellen Ormel,
Wei Du, et al 2010. SignSpeak?understanding,
recognition, and translation of sign languages. In
The 4th Workshop on the Representation and Pro-
cessing of Sign Languages: Corpora and Sign Lan-
guage Technologies (CSLT 2010), pages 22?23, Val-
letta, Malta.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 2, pages 205?208. As-
sociation for Computational Linguistics.
Angela Ferrari. 2008. Congiunzioni frasali, congiun-
zioni testuali e preposizioni: stessa logica, diversa
testualita`. In Emanuela Cresti, editor, Prospettive
nello studio del lessico italiano, Atti del IX Con-
gresso della Societa` Internazionale di Linguistica e
Filologia Italiana, pages 411?416, Florence, Italy.
Firenze University Press.
Yoav Freund and Robert E Schapire. 1997. A
decision-theoretic generalization of on-line learning
and an application to boosting. Journal of computer
and system sciences, 55(1):119?139.
Carlo Geraci, Marta Gozzi, Costanza Papagno, and
Carlo Cecchetto. 2008. How grammar can cope
with limited short-term memory: Simultaneity and
seriality in sign languages. Cognition, 106(2):780?
804.
Daniel Gildea. 2003. Loosely tree-based alignment
for machine translation. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 80?87. Association for
Computational Linguistics.
Stuart Lloyd. 1982. Least squares quantization in
pcm. Information Theory, IEEE Transactions on,
28(2):129?137.
Vincenzo Lombardo, Fabrizio Nunnari, and Rossana
Damiano. 2010. A virtual interpreter for the
Italian Sign Language. In Proceedings of the
10th International Conference on Intelligent Virtual
Agents, IVA?10, pages 201?207, Berlin, Heidelberg.
Springer-Verlag.
Vincenzo Lombardo, Cristina Battaglino, Rossana
Damiano, and Fabrizio Nunnari. 2011. An avatar-
based interface for the Italian Sign Language. In
2011 International Conference on Complex, Intelli-
gent and Software Intensive Systems (CISIS), pages
589?594. IEEE.
Vero?nica Lo?pez-Luden?a, Rube?n San-Segundo,
Syaheerah Lufti, Juan Manuel Lucas-Cuesta,
Julia?n David Echevarry, and Beatriz Mart??nez-
Gonza?lez. 2011. Source Language Categorization
for improving a Speech into Sign Language Trans-
lation System. In Proceedings of the Second
Workshop on Speech and Language Processing for
Assistive Technologies, pages 84?93, Edinburgh,
Scotland, UK, July.
Vero?nica Lo?pez-Luden?a, Rube?n San-Segundo,
Juan Manuel Montero, Ricardo Co?rdoba, Javier
Ferreiros, and Jose? Manuel Pardo. 2011. Automatic
categorization for improving Spanish into Spanish
Sign Language machine translation. Computer
Speech & Language.
Pengfei Lu and Matt Huenerfauth. 2010. Collecting
a Motion-Capture Corpus of American Sign Lan-
guage for Data-Driven Generation Research. In
Proceedings of the NAACL HLT 2010 Workshop
on Speech and Language Processing for Assistive
279
Technologies, pages 89?97, Los Angeles, California,
June. Association for Computational Linguistics.
Pengfei Lu and Matt Huenerfauth. 2012. Learn-
ing a Vector-Based Model of American Sign Lan-
guage Inflecting Verbs from Motion-Capture Data.
In Proceedings of the Third Workshop on Speech
and Language Processing for Assistive Technolo-
gies, pages 66?74, Montre?al, Canada, June. Asso-
ciation for Computational Linguistics.
James MacQueen. 1967. Some methods for classi-
fication and analysis of multivariate observations.
In Proceedings of the fifth Berkeley symposium on
mathematical statistics and probability, volume 1,
page 14. California, USA.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Pro-
ceedings of EMNLP, pages 360?368.
Alessandro Mazzei. 2012. Sign language generation
with expert systems and ccg. In Proceedings of the
Seventh International Natural Language Generation
Conference, INLG ?12, pages 105?109, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Simonetta Montemagni, Francesco Barsotti, Marco
Battista, Nicoletta Calzolari, Ornella Corazzari,
Alessandro Lenci, Antonio Zampolli, Francesca
Fanciulli, Maria Massetani, Remo Raffaelli, et al
2003. Building the Italian syntactic-semantic Tree-
bank. Treebanks, pages 189?210.
S. Morrissey, H. Somers, R. Smith, S. Gilchrist, and
S. Dandapat. 2010. Building Sign Language Cor-
pora for Use in Machine Translation. In 4th Work-
shop on the Representation and Processing of Sign
Languages: Corpora and Sign Language Technolo-
gies (CSLT 2010), pages 172?178, Valletta, Malta.
P. Prinetto, U. Shoaib, and G. Tiotto. 2011. The Ital-
ian Sign Language Sign Bank: Using WordNet for
Sign Language corpus creation. In 2011 Interna-
tional Conference on Communications and Informa-
tion Technology (ICCIT), pages 134?137.
Orazio Romeo. 1991. Dizionario dei segni: la lingua
dei segni in 1400 immagini. Zanichelli.
Virginia Volterra. 1987. La lingua italiana dei
segni: la comunicazione visivo-gestuale dei sordi.
Il Mulino.
280
Simple but effective feedback generation to tutor abstract problem solving
Xin Lu, Barbara Di Eugenio, Stellan Ohlsson and Davide Fossati
University of Illinois at Chicago
Chicago, IL 60607, USA
xinlu@northsideinc.com
bdieugen,stellan,dfossa1@uic.edu
Abstract
To generate natural language feedback for an
intelligent tutoring system, we developed a
simple planning model with a distinguishing
feature: its plan operators are derived auto-
matically, on the basis of the association rules
mined from our tutorial dialog corpus. Auto-
matically mined rules are also used for real-
ization. We evaluated 5 different versions of
a system that tutors on an abstract sequence
learning task. The version that uses our plan-
ning framework is significantly more effective
than the other four versions. We compared this
version to the human tutors we employed in
our tutorial dialogs, with intriguing results.
1 Introduction
Intelligent Tutoring Systems (ITSs) are software
systems that provide individualized instruction, like
human tutors do in one-on-one tutoring sessions.
Whereas ITSs have been shown to be effective in
engendering learning gains, they still are not equiv-
alent to human tutors. Hence, many researchers
are exploring Natural Language (NL) as the key to
bridging the gap between human tutors and current
ITSs. A few results are now available, that show that
ITS with relatively sophisticated language interfaces
are more effective than some other competitive con-
dition (Graesser et al, 2004; Litman et al, 2006;
Evens and Michael, 2006; Kumar et al, 2006; Van-
Lehn et al, 2007; Di Eugenio et al, 2008). Ascer-
taining which specific features of the NL interaction
are responsible for learning still remains an open re-
search question.
In our experiments, we contrasted the richness
with which human tutors respond to student ac-
tions with poorer forms of providing feedback, e.g.
only graphical. Our study starts exploring the role
that positive feedback plays in tutoring and in ITSs.
While it has long been observed that most tutors tend
to avoid direct negative feedback, e.g. (Fox, 1993;
Moore et al, 2004), ITSs mostly provide negative
feedback, as they react to student errors.
In this paper, we will first briefly describe our tu-
torial dialog collection. We will then present the
planning architecture that underlies our feedback
generator. Even if our ITS does not currently al-
low for student input, our generation architecture is
inspired by state-of-the art tutorial dialog manage-
ment (Freedman, 2000; Jordan et al, 2001; Zinn et
al., 2002). One limitation of these approaches is that
plan operators are difficult to maintain and extend,
partly because they are manually defined and tuned.
Crucially, our plan operators are automatically de-
rived via the association rules mined from our cor-
pus. Finally, we will devote a substantial amount
of space to evaluation. Our work is among the first
to show not only that a more sophisticated language
interface results in more learning, but that it favor-
ably compares with human tutors. Full details on
our work can be found in (Lu, 2007).
2 Task and curriculum
Our domain concerns extrapolating letter patterns,
such as inferring EFMGHM, given the pattern
ABMCDM and the new initial letter E. This task is
used in cognitive science to investigate human in-
formation processing (Kotovsky and Simon, 1973;
Reed and Johnson, 1994; Nokes and Ohlsson, 2005).
The curriculum we designed consists of 13 patterns
of increasing length and difficulty; it was used un-
changed in both our human data collection and ITS
experiments. The curriculum is followed by two
104
Tutor Moves
Answer student?s questions
Evaluate student?s actions
Summarize what done so far
Prompt student into activity
Diagnose what student is doing
Instruct
Demonstrate how to solve (portions of) problem
Support ? Encourage student
Conversation ? Acknowledgments, small talk
Student Moves
Question
Explain what student said or did
Reflect ? Evaluate own understanding
Answer tutor?s question
Action Response ? Perform non-linguistic action
(e.g. write letter down)
Complete tutor?s utterance
Conversation ? Acknowledgments, small talk
Table 1: Tutor and student moves
post-test problems, each 15 letter long: subjects (all
from the psychology subject pool) have n trials to
extrapolate each pattern, always starting from a dif-
ferent letter (n = 6 in the human conditions, and
n = 10 in the ITS conditions). While the ear-
lier example was kept simple for illustrative pur-
poses, our patterns become very complex. Start-
ing e.g. from the letter L, we invite the reader to
extrapolate problem 9 in our curriculum: BDDFF-
FCCEEGGGC, or the second test problem: ACZD-
BYYDFXGEWWGI.1
3 Human dialogs
Three tutors ? one expert, one novice, and one
(the lecturer) experienced in teaching, but not in
one-on-one tutoring ? were videotaped as they in-
teracted with 11 subjects each.2 A repeated mea-
sures ANOVA, followed by post-hoc tests, revealed
that students with the expert tutor performed signifi-
cantly better than the students with the other two tu-
tors on both test problems (p < 0.05 in both cases).
36 dialog excerpts were transcribed, taken from
1The solutions are, respectively: LNNPPPMMOOQQQM,
and LNZOMYYOQXRPWWRT .
2One goal of ours was to ascertain whether expert tutors are
indeed more effective than non-expert tutors, not at all a fore-
gone conclusion since very few studies have contrasted expert
and non expert tutors, e.g. (Glass et al, 1999).
18 different subjects (6 per tutor), for a total of
about 2600 tutor utterances and 660 student ut-
terances (transcription guidelines were taken from
(MacWhinney, 2000)). For each subject, these two
dialog excerpts cover the whole interaction with the
tutor on one easy and one difficult problem (# 2 and
9 respectively). 2 groups of 2 coders each, annotated
half of the transcripts each, with dialogue moves.
Our move inventory comprises 9 tutor moves and
7 student moves, as listed in Table 1.3 Table 2
presents an annotated fragment from one of the di-
alogues with the expert tutor. Kappa measures for
intercoder agreement had values in the following
ranges, according to the scale in (Rietveld and van
Hout, 1993): for tutor moves, from moderate (0.4
for Support) to excellent (0.82 for Prompt); for stu-
dent moves, from substantial (0.64 for Explanation)
to excellent (0.82 for Question, 0.97 for ActionRe-
sponse). Whereas some of these Kappa measures
are lower than what we had strived for, we decided
to nonetheless use the move inventory in its entirety,
after the coders reconciled their codings. In fact, our
ultimate evaluation measure concerns learning, and
indeed the ITS version that uses that entire move in-
ventory engenders the most learning. Please see (Lu
et al, 2007) for a detailed analysis of these dialogs
and for a discussion of differences among the tutors
in terms of tutor and student moves.
The transcripts were further annotated by one
coder for tutor attitude (whether the tutor agrees
with the student?s response ? positive, negative, neu-
tral), for correctness of student move and for stu-
dent confidence (positive, negative, neutral). Stu-
dent hesitation time (long, medium, short) was es-
timated by the transcribers. Additionally, we an-
notated for the problem features under discussion.
Of the 8 possible relationships between letters, most
relevant to the examples discussed in this paper are
forward, backward, progression and marker. E.g. in
ABMCDM, M functions as chunk marker, and the
sequence moves forward by one step, both within
one chunk and across chunks. Within and across are
two out of 4 relationship scopes, which encode the
coverage of a particular relationship within the se-
quence.
3There is no explicit tutor question move because we focus
on the goal of a tutor?s question, either prompt or diagnose.
105
Line Utterances Annotation
38 Tutor: how?d you actually get the n in the first place? Diagnose
39 Student: from here I count from c to g and then just from n to r. Answer
40 Tutor: okay so do the c to g. Prompt
41 Tutor: do it out loud so I can hear you do it. Prompt
42 Student: c d e f. Explain
43 Student: so it?s three spaces. Answer
44 Tutor: okay so it?s three spaces in between. Summarize
Table 2: An annotated fragment from a dialogue with the expert tutor
4 Learning rules to provide feedback
Once the corpus was annotated, we mined the ex-
pert tutor portion via Classification based on Associ-
ations (CBA) (Liu et al, 1998). CBA generates un-
derstandable rules and has been effectively applied
to various domains. CBA finds all rules that exist
in the data, which is especially important for small
training sets such as ours.
To modularize what the rules should learn, we de-
composed what the tutor should do into two com-
ponents pertaining to content: letter relationship and
relationship scope; and two components pertaining
to how to deliver that content: tutor move and tutor
attitude. Hence, we derived 4 sets of tutorial rules.
Features used in the rules are those annotated on the
tutoring dialogs, plus the student?s Knowledge State
(KS) on each type of letter relationship rel, com-
puted as follows:
KS(rel) = b
p? 0.5 + w
t
? 5c (1)
p is the number of partially correct student inputs, w
is the number of wrong student inputs and t is the to-
tal number of student inputs (?inputs? here are only
those relevant to the relationship rel, as logged by
the ITS from the beginning of the session). KS(rel)
ranges from 0 to 5. The higher the value, the worse
the performance on rel. The scale of 5 was chosen
to result in just enough values for KS(rel) to be use-
ful for classification.
We ran experiments with different lengths of dia-
log history, but using only the last utterance gave us
the best results. Three of the four rule sets have ac-
curacies between 88 and 90% (results are based on
6-way cross-validation, and the cardinality of each
set of rules is in the low hundreds. ). Whereas the
tutor move rule set only has 57% accuracy, as for
some of the low Kappa values mentioned earlier, our
relation-marker = No, relation-forward = Yes,
student-move = ActionResponse
? relation-forward = Yes
(Confidence = 100%, Support = 4.396%)
correctness = wrong, scope-within = No,
KS(backward) = 0, relation-forward = Yes
? tutor-move = Summarize
(Confidence = 100%, Support = 6.983%)
correctness = wrong, relation-forward = Yes,
KS(forward) = 1, hesitation = no
? tutor-attitude = negative
(Confidence = 100%, Support = 1.130%)
Figure 1: Example Tutorial Rules
ultimate evaluation measure is that the NL feedback
based on these rules does improve learning.
Figure 1 shows three example rules, for choosing
relationship, move and attitude respectively ? we?ll
discuss two of them. The first rule predicts that the
ITS will continue focusing on the forward relation,
if it was focusing on forward and not on marker, and
the student just input something. The second rule
chooses the summarize move if the student made a
mistake, the ITS was focusing on forward but not on
relationships within chunks, and the student showed
perfect knowledge of backward.
Two strength measurements are associated with
each rule X ? y. A rule holds with confidence
conf if conf% of cases that containX also contain y;
and with support sup if sup% of cases contain X or
y. Rules are ordered, with confidence having prece-
dence over support. Ties over confidence are solved
via support; any remaining ties are solved according
to the order rules were generated.
106
For each Tut-Move-Rule TMRi,k whose Left-Hand Side LHS matches ISi do:
1. Create and Populate New Plan pi,k:
(a) preconditions = ISi; actions = tutor move from TMRi,k; strength = confidence and support from TMRi,k
(b) Fill remaining slots in pi,k:
i. contents = relationship ? scope (from highest ranked rules that match ISi from relationship and scope
rule sets);
ii. modifiers = attitude (from highest ranked rule that matches ISi from tutor attitude rule set)
2. Augment Plan: do the following n times :
(a) make copy of ISi and name it ISi+1;
(b) change agent to ?tutor?;
(c) change corresponding elements in ISi+1 to move, attitude, letter relationship and scope from pi,k;
(d) from the two rule sets for tutor move and tutor attitude, retrieve highest ranked rules that match ISi+1,
TMRi+1,j and TARi+1,j
(e) add to actions tutor move from TMRi+1,j ; add to modifiers tutor attitude from TARi+1,j
Figure 2: Plan generation
5 From rules to plans
For our task of extrapolating abstract sequences, we
built a model-tracing ITS by means of the Tutoring
Development Kit (TDK) (Koedinger et al, 2003).
Model-tracing ITSs codify cognitive skills via pro-
duction rules. The student?s solution is monitored
by rules that fire according to the underlying model.
When the student steps differ from that model, an
error is recognized. A portion of the student inter-
face of the ITS is shown in Figure 4a. It mainly in-
cludes two rows, one showing the Example Pattern,
the other for the student to input the New Pattern
extrapolated starting with the letter in the first cell.
In model-tracing ITSs, production rules provide
the capability to generate simple template-based
messages. We developed a more sophisticated NL
feedback generator consisting of three major mod-
ules: update, planning and feedback realization.
The update module maintains the context, rep-
resented by the Information State (IS) (Larsson
and Traum, 2000), which captures the overall dia-
log context and interfaces with external knowledge
sources (e.g., curriculum, tutorial rules) and the pro-
duction rule system. As the student performs a new
action, the IS is updated. The planning module gen-
erates or revises the system plan and selects the next
tutoring move based on the newly updated IS. At
last the feedback realization module transforms this
move into NL feedback.
The planning module consists of three compo-
nents, plan generation, plan selection and plan mon-
itoring. A plan includes an ordered collection of tu-
toring moves meant to help the student correctly fill
a single cell. The structure of our plans is shown in
Figure 3.
Plan generation generates a plan set which con-
tains one plan for each tutor move rule that matches
the current ISi. Each of these plans is augmented at
plan generation time by ?simulating? the next ISi+1
that would result if the move is executed but its ef-
fects are not achieved. The algorithm is sketched
in Figure 2. The planner iterates through the tutor
move rule set.4 Recall that our four rule sets are to-
tally ordered. Also, note that each rule set contains a
default rule that fires when no rule matches the cur-
rent ISi. In Step 1b, at every iteration only the rules
that have not been checked yet from those three rule
sets are considered. In Step 2, n is set to 3, i.e., each
plan contains 3 additional moves and corresponding
attitudes, which will provide hints when no response
from the student occurs. Three hints plus one orig-
inal move makes 4, which is the average number of
moves in one turn of the expert tutor.
An example plan is shown in Figure 3. It is gen-
erated in reaction to the mistake in Figure 4a, and by
4Since there is no language input, rules which include stu-
dent moves other than ActionResponse in their LHS will never
be activated. Additionally, we recast tutor answers as confirm
moves, since students cannot ask questions.
107
Preconditions (same as the IS in Figure 4b)
Effects student?s input = W
Contents relationship = forward
scope = across
Actions summarize, evaluate, prompt,
summarize
Modifiers negative, negative, neutral, neutral
Strength conf = 100%, sup = 6.983%
Figure 3: An Example Plan
firing, among others, the rules in Figure 1. The IS in
Figure 4b reflects some of the history of this interac-
tion (in the slots Relationships, Scopes and KS), and
as such corresponds to the situation depicted in Fig-
ure 4a in a specific context (this plan was generated
in one of our user experiments).
The plan selection component retrieves the high-
est ranked plan in the newly generated plan set, se-
lects a template for each tutoring move in its ?Ac-
tions? slot and puts each tutoring move onto the di-
alog move (DM) stack. Earlier we mentioned that
rules are totally ordered according to confidence,
then support and finally rule generation order. When
a plan set contains more than one plan, plans are
also totally ordered, since they inherit strength mea-
surements from the rule that engenders the first tutor
move in the Actions slot.
After the student receives the message which
realizes the top tutoring move in the DM stack,
plan monitoring checks whether its intended effects
have been obtained. If the effects have not been ob-
tained, and the student?s input is unchanged, the next
move from the DM stack will be executed to pro-
vide the student with hint messages until either the
student?s input changes or the DM stack becomes
empty. If the DM stack becomes empty, the next
plan is selected from the original plan set and the
tutoring moves within that plan are pushed onto the
DM stack. Whenever the student?s input changes,
or after every plan in the plan set has been selected,
control returns to plan generation.
The realization module. A tutor move is pushed
onto the DM stack by plan selection together with
a template to realize it. 50 templates were writ-
ten manually upon inspection of the expert tutor di-
alogs. Since several templates can realize each tutor
move, we used CBA to learn rules to choose among
templates. Features used to learn when to use each
(a) A Student Action in Problem 4
1. Agent: student (producer of current move);
2. Relationships: forward, progress in length
3. Scopes: across (for ?forward?), within (for
?progress in length?);
4. Agent?s move: action response;
5. Agent?s attitude: positive (since student shows
no hesitation before inputting letter);
6. Correctness: wrong (correct letter is W);
7. Student?s input: X;
8. Student?s selection: 4th cell in New Pattern row;
9. Hesitation time: no;
10. Student?s knowledge state (KS): 1 (on ?for-
ward?), 3 (on ?progress in length?).
(b) The corresponding IS
Figure 4: A snapshot of an ITS interaction
template also include the tutor attitude. For the first
Summarize move in the plan in Figure 3, given the
IS in Figure 4b, the rule in Fig. 5 will fire (tutor at-
titude does not affect this specific rule). As a result,
the following feedback message is generated: ?From
V to X, you are going forward 2 in the alphabet.?
6 Evaluation
To demonstrate the utility of our feedback genera-
tor, we developed five different versions of our ITS,
named according to how feedback is generated:
1. No feedback: The ITS only provides the basic
interface, so that subjects can practice solving
the 13 problems in the curriculum, but does not
provide any kind of feedback.
2. Color only: The ITS provides graphic feed-
back by turning the input green if it is correct
or red if it is wrong.
3. Negative: In addition to the color feedback, the
108
scope-within = No, relation-marker = No,
relation-forward = Yes, move= Summarize ?
template = TPL11
[where TPL11: From ?<reference-pattern>?
to ?<input>?, you are going <input-relation>
<input-number> in the alphabet.]
Figure 5: Example Realization Rule
ITS provides feedback messages when the in-
put is wrong.
4. Positive: In addition to the color feedback, the
ITS provides feedback messages when the in-
put is correct.
5. Model: In addition to the color feedback, the
ITS provides feedback messages generated by
the feedback generator just described.
Feedback is given for each input letter. Positive
and negative verbal feedback messages are given
out whenever the student?s input is correct or incor-
rect, respectively. Positive feedback messages con-
firm the correct input and explain the relationships
which this input is involved in. Negative feedback
messages flag the incorrect input and deliver hints.
The feedback messages for the ?negative? and ?pos-
itive? versions were developed earlier in the project,
to avoid repetitions and inspired by the expert tu-
tor?s language but before we performed any anno-
tation and mining. They are directly generated by
TDK production rules.
Although in reality positive and negative feedback
are both present in tutoring sessions, one study for
the letter pattern task shows that positive/negative
feedback, given independently, perform different
functions (Corrigan-Halpern and Ohlsson, 2002). In
addition, our negative condition is meant to embody
the ?classical? model-tracing ITS, that only reacts
to student errors. Hence, in our experiments, we
elected to keep these two types of feedback separate,
other than in the ?model? version of the ITS.
To evaluate the five versions of the ITS, we ran a
between-subjects study in which each group of sub-
jects interacted with one version of the system. A
group of control subjects took the post-test with no
training at all but only read a short description of the
Score
Condition Prob 1 Prob 2 Total
0 Control 36.50 32.84 69.34
1 No feedback 58.21 75.27 133.48
2 Color only 68.32 66.30 134.62
3 Negative 70.33 66.06 141.83
4 Positive 75.06 79.00 154.06
5 Model 91.95 101.76 193.71
Table 3: Average Post-test Scores of the ITS
domain.5 Subjects were trained to solve the same
13 problems in the curriculum that were used in the
human tutoring condition. They also did the same
post-test (2 problems, each pattern 15 letters long).
For each post-test problem, each subject had 10 tri-
als, where each trial started with a new letter.
6.1 Results
Table 3 reports the average post-test scores of the
six groups of subjects, corresponding to the five ver-
sions of the ITS and the control condition. Perfor-
mance on each problem is measured by the number
of correct letters out of a total of 150 letters (15 let-
ters by 10 trials); hence, cumulative post-test score,
is the number of correct letters out of 300 possible.
A note before we proceed. In ITS research it is
common to administer the same test before (pre-test)
and after treatment (post-test), but we only have the
post-test. The pre/post-test paradigm is used for two
reasons. First, for evaluation proper, to gauge learn-
ing gains. Second, to verify that the groups have the
same level of pre-tutoring ability, as shown when the
pre-tests of the different groups are statistically in-
distinguishable, and hence, that they can be rightly
compared. Even without a pre-test we can assess
this. An ANOVA on ?time spent on the first 3 prob-
lems? revealed no significant differences across the
different groups. Since time spent on the first 3 prob-
lems is highly correlated with post-test score (multi-
ple regression, p < 0.03), this provides indirect evi-
dence that all subjects before treatment have equiva-
lent ability for this task. Hence, we can trust that our
evaluation, in terms of absolute scores, does reveal
differences between conditions.
Our main findings are based on one-way
ANOVAs, followed by Tukey post-hoc tests:
5The number of subjects in each condition varies from 32 to
38. Groups differ in size because of technical problems.
109
? A main effect of ITS (p ? 0.05). Subjects who
interacted with any version of the ITS had sig-
nificantly higher total post-test scores than sub-
jects in the control condition.
? A main effect of modeled feedback (p< 0.05).
Subjects who interacted with the ?model? ver-
sion of the ITS had significantly higher total
post-test scores than control subjects, and sub-
jects with any other version of the ITS.
? No other effects. Subjects trained by the three
versions ?color only?, ?negative?, ?positive?,
did not have significantly higher total post-test
scores than subjects with the ?no feedback?
version; neither did subjects trained by the two
versions ?negative?, ?positive?, wrt subjects
with the ?color-only? version.
If we examine individual problems, the same pat-
tern of results hold, other than, interestingly, the
model and positive versions are not significantly dif-
ferent any more. As customary, we also analyze ef-
fect sizes , i.e., how much more subjects learn with
the ?model? ITS in comparison to the other condi-
tions. On the Y axis, Figure 6 shows Cohen?s d, a
common measure of effect size. Each point repre-
sents the difference between the means of the scores
in the ?model? ITS and in one of the other condi-
tion, divided by the standard deviation of either con-
dition. According to (Cohen, 1988), the effect sizes
shown in Figure 6 are large as concerns the compari-
son with the ?no feedback?, ?color only? and ?nega-
tive? conditions, and moderate as concerns the ?pos-
itive? condition.6
ITSs and Human Tutors. After we established
that, at least cumulatively, the ?model? ITS is more
effective than the other ITSs, we wanted to assess
how well the ?model? ITS fares in comparison to
the expert tutor it is modeled on. Since in the human
data each post-test problem consists of only 6 trials,
the first 6 trials per problem from the ITSs are used
to run this comparison, for a maximum total score
of 180 (15 letters by 6 trials, by 2 problems). Fig-
ure 7 shows the overall post-test performance of all
9 conditions. The error bars in the figure represent
the standard deviations.
6A very large effect size with respect to control is not shown
in Figure 6.
Figure 6: Effect sizes: how much more subjects
learn with the ?model? ITS
Figure 7: Post-test performance ? all conditions
Paired t-tests between the model ITS and each of
the human tutors show that:7
? on problem 1, the ?model? ITS is indistinguish-
able from the expert tutor, and is significantly
better than the novice and the lecturer
(p = 0.05 and p = 0.039 respectively);
? on problem 2, the model ITS is significantly
worse than the expert tutor (p = 0.020), and
is not different from the other two tutors;
? cumulatively, there are no significant differ-
ences between the ?model? ITS and any of the
three human tutors.
7A 9-way ANOVA among all conditions is not appropriate,
since in a sense we have two ?super conditions?, human and
ITS. It is better to compare the ?model? ITS to each of the human
tutors via t-tests, as a follow-up to the differences highlighted by
the separate analyses on the two ?super conditions?.
110
7 Discussion and Conclusions
Our results add to the growing body of evidence
that language feedback engenders more learning not
only than simple practice, but also, than less sophis-
ticated language feedback. Importantly, our ?model?
ITS appears intriguingly close to our expert tutor in
effectiveness: on post-test problem 1, it is as effec-
tive as the expert tutor himself, and significantly bet-
ter than the other two tutors, as the expert tutor is. It
appears our ?model? ITS does capture at least some
features of successful tutoring.
As concerns the specific language the ITS gen-
erates, we compared different ways of providing
verbal feedback. A subject receives both positive
and negative verbal feedback when interacting with
the ?model? version, while a subject receives only
one type of verbal feedback when interacting with
the ?positive? and ?negative? versions (recall that
in all these versions including the ?model? ITS the
red/green graphical feedback is provided on every
input). While we cannot draw definite conclusions
regarding the functions of positive and negative
feedback, since the ?model? version provides other
tutorial moves beyond positive / negative feedback,
we have suggestive evidence that negative feedback
by itself is not as effective. Additionally, positive
feedback appears to play an important role. First,
the ?model? and the ?positive? versions are statisti-
cally equivalent when we analyze performance on
individual problems. Further, in the ?model? ver-
sion, the ratio of positive to negative messages turns
out to be 9 to 1. In our tutoring dialogs, positive
feedback still outnumbers negative feedback, but by
a lower margin, 4 to 1. We hypothesize that convey-
ing a positive attitude in an ITS is perhaps even more
important than in human tutoring since a human has
many more ways of conveying subtle shades of ap-
proval and disapproval.
From the NLG point of view, we have presented
a simple generation architecture that turns out to be
rather effective. Among its clear limitations are the
lack of hierarchical planning, and the fact that dif-
ferent components of a plan are generated indepen-
dently one from the other. Among its strengths are
that the plan operators are derived automatically via
the rules we mined, both for content planning and,
partly, for realization.
It clearly remains to be seen whether our NLG
framework can easily be ported to other domains
? the issue is not domain dependence, but whether
a more complex domain will require some form of
hierarchical planning. We are now working in the
domain of Computer Science data structures and al-
gorithms, where we continue exploring the role of
positive feedback. We collected data with two tu-
tors in that domain, and there again, we found that
in the human data positive feedback occurs about 8
times more often than negative feedback. We are
now annotating the data to mine it as we did here,
and developing the core ITS.
Acknowledgments
This work was supported by awards N00014-00-
1-0640 and N00014-07-1-0040 from the Office of
Naval Research, by Campus Research Board S02
and S03 awards from the University of Illinois at
Chicago, and in part, by awards IIS 0133123 and
ALT 0536968 from the National Science Founda-
tion.
References
J. Cohen. 1988. Statistical power analysis for the be-
havioral sciences (2nd ed.). Hillsdale, NJ: Lawrence
Earlbaum Associates.
Andrew Corrigan-Halpern and Stellan Ohlsson. 2002.
Feedback effects in the acquisition of a hierarchical
skill. In Proceedings of the 24th Annual Conference
of the Cognitive Science Society.
Barbara Di Eugenio, Davide Fossati, Susan Haller, Dan
Yu, and Michael Glass. 2008. Be brief, and they
shall learn: Generating concise language feedback for
a computer tutor. International Journal of AI in Edu-
cation, 18(4). To appear.
Martha W. Evens and Joel A. Michael. 2006. One-on-
one Tutoring by Humans and Machines. Mahwah, NJ:
Lawrence Erlbaum Associates.
Barbara A. Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the design of instructional systems.
Lawrence Erlbaum Associates, Hillsdale, NJ.
Reva K. Freedman. 2000. Plan-based dialogue manage-
ment in a physics tutor. In Proceedings of the Sixth
Applied Natural Language Conference, Seattle, WA,
May.
Michael Glass, Jung Hee Kim, Martha W. Evens, Joel A.
Michael, and Allen A. Rovick. 1999. Novice vs. ex-
pert tutors: A comparison of style. In MAICS-99, Pro-
ceedings of the Tenth Midwest AI and Cognitive Sci-
ence Conference, pages 43?49, Bloomington, IN.
111
Arthur C. Graesser, S. Lu, G.T. Jackson, H. Mitchell,
M. Ventura, A. Olney, and M.M. Louwerse. 2004.
AutoTutor: A tutor with dialogue in natural language.
Behavioral Research Methods, Instruments, and Com-
puters, 36:180?193.
Pamela Jordan, Carolyn Penstein Rose?, and Kurt Van-
Lehn. 2001. Tools for authoring tutorial dialogue
knowledge. In Proceedings of AI in Education 2001
Conference.
Kenneth R. Koedinger, Vincent Aleven, and Neil T. Hef-
fernan. 2003. Toward a rapid development environ-
ment for cognitive tutors. In 12th Annual Conference
on Behavior Representation in Modeling and Simula-
tion.
K. Kotovsky and H. Simon. 1973. Empirical tests of a
theory of human acquisition of information-processing
analysis. British Journal of Psychology, 61:243?257.
Rohit Kumar, Carolyn P. Rose?, Vincent Aleven, Ana Igle-
sias, and Allen Robinson. 2006. Evaluating the Ef-
fectiveness of Tutorial Dialogue Instruction in an Ex-
ploratory Learning Context. In Proceedings of the
Seventh International Conference on Intelligent Tutor-
ing Systems, Jhongli, Taiwan, June.
Staffan Larsson and David R. Traum. 2000. Information
state and dialogue management in the trindi dialogue
move engine toolkit. Natural Language Engineering,
6(3-4):323?340.
Diane J. Litman, Carolyn P. Rose?, Kate Forbes-Riley,
Kurt VanLehn, Dumisizwe Bhembe, and Scott Silli-
man. 2006. Spoken versus typed human and computer
dialogue tutoring. International Journal of Artificial
Intelligence in Education, 16:145?170.
Bing Liu, Wynne Hsu, and Yiming Ma. 1998. Inte-
grating classification and association rule mining. In
Knowledge Discovery and Data Mining, pages 80?86,
New York, August.
Xin Lu, Barbara Di Eugenio, Trina Kershaw, Stellan
Ohlsson, and Andrew Corrigan-Halpern. 2007. Ex-
pert vs. non-expert tutoring: Dialogue moves, in-
teraction patterns and multi-utterance turns. In CI-
CLING07, Proceedings of the 8th International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics, pages 456?467. Best Student Paper
Award.
Xin Lu. 2007. Expert tutoring and natural language
feedback in intelligent tutoring systems. Ph.D. thesis,
University of Illinois - Chicago.
Brian MacWhinney. 2000. The CHILDES project. Tools
for analyzing talk: Transcription Format and Pro-
grams, volume 1. Lawrence Erlbaum, Mahwah, NJ,
third edition.
Johanna D. Moore, Kaska Porayska-Pomsta, Sebastian
Varges, and Claus Zinn. 2004. Generating Tutorial
Feedback with Affect. In FLAIRS04, Proceedings of
the Seventeenth International Florida Artificial Intel-
ligence Research Society Conference.
Timothy J. Nokes and Stellan Ohlsson. 2005. Compar-
ing multiple paths to mastery: What is learned? Cog-
nitive Science, 29:769?796.
Jonathan Reed and Peder Johnson. 1994. Assessing im-
plicit learning with indirect tests: Determining what is
learned about sequence structure. Journal of Exper-
imental Psychology: Learning, Memory, and Cogni-
tion, 20(3):585?594.
Toni Rietveld and Roeland van Hout. 1993. Statistical
Techniques for the Study of Language and Language
Behaviour. Mouton de Gruyter, Berlin - New York.
Kurt VanLehn, Arthur C. Graesser, G. Tanner Jackson,
Pamela W. Jordan, Andrew Olney, and Carolyn P.
Rose?. 2007. When are tutorial dialogues more effec-
tive than reading? Cognitive Science, 31(1):3?62.
Claus Zinn, Johanna D. Moore, and Mark G. Core. 2002.
A 3-tier planning architecture for managing tutorial
dialogue. In ITS 2002, 6th. Intl. Conference on In-
telligent Tutoring Systems, pages 574?584, Biarritz,
France.
112
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 114?119,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Lucene and Maximum Entropy Model Based Hedge Detection System
Lin Chen
University of Illinois at Chicago
Chicago, IL, USA
lin@chenlin.net
Barbara Di Eugenio
University of Illinois at Chicago
Chicago, IL, USA
bdieugen@uic.edu
Abstract
This paper describes the approach to
hedge detection we developed, in order to
participate in the shared task at CoNLL-
2010. A supervised learning approach is
employed in our implementation. Hedge
cue annotations in the training data are
used as the seed to build a reliable hedge
cue set. Maximum Entropy (MaxEnt)
model is used as the learning technique to
determine uncertainty. By making use of
Apache Lucene, we are able to do fuzzy
string match to extract hedge cues, and
to incorporate part-of-speech (POS) tags
in hedge cues. Not only can our system
determine the certainty of the sentence,
but is also able to find all the contained
hedges. Our system was ranked third on
the Wikipedia dataset. In later experi-
ments with different parameters, we fur-
ther improved our results, with a 0.612
F-score on the Wikipedia dataset, and a
0.802 F-score on the biological dataset.
1 Introduction
A hedge is a mitigating device used to lessen the
impact of an utterance1. As a very important way
to precisely express the degree of accuracy and
truth assessment in human communication, hedg-
ing is widely used in both spoken and written lan-
guages. Detecting hedges in natural language text
can be very useful for areas like text mining and
information extraction. For example, in opinion
mining, hedges can be used to assess the degree
of sentiment, and refine sentiment classes from
{positive, negative, objective} to {positive, some-
how positive, objective, somehow objective, nega-
tive, somehow negative}.
1http://en.wikipedia.org/wiki/
Hedge(linguistics)
Hedge detection related work has been con-
ducted by several people. Light et al (2004)
started to do annotations on biomedicine article
abstracts, and conducted the preliminary work of
automatic classification for uncertainty. Medlock
and Briscoe (2007) devised detailed guidelines
for hedge annotations, and used a probabilistic
weakly supervised learning approach to classify
hedges. Ganter and Strube (2009) took Wikipedia
articles as training corpus, used weasel words? fre-
quency and syntactic patterns as features to clas-
sify uncertainty.
The rest of the paper is organized as follows.
Section 2 shows the architecture of our system.
Section 3 explains how we make use of Apache
Lucene to do fuzzy string match and incorporate
POS tag in hedge cues and our method to gener-
ate hedge cue candidates. Section 4 describes the
details of using MaxEnt model to classify uncer-
tainty. We present and discuss experiments and
results in section 5, and conclude in section 6.
2 System Architecture
Our system is divided into training and testing
modules. The architecture of our system is shown
in Figure 1.
In the training module, we use the training cor-
pus to learn a reliable hedge cue set with bal-
anced support and confidence, then train a Max-
Ent model for each hedge cue to classify the un-
certainty for sentences matched by that hedge cue.
In the testing module, the learned hedge cues
are used to match the sentences to classify, then
each matched sentence is classified using the cor-
responding MaxEnt model. A sentence will be
classified as uncertain if the MaxEnt model deter-
mines it is. Because of this design, our system is
not only able to check if a sentence is uncertain,
but also can detect the contained hedges.
114
Testing Data
Lucene Indexer
Index with POS
Hedge Cues
More Cue?
Get A Hedge Cue
Get Matched Sentences
More Sentence?
Marked
Uncertain?
Get the MaxEnt Model
Uncertain?
Mark Sentece Uncertainty
yes
no
no
yes
Get A Sentence
yes
yes
Output
Marked
Uncertainty 
no
Lucene Indexer
Training Data
Index with POS
Hedge Cues
Annotation Extender
Token Pruner
POS Tag Replacer
Confidence?
Support?
MaxEnt Models
MaxEnt Trainer
Hedge Cue Candidate Generator
Hedge Cue 
Candidates
yes
Trainging Testing
Figure 1: System Architecture
3 Learn Hedge Cues
The training data provided by CoNLL-2010
shared task contain ?<ccue></ccue>? annota-
tions for uncertain sentences. Most of the annota-
tions are either too strict, which makes them hard
to use to match other sentences, or too general,
which means that most of the matched sentences
are not uncertain.
Similar to how Liu (2007) measures the useful-
ness of association rules, we use support and con-
fidence to measure the usefulness of a hedge cue.
Support is the ratio of sentences containing a
hedge cue to all sentences. Because in a train-
ing dataset, the number of all the sentences is a
fixed constant, we only use the number of sen-
tences containing the hedge cue as support, see
formula 1. In the other part of this paper, sentences
matched by hedge cues means sentences contains
hedge cues. We use support to measure the degree
of generality of a hedge cue.
sup = count of matched sentences (1)
Confidence is the ratio of sentences which con-
tain a hedge cue and are uncertain to all the sen-
tences containing the hedge cue, as formula 2.
We use confidence to measure the reliability for
a word or phrase to be a hedge cue.
conf = count of matched and uncertaincount of matched sentences (2)
3.1 Usage of Apache Lucene
Apache Lucene2 is a full text indexing Java library
provided as an open source project of Apache
Foundation. It provides flexible indexing and
search capability for text documents, and it has
very high performance. To explain the integra-
tion of Lucene into our implementation, we need
to introduce several terms, some of which come
from McCandless et al (2010).
? Analyzer: Raw texts are preprocessed before
being added to the index: text preprocessing
components such as tokenization, stop words
removal, and stemming are parts of an ana-
lyzer.
? Document: A document represents a collec-
tion of fields, it could be a web page, a text
file, or only a paragraph of an article.
? Field: A field represents a document or the
meta-data associated with that document, like
the author, type, URL. A field has a name and
a value, and a bunch of options to control how
Lucene will index its value.
? Term: The very basic unit of a search. It con-
tains a field name and a value to search.
2http://lucene.apache.org
115
? Query: The root class used to do search upon
an index.
In our implementation, Lucene is used for the
following 3 purposes:
? Enable quick counting for combinations of
words and POS tags.
? Store the training and testing corpus for fast
counting and retrieval.
? Allow gap between words or POS tags in
hedge cues to match sentences.
Lucene provides the capability to build cus-
tomized analyzers for complex linguistics analy-
sis. Our customized Lucene analyzer employs to-
kenizer and POS tagger from OpenNLP tools3 to
do tokenization and POS tagging. For every word
in the sentence, we put two Lucene tokens in the
same position, by setting up the second token?s Po-
sitionIncremental attribute to be 0.
For example, for sentence it is believed to be
very good, our analyzer will make Lucene store it
as Figure 2 in its index.
It to beis believed very good
PRP TO VBVBZ VBN RB VBN
60 1 2 3 4 5
Figure 2: Customized Tokenizer Example
Indexing text in that way, we are able to match
sentences cross words and POS tags. For example,
the phrase it is believed will be matched by it is be-
lieved, it is VBN, it VBZ believed. This technique
enables us to generalize a hedge cue.
In our implementation, all the data for training
and testing are indexed. The indexing schema is: a
sentence is treated as a Lucene document; the con-
tent of the sentence is analyzed by our customized
analyzer; other information like sentence id, sen-
tence position, uncertainty is stored as fields of the
document. In this way, we can query all those
fields, and when we find a match, we can easily
get al the information out just from the index.
Lucene provides various types of queries to
search the indexed content. We use SpanNear-
Query and BooleanQuery to search the matched
sentences for hedge cues. We rely on SpanNear-
Query?s feature of allowing positional restriction
3http://opennlp.sourceforge.net
when matching sentences. When building a Span-
NearQuery, we can specify the position gap al-
lowed among the terms in the query. We build a
SpanNearQuery from a hedge cue, put each token
as a term of the query, and set the position gap to
be 2. Take Figure 3 as an example, because the
gap between token is and said is 1, is less than the
specified gap setting 2, so It is widely said to be
good will count as a match with hedge cue is said.
Figure 3: SpanNearQuery Matching Example
We use BooleanQuery with nested SpanNear-
Query and TermQuery to count uncertain sen-
tences, then to calculate the confidence of a hedge
cue.
3.2 Hedge Cue Candidate Generation
We firstly tried to use the token as the basic unit for
hedge cues. However, several pieces of evidence
suggest it is not appropriate.
? Low Coverage. We only get 42 tokens in
Wikipedia training data, using 20, 0.4 as the
thresholds for support and confidence.
? Irrelevant words or stop words with lower
thresholds. When we use 5, 0.3 as the thresh-
olds for coverage and confidence, we get 279
tokens, however, words like is, his, musical,
voters, makers appear in the list.
We noticed that many phrases with similar
structures or fixed collocations appear very often
in the annotations, like it is believed, it is thought,
many of them, many of these and etc. Based on this
observation, we calculated the support and confi-
dence for some examples, see table 1.
Hedge Cue Sup. Conf.
it is believed 14 .93
by some 30 .87
many of 135 .65
Table 1: Hedge Cue Examples
We decided to use the phrase or collocation as
the basic unit for hedge cues. There are two prob-
lems in using the original annotations as hedge
cues:
116
? High confidence but low coverage: annota-
tions that contain proper nouns always have
very high confidence, usually 100%, how-
ever, they have very low support.
? High coverage but low confidence: annota-
tions with only one token are very frequent,
but only a few of them result in enough con-
fidence.
To balance confidence and support, we built our
hedge cue candidate generator. Its architecture is
presented in Figure 4.
Cue Annotations
Cue Candidates
Annotation Extender
Tokens > 1
NO
Token Pruner
POS Tag Replacer
YES
Figure 4: Hedge Cue Candidate Generator
The three main components of the hedge cue
candidate generator are described below.
Annotation Extender: When the input hedge
cue annotation contains only 1 token, this compo-
nent will be used. It will generate 3 more hedge
cue candidates by adding the surrounding tokens.
We expect to discover candidates with higher con-
fidence.
Token Pruner: According to our observations,
proper nouns rarely contribute to the uncertainty
of a sentence, and our Lucene based string match-
ing method ensures that the matched sentences re-
main matched after we remove tokens from the
original cue annotation. So we remove proper
nouns in the original cue annotation to generate
hedge cue candidates. By using this component,
we expect to extract hedge cues with higher sup-
port.
POS Tag Replacer: This component is used to
generalize similar phrases, by using POS tags to
replace the concrete words. For example, we use
the POS tag VBN to replace believed in it is be-
lieved to generate it is VBN. Hence, when a sen-
tence contains it is thought in the testing dataset,
even if it is thought never appeared in the train-
ing data set, we will still be able to match it and
classify it against the trained MaxEnt model. We
expect that this component will be able to increase
support. Due to the O(2n) time complexity, we did
not try the brute force approach to replace every
word, only the words with the POS tags in Table 2
are replaced in the process.
POS Description Example
VBN past participle verb it is believed
NNS plural common noun some countries
DT determiner some of those
CD numeral, cardinal one of the best
Table 2: POS Tag Replacer Examples
After hedge cue candidates are generated, we
convert them to Lucene queries to calculate their
confidence and support. We prune those that fall
below the predefined confidence and support set-
tings.
4 Learn Uncertainty
Not all the learned hedge cues have 100% uncer-
tainty confidence, given a hedge cue, we need to
learn how to classify whether a matched sentence
is uncertain or not. The classification model is,
given a tuple of (Sentence, Hedge Cue), in which
the sentence contains the hedge cue, we classify it
to the outcome set {Certain, Uncertain}.
MaxEnt is a general purpose machine learn-
ing technique, it makes no assumptions in addi-
tion to what we know from the data. MaxEnt has
been widely used in Natural Language Processing
(NLP) tasks like POS tagging, word sense disam-
biguation, and proved its efficiency. Due to Max-
Ent?s capability to combine multiple and depen-
dent knowledge sources, we employed MaxEnt as
our machine learning model. Features we used to
train the model include meta information features
and collocation features.
Meta Information Features include three fea-
tures:
? Sentence Location: The location of the sen-
tence in the article, whether in the title or in
the content. We observed sentences in the ti-
tle are rarely uncertain.
? Number of Tokens: The number of tokens
in the sentence. Title of article is usually
shorter, and more likely to be certain.
117
? Hedge Cue Location: The location of
matched tokens in a sentence. We consider
them to be in the beginning, if the first token
of the matched part is the first token in the
sentence; to be at the end, if the last token of
the matched part is the last token of the sen-
tence; otherwise, they are in the middle. We
were trying to use this feature as a simplified
version to model the syntactic role of hedge
cues in sentences.
Collocation Features include the word and POS
tag collocation features:
? Word Collocation: Using a window size of 5,
extract all the word within that window, ex-
cluding punctuation.
? POS Tag Collocation: Using a window size
of 5, extract all the POS tags of tokens within
that window, excluding punctuation.
We use the OpenNLP MaxEnt4 Java library as
the MaxEnt trainer and classifier. For each hedge
cue, the training is iterated 100 times, with no cut
off threshold for events.
5 Experiments and Discussion
We first ran experiments to evaluate the perfor-
mance of the entire system. We used official
dataset as training and testing, with different con-
fidence and support thresholds. The result on offi-
cial Wikipedia dataset is presented in Table 3. Re-
sult on the biological dataset is listed in Table 4.
In the result tables, the first 2 columns are the con-
fidence and support threshold; ?Cues? is the num-
ber of generated hedge cues; the last 3 columns are
standard classifier evaluation measures.
Our submitted result used 0.35, 5 as the thresh-
olds for confidence and support. We officially
placed third on the Wikipedia dataset, with a
0.5741 F-score, and third from last on the biolog-
ical dataset, with a 0.7692 F-score. In later ex-
periments, we used different parameters, which re-
sulted in a 0.03 F-score improvement. We believe
the big difference of ranking on different datasets
comes from the incomplete training. Due to incor-
rect estimation of running time, we only used the
smaller training file in our submitted biological re-
sult.
From Table 3 and 4, we can see that a higher
confidence threshold gives higher precision, and
4http://maxent.sourceforge.net
Conf. Sup. Cues Prec. Recall F
0.4
10 360 0.658 0.561 0.606
15 254 0.672 0.534 0.595
20 186 0.682 0.508 0.582
0.45
10 293 0.7 0.534 0.606
15 190 0.717 0.503 0.591
20 137 0.732 0.476 0.577
0.5
5 480 0.712 0.536 0.612
10 222 0.736 0.492 0.590
15 149 0.746 0.468 0.575
20 112 0.758 0.443 0.559
Table 3: Evaluation Result on Wikipedia Dataset
Conf. Sup. Cues Prec. Recall F
0.4
10 330 0.68 0.884 0.769
15 229 0.681 0.861 0.76
20 187 0.679 0.842 0.752
0.45
10 317 0.689 0.878 0.772
15 220 0.69 0.857 0.764
20 179 0.688 0.838 0.756
0.5
5 586 0.724 0.899 0.802
10 297 0.742 0.841 0.788
15 206 0.742 0.819 0.779
20 169 0.74 0.8 0.769
Table 4: Evaluation Result on Biological Dataset
a lower support threshold leads to higher recall.
Since the lower support threshold could generate
more hedge cues, it will generate less training in-
stances for hedge cues with both low confidence
and support, which affects the performance of the
MaxEnt classifier. In both datasets, it appears that
0.5 and 5 are the best thresholds for confidence
and support, respectively.
Beyond the performance of the entire system,
our hedge cue generator yields very promising re-
sults. Using the best parameters we just noted
above, our hedge cue generator generates 52 hedge
cues with confidence 100% on the Wikipedia
dataset, and 332 hedge cues in the biological
dataset. Some hedge cue examples are shown in
Table 5.
We also ran experiments to verify the perfor-
mance of our MaxEnt classifier. We used the same
setting of datasets as for the system performance
evaluation. Given a hedge cue, we extracted all
the matched sentences from the training set to train
a MaxEnt classifier, and used it to classify the
matched sentences by the hedge cue in testing set.
118
Hedge Cue Sup. Conf. TestSize Prec. Recall F
indicated that 63 0.984 6 1.0 1.0 1.0
by some 30 0.867 29 0.966 1.000 0.983
are considered 29 0.724 10 0.750 0.857 0.800
some of NNS 62 0.613 27 1.000 0.778 0.875
the most JJ 213 0.432 129 0.873 0.475 0.615
Table 6: MaxEnt Classifier Performance
Hedge Cue Conf. Sup.
probably VBN 1.0 21
DT probably 1.0 15
many NNS believe 1.0 10
NNS suggested DT 1.0 248
results suggest 1.0 122
has VBN widely VBN 1.0 10
Table 5: Generated Hedge Cue Examples
Table 6 shows the results, the hedge cues were
manually chosen with relative higher support.
We can see that the performance of the MaxEnt
classifier correlates tightly with confidence and
support. Higher confidence means a more accu-
rate detection for a phrase to be hedge cue, while
higher support means more training instances for
the classifier: the best strategy would be to find
hedge cues with both high confidence and support.
While experimenting with the system, we found
several potential improvements.
? Normalize words. Take the word suggest as
an example. In the generated hedge cues,
we found that its other forms are everywhere,
like it suggested, NNS suggests a, and DT
suggesting that. As we put POS tags into
Lucene index, we can normalize words to
their base forms using a morphology parser,
and put base forms into index. After that, the
query with suggest will match all the forms.
? Use more sophisticated features to train the
MaxEnt classifier. Currently we only use
shallow linguistics information as features,
however we noticed that the role of the phrase
could be very important to decide whether it
indicates uncertainty. We can deep parse sen-
tences, extract the role information, and add
it to the feature list of classifier.
6 Conclusion
In this paper, we described the hedge detection
system we developed to participate in the shared
task of CoNLL-2010. Our system uses a heuristic
learner to learn hedge cues, and uses MaxEnt as
its machine learning model to classify uncertainty
for sentences matched by hedge cues. Hedge cues
in our system include both words and POS tags,
which make themmore general. Apache Lucene is
integrated into our system to efficiently run com-
plex linguistic queries on the corpus.
Acknowledgments
This work is supported by award IIS-0905593
from the National Science Foundation.
References
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore,
August. Association for Computational Linguistics.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings of
BioLink 2004 Workshop on Linking Biological Lit-
erature, Ontologies and Databases: Tools for Users,
pages 17?24, Boston, Mass, May.
Bing Liu. 2007. Web data mining: exploring hyper-
links, contents, and usage data. Springer.
Michael McCandless, Erik Hatcher, and Otis Gospod-
neti. 2010. Lucene in action. Manning Publications
Co, 2nd edition.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999. Association for Computational Lin-
guistics, June.
119
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 65?75,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Exploring Effective Dialogue Act Sequences
in One-on-one Computer Science Tutoring Dialogues
Lin Chen, Barbara Di Eugenio
Computer Science
U. of Illinois at Chicago
lchen43,bdieugen@uic.edu
Davide Fossati
Computer Science
Carnegie Mellon U. in Qatar
davide@fossati.us
Stellan Ohlsson, David Cosejo
Psychology
U. of Illinois at Chicago
stellan,dcosej1@uic.edu
Abstract
We present an empirical study of one-on-
one human tutoring dialogues in the domain
of Computer Science data structures. We
are interested in discovering effective tutor-
ing strategies, that we frame as discovering
which Dialogue Act (DA) sequences corre-
late with learning. We employ multiple lin-
ear regression, to discover the strongest mod-
els that explain why students learn during
one-on-one tutoring. Importantly, we define
?flexible? DA sequence, in which extraneous
DAs can easily be discounted. Our experi-
ments reveal several cognitively plausible DA
sequences which significantly correlate with
learning outcomes.
1 Introduction
One-on-one tutoring has been shown to be a very ef-
fective form of instruction compared to other educa-
tional settings. Much research on discovering why
this is the case has focused on the analysis of the
interaction between tutor and students (Fox, 1993;
Graesser et al, 1995; Lepper et al, 1997; Chi et al,
2001). In the last fifteen years, many such analyses
have been approached from a Natural Language Pro-
cessing (NLP) perspective, with the goal of build-
ing interfaces that allow students to naturally inter-
act with Intelligent Tutoring Systems (ITSs) (Moore
et al, 2004; Cade et al, 2008; Chi et al, 2010).
There have been two main types of approaches to
the analysis of tutoring dialogues. The first kind
of approach compares groups of subjects interact-
ing with different tutors (Graesser et al, 2004; Van-
Lehn et al, 2007), in some instances contrasting the
number of occurrences of relevant features between
the groups (Evens and Michael, 2006; Chi et al,
2010). However, as we already argued in (Ohlsson
et al, 2007), this code-and-count methodology only
focuses on what a certain type of tutor (assumed to
be better according to certain criteria) does differ-
ently from another tutor, rather than on strategies
that may be effective independently from their fre-
quencies of usage by different types of tutor. Indeed
we had followed this same methodology in previous
work (Di Eugenio et al, 2006), but a key turning
point for our work was to discover that our expert
and novice tutors were equally effective (please see
below).
The other kind of approach uses linear regression
analysis to find correlations between dialogue fea-
tures and learning gains (Litman and Forbes-Riley,
2006; Di Eugenio et al, 2009). Whereas linear
regression is broadly used to analyze experimental
data, only few analyses of tutorial data or tutoring
experiments use it. In this paper, we follow
Litman and Forbes-Riley (2006) in correlating se-
quences of Dialogue Acts (DAs) with learning gains.
We extend that work in that our bigram and trigram
DAs are not limited to tutor-student DA bigrams ?
Litman and Forbes-Riley (2006) only considers bi-
grams where one DA comes from the tutor?s turn
and one from the student?s turn, in either order. Im-
portantly, we further relax constraints on how these
sequences are built, in particular, we are able to
model DA sequences that include gaps. This allows
us to discount the noise resulting from intervening
DAs that do not contribute to the effectiveness of
the specific sequence. For example, if we want to
65
explore sequences in which the tutor first provides
some knowledge to solve the problem (DPI) and
then knowledge about the problem (DDI) (DPI and
DDI will be explained later), an exchange such as
the one in Figure 1 should be taken into account
(JAC and later LOW are the tutors, students are indi-
cated with a numeric code, such as 113 in Figure 1).
However, if we just use adjacent utterances, the ok
from the student (113) interrupts the sequence, and
we could not take this example into account. By al-
lowing gaps in our sequences, we test a large number
of linear regression models, some of which result in
significant models that can be used as guidelines to
design an ITS. Specifically, these guidelines will be
used for further improvement of iList, an ITS that
provides feedback on linked list problems and that
we have developed over the last few years. Five
different versions of iList have been evaluated with
220 users (Fossati et al, 2009; Fossati et al, 2010).
iList is available at http://www.digitaltutor.net, and
has been used by more than 550 additional users at
15 different institutions.
JAC: so we would set k equal to e and then delete. [DPI]
113: ok.
JAC: so we?ve inserted this whole list in here.[DDI]
113: yeah.
Figure 1: {DPI, DDI} Sequence Excerpt
The rest of the paper is organized as follows.
In Section 2, we describe the CS-Tutoring corpus,
including data collection, transcription, and anno-
tation. In Section 3, we introduce our methodol-
ogy that combines multiple linear regression with n-
grams of DAs that allow for gaps. We discuss our
experiments and results in Section 4.
2 The CS Tutoring Corpus
2.1 Data Collection
During the time span of 3 semesters, we collected a
corpus of 54 one-on-one tutoring sessions on Com-
puter Science data structures: linked list, stack and
binary search tree. (In the following context, we
will refer them as Lists, Stacks and Trees). Each stu-
dent only participated in one session, and was ran-
domly assigned to one of two tutors: LOW, an expe-
rienced Computer Science professor, with more than
30 years of teaching experience; or JAC, a senior un-
dergraduate student in Computer Science, with only
one semester of previous tutoring experience. In the
end 30 students interacted with LOW and 24 with
JAC.
Students took a pre-test right before the tutoring
session, and an identical post-test immediately after.
The test had two problems on Lists, two problems on
Stacks, and four problems on Trees. Each problem
was graded out of 5 points, for a possible maximum
score of 10 points each for Lists and Stacks, and 20
points for Trees. Pre and post-test scores for each
topic were later normalized to the [0..1] interval, and
learning gains were computed.
Table 1 includes information on session length.
Note that for each topic, the number of sessions is
lower than 54. The tutor was free to tutor on what
he felt was more appropriate, after he was given an
informal assessment of the student?s performance on
the pre-test (tutors were not shown pre-tests to avoid
that they?d tutor to the pre-test only). Hence, not
every student was tutored on every topic.
Topic N Session length (minutes)Min Max Total ? ?
Lists 52 3.4 41.4 750.4 14.4 5.8
Stacks 46 0.3 9.4 264.5 5.8 1.8
Trees 53 9.1 40.0 1017.6 19.2 6.6
Sessions 54 12.8 61.1 2032.5 37.6 6.1
Table 1: CS Tutoring Corpus - Descriptives
Each tutoring session was videotaped. The cam-
era was pointing at the sheets of paper on which tu-
tors and students were writing during the session.
The videos were all transcribed. The transcripts
were produced according to the rules and conven-
tions described in the transcription manual of the
CHILDES project (MacWhinney, 2000). Dialogue
excerpts included in this paper show some of the
transcription conventions. For example, ?+...?
denotes trailing, ?xxx? unintelligible speech and
?#? a short pause (see Figure 2). The CHILDES
transcription manual also provides directions on ut-
terance segmentation.
An additional group of 53 students (control
group) took the pre- and post-tests, but instead of
participating in a tutoring session they attended a
40 minute lecture about an unrelated CS topic. The
rationale for such a control condition was to assess
66
LOW: what?s the if? [Prompt]
LOW: well of course, don?t do this if t two is null so if t
two isn?t null we can do that and xxx properly # thinking
I put it in here. [DPI]
LOW: or else if t two is null that?s telling us that this is
the +. . . [Prompt,FB]
Figure 2: {Prompt,DPI,FB} sequence excerpt
whether by simply taking the pre-test students would
learn about data-structures, and hence, to tease out
whether any learning we would see in the tutored
conditions would be indeed due to tutoring.
The learning gain, expressed as the difference
between post-score and pre-score, of students that
received tutoring was significantly higher than the
learning gain of the students in the control group, for
all the topics. This was showed by ANOVA between
the aggregated group of tutored students and the
control group, and was significant at the p < 0.01
for each topic. There was no significant difference
between the two tutored conditions in terms of learn-
ing gain. The fact that students did not learn more
with the experienced tutor was an important finding
that led us to question the approach of comparing
and contrasting more and less experienced tutors.
Please refer to (Di Eugenio et al, 2009) for further
descriptive measurements of the corpus.
2.2 Dialogue Act Annotation
Many theories have been proposed as concerns DAs,
and there are many plausible inventories of DAs, in-
cluding for tutorial dialogue (Evens and Michael,
2006; Litman and Forbes-Riley, 2006; Boyer et al,
2010). We start from a minimalist point of view,
postulating that, according to current theories of
skill acquisition (Anderson, 1986; Sun et al, 2005;
Ohlsson, 2008), at least the following types of tuto-
rial intervention can be explained in terms of why
and how they might support learning:
1. A tutor can tell the student how to perform the
task.
2. A tutor can state declarative information about
the domain.
3. A tutor can provide feedback:
(a) positive, to confirm that a correct but tentative
step is in fact correct;
(b) negative, to help a student detect and correct an
error.
We first read through the entire corpus and exam-
ined it for impressions and trends, as suggested by
(Chi, 1997). Our informal assessment convinced us
that our minimalist set of tutoring moves was an ap-
propriate starting point. For example, contrary to
much that has been written about an idealized so-
cratic type of tutoring where students build knowl-
edge by themselves (Chi et al, 1994), our tutors
are rather directive in style, namely, they do a lot
of telling and stating. Indeed our tutors talk a lot,
to the tune of producing 93.5% of the total words!
We translated the four types above into the follow-
ing DAs: Direct Procedural Instruction (DPI), Di-
rect Declarative Instruction (DDI), Positive Feed-
back (+FB), and Negative Feedback (-FB). Besides
those 4 categories, we additionally annotated the
corpus for Prompt (PT), since our tutors did explic-
itly invite students to be active in the interaction.
We also annotated for Student Initiative (SI), to cap-
ture active participation on the part of the student?s.
SI occurs when the student proactively produces a
meaningful utterance, by providing unsolicited ex-
planation (see Figures 6 and 4), or by asking ques-
tions. As we had expected, SIs are not as frequent as
other moves (see below). However, this is precisely
the kind of move that a regression analysis would
tease out from others, if it correlates with learning,
even if it occurs relatively infrequently. This indeed
happens in two models, see Table 8.
Direct Procedural Instruction(DPI) occurs when
the tutor directly tells the student what task to per-
form. More specifically:
? Utterances containing correct steps that lead to
the solution of a problem, e.g. see Figure 1.
? Utterances containing high-level steps or sub-
goals (it wants us to put the new node that con-
tains G in it, after the node that contains B).
? Utterances containing tactics and strategies (so
with these kinds of problems, the first thing I
have to say is always draw pictures).
? Utterances where the tutor talked in the first-
person but in reality the tutor instructed the stu-
dent on what to do (So I?m pushing this value
onto a stack. So I?m pushing G back on).
Direct Declarative Instruction (DDI) occurred
when the tutor provided facts about the domain or
67
a specific problem. The key to determine if an ut-
terance is DDI is that the tutor is telling the student
something that he or she ostensibly does not already
know. Common sense knowledge is not DDI ( ten
is less than eleven ). Utterances annotated as DDI
include:
? Providing general knowledge about data struc-
tures (the standard format is right child is al-
ways greater than the parent, left child is al-
ways less than the parent).
? Telling the student information about a specific
problem (this is not a binary search tree).
? Conveying the results of a given action (so now
since we?ve eliminated nine, it?s gone).
? Describing pictures of data structures (and then
there is a link to the next node).
Prompts (PT) occur when the tutor attempts to
elicit a meaningful contribution from the student.
We code for six types of tutor prompts, including:
? Specific prompt: An attempt to get a specific
response from the student (that?s not b so what
do we want to do?).
? Diagnosing: The tutor attempts to determine
the student?s knowledge state (why did you put
a D there?).
? Confirm-OK: The tutor attempts to determine if
the student understood or if the student is pay-
ing attention (okay, got that idea?).
? Fill-in-the-blank: The tutor does not complete
an utterance thereby inviting the student to
complete the utterance, e.g. see Figure 2.
Up to now we have discussed annotations for ut-
terances that do not explicitly address what the stu-
dent has said or done. However, many tutoring
moves concern providing feedback to the student.
Indeed as already known but not often acted upon in
ITS interfaces, tutors do not just point out mistakes,
but also confirm that the student is making correct
steps. While the DAs discussed so far label single
utterances, our positive and negative feedback (+FB
and -FB) annotations comprise a sequence of con-
secutive utterances, that starts where the tutor starts
providing feedback. We opted for a sequence of ut-
terances rather than for labeling one single utterance
because we found it very difficult to pick one single
utterance as the one providing feedback, when the
tutor may include e.g. an explanation that we con-
sider to be part of feedback. Positive feedback oc-
curs when the student says or does something cor-
rect, either spontaneously or after being prompted
by the tutor. The tutor acknowledges the correctness
of the student?s utterance, and possibly elaborates on
it with further explanation. Negative feedback oc-
curs when the student says or does something incor-
rect, either spontaneously or after being prompted
by the tutor. The tutor reacts to the mistake and pos-
sibly provides some form of explanation.
After developing a first version of the coding
manual, we refined it iteratively. During each itera-
tion, two human annotators independently annotated
several dialogues for one DA at a time, compared
outcomes, discussed disagreements, and fine-tuned
the scheme accordingly. This process was repeated
until a sufficiently high inter-coder agreement was
reached. The Kappa values we obtained in the fi-
nal iteration of this process are listed in Table 2
(Di Eugenio and Glass, 2004; Artstein and Poesio,
2008). In Table 2, the ?Double Coded*? column
refers to the sessions that we double coded to cal-
culate the inter-coder agreement. This number does
not include the sessions which were double coded
when coders were developing the coding manual.
The numbers of double-coded sessions differ by DA
since it depends on the frequency on the particular
DA (recall that we coded for one DA at a time).
For example, since Student Initiatives (SI) are not as
frequent, we needed to double code more sessions
to find a number of SI?s high enough to compute a
meaningful Kappa (in our whole corpus, there are
1157 SIs but e.g. 4957 Prompts).
Category Double Coded* Kappa
DPI 10 .7133
Feedback 5 .6747
DDI 10 .8018
SI 14 .8686
Prompt 8 .9490
Table 2: Inter-Coder Agreement in Corpus
The remainder of the corpus was then indepen-
dently annotated by the two annotators. For our
final corpus, for the double coded sessions we did
not come to a consensus label when disagreements
arose; rather, we set up a priority order based on
68
topic and coder (e.g., during development of the
coding scheme, when coders came to consensus
coding, which coder?s interpretation was chosen
more often), and we chose the annotation by a cer-
tain coder based on that order.
As a final important note, given our coding
scheme some utterances have more than one label
(see Figures 2 and 4), whereas others are not la-
belled at all. Specifically, most student utterances,
and some tutor utterances, are not labelled (see Fig-
ures 1 and 4).
3 Method
3.1 Linear Regression Models
In this work, we adopt a multiple regression model,
because it can tell us how much variation in learning
outcomes is explained by the variation of individual
features in the data. The features we use include pre-
test score, the length of the tutoring sessions, and
DAs, both the single DAs we annotated for and DA
n-grams, i.e. DA sequences of length n. Pre-test
score is always included since the effect of previ-
ous knowledge on learning is well established, and
confirmed in our data (see all Models 1 in Table 4);
indeed multiple linear regression allows us to factor
out the effect of previous knowledge on learning, by
quantifying the predictive power of features that are
added beyond pre-test score.
3.2 n-gram Dialogue Act Model
n-grams (sequences of n units, such as words, POS
tags, dialogue acts) have been used to derive lan-
guage models in computational linguistics for a long
time, and have proven effective in tasks like part-of-
speech tagging, spell checking.
Our innovation with regard to using DA n-grams
is to allow gaps in the sequence. This allows us
to extract the sequences that are really effective,
and to eliminate noise. Note that from the point
of view of an effective sequence, noise is anything
that does not contribute to the sequence. For ex-
ample, a tutor?s turn may be interrupted by a stu-
dent?s acknowledgments, such as ?OK? or ?Uh-hah?
(see Figure 1). Whereas these acknowledgments
perform fundamental functions in conversation such
as grounding (Clark, 1992), they may not directly
correlate with learning (a hypothesis to test). If we
counted them in the sequence, they would contribute
two utterances, transforming a 3 DA sequence into a
5 DA sequence. As well known, the higher the n, the
sparser the data becomes, i.e., the fewer sequences
of length n we find, making the task of discover-
ing significant correlations all the harder. Note that
some of the bigrams in (Litman and Forbes-Riley,
2006) could be considered to have gaps, since they
pair one student move (say SI) with each tutor move
contained in the next tutor turn (eg, in our Figure 6
they would derive two bigrams [SI, FB], and [SI,
Prompt]). However, this does not result in a system-
atic exploration of all possible sequences of a certain
length n, with all possible gaps of length up to m, as
we do here.
The tool that allows us to leave gaps in sequences
is part of Apache Lucene,1 an open source full text
search library. It provides strong capabilities to
match and count efficiently. Our counting method
is based on two important features provided by
Lucene, that we already used in other work (Chen
and Di Eugenio, 2010) to detect uncertainty in dif-
ferent types of corpora.
? Synonym matching: We can specify several
different tokens at the same position in a field
of a document, so that each of them can be used
to match the query.
? Precise gaps: With Lucene, we can precisely
specify the gap between the matched query and
the indexed documents (sequences of DAs in
our case) using a special type of query called
SpanNearQuery.
To take advantage of Lucene as described above,
we use the following algorithm to index our corpus.
1. For each Tutor-Topic session, we generate n-
gram utterance sequences ? note that these are
sequences of utterances at this point, not of
DAs.
2. We prune utterance sequences where either 0
or only 1 utterance is annotated with a DA, be-
cause we are mining sequences with at least 2
DAs. Recall that given our annotation, some ut-
terances are not annotated (see e.g. Figure 1).
3. After pruning, for each utterance sequence, we
generate a Lucene document: each DA label on
an utterance will be treated as a token, multiple
1http://lucene.apache.org/
69
labels on the same utterance will be treated as
?synonyms?.
By indexing annotations as just described, we
avoid the problem of generating too many combina-
tions of labels. After indexing, we can use SpanN-
earQuery to query the index. SpanNearQuery allows
us to specify the position distance allowed between
each term in the query.
Figure 3 is the field of the generated Lucene doc-
ument corresponding to the utterance sequences in
Figure 4. We can see that each utterance of the tu-
tor is tagged with 2 DAs. Those 2 DAs produce 2
tokens, which are put into the same position. The
tokens in the same position act as synonyms to each
other during the query.
Figure 3: Lucene Document Example for DAs
258: okay.
JAC: its right child is eight. [DDI, FB]
258: uh no it has to be greater than ten. [SI]
JAC: right so it?s not a binary search tree # it?s not a b s t,
right? [DDI,Prompt]
Figure 4: {FB, SI, DDI} is most effective in Trees
4 Experiments and Results
Here we build on our previous results reported
in (Di Eugenio et al, 2009). There we had shown
that, for lists and stacks, models that include positive
and negative feedback are significant and explain
more of the variance with respect to models that only
include pre-test score, or include pre-test score and
session length. Table 4 still follows the same ap-
proach, but adds to the regression models the addi-
tional DAs, DPI, DDI, Prompt and SI that had not
been included in that earlier work. The column M
refers to three types of models, Model 1 only in-
cludes Pre-test, Model 2 adds session length to Pre-
test, and Model 3 adds to Pre-test all the DAs. As ev-
idenced by the table, only DPI provides a marginally
significant contribution, and only for lists. Note that
length is not included in Model 3?s. We did run all
the equivalent models to Model 3?s including length.
The R2?s stay the same (literally, to the second dec-
imal digit), or minimally decrease. However, in all
these Model 3+?s that include length no DA is sig-
nificant, hence we consider them as less explana-
tory than the Model 3?s in Table 4: finding that a
longer dialogue positively affects learning does not
tell us what happens during that dialogue which is
conducive to learning.
Note that the ? weights on the pre-test are al-
ways negative in every model, namely, students with
higher pre-test scores learn less than students with
lower pre-test scores. This is an example of the well-
known ceiling effect: students with more previous
knowledge have less learning opportunity. Also no-
ticeable is that the R2 for the Trees models are much
higher than for Lists and Stacks, and that for Trees
no DA is significant (although there will be signifi-
cant trigram models that involve DAs for Trees). We
have observed that Lists are in general more diffi-
cult than Stacks and Trees (well, at least than binary
search trees) for students.
Topic Pre-Test ? Gain ?
Lists .40 .27 .14 .25
Stacks .29 .30 .31 .24
Trees .50 .26 .30 .24
Table 3: Learning gains and t-test statistics
Indeed Table 3 shows that in the CS-tutoring cor-
pus the average learning gain is only .14 for Lists,
but .31 for Stacks and .30 for Trees; whereas stu-
dents have the lowest pre-test score on Stacks, and
hence they have more opportunities for learning,
they learn as much for Trees, but not for Lists.
We now examine whether DA sequences help us
explain why student learn. We have run 24 sets of
linear regression experiments, which are grouped as
the following 6 types of models.
? With DA bigrams (DA sequences of length 2):
? Gain ? DA Bigram
? Gain ? DA Bigram + Pre-test Score
? Gain ? DA Bigram + Pre-test Score +
Session Length
? With DA trigrams (DA sequences of length 3):
? Gain ? DA Trigram
? Gain ? DA Trigram + Pre-test Score
? Gain ? DA Trigram + Pre-test Score +
Session Length
For each type of model:
70
Topic M Predictor ? R2 P
Lists
1 Pre-test ?.47 .20 < .001
2
Pre-test ?.43 .29 < .001Length .01 < .001
3
Pre-test ?.500
.377
< .001
+FB .020 < .01
-FB .039 ns
DPI .004 < .1
DDI .001 ns
SI .005 ns
Prompt .001 ns
Stacks
1 Pre-test ?.46 .296 < .001
2 Pre-test ?.46 .280 < .001Length ?.002 ns
3
Pre-test ?.465
.275
< .001
+FB ?.017 < .01
-FB ?.045 ns
DPI .007 ns
DDI .001 ns
SI .008 ns
Prompt ?.006 ns
Trees
1 Pre-test ?.739 .676 < .001
2 Pre-test ?.733 .670 < .001Length .001 ns
3
Pre-test ?.712
.667
< .001
+FB ?.002 ns
-FB ?.018 ns
DPI ?.001 ns
DDI ?.001 ns
SI ?.001 ns
Prompt ?.001 ns
All
1 Pre-test ?.505 .305 < .001
2 Pre-test ?.528 .338 < .001Length .06 < .001
3
Pre-test ?.573
.382
< .001
+FB .009 < .001
-FB ?.024 ns
DPI .001 ns
DDI .001 ns
SI .001 ns
Prompt .001 ns
Table 4: Linear Regression ? Human Tutoring
1. We index the corpus according to the length of
the sequence (2 or 3) using the method we in-
troduced in section 3.2.
2. We generate all the permutations of all the DAs
we annotated for within the specified length;
count the number of occurrences of each per-
mutation using Lucene?s SpanNearQuery al-
lowing for gaps of specified length. Gaps can
span from 0 to 3 utterances; for example, the
excerpt in Figure 1 will be counted as a {DPI,
DDI} bigram with a gap of length 1. Gaps can
be discontinuous.
3. We run linear regressions2 on the six types of
models listed above, generating actual models
by replacing a generic DA bi- or tri-gram with
each possible DA sequence we generated in
step 2.
4. We output those regression results, in which the
whole model and every predictor are at least
marginally significant (p < 0.1).
The number of generated significant models is
shown in Figure 5. In the legend of the Figure,
B stands for Bigram DA sequence, T stands for
Trigram DA sequence, L stands for session Length,
P stands for Pre-test score. Not surprisingly, Fig-
ure 5 shows that, as the allowed gap increases in
length, the number of significant models increases
too, which give us more models to analyze.
0
10
20
30
40
50
60
Gap Allowed
N
um
be
r o
f S
ig
ni
fic
an
t M
od
el
s
0 1 2 3
?
?
?
??
?
?
?
?
?
Predictors
T
T+P
T+P+L
B
B+P
B+P+L
Figure 5: Gaps Allowed vs. Significant Models
Figure 5 shows that there are a high number of
significant models. In what follows we will present
first of all those that improve on the models that
do not use sequences of DAs, as presented in Ta-
ble 4. Improvement here means not only that the
R2 is higher, but that the model is more appropriate
as an approximation of a tutor strategy, and hence,
constitutes a better guideline for an ITS. For exam-
ple, take model 3 for Lists in Table 4. It tells us
that positive feedback (+FB) and direct procedural
instruction (DPI) positively correlate with learning
2We used rJava, http://www.rforge.net/rJava/
71
gains. However, this obviously cannot mean that our
ITS should only produce +FB and DPI. The ITS is
interacting with the student, and it needs to tune its
strategies according to what happens in the interac-
tion; model 3 doesn?t even tell us if +FB and DPI
should be used together or independently. Models
that include sequences of DAs will be more useful
for the design of an ITS, since they point out what
sequences of DAs the ITS may use, even if they still
don?t answer the question, when should the ITS en-
gage in a particular sequence ? we have addressed
related issues in our work on iList (Fossati et al,
2009; Fossati et al, 2010).
4.1 Bigram Models
{DPI, Feedback} Model Indeed the first signifi-
cant models that include a DA bigram include the
{DPI, Feedback} DA sequence. Note that we distin-
guish between models that employ Feedback (FB)
without distinguishing between positive and nega-
tive feedback; and models where the type of feed-
back is taken into account (+FB, -FB). Table 5 shows
that for Lists, a sequence that includes DPI followed
by any type of feedback (Feedback, +FB, -FB) pro-
duces significant models when the model includes
pre-test. Table 5 and all tables that follow include
the column Gap that indicates the length of the gap
within the DA sequence with which that model was
obtained. When, as in Table 5, multiple numbers
appear in the Gap column, this indicates that the
model is significant with all those gap settings. We
only show the ?, R2 and P values for the gap length
which generates the highest R2 for a model, and the
corresponding gap length is in bold font: for exam-
ple, the first model for Lists in Table 5 is obtained
with a gap length = 2. For Lists, these models are not
as predictive as Model 3 in Table 4, however we be-
lieve they are more useful from an ITS design point
of view: they tell us that when the tutor gives direct
instruction on how to solve the problem, within a
short span of dialogue the tutor produces feedback,
since (presumably) the student will have tried to ap-
ply that DPI. For Stacks, a {DPI, -FB} model (with-
out taking pre-test into account) significantly corre-
lates (p < 0.05) with learning gain, and marginally
significantly correlates with learning gain when the
model also includes pre-test score. This latter model
is actually more predictive than Model 3 for Stacks
in Table 4 that includes +FB but not DPI. We can
see the ? weight is negative for the sequence {DPI,
-FB} in the Stacks model. No models including the
bigram {DPI, -FB} are significant for Trees.
Topic Predictor ? R2 P Gap
Lists
DPI, -FB .039 .235 <.001 2, 3Pre-test ?.513 < .001
DPI, +FB .019
.339
<.001
0, 1, 2, 3Pre-test ?.492 < .001
Length .011 < 0.05
DPI, FB .016
.333
<.05
0, 1, 2, 3Pre-test ?.489 < .001
Length .011 < 0.05
Stacks
DPI, -FB ?.290 .136 <.05 0, 1, 2, 3
DPI, -FB ?.187 .342 <.1 0, 1, 2, 3Pre-test ?.401 < .001
Table 5: DPI, Feedback Model
{FB, DDI} Model A natural question arises:
since Feedback following DPI results in significant
models, are there any significant models which in-
clude sequences whose first component is a Feed-
back move? We found only two that are signif-
icant, when Feedback is followed by DDI (Direct
Declarative Instruction). Note that here we are not
distinguishing between negative and positive feed-
back. Those models are shown in Table 6. The
Lists model is not more effective than the original
Model 3 for Lists in Table 4, but the model for Trees
is slightly more explanatory than the best model
for Trees in that same table, and includes a bigram
model, whereas in Table 4, only pre-test is signifi-
cant for Trees.
Topic Predictor ? R2 P Gap
Lists
FB, DDI .1478
.321
<.1
1Pre-test ?.470 < .001
Length .011 < .05
Trees FB, DDI .0709 .6953 <.05 0Pre-test ?.7409 < .001
Table 6: {FB, DDI} Model
4.2 Trigram Models
{DPI, FB, DDI} Model Given our significant bi-
gram models for DPI followed by FB, and FB fol-
lowed by DDI, it is natural to ask whether the com-
bined trigram model {DPI, FB, DDI} results in a
significant model. It does for the topic List, as
shown in table 7, however again the R2 is lower than
72
that of Model 3 in Table 4. This suggests that an ef-
fective tutoring sequence is to provide instruction on
how to solve the problem (DPI), then Feedback on
what the student does, and finally some declarative
instruction (DDI).
Topic Predictor ? R2 P Gap
Lists
DPI, FB, DDI .156
.371
<.01
1Pre-test ?.528 < .001
Length .012 < .05
Table 7: {DPI, FB, DDI} Model
More effective trigram models include Prompt
and SI. Up to now, only one model including se-
quences of DAs was superior to the simpler models
in Table 4. Interestingly, different trigrams that still
include some form of Feedback, DPI or DDI, and
then either Prompt or SI (Student Initiative) result in
models that exhibit slightly higher R2; additionally
in all these models the trigram predictor is highly
significant. These models are listed in table 8 (note
that the two Trees models differ because in one FB is
generic Feedback, irregardless of orientation, in the
other it?s +FB, i.e., positive feedback). In detail, im-
provements in R2 are 0.0382 in topic Lists, 0.12 in
topic Stacks and 0.0563 in topic Trees. The highest
improvement is in Stacks.
Topic Predictor ? R2 P Gap
Lists
PT,DPI,FB .266
.415
<.01
0Pre-test ?.463 < .001
Length .011 < .05
Stacks DDI,FB,PT ?.06 .416 <.01 1Pre-test ?.52 < .001
Trees +FB,SI,DDI .049 .732 <.01 1Pre-test ?.746 < .001
Trees FB,SI,DDI .049 .732 <.01 1Pre-test ?.746 < .001
Table 8: Highest R2 Models
It is interesting to note that the model for Lists add
Prompt at the beginning to a bigram that had already
been found to contribute to a significant model. For
Trees, likewise, we add another DA to the bigram
{FB,DDI} that had been found to be significant; this
time, it is Student Initiative (SI) and it occurs in
the middle. This indicates that, after the tutor pro-
vides feedback, the student takes the initiative, and
the tutor responds with one piece of information the
student didn?t know (DDI). Of course, the role of
Prompts and SI is not surprising, although interest-
ingly they are significant only in association with
certain tutor moves. It is well known that students
learn more when they build knowledge by them-
selves, either by taking the initiative (SI), or after
the tutor prompts them to do so (Chi et al, 1994;
Chi et al, 2001).
LOW: it?s backwards # it?s got four elements, but they
are backwards. [DDI]
234: so we have do it again. [SI]
LOW: so do it again. [FB]
LOW: do what again? [Prompt]
Figure 6: {DDI, FB, PT} is most effective in Stacks
4.3 Other models
We found other significant models, specifically,
{DDI,DPI} for all three topics, and {-FB,SI} for
Lists. However, their R2 are very low, and much
lower than any of the other models presented so
far. Besides models that include only one DA se-
quence and pre-test score to predict learning gain,
we also ran experiments to see if adding multiple
DA sequences to pre-test score will lead to signifi-
cant models ? namely, we experimented with mod-
els which include two sequences as predictors, say,
the two bigrams {-FB,SI} and {FB,DDI}. However,
no significant models were found.
5 Conclusions
In this paper, we explored effective tutoring strate-
gies expressed as sequence of DAs. We first pre-
sented the CS-Tutoring corpus. By relaxing the DA
n-gram definition via the fuzzy matching provided
by Apache Lucene, we managed to discover several
DA sequences that significantly correlate with learn-
ing gain. Further, we discovered models with higher
R2 than models which include only one single DA,
which are also more informative from the point of
view of the design of interfaces to ITSs.
6 Acknowledgments
This work was mainly supported by ONR (N00014-
00-1-0640), and by the UIC Graduate College
(2008/2009 Dean?s Scholar Award). Partial sup-
port is also provided by NSF (ALT-0536968, IIS-
0905593).
73
References
John R. Anderson. 1986. Knowledge compilation: The
general learning mechanism. Machine learning: An
artificial intelligence approach, 2:289?310.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596. Survey Article.
Kristy Elizabeth Boyer, Robert Phillips, Amy Ingram,
Eun Young Ha, Michael Wallis, Mladen Vouk, and
James Lester. 2010. Characterizing the effectiveness
of tutorial dialogue with Hidden Markov Models. In
Intelligent Tutoring Systems, pages 55?64. Springer.
Whitney L. Cade, Jessica L. Copeland, Natalie K. Per-
son, and Sidney K. D?Mello. 2008. Dialogue modes
in expert tutoring. In Intelligent Tutoring Systems,
volume 5091 of Lecture Notes in Computer Science,
pages 470?479. Springer Berlin / Heidelberg.
Lin Chen and Barbara Di Eugenio. 2010. A lucene
and maximum entropy model based hedge detection
system. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning,
pages 114?119, Uppsala, Sweden, July. Association
for Computational Linguistics.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting self-
explanations improves understanding. Cognitive Sci-
ence, 18(3):439?477.
Michelene T. H. Chi, Stephanie A. Siler, Takashi Ya-
mauchi, and Robert G. Hausmann. 2001. Learning
from human tutoring. Cognitive Science, 25:471?533.
Min Chi, Kurt VanLehn, and Diane Litman. 2010. The
more the merrier? Examining three interaction hy-
potheses. In Proceedings of the 32nd Annual Confer-
ence of the Cognitive Science Society (CogSci2010),
Portland,OR.
Michelene T.H. Chi. 1997. Quantifying qualitative anal-
yses of verbal data: A practical guide. Journal of the
Learning Sciences, 6(3):271?315.
Herbert H. Clark. 1992. Arenas of Language Use. The
University of Chicago Press, Chicago, IL.
Barbara Di Eugenio and Michael Glass. 2004. The
Kappa statistic: a second look. Computational Lin-
guistics, 30(1):95?101. Squib.
Barbara Di Eugenio, Trina C. Kershaw, Xin Lu, Andrew
Corrigan-Halpern, and Stellan Ohlsson. 2006. To-
ward a computational model of expert tutoring: a first
report. In FLAIRS06, the 19th International Florida
AI Research Symposium, Melbourne Beach, FL.
Barbara Di Eugenio, Davide Fossati, Stellan Ohlsson,
and David Cosejo. 2009. Towards explaining effec-
tive tutorial dialogues. In Annual Meeting of the Cog-
nitive Science Society, pages 1430?1435, Amsterdam,
July.
Martha W. Evens and Joel A. Michael. 2006. One-on-
one Tutoring by Humans and Machines. Mahwah, NJ:
Lawrence Erlbaum Associates.
Davide Fossati, Barbara Di Eugenio, Christopher Brown,
Stellan Ohlsson, David Cosejo, and Lin Chen. 2009.
Supporting Computer Science curriculum: Exploring
and learning linked lists with iList. IEEE Transac-
tions on Learning Technologies, Special Issue on Real-
World Applications of Intelligent Tutoring Systems,
2(2):107?120, April-June.
Davide Fossati, Barbara Di Eugenio, Stellan Ohlsson,
Christopher Brown, and Lin Chen. 2010. Generat-
ing proactive feedback to help students stay on track.
In ITS 2010, 10th International Conference on Intelli-
gent Tutoring Systems. Poster.
Barbara A. Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the design of instructional systems.
Lawrence Erlbaum Associates, Hillsdale, NJ.
Arthur C. Graesser, Natalie K. Person, and Joseph P.
Magliano. 1995. Collaborative dialogue patterns in
naturalistic one-to-one tutoring. Applied Cognitive
Psychology, 9:495?522.
Arthur C. Graesser, Shulan Lu, George Tanner Jack-
son, Heather Hite Mitchell, Mathew Ventura, Andrew
Olney, and Max M. Louwerse. 2004. AutoTutor:
A tutor with dialogue in natural language. Behav-
ioral Research Methods, Instruments, and Computers,
36:180?193.
Mark R. Lepper, Michael F. Drake, and Teresa
O?Donnell-Johnson. 1997. Scaffolding techniques of
expert human tutors. In K. Hogan and M. Pressley, ed-
itors, Scaffolding student learning: Instructional ap-
proaches and issues. Cambridge, MA: Brookline.
Diane Litman and Kate Forbes-Riley. 2006. Correla-
tions between dialogue acts and learning in spoken
tutoring dialogues. Natural Language Engineering,
12(02):161?176.
Brian MacWhinney. 2000. The Childes Project: Tools
for Analyzing Talk: Transcription format and pro-
grams, volume 1. Psychology Press, 3 edition.
Johanna D. Moore, Kaska Porayska-Pomsta, Sebastian
Varges, and Claus Zinn. 2004. Generating Tutorial
Feedback with Affect. In FLAIRS04, Proceedings of
the Seventeenth International Florida Artificial Intel-
ligence Research Society Conference.
Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, Da-
vide Fossati, Xin Lu, and Trina C. Kershaw. 2007.
Beyond the code-and-count analysis of tutoring dia-
logues. In Proceedings of the 13th International Con-
ference on Artificial Intelligence in Education, pages
349?356, Los Angeles, CA, July. IOS Press.
Stellan Ohlsson. 2008. Computational models of skill
acquisition. The Cambridge handbook of computa-
tional psychology, pages 359?395.
74
Ron Sun, Paul Slusarz, and Chris Terry. 2005. The Inter-
action of the Explicit and the Implicit in Skill Learn-
ing: A Dual-Process Approach. Psychological Re-
view, 112:159?192.
Kurt VanLehn, Arthur C. Graesser, G. Tanner Jackson,
Pamela W. Jordan, Andrew Olney, and Carolyn P.
Rose?. 2007. When are tutorial dialogues more effec-
tive than reading? Cognitive Science, 31(1):3?62.
75
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 307?311,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Improving Pronominal and Deictic Co-Reference Resolution with
Multi-Modal Features
Lin Chen, Anruo Wang, Barbara Di Eugenio
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607, USA
{lchen43,awang28,bdieugen}@uic.edu
Abstract
Within our ongoing effort to develop a com-
putational model to understand multi-modal
human dialogue in the field of elderly care,
this paper focuses on pronominal and deictic
co-reference resolution. After describing our
data collection effort, we discuss our anno-
tation scheme. We developed a co-reference
model that employs both a simple notion of
markable type, and multiple statistical mod-
els. Our results show that knowing the type
of the markable, and the presence of simulta-
neous pointing gestures improve co-reference
resolution for personal and deictic pronouns.
1 Introduction
Our ongoing research project, called RoboHelper,
focuses on developing an interface for older people
to effectively communicate with a robotic assistant
that can help them perform Activities of Daily Liv-
ing (ADLs) (Krapp, 2002), so that they can safely re-
main living in their home (Di Eugenio et al, 2010).
We are devising a multi-modal interface since peo-
ple communicate with one another using a variety of
verbal and non-verbal signals, including haptics, i.e.,
force exchange (as when one person hands a bowl to
another person, and lets go only when s/he senses
that the other is holding it). We have collected a
mid size multi-modal human-human dialogue cor-
pus, that we are currently processing and analyz-
ing. Meanwhile, we have started developing one
core component of our multi-modal interface, a co-
reference resolution system. In this paper, we will
present the component of the system that resolves
pronouns, both personal (I, you, it, they), and deictic
(this, that, these, those, here, there). Hence, this pa-
per presents our first steps toward a full co-reference
resolution module, and ultimately, the multi-modal
interface.
Co-reference resolution is likely the discourse
and dialogue processing task that has received the
most attention. However, as Eisenstein and Davis
(2006) notes, research on co-reference resolution
has mostly been applied to written text; this task
is more difficult in dialogue. First, utterances may
be informal, ungrammatical or disfluent; second,
people spontaneously use hand gestures, body ges-
tures and gaze. Pointing gestures are the eas-
iest gestures to identify, and vision researchers in
our project are working on recognizing pointing and
other hand gestures (Di Eugenio et al, 2010). In this
paper, we replicate the results from (Eisenstein and
Davis, 2006), that pointing gestures help improve
co-reference, in a very different domain. Other work
has shown that gestures can help detect sentence
boundaries (Chen and Harper, 2010) or user inten-
tions (Qu and Chai, 2008).
The rest of the paper is organized as follows. In
Section 2 we describe the data collection and the on-
going annotation. In Section 3 we discuss our co-
reference resolution system, and we present experi-
ments and results in Section 4.
2 The ELDERLY-AT-HOME corpus
Due to the absence of multi-modal collaborative
human-human dialogue corpora that include haptic
data beyond what can be acquired via point-and-
touch interfaces, and in the population of interest,
307
Figure 1: Experiment Excerpts
we undertook a new data collection effort. Our ex-
periments were conducted in a fully functional stu-
dio apartment at Rush University in Chicago ? Fig-
ure 1 shows two screen-shots from our recorded ex-
periments. We equipped the room with 7 web cam-
eras to ensure multiple points of view. Each of the
two participants in the experiments wears a micro-
phone, and a data glove on their dominant hand to
collect haptics data. The ADLs we focused on in-
clude ambulating, getting up from a bed or a chair,
finding pots, opening cans and containers, putting
pots on a stove, setting the table etc. Two students
in gerontological nursing play the role of the helper
(HEL), both in pilot studies and with real subjects.
In 5 pilot dialogues, two faculty members played the
role of the elderly person (ELD). In the 15 real ex-
periments, ELD resides in an assisted living facil-
ity and was transported to the apartment mentioned
above. All elderly subjects are highly functioning at
a cognitive level and do not have any major physical
impairment.
The size of our collected video data is shown
in Table 1. The number of subjects refers to the
number of different ELD?s and does not include the
helpers; we do include our 5 pilot dialogues though,
since those pilot interactions do not measurably dif-
fer from those with the real subjects. Usually one
experiment lasts about 50? (recording starts after in-
formed consent and after the microphones and data
gloves have been put on). Further, we eliminated
irrelevant content such as interruptions, e.g. by the
person who accompanied the elderly subjects, and
further explanations of the tasks. This resulted in
about 15 minutes of what we call effective data for
each subject; the effective data comprises 4782 turns
(see Table 1).
Subjects Raw(Mins) Effective(Mins) Turns
20 482 301 4782
Table 1: ELDERLY-AT-HOME Corpus Size
The effective portion of the data was transcribed
by the first two authors using the Anvil video anno-
tation tool (Kipp, 2001). A subset of the transcribed
data was annotated for co-reference, yielding 114
sub-dialogues corresponding to the tasks subjects
perform, such as finding bowls, filling a pot with wa-
ter, etc. (see Table 2).
An annotation excerpt is shown in Figure 2.
Markable tokens are classified into PLC(Place),
PERS(Person), OBJ(Object) types, and numbered
by type, e.g., PLC#5. Accordingly, we mark pro-
nouns with types as well, RPLC, RPERS, ROBJ, e.g.
RPLC#5. If a subject produced a pointing gesture,
we generate a markable token to mark what is being
pointed to at the end of the utterance (see Utt. 4 and 5
in Figure 2). Within the same task, if two markables
have the same type and the same markable index,
they are taken to co-refer (hence, longer chains of
reference across tasks are cut into shorter spans).
Haptics annotation is at the beginning. We have
identified grab, hold, give and receive as high-level
haptics phonemes that may be useful from the lan-
guage point of view. We have recently started anno-
tating our corpus with those labels.
Subjects Tasks Utterances Gestures Pronouns
12 114 1920 896 1635
Table 2: Annotated Corpus Size
In order to test the reliability of our annotation,
we double coded about 18% of the data, namely 21
sub-dialogues comprising 213 pronouns, on which
we computed the Kappa coefficient (Carletta, 1996).
Similar to (Rodr?guez et al, 2010), we measured the
reliability of markable annotations, and of link to
the antecedent annotations. As concerns the mark-
able level, we obtained ?=0.945, which is high but
no surprisingly for such a simple task. At the link to
the antecedent level, we compared the links from
pronouns to antecedents in a specified context of 4
utterances, obtaining a reasonable ?=0.723.
308
3: PERS#1(HEL/NNP) : RPERS#1(I/PRP) do/VBP n?t/RB see/VB any/DT OBJ#3(pasta/NN) ./.
4: PERS#2(ELD/NNP) : Try/VB over/IN RPLC#5(there/RB) ./. {PLC#5(cabinet/NN)}
5: PERS#1(HEL/NNP) : This/DT RPLC#5(one/NN) ?/. {PLC#5(cabinet/NN)}
6: PERS#2(ELD/NNP) : Oh/UH ,/, yes/RB ./.
Figure 2: Annotation Excerpt
3 Our approach
Utterances and Gestures
Find Markables Generate Candidates
Coreference Pairs
Preprocessing
Markable Model Coreference Model
Figure 3: Co-reference System Architecture
The architecture of our co-reference resolution
system is shown in Figure 3.
We first pre-process a dialogue by splitting turns
into sentences, tokenizing sentences into tokens,
POS tagging tokens. The Markable model is used
to classify whether a token can be referred to and
what type of markable it is. The Markable model?s
feature set includes the POS tag of the token, the
word, the surrounding tokens? POS tags in a win-
dow size of 3. The model outputs markable classes:
Place/Object/Person, or None, which means the to-
ken is not markable. A pointed-to entity serves as a
markable by default.
To perform resolution, each pronoun to be re-
solved ( I, you, it, they; this, that, these, those, here,
there) is paired with markables in the context of the
previous 2 utterances, the current utterance and the
utterance that follows, by using {pronoun, markable
type} compatibility rules. For example, let?s con-
sider the excerpt in Figure 2. To resolve one in
utterance 5, the system will generate 3 candidate
token pairs: <one(5,2), pasta(3,6)>, <one(5,2),
cabinet(4,-1)>, <one(5,2), cabinet(5,-1)> (includ-
ing the pointed-to markable is a way of roughly ap-
proximating information that will be returned by the
vision component). The elements in those pairs
are tokens with their coordinates in the format (Sen-
tenceIndex, TokenIndex); markables pointed to are
given negative token indices.
The Co-reference model will filter out the pairs
<pronoun, markable> that it judges to be incor-
rect. For the Co-reference model, we adopted a
subset of features which are commonly used in co-
reference resolution in written text. These features
apply to each <pronoun, markable> pair and in-
clude: Lexical features, i.e. words and POS tags for
both anaphora and antecedent; Syntactic features,
i.e. syntactic constraints such as number and per-
son agreement; Distance features, i.e. sentence dis-
tance, token distance and markable distance. Addi-
tionally, the Co-reference model uses pointing ges-
ture information. If the antecedent in the <pronoun,
markable> was pointed to, the pair is tagged as Is-
Pointed. In our data, people often use pronouns
and hand gestures instead of nouns when introduc-
ing new entities. It is not possible to map these
pronouns to a textual antecedent since none exists.
This confirms the findings from (Kehler, 2000): in
a multi-modal corpus, he found that no pronoun is
used without a gesture when it refers to a referent
which is not in focus.
4 Experiments and Discussion
The classification models described above were im-
plemented using the Weka package (Hall et al,
2009). Specifically, for each model, we experi-
mented with J48 (a decision tree implementation)
and LibSVM (a Support Vector Machine implemen-
tation). All the results reported below are calculated
using 10 fold cross-validation.
We evaluated the performances of individual
models separately (Tables 3 and 4), and of the sys-
tem as a whole (Table 5).
Algorithm Precision Recall F-Measure
J48 0.984 0.984 0.984
LibSVM 0.979 0.936 0.954
Baseline 0.971 0.971 0.971
Table 3: Markable Model Performance
The results in Table 3 are not surprising, since de-
tecting the type of markables is a simple task. In-
deed the results of the baseline model are extremely
309
Method J48 LibSVMPrecision Recall F-Measure Precision Recall F-Measure
Text + Gesture 0.700 0.684 0.686 0.672 0.669 0.670
Text Only 0.655 0.656 0.656 0.624 0.624 0.624
Table 4: Co-reference Model Performance
Words Method Features Precision Recall F-Measure
All Pronouns
J48 Text Only 0.544 0.332 0.412Text + Gesture 0.482 0.783 0.596
LibSVM Text Only 0.56 0.27 0.364Text + Gesture 0.522 0.6 0.559
Baseline Text Only 0.367 0.254 0.300Text + Gesture 0.376 0.392 0.384
3rd Person + Deictic
J48 Text Only 0.264 0.028 0.05Text + Gesture 0.438 0.902 0.589
LibSVM Text Only 0.6 0.009 0.017Text + Gesture 0.525 0.695 0.598
Baseline Text Only 0.172 0.114 0.137Text + Gesture 0.301 0.431 0.354
Table 5: Co-reference System Performance (Markable + Co-reference Models)
high as well. We compute the baseline by assigning
to the potential markable (i.e., each word) its most
frequent class in the training set (recall that the four
classes include None as well).
For the Co-reference model, we conducted 2 sets
of experiments to ascertain the effect of including
Gesture in the model. As shown in Table 4, both J48
and LibSVM obtain better results when we include
gestures in the model. ?2 shows that differences in
precision and recall 1 are significant at the p ? 0.01
level, though the absolute improvement is not high.
As concerns the evaluation of the whole system,
we ran a 4-way experiment, where we examine the
performance of the system on all pronouns, and on
those pronouns left after eliminating first and second
person pronouns, without and with Gesture informa-
tion. We also ran two sets of baseline experiments.
In the baseline experiments, we link each pronoun
we want to resolve, to the most recent utterance-
markable token and to a pointed-to markable token
(if applicable). Markables are filtered by the same
compatibility rules mentioned above.
Regarding the metrics we used for evaluation, we
used the same method as Strube and Mu?ller (2003),
which is also similar to MUC standard (Hirschman,
1?2 does not apply to the F-Measure.
1997). As the golden set, we used the human an-
notated links from the pronouns to markables in the
same context of four utterances used by the system.
Then, we compared the co-reference links found by
the system against the golden set, and we finally cal-
culated precision, recall and F-Measure.
Table 5 shows that the F-measure is higher when
including gestures, no matter the type of pronouns.
When we include gestures, there is no difference be-
tween ?All Pronouns? and ?3rd Person + Deictic?.
In the ?3rd Person + Deictic? experiments, we ob-
served huge drops in recall, from 0.902 to 0.028 for
J48, and from 0.695 to 0.009 for LibSVM algorithm.
This confirms the point we made earlier, that 3rd
person pronouns/deictic words (Kehler, 2000) often
do not have textual antecedents, since when accom-
panied by simultaneous pointing they introduce new
entities in a dialogue.
Comparison to previous work is feasible only at a
high level, because of the usage of different corpora
and/or measurement metrics. This said, our model
with gestures outperforms Strube andMu?ller (2003),
who did not use gesture information to resolve pro-
nouns in spoken dialogue. Strube and Mu?ller (2003)
used the 20 Switchboard dialogues as their experi-
ment dataset, and used the MUC metrics. Our re-
310
sults are similar to Eisenstein and Davis (2006), but
there are two main differences. First, the corpus
they used is smaller than what we used in this pa-
per. Their corpus was collected by themselves and
consisted of 16 videos, each video was 2-3 minutes
in length. Second, they used a difference measure-
ment metrics called CEAF (Luo, 2005).
5 Conclusions
In this paper, we presented the new ELDERLY-AT-
HOME multi-modal corpus we collected. A co-
reference resolution system for personal and deic-
tic pronouns has been developed on the basis of the
annotated corpus. Our results confirm that gestures
improve co-reference resolution; a simple notion of
type also helps. The Markable and Co-reference
modules we presented are a first start in developing
a full multi-modal co-reference resolution module.
Apart from completing the annotation of our cor-
pus, we will develop an annotation scheme for hap-
tics, and investigate how haptics information affects
co-reference and other dialogue phenomena. Ulti-
mately, both pointing gestures and haptic informa-
tion will automatically be recognized by the collab-
orators in the project we are members of.
Acknowledgments
This work is supported by award IIS 0905593 from
the National Science Foundation. Thanks to the
other members of the RoboHelper project, for their
many contributions, especially to the data collection
effort.
References
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22:249?254.
Lei Chen and Mary P. Harper. 2010. Utilizing gestures
to improve sentence boundary detection. Multimedia
Tools and Applications, pages 1?33.
Barbara Di Eugenio, Milos? Z?efran, Jezekiel Ben-
Arie, Mark Foreman, Lin Chen, Simone Franzini,
Shankaranand Jagadeesan, Maria Javaid, and Kai
Ma. 2010. Towards Effective Communication with
Robotic Assistants for the Elderly: Integrating Speech,
Vision and Haptics. InDialog with Robots, AAAI 2010
Fall Symposium, Arlington, VA, USA, November.
Jacob Eisenstein and Randall Davis. 2006. Gesture
Improves Coreference Resolution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages 37?
40.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Lynette Hirschman. 1997. Muc-7 coreference task defi-
nition.
Andrew Kehler. 2000. Cognitive Status and Form of
Reference in Multimodal Human-Computer Interac-
tion. In AAAI 00, The 15th Annual Conference of the
American Association for Artificial Intelligence, pages
685?689.
Michael Kipp. 2001. Anvil-a generic annotation tool
for multimodal dialogue. In Proceedings of the 7th
European Conference on Speech Communication and
Technology, pages 1367?1370.
Kristine M. Krapp. 2002. The Gale Encyclopedia of
Nursing & Allied Health. Gale Group, Inc. Chapter
Activities of Daily Living Evaluation.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 25?
32, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Shaolin Qu and Joyce Y. Chai. 2008. Beyond attention:
the role of deictic gesture in intention recognition in
multimodal conversational interfaces. In Proceedings
of the 13th international conference on Intelligent user
interfaces, pages 237?246.
Kepa Joseba Rodr?guez, Francesca Delogu, Yannick Ver-
sley, Egon Stemle, and Massimo Poesio. 2010.
Anaphoric annotation of wikipedia and blogs in the
live memories corpus. In Proceedings of the 7th In-
ternational Conference on Language Ressources and
Evaluation (LREC 2010), pages 157?163.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics-Volume 1.
311
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 290?294,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Improving Sentence Completion in Dialogues with Multi-Modal Features
Anruo Wang, Barbara Di Eugenio, Lin Chen
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607, USA
awang28, bdieugen, lchen43@uic.edu
Abstract
With the aim of investigating how humans un-
derstand each other through language and ges-
tures, this paper focuses on how people un-
derstand incomplete sentences. We trained a
system based on interrupted but resumed sen-
tences, in order to find plausible completions
for incomplete sentences. Our promising re-
sults are based on multi-modal features.
1 Introduction
Our project, called RoboHelper, focuses on devel-
oping an interface for elderly people to effectively
communicate with robotic assistants that can help
them perform Activities of Daily Living (ADLs)
(Krapp, 2002), so that they can safely remain living
in their home (Di Eugenio et al, 2010; Chen et al,
2011). We are developing a multi-modal interface
since people communicate with each other using a
variety of verbal and non-verbal signals, including
haptics, i.e., force exchange (as when one person
hands a bowl to another person, and lets go only
when s/he senses that the other is holding it). We
collected a medium size multi-modal human-human
dialogue corpus, then processed and analyzed it. We
observed that a fair number of sentences are incom-
plete, namely, the speaker does not finish the utter-
ance. Because of that, we developed a core compo-
nent of our multi-modal interface, a sentence com-
pletion system, trained on the set of interrupted but
eventually completed sentences from our corpus. In
this paper, we will present the component of the sys-
tem that predicts reasonable completion structures
for an incomplete sentence.
Sentence completion has been addressed within
information retrieval, to satisfy user?s information
needs (Grabski and Scheffer, 2004). Completing
sentences in human-human dialogue is more diffi-
cult than in written text. First, utterances may be in-
formal, ungrammatical or dis-fluent; second, people
interrupt each other during conversations (DeVault
et al, 2010; Yang et al, 2011). Additionally, the
interaction is complex, as people spontaneously use
hand gestures, body language and gaze besides spo-
ken language. As noticed by (Bolden, 2003), during
face-to-face interaction, the completion problem is
not only an exclusively verbal phenomenon but ?an
action embedded within a complex web of differ-
ent meaning-making fields?. Accordingly, among
our features, we will include pointing gestures, and
haptic-ostensive (H-O) actions, e.g., referring to an
object by manipulating it in the real world (Landra-
gin et al, 2002; Foster et al, 2008).
The paper is organized as follows. In Section 2 we
describe our data collection and multi-modal anno-
tation. In Section 3 we discuss how we generate our
training data, and in Section 4 the model we train
for sentence completion, and the results we obtain.
2 Dataset
In contrast with other sentence completion systems
that focus on text input, the dataset we use in this
paper is a subset of the ELDERLY-AT-HOME cor-
pus, a multi-modal corpus in the domain of elderly
care, which includes collaborative human-human di-
alogues, pointing gestures and haptic-ostensive (H-
O) actions. Our experiments were conducted in
a fully functional apartment and included a helper
290
(HEL) and an elderly person (ELD). HEL helps
ELD to complete several realistic tasks, such as
putting on shoes, finding a pot, cooking pasta and
setting the table for dinner. We used 7 web cameras
to videotape the whole experiment, one microphone
each to record the audio and one data glove each to
collect haptics data. We ran 20 realistic experiments
in total, and then imported the videos and audios (in
avi format), haptics data (in csv format) and tran-
scribed utterances (in xml format) into Anvil (Kipp,
2001) to build the multi-modal corpus.
Among other annotations (for example Dialogue
Acts) we have annotated these dialogues for Point-
ing gestures and H-O actions. Due to the setting
of our experiments, the targets of pointing gestures
and H-O actions are real life objects, thus we de-
signed a reference index system to annotate them.
We give pre-defined indices to targets which can-
not be moved, such as cabinets, draws, and fridge.
We also assign runtime indices to targets which can
be moved, like pots, glasses, and plates. For exam-
ple, ?Glass1? refers to the first glass that appears in
one experiment. In our annotation, a ?Pointing? ges-
ture is defined as a hand gesture without any phys-
ical contact between human and objects. Hand
gestures with physical contact to objects are anno-
tated as H-O actions. H-O actions are further subdi-
vided into 7 subtypes, including ?Holding?, ?Touch-
ing?,?Open? and ?Close?. In order to verify the reli-
ability of our annotations, we double coded 15% of
the pointing gestures and H-O actions. Kappa val-
ues of 0.751 for pointing gestures, and of 0.703 for
H-O actions, are considered acceptable, especially
considering the complexity of these real life tasks
(Chen and Di Eugenio, 2012).
In this paper, we focus on specific sub-dialogues
in the corpus, which we call interruptions. An inter-
ruption can occur at any point in human-human dia-
logues: it happens when presumably the interrupter
(ITR) thinks s/he has already understood what the
speaker (SPK) means before listening to the entire
sentence. By observing the data from our corpus,
we conclude that there are generally three cases of
interruptions. First, the speaker (SPK) stops speak-
ing and does not complete the sentence ? these are
the incomplete sentences whose completion a robot
would need to infer. In the second type of inter-
ruption, after being interrupted SPK continues with
(a) few words, and then stops without finishing the
whole sentence: hence, there is a short time over-
lap between two sentences (7 cases). The third case
occurs when the SPK ignores the ITR and finishes
the entire sentence. In this case, the SPK and the
ITR speak simultaneously (198 cases). The number
of interruptions ranges from 1 to 37 in each experi-
ment. An excerpt from an interruption with a subse-
quent completion (an example of case 3) is shown
below. The interruption occurs at the start of the
overlap between the two speakers, marked by < and
>. This example also includes annotations for point-
ing gestures and for H-O actions.
Elder: I need some glasses from < that cabinet >.
[Point (Elder, Cabinet1)]
Helper: < From this > cabinet?
[Point (Helper, Cabinet2)]
Helper: Is this the glass you < ?re looking for? >
[Touching (Helper, Glass1)]
Elder: < No, that one.>
[Point (Elder, Cabinet1, Glass2)]
As concerns annotation for interruptions, it proceeds
from identifying interrupted sentences to finding
<interrupted sentences, candidate structure> pairs
which will be used for generating grammatical com-
pletion for an incomplete sentence. Each in-
terrupted sentence is marked with two categories:
incomplete form, from the start of the sentence
to where it is interrupted, such as ?I need some
glasses?; complete form, from the start of a sentence
to where the speaker stops, ?I need some glasses
from that cabinet.?
Table 2 shows distribution statistics for our
ELDERLY-AT-HOME corpus. It contains a total of
4839 sentences, which in turn contain 7219 clauses.
320 sentences are incomplete in the sense of case 1
(after interruption SPK never completes his/her sen-
tence); whereas 205 sentences are completed after
interruption (cases 2 and 3).
Sentences 4,839
Clauses 7,219
Pointing Gestures 362
H-O Actions 629
Incomplete sentences 320
Interrupted sentences 205
Table 1: Corpus Distributions
291
3 Candidate Pairs Generation
The question is now, how to generate plausible train-
ing instances to predict completions for incomplete
sentences. We use the 205 sentences that have
been interrupted but for which we have comple-
tions; however, we cannot only use those pairs for
training, since we would run the risk of overfit-
ting, and not being able to infer appropriate com-
pletions for other sentences. To generate addi-
tional<Interrupted sentences, candidate structure>
pairs, we need to match an interrupted sentence IntS
with its potential completions ? basically, to check
whether IntS can match the prefix of other sentences
in the corpus. We do so by comparing the POS se-
quence and parse tree of IntS with the POS sequence
and parse tree of the prefix of another sentence. Both
IntS and other sentences in the corpus are parsed via
the Stanford Parser (Klein and Manning, 2003).
Before discussing the details though, we need
to deal with one potential problem: the POS se-
quence for the incomplete portion of IntS may not
be correctly assigned. For example, when the sen-
tence ?The/DT, top/JJ, cabinet/NN.? is interrupted as
?The/DT, top/NN?, the POS tag of NN is assigned
to ?top?; this is incorrect, and engenders noise for
finding correct completions.
We first pre-process a dialogue by splitting turns
into sentences, tokenizing sentences into tokens, and
POS tagging tokens. Although for the interrupted
sentences, we could obtain a correct POS tag se-
quence by parsing the incomplete and resumed por-
tions together, this would not work for a truly incom-
plete sentence (whose completion is our goal). Thus,
to treat both interrupted sentences and incomplete
sentences in the same way, we train a POS tag Cor-
rection Model to correct fallaciously assigned POS
tags. The POS tag Correction Model?s feature set
includes the POS tag of the token, the word, and the
previous tokens? POS tags in a window size of 3.
The model outputs the corrected POS tags.
The POS tag Correction model described above
was implemented using the Weka package (Hall et
al., 2009). Specifically, we experimented with J48
(a decision tree implementation), Naive Bayes (NB),
and LibSVM (a Support Vector Machine implemen-
tation). All the results reported below are calculated
using 10 fold cross-validation.
J48 NB LibSVM
Accuracy 0.829 0.680 0.532
Table 2: POS tag Correction Model Performance
The results in Table 2 are not surprising, since de-
tecting the POS tag of a known word is a simple
task. Additionally, it is not surprising that J48 is
more accurate than NB, since NB is known to of-
ten behave as a baseline method. What is surprising
though is the poor performance of SVMs, which are
generally among the top performers for a broad va-
riety of tasks. We are investigating why this may be
the case. At any rate, by applying the J48 model, we
obtain more accurate POS tag assignments for inter-
rupted sentences (and in our future application, for
the incomplete sentence we need to complete).
Once we have corrected the POS assignments for
each interrupted sentence IntS, we retrieve poten-
tial grammatical structures for IntS, by comparing
IntS with the prefixes of all complete sentences in
the corpus via POS tags and parse trees. Note that
due to the complexity of building a parse tree cor-
rection model in our corpus, we only build a model
to correct the POS tags, but ignore the possible in-
correct parse trees of the incomplete portion of an
interrupted sentence. The matching starts from the
last word in IntS back to the first word, with weights
assigned to each position in decreasing order. Due to
the size of our corpus, it is not possible to find ex-
actly matched POS tag sequences for every incom-
plete sentence; thus, we also consider the parsed tree
structures and mismatched POS tags between IntS?s
and complete sentences by reducing weights accord-
ing to the size of the matched phrases and distances
of mismatched POS tags. After this, a matching
score is calculated for each incomplete and candi-
date structure pair.
Due to the large number of candidate structures,
only the top 150 candidate structures for each IntS
are selected and manually annotated with three
classifications: ?R?, when the candidate structure
provides a grammatically ?reasonable? structure,
which can be used as a template for completion;
?U?, which means the candidate structure gives
an ?ungrammatical? structure, thus this candidate
structure cannot be used as template for completion;
292
?T?, the candidate structure is exactly the same as
what the speaker was originally saying, as judged
based on the video and audio records. An example
of an incomplete sentence with candidate structures
in each of the three categories is shown below.
It/PRP, feels/VBZ | It/PRP, feels/VBZ, good/JJR
[R] It/PRP, ?s/VBZ, fine/JJ, like/IN, this/DT]
[U] We/PRP, did/VBD, n?t/RB
[T] It/PRP, is/VBZ, better/JJR
10543 interrupted sentences and candidate pairs
are generated. 5268 of those 10543 pairs
(49.97%) were annotated as ?Reasonable?, 4727
pairs (44.85%) were annotated as ?Unreasonable?,
and 545 pairs (5.17%) were annotated as ?Same with
original sentence?.
Incomplete Sentence and Structure pairs 10,543
Reasonable structures (R) 5,268
Unreasonable structures (U) 4,729
Exactly same structures (T) 545
Table 3: Distribution of completion classifications
4 Results and Discussion
On the basis of the annotation, we trained a ?Rea-
sonable Structure Selection (RSS)? model via su-
pervised learning methods. For each pair <IntS,
Candidate>, the feature set includes word and POS
tag of the tokens of IntS and its candidate structure
sentence. Co-occurring pointing gestures and H-O
actions for both IntS and Candidate are also included
in the model. Co-occurrence is defined as tempo-
ral overlap between the gesture (pointing or H-O ac-
tion) and the duration of the utterance. For each
training instance, we include the following features:
IntS: <words, POS tags>, <Pointing (Person / Ob-
ject / Location)>, <H-O action (Person / Object /
Location / Type)>;
Candidate: <words/POS tags)>, <Pointing (Per-
son / Object / Location)>, <H-O action (Person /
Object / Location / Type)>;
<Matching Score>;
<Classification: R, U, or T>.
We trained the RSS model also using the Weka
package. The same methods mentioned earlier
(J48, NB and SVM) are used, with 10-fold cross-
validations. Results are shown in Table 4. We
J48 NB LibSVM
Precision R, U, T 0.822 0.724 0.567
R, U 0.843 0.761 0.600
Recall R, U, T 0.820 0.725 0.512
R, U 0.842 0.762 0.563
F-Measure R, U, T 0.818 0.711 0.390
R, U 0.841 0.761 0.440
Table 4: Reasonable Structure Selection models
ran two different sets of experiments using two ver-
sions of training instances: Classification with three
classes, R, U and T, and classification with two
classes, R and U. When training with only two
classes, the T instances are marked as R. We exper-
imented with collapsing R and T candidates since T
candidates may lead to overfitting, and some R can-
didates might even provide better structures for an
incomplete sentence than what exactly one speaker
had originally said. Not surprisingly, results im-
prove for two-way classification. Based on the J48
model, we observed that the POS tag features play
a significant part in classification, whereas the word
features are redundant. Further, pointing gestures
and H-O actions do appear in some subtrees of the
larger decision tree, but not on every branch. We
speculate that this is due to the fact that pointing ges-
tures or H-O actions do not accompany every utter-
ance.
5 Conclusions and Future Work
In this paper, we introduced our multi-modal sen-
tence completion schema which includes pointing
gestures and H-O actions in the corpus ELDERLY-
AT-HOME. Our data shows that it is possible to pre-
dict what people will say, even if the utterance is
not complete. Our promising results include multi-
modal features, which as we have shown elsewhere
(Chen and Di Eugenio, 2012) improve traditional
co-reference resolution models. In the near future,
we will implement the last module of our sentence
completion system, the one that fills the chosen can-
didate structure with actual words.
293
References
G.B. Bolden. 2003. Multiple modalities in collaborative
turn sequences. Gesture, 3(2):187?212.
L. Chen and B. Di Eugenio. 2012. Co-reference via
pointing and haptics in multi-modal dialogues. In The
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Compu-
tational Linguistics. short paper, to appear.
L. Chen, A. Wang, and B. Di Eugenio. 2011. Im-
proving pronominal and deictic co-reference resolu-
tion with multi-modal features. In Proceedings of the
SIGDIAL 2011 Conference, pages 307?311. Associa-
tion for Computational Linguistics.
David DeVault, Kenji Sagae, and David Traum. 2010.
Incremental interpretation and prediction of utterance
meaning for interactive dialogue. Dialogue and Dis-
course, 2(1):143170.
B. Di Eugenio, M. Zefran, J. Ben-Arie, M. Foreman,
L. Chen, S. Franzini, S. Jagadeesan, M. Javaid, and
K. Ma. 2010. Towards effective communication with
robotic assistants for the elderly: Integrating speech,
vision and haptics. In 2010 AAAI Fall Symposium Se-
ries.
M.E. Foster, E.G. Bard, M. Guhe, R.L. Hill, J. Ober-
lander, and A. Knoll. 2008. The roles of haptic-
ostensive referring expressions in cooperative, task-
based human-robot dialogue. In Proceedings of the
3rd ACM/IEEE international conference on Human
robot interaction, pages 295?302. ACM.
K. Grabski and T. Scheffer. 2004. Sentence completion.
In Proceedings of the 27th annual international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 433?439. ACM.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, and Ian H. Wit-
ten. 2009. The WEKA data mining soft-
ware: An update. SIGKDD Explorations, 11(1).
http://www.cs.waikato.ac.nz/ml/weka/.
M. Kipp. 2001. Anvil-a generic annotation tool for mul-
timodal dialogue. In Seventh European Conference on
Speech Communication and Technology.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
K.M. Krapp. 2002. The Gale Encyclopedia of Nursing
& Allied Health: DH, volume 2. Gale Cengage.
F. Landragin, N. Bellalem, L. Romary, et al 2002. Re-
ferring to objects with spoken and haptic modalities.
F. Yang, P.A. Heeman, and A.L. Kun. 2011. An
investigation of interruptions and resumptions in
multi-tasking dialogues. Computational Linguistics,
37(1):75?104.
294
Proceedings of the 14th European Workshop on Natural Language Generation, pages 210?211,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
UIC-CSC: The Content Selection Challenge entry
from the University of Illinois at Chicago
Hareen Venigalla
Computer Science
University of Illinois at Chicago
Chicago, IL, USA
hareen058@gmail.com
Barbara Di Eugenio
Computer Science
University of Illinois at Chicago
Chicago, IL, USA
bdieugen@uic.edu
Abstract
This paper described UIC-CSC, the en-
try we submitted for the Content Selection
Challenge 2013. Our model consists of
heuristic rules based on co-occurrences of
predicates in the training data.
1 Introduction
The core of the Content Selection Challenge task
is formulated as Build a system which, given a set
of RDF triples containing facts about a celebrity
and a target text (for instance, a Wikipedia - style
article about that person), selects those triples that
are reflected in the target text. The organizers pro-
vided training data consisting of 62618 pairs of
texts and triple sets. The text is the introductory
text tfC of the Wikipedia article corresponding to
celebrity C; the set of triples trC concerning C was
grepped from the Freebase official weekly RDF
dump. It is important to note that we do not know
which specific triples from trC are rendered in tfC .
A sample triple in the file is as follows:
ns:m.04wqr
ns:award.award winner.awards won
ns:m.07ynmx5
In the above triple, ns:m.04wqr is
the subject id, of Marilyn Monroe in
this case (ns denotes namespace);
ns:award.award winner.awards won is
the predicate and ns:m.07ynmx5 is the object
id of the award. Since this format is not readable,
the organizers provided a script to transform the
turtle file into a human readable form, where the
object id is replaced by its actual value:
/award/award winner/awards won ??Golden
Globe Awards for Actress - Musical or
Comedy Film - 17th Golden Globe Awards
- Some Like It Hot - 1960 - earliye -
Award Honor?? /m/07ynmx5
In the following, we will refer to the first element
of these expressions as the predicate. Our ap-
proach relies on heuristics derived from clustering
predicates directly, or clustering them based on
the co-occurrence of the argument of predicate pi
in a text tf and in turtle files tr that contain both
pi and another predicate pj .
2 Deriving heuristic rules
We observed that in total there are 613 distinct
predicates. Out of these 613 predicate, only 11
are present in over 40 percent of the files and only
19 predicates are present in over 10 percent of the
files. This means that a large number of predi-
cates are present only in a few files. This makes it
harder to decide whether we have to include these
predicates or not. Conversely, nearly 40 percent of
text files only contain one or two sentences, which
compounds the sparsity problem.
Predicate Clustering. In the first method, we
generate predicate clusters by simply removing
the leaf from each predicate expression. For exam-
ple, /people/person/place of birth,
and /people/person/education belong to
the same cluster, labelled /people/person as
they have the same parent /people/person.
We found 35 such clusters. We then ana-
lyzed the frequency of each predicate pi on
its own, and conditional on other predicates in
the same cluster: for example, how frequent
/people/person/education is, and
how often it occurs in those triple files, where
/people/person/place of birth is also
present.
Intersection on Arguments. For each predicate
pi, we compute the set of its intersection sets ISi,j .
Each set isi,j comprises all the turtle files tri,j
where pi co-occurs with a second predicate pj . For
each tri,j , we retrieve the corresponding text file
tf (recall that each turtle file is associated with
one text file) and check whether the argument of
210
pi occurs in tf ? this is indirect evidence that the
text does include the information provided by pi
(of course this inference may be wrong, if this ar-
gument occurs in a context different from what pi
conveys). If the argument of pi does occur in tf ,
we keep tri,j , otherwise we discard it. As above,
we then proceed to compute the frequencies of the
occurrences of pi on its own, and of pi when pj
is also present, over all the turtle files tri,j ? isi,j
that have not been filtered out as just discussed.
Given these two methods, we derive rules such
as the following:
IF /baseball/baseball player/position ? trk
AND
/baseball/baseball player/batting stats
? trk
THEN
select
/baseball/baseball player/position
The set of rules is then filtered as follows. On
a small development set, we manually annotated
which triples are included in the corresponding
text files. We keep a rule if the F-measure concern-
ing predicate pi (i.e., concerning the triples whose
predicate is pi) improves when using the rule, as
opposed to including pi if it belongs to a set of
frequent predicates.
We also need to deal with multiple occurrences
of pi in one single turtle file. Predicates such as
/music/artist/track can have multiple in-
stances, up to 30, in a certain trk, with different
arguments; however, those predicates may occur
far fewer times in the corresponding text files ? be-
cause say trMM on Marilyn Monroe includes one
triple for each of her movies, but the correspond-
ing tfMM only mentions a few of those movies.
Hence, we impose an upper limit of 5 on the num-
ber of occurrences in the same turtle file, for a cer-
tain predicate to be included, for example:
IF /music/artist/track
AND its count ? 5
THEN select /music/artist/track
3 Evaluation
Apart from our participation in the Challenge, we
evaluated our system on a small test set composed
of 96 pairs of text and turtle files, randomly se-
lected from the data released by the organizers.
This resulted in a total of 153 unique predicates
(hence, about 14 of the total number of distinct
predicates). We manually annotated the predicates
in the turtle files as present/absent in the corre-
sponding text file.
We consider four domains:
1. Basic facts: general, very frequent informa-
tion, such as people/person/profession,
people/person/nationality.
2. Books: predicates whose root is book,
like book/author/works written,
book/book subject/works.
3. Sports: predicates whose root is a sport, like
baseball/baseball player/position s,
ice hockey/hockey player/former team.
4. Film and Music: predicates
whose root is film or music,
like /film/director/film,
/music/artist/track.
5. Television: predicates whose root is tv, like
/tv/tv director/episodes directed.
As apparent from Table 1, the performance of
our system varies considerably according to the
domain of the predicates. Specifically, we be-
lieve that the exceedingly low precision for pred-
icates of type book, film & music, tv is
due to the sparseness of the data. As we noted
above, 40% of the text files only include one or
two sentences. Hence, our system selects many
more predicates than are actually present in the
corresponding text file.
Table 1: Performance on in-house test set
Domain P R F-score
Basic Facts 79.83 51.25 62.40
Sports 79.84 49.22 60.90
Books 12.80 66.30 21.47
Film & Music 5.77 55.19 10.45
TV 5.46 43.36 9.70
4 Future Enhancements
UIC-CSC could be improved by more closely an-
alyzing the features of the text files, especially the
shortest ones: when they include only few sen-
tences, which kinds of predicates (and arguments)
do they include? For example, if only two movies
are mentioned as far as Monroe is concerned, what
else can we infer from the Monroe turtle file trMM
about those two movies?
211
Proceedings of the SIGDIAL 2013 Conference, pages 183?192,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Multimodality and Dialogue Act Classification in the RoboHelper Project
Lin Chen
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607
lchen43@uic.edu
Barbara Di Eugenio
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607
bdieugen@uic.edu
Abstract
We describe the annotation of a multi-
modal corpus that includes pointing ges-
tures and haptic actions (force exchanges).
Haptic actions are rarely analyzed as full-
fledged components of dialogue, but our
data shows haptic actions are used to ad-
vance the state of the interaction. We re-
port our experiments on recognizing Di-
alogue Acts in both offline and online
modes. Our results show that multimodal
features and the dialogue game aid in DA
classification.
1 Introduction
When people collaborate on physical or virtual
tasks that involve manipulation of objects, dia-
logues become rich in gestures of different kinds;
the actions themselves that collaborators engage
in also perform a communicative function. Col-
laborators gesture while speaking, e.g. saying
?Try there?? while pointing to a faraway location;
they perform actions to reply to their partner?s ut-
terances, e.g. opening a cabinet to comply with
?please check cabinet number two?. Conversely,
they use utterances to reply to their partner?s ges-
tures and actions, e.g. saying ?not there, try the
other one? after their partner opens a cabinet. Ges-
tures and actions are an important part of such di-
alogues; while the role of pointing gestures has
been explored, the role that haptic actions (force
exchanges) play in an interaction has not.
In this paper, we present our corpus of multi-
modal dialogues in a home care setting: a helper
is helping an elderly person perform activities of
daily living (ADLs) such as preparing dinner. We
investigate how to apply Dialogue Act (DA) clas-
sification to these multimodal dialogues. Many
challenges arise. First, an utterance may not di-
rectly follow a spoken utterance, but a gesture or a
haptic action. Likewise, the next move is not nec-
essarily an utterance, it can be a gesture (pointing
or haptics) only, or a multimodal utterance. Third,
when people use gestures and actions together
with utterances, the utterances become shorter,
hence the textual context that has been used to ad-
vantage in many previous models is impoverished.
Our contributions concern: exploring the dialogue
functions of what we call Haptic-Ostensive (H-O)
actions (Foster et al, 2008), namely haptics ac-
tions that often perform a referential function; ex-
perimenting with both offline and online DA clas-
sification, whereas most previous work only fo-
cuses on offline classification (Stolcke et al, 2000;
Hastie et al, 2002; Di Eugenio et al, 2010a); high-
lighting the role played by multimodal features
and dialogue structure (in the form of dialogue
games) as concerns DA classification.
Our work is part of the RoboHelper project (Di
Eugenio et al, 2010b) whose ultimate goal is to
deploy robotic assistants for the elderly so that
they can safely remain living in their home. The
models we derive from our experiments are the
building blocks of a multimodal information-state
based dialogue manager, whose architecture is
shown in Figure 1. The dialogue manager per-
forms reference resolution, specifically resolving
third person pronouns and deictics in utterances;
classifies utterances to DAs; infers the dialogue
games for utterances; updates the dialogue state,
and finally decides what the next step is in the in-
teraction. We have discussed our approach to mul-
timodal reference resolution in (Chen et al, 2011;
Chen and Di Eugenio, 2012). In this paper, we fo-
cus on the Dialogue Act classification component.
We will also touch on Dialogue Game inference.
Our collaborators are developing the speech pro-
cessing, vision and haptic recognition components
(Franzini and Ben-Arie, 2012; Ma and Ben-Arie,
2012; Javaid and Z?efran, 2012), that, when inte-
grated with the dialogue manager we are building,
183
Figure 1: System Architecture
will make the interface situated in and able to deal
with a real environment.
After discussing related work in Section 2, we
present our multimodal corpus and the multidi-
mensional annotation scheme we devised in Sec-
tion 3. In Section 4 we discuss all the features we
used to build machine learning models to classify
DAs. Sections 5 is devoted to our experiments and
the results we obtained. We conclude and discuss
future work in Section 6.
2 Related Work
Due to its importance in dialogue research, DA
classification has been the focus of a large body
of research (Stolcke et al, 2000; Sridhar et
al., 2009; Di Eugenio et al, 2010a; Boyer
et al, 2011). Some of this work has been
made possible by several available corpora tagged
with DAs, including HCRC Map Task (Ander-
son et al, 1991), CallHome (Levin et al, 1998),
Switchboard (Graff et al, 1998), ICSI Meeting
Recorder (MRDA) (Shriberg et al, 2004), and the
AMI multimodal corpus (Carletta, 2007).
Researchers have applied various approaches
to this task. Initially only simple textual fea-
tures were used, e.g. n-grams were used to
model the constraints for DA sequences in an
HMM model (Stolcke et al, 2000). Zimmermann
et al (2006) investigated the joint segmentation
and classification of DAs using prosodic features.
Sridhar et al (2009) showed that prosodic cues
can improve DA classification for a Maximum En-
tropy based model. Di Eugenio et al (2010a)
extended Latent Semantic Analysis with linguis-
tic features, including dialogue game information.
Boyer et al (2011) integrates facial expressions
to significantly improve the recognition of several
DAs, whereas Ha et al (2012) shows that auto-
matically recognized postural features may help to
disambiguate DAs.
It should be pointed out that most of this work
focuses on offline DA classification ? namely, DA
classification is performed on the corpus using
the gold-standard classification for the previous
DA(s). Since some sort of history of previous
DAs is used by all systems, using online classi-
fication for the previous DAs will unavoidably im-
pact performance (Sridhar et al, 2009; Kim et al,
2012). Additionally, for models such as HMMs
and CRF that approach the problem as sequence
labeling, online processing means that only a par-
tial sequence is available.
3 The ELDERLY-AT-HOME Corpus
This work is based on the ELDERLY-AT-HOME
corpus, a multimodal corpus in the domain of el-
derly care (Chen and Di Eugenio, 2012). The
corpus contains 20 human-human dialogues. In
each dialogue, a helper (HEL) and an elderly
person (ELD) perform Activities of Daily Liv-
ing (ADL) (Krapp, 2002), such as getting up from
chairs, finding pots, cooking pasta. The setting
is a fully equipped studio apartment used for
teaching and research in a partner university (see
Figure 2). The corpus contains 482 minutes of
recorded videos, which comprise 301 minutes of
what we call effective video, obtained by eliminat-
ing irrelevant content such as explanations of the
tasks and interruptions by the person who accom-
panied the elderly subject (who is not playing the
part of the helper). This 301 minutes contain 4782
spoken turns. The corpus includes video and au-
dio data in .avi and .wav format, haptics data col-
lected via instrumented gloves in .csv format, and
the transcribed utterances in xml format.
The Find subcorpus of our corpus comprises
only Find tasks, where subjects look for and re-
trieve various kitchen objects such as pots, silver-
ware, pasta, etc. from various locations in the
apartment. We define a Find task as a continuous
time span during which the two subjects are col-
laborating on finding objects. Find tasks naturally
arise while performing an ADL such as preparing
dinner. Figure 3 shows a Find task example.
184
Figure 2: Data Collection Experiment
Figure 3: Find Task Example
3.1 Annotation
We devised a multidimensional annotation scheme
since we are interested in investigating the role
played in the interaction by modalities different
from speech. Our annotation scheme comprises
three main components: the multimodal event an-
notation, which includes annotating for pointing
gestures, haptic-ostensive actions, their features,
and their relationships to utterances; the dialogue
act annotation; and the referential expression an-
notations already described in (Chen et al, 2011;
Chen and Di Eugenio, 2012).
3.1.1 Multimodal Event Annotation
To study the roles played by different sorts of mul-
timodal actions, and how they contribute to the
flow of the dialogue, pointing gestures, Haptic-
Ostensive (H-O) actions, and the relations among
them have been annotated on the Find subcorpus.
The Find subcorpus contains 137 Find tasks, col-
lected from the dialogues of 19 pairs of subjects
from the larger corpus. 1 The multimodal annota-
1One pair of subjects was excluded, because ELD ap-
peared confused. Our goal was to recruit elderly subjects with
tion tool Anvil (Kipp, 2001) was used to transcribe
all the utterances, and to annotate for all categories
described in this paper. Each annotation category
is an annotation group in Anvil. For each subject,
one track is defined for each annotation group, for
a total of 4 tracks per subject in Anvil.
Pointing gestures are used naturally when peo-
ple refer to a far away object. We define a pointing
gesture as a hand gesture without physical contact
with the target. Our definition of pointing gesture
does not include head or other body part move-
ments used to indicate targets. Our corpus in-
cludes very few occurrences of those; additionally,
our collaborators in the RoboHelper project focus
on recognizing hand gestures. We have identified
two types of pointing gestures. The first is, point-
ing gestures with an identifiable target, which is
usually indicated by a short time stable hand point-
ing. The other type is without a fixed target. It
usually happens when the subject points to several
targets in a short time, or the subject just points to
a large space area.
For a pointing gesture, we mark two attributes:
the time span and the target. The time span of
a pointing gesture starts when the subject initi-
ates the hand movement, ends when the subject
starts to draw the hand back. We have devised a
Referring Index System (Chen and Di Eugenio,
2012) to mark the different types of targets: sin-
gle identifiable target, multiple identifiable targets
and unidentifiable target.
During Find tasks, subjects need to physically
interact with the objects, e.g. they need to open
cabinets to get plates, to put a pot on the stove etc.
Those physical contact actions often perform a re-
ferring function as well, either adding new enti-
ties to the discourse model, or referring to an al-
ready established referent. For example, in Fig-
ure 3, the action [Touch(Hel,Drawer1)] that ac-
companies Utt4 disambiguates This by referring to
Drawer1, tantamount to a pointing gesture; con-
versely, the action [Takeout(HEL,spoon1)] associ-
ated with Utt8 establishes a referent for spoon1.
Following (Foster et al, 2008), we label Haptic-
Ostensive (H-O) those actions that involve physi-
cal contact with an object, and that can at the same
time perform a referring function. Note that target
objects here exclude the partner?s body parts, as
when HEL helps ELD get up from a chair.
No existing work that we know of identifies
intact cognitive functions, but this subject was an exception.
185
types of H-O actions. Hence, we had to define our
own categories, based on the following two princi-
ples: (1) The H-O types must be grounded in our
data, namely, the definitions are empirically based:
these H-O actions are frequently observed in the
corpus. (2) They are within the scope of what our
collaborators can recognize from the haptic sig-
nals. The five H-O action types we defined are:
? Touch: when the subject only touches the
targets, no immediate further actions are per-
formed
? MANIP-HOLD: when the subject takes out
or picks up an object and holds it stably for a
short period of time
? MANIP-NO-HOLD: when the subject takes
out or picks up an object, but without explic-
itly showing it to the other subject
? Open: starts when the subject has physical
contact with the handle of the fridge, a cabi-
net or a drawer, and starts to pull; ends when
the physical contact is off
? Close: when the subject has physical con-
tact with the handle of the fridge, a cabinet
or a drawer, and starts to push; ends when the
physical contact is off
For H-O action annotation, three attributes are
marked: time span, target and action type. The
?Target? attribute is similar to the ?Target? at-
tribute in pointing gesture annotation. Since H-
O actions are more accurate than pointing ges-
tures (Foster et al, 2008), the targets are all iden-
tifiable.
Table 1 provides distributions of the length in
seconds for different types of events in the Find
corpus. Table 2 shows the counts of different
events divided by type of participant. From these
two tables, it is apparent that:
? Pointing gestures and H-O actions were fre-
quently used: their total corresponds to 61%
of the number of utterances
? Utterances are short: only 1.7?, and 4.2
words on average
? ELD performed 66% of pointing gestures,
and HEL 97.5% of H-O actions
Multimodal Event Relation Annotation.
Pointing gestures and H-O actions can accompany
an utterance, e.g. see move 2 in Figure 3: HEL
Utterances Pointing H-O Actions Total
2555? 571? 1088? 4377?
Table 1: Find Subcorpus: Length in seconds
ELD HEL Total
Utterances 756 760 1516
Words 3612 2981 6593
Pointing 219 113 332
H-O Actions 15 582 597
Table 2: Find Subcorpus: Counts
asks ?Down there? while pointing to a drawer;
or can be used independently, e.g. see move 6
in Figure 3: HEL does not utter any words, but
opens the drawer after ELD confirms that is
the right drawer with ?Uh-huh?. In the latter
case, HEL used an action to respond to ELD.
Pointing gestures and H-O actions are followed
by utterances as well, e.g. move 11 in Figure 3:
after HEL opens a drawer, ELD says ?Yes, there
it is?.
To understand how pointing gestures and H-O
actions participate in the dialogues and how they
interact with utterances, we further annotated the
relationship between utterances, pointing gestures
and H-O actions. Just using timespans is not
sufficient. It is not necessarily the case that utter-
ance U is associated with gesture / H-O action G
if their timespans overlap. This type of annotation
is purely local: the fact that turns 2-5 in Figure 3
confirm which drawer to open, would be captured
at the dialogue game level.
First, we assign to each utterance, pointing ges-
ture and H-O action a unique event index, so that
we can refer to these events with their indices. For
pointing gestures and H-O actions, we define two
more attributes: ?associates? and ?follows?. If a
pointing gesture or H-O action is associated with
an utterance, the ?associates? value will be the in-
dex of that utterance; by default, the ?associates?
value is empty. If a pointing gesture or H-O ac-
tion independently follows an utterance, the ?fol-
lows? value will be that utterance?s index. E.g.,
for move 6 in Figure 3, we mark the H-O action
?Open? with ?follows [5]?.
For utterances, we only mark the ?follows? at-
tribute. If an utterance directly follows a point-
ing gesture or H-O action, we use the index of the
pointing gesture or H-O action as the ?follows?
value. By default, the ?follows? attribute of an ut-
terance is empty. It means that an utterance fol-
186
lows its immediate previous utterance.
We define a move as any combination of related
utterances, pointing gestures and H-O actions, per-
formed by the same subject. On the basis of the
event relation annotations, we can compute the di-
alogue?s move flow using the following algorithm.
1. Order all the utterances in a Find task session
by the utterance start time
2. Until all the utterances are processed, for
each unprocessed utterance ui:
(a) If ui follows a pointing gesture or H-O
action, that pointing gesture or H-O ac-
tion forms a new move mk; add mk to
the sequence before ui
(b) Find all the pointing gestures and H-
O actions labelled as associates of ui.
These events form the movemi together
with ui
(c) Recursively find the events which fol-
low the last generated move, together
with all their associated events to form
another move
This algorithm computes 1791 moves, as shown in
Table 3. More than 90% of pointing gestures are
used with utterances. Only 377 out of 596 H-O ac-
tions are included in themoves, mostly because the
H-O action ?Close? frequently follows an ?Open?
action (these cases are not detected by the algo-
rithm, because they don?t advance the dialogue).
ELD HEL Total
Utterances 545 507 1052
Pointing 9 11 20
H-O 5 213 218
Utterance&Pointing 209 100 309
Utterance&H-O 2 153 155
Total 770 984 1754
Table 3: Moves Statistics in Find Corpus
3.1.2 Dialogue Act Annotation
Since the Find corpus is task-oriented in nature,
we built on the dialogue act inventory of HCRC
MapTask, a well-known task oriented corpus (An-
derson et al, 1991). The MapTask tag set con-
tains 11 moves:2 instruct, explain, check, align,
query-w, query-yn; acknowledge, reply-y, reply-n,
reply-w, clarify. However, this inventory of DAs
does not cover utterances that are used to respond
2A twelfth move, Ready, does not appear in our corpus.
to gestures and actions, such as Utt.11 in Figure 3.
The semantics of the reply-{y/n/w} tags does not
cover these situations. Hence, we devised three
more tags, which apply only to statements that fol-
low a move composed exclusively of a gesture or
an action (in the sense of ?follow? just discussed):
? state-y: a statement which conveys ?yes?,
such as Utt.11 in Figure 3.
? state-n: a statement which conveys ?no?, e.g.
if Utt.11 had been Wait, try the third drawer.
? state: still a statement , but not conveying ac-
ceptance or rejection, e.g. So we got the soup.
Hence, the DAs in {state-y, state-n, state} are
used to tag responses to actions, and the DAs
in {reply-y, reply-n, reply-w} are used to tag re-
sponses to utterances. Table 4 shows the distribu-
tion of DAs by subject.
Dialogue Act ELD HEL Total Ratio
Instruct 295 19 314 20.7%
Acknowledge 22 186 208 13.7%
Reply-y 179 3 182 12.0%
Check 1 155 156 10.3%
Query-yn 23 133 156 10.3%
Query-w 3 144 147 9.7%
Reply-w 132 4 136 9.0%
State-y 40 36 76 5.0%
State-n 16 50 66 4.4%
Reply-n 27 9 36 2.4%
State 7 15 22 1.5%
Explain 10 4 14 0.9%
Align 1 2 3 0.3%
Total 756 760 1516 100%
Table 4: Dialogue Act Counts in Find Corpus
Intercoder Agreement. In order to verify the
reliability of our annotations, we double coded
15% of the data for pointing gestures, H-O actions
and DAs. These are the dialogues from 3 pairs of
subjects, and contain 22 Find tasks. Because the
pointing gestures and H-O actions are time span
based, when we calculate agreement, we use an
overlap based approach. If the two annotations
from the two coders overlap by more than 50% of
the event length, and the other attributes are the
same, we count this as a match. We used ? to
measure the reliability of the annotation (Cohen,
1960). We obtained reasonable values: for point-
ing gestures, ?=0.751, for H-O actions, ?=0.703,
and for DAs, ?=0.789.
187
4 Experimental Setup
We ran experiments classifying the DA tag for the
current utterance. We employ supervised learn-
ing approaches, specifically: Conditional Random
Field (CRF) (Lafferty et al, 2001), Maximum En-
tropy (MaxEnt), Naive Bayes (NB), and Decision
Tree (DT). These algorithms are widely used for
DA classification (Sridhar et al, 2009; Ivanovic,
2008; Ha et al, 2012; Kim et al, 2012). We
used Mallet (McCallum, 2002) to build CRF mod-
els. MaxEnt models were built using the Max-
Ent 3 package from the Apache OpenNLP pack-
age. Naive Bayes and Decision Tree models were
built with theWeka (Hall et al, 2009) package (for
decision trees, we used the J48 implementation).
All the results we will show below were obtained
using 10 fold cross validation.
4.1 Features
Among our goals were not only to obtain effec-
tive classifiers, but also to investigate which kind
of features are most effective for our tasks. As
a consequence, beyond textual features and dia-
logue history features, we experimented with mul-
timodal features extracted from other modalities,
utterance features, and automatically inferred dia-
logue game features.
Textual features (TX) are the most widely used
features for DA classification (Stolcke et al, 2000;
Bangalore et al, 2008; Sridhar et al, 2009; Di Eu-
genio et al, 2010a; Kim et al, 2010; Boyer et al,
2011; Ha et al, 2012; Kim et al, 2012). The tex-
tual features we use include lexical, syntactic, and
heuristic features.
? Lexical features: Unigrams of the words and
part-of-speech tags in the current utterance.
The words used in the features are processed
using the morphology tool from the Stanford
parser (De Marneffe and Manning, 2008).
? Syntactic features: The top node and its
first two child nodes from the sentence parse
tree. If an utterance contains multiple sen-
tences, we use the last sentence. Sentences
are parsed using the Stanford parser.
? Number of sentences and number of words in
the utterance. We use Apache OpenNLP li-
brary 4 to detect sentences and tokenize them.
3http://maxent.sourceforge.net
4http://opennlp.apache.org/
? Heuristic features: whether an utterance con-
tains WH words (e.g. what, where), whether
an utterance contains yes/no words (e.g. yes,
no, yeah, nope).
Utterance features (UT) are extracted from
the current utterance?s meta information. Previ-
ous research showed that utterance meta informa-
tion such as the utterance speaker can help classify
DAs (Ivanovic, 2008; Kim et al, 2010).
? The actor of the utterance
? The time length of the utterance
? The distance of the current utterance from the
beginning of the dialogue
The pointing gesture feature (PT) indicates
whether the actor of the current utterance ui is
making a pointing gesture G, i.e., whether G is
associated with ui, and hence, part of move mi.
Haptic-Ostensive features (H-O) indicate
whether the actor of the current utterance ui is per-
forming any H-O action G i.e., whether G is asso-
ciated with ui, and hence, part of move mi; and
the type of that action, if yes.
Location features (LO) include the locations
of the two actors, whether they are in the same
location, whether the actor of the current utter-
ance changes the location during the utterance.
Since we do not have precise measurement of sub-
jects? locations, we annotate approximate loca-
tions by dividing the apartment into four large ar-
eas: kitchen, table, lounge and bed.
The dialogue game feature (DG) models hi-
erarchical dialogue structure. Some previous re-
search on DA classification has shown that hier-
archical dialogue structure encoded via the no-
tion of conversational games (Carlson, 1983) sig-
nificantly improves DA classification (Hastie et
al., 2002; Sridhar et al, 2009; Di Eugenio et al,
2010a). In MapTask, a game is defined as a se-
quence of moves starting with an initiation (in-
struct, explain, check, align, query-yn, query-w)
and encompassing all utterances up until the pur-
pose of the game has been fulfilled, or abandoned.
In the Find corpus, dialogue games have not been
annotated. In order to use the DG feature, we use
a just-in-time approach to infer dialogue games.
For each dialogue, we maintain a stack for dia-
logue games. When an utterance is classified as
an initiating DA tag, we assume the dialogue has
188
entered a new dialogue game, and push the DA la-
bel as the dialog game to the top of the stack. The
DG feature value is the top element of the stack.
The dialogue game feature is always inferred at
run time during classification process, just before
an utterance is being processed. Hence, when we
classify the DA for the current utterance ui, the
DG value that we use is the closest preceding ini-
tiating DA.
Dialogue history features (DH) model what
happened before the current utterance (Sridhar et
al., 2009; Di Eugenio et al, 2010a). We encode:
? The previous move?s actor
? Whether the previous move has the same ac-
tor as the current move
? The type of the previous move; if it is an ut-
terance, its DA tag; if it is an H-O action, the
type of H-O action
5 DA Classification Experiments
We ran the DA classification experiments with
three goals. First, we wanted to assess the ef-
fectiveness of different types of features, espe-
cially, the effectiveness of gesture, H-O action, lo-
cation and dialogue game features. Second, we
wanted to compare the performances of different
machine learning algorithms on such a multimodal
dialogue dataset. Third, we wanted to investigate
the performances of different algorithms in the on-
line and offline experiment settings. The DA clas-
sification task could be treated as a sequence label-
ing problem (Stolcke et al, 2000). However, dif-
ferent from other sequence labeling problems such
as part-of-speech tagging, a dialogue system can-
not wait until the whole dialogue ends to classify
the current DA. A dialogue system needs online
DA classification models to classify the DAs when
a new utterance is processed by the system. There
are two differences between online and offline DA
classification modes. First, when we generate the
dialogue history and dialogue game features, we
use the previously classified DA tag results for on-
line mode, while we use the gold-standard DA tags
for offline mode. Second, MaxEnt (using beam
search) and CRF evaluate and classify all the ut-
terances in a dialogue at the same time in offline
mode; however in online mode, MaxEnt and CRF
can only work on the partial sequence up to the
utterance to classify. Whereas this may sound ob-
vious, it explains why the performance of these
classifiers may be even more negatively affected
in online mode with respect to their offline perfor-
mance, as compared to other classifiers. We will
see that indeed this will happen for CRF, but not
for MaxEnt.
To evaluate feature effectiveness, we group the
features into seven groups: textual features (TX),
utterance features (UT), pointing gesture fea-
ture (PT), H-O action features (H-O), location
features (LO), dialogue game feature (DG), dia-
logue history features (DH). Then we generate all
the combinations of feature groups to run exper-
iments. For each classification algorithm, we ran
10-fold cross-validation experiments, for each fea-
ture group combination, in both online and offline
mode. It would be impossible to report all our re-
sults. Similarly to (Ha et al, 2012), we report our
results with single feature groups and incremen-
tal feature group combinations, as shown in Ta-
ble 5. Whereas all combinations were tried, the
omitted results do not shed any additional light on
the problem. The majority baseline, which al-
ways assigns the most frequent tag to every utter-
ance, has an accuracy of 20.3%.
The CRF offline model performs best, which
confirms the results of (Kim et al, 2010; Kim
et al, 2012). This is due to the strong correla-
tion between dialogue history features (DH) and
the states of the CRF. In online mode, when there
is noise in the previous DA tags, the CRF?s per-
formance drops significantly (p?.005, using ?2).
A significant drop in performance from offline to
online mode also happens to NB (p?.005) and
DT (p<.025). MaxEnt performs very stably, the
best online model performs only .015 worse than
the best offline model. The best MaxEnt offline
model beats the other algorithms? best models ex-
cept CRF, while the MaxEnt online model outper-
forms all the other algorithms? online models. Our
results thus demonstrate that MaxEnt works best
for online DA classification on our data.
As concerns features, for online models, textual
features (TX) are the most predictive as a feature
type used by itself. When we add pointing ges-
ture (PT), H-O features (H-O) and location fea-
tures (LO) together to textual features, we notice
a significant performance improvement for most
models (except CRF models). For MaxEnt, which
gives the best results for online models, none of
the gesture, H-O action and location features alone
significantly improve the results, but all three to-
189
Features CRF MaxEnt NB DT
Offline Online Offline Online Offline Online Offline Online
1. TX (Textual) .654 .641 .630 .630 .449 .453 .450 .450
2. UT (Utterance) .506 .376 .353 .353 .417 .417 .392 .392
3. PT (Pointing) .225 .155 .210 .210 .212 .212 .212 .212
4. H-O (Haptic-Ostensive) .187 .147 .237 .237 .243 .243 .212 .212
5. LO (Location) .259 .176 .264 .264 .259 .259 .265 .265
6. DG (Dialogue Game) .737 .136 .305 .189 .212 .212 .212 .212
7. DH (Dialogue History) .895 .119 .480 .302 .478 .284 .471 .294
8. TX+PT .654 .651 .639 .639 .453 .453 .450 .450
9. TX+PT+H-O .670 .649 .637 .637 .456 .456 .449 .449
10. TX+PT+H-O+LO .648 .645 .657? .657? .523? .523? .536? .536?
11. TX+PT+H-O+LO+UT .668 .612 .685 .685 .563 .563 .568 .568
12. TX+PT+H-O+LO+UT+DG .770?? .528 .722?? .709?? .566 .591?? .576 .607??
13. TX+PT+H-O+LO+UT+DG+DH .847? .475 .757? .742? .635? .606 .671? .627
Table 5: Dialogue Act Classification Accuracy: * indicates significant improvement after adding PT+H-
O+LO to TX (cf. lines 1 and 10); ** indicates significant improvement after adding DG to TX+PT+H-
O+LO+UT (cf. lines 11 and 12); ?indicates significant improvement after adding DH to TX+PT+H-
O+LO+UT+DG (cf. lines 12 and 13); bold font indicates the feature group set giving best performance
for each column.
gether do. This confirms the finding of (Ha et al,
2012) that non-verbal features help DA classifica-
tion. To assess which feature is the most important
among those three non-verbal features, we exam-
ined the experiment results with a leave-one-out
strategy, that is for each classifier in offline and
online modes, we leave one of the gesture, H-O
and location features out from the full experiment
feature set (TX+PT+H-O+LO+UT+DG+DH). No
significant difference was discovered.
When the dialogue game features (DG) are
added to the models, performance increases sig-
nificantly for CRF offline model (p<.005), Max-
Ent offline (p<.005) and online (p<.05) mod-
els, NB online model (p<.05) and DT online
model (p<.005). It confirms previous findings, in-
cluding by our group (Di Eugenio et al, 2010a),
that dialogue game features (DG) play a very im-
portant role in DA classification, even via the sim-
ple approximation we used. When the dialogue
history features (DH) are added to the models,
performance increased significantly for all the of-
fline models and the MaxEnt online model, with
p<.005. This confirms previous findings that dia-
logue history helps with DA classification.
6 Conclusions and Future Work
In this paper we described our multimodal cor-
pus which is annotated with multimodal informa-
tion (pointing gestures and H-O actions) and dia-
logue acts. Our corpus analysis shows that peo-
ple actively use pointing gestures and H-O actions
alongside utterances in dialogues. The function of
H-O actions in dialogue had hardly been studied
before. Our experiments show that MaxEnt per-
forms best for the online DA classification task.
Multimodal and dialogue game features both im-
prove DA classification.
Short-term future work includes manual anno-
tation for dialogue games, in the hope that more
accurate dialogue game features may further im-
prove DA classification. Longer term future work
includes prediction of the specific next move ? the
specific DA and/or the specific gesture, pointing or
H-O action. We have now developed some of the
building blocks of an information-state based mul-
timodal dialogue manager. The major aspects we
still need to address are defining the information-
state for the Find task, and developing rules to up-
date the information-state with multimodal infor-
mation, the classified DAs, and the co-reference
resolution models we already built (Chen et al,
2011; Chen and Di Eugenio, 2012). Once the
information-state component is in place, we can
expect better and more detailed predictions.
Acknowledgments
This work is supported by award IIS 0905593
from the National Science Foundation. Thanks
to the other members of the RoboHelper project,
for their many contributions, especially to the data
collection effort.
190
References
Anne H. Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Gar-
rod, Stephen Isard, Jacqueline Kowtko, Jan McAl-
lister, Jim Miller, Catherine Sotillo, and Henry S.
1991. The HCRC Map Task corpus. Language and
Speech, 34(4):351.
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2008. Learning the structure of
task-driven human?human dialogs. IEEE Transac-
tions on Audio, Speech, and Language Processing,
16(7):1249?1259.
K.E. Boyer, J.F. Grafsgaard, E.Y. Ha, R. Phillips, and
J.C. Lester. 2011. An affect-enriched dialogue act
classification model for task-oriented dialogue. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 1190?1199.
Association for Computational Linguistics.
Jean Carletta. 2007. Unleashing the killer corpus:
experiences in creating the multi-everything AMI
meeting corpus. Language Resources and Evalua-
tion, 41(2):181?190.
Lauri Carlson. 1983. Dialogue games: An approach to
discourse analysis. D. Reidel Publishing Company.
Lin Chen and Barbara Di Eugenio. 2012. Co-reference
via pointing and haptics in multi-modal dialogues.
In The 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. The As-
sociation for Computational Linguistics.
Lin Chen, Anruo Wang, and Barbara Di Eugenio.
2011. Improving pronominal and deictic co-
reference resolution with multi-modal features. In
Proceedings of SIGdial 2011, the 12th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 307?311, Portland, Oregon, June.
Association for Computational Linguistics.
J. Cohen. 1960. A coefficient of agreement for nomi-
nal scales. Educational and psychological measure-
ment, 20(1):37?46.
Marie-Catherine De Marneffe and Christopher D.
Manning. 2008. The Stanford typed dependencies
representation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8. Association for Com-
putational Linguistics.
Barbara Di Eugenio, Zhuli Xie, and Riccardo Serafin.
2010a. Dialogue act classification, higher order di-
alogue structure, and instance-based learning. Dia-
logue & Discourse, 1(2):1?24.
Barbara Di Eugenio, Milos? Z?efran, Jezekiel Ben-
Arie, Mark Foreman, Lin Chen, Simone Franzini,
Shankaranand Jagadeesan, Maria Javaid, and Kai
Ma. 2010b. Towards Effective Communication
with Robotic Assistants for the Elderly: Integrating
Speech, Vision and Haptics. In Dialog with Robots,
AAAI 2010 Fall Symposium, Arlington, VA, USA,
November.
M.E. Foster, E.G. Bard, M. Guhe, R.L. Hill, J. Ober-
lander, and A. Knoll. 2008. The roles of haptic-
ostensive referring expressions in cooperative, task-
based human-robot dialogue. In Proceedings of the
3rd ACM/IEEE International Conference on Human
Robot Interaction, pages 295?302. ACM.
Simone Franzini and Jezekiel Ben-Arie. 2012. Speech
recognition by indexing and sequencing. Interna-
tional Journal of Computer Information Systems and
Industrial Management Applications, 4:358?365.
David Graff, Alexandra Canavan, and George Zip-
perlen. 1998. Switchboard-2 Phase I.
Eun Young Ha, Joseph F. Grafsgaard, Christopher
Mitchell, Kristy Elizabeth Boyer, and James C.
Lester. 2012. Combining verbal and nonverbal
features to overcome the ?information gap? in task-
oriented dialogue. In Proceedings of SIGdial 2012,
the 13th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 247?256,
Seoul, South Korea, July. Association for Computa-
tional Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Helen Wright Hastie, Massimo Poesio, and Stephen Is-
ard. 2002. Automatically predicting dialogue struc-
ture using prosodic features. Speech Communica-
tion, 36(1?2):63?79.
Edward Ivanovic. 2008. Automatic instant messaging
dialogue using statistical models and dialogue acts.
Master?s thesis, University of Melbourne.
Maria Javaid and Milos? Z?efran. 2012. Interpreting
communication through physical interaction during
collaborative manipulation. Draft, October.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-one
live chats. In Proceedings of EMNLP 2010, the Con-
ference on Empirical Methods in Natural Language
Processing, pages 862?871. Association for Com-
putational Linguistics.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2012. Classifying dialogue acts in multi-party
live chats. In Proceedings of the 26th Pacific Asia
Conference on Language, Information, and Compu-
tation, pages 463?472, Bali, Indonesia, November.
Faculty of Computer Science, Universitas Indone-
sia.
Michael Kipp. 2001. Anvil-a generic annotation tool
for multimodal dialogue. In Proceedings of the
7th European Conference on Speech Communica-
tion and Technology, pages 1367?1370.
191
Kristine M. Krapp. 2002. The Gale Encyclopedia of
Nursing & Allied Health. Gale Group, Inc. Chapter
Activities of Daily Living Evaluation.
John D. Lafferty, Andrew McCallum, and Fer-
nando C.N. Pereira. 2001. Conditional Random
Fields: Probabilistic Models for Segmenting and La-
beling Sequence Data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, pages 282?289. Morgan Kaufmann Publishers
Inc.
L. Levin, A. Thyme?-Gobbel, A. Lavie, K. Ries, and
K. Zechner. 1998. A discourse coding scheme for
conversational Spanish. In Fifth International Con-
ference on Spoken Language Processing.
K. Ma and J. Ben-Arie. 2012. Multi-view multi-
class object detection via exemplar compounding.
In IEEE-IAPR 21st International Conference on
Pattern Recognition (ICPR 2012), Tsukuba, Japan,
November.
Andrew Kachites McCallum. 2002. MALLET: A Ma-
chine Learning for Language Toolkit.
E. Shriberg, R. Dhillon, S.V. Bhagat, J. Ang, and
H. Carvey. 2004. The ICSI Meeting Recorder
Dialog Act (MRDA) Corpus. In Proceedings of
5th SIGdial Workshop on Discourse and Dialogue,
pages 97?100, Cambridge, MA, April 30-May 1.
V.K.R. Sridhar, S. Bangalore, and S. Narayanan. 2009.
Combining lexical, syntactic and prosodic cues for
improved online dialog act tagging. Computer
Speech & Language, 23(4):407?422.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,
and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339?373.
Matthias Zimmermann, Andreas Stolcke, and Eliza-
beth Shriberg. 2006. Joint segmentation and clas-
sification of dialog acts in multiparty meetings. In
ICASSP 2006, the IEEE International Conference
on Acoustics, Speech and Signal Processing, vol-
ume 1. IEEE.
192
Proceedings of the 8th International Natural Language Generation Conference, pages 6?10,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
PatientNarr: Towards generating patient-centric summaries
of hospital stays
Barbara Di Eugenio, Andrew D. Boyd, Camillo Lugaresi, Abhinaya Balasubramanian,
Gail M. Keenan, Mike Burton, Tamara G. Rezende Macieira, Karen Dunn Lopez
University of Illinois at Chicago
Chicago, IL, USA
Carol Friedman
Columbia University
New York, NY, USA
Jianrong Li, Yves A. Lussier
University of Arizona
Tucson, AZ, USA
Abstract
PatientNarr summarizes information taken
from textual discharge notes written by
physicians, and structured nursing docu-
mentation. It builds a graph that highlights
the relationships between the two types of
documentation; and extracts information
from the graph for content planning. Sim-
pleNLG is used for surface realization.
1 Introduction
Every year, 7.9% of the US population is hos-
pitalized (CDC, 2011). Patients need to under-
stand what happened to them in the hospital, and
what they should do after discharge. PatientNarr
will ultimately be able to generate concise, lay-
language summaries of hospital stays. We hy-
pothesize that such summaries will help patients
take care of themselves after they are discharged,
and supplement current approaches to patient ed-
ucation, which is not always effective (Olson and
Windish, 2010).
PatientNarr needs to summarize documentation
that is currently segregated by profession; as
a minimum, as physician discharge notes and
as nursing plans-of-care. We contend that both
sources are necessary to provide the patient with
full understanding, also because much of the direct
care provided by nurses will need to be continued
following discharge (Cain et al., 2012).
In our case, PatientNarr summarizes data that
is heterogeneous (textual for physician discharge
notes, structured for nursing documentation).
This paper describes the steps we have undetaken
so far: (a) To demonstrate that physician and nurse
documentations diverge, we map both to a graph,
and study the relationships therein. This graph
supports content planning. (b) We have devel-
oped the pipeline that extracts the information to
be communicated, and renders it in English via
SimpleNLG (Gatt and Reiter, 2009).
Related work. NLG and Summarization in the
biomedical domain have been pursued for a few
years (Di Eugenio and Green, 2010), but most
work addresses health care personnel: to navi-
gate cancer patients? medical histories (Hallett,
2008; Scott et al., 2013); to generate textual sum-
maries describing a hospitalized infant for nurses
(Portet et al., 2009); to generates reports of care
for hand-off between emergency workers (Schnei-
der et al., 2013). Most applications of NLG that
target patients focus on behavioral changes (Reiter
et al., 2003), or patient counseling (Green et al.,
2011). Only few NLG systems attempt at generat-
ing personalized medical information from med-
ical records or data (Williams et al., 2007; Ma-
hamood and Reiter, 2011).
2 A motivating example
So far, we have gained access to 28 de-identified
discharge notes of cardiology patients from the
University of Illinois Hospital and Health Science
System (UIHHSS). Figure 1 shows about 20% of
the physician discharge notes for Patient 9. It is
difficult to understand, not only because of jargon,
but also because of ignorance of relevant domain
relationships. Importantly, these notes do not talk
about issues that are potentially important for the
patient, like his state of mind, which are more of-
ten addressed by nurses. In our case, the nursing
documentation is not textual, but entered via the
HANDS tool, and stored in a relational database
(Keenan et al., 2002). A tiny portion of the ini-
tial plan-of-care (POC) for Patient 9 is shown in
Figure 2 (this nursing data is reconstructed, see
Section 3). One POC is documented at every
formal handoff (admission, shift change, or dis-
charge). HANDS employs the NANDA-I taxon-
6
Patient was admitted to Cardiology for new onset a fib in RVR. Was given an additional dose of diltazem 30mg po when first
seen. Patient was started on a heparin drip for possible TEE and cardioversion. Overnight his HR was in the 100-110s; however
did increase to 160s for which patient was given 120mg er of diltazem. HR improved; however, in the morning while awake
and moving around, HR did increase to the 130-140s. Patient was given another dose of IV dilt 20mg. [...] Upon discharge
was given two prescriptions for BP, HCTZ and losartan given LVH seen on echo. Patient was counseled on the risks of stroke
and different options for anticoagulation. [...]
Figure 1: Excerpt from physician discharge notes (Patient 9)
omy of nursing diagnoses, represented by squares
in Figure 2; the Nursing Outcomes Classification
(NOC) ? circles; and the Nursing Interventions
Classification (NIC) ? triangles (NNN, 2014). In
Figure 2, Acute Pain is a diagnosis, and Anxiety
Level and Pain Level (some of) its associated out-
comes. Anxiety Reduction is an intervention asso-
ciated with Anxiety Level; Pain Management and
Analgesic Administration are interventions associ-
ated with Pain Level. A scale from 1 to 5 indicates
the initial value associated with an outcome (i.e.,
the state the patient was in when s/he was admit-
ted), the expected rating, and the actual rating at
discharge. In Figure 2, the current level for Pain
Level and Anxiety Level is 2 each, with an expected
level of 5 at discharge, i.e., no pain/anxiety.
Figure 2: Excerpt from nursing documentation
Figures 1 and 2 suggest that physician and nurs-
ing documentations provide different perspectives
on patients: e.g., Anxiety is not even mentioned
in the discharge notes. One of the authors (a
nursing student) wrote summaries for five of the
28 discharge summaries and their corresponding
HANDS POCs ? Figure 3 shows the summary for
Patient 9. This initial round of human authoring
was meant to provide some preliminary guidelines
to generate automatic summaries. Please see Sec-
tion 5 for our plans on obtaining a much larger
quantity of more informed human-authored sum-
maries.
3 Extracting relationships between
physician notes and nursing data
To extract and relate information from our
two sources, we rely on UMLS, MedLEE and
HANDS. UMLS, the Unified Medical Language
System (NLM, 2009), includes 2.6 million con-
cepts (identified by Concept Unique Identifiers or
CUIs) organized in a network. Importantly, many
different medical and nursing terminologies have
been incorporated into UMLS, including those
used by HANDS (NANDA-I, NIC and NOC).
UMLS provides mapping between their concepts
and CUIs, via 8.6 million concept names and rela-
tionships between terminologies. Some relation-
ships are of a hierarchical nature, where one con-
cept is narrower than the other (e.g., Chest X-ray
and Diagnostic radiologic examination).
MedLEE is a medical information extraction
system (Friedman et al., 2004). In its semi-
structured output, recognized entities are mapped
to the corresponding CUI in UMLS.
HANDS has not been adopted at UIHHSS yet.
Hence, we reconstructed HANDS POCs for those
28 patients on the basis of 40,661 cases collected
at four hospitals where HANDS is in use. For
each of the 28 patients, the same nursing student
who authored the five summaries, selected simi-
lar cases, and used them to produce high-quality
records consistent with actual nursing practice.
To relate physician and nursing documenta-
tions, we seed a graph with two sets of CUIs:
those returned by MedLEE as a result of process-
ing the physician discharge notes; and the CUIs
corresponding to all the NANDA-I, NIC and NOC
terms from the HANDS POCs. We then grow the
graph by querying UMLS for the set of concepts
related to each of the concepts in our set; the con-
cepts that were not already part of the graph are
then used to begin a new round of growth (we
stop at round 2, to keep the time used by UMLS
to answer, reasonable). From this graph, we
keep the concepts that either belong to one of the
7
You were admitted with new onset of atrial fibrillation. You reported feeling weakness, chest pressure and increased shortness
of breath. You reported acute pain and you were anxious. During your hospitalization you were treated with analgesics for your
pain and pain management was performed by the nursing team. Your shortness of breath improved. Your decreased cardiac
output was treated with medication administration and knowledge about cardiac precautions, hypertension management, your
treatment regimen and the prescribed medication were taught to you by the nurses. A Transophageal Echocardiography was
performed. You met the expected outcomes for your condition and you were discharged under the condition improved for
your home. You have an appointment scheduled at Union Medical Center on [DATE] with Physician [DR.]. The list of your
medications is attached to this discharge.
Figure 3: Human-authored summary (Patient 9)
source lists, or that are required to form a con-
nection between a doctor-originated concept and
a nurse-originated concept that would otherwise
remain unconnected. All other concepts are re-
moved. The result is a graph with several separate
connected components, which correspond to clus-
ters of related concepts occurring in the discharge
notes or in the plans of care, or forming connec-
tions between the two sources.
We count distances in terms of relationships tra-
versed, starting from the nursing concepts since
they are fewer, and since path traversal is re-
versible.1 Concepts can overlap; or be directly
connected (distance one); or be directly connected
through an intermediate concept (distance two).
We do not consider distances beyond two. Table 1
shows results for our specific example, Patient 9,
and average results across our 28 test cases. As we
can see, there are very few concepts in common,
or even at distance 1. Our results provide quanti-
tative evidence for the hypothesis that physicians
and nurses talk differently, not just as far as ter-
minology is concerned, but as regards aspects of
patient care. This provides strong evidence for our
hypothesis that a hospitalization summary should
include both perspectives.
4 Automatically generating the summary
In this baseline version of PatientNarr, we focused
on understanding how the vital parameters have
improved over time, the treatment given for im-
provement and the issues addressed during the
process. The summary generated by PatientNarr
for Patient 9 is shown in Figure 4. We extract
information of interest from the graph obtained
at the previous step; we couch it as features of
phrasal constituents via the operations provided by
the SimpleNLG API. SimpleNLG then assembles
grammatical phrases in the right order, and helps
1In UMLS, any relationship from concept A to concept B,
has a corresponding relationship from B to A, not necessarily
symmetric.
in aggregating related sentences.
Since there are far fewer nursing than doctor
concepts, we start from the NANDA-I codes, i.e.,
the diagnoses. The name associated in UMLS
with the corresponding CUI is used. For each
NANDA-I node, we highlight the treatments given
(the NIC codes), e.g. see the sentence starting
with Acute onset pain was treated [...] in Fig-
ure 4. For both diagnosis and treatments, we at-
tempt to relate them to doctor?s nodes. Specif-
ically, we exploit the relationships in the UMLS
ontology and include nodes in the graph we con-
structed that are at distance 1 or 2, and that are
either doctor?s nodes, or intermediate nodes that
connect to a doctor?s node. For example, in Dys-
rhythmia management is remedy for tachycardia
and Atrial Fibrillation, Dysrhythmia management
is a NIC intervention that is related to Cardiac
Arrhythmia; in turn, Cardiac Arrhythmia is a di-
rect hypernym of tachycardia and Atrial Fibrilla-
tion which were both extracted from the physician
notes by MedLEE. Cardiac Arrhythmia was dis-
covered by our graph building procedure, as de-
scribed in Section 3.
We then highlight what improved during the
hospital stay. As we mentioned earlier, the NOC
codes (outcomes) are associated with a scale from
1 to 5 which indicates the initial value, the ex-
pected rating, and the actual rating at discharge. If
the relative increase between admission and dis-
charge encompasses more than 2 points on the
scale, it is considered significant; if it encompasses
1 or 2 points, it is considered slight. In those cases
in which more than one outcome is associated with
a diagnosis, but improvement is not uniform, we
include a cue ?On the other hand?. For Patient 9,
in the last POC recorded just before discharge,
Anxiety Level is up 2 points (to 4), whereas Pain
Level is up 3. We also indicate to the patient
if the final rating reached the rating that was ini-
tially hoped for; it did not for Anxiety Level (See
the two sentences starting from Pain level and Vi-
8
# CUIs from # CUIs from # of # of CUI pairs # of CUI pairs
Discharge Notes Nursing POCs common CUIs at Distance 1 at Distance 2
Patient 9 83.00 28.00 0.00 3.00 13.00
Average 90.64 22.43 0.46 3.00 9.11
Table 1: Concept overlap in discharge notes and nursing POCs
You were admitted for atrial fibrillation with rapid ventricular response. Acute onset pain related to pain was treated with
pain management, monitoring of blood pressure, temperature, pulse rate and respiratory rate and administration of analgesic.
Pain level and vital signs have improved significantly and outcomes have met the expectations. On the other hand, level of
anxiety has improved slightly. Cardiac Disease Self-Management, Disease Process (Heart disease), Hypertension Management,
Cardiac Disease Management, Hypertension Management and Treatment Regimen were taught. Low Cardiac Output related
to heart failure was treated with cardiac precautions, monitoring of blood pressure, temperature, pulse rate and respiratory
rate, and dysrhythmia management. Dysrhythmia management is remedy for tachycardia and atrial fibrillation. As a result,
cardiac pump effectiveness, cardiopulmonary status and cardiac tissue perfusion status have improved slightly. Actual Negative
Breathing Pattern related to respiration disorders was treated with respiration monitoring. Respiratory Status has improved
significantly. You have an appointment at Union Medical Center on DATE at TIME. The list of medication is attached to this
discharge.
Figure 4: PatientNarr generated summary (Patient 9)
tal signs [...]. On the other hand, [...]). For the
moment, we do not mention outcomes for which
no improvement, or a setback, has been recorded.
The summary also includes: mentions of edu-
cation that has been imparted; and reminders of
future appointments and of medicines to be taken.
5 Future Work
The research described in this paper lays the foun-
dations for our project, but clearly much work re-
mains to be done. To start with, we plan to build
a corpus of gold-standard summaries, in order to
derive (semi-)automatic models of the informa-
tion to be included from physician notes and from
nursing documentation. The five human authored
summaries we currently have at our disposal are
not sufficient, neither in quality nor (obviously)
in quantity. We intend to inform their generation
via a number of focus groups with all stakeholders
involved: patients, doctors and nurses. To start
with, the five summaries we do have were pre-
sented to the Patient Advisory Board of an unre-
lated project. These two patients noted that all un-
familiar terms should be explained, and that what
the patient should do to improve their health after
discharge, should be included.
Secondly, we will generate lay language
by taking advantage of resources such as the
Consumer Health Vocabulary (Doing-Harris and
Zeng-Treitler, 2011; CHV, 2013), which maps
medical terms to plain-language expressions. Ad-
ditionally, we will pursue more sophisticated ex-
traction and rendering of rhetorical relationships
among events and their outcomes (Mancini et al.,
2007).
Last but not least, we will perform user stud-
ies, both controlled evaluation of our summaries
while still at the development stage, and eventu-
ally longer-term assessments of whether our sum-
maries engender better adherence to medications
and better keeping of follow-up appointments, and
ultimately, better health.
Acknowledgements
We thank three anonymous reviewers for their
helpful comments. For partial financial support,
we acknowledge award NPRP 5-939-1-155 from
the Qatar National Research Fund, the UIC De-
partment of Biomedical and Health Information
Sciences, and the UIC Institute for Translational
Health Informatics.
References
Carol H. Cain, Estee Neuwirth, Jim Bellows, Christi
Zuber, and Jennifer Green. 2012. Patient expe-
riences of transitioning from hospital to home: an
ethnographic quality improvement project. Journal
of Hospital Medicine, 7(5):382?387.
CDC. 2011. Hospital utilization (in
non-federal short-stay hospitals). Cen-
ters for Disease Control and Prevention,
http://www.cdc.gov/nchs/fastats/hospital.htm.
Last accessed on 11/26/2013.
2013. Consumer Health Vocabulary Initiative.
9
http://www.layhealthinformatics.org/. Last ac-
cessed on 5/19/2014.
Barbara Di Eugenio and Nancy L. Green. 2010.
Emerging applications of natural language gener-
ation in information visualization, education, and
health-care. In Nitin Indurkhya and Fred J. Dam-
erau, editors, Handbook of Natural Language Pro-
cessing, Second Edition, chapter 23, pages 557?575.
CRC Press, Taylor and Francis Group, Boca Raton,
FL. ISBN 978-1420085921.
Kristina M. Doing-Harris and Qing Zeng-Treitler.
2011. Computer-assisted update of a consumer
health vocabulary through mining of social network
data. Journal of Medical Internet Research, 13(2).
C. Friedman, L. Shagina, Y. Lussier, and G. Hripc-
sak. 2004. Automated encoding of clinical docu-
ments based on natural language processing. Jour-
nal of the American Medical Informatics Associa-
tion, 11(5):392.
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: A
realisation engine for practical applications. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 90?93. Association for
Computational Linguistics.
N. Green, R. Dwight, K. Navoraphan, and B. Stadler.
2011. Natural language generation of biomedical ar-
gumentation for lay audiences. Argument and Com-
putation, 2(1):23?50.
C. Hallett. 2008. Multi-modal presentation of medical
histories. In IUI ?08: Proceedings of the 13th Inter-
national Conference on Intelligent User Interfaces,
pages 80?89, New York, NY, USA. ACM.
G.M. Keenan, J.R. Stocker, A.T. Geo-Thomas, N.R.
Soparkar, V.H. Barkauskas, and J.A.N.L. Lee. 2002.
The HANDS Project: Studying and Refining the
Automated Collection of a Cross-setting Clinical
Data set. CIN: Computers, Informatics, Nursing,
20(3):89?100.
Saad Mahamood and Ehud Reiter. 2011. Generat-
ing affective natural language for parents of neona-
tal infants. In Proceedings of the 13th European
Workshop on Natural Language Generation, pages
12?21, Nancy, France, September. Association for
Computational Linguistics.
Clara Mancini, Christian Pietsch, and Donia Scott.
2007. Visualising discourse structure in interactive
documents. In Proceedings of the Eleventh Euro-
pean Workshop on Natural Language Generation,
pages 89?92, Saarbru?cken, Germany, June.
NLM. 2009. UMLS Reference Manual. Techni-
cal report, National Library of Medicine, Septem-
ber. http://www.ncbi.nlm.nih.gov/books/NBK9676/
(Last accessed on 12/09/2013).
2014. NNN: Knowledge-based terminologies defin-
ing nursing. http://www.nanda.org/nanda-i-nic-
noc.html. (Last accessed on 5/19/2014).
Douglas P. Olson and Donna M. Windish. 2010. Com-
munication discrepancies between physicians and
hospitalized patients. Archives of Internal Medicine,
170(15):1302?1307.
Franc?ois Portet, Ehud Reiter, Jim Hunter, Somayajulu
Sripada, Yvonne Freer, and Cynthia Sykes. 2009.
Automatic generation of textual summaries from
neonatal intensive care data. Artificial Intelligence,
173:789?816, May.
Ehud Reiter, Roma Robertson, and Liesl Osman. 2003.
Lessons from a failure: Generating tailored smoking
cessation letters. Artificial Intelligence, 144:41?58.
Anne Schneider, Alasdair Mort, Chris Mellish, Ehud
Reiter, Phil Wilson, and Pierre-Luc Vaudry. 2013.
MIME - NLG Support for Complex and Unsta-
ble Pre-hospital Emergencies. In Proceedings of
the 14th European Workshop on Natural Language
Generation, pages 198?199, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Donia Scott, Catalina Hallett, and Rachel Fetti-
place. 2013. Data-to-text summarisation of pa-
tient records: Using computer-generated summaries
to access patient histories. Patient Education and
Counseling, 92(2):153?159.
Sandra Williams, Paul Piwek, and Richard Power.
2007. Generating monologue and dialogue to
present personalised medical information to pa-
tients. In Proceedings of the Eleventh European
Workshop on Natural Language Generation, pages
167?170, Saarbru?cken, Germany, June.
10
