Proceedings of the ACL Student Research Workshop, pages 38?45,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
What causes a causal relation?
Detecting Causal Triggers in Biomedical Scientific Discourse
Claudiu Miha?ila? and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science,
The University of Manchester,
131 Princess Street, Manchester M1 7DN, UK
claudiu.mihaila@manchester.ac.uk
sophia.ananiadou@manchester.ac.uk
Abstract
Current domain-specific information extrac-
tion systems represent an important resource
for biomedical researchers, who need to pro-
cess vaster amounts of knowledge in short
times. Automatic discourse causality recog-
nition can further improve their workload by
suggesting possible causal connections and
aiding in the curation of pathway models. We
here describe an approach to the automatic
identification of discourse causality triggers in
the biomedical domain using machine learn-
ing. We create several baselines and experi-
ment with various parameter settings for three
algorithms, i.e., Conditional Random Fields
(CRF), Support Vector Machines (SVM) and
Random Forests (RF). Also, we evaluate the
impact of lexical, syntactic and semantic fea-
tures on each of the algorithms and look at er-
rors. The best performance of 79.35% F-score
is achieved by CRFs when using all three fea-
ture types.
1 Introduction
The need to provide automated, efficient and accu-
rate means of retrieving and extracting user-oriented
biomedical knowledge has significantly increased
according to the ever-increasing amount of knowl-
edge pusblished daily in the form of research ar-
ticles (Ananiadou and McNaught, 2006; Cohen
and Hunter, 2008). Biomedical text mining has
seen significant recent advancements in recent years
(Zweigenbaum et al, 2007), including named en-
tity recognition (Fukuda et al, 1998), coreference
resolution (Batista-Navarro and Ananiadou, 2011;
Savova et al, 2011) and relation (Miwa et al, 2009;
Pyysalo et al, 2009) and event extraction (Miwa
et al, 2012b; Miwa et al, 2012a). Using biomed-
ical text mining technology, text can now be en-
riched via the addition of semantic metadata and
thus can support tasks such as analysing molecu-
lar pathways (Rzhetsky et al, 2004) and semantic
searching (Miyao et al, 2006).
However, more complex tasks, such as question
answering and automatic summarisation, require the
extraction of information that spans across several
sentences, together with the recognition of relations
that exist across sentence boundaries, in order to
achieve high levels of performance.
The notion of discourse can be defined as a co-
herent sequence of clauses and sentences. These
are connected in a logical manner by discourse re-
lations, such as causal, temporal and conditional,
which characterise how facts in text are related. In
turn, these help readers infer deeper, more com-
plex knowledge about the facts mentioned in the
discourse. These relations can be either explicit
or implicit, depending whether or not they are ex-
pressed in text using overt discourse connectives
(also known as triggers). Take, for instance, the case
in example (1), where the trigger Therefore signals
a justification between the two sentences: because
?a normal response to mild acid pH from PmrB re-
quires both a periplasmic histidine and several glu-
tamic acid residues?, the authors believe that the
?regulation of PmrB activity could involve protona-
tion of some amino acids?.
(1) In the case of PmrB, a normal response to mild
acid pH requires not only a periplasmic histidine
38
but also several glutamic acid residues.
Therefore, regulation of PmrB activity may in-
volve protonation of one or more of these amino
acids.
Thus, by identifying this causal relation, search
engines become able to discover relations between
biomedical entities and events or between experi-
mental evidence and associated conclusions. How-
ever, phrases acting as causal triggers in certain con-
texts may not denote causality in all cases. There-
fore, a dictionary-based approach is likely to pro-
duce a very high number of false positives. In
this paper, we explore several supervised machine-
learning approaches to the automatic identification
of triggers that actually denote causality.
2 Related Work
A large amount of work related to discourse pars-
ing and discourse relation identification exists in the
general domain, where researchers have not only
identified discourse connectives, but also developed
end-to-end discourse parsers (Pitler and Nenkova,
2009; Lin et al, 2012). Most work is based on
the Penn Discourse Treebank (PDTB) (Prasad et al,
2008), a corpus of lexically-grounded annotations of
discourse relations.
Until now, comparatively little work has been car-
ried out on causal discourse relations in the biomed-
ical domain, although causal associations between
biological entities, events and processes are central
to most claims of interest (Kleinberg and Hripcsak,
2011). The equivalent of the PDTB for the biomed-
ical domain is the BioDRB corpus (Prasad et al,
2011), containing 16 types of discourse relations,
e.g., temporal, causal and conditional. The number
of purely causal relations annotated in this corpus is
542. There are another 23 relations which are a mix-
ture between causality and one of either background,
temporal, conjunction or reinforcement relations. A
slightly larger corpus is the BioCause (Miha?ila? et
al., 2013), containing over 850 manually annotated
causal discourse relations in 19 full-text open-access
journal articles from the infectious diseases domain.
Using the BioDRB corpus as data, some re-
searchers explored the identification of discourse
connectives (Ramesh et al, 2012). However, they
do not distinguish between the types of discourse
relations. They obtain the best F-score of 75.7% us-
ing CRF, with SVM reaching only 65.7%. These
results were obtained by using only syntactic fea-
tures, as sematic features were shown to lower the
performance. Also, they prove that there exist dif-
ferences in discourse triggers between the biomedi-
cal and general domains by training a model on the
BioDRB and evaluating it against PDTB and vice-
versa.
3 Methodology
In this section, we describe our data and the features
of causal triggers. We also explain our evaluation
methodology.
3.1 Data
The data for the experiments comes from the Bio-
Cause corpus. BioCause is a collection of 19 open-
access full-text journal articles pertaining to the
biomedical subdomain of infectious diseases, manu-
ally annotated with causal relationships. Two types
of spans of text are marked in the text, namely causal
triggers and causal arguments. Each causal relation
is composed of three text-bound annotations: a trig-
ger, a cause or evidence argument and an effect argu-
ment. Some causal relations have implicit triggers,
so these are excluded from the current research.
Figure 1 shows an example of discourse causality
from BioCause, marking the causal trigger and the
two arguments with their respective relation. Named
entities are also marked in this example.
BioCause contains 381 unique explicit triggers in
the corpus, each being used, on average, only 2.10
times. The number decreases to 347 unique triggers
when they are lemmatised, corresponding to an av-
erage usage of 2.30 times per trigger. Both count
settings show the diversity of causality-triggering
phrases that are used in the biomedical domain.
3.2 Features
Three types of features have been employed in the
development of this causality trigger model, i.e., lex-
ical, syntactic and semantic. These features are cat-
egorised and described below.
3.2.1 Lexical features
The lexical features are built from the actual to-
kens present in text. Tokenisation is performed by
39
Figure 1: Causal relation in the BioCause.
the GENIA tagger (Tsuruoka et al, 2005) using the
biomedical model. The first two features represent
the token?s surface expression and its base form.
Neighbouring tokens have also been considered.
We included the token immediately to the left and
the one immediately to the right of the current to-
ken. This decision is based on two observations.
Firstly, in the case of tokens to the left, most trig-
gers are found either at the beginning of the sentence
(311 instances) or are preceded by a comma (238 in-
stances). These two left contexts represent 69% of
all triggers. Secondly, for the tokens to the right, al-
most 45% of triggers are followed by a determiner,
such as the, a or an, (281 instances) or a comma (71
instances).
3.2.2 Syntactic features
The syntax, dependency and predicate argument
structure are produced by the Enju parser (Miyao
and Tsujii, 2008). Figure 2 depicts a partial lexical
parse tree of a sentence which starts with a causal
trigger, namely Our results suggest that. From the
lexical parse trees, several types of features have
been generated.
The first two features represent the part-of-speech
and syntactic category of a token. For instance,
the figure shows that the token that has the part-of-
speech IN. These features are included due to the
fact that either many triggers are lexicalised as an
adverb or conjunction, or are part of a verb phrase.
For the same reason, the syntactical category path
from the root of the lexical parse tree to the token is
also included. The path also encodes, for each par-
ent constituent, the position of the token in its sub-
tree, i.e., beginning (B), inside (I) or end (E); if the
token is the only leaf node of the constituent, this is
marked differently, using a C. Thus, the path of that,
highlighted in the figure, is I-S/I-VP/B-CP/C-CX.
Secondly, for each token, we extracted the pred-
Figure 2: Partial lexical parse tree of a sentence starting
with a causal trigger.
icate argument structure and checked whether a re-
lation exista between the token and the previous and
following tokens. The values for this feature repre-
sent the argument number as allocated by Enju.
Thirdly, the ancestors of each token to the third
degree are instantiated as three different features. In
the case that such ancestors do not exist (i.e., the
root of the lexical parse tree is less than three nodes
away), a ?none? value is given. For instance, the
token that in Figure 2 has as its first three ancestors
the constituents marked with CX, CP and VP.
Finally, the lowest common ancestor in the lexi-
cal parse tree between the current token and its left
neighbour has been included. In the example, the
lowest common ancestor for that and suggest is VP.
These last two feature types have been produced
on the observation that the lowest common ancestor
for all tokens in a causal trigger is S or VP in over
70% of instances. Furthermore, the percentage of
cases of triggers with V or ADV as lowest common
ancestor is almost 9% in each case. Also, the aver-
40
age distance to the lowest common ancestor is 3.
3.2.3 Semantic features
We have exploited several semantic knowledge
sources to identify causal triggers more accurately,
as a mapping to concepts and named entities acts as
a back-off smoothing, thus increasing performance.
One semantic knowledge source is the BioCause
corpus itself. All documents annotated for causal-
ity in BioCause had been previously manually an-
notated with biomedical named entity and event in-
formation. This was performed in the context of var-
ious shared tasks, such as the BioNLP 2011 Shared
Task on Infectious Diseases (Pyysalo et al, 2011).
We therefore leverage this existing information to
add another semantic layer to the model. More-
over, another advantage of having a gold standard
annotation is the fact that it is now possible to sepa-
rate the task of automatic causal trigger recognition
from automatic named entity recognition and event
extraction. The named entity and event annotation
in the BioCause corpus is used to extract informa-
tion about whether a token is part of a named entity
or event trigger. Furthermore, the type of the named
entity or event is included as a separate feature.
The second semantic knowledge source is Word-
Net (Fellbaum, 1998). Using this resource, the hy-
pernym of every token in the text has been included
as a feature. Only the first sense of every token has
been considered, as no sense disambiguation tech-
nique has been employed.
Finally, tokens have been linked to the Unified
Medical Language System (UMLS) (Bodenreider,
2004) semantic types. Thus, we included a feature
to say whether a token is part of a UMLS type and
another for its semantic type if the previous is true.
3.3 Experimental setup
We explored with various machine learning algo-
rithms and various settings for the task of identifying
causal triggers.
On the one hand, we experimented with CRF
(Lafferty et al, 2001), a probabilistic modelling
framework commonly used for sequence labelling
tasks. In this work, we employed the CRFSuite im-
plementation1.
1http://www.chokkan.org/software/
crfsuite
On the other hand, we modelled trigger detection
as a classification task, using Support Vector Ma-
chines and Random Forests. More specifically, we
employed the implementation in Weka (Hall et al,
2009; Witten and Frank, 2005) for RFs, and Lib-
SVM (Chang and Lin, 2011) for SVMs.
4 Results and discussion
Several models have been developed and 10-fold
cross-evaluated to examine the complexity of the
task, the impact of various feature types (lexical,
syntactic, semantic). Table 1 shows the performance
evaluation of baseline systems and other classifiers.
These are described in the following subsections. It
should be noted that the dataset is highly skewed,
with a ratio of positive examples to negative exam-
ples of approximately 1:52.
Classifier P R F1
Ba
sel
ine Dict 8.36 100 15.43Dep 7.51 76.66 13.68
Dict+Dep 14.30 75.33 24.03
2-w
ay CRF 89.29 73.53 79.35SVM 81.62 61.05 69.85
RandFor 78.16 66.96 72.13
3-w
ay CRF 89.13 64.04 72.87SVM 74.21 56.82 64.36
RandFor 73.80 60.95 66.76
Table 1: Performance of various classifiers in identifying
causal connectives
4.1 Baseline
Several baselines have been devised. The first base-
line is a dictionary-based heuristic, named Dict. A
lexicon is populated with all annotated causal trig-
gers and then this is used to tag all instances of its
entries in the text as connectives. The precision of
this heuristic is very low, 8.36%, which leads to an
F-score of 15.43%, considering the recall is 100%.
This is mainly due to triggers which are rarely used
as causal triggers, such as and, by and that.
Building on the previously mentioned observation
about the lowest common ancestor for all tokens in a
causal trigger, we built a baseline system that checks
all constituent nodes in the lexical parse tree for the
S, V, VP and ADV tags and marks them as causal
41
triggers. The name of this system is Dep. Not only
does Dep obtain a lower precision than Dict, but it
also performs worse in terms of recall. The F-score
is 13.68%, largely due to the high number of inter-
mediate nodes in the lexical parse tree that have VP
as their category.
The third baseline is a combination of Dict and
Dep: we consider only constituents that have the
necessary category (S, V, VP or ADV) and include
a trigger from the dictionary. Although the recall
decreases slightly, the precision increases to almost
twice that of both Dict and Dep. This produces a
much better F-score of 24.03%.
4.2 Sequence labelling task
As a sequence labelling task, we have modelled
causal trigger detection as two separate tasks.
Firstly, each trigger is represented in the B-I-O for-
mat (further mentioned as the 3-way model). Thus,
the first word of every trigger is tagged as B (be-
gin), whilst the following words in the trigger are
tagged as I (inside). Non-trigger words are tagged
as O (outside).
The second model is a simpler version of the pre-
vious one: it does not distinguish between the first
and the following words in the trigger. In other
words, each word is tagged either as being part of
or outside the trigger, further known as the 2-way
model. Hence, a sequence of contiguous tokens
marked as part of a trigger form one trigger.
CRF performs reasonably well in detecting causal
triggers. In the 3-way model, it obtains an F-score of
almost 73%, much better than the other algorithms.
It also obtains the highest precision (89%) and recall
(64%). However, in the 2-way model, CRF?s perfor-
mance is slightly lower than that of Random Forests,
achieving only 79.35%. Its precision, on the other
hand, is the highest in this model. The results from
both models were obtained by combining features
from all three feature categories.
Table 2 show the effect of feature types on both
models of CRFs. As can be observed, the best per-
formances, in terms of F-score, including the previ-
ously mentioned ones, are obtained when combin-
ing all three types of features, i.e., lexical, syntactic
and semantic. The best precision and recall, how-
ever, are not necessarily achieved by using all three
feature types. In the two-way model, the best preci-
Features P R F1
2-w
ay
Lex 88.99 67.09 73.59
Syn 92.20 68.68 75.72
Sem 87.20 63.30 69.36
Lex-Syn 87.76 73.29 78.73
Lex+Sem 89.54 69.10 75.61
Syn+Sem 87.48 72.62 78.13
Lex-Syn-Sem 89.29 73.53 79.35
3-w
ay
Lex 85.87 56.34 65.18
Syn 87.62 61.44 70.22
Sem 80.78 51.43 59.39
Lex+Syn 87.80 63.04 72.59
Lex+Sem 85.50 58.11 66.80
Syn+Sem 84.83 64.94 72.41
Lex-Syn-Sem 89.13 64.04 72.87
Table 2: Effect of feature types on the sequence labelling
task, given in percentages.
sion is obtained by using the syntactic features only,
reaching over 92%, almost 3% higher than when all
three feature types are used. In the three-way model,
syntactic and semantic features produce the best re-
call (almost 65%), which is just under 1% higher
than the recall when all features are used.
4.3 Classification task
As a classification task, an algorithm has to decide
whether a token is part of a trigger or not, similarly
to the previous two-way subtask in the case of CRF.
Firstly, we have used RF for the classification
task. Various parameter settings regarding the num-
ber of constructed trees and the number of random
features have been explored.
The effect of feature types on the performance of
RF is shown in Table 3. As can be observed, the
best performance is obtained when combining lexi-
cal and semantic features. Due to the fact that causal
triggers do not have a semantic mapping to concepts
in the named entity and UMLS annotations, the trees
in the random forest classifier can easily produce
rules that distinguish triggers from non-triggers. As
such, the use of semantic features alone produce a
very good precision of 84.34%. Also, in all cases
where semantic features are combined with other
feature types, the precision increases by 0.5% in the
case of lexical features and 3.5% in the case of syn-
tactic features. However, the recall of semantic fea-
42
tures alone is the lowest. The best recall is obtained
when using only lexical features.
Features P R F1
Lex 78.47 68.30 73.03
Syn 68.19 62.36 65.15
Sem 84.34 56.83 67.91
Lex+Syn 77.11 65.92 71.09
Lex+Sem 79.10 67.91 73.08
Syn+Sem 71.83 64.45 67.94
Lex+Syn+Sem 77.98 67.31 72.25
Table 3: Effect of feature types on Random Forests.
Secondly, we explored the performance of SVMs
in detecting causal triggers. We have experimented
with two kernels, namely polynomial (second de-
gree) and radial basis function (RBF) kernels. For
each of these two kernels, we have evaluated vari-
ous combinations of parameter values for cost and
weight. Both these kernels achieved similar results,
indicating that the feature space is not linearly sepa-
rable and that the problem is highly complex.
The effect of feature types on the performance of
SVMs is shown in Table 4. As can be observed,
the best performance is obtained when combining
the lexical and semantic feature types (69.85% F-
score). The combination of all features produces the
best precision, whilst the best recall is obtained by
combining lexical and semantic features.
Features P R F1
Lex 80.80 60.94 69.47
Syn 82.94 55.60 66.57
Sem 85.07 56.51 67.91
Lex+Syn 86.49 53.63 66.81
Lex+Sem 81.62 61.05 69.85
Syn+Sem 84.49 55.31 66.85
Lex+Syn+Sem 87.70 53.96 66.81
Table 4: Effect of feature types on SVM.
4.4 Error analysis
As we expected, the majority of errors arise from se-
quences of tokens which are only used infrequently
as non-causal triggers. This applies to 107 trigger
types, whose number of false positives (FP) is higher
than the number of true positives (TP). In fact, 64
trigger types occur only once as a causal instance,
whilst the average number of FPs for these types is
14.25. One such example is and, for which the num-
ber of non-causal instances (2305) is much greater
than that of causal instances (1). Other examples
of trigger types more commonly used as causal trig-
gers, are suggesting (9 TP, 54 FP), indicating (8 TP,
41 FP) and resulting in (6 TP, 14 FP). For instance,
example (2) contains two mentions of indicating, but
neither of them implies causality.
(2) Buffer treated control cells showed intense
green staining with syto9 (indicating viabil-
ity) and a lack of PI staining (indicating no
dead/dying cells or DNA release).
5 Conclusions and Future Work
We have presented an approach to the automatic
identification of triggers of causal discourse rela-
tions in biomedical scientific text. The task has
proven to be a highly complex one, posing many
challenges. Shallow approaches, such as dictionary
matching and lexical parse tree matching, perform
very poorly, due to the high ambiguity of causal
triggers (with F-scores of approximately 15% each
and 24% when combined). We have explored vari-
ous machine learning algorithms that automatically
classify tokens into triggers or non-triggers and we
have evaluated the impact of multiple lexical, syn-
tactic and semantic features. The performance of
SVMs prove that the task of identifying causal trig-
gers is indeed complex. The best performing classi-
fier is CRF-based and combines lexical, syntactical
and semantical features in order to obtain an F-score
of 79.35%.
As future work, integrating the causal relations in
the BioDRB corpus is necessary to check whether a
data insufficiency problem exists and, if so, estimate
the optimal amount of necessary data. Furthermore,
evaluations against the general domain need to be
performed, in order to establish any differences in
expressing causality in the biomedical domain. One
possible source for this is the PDTB corpus. A more
difficult task that needs attention is that of identify-
ing implicit triggers. Finally, our system needs to be
extended in order to identify the two arguments of
43
causal relations, the cause and effect, thus allowing
the creation of a complete discourse causality parser.
Acknowledgements
This work was partially funded by the Engineer-
ing and Physical Sciences Research Council [grant
number EP/P505631/1].
References
Sophia Ananiadou and John McNaught, editors. 2006.
Text Mining for Biology And Biomedicine. Artech
House, Inc.
Riza Theresa B. Batista-Navarro and Sophia Anani-
adou. 2011. Building a coreference-annotated corpus
from the domain of biochemistry. In Proceedings of
BioNLP 2011, pages 83?91.
Olivier Bodenreider. 2004. The unified medical lan-
guage system (UMLS): integrating biomedical termi-
nology. Nucleic Acids Research, 32(suppl 1):D267?
D270.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Kevin Bretonnel Cohen and Lawrence Hunter. 2008.
Getting started in text mining. PLoS Computational
Biology, 4(1):e20, 01.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Ken-ichiro Fukuda, Tatsuhiko Tsunoda, Ayuchi Tamura,
and Toshihisa Takagi. 1998. Toward information ex-
traction: Identifying protein names from biological pa-
pers. In Proceedings of the Pacific Symposium on Bio-
computing, volume 707, pages 707?718.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18, November.
Samantha Kleinberg and George Hripcsak. 2011. A re-
view of causal inference for biomedical informatics.
Journal of Biomedical Informatics, 44(6):1102 ? 1112.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A
pdtb-styled end-to-end discourse parser. Natural Lan-
guage Engineering, FirstView:1?34, 10.
Claudiu Miha?ila?, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. BioCause: Annotating and
analysing causality in the biomedical domain. BMC
Bioinformatics, 14(1):2, January.
Makoto Miwa, Rune S?tre, Yusuke Miyao, and Jun?ichi
Tsujii. 2009. Protein-protein interaction extraction by
leveraging multiple kernels and parsers. International
Journal of Medical Informatics, 78(12):e39?e46, June.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012a. Boosting automatic event extraction from the
literature using domain adaptation and coreference
resolution. Bioinformatics, 28(13):1759?1765.
Makoto Miwa, Paul Thompson, John McNaught, Dou-
glas B. Kell, and Sophia Ananiadou. 2012b. Extract-
ing semantically enriched events from biomedical lit-
erature. BMC Bioinformatics, 13:108.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Computa-
tional Linguistics, 34(1):3580, March.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ninomiya,
and Jun?ichi Tsujii. 2006. Semantic retrieval for the
accurate identification of relational concepts in mas-
sive textbases. In ACL.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text. In
ACL/AFNLP (Short Papers), pages 13?16.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odjik, Stelios Piperidis,
and Daniel Tapias, editors, In Proceedings of the 6th
International Conference on language Resources and
Evaluation (LREC), pages 2961?2968.
Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind
Joshi, and Hong Yu. 2011. The biomedical discourse
relation bank. BMC Bioinformatics, 12(1):188.
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static relations: a piece in the
biomedical information extraction puzzle. In Proceed-
ings of the Workshop on Current Trends in Biomedical
Natural Language Processing, BioNLP ?09, pages 1?
9, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the infectious diseases (ID) task of
BioNLP shared task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 26?35,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
44
Polepalli Balaji Ramesh, Rashmi Prasad, Tim Miller,
Brian Harrington, and Hong Yu. 2012. Automatic dis-
course connective detection in biomedical text. Jour-
nal of the American Medical Informatics Association.
Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,
Michael Krauthammer, Pauline Kra, Mitzi Morris,
Hong Yu, Ariel Pablo Duboue?, Wubin Weng, W.John
Wilbur, Vasileios Hatzivassiloglou, and Carol Fried-
man. 2004. Geneways: a system for extracting, ana-
lyzing, visualizing, and integrating molecular pathway
data. Journal of Biomedical Informatics, 37(1):43 ?
53.
Guergana K Savova, Wendy W Chapman, Jiaping Zheng,
and Rebecca S Crowley. 2011. Anaphoric rela-
tions in the clinical narrative: corpus creation. Jour-
nal of the American Medical Informatics Association,
18(4):459?465.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances
in Informatics - 10th Panhellenic Conference on In-
formatics, volume 3746 of LNCS, pages 382?392.
Springer-Verlag, Volos, Greece, November.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques (Second
Edition). Morgan Kaufmann.
Pierre Zweigenbaum, Dina Demner-Fushman, Hong Yu,
and Kevin B. Cohen. 2007. Frontiers of biomedical
text mining: current progress. Briefings in Bioinfor-
matics, 8(5):358?375.
45
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43?48,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extending an interoperable platform to facilitate the creation
of multilingual and multimodal NLP applications
Georgios Kontonatsios?, Paul Thompson?, Riza Theresa Batista-Navarro?,
Claudiu Miha?ila??, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
131 Princess Street, Manchester M1 7DN, UK
{kontonag,batistar,thompsop,mihailac,
korkonti,ananiads}@cs.man.ac.uk
Abstract
U-Compare is a UIMA-based workflow
construction platform for building natu-
ral language processing (NLP) applica-
tions from heterogeneous language re-
sources (LRs), without the need for pro-
gramming skills. U-Compare has been
adopted within the context of the META-
NET Network of Excellence, and over
40 LRs that process 15 European lan-
guages have been added to the U-Compare
component library. In line with META-
NET?s aims of increasing communication
between citizens of different European
countries, U-Compare has been extended
to facilitate the development of a wider
range of applications, including both mul-
tilingual and multimodal workflows. The
enhancements exploit the UIMA Subject
of Analysis (Sofa) mechanism, that allows
different facets of the input data to be rep-
resented. We demonstrate how our cus-
tomised extensions to U-Compare allow
the construction and testing of NLP appli-
cations that transform the input data in dif-
ferent ways, e.g., machine translation, au-
tomatic summarisation and text-to-speech.
1 Introduction
Currently, there are many repositories that con-
tain a range of NLP components, e.g., OpenNLP1,
Stanford CoreNLP2, JULIE NLP Toolsuite3 and
NaCTeM software tools4. The ability to chain
components from these repositories into pipelines
is a prerequisite to facilitate the development of
?The authors have contributed equally to the development
of this work and production of the manuscript.
1http://opennlp.sourceforge.net/projects.html
2http://nlp.stanford.edu/software/corenlp.shtml
3http://www.julielab.de/Resources/Software/NLP Tools.html
4http://nactem.ac.uk/software.php
complex NLP applications. Combining together
heterogeneous components is not, however, al-
ways straightforward. The various components
used in a pipeline may be implemented using dif-
ferent programming languages, may have incom-
patible input/output formats, e.g., stand-off or in-
line annotations, or may require or produce incom-
patible data types, e.g., a particular named entity
recogniser (NER) may require specific types of
syntactic constituents as input, making it impor-
tant to choose the right type of syntactic parser to
run prior to the NER. Thus, the tools required to
build a new application may not be interoperable
with each other, and considerable extra work may
be required to make the tools talk to each other.
The Unstructured Information Management Ar-
chitecture (UIMA) (Ferrucci and Lally, 2004) was
created as a means to alleviate such problems. It
is a framework that facilitates the straightforward
combination of LRs, i.e., tools and corpora, into
workflow applications. UIMA is an OASIS stan-
dard that enables interoperability of LRs by defin-
ing a standard workflow metadata format and stan-
dard input/output representations.
U-Compare (Kano et al, 2011) is a graphical
NLP workflow construction platform built on top
of UIMA. It facilitates the rapid construction, test-
ing and evaluation of NLP workflows using drag-
and-drop actions within its graphical user inter-
face (GUI). U-Compare enhances interoperabil-
ity among UIMA-compliant LRs, by defining a
common and sharable Type System, i.e., a hier-
archy of annotation types, which models a wide
range of NLP data types, e.g., sentence, token,
part-of-speech tag, named entity and discourse
annotations. The aim is for all components in
U-Compare?s library to be compliant with this
type system. In the context of META-NET, U-
Compare?s library has been extended with 46 new
LRs supporting 15 European languages, all of
which are compliant with the same type system.
43
This makes U-Compare the world?s largest repos-
itory of type system-compatible LRs, allowing
users to seamlessly combine together resources to
create a range of NLP applications.
Previously, U-Compare was able to support the
development of a wide range of monolingual lex-
ical, syntactic and semantic processing tasks ap-
plications that enriched textual input documents
by adding annotations of various types. However,
not all NLP applications operate in this way; some
workflows transform the input data to create new
?views? of the input data. The META-NET project
aims to ensure equal access to information by all
European citizens. This aim implies the devel-
opment of both multilingual applications, which
transform input data from one language into an-
other, or multimodal applications, in which text
may be transformed into speech, or vice versa.
U-Compare has been extended in several ways
to support the construction of these more complex
workflow types. Specifically, information about
both the original and transformed data, together
with annotations associated with each view, can
now be visualised in a straightforward manner.
The changes support two new categories of work-
flow. Firstly, workflows that produce two or more
textual views of an input text are useful not only
for multilingual applications, such as those that
carry out machine translation, but also applica-
tions that transform the input text in other ways,
such as those that produce a summary of an in-
put text. Secondly, workflows that output audio as
well as textual views, e.g., text-to-speech applica-
tions, are also supported.
2 Related work
Over the past few years, an increasing num-
bers of researchers have begun to create and dis-
tribute their own workflow construction architec-
tures (Ferrucci and Lally, 2004; Cunningham et
al., 2002; Grishman et al, 1997; Scha?fer, 2006)
or platforms (Kano et al, 2011; Rak et al, 2012;
Ogrodniczuk and Karagiozov, 2011; Savova et al,
2010) that allow the rapid development of NLP ap-
plications.
GATE (Cunningham et al, 2002) is a workflow
construction framework that has been used to de-
velop several types of NLP applications, including
summarisation systems. It facilitates the develop-
ment of a wide range of NLP applications by pro-
viding a collection of components that can process
various languages, together with Java libraries that
handle character encoding for approximately 100
languages. However, GATE does not formally de-
fine any standards to model multilingual or mul-
timodal applications, but rather aims to boost the
development process of NLP applications.
TIPSTER (Grishman et al, 1997) is a generic
framework for the development of NLP applica-
tions. TIPSTER provides multilingual function-
alities by associating text segments of a paral-
lel document with one or more languages. This
allows language-dependent NLP components to
process only the appropriate mono-lingual sub-
documents. However, TIPSTER does not provide
explicit guidelines regarding the annotation types
and attributes that are produced by components.
This lack of a common and sharable system of
annotation types discourages interoperability be-
tween LRs. However, TIPSTER does not provide
a mechanism that facilitates the development of
multilingual or multimodal NLP applications.
Heart of Gold (Scha?fer, 2006) is an XML-
based workflow construction architecture that en-
ables interoperability of tools developed in dif-
ferent programming languages to be combined
into pipelines. Heart of Gold contains a rich li-
brary of shallow and deep parsing components
supporting several languages, e.g., English, Ger-
man, Japanese and Greek. Nonetheless, Heart of
Gold does not specifically support the construction
of multilingual or multimodal workflows.
In contrast to the other frameworks introduced
above, UIMA (Ferrucci and Lally, 2004) provides
an abstract-level mechanism that can be used to
support the development of workflows that carry
out transformations of the input data. This mech-
anism is called the Subject of Analysis or Sofa.
Multiple Sofas can be linked with an input file,
each of which stores different data and associ-
ated annotations. This mechanism can thus be ex-
ploited to represent alternative ?views? of the in-
put data, such as a source text and its translation.
The data stored in different Sofas is not restricted
to textual information; it can also correspond to
other modalities, such as audio data. This makes
the Sofa mechanism equally suitable for storing
the output of text-to-speech workflows. Our ex-
tensions to U-Compare are thus implemented by
reading and displaying the contents of different
types of Sofas.
The Sofa mechanism has previously been
44
under-exploited by UIMA developers, despite its
power in allowing more complex NLP workflows
to be constructed. Indeed, no other existing
UIMA-based platform (Kano et al, 2011; Rak et
al., 2012; Savova et al, 2010; Hahn et al, 2008)
has demonstrated the use of Sofas to construct
multilingual or multimodal applications. Thus, to
our knowledge, our enhancements to U-Compare
constitute the first attempt to make the construc-
tion of workflows that carry out transformations of
input data more readily available to UIMA users,
without the need for programming skills.
3 METANET4U Components in
U-Compare
The two dozen national and many regional lan-
guages of Europe present linguistic barriers that
can severely limit the free flow of goods, infor-
mation and services. The META-NET Network
of Excellence was created to respond to this is-
sue. Consisting of 60 research centres from 34
countries, META-NET has aimed to stimulate a
concerted, substantial and continent-wide effort to
push forward language technology research and
engineering, in order to ensure equal access to
information and knowledge for all European cit-
izens.
META-NET?s aims are dependent on the ready
availability of LRs that can carry out NLP and
text mining (TM) on a range of European lan-
guages. Such resources constitute the building
blocks for constructing language technology ap-
plications that can help European citizens to gain
easy access to the information they require. One
of the major outcomes of META-NET has been
the development of META-SHARE, an open, dis-
tributed facility for sharing and exchange of LRs
in a large number of European languages.
Within the context of META-NET, interoper-
ability of LRs is clearly of utmost importance, to
expedite the process of developing new NLP ap-
plications. In order to provide a concrete demon-
stration of the utility and power of promoting in-
teroperability within META-SHARE, one of the
sub-projects of META-NET, i.e., METANET4U,
has carried out a pilot study on interoperability,
making use of the UIMA framework and the U-
Compare platform. It is in this context that a set
of 46 new LRs, available in META-SHARE, were
wrapped as UIMA components and made avail-
able in U-Compare. Of these components, 37 op-
erate on one or more specific languages other than
English and 4 are language-independent. Table 1
shows the full set of categories of UIMA com-
ponents created during the METANET4U project,
together with the languages supported.
Several of these new components output mul-
tiple Sofas, i.e., two machine translation compo-
nents, two automatic summarisation components
and a text-to-speech component. It is hoped that
our U-Compare extensions will help to stimulate
the development of a greater number of related
UIMA components, and thus promote a new level
of complexity for future UIMA workflows.
Component Function Supported Languages
Language Identifier 54 modern languages
Paragraph breaker pt, mt
Sentence splitter en, pt ,mt, es, ca, ast,cy, gl, it
Tokeniser en, pt, mt, es, ca, ast,cy, gl, it, fr
Morph. Analyser en, pt, es, ca, ast,cy, gl, it, ro, eu, fr
POS Tagger en, es, ca, cy, gl, it,pt, ro, eu, fr, mt
Syntactic chunker en, es, ca, gl,ast, ro, fr
NP chunker ro
Segmenter ro, en
FDG Parser ro
Dependency Parser en, es, ca, gl, ast
Discourse Parser ro
NER Languageindependent
Summariser ro, en
Machine translation es?{gl,pt,ca}en?es, eu?es
Table 1: METANET4U UIMA components
4 Enhancements to U-Compare
In UIMA, an artefact, i.e., raw text, audio, im-
age, video, and its annotations, e.g., part-of-
speech tags, are represented in a standard format,
namely the Common Analysis Structure (CAS).
A CAS can contain any number of smaller sub-
CASes, i.e., Sofas, that carry different artefacts
with their linked annotations. Figure 1 illustrates
the different types of Sofas that are created by the
three types of workflows that we will demonstrate.
Firstly, for a machine translation workflow, at least
45
Multi-lingualMulti-modalWorkflowsDocuments aZ ??
CAS
?CASCAS
SOFA SOFA
SOFA SOFA
SOFA SOFA
Figure 1: UIMA based multilingual and multi-
modal workflow architecture
two CAS views, i.e., Sofas, are created, the first
corresponding to the text in the source language,
and the other Sofas corresponding to the transla-
tion(s) of the source text into target language(s).
The second type of workflow, i.e., automatic sum-
marisation, is related to the former workflow, in
that the two Sofas produced by the workflow are
both textual, one containing the input text and one
containing a summary of the original text. The
third type of workflow is different, in that a Sofa
containing audio data is used to represent the out-
put of a multimodal workflow.
Two specific extensions have been made to U-
Compare to handle both textual and audio So-
fas. When the output of a workflow consists of
multiple textual views (Sofas), the default anno-
tation viewer is automatically split to allow mul-
tiple views of the text to be displayed and side-
by-side. This can be useful, e.g., to allow careful
comparison of a source text and target translation
in a machine translation workflow. To handle au-
dio Sofas, we have developed a new, customised
viewer that can visualise and play audio data. The
visualisation consists of a graphical display of the
waveform, power information and spectrogram, as
well as segmentation of the audio data into re-
gions (such as individual tokens) and transcrip-
tions, if such information is present in the audio
Sofa. The viewer makes use the open-source li-
brary Java Speech Toolkit (JSTK)5.
5 Workflow applications
In order to provide a practical demonstration of
the enhanced capabilities of U-Compare, we show
5http://code.google.com/p/jstk
three different workflows that transform the input
data in different ways, namely translation, auto-
matic summarisation and speech synthesis. In this
section, we provide brief details of these work-
flows.
5.1 Machine translation
The University of Manchester has created UIMA
wrapper components corresponding to different
modules of Apertium (Corb??-Bellot et al, 2005), a
free rule-based machine translation engine. These
components consist of a morphological analyser,
POS tagger and translator. The three components
must be run in sequence to carry out translation,
although the first two components can be used
in other workflows to carry out monolingual
analyses. The UIMA components currently
handle a subset of the 27 languages dealt with
by the complete Apertium system, corresponding
to the languages of the METANET4U partners,
i.e., English?Spanish, Galician?Spanish,
Portuguese?Spanish, Catalan?Spanish and
Basque?Spanish. However, additional language
pairs can be added straightforwardly. Our sample
workflow includes as its initial component the
Language Identifier from the Romanian Academy
Research Institute for Artificial Intelligence
(RACAI), to automatically detect the language of
the text in the input Sofa. The subsequent compo-
nents in the workflow are the Apertium modules.
The workflow demonstrates how heterogeneous
components from different research groups can
be combined into workflows to create new NLP
applications. A sample output from running the
workflow is shown in Figure 2. The input text
was detected as English by the RACAI Language
Identifier. The English text was subsequently
analysed by the morphological analyser and POS
Tagger, and translated to Spanish by the translator.
Figure 2 illustrates the side-by-side display of the
contents of the two Sofas.
5.2 Automatic summarisation
Automatic summarisation for Romanian text can
be carried out by creating a workflow consisting
of two components developed by the Universitatea
?Alexandru Ioan Cuza? din Ias?i (UAIC). Firstly,
a segmenter (UAICSeg) splits the input text into
fragments, which are in turn used as input to the
summariser component (UAICSum). The length
of the output summary (percentage of the whole
document) is parameterised. As can be seen in
46
Figure 2: Translation of English text to Spanish
Figure 3: Summarisation of Romanian text
Figure 3, the output of this workflow is displayed
using the same parallel Sofa viewer. In this case,
the full text is displayed in the left-hand pane and
the summary is shown in the right-hand pane.
5.3 Speech synthesis
The Universitat Polite`cnica de Catalunya (UPC)
developed a speech synthesiser component that
is based around their Ogmios text-to-speech sys-
tem (Bonafonte et al, 2006). The UIMA com-
ponent version of this tool generates separate text
and audio Sofas; the former stores the textual to-
kens and textual representations of their pronun-
ciations, whilst the latter stores the start and end
time offsets of each of the tokens in the audio file,
together with their transcriptions. Fig. 4 shows
how the textual Sofa information is displayed in
U-Compare?s default annotation viewer, whilst the
audio Sofa information is shown in the new au-
dio visualiser mentioned above. The three differ-
ent types of visual information are displayed be-
low each other, and the segments (tokens) of the
audio file, together with their transcriptions, are
displayed at the bottom of the window. A ?Play?
button allows either the complete file or a selected
segment to be played.
6 Conclusions
The requirements of META-NET have motivated
several new enhancements to the U-Compare plat-
form, which, to our knowledge, make it the first
UIMA-based workflow construction platform that
is fully geared towards the development of NLP
applications that support a wide range of European
languages. The 46 new UIMA-wrapped LRs that
have been made available through U-Compare,
supporting 15 different European languages and
all compliant with the same type system, mean
that the improved U-Compare is essentially a hub
of multilingual resources, which can be freely and
flexibly combined to create new workflows. In
47
Figure 4: Speech Synthesis
addition, our enhancements to U-Compare mean
that various types of multilingual and multimodal
workflows can now be created with the minimum
effort. These enhancements are intended to make
U-Compare more attractive to users, and to help
stimulate the development of a new generation of
more complex UIMA-based NLP applications. As
future work, we intend to extend the library of
components that output multiple Sofas, and further
extend the functionalities of U-Compare to handle
other data modalities, e.g., video.
Acknowledgements
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; MetaNet4U project (ICT PSP Pro-
gramme) [grant number 270893]; and Engineer-
ing and Physical Sciences Research Council [grant
numbers EP/P505631/1, EP/J50032X/1].
References
A. Bonafonte, P. Agu?ero, J. Adell, J. Pe?rez, and
A. Moreno. 2006. Ogmios: The upc text-to-speech
synthesis system for spoken translation. In TC-
STAR Workshop on Speech-to-Speech Translation,
pages 199?204.
A. Corb??-Bellot, M. Forcada, S. Ortiz-Rojas, J. Pe?rez-
Ortiz, G. Ram??rez-Sa?nchez, F. Sa?nchez-Mart??nez,
I. Alegria, A. Mayor, and K. Sarasola. 2005.
An open-source shallow-transfer machine transla-
tion engine for the romance languages of Spain. In
Proceedings of the 10th Conference of the EAMT,
pages 79?86.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: an architecture for devel-
opment of robust HLT applications.
D. Ferrucci and A. Lally. 2004. Building an ex-
ample application with the unstructured information
management architecture. IBM Systems Journal,
43(3):455?475.
R. Grishman, B. Caid, J. Callan, J. Conley, H. Corbin,
J. Cowie, K. DiBella, P. Jacobs, M. Mettler, B. Og-
den, et al 1997. TIPSTER text phase ii architecture
design version 2.1 p 19 june 1996.
U. Hahn, E. Buyko, R. Landefeld, M. Mu?hlhausen,
M. Poprat, K. Tomanek, and J. Wermter. 2008. An
overview of JCoRe, the JULIE lab UIMA compo-
nent repository. In LREC?08 Workshop ?Towards
Enhanced Interoperability for Large HLT Systems:
UIMA for NLP?, pages 1?7, Marrakech, Morocco,
May.
Y. Kano, M. Miwa, K. Cohen, L. Hunter, S. Ananiadou,
and J. Tsujii. 2011. U-compare: A modular nlp
workflow construction and evaluation system. IBM
Journal of Research and Development, 55(3):11.
M. Ogrodniczuk and D. Karagiozov. 2011. Atlas - the
multilingual language processing platform. Proce-
samiento de Lenguaje Natural, 47(0):241?248.
R. Rak, A. Rowley, W. Black, and S. Ananiadou.
2012. Argo: an integrative, interactive, text mining-
based workbench supporting curation. Database:
The Journal of Biological Databases and Curation,
2012.
G. Savova, J. Masanz, P. Ogren, J. Zheng, S. Sohn,
K. Kipper-Schuler, and C. Chute. 2010. Mayo clin-
ical text analysis and knowledge extraction system
(ctakes): architecture, component evaluation and ap-
plications. Journal of the American Medical Infor-
matics Association, 17(5):507?513.
U. Scha?fer. 2006. Middleware for creating and com-
bining multi-dimensional nlp markup. In Proceed-
ings of the 5th Workshop on NLP and XML: Multi-
Dimensional Markup in Natural Language Process-
ing, pages 81?84. ACL.
48
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 79?88,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Towards a Better Understanding of Discourse:
Integrating Multiple Discourse Annotation Perspectives Using UIMA
Claudiu Miha?ila??, Georgios Kontonatsios?, Riza Theresa Batista-Navarro?,
Paul Thompson?, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
{mihailac,kontonag,batistar,thompsop,
korkonti,ananiads}@cs.man.ac.uk
Abstract
There exist various different discourse an-
notation schemes that vary both in the
perspectives of discourse structure consid-
ered and the granularity of textual units
that are annotated. Comparison and inte-
gration of multiple schemes have the po-
tential to provide enhanced information.
However, the differing formats of cor-
pora and tools that contain or produce
such schemes can be a barrier to their
integration. U-Compare is a graphical,
UIMA-based workflow construction plat-
form for combining interoperable natu-
ral language processing (NLP) resources,
without the need for programming skills.
In this paper, we present an extension
of U-Compare that allows the easy com-
parison, integration and visualisation of
resources that contain or output annota-
tions based on multiple discourse anno-
tation schemes. The extension works by
allowing the construction of parallel sub-
workflows for each scheme within a single
U-Compare workflow. The different types
of discourse annotations produced by each
sub-workflow can be either merged or vi-
sualised side-by-side for comparison. We
demonstrate this new functionality by us-
ing it to compare annotations belonging
to two different approaches to discourse
analysis, namely discourse relations and
functional discourse annotations. Integrat-
ing these different annotation types within
an interoperable environment allows us to
study the correlations between different
types of discourse and report on the new
insights that this allows us to discover.
?The authors have contributed equally to the development
of this work and production of the manuscript.
1 Introduction
Over the past few years, there has been an increas-
ing sophistication in the types of available natural
language processing (NLP) tools, with named en-
tity recognisers being complemented by relation
and event extraction systems. Such relations and
events are not intended to be understood in isola-
tion, but rather they are arranged to form a coher-
ent discourse. In order to carry out complex tasks
such as automatic summarisation to a high degree
of accuracy, it is important for systems to be able
to analyse the discourse structure of texts automat-
ically. To facilitate the development of such sys-
tems, various textual corpora containing discourse
annotations have been made available to the NLP
community. However, there is a large amount of
variability in the types of annotations contained
within these corpora, since different perspectives
on discourse have led to the development of a
number of different annotation schemes.
Corpora containing discourse-level annotations
usually treat the text as a sequence of coherent tex-
tual zones (e.g., clauses and sentences). One line
of research has been to identify which zones are
logically connected to each other, and to charac-
terise these links through the assignment of dis-
course relations. There are variations in the com-
plexity of the schemes used to annotate these dis-
course relations. For example, Rhetorical Struc-
ture Theory (RST) (Mann and Thompson, 1988)
defines 23 types of discourse relations that are
used to structure the text into complex discourse
trees. Whilst this scheme was used to enrich the
Penn TreeBank (Carlson et al, 2001), the Penn
Discourse TreeBank (PDTB) (Prasad et al, 2008)
used another scheme to identify discourse rela-
tions that hold between pairs of text spans. It cate-
gorises the relations into types such as ?causal?,
?temporal? and ?conditional?, which can be ei-
ther explicit or implicit, depending on whether or
79
not they are represented in text using overt dis-
course connectives. In the biomedical domain, the
Biomedical Discourse Relation Bank (BioDRB)
(Prasad et al, 2011) annotates a similar set of re-
lation types, whilst BioCause focusses exclusively
on causality (Miha?ila? et al, 2013).
A second line of research does not aim to link
textual zones, but rather to classify them accord-
ing to their specific function in the discourse. Ex-
amples of functional discourse annotations include
whether a particular zone asserts new information
into the discourse or represents a speculation or
hypothesis. In scientific texts, knowing the type
of information that a zone represents (e.g., back-
ground knowledge, hypothesis, experimental ob-
servation, conclusion, etc.) allows for automatic
isolation of new knowledge claims (Sa?ndor and de
Waard, 2012). Several annotation schemes have
been developed to classify textual zones accord-
ing to their rhetorical status or general informa-
tion content (Teufel et al, 1999; Mizuta et al,
2006; Wilbur et al, 2006; de Waard and Pan-
der Maat, 2009; Liakata et al, 2012a). Related
to these studies are efforts to capture information
relating to discourse function at the level of events,
i.e., structured representations of pieces of knowl-
edge which, when identified, facilitate sophisti-
cated semantic searching (Ananiadou et al, 2010).
Since there can be multiple events in a sentence
or clause, the identification of discourse informa-
tion at the event level can allow for a more de-
tailed analysis of discourse elements than is possi-
ble when considering larger units of text. Certain
event corpora such as ACE 2005 (Walker, 2006)
and GENIA-MK (Thompson et al, 2011) have
been annotated with various types of functional
discourse information.
It has previously been shown that considering
several functional discourse annotation schemes in
parallel can be beneficial (Liakata et al, 2012b),
since each scheme offers a different perspective.
For a common set of documents, the cited study
analysed and compared functional discourse an-
notations at different levels of textual granular-
ity (i.e., sentences, clauses and events), showing
how the different schemes could complement each
other in order to lay the foundations for a possible
future harmonisation of the schemes. The results
of this analysis provide evidence that it would be
useful to carry out further such analyses involv-
ing other such schemes, including an investiga-
tion of how discourse relations and functional dis-
course annotations could complement each other,
e.g., which types of functional annotations occur
within the arguments of discourse relations. There
are, however, certain barriers to carrying out such
an analysis. For example, a comparison of an-
notation schemes would ideally allow the differ-
ent types of annotations to be visualised simul-
taneously or seamlessly merged together. How-
ever, the fact that annotations in different corpora
are encoded using different formats (e.g., stand-off
or in-line) and different encoding schemes means
that this can be problematic.
A solution to the challenges introduced above is
offered by the Unstructured Information Manage-
ment Architecture (UIMA) (Ferrucci and Lally,
2004), which defines a common workflow meta-
data format facilitating the straightforward combi-
nation of NLP resources into a workflow. Based
on the interoperability of the UIMA framework,
numerous researchers distribute their own tools as
UIMA-compliant components (Kano et al, 2011;
Baumgartner et al, 2008; Hahn et al, 2008;
Savova et al, 2010; Gurevych et al, 2007; Rak
et al, 2012b). However, UIMA is only intended
to provide an abstract framework for the interop-
erability of language resources, leaving the actual
implementation to third-party developers. Hence,
UIMA does not explicitly address interoperability
issues of tools and corpora.
U-Compare (Kano et al, 2011) is a UIMA-
based workflow construction platform that pro-
vides a graphical user interface (GUI) via which
users can rapidly create NLP pipelines using a
drag-and-drop mechanism. Conforming to UIMA
standards, U-Compare components and pipelines
are compatible with any UIMA application via a
common and sharable type system (i.e., a hier-
archy of annotation types). In defining this type
system, U-Compare promotes interoperability of
tools and corpora, by exhaustively modelling a
wide range of NLP data types (e.g., sentences, to-
kens, part-of-speech tags, named entities). This
type system was recently extended to include dis-
course annotations to model three discourse phe-
nomena, namely causality, coreference and meta-
knowledge (Batista-Navarro et al, 2013).
In this paper, we describe our extensions to U-
Compare, supporting the integration and visuali-
sation of resources annotated according to mul-
tiple discourse annotation schemes. Our method
80
decomposes pipelines into parallel sub-workflows,
each linked to a different annotation scheme.
The resulting annotations produced by each sub-
workflow can be either merged within a single
document or visualised in parallel views.
2 Related work
Previous studies have shown the advantages of
comparing and integrating different annotation
schemes on a corpus of documents (Guo et al,
2010; Liakata et al, 2010; Liakata et al, 2012b).
Guo et al (2010) compared three different dis-
course annotation schemes applied to a corpus
of biomedical abstracts on cancer risk assess-
ment and concluded that two of the schemes pro-
vide more fine-grained information than the other
scheme. They also revealed a subsumption rela-
tion between two schemes. Such outcomes from
comparing schemes are meaningful for users who
wish to select the most appropriate scheme for an-
notating their data. Liakata et al (2012) under-
line that different discourse annotation schemes
capture different dimensions of discourse. Hence,
there might be complementary information across
different schemes. Based on this hypothesis, they
provide a comparison of three annotation schemes,
namely CoreSC (Liakata et al, 2012a), GENIA-
MK (Thompson et al, 2011) and DiscSeg (de
Waard, 2007), on a corpus of three full-text pa-
pers. Their results showed that the categories in
the three schemes can complement each other. For
example, the values of the Certainty Level dimen-
sion of the GENIA-MK scheme can be used to as-
sign confidence values to the Conclusion, Result,
Implication and Hypothesis categories of CoreSC
and DiscSeg. In contrast to previous studies, our
proposed approach automatically integrates mul-
tiple annotation schemes. The proposed mecha-
nism allows users to easily compare, integrate and
visualise multiple discourse annotation schemes
in an interoperable NLP infrastructure, i.e., U-
Compare.
There are currently a number of freely-available
NLP workflow infrastructures (Ferrucci and Lally,
2004; Cunningham et al, 2002; Scha?fer, 2006;
Kano et al, 2011; Grishman, 1996; Baumgartner
et al, 2008; Hahn et al, 2008; Savova et al, 2010;
Gurevych et al, 2007; Rak et al, 2012b). Most
of the available infrastructures support the devel-
opment of standard NLP applications, e.g., part-
of-speech tagging, deep parsing, chunking, named
entity recognition and several of them allow the
representation and analysis of discourse phenom-
ena (Kano et al, 2011; Cunningham et al, 2002;
Savova et al, 2010; Gurevych et al, 2007). How-
ever, none of them has demonstrated the integra-
tion of resources annotated according to multiple
annotation schemes within a single NLP pipeline.
GATE (Cunningham et al, 2002) is an open
source NLP infrastructure that has been used for
the development of various language processing
tasks. It is packaged with an exhaustive number
of NLP components, including discourse analy-
sis modules, e.g., coreference resolution. Further-
more, GATE offers a GUI environment and wrap-
pers for UIMA-compliant components. However,
GATE implements a limited workflow manage-
ment mechanism that does not support the execu-
tion of parallel or nested workflows. In addition to
this, GATE does not promote interoperability of
language resources since it does not define any hi-
erarchy of NLP data types and components do not
formally declare their input/output capabilities.
In contrast to GATE, UIMA implements a more
sophisticated workflow management mechanism
that supports the construction of both parallel
and nested pipelines. In this paper, we exploit
this mechanism to integrate multiple annotation
schemes in NLP workflows. cTAKES (Savova
et al, 2010) and DKPro (Gurevych et al, 2007)
are two repositories containing UIMA-compliant
components that are tuned for the medical and
general domain, respectively. However, both of
these repositories support the representation of
only one discourse phenomenon, i.e., coreference.
Argo (Rak et al, 2012a; Rak et al, 2012b) is a
web-based platform that allows multiple branch-
ing and merging of UIMA pipelines. It incorpo-
rates several U-Compare components and conse-
quently, supports the U-Compare type system.
3 A UIMA architecture for processing
multiple annotation schemes
In UIMA, a document, together with its associated
annotations, is represented as a standardised data
structure, namely the Common Analysis Struc-
ture (CAS). Each CAS can contain any number
of nested sub-CASes, i.e., Subjects of Analysis
(Sofas), each of which can associate a different
type of annotation with the input document. In
this paper, we employ this UIMA mechanism to
allow the integration and comparison of multiple
81
Collection of Documents
Multi-SofaReader
Parallel Annotation Viewer Annotation Merger
ComparingSchemes Integrating Schemes
ComponentC_1
Sofa S_1
ComponentC_2
Sofa S_2
ComponentC_N-1
Sofa S_N-1
ComponentC_N
Sofa S_N
sub-workflows
Figure 1: Integrating annotations from multiple
annotation schemes in UIMA workflows
annotation schemes in a single U-Compare work-
flow. Assume that we have a corpus of documents
which has been annotated according to n different
schemes, S1, S2, ..., Sn?1, Sn. Also, assume that
we will use a library of m text analysis compo-
nents, C1, C2, ..., Cm?1, Cm, to enrich the corpus
with further annotations.
Our implemented architecture is illustrated in
Figure 1. Using multiple Sofas, we are able to split
a UIMA workflow into parallel sub-workflows.
Starting from a Multi-Sofa reader, we create n
sub-workflows, i.e., Sofas, each of which is linked
to a particular scheme for a different annotation
type. Each sub-workflow can then apply the anal-
ysis components that are most suitable for pro-
cessing the annotations from the corresponding
scheme.
U-Compare offers two different modes for visu-
alising corpora that have been annotated accord-
ing to multiple schemes. In the comparison mode,
the default annotation viewer is automatically split
to allow annotations from different schemes to be
displayed side-by-side. The second type of visu-
alisation merges the annotations produced by the
parallel sub-workflows into a single view. The
most appropriate view may depend on the prefer-
ences of the user and the task at hand, e.g., iden-
tifying similarities, differences or complementary
information between different schemes.
4 Application Workflows
In this section, we demonstrate two workflow ap-
plications that integrate multiple discourse anno-
tation schemes. The first workflow exploits U-
Compare?s comparison mode to visualise in par-
allel functional discourse annotations from two
schemes, namely, CoreSC (Liakata et al, 2012a)
and GENIA-MK (Thompson et al, 2011). The
second application integrates functional discourse
annotations in the ACE 2005 corpus with dis-
course relations obtained by an automated tool.
4.1 Visualising functional discourse
annotations from different schemes
The purpose of this workflow application is to re-
veal the different interpretations given by two dis-
course annotation schemes applied to a biomed-
ical corpus of three full-text papers (Liakata et
al., 2012b). The pipeline contains two read-
ers that take as input the annotations (in the
BioNLP Shared Task stand-off format) from the
two schemes and map them to U-Compare?s
type system. In this way, the annotations be-
come interoperable with existing components in
U-Compare?s library. U-Compare detects that the
workflow contains two annotation schemes and
automatically creates two parallel sub-workflows
as explained earlier. Furthermore, we configure
the workflow to use the comparison mode. There-
fore, the annotation viewer will display the two
different types of annotations based on the input
schemes side-by-side. Figure 2 illustrates the par-
allel viewing of a document annotated according
to both the CoreSC (left-hand side) and GENIA-
MK (right-hand side) annotation schemes. The
CoreSC scheme assigns a single category per sen-
tence. The main clause in the highlighted sen-
tence on the left-hand side constitutes the hypoth-
esis that transcription factors bind to exon-1. Ac-
cordingly, as can be confirmed from the annota-
tion table on the far right-hand side of the figure,
the (Hyp)othesis category has been assigned to the
sentence.
In the GENIA-MK corpus, the different pieces
of information contained within the sentence have
been separately annotated as structured events.
One of these events corresponds to the hypothe-
sis, but this is not the only information expressed:
information about a previous experimental out-
come from the authors, i.e., that exon1 is impli-
cated in CCR3 transcription, is annotated as a sep-
82
Figure 2: Comparing discourse annotations schemes in U-Compare. The pipeline uses two Sofas corre-
sponding to the CoreSC (left panel) and GENIA-MK (right panel) schemes.
arate event. Since functional discourse informa-
tion is annotated directly at the event level in the
GENIA-MK corpus, the bind event is considered
independently from the other event as represent-
ing an Analysis. Furthermore, the word hypoth-
esized is annotated as a cue for this categorisa-
tion. There are several ways in which the an-
notations of the two schemes can be seen to be
complementary to each other. For example, the
finer-grained categorisation of analytical informa-
tion in the CoreSC scheme could help to determine
that the analytical bind event in the GENIA-MK
corpus specifically represents a hypothesis, rather
than, e.g., a conclusion. Conversely, the event-
based annotation in the GENIA-MK corpus can
help to determine exactly which part of the sen-
tence represents the hypothesis. Furthermore, the
cue phrases annotated in the GENIA-MK corpus
could be used as additional features in a system
trained to assign CoreSC categories. Although in
this paper we illustrate only the visualisation of
different types of functional discourse annotations,
it is worth noting that U-Compare provides sup-
port for further processing. Firstly, unlike annota-
tion platforms such as brat (Stenetorp et al, 2012),
U-Compare allows for analysis components to be
integrated into workflows in a straightforward and
user-interactive manner. If, for example, it is of in-
terest to determine the tokens (and the correspond-
ing parts-of-speech) which frequently act as cues
in Analysis events, syntactic analysis components
(e.g., tokenisers and POS taggers) can be incorpo-
rated via a drag-and-drop mechanism. Also, U-
Compare allows the annotations to be saved in a
computable format using the provided Xmi Writer
CAS Consumer component. This facilitates fur-
ther automatic comparison of annotations.
4.2 Integrating discourse relations with
functional discourse annotations
To demonstrate the integration of annotations orig-
inating from two completely different perspectives
on discourse, we have created a workflow that
merges traditional discourse relations with func-
tional discourse annotations in a general domain
corpus. For this application, we used the ACE
2005 corpus, which consists of 599 documents
coming from broadcast conversation, broadcast
news, conversational telephone speech, newswire,
weblog and usenet newsgroups. This corpus
contains event annotations which have been en-
riched by attributes such as polarity (positive or
negative), modality (asserted or other), generic-
ity (generic or specific) and tense (past, present,
future or unspecified). We treat the values of
these attributes as functional discourse annota-
tions, since they provide further insight into the
interpretation of the events. We created a compo-
nent that reads the event annotations in the corpus
and maps them to U-Compare?s type system.
To obtain discourse relation annotations (which
are not available in the ACE corpus) we em-
ployed an end-to-end discourse parser trained
on the Penn Discourse TreeBank (Lin et al,
2012). It outputs three general types of anno-
tations, namely, explicit relations, non-explicit
relations and attribution spans. Explicit rela-
tions (i.e., those having overt discourse connec-
tives) are further categorised into the following 16
PDTB level-2 types: Asynchronous, Synchrony,
Cause, Pragmatic cause, Contrast, Concession,
Conjunction, Instantiation, Restatement, Alterna-
tive, List, Condition, Pragmatic condition, Prag-
matic contrast, Pragmatic concession and Excep-
83
Figure 3: Integrating different discourse annotation schemes in U-Compare.
tion. Non-explicit relations, on the other hand,
consist of EntRel and NoRel types, in addition to
the same first 11 explicit types mentioned above.
We created a workflow consisting of the ACE
corpus reader and the discourse parser (available
in U-Compare as a UIMA web service). This al-
lowed us to merge traditional discourse relations
with event-based functional discourse annotations,
and to visualise them in the same document (Fig-
ure 3). Furthermore, with the addition of the
Xmi Writer CAS Consumer in the workflow, the
merged annotations can be saved in a computable
format for further processing, allowing users to
perform deeper analyses on the discourse annota-
tions. This workflow has enabled us to gain some
insights into the correlations between functional
discourse annotations and discourse relations.
5 Correlations between discourse
relations and functional discourse
annotations
Based on the merged annotation format described
in the previous section, we computed cases in
which at least one of the arguments of a discourse
relation also contains an event. Figure 4 is a
heatmap depicting the correlations between differ-
ent types of discourse relations and the attribute
values of ACE events that co-occur with these re-
lations. The darker the colour, the smaller the ratio
of the given discourse relation co-occurring with
the specified ACE event attribute value. For in-
stance, the Cause relation co-occurs mostly with
positive events (over 95%) and the correspond-
ing cell is a very light shade of green. These are
discussed and exemplified below. In the exam-
ples, the following marking convention is used:
discourse connectives are capitalised, whilst argu-
ments are underlined. Event triggers are shown in
bold, and cues relating to functional discourse cat-
egories are italicised.
For all discourse relation types, at least 50% of
co-occurring events are assigned the specific value
of the Genericity attribute. Specific events are
those that describe a specific occurrence or situ-
ation, rather than a more generic situation. In gen-
eral, this high proportion of specific events is to be
expected. The types of text contained within the
corpus, consisting largely of news and transcrip-
tions of conversions, would be expected to intro-
duce a large amount of information about specific
events.
For two types of discourse relations, i.e. Condi-
tion and Concession, there are more or less equal
numbers of specific and generic events. The na-
ture of these relation types helps to explain these
proportions. Conditional relations often describe
how a particular, i.e., specific, situation will hold
if some hypothetical situation is true. Since hypo-
thetical situations do not denote specific instances,
they will usually be labelled as generic. Con-
cessions, meanwhile, usually describe how a spe-
cific situation holds, even though another (more
generic) situation would normally hold, that would
be inconsistent with this. For the Instantiation re-
lation category, it may once again be expected that
similar proportions of generic and specific events
would co-occur within their arguments, since an
instantiation describes a specific instance of a
more generic situation. However, contrary to these
84
AlternativeAsynchronousAttributionCauseConcessionConditionConjunctionContrastEntRelInstantiationListNoRelRestatementSynchronous
GenericSpecific AssertedOther NegativePositive FuturePast PresentUnspecif
ied
0
1
0.5
0.25
0.75
Genericity Modality Polarity Tense0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
Figure 4: Heatmap showing the distribution of correlations between discourse relations and event-based
functional discourse categories. A darker shade indicates a smaller percentage of instances of a discourse
relation co-occurring with an event attribute.
expectations, the ratio of specific to generic events
is 3:1. A reason for this is that discourse argu-
ments corresponding to the description of a spe-
cific instance may contain several different events,
as illustrated in Example (1).
(1) Toefting has been convicted before. In
1999 he was given a 20-day suspended sentence
for assaulting a fan who berated him for
playing with German club Duisburg.
In terms of the Modality attribute, most dis-
course relations correlate with definite, asserted
events. Simillarly to the Genericity attribute, this
can be largely explained by the nature of the texts.
However, there are two relation types, i.e., Condi-
tion and Consession, which have particularly high
proportions of co-occurring events whose modal-
ity is other. Events that are assigned this attribute
value correspond to those that are not described as
though they are real occurrences. This includes,
e.g., speculated or hypothetical events. The fact
that Condition relations are usually hypothetical
in nature explain why 76% of events that co-occur
with such relations are assigned the other value
for the Modality attribute. Example (2) illustrates
a sentence containing this relation type.
(2) And I?ve said many times, IF we all
agreed on everything, everybody would want to
marry Betty and we would really be in a mess,
wouldn?t we, Bob.
An even higher proportion of Concession re-
lations co-occurs with events whose modality is
other. Example (3) helps to explain this. In the
first clause (the generic situation), the mention of
minimising civilian casualties is only described as
an effort, rather than a definite situation. The hedg-
ing of this generic situation is necessary in order to
concede that the more specific situation described
in the second clause could actually be true, i.e.,
that a large number of civilians have already been
killed. Due to the nature of news reporting, which
may come from potentially unreliable sources, the
killed event in this second clause is also hedged,
through the use of the word reportedly.
(3) ALTHOUGH the coalition leaders have
repeatedly assured that every effort would be
made to minimize civilian casualties in the
current Iraq war, at least 130 Iraqi civilians have
been reportedly killed since the war started five
days ago.
Almost 96% of events that co-occur with argu-
ments of discourse relations have positive polarity.
Indeed, for eight relation types, 100% of the cor-
responding events are positive. This can partly be
explained by the fact that, in texts reporting news,
85
there is an emphasis on reporting events that have
happened, rather than events that did not happen.
It can, however, be noted that events that co-occur
with certain discourse relation types have a greater
likelihood of having negative polarity. These rela-
tions include Contrast (9% of events having neg-
ative polarity) and Cause (5% negative events).
Contrasts can include comparisons of positive and
negative situations, as in Example (4), whilst for
Causes, it can sometimes be relevant to state that
a particular situation caused a specific event not to
take place, as shown in Example (5).
(4) The message from the Israeli government
is that its soldiers are not targeting journalists,
BUT that journalists who travel to places where
there could be live fire exchange between
Israeli forces and Palestinian gunmen have a
responsibility to take greater precautions.
(5) His father didn?t want to invade Iraq, BE-
CAUSE of all these problems they?re having
now.
For most relation types, around 60% of their co-
occurring events are annotated as describing past
tense situations. This nature of newswire and con-
versations mean that this is largely to be expected,
since they normally report mainly on events that
have already happened. The proportion of events
assigned the future tense value is highest when
they co-occur with discourse relations of type Al-
ternative. In this relation type, it is often the case
that one of the arguments describes a possible fu-
ture alternative to a current situation, as the case in
Example (6). This possible information pattern for
Alternative relations, where one of the arguments
represents a currently occurring situation, would
also help to explain why, even though very few
events in general are annotated as present tense,
almost 10% of events that co-occur with Alter-
native relations describe events that are currently
ongoing. As for events whose Tense value is un-
specified, two of the most common discourse re-
lation types with which they occur are Condition
and Concession. As exemplified above, Condition
relations are often hypothetical in nature, meaning
that no specific tense can be assigned. The generic
argument of a Concession relation can also remain
unmarked for tense. As in Example (3), it is not
clear whether the effort to minimise civilian casu-
alties has already been initiated, or will be initiated
in the future.
(6) Saddam wouldn?t be destroying missiles
UNLESS he thought he was going to be
destroyed if he didn?t.
6 Conclusions
Given the level of variability in existing discourse-
annotated corpora, it is meaningful for users to
identify the relative merits of different schemes.
In this paper, we have presented an extension of
the U-Compare infrastructure that facilitates the
comparison, integration and visualisation of doc-
uments annotated according to different annota-
tion schemes. U-Compare constructs multiple and
parallel annotation sub-workflows nested within a
single workflow, with each sub-workflow corre-
sponding to a distinct scheme. We have applied
the implemented method to visualise the similar-
ities and differences of two functional discourse
annotation schemes, namely CoreSC and GENIA-
MK. To demonstrate the integration of multiple
schemes in U-Compare, we developed a work-
flow that merged event annotations from the ACE
2005 corpus (which include certain types of func-
tional discourse information) with discourse rela-
tions obtained by an end-to-end parser. Moreover,
we have analysed the merged annotations obtained
by this workflow and this has allowed us to iden-
tify various correlations between the two different
types of discourse annotations.
Based on the intuition that there is comple-
mentary information across different types of dis-
course annotations, we intend to examine how the
integration of multiple discourse schemes, e.g.,
features obtained by merging annotations, affects
the performance of machine learners for discourse
analysis.
7 Acknowledgements
We are grateful to Dr. Ziheng Lin (Na-
tional University of Singapore) for providing us
with the discourse parser used for this work.
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; Engineering and Physical Sciences Re-
search Council [grant numbers EP/P505631/1,
EP/J50032X/1]; and MRC Text Mining and
Screening (MR/J005037/1).
86
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381 ? 390.
Riza Theresa B. Batista-Navarro, Georgios Kontonat-
sios, Claudiu Miha?ila?, Paul Thompson, Rafal Rak,
Raheel Nawaz, Ioannis Korkontzelos, and Sophia
Ananiadou. 2013. Facilitating the analysis of dis-
course phenomena in an interoperable NLP plat-
form. In Computational Linguistics and Intelligent
Text Processing, volume 7816 of Lecture Notes in
Computer Science, pages 559?571. Springer Berlin
Heidelberg, March.
William A. Baumgartner, Kevin Bretonnel Cohen, and
Lawrence Hunter. 2008. An open-source frame-
work for large-scale, flexible evaluation of biomedi-
cal text mining systems. Journal of biomedical dis-
covery and collaboration, 3:1+, January.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure Theory.
In Proceedings of the Second SIGdial Workshop on
Discourse and Dialogue - Volume 16, SIGDIAL ?01,
pages 1?10, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an architecture for development of robust HLT
applications. In In Recent Advanced in Language
Processing, pages 168?175.
Anita de Waard and Henk Pander Maat. 2009. Epis-
temic segment types in biology research articles. In
Proceedings of the Workshop on Linguistic and Psy-
cholinguistic Approaches to Text Structuring (LPTS
2009).
Anita de Waard. 2007. A pragmatic structure for re-
search articles. In Proceedings of the 2nd inter-
national conference on Pragmatic web, ICPW ?07,
pages 83?89, New York, NY, USA. ACM.
David Ferrucci and Adam Lally. 2004. Building an
example application with the unstructured informa-
tion management architecture. IBM Systems Jour-
nal, 43(3):455?475.
Ralph Grishman. 1996. TIPSTER Text Phase II archi-
tecture design version 2.1p 19 june 1996. In Pro-
ceedings of the TIPSTER Text Program: Phase II,
pages 249?305, Vienna, Virginia, USA, May. Asso-
ciation for Computational Linguistics.
Yufan Guo, Anna Korhonen, Maria Liakata,
Ilona Silins Karolinska, Lin Sun, and Ulla Ste-
nius. 2010. Identifying the information structure of
scientific abstracts: An investigation of three differ-
ent schemes. In Proceedings of the 2010 Workshop
on Biomedical Natural Language Processing, pages
99?107. Association for Computational Linguistics.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the GSCL.
Udo Hahn, Ekaterina Buyko, Rico Landefeld, Matthias
Mu?hlhausen, Michael Poprat, Katrin Tomanek, and
Joachim Wermter. 2008. An overview of JCoRe,
the JULIE lab UIMA component repository. In
LREC?08 Workshop ?Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP?,
pages 1?7, Marrakech, Morocco, May.
Yoshinobu Kano, Makoto Miwa, Kevin Cohen,
Lawrence Hunter, Sophia Ananiadou, and Jun?ichi
Tsujii. 2011. U-Compare: A modular NLP work-
flow construction and evaluation system. IBM Jour-
nal of Research and Development, 55(3):11.
Maria Liakata, Simone Teufel, Advaith Siddharthan,
and Colin Batchelor. 2010. Corpora for the concep-
tualisation and zoning of scientific papers. In Pro-
ceedings of LREC, volume 10.
Maria Liakata, Shyamasree Saha, Simon Dobnik,
Colin Batchelor, and Dietrich Rebholz-Schuhmann.
2012a. Automatic recognition of conceptualization
zones in scientific articles and two life science appli-
cations. Bioinformatics, 28(7):991?1000.
Maria Liakata, Paul Thompson, Anita de Waard, Ra-
heel Nawaz, Henk Pander Maat, and Sophia Ana-
niadou. 2012b. A three-way perspective on scien-
tic discourse annotation for knowledge extraction.
In Proceedings of the ACL Workshop on Detecting
Structure in Scholarly Discourse (DSSD), pages 37?
46, July.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, FirstView:1?34, 10.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional
theory of text organization. Text, 8(3):243?281.
Claudiu Miha?ila?, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. BioCause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14(1):2, January.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468 ? 487. Re-
cent Advances in Natural Language Processing for
Biomedical Applications Special Issue.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The Penn Discourse Tree-
Bank 2.0. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios
87
Piperidis, and Daniel Tapias, editors, In Proceedings
of the 6th International Conference on language Re-
sources and Evaluation (LREC), pages 2961?2968.
Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind
Joshi, and Hong Yu. 2011. The biomedical
discourse relation bank. BMC Bioinformatics,
12(1):188.
Rafal Rak, Andrew Rowley, and Sophia Ananiadou.
2012a. Collaborative development and evaluation
of text-processing workflows in a UIMA-supported
web-based workbench.
Rafal Rak, Andrew Rowley, William Black, and Sophia
Ananiadou. 2012b. Argo: an integrative, in-
teractive, text mining-based workbench supporting
curation. Database: The Journal of Biological
Databases and Curation, 2012.
A?gnes Sa?ndor and Anita de Waard. 2012. Identifying
claimed knowledge updates in biomedical research
articles. In Proceedings of the Workshop on De-
tecting Structure in Scholarly Discourse, ACL ?12,
pages 10?17, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Guergana Savova, James Masanz, Philip Ogren, Jiap-
ing Zheng, Sunghwan Sohn, Karin Kipper-Schuler,
and Christopher Chute. 2010. Mayo clini-
cal text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and
applications. Journal of the American Medical In-
formatics Association, 17(5):507?513.
Ulrich Scha?fer. 2006. Middleware for creating and
combining multi-dimensional nlp markup. In Pro-
ceedings of the 5th Workshop on NLP and XML:
Multi-Dimensional Markup in Natural Language
Processing, pages 81?84. ACL.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions Session at EACL 2012, Avignon, France, April.
Association for Computational Linguistics.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, EACL ?99,
pages 110?117, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedi-
cal event corpus with meta-knowledge annotation.
BMC Bioinformatics, 12(1):393.
Christopher Walker. 2006. ACE 2005 Multilingual
Training Corpus.
W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay.
2006. New directions in biomedical text annota-
tion: definitions, guidelines and corpus construction.
BMC Bioinformatics, 7(1):356.
88
