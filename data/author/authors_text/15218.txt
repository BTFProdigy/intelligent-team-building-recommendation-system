Error Measures and Bayes Decision Rules Revisited
with Applications to POS Tagging
Hermann Ney, Maja Popovic?, David Su?ndermann
Lehrstuhl fu?r Informatik VI - Computer Science Department
RWTH Aachen University
Ahornstrasse 55
52056 Aachen, Germany
{popovic,ney}@informatik.rwth-aachen.de
Abstract
Starting from first principles, we re-visit the
statistical approach and study two forms of
the Bayes decision rule: the common rule for
minimizing the number of string errors and a
novel rule for minimizing the number of symbols
errors. The Bayes decision rule for minimizing
the number of string errors is widely used, e.g.
in speech recognition, POS tagging and machine
translation, but its justification is rarely questioned.
To minimize the number of symbol errors as is
more suitable for a task like POS tagging, we show
that another form of the Bayes decision rule can
be derived. The major purpose of this paper is to
show that the form of the Bayes decision rule should
not be taken for granted (as it is done in virtually
all statistical NLP work), but should be adapted
to the error measure being used. We present first
experimental results for POS tagging tasks.
1 Introduction
Meanwhile, the statistical approach to natural
language processing (NLP) tasks like speech
recognition, POS tagging and machine translation
has found widespread use. There are three
ingredients to any statistical approach to NLP,
namely the Bayes decision rule, the probability
models (like trigram model, HMM, ...) and the
training criterion (like maximum likelihood, mutual
information, ...).
The topic of this paper is to re-consider the form
of the Bayes decision rule. In virtually all NLP
tasks, the specific form of the Bayes decision rule
is never questioned, and the decision rule is adapted
from speech recognition. In speech recognition, the
typical decision rule is to maximize the sentence
probability over all possible sentences. However,
this decision rule is optimal for the sentence error
rate and not for the word error rate. This difference
is rarely studied in the literature.
As a specific NLP task, we will consider part-
of-speech (POS) tagging. However, the problem
addressed comes up in any NLP task which is
tackled by the statistical approach and which makes
use of a Bayes decision rule. Other prominent
examples are speech recognition and machine
translation. The advantage of the POS tagging
task is that it will be easier to handle from the
mathematical point of view and will result in closed-
form solutions for the decision rules. From this
point-of-view, the POS tagging task serves as a
good opportunity to illustrate the key concepts of
the statistical approach to NLP.
Related Work: For the task of POS tagging,
statistical approaches were proposed already in the
60?s and 70?s (Stolz et al, 1965; Bahl and Mercer,
1976), before they started to find widespread use
in the 80?s (Beale, 1985; DeRose, 1989; Church,
1989).
To the best of our knowledge, the ?standard?
version of the Bayes decision rule, which minimizes
the number of string errors, is used in virtually all
approaches to POS tagging and other NLP tasks.
There are only two research groups that do not take
this type of decision rule for granted:
(Merialdo, 1994): In the context of POS tagging,
the author introduces a method that he calls
maximum likelihood tagging. The spirit of this
method is similar to that of this work. However, this
method is mentioned as an aside and its implications
for the Bayes decision rule and the statistical
approach are not addressed. Part of this work
goes back to (Bahl et al, 1974) who considered
a problem in coding theory.
(Goel and Byrne, 2003): The error measure
considered by the authors is the word error rate in
speech recognition, i.e. the edit distance. Due to
the mathematical complexity of this error measure,
the authors resort to numeric approximations
to compute the Bayes risk (see next section).
Since this approach does not results in explicit
closed-form equations and involves many numeric
approximations, it is not easy to draw conclusions
from this work.
2 Bayes Decision Rule for Minimum Error
Rate
2.1 The Bayes Posterior Risk
Knowing that any task in NLP tasks is a difficult
one, we want to keep the number of wrong
decisions as small as possible. This point-of-view
has been used already for more than 40 years in
pattern classification as the starting point for many
techniques in pattern classification. To classify an
observation vector y into one out of several classes
c, we resort to the so-called statistical decision
theory and try to minimize the average risk or loss
in taking a decision. The result is known as Bayes
decision rule (Chapter 2 in (Duda and Hart, 1973)):
y ? c? = argmin
c
{
?
c?
Pr(c|y) ? L[c, c?]
}
where L[c, c?] is the so-called loss function or error
measure, i.e. the loss we incur in making decision c
when the true class is c?.
In the following, we will consider two specific
forms of the loss function or error measure L[c, c?].
The first will be the measure for string errors,
which is the typical loss function used in virtually
all statistical approaches. The second is the
measure for symbol errors, which is the more
appropriate measure for POS tagging and also
speech recognition with no insertion and deletion
errors (such as isolated word recognition).
2.2 String Error
For POS tagging, the starting point is the observed
sequence of words y = wN1 = w1...wN , i.e. the
sequence of words for which the POS tag sequence
has c = gN1 = g1...gN has to be determined.
The first error measure we consider is the string
error: the error is equal to zero only if the POS
symbols of the two strings are identical at each
position. In this case, the loss function is:
L[gN1 , g?N1 ] = 1 ?
N
?
n=1
?(gn, g?n)
with the Kronecker delta ?(c, c?). In other words,
the errors are counted at the string level and not
at the level of single symbols. Inserting this cost
function into the Bayes risk (see Section 2.1), we
immediately obtain the following form of Bayes
decision rule for minimum string error:
wN1 ? g?N1 = argmax
gN1
{
Pr(gN1 |wN1 )
}
= argmax
gN1
{
Pr(gN1 , wN1 )
}
This is the starting point for virtually all statistical
approaches in NLP like speech recognition and
machine translation. However, this decision rule is
only optimal when we consider string errors, e.g.
sentence error rate in POS tagging and in speech
recognition. In practice, however, the empirical
errors are counted at the symbol level. Apart
from (Goel and Byrne, 2003), this inconsistency of
decision rule and error measure is never addressed
in the literature.
2.3 Symbol Error
Instead of the string error rate, we can also consider
the error rate of single POS tag symbols (Bahl et
al., 1974; Merialdo, 1994).
This error measure is defined by the loss function:
L[gN1 , g?N1 ] =
N
?
n=1
[1 ? ?(gn, g?n)]
This loss function has to be inserted into the Bayes
decision rule in Section 2.1. The computation of the
expected loss, i.e. the averaging over all classes c? =
g?N1 , can be performed in a closed form. We omit
the details of the straightforward calculations and
state only the result. It turns out that we will need
the marginal (and posterior) probability distribution
Prm(g|wN1 ) at positions m = 1, ..., N :
Prm(g|wN1 ) :=
?
gN1 : gm=g
Pr(gN1 |wN1 )
where the sum is carried out over all POS tag strings
gN1 with gm = g, i.e. the tag gm at position m is
fixed at gm = g. The question of how to perform
this summation efficiently will be considered later
after we have introduced the model distributions.
Thus we have obtained the Bayes decision rule
for minimum symbol error at position m = 1, ..., N :
(wN1 ,m) ? g?m = argmaxg
{
Prm(g|wN1 )
}
= argmax
g
{
Prm(g,wN1 )
}
By construction this decision rule has the special
property that it does not put direct emphasis on
local coherency of the POS tags produced. In other
words, this decision rule may produce a POS tag
string which is linguistically less likely.
3 The Modelling Approaches to POS
Tagging
The derivation of the Bayes decision rule assumes
that the probability distribution Pr(gN1 , wN1 ) (or
Pr(gN1 |wN1 )) is known. Unfortunately, this is not
the case in practice. Therefore, the usual approach
is to approximate the true but unknown distribution
by a model distribution p(gN1 , wN1 ) (or p(gN1 |wN1 )).
We will review two popular modelling approaches,
namely the generative model and the direct model,
and consider the associated Bayes decision rules for
both minimum string error and minimum symbol
error.
3.1 Generative Model: Trigram Model
We replace the true but unknown joint distribution
Pr(gN1 , wN1 ) by a model-based probability distribu-
tion p(gN1 , wN1 ):
Pr(gN1 , wN1 ) ? p(gN1 , wN1 ) = p(gN1 ) ? p(wN1 |gN1 )
We apply the so-called chain rule to factorize each
of the distributions p(gN1 ) and p(wN1 |gN1 ) into a
product of conditional probabilities using specific
dependence assumptions:
p(gN1 , wN1 ) =
N
?
n=1
[
p(gn|gn?1n?2) ? p(wn|gn)
]
with suitable definitions for the case n = 1.
Here, the specific dependence assumptions are that
the conditional probabilities can be represented
by a POS trigram model p(gn|gn?1n?2) and a word
membership model p(wn|gn). Thus we obtain
a probability model whose structure fits into
the mathematical framework of so-called Hidden
Markov Model (HMM). Therefore, this approach is
often also referred to as HMM-based POS tagging.
However, this terminology is misleading: The POS
tag sequence is observable whereas in the Hidden
Markov Model the state sequence is always hidden
and cannot be observed. In the experiments, we will
use a 7-gram POS model. It is clear how to extend
the equations from the trigram case to the 7-gram
case.
3.1.1 String Error
Using the above model distribution, we directly
obtain the decision rule for minimum string error:
wN1 ? g?N1 = argmax
gN1
{
p(gN1 , wN1 )
}
Since the model distribution is a basically a second-
order model (or trigram model), there is an efficient
algorithm for finding the most probable POS tag
string. This is achieved by a suitable dynamic
programming algorithm, which is often referred to
as Viterbi algorithm in the literature.
3.1.2 Symbol Error
To apply the Bayes decision rule for minimum
symbol error rate, we first compute the marginal
probability pm(g,wN1 ):
pm(g,wN1 ) =
?
gN1 : gm=g
p(gN1 , wN1 )
=
?
gN1 : gm=g
?
n
[
p(gn|gn?1n?2) ? p(wn|gn)
]
Again, since the model is a second-order model,
the sum over all possible POS tag strings gN1(with gm = g) can be computed efficiently
using a suitable extension of the forward-backward
algorithm (Bahl et al, 1974).
Thus we obtain the decision rule for minimum
symbol error at positions m = 1, ..., N :
(wN1 ,m) ? g?m = argmaxg
{
pm(g,wN1 )
}
Here, after the the marginal probability pm(g,wN1 )
has been computed, the task of finding the most
probable POS tag at position m is computationally
easy. Instead, the lion?s share for the computational
effort is required to compute the marginal probabil-
ity pm(g,wN1 ).
3.2 Direct Model: Maximum Entropy
We replace the true but unknown posterior distri-
bution Pr(gN1 |wN1 ) by a model-based probability
distribution p(gN1 |wN1 ):
Pr(gN1 |wN1 ) ? p(gN1 |wN1 )
and apply the chain rule:
p(gN1 |wN1 ) =
N
?
n=1
p(gn|gn?11 , wN1 )
=
N
?
n=1
p(gn|gn?1n?2 , wn+2n?2)
As for the generative model, we have made specific
assumptions: There is a second-order dependence
for the tags gn1 , and the dependence on the words
wN1 is limited to a window wn+2n?2 around position
n. The resulting model is still rather complex
and requires further specifications. The typical
procedure is to resort to log-linear modelling, which
is also referred to as maximum entropy modelling
(Ratnaparkhi, 1996; Berger et al, 1996).
3.2.1 String Error
For the minimum string error, we obtain the
decision rule:
wN1 ? g?N1 = argmax
gN1
{
p(gN1 |wN1 )
}
Since this is still a second-order model, we can use
dynamic programming to compute the most likely
POS string.
3.2.2 Symbol Error
For the minimum symbol error, the marginal
(and posterior) probability pm(g|wN1 ) has to be
computed:
pm(g|wN1 ) =
?
gN1 : gm=g
Pr(gN1 |wN1 )
=
?
gN1 : gm=g
?
n
p(gn|gn?1n?2 , wn+2n?2)
which, due to the specific structure of the model
p(gn|gn?1n?2 , wn+2n?2), can be calculated efficiently
using only a forward algorithm (without a
?backward? part).
Thus we obtain the decision rule for minimum
symbol error at positions m = 1, ..., N :
(wN1 ,m) ? g?m = argmaxg
{
pm(g|wN1 )
}
As in the case of the generative model, the
computational effort is to compute the posterior
probability pm(g|wN1 ) rather than to find the most
probable tag at position m.
4 The Training Procedure
So far, we have said nothing about how we train
the free parameters of the model distributions. We
use fairly conventional training procedures that we
mention only for the sake of completeness.
4.1 Generative Model
We consider the trigram-based model. The free
parameters here are the entries of the POS trigram
distribution p(g|g??, g?) and of the word membership
distribution p(w|g). These unknown parameters are
computed from a labelled training corpus, i.e. a
collection of sentences where for each word the
associated POS tag is given.
In principle, the free parameters of the models
are estimated as relative frequencies. For the test
data, we have to allow for both POS trigrams (or n-
grams) and (single) words that were not seen in the
training data. This problem is tackled by applying
smoothing methods that were originally designed
for language modelling in speech recognition (Ney
et al, 1997).
4.2 Direct Model
For the maximum entropy model, the free param-
eters are the so-called ?i or feature parameters
(Berger et al, 1996; Ratnaparkhi, 1996). The
training criterion is to optimize the logarithm
of the model probabilities p(gn|gn?2n?1 , wn+2n?2) over
all positions n in the training corpus. The
corresponding algorithm is referred to as GIS
algorithm (Berger et al, 1996). As usual
with maximum entropy models, the problem of
smoothing does not seem to be critical and is not
addressed explicitly.
5 Experimental Results
Of course, there have already been many papers
about POS tagging using statistical methods. The
goal of the experiments is to compare the two
decision rules and to analyze the differences in
performance. As the results for the WSJ corpus will
show, both the trigram method and the maximum
entropy method have an tagging error rate of 3.0%
to 3.5% and are thus comparable to the best results
reported in the literature, e.g. (Ratnaparkhi, 1996).
5.1 Task and Corpus
The experiments are performed on the Wall Street
Journal (WSJ) English corpus and on the Mu?nster
Tagging Project (MTP) German corpus.
The POS tagging part of The WSJ corpus
(Table 1) was compiled by the University of
Pennsylvania and consists of about one million
English words with manually annotated POS tags.
Text POS
Train Sentences 43508
Words+PMs 1061772
Singletons 21522 0
Word Vocabulary 46806 45
PM Vocabulary 25 9
Test Sentences 4478
Words+PMs 111220
OOVs 2879 0
Table 1: WSJ corpus statistics.
The MTP corpus (Table 2) was compiled at the
University of Mu?nster and contains tagged German
words from articles of the newspapers Die Zeit
and Frankfurter Allgemeine Zeitung (Kinscher and
Steiner, 1995).
For the corpus statistics, it is helpful to
distinguish between the true words and the
punctuation marks (see Table 1 and Table 2). This
distinction is made for both the text and the POS
corpus. In addition, the tables show the vocabulary
size (number of different tokens) for the words and
for the punctuation marks.
Punctuation marks (PMs) are all tokens which
do not contain letters or digits. The total number
of running tokens is indicated as Words+PMs.
Singletons are the tokens which occur only once in
Text POS
Train Sentences 19845
Words+PMs 349699
Singletons 32678 11
Word Vocabulary 51491 68
PM Vocabulary 27 5
Test Sentences 2206
Words+PMs 39052
OOVs 3584 2
Table 2: MTP corpus statistics.
the training data. Out-of-Vocabulary words (OOVs)
are the words in the test data that did not not occur
in the training corpus.
5.2 POS Tagging Results
The tagging experiments were performed for both
types of models, each of them with both types of
the decision rules. The generative model is based on
the approach described in (Su?ndermann and Ney,
2003). Here the optimal value of the n-gram order
is determined from the corpus statistics and has a
maximum of n = 7. The experiments for the direct
model were performed using the maximum entropy
tagger described in (Ratnaparkhi, 1996).
The tagging error rates are showed in Table 3 and
Table 4. In addition to the overall tagging error rate
(Overall), the tables show the tagging error rates for
the Out-of-Vocabulary words (OOVs) and for the
punctuation marks (PMs).
For the generative model, both decision rules
yield similar results. For the direct model, the
overall tagging error rate increases on each of the
two tasks (from 3.0 % to 3.3 % on WSJ and from
5.4 % to 5.6 % on MTP) when we use the symbol
decision rule instead of the string decision rule. In
particular, for OOVs, the error rate goes up clearly.
Right now, we do not have a clear explanation
for this difference between the generative model
and the direct model. It might be related to the
?forward? structure of the direct model as opposed to
the ?forward-backward? structure of the generative
model. Anyway, the refined bootstrap method
(Bisani and Ney, 2004) has shown that differences
in the overall tagging error rate are statistically not
significant.
5.3 Examples
A detailed analysis of the tagging results showed
that for both models there are sentences where the
one decision rule is more efficient and sentences
where the other decision rule is better.
For the generative model, these differences seem
to occur at random, but for the direct model, some
distinct tendencies can be observed. For example,
WSJ Task Decision Overall OOVs PMs
Rule
Generative string 3.5 16.9 0
Model symbol 3.5 16.7 0
Direct string 3.0 15.4 0.08
Model symbol 3.3 16.6 0.1
Table 3: POS tagging error rates [%] for WSJ task.
MTP Task Decision Overall OOVs PMs
Rule
Generative string 5.4 13.4 3.6
Model symbol 5.4 13.4 3.6
Direct string 5.4 12.7 3.8
Model symbol 5.6 13.4 3.7
Table 4: POS tagging error rates [%] for MTP task.
for the WSJ corpus, the string decision rule is
significantly better for the present and past tense of
verbs (VBP, VBN), and the symbol decision rule
is better for adverb (RB) and verb past participle
(VBN). Typical errors generated by the symbol
decision rule are tagging present tense as infinitive
(VB) and past tense as past participle (VBN), and
for string decision rule, adverbs are often tagged as
preposition (IN) or adjective (JJ) and past participle
as past tense (VBD).
For the German corpus, the string decision
rule better handles demonstrative determiners
(Rr) and subordinate conjunctions (Cs) whereas
symbol decision rule is better for definite articles
(Db). The symbol decision rule typically tags
the demonstrative determiner as definite article
(Db) and subordinate conjunctions as interrogative
adverbs (Bi), and the string decision rule tends to
assign the demonstrative determiner tag to definite
articles.
These typical errors for the symbol decision rule
are shown in Table 5, and for the string decision rule
in Table 6.
6 Conclusion
So far, the experimental tests have shown no
improvement when we use the Bayes decision rule
for minimizing the number of symbol errors rather
than the number of string errors. However, the
important result is that the new approach results in
comparable performance. More work is needed to
contrast the two approaches.
The main purpose of this paper has been to show
that, in addition to the widely used decision rule for
minimizing the string errors, it is possible to derive a
decision rule for minimizing the number of symbol
errors and to build up the associated mathematical
framework.
There are a number of open questions for future
work:
1) The error rates for the two decision rules are
comparable. Is that an experimental coincidence?
Are there situations for which we must expect a
significance difference between the two decision
rules? We speculate that the two decision rules
could always have similar performance if the error
rates are small.
2) Ideally, the training criterion should be closely
related to the error measure used in the decision
rule. Right now, we have used the training criteria
that had been developed in the past and that had
been (more or less) designed for the string error rate
as error measure. Can we come up with a training
criterion tailored to the symbol error rate?
3) In speech recognition and machine translation,
more complicated error measures such as the edit
distance and the BLEU measure are used. Is it
possible to derive closed-form Bayes decision rules
(or suitable analytic approximations) for these error
measures? What are the implications?
References
L. Bahl, J. Cocke, F. Jelinek and J. Raviv.
1974. Optimal Decoding of Linear Codes for
Minimizing Symbol Error Rate. IEEE Trans. on
Information Theory, No. 20, pages 284?287
L. Bahl and L. R. Mercer. 1976. Part of Speech
Assignment by a Statistical Decision Algorithm.
In IEEE Symposium on Information Theory,
abstract, pages 88?89, Ronneby, Sweden.
A. D. Beale. 1985. A Probabilistic Approach
to Grammatical Analysis of Written English by
Computer. In 2nd Conf. of the European Chapter
of the ACL, pages 159?169, Geneva, Switzerland.
A. L. Berger, S. Della Pietra and V. Della Pietra.
1996. A Maximum Entropy Approach to
Natural Language Processing. Computational
Linguistics, No. 22, Vol. 1, pages 39?71.
M. Bisani and H. Ney. 2004. Bootstrap Estimates
for Confidence Intervals in ASR Performance
Evaluation. In IEEE Int. Conf. on Acoustics,
Speech and Signal Processing, pages 409?412,
Montreal, Canada.
K. W. Church. 1989. A Stochastic Parts Program
Noun Phrase Parser for Unrestricted Text. In
IEEE Int. Conf. on Acoustics, Speech and Signal
Processing, pages 695?698, Glasgow, Scotland.
S. DeRose. 1989. Grammatical Category Disam-
biguation by Statistical Optimization. Computa-
tional Linguistics, No. 14, Vol. 1, pages 31?39
R. O. Duda and P. E. Hart. 1973. Pattern
Classification and Scene Analysis. John Wiley &
Sons, New York.
V. Goel and W. Byrne. 2003. Minimum Bayes-
risk Automatic Speech Recognition. In W. Chou
and B. H. Juang (editors): Pattern Recognition
in Speech and Language Processing. CRC Press,
Boca Rota, Florida.
J. Kinscher and P. Steiner. 1995. Mu?nster Tagging
Project (MTP). Handout for the 4th Northern
German Linguistic Colloquium, University of
Mu?nster, Internal report.
B. Merialdo. 1994. Tagging English Text with a
Probabilistic Model. Computational Linguistics,
No. 20, Vol. 2, pages 155?168.
H. Ney, S. Martin and F. Wessel. 1997.
Statistical Language Modelling by Leaving-One-
Out. In G. Bloothooft and S. Young (editors):
Corpus-Based Methods in Speech and Language,
pages 174?207. Kluwer Academic Publishers,
Dordrecht.
A. Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Conf.
on Empirical Methods in Natural Language
Processing and Very Large Corpora , pages 133?
142, Sommerset, NJ.
W. S. Stolz, P. H. Tannenbaum and F. V. Carstensen.
1965. Stochastic Approach to the Grammatical
Coding of English. Communications of the ACM,
No. 8, pages 399?405.
D. Su?ndermann and H. Ney. 2003. SYNTHER
- a New m-gram POS Tagger. In Proc. of
the Int. Conf. on Natural Language Processing
and Knowledge Engineering, pages 628?633,
Beijing, China.
VBP ? VB
reference ... investors/NNS already/RB have/VBP sharply/RB scaled/VBN ...
string ... investors/NNS already/RB have/VBP sharply/RB scaled/VBN ...
symbol ... investors/NNS already/RB have/VB sharply/RB scaled/VBN ...
reference We/PRP basically/RB think/VBP that/IN ...
string We/PRP basically/RB think/VBP that/IN ...
symbol We/PRP basically/RB think/VB that/IN ...
VBD ? VBN
reference ... plant-expansion/JJ program/NN started/VBD this/DT year/NN ...
string ... plant-expansion/NN program/NN started/VBD this/DT year/NN ...
symbol ... plant-expansion/NN program/NN started/VBN this/DT year/NN ...
reference ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBD agreements/NNS ...
string ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBD agreements/NNS ...
symbol ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBN agreements/NNS ...
Rr ? Db
reference Das/Db Sandma?nnchen/Ne ,/Fi das/Rr uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...
string Das/Db Sandma?nnchen/Ng ,/Fi das/Rr uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...
symbol Das/Db Sandma?nnchen/Ng ,/Fi das/Db uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...
reference ... fu?r/Po Leute/Ng ,/Fi die/Rr glauben/Vf ...
string ... fu?r/Po Leute/Ng ,/Fi die/Rr glauben/Vf ...
symbol ... fu?r/Po Leute/Ng ,/Fi die/Db glauben/Vf ...
Cs ? Bi
reference Denke/Vf ich/Rp nach/Qv ,/Fi warum/Cs mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...
string Denke/Vf ich/Rp nach/Qv ,/Fi warum/Cs mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...
symbol Denke/Vf ich/Rp nach/Qv ,/Fi warum/Bi mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...
Table 5: Examples of tagging errors for the symbol decision rule (direct model)
RB ? IN, JJ
reference The/DT negotiations/NNS allocate/VBP about/RB 15/CD %/NN ...
string The/DT negotiations/NNS allocate/VBP about/IN 15/CD %/NN ...
symbol The/DT negotiations/NNS allocate/VBP about/RB 15/CD %/NN ...
reference ... will/MD lead/VB to/TO a/DT much/RB stronger/JJR performance/NN ...
string ... will/MD lead/VB to/TO a/DT much/JJ stronger/JJR performance/NN ...
symbol ... will/MD lead/VB to/TO a/DT much/RB stronger/JJR performance/NN ...
VBN ? VBD
reference ... by/IN a/DT police/NN officer/NN named/VBN John/NNP Klute/NNP ...
string ... by/IN a/DT police/NN officer/NN named/VBD John/NNP Klute/NNP ...
symbol ... by/IN a/DT police/NN officer/NN named/VBN John/NNP Klute/NNP ...
Db ? Rr
reference er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Db Emotionen/Ng zu/Qi kanalisieren/Vi ...
string er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Rr Emotionen/Ng zu/Qi kanalisieren/Vi ...
symbol er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Db Emotionen/Ng zu/Qi kanalisieren/Vi ...
Table 6: Examples of tagging errors for the string decision rule (direct model)
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 349?356,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
A Handsome Set of Metrics to Measure Utterance Classification
Performance in Spoken Dialog Systems
David Suendermann, Jackson Liscombe, Krishna Dayanidhi, Roberto Pieraccini?
SpeechCycle Labs, New York, USA
{david, jackson, krishna, roberto}@speechcycle.com
Abstract
We present a set of metrics describing
classification performance for individual
contexts of a spoken dialog system as well
as for the entire system. We show how
these metrics can be used to train and tune
system components and how they are re-
lated to Caller Experience, a subjective
measure describing how well a caller was
treated by the dialog system.
1 Introduction
Most of the speech recognition contexts in com-
mercial spoken dialog systems aim at mapping the
caller input to one out of a set of context-specific
semantic classes (Knight et al, 2001). This is done
by providing a grammar to the speech recognizer
at a given recognition context. A grammar serves
two purposes:
? It constraints the lexical content the recog-
nizer is able to recognize in this context (the
language model) and
? It assigns one out of a set of possible classes
to the recognition hypothesis (the classifier).
This basic concept is independent of the nature of
a grammar: it can be a rule-based one, manually or
automatically generated; it can comprise a statisti-
cal language model and a classifier; it can consist
of sets of grammars, language models, or classi-
fiers; or it can be a holistic grammar, i.e., a sta-
tistical model combining a language model and a
classification model in one large search tree.
Most commercial dialog systems utilize gram-
mars that return a semantic parse in one of these
contexts:
? directed dialogs (e.g., yes/no contexts, menus
with several choices, collection of informa-
tion out of a restricted set [Which type of
modem do you have?]?usually, less than 50
classes)
? open-ended prompts (e.g. for call routing,
problem capture; likewise to collect infor-
mation out of a restricted set [Tell me what
?Patent pending.
you are calling about today]?possibly sev-
eral hundred classes (Gorin et al, 1997; Boye
and Wiren, 2007))
? information collection out of a huge (or infi-
nite) set of classes (e.g., collection of phone
numbers, dates, names, etc.)
When the performance of spoken dialog sys-
tems is to be measured, there is a multitude of
objective metrics to do so, many of which feature
major disadvantages. Examples include
? Completion rate is calculated as the number
of completed calls divided by the total num-
ber of calls. The main disadvantage of this
metric is that it is influenced by many fac-
tors out of the system?s control, such as caller
hang-ups, opt-outs, or call reasons that fall
out of the system?s scope. Furthermore, there
are several system characteristics that impact
this metric, such as recognition performance,
dialog design, technical stability, availability
of back-end integration, etc. As experience
shows, all of these factors can have unpre-
dictable influence on the completion rate. On
the one hand, a simple wording change in the
introduction prompt of a system can make
this rate improve significantly, whereas, on
the other hand, major improvement of the
open-ended speech recognition grammar fol-
lowing this very prompt may not have any
impact.
? Average holding time is a common term for
the average call duration. This metric is often
considered to be quite controversial since it is
unclear whether longer calls are preferred or
dispreferred. Consider the following two in-
congruous behaviors resulting in longer call
duration:
? The system fails to appropriately treat
callers, asking too many questions, per-
forming redundant operations, acting
unintelligently because of missing back-
end integration, or letting the caller wait
in never-ending wait music loops.
? The system is so well-designed that it
engages callers to interact with the sys-
tem longer.
349
? Hang-up and opt-out rates. These metrics
try to encapsulate how many callers choose
not to use the dialog system, either because
they hang up or because they request to speak
with a human operator. However, it is unclear
how such events are related to dialog system
performance. Certainly, many callers may
have a prejudice against speaking with auto-
mated systems and may hang up or request
a human regardless of how well-performing
the dialog system is with cooperative users.
Furthermore, callers who hang up may do so
because they are unable to get their problem
solved or they may hang up precisely because
their problem was solved (instead of waiting
for the more felicitous post-problem-solving
dialog modules).
? Retry rate is calculated as the average num-
ber of times that the system has to re-prompt
for caller input because the caller?s previ-
ous utterance was determined to be Out-of-
Grammar. The intuition behind this metric
is that the lower the retry rate, the better
the system. However, this metric is prob-
lematic because it is tied to grammar per-
formance itself. Consider a well-performing
grammar that correctly accepts In-Grammar
utterances and rejects Out-of-Grammar utter-
ances. This grammar will cause the system to
produce retries for all Out-of-Grammar utter-
ances. Consider a poorly designed grammar
that accepts everything (incorrectly), even
background noise. This grammar would de-
crease the retry rate but would not be indica-
tive of a well-performing dialog system.
As opposed to these objective measures, there is
a subjective measure directly related to the system
performance as perceived by the user:
? Caller Experience. This metric is used to
describe how well the caller is treated by the
system according to its design. Caller Expe-
rience is measured on a scale between 1 (bad)
and 5 (excellent). This is the only subjective
measure in this list and is usually estimated
based on averaging scores given by multi-
ple voice user interface experts which listen
to multiple full calls. Although this metric
directly represents the ultimate design goal
for spoken dialog systems?i.e., to achieve
highest possible user experience?it is very
expensive to be repeatedly produced and not
suitable to be generated on-the-fly.
Our former research has suggested, however,
that it may be possible to automatically esti-
mate Caller Experience based on several ob-
jective measures (Evanini et al, 2008). These
measures include the overall number of no-
matches and substitutions in a call, opera-
tor requests, hang-ups, non-heard speech, the
fact whether the call reason could be suc-
cessfully captured and whether the call rea-
son was finally satisfied. Initial experiments
showed a near-human accuracy of the auto-
matic predictor trained on several hundred
calls with available manual Caller Experi-
ence scores. The most powerful objective
metric turned out to be the overall number
of no-matches and substitutions, indicating a
high correlation between the latter and Caller
Experience.
No-matches and substitutions are objective met-
rics defined in the scope of semantic classification
of caller utterances. They are part of a larger set of
semantic classification metrics which we system-
atically demonstrate in Section 2. The remainder
of the paper examines three case studies exploring
the usefulness and interplay of different evaluation
metrics, including:
? the correlation between True Total (one of the
introduced metrics) and Caller Experience in
Section 3,
? the estimation of speech recognition and clas-
sification parameters based on True Total and
True Confirm Total (another metric) in Sec-
tion 4, and
? the tuning of large-scale spoken dialog sys-
tems to maximize True Total and its effect on
Caller Experience in Section 5.
2 Metrics for Utterance Classification
Acoustic events processed by spoken dialog sys-
tems are usually split into two main categories:
In-Grammar and Out-of-Grammar. In-Grammar
utterances are all those that belong to one of the
semantic classes processable by the system logic
in the given context. Out-of-Grammar utterances
comprise all remaining events, such as utterances
whose meanings are not handled by the grammar
or when the input is non-speech noise.
Spoken dialog systems usually respond to
acoustic events after being processed by the gram-
mar in one of three ways:
? The event gets rejected. This is when the sys-
tem either assumes that the event was Out-
of-Grammar, or it is so uncertain about its
(In-Grammar) finding that it rejects the utter-
ance. Most often, the callers get re-prompted
for their input.
350
Table 1: Event Acronyms
I In-Grammar
O Out-of-Grammar
A Accept
R Reject
C Correct
W Wrong
Y Confirm
N Not-Confirm
TA True Accept
FA False Accept
TR True Reject
FR False Reject
TAC True Accept Correct
TAW True Accept Wrong
FRC False Reject Correct
FRW False Reject Wrong
FAC False Accept Confirm
FAA False Accept Accept
TACC True Accept Correct Confirm
TACA True Accept Correct Accept
TAWC True Accept Wrong Confirm
TAWA True Accept Wrong Accept
TT True Total
TCT True Confirm Total
? The event gets accepted. This is when the
system is certain to have correctly detected
an In-Grammar semantic class.
? The event gets confirmed. This is when the
system assumes to have correctly detected an
In-Grammar class but still is not absolutely
certain about it. Consequently, the caller is
asked to verify the class. Historically, confir-
mations are not used in many contexts where
they would sound confusing or distracting,
for instance in yes/no contexts (?I am sorry.
Did you say NO????No!???This was NO,
yes????No!!!?).
Based on these categories, an acoustic event and
how the system responds to it can be described by
four binary questions:
1. Is the event In-Grammar?
2. Is the event accepted?
3. Is the event correctly classified?
4. Is the event confirmed?
Now, we can draw a diagram containing the first
two questions as in Table 2. See Table 1 for all
Table 2: In-Grammar? Accepted?
A R
I TA FR
O FA TR
Table 3: In-Grammar? Accepted? Correct?
A R
C W C W
I TAC TAW FRC FRW
O FA TR
acoustic event classification types used in the re-
mainder of this paper.
Extending the diagram to include the third ques-
tion is only applicable to In-Grammar events since
Out-of-Grammar is a single class and, therefore,
can only be either falsely accepted or correctly re-
jected as shown in Table 3.
Further extending the diagram to accomodate
the fourth question on whether a recognized class
was confirmed is similarly only applicable if an
event was accepted, as rejections are never con-
firmed; see Table 4. Table 5 gives one example for
each of the above introduced events for a yes/no
grammar.
When the performance of a given recognition
context is to be measured, one can collect a cer-
tain number of utterances recorded in this context,
look at the recognition and application logs to see
whether these utterances where accepted or con-
firmed and which class they were assigned to, tran-
scribe and annotate the utterances for their seman-
tic class and finally count the events and divide
them by the total number of utterances. If X is an
event from the list in Table 1, we want to refer to
x as this average score, e.g., tac is the fraction of
total events correctly accepted. One characteristic
of these scores is that they sum up to 1 for each of
the Diagrams 2 to 4 as for example
a + r = 1, (1)
i + o = 1, (2)
ta + fr + fa + tr = 1. (3)
In order to enable system tuning and to report
system performance at-a-glance, the multitude of
metrics must be consolidated into a single power-
ful metric. In the industry, one often uses weights
to combine metrics since they are assumed to have
different importance. For instance, a False Ac-
cept is considered worse than a False Reject since
the latter allows for correction in the first retry
whereas the former may lead the caller down the
351
Table 5: Examples for utterance classification metrics. This table shows the transcription of an utterance,
the semantic class it maps to (if In-Grammar), a binary flag for whether the utterance is In-Grammar, the
recognized class (i.e. the grammar output), a flag for whether the recognized class was accepted, a flag
for whether the recognized class was correct (i.e. matched the transcription?s semantic class), a flag
for whether the recognized class was confirmed, and the acronym of the type of event the respective
combination results in.
utterance class In-Grammar? rec. class accepted? correct? confirmed? event
yeah YES 1 I
what 0 O
NO 1 A
NO 0 R
no no no NO 1 NO 1 C
yes ma?am YES 1 NO 0 W
1 Y
0 N
i said no NO 1 YES 1 TA
oh my god 0 NO 1 FA
i can?t tell 0 NO 0 TR
yes always YES 1 YES 0 FR
yes i guess so YES 1 YES 1 1 TAC
no i don?t think so NO 1 YES 1 0 TAW
definitely yes YES 1 YES 0 1 FRC
no man NO 1 YES 0 0 FRW
sunshine 0 YES 1 1 FAC
choices 0 NO 1 0 FAA
right YES 1 YES 1 1 1 TACC
yup YES 1 YES 1 1 0 TACA
this is true YES 1 NO 1 0 1 TAWC
no nothing NO 1 YES 1 0 0 TAWA
Table 4: In-Grammar? Accepted? Correct? Con-
firmed?
A R
C W C W
Y TACC TAWC
I N TACA TAWA FRC FRW
Y FACO N FAA TR
wrong path. However, these weights are heavily
negotiable and depend on customer, application,
and even the recognition context, making it im-
possible to produce a comprehensive and widely
applicable consolidated metric. This is why we
propose to split the set of metrics into two groups:
good and bad. The sought-for consolidated met-
ric is the sum of all good metrics (hence, an over-
all accuracy) or, alternatively, the sum of all bad
events (overall error rate). In Tables 3 and 4, good
metrics are highlighted. Accordingly, we define
two consolidated metrics True Total and True Con-
firm Total as follows:
tt = tac + tr, (4)
tct = taca + tawc + fac + tr. (5)
In the aforementioned special case that a recog-
nition context never confirms, Equation 5 equals
Equation 4 since the confirmation terms tawc and
fac disappear.
The following sections report on three case
studies on the applicability of True Total and True
Confirm Total to the tuning of spoken dialog sys-
tems and how they relate to Caller Experience.
3 On the Correlation between True Total
and Caller Experience
As motivated in Section 1, initial experiments on
predicting Caller Experience based on objective
metrics indicated that there is a considerable cor-
relation between Caller Experience and semantic
352
Table 6: Pearson correlation coefficient for sev-
eral utterance classification metrics on the source
data.
A R
C W
I 0.394 -0.160 ......-0.230......
O -0.242 -0.155
r(TT) = 0.378
classification metrics such as those introduced in
Section 2. In the first of our case studies, this effect
is to be deeper analyzed and quantified. For this
purpose, we selected 446 calls from four different
spoken dialog systems of the customer service hot-
lines of three major cable service providers. The
spoken dialog systems comprised
? a call routing application?cf. (Suendermann
et al, 2008),
? a cable TV troubleshooting application,
? a broadband Internet troubleshooting appli-
cation, and
? a Voice-over-IP troubleshooting
application?see for instance (Acomb et
al., 2007).
The calls were evaluated by voice user interface
experts and Caller Experience was rated according
to the scale introduced in Section 1. Furthermore,
all speech recognition utterances (4480) were tran-
scribed and annotated with their semantic classes.
Thereafter, all utterance classification metrics in-
troduced in Section 2 were computed for every call
individually by averaging across all utterances of
a call. Finally, we applied the Pearson correlation
coefficient (Rodgers and Nicewander, 1988) to the
source data points to correlate the Caller Experi-
ence score of a single call to the metrics of the
same call. This was done in Table 6.
Looking at these numbers, whose magnitude is
rather low, one may be suspect of the findings.
E.g., |r(FR)| > |r(TAW)| suggesting that False
Reject has a more negative impact on Caller Expe-
rience than True Accept Wrong (aka Substitution)
which is against common experience. Reasons for
the messiness of the results are that
? Caller Experience is subjective and affected
by inter- and intra-expert inconsistency. E.g.,
in a consistency cross-validation test, we ob-
served identical calls rated by one subject as
1 and by another as 5.
Figure 1: Dependency between Caller Experience
and True Total.
? Caller Experience scores are discrete, and,
hence, can vary by ?1, even in case of strong
consistency.
? Although utterance classification metrics are
(almost) objective metrics measuring the per-
centage of how often certain events happen
in average, this average generated for indi-
vidual calls may not be very meaningful. For
instance, a very brief call with a single yes/no
utterance correctly classified results in the
same True Total score like a series of 50 cor-
rect recognitions in a 20-minutes conversa-
tion. While the latter is virtually impossible,
the former happens rather often and domi-
nates the picture.
? The sample size of the experiment conducted
in the present case study (446 calls) is per-
haps too small for deep analyses on events
rarely happening in the investigated calls.
Trying to overcome these problems, we com-
puted all utterance classification metrics intro-
duced in Section 2, grouping and averaging them
for the five distinct values of Caller Experience.
As an example, we show the almost linear graph
expressing the relationship between True Total and
Caller Experience in Figure 1. Applying the Pear-
son correlation coefficient to this five-point curve
yields r = 0.972 confirming that what we see is
pretty much a straight line. Comparing this value
to the coefficients produced by the individual met-
rics TAC, TAW, FR, FA, and TR as done in Ta-
ble 7, shows that no other line is as straight as the
one produced by True Total supposing its maxi-
mization to produce spoken dialog systems with
highest level of user experience.
353
Table 7: Pearson correlation coefficient for sev-
eral utterance classification metrics after group-
ing and averaging.
A R
C W
I 0.969 -0.917 ......-0.539......
O -0.953 -0.939
r(TT) = 0.972
4 Estimating Speech Parameters by
Maximizing True Total or True
Confirm Total
The previous section tried to shed some light on
the relationship between some of the utterance
classification metrics and Caller Experience. We
saw that, on average, increasing Caller Experience
comes with increasing True Total as the almost lin-
ear curve of Figure 1 supposes. As a consequence,
much of our effort was dedicated to maximizing
True Total in diverse scenarios. Speech recogni-
tion as well as semantic classification with all their
components (such as acoustic, language, and clas-
sification models) and parameters (such as acous-
tic and semantic rejection and confirmation confi-
dence thresholds, time-outs, etc.) was set up and
tuned to produce highest possible scores. This sec-
tion gives two examples of how parameter settings
influence True Total.
4.1 Acoustic Confirmation Threshold
When a speech recognizer produces a hypothesis
of what has been said, it also returns an acoustic
confidence score which the application can utilize
to decide whether to reject the utterance, confirm
it, or accept it right away. The setting of these
thresholds has obviously a large impact on Caller
Experience since the application is to reject as few
valid utterances as possible, not confirm every sin-
gle input, but, at the same time, not falsely accept
wrong hypotheses. It is also known that these set-
tings can strongly vary from context to context.
E.g., in announcements, where no caller input is
expected, but, nonetheless utterances like ?agent?
or ?help? are supposed to be recognized, rejection
must be used much more aggressively than in col-
lection contexts. True Total or True Confirm To-
tal are suitable measures to detect the optimum
tradeoff. Figure 2 shows the True Confirm Total
graph for a collection context with 30 distinguish-
able classes. At a confidence value of 0.12, there
is a local and global maximum indicating the opti-
mum setting for the confirmation threshold for this
grammar context.
Figure 2: Tuning the acoustic confirmation thresh-
old.
4.2 Maximum Speech Time-Out
This parameter influences the maximum time the
speech recognizer keeps recognizing once speech
has started until it gives up and discards the recog-
nition hypothesis. Maximum speech time-out is
primarily used to limit processor load on speech
recognition servers and avoid situations in which
line noise and other long-lasting events keep the
recognizer busy for an unnecessarily long time. As
it anecdotally happened to callers that they were
interrupted by the dialog system, on the one hand,
some voice user interface designers tend to chose
rather large values for this time-out setting, e.g.,
15 or 20 seconds. On the other hand, very long
speech input tends to produce more likely a clas-
sification error than shorter ones. Might there be a
setting which is optimum from the utterance clas-
sification point of view?
To investigate this behavior, we took 115,885
transcribed and annotated utterances collected in
the main collection context of a call routing ap-
plication and aligned them to their utterance dura-
Figure 3: Dependency between utterance duration
and True Total.
354
Figure 4: Dependency between maximum speech
time-out and True Total.
tions. Then, we ordered the utterances in descend-
ing order of their duration, grouped always 1000
successive utterances together, and averaged over
duration and True Total. This generated 116 data
points showing the relationship between the dura-
tion of an utterance and its expected True Total,
see Figure 3.
The figure shows a clear maximum somewhere
around 2.5 seconds and then descends with in-
creasing duration towards zero. Utterances with
a duration of 9 seconds exhibited a very low True
Total score (20%). Furthermore, it would appear
that one should never allow utterances to exceed
four second in this context. However, upon fur-
ther evaluation of the situation, we also have to
consider that long utterances occur much less fre-
quently than short ones. To integrate the frequency
distribution into this analysis, we produced an-
other graph that shows the average True Total ac-
cumulated over all utterances shorter than a cer-
tain duration. This simulates the effect of using
a different maximum speech time-out setting and
is displayed in Figure 4. We also show a graph
on how many of the utterances would have been
interrupted in Figure 5.
The curve shows an interesting down-up-down
trajection which can be explained as follows:
? Acoustic events shorter than 1.0 seconds are
mostly noise events which are correctly iden-
tified since the speech recognizer could not
even build a search tree and returns an empty
hypothesis which the classifier, in turn, cor-
rectly rejects.
? Utterances with a duration around 1.5s are
dominated by single words which cannot
properly evaluated by the (trigram) language
model. So, the acoustic model takes over the
main work and, because of its imperfectness,
lowers the True Total.
Figure 5: Percentage of utterances interrupted by
maximum speech time-out.
? Utterances with a moderate number of words
are best covered by the language model, so
we achieve highest accuracy for them (?3s).
? The longer the utterances continues after 4
seconds, the less likely the language model
and classfier are to have seen such utterances,
and True Total declines.
Evaluating the case from the pure classifier per-
formance perspective, the maximum speech time-
out would have to be set to a very low value
(around 3 seconds). However, at this point, about
20% of the callers would be interrupted. The deci-
sion whether this optimimum should be accepcted
depends on how elegantly the interruption can be
designed:
?I?m so sorry to interrupt, but I?m hav-
ing a little trouble getting that. So, let?s
try this a different way.?
5 Continuous Tuning of a Spoken Dialog
System to Maximize True Total and Its
Effect on Caller Experience
In the last two sections, we investigated the corre-
lation between True Total and Caller Experience
and gave examples on how system parameters can
be tuned by maximizing True Total. The present
section gives a practical example of how rigorous
improvement of utterance classification leads to
real improvement of Caller Experience.
The application in question is a combination of
the four systems listed in Section 3 which work
in an interconnected fashion. When callers access
the service hotline, they are first asked to briefly
describe their call reason. After up to two follow-
up questions to further disambiguate their reason,
they are either connected to a human operator or
one of the three automated troubleshooting sys-
tems. Escalation from one of them can connect
355
Figure 6: Increase of the True Total of a large-
vocabulary grammar with more than 250 classes
over release time.
the caller to an agent, transfer the caller back to
the call router or to one of the other troubleshoot-
ing systems.
When the application was launched in June
2008, its True Total averaged 78%. During the fol-
lowing three months, almost 2.2 million utterances
were collected, transcribed, and annotated for their
semantic classes to train statistical update gram-
mars in a continuously running process (Suender-
mann et al, 2009). Whenever a grammar sig-
nificantly outperformed the most recent baseline,
it was released and put into production leading
to an incremental improvement of performance
throughout the application. As an example, Fig-
ure 6 shows the True Total increase of the top-level
large-vocabulary grammar that distinguishes more
than 250 classes. The overall performance of the
application went up to more than 90% True Total
within three months of its launch.
Having witnessed a significant gain of a spoken
dialog system?s True Total, we would now like to
know to what extent this improvement manifests
itself in an increase of Caller Experience. Fig-
ure 7 shows that, indeed, Caller Experience was
strongly positively affected. Over the same three
month period, we achieved an iterative increase
from an initial Caller Experience of 3.4 to 4.6.
6 Conclusion
Several of our investigations have suggested a con-
siderable correlation between True Total, an objec-
tive utterance classification metric, and Caller Ex-
perience, a subjective score of overall system per-
formance usually rated by expert listeners. This
observation leads to our main conclusions:
? True Total and several of the other utterance
classification metrics introduced in this paper
can be used as input to a Caller Experience
predictor?as tentative results in (Evanini et
al., 2008) confirm.
Figure 7: Increase of Caller Experience over re-
lease time.
? Efforts towards improvement of speech
recognition in spoken dialog applications
should be focused on increasing True Total
since this will directly influence Caller Expe-
rience.
References
K. Acomb, J. Bloom, K. Dayanidhi, P. Hunter,
P. Krogh, E. Levin, and R. Pieraccini. 2007. Techni-
cal Support Dialog Systems: Issues, Problems, and
Solutions. In Proc. of the HLT-NAACL, Rochester,
USA.
J. Boye and M. Wiren. 2007. Multi-Slot Semantics for
Natural-Language Call Routing Systems. In Proc.
of the HLT-NAACL, Rochester, USA.
K. Evanini, P. Hunter, J. Liscombe, D. Suendermann,
K. Dayanidhi, and R. Pieraccini:. 2008. Caller Ex-
perience: A Method for Evaluating Dialog Systems
and Its Automatic Prediction. In Proc. of the SLT,
Goa, India.
A. Gorin, G. Riccardi, and J. Wright. 1997. How May
I Help You? Speech Communication, 23(1/2).
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing Grammar-
Based and Robust Approaches to Speech Under-
standing: A Case Study. In Proc. of the Eurospeech,
Aalborg, Denmark.
J. Rodgers and W. Nicewander. 1988. Thirteen Ways
to Look at the Correlation Coefficient. The Ameri-
can Statistician, 42(1).
D. Suendermann, P. Hunter, and R. Pieraccini. 2008.
Call Classification with Hundreds of Classes and
Hundred Thousands of Training Utterances ... and
No Target Domain Data. In Proc. of the PIT, Kloster
Irsee, Germany.
D. Suendermann, J. Liscombe, K. Evanini,
K. Dayanidhi, and R. Pieraccini. 2009. From
Rule-Based to Statistical Grammars: Continu-
ous Improvement of Large-Scale Spoken Dialog
Systems. In Proc. of the ICASSP, Taipei, Taiwan.
356
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 257?260,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
How to Drink from a Fire Hose:
One Person Can Annoscribe 693 Thousand Utterances in One Month
David Suendermann, Jackson Liscombe, Roberto Pieraccini
SpeechCycle Labs
New York, USA
{david, jackson, roberto}@speechcycle.com
Abstract
.
Transcription and semantic annotation
(annoscription) of utterances is crucial
part of speech performance analysis and
tuning of spoken dialog systems and other
natural language processing disciplines.
However, the fact that these are manual
tasks makes them expensive and slow. In
this paper, we will discuss how anno-
scription can be partially automated. We
will show that annoscription can reach a
throughput of 693 thousand utterances per
person month under certain assumptions.
1 Introduction
Ever since spoken dialog systems entered the com-
mercial market in the mid 1990s, the caller?s
speech input is subject to collection, transcription,
and often also semantic annotation. Utterance
transcriptions and annotations (annoscriptions) are
used to measure speech recognition and spoken
language understanding performance of the appli-
cation. Furthermore, they are used to improve
speech recognition and application functionality
by tuning grammars, introducing new transitions
in the call flow to cover more of the callers? de-
mands, or changing prompt wording or applica-
tion logic to influence the speech input. Anno-
scriptions are also crucial for training statistical
language models and utterance classifiers for call
routing or other unconstrained speech input con-
texts (Gorin et al, 1997). Since very recently, sta-
tistical methods are used to replace conventional
rule-based grammars in every recognition context
of commercial spoken dialog systems (Suender-
mann et al, 2009b). This replacement is only
possible by collecting massive amounts of anno-
scribed data from all contexts of an application.
To give the reader an idea of what massive means
in this case, in (Suendermann et al, 2009b), we
used 2,184,203 utterances to build a complex call
routing system. In (Suendermann et al, 2009a),
4,293,898 utterances were used to localize an En-
glish Internet troubleshooting application to Span-
ish.
Considering that professional service providers
may charge as much as 50 US cents for annoscrib-
ing a single utterance, the usage of these amounts
of data seems prohibitive since costs for such a
project could potentially add up to several million
US dollars. Furthermore, one has to consider the
average speed of annoscription which rarely ex-
ceeds 1000 utterances per hour and person. This
means that the turn-around of a project as men-
tioned above would be several years unless teams
of many people work simultaneously. However,
the integration of the work of a large team be-
comes the more tricky the more people are in-
volved. This is especially true for the annotation
portion since it requires a thorough understand-
ing of the spoken dialog system?s domain and de-
sign and very often can only be conducted under
close supervision by the interaction designer in
charge of the project. Furthermore, there are cru-
cial issues related to intra- and inter-labeler incon-
sistency becoming more critical the more people
work on the same or similar recognition contexts
of a given project.
This paper is to show how it is possible to au-
tomate large portions of both transcription and an-
notation while meeting human performance1 stan-
dards. As an example case, we show how the pro-
posed automation techniques can increase anno-
scription speed to nearly 693 thousand utterances
per person and month.
2 Automatic Transcription
2.1 Two Fundamentals
Automatic transcription of spoken utterances may
not sound as something new to the reader. In
fact, the entire field of automatic speech recogni-
tion is about machine transcription. So, why is it
worth dedicating a full section to something well-
covered in research and industry for half a cen-
tury? The reason is the demand for achieving hu-
man performance as formulated in the introduc-
tion which, as is also well-known, cannot be satis-
fied by any of the large-vocabulary speech recog-
nizers ever developed. In order to demonstrate that
there is indeed a way to achieve human transcrip-
tion performance using automatic speech recogni-
tion, we would like to refer to two fundamental
observations on the performance of speech recog-
1In this paper, performance stands for quality or accuracy
of transcription or annotation. It does not refer to speed or
throughput.
257
nition:
(1) Speech recognition performance can be very
high for contexts of constrained vocabulary. An
example is the recognition of isolated letters in
the scope of a name spelling task as discussed
in (Waibel and Lee, 1990) that achieved a word
error rate of only 1.1%. In contrast, the word error
rate of large-vocabulary continuous speech recog-
nition can be as high as 40 to 65% on telephone
speech (Yuk and Flanagan, 1999).
(2) The positive dependence between speech
recognition performance and amount of data used
to train acoustic and language models, so far, did
not reach a saturation point even considering bil-
lions of training tokens (Och, 2006).
Both of these fundamentals can be applied to the
transcription task for utterances collected on spo-
ken dialog production systems as follows:
(1) The vocabulary of spoken dialog systems can
be rather complex. E.g., the caller utterances used
for the localization project mentioned in Section 1
distinguish more than 13,000 types. However,
the nature of commercial spoken dialog applica-
tions being mostly system-driven strongly con-
strains the vocabulary in many recognition con-
texts. E.g., when the prompt reads
You can say: recording problems, new
installation, frozen screen, or won?t turn
on
callers mostly respond things matching the pro-
posed phrases, occasionally altering the wording,
and only seldomly using completely unexpected
utterances.
(2) The continuous data feed available on high-
traffic spoken dialog systems in production pro-
cessing millions of calls per month can provide
large numbers of utterances for every possible
recognition context. Even if the context appears to
be of a simple nature, as for a yes/no question, the
continuous collection of more data will still have
an impact on the performance of a language model
built using this data.
2.2 How to Achieve Human Performance
Even though we have suggested that the recog-
nition performance in many contexts of spoken
dialog systems may be very high, we have still
not shown how our observations can be utilized to
achieve human performance as demanded in Sec-
tion 1. How would a context-dependent speech
recognizer respond when the caller says some-
thing completely unexpected such as let?s wreck a
nice beach when asked for the cell phone number?
While a human transcriber may still be able to cor-
rectly transcribe this sentence, automatic speech
recognition will certainly fail even with the largest
possible training set. The answer to this question
is that the speech recognizer should not respond at
all in this case but admit that it had trouble rec-
ognizing this utterance. Rejection of hypotheses
based on confidence scores is common practice in
many speech and language processing tasks and
is heavily used in spoken dialog systems to avoid
mis-interpretation of user inputs.
So, we now know that we can limit automatic
transcriptions to hypotheses of a minimum relia-
bility. However, how do we prove that this limited
set resembles human performance? What is actu-
ally human performance? Does the human make
errors transcribing? And, if so, how do we mea-
sure human error? What do we compare it against?
To err is human. Accordingly, there is an error
associated with manual transcription which can
only be estimated by comparing somebody?s tran-
scription with somebody else?s due to a lack of
ground truth. Preferably, one should have a good
number of people transcribe the same speech ut-
terances and than compute the average word error
rate comparing every transcription batch with ev-
ery other producing a reliable estimate of the man-
ual error inherent to the transcription task of spo-
ken dialog system utterances. In order to do so,
we compared transcriptions of 258,843 utterances
collected from a variety of applications and recog-
nition contexts partially shared by up to six tran-
scribers and found that they averaged at an inter-
transcriber word error rate of WER0 = 1.3%.
Now, for every recognition context a language
model had been trained, we performed automatic
speech recognition on held-out test sets of N =
1000 utterances producing N hypotheses and their
associated confidence scores P = {p1, . . . pN}.
Now, we determined that minimum confidence
threshold p0 for which the word error rate between
the set of hypotheses and manual reference tran-
scriptions was not statistically significantly greater
than WER0:
p0 = arg min
p?P
WER(V (p)) ?6> WER0; (1)
V (p) = {?1, . . . , ?K} : ?k ? {1, . . . , N}, p?k ? p.
Statistical significance was achieved when the
delta resulted in a p value greater than 0.05 using
the ?2 calculus. For the number of test utterances,
1000, this point is reached when the word error
on the test set falls below WER1 = 2.2%. This
means that Equation 2.2?s ?not statistically signifi-
cantly greater than? sign can be replaced by a reg-
ular smaller-than sign as
WER ?6> WER0 ? WER < WER1. (2)
This essentially means that there is a chance that
the error produced by automatic transcription is
greater than that of manual transcription, however,
on the test set it could not be found to be of signifi-
cance. Requesting to lower the p value or even de-
manding that the test set performance falls below
the reported manual error can drastically lower the
automation rate and, in the latter case, is not even
reasonable?how can a machine possibly commit
258
tr
ai
n
in
g
u
tte
ra
n
ce
s
. tran
scriptio
n
auto
m
atio
n
rate
. training date
Figure 1: Dependency between amount of training
data and transcription automation rate
less errors than a human being as it is trained on
human transcriptions?
As a proof of concept, we ran automatic tran-
scription against the same set of utterances used
to determine the manual transcription error, and
we found that the average word error rate between
manual and automatic annotation was as low as
1.1% for all utterances whose confidence score ex-
ceeded the context-dependent threshold trained as
described above. In this initial experiment, a total
of 60,608 utterances, i.e., 23.4%, had been auto-
mated.
2.3 On Automation Rate
Formally, transcription automation rate is the ra-
tio of utterances whose confidence exeeded p0 in
Equation 2.2:
transcription automation rate = |V (p0)|
N
(3)
where |V | refers to the cardinality of the set V ,
i.e., the number of V ?s members.
The above example?s transcription automation
rate of 23.4% does not yet sound tremendously
high, so we should look at what can be done to
increase the automation rate as much as possible.
It is predictable that the two fundamentals formu-
lated in Section 2.1 have a large impact on recog-
nition performance and, hence, the transcription
automation rate:
(1) In large-scale experiments, we were able to
show a significant (negative) correlation between
the annotation automation rate and task complex-
ity. Since this study does not fit the present paper?s
scope, we will refrain from reporting on details at
this point.
(2) As an example which influence the amount of
training data can have on the transcription automa-
tion rate, Figure 1 shows statistics drawn from
twenty runs of language model training carried out
over the course of seven months while collecting
more and more data.
3 Automatic Annotation
Semantic annotation of utterances into one of a fi-
nal set of classes is a task which may require pro-
found understanding of the application and recog-
nition context the specific utterances were col-
lected in. Examples include simple contexts such
as yes/no questions which may be easily manage-
able also by annotators unfamiliar with the ap-
plication, high-resolution open prompt contexts
with hundreds of technical and highly application-
specific classes, or number collection contexts al-
lowing for billions of classes. All these contexts
can benefit from two rules which help to signifi-
cantly reduce an annotator?s workload:
(A) Never do anything twice. This simple state-
ment means that there should be functionality built
into the annotation software or the underlying
database that
? lets the annotator process multiple utterances
with identical transcription in a single step and
? makes sure that whenever a new utterance shows
up with a transcription identical to a formerly an-
notated one, the new utterance gets assigned the
same class automatically.
Figure 2 demonstrates the impact of Rule (A) with
two typical examples. The first is a yes/no context
allowing for the additional global commands help,
hold, agent, repeat, and i don?t know. The other is
an open prompt context distinguishing 79 classes.
When using the token/type distinction, the im-
pact of Rule (A) is that annotation effort becomes
linear with the number of types to work on. While
the ratio between types and tokens in a given cor-
pus can be very small (i.e., the automation rate is
very high, e.g., 95% in the above yes/no example),
this ratio reaches saturation at some point. In the
yes/no example, there is only a gradual difference
between the automation rates for 10 thousand and
1 million utterances. Hence, at a certain point, the
effort becomes virtually linear with the number of
tokens to be processed.
(B) Predict as much as possible. Most of the
recognition contexts for which utterances are tran-
scribed and annotated use grammars to implement
speech recognition functionality. Many of these
an
n
o
ta
tio
n
au
to
m
at
io
n
ra
te
. training utterances
Figure 2: Dependency between number of col-
lected utterances and annotation automation rate
based on Rule (A) for two different contexts
259
Table 1: Annotation automation rates for three dif-
ferent recognition contexts based on Rule (B)
.
grammar #symptoms ann. auto. rate
modem type 43 70.3%
blue/black/snow 10 77.0%
yes/no 10 88.6%
grammars will be rule-based grammars. Even if
the grammars are statistical, most often, earlier
in time, rule-based grammars had been used in
the same recognition context. Hence, we can as-
sume that we are given rule-based grammars for
many recognition contexts of the dialog system
in question. Per definition, rule-based grammars
shall contain canonical rules expressing the rela-
tionship between expected utterances in a given
context and the semantic classes these utterances
are to be associated with. Consequently, when-
ever for an utterance recorded in the context un-
der consideration there is a rule in the grammar,
it provides the correct class for this utterance, and
it can be excluded from annotation. These rules
can be strongly extended to allow for complex pre-
fix and suffix rules, repetitions, sub-grammars &c.
making sure that the majority of utterances will
be covered by the rule-based grammars thereby
minimizing the annotation effort. Table 1 shows
three example grammars of different complex-
ity: One that collects the type of the caller?s mo-
dem, one for the identification of a TV set?s pic-
ture color (blue/black/snow), and a yes/no con-
text with global commands. Annotation automa-
tion rates for these grammars that were not specif-
ically tuned for maximizing automation but di-
rectly taken from the production dialog systems
varied between 70.3% and 88.6%.
To never ever touch a formerly annotated utter-
ance type again and to blindly rely on (mayby out-
dated or erroneous) rule-based grammars to pro-
vide baseline annotations may result in annota-
tion mistakes, possibly major ones when frequent
utterances are concerned. So, how do we make
sure that high annotation performance standards
are met?
To answer this question, the authors have de-
veloped a set of techniques called C7 taking care
of completeness, consistency, congruence, corre-
lation, confusion, coverage, and corpus size of an
annotation set (Suendermann et al, 2008). The
mentioned techniques are also useful in the fre-
quent event of changes to the number or scope of
annotation classes. This can happen e.g. due to
functional changes to the application, changes to
prompts, user behavior, or to contexts preceeding
the current annotation context. Another frequent
reason is the introduction of additional classes to
enlarge the scope of the current context2.
2In a specific context, callers may be asked whether they
want A, B, or C, but they may respond D. The introduc-
tion of a new class D which the application is able to handle
4 693 Thousand Utterances
Finally, we want to return to the initial statement
of this paper claiming that one person is able to
annoscribe 693 thousand utterances within one
month. An approximated automation rate of 80%
for transcription and 90% for annotation is possi-
ble when there is already a massive database of
annoscriptions available to be exploited for au-
tomation. These rates result in about 139 thou-
sand transcriptions and 69 thousand annotations
outstanding. At a pace of 1000 transcribed or 2000
annotated utterances per hour, the required time
would be 139 hours transcription and 35 hours an-
notation which averages at 40 hours per week3.
5 Conclusion
This paper has demonstrated how automated
annoscription of utterances collected in the
production scope of spoken dialog systems can
effectively accelerate this conventionally entirely
manual effort. When allowing for some overtime,
we have shown that a single person is able to
produce 693 thousand annoscriptions within one
month.
References
A. Gorin, G. Riccardi, and J. Wright. 1997. How May
I Help You? Speech Communication, 23(1/2).
F. Och. 2006. Challenges in Machine Translation. In
Proc. of the TC-Star Workshop, Barcelona, Spain.
D. Suendermann, J. Liscombe, K. Evanini,
K. Dayanidhi, and R. Pieraccini. 2008. C5.
In Proc. of the SLT, Goa, India.
D. Suendermann, J. Liscombe, K. Dayanidhi, and
R. Pieraccini. 2009a. Localization of Speech
Recognition in Spoken Dialog Systems: How Ma-
chine Translation Can Make Our Lives Easier. In
Proc. of the Interspeech, Brighton, UK.
D. Suendermann, J. Liscombe, K. Evanini,
K. Dayanidhi, and R. Pieraccini. 2009b. From
Rule-Based to Statistical Grammars: Continu-
ous Improvement of Large-Scale Spoken Dialog
Systems. In Proc. of the ICASSP, Taipei, Taiwan.
A. Waibel and K.-F. Lee. 1990. Readings in Speech
Recognition. Morgan Kaufmann, San Francisco,
USA.
D. Yuk and J. Flanagan. 1999. Telephone Speech
Recognition Using Neural Networks and Hidden
Markov Models. In Proc. of the ICASSP, Phoenix,
USA.
requires the re-annotation of all utterances falling into D?s
scope.
3The original title of this paper claimed that one person
could annoscribe even one million utterances in a month.
However, after receiving multiple complaints about the un-
lawfulness of a 58-hour workweek, we had to change the title
accordingly to avoid disputes with the Department of Labor.
Furthermore, as discussed earlier, at the starting point of an
annoscription project, automation rates are much lower than
later.
260
