Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 173?176,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT: Web based Scoring Method for English Lexical Substitution 
Shiqi Zhao, Lin Zhao, Yu Zhang, Ting Liu, Sheng Li 
Information Retrieval Laboratory, School of Computer Science and Technology, 
Box 321, Harbin Institute of Technology 
Harbin, P.R. China, 150001 
{ zhaosq, lzhao, zhangyu, tliu, lisheng }@ir.hit.edu.cn 
 
 
Abstract 
This paper describes the HIT system and its 
participation in SemEval-2007 English 
Lexical Substitution Task. Two main steps 
are included in our method: candidate sub-
stitute extraction and candidate scoring. In 
the first step, candidate substitutes for each 
target word in a given sentence are ex-
tracted from WordNet. In the second step, 
the extracted candidates are scored and 
ranked using a web-based scoring method. 
The substitute ranked first is selected as the 
best substitute. For the multiword subtask, 
a simple WordNet-based approach is em-
ployed. 
1 Introduction 
Lexical substitution aims to find alternative words 
that can occur in given contexts. It is important in 
many applications, such as query reformulation in 
question answering, sentence generation, and 
paraphrasing. There are two key problems in the 
lexical substitution task, the first of which is 
candidate substitute extraction. Generally speaking, 
synonyms can be regarded as candidate substitutes 
of words. However, some looser lexical 
relationships can also be considered, such as 
Hypernyms and Hyponyms defined in WordNet 
(Fellbaum, 1998). In addition, since lexical 
substitution is context dependent, some words 
which do not have similar meanings in general 
may also be substituted in some certain contexts 
(Zhao et al, 2007). As a result, finding a lexical 
knowledge base for substitute extraction is a 
challenging task. 
The other problem is candidate scoring and 
ranking according to given contexts. In the lexical 
substitution task of SemEval-2007, context is con-
strained as a sentence. The system therefore has to 
score the candidate substitutes of each target word 
using the given sentence. The following questions 
should be considered here: (1) What words in the 
given sentence are ?useful? context? (2) How to 
combine the context words and use them in rank-
ing candidate substitutes? For the first question, we 
can use all words of the sentence, words in a win-
dow, or words having syntactic relations with the 
target word. For the second question, we can re-
gard the context words as ?bag of words?, n-grams, 
or syntactic structures. 
In HIT, we extract candidate substitutes from 
WordNet, in which both synonyms and hypernyms 
are investigated (Section 3.1). After that, we score 
the candidates using a web-based scoring method 
(Section 3.2). In this method, we first select frag-
ments containing the target word from the given 
sentence. Then we construct queries by replacing 
the target word in the fragments with the candidate 
substitute. Finally, we search Google using the 
constructed queries and score each candidate based 
on the counts of retrieved snippets. 
The rest of this paper is organized as follows: 
Section 2 reviews some related work on lexical 
substitution. Section 3 describes our system, espe-
cially the web-based scoring method. Section 4 
presents the results and analysis. 
2 Related Work 
Synonyms defined in WordNet have been widely 
used in lexical substitution and expansion (Smea-
ton et al, 1994; Langkilde and Knight, 1998; Bol-
173
shakov and Gelbukh, 2004). In addition, a lot of 
methods have been proposed to automatically con-
struct thesauri of synonyms. For example, Lin 
(1998) clustered words with similar meanings by 
calculating the dependency similarity. Barzilay and 
McKeown (2001) extracted paraphrases using mul-
tiple translations of literature works. Wu and Zhou 
(2003) extracted synonyms with multiple resources, 
including a monolingual dictionary, a bilingual 
corpus, and a monolingual corpus. Besides the 
handcrafted and automatic synonym resources, the 
web has been exploited as a resource for lexical 
substitute extraction (Zhao et al, 2007). 
As for substitute scoring, various methods have 
been investigated, among which the classification 
method is the most widely used (Dagan et al, 2006; 
Kauchak and Barzilay, 2006). In detail, a binary 
classifier is trained for each candidate substitute, 
using the contexts of the substitute as features. 
Then a new contextual sentence containing the tar-
get word can be classified as 1 (the candidate is a 
correct substitute in the given sentence) or 0 (oth-
erwise). The features used in the classification are 
usually similar with that in word sense disam-
biguation (WSD), including bag of word lemmas 
in the sentence, n-grams and parts of speech (POS) 
in a window, etc. There are other models presented 
for candidate substitute scoring. Glickman et al 
(2006) proposed a Bayesian model and a Neural 
Network model, which estimate the probability of 
a word may occur in a given context. 
3 HIT System 
3.1 Candidate Substitute Extraction 
In HIT, candidate substitutes are extracted from 
WordNet. Both synonyms and hypernyms defined 
in WordNet are investigated. Let w be a target 
word, pos the specified POS of w. n the number of 
w?s synsets defined in WordNet. Then the system 
extracts w?s candidate substitutes as follows: 
z Extracts all the synonyms in each synset 
under pos1 as candidate substitutes. 
z If w has no synonym for the i-th synset 
(1?i?n), then extracts the synonyms of its 
nearest hypernym. 
z If pos is r (or a), and no candidate substi-
tute can be extracted as described above, 
                                                 
1 In this task, four kinds of POS are specified: n - noun, v - 
verb, a - adjective, r - adverb.  
then extracts candidate substitutes under the 
POS a (or r). 
3.2 Candidate Substitute Scoring 
As mentioned above, all words in the given sen-
tence can be used as contextual information in the 
scoring of candidate substitutes. However, it is ob-
vious that not all context words are really useful 
when determining a word?s substitutes. An exam-
ple can be seen from Figure 1. 
 
 
She turns eyes <head>bright</head> with 
excitement towards Fiona , still tugging on the 
string of the minitiature airship-cum-dance 
card she has just received at the door . 
Figure 1. An example of a context sentence. 
 
In the example above, words turns, eyes, with, 
and excitement are useful context words, while the 
others are not. The useless contexts may even be 
noise if they are used in the scoring. As a result, it 
is important to select context words carefully. 
In HIT, we select context words based on the 
following assumption: useful context words for 
lexical substitute are those near the target word in 
the given sentence. In other words, the words that 
are far from the target word are not taken into con-
sideration. Obviously, this assumption is not al-
ways true. However, considering only the 
neighboring words can reduce the risk of bringing 
in noise. Besides, Edmonds (1997) has also dem-
onstrated in his paper that short-distance colloca-
tions with neighboring words are more useful in 
lexical choice than long ones. 
Let w be the target word, t a candidate substitute, 
S the context sentence. Our basic idea is that: One 
can substitute w in S with t, which generates a new 
sentence S?. If S? can be found on the web, then the 
substitute is admissible. The more times S? occurs 
on the web, the more probable the substitute is. In 
practice, however, it is difficult to find a whole 
sentence S? on the web due to sparseness. Instead, 
we use fragments of S? which contains t and sev-
eral neighboring context words (based on the as-
sumption above). Then the question is how to ob-
tain one (or more) fragment of S?. 
A window with fixed size can be used here. Su-
ppose p is the position of t in S?, for instance, we 
can construct a fragment using words from posi-
tion p-r to p+r, where r is the radius of window. 
174
However, a fixed r is difficult to set, since it may 
be too large for some sentences, which makes the 
fragments too specific, while too small for some 
other sentences, which makes the fragments too 
loose. An example can be seen in Table 1. 
 
1(a) But when Daniel turned <head>blue</head> 
one time and he totally stopped breathing. 
1(b) Daniel turned t one time 
2(a) We recommend that you <head>check</head> 
with us beforehand. 
2(b) that you t with us 
Table 1. Examples of fragments with fixed size. 
 
In Table1, 1(a) and 2(a) are two sentences from 
the test data of SemEval-2007Task10. 1(b) and 2(b) 
are fragments constructed according to 1(a) and 
2(a), where the window radius is 2 and t denotes 
any candidate substitute of the target word. It is 
obvious that 1(b) is a rather strict fragment, which 
makes it difficult to find sentences containing it on 
the web, while 2(b) is quite loose, which can 
hardly constrain the semantics of t. 
Having considered the problem above, we pro-
pose a rule-based method that constructs fragments 
with varied lengths. Let Ft be a fragment contain-
ing t, the construction rules are as follows: 
Rule-1: Ft must contain at least two words be-
sides t, at least one of which is non-stop word. 
Rule-2: Ft does not cross sub-sentence boundary 
(?,?). 
Rule-3: Ft should be the shortest fragment that 
satisfies Rule-1 and Rule-2. 
According to the rules above, we construct at 
most three fragments for each S?: (1) t occurs at the 
beginning of Ft, (2) t occurs in the middle of Ft, 
and (3) t occurs at the end of Ft. Here we have an-
other constraint: if one constructed fragment F1 is 
the substring of F2, then F2 is removed. Please 
note that the morphology is not taken into account 
when we construct queries. 
For the sentence 1(a) and 2(a) in Table 1, the 
constructed fragments are as follows: 
 
For 1(a): Daniel turned t; t one time; turned t 
one 
For 2(a): recommend that you t; t with us be-
forehand 
Table 2. Examples of the constructed fragments 
 
To score a candidate substitute, we replace ?t? in 
the fragments with each candidate substitute and 
use them as queries, which are then fed to Google. 
The score of t is computed according to the counts 
of retrieved snippets: 
?
=
=
n
i
tWebMining iFSnippetcountn
tScore
1
))((
1
)(     (1) 
where n is the number of constructed fragments, 
Fti is the i-th fragment (query) corresponding to t, 
and count(Snippet(Fti)) is the count of snippets 
retrieved by Fti. 
All candidate substitutes with scores larger than 
0 are ranked and the first 10 substitutes are re-
tained for the oot subtask. If the number of candi-
dates whose scores are larger than 0 is less than 10, 
the system ranks the rest of the candidates by their 
frequencies using a word frequency list. The spare 
capacity is filled with those candidates with largest 
frequencies. For the best subtask, we simply output 
the substitute that ranks first in oot. 
3.3 Detection of Multiwords 
The method used to detect multiword in the HIT 
system is quite similar to that employed in the 
baseline system. We also use WordNet to detect if 
a multiword that includes the target word occurs 
within a window of 2 words before and 2 words 
after the target word.  
A difference from the baseline system lies in 
that our system looks up WordNet using longer 
multiword candidates first. If a longer one is found 
in WordNet, then its substrings will be ignored. 
For example, if we find ?get alng with? in Word-
Net, we will output it as a multiword and will not 
check ?get alng? any more. 
4 Results 
Our system is the only one that participates all the 
three subtasks of Task10, i.e., best, oot, and mw. 
The evaluation results of our system can be found 
in Table 3 to Table 5. Our system ranks the fourth 
in the best subtask and seventh in the oot subtask. 
We have analyzed the results from two aspects, 
i.e., the ability of the system to extract candidate 
substitutes and the ability to rank the correct sub-
stitutes in front. There are a total of 6,873 manual 
substitutes for all the 1,710 items in the gold stan-
dard, only 2,168 (31.54%) of which have been ex-
tracted as candidate substitutes by our system. This 
result suggests that WordNet is not an appropriate 
175
source for lexical substitute extraction. In the fu-
ture work, we will try some other lexical resources, 
such as the Oxford American Writer Thesaurus 
and Encarta. In addition, we will also try the 
method that automatically constructs lexical re-
sources, such as the automatic clustering method. 
Further analysis shows that, 1,388 (64.02%) out 
of the 2,168 extracted correct candidates are 
ranked in the first 10 in the oot output of our sys-
tem. This suggests that there is a big space for our 
system to improve the candidate scoring method. 
In the future work, we will consider more and 
richer features, such as the syntactic features, in 
candidate substitute scoring. Furthermore, A dis-
advantage of this method is that the web mining 
process is quite inefficient. Therefore, we will try 
to use the Web 1T 5-gram Version 1 from Google 
(LDC2006T13) in the future. 
 
 P R ModeP ModeR
OVERALL 11.35 11.35 18.86 18.86 
Further Analysis 
NMWT 11.97 11.97 19.81 19.81 
NMWS 12.55 12.38 19.93 19.65 
RAND 11.81 11.81 20.03 20.03 
MAN 10.81 10.81 17.53 17.53 
Baselines 
WORDNET 9.95 9.95 15.58 15.58 
LIN 8.84 8.53 14.69 14.23 
Table 3. best results. 
 
 P R ModeP ModeR
OVERALL 33.88 33.88 46.91 46.91 
Further Analysis 
NMWT 35.60 35.60 48.48 48.48 
NMWS 36.63 36.63 49.33 49.33 
RAND 33.95 33.95 47.25 47.25 
MAN 33.81 33.81 46.53 46.53 
Baselines 
WORDNET 29.70 29.35 40.57 40.57 
LIN 27.70 26.72 40.47 39.19 
Table 4. oot results. 
 
 Our System WordNet BL 
 P R P R 
detection 45.34 56.15 43.64 36.92
identification 41.61 51.54 40.00 33.85
Table 5. mw results. 
 
Acknowledgements 
This research was supported by National Natural 
Science Foundation of China (60575042, 
60503072, 60675034). 
References 
Barzilay Regina and McKeown Kathleen R. 2001. Ex-
tracting paraphrases from a Parallel Corpus. In Pro-
ceedings of ACL/EACL. 
Bolshakov Igor A. and Gelbukh Alexander. 2004. Syn-
onymous Paraphrasing Using WordNet and Internet. 
In Proceedings of NLDB. 
Dagan Ido, Glickman Oren, Gliozzo Alfio, Marmor-
shtein Efrat, Strapparava Carlo. 2006. Direct Word 
Sense Matching for Lexical Substitution. In Proceed-
ings of ACL. 
Edmonds Philip. 1997. Choosing the Word Most Typi-
cal in Context Using a Lexical Co-occurrence Net-
work. In Proceedings of ACL. 
Fellbaum Christiane. 1998. WordNet: An Electronic 
Lexical Database. MIT Press, Cambridge, MA. 
Glickman Oren, Dagan Ido, Keller Mikaela, Bengio 
Samy. 2006. Investigating Lexical Substitution Scor-
ing for Subtitle Generation. In Proceedings of 
CoNLL. 
Kauchak David and Barzilay Regina. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of 
HLT-NAACL. 
Langkilde I. and Knight K. 1998. Generation that Ex-
ploits Corpus-based Statistical Knowledge. In Pro-
ceedings of the COLING-ACL. 
Lin Dekang. 1998. Automatic Retrieval and Clustering 
of Similar Words. In Proceedings of COLING-ACL. 
Smeaton Alan F., Kelledy Fergus, and O?Donell Ruari. 
1994. TREC-4 Experiments at Dublin City Univer-
sity: Thresholding Posting Lists, Query Expansion 
with WordNet and POS Tagging of Spanish. In Pro-
ceedings of TREC-4. 
Wu Hua and Zhou Ming. 2003. Optimizing Synonym 
Extraction Using Monolingual and Bilingual Re-
sources. In Proceedings of IWP. 
Zhao Shiqi, Liu Ting, Yuan Xincheng, Li Sheng, and 
Zhang Yu. 2007. Automatic Acquisition of Context-
Specific Lexical Paraphrases. In Proceedings of 
IJCAI-07. 
176
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 691?701,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Improving Multi-documents Summarization by Sentence Compression
based on Expanded Constituent Parse Trees
Chen Li1, Yang Liu1, Fei Liu2, Lin Zhao3, Fuliang Weng3
1 Computer Science Department, The University of Texas at Dallas
Richardson, TX 75080, USA
2 School of Computer Science, Carnegie Mellon University
Pittsburgh, PA 15213, USA
3 Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{chenli,yangl@hlt.utdallas.edu}
{feiliu@cs.cmu.edu}
{lin.zhao,fuliang.weng@us.bosch.com}
Abstract
In this paper, we focus on the problem
of using sentence compression techniques
to improve multi-document summariza-
tion. We propose an innovative sentence
compression method by considering every
node in the constituent parse tree and de-
ciding its status ? remove or retain. In-
teger liner programming with discrimina-
tive training is used to solve the problem.
Under this model, we incorporate various
constraints to improve the linguistic qual-
ity of the compressed sentences. Then we
utilize a pipeline summarization frame-
work where sentences are first compressed
by our proposed compression model to ob-
tain top-n candidates and then a sentence
selection module is used to generate the
final summary. Compared with state-of-
the-art algorithms, our model has simi-
lar ROUGE-2 scores but better linguistic
quality on TAC data.
1 Introduction
Automatic summarization can be broadly divided
into two categories: extractive and abstractive
summarization. Extractive summarization focuses
on selecting salient sentences from the document
collection and concatenating them to form a sum-
mary; while abstractive summarization is gener-
ally considered more difficult, involving sophisti-
cated techniques for meaning representation, con-
tent planning, surface realization, etc.
There has been a surge of interest in recent years
on generating compressed document summaries as
a viable step towards abstractive summarization.
These compressive summaries often contain more
information than sentence-based extractive sum-
maries since they can remove insignificant sen-
tence constituents and make space for more salient
information that is otherwise dropped due to the
summary length constraint. Two general strate-
gies have been used for compressive summariza-
tion. One is a pipeline approach, where sentence-
based extractive summarization is followed or pro-
ceeded by sentence compression (Lin, 2003; Zajic
et al., 2007; Vanderwende et al., 2007; Wang et al.,
2013). Another line of work uses joint compres-
sion and summarization. Such methods have been
shown to achieve promising performance (Daume?,
2006; Chali and Hasan, 2012; Almeida and Mar-
tins, 2013; Qian and Liu, 2013), but they are typi-
cally computationally expensive.
In this study, we propose an innovative sen-
tence compression model based on expanded con-
stituent parse trees. Our model uses integer lin-
ear programming (ILP) to search the entire space
of compression, and is discriminatively trained.
It is built based on the discriminative sentence
compression model from (McDonald, 2006) and
(Clarke and Lapata, 2008), but our method uses
an expanded constituent parse tree rather than only
the leaf nodes in previous work. Therefore we
can extract rich features for every node in the con-
stituent parser tree. This is an advantage of tree-
based compression technique (Knight and Marcu,
2000; Galley and McKeown, 2007; Wang et al.,
2013). Similar to (Li et al., 2013a), we use a
pipeline summarization framework where multi-
ple compression candidates are generated for each
pre-selected important sentence, and then an ILP-
691
based summarization model is used to select the
final compressed sentences. We evaluate our pro-
posed method on the TAC 2008 and 2011 data
sets using the standard ROUGE metric (Lin, 2004)
and human evaluation of the linguistic quality.
Our results show that using our proposed sentence
compression model in the summarization system
can yield significant performance gain in linguis-
tic quality, without losing much performance on
the ROUGE metric.
2 Related Work
Summarization research has seen great develop-
ment over the last fifty years (Nenkova and McKe-
own, 2011). Compared to the abstractive counter-
part, extractive summarization has received con-
siderable attention due to its clear problem for-
mulation: to extract a set of salient and non-
redundant sentences from the given document
set. Both unsupervised and supervised approaches
have been explored for sentence selection. Su-
pervised approaches include the Bayesian classi-
fier (Kupiec et al., 1995), maximum entropy (Os-
borne, 2002), skip-chain CRF (Galley, 2006), dis-
criminative reranking (Aker et al., 2010), among
others. The extractive summary sentence selec-
tion problem can also be formulated in an opti-
mization framework. Previous methods include
using integer linear programming (ILP) and sub-
modular functions to solve the optimization prob-
lem (Gillick et al., 2009; Li et al., 2013b; Lin and
Bilmes, 2010).
Compressive summarization receives increas-
ing attention in recent years, since it offers a vi-
able step towards abstractive summarization. The
compressed summaries can be generated through a
joint model of the sentence selection and compres-
sion processes, or through a pipeline approach that
integrates a sentence compression model with a
summary sentence pre-selection or post-selection
step.
Many studies have explored the joint sentence
compression and selection setting. Martins and
Smith (2009) jointly performed sentence extrac-
tion and compression by solving an ILP prob-
lem. Berg-Kirkpatrick et al. (2011) proposed an
approach to score the candidate summaries ac-
cording to a combined linear model of extrac-
tive sentence selection and compression. They
trained the model using a margin-based objec-
tive whose loss captures the final summary qual-
ity. Woodsend and Lapata (2012) presented an-
other method where the summary?s informative-
ness, succinctness, and grammaticality are learned
separately from data but optimized jointly using an
ILP setup. Yoshikawa et al. (2012) incorporated
semantic role information in the ILP model.
Our work is closely related with the pipeline
approach, where sentence-based extractive sum-
marization is followed or proceeded by sentence
compression. There have been many studies on
sentence compression, independent of the summa-
rization task. McDonald (2006) firstly introduced
a discriminative sentence compression model to
directly optimize the quality of the compressed
sentences produced. Clarke and Lapata (2008)
improved the above discriminative model by us-
ing ILP in decoding, making it convenient to
add constraints to preserve grammatical structure.
Nomoto (2007) treated the compression task as
a sequence labeling problem and used CRF for
it. Thadani and McKeown (2013) presented an
approach for discriminative sentence compression
that jointly produces sequential and syntactic rep-
resentations for output text. Filippova and Altun
(2013) presented a method to automatically build
a sentence compression corpus with hundreds of
thousands of instances on which deletion-based
compression algorithms can be trained.
In addition to the work on sentence compres-
sion as a stand-alone task, prior studies have also
investigated compression for the summarization
task. Knight and Marcu (2000) utilized the noisy
channel and decision tree method to perform sen-
tence compression in the summarization task. Lin
(2003) showed that pure syntactic-based compres-
sion may not significantly improve the summariza-
tion performance. Zajic et al. (2007) compared
two sentence compression approaches for multi-
document summarization, including a ?parse-and-
trim? and a noisy-channel approach. Galanis and
Androutsopoulos (2010) used the maximum en-
tropy model to generate the candidate compres-
sions by removing branches from the source sen-
tences. Woodsend and Lapata (2010) presented a
joint content selection and compression model for
single-document summarization. They operated
over a phrase-based representation of the source
document which they obtained by merging infor-
mation from PCFG parse trees and dependency
graphs. Liu and Liu (2013) adopted the CRF-
based sentence compression approach for summa-
692
rizing spoken documents. Unlike the word-based
operation, some of these models e.g (Knight and
Marcu, 2000; Siddharthan et al., 2004; Turner
and Charniak, 2005; Galanis and Androutsopou-
los, 2010; Wang et al., 2013), are tree-based ap-
proaches that operate on the parse trees and thus
the compression decision can be made for a con-
stituent, instead of a single word.
3 Sentence Compression Method
Sentence compression is a task of producing a
summary for a single sentence. The compressed
sentence should be shorter, contain important con-
tent from the original sentence, and be grammat-
ical. In some sense, sentence compression can
be described as a ?scaled down version of the
text summarization problem? (Knight and Marcu,
2002). Here similar to much previous work on
sentence compression, we just focus on how to re-
move/select words in the original sentence without
using operation like rewriting sentence.
3.1 Discriminative Compression Model by
ILP
McDonald (2006) presented a discriminative com-
pression model, and Clarke and Lapata (2008) im-
proved it by using ILP for decoding. Since our
proposed method is based upon this model, in
the following we briefly describe it first. Details
can be found in (Clarke and Lapata, 2008). In
this model, the following score function is used
to evaluate each compression candidate:
s(x, y) =
|y|
?
j=2
s(x, L(y
j?1
), L(y
j
)) (1)
where x = x
1
x
2
, ..., x
n
represents an original sen-
tence and y = y
1
y
2
, ..., y
m
denotes a compressed
sentence. Because the sentence compression prob-
lem is defined as a word deletion task, y
j
must oc-
cur in x. Function L(y
i
) ? [1...n] maps word y
i
in
the compression to the word index in the original
sentence x. Note that L(y
i
) < L(y
i+1
) is required,
that is, each word in x can only occur at most
once in compression y. In this model, a first or-
der Markov assumption is used for the score func-
tion. Decoding this model is to find the combina-
tion of bigrams that maximizes the score function
in Eq (1). Clarke and Lapata (2008) introduced the
following variables and used ILP to solve it:
?
i
=
{
1 if x
i
is in the compression
0 otherwise
?i ? [1..n]
?
i
=
{
1 if x
i
starts the compression
0 otherwise
?i ? [1..n]
?
i
=
{
1 if x
i
ends the compression
0 otherwise
?i ? [1..n]
?
ij
=
{
1 if x
i
, x
j
are in the compression
0 otherwise
?i ? [1..n ? 1]?j ? [i + 1..n]
Using these variables, the objective function can
be defined as:
max z =
n
?
i=1
?
i
? s(x, 0, i)
+
n?1
?
i=1
n
?
j=i+1
?
ij
? s(x, i, j)
+
n
?
i=1
?
i
? s(x, i, n + 1) (2)
The following four basic constraints are used to
make the compressed result reasonable:
n
?
i=1
?
i
= 1 (3)
?
j
? ?
j
?
j
?
i=1
?
ij
= 0 ?j ? [1..n] (4)
?
i
?
n
?
j=i+1
?
ij
? ?
i
= 0 ?i ? [1..n] (5)
n
?
i=1
?
i
= 1 (6)
Formula (3) and (6) denote that exactly one
word can begin or end a sentence. Formula (4)
means if a word is in the compressed sentence, it
must either start the compression or follow another
word; formula (5) represents if a word is in the
693
compressed sentence, it must either end the sen-
tence or be followed by another word.
Furthermore, discriminative models are used for
the score function:
s(x, y) =
|y|
?
j=2
w ? f(x, L(y
j?1
), L(y
j
)) (7)
High dimensional features are used and their cor-
responding weights are trained discriminatively.
Above is the basic supervised ILP formula-
tion for sentence compression. Linguistically and
semantically motivated constraints can be added
in the ILP model to ensure the correct grammar
structure in the compressed sentence. For exam-
ple, Clarke and Lapata (2008) forced the introduc-
ing term of prepositional phrases and subordinate
clauses to be included in the compression if any
word from within that syntactic constituent is also
included, and vice versa.
3.2 Compression Model based on Expanded
Constituent Parse Tree
In the above ILP model, variables are defined for
each word in the sentence, and the task is to pre-
dict each word?s status. In this paper, we propose
to adopt the above ILP framework, but operate di-
rectly on the nodes in the constituent parse tree,
rather than just the words (leaf nodes in the tree).
This way we can remove or retain a chunk of the
sentence rather than isolated words, which we ex-
pect can improve the readability and grammar cor-
rectness of the compressed sentences.
The top part of Fig1 is a standard constituent
parse tree. For some levels of the tree, the nodes
at that same level can not represent a sentence. We
extend the parse tree by duplicating non-POS con-
stituents so that leaf nodes (words and their corre-
sponding POS tags) are aligned at the bottom level
as shown in bottom of as Fig1. In the example tree,
the solid lines represent relationship of nodes from
the original parse tree, the long dot lines denote the
extension of the duplication nodes from the up-
per level to the lower level, and the nodes at the
same level are connected (arrowed lines) to repre-
sent that is a sequence. Based on this expanded
constituent parse tree, we can consider every level
as a ?sentence? and the tokens are POS tags and
parse tree labels. We apply the above compression
model in Section 3.1 on every level to decide every
node?s status in the final compressed sentence. In
order to make the compressed parsed tree reason-
able, we model the relationship of nodes between
PRP/
I 
VBP/ 
am 
DT/  
a 
NN/ 
worker 
IN/ 
from 
NNP/
USA
NP IN 
PRP 
PRP 
PRP 
DT/ 
the
 
NNP/
USA
 
IN/ 
from 
NN/ 
worker 
PP 
NP 
PRP/ 
I 
DT/ 
the
S 
VP NP 
VBP NP 
NP PP 
DT NN 
VBP 
VBP 
S 
VP NP 
VBP/ 
am 
NP 
NP 
DT/ 
a 
Figure 1: A regular constituent parse tree and its
Expanded constituent tree.
adjacent levels as following: if the parent node is
labeled as removed, all of its children will be re-
moved; one node will retain if at least one of its
children is kept.
Therefore, the objective function in the new ILP
formulation is:
max z =
height
?
l=1
(
n
l
?
i=1
?
l
i
? s(x, 0, l
i
)
+
n
l
?1
?
i=1
n
l
?
j=i+1
?
l
ij
? s(x, l
i
, l
j
)
+
n
l
?
i=1
?
l
i
? s(x, l
i
, n
l
+ 1) ) (8)
where height is the depth for a parse tree (starting
from level 1 for the tree), and n
l
means the length
of level l (for example, n
5
= 6 in the example
in Fig1). Then every level will have a set of pa-
rameters ?l
i
, ?
l
i
, ?
l
i
, and ?l
ij
, and the corresponding
constraints as shown in Formula (3) to (6). The re-
lationship between nodes from adjacent levels can
be expressed as:
?
l
i
? ?
(l+1)
j
(9)
?
l
i
?
?
?
(l+1)
j
(10)
in which node j at level (l+1) is the child of node
694
i at level l. In addition, 1 ? l ? height ? 1,
1 ? i ? n
l
and 1 ? j ? n
l+1
.
3.3 Linguistically Motivated Constraints
In our proposed model, we can jointly decide the
status of every node in the constituent parse tree
at the same time. One advantage is that we can
add constraints based on internal nodes or rela-
tionship in the parse tree, rather than only using
the relationship based on words. In addition to
the constraints proposed in (Clarke and Lapata,
2008), we introduce more linguistically motivated
constraints to keep the compressed sentence more
grammatically correct. The following describes
the constraints we used based on the constituent
parse tree.
? If a node?s label is ?SBAR?, its parent?s label
is ?NP? and its first child?s label is ?WHNP? or
?WHPP? or ?IN?, then if we can find a noun
in the left siblings of ?SBAR?, this subordi-
nate clause could be an attributive clause or
appositive clause. Therefore the found noun
node should be included in the compression
if the ?SBAR? is also included, because the
node ?SBAR? decorates the noun. For exam-
ple, the top part of Fig 2 is part of expanded
constituent parse tree of sentence ?Those who
knew David were all dead.? The nodes in el-
lipse should share the same status.
? If a node?s label is ?SBAR?, its parent?s label
is ?VP? and its first child?s label is ?WHNP?,
then if we can find a verb in the left siblings
of ?SBAR?, this subordinate clause could be
an objective clause. Therefore, the found
verb node should be included in the compres-
sion if the ?SBAR? node is also included, be-
cause the node ?SBAR? is the object of that
verb. An example is shown in the bottom part
of Fig 2. The nodes in ellipse should share the
same status.
? If a node?s label is ?SBAR?, its parent?s
label is ?VP? and its first child?s label is
?WHADVP?, then if the first leaf for this node
is a wh-word (e.g., ?where, when, why?) or
?how?, this clause may be an objective clause
(when the word is ?why, how, where?) or at-
tributive clause (when the word is ?where?) or
adverbial clause (when the word is ?when?).
Therefore, similar to above, if a verb or noun
is found in the left siblings of ?SBAR?, the
VBD/ 
knew 
NNP/ 
David 
NP 
DT 
DT 
DT 
VP 
WP 
WP/ 
who 
DT/ 
Those 
VBD 
S 
NP 
PRP/ 
he 
PRP/ 
    I 
VBP/ 
believe  
PRP VBP 
 WP/ 
what 
WHNP S PRP 
 
VBP 
VBD/ 
said 
SBAR 
VP NP 
NP VP WP PRP 
 
VBP 





















NP 
SBAR 
WHNP 
WHNP 
 
S 
Figure 2: Expanded constituent parse tree for ex-
amples.
found verb or noun node should be included
in the compression if the ?SBAR? node is also
included.
? If a node?s label is ?SBAR? and its parent?s la-
bel is ?ADJP?, then if we can find a ?JJ?, ?JJR?,
or ?JJS? in the left siblings of ?SBAR?, the
?SBAR? node should be included in the com-
pression if the found ?JJ?, ?JJR? or ?JJS? node
is also included because the node ?SBAR? is
decorated by the adjective.
? The node with a label of ?PRN? can be re-
moved without other constraints.
We also include some other constraints based on
the Stanford dependency parse tree. Table 1 lists
the dependency relations we considered.
? For type I relations, the parent and child node
with those relationships should have the same
value in the compressed result (both are kept
or removed).
? For type II relations, if the child node in
those relations is retained in the compressed
sentence, the parent node should be also re-
tained.
695
Dependency Relation Example
prt: phrase verb particle They shut down the station. prt(shut,down)
prep: prepositional modifier He lives in a small village. prep(lives,in)
I pobj: object of a preposition I sat on the chair. pobj(on,chair)
nsubj: nominal subject The boy is cute. nsubj(cute,boy)
cop: copula Bill is big. cop(big,is)
partmod: participial modifier Truffles picked during the spring are tasty. partmod(truffles,picked)
II nn: noun compound modifier Oil price futures. nn(futures,oil)
acomp: adjectival complement She looks very beautiful. acomp(looks,beautiful)
pcomp: prepositional complement He felt sad after learning that tragedy. pcomp(after,learning)
III ccomp: clausal complement I am certain that he did it. ccomp(certain,did)
tmod: temporal modifier Last night I swam in the pool. tmod(swam,night)
Table 1: Some dependency relations used for extra constraints. All the examples are from (Marneffe and
Manning, 2002)
? For type III relations, if the parent node in
these relations is retained, the child node
should be kept as well.
3.4 Features
So far we have defined the decoding process
and related constraints used in decoding. These
all rely on the score function s(x, y) = w ?
f(x, L(y
j?1
), L(y
j
)) for every level in the con-
stituent parse tree. We included all the features in-
troduced in (Clarke and Lapata, 2008) (those fea-
tures are designed for leaves). Table 2 lists the
additional features we used in our system.
General Features for Every Node
1. individual node label and concatenation of a pair of
nodes
2. distance of two nodes at the same level
3. is the node at beginning or end at that level?
4. do the two nodes have the same parent?
5. if two nodes do not have the same parent, then is the left
node the rightmost child of its parent? is the right node the
leftmost child of its parent?
6. combination of parent label if the node pair are not
under the same parent
7. number of node?s children: 1/0/>1
8. depth of nodes in the parse tree
Extra Features for Leaf nodes
1. word itself and concatenation of two words
2. POS and concatenation of two words? POS
3. whether the word is a stopword
4. node?s named entity tag
5. dependency relationship between two leaves
Table 2: Features used in our system besides those
used in (Clarke and Lapata, 2008).
3.5 Learning
To learn the feature weights during training, we
perform ILP decoding on every sentence in the
training set, to find the best hypothesis for each
node in the expanded constituent parse tree. If
the hypothesis is incorrect, we update the feature
weights using the structured perceptron learning
strategy (Collins, 2002). The reference label for
every node in the expanded constituent parse tree
is obtained automatically from the bottom to the
top of the tree. Since every leaf node (word) is
human annotated (removed or retain), we annotate
the internal nodes as removed if all of its children
are removed. Otherwise, the node is annotated as
retained.
During perceptron training, a fixed learning rate
is used and parameters are averaged to prevent
overfitting. In our experiment, we observe sta-
ble convergence using the held-out development
corpus, with best performance usually obtained
around 10-20 epochs.
4 Summarization System
Similar to (Li et al., 2013a), our summarization
system is , which consists of three key compo-
nents: an initial sentence pre-selection module
to select some important sentence candidates; the
above compression model to generate n-best com-
pressions for each sentence; and then an ILP sum-
marization method to select the best summary sen-
tences from the multiple compressed sentences.
The sentence pre-selection model is a simple su-
pervised support vector regression (SVR) model
that predicts a salience score for each sentence and
selects the top ranked sentences for further pro-
cessing (compression and summarization). The
target value for each sentence during training is
the ROUGE-2 score between the sentence and the
human written abstracts. We use three common
features: (1) sentence position in the document;
(2) sentence length; and (3) interpolated n-gram
document frequency as introduced in (Ng et al.,
2012).
The final sentence selection process follows the
696
ILP method introduced in (Gillick et al., 2009).
Word bi-grams are used as concepts, and their doc-
ument frequency is used as weights. Since we use
multiple compressions for one sentence, an addi-
tional constraint is used: for each sentence, only
one of its n-best compressions may be included in
the summary.
For the compression module, using the ILP
method described above only finds the best com-
pression result for a given sentence. To generate
n-best compression candidates, we use an iterative
approach ? we add one more constraints to prevent
it from generating the same answer every time af-
ter getting one solution.
5 Experimental Results
5.1 Experimental Setup
Summarization Data For summarization experi-
ments, we use the standard TAC data sets1, which
have been used in the NIST competitions. In par-
ticular, we used the TAC 2010 data set as train-
ing data for the SVR sentence pre-selection model,
TAC 2009 data set as development set for parame-
ter tuning, and the TAC 2008 and 2011 data as the
test set for reporting the final summarization re-
sults. The training data for the sentence compres-
sion module in the summarization system is sum-
mary guided compression corpus annotated by (Li
et al., 2013a) using TAC2010 data. In the com-
pression module, for each word we also used its
document level feature.2
Compression Data We also evaluate our com-
pression model using the data set from (Clarke
and Lapata, 2008). It includes 82 newswire arti-
cles with manually produced compression for each
sentence. We use the same partitions as (Martins
and Smith, 2009), i.e., 1,188 sentences for training
and 441 for testing.
Data Processing We use Stanford CoreNLP
toolkit3 to tokenize the sentences, extract name en-
tity tags, and generate the dependency parse tree.
Berkeley Parser (Petrov et al., 2006) is adopted
to obtain the constituent parse tree for every sen-
tence and POS tag for every token. We use Pocket
1http://www.nist.gov/tac/data/index.html
2Document level features for a word include information
such as the word?s document frequency in a topic. These
features cannot be extracted from a single sentence, as in the
standard sentence compression task, and are related to the
document summarization task.
3http://nlp.stanford.edu/software/corenlp.shtml
CRF4 to implement the CRF sentence compres-
sion model. SVMlight5 is used for the summary
sentence pre-selection model. Gurobi ILP solver6
does all ILP decoding.
5.2 Summarization Results
We compare our summarization system against
four recent studies, which have reported some of
the highest published results on this task. Berg-
Kirkpatrick et al. (2011) introduced a joint model
for sentence extraction and compression. Wood-
send and Lapata (2012) learned individual sum-
mary aspects from data, e.g., informativeness, suc-
cinctness, grammaticalness, stylistic writing con-
ventions, and jointly optimized the outcome in
an ILP framework. Ng et al. (2012) exploited
category-specific information for multi-document
summarization. Almeida and Martins (2013) pro-
posed compressive summarization method by dual
decomposition and multi-task learning. Our sum-
marization framework is the same as (Li et al.,
2013a), except they used a CRF-based compres-
sion model. In addition to the four previous stud-
ies, we also report the best achieved results in the
TAC competitions.
Table 3 shows the summarization results of our
method and others. The top part contains the re-
sults for TAC 2008 data and bottom part is for
TAC 2011 data. We use the ROUGE evaluation
metrics (Lin, 2004), with R-2 measuring the bi-
gram overlap between the system and reference
summaries and R-SU4 measuring the skip-bigram
with the maximum gap length of 4. In addition,
we evaluate the linguistic quality (LQ) of the sum-
maries for our system and (Li et al., 2013a).7 The
linguistic quality consists of two parts. One eval-
uates the grammar quality within a sentence. For
this, annotators marked if a compressed sentence
is grammatically correct. Typical grammar errors
include lack of verb or subordinate clause. The
other evaluates the coherence between sentences,
including the order of sentences and irrelevant sen-
tences. We invited 3 English native speakers to do
this evaluation. They gave every compressed sen-
tence a grammar score and a coherence score for
4http://sourceforge.net/projects/pocket-crf-1/
5http://svmlight.joachims.org/
6http://www.gurobi.com
7We chose to evaluate the linguistic quality for this system
because of two reasons: one is that we have an implementa-
tion of that method; the other more important one is that it
has the highest reported ROUGE results among the compared
methods.
697
System R-2 R-SU4 Gram Cohere
TAC?08 Best System 11.03 13.96 n/a n/a
(Berg-Kirk et al., 2011) 11.70 14.38 n/a n/a
(Woodsend et al., 2012) 11.37 14.47 n/a n/a
(Almeida et al.,2013) 12.30 15.18 n/a n/a
(Li et al., 2013a) 12.35 15.27 3.81 3.41
Our System 12.23 15.47 4.29 4.11
TAC?11 Best System 13.44 16.51 n/a n/a
(Ng et al., 2012) 13.93 16.83 n/a n/a
(Li et al., 2013a) 14.40 16.89 3.67 3.32
Our System 14.04 16.67 4.18 4.07
Table 3: Summarization results on the TAC 2008
and 2011 data sets.
each topic. The score is scaled and ranges from 1
(bad) to 5 (good). Therefore, in table 3, the gram-
mar score is the average score for each sentence
and coherence score is the average for each topic.
We measure annotators? agreement in the follow-
ing way: we consider the scores from each anno-
tator as a distribution and we find that these three
distributions are not statistically significantly dif-
ferent each other (p > 0.05 based on paired t-test).
We can see from the table that in general, our
system achieves better ROUGE results than most
previous work except (Li et al., 2013a) on both
TAC 2008 and TAC 2011 data. However, our
system?s linguistic quality is better than (Li et
al., 2013a). The CRF-based compression model
used in (Li et al., 2013a) can not well model the
grammar. Particularly, our results (ROUGE-2) are
statistically significantly (p < 0.05) higher than
TAC08 Best system, but are not statistically signif-
icant compared with (Li et al., 2013a) (p > 0.05).
The pattern is similar in TAC 2011 data. Our result
(R-2) is statistically significantly (p < 0.05) better
than TAC11 Best system, but not statistically (p >
0.05) significantly different from (Li et al., 2013a).
However, for the grammar and coherence score,
our results are statistically significantly (p < 0.05)
than (Li et al., 2013a). All the above statistics are
based on paired t-test.
5.3 Compression Results
The results above show that our summarization
system is competitive. In this section we focus
on the evaluation of our proposed compression
method. We compare our compression system
against four other models. HedgeTrimmer in Dorr
et al. (2003) applied a variety of linguistically-
motivated heuristics to guide the sentences com-
System C Rate (%) Uni-F1 Rel-F1
HedgeTrimmer 57.64 0.64 0.50
McDonald (2006) 70.95 0.77 0.55
Martins (2009) 71.35 0.77 0.56
Wang (2013) 68.06 0.79 0.59
Our System 71.19 0.77 0.58
Table 4: Sentence compression results. The hu-
man compression rate of the test set is 69%.
pression; McDonald (2006) used the output of two
parsers as features in a discriminative model that
decomposes over pairs of consecutive words; Mar-
tins and Smith (2009) built the compression model
in the dependency parse and utilized the relation-
ship between the head and modifier to preserve the
grammar relationship; Wang et al. (2013) devel-
oped a novel beam search decoder using the tree-
based compression model on the constituent parse
tree, which could find the most probable compres-
sion efficiently.
Table 4 shows the compression results of vari-
ous systems, along with the compression ratio (C
Rate) of the system output. We adopt the com-
pression metrics as used in (Martins and Smith,
2009) that measures the macro F-measure for the
retained unigrams (Uni-F1), and the one used
in (Clarke and Lapata, 2008) that calculates the
F1 score of the grammatical relations labeled by
(Briscoe and Carroll, 2002) (Rel-F1). We can see
that our proposed compression method performs
well, similar to the state-of-the-art systems.
To evaluate the power of using the expanded
parse tree in our model, we conducted another ex-
periment where we only consider the bottom level
of the constituent parse tree. In some sense, this
could be considered as the system in (Clarke and
Lapata, 2008). Furthermore, we use two differ-
ent setups: one uses the lexical features (about the
words) and the other does not. Table 5 shows the
results using the data in (Clarke and Lapata, 2008).
For a comparison, we also include the results us-
ing the CRF-based compression model (the one
used in (Nomoto, 2007; Li et al., 2013a)). We
report results using both the automatically calcu-
lated compression metrics and the linguistic qual-
ity score. Three English native speaker annotators
were asked to judge two aspects of the compressed
sentence compared with the gold result: one is the
content that looks at whether the important words
are kept and the other is the grammar score which
evaluates the sentence?s readability. Each of these
698
two scores ranges from 1(bad) to 5(good).
Table 5 shows that when using lexical features,
our system has statistically significantly (p < 0.05)
higher Grammar value and content importance
value than the CRF and the leaves only system.
When no lexical features are used, default system
can achieve statistically significantly (p < 0.01)
higher results than the CRF and the leaves only
system.
We can see that using the expanded parse tree
performs better than using the leaves only, espe-
cially when lexical features are not used. In ad-
dition, we observe that our proposed compression
method is more generalizable than the CRF-based
model. When our system does not use lexical
features in the leaves, it achieves better perfor-
mance than the CRF-based model. This is impor-
tant since such a model is more robust and may be
used in multiple domains, whereas a model rely-
ing on lexical information may suffer more from
domain mismatch. From the table we can see our
proposed tree based compression method consis-
tently has better linguistic quality. On the other
hand, the CRF compression model is the most
computationally efficient one among these three
compression methods. It is about 200 times faster
than our model using the expanded parse tree. Ta-
ble 6 shows some examples using different meth-
ods.
System C Rate(%) Uni-F1 Rel-F1 Gram Imp
Using lexical features
CRF 79.98 0.80 0.51 3.9 4.0
ILP(I) 80.54 0.79 0.57 4.0 4.2
ILP(II) 79.90 0.80 0.57 4.2 4.4
No lexical features
CRF 77.75 0.78 0.51 3.35 3.5
ILP(I) 77.77 0.78 0.56 3.7 3.9
ILP(II) 77.78 0.80 0.58 4.1 4.2
Table 5: Sentence compression results: effect of
lexical features and expanded parse tree. ILP(I)
represents the system using only bottom nodes in
constituent parse tree. ILP(II) is our system. Imp
means the content importance value.
6 Conclusion
In this paper, we propose a discriminative ILP sen-
tence compression model based on the expanded
constituent parse tree, which aims to improve the
linguistic quality of the compressed sentences in
the summarization task. Linguistically motivated
constraints are incorporated to improve the sen-
tence quality. We conduct experiments on the TAC
Using lexical features
Source:
Apart from drugs, detectives believe money is laun-
dered from a variety of black market deals involving
arms and high technology.
Human compress:
detectives believe money is laundered from a variety of
black market deals.
CRF result :
Apart from drugs detectives believe money is laundered
from a black market deals involving arms and technol-
ogy.
ILP(I) Result:
detectives believe money is laundered from a variety of
black deals involving arms.
ILP(II) Result:
detectives believe money is laundered from black mar-
ket deals.
No lexical features
Source:
Mrs Allan?s son disappeared in May 1989, after a party
during his back packing trip across North America.
Human compress:
Mrs Allan?s son disappeared in 1989, after a party dur-
ing his trip across North America.
CRF result :
Mrs Allan?s son disappeared May 1989, after during his
packing trip across North America.
ILP(I) Result:
Mrs Allan?s son disappeared in May, 1989, after a party
during his packing trip across North America .
ILP(II) Result:
Mrs Allan?s son disappeared in May 1989, after a party
during his trip.
Table 6: Examples of original sentences and their
compressed sentences from different systems.
2008 and 2011 summarization data sets and show
that by incorporating this sentence compression
model, our summarization system can yield signif-
icant performance gain in linguistic quality with-
out losing much ROUGE results. The analysis
of the compression module also demonstrates its
competitiveness, in particular the better linguistic
quality and less reliance on lexical cues.
Acknowledgments
We thank the anonymous reviewers for their de-
tailed and insightful comments on earlier drafts
of this paper. The work is also partially sup-
ported by NSF award IIS-0845484 and DARPA
Contract No. FA8750-13-2-0041. Any opinions,
findings, and conclusions or recommendations ex-
pressed are those of the authors and do not neces-
sarily reflect the views of the funding agencies.
699
References
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas.
2010. Multi-document summarization using a*
search and discriminative training. In Proceedings
of EMNLP.
Miguel B. Almeida and Andre F. T. Martins. 2013.
Fast and robust compressive summarization with
dual decomposition and multi-task learning. In Pro-
ceedings of ACL.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of LREC.
Yllias Chali and Sadid A. Hasan. 2012. On the effec-
tiveness of using sentence compression models for
query-focused multi-document summarization. In
Proceedings of COLING.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression an integer linear
programming approach. Journal of Artificial Intelli-
gence Research.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Hal Daume?. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. the-
sis, University of Southern California.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of NAACL.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of EMNLP.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen McKeown. 2007. Lexi-
calized markov grammars for sentence compression.
In Processings of NAACL.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of EMNLP.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
icsi/utd summarization system at tac 2009. In Pro-
ceedings of TAC.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of SIGIR.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013a.
Document summarization via guided sentence com-
pression. In Proceedings of the EMNLP.
Chen Li, Xian Qian, and Yang Liu. 2013b. Using su-
pervised bigram-based ilp for extractive summariza-
tion. In Proceedings of ACL.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proceedings of NAACL.
Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression - A pilot study.
In Proceeding of the Sixth International Workshop
on Information Retrieval with Asian Language.
Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of ACL.
Fei Liu and Yang Liu. 2013. Towards abstractive
speech summarization: Exploring unsupervised and
supervised approaches for spoken utterance com-
pression. IEEE Transactions on Audio, Speech, and
Language Processing.
Marie-Catherine de Marneffe and Christopher D Man-
ning. 2002. Stanford typed dependencies manual.
Andre F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extrac-
tion and compression. In Proceedings of the ACL
Workshop on Integer Linear Programming for Natu-
ral Language Processing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen
Kan, and Chew-Lim Tan. 2012. Exploiting
category-specific information for multi-document
summarization. In Proceedings of COLING.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Informa-
tion Processing and Management.
Miles Osborne. 2002. Using maximum entropy for
sentence extraction. In Proceedings of the ACL-02
Workshop on Automatic Summarization.
700
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL.
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings
of EMNLP.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document sum-
marization. In Proceedings of Coling.
Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of CoNLL.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplification
and lexical expansion. Information Processing &
Management.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of ACL.
Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic generation of story highlights. In Proceedings
of ACL.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP-CoNLL.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression
with semantic role constraints. In Proceedings of
ACL.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks. In Information Processing and Man-
agement.
701
