Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157?166,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Stacking Dependency Parsers
Andre? F. T. Martins?? Dipanjan Das? Noah A. Smith? Eric P. Xing?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,dipanjan,nasmith,epxing}@cs.cmu.edu
Abstract
We explore a stacked framework for learn-
ing to predict dependency structures for natu-
ral language sentences. A typical approach in
graph-based dependency parsing has been to
assume a factorized model, where local fea-
tures are used but a global function is opti-
mized (McDonald et al, 2005b). Recently
Nivre and McDonald (2008) used the output
of one dependency parser to provide features
for another. We show that this is an example
of stacked learning, in which a second pre-
dictor is trained to improve the performance
of the first. Further, we argue that this tech-
nique is a novel way of approximating rich
non-local features in the second parser, with-
out sacrificing efficient, model-optimal pre-
diction. Experiments on twelve languages
show that stacking transition-based and graph-
based parsers improves performance over ex-
isting state-of-the-art dependency parsers.
1 Introduction
In this paper we address a representation-efficiency
tradeoff in statistical natural language processing
through the use of stacked learning (Wolpert,
1992). This tradeoff is exemplified in dependency
parsing, illustrated in Fig. 1, on which we focus in
this paper:
? Exact algorithms for dependency parsing (Eis-
ner and Satta, 1999; McDonald et al, 2005b)
are tractable only when the model makes very
strong, linguistically unsupportable independence
assumptions, such as ?arc factorization? for non-
projective dependency parsing (McDonald and
Satta, 2007).
? Feature-rich parsers must resort to search or
greediness, (Ratnaparkhi et al, 1994; Sagae and
Lavie, 2005; Hall et al, 2006), so that parsing
solutions are inexact and learned models may be
subject to certain kinds of bias (Lafferty et al,
2001).
A solution that leverages the complementary
strengths of these two approaches?described in de-
tail by McDonald and Nivre (2007)?was recently
and successfully explored by Nivre and McDonald
(2008). Our contribution begins by reinterpreting
and generalizing their parser combination scheme as
a stacking of parsers.
We give a new theoretical motivation for stacking
parsers, in terms of extending a parsing model?s fea-
ture space. Specifically, we view stacked learning as
a way of approximating non-local features in a lin-
ear model, rather than making empirically dubious
independence (McDonald et al, 2005b) or structural
assumptions (e.g., projectivity, Eisner, 1996), using
search approximations (Sagae and Lavie, 2005; Hall
et al, 2006; McDonald and Pereira, 2006), solving a
(generally NP-hard) integer linear program (Riedel
and Clarke, 2006), or adding latent variables (Titov
and Henderson, 2007). Notably, we introduce the
use of very rich non-local approximate features in
one parser, through the output of another parser.
Related approaches are the belief propagation algo-
rithm of Smith and Eisner (2008), and the ?trading
of structure for features? explored by Liang et al
157
Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and neighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and Pereira, 2006). How-
ever, in the data-driven parsing setting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our current
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known to have exact non-projective
implementations.
We then switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related Work
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorithms (Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). In the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, as
is the case for edge-factored models (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$ Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and eighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and P reira, 2006). How-
ever, i the data-dr ven parsing etting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our curr nt
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known t have exact n n-projective
implementations.
We th switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related W rk
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorit ms (Yamad and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). I the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, a
is the case for edge-factored mod ls (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$
Figure 1: A projective dependency parse (top), and a non-
projective dependency parse (bottom) for two English
sentences; examples from McDonald and Satta (2007).
(2008).
This paper focuses on dependency parsing, which
has become widely used in relation extraction (Cu-
lotta and Sorensen, 2004), machine translation
(Ding and Palmer, 2005), question answering (Wang
et al, 2007), and many other NLP applications.
We show that stacking methods outperform the ap-
proximate ?second-order? parser of McDonald and
Pereira (2006) on twelve languages and can be used
within that approximatio to chieve even better re-
sults. These results are similar in sp rit t (Nivre and
McDonald, 008), but with the following novel con-
tributio s:
? a stacking interpretation,
? a ric r fe tu e set that includes non-l c l f atures
(shown here to improve erform nc ), and
? a variety of stacking architectures.
Using stacking with rich features, we obtain results
comp titive ith Nivre an McDonald (2008) while
preserving the fast qua ratic parsing ime of arc-
factored spanning tree algorithms.
The paper is organiz d as follows. We discuss re-
lated prior work on dependency parsing and stacking
in ?2. Our model is given in ?3. A novel analysis of
stacking in linear models is given in ?4. Experiments
are presented in ?5.
2 Background and Related Work
We briefly review work on the NLP task of depen-
dency parsing and the machine learning framework
known as stacked learning.
2.1 Dep dency Parsing
Dependency syntax is a lightweight syntactic rep-
resentation that models a sentence as a graph where
the words are vertices and syntactic relationships are
directed edges (arcs) connecting heads to their argu-
ments and modifiers.
Dependency parsing is often viewed computa-
tionally as a structured prediction problem: for each
input sentence x, with n words, exponentially many
candidate depend ncy trees y ? Y(x) are possible in
principle. We den te each tree by its set of vertices
and directed arcs, y = (Vy, Ay). A legal depen-
dency tree has n+ 1 verti es, each corresponding to
one word plus a ?wall? symbol, $, assumed to be the
hidden root of the sentence. In a valid dependency
tree, each vertex except the root has exactly one par-
ent. In the projective case, arcs cannot cross when
depicted on one side of the sentence; in the non-
projective cas , this constraint is not impos d (see
Fig. 1).
2.1.1 Graph-based vs. transition-based models
Most recent work on dependency parsing can be
categorized as graph-based or transition-based. In
graph-based parsing, dependency trees are scored
by factoring the tree into its arcs, and parsing is
perfo med by searching for the highest scoring tree
(Eis er 1996; McD nald et al, 2005b). Transition-
based parsers model the sequ nce of ecisions of
shift-reduce parser, given previous deci ions and
current state, and par ing is perfor ed by greedily
choosing the highest scori g transition out of each
successive parsing state or by searching for the best
sequence of transitions (Ratnaparkhi et al, 1994;
Yamada and Matsumoto, 2003; Nivre et al, 2004;
Sagae and Lavie, 2005; Hall et al, 2006).
Both approaches most commonly use linear mod-
els to assign scores to arcs or decisions, so that a
score is a dot-product of a feature vector f and a
learned weight vector w.
In sum, these two lines of researc use different
approximations to achieve trac ability. Transition-
based approaches solve sequence of local prob-
lems in sequ nce, sacrificing global opti ality guar-
antees and possibly expressive power (Abney et al,
1999). Graph-based methods perform global in-
ference using score factorizations that correspond
to strong independence assumptions (discussed in
158
?2.1.2). Recently, Nivre and McDonald (2008) pro-
posed combining a graph-based and a transition-
based parser and have shown a significant improve-
ment for several languages by letting one of the
parsers ?guide? the other. Our stacked formalism
(to be described in ?3) generalizes this approach.
2.1.2 Arc factorization
In the successful graph-based method of McDon-
ald et al (2005b), an arc factorization independence
assumption is used to ensure tractability. This as-
sumption forbids any feature that depends on two
or more arcs, permitting only ?arc-factored? features
(i.e. features that depend only on a single candidate
arc a ? Ay and on the input sequence x). This in-
duces a decomposition of the feature vector f(x, y)
as:
f(x, y) =
?
a?Ay
fa(x).
Parsing amounts to solving arg maxy?Y(x)
w>f(x, y), where w is a weight vector. With
a projectivity constraint and arc factorization, the
parsing problem can be solved in cubic time by
dynamic programming (Eisner, 1996), and with a
weaker ?tree? constraint (permitting nonprojective
parses) and arc factorization, a quadratic-time
algorithm exists (Chu and Liu, 1965; Edmonds,
1967), as shown by McDonald et al (2005b). In
the projective case, the arc-factored assumption can
be weakened in certain ways while maintaining
polynomial parser runtime (Eisner and Satta, 1999),
but not in the nonprojective case (McDonald and
Satta, 2007), where finding the highest-scoring tree
becomes NP-hard.
McDonald and Pereira (2006) adopted an approx-
imation based on O(n3) projective parsing followed
by rearrangement to permit crossing arcs, achieving
higher performance. In ?3 we adopt a framework
that maintains O(n2) runtime (still exploiting the
Chu-Liu-Edmonds algorithm) while approximating
non arc-factored features.
2.2 Stacked Learning
Stacked generalization was first proposed by
Wolpert (1992) and Breiman (1996) for regression.
The idea is to include two ?levels? of predictors. The
first level, ?level 0,? includes one or more predictors
g1, . . . , gK : Rd ? R; each receives input x ? Rd
and outputs a prediction gk(x). The second level,
?level 1,? consists of a single function h : Rd+K ?
R that takes as input ?x, g1(x), . . . gK(x)? and out-
puts a final prediction y? = h(x, g1(x), . . . gK(x)).
The predictor, then, combines an ensemble (the gk)
with a meta-predictor (h).
Training is done as follows: the training data are
split into L partitions, and L instances of the level
0 predictor are trained in a ?leave-one-out? basis.
Then, an augmented dataset is formed by letting
each instance output predictions for the partition that
was left out. Finally, each level 0 predictor is trained
using the original dataset, and the level 1 predictor
is trained on the augmented dataset, simulating the
test-time setting when h is applied to a new instance
x concatenated with ?gk(x)?k.
This framework has also been applied to classifi-
cation, for example with structured data. Some ap-
plications (including here) use only one classifier at
level 0; recent work includes sequence labeling (Co-
hen and de Carvalho, 2005) and inference in condi-
tional random fields (Kou and Cohen, 2007). Stack-
ing is also intuitively related to transformation-based
learning (Brill, 1993).
3 Stacked Dependency Parsing
We next describe how to use stacked learning for
efficient, rich-featured dependency parsing.
3.1 Architecture
The architecture consists of two levels. At level 0
we include a single dependency parser. At runtime,
this ?level 0 parser? g processes an input sentence x
and outputs the set of predicted edges that make up
its estimation of the dependency tree, y?0 = g(x). At
level 1, we apply a dependency parser?in this work,
always a graph-based dependency parser?that uses
basic factored features plus new ones from the edges
predicted by the level 0 parser. The final parser pre-
dicts parse trees as h(x, g(x)), so that the total run-
time is additive in calculating h(?) and g(?).
The stacking framework is agnostic about the
form of g and h and the methods used to learn them
from data. In this work we use two well-known,
publicly available dependency parsers, MSTParser
(McDonald et al, 2005b),1 which implements ex-
1http://sourceforge.net/projects/mstparser
159
act first-order arc-factored nonprojective parsing
(?2.1.2) and approximate second-order nonprojec-
tive parsing, and MaltParser (Nivre et al, 2006),
which is a state-of-the-art transition-based parser.2
We do not alter the training algorithms used in prior
work for learning these two parsers from data. Us-
ing the existing parsers as starting points, we will
combine them in a variety of ways.
3.2 Training
Regardless of our choices for the specific parsers and
learning algorithms at level 0 and level 1, training is
done as sketched in ?2.2. Let D be a set of training
examples {?xi, yi?}i.
1. Split training data D into L partitions
D1, . . . ,DL.
2. Train L instances of the level 0 parser in the fol-
lowing way: the l-th instance, gl, is trained on
D?l = D \ Dl. Then use gl to output predic-
tions for the (unseen) partition Dl. At the end,
an augmented dataset D? =
?L
l=1 D?
l is built, so
that D? = {?xi, g(xi), yi?}i.
3. Train the level 0 parser g on the original training
data D.
4. Train the level 1 parser h on the augmented train-
ing data D?.
The runtime of this algorithm is O(LT0+T1), where
T0 and T1 are the individual runtimes required for
training level 0 and level 1 alone, respectively.
4 Two Views of Stacked Parsing
We next describe two motivations for stacking
parsers: as a way of augmenting the features of a
graph-based dependency parser or as a way to ap-
proximate higher-order models.
4.1 Adding Input Features
Suppose that the level 1 classifier is an arc-factored
graph-based parser. The feature vectors will take the
form3
f(x, y) = f1(x, y) ^ f2(x, y?0, y)
=
?
a?Ay
f1,a(x) ^ f2,a(x, g(x)),
2http://w3.msi.vxu.se/?jha/maltparser
3We use^ to denote vector concatenation.
where f1(x, y) =
?
a?Ay f1,a(x) are regu-
lar arc-factored features, and f2(x, y?0, y) =?
a?Ay f2,a(x, g(x)) are the stacked features. An
example of a stacked feature is a binary feature
f2,a(x, g(x)) that fires if and only if the arc a was
predicted by g, i.e., if a ? Ag(x); such a feature was
used by Nivre and McDonald (2008).
It is difficult in general to decide whether the in-
clusion of such a feature yields a better parser, since
features strongly correlate with each other. How-
ever, a popular heuristic for feature selection con-
sists of measuring the information gain provided by
each individual feature. In this case, we may obtain
a closed-form expression for the information gain
that f2,a(x, g(x)) provides about the existence or not
of the arc a in the actual dependency tree y. Let A
and A? be binary random variables associated with
the events a ? Ay and a? ? Ag(x), respectively. We
have:
I(A;A?) =
?
a,a??{0,1}
p(a, a?) log2
p(a, a?)
p(a)p(a?)
= H(A?)?
?
a?{0,1}
p(a)H(A?|A = a).
Assuming, for simplicity, that at level 0 the prob-
ability of false positives equals the probability of
false negatives (i.e., Perr , p(a? = 0|a = 1) =
p(a? = 1|a = 0)), and that the probability of
true positives equals the probability of true negatives
(1 ? Perr = p(a? = 0|a = 0) = p(a? = 1|a = 1)),
the expression above reduces to:
I(A;A?) = H(A?) + Perr log2 Perr
+ (1? Perr) log2(1? Perr)
= H(A?)?Herr,
where Herr denotes the entropy of the probability of
error on each arc?s prediction by the level 0 classi-
fier. If Perr ? 0.5 (i.e. if the level 0 classifier is
better than random), then the information gain pro-
vided by this simple stacked feature increases with
(a) the accuracy of the level 0 classifier, and (b) the
entropy H(A?) of the distribution associated with its
arc predictions.
4.2 Approximating Non-factored Features
Another way of interpreting the stacking framework
is as a means to approximate a higher order model,
160
such as one that is not arc-factored, by using stacked
features that make use of the predicted structure
around a candidate arc. Consider a second-order
model where the features decompose by arc and by
arc pair:
f(x, y) =
?
a1?Ay
?
?fa1(x) ^
?
a2?Ay
fa1,a2(x)
?
? .
Exact parsing under such model, with arbitrary
second-order features, is intractable (McDonald and
Satta, 2007). Let us now consider a stacked model
in which the level 0 predictor outputs a parse y?. At
level 1, we use arc-factored features that may be
written as
f?(x, y) =
?
a1?Ay
?
?fa1(x) ^
?
a2?Ay?
fa1,a2(x)
?
? ;
this model differs from the previous one only by re-
placing Ay by Ay? in the index set of the second sum-
mation. Since y? is given, this makes the latter model
arc-factored, and therefore, tractable. We can now
view f?(x, y) as an approximation of f(x, y); indeed,
we can bound the score approximation error,
?s(x, y) =
?
?
?w?>f?(x, y)?w>f(x, y)
?
?
? ,
where w? and w stand respectively for the parameters
learned for the stacked model and those that would
be learned for the (intractable) exact second order
model. We can bound ?s(x, y) by spliting it into
two terms: ?s(x, y) =
?
?
?(w? ?w)>f?(x, y) + w>(f?(x, y)? f(x, y))
?
?
?
?
?
?
?(w? ?w)>f?(x, y)
?
?
?
? ?? ?
,?str(x,y)
+
?
?
?w>(f?(x, y)? f(x, y))
?
?
?
? ?? ?
,?sdec(x,y)
;
where we introduced the terms ?str and ?sdec that
reflect the portion of the score approximation error
that are due to training error (i.e., different parame-
terizations of the exact and approximate models) and
decoding error (same parameterizations, but differ-
ent feature vectors). Using Ho?lder?s inequality, the
former term can be bounded as:
?str(x, y) =
?
?
?(w? ?w)>f?(x, y)
?
?
?
? ?w? ?w?1 ? ?f?(x, y)??
? ?w? ?w?1 ;
where ?.?1 and ?.?? denote the `1-norm and sup-
norm, respectively, and the last inequality holds
when the features are binary (so that ?f?(x, y)?? ?
1). The proper way to bound the term ?w? ?w?1
depends on the training algorithm. As for the de-
coding error term, it can bounded for a given weight
vector w, sentence x, candidate tree y, and level 0
prediction y?. Decomposing the weighted vector as
w = w1 ^ w2, w2 being the sub-vector associ-
ated with the second-order features, we have respec-
tively: ?sdec(x, y) =
?
?
?w>(f?(x, y)? f(x, y))
?
?
?
=
?
?
?
?
?
?
?
a1?Ay
w>2
?
?
?
a2?Ay?
fa1,a2(x)?
?
a2?Ay
fa1,a2(x)
?
?
?
?
?
?
?
?
?
?
a1?Ay
?
a2?Ay??Ay
?
?
?w>2 fa1,a2(x)
?
?
?
?
?
a1?Ay
|Ay??Ay| ? max
a2?Ay??Ay
?
?
?w>2 fa1,a2(x)
?
?
?
=
?
a1?Ay
2L(y, y?) ? max
a2?Ay??Ay
?
?
?w>2 fa1,a2(x)
?
?
? ,
where Ay??Ay , (Ay? ?Ay) ? (Ay ?Ay?) denotes
the symmetric difference of the sets Ay? and Ay,
which has cardinality 2L(y, y?), i.e., twice the Ham-
ming distance between the sequences of heads that
characterize y and the predicted parse y?. Using
Ho?lder?s inequality, we have both
?
?
?w>2 fa1,a2(x)
?
?
? ? ?w2?1 ? ?fa1,a2(x)??
and
?
?
?w>2 fa1,a2(x)
?
?
? ? ?w2?? ? ?fa1,a2(x)?1.
Assuming that all features are binary valued, we
have that ?fa1,a2(x)?? ? 1 and that ?fa1,a2(x)?1 ?
Nf,2, where Nf,2 denotes the maximum number of
active second order features for any possible pair of
arcs (a1, a2). Therefore:
?sdec(x, y) ? 2nL(y, y?) min{?w2?1, Nf,2??w2??},
where n is the sentence length. Although this bound
can be loose, it suggests (intuitively) that the score
approximation degrades as the predicted tree y? gets
farther away from the true tree y (in Hamming dis-
tance). It also degrades with the magnitude of
weights associated with the second-order features,
161
Name Description
PredEdge Indicates whether the candidate edge
was present, and what was its label.
Sibling Lemma, POS, link label, distance and
direction of attachment of the previous
and and next predicted siblings
GrandParents Lemma, POS, link label, distance and
direction of attachment of the grandpar-
ent of the current modifier
PredHead Predicted head of the candidate modifier
(if PredEdge=0)
AllChildren Sequence of POS and link labels of all
the predicted children of the candidate
head
Table 1: Feature sets derived from the level 0 parser.
Subset Description
A PredEdge
B PredEdge+Sibling
C PredEdge+Sibling+GrandParents
D PredEdge+Sibling+GrandParents+PredHead
E PredEdge+Sibling+GrandParents+PredHead+
AllChildren
Table 2: Combinations of features enumerated in Table 1
used for stacking. A is a replication of (Nivre and Mc-
Donald, 2008), except for the modifications described in
footnote 4.
which suggests that a separate regularization of the
first-order and stacked features might be beneficial
in a stacking framework.
As a side note, if we set each component of
the weight vector to one, we obtain a bound
on the `1-norm of the feature vector difference,?
?
?f?(x, y)? f(x, y)
?
?
?
1
? 2nL(y, y?)Nf,2.
5 Experiments
In the following experiments we demonstrate the ef-
fectiveness of stacking parsers. As noted in ?3.1, we
make use of two component parsers, the graph-based
MSTParser and the transition-based MaltParser.
5.1 Implementation and Experimental Details
The publicly available version of MSTParser per-
forms parsing and labeling jointly. We adapted this
system to first perform unlabeled parsing, then la-
bel the arcs using a log-linear classifier with access
to the full unlabeled parse (McDonald et al, 2005a;
McDonald et al, 2005b; McDonald and Pereira,
2006). In stacking experiments, the arc labels from
the level 0 parser are also used as a feature.4
In the following subsections, we refer to our mod-
ification of the MSTParser as MST 1O (the arc-
factored version) and MST 2O (the second-order
arc-pair-factored version). All our experiments use
the non-projective version of this parser. We refer to
the MaltParser as Malt .
We report experiments on twelve languages from
the CoNLL-X shared task (Buchholz and Marsi,
2006).5 All experiments are evaluated using the
labeled attachment score (LAS), using the default
settings.6 Statistical significance is measured us-
ing Dan Bikel?s randomized parsing evaluation com-
parator with 10,000 iterations.7 The additional fea-
tures used in the level 1 parser are enumerated in
Table 1 and their various subsets are depicted in Ta-
ble 2. The PredEdge features are exactly the six fea-
tures used by Nivre and McDonald (2008) in their
MSTMalt parser; therefore, feature set A is a repli-
cation of this parser except for modifications noted
in footnote 4. In all our experiments, the number of
partitions used to create D? is L = 2.
5.2 Experiment: MST 2O + MST 2O
Our first experiment stacks the highly accurate
MST 2O parser with itself. At level 0, the parser
uses only the standard features (?5.1), and at level 1,
these are augmented by various subsets of features
of x along with the output of the level 0 parser, g(x)
(Table 2). The results are shown in Table 3. While
we see improvements over the single-parser baseline
4We made other modifications to MSTParser, implement-
ing many of the successes described by (McDonald et al,
2006). Our version of the code is publicly available at http:
//www.ark.cs.cmu.edu/MSTParserStacked. The
modifications included an approximation to lemmas for datasets
without lemmas (three-character prefixes), and replacing mor-
phology/word and morphology/lemma features with morphol-
ogy/POS features.
5The CoNLL-X shared task actually involves thirteen lan-
guages; our experiments do not include Czech (the largest
dataset), due to time constraints. Therefore, the average results
plotted in the last rows of Tables 3, 4, and 5 are not directly
comparable with previously published averages over thirteen
languages.
6http://nextens.uvt.nl/?conll/software.html
7http://www.cis.upenn.edu/?dbikel/software.
html
162
MST
2O
+MS
T 2O
, A
+MS
T 2O
, B
+MS
T 2O
, C
+MS
T 2O
, D
+MS
T 2O
, E
Arabic 67.88 66.91 67.41 67.68 67.37 68.02
Bulgarian 87.31 87.39 87.03 87.61 87.57 87.55
Chinese 87.57 87.16 87.24 87.48 87.42 87.48
Danish 85.27 85.39 85.61 85.57 85.43 85.57
Dutch 79.99 79.79 79.79 79.83 80.17 80.13
German 87.44 86.92 87.32 87.32 87.26 87.04
Japanese 90.93 91.41 91.21 91.35 91.11 91.19
Portuguese 87.12 87.26 86.88 87.02 87.04 86.98
Slovene 74.02 74.30 74.30 74.00 74.14 73.94
Spanish 82.43 82.17 82.35 82.81 82.53 82.75
Swedish 82.87 82.99 82.95 82.51 83.01 82.69
Turkish 60.11 59.47 59.25 59.47 59.45 59.31
Average 81.08 80.93 80.94 81.05 81.04 81.05
Table 3: Results of stacking MST 2O with itself at both level 0 and level 1. Column 2 enumerates LAS for MST 2O.
Columns 3?6 enumerate results for four different stacked feature subsets. Bold indicates best results for a particular
language.
for nine languages, the improvements are small (less
than 0.5%). One of the biggest concerns about this
model is the fact that it stacks two predictors that
are very similar in nature: both are graph-based and
share the features f1,a(x). It has been pointed out by
Breiman (1996), among others, that the success of
ensemble methods like stacked learning strongly de-
pends on how uncorrelated the individual decisions
made by each predictor are from the others? deci-
sions.8 This experiment provides further evidence
for the claim.
5.3 Experiment: Malt + MST 2O
We next use MaltParser at level 0 and the second-
order arc-pair-factored MST 2O at level 1. This
extends the experiments of Nivre and McDonald
(2008), replicated in our feature subset A.
Table 4 enumerates the results. Note that the
best-performing stacked configuration for each and
every language outperforms MST 2O, corroborat-
ing results reported by Nivre and McDonald (2008).
The best performing stacked configuration outper-
forms Malt as well, except for Japanese and Turk-
ish. Further, our non-arc-factored features largely
outperform subset A, except on Bulgarian, Chinese,
8This claim has a parallel in the cotraining method (Blum
and Mitchell, 1998), whose performance is bounded by the de-
gree of independence between the two feature sets.
and Japanese. On average, the best feature config-
uration is E, which is statistically significant over
Malt and MST 2O with p < 0.0001, and over fea-
ture subset A with p < 0.01.
5.4 Experiment: Malt + MST 1O
Finally, we consider stacking MaltParser with the
first-order, arc-factored MSTParser. We view this
approach as perhaps the most promising, since it is
an exact parsing method with the quadratic runtime
complexity of MST 1O.
Table 5 enumerates the results. For all twelve
languages, some stacked configuration outperforms
MST 1O and also, surprisingly, MST 2O, the sec-
ond order model. This provides empirical evi-
dence that using rich features from MaltParser at
level 0, a stacked level 1 first-order MSTParser can
outperform the second-order MSTParser.9 In only
two cases (Japanese and Turkish), the MaltParser
slightly outperforms the stacked parser.
On average, feature configuration D performs
the best, and is statistically significant over Malt ,
MST 1O, and MST 2O with p < 0.0001, and over
feature subset A with p < 0.05. Encouragingly, this
configuration is barely outperformed by configura-
9Recall that MST 2O uses approximate search, as opposed
to stacking, which uses approximate features.
163
Mal
t
MST
2O
Mal
t +
MST
2O
, A
Mal
t +
MST
2O
, B
Mal
t +
MST
2O
, C
Mal
t +
MST
2O
, D
Mal
t +
MST
2O
, E
Arabic 66.71 67.88 68.56 69.12 68.64 68.34 68.92
Bulgarian 87.41 87.31 88.99 88.89 88.89 88.93 88.91
Chinese 86.92 87.57 88.41 88.31 88.29 88.13 88.41
Danish 84.77 85.27 86.45 86.67 86.79 86.13 86.71
Dutch 78.59 79.99 80.75 81.47 81.47 81.51 81.29
German 85.82 87.44 88.16 88.50 88.56 88.68 88.38
Japanese 91.65 90.93 91.63 91.43 91.59 91.61 91.49
Portuguese 87.60 87.12 88.00 88.24 88.30 88.18 88.22
Slovene 70.30 74.02 76.62 76.00 76.60 76.18 76.72
Spanish 81.29 82.43 83.09 83.73 83.47 83.21 83.43
Swedish 84.58 82.87 84.92 84.60 84.80 85.16 84.88
Turkish 65.68 60.11 64.35 64.51 64.51 65.07 65.21
Average 80.94 81.08 82.52 82.58 82.65 82.59 82.71
Table 4: Results of stacking Malt and MST 2O at level 0 and level 1, respectively. Columns 2?4 enumerate LAS for
Malt , MST 2O and Malt + MST 2O as in Nivre and McDonald (2008). Columns 5?8 enumerate results for four other
stacked feature configurations. Bold indicates best result for a language.
tion A of Malt + MST 2O (see Table 4), the dif-
ference being statistically insignificant (p > 0.05).
This shows that stacking Malt with the exact, arc-
factored MST 1O bridges the difference between the
individual MST 1O and MST 2O models, by approx-
imating higher order features, but maintaining an
O(n2) runtime and finding the model-optimal parse.
5.5 Disagreement as a Confidence Measure
In pipelines or semisupervised settings, it is use-
ful when a parser can provide a confidence measure
alongside its predicted parse tree. Because stacked
predictors use ensembles with observable outputs,
differences among those outputs may be used to es-
timate confidence in the final output. In stacked de-
pendency parsing, this can be done (for example) by
measuring the Hamming distance between the out-
puts of the level 0 and 1 parsers, L(g(x), h(x)). In-
deed, the bound derived in ?4.2 suggests that the
second-order approximation degrades for candidate
parses y that are Hamming-far from g(x); therefore,
if L(g(x), h(x)) is large, the best score s(x, h(x))
may well be ?biased? due to misleading neighbor-
ing information provided by the level 0 parser.
We illustrate this point with an empirical analysis
of the level 0/1 disagreement for the set of exper-
iments described in ?5.3; namely, we compare the
0 2 4 6 8 10
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
L(g(x),h(x))
Sent
ence
 Ave
rage
d Ac
cura
cy
 
 Level 0Level 1Level 0 (Overall)Level 1 (Overall)
Figure 2: Accuracy as a function of token disagreement
between level 0 and level 1. The x-axis is the Hamming
distance L(g(x), h(x)), i.e., the number of tokens where
level 0 and level 1 disagree. The y-axis is the accuracy
averaged over sentences that have the specified Hamming
distance, both for level 0 and level 1.
164
Mal
t
MST
1O
MST
2O
Mal
t +
MST
1O
, A
Mal
t +
MST
1O
, B
Mal
t +
MST
1O
, C
Mal
t +
MST
1O
, D
Mal
t +
MST
1O
, E
Arabic 66.71 66.81 67.88 68.40 68.50 68.20 68.42 68.68
Bulgarian 87.41 86.65 87.31 88.55 88.67 88.75 88.71 88.79
Chinese 86.92 86.60 87.57 87.67 87.73 87.83 87.67 87.61
Danish 84.77 84.87 85.27 86.59 86.27 86.21 86.35 86.15
Dutch 78.59 78.95 79.99 80.53 81.51 80.71 81.61 81.37
German 85.82 86.26 87.44 88.18 88.30 88.20 88.36 88.42
Japanese 91.65 91.01 90.93 91.55 91.53 91.51 91.43 91.57
Portuguese 87.60 86.28 87.12 88.16 88.26 88.46 88.26 88.36
Slovene 70.30 73.96 74.02 75.84 75.64 75.42 75.96 75.64
Spanish 81.29 81.07 82.43 82.61 83.13 83.13 83.09 82.99
Swedish 84.58 81.88 82.87 84.86 84.62 84.64 84.82 84.76
Turkish 65.68 59.63 60.11 64.49 64.97 64.47 64.63 64.61
Average 80.94 80.33 81.08 82.28 82.42 82.29 82.44 82.41
Table 5: Results of stacking Malt and MST 1O at level 0 and level 1, respectively. Columns 2?4 enumerate LAS for
Malt , MST 1O and MST 2O. Columns 5?9 enumerate results for five different stacked feature configurations. Bold
indicates the best result for a language.
level 0 and level 1 predictions under the best overall
configuration (configuration E of Malt+MST2O).
Figure 2 depicts accuracy as a function of level 0-
level 1 disagreement (in number of tokens), aver-
aged over all datasets.
We can see that performance degrades steeply
when the disagreement between levels 0 and 1 in-
creases in the range 0?4, and then behaves more ir-
regularly but keeping the same trend. This suggests
that the Hamming distance L(g(x), h(x)) is infor-
mative about parser performance and may be used
as a confidence measure.
6 Conclusion
In this work, we made use of stacked learning to im-
prove dependency parsing. We considered an archi-
tecture with two layers, where the output of a stan-
dard parser in the first level provides new features
for a parser in the subsequent level. During learning,
the second parser learns to correct mistakes made by
the first one. The novelty of our approach is in the
exploitation of higher-order predicted edges to simu-
late non-local features in the second parser. We pro-
vided a novel interpretation of stacking as feature
approximation, and our experimental results show
rich-featured stacked parsers outperforming state-
of-the-art single-layer and ensemble parsers. No-
tably, using a simple arc-factored parser at level 1,
we obtain an exact O(n2) stacked parser that outper-
forms earlier approximate methods (McDonald and
Pereira, 2006).
Acknowledgments
The authors thank the anonymous reviewers for
helpful comments, Vitor Carvalho, William Cohen,
and David Smith for interesting discussions, and
Ryan McDonald and Joakim Nivre for providing
us their code and preprocessed datasets. A.M. was
supported by a grant from FCT through the CMU-
Portugal Program and the Information and Com-
munications Technologies Institute (ICTI) at CMU.
N.S. was supported by NSF IIS-0713265 and an
IBM faculty award. E.X. was supported by NSF
DBI-0546594, DBI-0640543, and IIS-0713379.
References
S. P. Abney, D. A. McAllester, and F. Pereira. 1999. Re-
lating probabilistic grammars and automata. In Pro-
ceedings of ACL.
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of COLT.
L. Breiman. 1996. Stacked regressions. Machine Learn-
ing, 24:49.
165
E. Brill. 1993. A Corpus-Based Approach to Language
Learning. Ph.D. thesis, University of Pennsylvania.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proceedings
of CoNLL.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
W. W. Cohen and V. Rocha de Carvalho. 2005. Stacked
sequential learning. In Proceedings of IJCAI.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proceedings of ACL.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mar. In Proceedings of ACL.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Eisner and G. Satta. 1999. Efficient parsing for bilex-
ical context-free grammars and head automaton gram-
mars. In Proceedings of ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative
classifiers for deterministic dependency parsing. In
Proceedings of ACL.
Z. Kou and W. W. Cohen. 2007. Stacked graphical mod-
els for efficient inference in Markov random fields. In
Proceedings of SDM.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
P. Liang, H. Daume?, and D. Klein. 2008. Structure com-
pilation: trading structure for features. In Proceedings
of ICML.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL.
R. T. McDonald and F. C. N. Pereira. 2006. Online learn-
ing of approximate dependency parsing algorithms. In
Proceedings of EACL.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proceedings of IWPT.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Ha-
jic. 2005b. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proceedings of HLT-
EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proceedings CoNLL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-HLT.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings of
CoNLL.
A. Ratnaparkhi, S. Roukos, and R. T. Ward. 1994. A
maximum entropy model for parsing. In Proceedings
of ICSLP.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proceedings of EMNLP.
K. Sagae and A. Lavie. 2005. A classifier-based parser
with linear run-time complexity. In Proceedings of
IWPT.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proceedings of EMNLP.
I. Titov and J. Henderson. 2007. A latent variable model
for generative dependency parsing. In Proceedings of
IWPT.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proceedings of EMNLP-CoNLL.
D. Wolpert. 1992. Stacked generalization. Neural Net-
works, 5(2):241?260.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of IWPT.
166
Automatic Extraction of Briefing Templates
Dipanjan Das Mohit Kumar
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
{dipanjan, mohitkum, air}@cs.cmu.edu
Alexander I. Rudnicky
Abstract
An approach to solving the problem of au-
tomatic briefing generation from non-textual
events can be segmenting the task into two
major steps, namely, extraction of briefing
templates and learning aggregators that col-
late information from events and automati-
cally fill up the templates. In this paper, we
describe two novel unsupervised approaches
for extracting briefing templates from hu-
man written reports. Since the problem is
non-standard, we define our own criteria for
evaluating the approaches and demonstrate
that both approaches are effective in extract-
ing domain relevant templates with promis-
ing accuracies.
1 Introduction
Automated briefing generation from non-textual
events is an unsolved problem that currently lacks a
standard approach in the NLP community. Broadly,
it intersects the problem of language generation
from structured data and summarization. The prob-
lem is relevant in several domains where the user
has to repeatedly write reports based on events in
the domain, for example, weather reports (Reiter
et al, 2005), medical reports (Elhadad et al, 2005),
weekly class project reports (Kumar et al, 2007) and
so forth. On observing the data from these domains,
we notice a templatized nature of report items. Ex-
amples (1)-(3) demonstrate equivalents in a particu-
lar domain (Reiter et al, 2005).
(1) [A warm front] from [Iceland] to
[northern Scotland] will move [SE]
across [the northern North Sea] [today
and tomorrow]
(2) [A warm front] from [Iceland] to [the
Faeroes] will move [ENE] across [the
Norwegian Sea] [this evening]
(3) [A ridge] from [the British Isles] to
[Iceland] will move [NE] across [the
North Sea] [today]
In each sentence, the phrases in square brackets at
the same relative positions form the slots that take
up different values at different occasions. The cor-
responding template is shown in (4) with slots con-
taining their respective domain entity types. Instan-
tiations of (4) may produce (1)-(3) and similar sen-
tences. This kind of sentence structure motivates an
approach of segmenting the problem of closed do-
main summarization into two major steps of auto-
matic template extraction and learning aggregators,
which are pattern detectors that assimilate informa-
tion from the events, to populate these templates.
(4) [PRESSURE ENTITY] from [LOCATION] to
[LOCATION] will move [DIRECTION] across
[LOCATION] [TIME]
In the current work we address the first problem of
automatically extracting domain templates from hu-
man written reports. We take a two-step approach to
the problem; first, we cluster report sentences based
on similarity and second, we extract template(s) cor-
responding to each cluster by aligning the instances
in the cluster. We experimented with two indepen-
dent, arguably complementary techniques for clus-
tering and aligning ? a predicate argument based ap-
proach that extracts more general templates contain-
ing one predicate and a ROUGE (Lin, 2004) based
265
approach that can extract templates containing mul-
tiple verbs. As we will see below, both approaches
show promise.
2 Related Work
There has been instances of template based sum-
marization in popular Information Extraction (IE)
evaluations like MUC (Marsh & Perzanowski, 1998;
Onyshkevych, 1994) and ACE (ACE, 2007) where
hand engineered slots were to be filled for events in
text; but the focus lay on template filling rather than
their creation. (Riloff, 1996) describes an interesting
work on the generation of extraction patterns from
untagged text, but the analysis is syntactic and the
patterns do not resemble the templates that we aim
to extract. (Yangarber et al, 2000) describe another
system called ExDisco, that extracts event patterns
from un-annotated text starting from seed patterns.
Once again, the text analysis is not deep and the pat-
terns extracted are not sentence surface forms.
(Collier, 1998) proposed automatic domain tem-
plate extraction for IE purposes where MUC type
templates for particular types of events were con-
structed. The method relies on the idea from (Luhn,
1958) where statistically significant words of a cor-
pus were extracted. Based on these words, sen-
tences containing them were chosen and aligned
using subject-object-verb patterns. However, this
method did not look at arbitrary syntactic patterns.
(Filatova et al, 2006) improved the paradigm by
looking at the most frequent verbs occurring in a
corpus and aligning subtrees containing the verb,
by using the syntactic parses as a similarity metric.
However, long distance dependencies of verbs with
constituents were not looked at and deep semantic
analysis was not performed on the sentences to find
out similar verb subcategorization frames. In con-
trast, in our predicate argument based approach we
look into deeper semantic structures, and align sen-
tences not only based on similar syntactic parses,
but also based on the constituents? roles with re-
spect to the main predicate. Also, they relied on
typical Named Entities (NEs) like location, organi-
zation, person etc. and included another entity that
they termed as NUMBER. However, for specific
domains like weather forecasts, medical reports or
student reports, more varied domain entities form
slots in templates, as we observe in our data; hence,
existence of a module handling domain specific en-
tities become essential for such a task. (Surdeanu
et al, 2003) identify arguments for predicates in a
sentence and emphasize how semantic role infor-
mation may assist in IE related tasks, but their pri-
mary focus remained on the extraction of PropBank
(Kingsbury et al, 2002) type semantic roles.
To our knowledge, the ROUGE metric has not
been used for automatic extraction of templates.
3 The Data
3.1 Data Description
Since our focus is on creating summary items from
events or structured data rather than from text, we
used a corpus from the domain of weather fore-
casts (Reiter et al, 2005). This is a freely avail-
able parallel corpus1 consisting of weather data
and human written forecasts describing them. The
dataset showed regularity in sentence structure and
belonged to a closed domain, making the variations
in surface forms more constrained than completely
free text. After sentence segmentation we arrived at
a set of 3262 sentences. From this set, we selected
3000 for template extraction and kept aside 262 sen-
tences for testing.
3.2 Preprocessing
For semantic analysis, we used the ASSERT toolkit
(Pradhan et al, 2004) that produces shallow seman-
tic parses using the PropBank conventions. As a
by product, it also produces syntactic parses of sen-
tences, using the Charniak parser (Charniak, 2001).
For each sentence, we maintained a part-of-speech
tagged (leaves of the parse tree), parsed, baseNP2
tagged and semantic role tagged version. The
baseNPs were retrieved by pruning the parse trees
and not by using a separate NP chunker. The rea-
son for having a baseNP tagged corpus will become
clear as we go into the detail of our template ex-
traction techniques. Figure 1 shows a typical out-
put from the Charniak parser and Figure 2 shows the
same tree with nodes under the baseNPs pruned.
We identified the need to have a domain entity
tagger for matching constituents in the sentences.
1http://www.csd.abdn.ac.uk/research/sumtime/
2A baseNP is a noun-phrase with no internal noun-phrase
266
ADVP
IN
A low theover Norwegian Sea will move North and weaken
DT NN DT JJ NN MD VB RB CC VB
NP
NP
PP
NP
S
VP
VP
VP
VP
Figure 1: Parse tree for a sentence in the data.
ADVP
IN
A low theover Norwegian Sea will move North and weaken
MD VB RB CC VB
NP
NP
PP
NP
S
VP
VP
VP
VP
Figure 2: Pruned parse tree for a sentence in the cor-
pus
Any tagger for named entities was not suitable for
weather forecasts since unique constituent types as-
sumed significance unlike newswire data. Since the
development of such a tagger was beyond the scope
of the present work, we developed a module that
took baseNP tagged sentences as input and produced
tags across words and baseNPs that were domain en-
tities. The development of such a module by hand
was easy because of a limited vocabulary (< 1000
words) of the data and the closed set nature of most
entity types (e.g the direction entity could take up a
finite set of values). From inspection, thirteen dis-
tinct entity types were recognized in the domain.
Figure 3 shows an example output from the entity
recognizer with the sentence from Figure 2 as input.
[ A low ]
DIRECTION and weaken
A low over the Norwegian Sea will move North and weaken 
ENTITY RECOGNIZER
LOCATIONover [ the Norwegian Sea ]PRESSURE ENTITY
will move [ North ]
Figure 3: Example output of the entity recognizer
We now provide a detailed description of our clus-
tering and template extraction algorithms.
4 Approach and Experiments
We adopted two parallel approaches. First, we
investigated a predicate-argument based approach
where we consider the set of all propositions in our
dataset, and cluster them based on their verb sub-
categorization frame. Second, we used ROUGE,
a summarization evaluation metric that is generally
used to compare machine generated and human writ-
ten summaries. We uniquely used this metric for
clustering similar summary items, after abstracting
the surface forms to a representation that facilitates
comparison of a pair of sentences. The following
subsections detail both the techniques.
4.1 A Predicate-Argument Based Approach
Analysis of predicate-argument structures seemed
appropriate for template extraction for a few rea-
sons: Firstly, complicated sentences with multiple
verbs are broken down into propositions by a seman-
tic role labeler. The propositions3 are better gen-
eralizable units than whole sentences across a cor-
pus. Secondly, long distance dependencies of con-
stituents with a particular verb, are captured well by
a semantic role labeler. Finally, if verbs are con-
sidered to be the center of events, then groups of
sentences with the same semantic role sequences
seemed to form clusters conveying similar meaning.
We explain the complete algorithm for template ex-
traction in the following subsections.
(5) [ARG0 A low over the Norwegian Sea]
[AGM-MOD will] [TARGET move ]
[ARGM-DIR North ] and weaken
(6) [ARG0 A high pressure area ] [AGM-MOD
will ] [TARGET move] [ARGM-DIR
southwestwards] and build on Sunday.
4.1.1 Verb based clustering
We performed a verb based clustering as the first
step. Instead of considering a unique set of verbs,
we considered related verbs as a single verb type.
The relatedness of verbs was derived from Word-
net (Fellbaum, 1998), by merging verbs that appear
in the same synset. This kind of clustering is not
3sentence fragments with one verb
267
ideal in a corpus containing a huge variation in event
streams, like newswire. However, the results were
good for the weather domain where the number of
verbs used is limited. The grouping procedure re-
sulted in a set of 82 clusters with 6632 propositions.
4.1.2 Matching Role Sequences
Each verb cluster was considered next. Instead
of finding structural similarities of the propositions
in one go, we first considered the semantic role
sequences for each proposition. We searched for
propositions that had exactly similar role sequences
and grouped them together. To give an exam-
ple, both sentences 5 and 6 have the matching role
sequence ARG0?ARGM-MOD?TARGET?ARGM-
DIR. The intuition behind such clustering is straight-
forward. Propositions with a matching verb type
with the same set of roles arranged in a similar fash-
ion would convey similar meaning. We observed
that this was indeed true for sentences tagged with
correct semantic role labels.
Instead of considering matching role sequences
for a set of propositions, we could as well have
considered matching bag of roles. However, for
the present corpus, we decided to use strict role se-
quence instead because of the sentences? rigid struc-
ture and absence of any passive sentences. This
subclustering step resulted in smaller clusters, and
many of them contained a single proposition. We
threw out these clusters on the assumption that the
human summarizers did not necessarily have a tem-
plate in mind while writing those summary items.
As a result, many verb types were eliminated and
only 33 verb-type clusters containing several sub-
clusters each were produced.
4.1.3 Looking inside Roles
Groups of propositions with the same verb-type
and semantic role sequences were considered in this
step. For each group, we looked at individual se-
mantic roles to find out similarity between them. We
decided at first to look at syntactic parse tree similar-
ities between constituents. However, there is a need
to decide at what level of abstraction should one con-
sider matching the parse trees. After considerable
speculation, we decided on pruning the constituents?
parse trees till the level of baseNPs and then match
the resulting tag sequences.
Scotland
IN
A low theover Sea
NP
NP
PP
NP
NP
NP PP
NP
Norwegian A frontal trough
IN
across
Figure 4: Matching ARG0s for two propositions
LOCATIONIN
A low theover SeaNorwegian A frontal trough
IN
across Scotland
PRESSURE ENTITY
LOCATION
PRESSURE ENTITY
Figure 5: Abstracted tag sequences for two con-
stituents
The parses with pruned trees from the preprocess-
ing steps provide the necessary information for con-
stituent matching. Figure 4 shows matching syntac-
tic trees for two ARG0s from two propositions of a
cluster. It is at this step that we use the domain entity
tags to abstract away the constituents? syntactic tags.
Figure 5 shows the constituents of Figure 4 with the
tree structure reduced to tag sequences and domain
entity types replacing the tags whenever necessary.
This abstraction step produces a number of unique
domain entity augmented tag sequences for a partic-
ular semantic role. As a final step of template gen-
eration, we concatenate these abstracted constituent
types for all the semantic roles in the given group.
To focus on template-like structures we only con-
sider tag sequences that occur twice or more in the
group.
The templates produced at the end of this step are
essentially tag sequences interspersed with domain
entities. In our definition of templates, the slots are
the entity types and the fixed parts are constituted
by word(s) used by the human experts for a partic-
ular tag sequence. Figure 6 shows some example
templates. The upper case words in the figure corre-
spond to the domain entities identified by the entity
tagger and they form the slots in the templates. A
total of 209 templates were produced.
268
PRESSURE_ENTITY to DIRECTION of LOCATION will drift slowly
WAVE will run_0.5/move_0.5 DIRECTION then DIRECTION
Associated PRESSURE_ENTITY will move DIRECTION across LOCATION TIME
PRESSURE_ENTITY expected over LOCATION by_0.5/on_0.5 DAY
Figure 6: Example Templates. Upper case tokens
correspond to slots. For fixed parts, when there is a
choice between words, the probability of the occur-
rence of words in that particular syntactic structure
are tagged alongside.
4.2 A ROUGE Based Approach
ROUGE (Lin, 2004) is the standard automatic eval-
uation metric in the Summarization community. It is
derived from the BLEU (Papineni et al, 2001) score
which is the evaluation metric used in the Machine
Translation community. The underlying idea in the
metric is comparing the candidate and the refer-
ence sentences (or summaries) based on their token
co-occurrence statistics. For example, a unigram
based measure would compare the vocabulary over-
lap between the candidate and reference sentences.
Thus, intuitively, we may use the ROUGE score as
a measure for clustering the sentences. Amongst
the various ROUGE statistics, the most appealing is
Weighted Longest Common Subsequence(WLCS).
WLCS favors contiguous LCS which corresponds
to the intuition of finding the common template.
We experimented with other ROUGE statistics but
we got better and easily interpretable results using
WLCS and so we chose it as the final metric. In
all the approaches the data was first preprocessed
(baseNP and NE tagged) as described in the previ-
ous subsection. In the following subsections, we de-
scribe the various clustering techniques that we tried
using the ROUGE score followed by the alignment
technique.
4.2.1 Clustering
Unsupervised Clustering: As the ROUGE score
defines a distance metric, we can use this score for
doing unsupervised clustering. We tried hierarchical
clustering approaches but did not obtain good clus-
ters, evaluated empirically. In empirical evaluation,
we manually looked at the output clusters and made
a judgement call whether the candidate clusters are
reasonably coherent and potentially correspond to
templates. The reason for the poor performance of
the approach was the classical parameter estimation
problem of determining a priori the number of clus-
ters. We could not find an elegant solution for the
problem without losing the motivation of an auto-
mated approach.
Figure 7: Deterministic clustering based on Graph
connectivity. In the figure the squares with the same
pattern belong to the same cluster.
Non-parametric Unsupervised Clustering:
Since the unsupervised technique did not give
good results, we experimented with a non-
parametric clustering approach, namely, Cross-
Association(Chakrabarti et al, 2004). It is a
non-parametric unsupervised clustering algorithm
for similarity (boolean) matrices. We obtain the
similarity matrix in our domain by thresholding the
ROUGE similarity score matrix. This technique
also did not give us good clusters, evaluated empiri-
cally. The plausible reason for the poor performance
seems to be that the technique is based on MDL
(Minimum Description Length) principle. Since in
our domain we expect a large number of clusters
with small membership along many singletons,
MDL principle is not likely to perform well.
Deterministic Clustering:
As the unsupervised techniques did not perform
well, we tried deterministic clustering based on
graph connectivity. The underlying intuition is that
all the sentences X1...n that are ?similar? to any
other sentence Yi should be in the same cluster even
though Xj and Xk may not be ?similar? to each
other. Thus we find the connected components in the
similarity matrix and label them as individual clus-
ters.4
4This approach is similar to agglomerative single linkage
clustering.
269
We created a similarity matrix by thresholding the
ROUGE score. In the event, the clusters obtained by
this approach were also not good, evaluated empir-
ically. This led us to revisit the similarity function
and tune it. We factored the ROUGE-WLCS score,
which is an F-measure score, into its component Pre-
cision and Recall scores and experimented with var-
ious combinations of using the Precision and Recall
scores. We finally chose a combined Precision and
Recall measure (not f-measure) in which both the
scores were independently thresholded. The moti-
vation for the measure is that in our domain we de-
sire to have high precision matches. Additionally
we need to control the length of the sentences in the
cluster for which we require a Recall threshold. F-
measure (which is the harmonic mean of Precision
and Recall) does not give us the required individual
control. We set up our experiments such that while
comparing two sentences the longer sentence is al-
ways treated as the reference and the shorter one as
the candidate. This helps us in interpreting the Pre-
cision/Recall measures better and thresholding them
accordingly. The approach gave us 149 clusters,
which looked good on empirical evaluation. We can
argue that using this modified similarity function for
previous unsupervised approaches could have given
better results, but we did not reevaluate those ap-
proaches as our aim of getting a reasonable cluster-
ing approach is fulfilled with this simple scheme and
tuning the unsupervised approaches can be interest-
ing future work.
4.3 Alignment
After obtaining the clusters using the Deterministic
approach we needed to find out the template corre-
sponding to each of the cluster. Fairly intuitively we
computed the Longest Common Subsequence(LCS)
between the sentences in each cluster which we then
claim to be the template corresponding to the clus-
ter. This resulted in a set of 149 templates, similar to
the Predicate Argument based approach, as shown
in figure 6.
5 Results
5.1 Evaluation Scheme
Since there is no standard way to evaluate template
extraction for summary creation, we adopted a mix
of subjective and automatic measures for evaluating
the templates extracted. We define precision for this
particular problem as:
precision = number of domain relevant templatestotal number of extracted templates
This is a subjective measure and we undertook a
study involving three subjects who were accustomed
to the language used in the corpus. We asked the
human subjects to mark each template as relevant
or non-relevant to the weather forecast domain. We
also asked them to mark the template as grammatical
or ungrammatical if it is non-relevant.
Our other metric for evaluation is automatic re-
call. It is based on using the ROUGE-WLCS met-
ric to determine a match between the preprocessed
(baseNP and NE tagged) test corpora with the pro-
posed set of correct templates, a set determined
by taking an intersection of only the relevant tem-
plates marked by each judge. For the ROUGE based
method, the test corpus consists of 262 sentences,
while for the predicate-argument based method it
consists of a set of 263 propositions extracted from
the 262 sentences using ASSERT followed by a fil-
tering of invalid propositions (e.g. ones starting
with a verb). Amongst different ROUGE scores
(precision/recall/f-measure), we consider precision
as the criterion for deciding a match and experi-
mented with different thresholding values.
Main Verb Precision Main Verb Precision
deepen 0.67 weaken 0.83
expect 0.76 lie 0.57
drift 0.93 continue 0.97
build 0.95 fill 0.80
cross 0.78 move 0.86
Table 1: Precision for top 10 most frequently occur-
ring verbs
5.2 Results: Predicate-Argument Based
Approach
Table 1 shows the precision values for top 10 most
frequently occurring verbs. (Since a major propor-
tion (> 90%) of the templates are covered by these
verbs, we don?t show all the precision values; it also
helps to contain space.) The overall precision value
achieved was 84.21%, the inter-rater Fleiss? kappa
measure (Fleiss, 1971) between the judges being
270
? = 0.69, demonstrating substantial agreement. The
precision values are encouraging, and in most cases
the reason for low precision is because of erroneous
performance of the semantic role labeler system,
which is corroborated by the percentage (47.47%) of
ungrammatical templates among the irrelevant ones.
Results for the automated recall values are shown
in Figure 8, where precision values are varied to
observe the recall. For 0.9 precision in ROUGE-
WLCS, the recall is 0.3 which shows that there is
a 30% near exact coverage over propositions, while
for 0.6 precision in ROUGE-WLCS, the recall is an
encouraging 81%.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0.4 0.5 0.6 0.7 0.8 0.9
Rec
all
Precision Threshold forMatching Test Sentences
ROUGESRL
Figure 8: Automated Recall based on ROUGE-
WLCS measure comparing the test corpora with
the set of templates extracted by the Predicate-
Argument (SRL) and the ROUGE based method.
5.3 Results: ROUGE based approach
Various precision and recall thresholds for ROUGE
were considered for clustering. We empirically set-
tled on a recall threshold of 0.8 since this produces
the set of clusters with optimum number of sen-
tences. The number of clusters and number of sen-
tences in clusters at this recall values are shown in
Figure 9 for various precision thresholds.
Precision was measured in the same way as the
predicate argument approach and the value obtained
was 76.3%, with Fleiss? kappa measure of ? = 0.79.
The percentage of ungrammatical templates among
the irrelevant ones was 96.7%, strongly indicating
that post processing the templates using a parser can,
in future, give substantial improvement. During er-
ror analysis, we observed simple grammatical er-
rors in templates; first or last word being preposi-
 130
 140
 150
 160
 170
 180
 190
 0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0
 200
 400
 600
 800
 1000
 1200
 1400
No. o
f Clus
ters
No. o
f Se
nten
ces
 in 
Cluste
rs
Precision Threshold
No. of ClustersNo. of Sentences in Clusters
Figure 9: Number of clusters and total number of
sentences in clusters for various Precision Thresh-
olds at Recall Threshold=0.8
tions. So a fairly simple error recovery module that
strips the leading and trailing prepositions was in-
troduced. 20 templates out of the 149 were mod-
ified by the error recovery module and they were
evaluated again by the three judges. The precision
obtained for the modified templates was 35%, with
Fleiss? kappa ? = 1, boosting the overall precision
to 80.98%. The overall high precision is motivat-
ing as this is a fairly general approach that does not
require any NLP resources. Figure 8 shows the auto-
mated recall values for the templates and abstracted
sentences from the held-out dataset. For high preci-
sion points, the recall is low because there is not an
exact match for most cases.
6 Conclusion and Future Work
In this paper, we described two new approaches
for template extraction for briefing generation. For
both approaches, high precision values indicate that
meaningful templates are being extracted. However,
the recall values were moderate and they hint at
possible improvements. An interesting direction of
future research is merging the two approaches and
have one technique benefit from the other. The ap-
proaches seem complementary as the ROUGE based
technique does not use the structure of the sentence
at all whereas the predicate-argument approach is
heavily dependent on it. Moreover, the predicate
argument based approach gives general templates
with one predicate while ROUGE based approach
271
can extract templates containing multiple verbs. It
would also be desirable to establish the generality
of the techniques, by using other domains such as
newswire, medical reports and others.
Acknowledgements We would like to express our
gratitude to William Cohen and Noah Smith for their
valuable suggestions and inputs during the course of
this work. We also thank the three anonymous re-
viewers for helpful suggestions. This work was sup-
ported by DARPA grant NBCHD030010. The con-
tent of the information in this publication does not
necessarily reflect the position or the policy of the
US Government, and no official endorsement should
be inferred.
References
ACE (2007). Automatic content extraction program.
http://www.nist.gov/speech/tests/ace/.
Chakrabarti, D., Papadimitriou, S., Modha, D. S.,
& Faloutsos, C. (2004). Fully automatic cross-
associations. Proceedings of KDD ?04 (pp. 79?
88). New York, NY, USA: ACM Press.
Charniak, E. (2001). Immediate-head parsing for
language models. Proceedings of ACL ?01 (pp.
116?123).
Collier, R. (1998). Automatic template creation
for information extraction. Doctoral dissertation,
University of Sheffield.
Elhadad, N., Kan, M.-Y., Klavans, J. L., & McKe-
own, K. (2005). Customization in a unified frame-
work for summarizing medical literature. Artifi-
cial Intelligence in Medicine, 33, 179?198.
Fellbaum, C. (1998). WordNet ? An Electronic Lex-
ical Database. MIT Press.
Filatova, E., Hatzivassiloglou, V., & McKeown,
K. (2006). Automatic creation of domain tem-
plates. Proceedings of COLING/ACL 2006 (pp.
207?214).
Fleiss, J. (1971). Measuring nominal scale agree-
ment among many raters. Psychological Bulletin
(pp. 378?382).
Kingsbury, P., Palmer, M., & Marcus, M. (2002).
Adding semantic annotation to the penn treebank.
Proceedings of the HLT?02.
Kumar, M., Garera, N., & Rudnicky, A. I. (2007).
Learning from the report-writing behavior of in-
dividuals. IJCAI (pp. 1641?1646).
Lin, C.-Y. (2004). ROUGE: A package for auto-
matic evaluation of summaries. Proceedings of
Workshop on Text Summarization.
Luhn, H. P. (1958). The automatic creation of litera-
ture abstracts. IBM Journal of Research Develop-
ment, 2, 159?165.
Marsh, E., & Perzanowski, D. (1998). MUC-7 Eval-
uation of IE Technology: Overview of Results.
Proceedings of MUC-7. Fairfax, Virginia.
Onyshkevych, B. (1994). Issues and methodology
for template design for information extraction.
Proceedings of HLT ?94 (pp. 171?176). Morris-
town, NJ, USA.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.
(2001). Bleu: a method for automatic evaluation
of machine translation.
Pradhan, S., Ward, W., Hacioglu, K., Martin, J., &
Jurafsky, D. (2004). Shallow semantic parsing
using support vector machines. Proceedings of
HLT/NAACL ?04. Boston, MA.
Reiter, E., Sripada, S., Hunter, J., Yu, J., & Davy,
I. (2005). Choosing words in computer-generated
weather forecasts. Artif. Intell., 167, 137?169.
Riloff, E. (1996). Automatically generating extrac-
tion patterns from untagged text. AAAI/IAAI, Vol.
2 (pp. 1044?1049).
Surdeanu, M., Harabagiu, S., Williams, J., &
Aarseth, P. (2003). Using predicate-argument
structures for information extraction. Proceedings
of ACL 2003.
Yangarber, R., Grishman, R., Tapanainen, P., & Hut-
tunen, S. (2000). Automatic acquisition of domain
knowledge for information extraction. Proceed-
ings of the 18th conference on Computational lin-
guistics (pp. 940?946). Morristown, NJ, USA.
272
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 468?476,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition
Dipanjan Das and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dipanjan,nasmith}@cs.cmu.edu
Abstract
We present a novel approach to decid-
ing whether two sentences hold a para-
phrase relationship. We employ a gen-
erative model that generates a paraphrase
of a given sentence, and we use proba-
bilistic inference to reason about whether
two sentences share the paraphrase rela-
tionship. The model cleanly incorporates
both syntax and lexical semantics using
quasi-synchronous dependency grammars
(Smith and Eisner, 2006). Furthermore,
using a product of experts (Hinton, 2002),
we combine the model with a comple-
mentary logistic regression model based
on state-of-the-art lexical overlap features.
We evaluate our models on the task of
distinguishing true paraphrase pairs from
false ones on a standard corpus, giving
competitive state-of-the-art performance.
1 Introduction
The problem of modeling paraphrase relation-
ships between natural language utterances (McK-
eown, 1979) has recently attracted interest. For
computational linguists, solving this problem may
shed light on how best to model the semantics
of sentences. For natural language engineers, the
problem bears on information management sys-
tems like abstractive summarizers that must mea-
sure semantic overlap between sentences (Barzi-
lay and Lee, 2003), question answering modules
(Marsi and Krahmer, 2005) and machine transla-
tion (Callison-Burch et al, 2006).
The paraphrase identification problem asks
whether two sentences have essentially the same
meaning. Although paraphrase identification is
defined in semantic terms, it is usually solved us-
ing statistical classifiers based on shallow lexical,
n-gram, and syntactic ?overlap? features. Such
overlap features give the best-published classifi-
cation accuracy for the paraphrase identification
task (Zhang and Patrick, 2005; Finch et al, 2005;
Wan et al, 2006; Corley and Mihalcea, 2005, in-
ter alia), but do not explicitly model correspon-
dence structure (or ?alignment?) between the parts
of two sentences. In this paper, we adopt a model
that posits correspondence between the words in
the two sentences, defining it in loose syntactic
terms: if two sentences are paraphrases, we expect
their dependency trees to align closely, though
some divergences are also expected, with some
more likely than others. Following Smith and Eis-
ner (2006), we adopt the view that the syntactic
structure of sentences paraphrasing some sentence
s should be ?inspired? by the structure of s.
Because dependency syntax is still only a crude
approximation to semantic structure, we augment
the model with a lexical semantics component,
based on WordNet (Miller, 1995), that models how
words are probabilistically altered in generating
a paraphrase. This combination of loose syntax
and lexical semantics is similar to the ?Jeopardy?
model of Wang et al (2007).
This syntactic framework represents a major de-
parture from useful and popular surface similarity
features, and the latter are difficult to incorporate
into our probabilistic model. We use a product of
experts (Hinton, 2002) to bring together a logis-
tic regression classifier built from n-gram overlap
features and our syntactic model. This combined
model leverages complementary strengths of the
two approaches, outperforming a strong state-of-
the-art baseline (Wan et al, 2006).
This paper is organized as follows. We intro-
duce our probabilistic model in ?2. The model
makes use of three quasi-synchronous grammar
models (Smith and Eisner, 2006, QG, hereafter) as
components (one modeling paraphrase, one mod-
eling not-paraphrase, and one a base grammar);
these are detailed, along with latent-variable in-
ference and discriminative training algorithms, in
?3. We discuss the Microsoft Research Paraphrase
Corpus, upon which we conduct experiments, in
?4. In ?5, we present experiments on paraphrase
468
identification with our model and make compar-
isons with the existing state-of-the-art. We de-
scribe the product of experts and our lexical over-
lap model, and discuss the results achieved in ?6.
We relate our approach to prior work (?7) and con-
clude (?8).
2 Probabilistic Model
Since our task is a classification problem, we re-
quire our model to provide an estimate of the pos-
terior probability of the relationship (i.e., ?para-
phrase,? denoted p, or ?not paraphrase,? denoted
n), given the pair of sentences.1 Here, pQ denotes
model probabilities, c is a relationship class (p or
n), and s1 and s2 are the two sentences. We choose
the class according to:
c? = argmax
c?{p,n}
pQ(c | s1, s2)
= argmax
c?{p,n}
pQ(c)? pQ(s1, s2 | c) (1)
We define the class-conditional probabilities of
the two sentences using the following generative
story. First, grammar G0 generates a sentence s.
Then a class c is chosen, corresponding to a class-
specific probabilistic quasi-synchronous grammar
Gc. (We will discuss QG in detail in ?3. For the
present, consider it a specially-defined probabilis-
tic model that generates sentences with a specific
property, like ?paraphrases s,? when c = p.) Given
s, Gc generates the other sentence in the pair, s?.
When we observe a pair of sentences s1 and s2
we do not presume to know which came first (i.e.,
which was s and which was s?). Both orderings
are assumed to be equally probable. For class c,
pQ(s1, s2 | c) =
0.5? pQ(s1 | G0)? pQ(s2 | Gc(s1))
+ 0.5? pQ(s2 | G0)? pQ(s1 | Gc(s2))(2)
where c can be p or n; Gp(s) is the QG that gen-
erates paraphrases for sentence s, while Gn(s) is
the QG that generates sentences that are not para-
phrases of sentence s. This latter model may seem
counter-intuitive: since the vast majority of pos-
sible sentences are not paraphrases of s, why is a
special grammar required? Our use of a Gn fol-
lows from the properties of the corpus currently
used for learning, in which the negative examples
1Although we do not explore the idea here, the model
could be adapted for other sentence-pair relationships like en-
tailment or contradiction.
were selected to have high lexical overlap. We re-
turn to this point in ?4.
3 QG for Paraphrase Modeling
Here, we turn to the models Gp and Gn in detail.
3.1 Background
Smith and Eisner (2006) introduced the quasi-
synchronous grammar formalism. Here, we de-
scribe some of its salient aspects. The model
arose out of the empirical observation that trans-
lated sentences have some isomorphic syntactic
structure, but divergences are possible. Therefore,
rather than an isomorphic structure over a pair of
source and target sentences, the syntactic tree over
a target sentence is modeled by a source sentence-
specific grammar ?inspired? by the source sen-
tence?s tree. This is implemented by associating
with each node in the target tree a subset of the
nodes in the source tree. Since it loosely links
the two sentences? syntactic structures, QG is well
suited for problems like word alignment for MT
(Smith and Eisner, 2006) and question answering
(Wang et al, 2007).
Consider a very simple quasi-synchronous
context-free dependency grammar that generates
one dependent per production rule.2 Let s =
?s1, ..., sm? be the source sentence. The grammar
rules will take one of the two forms:
?t, l? ? ?t, l??t?, k? or ?t, l? ? ?t?, k??t, l?
where t and t? range over the vocabulary of the
target language, and l and k ? {0, ...,m} are in-
dices in the source sentence, with 0 denoting null.3
Hard or soft constraints can be applied between l
and k in a rule. These constraints imply permissi-
ble ?configurations.? For example, requiring l 6= 0
and, if k 6= 0 then sk must be a child of sl in the
source tree, we can implement a synchronous de-
pendency grammar similar to (Melamed, 2004).
Smith and Eisner (2006) used a quasi-
synchronous grammar to discover the correspon-
dence between words implied by the correspon-
dence between the trees. We follow Wang et al
(2007) in treating the correspondences as latent
variables, and in using a WordNet-based lexical
semantics model to generate the target words.
2Our actual model is more complicated; see ?3.2.
3A more general QG could allow one-to-many align-
ments, replacing l and k with sets of indices.
469
3.2 Detailed Model
We describe how we model pQ(t | Gp(s)) and
pQ(t | Gn(s)) for source and target sentences s
and t (appearing in Eq. 2 alternately as s1 and s2).
A dependency tree on a sequence w =
?w1, ..., wk? is a mapping of indices of words to
indices of syntactic parents, ?p : {1, ..., k} ?
{0, ..., k}, and a mapping of indices of words to
dependency relation types in L, ?` : {1, ..., k} ?
L. The set of indices children of wi to its left,
{j : ?w(j) = i, j < i}, is denoted ?w(i), and
?w(i) is used for right children. wi has a single
parent, denoted by w?p(i). Cycles are not allowed,
and w0 is taken to be the dummy ?wall? symbol,
$, whose only child is the root word of the sen-
tence (normally the main verb). The label for wi
is denoted by ?`(i). We denote the whole tree of
a sentence w by ?w, the subtree rooted at the ith
word by ?w,i.
Consider two sentences: let the source sen-
tence s contain m words and the target sentence
t contain n words. Let the correspondence x :
{1, ..., n} ? {0, ...,m} be a mapping from in-
dices of words in t to indices of words in s. (We
require each target word to map to at most one
source word, though multiple target words can
map to the same source word, i.e., x(i) = x(j)
while i 6= j.) When x(i) = 0, the ith target word
maps to the wall symbol, equivalently a ?null?
word. Each of our QGs Gp and Gn generates the
alignments x, the target tree ? t, and the sentence
t. Both Gp and Gn are structured in the same way,
differing only in their parameters; henceforth we
discuss Gp; Gn is similar.
We assume that the parse trees of s and t are
known.4 Therefore our model defines:
pQ(t | Gp(s)) = p(?
t | Gp(?
s))
=
?
x p(?
t, x | Gp(? s)) (3)
Because the QG is essentially a context-free de-
pendency grammar, we can factor it into recur-
sive steps as follows (let i be an arbitrary index
in {1, ..., n}):
P (? t,i | ti, x(i), ?
s) = pval (|?
t(i)|, |?t(i)| | ti)
4In our experiments, we use the parser described by Mc-
Donald et al (2005), trained on sections 2?21 of the WSJ
Penn Treebank, transformed to dependency trees following
Yamada and Matsumoto (2003). (The same treebank data
were also to estimate many of the parameters of our model, as
discussed in the text.) Though it leads to a partial ?pipeline?
approximation of the posterior probability p(c | s, t), we be-
lieve that the relatively high quality of English dependency
parsing makes this approximation reasonable.
?
?
j??t(i)??t(i)
m?
x(j)=0
P (? t,j | tj , x(j), ?
s)
?pkid (tj , ?
t
` (j), x(j) | ti, x(i), ?
s) (4)
where pval and pkid are valence and child-
production probabilities parameterized as dis-
cussed in ?3.4. Note the recursion in the second-
to-last line.
We next describe a dynamic programming so-
lution for calculating p(? t | Gp(? s)). In ?3.4 we
discuss the parameterization of the model.
3.3 Dynamic Programming
Let C(i, l) refer to the probability of ? t,i, assum-
ing that the parent of ti, t?tp(i), is aligned to sl. For
leaves of ? t, the base case is:
C(i, l) = pval (0, 0 | ti)? (5)
?m
k=0 pkid (ti, ?
t
` (i), k | t?tp(i), l, ?
s)
where k ranges over possible values of x(i), the
source-tree node to which ti is aligned. The recur-
sive case is:
C(i, l) = pval (|?
t(i)|, |?t(i)| | ti) (6)
?
?m
k=0 pkid (ti, ?
t
` (i), k | t?tp(i), l, ?
s)
?
?
j??t(i)??t(i)C(j, k)
We assume that the wall symbols t0 and s0 are
aligned, so p(? t | Gp(? s)) = C(r, 0), where r is
the index of the root word of the target tree ? t. It
is straightforward to show that this algorithm re-
quires O(m2n) runtime and O(mn) space.
3.4 Parameterization
The valency distribution pval in Eq. 4 is estimated
in our model using the transformed treebank (see
footnote 4). For unobserved cases, the conditional
probability is estimated by backing off to the par-
ent POS tag and child direction.
We discuss next how to parameterize the prob-
ability pkid that appears in Equations 4, 5, and 6.
This conditional distribution forms the core of our
QGs, and we deviate from earlier research using
QGs in defining pkid in a fully generative way.
In addition to assuming that dependency parse
trees for s and t are observable, we also assume
each word wi comes with POS and named entity
tags. In our experiments these were obtained au-
tomatically using MXPOST (Ratnaparkhi, 1996)
and BBN?s Identifinder (Bikel et al, 1999).
470
For clarity, let j = ? tp(i) and let l = x(j).
pkid(ti, ?
t
` (i), x(i) | tj , l, ?
s) =
pconfig(config(ti, tj , sx(i), sl) | tj , l, ?
s) (7)
?punif (x(i) | config(ti, tj , sx(i), sl)) (8)
?plab(?
t
` (i) | config(ti, tj , sx(i), sl)) (9)
?ppos(pos(ti) | pos(sx(i))) (10)
?pne(ne(ti) | ne(sx(i))) (11)
?plsrel (lsrel(ti) | sx(i)) (12)
?pword (ti | lsrel(ti), sx(i)) (13)
We consider each of the factors above in turn.
Configuration In QG, ?configurations? refer to
the tree relationship among source-tree nodes
(above, sl and sx(i)) aligned to a pair of parent-
child target-tree nodes (above, tj and ti). In deriv-
ing ? t,j , the model first chooses the configuration
that will hold among ti, tj , sx(i) (which has yet
to be chosen), and sl (line 7). This is defined for
configuration c log-linearly by:5
pconfig(c | tj , l, ?
s) =
?c
?
c?:?sk,config(ti,tj ,sk,sl)=c?
?c?
(14)
Permissible configurations in our model are shown
in Table 1. These are identical to prior work
(Smith and Eisner, 2006; Wang et al, 2007),
except that we add a ?root? configuration that
aligns the target parent-child pair to null and the
head word of the source sentence, respectively.
Using many permissible configurations helps re-
move negative effects from noisy parses, which
our learner treats as evidence. Fig. 1 shows some
examples of major configurations that Gp discov-
ers in the data.
Source tree alignment After choosing the config-
uration, the specific node in ? s that ti will align
to, sx(i) is drawn uniformly (line 8) from among
those in the configuration selected.
Dependency label, POS, and named entity class
The newly generated target word?s dependency
label, POS, and named entity class drawn from
multinomial distributions plab , ppos , and pne that
condition, respectively, on the configuration and
the POS and named entity class of the aligned
source-tree word sx(i) (lines 9?11).
5We use log-linear models three times: for the configura-
tion, the lexical semantics class, and the word. Each time,
we are essentially assigning one weight per outcome and
renormalizing among the subset of outcomes that are possible
given what has been derived so far.
Configuration Description
parent-child ? sp(x(i)) = x(j), appended with ?
s
` (x(i))
child-parent x(i) = ? sp(x(j)), appended with ?
s
` (x(j))
grandparent-
grandchild
? sp(?
s
p(x(i))) = x(j), appended with
? s` (x(i))
siblings ? sp(x(i)) = ?
s
p(x(j)), x(i) 6= x(j)
same-node x(i) = x(j)
c-command the parent of one source-side word is an
ancestor of the other source-side word
root x(j) = 0, x(i) is the root of s
child-null x(i) = 0
parent-null x(j) = 0, x(i) is something other than
root of s
other catch-all for all other types of configura-
tions, which are permitted
Table 1: Permissible configurations. i is an index in t whose
configuration is to be chosen; j = ? tp(i) is i?s parent.
WordNet relation(s) The model next chooses a
lexical semantics relation between sx(i) and the
yet-to-be-chosen word ti (line 12). Following
Wang et al (2007),6 we employ a 14-feature log-
linear model over all logically possible combina-
tions of the 14 WordNet relations (Miller, 1995).7
Similarly to Eq. 14, we normalize this log-linear
model based on the set of relations that are non-
empty in WordNet for the word sx(i).
Word Finally, the target word is randomly chosen
from among the set of words that bear the lexical
semantic relationship just chosen (line 13). This
distribution is, again, defined log-linearly:
pword (ti | lsrel(ti) = R, sx(i)) =
?ti
?
w?:sx(i)Rw? ?w?
(15)
Here ?w is the Good-Turing unigram probability
estimate of a word w from the Gigaword corpus
(Graff, 2003).
3.5 Base Grammar G0
In addition to the QG that generates a second sen-
tence bearing the desired relationship (paraphrase
or not) to the first sentence s, our model in ?2 also
requires a base grammar G0 over s.
We view this grammar as a trivial special case
of the same QG model already described. G0 as-
sumes the empty source sentence consists only of
6Note that Wang et al (2007) designed pkid as an inter-
polation between a log-linear lexical semantics model and a
word model. Our approach is more fully generative.
7These are: identical-word, synonym, antonym (includ-
ing extended and indirect antonym), hypernym, hyponym,
derived form, morphological variation (e.g., plural form),
verb group, entailment, entailed-by, see-also, causal relation,
whether the two words are same and is a number, and no re-
lation.
471
(a) parent-child
fill
questionnaire
complete
questionnaire
dozens
wounded
injured
dozens
(b) child-parent (c) grandparent-grandchild
will
chief
will
Secretary
Liscouski
quarter
first
first-quarter
(e) same-node
U.S
refunding
massive
(f) siblings
U.S
treasury
treasury
(g) root
null
fell
null
dropped
(d) c-command
signatures
necessary
signatures
needed
897,158
the
twice
approaching
collected
Figure 1: Some example configurations from Table 1 that Gp discovers in the dev. data. Directed arrows show head-modifier
relationships, while dotted arrows show alignments.
a single wall node. Thus every word generated un-
der G0 aligns to null, and we can simplify the dy-
namic programming algorithm that scores a tree
? s under G0:
C ?(i) = pval (|?
t(i)|, |?t(i)| | si)
?plab(?
t
` (i))? ppos(pos(ti))? pne(ne(ti))
?pword(ti)?
?
j:?t(j)=iC
?(j) (16)
where the final product is 1 when ti has no chil-
dren. It should be clear that p(s | G0) = C ?(0).
We estimate the distributions over dependency
labels, POS tags, and named entity classes using
the transformed treebank (footnote 4). The dis-
tribution over words is taken from the Gigaword
corpus (as in ?3.4).
It is important to note thatG0 is designed to give
a smoothed estimate of the probability of a partic-
ular parsed, named entity-tagged sentence. It is
never used for parsing or for generation; it is only
used as a component in the generative probability
model presented in ?2 (Eq. 2).
3.6 Discriminative Training
Given training data
?
?s(i)1 , s
(i)
2 , c
(i)?
?N
i=1
, we train
the model discriminatively by maximizing regu-
larized conditional likelihood:
max
?
N?
i=1
log pQ(c
(i) | s(i)1 , s
(i)
2 ,?)
? ?? ?
Eq. 2 relates this to G{0,p,n}
?C???22
(17)
The parameters ? to be learned include the class
priors, the conditional distributions of the depen-
dency labels given the various configurations, the
POS tags given POS tags, the NE tags given NE
tags appearing in expressions 9?11, the configura-
tion weights appearing in Eq. 14, and the weights
of the various features in the log-linear model for
the lexical-semantics model. As noted, the distri-
butions pval , the word unigram weights in Eq. 15,
and the parameters of the base grammar are fixed
using the treebank (see footnote 4) and the Giga-
word corpus.
Since there is a hidden variable (x), the objec-
tive function is non-convex. We locally optimize
using the L-BFGS quasi-Newton method (Liu and
Nocedal, 1989). Because many of our parameters
are multinomial probabilities that are constrained
to sum to one and L-BFGS is not designed to han-
dle constraints, we treat these parameters as un-
normalized weights that get renormalized (using a
softmax function) before calculating the objective.
4 Data and Task
In all our experiments, we have used the Mi-
crosoft Research Paraphrase Corpus (Dolan et al,
2004; Quirk et al, 2004). The corpus contains
5,801 pairs of sentences that have been marked
as ?equivalent? or ?not equivalent.? It was con-
structed from thousands of news sources on the
web. Dolan and Brockett (2005) remark that
this corpus was created semi-automatically by first
training an SVM classifier on a disjoint annotated
10,000 sentence pair dataset and then applying
the SVM on an unseen 49,375 sentence pair cor-
pus, with its output probabilities skewed towards
over-identification, i.e., towards generating some
false paraphrases. 5,801 out of these 49,375 pairs
were randomly selected and presented to human
judges for refinement into true and false para-
phrases. 3,900 of the pairs were marked as having
472
About 120 potential jurors were being asked to complete a lengthy questionnaire . 
The jurors were taken into the courtroom in groups of 40 and asked to fill out a questionnaire .
Figure 2: Discovered alignment of Ex. 19 produced by Gp. Observe that the model aligns identical words and also ?complete?
and ?fill? in this specific case. This kind of alignment provides an edge over a simple lexical overlap model.
?mostly bidirectional entailment,? a standard def-
inition of the paraphrase relation. Each sentence
was labeled first by two judges, who averaged 83%
agreement, and a third judge resolved conflicts.
We use the standard data split into 4,076 (2,753
paraphrase, 1,323 not) training and 1,725 (1147
paraphrase, 578 not) test pairs. We reserved a ran-
domly selected 1,075 training pairs for tuning.We
cite some examples from the training set here:
(18) Revenue in the first quarter of the year dropped 15
percent from the same period a year earlier.
With the scandal hanging over Stewart?s company,
revenue in the first quarter of the year dropped 15
percent from the same period a year earlier.
(19) About 120 potential jurors were being asked to
complete a lengthy questionnaire.
The jurors were taken into the courtroom in groups of
40 and asked to fill out a questionnaire.
Ex. 18 is a true paraphrase pair. Notice the high
lexical overlap between the two sentences (uni-
gram overlap of 100% in one direction and 72%
in the other). Ex. 19 is another true paraphrase
pair with much lower lexical overlap (unigram
overlap of 50% in one direction and 30% in the
other). Notice the use of similar-meaning phrases
and irrelevant modifiers that retain the same mean-
ing in both sentences, which a lexical overlap
model cannot capture easily, but a model like a QG
might. Also, in both pairs, the relationship cannot
be called total bidirectional equivalence because
there is some extra information in one sentence
which cannot be inferred from the other.
Ex. 20 was labeled ?not paraphrase?:
(20) ?There were a number of bureaucratic and
administrative missed signals - there?s not one person
who?s responsible here,? Gehman said.
In turning down the NIMA offer, Gehman said, ?there
were a number of bureaucratic and administrative
missed signals here.
There is significant content overlap, making a de-
cision difficult for a na??ve lexical overlap classifier.
(In fact, pQ labels this example n while the lexical
overlap models label it p.)
The fact that negative examples in this corpus
were selected because of their high lexical over-
lap is important. It means that any discrimina-
tive model is expected to learn to distinguish mere
overlap from paraphrase. This seems appropriate,
but it does mean that the ?not paraphrase? relation
ought to be denoted ?not paraphrase but decep-
tively similar on the surface.? It is for this reason
that we use a special QG for the n relation.
5 Experimental Evaluation
Here we present our experimental evaluation using
pQ. We trained on the training set (3,001 pairs)
and tuned model metaparameters (C in Eq. 17)
and the effect of different feature sets on the de-
velopment set (1,075 pairs). We report accuracy
on the official MSRPC test dataset. If the poste-
rior probability pQ(p | s1, s2) is greater than 0.5,
the pair is labeled ?paraphrase? (as in Eq. 1).
5.1 Baseline
We replicated a state-of-the-art baseline model for
comparison. Wan et al (2006) report the best pub-
lished accuracy, to our knowledge, on this task,
using a support vector machine. Our baseline is
a reimplementation of Wan et al (2006), using
features calculated directly from s1 and s2 with-
out recourse to any hidden structure: proportion
of word unigram matches, proportion of lemma-
tized unigram matches, BLEU score (Papineni et
al., 2001), BLEU score on lemmatized tokens, F
measure (Turian et al, 2003), difference of sen-
tence length, and proportion of dependency rela-
tion overlap. The SVM was trained to classify
positive and negative examples of paraphrase us-
ing SVMlight (Joachims, 1999).8 Metaparameters,
tuned on the development data, were the regu-
larization constant and the degree of the polyno-
mial kernel (chosen in [10?5, 102] and 1?5 respec-
tively.).9
It is unsurprising that the SVM performs very
well on the MSRPC because of the corpus creation
process (see Sec. 4) where an SVM was applied
as well, with very similar features and a skewed
decision process (Dolan and Brockett, 2005).
8http://svmlight.joachims.org
9Our replication of the Wan et al model is approxi-
mate, because we used different preprocessing tools: MX-
POST for POS tagging (Ratnaparkhi, 1996), MSTParser
for parsing (McDonald et al, 2005), and Dan Bikel?s
interface (http://www.cis.upenn.edu/?dbikel/
software.html#wn) to WordNet (Miller, 1995) for
lemmatization information. Tuning led to C = 17 and poly-
nomial degree 4.
473
Model Accuracy Precision Recall
baselines
all p 66.49 66.49 100.00
Wan et al SVM (reported) 75.63 77.00 90.00
Wan et al SVM (replication) 75.42 76.88 90.14
pQ
lexical semantics features removed 68.64 68.84 96.51
all features 73.33 74.48 91.10
c-command disallowed (best; see text) 73.86 74.89 91.28
?6 pL 75.36 78.12 87.44
product of experts 76.06 79.57 86.05
oracles
Wan et al SVM and pL 80.17 100.00 92.07
Wan et al SVM and pQ 83.42 100.00 96.60
pQ and pL 83.19 100.00 95.29
Table 2: Accuracy,
p-class precision, and
p-class recall on the test
set (N = 1,725). See
text for differences in
implementation
between Wan et al and
our replication; their
reported score does not
include the full test set.
5.2 Results
Tab. 2 shows performance achieved by the base-
line SVM and variations on pQ on the test set. We
performed a few feature ablation studies, evaluat-
ing on the development data. We removed the lex-
ical semantics component of the QG,10 and disal-
lowed the syntactic configurations one by one, to
investigate which components of pQ contributes to
system performance. The lexical semantics com-
ponent is critical, as seen by the drop in accu-
racy from the table (without this component, pQ
behaves almost like the ?all p? baseline). We
found that the most important configurations are
?parent-child,? and ?child-parent? while damage
from ablating other configurations is relatively
small. Most interestingly, disallowing the ?c-
command? configuration resulted in the best ab-
solute accuracy, giving us the best version of pQ.
The c-command configuration allows more distant
nodes in a source sentence to align to parent-child
pairs in a target (see Fig. 1d). Allowing this con-
figuration guides the model in the wrong direction,
thus reducing test accuracy. We tried disallowing
more than one configuration at a time, without get-
ting improvements on development data. We also
tried ablating the WordNet relations, and observed
that the ?identical-word? feature hurt the model
the most. Ablating the rest of the features did not
produce considerable changes in accuracy.
The development data-selected pQ achieves
higher recall by 1 point than Wan et al?s SVM,
but has precision 2 points worse.
5.3 Discussion
It is quite promising that a linguistically-motivated
probabilistic model comes so close to a string-
similarity baseline, without incorporating string-
local phrases. We see several reasons to prefer
10This is accomplished by eliminating lines 12 and 13 from
the definition of pkid and redefining pword to be the unigram
word distribution estimated from the Gigaword corpus, as in
G0, without the help of WordNet.
the more intricate QG to the straightforward SVM.
First, the QG discovers hidden alignments be-
tween words. Alignments have been leveraged in
related tasks such as textual entailment (Giampic-
colo et al, 2007); they make the model more inter-
pretable in analyzing system output (e.g., Fig. 2).
Second, the paraphrases of a sentence can be con-
sidered to be monolingual translations. We model
the paraphrase problem using a direct machine
translation model, thus providing a translation in-
terpretation of the problem. This framework could
be extended to permit paraphrase generation, or to
exploit other linguistic annotations, such as repre-
sentations of semantics (see, e.g., Qiu et al, 2006).
Nonetheless, the usefulness of surface overlap
features is difficult to ignore. We next provide an
efficient way to combine a surface model with pQ.
6 Product of Experts
Incorporating structural alignment and surface
overlap features inside a single model can make
exact inference infeasible. As an example, con-
sider features like n-gram overlap percentages that
provide cues of content overlap between two sen-
tences. One intuitive way of including these fea-
tures in a QG could be including these only at
the root of the target tree, i.e. while calculating
C(r, 0). These features have to be included in
estimating pkid, which has log-linear component
models (Eq. 7- 13). For these bigram or trigram
overlap features, a similar log-linear model has
to be normalized with a partition function, which
considers the (unnormalized) scores of all possible
target sentences, given the source sentence.
We therefore combine pQ with a lexical overlap
model that gives another posterior probability es-
timate pL(c | s1, s2) through a product of experts
(PoE; Hinton, 2002), pJ(c | s1, s2)
=
pQ(c | s1, s2)? pL(c | s1, s2)
?
c??{p,n}
pQ(c
? | s1, s2)? pL(c
? | s1, s2)
(21)
474
Eq. 21 takes the product of the two models? poste-
rior probabilities, then normalizes it to sum to one.
PoE models are used to efficiently combine several
expert models that individually constrain different
dimensions in high-dimensional data, the product
therefore constraining all of the dimensions. Com-
bining models in this way grants to each expert
component model the ability to ?veto? a class by
giving it low probability; the most probable class
is the one that is least objectionable to all experts.
Probabilistic Lexical Overlap Model We de-
vised a logistic regression (LR) model incorpo-
rating 18 simple features, computed directly from
s1 and s2, without modeling any hidden corre-
spondence. LR (like the QG) provides a proba-
bility distribution, but uses surface features (like
the SVM). The features are of the form precisionn
(number of n-gram matches divided by the num-
ber of n-grams in s1), recalln (number of n-gram
matches divided by the number of n-grams in s2)
and Fn (harmonic mean of the previous two fea-
tures), where 1 ? n ? 3. We also used lemma-
tized versions of these features. This model gives
the posterior probability pL(c | s1, s2), where
c ? {p, n}. We estimated the model parameters
analogously to Eq. 17. Performance is reported in
Tab. 2; this model is on par with the SVM, though
trading recall in favor of precision. We view it as a
probabilistic simulation of the SVM more suitable
for combination with the QG.
Training the PoE Various ways of training a PoE
exist. We first trained pQ and pL separately as
described, then initialized the PoE with those pa-
rameters. We then continued training, maximizing
(unregularized) conditional likelihood.
Experiment We used pQ with the ?c-command?
configuration excluded, and the LR model in the
product of experts. Tab. 2 includes the final re-
sults achieved by the PoE. The PoE model outper-
forms all the other models, achieving an accuracy
of 76.06%.11 The PoE is conservative, labeling a
pair as p only if the LR and the QG give it strong
p probabilities. This leads to high precision, at the
expense of recall.
Oracle Ensembles Tab. 2 shows the results of
three different oracle ensemble systems that cor-
rectly classify a pair if either of the two individual
systems in the combination is correct. Note that
the combinations involving pQ achieve 83%, the
11This accuracy is significant over pQ under a paired t-test
(p < 0.04), but is not significant over the SVM.
human agreement level for the MSRPC. The LR
and SVM are highly similar, and their oracle com-
bination does not perform as well.
7 Related Work
There is a growing body of research that uses the
MSRPC (Dolan et al, 2004; Quirk et al, 2004)
to build models of paraphrase. As noted, the most
successful work has used edit distance (Zhang and
Patrick, 2005) or bag-of-words features to mea-
sure sentence similarity, along with shallow syn-
tactic features (Finch et al, 2005; Wan et al, 2006;
Corley and Mihalcea, 2005). Qiu et al (2006)
used predicate-argument annotations.
Most related to our approach, Wu (2005) used
inversion transduction grammars?a synchronous
context-free formalism (Wu, 1997)?for this task.
Wu reported only positive-class (p) precision (not
accuracy) on the test set. He obtained 76.1%,
while our PoE model achieves 79.6% on that mea-
sure. Wu?s model can be understood as a strict
hierarchical maximum-alignment method. In con-
trast, our alignments are soft (we sum over them),
and we do not require strictly isomorphic syntac-
tic structures. Most importantly, our approach is
founded on a stochastic generating process and es-
timated discriminatively for this task, while Wu
did not estimate any parameters from data at all.
8 Conclusion
In this paper, we have presented a probabilistic
model of paraphrase incorporating syntax, lexi-
cal semantics, and hidden loose alignments be-
tween two sentences? trees. Though it fully de-
fines a generative process for both sentences and
their relationship, the model is discriminatively
trained to maximize conditional likelihood. We
have shown that this model is competitive for de-
termining whether there exists a semantic rela-
tionship between them, and can be improved by
principled combination with more standard lexical
overlap approaches.
Acknowledgments
The authors thank the three anonymous review-
ers for helpful comments and Alan Black, Freder-
ick Crabbe, Jason Eisner, Kevin Gimpel, Rebecca
Hwa, David Smith, and Mengqiu Wang for helpful
discussions. This work was supported by DARPA
grant NBCH-1080004.
475
References
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In Proc. of NAACL.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34(1-3):211?231.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proc. of HLT-NAACL.
Courtney Corley and Rada Mihalcea. 2005. Mea-
suring the semantic similarity of texts. In Proc. of
ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment.
William B. Dolan and Chris Brockett. 2005. Auto-
matically constructing a corpus of sentential para-
phrases. In Proc. of IWP.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources. In
Proc. of COLING.
Andrew Finch, Young Sook Hwang, and Eiichiro
Sumita. 2005. Using machine translation evalua-
tion techniques to determine sentence-level seman-
tic equivalence. In Proc. of IWP.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recog-
nizing textual entailment challenge. In Proc. of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
David Graff. 2003. English Gigaword. Linguistic
Data Consortium.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Computation, 14:1771?1800.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming (Ser. B), 45(3):503?528.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proc. of EWNLG.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. of ACL.
Kathleen R. McKeown. 1979. Paraphrasing using
given and new information in a question-answer sys-
tem. In Proc. of ACL.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proc. of ACL.
George A. Miller. 1995. Wordnet: a lexical database
for English. Commun. ACM, 38(11):39?41.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proc. of EMNLP.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proc. of EMNLP.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proc. of
EMNLP.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projec-
tion of syntactic dependencies. In Proc. of the HLT-
NAACL Workshop on Statistical Machine Transla-
tion.
Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003. Evaluation of machine translation and its
evaluation. In Proc. of Machine Translation Summit
IX.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile
Paris. 2006. Using dependency-based features to
take the ?para-farce? out of paraphrase. In Proc. of
ALTW.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proc. of EMNLP-
CoNLL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3).
Dekai Wu. 2005. Recognizing paraphrases and textual
entailment using inversion transduction grammars.
In Proc. of the ACL Workshop on Empirical Model-
ing of Semantic Equivalence and Entailment.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of IWPT.
Yitao Zhang and Jon Patrick. 2005. Paraphrase identi-
fication by text canonicalization. In Proc. of ALTW.
476
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67?71,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Non-textual Event Summarization by Applying Machine Learning to
Template-based Language Generation
Mohit Kumar and Dipanjan Das and Sachin Agarwal and Alexander I. Rudnicky
Language Technologies Institute
Carnegie Mellon University, Pittsburgh, USA
mohitkum,dipanjan,sachina,air@cs.cmu.edu
Abstract
We describe a learning-based system that
creates draft reports based on observation
of people preparing such reports in a tar-
get domain (conference replanning). The
reports (or briefings) are based on a mix
of text and event data. The latter consist
of task creation and completion actions,
collected from a wide variety of sources
within the target environment. The report
drafting system is part of a larger learning-
based cognitive assistant system that im-
proves the quality of its assistance based
on an opportunity to learn from observa-
tion. The system can learn to accurately
predict the briefing assembly behavior and
shows significant performance improve-
ments relative to a non-learning system,
demonstrating that it?s possible to create
meaningful verbal descriptions of activity
from event streams.
1 Introduction
We describe a system for recommending items for
a briefing created after a session with a crisis man-
agement system in a conference replanning do-
main. The briefing system is learning-based, in
that it initially observes how one set of users cre-
ates such briefings then generates draft reports for
another set of users. This system, the Briefing
Assistant(BA), is part of a set of learning-based
cognitive assistants each of which observes users
and learns to assist users in performing their tasks
faster and more accurately.
The difference between this work from
most previous efforts, primarily based on text-
extraction approaches is the emphasis on learning
to summarize event patterns. This work also
differs in its emphasis on learning from user
behavior in the context of a task.
Report generation from non-textual sources has
been previously explored in the Natural Language
Generation (NLG) community in a variety of do-
mains, based on, for example, a database of events.
However, a purely generative approach is not suit-
able in our circumstances, as we want to summa-
rize a variety of tasks that the user is performing
and present a summary tailored to a target audi-
ence, a desirable characteristic of good briefings
(Radev and McKeown, 1998). Thus we approach
the problem by applying learning techniques com-
bined with a template-based generation system to
instantiate the briefing-worthy report items. The
task of instantiating the briefing-worthy items is
similar to the task of Content Selection (Duboue,
2004) in the Generation pipeline however our ap-
proach minimizes linguistic involvement. Our
choice of a template-based generative system was
motivated by recent discussions in the NLG com-
munity (van Deemter et al, 2005) about the prac-
ticality and effectiveness of this approach.
The plan of the paper is as follows. We describe
relevant work from existing literature in the next
section. Then, we provide brief system description
followed by experiments and results. We conclude
with a summary of the work.
2 Related Work
Event based summarization has been studied in the
summarization community. (Daniel et al, 2003)
described identification of sub-events in multiple
documents. (Filatova and Hatzivassiloglou, 2004)
mentioned the use of event-based features in ex-
tractive summarization and (Wu, 2006; Li et al,
2006) describe similar work based on events oc-
curring in text. However, unlike the case at hand,
all the work on event-based summarization used
text as source material.
Non-textual summarization has also been ex-
plored in the Natural Language Generation (NLG)
community within the broad task of generating
67
reports based on database of events in specific
domains such as medical (Portet et al, 2009),
weather (Belz, 2007), sports (Oh and Shrobe,
2008) etc. However, in our case we want to sum-
marize a variety of tasks that the user is perform-
ing and present a summary to an intended audi-
ence (as defined by a report request).
Recent advances in NLG research use statis-
tical approaches at various stages of processing
in the generation pipeline like content selection
(Duboue and McKeown, 2003; Barzilay and Lee,
2004), probabilistic generation rules (Belz, 2007).
Our proposed approach differs from these in that
we apply machine learning after generation of all
the templates, as a post-processing step, to rank
them for inclusion in the final briefing. We could
have used a general purpose template-based gen-
eration framework like TG/2 (Busemann, 2005),
but since the number of templates and their corre-
sponding aggregators is limited, we chose an ap-
proach based on string manipulation.
We found in our work that an approach based
on modeling individual users and then combining
the outputs of such models using a voting scheme
gives the best results, although our approach is
distinguishable from collaborative filtering tech-
niques used for driving recommendation systems
(Hofmann, 2004). We believe this is due to the
fact that the individual sessions from which rank-
ing models are learned, although they range over
the same collection of component tasks, can lead
to very different (human-generated) reports. That
is, the particular history of a session will affect
what is considered to be briefing-worthy.
3 System Overview
Figure 1: Briefing Assistant Data Flow.
The Briefing Assistant Model: We treat the
task of briefing generation in the current domain1
as non-textual event-based summarization. The
1More details about the domain and the interaction of BA
with the larger system are mentioned in a longer version of
the paper (Kumar et al, 2009)
Figure 2: The category tree showing the informa-
tion types that we expect in a briefing.
events are the task creation and task completion
actions logged by various cognitive assistants in
the system (so-called specialists). As part of the
design phase for the template-based generation
component, we identified a set of templates, based
on the actual briefings written by users in a sepa-
rate experiment. Ideally, we would like to adopt
a corpus-based approach to automatically extract
the templates in the domain, like (Kumar et al,
2008), but since the sample briefings available to
us were very few, the application of such corpus-
based techniques was not necessary. Based on
this set of templates we identified the patterns that
needed to be extracted from the event logs in order
to populate the templates. A ranking model was
also designed for ordering instantiations of this set
of templates and to recommend the top 4 most rel-
evant ones for a given session.
The overall data flow for BA during a session
(runtime) is shown in Figure 1. The various spe-
cialist modules generate task related events that
are logged in a database. The aggregators operate
over this database and emails to extract relevant
patterns. These patterns in turn are used to popu-
late templates which constitute candidate briefing
items. The candidate briefing items are then or-
dered by the ranking module and presented to the
user.
Template Design and Aggregators: The set
of templates used in the current instantiation of
the BA was derived from a corpus of human-
generated briefings collected in a previous exper-
iment using the same crisis management system.
The set of templates was designed to cover the
range of items that users in that experiment chose
to include in their reports corresponding to nine
categories shown in Figure 2. We found that in-
formation can be conveyed at different levels of
granularity (for example, qualitatively or quantita-
tively). The appropriate choice of granularity for
68
a particular session is a factor that the system can
learn2.
Ranking Model, Classifiers and Features: The
ranking module orders candidate templates so
that the four most relevant ones appear in the
briefing draft. The ranking system consists of
a consensus-based classifier, based on individual
classifier models for each user in the training set.
The prediction from each classifier are combined
(averaged) to produce a final rank of each tem-
plate.
We used the Minorthird package (Cohen, 2004)
for modeling. Specifically we allowed the sys-
tem to experiment with eleven different learning
schemes and select the best one based on cross-
validation within the training corpus. The schemes
were Naive Bayes, Voted Perceptron, Support
Vector Machines, Ranking Perceptron, K Nearest
Neighbor, Decision Tree, AdaBoost, Passive Ag-
gressive learner, Maximum Entropy learner, Bal-
anced Winnow and Boosted Ranking learner.
The features3 used in the system are static or
dynamic. Static features reflect the properties of
the templates irrespective of the user?s activity
whereas the dynamic features are based on the
actual events that took place. We used the In-
formation Gain (IG) metric for feature selection,
experimenting with seven different cut-off values
All, 20, 15, 10, 7, 5, 4 for the total number of se-
lected features.
4 Experiments and Results
Experimental Setup: Two experimental condi-
tions were used to differentiate performance based
on knowledge engineering, designated MinusL
and performance based on learning, designated
PlusL.4
Email Trigger: In the simulated conference
replanning crisis, the briefing was triggered by
an email containing explicit information requests,
not known beforehand. To customize the brief-
ing according to the request, a natural language
processing module identified the categories of in-
formation requested. The details of the module
are beyond the scope of the current paper as it
2The details of template design process including sample
templates, categories of templates and details of aggregators
are presented in (Kumar et al, 2009)
3Detailed description of the features are mentioned in
(Kumar et al, 2009)
4The details of the experimental setup as part of the larger
cognitive assistant system are presented in (Kumar et al,
2009).
is external to our system; it took into account
the template categories we earlier identified. Fig-
ure 4 shows a sample briefing email stimulus.
The mapping from the sample email in the figure
to the categories is as follows: ?expected atten-
dance? - Property-Session; ?how many sessions
have been rescheduled?, ?how many still need to
be rescheduled?, ?any problems you see as you
try to reschedule? - Session-Reschedule; ?status
of food service (I am worried about the keynote
lunch)? - Catering Vendors.
Training: Eleven expert users5 were asked to
provide training by using the system then generat-
ing the end of session briefing using the BA GUI.
For this training phase, no item ranking was per-
formed by the system, i.e. all the templates were
populated by the aggregators and recommenda-
tions were random. The expert user was asked
to select the best possible four items and was fur-
ther asked to judge the usefulness of the remaining
items. The resulting training data consists of the
activity log, extracted features and the user-labeled
items. The trigger message for the training users
did not contain any specific information request.
Test: Subjects were recruited to use the crisis
management system in MinusL and PlusL condi-
tion, although they were not aware of the condition
of the system and they were not involved with the
project. There were 54 test runs in the MinusL
condition and 47 in the PlusL condition. Out of
these runs, 29 subjects in MinusL and 43 subjects
in PlusL wrote a briefing using the BA. We report
the evaluation scores for this latter set.
Evaluation: The base performance metric is
Recall, defined in terms of the briefing templates
recommended by the system compared to the tem-
plates ultimately selected by the user. We justify
this by noting that Recall can be directly linked to
the expected time savings for the users. We cal-
culate two variants of Recall: Category-based?
calculated by matching the categories of the BA
recommended templates and user selected ones
ignoring the granularity and Template-based?
calculated by matching the exact templates. The
first metric indicates whether the right category of
information was selected and the latter indicates
whether the information was presented at the ap-
propriate level of detail.
We also performed subjective human evaluation
5Members of the project from other groups who were
aware of the scenario and various system functionalities but
not the ML methods
69
using a panel of three judges. The judges assigned
scores (0-4) to each of the bullets based on the
coverage of the crisis, clarity and conciseness, ac-
curacy and the correct level of granularity. They
were advised about certain briefing-specific char-
acteristics (e.g. negative bullet items are useful
and hence should be rated favorably). They were
also asked to provide a global assessment of report
quality, and evaluate the coverage of the requests
in the briefing stimulus email message. This pro-
cedure was very similar to the one used as the basis
for template selection.
Experiment: The automatic evaluation met-
ric used for the trained system configuration is
the Template-based recall measure. To obtain
the final system configuration, we automatically
evaluate the system under the various combina-
tions of parameter settings with eleven different
learning schemes and seven different feature se-
lection threshold (as mentioned in previous sec-
tions). Thus a total of 77 different configurations
are tested. For each configuration, we do a eleven-
fold cross-validation between the 11 training users
i.e. we leave one user as the test user and consider
the remaining ten users as training users. We av-
erage the performance across the 11 test cases and
obtain the final score for the configuration. We
choose the configuration with the highest score as
the final trained system configuration. The learned
system configuration in the current test includes
Balanced Winnow (Littlestone, 1988) and top 7
features.
Results: We noticed that four users in PlusL
condition took more than 8 minutes to complete
the briefing when the median time taken by the
users in PlusL condition was 55 seconds, so we
did not include these users in our analysis in order
to maintain the homogeneity of the dataset. These
four data points were identified as extreme outliers
using a procedure suggested by (NIST, 2008)6.
There were no extreme outliers in MinusL condi-
tion.
Figure 3a shows the Recall values for the Mi-
nusL and PlusL conditions. The learning delta
i.e. the difference between the recall values of
PlusL and MinusL is 33% for Template-based re-
call and 21% for Category-based recall. These
differences are significant at the p < 0.001 level.
6Extreme outliers are defined as data points that are out-
side the range [Q1?3?IQ,Q3+3?IQ] in a box plot. Q1 is
lower quartile, Q3 is upper quartile and IQ is the difference
(Q3?Q1) is the interquartile range.
The statistical significance for the Template-based
metric, which was the metric used for select-
ing system parameters during the training phase,
shows that learning is effective in this case. Since
the email stimulus processing module extracts the
briefing categories from the email the Category-
based and Template-based recall is expected to be
high for the baseline MinusL case. In our test, the
email stimuli had 3 category requests and so the
Category-based recall of 0.77 and Template-based
recall of 0.67 in MinusL is not unexpected.
Figure 3b shows the Judges? panel scores for
the briefings in MinusL and PlusL condition. The
learning delta in this case is 3.6% which is also
statistically significant, at p < 0.05. The statistical
significance of the learning delta validates that the
briefings generated during PlusL conditions are
better than MinusL condition. The absolute differ-
ence in the qualitative briefing scores between the
two conditions is small because MinusL users can
select from all candidates, while the recommenda-
tions they receive are random. Consequently they
need to spend more time in finding the right items.
The average time taken for a briefing in MinusL
condition is about 83 seconds and 62 seconds in
PlusL (see Figure 3c). While the time difference
is high (34%) it is not statistically significant due
to high variance.
Four of the top 10 most frequently selected fea-
tures across users for this system are dynamic fea-
tures. This indicates that the learning model is
capturing the user?s world state and the recom-
mendations are related to the underlying events.
We believe this validates the process we used to
generate briefing reports from non-textual events.
5 Summary
The Briefing Assistant is not designed to learn
the generic attributes of good reports; rather it?s
meant to rapidly learn the attributes of good re-
ports within a particular domain and to accom-
modate specific information needs on a report-by-
report basis. We found that learned customiza-
tion produces reports that are judged to be of bet-
ter quality. We also found that a consensus-based
modeling approach, which incorporates informa-
tion from multiple users, yields the best perfor-
mance. We believe that our approach can be used
to create flexible summarization systems for a va-
riety of applications.
70
(a) (b) (c)
Figure 3: (a) Recall values for MinusL and PlusL conditions (b) Briefing scores from the judges? panel
for MinusL and PlusL conditions (c) Briefing time taken for MinusL and PlusL conditions.
Figure 4: Template categories corresponding to
the Briefing request email.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of HLT-NAACL.
Stephan Busemann. 2005. Ten years after: An update
on TG/2 (and friends). In Proceedings of European
Natural Language Generation Workshop.
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net, 10th Jun 2009.
Naomi Daniel, Dragomir Radev, and Timothy Allison.
2003. Sub-event based multi-document summariza-
tion. In Proceedings of HLT-NAACL.
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules for
natural language generation. In Proceedings of
EMNLP.
Pablo A. Duboue. 2004. Indirect supervised learning
of content selection logic. In Proceedings of INLG.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
Event-based extractive summarization. In Text Sum-
marization Branches Out: Proceedings of the ACL-
04 Workshop.
Thomas Hofmann. 2004. Latent semantic models for
collaborative filtering. ACM Transactions on Infor-
mation Systems, 22(1):89?115.
Mohit Kumar, Dipanjan Das, and Alexander I. Rud-
nicky. 2008. Automatic extraction of briefing tem-
plates. In Proceedings of IJCNLP.
Mohit Kumar, Dipanjan Das, Sachin Agarwal, and
Alexander I. Rudnicky. 2009. Non-textual event
summarization by applying machine learning to
template-based language generation. Technical Re-
port CMU-LTI-09-012, Language Technologies In-
stitute, Carnegie Mellon University.
Wenjie Li, Mingli Wu, Qin Lu, Wei Xu, and Chunfa
Yuan. 2006. Extractive summarization using inter-
and intra- event relevance. In Proceedings of ACL.
Nick Littlestone. 1988. Learning quickly when irrele-
vant attributes abound: A new linear-threshold algo-
rithm. Machine Learning, 2(4):285?318.
NIST. 2008. NIST/SEMATECH e-
handbook of statistical methods.
http://www.itl.nist.gov/div898/handbook/, 10th
Jun 2009.
Alice Oh and Howard Shrobe. 2008. Generating base-
ball summaries from multiple perspectives by re-
ordering content. In Proceedings of INLG.
Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy
Sykes. 2009. Automatic generation of textual sum-
maries from neonatal intensive care data. Artificial
Intelligence, 173(7-8):789?816.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):470?500.
Kees van Deemter, Emiel Krahmer, and Mariet The-
une. 2005. Real versus template-based natural lan-
guage generation: A false opposition? Computa-
tional Linguistics, 31(1):15?24.
Mingli Wu. 2006. Investigations on event-based sum-
marization. In Proceedings of ACL.
71
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50?61,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Structure Prediction
with Non-Parallel Multilingual Guidance
Shay B. Cohen Dipanjan Das Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{scohen,dipanjan,nasmith}@cs.cmu.edu
Abstract
We describe a method for prediction of lin-
guistic structure in a language for which only
unlabeled data is available, using annotated
data from a set of one or more helper lan-
guages. Our approach is based on a model
that locally mixes between supervised mod-
els from the helper languages. Parallel data
is not used, allowing the technique to be ap-
plied even in domains where human-translated
texts are unavailable. We obtain state-of-the-
art performance for two tasks of structure pre-
diction: unsupervised part-of-speech tagging
and unsupervised dependency parsing.
1 Introduction
A major focus of recent NLP research has involved
unsupervised learning of structure such as POS
tag sequences and parse trees (Klein and Manning,
2004; Johnson et al, 2007; Berg-Kirkpatrick et al,
2010; Cohen and Smith, 2010, inter alia). In its
purest form, such research has improved our un-
derstanding of unsupervised learning practically and
formally, and has led to a wide range of new algo-
rithmic ideas. Another strain of research has sought
to exploit resources and tools in some languages (es-
pecially English) to construct similar resources and
tools for other languages, through heuristic ?projec-
tion? (Yarowsky and Ngai, 2001; Xi and Hwa, 2005)
or constraints in learning (Burkett and Klein, 2008;
Smith and Eisner, 2009; Das and Petrov, 2011; Mc-
Donald et al, 2011) or inference (Smith and Smith,
2004). Joint unsupervised learning (Snyder and
Barzilay, 2008; Naseem et al, 2009; Snyder et al,
2009) is yet another research direction that seeks to
learn models for many languages at once, exploiting
linguistic universals and language similarity. The
driving force behind all of this work has been the
hope of building NLP tools for languages that lack
annotated resources.1
In this paper, we present an approach to using
annotated data from one or more languages (helper
languages) to learn models for another language that
lacks annotated data (the target language). Unlike
the previous work mentioned above, our framework
does not rely on parallel data in any form. This is
advantageous because parallel text exists only in a
few text domains (e.g., religious texts, parliamentary
proceedings, and news).
We focus on generative probabilistic models pa-
rameterized by multinomial distributions. We be-
gin with supervised maximum likelihood estimates
for models of the helper languages. In the second
stage, we learn a model for the target language using
unannotated data, maximizing likelihood over inter-
polations of the helper language models? distribu-
tions. The tying is performed at the parameter level,
through coarse, nearly-universal syntactic categories
(POS tags). The resulting model is then used to ini-
tialize learning of the target language?s model using
standard unsupervised parameter estimation.
Some previous multilingual research, such as
Bayesian parameter tying across languages (Co-
hen and Smith, 2009) or models of parameter
1Although the stated objective is often to build systems for
resource-poor languages and domains, for evaluation purposes,
annotated treebank test data figure prominently in this research
(including in this paper).
50
drift down phylogenetic trees (Berg-Kirkpatrick and
Klein, 2010) is comparable, but the practical as-
sumption of supervised helper languages is new to
this work. Naseem et al (2010) used universal
syntactic categories and rules to improve grammar
induction, but their model required expert hand-
written rules as constraints.
Herein, we specifically focus on two problems
in linguistic structure prediction: unsupervised POS
tagging and unsupervised dependency grammar in-
duction. Our experiments demonstrate that the pre-
sented method outperforms strong state-of-the-art
unsupervised baselines for both tasks. Our approach
can be applied to other problems in which a sub-
set of the model parameters can be linked across
languages. We also experiment with unsupervised
learning of dependency structures from words, by
combining our tagger and parser. Our results show
that combining our tagger and parser with joint
inference outperforms pipeline inference, and, in
several cases, even outperforms models built using
gold-standard part-of-speech tags.
2 Overview
For each language `, we assume the presence of a
set of fine-grained POS tags F`, used to annotate the
language?s treebank. Furthermore, we assume that
there is a set of universal, coarse-grained POS tags
C such that, for every language `, there is a determin-
istic mapping from fine-grained to coarse-grained
tags, ?` : F` ? C. Our approach can be summa-
rized using the following steps for a given task:
1. Select a set of L helper languages for which there
exists annotated data ?D1, . . . ,DL?. Here, we use
treebanks in these languages.
2. For all ` ? {1, . . . , L}, convert the examples in
D` by applying ?` to every POS tag in the data,
resulting in D?`. Estimate the parameters of a
probabilistic model using D?`. In this work, such
models are generative probabilistic models based
on multinomial distributions,2 including an HMM
and the dependency model with valence (DMV)
of Klein and Manning (2004). Denote the subset
of parameters that are unlexicalized by ?(`). (Lex-
icalized parameters will be denoted ?(`).)
2In ?4 we also consider a feature-based parametrization.
3. For the target language, define the set of valid un-
lexicalized parameters
? =
{
?
??????k =
L?
`=1
?`,k?(`)k ,
L?
`=1
?`,k = 1,? ? 0
}
,
(1)
for each group of parameters k, and maximize
likelihood over that set, using the target-language
unannotated data U . Because the syntactic cate-
gories referenced by each ?(`) and all models in ?
are in C, the models will be in the same parametric
family. (Figure 1 gives a graphical interpretation
of ?.) Let the resulting model be ?.
4. Transform ? by expanding the coarse-grained
syntactic categories into the target language?s
fine-grained categories. Use the resulting model
to initialize parameter estimation, this time over
fine-grained tags, again using the unannotated
target-language data U . Initialize lexicalized pa-
rameters ? for the target language using standard
methods (e.g., uniform initialization with random
symmetry breaking).
The main idea in the approach is to estimate a
certain model family for one language, while using
supervised models from other languages. The link
between the languages is achieved through coarse-
grained categories, which are now now common-
place (and arguably central to any theory of natural
language syntax). A key novel contribution is the
use of helper languages for initialization, and of un-
supervised learning to learn the contribution of each
helper language to that initialization (step 3). Addi-
tional treatment is required in expanding the coarse-
grained model to the fine-grained one (step 4).
3 Interpolated Multilingual Probabilistic
Context-Free Grammars
Our focus in this paper is on models that consist
of multinomial distributions that have relationships
between them through a generative process such as
a probabilistic context-free grammar (PCFG). More
specifically, we assume that we have a model defin-
ing a probability distribution over observed surface
forms x and derivations y parametrized by ?:
51
(0,1,0)
(0,0,1) (1,0,0)
English
Czech
German
Italian
Figure 1: A simple case of interpolation within the 3-
event probability simplex. The shaded area corresponds
to a convex hull inside the probability simplex, indicating
a mixture of the parameters of the four languages shown
in the figure.
p(x,y | ?) =
K?
k=1
Nk?
i=1
?fk,i(x,y)k,i (2)
= exp
K?
k=1
Nk?
i=1
fk,i(x,y) log ?k,i (3)
where fk,i is a function that ?counts? the number
of times the kth distribution?s ith event occurs in
the derivation. The parameters ? are a collection
of K multinomials ??1, . . . ,?K?, the kth of which
includes Nk events. Letting ?k = ??k,1, . . . , ?k,Nk?,
each ?k,i is a probability, such that ?k,?i, ?k,i ? 0
and ?k,?Nki=1 ?k,i = 1.
3.1 Multilingual Interpolation
Our framework places additional, temporary con-
straints on the parameters ?. More specifically, we
assume that we have L existing, parameter estimates
for the multinomial families from Eq. 3. Each such
estimate ?(`), for 1 ? ` ? L, corresponds to a the
maximum likelihood estimate based on annotated
data for the `th helper language. Then, to create a
model for new language, we define a new set of pa-
rameters ? as:
?k,i =
L?
`=1
?`,k?(`)k,i , (4)
where ? is the set of coefficients that we will now
be interested in estimating (instead of directly esti-
mating ?). Note that for each k,?L`=1 ?`,k = 1 and
?`,k ? 0.
3.2 Grammatical Interpretation
We now give an interpretation of our approach relat-
ing it to PCFGs. We assume familiarity with PCFGs.
For a PCFG ?G,?? we denote the set of nontermi-
nal symbols by N , the set of terminal symbols by
?, and the set of rewrite rules for each nonterminal
A ? N by R(A). Each r ? R(A) has the form
A ? ? where ? ? (N ? ?)?. In addition, there is
a probability attached to each rule ?A?? such that
?A ? N ,??:(A??)?R(A) ?A?? = 1. A PCFG can
be framed as a model using Eq. 3, where ? corre-
spond to K = |N | multinomial distributions, where
each distribution attaches probabilities to rules with
a specific left hand symbol.
We assume that the model we are trying to
estimate (over coarse part-of-speech tags) can be
framed as a PCFG ?G,??. This is indeed the case
for part-of-speech tagging and dependency grammar
induction we experiment with in ?6. In that case,
our approach can be framed for PCFGs as follow-
ing. We assume that there exists L set of parameters
for this PCFG ?(1), . . . ,?(L), each corresponding to
a helper language. We then create a new PCFG G?
with parameters ?? and ? as follows:
1. G? contains all nonterminal and terminal symbols
inG, and none of the rules inG.
2. For each nonterminal A in G, we create a new
nonterminal aA,` for ` ? {1, . . . , L}.
3. For each nonterminal A in G, we create rules
A ? aA,` for ` ? {1, . . . , L} which have proba-
bilities ?A?aA,` .
4. For each rule A? ? inG, we add toG? the rule
aA,` ? ? with
??aA,`?? = ?
(`)
A??. (5)
where ?(`)A?? is the probability associated with
rule A? ? in the `th helper language.
At each point, the derivational process of this
PCFG uses the nonterminal?s specific ? coefficients
52
to choose one of the helper languages. It then se-
lects a rule according to the multinomial from that
language. This step is repeated until a whole deriva-
tion is generated.
This PCFG representation of the approach in ?3
points to a possible generalization. Instead of using
an identical CFG backbone for each language, we
can use a set of PCFGs, ?G(`),?(`)? with an iden-
tical nonterminal set and alphabet, and repeat the
same construction as above, replacing step 4 with
the addition of rules of the form aA,` ? ? for each
rule A ? ? in G(`). Such a construction allows
more syntactic variability in the language we are try-
ing to estimate, originating in the syntax of the var-
ious helper languages. In this paper, we do not use
this generalization, and always use the same PCFG
backbone for all languages.
Note that the interpolated model can still be un-
derstood in terms of the exponential model of Eq. 3.
For a given collection of multinomials and base
models of the form of Eq. 3, we can analogously
define a new log-linear model over a set of ex-
tended derivations. These derivations will now in-
clude L ? K features of the form g`,k(x,y), cor-
responding to a count of the event of choosing the
`th mixture component for multinomial k. In addi-
tion, the feature set fk,i(x,y) will be extended to
a feature set of the form f`,k,i(x,y), analogous to
step 4 in constructed PCFG above. The model pa-
rameterized according to Eq. 4 can be recovered by
marginalizing out the ?g? features. We will refer to
the model with these new set of features as ?the ex-
tended model.?
4 Inference and Parameter Estimation
The main building block commonly required for un-
supervised learning in NLP is that of computing fea-
ture expectations for a given model. These feature
expectations can be used with an algorithm such as
expectation-maximization (where the expectations
are normalized to obtain a new set of multinomial
weights) or with other gradient based log-likelihood
optimization algorithms such as L-BFGS (Liu and
Nocedal, 1989) for feature-rich models.
Estimating Multinomial Distributions Given a
surface form x, a multinomial k and an event i in the
multinomial, ?feature expectation? refers to the cal-
culation of the following quantities (in the extended
model):
E[f`,k,i(x,y)] = ?y p(x,y | ?)f`,k,i(x,y) (6)
E[g`,k(x,y)] = ?y p(x,y | ?)g`,k(x,y) (7)
These feature expectations can usually be computed
using algorithms such as the forward-backward al-
gorithm for hidden Markov models, or more gener-
ally, the inside-outside algorithm for PCFGs. In this
paper, however, the task of estimation is different
than the traditional task. As mentioned in ?2, we are
interested in estimating ? from Eq. 4, while fixing
?(`). Therefore, we are only interested in computing
expectations of the form of Eq. 7.
As explained in ?3.2, any model interpolating
with the ? parameters can be reduced to a new log-
linear model with additional features representing
the mixture coefficients of ?. We can then use the
inside-outside algorithm to obtain the necessary fea-
ture expectations for features of the form g`,k(x,y),
expectations which assist in the estimation of the ?
parameters.
These feature expectations can readily be used
in estimation algorithms such as expectation-
maximization (EM). With EM, the update at itera-
tion t would be:
?(t)`,k =
E[g`,k(x,y)]?
` E[g`,k(x,y)]
, (8)
where the expectations are taken with respect to
?(t?1) and the fixed ?(l) for ` = 1, . . . , L.
Estimating Feature-Rich Directed Models Re-
cently Berg-Kirkpatrick et al (2010) found that
replacing traditional multinomial parameterizations
with locally normalized, feature-based log-linear
models was advantageous. This can be understood
as parameterizing ?:
?k,i =
exp?>h(k, i)?
i?
exp?>h(k, i?)
(9)
where h(k, i) are a set of features looking at event i
in context k. For such a feature-rich model, our mul-
tilingual modeling framework still substitutes ? with
a mixture of supervised multinomials for L helper
languages as in Eq. 4. However, for computational
53
convenience, we also reparametrize the mixture co-
efficients ?:
?`,k =
exp ?`,k?L
`?=1 exp ?`?,k
(10)
Here, each ?`,k is an unconstrained parameter, and
the above ?softmax? transformation ensures that ?
lies within the probability simplex for context k.
This is done so that a gradient-based optimization
method like L-BFGS (Liu and Nocedal, 1989) can
be used to estimate ? without having to worry about
additional simplex constraints. For optimization,
derivatives of the data log-likelihood with respect to
? need to be computed. We calculate the derivatives
following Berg-Kirkpatrick et al (2010, ?3.1), mak-
ing use of feature expectations, calculated exactly as
before.
In addition to these estimation techniques, which
are based on the optimization of the log-likelihood,
we also consider a trivially simple technique for es-
timating ?: setting ?l,k to the uniform weight L?1,
where L is the number of helper languages.
5 Coarse-to-Fine Multinomial Expansion
To expand these multinomials involving coarse-
grained categories into multinomials over fine-
grained categories specific to the target language t,
we do the following:
? Whenever a multinomial conditions on a coarse
category c ? C, we make copies of it for each fine-
grained category in ??1t (c) ? Ft.3 If the multino-
mial does not condition on coarse categories, it is
simply copied.
? Whenever a probability ?i within a multinomial
distribution involves a coarse-grained category c
as an event (i.e., it is on the left side of the condi-
tional bar), we expand the event into |??1t (c)| new
events, one per corresponding fine-grained cate-
gory, each assigned the value ?i|??1t (c)| .
4
3We note that in the models we experiment with, we always
condition on at most one fine-grained category.
4During this expansion process for a coarse event, we tried
adding random noise to ?i|??1t (c)| and renormalizing, to breaksymmetry between the fine events, but that was found to be
harmful in preliminary experiments.
The result of this expansion is a model in the
desired family; we use it to initialize conventional
unsupervised parameter estimation. Lexical param-
eters, if any, do not undergo this expansion pro-
cess, and they are estimated anew in the fine grained
model during unsupervised learning, and are initial-
ized using standard methods.
6 Experiments and Results
In this section, we describe the experiments under-
taken and the results achieved. We first note the
characteristics of the datasets and the universal POS
tags used in multilingual modeling.
6.1 Data
For our experiments, we fixed a set of four helper
languages with relatively large amounts of data,
displaying nontrivial linguistic diversity: Czech
(Slavic), English (West-Germanic), German (West-
Germanic), and Italian (Romance). The datasets are
the CoNLL-X shared task data for Czech and Ger-
man (Buchholz and Marsi, 2006),5 the Penn Tree-
bank for English (Marcus et al, 1993), and the
CoNLL 2007 shared task data for Italian (Monte-
magni et al, 2003). This was the only set of helper
languages we tested; improvements are likely pos-
sible. We leave an exploration of helper language
choice (a subset selection problem) to future re-
search, instead demonstrating that the concept has
merit.
We considered ten target languages: Bulgarian
(Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese
(Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es),
Swedish (Sv), and Turkish (Tr). The data come
from the CoNLL-X and CoNLL 2007 shared tasks
(Buchholz and Marsi, 2006; Nivre et al, 2007). For
all the experiments conducted, we trained models
on the training section of a language?s treebank and
tested on the test set. Table 1 shows the number of
sentences in the treebanks and the size of fine POS
tagsets for each language.
Following standard practice, in unsupervised
grammar induction experiments we remove punctu-
ation and then eliminate sentences from the data of
length greater than 10.
5These are based on the Prague Dependency Treebank
(Hajic?, 1998) and the Tiger treebank (Brants et al, 2002) re-
spectively.
54
Pt Tr Bg Jp El Sv Es Sl Nl Da
Training sentences 9,071 4,997 12,823 17,044 2,705 11,042 3,306 1,534 13,349 5,190
Test sentences 288 623 398 709 197 389 206 402 386 322
Size of POS tagset 22 31 54 80 38 41 47 29 12 25
Table 1: The first two rows show the sizes of the training and test datasets for each language. The third row shows the
number of fine POS tags in each language including punctuations.
6.2 Universal POS Tags
Our coarse-grained, universal POS tag set consists
of the following 12 tags: NOUN, VERB, ADJ
(adjective), ADV (adverb), PRON (pronoun), DET
(determiner), ADP (preposition or postposition),
NUM (numeral), CONJ (conjunction), PRT (parti-
cle), PUNC (punctuation mark) and X (a catch-all
for other categories such as abbreviations or foreign
words). These follow recent work by Das and Petrov
(2011) on unsupervised POS tagging in a multilin-
gual setting with parallel data, and have been de-
scribed in detail by Petrov et al (2011).
While there might be some controversy about
what an appropriate universal tag set should include,
these 12 categories (or a subset) cover the most fre-
quent parts of speech and exist in one form or an-
other in all of the languages that we studied. For
each language in our data, a mapping from the
fine-grained treebank POS tags to these universal
POS tags was constructed manually by Petrov et al
(2011).
6.3 Part-of-Speech Tagging
Our first experimental task is POS tagging, and here
we describe the specific details of the model, train-
ing and inference and the results attained.
6.3.1 Model
The model is a hidden Markov model (HMM),
which has been popular for unsupervised tagging
tasks (Merialdo, 1994; Elworthy, 1994; Smith and
Eisner, 2005; Berg-Kirkpatrick et al, 2010).6 We
use a bigram model and a locally normalized log-
linear parameterization, like Berg-Kirkpatrick et al
(2010). These locally normalized log-linear mod-
els can look at various aspects of the observation x
given a tag y, or the pair of tags in a transition, in-
corporating overlapping features. In basic monolin-
6HMMs can be understood as a special case of PCFGs.
gual experiments, we used the same set of features
as Berg-Kirkpatrick et al (2010). For the transi-
tion log-linear model, Berg-Kirkpatrick et al (2010)
used only a single indicator feature of a tag pair, es-
sentially equating to a traditional multinomial dis-
tribution. For the emission log-linear model, sev-
eral features were used: an indicator feature con-
joining the state y and the word x, a feature checking
whether x contains a digit conjoined with the state y,
another feature indicating whether x contains a hy-
phen conjoined with y, whether the first letter of x is
upper case along with the state y, and finally indica-
tor features corresponding to suffixes up to length 3
present in x conjoined with the state y.
Since only the unlexicalized transition distribu-
tions are common across multiple languages, assum-
ing that they all use a set of universal POS tags, akin
to Eq. 4, we can have a multilingual version of the
transition distributions, by incorporating supervised
helper transition probabilities. Thus, we can write:
?y?y? =
L?
`=1
?`,y?(`)y?y? (11)
We use the above expression to replace the transi-
tion distributions, obtaining a multilingual mixture
version of the model. Here, the transition probabili-
ties ?(`)y?y? for the `th helper language are fixed afterbeing estimated using maximum likelihood estima-
tion on the helper language?s treebank.
6.3.2 Training and Inference
We trained both the basic feature-based HMM
model as well as the multilingual mixture model by
optimizing the following objective function:7
L(?) =
N?
i=1
log
?
y
p(x(i),y | ?)? C???22
7Note that in the objective function, for brevity, we abuse
notation by using ? for both models ? monolingual and multi-
lingual; the latter model is also parameterized by ?.
55
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform+DG 45.7 43.6 38.0 60.4 36.7 37.7 31.8 35.9 43.7 36.2 41.0
Mixture+DG 51.5 38.6 35.8 61.7 38.9 39.9 40.5 36.0 50.2 39.9 43.3
DG (B-K et al, 2010) 53.5 27.9 34.7 52.3 35.3 34.4 40.0 33.4 45.4 48.8 40.6
(a)
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform+DG 83.8 50.4 81.3 77.9 80.3 69.0 82.3 82.8 79.3 82.0 76.9
Mixture+DG 84.7 50.0 82.6 79.9 80.3 67.0 83.3 82.8 80.0 82.0 77.3
DG (B-K et al, 2010) 75.4 50.4 80.7 83.4 88.0 61.5 82.3 75.6 79.2 82.3 75.9
(b)
Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con-
structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient
method of Berg-Kirkpatrick et al (2010) using a monolingual feature-based HMM. ?Mixture+DG? is the model where
multilingual mixture coefficients ? of helper languages are estimated using coarse tags (?4), followed by expansion
(?5), and then initializing DG with the expanded transition parameters. ?Uniform+DG? is the case where ? are set
to 1/4, transitions of helper languages are mixed, expanded, and then DG is initialized with the result. For (a), eval-
uation is performed using one-to-one mapping accuracy. In case of (b), the tag dictionary solves the problem of tag
identification and performance is measured using per word POS accuracy. ?Avg? denotes macro-average across the
ten languages.
Note that this involves marginalizing out all possible
state configurations y for a sentence x, resulting in
a non-convex objective. As described in ?4, we opti-
mized this function using L-BFGS. For the mono-
lingual model, derivatives of the feature weights
took the exact same form as Berg-Kirkpatrick et al
(2010), while for the mixture case, we computed
gradients with respect to ?, the unconstrained pa-
rameters used to express the mixture coefficients ?
(see Eq. 10). The regularization constant C was set
to 1.0 for all experiments, and L-BFGS was run till
convergence.
During training, for the basic monolingual
feature-based HMM model, we initialized all param-
eters using small random real values, sampled from
N (0, 0.01). For estimation of the mixture parame-
ters ? for our multilingual model (step 3 in ?2), we
similarly sampled real values fromN (0, 0.01) as an
initialization point. Moreover, during this stage, the
emission parameters also go through parameter es-
timation, but they are monolingual, and are initial-
ized with real values sampled from N (0, 0.01); as
explained in ?2, coarse universal tags are used both
in the transitions and emissions during multilingual
estimation.
After the mixture parameters ? are estimated, we
compute the mixture probabilities ? using Eq. 10.
Next, for each tag pair y, y?, we compute ?y?y? ,
which are the coarse transition probabilities inter-
polated using ?, given the helper languages. We
then expand these transition probabilities (see ?5) to
result in transition probabilities based on fine tags.
Finally, we train a feature-HMM by initializing its
transition parameters with natural logarithms of the
expanded ? parameters, and the emission parame-
ters using small random real values sampled from
N (0, 0.01). This implies that the lexicalized emis-
sion parameters ? that were previously estimated in
the coarse multilingual model are thrown away and
not used for initialization; instead standard initial-
ization is used.
For inference at the testing stage, we use min-
imum Bayes-risk decoding (or ?posterior decod-
ing?), by choosing the most probable tag for each
word position, given the entire observation x. We
chose this strategy because it usually performs
slightly better than Viterbi decoding (Cohen and
Smith, 2009; Ganchev et al, 2010).
6.3.3 Experimental Setup
For experiments, we considered three configura-
tions, and for each, we implemented two variants of
POS induction, one without any kind of supervision,
and the other with a tag dictionary. Our baseline is
56
the direct gradient approach of Berg-Kirkpatrick et
al. (2010), which is the current state of the art for this
task, outperforming classical HMMs. Because this
model achieves strong performance using straight-
forward MLE, it also serves as the core model within
our approach. This model has also been applied in
a multilingual setting with parallel data (Das and
Petrov, 2011). In this baseline, we set the number
of HMM states to the number of fine-grained tree-
bank tags for the given language.
We test two versions of our model. The first ini-
tializes training of the target language?s POS model
using a uniform mixture of the helper language mod-
els (i.e., each ?`,y = 1L = 14 ), and expansion fromcoarse-grained to fine-grained POS tags as described
in ?5. We call this model ?Uniform+DG.?
The second version estimates the mixture coeffi-
cients to maximize likelihood, then expands the POS
tags (?5), using the result to initialize training of the
final model. We call this model ?Mixture+DG.?
No Tag Dictionary For each of the above configura-
tions, we ran purely unsupervised training without a
tag dictionary, and evaluated using one-to-one map-
ping accuracy constraining at most one HMM state
to map to a unique treebank tag in the test data, us-
ing maximum bipartite matching. This is a variant of
the greedy one-to-one mapping scheme of Haghighi
and Klein (2006).8
With a Tag Dictionary We also ran a second ver-
sion of each experimental configuration, where we
used a tag dictionary to restrict the possible path se-
quences of the HMM during both learning and infer-
ence. This tag dictionary was constructed only from
the training section of a given language?s treebank.
It is widely known that such knowledge improves
the quality of the model, though it is an open debate
whether such knowledge is realistic to assume. For
this experiment we removed punctuation from the
training and test data, enabling direct use within the
dependency grammar induction experiments.
8We also evaluated our approach using the greedy version of
this evaluation metric, and results followed the same trends with
only minor differences. We did not choose the other variant,
many-to-one mapping accuracy, because quite often the metric
mapped several HMM states to one treebank tag, leaving many
treebank tags unaccounted for.
6.3.4 Results
All results for POS induction are shown in Ta-
ble 2. Without a tag dictionary, in eight out of ten
cases, either Uniform+DG or Mixture+DG outper-
forms the monolingual baseline (Table 2a). For six
of these eight languages, the latter model where the
mixture coefficients are learned automatically fares
better than uniform weighting. With a tag dictionary,
the multilingual variants outperform the baseline in
seven out of ten cases, and the learned mixture out-
performs or matches the uniform mixture in five of
those seven (Table 2b).
6.4 Dependency Grammar Induction
We next describe experiments for dependency gram-
mar induction. As the basic grammatical model,
we adopt the dependency model with valence (Klein
and Manning, 2004), which forms the basis for state-
of-the-art results for dependency grammar induc-
tion in various settings (Cohen and Smith, 2009;
Spitkovsky et al, 2010; Gillenwater et al, 2010;
Berg-Kirkpatrick and Klein, 2010). As shown in Ta-
ble 3, DMV obtains much higher accuracy in the su-
pervised setting than the unsupervised setting, sug-
gesting that more can be achieved with this model
family.9 For this reason, and because DMV is eas-
ily interpreted as a PCFG, it is our starting point and
baseline.
We consider four conditions. The independent
variables are (1) whether we use uniform? (all set to
1
4 ) or estimate them using EM (as described in ?4),and (2) whether we simply use the mixture model to
decode the test data, or to initialize EM for the DMV.
The four settings are denoted ?Uniform,? ?Mixture,?
?Uniform+EM,? and ?Mixture+EM.?
The results are given in Table 3. In general, the
use of data from other languages improves perfor-
mance considerably; all of our methods outperform
the Klein and Manning (2004) initializer, and we
achieve state-of-the-art performance for eight out of
ten languages. Uniform and Mixture behave simi-
larly, with a slight advantage to the trained mixture
setting. Using EM to train the mixture coefficients
more often hurts than helps (six languages out of
ten). It is well known that likelihood does not cor-
9Its supervised performance is still far from the supervised
state of the art in dependency parsing.
57
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform 78.6 45.0 75.6 56.3 57.0 74.0 73.2 46.1 50.7 59.2 61.6
Mixture 76.8 45.3 75.5 58.3 59.5 73.2 75.9 46.0 51.1 59.9 62.2
Uniform+EM 78.7 43.9 74.7 59.8 73.0 70.5 75.5 41.3 45.9 51.3 61.5
Mixture+EM 79.8 44.1 72.8 63.9 72.3 68.7 76.7 41.0 46.0 55.2 62.1
EM (K & M, 2004) 42.5 36.3 54.3 43.0 41.0 42.3 38.1 37.0 38.6 41.4 41.4
PR (G et al, ?10) 47.8 53.4 54.0 60.2 - 42.2 62.4 50.3 37.9 44.0 -
Phylo. (B-K & K, ?10) 63.1 - - - - 58.3 63.8 49.6 45.1 41.6 -
Supervised (MLE) 81.7 75.7 83.0 89.2 81.8 83.2 79.0 74.5 64.8 80.8 79.3
Table 3: Results for dependency grammar induction given gold-standard POS tags, reported as attachment accuracy
(fraction of parents which are correct). The three existing methods are: our replication of EM with the initializer from
Klein and Manning (2004), denoted ?EM?; reported results from Gillenwater et al (2010) for posterior regularization
(?PR?); and reported results from Berg-Kirkpatrick and Klein (2010), denoted ?Phylo.? ?Supervised (MLE)? are oracle
results of estimating parameters from gold-standard annotated data using maximum likelihood estimation. ?Avg?
denotes macro-average across the ten languages.
Figure 2: Projection of the learned mixture coefficients
through PCA. In green, Japanese. In red, Dutch, Danish
and Swedish. In blue, Bulgarian and Slovene. In ma-
genta, Portuguese and Spanish. In black, Greek. In cyan,
Turkish.
relate with the true accuracy measurement, and so
it is unsurprising that this holds in the constrained
mixture family as well. In future work, a different
parametrization of the mixture coefficients, through
features, or perhaps a Bayesian prior on the weights,
might lead to an objective that better simulates ac-
curacy.
Table 3 shows that even uniform mixture coef-
ficients are sufficient to obtain accuracy which su-
percedes most unsupervised baselines. We were in-
terested in testing whether the coefficients which are
learned actually reflect similarities between the lan-
guages. To do that, we projected the learned vectors
? for each tested language using principal compo-
nent analysis and plotted the result in Figure 2. It
is interesting to note that languages which are closer
phylogenetically tend to appear closer to each other
in the plot.
Our experiments also show that multilingual
learning performs better for dependency grammar
induction than part-of-speech tagging. We believe
that this happens because of the nature of the mod-
els and data we use. The transition matrix in part-
of-speech tagging largely depends on word order in
the various helper languages, which differs greatly.
This means that a mixture of transition matrices will
not necessarily yield a meaningful transition matrix.
However, for dependency grammar, there are certain
universal dependencies which appear in all helper
languages, and therefore, a mixture between multi-
nomials for these dependencies still yields a useful
multinomial.
6.5 Inducing Dependencies from Words
Finally, we combine the models for POS tagging and
grammar induction to perform grammar induction
directly from words, instead of gold-standard POS
tags. Our approach is as follows:
1. With a tag dictionary, learn a fine-grained POS
tagging model unsupervised, using either DG or
Mixture+DG as described in ?6.3 and shown in
Table 2b.
58
Method Tags Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Joint DG 68.4 52.4 62.4 61.4 63.5 58.2 67.7 47.2 48.3 50.4 57.9
Joint Mixture+DG 62.2 47.4 67.0 69.5 52.2 49.1 69.3 36.8 52.2 50.1 55.6
Pipeline DG 60.0 50.8 57.7 64.2 68.2 57.9 65.8 45.8 49.9 48.9 56.9
Pipeline Mixture+DG 59.8 47.1 62.9 68.6 50.0 47.6 68.1 36.4 51.2 48.3 54.0
Gold-standard tags 79.8 45.3 75.6 63.9 73.0 74.0 76.7 46.1 50.7 59.9 64.5
Table 4: Results for dependency grammar induction over words. ?Joint?/?Pipeline? refers to joint/pipeline decoding
of tags and dependencies as described in the text. See ?6.3 for a description of DG and Mixture+DG. For the induction
of dependencies we use the Mixture+EM setting as described in ?6.4. All tag induction uses a dictionary as specified
in ?6.3. The last row in this table indicates the best results using multilingual guidance taken from our methods in
Table 3. ?Avg? denotes macro-average across the ten languages.
2. Apply the fine-grained tagger to the words in the
training data for the dependency parser. We con-
sider two variants: the most probable assignment
of tags to words (denoted ?Pipeline?), and the pos-
terior distribution over tags for each word, repre-
sented as a weighted ?sausage? lattice (denoted
?Joint?). This idea was explored for joint infer-
ence by Cohen and Smith (2007).
3. We apply the Mixture+EM unsupervised parser
learning method from ?6.4 to the automatically
tagged sentences, or the lattices.
4. Given the two models, we infer POS tags on the
test data using DG or Mixture+DG to get a lattice
(Joint) or a sequence (Pipeline) and then parse us-
ing the model from the previous step.10 The re-
sulting dependency trees are evaluated against the
gold standard.
Results are reported in Table 4. In almost all cases,
joint decoding of tags and trees performs better than
the pipeline. Even though our part-of-speech tagger
with multilingual guidance outperforms the com-
pletely unsupervised baseline, there is not always an
advantage of using this multilingually guided part-
of-speech tagger for dependency grammar induc-
tion. For Turkish, Japanese, Slovene and Dutch, our
unsupervised learner from words outperforms unsu-
pervised parsing using gold-standard part-of-speech
tags.
We note that some recent work gives a treatment
to unsupervised parsing (but not of dependencies)
10The decoding method on test data (Joint or Pipeline) was
matched to the training method, though they are orthogonal in
principle.
directly from words (Seginer, 2007). Earlier work
that induced part-of-speech tags and then performed
unsupervised parsing in a pipeline includes Klein
and Manning (2004) and Smith (2006). Headden
et al (2009) described the use of a lexicalized vari-
ant of the DMV model, with the use of gold part-of-
speech tags.
7 Conclusion
We presented an approach to exploiting annotated
data in helper languages to infer part-of-speech tag-
ging and dependency parsing models in a different,
target language, without parallel data. Our approach
performs well in many cases. We also described a
way to do joint decoding of part-of-speech tags and
dependencies which performs better than a pipeline.
Future work might consider exploiting a larger num-
ber of treebanks, and more powerful techniques for
combining models than simple local mixtures.
Acknowledgments
We thank Ryan McDonald and Slav Petrov for help-
ful comments on an early draft of the paper. This re-
search has been funded by NSF grants IIS-0844507 and
IIS-0915187 and by U.S. Army Research Office grant
W911NF-10-1-0533.
References
T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic
grammar induction. In Proceedings of ACL.
T. Berg-Kirkpatrick, A. B. Co?te?, J. DeNero, and D. Klein.
2010. Painless unsupervised learning with features. In
Proceedings of NAACL-HLT.
59
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proceedings of the
Workshop on Treebanks and Linguistic Theories.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proceedings
of CoNLL.
D. Burkett and D. Klein. 2008. Two languages are bet-
ter than one (for syntactic parsing). In Proceedings of
EMNLP.
S. B. Cohen and N. A. Smith. 2007. Joint morpholog-
ical and syntactic disambiguation. In Proceedings of
EMNLP-CoNLL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proceedings of HLT-
NAACL.
S. B. Cohen and N. A. Smith. 2010. Covariance in unsu-
pervised learning of probabilistic grammars. Journal
of Machine Learning Research, 11:3017?3051.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proceedings of ACL-HLT.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceedings of ACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search, 11:2001?2049.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Sparsity in dependency grammar
induction. In Proceedings of ACL.
A. Haghighi and D. Klein. 2006. Prototype driven learn-
ing for sequence models. In Proceedings of HLT-
NAACL.
J. Hajic?. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning. Studies in Honor of
Jarmila Panevova?. Prague Karolinum, Charles Univer-
sity Press.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proceedings of
NAACL-HLT.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proceedings of NAACL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proceedings of ACL.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45:503?528.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the Penn treebank. Computational Linguistics, 19.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source
transfer of delexicalized dependency parsers. In Pro-
ceedings of EMNLP.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Compulational Lingustics, 20(2):155?
72.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Zampolli, F. Fanciulli, M. Massetani,
R. Raffaelli, R. Basili, M. T. Pazienza, D. Saracino,
F. Zanzotto, N. Mana, F. Pianesi, and R. Delmonte.
2003. Building the Italian Syntactic-Semantic Tree-
bank. In Building and using Parsed Corpora, Lan-
guage and Speech Series. Kluwer, Dordrecht.
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. JAIR, 36.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010.
Using universal linguistic knowledge to guide gram-
mar induction. In Proceedings of EMNLP.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of CoNLL.
S. Petrov, D. Das, and R. McDonald. 2011. A universal
part-of-speech tagset. ArXiv:1104.2086.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In Proceedings of ACL.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of ACL.
D. A. Smith and J. Eisner. 2009. Parser adaptation and
projection with quasi-synchronous grammar features.
In Proceedings of EMNLP.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing
with factored estimation: Using English to parse Ko-
rean. In Proceedings of EMNLP.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
Proceedings of ACL.
B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsuper-
vised multilingual grammar induction. In Proceedings
of ACL-IJCNLP.
V. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010. From
baby steps to leapfrog: How ?less is more? in unsuper-
vised dependency parsing. In Proceedings of NAACL.
C. Xi and R. Hwa. 2005. A backoff model for bootstrap-
ping resources for non-English languages. In Proceed-
ings of HLT-EMNLP.
60
D. Yarowsky and G. Ngai. 2001. Inducing multilingual
POS taggers and NP bracketers via robust projection
across aligned corpora. In Proceedings of NAACL.
61
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1996?2006,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Cross-Lingual Discriminative Learning of Sequence Models
with Posterior Regularization
Kuzman Ganchev
Google Research
76 9th Avenue
New York, NY 10011
kuzman@google.com
Dipanjan Das
Google Research
76 9th Avenue
New York, NY 10011
dipanjand@google.com
Abstract
We present a framework for cross-lingual
transfer of sequence information from a
resource-rich source language to a resource-
impoverished target language that incorporates
soft constraints via posterior regularization. To
this end, we use automatically word aligned
bitext between the source and target language
pair, and learn a discriminative conditional ran-
dom field model on the target side. Our poste-
rior regularization constraints are derived from
simple intuitions about the task at hand and
from cross-lingual alignment information. We
show improvements over strong baselines for
two tasks: part-of-speech tagging and named-
entity segmentation.
1 Introduction
Supervised systems for NLP tasks are available for
a handful of languages. These systems achieve high
accuracy for many applications; a variety of robust
algorithms to train them from labeled data have been
developed. Here, we focus on learning sequence mod-
els for the languages that lack annotated resources.
For a given resource-poor target language of inter-
est, we assume that parallel data with a resource-rich
source language exists. With the help of this bitext
and a supervised system in the source language, we
infer constraints over the label distribution in the tar-
get language, and train a discriminative model using
posterior regularization (Ganchev et al, 2010).
Cross-lingual learning of structured prediction
models via parallel data has been applied for several
natural language processing problems, including part-
of-speech (POS) tagging (Yarowsky and Ngai, 2001),
syntactic parsing (Hwa et al, 2005) and named-entity
recognition (Kim et al, 2012). These methods are
useful in several ways. First, they help in fast proto-
typing of natural language systems for new languages
that do not boast human annotations. Second, the
output of such systems could be used to bootstrap
more extensive human annotation projects (Vlachos,
2006). Finally, they are significantly more accurate
than purely unsupervised systems (McDonald et al,
2011; Das and Petrov, 2011).
Recently, Ta?ckstro?m et al (2013) presented a tech-
nique for coupling token constraints derived from pro-
jected cross-lingual information and type constraints
derived from noisy tag dictionaries to learn POS tag-
gers. Although this technique resulted in state-of-
the-art weakly supervised taggers, the authors used a
heuristic to combine the aforementioned two sources
of constraints: the dictionary constraints pruned the
tagger?s search space, and the intersected token-level
projections were treated as hard observations. On
the other hand, Ganchev et al (2009) presented a
framework for learning weakly-supervised systems
(in their case, dependency parsers) that incorporated
alignment-based information too, but used the cross-
lingual information only as soft constraints, via poste-
rior regularization. The advantage of this framework
lay in the fact that the projections were only trusted
to a certain degree, determined by a strength hyper-
parameter, which unfortunately the authors did not
have an elegant way to tune. In this paper, we ex-
ploit the better aspects of these two lines of work:
first, we extend the framework of Ta?ckstro?m et al
by treating the alignment-based projections only as
soft constraints (see ?3.4); second, we choose the
constraint strength by utilizing the tag ambiguity of
tokens for a given resource-poor language (see ?6.1).
Other than validating our framework on part-of-
speech tagging, we experiment on named-entity seg-
mentation in a cross-lingual framework. For this
1996
task, we present a novel method to perform high-
precision phrase-level entity transfer (?5.2.2); we
also provide ways to balance precision and recall
with posterior regularization (?6.2) by incorporating
intuitive soft constraints during learning. We mea-
sure performance on standard benchmark datasets for
both of these tasks, and report improvements over
state-of-the-art baselines.
2 Prior Work
Cross-lingual projection methods can be classified
by their use of two very broad ideas. The first idea
utilizes parallel data to create full or partial annota-
tions in the low-resource language and trains from
this data. This was popularized by Yarowsky and
Ngai (2001) who applied this to POS tagging and
shallow parsing. It was later applied to parsing (Hwa
et al, 2005) and named entity recognition (Kim et
al., 2012). The second idea, first proposed by Ze-
man and Resnik (2008) and applied more broadly
by McDonald et al (2011), is to train a model on
a resource-rich language and apply it to a resource-
poor language directly. The disparity between the
languages is mitigated by the choice of features. In
addition to cross-lingual projection, purely unsuper-
vised methods have been explored but with limited
success (Christodoulopoulos et al, 2010). Here, we
resort to cross-lingual projection and incorporate the
first idea; we also follow Li et al (2012) and use
Wiktionary to further constrain the POS tagging task.
Our learning setup is similar to that of Ganchev et
al. (2009), who also use posterior regularization but
focus on dependency parsing alone. Our work differs
with respect to the tasks, the learning algorithm and
also in that we use corpus-wide constraints, while
Ganchev et al use one constraint per sentence. For
the part-of-speech tagging task, our approach is sim-
ilar to that of Ta?ckstro?m et al (2013), who use an
almost identical learning setup but only make use of
hard constraints. By relaxing these constraints, we
allow the model to identify and ignore inconsistently
labeled parts of sentences, and achieve better results
using identical training and test data.
3 Approach
We give an overview of our approach, and present the
details of our model used for cross-lingual learning.
Algorithm 1 Cross-Lingual Learning with Posterior
Regularization
Require: Parallel source and target language data
De and Df , source language model (M)e, task-
specific target language constraints C.
Ensure: ?f , a set of target language parameters.
1: De?f ? word-align-bitext(De,Df )
2: D?e ? label-supervised(De)
3: D?f ? project-and-filter-labels(De?f , D?e)
4: ?f ? learn-posterior-constrained(D?f , C)
5: Return ?f
3.1 General Overview
The general overview of our framework is provided
in Algorithm 1. The process of learning parame-
ters for a target language for a given task involves
four subtasks. First, we run word alignment over a
large corpus of parallel data between the resource-
rich source language and the resource-impoverished
target language (see ?4.3). In the second step, we
use a supervised model to label the source side of
the parallel data (see ?5.1.1 and ?5.2.1). The third
step involves a task-specific word-alignment filter-
ing step; this step involves heuristics for which we
use cues from prior state-of-the-art (Das and Petrov,
2011; Ta?ckstro?m et al, 2013, see ?5.1.2) and also
introduce some novel ones for the NE segmentation
problem (see ?5.2.2). In the fourth step, we train a
linear chain conditional random field (Lafferty et al,
2001, CRF henceforth) using posterior regularization.
In the next subsection, we turn to a brief summary of
this final step of estimating parameters of a discrimi-
native model with posterior regularization.
3.2 Learning with Posterior Regularization
In this work, we utilize discriminative CRF mod-
els, and use posterior regularization (PR) to optimize
their parameters. As a framework, posterior regular-
ization is described in detail by Ganchev et al (2010).
However in our work, we adopt a different optimiza-
tion technique; in what follows, we summarize the
optimization algorithm in the context of CRF models.
Let x be an input sentence with a set of possible
labelings Y(x) and let y ? Y(x) be a particular la-
beling for sentence x. We use bold capital letters
X = {x1 . . .xn} and Y = {y1 . . .yn} to denote
1997
a corpus of sentences and labelings for the corpus
respectively. A CRF models the probability distri-
bution over possible labels for a sentence p?(y|x)
as:
p?(y | x) ? exp(? ? f(x,y)) (1)
where ? are the model parameters and f(.) is a fea-
ture function. The model examines sentences in iso-
lation, and the probability of a particular labeling for
a corpus is defined as a product over the individual
sentences:
p?(Y | X) =
?
(x,y)?(X,Y)
p?(y | x). (2)
Traditionally, CRF models have been trained to op-
timize the regularized log-likelihood of the training
data
max
?
L(?) = max
?
log(p?(Y | X))? ? ||?|| (3)
In our setting, we do not have a fully labeled cor-
pus, but we have constraints on the distribution of
labels. For example, we may know that a particular
token could be labeled only by a label inventory li-
censed by a dictionary, or that a labeling projected
from a source language is usually (but not always)
correct. We define these constraints in terms of fea-
ture expectations. Let q(Y) be a distribution over all
possible labelings of our corpus Y(X). Let Q be a
set of distributions defined by:
Q = {q(Y) : Eq[?(X,Y)] ? b}, (4)
where ? is a constraint feature function and b is a vec-
tor of non-negative values that serve as upper bounds
to the expectations of every constraint feature. The
vector b is used to encode our prior knowledge about
desirable distributions q(Y). Note that the constraint
features ? are not related to the model features f . The
model features, together with the model parameters ?
define the CRF model; the model features need to be
computed at inference time for prediction. By con-
trast, the constraint features and their corresponding
constraint values are used to define our training ob-
jective function (and are only used during learning).
The PR objective with no labeled data is defined with
respect to Q as:
PR: max
?
JQ(?) =
max
?
?KL(Q?p?(Y | X))? ? ||?|| (5)
where KL(Q||p) = minq?QKL(q||p) is the
KL-divergence (Kullback and Leibler, 1951)
from a set to a point. Note that as we add more
constraints, Q becomes a smaller set. In the
limit, Q = {q(Y) : q(Y?) = 1} contains just one
distribution concentrated on a single labeling Y?.
In this limit, posterior regularization degenerates
into the convex log-likelihood objective normally
used for supervised data JQ(?) = L(?). However,
in the general case, the PR objective JQ is not
necessarily convex. Prior work, including that of
Ganchev et al propose an algorithm similar to
Expectation-Maximization (Dempster et al, 1977,
EM henceforth) to optimize JQ, but we follow Liang
et al (2009) in using a schochastic update-based
algorithm described below.
Note: To make it easier to reason about constraint
values b, we scale constraint features ?(X,Y) to lie
in [0, 1] by computing maxY ?(X,Y) for the corpus
to which ? is applied.
3.3 Optimization
The optimization procedure proposed by
Ganchev et al is similar to the EM algorithm,
and computes the minimization minq?QKL(q||p)
at each step, using its dual form; this minimization is
convex, so there is no duality gap. They show that
the optimal primal variables q?(Y) are related to the
optimal dual variables ?? by:
q?(Y) =
p?(Y|X)e??
???(X,Y)
Z(??)
. (6)
where Z(??) is the normalizer. The dual problem is
given by:
max
??0
?b ? ?? logZ(?). (7)
Substituting Eq. 7 into the objective in Eq. 5, we get
the saddle-point problem:
max
?
min
??0
b ? ?+ log
?
Y
p?(Y|X)e
?????(X,Y)
? ? ||?|| . (8)
To optimize the above objective function, we need to
compute partial derivatives with respect to both ? and
?. First, to compute the partial derivatives of Eq. 8
1998
with respect to ?, we need to find expectations of the
model features f given the current distribution p? and
the constraint distribution q. To perform tractable
inference, a linear-chain CRF model assumes that
the feature function factorizes according to smaller
parts; in particular the factorization uses the follow-
ing structure:
f(x,y) =
?
i
f(x, yi, yi?1) (9)
where i ranges over the tokens in the sentence. This
factorization allows us to efficiently compute expec-
tations over the labels yi and label-pairs (yi, yi+1).
To compute the partial gradient of Eq. 8 with respect
to ?, we need to find the expectations of the con-
straint features ?. In order to be tractable here too,
we ensure that ? also factorize according to the same
structure as f . Therefore, the gradient computation
w.r.t. ? turns out to be straightforward.
For all the experiments in this paper, we optimize
Eq. 8 using stochastic projected gradient. For each
training sentence, we compute the gradient of ? and
? with respect to Eq. 8, take a gradient step in each
one, and truncate the negative entries in ? to zero.
We use a step size of 1 for all experiments.1
3.4 Relationship with Ta?ckstro?m et al (2013)
In this subsection, we focus briefly on the relationship
between this work and the work of Ta?ckstro?m et al
(2013), who focused on constrained learning of POS
taggers. Ta?ckstro?m et al define constrained lattices
and train by optimizing marginal conditional log-
likelihood. In our notation, they define their objective
as:
max
?
log
?
Y?Y?(X)
p?(Y|X)? ???? (10)
where Y?(X) are the constrained lattices of label se-
quences that agree with both a dictionary and cross-
lingually projected POS tags for each sentence of
the training corpus. Let us define a constraint fea-
ture ?(X,Y) which counts the number of tags in Y
which are outside the constraint set Y?(X) and require
?(X,Y) ? 0. Note that,
arg min
q
KL(q||p?(Y|X)) s. t. ?(X,Y) ? 0
1Note that we did not implement regularization of ? in the
stochastic optimizer, hence our PR objective (Eq. 8) was unregu-
larized; however, the baseline models use `2 regularization.
gives the same distribution as Eq. 10. Given this
equivalence, it is easy to see that the gradient of
Eq. 5 with respect to ? is the same as that of Eq. 10.
By using such constrained lattices, Ta?ckstro?m et al
avoid maintaining a parameter for the constraint, but
lose the ability to relax the constraint value and al-
low some probability mass outside the pruned lat-
tice. Their paper also differs from ours in that they
use L-BFGS (Liu and Nocedal, 1989), while we use
an online optimization procedure. Since the objec-
tives are non-convex, the two optimization techniques
could lead to different local optima even when the
constraint is not relaxed (b = 0).
4 Tasks and Data
In this section, we focus on the nature of the two tasks
that we attempt to solve, describe the source language
datasets we use to train our supervised models for
transfer, the target language datasets on which we
evaluate our models and the parallel data we use for
cross-lingual transfer.
4.1 Part-of-Speech Tagging
First, we focus on the task of part-of-speech tagging.
Following previous work on cross-lingual POS tag-
ging (Das and Petrov, 2011; Ta?ckstro?m et al, 2013),
we adopt the POS tags of Petrov et al (2012), ver-
sion 1.03;2 we use the October 2012 version of Wik-
tionary3 as our tag dictionary.
After pruning the search space with the dictionary,
we place soft constraints derived by projecting POS
tags across word alignments. The alignments are fil-
tered for confidence (see ?5.1.2), but we also filter
any projected tags that are not licensed by the dictio-
nary. The example in Figure 1 illustrates why this
dictionary filtering step is important. Consider the
English-Spanish phrase pair from Figure 1, which
we observed in our training data. Our supervised tag-
ger correctly tags Asian with the ADJ tag as shown
in the figure. Asian is aligned to the Spanish word
Asia, which should be tagged NOUN. Because the
Spanish Wiktionary only allows the NOUN tag for
Asia, we do not project the ADJ tag from the English
word Asian. By contrast, we do project the NOUN
tag from the English word sponges to the Spanish
2http://code.google.com/p/universal-pos-tags
3http://meta.wikimedia.org/wiki/Wiktionary
1999
of    [ Asian ]        sponges
de   las   esponjas   de   Asia
ADP
ADJ NOUN
MISC
Figure 1: An English (top) ? Spanish (bottom) phrase pair
from our parallel data. The correct POS tags and NER
annotations are shown for the English phrase. Word align-
ments are shown as links between English and Spanish
words.
word esponjas because this tag is in our dictionary
for the latter word.
For all our POS experiments, we evaluate on sev-
enteen target languages. Fifteen of these languages
were part of the experiments conducted by Ta?ckstro?m
et al (2013); we add Arabic and Hungarian to the set.
The first column of Table 1 lists all seventeen lan-
guages using their two-letter abbreviation codes from
the ISO 639-1 standard. The evaluation datasets cor-
respond to the test sets from the CoNLL shared tasks
on dependency parsing (Buchholz and Marsi, 2006;
Nivre et al, 2007). For French we use the treebank
of Abeille? et al (2003). English serves as our source
language and we use the Penn Treebank (Marcus et
al., 1993, with tags mapped to the universal tags) to
train our supervised source-side model.
4.2 Named-Entity Segmentation
Second, we investigate the task of named-entity seg-
mentation. The goal of this task is to identify the
boundaries of named-entities for a given language
without classifying them by type. This is the un-
labeled version of named-entity recognition, and is
more amenable to cross-lingual supervision. To un-
derstand why that is, consider again the example
from Figure 1. The English supervised NE tagger
correctly identifies Asian as a named entity of type
MISC (miscellaneous). The word-alignments sug-
gest we should transfer this annotation to the Spanish
word Asia which is also an entity. However, this
should be labeled LOC (location) according to the
CoNLL annotation guidelines (Tjong Kim Sang and
De Meulder, 2003). Because syntactic variations
of this kind are common, it makes cross-lingual de-
tection of NE boundaries as well as types hard.4 In
this paper, we focus on named-entity segmentation
alone, consider the full NER task out of scope. We
use English as a source language and train a super-
vised English named-entity tagger with the labels in
place, using the CoNLL 2003 shared task data (Tjong
Kim Sang and De Meulder, 2003). We project the
spans using the maximal-span heuristic (Yarowsky
and Ngai, 2001). We project into Dutch, German and
Spanish and evaluate on the standard CoNLL 2002
and 2003 shared task data sets (Tjong Kim Sang,
2002; Tjong Kim Sang and De Meulder, 2003).
4.3 Parallel Data
For both tasks we use parallel data gathered automat-
ically from the web using the method of Uszkoreit
et al (2010), as well as data from Europarl (Koehn,
2005) and the UN parallel corpus (UN, 2006), for
languages covered by the latter two corpora. The
parallel sentences are word aligned with the aligner
of DeNero and Macherey (2011). The size of the
parallel corpus is larger than we need for our tasks,
so we follow Ta?ckstro?m et al (2013) in sampling
500k tokens for POS tagging and 10k sentences for
named-entity segmentation (see ?5.1.2 and ?5.2.2).
5 Experimental Details
In this section, we provide details about task-
specific implementations of the supervised source-
side model and the word-alignment filtering tech-
niques (steps 2 and 3 in Algorithm 1 respectively);
we also briefly describe the setup of the cross-lingual
experiments for each task.
5.1 Part-of-Speech Tagging
We first focus on the experimental setup for the POS
tagging task. When describing feature sets we refer to
features conjoined with just a single tag as emission
features and with consecutive tag pairs as transition
features.
4We tried using English and German gazetteers from the
CoNLL 2002 and 2003 shared tasks as a label dictionary similar
to the way we use Wiktionary for POS tagging. This did not work
well because the CoNLL gazetteers do not have good coverage
on our parallel datasets, which we use for training.
2000
5.1.1 Supervised Source-Side Model
We tag the English side of our parallel data with
a supervised first-order linear-chain CRF POS tag-
ger. We use standard features for tagging. Our emis-
sion features are a bias feature, the current word,
its suffixes up to length 3, its capitalization shape,
whether it contains a hyphen, digit or punctuation
and its cluster identity. Our transition features are a
bias feature and the cluster identities of each word
in the transition. For the cluster-based features, we
use monolingual word clusters induced with the ex-
change algorithm of Uszkoreit and Brants (2008),
which implements the same objective as Brown et al
(1992); these clusters have shown improvements for
sequence labeling tasks (Turian et al, 2010; Ta?ck-
stro?m et al, 2012). We set the number of clusters to
256 for both the source side tagger and all the other
languages. On Section 23 of the WSJ section of the
Penn Treebank, the source side tagger achieves an
accuracy of 96.2%.
5.1.2 Word Alignment Filtering
Following Ta?ckstro?m et al (2013), we tag the En-
glish side of our parallel data using the source-side
POS tagger, intersect the word alignments and filter
alignments with confidence below 0.95. We sam-
ple 500,000 tokens of target side sentences for each
language, and use this as training data for learning
weakly-supervised taggers.
5.1.3 Setup for Cross-Lingual Experiments
Following Ta?ckstro?m et al (2013) we use a re-
duced feature set for the cross-lingual models. The
emission features are the same as the supervised
model but without the punctuation feature,5 and we
use only the bias transition feature. Because this
limits the ability of the model to use context, we
also experiment with an extended feature set that
has transition features for the clusters of each word
in the transition, and their suffixes up to length 3.
We refer to the extended-feature models as ?BASE+?
and ?PR+? to distinguish them from the models with
fewer features, labeled ?BASE? and ?PR?.
We train BASE and BASE+ using L-BFGS with
an `2 regularization weight of 1 for 100 iterations to
reproduce the setup used by Ta?ckstro?m et al (2013).
5The dictionary licenses punctuations, only by the ?.? tag.
We have only one constraint feature in our poste-
rior regularization models that fires for the unpruned
projected tags on words xi. This feature controls
how often our model trusts a projected tag; we ex-
plain how its strength is chosen in ?6.1. The PR and
PR+ models are trained using the stochastic gradient
method described in ?3.3.
5.2 Named-Entity Segmentation
In this subsection, we turn to the experimental details
of the named-entity segmentation system.
5.2.1 Supervised Source-Side Model
To train our supervised source-side NER model,
we implemented a linear-chain first order CRF model.
Our feature set was inspired by the model of Kazama
and Torisawa (2007, ?6.1); we used all the local fea-
tures from their model except the gazetteer features,
and added cluster emission features for offsets in the
range [-2, 2] and transition features for offsets in the
range [-1, 1] as well as a sentence-start feature. We
use automatic POS tags for all the experiments.
We use a BIO encoding of the four NER labels
(PER, LOC, ORG and MISC). We also experi-
mented with omitting the NE labels from the tagger,
still with a BIO encoding for segments, but the results
were worse on average than what we report in Table 2.
We train the source-side model on the CoNLL 2003
English training set with log-loss using L-BFGS for
100 iterations with `2 regularization weight of 0.1.
The model gets 90.9% and 87.5% labeled F1 on the
CoNLL development and test sets respectively.6
5.2.2 Word-Alignment Filtering
Projecting named entities across languages can
be error prone for several reasons. Mistakes intro-
duced by the automatic word aligner is one of them.
Word alignment errors are particularly problematic
for entity mentions because of the garbage collector
effect (Brown et al, 1993); due to differences in the
word order between languages, a few alignment er-
rors can result in many errors in the other language.
Additionally, entities can occur on just one side of
the bitext.7 Another source of error is the automatic
6These performance values would place us among the top
three competitors of the CoNLL 2003 shared task.
7For example, ?It?s all Greek to me.? in one language and ?I
don?t understand it.? in another.
2001
labeling on the source side, which is inaccurate if the
parallel corpus is out of domain. To mitigate these
errors, we aggressively filter the training data for this
task. We discard sentence pairs where more than
30% of the source language tokens are unaligned,
where any source entities are unaligned or where
any source entities are more than 4 tokens long. We
also compute a confidence score over entity anno-
tations as the minimum posterior over the tags that
comprise the entity and discard sentence pairs that
have an entity with confidence below 0.9. Finally,
we discard any sentences that contain no projected
entities. These filtering steps allow us to keep 7.4%,
9.7% and 10.4% of the aligned sentence pairs for Ger-
man, Spanish and Dutch, respectively, resulting in
very high-precision named-entity projections (see Ta-
ble 2). For comparison, we also perform experiments
without this filtering step.
5.2.3 Setup for Cross-Lingual Experiments
We use a CRF with the same feature set and BIO
encoding for the cross-lingual models as the source-
side NER model. We compare our approach (?PR?
in Table 2) to a baseline (?BASE? in Table 2) which
treats the projected annotations as fully observed.
The PR model treats the projected NE spans of a
sentence as observed, and allows all labels on the
remaining tokens. Since the ?O? tag is never seen, an
unconstrained model would learn to never predict it.
We add two features that fire when the current word
is tagged ?O?: a bias feature and a feature that fires
when the automatic POS tag is a proper noun. We set
upQ so the desired expectations are at least 0.98 and
at most 0.1 for these constraint features respectively.
6 Results
In this section, we turn to our experimental results;
first, we focus on POS tagging and then turn to the
NE segmentation task.
6.1 Part-of-Speech Tagging
Constraint Strength: As discussed in ?4.1, it is
important to filter out projected annotations not li-
censed by Wiktionary. Thus, the quality of weakly-
supervised POS taggers learned from projections
is closely correlated with the coverage of the Wik-
tionary. To quantify the effect of Wiktionary cover-
age, we counted the expected number of possible tags
 0.7
 0.8
 0.9
 1
 0.1  0.2  0.3  0.4
optim
al b
1/TpT
Figure 2: Correlation between optimal constraint value b
and dictionary pruning efficiency. Each blue square is a
language, the green line is a linear approximation of the
data.
per token (TpT) for our unlabeled corpora. Specif-
ically, for each token, we counted the number of
tags licensed by the dictionary, or all tags for word
forms not in the dictionary. For each language, we
also ran our system with constraint strengths in {0.7,
0.75, 0.8, 0.85, 0.9, 0.92, 0.95, 0.98, 1.00}, and com-
puted the optimal constraint strength from this set.
We found that the best constraint strength is closely
correlated with the average number of tags available
for each token. Figure 2 shows the best constraint
strength as a function of the inverse of the number of
unpruned tags per token. As observed in the figure,
the relationship between the optimal strength and
1/TpT is roughly linear. Figure 2 also shows a linear
approximation to the data plotted. When applying
this technique to a new language, we would not be
able to estimate the optimal constraint strength, but
we could use the linear approximation and knowl-
edge of 1/TpT to estimate it. For our experiments
below, we perform this estimation for each language
using the linear approximation computed from the
remaining languages.
Results: The results for our part-of-speech tagging
experiments are in Table 1. We compare our results
to BASE, which corresponds to reruns of the best
model of Ta?ckstro?m et al (2013, Column 9 of Ta-
ble 2), and closely aligns with the numbers reported
by the authors. We see in Table 1 that for both fea-
ture sets (i.e., with and without the ?+? extension),
our estimated constraint strength is usually better
than using a constraint strength of 1. The results
in the column labeled PR are better than BASE for
12 out of 17 languages, and the results for PR+ are
2002
BASE BASE+ PR PR+
ar 37.84 44.96 ? 49.04? 50.10?
bg 88.04 87.93 88.02 88.42?
cs 79.67 80.01 ? 80.20? 80.68?
da 88.14 87.92 88.24? 87.90
de 90.32 89.97 90.41? 90.29
el 90.03 89.03 90.63? 90.24?
es 86.99 86.81 87.20? 87.21?
fr 87.07 87.53 ? 87.44? 87.48?
hu 82.05 82.05 82.14? 83.13?
it 89.48 89.89 ? 89.52 89.72?
ja 80.63 78.54 80.02 79.68
nl 85.89 85.77 85.59 85.98?
pt 90.93 91.60 ? 91.48? 91.56?
sl 82.46 82.08 83.16? 83.49?
sv 89.06 88.72 89.25? 88.77
tr 64.39 65.74 ? 63.88 66.47?
zh 73.98 72.82 74.51? 68.43
Avg 81.59 81.85 ? 82.40? 82.33?
-zh-ar 85.01 84.91 85.15? 85.40?
Table 1: POS tagging results. BASE represents the best
model of Ta?ckstro?m et al (2013). PR is a system with
the same features but with relaxed constraints. BASE+
and PR+ add additional model features (see ?5.2.3). ? in-
dicates improvements over the previous state of the art
(BASE), and bold values indicate the best score for a lan-
guage. ?Avg? indicates averaged results for all 17 lan-
guages, while ?-zh-ar? shows averaged results without
Chinese and Arabic.
better than BASE+ for 13 out of 17 languages. Ad-
ditionally, adding features does not tend to help the
baseline model to a large extent (the wins are for
6 languages), but does tend to help the PR model
(for 11 languages); however, there is a large drop in
performance for Chinese.
Error Analysis: Here, we analyze the nature of
improvements that the PR models get. For the lan-
guages where PR results in large improvements, it
stems from the ability to allow the sentential con-
text to sometimes override the tag projected via the
parallel data. For example, the Czech word se can
either be a reflexive pronoun (such as ourselves in
English) or translate to the preposition with. The
pronominal sense comprises about 95% of occur-
rences in the Czech annotations, but it would not
appear in an English translation. For example, the
phrase ?pod??vali jsme se? translates to ?we looked?,
and the word jsme would typically be aligned to we;
se, which serves as a reflexive pronoun here, remains
unaligned. Consequently, in our data, over 7000 oc-
currences of se appear, but only 17 instances have a
tag projection that is not filtered by Wiktionary. Since
the remaining are tagged with the preposition tag, the
hard-constrained baseline always tags se as a prepo-
sition. By contrast, the soft-constrained PR model
predicts the pronominal sense in cases where the con-
text is most indicative of a pronoun ? 38% of the time.
It still mistags many of the pronominal cases where
the contextual evidence is not strong enough. We get
very similar behavior with the Hungarian word hogy
which can translate to the conjunction that (as in ?I
see that you are here?) or the adverb how.
We found that the drastic drop in performance for
Chinese under the PR+ model is due to the possessive
marker ??? which serves exclusively as a particle
in the test data. Wiktionary also allows the noun and
adverb tags. The adverbial use is actually a different
token (?? ? really, truly) containing the same
character. Because the cross-lingual training data is
based on machine-learned alignments, 99.4% of the
training examples of? have no annotations, and only
0.6% have the particle annotation projected from the
English ?s possessive marker. If we remove the noun
and adverb senses from the Wiktionary performance
of PR+ improves to 72.87%. Alternatively, we could
add another constraint to prefer closed-class words
over open-class words when both are licensed by the
dictionary. When we add such a constraint to Chinese
with a constraint value of 0.95, we recover most of the
loss (68.43? 72.94); however, we do not report this
specific change to the Chinese experimental setup in
Table 1 to maintain generality.
6.2 Named-Entity Segmentation
Results: Table 2 shows the results for the named en-
tity segmentation experiments. First, we observe that
the word alignment filtering step (?5.2.2) improves
results for all three languages by significant margins,
for both the BASE and PR models. Both with and
without filtering, we observe that the baseline mod-
els are very strongly biased towards precision. The
filtering step tends to help with recall more than pre-
cison for both models. By having a soft constraint via
PR and allowing some segmentations to fall outside
of the transferred one, we get an increase in recall,
2003
No Filtering Filtering (?5.2.2)
Lang Metric BASE PR BASE PR
Prec 74.29 73.85 75.36 76.47
de Recall 41.69 54.50 54.71 64.61
F1 53.41 62.71 63.39 70.04
Prec 74.53 62.10 82.50 70.22
es Recall 56.39 78.33 67.27 81.10
F1 64.20 69.28 74.11 75.27
Prec 81.90 75.12 86.39 76.09
nl Recall 50.54 76.11 65.45 79.11
F1 62.51 75.61 74.47 77.57
Above: dev, below: test
Prec 73.23 71.67 69.90 70.94
de Recall 39.70 51.81 52.52 61.42
F1 51.49 60.14 59.97 65.84
Prec 75.38 65.40 83.50 73.68
es Recall 56.00 80.30 67.55 83.31
F1 64.26 72.09 74.68 78.20
Prec 79.45 73.55 86.01 77.05
nl Recall 47.45 75.37 65.16 80.11
F1 59.42 74.45 74.14 78.55
Table 2: Result for the named-entity segmentation exper-
iments. The highest score in each category is shown in
bold. Note that ?No Filtering? still discards sentences with
no projected entities.
and in turn an improved F1 score. On average the
PR model improves F-score by 3.6% on the develop-
ment set and 4.6% on the test set over the baseline
(when filtering is used). Note that because we focus
on named entity segmentation, our results are not
directly comparable to those of Ta?ckstro?m (2012),
who train a de-lexicalized named entity recognizer
on one language and apply it to other languages.
Error Analysis: In order to get a sense for the types
of errors made by the baseline which are corrected
by the PR model, we collected statistics about the
most frequent errors in the segments extracted by
the baseline and by our model. We divided the er-
rors into missing segments, extraneous segments and
overlapping segments.
From Table 2, it is clear that the most common
errors for the baseline models are missing entities.
From our analysis of the CoNLL development data,
we found that the entities that occur with little context
(such as the location and publisher of an item) at the
onset of news articles are most frequently missed. For
German, dpa (Deutsche Presse-Agentur) and Reuter
are the two most common missing segmentations;
the Spanish counterparts are Gobierno (Government)
and Barcelona, while for Dutch they are De Morgen
and Brussel. While filtering parallel sentences and
using a soft constraint both increase recall, even our
strongest model does not get enough information to
predict these entities, and they continue to be major
sources of error. By contrast, the names mentioned
in context are the ones that are most frequently added
to the analysis when PR is used. In a sense this
is desirable, since a machine-learned named-entity
segmentation system is most useful for the long tail
of entity mentions.
If we filter the training data and use the PR model
to further increase recall, precision errors tend to
become relatively more frequent (this trend is ob-
servable in Table 2). For German, the most frequent
precision error is Mark referring to the Deutsche
Mark. For Spanish, the most frequent precision er-
rors are due to boundary errors. The Spanish an-
notation guidelines include enclosing quotes as part
of the entity name, and failing to include them ac-
counts for just under 1% of the precision errors of
the PR system that uses filtering. The second most
frequent error is failing to segment Inter de Mila?n.
The model segments out either Inter or Mila?n or both
by themselves depending on context.
7 Conclusions
In this paper, we presented a framework for cross-
lingual transfer of sequence information from a
resource-rich source language to a resource-poor tar-
get language. Our framework incorporates soft con-
straints while training with projected information
via posterior regularization. We presented the effi-
cacy of our framework on two very useful natural
language tasks: POS tagging and named-entity seg-
mentation. The soft constraints used in our work
model intuitions about a given task. For the POS
tagging problem, we designed constraints that also
incorporate projected token-level information, and
presented a principled method for choosing the extent
to which this information should be trusted within
the PR framework. This approach generalizes the
state of the art in cross-lingual projection work in
the context of POS tagging, and improves upon it.
2004
Across seventeen languages, our models outperform
the previous state of the art by an average of 0.8%
(greater than 4% error reduction), and outperforms
it on twelve out of seventeen languages. For named-
entity segmentation, our model results in 3.6% and
4.6% absolute improvements in F1-score on our de-
velopment and test sets respectively, when averaged
across three languages.
Acknowledgments
We would like to thank Ryan McDonald, Fernando
Pereira, Slav Petrov and Oscar Ta?ckstro?m for numer-
ous discussions on this topic and providing detailed
feedback on early drafts of this paper. We are also
grateful to the four anonymous reviewers for their
valuable comments.
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a Treebank for French. In A. Abeille?,
editor, Treebanks: Building and Using Parsed Corpora,
chapter 10. Kluwer.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Meredith J. Goldsmith, Jan Hajic,
Robert L. Mercer, and Surya Mohanty. 1993. But
dictionaries are data too. In Proceedings of the Work-
shop on Human Language Technology.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1?38.
John DeNero and Klaus Macherey. 2011. Model-based
aligner combination using dual decomposition. In Pro-
ceedings of ACL-HLT.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of ACL-IJCNLP.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine Learn-
ing Research, 11:2001?2049.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Jun?ichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proceedings of EMNLP.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu. 2012.
Multilingual named entity recognition using parallel
data and metadata from wikipedia. In Proceedings of
ACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
Solomon Kullback and Richard A. Leibler. 1951. On
information and sufficiency. Annals of Mathematical
Statistics, 22:49?86.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of ICML.
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP-CoNLL.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential families.
In Proceedings of ICML.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
2005
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan Mc-
Donald, and Joakim Nivre. 2013. Token and type con-
straints for cross-lingual part-of-speech tagging. Trans-
actions of the Association for Computational Linguis-
tics, 1:1?12.
Oscar Ta?ckstro?m. 2012. Nudging the envelope of direct
transfer methods for multilingual named entity recogni-
tion. In Proceedings of the NAACL-HLT Workshop on
the Induction of Linguistic Structure.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In Proceedings
of CoNLL.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
UN. 2006. ODS UN parallel corpus.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of COLING.
Andreas Vlachos. 2006. Active annotation. Proceedings
of EACL.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In Pro-
ceedings of IJCNLP Workshop: NLP for Less Privi-
leged Languages.
2006
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1273?1283,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning Compact Lexicons for CCG Semantic Parsing
Yoav Artzi
?
Computer Science & Engineering
University of Washington
Seattle, WA 98195
yoav@cs.washington.edu
Dipanjan Das Slav Petrov
Google Inc.
76 9th Avenue
New York, NY 10011
{dipanjand,slav}@google.com
Abstract
We present methods to control the lexicon
size when learning a Combinatory Cate-
gorial Grammar semantic parser. Existing
methods incrementally expand the lexicon
by greedily adding entries, considering a
single training datapoint at a time. We pro-
pose using corpus-level statistics for lexi-
con learning decisions. We introduce vot-
ing to globally consider adding entries to
the lexicon, and pruning to remove entries
no longer required to explain the training
data. Our methods result in state-of-the-art
performance on the task of executing se-
quences of natural language instructions,
achieving up to 25% error reduction, with
lexicons that are up to 70% smaller and are
qualitatively less noisy.
1 Introduction
Combinatory Categorial Grammar (Steedman,
1996, 2000, CCG, henceforth) is a commonly
used formalism for semantic parsing ? the task
of mapping natural language sentences to for-
mal meaning representations (Zelle and Mooney,
1996). Recently, CCG semantic parsers have been
used for numerous language understanding tasks,
including querying databases (Zettlemoyer and
Collins, 2005), referring to physical objects (Ma-
tuszek et al., 2012), information extraction (Kr-
ishnamurthy and Mitchell, 2012), executing in-
structions (Artzi and Zettlemoyer, 2013b), gen-
erating regular expressions (Kushman and Barzi-
lay, 2013), question-answering (Cai and Yates,
2013) and textual entailment (Lewis and Steed-
man, 2013). In CCG, a lexicon is used to map
words to formal representations of their meaning,
which are then combined using bottom-up opera-
tions. In this paper we present learning techniques
?
This research was carried out at Google.
chair ` N : ?x.chair(x)
chair ` N : ?x.sofa(x)
chair ` AP : ?a.len(a, 3)
chair ` NP : A(?x.corner(x))
chair ` ADJ : ?x.hall(x)
Figure 1: Lexical entries for the word chair as learned
with no corpus-level statistics. Our approach is able to
correctly learn only the top two bolded entries.
to explicitly control the size of the CCG lexicon,
and show that this results in improved task perfor-
mance and more compact models.
In most approaches for inducing CCGs for se-
mantic parsing, lexicon learning and parameter es-
timation are performed jointly in an online algo-
rithm, as introduced by Zettlemoyer and Collins
(2007). To induce the lexicon, words extracted
from the training data are paired with CCG cat-
egories one sample at a time (for an overview of
CCG, see ?2). Joint approaches have the potential
advantage that only entries participating in suc-
cessful parses are added to the lexicon. However,
new entries are added greedily and these decisions
are never revisited at later stages. In practice, this
often results in a large and noisy lexicon.
Figure 1 lists a sample of CCG lexical entries
learned for the word chair with a greedy joint al-
gorithm (Artzi and Zettlemoyer, 2013b). In the
studied navigation domain, the word chair is often
used to refer to chairs and sofas, as captured by the
first two entries. However, the system also learns
several spurious meanings: the third shows an er-
roneous usage of chair as an adverbial phrase de-
scribing action length, while the fourth treats it as
a noun phrase and the fifth as an adjective. In con-
trast, our approach is able to correctly learn only
the top two lexical entries.
We present a batch algorithm focused on con-
trolling the size of the lexicon when learning CCG
semantic parsers (?3). Because we make updates
only after processing the entire training set, we
1273
can take corpus-wide statistics into account be-
fore each lexicon update. To explicitly control
the size of the lexicon, we adopt two complemen-
tary strategies: voting and pruning. First, we con-
sider the lexical evidence each sample provides as
a vote towards potential entries. We describe two
voting strategies for deciding which entries to add
to the model lexicon (?4). Second, even though
we use voting to only conservatively add new lex-
icon entries, we also prune existing entries if they
are no longer necessary for parsing the training
data. These steps are incorporated into the learn-
ing framework, allowing us to apply stricter crite-
ria for lexicon expansion while maintaining a sin-
gle learning algorithm.
We evaluate our approach on the robot navi-
gation semantic parsing task (Chen and Mooney,
2011; Artzi and Zettlemoyer, 2013b). Our exper-
imental results show that we outperform previous
state of the art on executing sequences of instruc-
tions, while learning significantly more compact
lexicons (?6 and Table 3).
2 Task and Inference
To present our lexicon learning techniques, we
focus on the task of executing natural language
navigation instructions (Chen and Mooney, 2011).
This domain captures some of the fundamental
difficulties in recent semantic parsing problems.
In particular, it requires learning from weakly-
supervised data, rather than data annotated with
full logical forms, and parsing sentences in a
situated environment. Additionally, successful
task completion requires interpreting and execut-
ing multiple instructions in sequence, requiring
accurate models to avoid cascading errors. Al-
though this overview centers around the aforemen-
tioned task, our methods are generalizable to any
semantic parsing approach that relies on CCG.
We approach the navigation task as a situated
semantic parsing problem, where the meaning of
instructions is represented with lambda calculus
expressions, which are then deterministically ex-
ecuted. Both the mapping of instructions to logi-
cal forms and their execution consider the current
state of the world. This problem was recently ad-
dressed by Artzi and Zettlemoyer (2013b) and our
experimental setup mirrors theirs. In this section,
we provide a brief background on CCG and de-
scribe the task and our inference method.
walk forward twice
S/NP NP AP
?x.?a.move(a) ? direction(a, x) forward ?a.len(a, 2)
>
S S\S
?a.move(a) ? direction(a, forward) ?f.?a.f(a) ? len(a, 2)
<
S
?a.move(a) ? direction(a, forward) ? len(a, 2)
in the red hallway
PP/NP NP/N ADJ N
?x.?y.intersect(y, x) ?f.?(f) ?x.brick(x) ?x.hall(x)
N/N
?f.?x.f(x)?
brick(x)
<
N
?x.hall(x) ? brick(x)
>
NP
?(?x.hall(x) ? brick(x)
>
PP
?y.intersect(y, ?(?x.hall(x) ? brick(x)))
Figure 2: Two CCG parses. The top shows a complete
parse with an adverbial phrase (AP ), including unary
type shifting and forward (>) and backward (<) ap-
plication. The bottom fragment shows a prepositional
phrase (PP ) with an adjective (ADJ).
2.1 Combinatory Categorial Grammar
CCG is a linguistically-motivated categorial for-
malism for modeling a wide range of language
phenomena (Steedman, 1996; Steedman, 2000).
In CCG, parse tree nodes are categories, which are
assigned to strings (single words or n-grams) and
combined to create a complete derivation. For ex-
ample, S/NP : ?x.?a.move(a)? direction(a, x)
is a CCG category describing an imperative verb
phrase. The syntactic type S/NP indicates the
category is expecting an argument of type NP
on its right, and the returned category will have
the syntax S. The directionality is indicated by
the forward slash /, where a backward slash \
would specify the argument is expected on the left.
The logical form in the category represents its se-
mantic meaning. For example, ?x.?a.move(a) ?
direction(a, x) in the category above is a function
expecting an argument, the variable x, and return-
ing a function from events to truth-values, the se-
mantic representation of imperatives. In this do-
main, the conjunction in the logical form specifies
conditions on events. Specifically, the event must
be a move event and have a specified direction.
A CCG is defined by a lexicon and a set of com-
binators. The lexicon provides a mapping from
strings to categories. Figure 2 shows two CCG
parses in the navigation domain. Parse trees are
read top to bottom. Parsing starts by matching cat-
egories to strings in the sentence using the lexicon.
For example, the lexical entry walk ` S/NP :
?x.?a.move(a) ? direction(a, x) pairs the string
walk with the example category above. Each in-
termediate parse node is constructed by applying
1274
one of a small set of binary CCG combinators or
unary operators. For example, in Figure 2 the cat-
egory of the span walk forward is combined with
the category of twice using backward application
(<). Parsing concludes with a logical form that
captures the meaning of the complete sentence.
We adopt a factored representation for CCG
lexicons (Kwiatkowski et al., 2011), where
entries are dynamically generated by combining
lexemes and templates. A lexeme is a pair
that consists of a natural language string and
a set of logical constants, while the template
contains the syntactic and semantic components
of a CCG category, abstracting over logical
constants. For example, consider the lexical entry
walk ` S/NP : ?x.?a.move(a) ? direction(a, x).
Under the factored representation, this entry
can be constructed by combining the lexeme
?walk, {move,direction}? and the template
?v
1
.?v
2
.[S/NP : ?x.?a.v
1
(a) ? v
2
(a, x)]. This
representation allows for better generalization
over unseen lexical entries at inference time,
allowing for pairings of templates and lexemes
not seen during training.
2.2 Situated Log-Linear CCGs
We use a CCG to parse sentences to logical forms,
which are then executed. Let S be a set of states,
X be the set of all possible sentences, and E be
the space of executions, which are S ? S func-
tions. For example, in the navigation task from
Artzi and Zettlemoyer (2013b), S is a set of po-
sitions on a map, as illustrated in Figure 3. The
map includes an agent that can perform four ac-
tions: LEFT, RIGHT, MOVE, and NULL. An execu-
tion e is a sequence of actions taken consecutively.
Given a state s ? S and a sentence x ? X , we aim
to find the execution e ? E described in x. Let Y
be the space of CCG parse trees and Z the space
of all possible logical forms. Given a sentence x
we generate a CCG parse y ? Y , which includes a
logical form z ? Z . An execution e is then gener-
ated from z using a deterministic process.
Parsing with a CCG requires choosing appro-
priate lexical entries from an often ambiguous lex-
icon and the order in which operations are ap-
plied. In a situated scenario such choices must
account for the current state of the world. In gen-
eral, given a CCG, there are many parses for each
sentence-state pair. To discriminate between com-
peting parses, we use a situated log-linear CCG,
facing the chair in the intersection move forward twice
?a.pre(a, front(you, ?(?x.chair(x)?
intersect(x, ?(?y.intersection(y))))))?
move(a) ? len(a, 2)
?FORWARD, FORWARD?
turn left
?a.turn(a) ? direction(a, left)
?LEFT?
go to the end of the hall
?x.move(a) ? to(a, ?(?x.end(x, ?(?y.hall(y)))))
?FORWARD, FORWARD?
Figure 3: Fragment of a map and instructions for the
navigation domain. The fragment includes two inter-
secting hallways (red and blue), two chairs and an agent
facing left (green pentagon), which follows instructions
such as these listed below. Each instruction is paired
with a logical form representing its meaning and its ex-
ecution in the map.
inspired by Clark and Curran (2007).
Let GEN(x, s; ?) ? Y be the set of all possi-
ble CCG parses given the sentence x, the current
state s and the lexicon ?. In GEN(x, s; ?), multi-
ple parse trees may have the same logical form;
let Y(z) ? GEN(x, s; ?) be the subset of such
parses with the logical form z at the root. Also,
let ? ? R
d
be a d-dimensional parameter vector.
We define the probability of the logical form z as:
p(z|x, s; ?,?) =
?
y?Y(z)
p(y|x, s; ?,?) (1)
Above, we marginalize out the probabilities of all
parse trees with the same logical form z at the root.
The probability of a parse tree y is defined as:
p(y|x, s; ?,?) =
e
???(x,s,y)
?
y
?
?GEN(x,s;?)
e
???(x,s,y
?
)
(2)
Where ?(x, s, y) ? R
d
is a feature vector. Given
a logical form z, we deterministically map it to an
execution e ? E . At inference time, given a sen-
tence x and state s, we find the best logical form
z
?
(and its corresponding execution) by solving:
z
?
= arg max
z
p(z|x, s; ?,?) (3)
1275
The above arg max operation sums over all trees
y ? Y(z), as described in Equation 1. We use a
CKY chart for this computation. The chart signa-
ture in each span is a CCG category. Since ex-
act inference is prohibitively expensive, we fol-
low previous work and perform bottom-up beam
search, maintaining only the k-best categories for
each span in the chart. The logical form z
?
is taken
from the k-best categories at the root of the chart.
The partition function in Equation 2 is approxi-
mated by summing the inside scores of all cate-
gories at the root. We describe the choices of hy-
perparameters and details of our feature set in ?5.
3 Learning
Learning a CCG semantic parser requires inducing
the entries of the lexicon ? and estimating pars-
ing parameters ?. We describe a batch learning
algorithm (Figure 4), which explicitly attempts to
induce a compact lexicon, while fully explaining
the training data. At training time, we assume ac-
cess to a set of N examples D =
{
d
(i)
}
N
1
, where
each datapoint d
(i)
= ?x
(i)
, s
(i)
, e
(i)
?, consists of
an instruction x
(i)
, the state s
(i)
where the instruc-
tion is issued and its execution demonstration e
(i)
.
In particular, we know the correct execution for
each state and instruction, but we do not know the
correct CCG parse and logical form. We treat the
choices that determine them, including selection
of lexical entries and parsing operators, as latent.
Since there can be many logical forms z ? Z that
yield the same execution e
(i)
, we marginalize over
the logical forms (using Equation 1) when maxi-
mizing the following regularized log-likelihood:
L (?,?,D) = (4)
?
d
(i)
?D
?
z?Z(e
(i)
)
p(z|x
(i)
, s
(i)
; ?,?)?
?
2
???
2
2
WhereZ(e
(i)
) is the set of logical forms that result
in the execution e
(i)
and the hyperparameter ? is
a regularization constant. Due to the large number
of potential combinations,
1
it is impractical to con-
sider the complete set of lexical entries, where all
strings (single words and n-grams) are associated
with all possible CCG categories. Therefore, simi-
lar to prior work, we gradually expand the lexicon
during learning. As a result, the parameter space
1
For the navigation task, given the set of CCG category
templates (see ?2.1) and parameters used there would be be-
tween 7.5-10.2M lexical entries to consider, depending on the
corpus used (?5).
Algorithm 1 Batch algorithm for maximizing L (?,?,D).
See ?3.1 for details.
Input: Training dataset D =
{
d
(i)
}
N
1
, number of learning
iterations T , seed lexicon ?
0
, a regularization constant
?, and a learning rate ?. VOTE is defined in ?4.
Output: Lexicon ? and model parameters ?
1: ?? ?
0
2: for t = 1 to T do
?
Generate lexical entries for all datapoints.
3: for i = 1 to N do
4: ?
(i)
? GENENTRIES(d
(i)
, ?,?)
?
Add corpus-wide voted entries to model lexicon.
5: ?? ? ? VOTE(?, {?
(1)
, . . . , ?
(N)
})
?
Compute gradient and entries to prune.
6: for i = 1 to N do
7: ??
(i)
?
,?
(i)
? ? COMPUTEUPDATE(d
(i)
, ?,?)
?
Prune lexicon.
8: ?? ? \
N?
i=1
?
(i)
?
?
Update model parameters.
9: ? ? ? + ?
N?
i=1
?
(i)
? ??
10: return ? and ?
Algorithm 2 GENENTRIES: Algorithm to generate lexical
entries from one training datapoint. See ?3.2 for details.
Input: Single datapoint d = ?x, s, e?, current model param-
eters ? and lexicon ?.
Output: Datapoint-specific lexicon entries ?.
?
Augment lexicon with sentence-specific entries.
1: ?
+
? ? ? GENLEX(d,?, ?)
?
Get max-scoring parses producing correct execution.
2: y
+
? GENMAX(x, s, e; ?
+
, ?)
?
Extract lexicon entries from max-scoring parses.
3: ??
?
y?y
+
LEX(y)
4: return ?
Algorithm 3 COMPUTEUPDATE: Algorithm to compute the
gradient and the set of lexical entries to prune for one data-
point. See ?3.3 for details.
Input: Single datapoint d = ?x, s, e?, current model param-
eters ? and lexicon ?.
Output: ??
?
,??, lexical entries to prune for d and gradient.
?
Get max-scoring correct parses given ? and ?.
1: y
+
? GENMAX(x, s, e; ?, ?)
?
Create the set of entries to prune.
2: ?
?
? ? \
?
y?y
+
LEX(y)
?
Compute gradient.
3: ?? E(y | x, s, e; ?,?)? E(y | x, s; ?,?)
4: return ??
?
,??
Figure 4: Our learning algorithm and its subroutines.
changes throughout training whenever the lexicon
is modified. The learning problem involves jointly
finding the best set of parameters and lexicon en-
tries. In the remainder of this section, we describe
how we optimize Equation 4, while explicitly con-
trolling the lexicon size.
1276
3.1 Optimization Algorithm
We present a learning algorithm to optimize the
data log-likelihood, where both lexicon learning
and parameter updates are performed in batch, i.e.,
after observing all the training corpus. The batch
formulation enables us to use information from the
entire training set when updating the model lexi-
con. Algorithm 1 presents the outline of our op-
timization procedure. It takes as input a training
dataset D, number of iterations T , seed lexicon
?
0
, learning rate ? and regularization constant ?.
Learning starts with initializing the model lex-
icon ? using ?
0
(line 1). In lines 2-9, we run T
iterations; in each, we make two passes over the
corpus, first to generate lexical entries, and second
to compute gradient updates and lexical entries to
prune. To generate lexical entries (lines 3-4) we
use the subroutine GENENTRIES to independently
generate entries for each datapoint, as described
in ?3.2. Given the entries for each datapoint, we
vote on which to add to the model lexicon. The
subroutine VOTE (line 5) chooses a subset of the
proposed entries using a particular voting strategy
(see ?4). Given the updated lexicon, we process
the corpus a second time (lines 6-7). The sub-
routine COMPUTEUPDATE, as described in ?3.3,
computes the gradient update for each datapoint
d
(i)
, and also generates the set of lexical entries not
included in the max-scoring parses of d
(i)
, which
are candidates for pruning. We prune from the
model lexicon all lexical entries not used in any
correct parse (line 8). During this pruning step, we
ensure that no entries from ?
0
are removed from
?. Finally, the gradient updates are accumulated
to update the model parameters (line 9).
3.2 Lexical Entries Generation
For each datapoint d = ?x, s, e?, the subroutine
GENENTRIES, as described in Algorithm 2, gen-
erates a set of potential entries. The subroutine
uses the function GENLEX, originally proposed
by Zettlemoyer and Collins (2005), to generate
lexical entries from sentences paired with logical
forms. We use the weakly-supervised variant of
Artzi and Zettlemoyer (2013b). Briefly, GENLEX
uses the sentence and expected execution to gen-
erate new lexemes, which are then paired with a
set of templates factored from ?
0
to generate new
lexical entries. For more details, see ?8 of Artzi
and Zettlemoyer (2013b).
Since GENLEX over-generates entries, we need
to determine the set of entries that participate
in max-scoring parses that lead to the correct
execution e. We therefore create a sentence-
specific lexicon ?
+
by taking the union of the
GENLEX-generated entries for the current sen-
tence and the model lexicon (line 1). We define
GENMAX(x, s, e; ?
+
, ?) to be the set of all max-
scoring parses according to the parameters ? that
are in GEN(x, s; ?
+
) and result in the correct ex-
ecution e (line 2). In line 3 we use the function
LEX(y), which returns the lexical entries used in
the parse y, to compute the set of all lexical en-
tries used in these parses. This final set contains
all newly generated entries for this datapoint and
is returned to the optimization algorithm.
3.3 Pruning and Gradient Computation
Algorithm 3 describes the subroutine COMPUTE-
UPDATE that, given a datapoint d, the current
model lexicon ? and model parameters ?, returns
the gradient update and the set of lexical entries
to prune for d. First, similar to GENENTRIES we
compute the set of correct max-scoring parses us-
ing GENMAX (line 1). This time, however, we do
not use a sentence-specific lexicon, but instead use
the model lexicon that has been expanded with all
voted entries. As a result, the set of max-scoring
parses producing the correct execution may be
different compared to GENENTRIES. LEX(y) is
then used to extract the lexical entries from these
parses, and the set difference (?
?
) between the
model lexicon and these entries is set to be pruned
(line 2). Finally, the partial derivative for the data-
point is computed using the difference of two ex-
pected feature vectors, according to two distribu-
tions (line 3): (a) parses conditioned on the correct
execution e, the sentence x, state s and the model,
and (b) all parses not conditioned on the execution
e. The derivatives are approximate due to the use
of beam search, as described in ?2.2.
4 Global Voting for Lexicon Learning
Our goal is to learn compact and accurate CCG
lexicons. To this end, we globally reason about
adding new entries to the lexicon by voting (VOTE,
Algorithm 1, line 5), and remove entries by prun-
ing the ones no longer required for explaining the
training data (Algorithm 1, line 8). In voting, each
datapoint can be considered as attempting to in-
fluence the learning algorithm to update the model
lexicon with the entries required to parse it. In this
1277
Round 1 Round 2 Round 3 Round 4
d
(1)
?chair, {chair}?
?chair, {hatrack}?
?chair, {turn,direction}?
1
/3
1
/3
1
/3
?chair, {chair}?
?chair, {hatrack}?
1
/2
1
/2
?chair, {chair}? 1 ?chair, {chair}? 1
d
(2)
?chair, {chair}?
?chair, {hatrack}?
1
/2
1
/2
?chair, {chair}?
?chair, {hatrack}?
1
/2
1
/2
?chair, {chair}? 1 ?chair, {chair}? 1
d
(3)
?chair, {chair}?
?chair, {easel}?
1
/2
1
/2
?chair, {chair}?
?chair, {easel}?
1
/2
1
/2
?chair, {chair}?
?chair, {easel}?
1
/2
1
/2
?chair, {chair}? 1
d
(4)
?chair, {easel}? 1 ?chair, {easel}? 1 ?chair, {easel}? 1 ?chair, {easel}? 1
Votes
?chair, {chair}?
?chair, {easel}?
?chair, {hatrack}?
?chair, {turn,direction}?
1
1
/3
1
1
/2
5
/6
1
/3
?chair, {chair}?
?chair, {easel}?
?chair, {hatrack}?
1
1
/2
1
1
/2
1
?chair, {chair}?
?chair, {easel}?
2
1
/2
1
1
/2
?chair, {chair}?
?chair, {easel}?
3
1
Discard ?chair, {turn, direction}? ?chair, {hatrack}? ?chair, {easel}?
Figure 5: Four rounds of CONSENSUSVOTE for the string chair for four training datapoints. For each datapoint,
we specify the set of lexemes generated in the Round 1 column, and update this set after each round. At the end,
the highest voted new lexeme according to the final votes is returned. In this example, MAXVOTE and CONSEN-
SUSVOTE lead to different outcomes. MAXVOTE, based on the initial sets only, will select ?chair, {easel}?.
section we describe two alternative voting strate-
gies. Both strategies ensure that new entries are
only added when they have wide support in the
training data, but count this support in different
ways. For reproducibility, we also provide step-
by-step pseudocode for both methods in the sup-
plementary material.
Since we only have access to executions and
treat parse trees as latent, we consider as correct
all parses that produce correct executions. Fre-
quently, however, incorrect parses spuriously lead
to correct executions. Lexical entries extracted
from such spurious parses generalize poorly. The
goal of voting is to eliminate such entries.
Voting is formulated on the factored lexicon
representation, where each lexical entry is factored
into a lexeme and a template, as described in ?2.1.
Each lexeme is a pair containing a natural lan-
guage string and a set of logical constants.
2
A lex-
eme is combined with a template to create a lexical
entry. In our lexicon learning approach only new
lexemes are generated, while the set of templates
is fixed; hence, our voting strategies reason over
lexemes and only create complete lexicon entries
at the end. Decisions are made for each string in-
dependently of all other strings, but considering all
occurrences of that string in the training data.
In lines 3-4 of Algorithm 1 GENENTRIES is
used to propose new lexical entries for each train-
ing datapoint d
(i)
. For each d
(i)
a set ?
(i)
, that
includes all lexical entries participating in parses
that lead to the correct execution, is generated. In
these sets, the same string can appear in multiple
2
Recall, for example, that in one lexeme the string walk
may be paired with the set of constants {move, direction}.
lexemes. To normalize its influence, each data-
point is given a vote of 1.0 for each string, which
is distributed uniformly among all lexemes con-
taining the same string.
For example, a specific ?
(i)
may consist of
the following three lexemes: ?chair, {chair}?,
?chair, {hatrack}?, ?face, {post, front, you}?. In
this set, the phrase chair has two possible mean-
ings, which will therefore each receive a vote of
0.5, while the third lexeme will be given a vote of
1.0. Such ambiguity is common and occurs when
the available supervision is insufficient to discrim-
inate between different parses, for example, if they
lead to identical executions.
Each of the two following strategies reasons
over these votes to globally select the best lex-
emes. To avoid polluting the model lexicon, both
strategies adopt a conservative approach and only
select at most one lexeme for each string in each
training iteration.
4.1 Strategy 1: MAXVOTE
The first strategy for selecting voted lexical entries
is straightforward. For each string it simply aggre-
gates all votes and selects the new lexeme with the
most votes. A lexeme is considered new if it is
not already in the model lexicon. If no such sin-
gle lexeme exists (e.g., no new entries were used
in correctly executing parses or in the case of a tie)
no lexeme is selected in this iteration.
A potential limitation of MAXVOTE is that the
votes for all rejected lexemes are lost. However,
it is often reasonable to re-allocate these votes to
other lexemes. For example, consider the sets of
lexemes for the word chair in the Round 1 col-
1278
umn of Figure 5. Using MAXVOTE on these sets
will select the lexeme ?chair, {easel}?, rather than
the correct lexeme ?chair, {chair}?. This occurs
when the datapoints supporting the correct lexeme
distribute their votes over many spurious lexemes.
4.2 Strategy 2: CONSENSUSVOTE
Our second strategy CONSENSUSVOTE aims to
capture the votes that are lost in MAXVOTE. In-
stead of discarding votes that do not go to the max-
imum scoring lexeme, voting is done in several
rounds. In each round the lowest scoring lexeme
is discarded and votes are re-assigned uniformly
to the remaining lexemes. This procedure is con-
tinued until convergence. Finally, given the sets of
lexemes in the last round, the votes are computed
and the new lexeme with most votes is selected.
Figure 5 shows a complete voting process for
four training datapoints. In each round, votes
are aggregated over the four sets of lexemes, and
the lexeme with the fewest votes is discarded.
For each set of lexemes, the discarded lexeme
is removed, unless it will lead to an empty set.
3
In the example, while ?chair, {easel}? is dis-
carded in Round 3, it remains in the set of d
(4)
.
The process converges in the fourth round, when
there are no more lexemes to discard. The fi-
nal sets include two entries: ?chair, {chair}? and
?chair, {easel}?. By avoiding wasting votes on
lexemes that have no chance of being selected, the
more widely supported lexeme ?chair, {chair}?
receives the most votes, in contrast to Round 1,
where ?chair, {easel}? was the highest voted one.
5 Experimental Setup
To isolate the effect of our lexicon learning tech-
niques we closely follow the experimental setup of
previous work (Artzi and Zettlemoyer, 2013b, ?9)
and use its publicly available code.
4
This includes
the provided beam-search CKY parser, two-pass
parsing for testing, beam search for executing se-
quences of instructions and the same seed lexicon,
weight initialization and features. Finally, except
3
This restriction is meant to ensure that discarding lex-
emes will not change the set of sentences that can be parsed.
In addition, it means that the total amount of votes given to a
string is invariant between rounds. Allowing for empty sets
will change the sum of votes, and therefore decrease the num-
ber of datapoints contributing to the decision.
4
Their implementation, based on the University of Wash-
ington Semantic Parsing Framework (Artzi and Zettlemoyer,
2013a), is available at http://yoavartzi.com/navi.
the optimization parameters specified below, we
use the same parameter settings.
Data For evaluation we use two related cor-
pora: SAIL (Chen and Mooney, 2011) and ORA-
CLE (Artzi and Zettlemoyer, 2013b). Due to how
the original data was collected (MacMahon et al.,
2006), SAIL includes many wrong executions and
about 30% of all instruction sequences are infeasi-
ble (e.g., instructing the agent to walk into a wall).
To better understand system performance and the
effect of noise, ORACLE was created with the
subset of valid instructions from SAIL paired with
their gold executions. Following previous work,
we use a held-out set for the ORACLE corpus and
cross-validation for the SAIL corpus.
Systems We report two baselines. Our batch
baseline uses the same regularized algorithm, but
updates the lexicon by adding all entries without
voting and skips pruning. Additionally, we added
post-hoc pruning to the algorithm of Artzi and
Zettlemoyer (2013b) by discarding all learned en-
tries that are not participating in max-scoring cor-
rect parses at the end of training. For ablation,
we study the influence of the two voting strategies
and pruning, while keeping the same regulariza-
tion setting. Finally, we compare our approach to
previous published results on both corpora.
Optimization Parameters We optimized the
learning parameters using cross validation on the
training data to maximize recall of complete se-
quence execution and minimize lexicon size. We
use 10 training iterations and the learning rate
? = 0.1. For SAIL we set the regularization pa-
rameter ? = 1.0 and for ORACLE ? = 0.5.
Full Sequence Inference To execute sequences
of instructions we use the beam search procedure
of Artzi and Zettlemoyer (2013b) with an identical
beam size of 10. The beam stores states, and is
initialized with the starting state. Instructions are
executed in order, each is attempted from all states
currently in the beam, the beam is then updated
and pruned to keep the 10-best states. At the end,
the best scoring state in the beam is returned.
Evaluation Metrics We evaluate the end-to-end
task of executing complete sequences of instruc-
tions against an oracle final state. In addition, to
better understand the results, we also measure task
completion for single instructions. We repeated
1279
ORACLE corpus cross-validation
Single sentence Sequence Lexicon
P R F1 P R F1 size
Artzi and Zettlemoyer (2013b) 84.59 82.74 83.65 68.35 58.95 63.26 5383
w/ post-hoc pruning 84.32 82.89 83.60 66.83 61.23 63.88 3104
Batch baseline 85.14 81.91 83.52 72.64 60.13 65.76 6323
w/ MAXVOTE 84.04 82.25 83.14 72.79 64.86 68.55 2588
w/ CONSENSUSVOTE 84.51 82.23 83.36 72.99 63.45 67.84 2446
w/ pruning 85.58 83.51 84.53 75.15 65.97 70.19 2791
w/ MAXVOTE + pruning 84.50 82.89 83.69 72.91 66.40 69.47 2186
w/ CONSENSUSVOTE + pruning 85.22 83.00 84.10 75.65 66.15 70.55 2101
Table 1: Ablation study using cross-validation on the ORACLE corpus training data. We report mean precision
(P), recall (R) and harmonic mean (F1) of execution accuracy on single sentences and sequences of instructions
and mean lexicon sizes. Bold numbers represent the best performing method on a given metric.
Final results
Single sentence Sequence Lexicon
P R F1 P R F1 size
SAIL
Chen and Mooney (2011) 54.40 16.18
Chen (2012) 57.28 19.18
+ additional data 57.62 20.64
Kim and Mooney (2012) 57.22 20.17
Kim and Mooney (2013) 62.81 26.57
Artzi and Zettlemoyer (2013b) 67.60 65.28 66.42 38.06 31.93 34.72 10051
Our Approach 66.67 64.36 65.49 41.30 35.44 38.14 2873
ORACLE
Artzi and Zettlemoyer (2013b) 81.17 (0.68) 78.63 (0.84) 79.88 (0.76) 68.07 (2.72) 58.05 (3.12) 62.65 (2.91) 6213 (217)
Our Approach 79.86 (0.50) 77.87 (0.41) 78.85 (0.45) 76.05 (1.79) 68.53 (1.76) 72.10 (1.77) 2365 (57)
Table 2: Our final results compared to previous work on the SAIL and ORACLE corpora. We report mean precision
(P), recall (R), harmonic mean (F1) and lexicon size results and standard deviation between runs (in parenthesis)
when appropriate. Our Approach stands for batch learning with a consensus voting and pruning. Bold numbers
represent the best performing method on a given metric.
each experiment five times and report mean preci-
sion, recall,
5
harmonic mean (F1) and lexicon size.
For held-out test results we also report standard
deviation. For the baseline online experiments we
shuffled the training data between runs.
6 Results
Table 1 shows ablation results for 5-fold cross-
validation on the ORACLE training data. We
evaluate against the online learning algorithm of
Artzi and Zettlemoyer (2013b), an extension of it
to include post-hoc pruning and a batch baseline.
Our best sequence execution development result
is obtained with CONSENSUSVOTE and pruning.
The results provide a few insights. First, sim-
ply switching to batch learning provides mixed re-
sults: precision increases, but recall drops and the
learned lexicon is larger. Second, adding pruning
results in a much smaller lexicon, and, especially
in batch learning, boosts performance. Adding
voting further reduces the lexicon size and pro-
vides additional gains for sequence execution. Fi-
nally, while MAXVOTE and CONSENSUSVOTE
give comparable performance on their own, CON-
SENSUSVOTE results in more precise and compact
5
Recall is identical to accuracy as reported in prior work.
models when combined with pruning.
Table 2 lists our test results. We significantly
outperform previous state of the art on both cor-
pora when evaluating sequence accuracy. In both
scenarios our lexicon is 60-70% smaller. In con-
trast to the development results, single sentence
performance decreases slightly compared to Artzi
and Zettlemoyer (2013b). The discrepancy be-
tween single sentence and sequence results might
be due to the beam search performed when execut-
ing sequences of instructions. Models with more
compact lexicons generate fewer logical forms for
each sentence: we see a decrease of roughly 40%
in our models compared to Artzi and Zettlemoyer
(2013b). This is especially helpful during se-
quence execution, where we use a beam size of
10, resulting in better sequences of executions. In
general, this shows the potential benefit of using
more compact models in scenarios that incorpo-
rate reasoning about parsing uncertainty.
To illustrate the types of errors avoided with
voting and pruning, Table 3 describes common
error classes and shows example lexical entries
for batch trained models with CONSENSUSVOTE
and pruning and without. Quantitatively, the mean
number of entries per string on development folds
1280
String
# lexical entries
Example categoriesBatch With voting
baseline and pruning
The algorithm often treats common bigrams as multiword phrases, and later learns the more general separate entries.
Without pruning the initial entries remain in the lexicon and compete with the correct ones during inference.
octagon carpet 45 0 N : ?x.wall(x) N : ?x.hall(x)
N : ?x.honeycomb(x)
carpet 51 5 N : ?x.hall(x)
N/N : ?f.?x.x == argmin(f, ?y.dist(y))
octagon 21 5 N : ?x.honeycomb(x) N : ?x.cement(x)
ADJ : ?x.honeycomb(x)
We commonly see in the lexicon a long tail of erroneous entries, which compete with correctly learned ones. With voting
and pruning we are often able to avoid such noisy entries. However, some noise still exists, e.g., the entry for ?intersection?.
intersection 45 7 N : ?x.intersection(x) S\N : ?f.intersect(you, (f))
AP : ?a.len(a, 1) N/NP : ?x.?y.intersect(y, x)
twice 46 2 AP : ?a.len(a, 2) AP : ?a.pass(a,A(?x.empty(x)))
AP : ?a.pass(a,A(?x.hall(x)))
stone 31 5 ADJ : ?x.stone(x) ADJ : ?x.brick(x)
ADJ : ?x.honeycomb(x) NP/N : ?f.A(f)
Not all concepts mentioned in the corpus are relevant to the task and some of these are not semantically modeled. However,
the baseline learner doesn?t make this distinction and induces many erroneous entries. With voting the model better handles
such cases, either by pairing such words with semantically empty entries or learning no entries for them. During inference
the system can then easily skip such words.
now 28 0 AP : ?a.len(a, 3) AP : ?a.direction(a, forward)
only 38 0 N/NP : ?x.?y.intersect(y, x)
N/NP : ?x.?y.front(y, x)
here 31 8 NP : you S/S : ?x.x
S\N : ?f.intersect(you,A(f))
Without pruning the learner often over-splits multiword phrases and has no way to reverse such decisions.
coat 25 0 N : ?x.intersection(x) ADJ : ?x.hatrack(x)
rack 45 0 N : ?x.hatrack(x) N : ?x.furniture(x)
coat rack 55 5 N : ?x.hatrack(x) N : ?x.wall(x)
N : ?x.furniture(x)
Voting helps to avoid learning entries for rare words when the learning signal is highly ambiguous.
orange 20 0 N : ?x.cement(x) N : ?x.grass(x)
pics of towers 26 0 N?x.intersection(x) N : ?x.hall(x)
Table 3: Example entries from a learned ORACLE corpus lexicon using batch learning. For each string we
report the number of lexical entries without voting (CONSENSUSVOTE) and pruning and with, and provide a few
examples. Struck entries were successfully avoided when using voting and pruning.
decreases from 16.77 for online training to 8.11.
Finally, the total computational cost of our ap-
proach is roughly equivalent to online approaches.
In both approaches, each pass over the data makes
the same number of inference calls, and in prac-
tice, Artzi and Zettlemoyer (2013b) used 6-8 it-
erations for online learning while we used 10. A
benefit of the batch method is its insensitivity to
data ordering, as expressed by the lower standard
deviation between randomized runs in Table 2.
6
7 Related Work
There has been significant work on learning for se-
mantic parsing. The majority of approaches treat
grammar induction and parameter estimation sep-
arately, e.g. Wong and Mooney (2006), Kate and
Mooney (2006), Clarke et al. (2010), Goldwasser
et al. (2011), Goldwasser and Roth (2011), Liang
6
Results still vary slightly due to multi-threading.
et al. (2011), Chen and Mooney (2011), and Chen
(2012). In all these approaches the grammar struc-
ture is fixed prior to parameter estimation.
Zettlemoyer and Collins (2005) proposed the
learning regime most related to ours. Their learner
alternates between batch lexical induction and on-
line parameter estimation. Our learning algo-
rithm design combines aspects of previously stud-
ied approaches into a batch method, including
gradient updates (Kwiatkowski et al., 2010) and
using weak supervision (Artzi and Zettlemoyer,
2011). In contrast, Artzi and Zettlemoyer (2013b)
use online perceptron-style updates to optimize a
margin-based loss. Our work also focuses on CCG
lexicon induction but differs in the use of corpus-
level statistics through voting and pruning for ex-
plicitly controlling the size of the lexicon.
Our approach is also related to the grammar in-
duction algorithm introduced by Carroll and Char-
1281
niak (1992). Similar to our method, they process
the data using two batch steps: the first proposes
grammar rules, analogous to our step that gener-
ates lexical entries, and the second estimates pars-
ing parameters. Both methods use pruning after
each iteration, to remove unused entries in our ap-
proach, and low probability rules in theirs. How-
ever, while we use global voting to add entries
to the lexicon, they simply introduce all the rules
generated by the first step. Their approach also
relies on using disjoint subsets of the data for the
two steps, while we use the entire corpus.
Using voting to aggregate evidence has been
studied for combining decisions from an ensem-
ble of classifiers (Ho et al., 1994; Van Erp and
Schomaker, 2000). MAXVOTE is related to ap-
proval voting (Brams and Fishburn, 1978), where
voters are required to mark if they approve each
candidate or not. CONSENSUSVOTE combines
ideas from approval voting, Borda counting, and
instant-runoff voting. Van Hasselt (2011) de-
scribed all three systems and applied them to pol-
icy summation in reinforcement learning.
8 Conclusion
We considered the problem of learning for se-
mantic parsing, and presented voting and pruning
methods based on corpus-level statistics for induc-
ing compact CCG lexicons. We incorporated these
techniques into a batch modification of an exist-
ing learning approach for joint lexicon induction
and parameter estimation. Our evaluation demon-
strates that both voting and pruning contribute to-
wards learning a compact lexicon and illustrates
the effect of lexicon quality on task performance.
In the future, we wish to study various aspects
of learning more robust lexicons. For example, in
our current approach, words not appearing in the
training set are treated as unknown and ignored at
inference time. We would like to study the bene-
fit of using large amounts of unlabeled text to al-
low the model to better hypothesize the meaning
of such previously unseen words. Moreover, our
model?s performance is currently sensitive to the
set of seed lexical templates provided. While we
are able to learn the meaning of new words, the
model is unable to correctly handle syntactic and
semantic structures not covered by the seed tem-
plates. To alleviate this problem, we intend to fur-
ther explore learning novel lexical templates.
Acknowledgements
We thank Kuzman Ganchev, Emily Pitler, Luke
Zettlemoyer, Tom Kwiatkowski and Nicholas
FitzGerald for their comments on earlier drafts,
and the anonymous reviewers for their valuable
feedback. We also wish to thank Ryan McDon-
ald and Arturas Rozenas for their valuable input
about voting procedures.
References
Yoav Artzi and Luke S. Zettlemoyer. 2011. Bootstrap-
ping semantic parsers from conversations. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Yoav Artzi and Luke S. Zettlemoyer. 2013a. UW
SPF: The University of Washington Semantic Pars-
ing Framework.
Yoav Artzi and Luke S. Zettlemoyer. 2013b. Weakly
supervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Steven J. Brams and Peter C. Fishburn. 1978. Ap-
proval voting. The American Political Science Re-
view, pages 831?847.
Qingqing Cai and Alexander Yates. 2013. Seman-
tic parsing freebase: Towards open-domain semantic
parsing. In Proceedings of the Joint Conference on
Lexical and Computational Semantics.
Gelnn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. Working Notes of the Workshop
Statistically-Based NLP Techniques.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the Na-
tional Conference on Artificial Intelligence.
David L. Chen. 2012. Fast online lexicon learning for
grounded language acquisition. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proceedings of the Conference
on Computational Natural Language Learning.
Dan Goldwasser and Dan Roth. 2011. Learning from
natural instructions. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence.
1282
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proceedings of the Association
of Computational Linguistics.
Tin K. Ho, Jonathan J. Hull, and Sargur N. Srihari.
1994. Decision combination in multiple classifier
systems. IEEE Transactions on Pattern Analysis
and Machine Intelligence, pages 66?75.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the Conference of the Association for
Computational Linguistics.
Joohyun Kim and Raymond J. Mooney. 2012. Un-
supervised pcfg induction for grounded language
learning with highly ambiguous supervision. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Joohyun Kim and Raymond J. Mooney. 2013. Adapt-
ing discriminative reranking to grounded language
learning. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Nate Kushman and Regina Barzilay. 2013. Using se-
mantic unification to generate regular expressions
from natural language. In Proceedings of the Hu-
man Language Technology Conference of the North
American Association for Computational Linguis-
tics.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing prob-
abilistic CCG grammars from logical form with
higher-order unification. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2011. Lexical Gener-
alization in CCG Grammar Induction for Semantic
Parsing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1(1):179?192.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the Conference of the As-
sociation for Computational Linguistics.
Matt MacMahon, Brian Stankiewics, and Benjamin
Kuipers. 2006. Walk the talk: Connecting language,
knowledge, action in route instructions. In Proceed-
ings of the National Conference on Artificial Intelli-
gence.
Cynthia Matuszek, Nicholas FitzGerald, Luke S.
Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A
joint model of language and perception for grounded
attribute learning. In Proceedings of the Interna-
tional Conference on Machine Learning.
Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press.
Merijn Van Erp and Lambert Schomaker. 2000.
Variants of the borda count method for combining
ranked classifier hypotheses. In In the International
Workshop on Frontiers in Handwriting Recognition.
Hado Van Hasselt. 2011. Insights in Reinforcement
Learning: formal analysis and empirical evaluation
of temporal-difference learning algorithms. Ph.D.
thesis, University of Utrecht.
Yuk W. Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Human Language
Technology Conference of the North American Asso-
ciation for Computational Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Conference on Un-
certainty in Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
1283
Frame-Semantic Parsing
Dipanjan Das?
Google Inc.
Desai Chen??
Massachusetts Institute of Technology
Andre? F. T. Martins?
Priberam Labs
Instituto de Telecomunicac?o?es
Nathan Schneider?
Carnegie Mellon University
Noah A. Smith?
Carnegie Mellon University
Frame semantics is a linguistic theory that has been instantiated for English in the FrameNet
lexicon. We solve the problem of frame-semantic parsing using a two-stage statistical model
that takes lexical targets (i.e., content words and phrases) in their sentential contexts and
predicts frame-semantic structures. Given a target in context, the first stage disambiguates it to a
semantic frame. This model uses latent variables and semi-supervised learning to improve frame
disambiguation for targets unseen at training time. The second stage finds the target?s locally
expressed semantic arguments. At inference time, a fast exact dual decomposition algorithm
collectively predicts all the arguments of a frame at once in order to respect declaratively stated
linguistic constraints, resulting in qualitatively better structures than na??ve local predictors.
Both components are feature-based and discriminatively trained on a small set of annotated
frame-semantic parses. On the SemEval 2007 benchmark data set, the approach, along with a
heuristic identifier of frame-evoking targets, outperforms the prior state of the art by significant
margins. Additionally, we present experiments on the much larger FrameNet 1.5 data set. We
have released our frame-semantic parser as open-source software.
? Google Inc., New York, NY 10011. E-mail: dipanjand@google.com.
?? Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology,
Cambridge, MA 02139. E-mail: desaic@csail.mit.edu.
? Alameda D. Afonso Henriques, 41 - 2.? Andar, 1000-123, Lisboa, Portugal. E-mail: atm@priberam.pt.
? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: nschneid@cs.cmu.edu.
? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: nasmith@cs.cmu.edu.
Submission received: 4 May 2012; revised submission received: 10 November 2012; accepted for publication:
22 December 2012.
doi:10.1162/COLI a 00163
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
1. Introduction
FrameNet (Fillmore, Johnson, and Petruck 2003) is a linguistic resource storing consider-
able information about lexical and predicate-argument semantics in English. Grounded
in the theory of frame semantics (Fillmore 1982), it suggests?but does not formally
define?a semantic representation that blends representations familiar from word-sense
disambiguation (Ide and Ve?ronis 1998) and semantic role labeling (SRL; Gildea and
Jurafsky 2002). Given the limited size of available resources, accurately producing
richly structured frame-semantic structures with high coverage will require data-driven
techniques beyond simple supervised classification, such as latent variable modeling,
semi-supervised learning, and joint inference.
In this article, we present a computational and statistical model for frame-semantic
parsing, the problem of extracting from text semantic predicate-argument structures
such as those shown in Figure 1. We aim to predict a frame-semantic representation
with two statistical models rather than a collection of local classifiers, unlike earlier ap-
proaches (Baker, Ellsworth, and Erk 2007). We use a probabilistic framework that cleanly
integrates the FrameNet lexicon and limited available training data. The probabilistic
framework we adopt is highly amenable to future extension through new features, more
relaxed independence assumptions, and additional semi-supervised models.
Carefully constructed lexical resources and annotated data sets from FrameNet,
detailed in Section 3, form the basis of the frame structure prediction task. We de-
compose this task into three subproblems: target identification (Section 4), in which
frame-evoking predicates are marked in the sentence; frame identification (Section 5),
in which the evoked frame is selected for each predicate; and argument identification
(Section 6), in which arguments to each frame are identified and labeled with a role from
that frame. Experiments demonstrating favorable performance to the previous state of
the art on SemEval 2007 and FrameNet data sets are described in each section. Some
novel aspects of our approach include a latent-variable model (Section 5.2) and a semi-
supervised extension of the predicate lexicon (Section 5.5) to facilitate disambiguation of
words not in the FrameNet lexicon; a unified model for finding and labeling arguments
Figure 1
An example sentence from the annotations released as part of FrameNet 1.5 with three targets
marked in bold. Note that this annotation is partial because not all potential targets have been
annotated with predicate-argument structures. Each target has its evoked semantic frame
marked above it, enclosed in a distinct shape or border style. For each frame, its semantic roles
are shown enclosed within the same shape or border style, and the spans fulfilling the roles are
connected to the latter using dotted lines. For example, manner evokes the CONDUCT frame, and
has the AGENT and MANNER roles fulfilled by Austria and most un-Viennese, respectively.
10
Das et al. Frame-Semantic Parsing
(Section 6) that diverges from prior work in semantic role labeling; and an exact dual
decomposition algorithm (Section 7) that collectively predicts all the arguments of a
frame together, thereby incorporating linguistic constraints in a principled fashion.
Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Represen-
tations)1 achieves the best published results to date on the SemEval 2007 frame-semantic
structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present
results on newly released data with FrameNet 1.5, the latest edition of the lexicon.
Some of the material presented in this article has appeared in previously published
conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011)
described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a
sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the
dual decomposition algorithm for constrained joint argument identification. We present
here a synthesis of those results and several additional details:
1. The set of features used in the two statistical models for frame identification and
argument identification.
2. Details of a greedy beam search algorithm for argument identification that avoids
illegal argument overlap.
3. Error analysis pertaining to the dual decomposition argument identification algo-
rithm, in contrast with the beam search algorithm.
4. Results on full frame-semantic parsing using graph-based semi-supervised learn-
ing with sparsity-inducing penalties; this expands the small FrameNet predicate
lexicon, enabling us to handle unknown predicates.
Our primary contributions are the use of efficient structured prediction tech-
niques suited to shallow semantic parsing problems, novel methods in semi-supervised
learning that improve the lexical coverage of our parser, and making frame-semantic
structures a viable computational semantic representation usable in other language
technologies. To set the stage, we next consider related work in the automatic prediction
of predicate-argument semantic structures.
2. Related Work
In this section, we will focus on previous scientific work relevant to the problem of
frame-semantic parsing. First, we will briefly discuss work done on PropBank-style
semantic role labeling, following which we will concentrate on the more relevant prob-
lem of frame-semantic structure extraction. Next, we review previous work that has
used semi-supervised learning for shallow semantic parsing. Finally, we discuss prior
work on joint structure prediction relevant to frame-semantic parsing.
2.1 Semantic Role Labeling
Since Gildea and Jurafsky (2002) pioneered statistical semantic role labeling, there
has been a great deal of computational work using predicate-argument structures
for semantics. The development of PropBank (Kingsbury and Palmer 2002), followed
by CoNLL shared tasks on semantic role labeling (Carreras and Ma`rquez 2004,
2005) boosted research in this area. Figure 2(a) shows an annotation from PropBank.
PropBank annotations are closely tied to syntax, because the data set consists of the
1 See http://www.ark.cs.cmu.edu/SEMAFOR.
11
Computational Linguistics Volume 40, Number 1
(a)
(b)
Figure 2
(a) A phrase-structure tree taken from the Penn Treebank and annotated with PropBank
predicate-argument structures. The verbs created and pushed serve as predicates in this
sentence. Dotted arrows connect each predicate to its semantic arguments (bracketed phrases).
(b) A partial depiction of frame-semantic structures for the same sentence. The words in bold
are targets, which instantiate a (lemmatized and part-of-speech?tagged) lexical unit and evoke
a semantic frame. Every frame annotation is shown enclosed in a distint shape or border style,
and its argument labels are shown together on the same vertical tier below the sentence.
See text for explanation of abbreviations.
phrase-structure syntax trees from the Wall Street Journal section of the Penn Treebank
(Marcus, Marcinkiewicz, and Santorini 1993) annotated with predicate-argument
structures for verbs. In Figure 2(a), the syntax tree for the sentence is marked with
various semantic roles. The two main verbs in the sentence, created and pushed, are
the predicates. For the former, the constituent more than 1.2 million jobs serves as the
semantic role ARG1 and the constituent In that time serves as the role ARGM-TMP. Similarly
for the latter verb, roles ARG1, ARG2, ARGM-DIR, and ARGM-TMP are shown in the figure.
PropBank defines core roles ARG0 through ARG5, which receive different interpretations
for different predicates. Additional modifier roles ARGM-* include ARGM-TMP (temporal)
and ARGM-DIR (directional), as shown in Figure 2(a). The PropBank representation
therefore has a small number of roles, and the training data set comprises some
40,000 sentences, thus making the semantic role labeling task an attractive one from the
perspective of machine learning.
There are many instances of influential work on semantic role labeling using
PropBank conventions. Pradhan et al. (2004) present a system that uses support vector
machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic
roles, followed by classification of the identified arguments to role names via a collection
of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses inte-
ger linear programming for inference and uses several global constraints to find the best
12
Das et al. Frame-Semantic Parsing
suited predicate-argument structures. Joint modeling for semantic role labeling with
discriminative log-linear models is presented by Toutanova, Haghighi, and Manning
(2005), where global features looking at all arguments of a particular verb together are
incorporated into a dynamic programming and reranking framework. The Computa-
tional Linguistics special issue on semantic role labeling (Ma`rquez et al. 2008) includes
other interesting papers on the topic, leveraging the PropBank conventions for labeling
shallow semantic structures. Recently, there have been initiatives to predict syntactic
dependencies as well as PropBank-style predicate-argument structures together using
one joint model (Surdeanu et al. 2008; Hajic? et al. 2009).
Here, we focus on the related problem of frame-semantic parsing. Note from the
annotated semantic roles for the two verbs in the sentence of Figure 2(a) that it is
unclear what the core roles ARG1 or ARG2 represent linguistically. To better understand
the roles? meaning for a given verb, one has to refer to a verb-specific file provided along
with the PropBank corpus. Although collapsing these verb-specific core roles into tags
ARG0-ARG5 leads to a small set of classes to be learned from a reasonable sized corpus,
analysis shows that the roles ARG2?ARG5 serve many different purposes for different
verbs. Yi, Loper, and Palmer (2007) point out that these four roles are highly overloaded
and inconsistent, and they mapped them to VerbNet (Schuler 2005) thematic roles to
get improvements on the SRL task. Recently, Bauer and Rambow (2011) presented
a method to improve the syntactic subcategorization patterns for FrameNet lexical
units using VerbNet. Instead of working with PropBank, we focus on shallow semantic
parsing of sentences in the paradigm of frame semantics (Fillmore 1982), to which we
turn next.
2.2 Frame-Semantic Parsing
The FrameNet lexicon (Fillmore, Johnson, and Petruck 2003) contains rich linguistic
information about lexical items and predicate-argument structures. A semantic frame
present in this lexicon includes a list of lexical units, which are associated words
and phrases that can potentially evoke it in a natural language utterance. Each frame
in the lexicon also enumerates several roles corresponding to facets of the scenario
represented by the frame. In a frame-analyzed sentence, predicates evoking frames
are known as targets, and a word or phrase filling a role is known as an argument.
Figure 2(b) shows frame-semantic annotations for the same sentence as in Figure 2(a).
(In the figure, for example, the CARDINAL NUMBERS frame, ?M? denotes the role Multiplier
and ?E? denotes the role Entity.) Note that the verbs created and pushed evoke the frames
INTENTIONALLY CREATE and CAUSE CHANGE POSITION ON A SCALE, respectively. The correspond-
ing lexical units2 from the FrameNet lexicon, create.V and push.V, are also shown.
The PropBank analysis in Figure 2(a) also has annotations for these two verbs. While
PropBank labels the roles of these verbs with its limited set of tags, the frame-
semantic parse labels each frame?s arguments with frame-specific roles shown in the
figure, making it immediately clear what those arguments mean. For example, for the
INTENTIONALLY CREATE frame, more than 1.2 million jobs is the Created entity, and In that time is
the Time when the jobs were created. FrameNet also allows non-verbal words and phrases
to evoke semantic frames: in this sentence, million evokes the frame CARDINAL NUMBERS
and doubles as its Number argument, with 1.2 as Multiplier, jobs as the Entity being quantified,
and more than as the Precision of the quantity expression.
2 See Section 5.1 for a detailed description of lexical units.
13
Computational Linguistics Volume 40, Number 1
EVENT
Place
Time
Event
TRANSITIVE_ACTION
Agent
Patient
Event
Cause
Place
TimeOBJECTIVE_INFLUENCE
Dependent_entity
uencing_situation
Place
Time
uencing_entity
CAUSE_TO_MAKE_NOISE
Agent
Sound_maker
Cause
Place
Time
MAKE_NOISE
Noisy_event
Sound
Sound_source
Place
Time
cough.v, gobble.v, 
hiss.v, ring.v, yodel.v, ...
blare.v, honk.v, play.v, 
ring.v, toot.v, ...?
affect.v, effect.n, 
impact.n, impact.v, ...
event.n, happen.v, 
occur.v, take place.v, ...
Inheritance relation Causative_of relation
Excludes relation
Purpose
Figure 3
Partial illustration of frames, roles, and lexical units related to the CAUSE TO MAKE NOISE frame,
from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are
unfilled bars. No particular significance is ascribed to the ordering of a frame?s roles in its
lexicon entry (the selection and ordering of roles above is for illustrative convenience).
CAUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here.
Whereas PropBank contains verbal predicates and NomBank (Meyers et al. 2004) con-
tains nominal predicates, FrameNet counts these as well as allowing adjectives, adverbs,
and prepositions among its lexical units. Finally, FrameNet frames organize predicates
according to semantic principles, both by allowing related terms to evoke a common
frame (e.g., push.V, raise.V, and growth.N for CAUSE CHANGE POSITION ON A SCALE) and by
defining frames and their roles within a hierarchy (see Figure 3). PropBank does not
explicitly encode relationships among predicates.
Most early work on frame-semantic parsing has made use of the exemplar sentences
in the FrameNet corpus (see Section 3.1), each of which is annotated for a single frame
and its arguments. Gildea and Jurafsky (2002) presented a discriminative model for
arguments given the frame; Thompson, Levy, and Manning (2003) used a generative
model for both the frame and its arguments. Fleischman, Kwon, and Hovy (2003) first
used maximum entropy models to find and label arguments given the frame. Shi and
Mihalcea (2004) developed a rule-based system to predict frames and their arguments
in text, and Erk and Pado? (2006) introduced the Shalmaneser tool, which uses naive
Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti
2006, for instance) have used SVMs. Most of this work was done on an older, smaller
version of FrameNet, containing around 300 frames and fewer than 500 unique semantic
roles. Unlike this body of work, we experimented with the larger SemEval 2007 shared
task data set, and also the newer FrameNet 1.5,3 which lists 877 frames and 1,068 role
types?thus handling many more labels, and resulting in richer frame-semantic parses.
Recent work in frame-semantic parsing?in which sentences may contain multiple
frames which need to be recognized along with their arguments?was undertaken
as the SemEval 2007 task 19 of frame-semantic structure extraction (Baker, Ellsworth,
and Erk 2007). This task leveraged FrameNet 1.3, and also released a small corpus
3 Available at http://framenet.icsi.berkeley.edu as of 19 January 2013.
14
Das et al. Frame-Semantic Parsing
containing a little more than 2,000 sentences with full text annotations. The LTH system
of Johansson and Nugues (2007), which we use as our baseline (Section 3.4), had the
best performance in the SemEval 2007 task in terms of full frame-semantic parsing.
Johansson and Nugues broke down the task as identifying targets that could evoke
frames in a sentence, identifying the correct semantic frame for a target, and finally
determining the arguments that fill the semantic roles of a frame. They used a series
of SVMs to classify the frames for a given target, associating unseen lexical items to
frames and identifying and classifying token spans as various semantic roles. Both
the full text annotation corpus as well as the FrameNet exemplar sentences were
used to train their models. Unlike Johansson and Nugues, we use only the full text
annotated sentences as training data, model the whole problem with only two statis-
tical models, and obtain significantly better overall parsing scores. We also model the
argument identification problem using a joint structure prediction model and use semi-
supervised learning to improve predicate coverage. We also present experiments on
recently released FrameNet 1.5 data.
In other work based on FrameNet, Matsubayashi, Okazaki, and Tsujii (2009) in-
vestigated various uses of FrameNet?s taxonomic relations for learning generalizations
over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features
for the subtask of argument identification. Another line of work has sought to extend
the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea
2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries
and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado
and Lapata 2005; Fu?rstenau and Lapata 2009b). Others have explored the application
of frame-semantic structures to tasks such as information extraction (Moschitti,
Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt
and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu
2004; Shen and Lapata 2007), and paraphrase recognition (Pado? and Erk 2005).
2.3 Semi-Supervised Methods
Although there has been a significant amount of work in supervised shallow semantic
parsing using both PropBank- and FrameNet-style representations, a few improve-
ments over vanilla supervised methods using unlabeled data are notable. Fu?rstenau and
Lapata (2009b) present a method of projecting predicate-argument structures from some
seed examples to unlabeled sentences, and use a linear program formulation to find
the best alignment explaining the projection. Next, the projected information as well
as the seeds are used to train statistical model(s) for SRL. The authors ran experiments
using a set of randomly chosen verbs from the exemplar sentences of FrameNet and
found improvements over supervised methods. In an extension to this work, Fu?rstenau
and Lapata (2009a) present a method for finding examples for unseen verbs using a
graph alignment method; this method represents sentences and their syntactic analysis
as graphs and graph alignment is used to project annotations from seed examples to
unlabeled sentences. This alignment problem is again modeled as a linear program.
Fu?rstenau and Lapata (2012) present an detailed expansion of the aforementioned
papers. Although this line of work presents a novel direction in the area of SRL, the
published approach does not yet deal with non-verbal predicates and does not evaluate
the presented methods on the full text annotations of the FrameNet releases.
Deschacht and Moens (2009) present a technique of incorporating additional infor-
mation from unlabeled data by using a latent words language model. Latent variables
are used to model the underlying representation of words, and parameters of this model
15
Computational Linguistics Volume 40, Number 1
are estimated using standard unsupervised methods. Next, the latent information is
used as features for an SRL model. Improvements over supervised SRL techniques
are observed with the augmentation of these extra features. The authors also compare
their method with the aforementioned two methods of Fu?rstenau and Lapata (2009a,
2009b) and show relative improvements. Experiments are performed on the CoNLL
2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conven-
tions and only labels verbal and nominal predicates?in contrast to our work, which
includes most lexicosyntactic categories. A similar approach is presented by Weston,
Ratle, and Collobert (2008), who use neural embeddings of words, which are eventu-
ally used for SRL; improvements over state-of-the-art PropBank-style SRL systems are
observed.
Recently, there has been related work in unsupervised semantic role labeling (Lang
and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic
roles automatically from unannotated data. This line of work may be useful in discov-
ering new semantic frames and roles, but here we stick to the concrete representation
provided in FrameNet, without seeking to expand its inventory of semantic types. We
present a new semi-supervised technique to expand the set of lexical items with the
potential semantic frames that they could evoke; we use a graph-based semi-supervised
learning framework to achieve this goal (Section 5.5).
2.4 Joint Inference and Shallow Semantic Parsing
Most high-performance SRL systems that use conventions from PropBank (Kingsbury
and Palmer 2002) and NomBank (Meyers et al. 2004) utilize joint inference for seman-
tic role labeling (Ma`rquez et al. 2008). To our knowledge, the separate line of work
investigating frame-semantic parsing has not previously dealt with joint inference. A
common trait in prior work, both in PropBank and FrameNet conventions, has been
the use of a two-stage model that identifies arguments first, then labels them, often
using dynamic programming or integer linear programs (ILPs); we treat both problems
together here.4
Recent work in natural language processing (NLP) problems has focused on ILP for-
mulations for complex structure prediction tasks like dependency parsing (Riedel and
Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth
and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work
in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush
et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian
relaxation) as a way of exploiting the structure of the problem and existing combina-
torial algorithms. The method allows the combination of models that are individually
tractable, but not jointly tractable, by solving a relaxation of the original problem. Since
then, dual decomposition has been used to build more accurate models for dependency
parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing
(Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and
Macherey 2011; Rush and Collins 2011).
Recently, Martins et al. (2011b) showed that the success of subgradient-based dual
decomposition strongly relies on breaking down the original problem into a ?good?
4 In prior work, there are exceptions where identification and classification of arguments have been treated
in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on
semantic role labeling (Carreras and Ma`rquez 2004).
16
Das et al. Frame-Semantic Parsing
decomposition, that is, one with few overlapping components. This leaves out many
declarative constrained problems, for which such a good decomposition is not readily
available. For those, Martins et al. proposed the Alternating Directions Dual Decom-
position (AD3) algorithm, which retains the modularity of previous methods, but can
handle thousands of small overlapping components. We adopt that algorithm as it
perfectly suits the problem of argument identification, as we observe in Section 7.5 We
also contribute an exact branch-and-bound technique wrapped around AD3.
Before delving into the details of our modeling framework, we describe in detail the
structure of the FrameNet lexicon and the data sets used to train our models.
3. Resources and Task
We consider frame-semantic parsing resources consisting of a lexicon and annotated
sentences with frame-semantic structures, evaluation strategies, and previous baselines.
3.1 FrameNet Lexicon
The FrameNet lexicon is a taxonomy of manually identified general-purpose semantic
frames for English.6 Listed in the lexicon with each frame are a set of lemmas (with
parts of speech) that can denote the frame or some aspect of it?these are called lexical
units (LUs). In a sentence, word or phrase tokens that evoke a frame are known as
targets. The set of LUs listed for a frame in FrameNet may not be exhaustive; we may
see a target in new data that does not correspond to an LU for the frame it evokes.
Each frame definition also includes a set of frame elements, or roles, corresponding
to different aspects of the concept represented by the frame, such as participants,
props, and attributes. We use the term argument to refer to a sequence of word tokens
annotated as filling a frame role. Figure 1 shows an example sentence from the training
data with annotated targets, LUs, frames, and role-argument pairs. The FrameNet
lexicon also provides information about relations between frames and between roles
(e.g., INHERITANCE). Figure 3 shows a subset of the relations between five frames and
their roles.
Accompanying most frame definitions in the FrameNet lexicon is a set of lexico-
graphic exemplar sentences (primarily from the British National Corpus) annotated
for that frame. Typically chosen to illustrate variation in argument realization pat-
terns for the frame in question, these sentences only contain annotations for a single
frame.
In preliminary experiments, we found that using exemplar sentences directly to
train our models hurt performance as evaluated on SemEval 2007 data, which formed
a benchmark for comparison with previous state of the art. This was a noteworthy
observation, given that the number of exemplar sentences is an order of magnitude
larger than the number of sentences in training data that we consider in our experiments
(Section 3.2). This is presumably because the exemplars are not representative as a
sample, do not have complete annotations, and are not from a domain similar to the
5 AD3 was previously referred to as ?DD-ADMM,? in reference to the use of dual decomposition with the
alternating directions method of multipliers.
6 Like the SemEval 2007 participants, we used FrameNet 1.3 and also the newer version of the lexicon,
FrameNet 1.5 (http://framenet.icsi.berkeley.edu).
17
Computational Linguistics Volume 40, Number 1
Table 1
Salient statistics of the data sets used in our experiments. There is a significant overlap between
the two data sets.
SemEval 2007 Data FrameNet 1.5 Release
count count
Exemplar sentences 139,439 154,607
Frame labels (types) 665 877
Role labels (types) 720 1,068
Sentences in training data 2,198 3,256
Targets in training data 11,195 19,582
Sentences in test data 120 2,420
Targets in test data 1,059 4,458
Unseen targets in test data 210 144
test data. Instead, we make use of these exemplars in the construction of features
(Section 5.2).
3.2 Data
In our experiments on frame-semantic parsing, we use two sets of data:
1. SemEval 2007 data: In benchmark experiments for comparison with previous
state of the art, we use a data set that was released as part of the SemEval 2007
shared task on frame-semantic structure extraction (Baker, Ellsworth, and Erk 2007).
Full text annotations in this data set consisted of a few thousand sentences con-
taining multiple targets, each annotated with a frame and its arguments. The then-
current version of the lexicon (FrameNet 1.3) was used for the shared task as the
inventory of frames, roles, and lexical units (Figure 3 illustrates a small portion
of the lexicon). In addition to the frame hierarchy, FrameNet 1.3 also contained
139,439 exemplar sentences containing one target each. Statistics of the data used
for the SemEval 2007 shared task are given in the first column of Table 1. A total
of 665 frame types and 720 role types appear in the exemplars and the training
portion of the data. We adopted the same training and test split as the SemEval
2007 shared task; however, we removed four documents from the training set7 for
development. Table 2 shows some additional information about the SemEval data
set; the variety of lexicosyntactic categories of targets stands in marked contrast
with the PropBank-style SRL data and task.
2. FrameNet 1.5 release: A more recent version of the FrameNet lexicon was released
in 2010.8 We also test our statistical models (only frame identification and argu-
ment identification) on this data set to get an estimate of how much improvement
additional data can provide. Details of this data set are shown in the second col-
umn of Table 1. Of the 78 documents in this release with full text annotations, we
selected 55 (19,582 targets) for training and held out the remaining 23 (4,458 tar-
gets) for testing. There are fewer target annotations per sentence in the test set than
7 These were: StephanopoulousCrimes, Iran Biological, NorthKorea Introduction, and WMDNews 042106.
8 Released on 15 September 2010, and downloadable from http://framenet.icsi.berkeley.edu as of
13 February 2013. In our experiments, we used a version downloaded on 22 September 2010.
18
Das et al. Frame-Semantic Parsing
Table 2
Breakdown of targets and arguments in the SemEval 2007 training set in terms of part of speech.
The target POS is based on the LU annotation for the frame instance. For arguments, this reflects
the part of speech of the head word (estimated from an automatic dependency parse); the
percentage is out of all overt arguments.
targets arguments
count % count %
Noun 5,155 52 Noun 9,439 55
Verb 2,785 28 Preposition or
complementizerAdjective 1,411 14 2,553 15
Preposition 296 3 Adjective 1,744 10
Adverb 103 1 Verb 1,156 7
Number 63 1 Pronoun 736 4
Conjunction 8 Adverb 373 2
Article 3 Other 1,047 6
9,824 17,048
the training set.9 Das and Smith (2011, supplementary material) give the names
of the test documents for fair replication of our work. We also randomly selected
4,462 targets from the training data for development of the argument identification
model (Section 6.1).
Preprocessing. We preprocessed sentences in our data set with a standard set of anno-
tations: POS tags from MXPOST (Ratnaparkhi 1996) and dependency parses from the
MST parser (McDonald, Crammer, and Pereira 2005); manual syntactic parses are not
available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum
1998) for lemmatization. Our models treat these pieces of information as observations.
We also labeled each verb in the data as having ACTIVE or PASSIVE voice, using code
from the SRL system described by Johansson and Nugues (2008).
3.3 Task and Evaluation Methodology
Automatic annotations of frame-semantic structure can be broken into three parts:
(1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the
lexicon, evoked by each target; and (3) the arguments, or spans of words that serve
to fill roles defined by each evoked frame. These correspond to the three subtasks
in our parser, each described and evaluated in turn: target identification (Section 4),
frame identification (Section 5, not unlike word-sense disambiguation), and argument
identification (Section 6, essentially the same as semantic role labeling).
The standard evaluation script from the SemEval 2007 shared task calculates pre-
cision, recall, and F1-measure for frames and arguments; it also provides a score that
gives partial credit for hypothesizing a frame related to the correct one. We present
9 For creating the splits, we first included the documents that had incomplete annotations as mentioned in
the initial FrameNet 1.5 data release in the test set; because we do not evaluate target identification for
this version of data, the small number of targets per sentence does not matter. After these documents
were put into the test set, we randomly selected 55 remaining documents for training, and picked the
rest for additional testing. The final test set contains a total of 23 documents. When these documents
are annotated in their entirety, the test set will become larger and the training set will be unaltered.
19
Computational Linguistics Volume 40, Number 1
precision, recall, and F1-measure microaveraged across the test documents, report labels-
only matching scores (spans must match exactly), and do not use named entity labels.10
More details can be found in the task description paper from SemEval 2007 (Baker,
Ellsworth, and Erk 2007). For our experiments, statistical significance is measured using
a reimplementation of Dan Bikel?s randomized parsing evaluation comparator, a strat-
ified shuffling test whose original implementation11 is accompanied by the following
description (quoted verbatim, with explanations of our use of the test given in square
brackets):
The null hypothesis is that the two models that produced the observed results are the
same, such that for each test instance [here, a set of predicate-argument structures for a
sentence], the two observed scores are equally likely. This null hypothesis is tested by
randomly shuffling individual sentences? scores between the two models and then
re-computing the evaluation metrics [precision, recall or F1 score in our case]. If the
difference in a particular metric after a shuffling is equal to or greater than the original
observed difference in that metric, then a counter for that metric is incremented. Ideally,
one would perform all 2n shuffles, where n is the number of test cases (sentences), but
given that this is often prohibitively expensive, the default number of iterations is
10,000 [we use independently sampled 10,000 shuffles]. After all iterations, the
likelihood of incorrectly rejecting the null [hypothesis, i.e., the p-value] is simply
(nc + 1)/(nt + 1), where nc is the number of random differences greater than the
original observed difference, and nt is the total number of iterations.
3.4 Baseline
A strong baseline for frame-semantic parsing is the system presented by Johansson and
Nugues (2007, hereafter J&N?07), the best system in the SemEval 2007 shared task. That
system is based on a collection of SVMs. They used a set of rules for target identification
which we describe in Appendix A. For frame identification, they used an SVM classifier
to disambiguate frames for known frame-evoking words. They used WordNet synsets
to extend the vocabulary of frame-evoking words to cover unknown words, and then
used a collection of separate SVM classifiers?one for each frame?to predict a single
evoked frame for each occurrence of a word in the extended set.
J&N?07 followed Xue and Palmer (2004) in dividing the argument identification
problem into two subtasks: First, they classified candidate spans as to whether they
were arguments or not; then they assigned roles to those that were identified as ar-
guments. Both phases used SVMs. Thus, their formulation of the problem involves
a multitude of independently trained classifiers that share no information?whereas
ours uses two log-linear models, each with a single set of parameters shared across all
contexts, to find a full frame-semantic parse.
We compare our models with J&N?07 using the benchmark data set from SemEval
2007. However, because we are not aware of any other work using the FrameNet 1.5 full
text annotations, we report our results on that data set without comparison to any other
system.
10 For microaveraging, we concatenated all sentences of the test documents and measured precision and
recall over the concatenation. Macroaveraging, on the other hand, would mean calculating these metrics
for each document, then averaging them. Microaveraging treats every frame or argument as a unit,
regardless of the length of the document in which it occurs.
11 See http://www.cis.upenn.edu/dbikel/software.html#comparator.
20
Das et al. Frame-Semantic Parsing
4. Target Identification
Target identification is the problem of deciding which word tokens (or word token
sequences) evoke frames in a given sentence. In other semantic role labeling schemes
(e.g., PropBank), simple part-of-speech criteria typically distinguish targets from non-
targets. But in frame semantics, verbs, nouns, adjectives, and even prepositions can
evoke frames under certain conditions. One complication is that semantically impov-
erished support predicates (such as make in make a request) do not evoke frames in the
context of a frame-evoking, syntactically dependent noun (request). Furthermore, only
temporal, locative, and directional senses of prepositions evoke frames.12
Preliminary experiments using a statistical method for target identification gave
unsatisfactory results; instead, we followed J&N?07 in using a small set of rules to
identify targets. First, we created a master list of all the morphological variants of
targets that appear in the exemplar sentences and a given training set. For a sentence in
new data, we considered as candidate targets only those substrings that appear in this
master list. We also did not attempt to capture discontinuous frame targets: for example,
we treat there would have been as a single span even though the corresponding LU is
there be.V.13
Next, we pruned the candidate target set by applying a series of rules identical
to the ones described by Johansson and Nugues (2007, see Appendix A), with two
exceptions. First, they identified locative, temporal, and directional prepositions using
a dependency parser so as to retain them as valid LUs. In contrast, we pruned all types
of prepositions because we found them to hurt our performance on the development
set due to errors in syntactic parsing. In a second departure from their target extraction
rules, we did not remove the candidate targets that had been tagged as support verbs
for some other target. Note that we used a conservative white list that filters out targets
whose morphological variants were not seen either in the lexicon or the training data.14
Therefore, when this conservative process of automatic target identification is used, our
system loses the capability to predict frames for completely unseen LUs, despite the fact
that our powerful frame identification model (Section 5) can accurately label frames for
new LUs.15
Results. Table 3 shows results on target identification tested on the SemEval 2007 test
set; our system gains 3 F1 points over the baseline. This is statistically significant with
p < 0.01. Our results are also significant in terms of precision (p < 0.05) and recall (p <
0.01). There are 85 distinct LUs for which the baseline fails to identify the correct target
while our system succeeds. A considerable proportion of these units have more than
12 Note that there have been dedicated shared tasks to determine relationships between nominals (Girju
et al. 2007) and word-sense disambiguation of prepositions (Litkowski and Hargraves 2007), but we do
not build specific models for predicates of these categories.
13 There are 629 multiword LUs in the lexicon, and they correspond to 4.8% of the targets in the training
set; among them are screw up.V, shoot the breeze.V, and weapon of mass destruction.N. In the SemEval 2007
training data, there are just 99 discontinuous multiword targets (1% of all targets).
14 This conservative approach violates theoretical linguistic assumptions about frame-evoking targets as
governed by frame semantics. It also goes against the spirit of using linguistic constraints to improve
the separate subtask of argument identification (see Section 7); however, due to varying distributions
of target annotations, high empirical error in identifying locative, temporal, and directional prepositions,
and support verbs, we resorted to this aggressive filtering heuristic to avoid making too many target
identification mistakes.
15 To predict frames and roles for new and unseen LUs, SEMAFOR provides the user with an option to
mark those LUs in the input.
21
Computational Linguistics Volume 40, Number 1
Table 3
Target identification results for our system and the baseline on the SemEval?07 data set. Scores in
bold denote significant improvements over the baseline (p < 0.05).
TARGET IDENTIFICATION P R F1
Our technique (?4) 89.92 70.79 79.21
Baseline: J&N?07 87.87 67.11 76.10
one token (e.g., chemical and biological weapon.N, ballistic missile.N), which J&N?07 do not
model. The baseline also does not label variants of there be.V (e.g., there are and there has
been), which we correctly label as targets. Some examples of other single token LUs that
the baseline fails to identify are names of months, LUs that belong to the ORIGIN frame
(e.g., iranian.A), and directions (e.g., north.A or north-south.A).16
5. Frame Identification
Given targets, our parser next identifies their frames, using a statistical model.
5.1 Lexical Units
FrameNet specifies a great deal of structural information both within and among
frames. For frame identification we make use of frame-evoking lexical units, the (lem-
matized and POS-tagged) words and phrases listed in the lexicon as referring to specific
frames. For example, listed with the BRAGGING frame are 10 LUs, including boast.N,
boast.V, boastful.A, brag.V, and braggart.N. Of course, due to polysemy and homonymy,
the same LU may be associated with multiple frames; for example, gobble.V is listed
under both the INGESTION and MAKE NOISE frames. We thus term gobble.V an ambiguous
LU. All targets in the exemplar sentences, our training data, and most in our test data,
correspond to known LUs. (See Section 5.4 for statistics of unknown LUs in the test sets.)
To incorporate frame-evoking expressions found in the training data but not the
lexicon?and to avoid the possibility of lemmatization errors?our frame identification
model will incorporate, via a latent variable, features based directly on exemplar and
training targets rather than LUs. Let L be the set of (unlemmatized and automati-
cally POS-tagged) targets found in the exemplar sentences of the lexicon and/or the
sentences in our training set. Let Lf ? L be the subset of these targets annotated as
evoking a particular frame f .17 Let Ll and Llf denote the lemmatized versions of L and
Lf , respectively. Then, we write boasted.VBD ? LBRAGGING and boast.VBD ? LlBRAGGINGto
indicate that this inflected verb boasted and its lemma boast have been seen to evoke the
BRAGGING frame. Significantly, however, another target, such as toot your own horn, might
be used elsewhere to evoke this frame. We thus face the additional hurdle of predicting
frames for unknown words.
16 We do not evaluate the target identification module on the FrameNet 1.5 data set; we instead ran
controlled experiments on those data to measure performance of the statistical frame identification and
argument identification subtasks, assuming that the correct targets were given. Moreover, as discussed
in Section 3.2, the target annotations on the FrameNet 1.5 test set were fewer in number in comparison
to the training set, resulting in a mismatch of target distributions between train and test settings.
17 For example, on average, there are 34 targets per frame in the SemEval 2007 data set; the average frame
ambiguity of each target in L is 1.17.
22
Das et al. Frame-Semantic Parsing
In producing full text annotations for the SemEval 2007 data set, annotators created
several domain-critical frames that were not already present in version 1.3 of the lexicon.
For our experiments we omit frames attested in neither the training data nor the exem-
plar sentences from the lexicon.18 This leaves a total of 665 frames for the SemEval 2007
data set and a total of 877 frames for the FrameNet 1.5 data set.
5.2 Model
For a given sentence x with frame-evoking targets t, let ti denote the ith target (a word
sequence).19 Let tli denote its lemma. We seek a list f = ?f1, . . . , fm? of frames, one per
target. In our model, the set of candidate frames for ti is defined to include every frame
f such that tli ? Llf ?or if tli ? Ll, then every known frame (the latter condition applies
for 4.7% of the annotated targets in the SemEval 2007 development set). In both cases,
we let Fi be the set of candidate frames for the ith target in x. We denote the entire set
of frames in the lexicon as F .
To allow frame identification for targets whose lemmas were seen in neither the
exemplars nor the training data, our model includes an additional variable, i. This
variable ranges over the seen targets in Lfi , which can be thought of as prototypes
for the expression of the frame. Importantly, frames are predicted, but prototypes are
summed over via the latent variable. The prediction rule requires a probabilistic model
over frames for a target:
fi ? argmax
f?Fi
?
?Lf
p?(f,  | ti, x) (1)
We model the probability of a frame f and the prototype unit , given the target and the
sentence x as:
p?(f,  | ti, x) =
exp?g(f, , ti, x)
?
f ??F
?
??Lf?
exp?g(f ?, ?, ti, x)
(2)
This is a conditional log-linear model: for f ? Fi and  ? Lf , where ? are the model
weights, and g is a vector-valued feature function. This discriminative formulation is
very flexible, allowing for a variety of (possibly overlapping) features; for example, a
feature might relate a frame type to a prototype, represent a lexical-semantic relation-
ship between a prototype and a target, or encode part of the syntax of the sentence.
Previous work has exploited WordNet for better coverage during frame identifica-
tion (Burchardt, Erk, and Frank 2005; Johansson and Nugues 2007, e.g., by expanding
the set of targets using synsets), and others have sought to extend the lexicon itself.
We differ in our use of a latent variable to incorporate lexical-semantic features in a
discriminative model, relating known lexical units to unknown words that may evoke
frames. Here we are able to take advantage of the large inventory of partially annotated
18 Automatically predicting new frames is a challenge not yet attempted to our knowledge (including here).
Note that the scoring metric (Section 3.3) gives partial credit for related frames (e.g., a more general frame
from the lexicon).
19 Here each ti is a word sequence ?wu, . . . , wv?, 1 ? u ? v ? n, though in principle targets can be
noncontiguous.
23
Computational Linguistics Volume 40, Number 1
Table 4
Features used for frame identification (Equation (2)). All also incorporate f , the frame being
scored.  = ?w,?? consists of the words and POS tags20 of a target seen in an exemplar or
training sentence as evoking f . The features with starred bullets were also used by Johansson
and Nugues (2007).
? the POS of the parent of the head word of ti
?? the set of syntactic dependencies of the head word21 of ti
?? if the head word of ti is a verb, then the set of dependency labels of its children
? the dependency label on the edge connecting the head of ti and its parent
? the sequence of words in the prototype, w
? the lemmatized sequence of words in the prototype
? the lemmatized sequence of words in the prototype and their part-of-speech tags ?
? WordNet relation22 ? holds between  and ti
? WordNet relation22 ? holds between  and ti, and the prototype is 
? WordNet relation22 ? holds between  and ti, the POS tag sequence of  is ?, and the POS
tag sequence of ti is ?t
exemplar sentences. Note that this model makes an independence assumption: Each
frame is predicted independently of all others in the document. In this way the model
is similar to J&N?07. However, ours is a single conditional model that shares features
and weights across all targets, frames, and prototypes, whereas the approach of J&N?07
consists of many separately trained models. Moreover, our model is unique in that it
uses a latent variable to smooth over frames for unknown or ambiguous LUs.
Frame identification features depend on the preprocessed sentence x, the prototype
 and its WordNet lexical-semantic relationship with the target ti, and of course the
frame f . Our model uses binary features, which are detailed in Table 4.
5.3 Parameter Estimation
Given a training data set (either SemEval 2007 data set or the FrameNet 1.5 full text
annotations), which is of the form ??x(j), t(j), f(j),A(j)??Nj=1, we discriminatively train the
frame identification model by maximizing the training data log-likelihood:23
max
?
N
?
j=1
mj
?
i=1
log
?
?L
f ( j)i
p?( f
(j)
i ,  | t
(j)
i , x
(j) ) (3)
In Equation (3), mj denotes the number of frames in a sentence indexed by j. Note
that the training problem is non-convex because of the summed-out prototype latent
20 POS tags are found automatically during preprocessing.
21 If the target is not a subtree in the parse, we consider the words that have parents outside the span,
and apply three heuristic rules to select the head: (1) choose the first word if it is a verb; (2) choose the
last word if the first word is an adjective; (3) if the target contains the word of, and the first word is a
noun, we choose it. If none of these hold, choose the last word with an external parent to be the head.
22 These are: IDENTICAL-WORD, SYNONYM, ANTONYM (including extended and indirect antonyms),
HYPERNYM, HYPONYM, DERIVED FORM, MORPHOLOGICAL VARIANT (e.g., plural form), VERB
GROUP, ENTAILMENT, ENTAILED-BY, SEE-ALSO, CAUSAL RELATION, and NO RELATION.
23 We found no benefit on either development data set from using an L2 regularizer (zero-mean
Gaussian prior).
24
Das et al. Frame-Semantic Parsing
Table 5
Frame identification results on both the SemEval 2007 data set and the FrameNet 1.5 release.
Precision, recall, and F1 were evaluated under exact and partial frame matching; see Section 3.3.
Bold indicates best results on the SemEval 2007 data, which are also statistically significant with
respect to the baseline (p < 0.05).
FRAME IDENTIFICATION (?5.2) exact matching partial matchingP R F1 P R F1
SemEval 2007 Data
gold targets 60.21 60.21 60.21 74.21 74.21 74.21
automatic targets (?4) 69.75 54.91 61.44 77.51 61.03 68.29
J&N?07 targets 65.34 49.91 56.59 74.30 56.74 64.34
Baseline: J&N?07 66.22 50.57 57.34 73.86 56.41 63.97
FrameNet 1.5 Release
gold targets 82.97 82.97 82.97 90.51 90.51 90.51
? unsupported features 80.30 80.30 80.30 88.91 88.91 88.91
& ? latent variable 75.54 75.54 75.54 85.92 85.92 85.92
variable  for each frame. To calculate the objective function, we need to cope with a
sum over frames and prototypes for each target (see Equation (2)), often an expensive
operation. We locally optimize the function using a distributed implementation of L-
BFGS.24 This is the most expensive model that we train: With 100 parallelized CPUs
using MapReduce (Dean and Ghemawat 2008), training takes several hours.25 Decoding
takes only a few minutes on one CPU for the test set.
5.4 Supervised Results
SemEval 2007 Data. On the SemEval 2007 data set, we evaluate the performance of
our frame identification model given gold-standard targets and automatically identified
targets (Section 4); see Table 5. Together, our target and frame identification outperform
the baseline by 4 F1 points. To compare the frame identification stage in isolation with
that of J&N?07, we ran our frame identification model with the targets identified by their
system as input. With partial matching, our model achieves a relative improvement of
0.6% F1 over J&N?07, as shown in the third row of Table 5 (though this is not significant).
Note that for exact matching, the F1 score of the automatic targets setting is better than
the gold target setting. This is due to the fact that there are many unseen predicates in
the test set on which the frame identification model performs poorly; however, for the
automatic targets that are mostly seen in the lexicon and training data, the model gets
high precision, resulting in better overall F1 score.
Our frame identification model thus performs on par with the previous state of the
art for this task, and offers several advantages over J&N?s formulation of the problem:
It requires only a single model, learns lexical-semantic features as part of that model
rather than requiring a preprocessing step to expand the vocabulary of frame-evoking
words, and is probabilistic, which can facilitate global reasoning.
24 We do not experiment with the initialization of model parameters during this non-convex optimization
process; all parameters are initialized to 0.0 before running the optimizer. However, in future work,
experiments can be conducted with different random initialization points to seek non-local optima.
25 In later experiments, we used another implementation with 128 parallel cores in a multi-core MPI setup
(Gropp, Lusk, and Skjellum 1994), where training took several hours.
25
Computational Linguistics Volume 40, Number 1
In the SemEval 2007 data set, for gold-standard targets, 210 out of 1,059 lemmas
were not present in the white list that we used for target identification (see Section 4).
Our model correctly identifies the frames for 4 of these 210 lemmas. For 44 of these
lemmas, the evaluation script assigns a score of 0.5 or more, suggesting that our model
predicts a closely related frame. Finally, for 190 of the 210 lemmas, a positive score is
assigned by the evaluation script. This suggests that the hidden variable model helps
in identifying related (but rarely exact) frames for unseen targets, and explains why
under exact?but not partial?frame matching, the F1 score using automatic targets is
commensurate with the score for oracle targets.26
For automatically identified targets, the F1 score falls because the model fails
to predict frames for unseen lemmas. However, our model outperforms J&N?07 by
4 F1 points. The partial frame matching F1 score of our model represents a significant
improvement over the baseline (p < 0.01). The precision and recall measures are
significant as well (p < 0.05 and p < 0.01, respectively). However, because targets
identified by J&N?07 and frames classified by our frame identification model resulted
in scores on par with the baseline, we note that the significant results follow due to
better target identification. Note from the results that the automatic target identification
model shows an increase in precision, at the expense of recall. This is because the white
list for target identification restricts the model to predict frames only for known LUs.
If we label the subset of test set with already seen LUs (seen only in the training set,
excluding the exemplars) with their corresponding most frequent frame, we achieve
an exact match accuracy between 52.9% and 91.2%, depending on the accuracy of the
unseen LUs (these bounds assume, respectively, that they are all incorrectly labeled or
all correctly labeled).
FrameNet 1.5 Release. The bottom three rows of Table 5 show results on the full text
annotation test set of the FrameNet 1.5 release. Because the number of annotations
nearly doubled, we see large improvements in frame identification accuracy. Note that
we only evaluate with gold targets as input to frame identification. (As mentioned in
Section 3.2, some documents in the test set have not been annotated for all targets, so
evaluating automatic target identification would not be informative.) We found that
50.1% of the targets in the test set were ambiguous (i.e., they associated with more than
one frame either in FrameNet or our training data). On these targets, the exact frame
identification accuracy is 73.10% and the partial accuracy is 85.77%, which indicates that
the frame identification model is robust to target ambiguity. On this data set, the most
frequent frame baseline achieves an exact match accuracy between 74.0% and 88.1%,
depending on the accuracy of the unseen LUs.
We conducted further experiments with ablation of the latent variable in our frame
identification model. Recall that the decoding objective used to choose a frame by
marginalizing over a latent variable , whose values range over targets known to
associate with the frame f being considered (see Equations (1) and (2)) in training. How
much do the prototypes, captured by the latent variable, contribute to performance?
Instead of treating  as a marginalized latent variable, we can fix its value to the observed
target.
26 J&N?07 did not report frame identification results for oracle targets; thus directly comparing the frame
identification models is difficult.
26
Das et al. Frame-Semantic Parsing
An immediate effect of this choice is a blow-up in the number of features; we
must instantiate features (see Table 4) for all 4,194 unique targets observed in training.
Because each of these features needs to be associated with all 877 frames in the partition
function of Equation (2), the result is an 80-fold blowup of the feature space (the latent
variable model had 465,317 features). Such a model is not computationally feasible in
our engineering framework, so we considered a model using only features observed to
fire at some point in the training data (called ?supported? features),27 resulting in only
72,058 supported features. In Table 5, we see a significant performance drop (on both
exact and partial matching accuracy) with this latent variable?free model, compared
both with our latent variable model with all features and with only supported features
(of which there are 165,200). This establishes that the latent variable in our frame
identification model helps in terms of accuracy, and lets us use a moderately sized
feature set incorporating helpful unsupported features.
Finally, in our test set, we found that 144 out of the 4,458 annotated targets were un-
seen, and our full frame identification model only labeled 23.1% of the frames correctly
for those unseen targets; in terms of partial match accuracy, the model achieved a score
of 46.6%. This, along with the results on the SemEval 2007 unseen targets, shows that
there is substantial opportunity for improvement when unseen targets are presented to
the system. We address this issue next.
5.5 Semi-Supervised Lexicon Expansion
We next address the poor performance of our frame identification model on targets that
were unseen as LUs in FrameNet or as instances in training data, and briefly describe
a technique for expanding the set of lexical units with potential semantic frames that
they can associate with. These experiments were carried out on the FrameNet 1.5 data
only. We use a semi-supervised learning (SSL) technique that uses a graph constructed
from labeled and unlabeled data. The widely used graph-based SSL framework?see
Bengio, Delalleau, and Le Roux (2006) and Zhu (2008) for introductory material on this
topic?has been shown to perform better than several other semi-supervised algorithms
on benchmark data sets (Chapelle, Scho?lkopf, and Zien 2006, chapter 21). The method
constructs a graph where a small portion of vertices correspond to labeled instances,
and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting
the similarity between the pair. Traditionally, Markov random walks (Szummer and
Jaakkola 2001; Baluja et al. 2008) or optimization of a loss function based on smoothness
properties of the graph (e.g., Corduneanu and Jaakkola 2003; Zhu, Ghahramani, and
Lafferty 2003; Subramanya and Bilmes 2008) are performed to propagate labels from
the labeled vertices to the unlabeled ones. In our work, we are interested in multi-class
generalizations of graph-propagation algorithms suitable for NLP applications, where
each graph vertex can assume one or more out of many possible labels (Subramanya and
Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to
natural language types (not tokens) and undirected edges between them are weighted
using a similarity metric. Recently, this set-up has been used to learn soft labels on
natural language types (say, word n-grams or in our case, syntactically disambiguated
27 The use of unsupported features (i.e., those that can fire for an analysis in the partition function but not
observed to fire in the training data) has been observed to give performance improvements in NLP
problems; see, for example, Sha and Pereira (2003) and Martins et al. (2010).
27
Computational Linguistics Volume 40, Number 1
predicates) from seed data, resulting in large but noisy lexicons, which are used to
constrain structured prediction models. Applications have ranged from domain adap-
tation of sequence models (Subramanya, Petrov, and Pereira 2010) to unsupervised
learning of POS taggers by using bilingual graph-based projections (Das and Petrov
2011).
We describe our approach to graph construction, propagation for lexicon expansion,
and the use of the result to impose constraints on frame identification.
5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each
vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag.
We use two resources for graph construction. First, we take all the words and phrases
present in a dependency-based thesaurus constructed using syntactic cooccurrence
statistics (Lin 1998), and aggregate words and phrases that share the same lemma and
coarse POS tag. To construct this resource, Lin used a corpus containing 64 million
words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic
contexts were used to find similar lexical items for a given word or phrase. Lin sepa-
rately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the
thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs
present in FrameNet data.
The second component of graph construction comes from FrameNet itself. We
scanned the exemplar sentences in FrameNet 1.5 and the training section of the full
text annotations and gathered a distribution over frames for each LU appearing in
FrameNet data. For a pair of LUs, we measured the Euclidean distance between their
frame distributions. This distance was next converted to a similarity score and inter-
polated with the similarity score from Lin?s dependency thesaurus. We omit further
details about the interpolation and refer the reader to full details given in Das and Smith
(2011).
For each LU, we create a vertex and link it to the K nearest neighbor LUs under the
interpolated similarity metric. The resulting graph has 64,480 vertices, 9,263 of which
are labeled seeds from FrameNet 1.5 and 55,217 of which are unlabeled. Each vertex has
a possible set of labels corresponding to the 877 frames defined in the lexicon. Figure 4
shows an excerpt from the constructed graph.
Figure 4
Excerpt from our constructed graph over LUs. Green LUs are observed in the FrameNet 1.5 data.
Above/below them are shown the most frequently observed frame that these LUs associate
with. The black LUs are unobserved and graph propagation produces a distribution over most
likely frames that they could evoke as target instances.
28
Das et al. Frame-Semantic Parsing
5.5.2 Propagation by Optimization. Once the graph is constructed, the 9,263 seed ver-
tices with supervised frame distributions are used to propagate the semantic frame
information via their nearest neighbors to all vertices. Here we discuss two graph-
based SSL objective functions. Das and Smith (2012) compare several other graph-based
SSL algorithms for this problem; we refer the interested reader to that paper. Let V
denote the set of all vertices in our graph, V? ? V be the set of seed vertices, and F
denote the set of all frames. Let N (v) denote the set of neighbors of vertex v ? V. Let
q = {q1, q2, . . . , q|V|} be the set of frame distributions, one per vertex. For each seed
vertex v ? V?, we have a supervised frame distribution q?v. All edges in the graph are
weighted according to the aforementioned interpolated similarity score, denoted wuv
for the edge adjacent to vertices u and v. We find q by solving:
NGF-2 : arg min
q, s.t. q?0,
?v?V,?qv?1=1
?
v?V?
?q?v ? qv?22 + ?
?
v?V,u?N (v)
wuv?qv ? qu?22 + ?
?
v?V
?qv ? 1|F|?22
(4)
We call the objective in Equation (4) NGF-2 because it uses normalized probability dis-
tributions at each vertex and is a Gaussian field; it also utilizes a uniform 2 penalty?the
third term in the objective function. This is a multiclass generalization of the quadratic
cost criterion (Bengio, Delalleau, and Le Roux 2006), also used by Subramanya, Petrov,
and Pereira (2010) and Das and Petrov (2011). Our second graph objective function is as
follows:
UJSF-1,2 : arg min
q, s.t. q?0
?
v?V?
DJS(q?v?qv) + ?
?
v?V,u?N (v)
wuvDJS(qv?qu) + ?
?
v?V
?qv?21 (5)
We call it UJSF-1,2 because it uses unnormalized probability measures at each vertex
and is a Jensen-Shannon field, utilizing pairwise Jensen-Shannon divergences (Lin 1991;
Burbea and Rao 2006) and a sparse 1,2 penalty (Kowalski and Torre?sani 2009) as the
third term. Das and Smith (2012) proposed the objective function in Equation (5). It seeks
at each graph vertex a sparse measure, as we expect in a lexicon (i.e., few frames have
nonzero probability for a given target). These two graph objectives can be optimized
by iterative updates, whose details we omit in this article; more information about the
motivation behind using the 1,2 penalty in the UJSF-1,2 objective, the optimization
procedure, and an empirical comparison of these and other objectives on another NLP
task can be found in Das and Smith (2012).
5.5.3 Constraints for Frame Identification. Once a graph-based SSL objective function is
minimized, we arrive at the optimal set of frame distributions q?, which we use to
constrain our frame identification inference rule, expressed in Equation (1). In that
rule, ti is the ith target in a sentence x, and fi is the corresponding evoked frame. We
now add a constraint to that rule. Recall from Section 5.2 that for targets with known
lemmatized forms, Fi was defined to be the set of frames that associate with lemma tli
in the supervised data. For unknown lemmas, Fi was defined to be all the frames in the
lexicon. If the LU corresponding to ti is present in the graph, let it be the vertex vi. For
such targets ti covered by the graph, we redefine Fi as:
Fi = {f : f ? M-best frames under q?vi} (6)
29
Computational Linguistics Volume 40, Number 1
Table 6
Exact and partial frame identification accuracy on the FrameNet 1.5 data set with the size of
lexicon (in terms of non-zero frame components in the truncated frame distributions) used for
frame identification, given gold targets. The supervised model is compared with alternatives in
Table 5. Bold indicates best results. UJSF-1,2 produces statistically significant results (p < 0.001)
for all metrics with respect to the supervised baseline for both the unseen LUs as well as the
whole test set. Although the NGF-2 and UJSF-1,2 models are statistically indistinguishable,
it is noteworthy that the UJSF-1,2 objective produces a much smaller lexicon.
UNKNOWN TARGETS ALL TARGETS Graph
exact partial exact partial Lexicon
frame matching frame matching frame matching frame matching Size
Supervised 23.08 46.62 82.97 90.51 ?
Self-training 18.88 42.67 82.27 90.02 ?
NGF-2 39.86 62.35 83.51 91.02 128,960
UJSF-1,2 42.67 65.29 83.60 91.12 45,544
For targets ti in test data whose LUs are not present in the graph (and hence in
supervised data), Fi is the set of all frames. Note that in this semi-supervised extension
of our frame identification inference procedure, we introduced several hyperparam-
eters, namely, ?, ?, K (the number of nearest neighbors for each vertex included in
the graph) and M (the number of highest scoring frames per vertex according to the
induced frame distribution). We choose these hyperparameters using cross-validation
by tuning the frame identification accuracy on unseen targets. (Different values of the
first three hyperparameters were chosen for the different graph objectives and we omit
their values here for brevity; M turned out to be 2 for all models.)
Table 6 shows frame identification accuracy, both using exact match as well as
partial match. Performance is shown on the portion of the test set containing unknown
LUs, as well as the whole test set. The final column presents lexicon size in terms
of the set of truncated frame distributions (filtered according to the top M frames in
qv for a vertex v) for all the LUs in a graph. For comparison with a semi-supervised
baseline, we consider a self-trained system. For this system, we used the supervised
frame identification system to label 70,000 sentences from the English Gigaword corpus
with frame-semantic parses. For finding targets in a raw sentence, we used a relaxed
target identification scheme, where we marked as potential frame-evoking units all
targets seen in the lexicon and all other words which were not prepositions, particles,
proper nouns, foreign words, or WH-words. We appended these automatic annotations
to the training data, resulting in 711,401 frame annotations, more than 36 times the
annotated data. These data were next used to train a frame identification model.28 This
set-up is very similar to that of Bejan (2009) who used self-training to improve frame
identification. In our setting, however, self-training hurts relative to the fully supervised
approach (Table 6).
Note that for the unknown part of the test set the graph-based objectives outperform
both the supervised model as well as the self-training baseline by a margin of ?20%
28 We ran self-training with smaller amounts of data, but found no significant difference with the results
achieved with 711,401 frame annotations. As we observe in Table 6, in our case, self-training performs
worse than the supervised model, and we do not hope to improve with even more data.
30
Das et al. Frame-Semantic Parsing
absolute. The best model is UJSF-1,2, and its performance is significantly better than
the supervised model (p < 0.01). It also produces a smaller lexicon (using the sparsity-
inducing penalty) than NGF-2, requiring less memory during frame identification
inference. The small footprint can be attributed to the removal of LUs for which all
frame components were zero (qi = 0). The improvements of the graph-based objectives
over the supervised and the self-trained models are modest for the whole test set,
but the best model still has statistically significant improvements over the supervised
model (p < 0.01).
6. Argument Identification
Given a sentence x = ?x1, . . . , xn?, the set of targets t = ?t1, . . . , tm?, and a list of evoked
frames f = ?f1, . . . , fm? corresponding to each target, argument identification is the task
of choosing which of each fi?s roles are filled, and by which parts of x. This task is most
similar to the problem of semantic role labeling, but uses a richer set of frame-specific
labels than PropBank annotations.
6.1 Model
Let Rfi = {r1, . . . , r|Rfi |} denote frame fi?s roles (named frame element types) observed
in an exemplar sentence and/or our training set. A subset of each frame?s roles are
marked as core roles; these roles are conceptually and/or syntactically necessary for
any given use of the frame, though they need not be overt in every sentence involving
the frame. These are roughly analogous to the core arguments ARG0?ARG5 in PropBank.
Non-core roles?analogous to the various ARGM-* in PropBank?loosely correspond to
syntactic adjuncts, and carry broadly applicable information such as the time, place,
or purpose of an event. The lexicon imposes some additional structure on roles, in-
cluding relations to other roles in the same or related frames, and semantic types with
respect to a small ontology (marking, for instance, that the entity filling the protag-
onist role must be sentient for frames of cognition). Figure 3 illustrates some of the
structural elements comprising the frame lexicon by considering the CAUSE TO MAKE NOISE
frame.
We identify a set S of spans that are candidates for filling any role r ? Rfi . In
principle, S could contain any subsequence of x, but in this work we only consider
the set of contiguous spans that (a) contain a single word or (b) comprise a valid subtree
of a word and all its descendants in the dependency parse produced by the MST parser.
This covers approximately 80% of arguments in the development data for both data
sets.
The empty span, denoted ?, is also included in S , since some roles are not explicitly
filled; in the SemEval 2007 development data, the average number of roles an evoked
frame defines is 6.7, but the average number of overt arguments is only 1.7.29 In
29 In the annotated data, each core role is filled with one of three types of null instantiations indicating how
the role is conveyed implicitly. For instance, the imperative construction implicitly designates a role as
filled by the addressee, and the corresponding filler is thus CNI (constructional null instantiation). In this
work we do not distinguish different types of null instantiation. The interested reader may refer to Chen
et al. (2010), who handle the different types of null instantions during argument identification.
31
Computational Linguistics Volume 40, Number 1
training, if a labeled argument is not a subtree of the dependency parse, we add its
span to S .30
Let Ai denote the mapping of roles in Rfi to spans in S . Our model makes a
prediction for each Ai(rk) (for all roles rk ? Rfi ) using:
Ai(rk) ? argmax
s?S
p?(s | rk, fi, ti, x) (7)
We use a conditional log-linear model over spans for each role of each evoked frame:
p?(Ai(rk) = s | fi, ti, x) =
exp?h(s, rk, fi, ti, x)
?
s??S
exp?h(s?, rk, fi, ti, x)
(8)
Note that our model chooses the span for each role separately from the other roles
and ignores all frames except the frame the role belongs to. Our model departs
from the traditional SRL literature by modeling the argument identification problem
in a single stage, rather than first classifying token spans as arguments and then
labeling them. A constraint implicit in our formulation restricts each role to have at
most one overt argument, which is consistent with 96.5% of the role instances in the
SemEval 2007 training data and 96.4% of the role instances in the FrameNet 1.5 full text
annotations.
Out of the overt argument spans in the training data, 12% are duplicates, having
been used by some previous frame in the sentence (supposing some arbitrary ordering
of frames). Our role-filling model, unlike a sentence-global argument detection-and-
classification approach,31 permits this sort of argument sharing among frames. Word
tokens belong to an average of 1.6 argument spans, including the quarter of words that
do not belong to any argument.
Appending together the local inference decisions from Equation (7) gives us the best
mapping A?t for target t. Features for our log-linear model (Equation (8)) depend on the
preprocessed sentence x; the target t; a role r of frame f ; and a candidate argument span
s ? S .32 For features using the head word of the target t or a candidate argument span
s, we use the heuristic described in footnote 21 for selecting the head of non-subtree
spans.
Table 7 lists the feature templates used in our model. Every feature template has
a version that does not take into account the role being filled (so as to incorporate
overall biases). The  symbol indicates that the feature template also has a variant that
is conjoined with r, the name of the role being filled; and  indicates that the feature
30 Here is an example in the FrameNet 1.5 training data where this occurs. In the sentence: As capital of
Europe?s most explosive economy, Dublin seems to be changing before your very eyes, the word economy
evokes the ECONOMY frame with the phrase most explosive fulfilling the Descriptor role. However,
in the dependency parse for the sentence the phrase is not a subtree because both words in the frame
attach to the word economy. Future work may consider better heuristics to select potential arguments
from the dependency parses to recover more gold arguments than what the current work achieves.
31 J&N?07, like us, identify arguments for each target.
32 In this section we use t, f , and r without subscripts because the features only consider a single role of a
single target?s frame.
32
Das et al. Frame-Semantic Parsing
Table 7
Features used for argument identification. Section 6.1 describes the meanings of the different
circles attached to each feature.
Features with both null and non-null variants: These features come in two flavors:
if the argument is null, then one version fires; if it is overt (non-null), then another
version fires.
 some word in t has lemma ?  some word in t has POS ?
 some word in t has lemma ?, and the
sentence uses PASSIVE voice
 some word in t has lemma ?, and the
sentence uses ACTIVE voice
 the head of t has subcategorization
sequence ? = ??1, ?2, . . . ?
 some syntactic dependent of the head of t
has dependency type ?
 the head of t has c syntactic dependents  bias feature (always fires)
Span content features: apply to overt argument candidates.
 POS tag ? occurs for some word in s  the head word of s has POS ?
 the first word of s has POS ?  |s|, the number of words in the span
 the last word of s has POS ?  the first word of s has lemma ?
 the head word of s has syntactic
dependency type ?
 the first word of s: ws1 , and its POS tag ?s1 ,
if ?s1 is a closed-class POS
 ws2 and its closed-class POS tag ?s2 ,
provided that |s| ? 2
 the syntactic dependency type ?s1 of the
first word with respect to its head
 the head word of s has lemma ?  ?s2 , provided that |s| ? 2
 the last word of s: ws|s| has lemma ?  ?s|s| , provided that |s| ? 3
 ws|s| , and its closed-class POS tag ?s|s| ,
provided that |s| ? 3
 lemma ? is realized in some word in s
 lemma ? is realized in some word in s, the
voice denoted in the span, s?s position
with respect to t (BEFORE, AFTER, or
OVERLAPPING)
 lemma ? is realized in some word in s,
the voice denoted in the span (ACTIVE
or PASSIVE)
Syntactic features: apply to overt argument candidates.
 dependency path: sequence of labeled,
directed edges from the head word of s to
the head word of t
 length of the dependency path
Span context POS features: for overt candidates, up to 6 of these features will be active.
 a word with POS ? occurs up to 3 words
before the first word of s
 a word with POS ? occurs up to 3 words
after the last word of s
Ordering features: apply to overt argument candidates.
 the position of s with respect to the span
of t: BEFORE, AFTER, or OVERLAPPING (i.e.
there is at least one word shared by s and t)
 target-argument crossing: there is at least
one word shared by s and t, at least one
word in s that is not in t, and at least one
word in t that is not in s
 linear word distance between the nearest
word of s and the nearest word of t,
provided s and t do not overlap
 linear word distance between the middle
word of s and the middle word of t,
provided s and t do not overlap
template additionally has a variant that is conjoined with both r and f , the name of the
frame.33 The role-name-only variants provide for smoothing over frames for common
types of roles such as Time and Place; see Matsubayashi, Okazaki, and Tsujii (2009) for
a detailed analysis of the effects of using role features at varying levels of granularity.
Certain features in our model rely on closed-class POS tags, which are defined to be
all Penn Treebank tags except for CD and tags that start with V, N, J, or R. Finally, the
33 That is, the  symbol subsumes , which in turn subsumes .
33
Computational Linguistics Volume 40, Number 1
features that encode a count or a number are binned into groups: (??,?20], [?19,?10],
[?9,?5], ?4, ?3, ?2, ?1, 0, 1, 2, 3, 4, [5, 9], [10, 19], [20,?).
6.2 Parameter Estimation
We train the argument identification model by:
max
?
N
?
j=1
mj
?
i=1
|R
f (j)i
|
?
k=1
log p?(A
(j)
i (rk) | f
(j)
i , t
(j)
i , x
(j) ) ? C ???22 (9)
Here, N is the number of data points (sentences) in the training set, and m is the number
of frame annotations per sentence. This objective function is concave. For experiments
with the SemEval 2007 data, we trained the model using stochastic gradient ascent
(Bottou 2004) with no Gaussian regularization (C = 0).34 Early stopping was done by
tuning on the development set, and the best results were obtained with a batch size of 2
and 23 passes through the data.
On the FrameNet 1.5 release, we trained this model using L-BFGS (Liu and Nocedal
1989) and ran it for 1,000 iterations. C was tuned on the development data, and we
obtained best results for C = 1.0. We did not use stochastic gradient descent for this
data set as the number of training instances increased and parallelization of L-BFGS
on a multicore setup implementing MPI (Gropp, Lusk, and Skjellum 1994) gave faster
training speeds.
6.3 Decoding with Beam Search
Naive prediction of roles using Equation (7) may result in overlap among arguments
filling different roles of a frame, because the argument identification model fills each role
independently of the others. We want to enforce the constraint that two roles of a single
frame cannot be filled by overlapping spans.35 Toutanova, Haghighi, and Manning
(2005) presented a dynamic programming algorithm to prevent overlapping arguments
for SRL; however, their approach used an orthogonal view to the argument identi-
fication stage, wherein they labeled phrase-structure tree constituents with semantic
roles. That formulation admitted a dynamic programming approach; our formulation
of finding the best argument span for each role does not.
To eliminate illegal overlap, we adopt the beam search technique detailed in
Algorithm 1. The algorithm produces a set of k-best hypotheses for a frame instance?s
full set of role-span pairs, but uses an approximation in order to avoid scoring an
exponential number of hypotheses. After determining which roles are most likely not
explicitly filled, it considers each of the other roles in turn: In each iteration, hypotheses
incorporating a subset of roles are extended with high-scoring spans for the next role,
always maintaining k alternatives. We set k=10,000 as the beam width.36
34 This was the setting used by Das et al. (2010) and we kept it unchanged.
35 On rare occasions a frame annotation may include a secondary frame element layer, allowing arguments to
be shared among multiple roles in the frame; see Ruppenhofer et al. (2006) for details. The evaluation for
this task only considers the primary layer, which is guaranteed to have disjoint arguments.
36 We show the effect of varying beam widths in Table 9, where we present performance of an exact
algorithm for argument identification.
34
Das et al. Frame-Semantic Parsing
Algorithm 1 Joint decoding of frame fi?s arguments via beam search. topk(S , p?, rj)
extracts the k most probable spans from S , under p?, for role rj. extend(D0:(j?1),S ?)
extends each span vector in D0:(j?1) with the most probable non-overlapping span from
S ?, resulting in k best extensions overall.
Require: k > 0, Rfi , S , the distribution p? from Equation 8 for each role rj ? Rfi
Ensure: A?i, a high-scoring mapping of roles of fi to spans with no token overlap among
the spans
1: Calculate Ai according to Equation 7
2: ?r ? Rfi such that Ai(r) = ?, let A?i(r) ? ?
3: R+fi ? {r : r ? Rfi ,Ai(r) = ?}
4: n ? |R+fi |
5: Arbitrarily order R+fi as {r1, r2, . . . rn}
6: Let D0:j = ?D0:j1 , . . . , D
0:j
k ? refer to the k-best list of vectors of compatible filler spans
for roles r1 through rj
7: Initialize D0:0 to be empty
8: for j = 1 to n do
9: D0:j ? extend(D0:(j?1), topk(S , p?, rj))
10: end for
11: ?j ? {1, . . . , n}, A?i(rj) ? D0:n1 [j]
12: return A?i
6.4 Results
Performance of the argument identification model is presented in Table 8 for both data
sets in consideration. We analyze them here.
SemEval 2007 Data: For the SemEval data set, the table shows how performance
varies given different types of input: correct targets and correct frames, correct targets
but automatically identified frames, and ultimately, no oracle input (the full frame
parsing scenario). Rows 1?2 isolate the argument identification task from the frame
identification task. Given gold targets and frames, our argument identification model
(without beam search) gets an F1 score of 68.09%; when beam search is applied, this
increases to 68.46%, with a noticeable increase in precision. Note that an estimated 19%
of correct arguments are excluded because they are neither single words nor complete
subtrees (see Section 6.1) of the automatic dependency parses.37
Qualitatively, the problem of candidate span recall seems to be largely due to
syntactic parse errors.38 Although our performance is limited by errors when using
the syntactic parse to determine candidate spans, it could still improve; this suggests
37 We found that using all constituents from the 10-best syntactic parses would improve oracle recall of
spans in the development set by just a couple of percentage points, at the computational cost of a larger
pool of candidate arguments per role.
38 Note that, because of our labels-only evaluation scheme (Section 3.3), arguments missing a word or
containing an extra word receive no credit. In fact, of the frame roles correctly predicted as having an
overt span, the correct span was predicted 66% of the time, while 10% of the time the predicted starting
and ending boundaries of the span were off by a total of one or two words.
35
Computational Linguistics Volume 40, Number 1
Ta
b
le
8
A
rg
u
m
en
ti
d
en
ti
fi
ca
ti
on
re
su
lt
s
on
bo
th
th
e
Se
m
E
va
l?0
7
d
at
a
as
w
el
la
s
th
e
fu
ll
te
xt
an
no
ta
ti
on
s
of
Fr
am
eN
et
1.
5.
Fo
r
d
ec
od
in
g,
be
am
an
d
na
iv
e
in
d
ic
at
e
w
he
th
er
th
e
ap
p
ro
xi
m
at
e
jo
in
td
ec
od
in
g
al
go
ri
th
m
ha
s
be
en
u
se
d
or
lo
ca
li
nd
ep
en
d
en
td
ec
is
io
ns
ha
ve
be
en
m
ad
e
fo
r
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
,
re
sp
ec
ti
ve
ly
.O
n
th
e
Se
m
E
va
l2
00
7
d
at
a,
fo
r
fu
ll
p
ar
si
ng
(a
u
to
m
at
ic
ta
rg
et
,f
ra
m
e,
an
d
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
),
b
ol
d
sc
or
es
in
d
ic
at
e
be
st
re
su
lt
s,
w
hi
ch
ar
e
al
so
si
gn
ifi
ca
nt
im
p
ro
ve
m
en
ts
re
la
ti
ve
to
th
e
ba
se
lin
e
(p
<
0.
05
).
O
n
th
e
Fr
am
eN
et
1.
5
d
at
a
se
t,
b
ol
d
sc
or
es
in
d
ic
at
e
be
st
re
su
lt
s
on
au
to
m
at
ic
fr
am
e
an
d
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
?
th
is
is
ac
hi
ev
ed
by
th
e
fr
am
e
id
en
ti
fi
ca
ti
on
m
od
el
th
at
u
se
s
th
e
U
JS
F-
 1
,2
gr
ap
h-
ob
je
ct
iv
e
an
d
au
to
m
at
ic
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
u
si
ng
be
am
se
ar
ch
.T
hi
s
re
su
lt
is
st
at
is
ti
ca
lly
si
gn
ifi
ca
nt
ov
er
th
e
su
p
er
vi
se
d
re
su
lt
s
sh
ow
n
in
ro
w
9
(p
<
0.
00
1)
.I
n
te
rm
s
of
p
re
ci
si
on
an
d
F 1
sc
or
e
m
ea
su
re
d
w
it
h
p
ar
ti
al
fr
am
e
m
at
ch
in
g,
th
e
re
su
lt
s
w
it
h
th
e
U
JS
F-
 1
,2
m
od
el
is
st
at
is
ti
ca
lly
si
gn
ifi
ca
nt
ov
er
th
e
N
G
F-
 2
m
od
el
(p
<
0.
05
).
Fo
r
re
ca
ll
w
it
h
p
ar
ti
al
fr
am
e
m
at
ch
in
g,
an
d
fo
r
al
lt
he
th
re
e
m
et
ri
cs
w
it
h
ex
ac
tf
ra
m
e
m
at
ch
in
g,
th
e
re
su
lt
s
w
it
h
th
e
tw
o
gr
ap
h
ob
je
ct
iv
es
ar
e
st
at
is
ti
ca
lly
in
d
is
ti
ng
u
is
ha
bl
e.
N
ot
e
th
at
ce
rt
ai
n
p
ar
ti
al
m
at
ch
re
su
lt
s
ar
e
m
is
si
ng
be
ca
u
se
in
th
os
e
se
tt
in
gs
,g
ol
d
fr
am
es
ha
ve
be
en
u
se
d
fo
r
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
.
A
R
G
U
M
E
N
T
ID
E
N
T
IF
IC
A
T
IO
N
ta
rg
et
s
fr
am
es
de
co
di
ng
ex
ac
tm
at
ch
in
g
p
ar
ti
al
m
at
ch
in
g
P
R
F 1
P
R
F 1
S
em
E
va
l?
07
D
at
a
A
rg
u
m
en
t
id
en
ti
fi
ca
ti
on
(f
u
ll)
go
ld
go
ld
na
iv
e
77
.4
3
60
.7
6
68
.0
9
1
go
ld
go
ld
be
am
78
.7
1
60
.5
7
68
.4
6
2
P
ar
si
ng
(o
ra
cl
e
ta
rg
et
s)
go
ld
su
p
er
vi
se
d
(?
5.
2)
be
am
49
.6
8
42
.8
2
46
.0
0
57
.8
5
49
.8
6
53
.5
6
3
P
ar
si
ng
(f
u
ll)
au
to
su
p
er
vi
se
d
(?
5.
2)
be
am
58
.0
8
38
.7
6
46
.4
9
62
.7
6
41
.8
9
50
.2
4
4
P
ar
si
ng
(J
&
N
?0
7
ta
rg
et
s
an
d
fr
am
es
)
au
to
su
p
er
vi
se
d
(?
3.
4)
be
am
56
.2
6
36
.6
3
44
.3
7
60
.9
8
39
.7
0
48
.0
9
5
B
as
el
in
e:
J&
N
?0
7
au
to
su
p
er
vi
se
d
(?
3.
4)
N
/
A
51
.5
9
35
.4
4
42
.0
1
56
.0
1
38
.4
8
45
.6
2
6
Fr
am
eN
et
1.
5
R
el
ea
se
A
rg
u
m
en
t
id
en
ti
fi
ca
ti
on
(f
u
ll)
go
ld
go
ld
na
iv
e
82
.0
0
76
.3
6
79
.0
8
7
go
ld
go
ld
be
am
83
.8
3
76
.2
8
79
.8
8
8
P
ar
si
ng
(o
ra
cl
e
ta
rg
et
s)
go
ld
su
p
er
vi
se
d
(?
5.
2)
be
am
67
.8
1
60
.6
8
64
.0
5
72
.4
7
64
.8
5
68
.4
5
9
go
ld
SS
L
(N
G
F-
 2
,?
5.
5)
be
am
68
.2
2
61
.0
4
64
.4
3
72
.8
7
65
.2
0
68
.8
2
10
go
ld
SS
L
(U
JS
F-
 1
,2
,?
5.
5)
be
am
68
.3
3
61
.1
4
64
.5
4
72
.9
8
65
.3
0
68
.9
3
11
36
Das et al. Frame-Semantic Parsing
that the model has trouble discriminating between good and bad arguments, and that
additional feature engineering or jointly decoding arguments of a sentence?s frames
may be beneficial.
Rows 3?4 show the effect of automatic supervised frame identification on overall
frame parsing performance. There is a 22% absolute decrease in F1 (18% when partial
credit is given for related frames), suggesting that improved frame identification
or joint prediction of frames and arguments is likely to have a sizeable impact on
overall performance. Rows 4?6 compare our full model (target, frame, and argument
identification) with the baseline, showing significant improvement of more than 4.4
F1 points for both exact and partial frame matching. As with frame identification, we
compared the argument identification stage with that of J&N?07 in isolation, using the
automatically identified targets and frames from the latter as input to our model. As
shown in row 5, with partial frame matching, this gave us an F1 score of 48.1% on the
test set?significantly better (p < 0.05) than 45.6%, the full parsing result from J&N?07
(row 6 in Table 8). This indicates that our argument identification model?which uses a
single discriminative model with a large number of features for role filling (rather than
argument labeling)?is more accurate than the previous state of the art.
FrameNet 1.5 Release: Rows 7?12 show results on the newer data set, which is part
of the FrameNet 1.5 release. As in the frame identification results of Table 5, we do not
show results using predicted targets, as we only test the performance of the statistical
models. First, we observe that for results with gold frames, the F1 score is 79.08%
with naive decoding, which is significantly higher than the SemEval counterpart. This
indicates that increased training data greatly improves performance on the task. We also
observe that beam search improves precision by nearly 2%, while getting rid of overlap-
ping arguments. When both model frames and model arguments are used, we get an
F1 score of 68.45%, which is encouraging in comparison to the best results we achieved
on the SemEval 2007 data set. Semi-supervised lexicon expansion for frame identifi-
cation further improves parsing performance. We observe the best results when the
UJSF-1,2 graph objective is used for frame identification, significantly outperforming
the fully supervised model on parsing (p < 0.001) for all evaluation metrics. The im-
provements with SSL can be explained by noting that frame identification performance
goes up when the graph objectives are used, which carries over to argument iden-
tification. Figure 5 shows an example where the graph-based model UJSF-1,2 corrects
an error made by the fully supervised model for the unseen LU discrepancy.N, both for
frame identification and full frame-semantic parsing.
7. Collective Argument Identification with Constraints
The argument identification strategy described in the previous section does not capture
some facets of semantic knowledge represented declaratively in FrameNet. In this
section, we present an approach that exploits such knowledge in a principled, unified,
and intuitive way. In prior NLP research using FrameNet, these interactions have been
largely ignored, though they have the potential to improve the quality and consistency
of semantic analysis. The beam search technique (Algorithm 1) handles one kind of
constraint: avoiding argument overlaps. It is, however, approximate and cannot handle
other forms of constraints.
Here, we present an algorithm that exactly identifies the best full collection of argu-
ments of a target given its semantic frame. Although we work within the conventions of
37
Computational Linguistics Volume 40, Number 1
Figure 5
(a) Output of the supervised frame-semantic parsing model, with beam search for argument
identification, given the target discrepancies. The output is incorrect. (b) Output using the
constrained frame identification model that takes into account the graph-based frame
distributions over unknown predicates. In this particular example, the UJSF-1,2 graph
objective is used. This output matches the gold annotation. The LU discrepancy.N is unseen
in supervised FrameNet data.
FrameNet, our approach is generalizable to other SRL frameworks. We model argument
identification as constrained optimization, where the constraints come from expert
knowledge encoded in FrameNet. Following prior work on PropBank-style SRL that
dealt with similar constrained problems (Punyakanok et al. 2004; Punyakanok, Roth,
and Yih 2008, inter alia), we incorporate this declarative knowledge in an integer linear
program.
Because general-purpose ILP solvers are proprietary and do not fully exploit the
structure of the problem, we turn to a class of optimization techniques called dual
decomposition (Komodakis, Paragios, and Tziritas 2007; Rush et al. 2010; Martins et al.
2011a). We derive a modular, extensible, parallelizable approach in which semantic con-
straints map not just to declarative components in the algorithm, but also to procedural
ones, in the form of ?workers.? Although dual decomposition algorithms only solve
a relaxation of the original problem, we make our approach exact by wrapping the
algorithm in a branch-and-bound search procedure. 39
We experimentally find that our algorithm achieves accuracy comparable to the
results presented in Table 8, while respecting all imposed linguistic constraints. In
comparison with beam search, which violates many of these constraints, the presented
exact decoder is slower, but it decodes nine times faster than CPLEX, a state-of-the-art,
proprietary, general-purpose exact ILP solver.40
39 Open-source code in C++ implementing the AD3 algorithm can be found at
http://www.ark.cs.cmu.edu/AD3.
40 See http://www-01.ibm.com/software/integration/optimization/cplex-optimizer.
38
Das et al. Frame-Semantic Parsing
7.1 Joint Inference
Here, we take a declarative approach to modeling argument identification using an ILP
and relate our formulation to prior work in shallow semantic parsing. We show how
knowledge specified in a linguistic resource (FrameNet in our case) can be used to
derive the constraints in our ILP. Finally, we draw connections of our specification to
graphical models, a popular formalism in artificial intelligence and machine learning,
and describe how the constraints can be treated as factors in a factor graph.
7.1.1 Declarative Specification. Let us simplify notation by considering a given target t and
not considering its index in a sentence x; let the semantic frame it evokes be f . To solely
evaluate argument identification, we assume that the semantic frame f is given, which is
traditionally the case in controlled experiments used to evaluate SRL systems (Ma`rquez
et al. 2008). Let the set of roles associated with the frame f be Rf . In sentence x, the set
of candidate spans of words that might fill each role is enumerated, usually following
an overgenerating heuristic, which is described in Section 6.1; as before, we call this set
of spans S . As before, this set also includes the null span ?; connecting it to a role r ? Rf
denotes that the role is not overt. Our approach assumes a scoring function that gives
a strength of association between roles and candidate spans. For each role r ? Rf and
span s ? S , this score is parameterized as:
c(r, s) = ?h(s, r, f, t, x), (10)
where ? are model weights and h is a feature function that looks at the target t,
the evoked frame f , sentence x, and its syntactic analysis, along with r and s. This
scoring function is identical in form to the numerator?s exponent in the log-linear model
described in Equation (8). The SRL literature provides many feature functions of this
form and many ways to use machine learning to acquire ?. Our presented method does
not make any assumptions about the score except that it has the form in Equation (10).
We define a vector z of binary variables zr,s ? {0, 1} for every role and span pair. We
have that: z ? {0, 1}d, where d = |Rf | ? |S|. zr,s = 1 means that role r is filled by span s.
Given the binary z vector, it is straightforward to recover the collection of arguments
by checking which components zr,s have an assignment of 1; we use this strategy to
find arguments, as described in Section 7.3 (strategies 4 and 6). The joint argument
identification task can be represented as a constrained optimization problem:
maximize
?
r?Rf
?
s?S
c(r, s) ? zr,s
with respect to z ? {0, 1}d
such that Az ? b (11)
In the last line, A is a k ? d matrix and b is a vector of length k. Thus, Az ? b is a set of
k inequalities representing constraints that are imposed on the mapping between roles
and spans; these are motivated on linguistic grounds and are described next. 41
41 Note that equality constraints a ? z = b can be transformed into double-side inequalities a ? z ? b and
?a ? z ? ?b.
39
Computational Linguistics Volume 40, Number 1
Uniqueness. Each role r is filled by at most one span in S . This constraint can be
expressed by:
?r ? Rf ,
?
s?S
zr,s = 1 (12)
There are O(|Rf |) such constraints. Note that because S contains the null span ?, non-
overt roles are also captured using the above constraints. Such a constraint is used
extensively in prior literature (Punyakanok, Roth, and Yih 2008, Section 3.4.1).
Overlap. SRL systems commonly constrain roles to be filled by non-overlapping spans.
For example, Toutanova, Haghighi, and Manning (2005) used dynamic programming
over a phrase structure tree to prevent overlaps between arguments, and Punyakanok,
Roth, and Yih (2008) used constraints in an ILP to respect this requirement. Inspired by
the latter, we require that each input sentence position of x be covered by at most one
argument of t. We define:
G(i) = {s | s ? S , s covers position i in x} (13)
We can define our overlap constraints in terms of G as follows, for every sentence
position i:
?i ? {1, . . . , |x|},
?
r?Rf
?
s?G(i)
zr,s ? 1 (14)
This gives us O(|x|) constraints. It is worth noting that this constraint aims to achieve the
same effect as beam search, as described in Section 6.3, which tries to avoid argument
overlap greedily.
Pairwise ?Exclusions.? For many target classes, there are pairs of roles forbidden to
appear together in the analysis of a single target token. Consider the following two
sentences:
(1) A blackberry
Entity 1
resembles a loganberry
Entity 2
.
(2) Most berries
Entities
resemble each other.
Consider the uninflected target resemble in both sentences, evoking the same
meaning. In Example (1), two roles?which we call Entity 1 and Entity 2?describe two
entities that are similar to each other. In the second sentence, a phrase fulfills a third
role, called Entities, that collectively denotes some objects that are similar. It is clear that
the roles Entity 1 and Entities cannot be overt for the same target at once, because the latter
already captures the function of the former; a similar argument holds for the Entity 2 and
Entities roles. We call this phenomenon the ?excludes? relationship. Let us define a set of
pairs from Rf that have this relationship:
Exclf = {(ri, rj) | ri and rj exclude each other}
40
Das et al. Frame-Semantic Parsing
Using the given set, we define the constraint:
?(ri, rj) ? Exclf , zri,? + zrj,? ? 1 (15)
If both roles are overt in a parse, this constraint will be violated, contravening the
?excludes? relationship specified between the pair of roles. If neither or only one of
the roles is overt, the constraint is satisfied. The total number of such constraints is
O(|Exclf |), which is the number of pairwise ?excludes? relationships of a given frame.
Pairwise ?Requirements.? The sentence in Example (1) illustrates another kind of con-
straint. The target resemble cannot have only one of Entity 1 and Entity 2 as roles in text.
For example,
(3) * A blackberry
Entity 1
resembles.
Enforcing the overtness of two roles sharing this ?requires? relationship is straight-
forward. We define the following set for a frame f :
Reqf = {(ri, rj) | ri and rj require each other}
This leads to constraints of the form
?(ri, rj) ? Reqf , zri,? ? zrj,? = 0 (16)
If one role is overt (or absent), the other must be as well. A related constraint has
been used previously in the SRL literature, enforcing joint overtness relationships be-
tween core arguments and referential arguments (Punyakanok, Roth, and Yih 2008,
Section 3.4.1), which are formally similar to our example.42
7.1.2 Integer Linear Program and Relaxation. Plugging the constraints in Equations 12,
14, 15, and 16 into the last line of Equation (11), we have the argument identification
problem expressed as an ILP, since the indicator variables z are binary. Here, apart from
the ILP formulation, we will consider the following relaxation of Equation (11), which
replaces the binary constraint z ? {0, 1}d by a unit interval constraint z ? [0, 1]d, yielding
a linear program:
maximize
?
r?Rf
?
s?S
c(r, s) ? zr,s
with respect to z ? [0, 1]d
such that Az ? b. (17)
42 We noticed that, in the annotated data, in some cases, the ?requires? constraint is violated by the
FrameNet annotators. This happens mostly when one of the required roles is absent in the sentence
containing the target, but is rather instantiated in an earlier sentence (Gerber and Chai 2010). We apply
the hard constraint in Equation (16), though extending our algorithm to seek arguments outside the
sentence is straightforward. For preliminary work extending SEMAFOR this way, see Chen et al. (2010).
41
Computational Linguistics Volume 40, Number 1
There are several LP and ILP solvers available, and a great deal of effort has been
spent by the optimization community to devise efficient generic solvers. An example
is CPLEX, a state-of-the-art solver for mixed integer programming that we use as a
baseline to solve the ILP in Equation (11) as well as its LP relaxation in Equation (17).
Like many of the best implementations, CPLEX is proprietary.
7.1.3 Linguistic Constraints from FrameNet. Although enforcing the four different sets of
constraints is intuitive from a general linguistic perspective, we ground their use in
definitive linguistic information present in the FrameNet lexicon. From the annotated
data in the FrameNet 1.5 release, we gathered that only 3.6% of the time is a role
instantiated multiple times by different spans in a sentence. This justifies the uniqueness
constraint enforced by Equation (12). Use of such a constraint is also consistent with
prior work in frame-semantic parsing (Johansson and Nugues 2007). Similarly, we
found that in the annotations, no arguments overlapped with each other for a given
target. Hence, the overlap constraints in Equation (14) are also justified.
Our third and fourth sets of constraints, presented in Equations (15) and (16), come
from FrameNet, too. Examples (1) and (2) are instances where the target resemble
evokes the SIMILARITY frame, which is defined in FrameNet as:
Two or more distinct entities, which may be concrete or abstract objects or types, are
characterized as being similar to each other. Depending on figure/ground relations, the
entities may be expressed in two distinct frame elements and constituents, Entity 1 and
Entity 2, or jointly as a single frame element and constituent, Entities.
For this frame, the lexicon lists several roles other than the three we have already
observed, such as Dimension (the dimension along which the entities are similar), Differ-
entiating fact (a fact that reveals how the concerned entities are similar or different), and
so forth. Along with the roles, FrameNet also declares the ?excludes? and ?requires?
relationships noted in our discussion in Section 7.1.1. The case of the SIMILARITY frame
is not unique; in Figure 1, the frame COLLABORATION, evoked by the target partners, also
has two roles Partner 1 and Partner 2 that share the ?requires? relationship. In fact, out
of 877 frames in FrameNet 1.5, 204 frames have at least a pair of roles for which the
?excludes? relationship holds, and 54 list at least a pair of roles that share the ?requires?
relationship.
7.1.4 Constraints as Factors in a Graphical Model. The LP in Equation (17) can be repre-
sented as a maximum a posteriori inference problem in an undirected graphical model.
In the factor graph, each component (zr,s) of the vector z corresponds to a binary
variable, and each instantiation of a constraint in Equations (12), (14), (15), and (16)
corresponds to a factor. Smith and Eisner (2008) and Martins et al. (2010) used such
a representation to impose constraints in a dependency parsing problem; the latter
discussed the equivalence of linear programs and factor graphs for representing dis-
crete optimization problems. All of our constraints take standard factor forms we can
describe using the terminology of Smith and Eisner and Martins et al. The uniqueness
constraint in Equation (12) corresponds to an XOR factor, while the overlap constraint
in Equation (14) corresponds to an ATMOSTONE factor. The constraints in Equation (15)
enforcing the ?excludes? relationship can be represented with an OR factor. Finally,
each ?requires? constraints in Equation (16) is equivalent to an XORWITHOUTPUT
factor.
42
Das et al. Frame-Semantic Parsing
In the following section, we describe how we arrive at solutions for the LP in
Equation (17) using dual decomposition, and how we adapt it to efficiently recover the
exact solution of the ILP (Equation (11)), without the need of an off-the-shelf ILP solver.
7.2 ?Augmented? Dual Decomposition
Dual decomposition methods address complex optimization problems in the dual, by
dividing them into simple worker problems (subproblems), which are repeatedly solved
until a consensus is reached. The simplest technique relies on the subgradient algorithm
(Komodakis, Paragios, and Tziritas 2007; Rush et al. 2010); as an alternative, Martins
et al. (2011a, 2011b) proposed an augmented Lagrangian technique, which is more
suitable when there are many small components ?commonly the case in declarative
constrained problems, like the one at hand. Here, we present a brief overview of the
latter, which is called AD3.
Let us start by establishing some notation. Let m ? {1, . . . , M} index a factor, and
denote by i(m) the vector of indices of variables linked to that factor. (Recall that each
factor represents the instantiation of a constraint.) We introduce a new set of variables,
u ? Rd, called the ?witness? vector. We split the vector z into M overlapping pieces
z1, . . . , zM, where each zm ? [0, 1]|i(m)|, and add M constraints zm = ui(m) to impose that
all the pieces must agree with the witness (and therefore with each other). Each of the M
constraints described in Section 7.1 can be encoded with its own matrix Am and vector
bm (which jointly define A and b in Equation (17)). For convenience, we denote by c ? Rd
the score vector, whose components are c(r, s), for each r ? Rf and s ? S (Equation (10)),
and define the following scores for the mth subproblem:
cm(r, s) = ?(r, s)?1c(r, s), ?(r, s) ? i(m)
where ?(r, s) is the number of constraints that involve role r and span s. Note that
according to this definition, c ? z =
?M
m=1 cm ? zm. We can rewrite the LP in Equation (17)
in the following equivalent form:
maximize
M
?
m=1
cm ? zm
with respect to u ? Rd, zm ? [0, 1]i(m), ?m
such that Amzm ? bm, ?m
zm = ui(m), ?m (18)
We introduce Lagrange multipliers ?m for the equality constraints in the last line.
The AD3 algorithm is depicted as Algorithm 2. Like dual decomposition approaches,
it repeatedly performs a broadcast operation (the zm-updates, which can be done in
parallel, one constraint per ?worker?) and a gather operation (the u- and ?-updates).
Each u-operation can be seen as an averaged voting which takes into consideration each
worker?s results.
Like in the subgradient method, the ?-updates can be regarded as price adjust-
ments, which will affect the next round of zm-updates. The only difference with respect
to the subgradient method (Rush et al. 2010) is that each subproblem involved in a
zm-update also has a quadratic penalty that penalizes deviations from the previous
43
Computational Linguistics Volume 40, Number 1
Algorithm 2 AD3 for Argument Identification
Require: role-span matching scores c := ?c(r, s)?r,s, structural constraints ?Am, bm?Mm=1,
penalty ? > 0
1: initialize t ? 1
2: initialize u1 uniformly (i.e., u(r, s) = 0.5, ?r, s)
3: initialize each ?1m = 0, ?m ? {1, . . . , M}
4: repeat
5: for each m = 1, . . . , M do
6: make a zm-update by finding the best scoring analysis for the mth constraint,
with penalties for deviating from the consensus u:
z(t+1)m ? argmax
Amztm?bm
(cm + ?tm) ? zm ?
?
2?zm ? u
t
i(m)?2 (19)
7: end for
8: make a u-update by updating the consensus solution, averaging z1, . . . , zm:
u(t+1)(r, s) ? 1
?(r, s)
?
m:(r,s)?i(m)
z(t+1)m (r, s)
9: make a ?-update:
?(t+1)m ? ?tm ? ?(z(t+1)m ? u(t+1)i(m) ), ?m
10: t ? t + 1
11: until convergence
Ensure: relaxed primal solution u? and dual solution ??. If u? is integer, it will encode
an assignment of spans to roles. Otherwise, it will provide an upper bound of the
true optimum.
average voting; it is this term that accelerates consensus and therefore convergence.
Martins et al. (2011b) also provide stopping criteria for the iterative updates using
primal and dual residuals that measure convergence; we refer the reader to that paper
for details.
A key attraction of this algorithm is that all the components of the declarative
specification remain intact in the procedural form. Each worker corresponds exactly
to one constraint in the ILP, which corresponds to one linguistic constraint. There is no
need to work out when, during the procedure, each constraint might have an effect, as
in beam search.
7.2.1 Solving the Subproblems. In a different application, Martins et al. (2011b, Section 4)
showed how to solve each zm-subproblem associated with the XOR, XORWITHOUTPUT
and OR factors in runtime O(|i(m)| log |i(m)|). The only subproblem that remains is that
of the ATMOSTONE factor; a solution with the same runtime is given in Appendix B.
7.2.2 Exact Decoding. It is worth recalling that AD3, like other dual decomposition
algorithms, solves a relaxation of the actual problem. Although we have observed that
the relaxation is often tight (cf. Section 7.3), this is not always the case. Specifically, a
fractional solution may be obtained, which is not interpretable as an argument, and
therefore it is desirable to have a strategy to recover the exact solution. Two observations
44
Das et al. Frame-Semantic Parsing
are noteworthy. First, the optimal value of the relaxed problem (Equation (17)) provides
an upper bound to the original problem (Equation (11)). This is because Equation (11)
has the additional integer constraint on the variables. In particular, any feasible dual
point provides an upper bound to the original problem?s optimal value. Second, dur-
ing execution of the AD3 algorithm, we always keep track of a sequence of feasible
dual points. Therefore, each iteration constructs tighter and tighter upper bounds.
With this machinery, we have all that is necessary for implementing a branch-and-
bound search that finds the exact solution of the ILP. The procedure works recursively
as follows:
1. Initialize L = ?? (our best value so far).
2. Run Algorithm 2. If the solution u? is integer, return u? and set L to the objec-
tive value. If along the execution we obtain an upper bound less than L, then
Algorithm 2 can be safely stopped and return ?infeasible??this is the bound part.
Otherwise (if u? is fractional) go to step 3.
3. Find the ?most fractional? component of u? (call it u?j ) and branch: constrain uj = 0
and go to step 2, eventually obtaining an integer solution u?0 or infeasibility; and
then constrain uj = 1 and do the same, obtaining u?1 . Return the u
? ? {u?0 , u?1} that
yields the largest objective value.
Although this procedure may have worst-case exponential runtime, we found it empir-
ically to rapidly obtain the exact solution in all test cases.
7.3 Results with Collective Argument Identification
We present experiments only on argument identification in this section, as our goal is
to exhibit the importance of incorporating the various linguistic constraints during our
inference procedure. We present results on the full text annotations of FrameNet 1.5, and
do not experiment on the SemEval 2007 benchmark, as we have already established our
constraint-agnostic models as state-of-the-art. The model weights ? used in the scoring
function c were learned as in Section 6.1 (i.e., by training a logistic regression model to
maximize conditional log-likelihood). The AD3 parameter ? was initialized to 0.1, and
we followed Martins et al. (2011b) in dynamically adjusting it to keep a balance between
the primal and dual residuals.
We compare the following algorithms to demonstrate the efficacy of our collective
argument identification approach:43
1. Naive: This strategy selects the best span for each role r according to the score
function c(r, s), independently of all other roles?the decoding rule formalized in
Equation (7) of Section 6.1. It ignores all constraints except ?uniqueness.?
2. Beam: This strategy employs greedy beam search to eliminate overlaps between
predicted arguments, as described in Algorithm 1. Note that it does not try to
respect the ?excludes? and ?requires? constraints between pairs of roles. The
default size of the beam in Section 1 was a safe 10,000; this resulted in extremely
slow decoding times. For time comparison, we tried beam sizes of 100 and 2 (the
latter being the smallest size that achieves the same F1 score on the FrameNet 1.5
dev set).
43 The first two strategies correspond to rows 7 and 9, respectively, of Table 8.
45
Computational Linguistics Volume 40, Number 1
Table 9
Comparison of decoding strategies in Section 7.3 on the data set released with the FrameNet 1.5
Release, given gold frames. We evaluate in terms of precision, recall, and F1 score on our test
set containing 4,458 targets. We also compute the number of constraint violations each model
makes: the three values are the numbers of overlapping arguments and violations of the
?requires? and ?excludes? constraints of Section 7.1. Finally, decoding time (without feature
computation steps) on the whole test set is shown in the last column averaged over five runs.
ARGUMENT IDENTIFICATION
Method P R F1 Violations Time (s)
naive 82.00 76.36 79.08 441 45 15 1.26 ? 0.01
beam = 2 83.68 76.22 79.78 0 49 0 2.74 ? 0.10
beam = 100 83.83 76.28 79.88 0 50 1 29.00 ? 0.25
beam = 10, 000 83.83 76.28 79.88 0 50 1 440.67 ? 5.53
CPLEX, LP 83.80 76.16 79.80 0 1 0 32.67 ? 1.29
CPLEX, exact 83.78 76.17 79.79 0 0 0 43.12 ? 1.26
AD3, LP 83.77 76.17 79.79 2 2 0 4.17 ? 0.01
AD3, exact 83.78 76.17 79.79 0 0 0 4.78 ? 0.04
3. CPLEX, LP: This uses CPLEX to solve the relaxed LP in Equation (17). To han-
dle fractional z, for each role r, we choose the best span s? such that s? =
argmaxs?Sr zr,s, breaking ties arbitrarily.
4. CPLEX, exact: This tackles the actual ILP (Equation (11)) with CPLEX.
5. AD3, LP: The relaxed problem is solved using AD3. We choose a span for each role
as in strategy 3.
6. AD3, exact: This couples AD3 with branch-and-bound search to get the exact
integer solution.
Table 9 shows performance of these decoding strategies on the test set. We report
precision, recall, and F1 scores. As with experiments in previous sections, we use the
evaluation script from SemEval 2007 shared task. Because these scores do not penalize
constraint violations, we also report the number of overlap, ?excludes,? and ?requires?
constraints that were violated in the test set. Finally, we tabulate each setting?s decoding
time in seconds on the whole test set averaged over five runs.44 The naive model
is very fast but suffers degradation in precision and violates one constraint roughly
per nine targets. The decoding strategy of Section 6.1 used a default beam size of
10,000, which is extremely slow; a faster version of beam size 100 results in the same
precision and recall values, but is 15 times faster on our test set. Beam size 2 results
in slightly worse precision and recall values, but is even faster. All of these, however,
result in many constraint violations. Strategies involving CPLEX and AD3 perform
similarly to each other and to beam search on precision and recall, but eliminate most
or all of the constraint violations. With respect to precision and recall, exact AD3 and
beam search with a width of 10,000 were found to be statistically indistinguishable
(p > 0.01). The decoding strategy with beam size 2 is 11?16 times faster than the
44 Experiments were conducted on a 64-bit machine with two 2.6-GHz dual-core CPUs (i.e., four processors
in all) and a total of 8 GB of RAM. The workers in AD3 were not parallelized, whereas CPLEX
automatically parallelized execution.
46
Das et al. Frame-Semantic Parsing
.
(a) Gold annotation.
.
(b) Beam search output.
Figure 6
An example from the test set where (a) exhibits the gold annotation for a target that evokes
the COLLABORATION frame, with the Partners role filled by the span international. (b) shows
the prediction made by the beam search decoding scheme (beam = 10,000), where it marks
international with the Partner 1 role, violating the ?requires? constraint; FrameNet notes that this
role should be present with the Partner 2 role. AD3 is conservative and predicts no role?it is
penalized by the evaluation script, but does not produce output that violates
linguistic constraints.
CPLEX strategies, but is only twice as fast as AD3, and results in significantly more
constraint violations. The exact algorithms are slower than the LP versions, but com-
pared with CPLEX, AD3 is significantly faster and has a narrower gap between its
exact and LP versions. We found that relaxation was tight 99.8% of the time on the test
examples.
The example in Figure 1 is taken from our test set, and shows an instance where two
roles, Partner 1 and Partner 2, share the ?requires? relationship; for this example, the beam
search decoder misses the Partner 2 role, which is a violation, while our AD3 decoder
identifies both arguments correctly. Note that beam search makes plenty of linguistic
violations. We found that beam search, when violating many ?requires? constraints,
often finds one role in the pair, which increases its recall. AD3 is sometimes more
conservative in such cases, predicting neither role. Figure 6 shows such an example
where beam search finds one role (Partner 1) while AD3 is more conservative and predicts
no roles. Figure 7 shows another example contrasting the output of beam search and
AD3 where the former predicts two roles sharing an ?excludes? relationship; AD3 does
not violate this constraint and tries to predict a more consistent argument set. Overall,
we found it interesting that imposing the constraints did not have much effect on
standard measures of accuracy.
Table 9 only shows results with gold frames. We ran the exact version of AD3 with
automatic frames as well. When the semi-supervised graph objective UJSF-1,2 is used
for frame identification, the performance with AD3 is only a bit worse in comparison
with beam search (row 11 in Table 8) when frame and argument identification are
evaluated together. We get a precision of 72.92, a recall of 65.22 and an F1 score of 68.86
(partial frame matching). Again, all linguistic constraints are respected, unlike beam
search.
47
Computational Linguistics Volume 40, Number 1
8. Conclusion
We have presented an approach to rich frame-semantic parsing, based on a combination
of knowledge from FrameNet, two probabilistic models trained on full text annota-
tions released along with the FrameNet lexicon, and expedient heuristics. The frame
identification model uses latent variables in order to generalize to predicates unseen
in either the FrameNet lexicon or training data, and our results show that, quite often,
this model chooses a frame closely related to the gold-standard annotation. We also
presented an extension of this model that uses graph-based semi-supervised learning
to better generalize to new predicates; this achieves significant improvements over the
fully supervised approach. Our argument identification model, trained using maximum
conditional log-likelihood, unifies the traditionally separate steps of detecting and
(a) Gold annotation.
(b) Beam search output.
(c) AD3 output.
Figure 7
An example from the test set where (a) exhibits the gold annotation for a target that evokes
the DISCUSSION frame, with the Interlocutor 1 role filled by the span neighbors. (b) shows
the prediction made by the beam search decoding scheme (beam = 10,000), where it marks
The next morning his households and neighbors with the Interlocutors role, which violates
the ?excludes? constraint with respect to the Interlocutor 2 role. In (c), AD3 marks the wrong
span as the Interlocutor 1 role, but it does not violate the constraint. Both beam and
AD3 inference miss the Topic role.
48
Das et al. Frame-Semantic Parsing
labeling arguments. Our system achieves improvements over the previous state of the
art on the SemEval 2007 benchmark data set at each stage of processing and collectively.
We also report stronger results on the more recent, larger FrameNet 1.5 release.
We applied the AD3 algorithm to collective prediction of a target?s arguments,
incorporating declarative linguistic knowledge as constraints. It outperforms the naive
local decoding scheme that is oblivious to the constraints. Furthermore, it is significantly
faster than a decoder employing a state-of-the-art proprietary solver; it is only twice as
slow as beam search (our chosen decoding method for comparison with the state of
the art), which is inexact and does not respect all linguistic constraints. This method is
easily amenable to the inclusion of additional constraints.
From our results, we observed that in comparison to the SemEval 2007 data
set, frame-semantic parsing performance significantly increases when we use the
FrameNet 1.5 release; this suggests that the increase in the number of full text anno-
tations and the size of the FrameNet lexicon is beneficial. We believe that with more
annotations in the future (say, in the range of the number of PropBank annotations), our
frame-semantic parser can reach even better accuracy, making it more useful for NLP
applications that require semantic analysis.
There are several open problems to be addressed. Firstly, we could further im-
prove the coverage of the frame-semantic parser by improving our semi-supervised
learning approach; two possibilities are custom metric learning approaches (Dhillon,
Talukdar, and Crammer 2010) that suit the frame identification problem in graph-based
SSL, and sparse word representations (Turian, Ratinov, and Bengio 2010) as features
in frame identification. The argument identification model might also benefit from
semi-supervised learning. Further feature engineering and improved preprocessing,
including tokenization into lexical units, improved syntactic parsing, and the use of
external knowledge bases, is expected to improve the system?s accuracy. Finally, the
FrameNet lexicon does not contain exhaustive semantic knowledge. Automatic frame
and role induction is an exciting direction of future research that could further enhance
our methods of automatic frame-semantic parsing. The parser described in this article
is available for download at http://www.ark.cs.cmu.edu/SEMAFOR.
Appendix
A. Target Identification Heuristics from J&N?07
We describe here the filtering rules that Johansson and Nugues (2007) used for identify-
ing frame evoking targets in their SemEval 2007 shared task paper. They built a filtering
component based on heuristics that removed words that appear in certain contexts, and
kept the remaining ones.45 These are:
 have was retained only if had an object,
 be was retained only if it was preceded by there,
 will was removed in its modal sense,
 of course and in particular were removed,
45 Although not explicitly mentioned in the paper, we believe that these rules were applied on a white list of
potential targets seen in FrameNet and the SemEval 2007 training data.
49
Computational Linguistics Volume 40, Number 1
 the prepositions above, against, at, below, beside, by, in, on, over, and under
were removed unless their head was marked as locative,
 after and before were removed unless their head was marked as temporal,
 into, to, and through were removed unless their head was marked as
direction,
 as, for, so, and with were always removed,
 because the only sense of the word of was the frame PARTITIVE, it was
removed unless it was preceded by only, member, one, most, many, some, few,
part, majority, minority, proportion, half, third, quarter, all, or none, or it was
followed by all, group, them, or us,
 all targets marked as support verbs for some other target were removed.
Note that J&N?07 used a syntactic parser that provided dependency labels correspond-
ing to locative, temporal, and directional arguments, which our syntactic parser of
choice (the MST parser) does not provide.
B. Solving ATMOSTONE subproblems in AD3
The ATMOSTONE subproblem can be transformed into that of projecting a point
(a1, . . . , ak) onto the set
Sm =
{
zm ? [0, 1]|i(m)| |
?|i(m)|
j=1 zm,j ? 1
}
This projection can be computed as follows:
1. Clip each aj into the interval [0, 1] (i.e., set a?j = min{max{aj, 0}, 1}). If the result
satisfies
?k
j=1 a
?
j ? 1, then return (a?1, . . . , a?k).
2. Otherwise project (a1, . . . , ak) onto the probability simplex:
{
zm ? [0, 1]|i(m)| |
?|i(m)|
j=1 zm,j = 1
}
This is precisely the XOR subproblem and can be solved in time O(|i(m)|
log |i(m)|).
The proof of this procedure?s correctness follows from the proof in Appendix B of
Martins et al. (2011b).
Acknowledgments
We thank Collin Baker, Katrin Erk, Richard
Johansson, and Nils Reiter for software, data,
evaluation scripts, and methodological
details. We thank the reviewers of this and
the earlier papers, Alan Black, Ric Crabbe,
Michael Ellsworth, Rebecca Hwa, Dan Klein,
Russell Lee-Goldman, Slav Petrov, Dan Roth,
Josef Ruppenhofer, Amarnag Subramanya,
Partha Talukdar, and members of the ARK
group for helpful comments. This work was
supported by DARPA grant NBCH-1080004,
NSF grants IIS-0836431 and IIS-0915187,
Qatar National Research Foundation grant
NPRP 08-485-1-083, Google?s support of the
Worldly Knowledge Project at CMU,
computational resources provided by Yahoo,
and TeraGrid resources provided by the
Pittsburgh Supercomputing Center under
grant TG-DBS110003.
50
Das et al. Frame-Semantic Parsing
References
Auli, Michael and Adam Lopez. 2011.
A comparison of loopy belief propagation
and dual decomposition for integrated
CCG supertagging and parsing.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 470?480, Portland, OR.
Baker, Collin, Michael Ellsworth, and
Katrin Erk. 2007. SemEval-2007 task 19:
Frame semantic structure extraction.
In Proceedings of the Fourth International
Workshop on Semantic Evaluations,
pages 99?104, Prague.
Baluja, Shumeet, Rohan Seth, D. Sivakumar,
Yushi Jing, Jay Yagnik, Shankar Kumar,
Deepak Ravichandran, and Mohamed
Aly. 2008. Video suggestion and discovery
for YouTube: taking random walks
through the view graph. In Proceedings
of the 17th International Conference on
the World Wide Web, pages 895?904,
Beijing.
Bauer, Daniel and Owen Rambow. 2011.
Increasing coverage of syntactic
subcategorization patterns in FrameNet
using VerbNet. In Proceedings of the 2011
IEEE Fifth International Conference on
Semantic Computing, pages 181?184,
Washington, DC.
Bejan, Cosmin A. 2009. Learning Event
Structures From Text. Ph.D. thesis, The
University of Texas at Dallas.
Bengio, Yoshua, Olivier Delalleau, and
Nicolas Le Roux. 2006. Label propagation
and quadratic criterion. In Olivier
Chapelle, Bernhard Scho?lkopf, and
Alexander Zien, editors, Semi-Supervised
Learning. MIT Press, Cambridge, MA,
pages 193?216.
Boas, Hans C. 2002. Bilingual FrameNet
dictionaries for machine translation.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation, pages 1,364?1,371, Las Palmas.
Bottou, Le?on. 2004. Stochastic learning.
In Olivier Bousquet and Ulrike von
Luxburg, editors, Advanced Lectures on
Machine Learning, Lecture Notes in
Artificial Intelligence, LNAI 3176.
Springer Verlag, Berlin, pages 146?168.
Burbea, Jacob and Calyampudi R. Rao. 2006.
On the convexity of some divergence
measures based on entropy functions.
IEEE Transactions on Information Theory,
28(3):489?495.
Burchardt, Aljoscha, Katrin Erk, and
Anette Frank. 2005. A WordNet detour
to FrameNet. In Bernhard Fisseni,
Hans-Christian Schmitz, Bernhard
Schro?der, and Petra Wagner, editors,
Sprachtechnologie, mobile Kommunikation
und linguistische Resourcen, volume 8 of
Computer Studies in Language and Speech.
Peter Lang, Frankfurt am Main,
pages 408?421.
Burchardt, Aljoscha and Anette Frank. 2006.
Approaching textual entailment with LFG
and FrameNet frames. In Proceedings of the
Second PASCAL RTE Challenge Workshop,
pages 92?97, Venice.
Burchardt, Aljoscha, Marco Pennacchiotti,
Stefan Thater, and Manfred Pinkal. 2009.
Assessing the impact of frame semantics
on textual entailment. Natural Language
Engineering, 15(4):527?550.
Carreras, Xavier and Llu??s Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of the Eighth Conference on Computational
Natural Language Learning, pages 89?97,
Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning, pages 152?164,
Ann Arbor, MI.
Chang, Yin-Wen and Michael Collins.
2011. Exact decoding of phrase-based
translation models through Lagrangian
relaxation. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 26?37,
Edinburgh.
Chapelle, Olivier, Bernhard Scho?lkopf,
and Alexander Zien, editors. 2006.
Semi-Supervised Learning. MIT Press,
Cambridge, MA.
Chen, Desai, Nathan Schneider, Dipanjan
Das, and Noah A. Smith. 2010. SEMAFOR:
Frame argument resolution with
log-linear models. In Proceedings of the
5th International Workshop on Semantic
Evaluation, pages 264?267, Upssala.
Corduneanu, Adrian and Tommi Jaakkola.
2003. On information regularization.
In Proceedings of the Nineteenth Conference
on Uncertainty in Artificial Intelligence,
pages 151?158, Acapulco.
Das, Dipanjan, Andre? F. T. Martins, and
Noah A. Smith. 2012. An exact dual
decomposition algorithm for shallow
semantic parsing with constraints.
In Proceedings of the First Joint Conference
on Lexical and Computational Semantics,
pages 209?217, Montre?al.
51
Computational Linguistics Volume 40, Number 1
Das, Dipanjan and Slav Petrov. 2011.
Unsupervised part-of-speech tagging
with bilingual graph-based projections.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 600?609, Portland, OR.
Das, Dipanjan, Nathan Schneider, Desai
Chen, and Noah A. Smith. 2010.
Probabilistic frame-semantic parsing.
In Proceedings of the Human Language
Technologies Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 948?956,
Los Angeles, CA.
Das, Dipanjan and Noah A. Smith. 2011.
Semi-supervised frame-semantic parsing
for unknown predicates. In Proceedings of
the 49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 1,435?1,444,
Portland, OR.
Das, Dipanjan and Noah A. Smith. 2012.
Graph-based lexicon expansion with
sparsity-inducing penalties. In Proceedings
of the Human Language Technologies
Conference of the North American Chapter
of the Association for Computational
Linguistics, pages 677?687, Montre?al.
Dean, Jeffrey and Sanjay Ghemawat. 2008.
MapReduce: Simplified data processing
on large clusters. Communications of the
ACM, 51(1):107?113.
DeNero, John and Klaus Macherey. 2011.
Model-based aligner combination using
dual decomposition. In Proceedings of
the 49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 420?429,
Portland, OR.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the Latent Words Language
Model. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language
Processing, pages 21?29, Singapore.
Dhillon, Paramveer S., Partha Pratim
Talukdar, and Koby Crammer. 2010.
Learning better data representation
using inference-driven metric
learning. In Proceedings of the ACL
2010 Conference Short Papers,
pages 377?381, Uppsala.
Erk, Katrin and Sebastian Pado?. 2006.
Shalmaneser?a toolchain for shallow
semantic parsing. In Proceedings of the
Fifth International Conference on Language
Resources and Evaluation, pages 527?532,
Genoa.
Fellbaum, Christiane, editor. 1998. WordNet:
an electronic lexical database. MIT Press,
Cambridge, MA.
Fillmore, Charles J. 1982. Frame semantics.
In Linguistics in the Morning Calm.
Hanshin Publishing Co., Seoul,
pages 111?137.
Fillmore, Charles J., Christopher R. Johnson,
and Miriam R. L. Petruck. 2003.
Background to FrameNet. International
Journal of Lexicography, 16.3:235?250.
Fleischman, Michael, Namhee Kwon, and
Eduard Hovy. 2003. Maximum entropy
models for FrameNet classification.
In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language
Processing, pages 49?56, Sapporo.
Fung, Pascale and Benfeng Chen. 2004.
BiFrameNet: Bilingual frame semantics
resource construction by cross-lingual
induction. In Proceedings of the 20th
International Conference on Computational
Linguistics, pages 931?937, Geneva.
Fu?rstenau, Hagen and Mirella Lapata. 2009a.
Graph alignment for semi-supervised
semantic role labeling. In Proceedings of the
2009 Conference on Empirical Methods in
Natural Language Processing, pages 11?20,
Singapore.
Fu?rstenau, Hagen and Mirella Lapata. 2009b.
Semi-supervised semantic role labeling.
In Proceedings of the 12th Conference of the
European Chapter of the ACL, pages 220?228,
Athens.
Fu?rstenau, Hagen and Mirella Lapata. 2012.
Semi-supervised semantic role labeling
via structural alignment. Computational
Linguistics, 38(1):135?171.
Gerber, Matthew and Joyce Chai. 2010.
Beyond NomBank: A study of implicit
arguments for nominal predicates. In
Proceedings of ACL, pages 1,583?1,592,
Uppsala.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Girju, Roxana, Preslav Nakov, Vivi Nastase,
Stan Szpakowicz, Peter Turney, and Deniz
Yuret. 2007. SemEval-2007 task 04:
Classification of semantic relations
between nominals. In Proceedings of the
Fourth International Workshop on Semantic
Evaluations, pages 13?18, Prague.
Giuglea, Ana-Maria and Alessandro
Moschitti. 2006. Shallow semantic
parsing based on FrameNet, VerbNet
and PropBank. In Proceedings of the
17th European Conference on Artificial
Intelligence, pages 563?567, Amsterdam.
52
Das et al. Frame-Semantic Parsing
Gropp, W., E. Lusk, and A. Skjellum. 1994.
Using MPI: Portable Parallel Programming
with the Message-Passing Interface.
MIT Press, Cambridge, MA.
Hajic?, Jan, Massimiliano Ciaramita,
Richard Johansson, Daisuke Kawahara,
Maria Anto`nia Mart??, Llu??s Ma`rquez,
Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai
Surdeanu, Nianwen Xue, and Yi Zhang.
2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in
multiple languages. In Proceedings of the
Thirteenth Conference on Computational
Natural Language Learning, pages 1?18,
Boulder, CO.
Ide, Nancy and Jean Ve?ronis. 1998.
Introduction to the special issue on word
sense disambiguation: The state of the art.
Computational Linguistics, 24(1):2?40.
Johansson, Richard and Pierre Nugues.
2007. LTH: Semantic structure extraction
using nonprojective dependency trees.
In Proceedings of the 4th International
Workshop on Semantic Evaluations,
pages 227?230, Prague.
Johansson, Richard and Pierre Nugues. 2008.
Dependency-based semantic role labeling
of PropBank. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 69?78,
Honolulu, HI.
Kingsbury, Paul and Martha Palmer. 2002.
From TreeBank to PropBank. In Proceedings
of the 3rd International Conference on
Language Resources and Evaluation,
pages 1,989?1,993, Las Palmas.
Komodakis, Nikos, Nikos Paragios, and
Georgios Tziritas. 2007. MRF optimization
via dual decomposition: Message-passing
revisited. In Eleventh International
Conference on Computer Vision, pages 1?8,
Rio de Janeiro.
Koo, Terry, Alexander M. Rush, Michael
Collins, Tommi Jaakkola, and David
Sontag. 2010. Dual decomposition for
parsing with non-projective head
automata. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,288?1,298,
Cambridge, MA.
Kowalski, Matthieu and Bruno Torre?sani.
2009. Sparsity and persistence: Mixed
norms provide simple signal models with
dependent coefficients. Signal, Image and
Video Processing, 3:251?264.
Lang, Joel and Mirella Lapata. 2010.
Unsupervised induction of semantic roles.
In Proceedings of the Human Language
Technologies Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 939?947,
Los Angeles, CA.
Lang, Joel and Mirella Lapata. 2011.
Unsupervised semantic role induction
with graph partitioning. In Proceedings
of the 2011 Conference on Empirical
Methods in Natural Language Processing,
pages 1320?1331, Edinburgh.
Lin, Dekang. 1993. Principle-based parsing
without overgeneration. In Proceedings of
the 31st Annual Meeting of the Association for
Computational Linguistics, pages 112?120,
Columbus, OH.
Lin, Dekang. 1994. Principar?an efficient,
broad-coverage, principle-based parser.
In Proceedings of the 15th Conference on
Computational Linguistics, pages 482?488,
Kyoto.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th
International Conference on Computational
Linguistics, pages 768?774, Montreal.
Lin, Jianhua. 1991. Divergence measures
based on the Shannon entropy. IEEE
Transactions on Information Theory,
37(1):145?151.
Litkowski, Kenneth C. and Orin Hargraves.
2007. SemEval-2007 task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 24?29, Prague.
Liu, Dong C. and Jorge Nocedal. 1989. On
the limited memory BFGS method for
large scale optimization. Mathematical
Programming, 45(3):503?528.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: the Penn treebank. Computational
Linguistics, 19(2):313?330, June.
Ma`rquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008.
Semantic role labeling: an introduction to
the special issue. Computational Linguistics,
34(2):145?159, June.
Martins, Andre? F. T., Mario A. T. Figueiredo,
Pedro M. Q. Aguiar, Noah A. Smith, and
Eric P. Xing. 2011a. An augmented
Lagrangian approach to constrained MAP
inference. In Proceedings of the 28th
International Conference on Machine
Learning, pages 169?176, Bellevue, WA.
Martins, Andre? F. T., Noah A. Smith, Pedro
M. Q. Aguiar, and Mario A. T. Figueiredo.
53
Computational Linguistics Volume 40, Number 1
2011b. Dual decomposition with many
overlapping components. In Proceedings
of the 2011 Conference on Empirical
Methods in Natural Language Processing,
pages 238?249, Edinburgh.
Martins, Andre? F. T., Noah A. Smith, and
Eric P. Xing. 2009. Concise integer
linear programming formulations for
dependency parsing. In Proceedings of
the Joint Conference of the 47th Annual
Meeting of the Association for Computational
Linguistics and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP, pages 342?350,
Suntec.
Martins, Andre? F. T., Noah A. Smith, Eric P.
Xing, Mario A. T. Figueiredo, and Pedro
M. Q. Aguiar. 2010. Turbo parsers:
Dependency parsing by approximate
variational inference. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing, pages 34?44,
Cambridge, MA.
Matsubayashi, Yuichiroh, Naoaki Okazaki,
and Jun?ichi Tsujii. 2009. A comparative
study on generalization of semantic roles
in FrameNet. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the
Association for Computational Linguistics and
the 4th International Joint Conference on
Natural Language Processing of the AFNLP,
pages 19?27, Suntec.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 91?98, Ann Arbor, MI.
Meyers, Adam, Ruth Reeves, Catherine
Macleod, Rachel Szekely, Veronika
Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project:
An interim report. In Proceedings of the
NAACL-HLT Workshop on Frontiers
in Corpus Annotation, pages 24?31,
Boston, MA.
Moschitti, Alessandro, Paul Morarescu, and
Sanda M. Harabagiu. 2003. Open-domain
information extraction via automatic
semantic labeling. In Ingrid Russell and
Susan M. Haller, editors, Proceedings of the
Sixteenth International Florida Artificial
Intelligence Research Society Conference,
pages 397?401, St. Augustine, FL.
Narayanan, Srini and Sanda Harabagiu.
2004. Question answering based on
semantic structures. In Proceedings of
the 20th International Conference on
Computational Linguistics, Geneva.
Pado?, Sebastian and Katrin Erk. 2005.
To cause or not to cause: cross-lingual
semantic matching for paraphrase
modelling. In Proceedings of the
Cross-Language Knowledge Induction
Workshop, Cluj-Napoca.
Pado, Sebastian and Mirella Lapata. 2005.
Cross-linguistic projection of role-semantic
information. In Proceedings of Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 859?866,
Vancouver.
Pennacchiotti, Marco, Diego De Cao, Roberto
Basili, Danilo Croce, and Michael Roth.
2008. Automatic induction of FrameNet
lexical units. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 457?465,
Honolulu, HI.
Pradhan, Sameer S., Wayne H. Ward,
Kadri Hacioglu, James H. Martin, and
Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines.
In Proceedings of the Human Language
Technologies Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 233?240,
Boston, MA.
Punyakanok, Vasin, Dan Roth, and Wen-tau
Yih. 2008. The importance of syntactic
parsing and inference in semantic role
labeling. Computational Linguistics,
34(2):257?287.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
and Dav Zimak. 2004. Semantic role
labeling via integer linear programming
inference. In Proceedings of the 20th
International Conference on Computational
Linguistics, pages 1,346?1,352, Geneva.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech tagging.
In Proceedings of the 1996 Empirical
Methods in Natural Language Processing,
pages 133?142, Copenhagen.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 129?137, Sydney.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks.
In Proceedings of the Eighth Conference on
Computational Natural Language Learning,
pages 1?8, Boston, MA.
Ruppenhofer, Josef, Michael Ellsworth,
Miriam R. L. Petruck, Christopher R.
54
Das et al. Frame-Semantic Parsing
Johnson, and Jan Scheffczyk. 2006.
FrameNet II: extended theory and
practice. International Computer Science
Institute, Berkeley, CA.
Rush, Alexander M. and Michael Collins.
2011. Exact decoding of syntactic
translation models through Lagrangian
relaxation. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 72?82, Portland, OR.
Rush, Alexander M, David Sontag, Michael
Collins, and Tommi Jaakkola. 2010.
On dual decomposition and linear
programming relaxations for natural
language processing. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing, pages 1?11,
Cambridge, MA.
Schuler, Karin K. 2005. VerbNet: a
broad-coverage, comprehensive verb lexicon.
Ph.D. thesis, University of Pennsylvania.
Sha, Fei and Fernando Pereira. 2003. Shallow
parsing with conditional random fields.
In Proceedings of the Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 134?141, Edmonton.
Shen, Dan and Mirella Lapata. 2007. Using
semantic roles to improve question
answering. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning, pages 12?21,
Prague.
Shi, Lei and Rada Mihalcea. 2004. An
algorithm for open text semantic parsing.
In Proceedings of Workshop on Robust
Methods in Analysis of Natural Language
Data, pages 59?67, Geneva.
Shi, Lei and Rada Mihalcea. 2005. Putting
pieces together: Combining FrameNet,
VerbNet and WordNet for robust
semantic parsing. In Proceedings of the 6th
International Conference on Computational
Linguistics and Intelligent Text Processing,
pages 100?111, Mexico City.
Smith, David A. and Jason Eisner. 2008.
Dependency parsing by belief
propagation. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 145?156,
Honolulu, HI.
Subramanya, Amarnag and Jeff Bilmes.
2008. Soft-supervised learning for text
classification. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 1,090?1,099,
Honolulu, HI.
Subramanya, Amarnag, Slav Petrov, and
Fernando Pereira. 2010. Efficient
graph-based semi-supervised learning of
structured tagging models. In Proceedings
of the 2010 Conference on Empirical Methods
in Natural Language Processing,
pages 167?176, Cambridge, MA.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth. 2003.
Using predicate-argument structures for
information extraction. In Proceedings of the
41st Annual Meeting on Association for
Computational Linguistics, pages 8?15,
Sapporo.
Surdeanu, Mihai, Richard Johansson,
Adam Meyers, Llu??s Ma`rquez, and Joakim
Nivre. 2008. The CoNLL 2008 shared task
on joint parsing of syntactic and semantic
dependencies. In Proceedings of the Twelfth
Conference on Computational Natural
Language Learning, pages 159?177,
Manchester.
Szummer, Martin and Tommi Jaakkola.
2001. Partially labeled classification with
Markov random walks. In Advances in
Neural Information Processing Systems 14,
pages 945?952, Vancouver.
Talukdar, Partha Pratim and Koby Crammer.
2009. New regularized algorithms for
transductive learning. In Proceedings of the
European Conference on Machine Learning
and Knowledge Discovery in Databases,
pages 442?457, Bled.
Thompson, Cynthia A., Roger Levy,
and Christopher D. Manning. 2003.
A generative model for semantic role
labeling. In Proceedings of the European
Conference on Machine Learning,
pages 397?408, Cavtat-Dubrovnik.
Titov, Ivan and Alexandre Klementiev. 2012.
A Bayesian approach to unsupervised
semantic role induction. In Proceedings of
the 13th Conference of the European Chapter of
the Association for Computational Linguistics,
pages 12?22, Avignon.
Toutanova, Kristina, Aria Haghighi,
and Christopher Manning. 2005.
Joint learning improves semantic
role labeling. In Proceedings of the
43rd Annual Meeting of the Association
for Computational Linguistics,
pages 589?596, Ann Arbor, MI.
Turian, Joseph, Lev-Arie Ratinov,
and Yoshua Bengio. 2010. Word
representations: A simple and general
method for semi-supervised learning.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics, pages 384?394, Uppsala.
55
Computational Linguistics Volume 40, Number 1
Weston, Jason, Fre?de?ric Ratle, and
Ronan Collobert. 2008. Deep learning
via semi-supervised embedding.
In Proceedings of the 25th International
Conference on Machine Learning,
pages 1,168?1,175, Helsinki.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of the 2004
Conference on Empirical Methods in Natural
Language Processing, pages 88?94,
Barcelona.
Yi, Szu-ting, Edward Loper, and Martha
Palmer. 2007. Can semantic roles
generalize across genres? In Proceedings of
the Human Language Technologies Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 548?555, Rochester, NY.
Zhu, Xiaojin. 2008. Semi-supervised
learning literature survey. Available at
http://pages.cs.wisc.edu/?jerryzhu/
pub/ssl survey.pdf. Last Accessed
July 2013.
Zhu, Xiaojin, Zoubin Ghahramani, and
John Lafferty. 2003. Semi-supervised
learning using Gaussian fields and
harmonic functions. In Proceedings of the
20th International Conference on Machine
Learning, pages 912?919, Washington, DC.
56
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 293?296,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Movie Reviews and Revenues: An Experiment in Text Regression?
Mahesh Joshi Dipanjan Das Kevin Gimpel Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{maheshj,dipanjan,kgimpel,nasmith}@cs.cmu.edu
Abstract
We consider the problem of predicting a
movie?s opening weekend revenue. Previous
work on this problem has used metadata about
a movie?e.g., its genre, MPAA rating, and
cast?with very limited work making use of
text about the movie. In this paper, we use
the text of film critics? reviews from several
sources to predict opening weekend revenue.
We describe a new dataset pairing movie re-
views with metadata and revenue data, and
show that review text can substitute for meta-
data, and even improve over it, for prediction.
1 Introduction
Predicting gross revenue for movies is a problem
that has been studied in economics, marketing,
statistics, and forecasting. Apart from the economic
value of such predictions, we view the forecasting
problem as an application of NLP. In this paper, we
use the text of critics? reviews to predict opening
weekend revenue. We also consider metadata for
each movie that has been shown to be successful for
similar prediction tasks in previous work.
There is a large body of prior work aimed at pre-
dicting gross revenue of movies (Simonoff and Spar-
row, 2000; Sharda and Delen, 2006; inter alia). Cer-
tain information is used in nearly all prior work on
these tasks, such as the movie?s genre, MPAA rating,
running time, release date, the number of screens on
which the movie debuted, and the presence of partic-
ular actors or actresses in the cast. Most prior text-
based work has used automatic text analysis tools,
deriving a small number of aggregate statistics. For
example, Mishne and Glance (2006) applied sen-
timent analysis techniques to pre-release and post-
release blog posts about movies and showed higher
?We appreciate reviewer feedback and technical advice
from Brendan O?Connor. This work was supported by NSF IIS-
0803482, NSF IIS-0844507, and DARPA NBCH-1080004.
correlation between actual revenue and sentiment-
based metrics, as compared to mention counts of the
movie. (They did not frame the task as a revenue
prediction problem.) Zhang and Skiena (2009) used
a news aggregation system to identify entities and
obtain domain-specific sentiment for each entity in
several domains. They used the aggregate sentiment
scores and mention counts of each movie in news
articles as predictors.
While there has been substantial prior work on
using critics? reviews, to our knowledge all of this
work has used polarity of the review or the number
of stars given to it by a critic, rather than the review
text directly (Terry et al, 2005).
Our task is related to sentiment analysis (Pang et
al., 2002) on movie reviews. The key difference is
that our goal is to predict a future real-valued quan-
tity, restricting us from using any post-release text
data such as user reviews. Further, the most im-
portant clues about revenue may have little to do
with whether the reviewer liked the movie, but rather
what the reviewer found worth mentioning. This pa-
per is more in the tradition of Ghose et al (2007) and
Kogan et al (2009), who used text regression to di-
rectly quantify review ?value? and make predictions
about future financial variables, respectively.
Our aim in using the full text is to identify partic-
ular words and phrases that predict the movie-going
tendencies of the public. We can also perform syn-
tactic and semantic analysis on the text to identify
richer constructions that are good predictors. Fur-
thermore, since we consider multiple reviews for
each movie, we can compare these features across
reviews to observe how they differ both in frequency
and predictive performance across different media
outlets and individual critics.
In this paper, we use linear regression from text
and non-text (meta) features to directly predict gross
revenue aggregated over the opening weekend, and
the same averaged per screen.
293
Domain train dev test total
Austin Chronicle 306 94 62 462
Boston Globe 461 154 116 731
LA Times 610 2 13 625
Entertainment Weekly 644 208 187 1039
New York Times 878 273 224 1375
Variety 927 297 230 1454
Village Voice 953 245 198 1396
# movies 1147 317 254 1718
Table 1: Total number of reviews from each domain for
the training, development and test sets.
2 Data
We gathered data for movies released in 2005?2009.
For these movies, we obtained metadata and a list
of hyperlinks to movie reviews by crawling Meta-
Critic (www.metacritic.com). The metadata
include the name of the movie, its production house,
the set of genres it belongs to, the scriptwriter(s),
the director(s), the country of origin, the primary
actors and actresses starring in the movie, the re-
lease date, its MPAA rating, and its running time.
From The Numbers (www.the-numbers.com),
we retrieved each movie?s production budget, open-
ing weekend gross revenue, and the number of
screens on which it played during its opening week-
end. Only movies found on both MetaCritic and The
Numbers were included.
Next we chose seven review websites that most
frequently appeared in the review lists for movies at
Metacritic, and obtained the text of the reviews by
scraping the raw HTML. The sites chosen were the
Austin Chronicle, the Boston Globe, the LA Times,
Entertainment Weekly, the New York Times, Vari-
ety, and the Village Voice. We only chose those
reviews that appeared on or before the release date
of the movie (to ensure that revenue information is
not present in the review), arriving at a set of 1718
movies with at least one review. We partitioned this
set of movies temporally into training (2005?2007),
development (2008) and test (2009) sets. Not all
movies had reviews at all sites (see Table 1).
3 Predictive Task
We consider two response variables, both in
U.S. dollars: the total revenue generated by a movie
during its release weekend, and the per screen rev-
enue during the release weekend. We evaluate these
predictions using (1) mean absolute error (MAE) in
U.S. dollars and (2) Pearson?s correlation between
the actual and predicted revenue.
We use linear regression to directly predict the
opening weekend gross earnings, denoted y, based
on features x extracted from the movie metadata
and/or the text of the reviews. That is, given an input
feature vector x ? Rp, we predict an output y? ? R
using a linear model: y? = ?0 + x>?. To learn val-
ues for the parameters ? = ??0,??, the standard
approach is to minimize the sum of squared errors
for a training set containing n pairs ?xi, yi? where
xi ? Rp and yi ? R for 1 ? i ? n:
?? = argmin
?=(?0,?)
1
2n
n?
i=1
(
yi ? (?0 + x>i ?)
)2
+?P (?)
A penalty term P (?) is included in the objective for
regularization. Classical solutions use an `2 or `1
norm, known respectively as ridge and lasso regres-
sion. Introduced recently is a mixture of the two,
called the elastic net (Zou and Hastie, 2005):
P (?) =
?p
j=1
(
1
2(1? ?)?
2
j + ?|?j |
)
where ? ? (0, 1) determines the trade-off be-
tween `1 and `2 regularization. For our experi-
ments we used the elastic net and specifically the
glmnet package which contains an implementa-
tion of an efficient coordinate ascent procedure for
training (Friedman et al, 2008).
We tune the ? and ? parameters on our develop-
ment set and select the model with the ??, ?? com-
bination that yields minimum MAE on the develop-
ment set.
4 Experiments
We compare predictors based on metadata, predic-
tors based on text, and predictors that use both kinds
of information. Results for two simple baselines of
predicting the training set mean and median are re-
ported in Table 2 (Pearson?s correlation is undefined
since the standard deviation is zero).
4.1 Metadata Features
We considered seven types of metadata features, and
evaluated their performance by adding them to our
pool of features in the following order: whether the
294
film is of U.S. origin, running time (in minutes), the
logarithm of its budget, # opening screens, genre
(e.g., Action, Comedy) and MPAA rating (e.g., G,
PG, PG-13), whether the movie opened on a holiday
weekend or in summer months, total count as well as
of presence of individual Oscar-winning actors and
directors and high-grossing actors. For the first task
of predicting the total opening weekend revenue of
a movie, the best-performing feature set in terms of
MAE turned out to be all the features. However, for
the second task of predicting the per screen revenue,
addition of the last feature subset consisting of infor-
mation related to the actors and directors hurt perfor-
mance (MAE increased). Therefore, for the second
task, the best performing set contained only the first
six types of metadata features.
4.2 Text Features
We extract three types of text features (described be-
low). We only included feature instances that oc-
curred in at least five different movies? reviews. We
stem and downcase individual word components in
all our features.
I. n-grams. We considered unigrams, bigrams, and
trigrams. A 25-word stoplist was used; bigrams
and trigrams were only filtered if all words were
stopwords.
II. Part-of-speech n-grams. As with words, we
added unigrams, bigrams, and trigrams. Tags
were obtained from the Stanford part-of-speech
tagger (Toutanova and Manning, 2000).
III. Dependency relations. We used the Stanford
parser (Klein and Manning, 2003) to parse the
critic reviews and extract syntactic dependen-
cies. The dependency relation features consist
of just the relation part of a dependency triple
?relation, head word, modifier word?.
We consider three ways to combine the collec-
tion of reviews for a given movie. The first (???)
simply concatenates all of a movie?s reviews into
a single document before extracting features. The
second (?+?) conjoins each feature with the source
site (e.g., New York Times) from whose review it was
extracted. A third version (denoted ?B?) combines
both the site-agnostic and site-specific features.
Features Site
Total Per Screen
MAE MAE
($M) r ($K) r
Predict mean 11.672 ? 6.862 ?
Predict median 10.521 ? 6.642 ?
m
et
a
Best 5.983 0.722 6.540 0.272
te
xt
I
? 8.013 0.743 6.509 0.222
+ 7.722 0.781 6.071 0.466
see Tab. 3 B 7.627 0.793 6.060 0.411
I ? II
? 8.060 0.743 6.542 0.233
+ 7.420 0.761 6.240 0.398
B 7.447 0.778 6.299 0.363
I ? III
? 8.005 0.744 6.505 0.223
+ 7.721 0.785 6.013 0.473
B 7.595 0.796 ?6.010 0.421
m
et
a
?
te
xt
I
? 5.921 0.819 6.509 0.222
+ 5.757 0.810 6.063 0.470
B 5.750 0.819 6.052 0.414
I ? II
? 5.952 0.818 6.542 0.233
+ 5.752 0.800 6.230 0.400
B 5.740 0.819 6.276 0.358
I ? III
? 5.921 0.819 6.505 0.223
+ 5.738 0.812 6.003 0.477
B 5.750 0.819 ?5.998 0.423
Table 2: Test-set performance for various models, mea-
sured using mean absolute error (MAE) and Pearson?s
correlation (r), for two prediction tasks. Within a column,
boldface shows the best result among ?text? and ?meta ?
text? settings. ?Significantly better than the meta baseline
with p < 0.01, using the Wilcoxon signed rank test.
4.3 Results
Table 2 shows our results for both prediction tasks.
For the total first-weekend revenue prediction task,
metadata features baseline result (r2 = 0.521) is
comparable to that reported by Simonoff and Spar-
row (2000) on a similar task of movie gross predic-
tion (r2 = 0.446). Features from critics? reviews
by themselves improve correlation on both predic-
tion tasks, however improvement in MAE is only
observed for the per screen revenue prediction task.
A combination of the meta and text features
achieves the best performance both in terms of MAE
and r. While the text-only models have some high
negative weight features, the combined models do
not have any negatively weighted features and only
a very few metadata features. That is, the text is able
to substitute for the other metadata features.
Among the different types of text-based features
that we tried, lexical n-grams proved to be a strong
baseline to beat. None of the ?I ? ?? feature sets are
significantly better than n-grams alone, but adding
295
the dependency relation features (set III) to the n-
grams does improve the performance enough to
make it significantly better than the metadata-only
baseline for per screen revenue prediction.
Salient Text Features: Table 3 lists some of the
highly weighted features, which we have catego-
rized manually. The features are from the text-only
model annotated in Table 2 (total, not per screen).
The feature weights can be directly interpreted as
U.S. dollars contributed to the predicted value y? by
each occurrence of the feature. Sentiment-related
features are not as prominent as might be expected,
and their overall proportion in the set of features
with non-zero weights is quite small (estimated in
preliminary trials at less than 15%). Phrases that
refer to metadata are the more highly weighted
and frequent ones. Consistent with previous re-
search, we found some positively-oriented sentiment
features to be predictive. Some other prominent
features not listed in the table correspond to spe-
cial effects (?Boston Globe: of the art?, ?and cgi?),
particular movie franchises (?shrek movies?, ?Vari-
ety: chronicle of?, ?voldemort?), hype/expectations
(?blockbuster?, ?anticipation?), film festival (?Vari-
ety: canne? with negative weight) and time of re-
lease (?summer movie?).
5 Conclusion
We conclude that text features from pre-release re-
views can substitute for and improve over a strong
metadata-based first-weekend movie revenue pre-
diction. The dataset used in this paper has been
made available for research at http://www.
ark.cs.cmu.edu/movie$-data.
References
J. Friedman, T. Hastie, and R. Tibshirani. 2008. Regular-
ized paths for generalized linear models via coordinate
descent. Technical report, Stanford University.
A. Ghose, P. G. Ipeirotis, and A. Sundararajan. 2007.
Opinion mining using econometrics: A case study on
reputation systems. In Proc. of ACL.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
S. Kogan, D. Levin, B. R. Routledge, J. Sagi, and N. A.
Smith. 2009. Predicting risk from financial reports
with regression. In Proc. of NAACL, pages 272?280.
Feature Weight ($M)
ra
ti
ng
pg +0.085
New York Times: adult -0.236
New York Times: rate r -0.364
se
qu
el
s this series +13.925
LA Times: the franchise +5.112
Variety: the sequel +4.224
pe
op
le Boston Globe: will smith +2.560
Variety: brittany +1.128
? producer brian +0.486
ge
nr
e
Variety: testosterone +1.945
Ent. Weekly: comedy for +1.143
Variety: a horror +0.595
documentary -0.037
independent -0.127
se
nt
im
en
t Boston Globe: best parts of +1.462
Boston Globe: smart enough +1.449
LA Times: a good thing +1.117
shame $ -0.098
bogeyman -0.689
pl
ot
Variety: torso +9.054
vehicle in +5.827
superhero $ +2.020
Table 3: Highly weighted features categorized manu-
ally. ? and $ denote sentence boundaries. ?brittany?
frequently refers to Brittany Snow and Brittany Murphy.
?? producer brian? refers to producer Brian Grazer (The
Da Vinci Code, among others).
G. Mishne and N. Glance. 2006. Predicting movie sales
from blogger sentiment. In AAAI Spring Symposium
on Computational Approaches to Analysing Weblogs.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP, pages 79?86.
R. Sharda and D. Delen. 2006. Predicting box office suc-
cess of motion pictures with neural networks. Expert
Systems with Applications, 30(2):243?254.
J. S. Simonoff and I. R. Sparrow. 2000. Predicting movie
grosses: Winners and losers, blockbusters and sleep-
ers. Chance, 13(3):15?24.
N. Terry, M. Butler, and D. De?Armond. 2005. The de-
terminants of domestic box office performance in the
motion picture industry. Southwestern Economic Re-
view, 32:137?148.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. of EMNLP, pages 63?70.
W. Zhang and S. Skiena. 2009. Improving movie gross
prediction through news analysis. In Web Intelligence,
pages 301?304.
H. Zou and T. Hastie. 2005. Regularization and variable
selection via the elastic net. Journal Of The Royal Sta-
tistical Society Series B, 67(5):768?768.
296
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 948?956,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Probabilistic Frame-Semantic Parsing
Dipanjan Das Nathan Schneider Desai Chen Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dipanjan@cs,nschneid@cs,desaic@andrew,nasmith@cs}.cmu.edu
Abstract
This paper contributes a formalization of
frame-semantic parsing as a structure predic-
tion problem and describes an implemented
parser that transforms an English sentence
into a frame-semantic representation. It finds
words that evoke FrameNet frames, selects
frames for them, and locates the arguments
for each frame. The system uses two feature-
based, discriminative probabilistic (log-linear)
models, one with latent variables to permit
disambiguation of new predicate words. The
parser is demonstrated to significantly outper-
form previously published results.
1 Introduction
FrameNet (Fillmore et al, 2003) is a rich linguistic
resource containing considerable information about
lexical and predicate-argument semantics in En-
glish. Grounded in the theory of frame semantics
(Fillmore, 1982), it suggests?but does not formally
define?a semantic representation that blends word-
sense disambiguation and semantic role labeling.
In this paper, we present a computational and
statistical model for frame-semantic parsing, the
problem of extracting from text semantic predicate-
argument structures such as those shown in Fig. 1.
We aim to predict a frame-semantic representation
as a structure, not as a pipeline of classifiers. We
use a probabilistic framework that cleanly integrates
the FrameNet lexicon and (currently very limited)
available training data. Although our models often
involve strong independence assumptions, the prob-
abilistic framework we adopt is highly amenable to
future extension through new features, relaxed in-
dependence assumptions, and semisupervised learn-
ing. Some novel aspects of our current approach
include a latent-variable model that permits disam-
biguation of words not in the FrameNet lexicon, a
unified model for finding and labeling arguments,
TRANSITIVE_
ACTION
Agent
Patient
Event
Cause
Place
Time
CAUSE_TO_
MAKE_NOISE
Agent
Sound_maker
Cause
Place
Time
MAKE_NOISE
Noisy_event
Sound
Sound_source
Place
Time
cough.v, gobble.v, 
ring.v, yodel.v, ...
blare.v, play.v, 
ring.v, toot.v, ...
?
Inheritance relation Causative_of relation
Excludes relation
Purpose
Figure 2. Partial illustration of frames, roles, and LUs
related to the CAUSE TO MAKE NOISE frame, from the
FrameNet lexicon. ?Core? roles are filled ovals. 8 addi-
tional roles of CAUSE TO MAKE NOISE are not shown.
and a precision-boosting constraint that forbids ar-
guments of the same predicate to overlap. Our parser
achieves the best published results to date on the
SemEval?07 FrameNet task (Baker et al, 2007).
2 Resources and Task
We consider frame-semantic parsing resources.
2.1 FrameNet Lexicon
The FrameNet lexicon is a taxonomy of manu-
ally identified general-purpose frames for English.1
Listed in the lexicon with each frame are several
lemmas (with part of speech) that can denote the
frame or some aspect of it?these are called lexi-
cal units (LUs). In a sentence, word or phrase to-
kens that evoke a frame are known as targets. The
set of LUs listed for a frame in FrameNet may not
be exhaustive; we may see a target in new data that
does not correspond to an LU for the frame it evokes.
Each frame definition also includes a set of frame el-
ements, or roles, corresponding to different aspects
of the concept represented by the frame, such as par-
ticipants, props, and attributes. We use the term ar-
1Like the SemEval?07 participants, we used FrameNet v. 1.3
(http://framenet.icsi.berkeley.edu).
948
bell.n
ring.v
there be.v
enough.a
LU
NOISE_MAKERS
SUFFICIENCY
Frame
EXISTENCE
CAUSE_TO_MAKE_NOISE
.bells
 
 
N_m
more than six of the eight
Sound_maker
Enabled_situation
ringtoringers
Item
enough
Entity
Agent
n'tarestillthereBut
Figure 1. A sentence from PropBank and the SemEval?07 training data, and a partial depiction of gold FrameNet
annotations. Each frame is a row below the sentence (ordered for readability). Thick lines indicate targets that evoke
frames; thin solid/dotted lines with labels indicate arguments. ?N m? under bells is short for the Noise maker role of
the NOISE MAKERS frame. The last row indicates that there. . . are is a discontinuous target. In PropBank, the verb
ring is the only annotated predicate for this sentence, and it is not related to other predicates with similar meanings.
FRAMENET LEXICON V. 1.3
lexical exemplars
entries counts coverage
8379 LUs 139K sentences, 3.1M words 70% LUs
795 frames 1 frame annotation / sentence 63% frames
7124 roles 285K overt arguments 56% roles
Table 1. Snapshot of lexicon entries and exemplar sen-
tences. Coverage indicates the fraction of types attested
in at least one exemplar.
gument to refer to a sequence of word tokens anno-
tated as filling a frame role. Fig. 1 shows an exam-
ple sentence from the training data with annotated
targets, LUs, frames, and role-argument pairs. The
FrameNet lexicon also provides information about
relations between frames and between roles (e.g.,
INHERITANCE). Fig. 2 shows a subset of the rela-
tions between three frames and their roles.
Accompanying most frame definitions in the
FrameNet lexicon is a set of lexicographic exemplar
sentences (primarily from the British National Cor-
pus) annotated for that frame. Typically chosen to il-
lustrate variation in argument realization patterns for
the frame in question, these sentences only contain
annotations for a single frame. We found that using
exemplar sentences directly to train our models hurt
performance as evaluated on SemEval?07 data, even
though the number of exemplar sentences is an order
of magnitude larger than the number of sentences in
our training set (?2.2). This is presumably because
the exemplars are neither representative as a sample
nor similar to the test data. Instead, we make use of
these exemplars in features (?4.2).
2.2 Data
Our training, development, and test sets consist
of documents annotated with frame-semantic struc-
tures for the SemEval?07 task, which we refer to col-
FULL-TEXT SemEval?07 data
ANNOTATIONS train dev test
Size (words sentences documents)
all 43.3K1.7K 22 6.3K 251 4 2.8K 120 3
ANC (travel) 3.9K 154 2 .8K 32 1 1.3K 67 1
NTI (bureaucratic) 32.2K1.2K 15 5.5K 219 3 1.5K 53 2
PropBank (news) 7.3K 325 5 0 0 0 0 0 0
Annotations (frames/word overt arguments/word)
all 0.23 0.39 0.22 0.37 0.37 0.65
Coverage of lexicon (% frames % roles % LUs)
all 64.1 27.4 21.0 34.0 10.2 7.3 29.3 7.7 4.9
Out-of-lexicon types (frames roles LUs)
all 14 69 71 2 4 2 39 99 189
Out-of-lexicon tokens (% frames % roles % LUs)
all 0.7 0.9 1.1 1.0 0.4 0.2 9.8 11.2 25.3
Table 2. Snapshot of the SemEval?07 annotated data.
lectively as the SemEval?07 data.2 For the most
part, the frames and roles used in annotating these
documents were defined in the FrameNet lexicon,
but there are some exceptions for which the annota-
tors defined supplementary frames and roles; these
are included in the possible output of our parser.
Table 2 provides a snapshot of the SemEval?07
data. We randomly selected three documents from
the original SemEval training data to create a devel-
opment set for tuning model hyperparameters. No-
tice that the test set contains more annotations per
word, both in terms of frames and arguments. More-
over, there are many more out-of-lexicon frame,
role, and LU types in the test set than in the training
set. This inconsistency in the data results in poor re-
call scores for all models trained on the given data
split, a problem we have not sought to address here.
2http://framenet.icsi.berkeley.edu/
semeval/FSSE.html
949
Preprocessing. We preprocess sentences in our
dataset with a standard set of annotations: POS
tags from MXPOST (Ratnaparkhi, 1996) and depen-
dency parses from the MST parser (McDonald et al,
2005) since manual syntactic parses are not available
for most of the FrameNet-annotated documents. We
used WordNet (Fellbaum, 1998) for lemmatization.
We also labeled each verb in the data as having AC-
TIVE or PASSIVE voice, using code from the SRL
system described by Johansson and Nugues (2008).
2.3 Task and Evaluation
Automatic annotations of frame-semantic structure
can be broken into three parts: (1) targets, the words
or phrases that evoke frames; (2) the frame type,
defined in the lexicon, evoked by each target; and
(3) the arguments, or spans of words that serve to
fill roles defined by each evoked frame. These cor-
respond to the three subtasks in our parser, each
described and evaluated in turn: target identifica-
tion (?3), frame identification (?4, not unlike word-
sense disambiguation), and argument identification
(?5, not unlike semantic role labeling).
The standard evaluation script from the
SemEval?07 shared task calculates precision,
recall, and F1-measure for frames and arguments;
it also provides a score that gives partial credit
for hypothesizing a frame related to the correct
one. We present precision, recall, and F1-measure
microaveraged across the test documents, report
labels-only matching scores (spans must match
exactly), and do not use named entity labels. More
details can be found in Baker et al (2007). For our
experiments, statistical significance is measured us-
ing a reimplementation of Dan Bikel?s randomized
parsing evaluation comparator.3
2.4 Baseline
A strong baseline for frame-semantic parsing is
the system presented by Johansson and Nugues
(2007, hereafter J&N?07), the best system in the
SemEval?07 shared task. For frame identifica-
tion, they used an SVM classifier to disambiguate
frames for known frame-evoking words. They used
WordNet synsets to extend the vocabulary of frame-
evoking words to cover unknown words, and then
3http://www.cis.upenn.edu/?dbikel/
software.html#comparator
TARGET IDENTIFICATION P R F1
Our technique (?3) 89.92 70.79 79.21
Baseline: J&N?07 87.87 67.11 76.10
Table 3. Target identification results for our system and
the baseline. Scores in bold denote significant improve-
ments over the baseline (p < 0.05).
used a collection of separate SVM classifiers?one
for each frame?to predict a single evoked frame for
each occurrence of a word in the extended set.
J&N?07 modeled the argument identification
problem by dividing it into two tasks: first, they
classified candidate spans as to whether they were
arguments or not; then they assigned roles to those
that were identified as arguments. Both phases used
SVMs. Thus, their formulation of the problem in-
volves a multitude of classifiers?whereas ours uses
two log-linear models, each with a single set of
weights, to find a full frame-semantic parse.
3 Target Identification
Target identification is the problem of deciding
which word tokens (or word token sequences) evoke
frames in a given sentence. In other semantic role
labeling schemes (e.g. PropBank), simple part-of-
speech criteria typically distinguish predicates from
non-predicates. But in frame semantics, verbs,
nouns, adjectives, and even prepositions can evoke
frames under certain conditions. One complication
is that semantically-impoverished support predi-
cates (such as make in make a request) do not
evoke frames in the context of a frame-evoking,
syntactially-dependent noun (request). Further-
more, only temporal, locative, and directional senses
of prepositions evoke frames.
We found that, because the test set is more com-
pletely annotated?that is, it boasts far more frames
per token than the training data (see Table 2)?
learned models did not generalize well and achieved
poor test recall. Instead, we followed J&N?07 in us-
ing a small set of rules to identify targets.
For a span to be a candidate target, it must ap-
pear (up to morphological variation) as a target in the
training data or the lexicon. We consider multiword
targets,4 unlike J&N?07 (though we do not consider
4There are 629 multiword LUs in the lexicon, and they cor-
respond to 4.8% of the targets in the training set; among them
are screw up.V, shoot the breeze.V, and weapon of mass de-
950
FRAME IDENTIFICATION exact frame matching partial frame matching
(?4) targets P R F1 P R F1
Frame identification (oracle targets) ? 60.21 60.21 60.21 74.21 74.21 74.21
Frame identification (predicted targets) auto ?3 69.75 54.91 61.44 77.51 61.03 68.29
Baseline: J&N?07 auto 66.22 50.57 57.34 73.86 56.41 63.97
Table 4. Frame identification results. Precision, recall, and F1 were evaluated under exact and partial frame matching;
see ?2.3. Bold indicates statistically significant results with respect to the baseline (p < 0.05).
discontinuous targets). Using rules from ?3.1.1 of
J&N?07, we further prune the list, with two modi-
fications: we prune all prepositions, including loca-
tive, temporal, and directional ones, but do not prune
support verbs. This is a conservative approach; our
automatic target identifier will never propose a target
that was not seen in the training data or FrameNet.
Results. Table 3 shows results on target identifica-
tion; our system gains 3 F1 points over the baseline.
4 Frame Identification
Given targets, the parser next identifies their frames.
4.1 Lexical units
FrameNet specifies a great deal of structural infor-
mation both within and among frames. For frame
identification we make use of frame-evoking lexical
units, the (lemmatized and POS-tagged) words and
phrases listed in the lexicon as referring to specific
frames. For example, listed with the BRAGGING
frame are 10 LUs, including boast.N, boast.V, boast-
ful.A, brag.V, and braggart.N. Of course, due to pol-
ysemy and homonymy, the same LU may be associ-
ated with multiple frames; for example, gobble.V is
listed under both the INGESTION and MAKE NOISE
frames. All targets in the exemplar sentences, and
most in our training and test data, correspond to
known LUs (see Table 2).
To incorporate frame-evoking expressions found
in the training data but not the lexicon?and to avoid
the possibility of lemmatization errors?our frame
identification model will incorporate, via a latent
variable, features based directly on exemplar and
training targets rather than LUs. Let L be the set of
(unlemmatized and automatically POS-tagged) tar-
gets found in the exemplar sentences of the lexi-
con and/or the sentences in our training set. Let
Lf ? L be the subset of these targets annotated as
struction.N. In the SemEval?07 training data, there are just 99
discontinuous multiword targets (1% of all targets).
evoking a particular frame f . Let Ll and Llf de-
note the lemmatized versions of L and Lf respec-
tively. Then, we write boasted.VBD ? LBRAGGING
and boast.VBD ? LlBRAGGING to indicate that this in-
flected verb boasted and its lemma boast have been
seen to evoke the BRAGGING frame. Significantly,
however, another target, such as toot your own horn,
might be used in other data to evoke this frame. We
thus face the additional hurdle of predicting frames
for unknown words.
The SemEval annotators created 47 new frames
not present in the lexicon, out of which 14 belonged
to our training set. We considered these with the 795
frames in the lexicon when parsing new data. Pre-
dicting new frames is a challenge not yet attempted
to our knowledge (including here). Note that the
scoring metric (?2.3) gives partial credit for related
frames (e.g., a more general frame from the lexicon).
4.2 Model
For a given sentence x with frame-evoking targets t,
let ti denote the ith target (a word sequence). Let tli
denote its lemma. We seek a list f = ?f1, . . . , fm?
of frames, one per target. In our model, the set of
candidate frames for ti is defined to include every
frame f such that tli ? L
l
f?or if t
l
i 6? L
l, then every
known frame (the latter condition applies for 4.7%
of the gold targets in the development set). In both
cases, we let Fi be the set of candidate frames for
the ith target in x.
To allow frame identification for targets whose
lemmas were seen in neither the exemplars nor the
training data, our model includes an additional vari-
able, `i. This variable ranges over the seen targets
in Lfi , which can be thought of as prototypes for
the expression of the frame. Importantly, frames are
predicted, but prototypes are summed over via the
latent variable. The prediction rule requires a prob-
abilistic model over frames for a target:
fi ? argmaxf?Fi
?
`?Lf
p(f, ` | ti,x) (1)
951
We adopt a conditional log-linear model: for f ? Fi
and ` ? Lf , p?(f, ` | ti,x) =
exp?>g(f, `, ti,x)
?
f ??Fi
?
`??Lf ?
exp?>g(f ?, `?, ti,x)
(2)
where ? are the model weights, and g is a vector-
valued feature function. This discriminative formu-
lation is very flexible, allowing for a variety of (pos-
sibly overlapping) features; e.g., a feature might re-
late a frame type to a prototype, represent a lexical-
semantic relationship between a prototype and a tar-
get, or encode part of the syntax of the sentence.
Previous work has exploited WordNet for better
coverage during frame identification (Johansson and
Nugues, 2007; Burchardt et al, 2005, e.g., by ex-
panding the set of targets using synsets), and others
have sought to extend the lexicon itself (see ?6). We
differ in our use of a latent variable to incorporate
lexical-semantic features in a discriminative model,
relating known lexical units to unknown words that
may evoke frames. Here we are able to take advan-
tage of the large inventory of partially-annotated ex-
emplar sentences.
Note that this model makes a strong independence
assumption: each frame is predicted independently
of all others in the document. In this way the model
is similar to J&N?07. However, ours is a single
conditional model that shares features and weights
across all targets, frames, and prototypes, whereas
the approach of J&N?07 consists of many separately
trained models. Moreover, our model is unique in
that it uses a latent variable to smooth over frames
for unknown or ambiguous LUs.
Frame identification features depend on the pre-
processed sentence x, the prototype ` and its
WordNet lexical-semantic relationship with the tar-
get ti, and of course the frame f . Our model instan-
tiates 662,020 binary features; see Das et al (2010).
4.3 Training
Given the training subset of the SemEval?07 data,
which is of the form
?
?x(j), t(j), f (j),A(j)?
?N
j=1
(N = 1663 is the number of sentences), we dis-
criminatively train the frame identification model by
maximizing the following log-likelihood:5
5We found no benefit on development data from using an L2
regularizer (zero-mean Gaussian prior).
max
?
N?
j=1
mj?
i=1
log
?
`?L
f
(j)
i
p?(f
(j)
i , ` | t
(j)
i ,x
(j)) (3)
Note that the training problem is non-convex be-
cause of the summed-out prototype latent variable
` for each frame. To calculate the objective func-
tion, we need to cope with a sum over frames and
prototypes for each target (see Eq. 2), often an ex-
pensive operation. We locally optimize the function
using a distributed implementation of L-BFGS. This
is the most expensive model that we train: with 100
CPUs, training takes several hours. (Decoding takes
only a few minutes on one CPU for the test set.)
4.4 Results
We evaluate the performance of our frame identifi-
cation model given gold-standard targets and auto-
matically identified targets (?3); see Table 4.
Given gold-standard targets, our model is able
to predict frames for lemmas not seen in training,
of which there are 210. The partial-match evalua-
tion gives our model some credit for 190 of these,
4 of which are exactly correct. The hidden vari-
able model, then, is finding related (but rarely exact)
frames for unknown target words. The net effect of
our conservative target identifier on F1 is actually
positive: the frame identifier is far more precise for
targets seen explicitly in training. Together, our tar-
get and frame identification outperform the baseline
by 4 F1 points. To compare the frame identification
stage in isolation with that of J&N?07, we ran our
frame identification model with the targets identified
by their system as input. With partial matching, our
model achieves a relative improvement of 0.6% F1
over J&N?07 (though this is not significant).
While our frame identification model thus per-
forms on par with the current state of the art for
this task, it improves upon J&N?s formulation of
the problem because it requires only a single model,
learns lexical-semantic features as part of that model
rather than requiring a preprocessing step to expand
the vocabulary of frame-evoking words, and is prob-
abilistic, which can facilitate global reasoning.
5 Argument Identification
Given a sentence x = ?x1, . . . , xn?, the set of tar-
gets t = ?t1, . . . , tm?, and a list of evoked frames
952
f = ?f1, . . . , fm? corresponding to each target, ar-
gument identification is the task of choosing which
of each fi?s roles are filled, and by which parts of x.
This task is most similar to the problem of semantic
role labeling, but uses frame-specific labels that are
richer than the PropBank annotations.
5.1 Model
Let Rfi = {r1, . . . , r|Rfi |} denote frame fi?s roles
(named frame element types) observed in an exem-
plar sentence and/or our training set. A subset of
each frame?s roles are marked as core roles; these
roles are conceptually and/or syntactically necessary
for any given use of the frame, though they need
not be overt in every sentence involving the frame.
These are roughly analogous to the core arguments
A0?A5 and AA in PropBank. Non-core roles?
analogous to the various AMs in PropBank?loosely
correspond to syntactic adjuncts, and carry broadly-
applicable information such as the time, place, or
purpose of an event. The lexicon imposes some
additional structure on roles, including relations to
other roles in the same or related frames, and se-
mantic types with respect to a small ontology (mark-
ing, for instance, that the entity filling the protag-
onist role must be sentient for frames of cogni-
tion). Fig. 2 illustrates some of the structural ele-
ments comprising the frame lexicon by considering
the CAUSE TO MAKE NOISE frame.
We identify a set S of spans that are candidates for
filling any role r ? Rfi . In principle, S could con-
tain any subsequence of x, but in this work we only
consider the set of contiguous spans that (a) contain
a single word or (b) comprise a valid subtree of a
word and all its descendants in the dependency parse
produced by the MST parser. This covers 81% of ar-
guments in the development data. The empty span
is also included in S, since some roles are not ex-
plicitly filled; in the development data, the average
number of roles an evoked frame defines is 6.7, but
the average number of overt arguments is only 1.7.6
In training, if a labeled argument is not a valid sub-
6In the annotated data, each core role is filled with one of
three types of null instantiations indicating how the role is con-
veyed implicitly. E.g., the imperative construction implicitly
designates a role as filled by the addressee, and the correspond-
ing filler is thus CNI (constructional null instantiation). In this
work we do not distinguish different types of null instantiations.
tree of the dependency parse, we add its span to S .
Let Ai denote the mapping of roles in Rfi to
spans in S. Our model makes a prediction for each
Ai(rk) (for all roles rk ? Rfi) using:
Ai(rk)? argmaxs?S p(s | rk, fi, ti,x) (4)
We use a conditional log-linear model over spans for
each role of each evoked frame:
p?(Ai(rk) = s | fi, ti,x) = (5)
exp?>h(s, rk, fi, ti,x)
?
s??S exp?
>h(s?, rk, fi, ti,x)
Note that our model chooses the span for each
role separately from the other roles and ignores all
frames except the frame the role belongs to. Our
model departs from the traditional SRL literature by
modeling the argument identification problem in a
single stage, rather than first classifying token spans
as arguments and then labeling them. A constraint
implicit in our formulation restricts each role to have
at most one overt argument, which is consistent with
96.5% of the role instances in the training data.
Out of the overt argument spans in the training
data, 12% are duplicates, having been used by some
previous frame in the sentence (supposing some ar-
bitrary ordering of frames). Our role-filling model,
unlike a sentence-global argument detection-and-
classification approach,7 permits this sort of argu-
ment sharing among frames. The incidence of span
overlap among frames is much higher; Fig. 1 illus-
trates a case with a high degree of overlap. Word
tokens belong to an average of 1.6 argument spans
each, including the quarter of words that do not be-
long to any argument.
Features for our log-linear model (Eq. 5) depend
on the preprocessed sentence x; the target t; a
role r of frame f ; and a candidate argument span
s ? S. Our model includes lexicalized and unlexi-
calized features considering aspects of the syntactic
parse (most notably the dependency path in the parse
from the target to the argument); voice; word order-
ing/overlap/distance of the argument with respect to
the target; and POS tags within and around the argu-
ment. Many features have a version specific to the
frame and role, plus a smoothed version incorporat-
ing the role name, but not the frame. These features
7J&N?07, like us, identify arguments for each target.
953
are fully enumerated in (Das et al, 2010); instanti-
ating them for our data yields 1,297,857 parameters.
5.2 Training
We train the argument identification model by:
max
?
N?
j=1
mj?
i=1
|R
f
(j)
i
|
?
k=1
log p?(A
(j)
i (rk) | f
(j)
i , t
(j)
i ,x
(j))
(6)
This objective function is concave, and we globally
optimize it using stochastic gradient ascent (Bottou,
2004). We train this model until the argument iden-
tification F1 score stops increasing on the develop-
ment data. Best results on this dataset were obtained
with a batch size of 2 and 23 passes through the data.
5.3 Approximate Joint Decoding
Na??ve prediction of roles using Eq. 4 may result
in overlap among arguments filling different roles
of a frame, since the argument identification model
fills each role independently of the others. We want
to enforce the constraint that two roles of a single
frame cannot be filled by overlapping spans. We dis-
allow illegal overlap using a 10000-hypothesis beam
search; the algorithm is given in (Das et al, 2010).
5.4 Results
Performance of the argument identification model
is presented in Table 5. The table shows how per-
formance varies given different types of perfect in-
put: correct targets, correct frames, and the set of
correct spans; correct targets and frames, with the
heuristically-constructed set of candidate spans; cor-
rect targets only, with model frames; and ultimately,
no oracle input (the full frame parsing scenario).
The first four rows of results isolate the argu-
ment identification task from the frame identifica-
tion task. Given gold targets and frames and an ora-
cle set of argument spans, our local model achieves
about 87% precision and 75% recall. Beam search
decoding to eliminate illegal argument assignments
within a frame (?5.3) further improves precision by
about 1.6%, with negligible harm to recall. Note
that 96.5% recall is possible under the constraint that
roles are not multiply-filled (?5.1); there is thus con-
siderable room for improvement with this constraint
in place. Joint prediction of each frame?s arguments
is worth exploring to capture correlations not en-
coded in our local models or joint decoding scheme.
The 15-point drop in recall when the heuristically-
built candidate argument set replaces the set of true
argument spans is unsurprising: an estimated 19% of
correct arguments are excluded because they are nei-
ther single words nor complete subtrees (see ?5.1).
Qualitatively, the problem of candidate span recall
seems to be largely due to syntactic parse errors.8
Still, the 10-point decrease in precision when using
the syntactic parse to determine candidate spans sug-
gests that the model has trouble discriminating be-
tween good and bad arguments, and that additional
feature engineering or jointly decoding arguments of
a sentence?s frames may be beneficial in this regard.
The fifth and sixth rows show the effect of auto-
matic frame identification on overall frame parsing
performance. There is a 22% decrease in F1 (18%
when partial credit is given for related frames), sug-
gesting that improved frame identification or joint
prediction of frames and arguments is likely to have
a sizeable impact on overall performance.
The final two rows of the table compare our full
model (target, frame, and argument identification)
with the baseline, showing significant improvement
of more than 4.4 F1 points for both exact and partial
frame matching. As with frame identification, we
compared the argument identification stage with that
of J&N?07 in isolation, using the automatically iden-
tified targets and frames from the latter as input to
our model. With partial frame matching, this gave us
an F1 score of 48.1% on the test set?significantly
better (p < 0.05) than 45.6%, the full parsing re-
sult from J&N?07. This indicates that our argument
identification model?which uses a single discrim-
inative model with a large number of features for
role filling (rather than argument labeling)?is more
powerful than the previous state of the art.
6 Related work
Since Gildea and Jurafsky (2002) pioneered statis-
tical semantic role labeling, a great deal of com-
8Note that, because of our labels-only evaluation scheme
(?2.3), arguments missing a word or containing an extra word
receive no credit. In fact, of the frame roles correctly predicted
as having an overt span, the correct span was predicted 66% of
the time, while 10% of the time the predicted starting and end-
ing boundaries of the span were off by a total of 1 or 2 words.
954
ARGUMENT IDENTIFICATION exact frame matching
targets frames spans decoding P R F1
Argument identifica-
tion (oracle spans)
? ? ? na??ve 86.61 75.11 80.45
? ? ? beam ?5.3 88.29 74.77 80.97
Argument identifica-
tion (full)
? ? model ?5 na??ve 77.43 60.76 68.09 partial frame matching
? ? model ?5 beam ?5.3 78.71 60.57 68.46 P R F1
Parsing (oracle targets) ? model ?4 model ?5 beam ?5.3 49.68 42.82 46.00 57.85 49.86 53.56
Parsing (full) auto ?3 model ?4 model ?5 beam ?5.3 58.08 38.76 46.49 62.76 41.89 50.24
Baseline: J&N?07 auto model model N/A 51.59 35.44 42.01 56.01 38.48 45.62
Table 5. Argument identification results. ? indicates that gold-standard labels were used for a given pipeline stage.
For full parsing, bolded scores indicate significant improvements relative to the baseline (p < 0.05).
putational work has investigated predicate-argument
structures for semantics. Briefly, we highlight some
relevant work, particularly research that has made
use of FrameNet. (Note that much related research
has focused on PropBank (Kingsbury and Palmer,
2002), a set of shallow predicate-argument annota-
tions for Wall Street Journal articles from the Penn
Treebank (Marcus et al, 1993); a recent issue of CL
(Ma`rquez et al, 2008) was devoted to the subject.)
Most work on frame-semantic role labeling has
made use of the exemplar sentences in the FrameNet
corpus (see ?2.1), each of which is annotated for a
single frame and its arguments. On the probabilis-
tic modeling front, Gildea and Jurafsky (2002) pre-
sented a discriminative model for arguments given
the frame; Thompson et al (2003) used a gener-
ative model for both the frame and its arguments;
and Fleischman et al (2003) first used maximum
entropy models to find and label arguments given
the frame. Shi and Mihalcea (2004) developed a
rule-based system to predict frames and their argu-
ments in text, and Erk and Pado? (2006) introduced
the Shalmaneser tool, which employs Na??ve Bayes
classifiers to do the same. Other FrameNet SRL
systems (Giuglea and Moschitti, 2006, for instance)
have used SVMs. Most of this work was done on an
older, smaller version of FrameNet.
Recent work on frame-semantic parsing?in
which sentences may contain multiple frames to be
recognized along with their arguments?has used
the SemEval?07 data (Baker et al, 2007). The LTH
system of Johansson and Nugues (2007), our base-
line (?2.4), performed the best in the SemEval?07
task. Matsubayashi et al (2009) trained a log-
linear model on the SemEval?07 data to evaluate
argument identification features exploiting various
types of taxonomic relations to generalize over roles.
A line of work has sought to extend the coverage
of FrameNet by exploiting VerbNet, WordNet, and
Wikipedia (Shi and Mihalcea, 2005; Giuglea and
Moschitti, 2006; Pennacchiotti et al, 2008; Tonelli
and Giuliano, 2009), and projecting entries and an-
notations within and across languages (Boas, 2002;
Fung and Chen, 2004; Pado? and Lapata, 2005;
Fu?rstenau and Lapata, 2009). Others have applied
frame-semantic structures to question answering,
paraphrase/entailment recognition, and information
extraction (Narayanan and Harabagiu, 2004; Shen
and Lapata, 2007; Pado? and Erk, 2005; Burchardt,
2006; Moschitti et al, 2003; Surdeanu et al, 2003).
7 Conclusion
We have provided a supervised model for rich
frame-semantic parsing, based on a combination
of knowledge from FrameNet, two probabilistic
models trained on SemEval?07 data, and expedi-
ent heuristics. Our system achieves improvements
over the state of the art at each stage of process-
ing and collectively, and is amenable to future ex-
tension. Our parser is available for download at
http://www.ark.cs.cmu.edu/SEMAFOR.
Acknowledgments
We thank Collin Baker, Katrin Erk, Richard Johansson,
and Nils Reiter for software, data, evaluation scripts, and
methodological details. We thank the reviewers, Alan
Black, Ric Crabbe, Michael Ellsworth, Rebecca Hwa,
Dan Klein, Russell Lee-Goldman, Dan Roth, Josef Rup-
penhofer, and members of the ARK group for helpful
comments. This work was supported by DARPA grant
NBCH-1080004, NSF grant IIS-0836431, and computa-
tional resources provided by Yahoo.
955
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: frame semantic structure extraction. In
Proc. of SemEval.
H. C. Boas. 2002. Bilingual FrameNet dictionaries for
machine translation. In Proc. of LREC.
L. Bottou. 2004. Stochastic learning. In Advanced Lec-
tures on Machine Learning. Springer-Verlag.
A. Burchardt, K. Erk, and A. Frank. 2005. A WordNet
detour to FrameNet. In B. Fisseni, H.-C. Schmitz,
B. Schro?der, and P. Wagner, editors, Sprachtech-
nologie, mobile Kommunikation und linguistische Re-
sourcen, volume 8. Peter Lang.
A. Burchardt. 2006. Approaching textual entailment
with LFG and FrameNet frames. In Proc. of the Sec-
ond PASCAL RTE Challenge Workshop.
D. Das, N. Schneider, D. Chen, and N. A. Smith.
2010. SEMAFOR 1.0: A probabilistic frame-semantic
parser. Technical Report CMU-LTI-10-001, Carnegie
Mellon University.
K. Erk and S. Pado?. 2006. Shalmaneser - a toolchain for
shallow semantic parsing. In Proc. of LREC.
C. Fellbaum, editor. 1998. WordNet: an electronic lexi-
cal database. MIT Press, Cambridge, MA.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in
the Morning Calm, pages 111?137. Hanshin Publish-
ing Co., Seoul, South Korea.
M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum
entropy models for FrameNet classification. In Proc.
of EMNLP.
P. Fung and B. Chen. 2004. BiFrameNet: bilin-
gual frame semantics resource construction by cross-
lingual induction. In Proc. of COLING.
H. Fu?rstenau and M. Lapata. 2009. Semi-supervised se-
mantic role labeling. In Proc. of EACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
A.-M. Giuglea and A. Moschitti. 2006. Shallow
semantic parsing based on FrameNet, VerbNet and
PropBank. In Proc. of ECAI 2006.
R. Johansson and P. Nugues. 2007. LTH: semantic struc-
ture extraction using nonprojective dependency trees.
In Proc. of SemEval.
R. Johansson and P. Nugues. 2008. Dependency-based
semantic role labeling of PropBank. In Proc. of
EMNLP.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. of LREC.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics, 19(2).
L. Ma`rquez, X. Carreras, K. C. Litkowski, and S. Steven-
son. 2008. Semantic role labeling: an introduction to
the special issue. Computational Linguistics, 34(2).
Y. Matsubayashi, N. Okazaki, and J. Tsujii. 2009. A
comparative study on generalization of semantic roles
in FrameNet. In Proc. of ACL-IJCNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
A. Moschitti, P. Mora?rescu, and S. M. Harabagiu. 2003.
Open-domain information extraction via automatic se-
mantic labeling. In Proc. of FLAIRS.
S. Narayanan and S. Harabagiu. 2004. Question answer-
ing based on semantic structures. In Proc. of COLING.
S. Pado? and K. Erk. 2005. To cause or not to cause:
cross-lingual semantic matching for paraphrase mod-
elling. In Proc. of the Cross-Language Knowledge In-
duction Workshop.
S. Pado? and M. Lapata. 2005. Cross-linguistic projec-
tion of role-semantic information. In Proc. of HLT-
EMNLP.
M. Pennacchiotti, D. De Cao, R. Basili, D. Croce, and
M. Roth. 2008. Automatic induction of FrameNet
lexical units. In Proc. of EMNLP.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proc. of EMNLP-
CoNLL.
L. Shi and R. Mihalcea. 2004. An algorithm for open
text semantic parsing. In Proc. of Workshop on Robust
Methods in Analysis of Natural Language Data.
L. Shi and R. Mihalcea. 2005. Putting pieces together:
combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Computational Linguis-
tics and Intelligent Text Processing: Proc. of CICLing
2005. Springer-Verlag.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proc. of ACL.
C. A. Thompson, R. Levy, and C. D. Manning. 2003. A
generative model for semantic role labeling. In Proc.
of ECML.
S. Tonelli and C. Giuliano. 2009. Wikipedia as frame
information repository. In Proc. of EMNLP.
956
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 677?687,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Graph-Based Lexicon Expansion with Sparsity-Inducing Penalties
Dipanjan Das and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dipanjan,nasmith}@cs.cmu.edu
Abstract
We present novel methods to construct com-
pact natural language lexicons within a graph-
based semi-supervised learning framework,
an attractive platform suited for propagating
soft labels onto new natural language types
from seed data. To achieve compactness,
we induce sparse measures at graph vertices
by incorporating sparsity-inducing penalties
in Gaussian and entropic pairwise Markov
networks constructed from labeled and unla-
beled data. Sparse measures are desirable for
high-dimensional multi-class learning prob-
lems such as the induction of labels on natu-
ral language types, which typically associate
with only a few labels. Compared to standard
graph-based learning methods, for two lexicon
expansion problems, our approach produces
significantly smaller lexicons and obtains bet-
ter predictive performance.
1 Introduction
Semi-supervised learning (SSL) is attractive for the
learning of complex phenomena, for example, lin-
guistic structure, where data annotation is expen-
sive. Natural language processing applications have
benefited from various SSL techniques, such as dis-
tributional word representations (Huang and Yates,
2009; Turian et al, 2010; Dhillon et al, 2011),
self-training (McClosky et al, 2006), and entropy
regularization (Jiao et al, 2006; Smith and Eisner,
2007). In this paper, we focus on semi-supervised
learning that uses a graph constructed from labeled
and unlabeled data. This framework, graph-based
SSL?see Bengio et al (2006) and Zhu (2008) for
introductory material on this topic?has been widely
used and has been shown to perform better than sev-
eral other semi-supervised algorithms on benchmark
datasets (Chapelle et al, 2006, ch. 21). The method
constructs a graph where a small portion of ver-
tices correspond to labeled instances, and the rest
are unlabeled. Pairs of vertices are connected by
weighted edges denoting the similarity between the
pair. Traditionally, Markov random walks (Szum-
mer and Jaakkola, 2001; Baluja et al, 2008) or op-
timization of a loss function based on smoothness
properties of the graph (Corduneanu and Jaakkola,
2003; Zhu et al, 2003; Subramanya and Bilmes,
2008, inter alia) are performed to propagate labels
from the labeled vertices to the unlabeled ones.
In this work, we are interested in multi-class gen-
eralizations of graph-propagation algorithms suit-
able for NLP applications, where each graph ver-
tex can assume one or more out of many possible
labels (Talukdar and Crammer, 2009; Subramanya
and Bilmes, 2008, 2009). For us, graph vertices cor-
respond to natural language types (not tokens) and
undirected edges between them are weighted using a
similarity metric. Recently, this setup has been used
to learn soft labels on natural language types (say,
word n-grams or syntactically disambiguated pred-
icates) from seed data, resulting in large but noisy
lexicons, which are used to constrain structured pre-
diction models. Applications have ranged from
domain adaptation of part-of-speech (POS) taggers
(Subramanya et al, 2010), unsupervised learning of
POS taggers by using bilingual graph-based projec-
tions (Das and Petrov, 2011), and shallow seman-
tic parsing for unknown predicates (Das and Smith,
2011). However, none of the above captured the em-
pirical fact that only a few categories typically asso-
ciate with a given type (vertex). Take the case of
POS tagging: Subramanya et al (2010) construct a
graph over trigram types as vertices, with 45 pos-
sible tags for the middle word of a trigram as the
677
label set for each vertex. It is empirically observed
that contextualized word types can assume very few
(most often, one) POS tags. However, along with
graph smoothness terms, they apply a penalty that
encourages distributions to be close to uniform, the
premise being that it would maximize the entropy
of the distribution for a vertex that is far away or
disconnected from a labeled vertex. To prefer maxi-
mum entropy solutions in low confidence regions of
graphs, a similar entropic penalty is applied by Sub-
ramanya and Bilmes (2008, 2009).
In this paper, we make two major algorithmic con-
tributions. First, we relax the assumption made by
most previous work (Zhu and Ghahramani, 2002;
Baluja et al, 2008; Subramanya and Bilmes, 2008;
Subramanya and Bilmes, 2009; Subramanya et al,
2010; Das and Petrov, 2011; Das and Smith, 2011)
that the `1 norm of the masses assigned to the la-
bels for a given vertex must be 1. In other words,
in our framework, the label distribution at each ver-
tex is unnormalized?the only constraint we put on
the vertices? vectors is that they must be nonnega-
tive.1 This relaxation simplifies optimization: since
only a nonnegativity constraint for each label?s mass
at each vertex needs to be imposed, we can apply a
generic quasi-Newton method (Zhu et al, 1997).
Second, we replace the penalties that prefer max-
imum entropy, used in prior work, with penalties
that aim to identify sparse unnormalized measures
at each graph vertex. We achieve this by penalizing
the graph propagation objective with the `1 norm or
the mixed `1,2 norm (Kowalski and Torre?sani, 2009)
of the measures at each vertex, aiming for global and
vertex-level sparsity, respectively. Importantly, the
proposed graph objective functions are convex, so
we avoid degenerate solutions and local minima.
We present experiments on two natural language
lexicon expansion problems in a semi-supervised
setting: (i) inducing distributions of POS tags over
n-gram types in the Wall Street Journal section of
the Penn Treebank corpus (Marcus et al, 1993)
and (ii) inducing distributions of semantic frames
(Fillmore, 1982) over predicates unseen in anno-
1Moreover, we also assume the edge weights in a given
graph are unconstrained, consistent with prior work on graph-
based SSL (Das and Petrov, 2011; Das and Smith, 2011; Subra-
manya and Bilmes, 2008; Subramanya and Bilmes, 2009; Sub-
ramanya et al, 2010; Zhu and Ghahramani, 2002).
tated data. Our methods produce sparse measures
at graph vertices resulting in compact lexicons, and
also result in better performance with respect to la-
bel propagation using Gaussian penalties (Zhu and
Ghahramani, 2002) and entropic measure propaga-
tion (Subramanya and Bilmes, 2009), two state-of-
the-art graph propagation algorithms.
2 Model
2.1 Graph-Based SSL as MAP Inference
Let Dl = {(xj , rj)}lj=1 denote l annotated data
types;2 xj?s empirical label distribution is rj . Let
the unlabeled data types be denoted by Du =
{xi}mi=l+1. Usually, l  m. Thus, the entire dataset
can be called D , Dl ? Du. Traditionally, the
graph-based SSL problem has been set up as fol-
lows. Let G = (V,E) correspond to an undirected
graph with vertices V and edges E. G is constructed
by transforming each data type xi ? D to a ver-
tex; thus V = {1, 2, . . . ,m}, and E ? V ? V .
Let Vl (Vu) denote the labeled (unlabeled) vertices.
Moreover, we assume a symmetric weight matrixW
that defines the similarity between a pair of vertices
i, k ? V . We first define a component of this ma-
trix as wij , [W]ik = sim(xi,xk). We also fix
wii = 0 and set wik = wki = 0 if k 6? N (i)
and i 6? N (k), where N (j) denotes the K-nearest
neighbors of vertex j, to reduce the density of the
graph. We next define an unnormalized measure qi
for every vertex i ? V . As mentioned before, we
have rj , a probability distribution estimated from
annotated data for a labeled vertex j ? Vl. qi and
rj are |Y |-dimensional measures, where Y is the
possible set of labels; while rj lies within the |Y |-
dimensional probability simplex,3 qi are unnormal-
ized with each component qi(y) ? 0. For most NLP
problems, rj are expected to be sparse, with very
few components active, the rest being zero.
Graph-based SSL aims at finding the best q =
{qi : 1 ? i ? m} given the empirical distribu-
tions rj , and the weight matrix W, which provides
2As explained in more detail in ?4, these types are entities
like n-grams or individual predicates, not tokens in running text.
3Note that our framework does not necessitate that rj be a
normalized probability distribution; we could have unnormal-
ized rj to allow strongly evident types appearing in more data
to have larger influence than types that appear infrequently. We
leave this extension to future work.
678
the geometry of all the vertices. We visualize this
problem using a pairwise Markov network (MN).
For every vertex (including labeled ones) i ? V , we
create a variable Xi. Additionally, for labeled ver-
tices j ? Vl, we create variables X?j . All variables in
the MN are defined to be vector-valued; specifically,
variables Xi, ?i ? V , take value qi, and variables
X?j corresponding to the labeled vertices in G are ob-
served with values rj . An example factor graph for
this MN, with only four vertices, is shown in Fig-
ure 1. In the figure, the variables indexed by 1 and 4
correspond to labeled vertices. Factor ?j with scope
{Xj , X?j} encourages qj to be close to rj . For every
edge i ? k ? E, factor ?i?k encourages similarity
between qi and qk, making use of the weight matrix
W (i.e., when wik is larger, the two measures are
more strongly encouraged to be close). These fac-
tors are white squares with solid boundaries in the
figure. Finally, we define unary factors on all vari-
ablesXi, i ? V , named ?i(Xi), that can incorporate
prior information. In Figure 1, these factors are rep-
resented by white squares with dashed boundaries.
According to the factor graph, the joint probabil-
ity for all the measures qi, ?i ? V that we want to
induce, is defined as: P (X; ?) =
1
Z
l?
j=1
?j(Xj , X?j) ?
?
i?k?E
?i?k(Xi, Xk) ?
m?
i=1
?i(Xi)
where ? is the set of all factors in the factor graph,
and Z is a partition function that normalizes the fac-
tor products for a given configuration of q. Since the
graph-based SSL problem aims at finding the best q,
we optimize lnP (X; ?); equivalently,
arg max
q s.t. q?0
l?
j=1
ln?j(Xj , X?j) +
?
i?k?E
ln?i?k(Xi, Xk)
+
m?
i=1
ln?i(Xi) (1)
The above denotes an optimization problem with
only non-negativity constraints. It equates to max-
imum a posteriori (MAP) inference; hence, the par-
tition function Z can be ignored. We next discuss
the nature of the three different factors in Eq. 1.
2.2 Log-Factors as Penalties
The nature of the three types of factors in Eq. 1
governs the behavior of a graph-based SSL algo-
rithm. Hence, the equation specifies a family of
X1
X4 X3
X2
Figure 1: An example factor graph for the graph-based
SSL problem. See text for the significance of the shaded
and dotted factors, and the shaded variables.
graph-based methods that generalize prior research.
We desire the following properties to be satisfied in
the factors: (i) convexity of Eq. 1, (ii) amenability
to scalable optimization algorithms, and (iii) sparse
solutions as expected in natural language lexicons.
Pairwise factors: In our work, for the pairwise
factors ?j(Xj , X?j) and ?i?k(Xi, Xk), we examine
two functions that penalize inconsistencies between
neighboring vertices: the squared `2 norm and the
Jensen-Shannon (JS) divergence (Burbea and Rao,
1982; Lin, 1991), which is a symmetrized gener-
alization of the Kullback-Leibler (KL) divergence
(Kullback and Leibler, 1951; Cover and Thomas,
1991). These two divergences are symmetric. Both
are inspired by previous work; however, the use of
the JS divergence is a novel extension to Subra-
manya and Bilmes (2008). Specifically, the factors
are:
ln?j(Xj , X?j) = ??(qj , rj) (2)
ln?i?k(Xi, Xk) = ?2 ? ? ? wik ? ?(qi, qk) (3)
where ? is a hyperparameter whose choice we dis-
cuss in ?4. The function ?(u, v) for two vectors u
and v is defined in two ways:
?(u, v)
Gaussian
= ?u? v?22 (4)
?(u, v)
Entropic
= 12
?
y?Y
(
u(y) ? ln 2 ? u(y)u(y) + v(y)
+ v(y) ? ln 2 ? v(y)u(y) + v(y)
)
(5)
We call the version of ?(u, v) that uses the squared
`2 distance (Eq. 4) Gaussian, as it represents the idea
of label propagation via Gaussian fields proposed by
Zhu et al (2003). A minor difference lies in the
fact that we include variables Xj , j ? Vl for labeled
679
vertices too, and allow them to change, but penal-
ize them if they go too far away from the observed
labeled distributions rj . The other ?(u, v) shown in
Eq. 5 uses the generalized JS-divergence defined in
terms of the generalized KL-divergence for unnor-
malized measures (O?Sullivan, 1998).4
Eq. 5 improves prior work by replacing the asym-
metric KL-divergence used to bring the distributions
at labeled vertices close to the corresponding ob-
served distributions, as well as replacing the KL-
based graph smoothness term with the symmetric
JS-divergence (Subramanya and Bilmes, 2008, see
first two terms in Eq. 1). Empirical evidence shows
that entropic divergences help in multiclass prob-
lems where a vertex can assume multiple labels, and
may perform better than objectives with quadratic
penalties (Subramanya and Bilmes, 2008, 2009).
A major departure from prior work is the use of
unnormalized measures in Eq. 4-5, which simplifies
optimization even with the complex JS-divergence
in the objective function (see ?3), and, we will see,
produces comparable and often better results than
baselines using normalized distributions (see ?4).
Unary factors: The unary factors in our factor
graph ?i(Xi) can incorporate prior information spe-
cific to a particular vertex xi embodied by the vari-
able Xi. Herein, we examine three straightforward
penalties, which can be thought of as penalties that
encourage either uniformity or sparsity:
Uniform squared `2: ln?i(Xi) = ?? ?
?
?
?qi ? 1|Y |
?
?
?
2
2
(6)
Sparse `1: ln?i(Xi) = ?? ? ?qi?1 (7)
Sparse `1,2: ln?i(Xi) = ?? ? ?qi?21 (8)
where ? is a hyperparameter whose choice we dis-
cuss in ?4. The penalty expressed in Eq. 6 penal-
izes qi if it is far away from the uniform distribu-
tion. This penalty has been used previously (Das and
Petrov, 2011; Das and Smith, 2011; Subramanya et
al., 2010), and is similar to the maximum entropy
penalty of Subramanya and Bilmes (2008, 2009).
The intuition behind its use is that for low confi-
dence or disconnected regions, one would prefer to
have a uniform measure on a graph vertex. The
penalties in equations 7?8, on the other hand, en-
courage sparsity in the measure qi; these are related
4The generalized KL divergence is defined asDKL(u?v) =
?
y
(
u(y) ln u(y)v(y) ? u(y) + v(y)
)
.
to regularizers for generalized linear models: the
lasso (Tibshirani, 1996) and the elitist lasso (Kowal-
ski and Torre?sani, 2009). The former encourages
global sparsity, the latter sparsity per vertex.5 For
each vertex, the `1,2 penalty takes the form:
?qi?21 =
?
?
?
y?Y
|qi(y)|
?
?
2
(9)
The `1 norm encourages its argument to be sparse,
while the usual observed effect of an `2 norm is a
dense vector without many extreme values. The `1,2
penalty is the squared `2 norm of the `1 norms of
every qi, hence it promotes sparsity within each ver-
tex, but we observe density over the vertices that are
selected.
Talukdar (2010) enforced label sparsity for infor-
mation extraction by discarding poorly scored la-
bels during graph propagation updates, but did not
use a principled mechanism to arrive at sparse mea-
sures at graph vertices. Unlike the uniform penalty
(Eq. 6), sparsity corresponds to the idea of entropy
minimization (Grandvalet and Bengio, 2004). Since
we use unnormalized measures at each variable Xi,
for low confidence graph regions or disconnected
vertices, sparse penalties will result in all zero com-
ponents in qi, which conveys that the graph prop-
agation algorithm is not confident on any potential
label, a condition that is perfectly acceptable.
Model variants: We compare six objective func-
tions: we combine factor representations from each
of Eqs. 4?5 with those from each of Eqs. 6?8, replac-
ing them in the generic graph objective function of
Eq. 1. The nature of these six models is succinctly
summarized in Table 1. For each model, we find
the best set of measures q that maximize the corre-
sponding graph objective functions, such that q ? 0.
Note that in each of the graph objectives, we have
two hyperparameters ? and ? that control the influ-
ence of the second and the third terms of Eq. 1 re-
5One could additionally consider a non-sparse penalty based
on the squared `2 norm with zero mean: ln?i(Xi) = ?? ?
?qi?
2
2. We experimented with this unary penalty (along with the
pairwise Gaussian penalty for binary factors) for the semantic
frame lexicon expansion problem, and found that it performs
exactly on par with the squared `2 penalty with uniform mean.
To limit the number of non-sparse graph objectives, we omit
detailed discussion of experiments with this unary penalty.
680
abbrev.
factors
pairwise unary
UGF-`2 Gaussian Uniform squared `2
UGF-`1 Gaussian Sparse `1
UGF-`1,2 Gaussian Sparse `1,2
UJSF-`2 Entropic Uniform squared `2
UJSF-`1 Entropic Sparse `1
UJSF-`1,2 Entropic Sparse `1,2
Table 1: Six variants of graph objective functions novel
to this work. These variants combine the pairwise factor
representations from Eqs. 4?5 with unary factor repre-
sentations from each of Eqs. 6?8 (which either encour-
age uniform or sparse measures), to be used in the graph
objective function expressed in Eq. 1.
spectively. We discuss how these hyperparameters
are chosen in ?4.
Baseline Models: We compare the performance
of the six graph objectives of Table 1 with two
strong baselines that have been used in previous
work. These two models use the following two ob-
jective functions, and find q s.t. q ? 0 and ?i ?
V,
?
y?Y qi(y) = 1. The first is a normalized Gaus-
sian field with a squared uniform `2 penalty as the
unary factor (NGF-`2):
arg min
q, s.t. q?0,
?i?V,?qi?1=1
l?
j=1
?qj ? rj?22 +
m?
i=1
?
??
?
k?N (i)
wik ?qi ? qk?22 + ?
?
?
?qi ? 1|Y |
?
?
?
2
2
?
?(10)
The second is a normalized KL field with an entropy
penalty as the unary factor (NKLF-ME):
arg min
q, s.t. q?0,
?i?V,?qi?1=1
l?
j=1
DKL(rj ? qj)+
m?
i=1
?
??
?
k?N (i)
wikDKL(qi ? qk)? ? ?H(qi)
?
?(11)
whereH(qi) denotes the Shannon entropy of the dis-
tribution qi. Both these objectives are constrained
by the fact that every qi must be within the |Y |-
dimensional probability simplex. The objective
function in 10 has been used previously (Das and
Smith, 2011; Subramanya et al, 2010) and serves as
a generalization of Zhu et al (2003). The entropic
objective function in 11, originally called measure
propagation, performed better at multiclass prob-
lems when compared to graph objectives using the
quadratic criterion (Subramanya and Bilmes, 2008).
3 Optimization
The six variants of Eq. 1 in Table 1 are convex
in q. This is because the `1, squared `2 and the
`1,2 penalties are convex. Moreover, the general-
ized JS-divergence term, which is a sum of two KL-
divergence terms, is convex (Cover and Thomas,
1991). Since we choose ?, ? and wik to be non-
negative, these terms? sums are also convex. The
graph objectives of the two baselines noted in ex-
pressions 10?11 are also convex because negative
entropy in expression 11 is convex, and rest of the
penalties are the same as our six objectives. In our
work, to optimize the objectives of Table 1, we use a
generic quasi-Newton gradient-based optimizer that
can handle bound-inequality constraints, called L-
BFGS-B (Zhu et al, 1997). Partial derivatives of
the graph objectives are computed with respect to
each parameter ?i, y, qi(y) of q and passed on to
the optimizer which updates them such that the ob-
jective function of Eq. 1 is maximized. Note that
since the `1 and `1,2 penalties are non-differentiable
at 0, special techniques are usually used to compute
updates for unconstrained parameters (Andrew and
Gao, 2007). However, since q ? 0, their absolute
value can be assumed to be right-continuous, mak-
ing the function differentiable. Thus,
?
?qi(y)
?qi?1 = 1
?
?qi(y)
?qi?21 = 2 ? ?qi?1
(We omit the form of the derivatives of the other
penalties for space.) There are several advantages to
taking this route towards optimization. The `2 and
the JS-divergence penalties for the pairwise terms
can be replaced with more interesting convex di-
vergences if required, and still optimization will be
straightforward. Moreover, the nonnegative con-
straints make optimization with sparsity inducing
penalties easy. Finally, computing the objective
function and the partial derivatives is easily paral-
lelizable on MPI (Gropp et al, 1994) or MapReduce
(Dean and Ghemawat, 2008) architectures, by divid-
ing up the computation across graph vertices.
In comparison, constrained problems such as the
one in Eq. 11 require a specialized alternating mini-
681
mization technique (Subramanya and Bilmes, 2008,
2009), that performs two passes through the graph
vertices during one iteration of updates, introduces
an auxiliary set of probability distributions (thus, in-
creasing memory requirements) and another hyper-
parameter ? that is used to transform the weight
matrix W to be suitable for the alternating mini-
mization procedure. To optimize the baseline ob-
jectives, we borrow the gradient-free iterative up-
dates described by Subramanya and Bilmes (2009)
and Subramanya et al (2010).
4 Experiments
In this section, we compare the six graph objective
functions in Table 1 with the two baseline objectives
on two lexicon expansion tasks.
4.1 POS Lexicon Expansion
We expand a POS lexicon for word types with a con-
text word on each side, using distributional similar-
ity in an unlabeled corpus and few labeled trigrams.
Data and task: We constructed a graph over word
trigram types as vertices, using co-occurrence statis-
tics. Following Das and Petrov (2011) and Sub-
ramanya et al (2010), a similarity score between
two trigram types was computed by measuring the
cosine similarity between their empirical senten-
tial context statistics. This similarity score resulted
in the symmetric weight matrix W, defining edge
weights between pairs of graph vertices. Details
of the similarity computation are given in those pa-
pers. W is thresholded so that only the K near-
est neighbors for each vertex have similarity greater
than zero, giving a sparse graph. We set K = 8 as it
resulted in the sparsest graph which was fully con-
nected.6 For this task, Y is the set of 45 POS tags
defined in the Penn Treebank (Marcus et al, 1993),
and the measure qi for vertex i (for trigram type xi)
corresponds to the set of tags that can be associated
with the middle word of xi. The trigram represen-
tation, as in earlier work, helps reduce the ambi-
guity of POS tags for the middle word, and helps
in graph construction. The 690,705-vertex graph
was constructed over all trigram types appearing in
6Our proposed methods can deal with graphs containing dis-
connected components perfectly well. Runtime is asymptoti-
cally linear in K for all objectives considered here.
Sections 00?21 (union of the training and develop-
ment sets used for POS tagging experiments in prior
work) of the WSJ section of the Penn Treebank, but
co-occurrence statistics for graph construction were
gathered from a million sentences drawn from the
English Gigaword corpus (Graff, 2003).
Given the graph G with m vertices, we assume
that the tag distributions r for l labeled vertices are
also provided. Our goal is to find the best set of
measures q over the 45 tags for all vertices in the
graph. Prior work used a similar lexicon for POS
domain adaptation and POS induction for resource-
poor languages (Das and Petrov, 2011; Subramanya
et al, 2010); such applications of a POS lexicon are
out of scope here; we consider only the lexicon ex-
pansion problem and do an intrinsic evaluation at a
type-level to compare the different graph objectives.
Experimental details: To evaluate, we randomly
chose 6,000 out of the 690,705 types for devel-
opment. From the remaining types, we randomly
chose 588,705 vertices for testing. This left us with
96,000 types from which we created sets of differ-
ent sizes containing 3,000, 6,000, 12,000, 24,000,
48,000 and 96,000 labeled types, creating 6 increas-
ingly easy transduction settings. The development
and the test types were kept constant for direct per-
formance comparison across the six settings and our
eight models. After running inference, the mea-
sure qi at vertex i was normalized to 1. Next, for
all thresholds ranging from 0 to 1, with steps of
0.001, we measured the average POS tag precision
and recall on the development data ? this gave us
the area under the precision-recall curve (prAUC),
which is often used to measure performance on re-
trieval tasks. Given a transduction setting and the
final q for an objective, hyperparameters ? and ?
were tuned on the development set by performing a
grid search, targeting prAUC.7 We ran 100 rounds
7For the objectives using the uniform `2 and the maxi-
mum entropy penalties, namely UGF-`2, UJSF-`2, NGF-`2
and NKLF-ME, we chose ? from {0, 10?6, 10?4, 0.1}. For the
rest of the models using sparsity inducing penalties, we chose
? from {10?6, 10?4, 0.1}. This suggests that for the former
type of objectives, we allowed a zero unary penalty if that set-
ting resulted in the best development performance, while for the
latter type of models, we enforced a positive unary penalty. In
fact, ? = 0 was chosen in several cases for the objectives with
uniform penalties indicating that uniformity hurts performance.
We chose ? from {0.1, 0.5, 1.0}.
682
|Dl|: 3K 6K 12K 24K 48K 96K
NGF-`2 0.208 0.219 0.272 0.335 0.430 0.544
NKLF-ME 0.223 0.227 0.276 0.338 0.411 0.506
UGF-`2 0.223 0.257 0.314 0.406 0.483 0.564
UGF-`1 0.223 0.257 0.309 0.406 0.483 0.556
UGF-`1,2 0.223 0.256 0.313 0.403 0.478 0.557
UJSF-`2 0.271 0.250 0.310 0.364 0.409 0.481
UJSF-`1 0.227 0.257 0.317 0.369 0.410 0.481
UJSF-`1,2 0.227 0.258 0.309 0.369 0.409 0.479
Table 2: Area under the precision recall curve for the two
baseline objectives and our methods for POS tag lexicon
induction. This is a measure of how well the type lexicon
(for some types unlabeled during training) is recovered
by each method. The test set contains 588,705 types.
of iterative updates for all 8 graph objectives.
Type-level evaluation: To measure the quality of
the lexicons, we perform type level evaluation us-
ing area under the precision-recall curve (prAUC).
The same measure (on development data) was used
to tune the two hyperparameters. Table 2 shows the
results measured on 588,705 test vertices (the same
test set was used for all the transduction settings).
The general pattern we observe is that our unnor-
malized approaches almost always perform better
than the normalized baselines. (The exception is
the 3,000 labeled example case, where most unnor-
malized models are on par with the better baseline.)
In scenarios with fewer labeled types, pairwise en-
tropic penalties perform better than Gaussian ones,
and the pattern reverses as more labeled types come
available. This trend is the same when we compare
only the two baselines. In four out of the six trans-
duction settings, one of the sparsity-inducing graph
objectives achieves the best performance in terms of
prAUC, which is encouraging given that they gener-
ally produce smaller models than the baselines.
Overall, though, using sparsity-inducing unary
factors seems to have a weak negative effect on per-
formance. Their practical advantage, however is ap-
parent when we consider the size of the model. Af-
ter the induction of the set of measures q for all
transduction settings and all graph objectives, we
noticed that our numerical optimizer (LBFGS-B) of-
ten assigns extremely small positive values rather
than zero. This problem can be attributed to sev-
eral artifacts, including our limit of 100 iterations of
optimization. Hence, we use a global threshold of
10?6, and treat any real value below this threshold
0M
5M
11M
16M
21M
27M
32M
3k 6k 12k 24k 48k 96k
                           
                              
UGF-?1 UGF-?1,2 UJSF-?2
UJSF-?1 UJSF-?1,2
Figure 2: The number of non-zero components in q for
five graph objective functions proposed in this work, plot-
ted against various numbers of labeled datapoints. Note
that NGF-`2, NKLF-ME and UGF-`2 produce non-zero
components for virtually all q, and are therefore not
shown (the dotted line marks the maximally non-sparse
solution, with 31,081,725 components). All of these five
objectives result in sparsity. On average, the objectives
employing entropic pairwise penalties with sparse unary
penalties UJSF-`1 and UJSF-`1,2 produce very sparse
lexicons. Although UGF-`2 produces no sparsity at all,
its entropic counterpart UJSF-`2 produces considerable
sparsity, which we attribute to JS-divergence as a pair-
wise penalty.
to be zero. Figure 2 shows the number of non-zero
components in q (or, the lexicon size) for the graph
objectives that achieve sparsity (baselines NGF-`2
and NKLF-ME, plus our UGF-`2 are not expected
to, and do not, achieve sparsity; surprisingly UJSF-
`2 does and is shown). Even though the hyperpa-
rameters ? and ? in the graph objective functions
were not tuned towards sparsity, we see that sparsity-
inducing factors are able to achieve far more com-
pact lexicons. Sparsity is desirable in settings where
labeled development data for tuning thresholds that
select the most probable labels for a given type is
unavailable (e.g., Das and Petrov, 2011).
4.2 Expansion of a Semantic Frame Lexicon
In a second set of experiments, we follow Das and
Smith (2011, D&S11 henceforth) in expanding a
lexicon that associates lexical predicates (targets)
with semantic frames (abstract events or scenarios
that a predicate evokes when used in a sentential
context) as labels. More concretely, each vertex in
the graph corresponds to a lemmatized word type
with its coarse part of speech, and the labels are
frames from the FrameNet lexicon (Fillmore et al,
2003). Graph construction leverages distributional
683
UNKNOWN ALL
PREDICATES PREDICATES lexicon
exact partial exact partial size
Supervised 23.08 46.62 82.97 90.51 -
?NGF-`2 39.86 62.35 83.51 91.02 128,960
NKLF-ME 36.36 60.07 83.40 90.95 128,960
UGF-`2 37.76 60.81 83.44 90.97 128,960
UGF-`1 39.86 62.85 83.51 91.04 122,799
UGF-`1,2 39.86 62.85 83.51 91.04 128,732
UJSF-`2 40.56 62.81 83.53 91.04 128,232
UJSF-`1 39.16 62.43 83.49 91.02 128,771
UJSF-`1,2 42.67 65.29 83.60 91.12 45,544
Table 3: Exact and partial frame identification accuracy
with lexicon size (non-zero frame components). The ?un-
known predicates? section of the test data contains 144
targets, while the entire test set contains 4,458 targets.
Bold indicates best results. The UJSF-`1,2 model pro-
duces statistically significant results (p < 0.001) for all
metrics with respect to the supervised baseline used in
D&S11. For both the unknown targets as well as the
whole test set. However, it is weakly significant (p < 0.1)
compared to the NGF-`2 model for the unseen portion of
the test set, when partial frame matching is used. For rest
of the settings, the two are statistically indistinguishable.
? indicates the best results in D&S11.
similarity as well as linguistic annotations.
Data: We borrow the graph-based SSL process of
D&S11 in its entirety. The constructed graph con-
tains 64,480 vertices, each corresponding to a tar-
get, out of which 9,263 were drawn from the labeled
data. The possible set of labels Y is the set of 877
frames defined in FrameNet; the measure qi corre-
sponds to the set of frames that a target can evoke.
The targets drawn from FrameNet annotated data
(l = 9,263) have frame distributions ri with which
the graph objectives are seeded.8
Evaluation: The evaluation metric used for this
task is frame disambiguation accuracy on a blind test
set containing marked targets in free text. A section
of this test set contained 144 targets, previously un-
seen in annotated FrameNet data; this section is of
interest to us and we present separate accuracy re-
sults on it. Given the measure qi over frames in-
duced using graph-based SSL for target i, we trun-
cate it to keep at most the top M frames that get
the highest mass under qi, only retaining those with
non-zero values. If all components of qi are zero,
we remove target i from the lexicon, which is of-
ten the case in the sparsity-inducing graph objec-
tives. If a target is unseen in annotated data, a sep-
arate probabilistic model (which serves as a super-
vised baseline like in D&S11, row 1 in Table 3) dis-
ambiguates among the M filtered frames observing
the sentential context of the target instance. This
can be thought of as combining type- and token-
level information for inference. If the target was
previously seen, it is disambiguated using the su-
8We refer the reader to D&S11 for the details of the graph
construction method, the FrameNet dataset used, example se-
mantic frames, and an excerpt of the graph over targets.
pervised baseline. The test set and the probabilis-
tic model are identical to the ones in D&S11. We
fixed K, the number of nearest neighbors for each
vertex, to be 10. For each graph objective, ?, ? and
M were chosen by five-fold cross-validation. The
cross-validation sets were the same as the ones de-
scribed in ?6.3 of D&S11.9
Results and discussion: Table 3 shows frame iden-
tification accuracy, both using exact match as well
as partial match that assigns partial credit when a re-
lated frame is predicted (Baker et al, 2007). The
final column presents lexicon size in terms of the set
of truncated frame distributions (filtered according
to the top M frames in qi) for all the targets in a
graph. All the graph-based models are better than
the supervised baseline; for our objectives using
pairwise Gaussian fields with sparse unary penal-
ties, the accuracies are equal or better with respect
to NGF-`2; however, the lexicon sizes are reduced
by a few hundred to a few thousand entries. Massive
reduction in lexicon sizes (as in the POS problem in
?4.1) is not visible for these objectives because we
throw out most of the components of the entire set
of distributions q and keep only at most the top M
(which is automatically chosen to be 2 for all ob-
jectives) frames per target. Although a significant
number of components in the whole distribution q
in the sparse objectives get zero mass, the M com-
ponents for a target tend to be non-zero for a major-
ity of the targets. Better results are observed for the
objectives using entropic pairwise penalties; the ob-
9We chose ? from {0.01, 0.1, 0.3, 0.5, 1.0}; ? was chosen
from the same sets as the POS problem. The graph construction
hyperparameter ? described by D&S11 was fixed to 0.2. As in
D&S11, M was chosen from {2, 3, 5, 10}.
684
(a)
t = discrepancy.N t = contribution.N t = print.V t = mislead.V
?SIMILARITY ?GIVING ?TEXT CREATION EXPERIENCER OBJ
NATURAL FEATURES MONEY SENDING ?PREVARICATION
PREVARICATION COMMITMENT DISPERSAL MANIPULATE INTO DOING
QUARRELING ASSISTANCE READING COMPLIANCE
DUPLICATION EARNINGS AND LOSSES STATEMENT EVIDENCE
t = abused.A t = maker.N t = inspire.V t = failed.A
OFFENSES COMMERCE SCENARIO CAUSE TO START SUCCESS OR FAILURE
KILLING ?MANUFACTURING EXPERIENCER OBJ ?SUCCESSFUL ACTION
COMPLIANCE BUSINESSES ?SUBJECTIVE INFLUENCE UNATTRIBUTED INFORMATION
DIFFERENTIATION BEHIND THE SCENES EVOKING PIRACY
COMMITTING CRIME SUPPLY ATTEMPT SUASION WANT SUSPECT
(b)
t = discrepancy.N t = contribution.N t = print.V t = mislead.V
?SIMILARITY ?GIVING ?TEXT CREATION ?PREVARICATION
NON-COMMUTATIVE STATEMENT COMMERCE PAY STATE OF ENTITY EXPERIENCER OBJ
NATURAL FEATURES COMMITMENT DISPERSAL MANIPULATE INTO DOING
ASSISTANCE CONTACTING REASSURING
EARNINGS AND LOSSES READING EVIDENCE
t = abused.A t = maker.N t = inspire.V t = failed.A
?MANUFACTURING CAUSE TO START ?SUCCESSFUL ACTION
BUSINESSES ?SUBJECTIVE INFLUENCE SUCCESSFULLY COMMUNICATE MESSAGE
COMMERCE SCENARIO OBJECTIVE INFLUENCE
SUPPLY EXPERIENCER OBJ
BEING ACTIVE SETTING FIRE
Table 4: Top 5 frames (if there are ? 5 frames with mass greater than zero) according to the graph posterior qt(f)
for (a) NGF-`2 and (b) UJSF-`1,2, given eight unseen predicates in annotated FrameNet data. ? marks the correct
frame, according to the predicate instances in test data (each of these predicates appear only once in test data). Note
that UJSF-`1,2 ranks the correct frame higher than NGF-`2 for several predicates, and produces sparsity quite often;
for the predicate abused.A, the correct frame is not listed by NGF-`2, while UJSF-`1,2 removes it altogether from the
expanded lexicon, resulting in compactness.
jective UJSF-`1,2 gives us the best absolute result by
outperforming the baselines by strong margins, and
also resulting in a tiny lexicon, less than half the size
of the baseline lexicons. The size can be attributed to
the removal of predicates for which all frame com-
ponents were zero (qi = 0). Table 4 contrasts the
induced frames for several unseen predicates for the
NGF-`2 and the UJSF-`2 objectives; the latter often
ranks the correct frame higher, and produces a small
set of frames per predicate.
5 Conclusion
We have presented a family of graph-based SSL ob-
jective functions that incorporate penalties encour-
aging sparse measures at each graph vertex. Our
methods relax the oft-used assumption that the mea-
sures at each vertex form a normalized probabil-
ity distribution, making optimization and the use of
complex penalties easier than prior work. Optimiza-
tion is also easy when there are additional terms in
a graph objective suited to a specific problem; our
generic optimizer would simply require the compu-
tation of new partial derivatives, unlike prior work
that required specialized techniques for a novel ob-
jective function. Finally, experiments on two natural
language lexicon learning problems show that our
methods produce better performance with respect to
state-of-the-art graph-based SSL methods, and also
result in much smaller lexicons.
Acknowledgments
We thank Andre? Martins, Amar Subramanya, and Partha
Talukdar for helpful discussion during the progress of this
work and the three anonymous reviewers for their valu-
able feedback. This research was supported by Qatar Na-
tional Research Foundation grant NPRP 08-485-1-083,
Google?s support of the Worldly Knowledge Project, and
TeraGrid resources provided by the Pittsburgh Supercom-
puting Center under NSF grant number TG-DBS110003.
685
References
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In Proc. of ICML.
C. Baker, M. Ellsworth, and K. Erk. 2007. Task 19:
frame semantic structure extraction. In Proc. of Se-
mEval.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for Youtube: taking random
walks through the view graph. In Proc. of WWW.
Y. Bengio, O. Delalleau, and N. Le Roux. 2006. La-
bel propagation and quadratic criterion. In Olivier
Chapelle, Bernhard Scho?lkopf, and Alexander Zien,
editors, Semi-Supervised Learning, pages 193?216.
MIT Press.
J. Burbea and C. R. Rao. 1982. On the convexity of
some divergence measures based on entropy functions.
IEEE Transactions on Information Theory, 28:489?
495.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press.
A. Corduneanu and T. Jaakkola. 2003. On information
regularization. In Proc. of UAI.
T. M. Cover and J. A. Thomas. 1991. Elements of infor-
mation theory. Wiley-Interscience.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proc. of ACL.
D. Das and N. A. Smith. 2011. Semi-supervised frame-
semantic parsing for unknown predicates. In Proc. of
ACL.
J. Dean and S. Ghemawat. 2008. MapReduce: simplified
data processing on large clusters. Communications of
the ACM, 51:107?113, January.
P. S. Dhillon, D. Foster, and L. Ungar. 2011. Multi-
view learning of word embeddings via cca. In Proc. of
NIPS.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in
the Morning Calm, pages 111?137. Hanshin Publish-
ing Co., Seoul, South Korea.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium.
Y. Grandvalet and Y. Bengio. 2004. Semi-supervised
learning by entropy minimization. In Proc. of NIPS.
W. Gropp, E. Lusk, and A. Skjellum. 1994. Using MPI:
Portable Parallel Programming with the Message-
Passing Interface. MIT Press.
F. Huang and A. Yates. 2009. Distributional representa-
tions for handling sparsity in supervised sequence la-
beling. In Proc. of ACL.
F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schu-
urmans. 2006. Semi-supervised conditional random
fields for improved sequence segmentation and label-
ing. In Proc. of ACL.
M. Kowalski and B. Torre?sani. 2009. Sparsity and per-
sistence: mixed norms provide simple signal models
with dependent coefficients. Signal, Image and Video
Processing, 3:251?264.
S. Kullback and R. A. Leibler. 1951. On information and
sufficiency. Annals of Mathematical Statistics, 22.
J. Lin. 1991. Divergence measures based on the shan-
non entropy. IEEE Transactions on Information the-
ory, 37:145?151.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics, 19(2).
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proc. of HLT-
NAACL.
J. A. O?Sullivan. 1998. Alternating minimization
algorithms: from Blahut-Arimoto to Expectation-
Maximization. In A. Vardy, editor, Codes, Curves,
and Signals: Common Threads in Communications,
pages 173?192. Kluwer.
D. A. Smith and J. Eisner. 2007. Bootstrapping feature-
rich dependency parsers with entropic priors. In Proc.
of EMNLP.
A. Subramanya and J. Bilmes. 2008. Soft-supervised
learning for text classification. In Proc. of EMNLP.
A. Subramanya and J. Bilmes. 2009. Entropic graph reg-
ularization in non-parametric semi-supervised classifi-
cation. In Proc. of NIPS.
A. Subramanya, S. Petrov, and F. Pereira. 2010. Efficient
Graph-based Semi-Supervised Learning of Structured
Tagging Models. In Proc. of EMNLP.
M. Szummer and T. Jaakkola. 2001. Partially labeled
classification with Markov random walks. In Proc. of
NIPS. MIT Press.
P. P. Talukdar and K. Crammer. 2009. New regularized
algorithms for transductive learning. In Proc. of the
ECML-PKDD.
P. P. Talukdar. 2010. Graph-Based Weakly-Supervised
Methods for Information Extraction and Integration.
Ph.D. thesis, University of Pennsylvania.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society
(Series B), 58:267?288.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proc. of ACL.
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with Label Propagation. Technical
report, Carnegie Mellon University.
686
C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal. 1997. Algo-
rithm 778: L-BFGS-B: Fortran subroutines for large-
scale bound-constrained optimization. ACM Transac-
tions on Mathematical Software, 23:550?560.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of ICML.
X. Zhu. 2008. Semi-Supervised Learning Literature Sur-
vey. Online publication., July.
687
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 600?609,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Part-of-Speech Tagging
with Bilingual Graph-Based Projections
Dipanjan Das?
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dipanjan@cs.cmu.edu
Slav Petrov
Google Research
New York, NY 10011, USA
slav@google.com
Abstract
We describe a novel approach for inducing
unsupervised part-of-speech taggers for lan-
guages that have no labeled training data, but
have translated text in a resource-rich lan-
guage. Our method does not assume any
knowledge about the target language (in par-
ticular no tagging dictionary is assumed),
making it applicable to a wide array of
resource-poor languages. We use graph-based
label propagation for cross-lingual knowl-
edge transfer and use the projected labels
as features in an unsupervised model (Berg-
Kirkpatrick et al, 2010). Across eight Eu-
ropean languages, our approach results in an
average absolute improvement of 10.4% over
a state-of-the-art baseline, and 16.7% over
vanilla hidden Markov models induced with
the Expectation Maximization algorithm.
1 Introduction
Supervised learning approaches have advanced the
state-of-the-art on a variety of tasks in natural lan-
guage processing, resulting in highly accurate sys-
tems. Supervised part-of-speech (POS) taggers,
for example, approach the level of inter-annotator
agreement (Shen et al, 2007, 97.3% accuracy for
English). However, supervised methods rely on la-
beled training data, which is time-consuming and
expensive to generate. Unsupervised learning ap-
proaches appear to be a natural solution to this prob-
lem, as they require only unannotated text for train-
?This research was carried out during an internship at Google
Research.
ing models. Unfortunately, the best completely un-
supervised English POS tagger (that does not make
use of a tagging dictionary) reaches only 76.1% ac-
curacy (Christodoulopoulos et al, 2010), making its
practical usability questionable at best.
To bridge this gap, we consider a practically mo-
tivated scenario, in which we want to leverage ex-
isting resources from a resource-rich language (like
English) when building tools for resource-poor for-
eign languages.1 We assume that absolutely no la-
beled training data is available for the foreign lan-
guage of interest, but that we have access to parallel
data with a resource-rich language. This scenario is
applicable to a large set of languages and has been
considered by a number of authors in the past (Al-
shawi et al, 2000; Xi and Hwa, 2005; Ganchev et
al., 2009). Naseem et al (2009) and Snyder et al
(2009) study related but different multilingual gram-
mar and tagger induction tasks, where it is assumed
that no labeled data at all is available.
Our work is closest to that of Yarowsky and Ngai
(2001), but differs in two important ways. First,
we use a novel graph-based framework for project-
ing syntactic information across language bound-
aries. To this end, we construct a bilingual graph
over word types to establish a connection between
the two languages (?3), and then use graph label
propagation to project syntactic information from
English to the foreign language (?4). Second, we
treat the projected labels as features in an unsuper-
1For simplicity of exposition we refer to the resource-poor lan-
guage as the ?foreign language.? Similarly, we use English
as the resource-rich language, but any other language with la-
beled resources could be used instead.
600
vised model (?5), rather than using them directly for
supervised training. To make the projection practi-
cal, we rely on the twelve universal part-of-speech
tags of Petrov et al (2011). Syntactic universals are
a well studied concept in linguistics (Carnie, 2002;
Newmeyer, 2005), and were recently used in similar
form by Naseem et al (2010) for multilingual gram-
mar induction. Because there might be some contro-
versy about the exact definitions of such universals,
this set of coarse-grained POS categories is defined
operationally, by collapsing language (or treebank)
specific distinctions to a set of categories that ex-
ists across all languages. These universal POS cat-
egories not only facilitate the transfer of POS in-
formation from one language to another, but also
relieve us from using controversial evaluation met-
rics,2 by establishing a direct correspondence be-
tween the induced hidden states in the foreign lan-
guage and the observed English labels.
We evaluate our approach on eight European lan-
guages (?6), and show that both our contributions
provide consistent and statistically significant im-
provements. Our final average POS tagging accu-
racy of 83.4% compares very favorably to the av-
erage accuracy of Berg-Kirkpatrick et al?s mono-
lingual unsupervised state-of-the-art model (73.0%),
and considerably bridges the gap to fully supervised
POS tagging performance (96.6%).
2 Approach Overview
The focus of this work is on building POS taggers
for foreign languages, assuming that we have an En-
glish POS tagger and some parallel text between
the two languages. Central to our approach (see
Algorithm 1) is a bilingual similarity graph built
from a sentence-aligned parallel corpus. As dis-
cussed in more detail in ?3, we use two types of
vertices in our graph: on the foreign language side
vertices correspond to trigram types, while the ver-
tices on the English side are individual word types.
Graph construction does not require any labeled
data, but makes use of two similarity functions. The
edge weights between the foreign language trigrams
are computed using a co-occurence based similar-
ity function, designed to indicate how syntactically
2See Christodoulopoulos et al (2010) for a discussion of met-
rics for evaluating unsupervised POS induction systems.
Algorithm 1 Bilingual POS Induction
Require: Parallel English and foreign language
data De and Df , unlabeled foreign training data
?f ; English tagger.
Ensure: ?f , a set of parameters learned using a
constrained unsupervised model (?5).
1: De?f ? word-align-bitext(De,Df )
2: D?e ? pos-tag-supervised(De)
3: A ? extract-alignments(De?f , D?e)
4: G? construct-graph(?f ,Df ,A)
5: G?? graph-propagate(G)
6: ?? extract-word-constraints(G?)
7: ?f ? pos-induce-constrained(?f ,?)
8: Return ?f
similar the middle words of the connected trigrams
are (?3.2). To establish a soft correspondence be-
tween the two languages, we use a second similar-
ity function, which leverages standard unsupervised
word alignment statistics (?3.3).3
Since we have no labeled foreign data, our goal
is to project syntactic information from the English
side to the foreign side. To initialize the graph we
tag the English side of the parallel text using a su-
pervised model. By aggregating the POS labels of
the English tokens to types, we can generate label
distributions for the English vertices. Label propa-
gation can then be used to transfer the labels to the
peripheral foreign vertices (i.e. the ones adjacent to
the English vertices) first, and then among all of the
foreign vertices (?4). The POS distributions over the
foreign trigram types are used as features to learn a
better unsupervised POS tagger (?5). The follow-
ing three sections elaborate these different stages is
more detail.
3 Graph Construction
In graph-based learning approaches one constructs
a graph whose vertices are labeled and unlabeled
examples, and whose weighted edges encode the
degree to which the examples they link have the
same label (Zhu et al, 2003). Graph construction
for structured prediction problems such as POS tag-
ging is non-trivial: on the one hand, using individ-
ual words as the vertices throws away the context
3The word alignment methods do not use POS information.
601
necessary for disambiguation; on the other hand,
it is unclear how to define (sequence) similarity if
the vertices correspond to entire sentences. Altun
et al (2005) proposed a technique that uses graph
based similarity between labeled and unlabeled parts
of structured data in a discriminative framework for
semi-supervised learning. More recently, Subra-
manya et al (2010) defined a graph over the cliques
in an underlying structured prediction model. They
considered a semi-supervised POS tagging scenario
and showed that one can use a graph over trigram
types, and edge weights based on distributional sim-
ilarity, to improve a supervised conditional random
field tagger.
3.1 Graph Vertices
We extend Subramanya et al?s intuitions to our
bilingual setup. Because the information flow in
our graph is asymmetric (from English to the foreign
language), we use different types of vertices for each
language. The foreign language vertices (denoted by
Vf ) correspond to foreign trigram types, exactly as
in Subramanya et al (2010). On the English side,
however, the vertices (denoted by Ve) correspond to
word types. Because all English vertices are going
to be labeled, we do not need to disambiguate them
by embedding them in trigrams. Furthermore, we do
not connect the English vertices to each other, but
only to foreign language vertices.4
The graph vertices are extracted from the differ-
ent sides of a parallel corpus (De, Df ) and an ad-
ditional unlabeled monolingual foreign corpus ?f ,
which will be used later for training. We use two dif-
ferent similarity functions to define the edge weights
among the foreign vertices and between vertices
from different languages.
3.2 Monolingual Similarity Function
Our monolingual similarity function (for connecting
pairs of foreign trigram types) is the same as the one
used by Subramanya et al (2010). We briefly re-
view it here for completeness. We define a sym-
metric similarity function K(ui, uj) over two for-
4This is because we are primarily interested in learning foreign
language taggers, rather than improving supervised English
taggers. Note, however, that it would be possible to use our
graph-based framework also for completely unsupervised POS
induction in both languages, similar to Snyder et al (2009).
Description Feature
Trigram + Context x1 x2 x3 x4 x5
Trigram x2 x3 x4
Left Context x1 x2
Right Context x4 x5
Center Word x3
Trigram ? Center Word x2 x4
Left Word + Right Context x2 x4 x5
Left Context + Right Word x1 x2 x4
Suffix HasSuffix(x3)
Table 1: Various features used for computing edge
weights between foreign trigram types.
eign language vertices ui, uj ? Vf based on the
co-occurrence statistics of the nine feature concepts
given in Table 1. Each feature concept is akin to a
random variable and its occurrence in the text corre-
sponds to a particular instantiation of that random
variable. For each trigram type x2 x3 x4 in a se-
quence x1 x2 x3 x4 x5, we count how many times
that trigram type co-occurs with the different instan-
tiations of each concept, and compute the point-wise
mutual information (PMI) between the two.5 The
similarity between two trigram types is given by
summing over the PMI values over feature instan-
tiations that they have in common. This is similar to
stacking the different feature instantiations into long
(sparse) vectors and computing the cosine similarity
between them. Finally, note that while most feature
concepts are lexicalized, others, such as the suffix
concept, are not.
Given this similarity function, we define a near-
est neighbor graph, where the edge weight for the n
most similar vertices is set to the value of the simi-
larity function and to 0 for all other vertices. We use
N (u) to denote the neighborhood of vertex u, and
fixed n = 5 in our experiments.
3.3 Bilingual Similarity Function
To define a similarity function between the English
and the foreign vertices, we rely on high-confidence
word alignments. Since our graph is built from a
parallel corpus, we can use standard word align-
ment techniques to align the English sentences De
5Note that many combinations are impossible giving a PMI
value of 0; e.g., when the trigram type and the feature instanti-
ation don?t have words in common.
602
and their foreign language translations Df .6 Label
propagation in the graph will provide coverage and
high recall, and we therefore extract only intersected
high-confidence (> 0.9) alignments De?f .
Based on these high-confidence alignments we
can extract tuples of the form [u ? v], where u is
a foreign trigram type, whose middle word aligns
to an English word type v. Our bilingual similarity
function then sets the edge weights in proportion to
these tuple counts.
3.4 Graph Initialization
So far the graph has been completely unlabeled. To
initialize the graph for label propagation we use a su-
pervised English tagger to label the English side of
the bitext.7 We then simply count the individual la-
bels of the English tokens and normalize the counts
to produce tag distributions over English word types.
These tag distributions are used to initialize the label
distributions over the English vertices in the graph.
Note that since all English vertices were extracted
from the parallel text, we will have an initial label
distribution for all vertices in Ve.
3.5 Graph Example
A very small excerpt from an Italian-English graph
is shown in Figure 1. As one can see, only the
trigrams [suo incarceramento ,], [suo iter ,] and
[suo carattere ,] are connected to English words. In
this particular case, all English vertices are labeled
as nouns by the supervised tagger. In general, the
neighborhoods can be more diverse and we allow a
soft label distribution over the vertices. It is worth
noting that the middle words of the Italian trigrams
are nouns too, which exhibits the fact that the sim-
ilarity metric connects types having the same syn-
tactic category. In the label propagation stage, we
propagate the automatic English tags to the aligned
Italian trigram types, followed by further propaga-
tion solely among the Italian vertices.
6We ran six iterations of IBM Model 1 (Brown et al, 1993),
followed by six iterations of the HMM model (Vogel et al,
1996) in both directions.
7We used a tagger based on a trigram Markov model (Brants,
2000) trained on the Wall Street Journal portion of the Penn
Treebank (Marcus et al, 1993), for its fast speed and reason-
able accuracy (96.7% on sections 22-24 of the treebank, but
presumably much lower on the (out-of-domain) parallel cor-
pus).
[ suo iter , ]
[ suo incarceramento , ]
[ suo fidanzato , ]
[ suo carattere , ]
[ imprisonment ]
[ enactment ]
[ character ]
[ del fidanzato , ]
[ il fidanzato , ]
NOUN
NOUN
NOUN
[ al fidanzato e ]
Figure 1: An excerpt from the graph for Italian. Three of
the Italian vertices are connected to an automatically la-
beled English vertex. Label propagation is used to propa-
gate these tags inwards and results in tag distributions for
the middle word of each Italian trigram.
4 POS Projection
Given the bilingual graph described in the previous
section, we can use label propagation to project the
English POS labels to the foreign language. We use
label propagation in two stages to generate soft la-
bels on all the vertices in the graph. In the first stage,
we run a single step of label propagation, which
transfers the label distributions from the English
vertices to the connected foreign language vertices
(say, V lf ) at the periphery of the graph. Note that
because we extracted only high-confidence align-
ments, many foreign vertices will not be connected
to any English vertices. This stage of label propa-
gation results in a tag distribution ri over labels y,
which encodes the proportion of times the middle
word of ui ? Vf aligns to English words vy tagged
with label y:
ri(y) =
?
vy
#[ui ? vy]
?
y?
?
vy?
#[ui ? vy? ]
(1)
The second stage consists of running traditional
label propagation to propagate labels from these pe-
ripheral vertices V lf to all foreign language vertices
603
in the graph, optimizing the following objective:
C(q) =
?
ui?Vf\V lf ,uj?N (ui)
wij?qi ? qj?
2
+ ?
?
ui?Vf\V lf
?qi ? U?
2
s.t.
?
y
qi(y) = 1 ?ui
qi(y) ? 0 ?ui, y
qi = ri ?ui ? V
l
f (2)
where the qi (i = 1, . . . , |Vf |) are the label distribu-
tions over the foreign language vertices and ? and
? are hyperparameters that we discuss in ?6.4. We
use a squared loss to penalize neighboring vertices
that have different label distributions: ?qi ? qj?2 =?
y(qi(y)?qj(y))
2, and additionally regularize the
label distributions towards the uniform distribution
U over all possible labels Y . It can be shown that
this objective is convex in q.
The first term in the objective function is the graph
smoothness regularizer which encourages the distri-
butions of similar vertices (large wij) to be similar.
The second term is a regularizer and encourages all
type marginals to be uniform to the extent that is al-
lowed by the first two terms (cf. maximum entropy
principle). If an unlabeled vertex does not have a
path to any labeled vertex, this term ensures that the
converged marginal for this vertex will be uniform
over all tags, allowing the middle word of such an
unlabeled vertex to take on any of the possible tags.
While it is possible to derive a closed form so-
lution for this convex objective function, it would
require the inversion of a matrix of order |Vf |. In-
stead, we resort to an iterative update based method.
We formulate the update as follows:
q(m)i (y) =
?
?
?
ri(y) if ui ? V lf
?i(y)
?i
otherwise
(3)
where ?ui ? Vf \ V lf , ?i(y) and ?i are defined as:
?i(y) =
?
uj?N (ui)
wijq
(m?1)
j (y) + ? U(y) (4)
?i = ? +
?
uj?N (ui)
wij (5)
We ran this procedure for 10 iterations.
5 POS Induction
After running label propagation (LP), we com-
pute tag probabilities for foreign word types x by
marginalizing the POS tag distributions of foreign
trigrams ui = x? x x+ over the left and right con-
text words:
p(y|x) =
?
x?,x+
qi(y)
?
x?,x+,y?
qi(y
?)
(6)
We then extract a set of possible tags tx(y) by elimi-
nating labels whose probability is below a threshold
value ? :
tx(y) =
{
1 if p(y|x) ? ?
0 otherwise
(7)
We describe how we choose ? in ?6.4. This vector
tx is constructed for every word in the foreign vo-
cabulary and will be used to provide features for the
unsupervised foreign language POS tagger.
We develop our POS induction model based on
the feature-based HMM of Berg-Kirkpatrick et al
(2010). For a sentence x and a state sequence z, a
first order Markov model defines a distribution:
P?(X = x,Z = z) = P?(Z1 = z1)?
?|x|
i=1 P?(Zi+1 = zi+1 | Zi = zi)? ?? ?
transition
?
P?(Xi = xi | Zi = zi)
? ?? ?
emission
(8)
In a traditional Markov model, the emission distri-
bution P?(Xi = xi | Zi = zi) is a set of multinomi-
als. The feature-based model replaces the emission
distribution with a log-linear model, such that:
P?(X = x | Z = z) =
exp ?>f(x, z)
?
x??Val(X)
exp ?>f(x?, z)
(9)
where Val(X) corresponds to the entire vocabulary.
This locally normalized log-linear model can look at
various aspects of the observation x, incorporating
overlapping features of the observation. In our ex-
periments, we used the same set of features as Berg-
Kirkpatrick et al (2010): an indicator feature based
604
on the word identity x, features checking whether x
contains digits or hyphens, whether the first letter of
x is upper case, and suffix features up to length 3.
All features were conjoined with the state z.
We trained this model by optimizing the following
objective function:
L(?) =
N?
i=1
log
?
z
P?(X = x
(i),Z = z(i))
?C???22 (10)
Note that this involves marginalizing out all possible
state configurations z for a sentence x, resulting in
a non-convex objective. To optimize this function,
we used L-BFGS, a quasi-Newton method (Liu and
Nocedal, 1989). For English POS tagging, Berg-
Kirkpatrick et al (2010) found that this direct gra-
dient method performed better (>7% absolute ac-
curacy) than using a feature-enhanced modification
of the Expectation-Maximization (EM) algorithm
(Dempster et al, 1977).8 Moreover, this route of
optimization outperformed a vanilla HMM trained
with EM by 12%.
We adopted this state-of-the-art model because it
makes it easy to experiment with various ways of
incorporating our novel constraint feature into the
log-linear emission model. This feature ft incor-
porates information from the smoothed graph and
prunes hidden states that are inconsistent with the
thresholded vector tx. The function ? : F ? C
maps from the language specific fine-grained tagset
F to the coarser universal tagset C and is described
in detail in ?6.2:
ft(x, z) = log(tx(y)), if ?(z) = y (11)
Note that when tx(y) = 1 the feature value is 0
and has no effect on the model, while its value is
?? when tx(y) = 0 and constrains the HMM?s
state space. This formulation of the constraint fea-
ture is equivalent to the use of a tagging dictionary
extracted from the graph using a threshold ? on the
posterior distribution of tags for a given word type
(Eq. 7). It would have therefore also been possible to
use the integer programming (IP) based approach of
8See ?3.1 of Berg-Kirkpatrick et al (2010) for more details
about their modification of EM, and how gradients are com-
puted for L-BFGS.
Ravi and Knight (2009) instead of the feature-HMM
for POS induction on the foreign side. However, we
do not explore this possibility in the current work.
6 Experiments and Results
Before presenting our results, we describe the
datasets that we used, as well as two baselines.
6.1 Datasets
We utilized two kinds of datasets in our experiments:
(i) monolingual treebanks9 and (ii) large amounts of
parallel text with English on one side. The availabil-
ity of these resources guided our selection of foreign
languages. For monolingual treebank data we re-
lied on the CoNLL-X and CoNLL-2007 shared tasks
on dependency parsing (Buchholz and Marsi, 2006;
Nivre et al, 2007). The parallel data came from the
Europarl corpus (Koehn, 2005) and the ODS United
Nations dataset (UN, 2006). Taking the intersection
of languages in these resources, and selecting lan-
guages with large amounts of parallel data, yields
the following set of eight Indo-European languages:
Danish, Dutch, German, Greek, Italian, Portuguese,
Spanish and Swedish.
Of course, we are primarily interested in apply-
ing our techniques to languages for which no la-
beled resources are available. However, we needed
to restrict ourselves to these languages in order to
be able to evaluate the performance of our approach.
We paid particular attention to minimize the number
of free parameters, and used the same hyperparam-
eters for all language pairs, rather than attempting
language-specific tuning. We hope that this will al-
low practitioners to apply our approach directly to
languages for which no resources are available.
6.2 Part-of-Speech Tagset and HMM States
We use the universal POS tagset of Petrov et al
(2011) in our experiments.10 This set C consists
of the following 12 coarse-grained tags: NOUN
(nouns), VERB (verbs), ADJ (adjectives), ADV
(adverbs), PRON (pronouns), DET (determiners),
ADP (prepositions or postpositions), NUM (numer-
als), CONJ (conjunctions), PRT (particles), PUNC
9We extracted only the words and their POS tags from the tree-
banks.
10Available at http://code.google.com/p/universal-pos-tags/.
605
(punctuation marks) and X (a catch-all for other
categories such as abbreviations or foreign words).
While there might be some controversy about the
exact definition of such a tagset, these 12 categories
cover the most frequent part-of-speech and exist in
one form or another in all of the languages that we
studied.
For each language under consideration, Petrov et
al. (2011) provide a mapping ? from the fine-grained
language specific POS tags in the foreign treebank
to the universal POS tags. The supervised POS tag-
ging accuracies (on this tagset) are shown in the last
row of Table 2. The taggers were trained on datasets
labeled with the universal tags.
The number of latent HMM states for each lan-
guage in our experiments was set to the number of
fine tags in the language?s treebank. In other words,
the set of hidden states F was chosen to be the fine
set of treebank tags. Therefore, the number of fine
tags varied across languages for our experiments;
however, one could as well have fixed the set of
HMM states to be a constant across languages, and
created one mapping to the universal POS tagset.
6.3 Various Models
To provide a thorough analysis, we evaluated three
baselines and two oracles in addition to two variants
of our graph-based approach. We were intentionally
lenient with our baselines:
? EM-HMM: A traditional HMM baseline, with
multinomial emission and transition distribu-
tions estimated by the Expectation Maximiza-
tion algorithm. We evaluated POS tagging ac-
curacy using the lenient many-to-1 evaluation
approach (Johnson, 2007).
? Feature-HMM: The vanilla feature-HMM of
Berg-Kirkpatrick et al (2010) (i.e. no ad-
ditional constraint feature) served as a sec-
ond baseline. Model parameters were esti-
mated with L-BFGS and evaluation again used
a greedy many-to-1 mapping.
? Projection: Our third baseline incorporates
bilingual information by projecting POS tags
directly across alignments in the parallel data.
For unaligned words, we set the tag to the most
frequent tag in the corresponding treebank. For
each language, we took the same number of
sentences from the bitext as there are in its tree-
bank, and trained a supervised feature-HMM.
This can be seen as a rough approximation of
Yarowsky and Ngai (2001).
We tried two versions of our graph-based approach:
? No LP: Our first version takes advantage of
our bilingual graph, but extracts the constraint
feature after the first stage of label propagation
(Eq. 1). Because many foreign word types are
not aligned to an English word (see Table 3),
and we do not run label propagation on the for-
eign side, we expect the projected information
to have less coverage. Furthermore we expect
the label distributions on the foreign to be fairly
noisy, because the graph constraints have not
been taken into account yet.
? With LP: Our full model uses both stages
of label propagation (Eq. 2) before extracting
the constraint features. As a result, we are
able to extract the constraint feature for all for-
eign word types and furthermore expect the
projected tag distributions to be smoother and
more stable.
Our oracles took advantage of the labeled treebanks:
? TB Dictionary: We extracted tagging dictio-
naries from the treebanks and and used them as
constraint features in the feature-based HMM.
Evaluation was done using the prespecified
mappings.
? Supervised: We trained the supervised model
of Brants (2000) on the original treebanks and
mapped the language-specific tags to the uni-
versal tags for evaluation.
6.4 Experimental Setup
While we tried to minimize the number of free pa-
rameters in our model, there are a few hyperparam-
eters that need to be set. Fortunately, performance
was stable across various values, and we were able
to use the same hyperparameters for all languages.
We used C = 1.0 as the L2 regularization con-
stant in (Eq. 10) and trained both EM and L-BFGS
for 1000 iterations. When extracting the vector
606
Model Danish Dutch German Greek Italian Portuguese Spanish Swedish Avg
baselines
EM-HMM 68.7 57.0 75.9 65.8 63.7 62.9 71.5 68.4 66.7
Feature-HMM 69.1 65.1 81.3 71.8 68.1 78.4 80.2 70.1 73.0
Projection 73.6 77.0 83.2 79.3 79.7 82.6 80.1 74.7 78.8
our approach
No LP 79.0 78.8 82.4 76.3 84.8 87.0 82.8 79.4 81.3
With LP 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
oracles
TB Dictionary 93.1 94.7 93.5 96.6 96.4 94.0 95.8 85.5 93.7
Supervised 96.9 94.9 98.2 97.8 95.8 97.2 96.8 94.8 96.6
Table 2: Part-of-speech tagging accuracies for various baselines and oracles, as well as our approach. ?Avg? denotes
macro-average across the eight languages.
tx used to compute the constraint feature from the
graph, we tried three threshold values for ? (see
Eq. 7). Because we don?t have a separate develop-
ment set, we used the training set to select among
them and found 0.2 to work slightly better than 0.1
and 0.3. For seven out of eight languages a thresh-
old of 0.2 gave the best results for our final model,
which indicates that for languages without any val-
idation set, ? = 0.2 can be used. For graph prop-
agation, the hyperparameter ? was set to 2 ? 10?6
and was not tuned. The graph was constructed using
2 million trigrams; we chose these by truncating the
parallel datasets up to the number of sentence pairs
that contained 2 million trigrams.
6.5 Results
Table 2 shows our complete set of results. As ex-
pected, the vanilla HMM trained with EM performs
the worst. The feature-HMM model works better for
all languages, generalizing the results achieved for
English by Berg-Kirkpatrick et al (2010). Our ?Pro-
jection? baseline is able to benefit from the bilingual
information and greatly improves upon the mono-
lingual baselines, but falls short of the ?No LP?
model by 2.5% on an average. The ?No LP? model
does not outperform direct projection for German
and Greek, but performs better for six out of eight
languages. Overall, it gives improvements ranging
from 1.1% for German to 14.7% for Italian, for an
average improvement of 8.3% over the unsupervised
feature-HMM model. For comparison, the com-
pletely unsupervised feature-HMM baseline accu-
racy on the universal POS tags for English is 79.4%,
and goes up to 88.7% with a treebank dictionary.
Our full model (?With LP?) outperforms the un-
supervised baselines and the ?No LP? setting for all
languages. It falls short of the ?Projection? base-
line for German, but is statistically indistinguish-
able in terms of accuracy. As indicated by bolding,
for seven out of eight languages the improvements
of the ?With LP? setting are statistically significant
with respect to the other models, including the ?No
LP? setting.11 Overall, it performs 10.4% better
than the hitherto state-of-the-art feature-HMM base-
line, and 4.6% better than direct projection, when we
macro-average the accuracy over all languages.
6.6 Discussion
Our full model outperforms the ?No LP? setting
because it has better vocabulary coverage and al-
lows the extraction of a larger set of constraint fea-
tures. We tabulate this increase in Table 3. For all
languages, the vocabulary sizes increase by several
thousand words. Although the tag distributions of
the foreign words (Eq. 6) are noisy, the results con-
firm that label propagation within the foreign lan-
guage part of the graph adds significant quality for
every language.
Figure 2 shows an excerpt of a sentence from the
Italian test set and the tags assigned by four different
models, as well as the gold tags. While the first three
models get three to four tags wrong, our best model
gets only one word wrong and is the most accurate
among the four models for this example. Examin-
ing the word fidanzato for the ?No LP? and ?With
LP? models is particularly instructive. As Figure 1
shows, this word has no high-confidence alignment
in the Italian-English bitext. As a result, its POS tag
needs to be induced in the ?No LP? case, while the
11A word level paired-t-test is significant at p < 0.01 for Dan-
ish, Greek, Italian, Portuguese, Spanish and Swedish, and
p < 0.05 for Dutch.
607
Gold:
si          trovava          in           un          parco          con          il            fidanzato         Paolo        F.     ,     27    anni       ,   rappresentante   
EM-HMM:
Feature-HMM:
No LP:
With LP:
CONJ NOUN DET DET NOUN ADP DET NOUN .
NOUN
.
NUM NOUN .
NOUN
PRON VERB ADP DET NOUN CONJ DET NOUN NOUN
NOUN
.
ADP NOUN .
VERB
PRON VERB ADP DET NOUN ADP DET NOUN NOUN
NOUN
.
NUM NOUN .
NOUN
VERB VERB ADP DET NOUN ADP DET ADJ NOUN
ADJ
.
NUM NOUN .
NOUN
VERB VERB ADP DET NOUN ADP DET NOUN NOUN
NOUN
.
NUM NOUN .
NOUN
Figure 2: Tags produced by the different models along with the reference set of tags for a part of a sentence from the
Italian test set. Italicized tags denote incorrect labels.
Language
# words with constraints
?No LP? ?With LP?
Danish 88,240 128, 391
Dutch 51,169 74,892
German 59,534 107,249
Greek 90,231 114,002
Italian 48,904 62,461
Portuguese 46,787 65,737
Spanish 72,215 82,459
Swedish 70,181 88,454
Table 3: Size of the vocabularies for the ?No LP? and
?With LP? models for which we can impose constraints.
correct tag is available as a constraint feature in the
?With LP? case.
7 Conclusion
We have shown the efficacy of graph-based label
propagation for projecting part-of-speech informa-
tion across languages. Because we are interested in
applying our techniques to languages for which no
labeled resources are available, we paid particular
attention to minimize the number of free parame-
ters and used the same hyperparameters for all lan-
guage pairs. Our results suggest that it is possible to
learn accurate POS taggers for languages which do
not have any annotated data, but have translations
into a resource-rich language. Our results outper-
form strong unsupervised baselines as well as ap-
proaches that rely on direct projections, and bridge
the gap between purely supervised and unsupervised
POS tagging models.
Acknowledgements
We would like to thank Ryan McDonald for numer-
ous discussions on this topic. We would also like to
thank Amarnag Subramanya for helping us with the
implementation of label propagation and Shankar
Kumar for access to the parallel data. Finally, we
thank Kuzman Ganchev and the three anonymous
reviewers for helpful suggestions and comments on
earlier drafts of this paper.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Head-transducer models for speech translation
and their automatic acquisition from bilingual data.
Machine Translation, 15.
Yasemin Altun, David McAllester, and Mikhail Belkin.
2005. Maximum margin semi-supervised learning for
structured variables. In Proc. of NIPS.
Taylor Berg-Kirkpatrick, Alexandre B. Co?te?, John DeN-
ero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proc. of NAACL-HLT.
Thorsten Brants. 2000. TnT - a statistical part-of-speech
tagger. In Proc. of ANLP.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Andrew Carnie. 2002. Syntax: A Generative Introduc-
tion (Introducing Linguistics). Blackwell Publishing.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proc. of
EMNLP.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proc. of ACL-IJCNLP.
608
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP-CoNLL.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. JAIR, 36.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowledge
to guide grammar induction. In Proc. of EMNLP.
Frederick J. Newmeyer. 2005. Possible and Probable
Languages: A Generative Perspective on Linguistic
Typology. Oxford University Press.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proc. of
ACL-IJCNLP.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proc. of ACL.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proc. of ACL-IJCNLP.
Amar Subramanya, Slav Petrov, and Fernando Pereira.
2010. Efficient graph-based semi-supervised learning
of structured tagging models. In Proc. of EMNLP.
UN. 2006. ODS UN parallel corpus.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. of COLING.
Chenhai Xi and Rebecca Hwa. 2005. A backoff
model for bootstrapping resources for non-English
languages. In Proc. of HLT-EMNLP.
David Yarowsky and Grace Ngai. 2001. Inducing multi-
lingual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. In Proc. of NAACL.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In Proc. of ICML.
609
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1435?1444,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semi-Supervised Frame-Semantic Parsing for Unknown Predicates
Dipanjan Das and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dipanjan,nasmith}@cs.cmu.edu
Abstract
We describe a new approach to disambiguat-
ing semantic frames evoked by lexical predi-
cates previously unseen in a lexicon or anno-
tated data. Our approach makes use of large
amounts of unlabeled data in a graph-based
semi-supervised learning framework. We con-
struct a large graph where vertices correspond
to potential predicates and use label propa-
gation to learn possible semantic frames for
new ones. The label-propagated graph is used
within a frame-semantic parser and, for un-
known predicates, results in over 15% abso-
lute improvement in frame identification ac-
curacy and over 13% absolute improvement
in full frame-semantic parsing F1 score on a
blind test set, over a state-of-the-art supervised
baseline.
1 Introduction
Frame-semantic parsing aims to extract a shallow se-
mantic structure from text, as shown in Figure 1.
The FrameNet lexicon (Fillmore et al, 2003) is
a rich linguistic resource containing expert knowl-
edge about lexical and predicate-argument seman-
tics. The lexicon suggests an analysis based on the
theory of frame semantics (Fillmore, 1982). Recent
approaches to frame-semantic parsing have broadly
focused on the use of two statistical classifiers cor-
responding to the aforementioned subtasks: the first
one to identify the most suitable semantic frame for
a marked lexical predicate (target, henceforth) in a
sentence, and the second for performing semantic
role labeling (SRL) given the frame.
The FrameNet lexicon, its exemplar sentences
containing instantiations of semantic frames, and
full-text annotations provide supervision for learn-
ing frame-semantic parsers. Yet these annotations
lack coverage, including only 9,300 annotated tar-
get types. Recent papers have tried to address the
coverage problem. Johansson and Nugues (2007)
used WordNet (Fellbaum, 1998) to expand the list of
targets that can evoke frames and trained classifiers
to identify the best-suited frame for the newly cre-
ated targets. In past work, we described an approach
where latent variables were used in a probabilistic
model to predict frames for unseen targets (Das et
al., 2010a).1 Relatedly, for the argument identifica-
tion subtask, Matsubayashi et al (2009) proposed
a technique for generalization of semantic roles to
overcome data sparseness. Unseen targets continue
to present a major obstacle to domain-general se-
mantic analysis.
In this paper, we address the problem of idenfi-
fying the semantic frames for targets unseen either
in FrameNet (including the exemplar sentences) or
the collection of full-text annotations released along
with the lexicon. Using a standard model for the ar-
gument identification stage (Das et al, 2010a), our
proposed method improves overall frame-semantic
parsing, especially for unseen targets. To better han-
dle these unseen targets, we adopt a graph-based
semi-supervised learning stategy (?4). We construct
a large graph over potential targets, most of which
1Notwithstanding state-of-the-art results, that approach was
only able to identify the correct frame for 1.9% of unseen tar-
gets in the test data available at that time. That system achieves
about 23% on the test set used in this paper.
1435
bell.n
ring.v
there be.v
enough.a
LU
NOISE_MAKERS
SUFFICIENCY
Frame
EXISTENCE
CAUSE_TO_MAKE_NOISE
.bells
 
 
N_m
more than six of the eight
Sound_maker
Enabled_situation
ringtoringers
Item
enough
Entity
Agent
n'tarestillthereBut
Figure 1: An example sentence from the PropBank section of the full-text annotations released as part of FrameNet
1.5. Each row under the sentence correponds to a semantic frame and its set of corresponding arguments. Thick lines
indicate targets that evoke frames; thin solid/dotted lines with labels indicate arguments. N m under ?bells? is short
for the Noise maker role of the NOISE MAKERS frame.
are drawn from unannotated data, and a fraction
of which come from seen FrameNet annotations.
Next, we perform label propagation on the graph,
which is initialized by frame distributions over the
seen targets. The resulting smoothed graph con-
sists of posterior distributions over semantic frames
for each target in the graph, thus increasing cover-
age. These distributions are then evaluated within
a frame-semantic parser (?5). Considering unseen
targets in test data (although few because the test
data is also drawn from the training domain), sig-
nificant absolute improvements of 15.7% and 13.7%
are observed for frame identification and full frame-
semantic parsing, respectively, indicating improved
coverage for hitherto unobserved predicates (?6).
2 Background
Before going into the details of our model, we pro-
vide some background on two topics relevant to
this paper: frame-semantic parsing and graph-based
learning applied to natural language tasks.
2.1 Frame-semantic Parsing
Gildea and Jurafsky (2002) pioneered SRL, and
since then there has been much applied research
on predicate-argument semantics. Early work on
frame-semantic role labeling made use of the ex-
emplar sentences in the FrameNet corpus, each of
which is annotated for a single frame and its argu-
ments (Thompson et al, 2003; Fleischman et al,
2003; Shi and Mihalcea, 2004; Erk and Pado?, 2006,
inter alia). Most of this work was done on an older,
smaller version of FrameNet. Recently, since the re-
lease of full-text annotations in SemEval?07 (Baker
et al, 2007), there has been work on identifying
multiple frames and their corresponding sets of ar-
guments in a sentence. The LTH system of Jo-
hansson and Nugues (2007) performed the best in
the SemEval?07 shared task on frame-semantic pars-
ing. Our probabilistic frame-semantic parser out-
performs LTH on that task and dataset (Das et al,
2010a). The current paper builds on those proba-
bilistic models to improve coverage on unseen pred-
icates.2
Expert resources have limited coverage, and
FrameNet is no exception. Automatic induction of
semantic resources has been a major effort in re-
cent years (Snow et al, 2006; Ponzetto and Strube,
2007, inter alia). In the domain of frame semantics,
previous work has sought to extend the coverage
of FrameNet by exploiting resources like VerbNet,
WordNet, or Wikipedia (Shi and Mihalcea, 2005;
Giuglea and Moschitti, 2006; Pennacchiotti et al,
2008; Tonelli and Giuliano, 2009), and projecting
entries and annotations within and across languages
(Boas, 2002; Fung and Chen, 2004; Pado? and La-
pata, 2005). Although these approaches have in-
creased coverage to various degrees, they rely on
other lexicons and resources created by experts.
Fu?rstenau and Lapata (2009) proposed the use of un-
labeled data to improve coverage, but their work was
limited to verbs. Bejan (2009) used self-training to
improve frame identification and reported improve-
ments, but did not explicitly model unknown tar-
gets. In contrast, we use statistics gathered from
large volumes of unlabeled data to improve the cov-
erage of a frame-semantic parser on several syntactic
categories, in a novel framework that makes use of
graph-based semi-supervised learning.
2SEMAFOR, the system presented by Das et al (2010a) is
publicly available at http://www.ark.cs.cmu.edu/
SEMAFOR and has been extended in this work.
1436
2.2 Graph-based Semi-Supervised Learning
In graph-based semi-supervised learning, one con-
structs a graph whose vertices are labeled and unla-
beled examples. Weighted edges in the graph, con-
necting pairs of examples/vertices, encode the de-
gree to which they are expected to have the same
label (Zhu et al, 2003). Variants of label propaga-
tion are used to transfer labels from the labeled to the
unlabeled examples. There are several instances of
the use of graph-based methods for natural language
tasks. Most relevant to our work an approach to
word-sense disambiguation due to Niu et al (2005).
Their formulation was transductive, so that the test
data was part of the constructed graph, and they did
not consider predicate-argument analysis. In con-
trast, we make use of the smoothed graph during in-
ference in a probabilistic setting, in turn using it for
the full frame-semantic parsing task. Recently, Sub-
ramanya et al (2010) proposed the use of a graph
over substructures of an underlying sequence model,
and used a smoothed graph for domain adaptation of
part-of-speech taggers. Subramanya et al?s model
was extended by Das and Petrov (2011) to induce
part-of-speech dictionaries for unsupervised learn-
ing of taggers. Our semi-supervised learning setting
is similar to these two lines of work and, like them,
we use the graph to arrive at better final structures, in
an inductive setting (i.e., where a parametric model
is learned and then separately applied to test data,
following most NLP research).
3 Approach Overview
Our overall approach to handling unobserved targets
consists of four distinct stages. Before going into the
details of each stage individually, we provide their
overview here:
Graph Construction: A graph consisting of ver-
tices corresponding to targets is constructed us-
ing a combination of frame similarity (for ob-
served targets) and distributional similarity as
edge weights. This stage also determines a
fixed set of nearest neighbors for each vertex
in the graph.
Label Propagation: The observed targets (a small
subset of the vertices) are initialized with
empirical frame distributions extracted from
FrameNet annotations. Label propagation re-
sults in a distribution of frames for each vertex
in the graph.
Supervised Learning: Frame identification and ar-
gument identification models are trained fol-
lowing Das et al (2010a). The graph is used
to define the set of candidate frames for unseen
targets.
Parsing: The frame identification model of
Das et al disambiguated among only those
frames associated with a seen target in the
annotated data. For an unseen target, all frames
in the FrameNet lexicon were considered (a
large number). The current work replaces that
strategy, considering only the top M frames in
the distribution produced by label propagation.
This strategy results in large improvements
in frame identification for the unseen targets
and makes inference much faster. Argument
identification is done exactly like Das et al
(2010a).
4 Semi-Supervised Learning
We perform semi-supervised learning by construct-
ing a graph of vertices representing a large number
of targets, and learn frame distributions for those
which were not observed in FrameNet annotations.
4.1 Graph Construction
We construct a graph with targets as vertices. For
us, each target corresponds to a lemmatized word
or phrase appended with a coarse POS tag, and it
resembles the lexical units in the FrameNet lexicon.
For example, two targets corresponding to the same
lemma would look like boast.N and boast.V. Here,
the first target is a noun, while the second is a verb.
An example multiword target is chemical weapon.N.
We use two resources for graph construction.
First, we take all the words and phrases present in
the dependency-based thesaurus constructed using
syntactic cooccurrence statistics (Lin, 1998).3 To
construct this resource, a corpus containing 64 mil-
lion words was parsed with a fast dependency parser
(Lin, 1993; Lin, 1994), and syntactic contexts were
used to find similar lexical items for a given word
3This resource is available at http://webdocs.cs.
ualberta.ca/?lindek/Downloads/sim.tgz
1437
difference.N
similarity.N
discrepancy.N
resemble.V
disparity.N
resemblance.Ninequality.N
variant.N
divergence.N
poverty.N
homelessness.N
wealthy.Arich.A
deprivation.N
destitution.N
joblessness.N
unemployment.N employment.N
unemployment rate.N
powerlessness.N
UNEMPLOYMENT_RATE UNEMPLOYMENT_RATE
UNEMPLOYMENT_RATE
POVERTY POVERTY
POVERTY
SIMILARITY SIMILARITY
SIMILARITY
SIMILARITYSIMILARITY
Figure 2: Excerpt from a graph
over targets. Green targets are
observed in the FrameNet data.
Above/below them are shown the
most frequently observed frame
that these targets evoke. The black
targets are unobserved and label
propagation produces a distribution
over most likely frames that they
could evoke.
or phrase. Lin separately treated nouns, verbs and
adjectives/adverbs and the thesaurus contains three
parts for each of these categories. For each item in
the thesaurus, 200 nearest neighbors are listed with a
symmetric similarity score between 0 and 1. We pro-
cessed this thesaurus in two ways: first, we lower-
cased and lemmatized each word/phrase and merged
entries which shared the same lemma; second, we
separated the adjectives and adverbs into two lists
from Lin?s original list by scanning a POS-tagged
version of the Gigaword corpus (Graff, 2003) and
categorizing each item into an adjective or an ad-
verb depending on which category the item associ-
ated with more often in the data. The second step
was necessary because FrameNet treats adjectives
and adverbs separately. At the end of this processing
step, we were left with 61,702 units?approximately
six times more than the targets found in FrameNet
annotations?each labeled with one of 4 coarse tags.
We considered only the top 20 most similar targets
for each target, and noted Lin?s similarity between
two targets t and u, which we call simDL(t, u).
The second component of graph construction
comes from FrameNet itself. We scanned the exem-
plar sentences in FrameNet 1.54 and the training sec-
tion of the full-text annotations that we use to train
the probabilistic frame parser (see ?6.1), and gath-
ered a distribution over frames for each target. For
a pair of targets t and u, we measured the Euclidean
distance5 between their frame distributions. This
distance was next converted to a similarity score,
namely, simFN (t, u) between 0 and 1 by subtract-
ing each one from the maximum distance found in
4http://framenet.icsi.berkeley.edu
5This could have been replaced by an entropic distance metric
like KL- or JS-divergence, but we leave that exploration to fu-
ture work.
the whole data, followed by normalization. Like
simDL(t, u), this score is symmetric. This resulted
in 9,263 targets, and again for each, we considered
the 20 most similar targets. Finally, the overall sim-
ilarity between two given targets t and u was com-
puted as:
sim(t, u) = ? ? simFN (t, u) + (1??) ? simDL(t, u)
Note that this score is symmetric because its two
components are symmetric. The intuition behind
taking a linear combination of the two types of sim-
ilarity functions is as follows. We hope that distri-
butionally similar targets would have the same se-
mantic frames because ideally, lexical units evoking
the same set of frames appear in similar syntactic
contexts. We would also like to involve the anno-
tated data in graph construction so that it can elim-
inate some noise in the automatically constructed
thesaurus.6 Let K(t) denote the K most similar tar-
gets to target t, under the score sim. We link vertices
t and u in the graph with edge weight wtu, defined
as:
wtu =
{
sim(t, u) if t ? K(u) or u ? K(t)
0 otherwise
(1)
The hyperparameters ? and K are tuned by cross-
validation (?6.3).
4.2 Label Propagation
First, we softly label those vertices of the con-
structed graph for which frame distributions are
available from the FrameNet data (the same distri-
butions that are used to compute simFN ). Thus, ini-
tially, a small fraction of the vertices in the graph
6In future work, one might consider learning a similarity metric
from the annotated data, so as to exactly suit the frame identi-
fication task.
1438
have soft frame labels on them. Figure 2 shows an
excerpt from a constructed graph. For simplicity,
only the most probable frames under the empirical
distribution for the observed targets are shown; we
actually label each vertex with the full empirical dis-
tribution over frames for the corresponding observed
target in the data. The dotted lines demarcate parts
of the graph that associate with different frames. La-
bel propagation helps propagate the initial soft labels
throughout the graph. To this end, we use a vari-
ant of the quadratic cost criterion of Bengio et al
(2006), also used by Subramanya et al (2010) and
Das and Petrov (2011).7
Let V denote the set of all vertices in the graph,
Vl ? V be the set of known targets and F denote the
set of all frames. Let N (t) denote the set of neigh-
bors of vertex t ? V . Let q = {q1, q2, . . . , q|V |}
be the set of frame distributions, one per vertex. For
each known target t ? Vl, we have an initial frame
distribution rt. For every edge in the graph, weights
are defined as in Eq. 1. We find q by solving:
argminq
?
t?Vl
?rt ? qt?2
+ ?
?
t?V,u?N (t)wtu?qt ? qu?
2
+ ?
?
t?V ?qt ?
1
|F|?
2
s.t. ?t ? V,
?
f?F qt(f) = 1
?t ? V, f ? F , qt(f) ? 0
(2)
We use a squared loss to penalize various pairs of
distributions over frames: ?a?b?2 =
?
f?F (a(f)?
b(f))2. The first term in Eq. 2 requires that, for
known targets, we stay close to the initial frame dis-
tributions. The second term is the graph smooth-
ness regularizer, which encourages the distributions
of similar nodes (large wtu) to be similar. The fi-
nal term is a regularizer encouraging all distributions
to be uniform to the extent allowed by the first two
terms. (If an unlabeled vertex does not have a path
to any labeled vertex, this term ensures that its con-
verged marginal will be uniform over all frames.) ?
and ? are hyperparameters whose choice we discuss
in ?6.3.
Note that Eq. 2 is convex in q. While it is possible
to derive a closed form solution for this objective
7Instead of a quadratic cost, an entropic distance measure could
have been used, e.g., KL-divergence, considered by Subra-
manya and Bilmes (2009). We do not explore that direction
in the current paper.
function, it would require the inversion of a |V |?|V |
matrix. Hence, like Subramanya et al (2010), we
employ an iterative method with updates defined as:
?t(f) ? rt(f)1{t ? Vl} (3)
+ ?
?
u?N (t)
wtuq
(m?1)
u (f) +
?
|F|
?t ? 1{t ? Vl}+ ? + ?
?
u?N (t)
wtu (4)
q(m)t (f) ? ?t(f)/?t (5)
Here, 1{?} is an indicator function. The iterative
procedure starts with a uniform distribution for each
q(0)t . For all our experiments, we run 10 iterations
of the updates. The final distribution of frames for a
target t is denoted by q?t .
5 Learning and Inference for
Frame-Semantic Parsing
In this section, we briefly review learning and infer-
ence techniques used in the frame-semantic parser,
which are largely similar to Das et al (2010a), ex-
cept the handling of unknown targets. Note that in
all our experiments, we assume that the targets are
marked in a given sentence of which we want to ex-
tract a frame-semantic analysis. Therefore, unlike
the systems presented in SemEval?07, we do not de-
fine a target identification module.
5.1 Frame Identification
For a given sentence x with frame-evoking targets
t, let ti denote the ith target (a word sequence). We
seek a list f = ?f1, . . . , fm? of frames, one per tar-
get. LetL be the set of targets found in the FrameNet
annotations. Let Lf ? L be the subset of these tar-
gets annotated as evoking a particular frame f .
The set of candidate frames Fi for ti is defined to
include every frame f such that ti ? Lf . If ti 6? L
(in other words, ti is unseen), then Das et al (2010a)
considered all frames F in FrameNet as candidates.
Instead, in our work, we check whether ti ? V ,
where V are the vertices of the constructed graph,
and set:
Fi = {f : f ?M -best frames under q
?
ti} (6)
The integer M is set using cross-validation (?6.3).
If ti 6? V , then all frames F are considered as Fi.
1439
The frame prediction rule uses a probabilistic model
over frames for a target:
fi ? argmaxf?Fi
?
`?Lf
p(f, ` | ti,x) (7)
Note that a latent variable ` ? Lf is used, which
is marginalized out. Broadly, lexical semantic re-
lationships between the ?prototype? variable ` (be-
longing to the set of seen targets for a frame f ) and
the target ti are used as features for frame identifi-
cation, but since ` is unobserved, it is summed out
both during inference and training. A conditional
log-linear model is used to model this probability:
for f ? Fi and ` ? Lf , p?(f, ` | ti,x) =
exp?>g(f, `, ti,x)
?
f ??Fi
?
`??Lf ?
exp?>g(f ?, `?, ti,x)
(8)
where ? are the model weights, and g is a vector-
valued feature function. This discriminative formu-
lation is very flexible, allowing for a variety of (pos-
sibly overlapping) features; e.g., a feature might re-
late a frame f to a prototype `, represent a lexical-
semantic relationship between ` and ti, or encode
part of the syntax of the sentence (Das et al, 2010b).
Given some training data, which is of the form
?
?x(j), t(j), f (j),A(j)?
?N
j=1 (where N is the number
of sentences in the data and A is the set of argu-
ment in a sentence), we discriminatively train the
frame identification model by maximizing the fol-
lowing log-likelihood:8
max
?
N?
j=1
mj?
i=1
log
?
`?L
f
(j)
i
p?(f
(j)
i , ` | t
(j)
i ,x
(j)) (9)
This non-convex objective function is locally op-
timized using a distributed implementation of L-
BFGS (Liu and Nocedal, 1989).9
5.2 Argument Identification
Given a sentence x = ?x1, . . . , xn?, the set of tar-
gets t = ?t1, . . . , tm?, and a list of evoked frames
8We found no benefit from using an L2 regularizer.
9While training, in the partition function of the log-linear
model, all frames F in FrameNet are summed up for a target ti
instead of only Fi (as in Eq. 8), to learn interactions between
the latent variables and different sentential contexts.
f = ?f1, . . . , fm? corresponding to each target, ar-
gument identification or SRL is the task of choos-
ing which of each fi?s roles are filled, and by which
parts of x. We directly adopt the model of Das et
al. (2010a) for the argument identification stage and
briefly describe it here.
Let Rfi = {r1, . . . , r|Rfi |} denote frame fi?s
roles observed in FrameNet annotations. A set S of
spans that are candidates for filling any role r ? Rfi
are identified in the sentence. In principle, S could
contain any subsequence of x, but we consider only
the set of contiguous spans that (a) contain a sin-
gle word or (b) comprise a valid subtree of a word
and all its descendants in a dependency parse. The
empty span is also included in S, since some roles
are not explicitly filled. During training, if an argu-
ment is not a valid subtree of the dependency parse
(this happens due to parse errors), we add its span
to S. Let Ai denote the mapping of roles in Rfi to
spans in S. The model makes a prediction for each
Ai(rk) (for all roles rk ? Rfi):
Ai(rk)? argmaxs?S p(s | rk, fi, ti,x) (10)
A conditional log-linear model over spans for each
role of each evoked frame is defined as:
p?(Ai(rk) = s | fi, ti,x) = (11)
exp?>h(s, rk, fi, ti,x)
?
s??S exp?
>h(s?, rk, fi, ti,x)
This model is trained by optimizing:
max
?
N?
j=1
mj?
i=1
|R
f
(j)
i
|
?
k=1
log p?(A
(j)
i (rk) | f
(j)
i , t
(j)
i ,x
(j))
This objective function is convex, and we globally
optimize it using the distributed implementation of
L-BFGS. We regularize by including ? 110???
2
2 in
the objective (the strength is not tuned). Na??ve pre-
diction of roles using Equation 10 may result in
overlap among arguments filling different roles of a
frame, since the argument identification model fills
each role independently of the others. We want
to enforce the constraint that two roles of a sin-
gle frame cannot be filled by overlapping spans.
Hence, illegal overlap is disallowed using a 10,000-
hypothesis beam search.
1440
UNKNOWN TARGETS ALL TARGETS
Model
Exact
Match
Partial
Match
Exact
Match
Partial
Match
SEMAFOR 23.08 46.62 82.97 90.51
Self-training 18.88 42.67 82.45 90.19
LinGraph 36.36 59.47 83.40 90.93
FullGraph 39.86 62.35? 83.51 91.02?
Table 1: Frame identification results in percentage accu-
racy on 4,458 test targets. Bold scores indicate significant
improvements relative to SEMAFOR and (?) denotes sig-
nificant improvements over LinGraph (p < 0.05).
6 Experiments and Results
Before presenting our experiments and results, we
will describe the datasets used in our experiments,
and the various baseline models considered.
6.1 Data
We make use of the FrameNet 1.5 lexicon released
in 2010. This lexicon is a superset of previous ver-
sions of FrameNet. It contains 154,607 exemplar
sentences with one marked target and frame-role an-
notations. 78 documents with full-text annotations
with multiple frames per sentence were also released
(a superset of the SemEval?07 dataset). We ran-
domly selected 55 of these documents for training
and treated the 23 remaining ones as our test set.
After scanning the exemplar sentences and the train-
ing data, we arrived at a set of 877 frames, 1,068
roles,10 and 9,263 targets. Our training split of
the full-text annotations contained 3,256 sentences
with 19,582 frame annotatations with correspond-
ing roles, while the test set contained 2,420 sen-
tences with 4,458 annotations (the test set contained
fewer annotated targets per sentence). We also di-
vide the 55 training documents into 5 parts for cross-
validation (see ?6.3). The raw sentences in all the
training and test documents were preprocessed us-
ing MXPOST (Ratnaparkhi, 1996) and the MST de-
pendency parser (McDonald et al, 2005) following
Das et al (2010a). In this work we assume the
frame-evoking targets have been correctly identified
in training and test data.
10Note that the number of listed roles in the lexicon is nearly
9,000, but their number in actual annotations is a lot fewer.
6.2 Baselines
We compare our model with three baselines. The
first baseline is the purely supervised model of Das
et al (2010a) trained on the training split of 55
documents. Note that this is the strongest baseline
available for this task;11 we refer to this model as
?SEMAFOR.?
The second baseline is a semi-supervised self-
trained system, where we used SEMAFOR to label
70,000 sentences from the Gigaword corpus with
frame-semantic parses. For finding targets in a raw
sentence, we used a relaxed target identification
scheme, where we marked every target seen in the
lexicon and all other words which were not prepo-
sitions, particles, proper nouns, foreign words and
Wh-words as potential frame evoking units. This
was done so as to find unseen targets and get frame
annotations with SEMAFOR on them. We appended
these automatic annotations to the training data, re-
sulting in 711,401 frame annotations, more than 36
times the supervised data. These data were next used
to train a frame identification model (?5.1).12 This
setup is very similar to Bejan (2009) who used self-
training to improve frame identification. We refer to
this model as ?Self-training.?
The third baseline uses a graph constructed only
with Lin?s thesaurus, without using supervised data.
In other words, we followed the same scheme as in
?4.1 but with the hyperparameter ? = 0. Next, la-
bel propagation was run on this graph (and hyper-
parameters tuned using cross validation). The poste-
rior distribution of frames over targets was next used
for frame identification (Eq. 6-7), with SEMAFOR
as the trained model. This model, which is very sim-
ilar to our full model, is referred to as ?LinGraph.?
?FullGraph? refers to our full system.
6.3 Experimental Setup
We used five-fold cross-validation to tune the hy-
perparameters ?, K, ?, and M in our model. The
11We do not compare our model with other systems, e.g. the
ones submitted to SemEval?07 shared task, because SE-
MAFOR outperforms them significantly (Das et al, 2010a)
on the previous version of the data. Moreover, we trained our
models on the new FrameNet 1.5 data, and training code for
the SemEval?07 systems was not readily available.
12Note that we only self-train the frame identification model and
not the argument identification model, which is fixed through-
out.
1441
UNKNOWN TARGETS ALL TARGETS
Model
Exact Match Partial Match Exact Match Partial Match
P R F1 P R F1 P R F1 P R F1
SEMAFOR 19.59 16.48 17.90 33.03 27.80 30.19 66.15 61.64 63.82 70.68 65.86 68.18
Self-training 15.44 13.00 14.11 29.08 24.47 26.58 65.78 61.30 63.46 70.39 65.59 67.90
LinGraph 29.74 24.88 27.09 44.08 36.88 40.16 66.43 61.89 64.08 70.97 66.13 68.46
FullGraph 35.27? 28.84? 31.74? 48.81? 39.91? 43.92? 66.59? 62.01? 64.22? 71.11? 66.22? 68.58?
Table 2: Full frame-semantic parsing precision, recall and F1 score on 2,420 test sentences. Bold scores indicate
significant improvements relative to SEMAFOR and (?) denotes significant improvements over LinGraph (p < 0.05).
uniform regularization hyperparameter ? for graph
construction was set to 10?6 and not tuned. For
each cross-validation split, four folds were used to
train a frame identification model, construct a graph,
run label propagation and then the model was tested
on the fifth fold. This was done for all hyperpa-
rameter settings, which were ? ? {0.2, 0.5, 0.8},
K ? {5, 10, 15, 20}, ? ? {0.01, 0.1, 0.3, 0.5, 1.0}
and M ? {2, 3, 5, 10}. The joint setting which per-
formed the best across five-folds was ? = 0.2,K =
10, ? = 1.0,M = 2. Similar tuning was also done
for the baseline LinGraph, where ? was set to 0,
and rest of the hyperparameters were tuned (the se-
lected hyperparameters were K = 10, ? = 0.1 and
M = 2). With the chosen set of hyperparameters,
the test set was used to measure final performance.
The standard evaluation script from the Se-
mEval?07 task calculates precision, recall, and F1-
score for frames and arguments; it also provides a
score that gives partial credit for hypothesizing a
frame related to the correct one in the FrameNet lex-
icon. We present precision, recall, and F1-measure
microaveraged across the test documents, report
labels-only matching scores (spans must match ex-
actly), and do not use named entity labels. This eval-
uation scheme follows Das et al (2010a). Statistical
significance is measured using a reimplementation
of Dan Bikel?s parsing evaluation comparator.13
6.4 Results
Tables 1 and 2 present results for frame identifica-
tion and full frame-semantic parsing respectively.
They also separately tabulate the results achieved
for unknown targets. Our full model, denoted by
?FullGraph,? outperforms all the baselines for both
tasks. Note that the Self-training model even falls
13http://www.cis.upenn.edu/?dbikel/
software.html#comparator
short of the supervised baseline SEMAFOR, unlike
what was observed by Bejan (2009) for the frame
identification task. The model using a graph con-
structed solely from the thesaurus (LinGraph) out-
performs both the supervised and the self-training
baselines for all tasks, but falls short of the graph
constructed using the similarity metric that is a lin-
ear combination of distributional similarity and su-
pervised frame similarity. This indicates that a graph
constructed with some knowledge of the supervised
data is more powerful.
For unknown targets, the gains of our approach
are impressive: 15.7% absolute accuracy improve-
ment over SEMAFOR for frame identification, and
13.7% absolute F1 improvement over SEMAFOR
for full frame-semantic parsing (both significant).
When all the test targets are considered, the gains
are still significant, resulting in 5.4% relative error
reduction over SEMAFOR for frame identification,
and 1.3% relative error reduction over SEMAFOR
for full-frame semantic parsing.
Although these improvements may seem modest,
this is because only 3.2% of the test set targets are
unseen in training. We expect that further gains
would be realized in different text domains, where
FrameNet coverage is presumably weaker than in
news data. A semi-supervised strategy like ours is
attractive in such a setting, and future work might
explore such an application.
Our approach also makes decoding much faster.
For the unknown component of the test set, SE-
MAFOR takes a total 111 seconds to find the best
set of frames, while the FullGraph model takes only
19 seconds to do so, thus bringing disambiguation
time down by a factor of nearly 6. This is be-
cause our model now disambiguates between only
M = 2 frames instead of the full set of 877 frames
in FrameNet. For the full test set too, the speedup
1442
t = discrepancy.N t = contribution.N t = print.V t = mislead.V
f q?t (f) f q
?
t (f) f q
?
t (f) f q
?
t (f)
?SIMILARITY 0.076 ?GIVING 0.167 ?TEXT CREATION 0.081 EXPERIENCER OBJ 0.152
NATURAL FEATURES 0.066 MONEY 0.046 SENDING 0.054 ?PREVARICATION 0.130
PREVARICATION 0.012 COMMITMENT 0.046 DISPERSAL 0.054 MANIPULATE INTO DOING 0.046
QUARRELING 0.007 ASSISTANCE 0.040 READING 0.042 COMPLIANCE 0.041
DUPLICATION 0.007 EARNINGS AND LOSSES 0.024 STATEMENT 0.028 EVIDENCE 0.038
Table 3: Top 5 frames according to the graph posterior distribution q?t (f) for four targets: discrepancy.N, contri-
bution.N, print.V and mislead.V. None of these targets were present in the supervised FrameNet data. ? marks the
correct frame, according to the test data. EXPERIENCER OBJ is described in FrameNet as ?Some phenomenon (the
Stimulus) provokes a particular emotion in an Experiencer.?
is noticeable, as SEMAFOR takes 131 seconds for
frame identification, while the FullGraph model only
takes 39 seconds.
6.5 Discussion
The following is an example from our test set show-
ing SEMAFOR?s output (for one target):
REASON
Discrepancies
discrepancy.N
between North Korean de-
clarations and IAEA inspection findingsAction
indicate that North Korea might have re-
processed enough plutonium for one or
two nuclear weapons.
Note that the model identifies an incorrect frame
REASON for the target discrepancy.N, in turn identi-
fying the wrong semantic role Action for the under-
lined argument. On the other hand, the FullGraph
model exactly identifies the right semantic frame,
SIMILARITY, as well as the correct role, Entities. This
improvement can be easily explained. The excerpt
from our constructed graph in Figure 2 shows the
same target discrepancy.N in black, conveying that
it did not belong to the supervised data. However,
it is connected to the target difference.N drawn from
annotated data, which evokes the frame SIMILARITY.
Thus, after label propagation, we expect the frame
SIMILARITY to receive high probability for the target
discrepancy.N.
Table 3 shows the top 5 frames that are assigned
the highest posterior probabilities in the distribu-
tion q?t for four hand-selected test targets absent in
supervised data, including discrepancy.N. For all
of them, the FullGraph model identifies the correct
frames for all four words in the test data by rank-
ing these frames in the top M = 2. LinGraph
also gets all four correct, Self-training only gets
print.V/TEXT CREATION, and SEMAFOR gets none.
Across unknown targets, on average the M = 2
most common frames in the posterior distribution
q?t found by FullGraph have q
(?)
t (f) =
7
877 , or
seven times the average across all frames. This sug-
gests that the graph propagation method is confi-
dent only in predicting the top few frames out of
the whole possible set. Moreover, the automatically
selected number of frames to extract per unknown
target, M = 2, suggests that only a few meaningful
frames were assigned to unknown predicates. This
matches the nature of FrameNet data, where the av-
erage frame ambiguity for a target type is 1.20.
7 Conclusion
We have presented a semi-supervised strategy to
improve the coverage of a frame-semantic pars-
ing model. We showed that graph-based label
propagation and resulting smoothed frame distri-
butions over unseen targets significantly improved
the coverage of a state-of-the-art semantic frame
disambiguation model to previously unseen pred-
icates, also improving the quality of full frame-
semantic parses. The improved parser is available at
http://www.ark.cs.cmu.edu/SEMAFOR.
Acknowledgments
We are grateful to Amarnag Subramanya for helpful dis-
cussions. We also thank Slav Petrov, Nathan Schneider,
and the three anonymous reviewers for valuable com-
ments. This research was supported by NSF grants IIS-
0844507, IIS-0915187 and TeraGrid resources provided
by the Pittsburgh Supercomputing Center under NSF
grant number TG-DBS110003.
1443
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: frame semantic structure extraction. In
Proc. of SemEval.
C. A. Bejan. 2009. Learning Event Structures From Text.
Ph.D. thesis, The University of Texas at Dallas.
Y. Bengio, O. Delalleau, and N. Le Roux. 2006. La-
bel propagation and quadratic criterion. In Semi-
Supervised Learning. MIT Press.
H. C. Boas. 2002. Bilingual FrameNet dictionaries for
machine translation. In Proc. of LREC.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proc. of ACL-HLT.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010a.
Probabilistic frame-semantic parsing. In Proc. of
NAACL-HLT.
D. Das, N. Schneider, D. Chen, and N. A. Smith.
2010b. SEMAFOR 1.0: A probabilistic frame-
semantic parser. Technical Report CMU-LTI-10-001,
Carnegie Mellon University.
K. Erk and S. Pado?. 2006. Shalmaneser - a toolchain for
shallow semantic parsing. In Proc. of LREC.
C. Fellbaum, editor. 1998. WordNet: an electronic lexi-
cal database. MIT Press, Cambridge, MA.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in
the Morning Calm, pages 111?137. Hanshin Publish-
ing Co., Seoul, South Korea.
M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum
entropy models for FrameNet classification. In Proc.
of EMNLP.
P. Fung and B. Chen. 2004. BiFrameNet: bilin-
gual frame semantics resource construction by cross-
lingual induction. In Proc. of COLING.
H. Fu?rstenau and M. Lapata. 2009. Semi-supervised se-
mantic role labeling. In Proc. of EACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
A.-M. Giuglea and A. Moschitti. 2006. Shallow se-
mantic parsing based on FrameNet, VerbNet and Prop-
Bank. In Proc. of ECAI 2006.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium.
R. Johansson and P. Nugues. 2007. LTH: semantic struc-
ture extraction using nonprojective dependency trees.
In Proc. of SemEval.
D. Lin. 1993. Principle-based parsing without overgen-
eration. In Proc. of ACL.
D. Lin. 1994. Principar?an efficient, broadcoverage,
principle-based parser. In Proc. of COLING.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING-ACL.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Programming, 45(3).
Y. Matsubayashi, N. Okazaki, and J. Tsujii. 2009. A
comparative study on generalization of semantic roles
in FrameNet. In Proc. of ACL-IJCNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
Z.-Y. Niu, D.-H. Ji, and C. L. Tan. 2005. Word sense
disambiguation using label propagation based semi-
supervised learning. In Proc. of ACL.
S. Pado? and M. Lapata. 2005. Cross-linguistic projec-
tion of role-semantic information. In Proc. of HLT-
EMNLP.
M. Pennacchiotti, D. De Cao, R. Basili, D. Croce, and
M. Roth. 2008. Automatic induction of FrameNet
lexical units. In Proc. of EMNLP.
S. P. Ponzetto and M. Strube. 2007. Deriving a large
scale taxonomy from wikipedia. In Proc. of AAAI.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
L. Shi and R. Mihalcea. 2004. An algorithm for open
text semantic parsing. In Proc. of Workshop on Robust
Methods in Analysis of Natural Language Data.
L. Shi and R. Mihalcea. 2005. Putting pieces together:
combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Computational Linguis-
tics and Intelligent Text Processing: Proc. of CICLing
2005. Springer-Verlag.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
Proc. of COLING-ACL.
A. Subramanya and J. A. Bilmes. 2009. Entropic graph
regularization in non-parametric semi-supervised clas-
sification. In Proc. of NIPS.
A. Subramanya, S. Petrov, and F. Pereira. 2010. Efficient
Graph-based Semi-Supervised Learning of Structured
Tagging Models. In Proc. of EMNLP.
C. A. Thompson, R. Levy, and C. D. Manning. 2003. A
generative model for semantic role labeling. In Proc.
of ECML.
S. Tonelli and C. Giuliano. 2009. Wikipedia as frame
information repository. In Proc. of EMNLP.
X. Zhu, Z. Ghahramani, and J. D. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of ICML.
1444
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 42?47,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills,
Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith
School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA
{kgimpel,nschneid,brenocon,dipanjan,dpmills,
jacobeis,mheilman,dyogatama,jflanigan,nasmith}@cs.cmu.edu
Abstract
We address the problem of part-of-speech tag-
ging for English data from the popular micro-
blogging service Twitter. We develop a tagset,
annotate data, develop features, and report
tagging results nearing 90% accuracy. The
data and tools have been made available to the
research community with the goal of enabling
richer text analysis of Twitter and related so-
cial media data sets.
1 Introduction
The growing popularity of social media and user-
created web content is producing enormous quanti-
ties of text in electronic form. The popular micro-
blogging service Twitter (twitter.com) is one
particularly fruitful source of user-created content,
and a flurry of recent research has aimed to under-
stand and exploit these data (Ritter et al, 2010; Shar-
ifi et al, 2010; Barbosa and Feng, 2010; Asur and
Huberman, 2010; O?Connor et al, 2010a; Thelwall
et al, 2011). However, the bulk of this work eschews
the standard pipeline of tools which might enable
a richer linguistic analysis; such tools are typically
trained on newstext and have been shown to perform
poorly on Twitter (Finin et al, 2010).
One of the most fundamental parts of the linguis-
tic pipeline is part-of-speech (POS) tagging, a basic
form of syntactic analysis which has countless appli-
cations in NLP. Most POS taggers are trained from
treebanks in the newswire domain, such as the Wall
Street Journal corpus of the Penn Treebank (PTB;
Marcus et al, 1993). Tagging performance degrades
on out-of-domain data, and Twitter poses additional
challenges due to the conversational nature of the
text, the lack of conventional orthography, and 140-
character limit of each message (?tweet?). Figure 1
shows three tweets which illustrate these challenges.
(a) @Gunservatively@ obozo? willV goV nutsA
whenR PA? electsV aD RepublicanA GovernorN
nextP Tue? ., CanV youO sayV redistrictingV ?,
(b) SpendingV theD dayN withhhP mommmaN !,
(c) lmao! ..., s/oV toP theD coolA assN asianA
officerN 4P #1$ notR runninV myD licenseN and&
#2$ notR takinV druN booN toP jailN ., ThankV
uO God? ., #amen#
Figure 1: Example tweets with gold annotations. Under-
lined tokens show tagger improvements due to features
detailed in Section 3 (respectively: TAGDICT, METAPH,
and DISTSIM).
In this paper, we produce an English POS tagger
that is designed especially for Twitter data. Our con-
tributions are as follows:
? we developed a POS tagset for Twitter,
? we manually tagged 1,827 tweets,
? we developed features for Twitter POS tagging
and conducted experiments to evaluate them, and
? we provide our annotated corpus and trained POS
tagger to the research community.
Beyond these specific contributions, we see this
work as a case study in how to rapidly engi-
neer a core NLP system for a new and idiosyn-
cratic dataset. This project was accomplished in
200 person-hours spread across 17 people and two
months. This was made possible by two things:
(1) an annotation scheme that fits the unique char-
acteristics of our data and provides an appropriate
level of linguistic detail, and (2) a feature set that
captures Twitter-specific properties and exploits ex-
isting resources such as tag dictionaries and phonetic
normalization. The success of this approach demon-
strates that with careful design, supervised machine
learning can be applied to rapidly produce effective
language technology in new domains.
42
Tag Description Examples %
Nominal, Nominal + Verbal
N common noun (NN, NNS) books someone 13.7
O pronoun (personal/WH; not
possessive; PRP, WP)
it you u meeee 6.8
S nominal + possessive books? someone?s 0.1
? proper noun (NNP, NNPS) lebron usa iPad 6.4
Z proper noun + possessive America?s 0.2
L nominal + verbal he?s book?ll iono
(= I don?t know)
1.6
M proper noun + verbal Mark?ll 0.0
Other open-class words
V verb incl. copula,
auxiliaries (V*, MD)
might gonna
ought couldn?t is
eats
15.1
A adjective (J*) good fav lil 5.1
R adverb (R*, WRB) 2 (i.e., too) 4.6
! interjection (UH) lol haha FTW yea
right
2.6
Other closed-class words
D determiner (WDT, DT,
WP$, PRP$)
the teh its it?s 6.5
P pre- or postposition, or
subordinating conjunction
(IN, TO)
while to for 2 (i.e.,
to) 4 (i.e., for)
8.7
& coordinating conjunction
(CC)
and n & + BUT 1.7
T verb particle (RP) out off Up UP 0.6
X existential there,
predeterminers (EX, PDT)
both 0.1
Y X + verbal there?s all?s 0.0
Twitter/online-specific
# hashtag (indicates
topic/category for tweet)
#acl 1.0
@ at-mention (indicates
another user as a recipient
of a tweet)
@BarackObama 4.9
~ discourse marker,
indications of continuation
of a message across
multiple tweets
RT and : in retweet
construction RT
@user : hello
3.4
U URL or email address http://bit.ly/xyz 1.6
E emoticon :-) :b (: <3 o O 1.0
Miscellaneous
$ numeral (CD) 2010 four 9:30 1.5
, punctuation (#, $, '', (,
), ,, ., :, ``)
!!! .... ?!? 11.6
G other abbreviations, foreign
words, possessive endings,
symbols, garbage (FW,
POS, SYM, LS)
ily (I love you) wby
(what about you) ?s
 -->
awesome...I?m
1.1
Table 1: The set of tags used to annotate tweets. The
last column indicates each tag?s relative frequency in the
full annotated data (26,435 tokens). (The rates for M and
Y are both < 0.0005.)
2 Annotation
Annotation proceeded in three stages. For Stage 0,
we developed a set of 20 coarse-grained tags based
on several treebanks but with some additional cate-
gories specific to Twitter, including URLs and hash-
tags. Next, we obtained a random sample of mostly
American English1 tweets from October 27, 2010,
automatically tokenized them using a Twitter tok-
enizer (O?Connor et al, 2010b),2 and pre-tagged
them using the WSJ-trained Stanford POS Tagger
(Toutanova et al, 2003) in order to speed up man-
ual annotation. Heuristics were used to mark tokens
belonging to special Twitter categories, which took
precedence over the Stanford tags.
Stage 1 was a round of manual annotation: 17 re-
searchers corrected the automatic predictions from
Stage 0 via a custom Web interface. A total of
2,217 tweets were distributed to the annotators in
this stage; 390 were identified as non-English and
removed, leaving 1,827 annotated tweets (26,436 to-
kens).
The annotation process uncovered several situa-
tions for which our tagset, annotation guidelines,
and tokenization rules were deficient or ambiguous.
Based on these considerations we revised the tok-
enization and tagging guidelines, and for Stage 2,
two annotators reviewed and corrected all of the
English tweets tagged in Stage 1. A third anno-
tator read the annotation guidelines and annotated
72 tweets from scratch, for purposes of estimating
inter-annotator agreement. The 72 tweets comprised
1,021 tagged tokens, of which 80 differed from the
Stage 2 annotations, resulting in an agreement rate
of 92.2% and Cohen?s ? value of 0.914. A final
sweep was made by a single annotator to correct er-
rors and improve consistency of tagging decisions
across the corpus. The released data and tools use
the output of this final stage.
2.1 Tagset
We set out to develop a POS inventory for Twitter
that would be intuitive and informative?while at
the same time simple to learn and apply?so as to
maximize tagging consistency within and across an-
1We filtered to tweets sent via an English-localized user in-
terface set to a United States timezone.
2http://github.com/brendano/tweetmotif
43
notators. Thus, we sought to design a coarse tagset
that would capture standard parts of speech3 (noun,
verb, etc.) as well as categories for token varieties
seen mainly in social media: URLs and email ad-
dresses; emoticons; Twitter hashtags, of the form
#tagname, which the author may supply to catego-
rize a tweet; and Twitter at-mentions, of the form
@user, which link to other Twitter users from within
a tweet.
Hashtags and at-mentions can also serve as words
or phrases within a tweet; e.g. Is #qadaffi going down?.
When used in this way, we tag hashtags with their
appropriate part of speech, i.e., as if they did not start
with #. Of the 418 hashtags in our data, 148 (35%)
were given a tag other than #: 14% are proper nouns,
9% are common nouns, 5% are multi-word express-
sions (tagged as G), 3% are verbs, and 4% are some-
thing else. We do not apply this procedure to at-
mentions, as they are nearly always proper nouns.
Another tag, ~, is used for tokens marking spe-
cific Twitter discourse functions. The most popular
of these is the RT (?retweet?) construction to publish
a message with attribution. For example,
RT @USER1 : LMBO ! This man filed an
EMERGENCY Motion for Continuance on
account of the Rangers game tonight ! 
Wow lmao
indicates that the user @USER1 was originally the
source of the message following the colon. We ap-
ply ~ to the RT and : (which are standard), and
also, which separates the author?s comment from
the retweeted material.4 Another common discourse
marker is ellipsis dots (. . . ) at the end of a tweet,
indicating a message has been truncated to fit the
140-character limit, and will be continued in a sub-
sequent tweet or at a specified URL.
Our first round of annotation revealed that, due to
nonstandard spelling conventions, tokenizing under
a traditional scheme would be much more difficult
3Our starting point was the cross-lingual tagset presented by
Petrov et al (2011). Most of our tags are refinements of those
categories, which in turn are groupings of PTB WSJ tags (see
column 2 of Table 1). When faced with difficult tagging deci-
sions, we consulted the PTB and tried to emulate its conventions
as much as possible.
4These ?iconic deictics? have been studied in other online
communities as well (Collister, 2010).
than for Standard English text. For example, apos-
trophes are often omitted, and there are frequently
words like ima (short for I?m gonna) that cut across
traditional POS categories. Therefore, we opted not
to split contractions or possessives, as is common
in English corpus preprocessing; rather, we intro-
duced four new tags for combined forms: {nominal,
proper noun} ? {verb, possessive}.5
The final tagging scheme (Table 1) encompasses
25 tags. For simplicity, each tag is denoted with a
single ASCII character. The miscellaneous category
G includes multiword abbreviations that do not fit
in any of the other categories, like ily (I love you), as
well as partial words, artifacts of tokenization errors,
miscellaneous symbols, possessive endings,6 and ar-
rows that are not used as discourse markers.
Figure 2 shows where tags in our data tend to oc-
cur relative to the middle word of the tweet. We
see that Twitter-specific tags have strong positional
preferences: at-mentions (@) and Twitter discourse
markers (~) tend to occur towards the beginning of
messages, whereas URLs (U), emoticons (E), and
categorizing hashtags (#) tend to occur near the end.
3 System
Our tagger is a conditional random field (CRF; Laf-
ferty et al, 2001), enabling the incorporation of ar-
bitrary local features in a log-linear model. Our
base features include: a feature for each word type,
a set of features that check whether the word con-
tains digits or hyphens, suffix features up to length 3,
and features looking at capitalization patterns in the
word. We then added features that leverage domain-
specific properties of our data, unlabeled in-domain
data, and external linguistic resources.
TWORTH: Twitter orthography. We have features
for several regular expression-style rules that detect
at-mentions, hashtags, and URLs.
NAMES: Frequently-capitalized tokens. Micro-
bloggers are inconsistent in their use of capitaliza-
tion, so we compiled gazetteers of tokens which are
frequently capitalized. The likelihood of capital-
ization for a token is computed as Ncap+?CN+C , where
5The modified tokenizer is packaged with our tagger.
6Possessive endings only appear when a user or the tok-
enizer has separated the possessive ending from a possessor; the
tokenizer only does this when the possessor is an at-mention.
44
Figure 2: Average position, relative to the middle word in the tweet, of tokens labeled with each tag. Most tags fall
between ?1 and 1 on this scale; these are not shown.
N is the token count, Ncap is the capitalized to-
ken count, and ? and C are the prior probability
and its prior weight.7 We compute features for
membership in the top N items by this metric, for
N ? {1000, 2000, 3000, 5000, 10000, 20000}.
TAGDICT: Traditional tag dictionary. We add
features for all coarse-grained tags that each word
occurs with in the PTB8 (conjoined with their fre-
quency rank). Unlike previous work that uses tag
dictionaries as hard constraints, we use them as soft
constraints since we expect lexical coverage to be
poor and the Twitter dialect of English to vary sig-
nificantly from the PTB domains. This feature may
be seen as a form of type-level domain adaptation.
DISTSIM: Distributional similarity. When train-
ing data is limited, distributional features from un-
labeled text can improve performance (Schu?tze and
Pedersen, 1993). We used 1.9 million tokens from
134,000 unlabeled tweets to construct distributional
features from the successor and predecessor proba-
bilities for the 10,000 most common terms. The suc-
cessor and predecessor transition matrices are hori-
zontally concatenated into a sparse matrixM, which
we approximate using a truncated singular value de-
composition: M ? USVT, where U is limited to
50 columns. Each term?s feature vector is its row
in U; following Turian et al (2010), we standardize
and scale the standard deviation to 0.1.
METAPH: Phonetic normalization. Since Twitter
includes many alternate spellings of words, we used
the Metaphone algorithm (Philips, 1990)9 to create
a coarse phonetic normalization of words to simpler
keys. Metaphone consists of 19 rules that rewrite
consonants and delete vowels. For example, in our
7? = 1100 , C = 10; this score is equivalent to the posterior
probability of capitalization with a Beta(0.1, 9.9) prior.
8Both WSJ and Brown corpora, no case normalization. We
also tried adding the WordNet (Fellbaum, 1998) and Moby
(Ward, 1996) lexicons, which increased lexical coverage but did
not seem to help performance.
9Via the Apache Commons implementation: http://
commons.apache.org/codec/
data, {thangs thanks thanksss thanx thinks thnx}
are mapped to 0NKS, and {lmao lmaoo lmaooooo}
map to LM. But it is often too coarse; e.g. {war we?re
wear were where worry} map to WR.
We include two types of features. First, we use
the Metaphone key for the current token, comple-
menting the base model?s word features. Second,
we use a feature indicating whether a tag is the most
frequent tag for PTB words having the same Meta-
phone key as the current token. (The second feature
was disabled in both ?TAGDICT and ?METAPH ab-
lation experiments.)
4 Experiments
Our evaluation was designed to test the efficacy of
this feature set for part-of-speech tagging given lim-
ited training data. We randomly divided the set of
1,827 annotated tweets into a training set of 1,000
(14,542 tokens), a development set of 327 (4,770 to-
kens), and a test set of 500 (7,124 tokens). We com-
pare our system against the Stanford tagger. Due
to the different tagsets, we could not apply the pre-
trained Stanford tagger to our data. Instead, we re-
trained it on our labeled data, using a standard set
of features: words within a 5-word window, word
shapes in a 3-word window, and up to length-3
prefixes, length-3 suffixes, and prefix/suffix pairs.10
The Stanford system was regularized using a Gaus-
sian prior of ?2 = 0.5 and our system with a Gaus-
sian prior of ?2 = 5.0, tuned on development data.
The results are shown in Table 2. Our tagger with
the full feature set achieves a relative error reduction
of 25% compared to the Stanford tagger. We also
show feature ablation experiments, each of which
corresponds to removing one category of features
from the full set. In Figure 1, we show examples
that certain features help solve. Underlined tokens
10We used the following feature modules in the Stanford tag-
ger: bidirectional5words, naacl2003unknowns,
wordshapes(-3,3), prefix(3), suffix(3),
prefixsuffix(3).
45
Dev. Test
Our tagger, all features 88.67 89.37
independent ablations:
?DISTSIM 87.88 88.31 (?1.06)
?TAGDICT 88.28 88.31 (?1.06)
?TWORTH 87.51 88.37 (?1.00)
?METAPH 88.18 88.95 (?0.42)
?NAMES 88.66 89.39 (+0.02)
Our tagger, base features 82.72 83.38
Stanford tagger 85.56 85.85
Annotator agreement 92.2
Table 2: Tagging accuracies on development and test
data, including ablation experiments. Features are or-
dered by importance: test accuracy decrease due to ab-
lation (final column).
Tag Acc. Confused Tag Acc. Confused
V 91 N ! 82 N
N 85 ? L 93 V
, 98 ~ & 98 ?
P 95 R U 97 ,
? 71 N $ 89 P
D 95 ? # 89 ?
O 97 ? G 26 ,
A 79 N E 88 ,
R 83 A T 72 P
@ 99 V Z 45 ?
~ 91 ,
Table 3: Accuracy (recall) rates per class, in the test set
with the full model. (Omitting tags that occur less than
10 times in the test set.) For each gold category, the most
common confusion is shown.
are incorrect in a specific ablation, but are corrected
in the full system (i.e. when the feature is added).
The ?TAGDICT ablation gets elects, Governor,
and next wrong in tweet (a). These words appear
in the PTB tag dictionary with the correct tags, and
thus are fixed by that feature. In (b), withhh is ini-
tially misclassified an interjection (likely caused by
interjections with the same suffix, like ohhh), but is
corrected by METAPH, because it is normalized to the
same equivalence class as with. Finally, s/o in tweet
(c) means ?shoutout?, which appears only once in
the training data; adding DISTSIM causes it to be cor-
rectly identified as a verb.
Substantial challenges remain; for example, de-
spite the NAMES feature, the system struggles to
identify proper nouns with nonstandard capitaliza-
tion. This can be observed from Table 3, which
shows the recall of each tag type: the recall of proper
nouns (?) is only 71%. The system also struggles
with the miscellaneous category (G), which covers
many rare tokens, including obscure symbols and ar-
tifacts of tokenization errors. Nonetheless, we are
encouraged by the success of our system on the
whole, leveraging out-of-domain lexical resources
(TAGDICT), in-domain lexical resources (DISTSIM),
and sublexical analysis (METAPH).
Finally, we note that, even though 1,000 train-
ing examples may seem small, the test set accuracy
when training on only 500 tweets drops to 87.66%,
a decrease of only 1.7% absolute.
5 Conclusion
We have developed a part-of-speech tagger for Twit-
ter and have made our data and tools available to the
research community at http://www.ark.cs.
cmu.edu/TweetNLP. More generally, we be-
lieve that our approach can be applied to address
other linguistic analysis needs as they continue to
arise in the era of social media and its rapidly chang-
ing linguistic conventions. We also believe that the
annotated data can be useful for research into do-
main adaptation and semi-supervised learning.
Acknowledgments
We thank Desai Chen, Chris Dyer, Lori Levin, Behrang
Mohit, Bryan Routledge, Naomi Saphra, and Tae Yano
for assistance in annotating data. This research was sup-
ported in part by: the NSF through CAREER grant IIS-
1054319, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
ber W911NF-10-1-0533, Sandia National Laboratories
(fellowship to K. Gimpel), and the U. S. Department of
Education under IES grant R305B040063 (fellowship to
M. Heilman).
References
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proc. of WI-
IAT.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proc. of COLING.
Lauren Collister. 2010. Meaning variation of the iconic
deictics ? and <? in an online community. In New
Ways of Analyzing Variation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
46
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010. An-
notating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010b. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proc. of ICWSM (demo
track).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Lawrence Philips. 1990. Hanging on the Metaphone.
Computer Language, 7(12).
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In Proc.
of NAACL.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. of NAACL.
Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in Twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406?418.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
HLT-NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.
Grady Ward. 1996. Moby lexicon. http://icon.
shef.ac.uk/Moby.
47
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Universal Dependency Annotation for Multilingual Parsing
Ryan McDonald? Joakim Nivre?? Yvonne Quirmbach-Brundage? Yoav Goldberg??
Dipanjan Das? Kuzman Ganchev? Keith Hall? Slav Petrov? Hao Zhang?
Oscar Ta?ckstro?m?? Claudia Bedini? Nu?ria Bertomeu Castello?? Jungmee Lee?
Google, Inc.? Uppsala University? Appen-Butler-Hill? Bar-Ilan University?
Contact: ryanmcd@google.com
Abstract
We present a new collection of treebanks
with homogeneous syntactic dependency
annotation for six languages: German,
English, Swedish, Spanish, French and
Korean. To show the usefulness of such a
resource, we present a case study of cross-
lingual transfer parsing with more reliable
evaluation than has been possible before.
This ?universal? treebank is made freely
available in order to facilitate research on
multilingual dependency parsing.1
1 Introduction
In recent years, syntactic representations based
on head-modifier dependency relations between
words have attracted a lot of interest (Ku?bler et
al., 2009). Research in dependency parsing ? com-
putational methods to predict such representations
? has increased dramatically, due in large part to
the availability of dependency treebanks in a num-
ber of languages. In particular, the CoNLL shared
tasks on dependency parsing have provided over
twenty data sets in a standardized format (Buch-
holz and Marsi, 2006; Nivre et al, 2007).
While these data sets are standardized in terms
of their formal representation, they are still hetero-
geneous treebanks. That is to say, despite them
all being dependency treebanks, which annotate
each sentence with a dependency tree, they sub-
scribe to different annotation schemes. This can
include superficial differences, such as the renam-
ing of common relations, as well as true diver-
gences concerning the analysis of linguistic con-
structions. Common divergences are found in the
1Downloadable at https://code.google.com/p/uni-dep-tb/.
analysis of coordination, verb groups, subordinate
clauses, and multi-word expressions (Nilsson et
al., 2007; Ku?bler et al, 2009; Zeman et al, 2012).
These data sets can be sufficient if one?s goal
is to build monolingual parsers and evaluate their
quality without reference to other languages, as
in the original CoNLL shared tasks, but there are
many cases where heterogenous treebanks are less
than adequate. First, a homogeneous represen-
tation is critical for multilingual language tech-
nologies that require consistent cross-lingual anal-
ysis for downstream components. Second, consis-
tent syntactic representations are desirable in the
evaluation of unsupervised (Klein and Manning,
2004) or cross-lingual syntactic parsers (Hwa et
al., 2005). In the cross-lingual study of McDonald
et al (2011), where delexicalized parsing models
from a number of source languages were evalu-
ated on a set of target languages, it was observed
that the best target language was frequently not the
closest typologically to the source. In one stun-
ning example, Danish was the worst source lan-
guage when parsing Swedish, solely due to greatly
divergent annotation schemes.
In order to overcome these difficulties, some
cross-lingual studies have resorted to heuristics to
homogenize treebanks (Hwa et al, 2005; Smith
and Eisner, 2009; Ganchev et al, 2009), but we
are only aware of a few systematic attempts to
create homogenous syntactic dependency anno-
tation in multiple languages. In terms of auto-
matic construction, Zeman et al (2012) attempt
to harmonize a large number of dependency tree-
banks by mapping their annotation to a version of
the Prague Dependency Treebank scheme (Hajic?
et al, 2001; Bo?hmova? et al, 2003). Addition-
ally, there have been efforts to manually or semi-
manually construct resources with common syn-
92
tactic analyses across multiple languages using al-
ternate syntactic theories as the basis for the repre-
sentation (Butt et al, 2002; Helmreich et al, 2004;
Hovy et al, 2006; Erjavec, 2012).
In order to facilitate research on multilingual
syntactic analysis, we present a collection of data
sets with uniformly analyzed sentences for six lan-
guages: German, English, French, Korean, Span-
ish and Swedish. This resource is freely avail-
able and we plan to extend it to include more data
and languages. In the context of part-of-speech
tagging, universal representations, such as that of
Petrov et al (2012), have already spurred numer-
ous examples of improved empirical cross-lingual
systems (Zhang et al, 2012; Gelling et al, 2012;
Ta?ckstro?m et al, 2013). We aim to do the same for
syntactic dependencies and present cross-lingual
parsing experiments to highlight some of the bene-
fits of cross-lingually consistent annotation. First,
results largely conform to our expectations of
which target languages should be useful for which
source languages, unlike in the study of McDon-
ald et al (2011). Second, the evaluation scores
in general are significantly higher than previous
cross-lingual studies, suggesting that most of these
studies underestimate true accuracy. Finally, un-
like all previous cross-lingual studies, we can re-
port full labeled accuracies and not just unlabeled
structural accuracies.
2 Towards A Universal Treebank
The Stanford typed dependencies for English
(De Marneffe et al, 2006; de Marneffe and Man-
ning, 2008) serve as the point of departure for our
?universal? dependency representation, together
with the tag set of Petrov et al (2012) as the under-
lying part-of-speech representation. The Stanford
scheme, partly inspired by the LFG framework,
has emerged as a de facto standard for depen-
dency annotation in English and has recently been
adapted to several languages representing different
(and typologically diverse) language groups, such
as Chinese (Sino-Tibetan) (Chang et al, 2009),
Finnish (Finno-Ugric) (Haverinen et al, 2010),
Persian (Indo-Iranian) (Seraji et al, 2012), and
Modern Hebrew (Semitic) (Tsarfaty, 2013). Its
widespread use and proven adaptability makes it a
natural choice for our endeavor, even though ad-
ditional modifications will be needed to capture
the full variety of grammatical structures in the
world?s languages.
Alexandre re?side avec sa famille a` Tinqueux .
NOUN VERB ADP DET NOUN ADP NOUN P
NSUBJ ADPMOD
ADPOBJ
POSS
ADPMOD
ADPOBJ
P
Figure 1: A sample French sentence.
We use the so-called basic dependencies (with
punctuation included), where every dependency
structure is a tree spanning all the input tokens,
because this is the kind of representation that most
available dependency parsers require. A sample
dependency tree from the French data set is shown
in Figure 1. We take two approaches to generat-
ing data. The first is traditional manual annotation,
as previously used by Helmreich et al (2004) for
multilingual syntactic treebank construction. The
second, used only for English and Swedish, is to
automatically convert existing treebanks, as in Ze-
man et al (2012).
2.1 Automatic Conversion
Since the Stanford dependencies for English are
taken as the starting point for our universal annota-
tion scheme, we begin by describing the data sets
produced by automatic conversion. For English,
we used the Stanford parser (v1.6.8) (Klein and
Manning, 2003) to convert the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993) to basic dependency trees, including punc-
tuation and with the copula verb as head in cop-
ula constructions. For Swedish, we developed a
set of deterministic rules for converting the Tal-
banken part of the Swedish Treebank (Nivre and
Megyesi, 2007) to a representation as close as pos-
sible to the Stanford dependencies for English.
This mainly consisted in relabeling dependency
relations and, due to the fine-grained label set used
in the Swedish Treebank (Teleman, 1974), this
could be done with high precision. In addition,
a small number of constructions required struc-
tural conversion, notably coordination, which in
the Swedish Treebank is given a Prague style anal-
ysis (Nilsson et al, 2007). For both English and
Swedish, we mapped the language-specific part-
of-speech tags to universal tags using the map-
pings of Petrov et al (2012).
2.2 Manual Annotation
For the remaining four languages, annotators were
given three resources: 1) the English Stanford
93
guidelines; 2) a set of English sentences with Stan-
ford dependencies and universal tags (as above);
and 3) a large collection of unlabeled sentences
randomly drawn from newswire, weblogs and/or
consumer reviews, automatically tokenized with a
rule-based system. For German, French and Span-
ish, contractions were split, except in the case of
clitics. For Korean, tokenization was more coarse
and included particles within token units. Annota-
tors could correct this automatic tokenization.
The annotators were then tasked with producing
language-specific annotation guidelines with the
expressed goal of keeping the label and construc-
tion set as close as possible to the original English
set, only adding labels for phenomena that do not
exist in English. Making fine-grained label dis-
tinctions was discouraged. Once these guidelines
were fixed, annotators selected roughly an equal
amount of sentences to be annotated from each do-
main in the unlabeled data. As the sentences were
already randomly selected from a larger corpus,
annotators were told to view the sentences in or-
der and to discard a sentence only if it was 1) frag-
mented because of a sentence splitting error; 2) not
from the language of interest; 3) incomprehensible
to a native speaker; or 4) shorter than three words.
The selected sentences were pre-processed using
cross-lingual taggers (Das and Petrov, 2011) and
parsers (McDonald et al, 2011).
The annotators modified the pre-parsed trees us-
ing the TrEd2 tool. At the beginning of the annota-
tion process, double-blind annotation, followed by
manual arbitration and consensus, was used itera-
tively for small batches of data until the guidelines
were finalized. Most of the data was annotated
using single-annotation and full review: one an-
notator annotating the data and another reviewing
it, making changes in close collaboration with the
original annotator. As a final step, all annotated
data was semi-automatically checked for annota-
tion consistency.
2.3 Harmonization
After producing the two converted and four an-
notated data sets, we performed a harmonization
step, where the goal was to maximize consistency
of annotation across languages. In particular, we
wanted to eliminate cases where the same label
was used for different linguistic relations in dif-
ferent languages and, conversely, where one and
2Available at http://ufal.mff.cuni.cz/tred/.
the same relation was annotated with different la-
bels, both of which could happen accidentally be-
cause annotators were allowed to add new labels
for the language they were working on. Moreover,
we wanted to avoid, as far as possible, labels that
were only used in one or two languages.
In order to satisfy these requirements, a number
of language-specific labels were merged into more
general labels. For example, in analogy with the
nn label for (element of a) noun-noun compound,
the annotators of German added aa for compound
adjectives, and the annotators of Korean added vv
for compound verbs. In the harmonization step,
these three labels were merged into a single label
compmod for modifier in compound.
In addition to harmonizing language-specific la-
bels, we also renamed a small number of relations,
where the name would be misleading in the uni-
versal context (although quite appropriate for En-
glish). For example, the label prep (for a mod-
ifier headed by a preposition) was renamed adp-
mod, to make clear the relation to other modifier
labels and to allow postpositions as well as prepo-
sitions.3 We also eliminated a few distinctions in
the original Stanford scheme that were not anno-
tated consistently across languages (e.g., merging
complm with mark, number with num, and purpcl
with advcl).
The final set of labels is listed with explanations
in Table 1. Note that relative to the universal part-
of-speech tagset of Petrov et al (2012) our final
label set is quite rich (40 versus 12). This is due
mainly to the fact that the the former is based on
deterministic mappings from a large set of annota-
tion schemes and therefore reduced to the granu-
larity of the greatest common denominator. Such a
reduction may ultimately be necessary also in the
case of dependency relations, but since most of our
data sets were created through manual annotation,
we could afford to retain a fine-grained analysis,
knowing that it is always possible to map from
finer to coarser distinctions, but not vice versa.4
2.4 Final Data Sets
Table 2 presents the final data statistics. The num-
ber of sentences, tokens and tokens/sentence vary
3Consequently, pobj and pcomp were changed to adpobj
and adpcomp.
4The only two data sets that were created through con-
version in our case were English, for which the Stanford de-
pendencies were originally defined, and Swedish, where the
native annotation happens to have a fine-grained label set.
94
Label Description
acomp adjectival complement
adp adposition
adpcomp complement of adposition
adpmod adpositional modifier
adpobj object of adposition
advcl adverbial clause modifier
advmod adverbial modifier
amod adjectival modifier
appos appositive
attr attribute
aux auxiliary
auxpass passive auxiliary
cc conjunction
ccomp clausal complement
Label Description
compmod compound modifier
conj conjunct
cop copula
csubj clausal subject
csubjpass passive clausal subject
dep generic
det determiner
dobj direct object
expl expletive
infmod infinitival modifier
iobj indirect object
mark marker
mwe multi-word expression
neg negation
Label Description
nmod noun modifier
nsubj nominal subject
nsubjpass passive nominal subject
num numeric modifier
p punctuation
parataxis parataxis
partmod participial modifier
poss possessive
prt verb particle
rcmod relative clause modifier
rel relative
xcomp open clausal complement
Table 1: Harmonized label set based on Stanford dependencies (De Marneffe et al, 2006).
source(s) # sentences # tokens
DE N, R 4,000 59,014
EN PTB? 43,948 1,046,829
SV STB? 6,159 96,319
ES N, B, R 4,015 112,718
FR N, B, R 3,978 90,000
KO N, B 6,194 71,840
Table 2: Data set statistics. ?Automatically con-
verted WSJ section of the PTB. The data release
includes scripts to generate this data, not the data
itself. ?Automatically converted Talbanken sec-
tion of the Swedish Treebank. N=News, B=Blogs,
R=Consumer Reviews.
due to the source and tokenization. For example,
Korean has 50% more sentences than Spanish, but
?40k less tokens due to a more coarse-grained to-
kenization. In addition to the data itself, anno-
tation guidelines and harmonization rules are in-
cluded so that the data can be regenerated.
3 Experiments
One of the motivating factors in creating such a
data set was improved cross-lingual transfer eval-
uation. To test this, we use a cross-lingual transfer
parser similar to that of McDonald et al (2011).
In particular, it is a perceptron-trained shift-reduce
parser with a beam of size 8. We use the features
of Zhang and Nivre (2011), except that all lexical
identities are dropped from the templates during
training and testing, hence inducing a ?delexical-
ized? model that employs only ?universal? proper-
ties from source-side treebanks, such as part-of-
speech tags, labels, head-modifier distance, etc.
We ran a number of experiments, which can be
seen in Table 3. For these experiments we ran-
domly split each data set into training, develop-
ment and testing sets.5 The one exception is En-
glish, where we used the standard splits. Each
row in Table 3 represents a source training lan-
guage and each column a target evaluation lan-
guage. We report both unlabeled attachment score
(UAS) and labeled attachment score (LAS) (Buch-
holz and Marsi, 2006). This is likely the first re-
liable cross-lingual parsing evaluation. In partic-
ular, previous studies could not even report LAS
due to differences in treebank annotations.
We can make several interesting observations.
Most notably, for the Germanic and Romance tar-
get languages, the best source language is from
the same language group. This is in stark contrast
to the results of McDonald et al (2011), who ob-
serve that this is rarely the case with the heteroge-
nous CoNLL treebanks. Among the Germanic
languages, it is interesting to note that Swedish
is the best source language for both German and
English, which makes sense from a typological
point of view, because Swedish is intermediate be-
tween German and English in terms of word or-
der properties. For Romance languages, the cross-
lingual parser is approaching the accuracy of the
supervised setting, confirming that for these lan-
guages much of the divergence is lexical and not
structural, which is not true for the Germanic lan-
guages. Finally, Korean emerges as a very clear
outlier (both as a source and as a target language),
which again is supported by typological consider-
ations as well as by the difference in tokenization.
With respect to evaluation, it is interesting to
compare the absolute numbers to those reported
in McDonald et al (2011) for the languages com-
5These splits are included in the release of the data.
95
Source
Training
Language
Target Test Language
Unlabeled Attachment Score (UAS) Labeled Attachment Score (LAS)
Germanic Romance Germanic Romance
DE EN SV ES FR KO DE EN SV ES FR KO
DE 74.86 55.05 65.89 60.65 62.18 40.59 64.84 47.09 53.57 48.14 49.59 27.73
EN 58.50 83.33 70.56 68.07 70.14 42.37 48.11 78.54 57.04 56.86 58.20 26.65
SV 61.25 61.20 80.01 67.50 67.69 36.95 52.19 49.71 70.90 54.72 54.96 19.64
ES 55.39 58.56 66.84 78.46 75.12 30.25 45.52 47.87 53.09 70.29 63.65 16.54
FR 55.05 59.02 65.05 72.30 81.44 35.79 45.96 47.41 52.25 62.56 73.37 20.84
KO 33.04 32.20 27.62 26.91 29.35 71.22 26.36 21.81 18.12 18.63 19.52 55.85
Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
mon to both studies (DE, EN, SV and ES). In that
study, UAS was in the 38?68% range, as compared
to 55?75% here. For Swedish, we can even mea-
sure the difference exactly, because the test sets
are the same, and we see an increase from 58.3%
to 70.6%. This suggests that most cross-lingual
parsing studies have underestimated accuracies.
4 Conclusion
We have released data sets for six languages with
consistent dependency annotation. After the ini-
tial release, we will continue to annotate data in
more languages as well as investigate further au-
tomatic treebank conversions. This may also lead
to modifications of the annotation scheme, which
should be regarded as preliminary at this point.
Specifically, with more typologically and morpho-
logically diverse languages being added to the col-
lection, it may be advisable to consistently en-
force the principle that content words take func-
tion words as dependents, which is currently vi-
olated in the analysis of adpositional and copula
constructions. This will ensure a consistent analy-
sis of functional elements that in some languages
are not realized as free words or are not obliga-
tory, such as adpositions which are often absent
due to case inflections in languages like Finnish. It
will also allow the inclusion of language-specific
functional or morphological markers (case mark-
ers, topic markers, classifiers, etc.) at the leaves of
the tree, where they can easily be ignored in appli-
cations that require a uniform cross-lingual repre-
sentation. Finally, this data is available on an open
source repository in the hope that the community
will commit new data and make corrections to ex-
isting annotations.
Acknowledgments
Many people played critical roles in the pro-
cess of creating the resource. At Google, Fer-
nando Pereira, Alfred Spector, Kannan Pashu-
pathy, Michael Riley and Corinna Cortes sup-
ported the project and made sure it had the re-
quired resources. Jennifer Bahk and Dave Orr
helped coordinate the necessary contracts. Andrea
Held, Supreet Chinnan, Elizabeth Hewitt, Tu Tsao
and Leigha Weinberg made the release process
smooth. Michael Ringgaard, Andy Golding, Terry
Koo, Alexander Rush and many others provided
technical advice. Hans Uszkoreit gave us per-
mission to use a subsample of sentences from the
Tiger Treebank (Brants et al, 2002), the source of
the news domain for our German data set. Anno-
tations were additionally provided by Sulki Kim,
Patrick McCrae, Laurent Alamarguy and He?ctor
Ferna?ndez Alcalde.
References
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Parsed Cor-
pora, pages 103?127. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
the 2002 workshop on Grammar engineering and
evaluation-Volume 15.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009.
96
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC.
Tomaz Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46:131?142.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Grac?a. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking finnish. In Proceedings of
The Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9).
Stephen Helmreich, David Farwell, Bonnie Dorr, Nizar
Habash, Lori Levin, Teruko Mitamura, Florence
Reeder, Keith Miller, Eduard Hovy, Owen Rambow,
and Advaith Siddharthan. 2004. Interlingual anno-
tation of multilingual text corpora. In Proceedings
of the HLT-EACL Workshop on Frontiers in Corpus
Annotation.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of ACL.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of ACL.
Joakim Nivre and Bea?ta Megyesi. 2007. Bootstrap-
ping a Swedish treebank using cross-corpus harmo-
nization and annotation projection. In Proceedings
of the 6th International Workshop on Treebanks and
Linguistic Theories.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Mojgan Seraji, Bea?ta Megyesi, and Nivre Joakim.
2012. Bootstrapping a Persian dependency tree-
bank. Linguistic Issues in Language Technology,
7(18):1?10.
David A. Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivn-
ing av talad och skriven svenska. Studentlitteratur.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. Proceedings of
ACL.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan S?tepa?nek, Zdene?k
Z?abokrtsky`, and Jan Hajic. 2012. Hamledt: To
parse or not to parse. In Proceedings of LREC.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a universal
pos tagset. In Proceedings of EMNLP.
97
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1448?1458,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Semantic Frame Identification with Distributed Word Representations
Karl Moritz Hermann
??
Dipanjan Das
?
Jason Weston
?
Kuzman Ganchev
?
?
Department of Computer Science, University of Oxford, Oxford OX1 3QD, United Kingdom
?
Google Inc., 76 9th Avenue, New York, NY 10011, United States
karl.moritz.hermann@cs.ox.ac.uk
{dipanjand,kuzman}@google.com jaseweston@gmail.com
Abstract
We present a novel technique for semantic
frame identification using distributed rep-
resentations of predicates and their syntac-
tic context; this technique leverages auto-
matic syntactic parses and a generic set
of word embeddings. Given labeled data
annotated with frame-semantic parses, we
learn a model that projects the set of word
representations for the syntactic context
around a predicate to a low dimensional
representation. The latter is used for se-
mantic frame identification; with a stan-
dard argument identification method in-
spired by prior work, we achieve state-of-
the-art results on FrameNet-style frame-
semantic analysis. Additionally, we report
strong results on PropBank-style semantic
role labeling in comparison to prior work.
1 Introduction
Distributed representations of words have proved
useful for a number of tasks. By providing richer
representations of meaning than what can be en-
compassed in a discrete representation, such ap-
proaches have successfully been applied to tasks
such as sentiment analysis (Socher et al, 2011),
topic classification (Klementiev et al, 2012) or
word-word similarity (Mitchell and Lapata, 2008).
We present a new technique for semantic frame
identification that leverages distributed word rep-
resentations. According to the theory of frame se-
mantics (Fillmore, 1982), a semantic frame rep-
resents an event or scenario, and possesses frame
elements (or semantic roles) that participate in the
?
The majority of this research was carried out during an
internship at Google.
event. Most work on frame-semantic parsing has
usually divided the task into two major subtasks:
frame identification, namely the disambiguation of
a given predicate to a frame, and argument iden-
tification (or semantic role labeling), the analysis
of words and phrases in the sentential context that
satisfy the frame?s semantic roles (Das et al, 2010;
Das et al, 2014).
1
Here, we focus on the first sub-
task of frame identification for given predicates;
we use our novel method (?3) in conjunction with
a standard argument identification model (?4) to
perform full frame-semantic parsing.
We present experiments on two tasks. First, we
show that for frame identification on the FrameNet
corpus (Baker et al, 1998; Fillmore et al, 2003),
we outperform the prior state of the art (Das et al,
2014). Moreover, for full frame-semantic parsing,
with the presented frame identification technique
followed by our argument identification method,
we report the best results on this task to date. Sec-
ond, we present results on PropBank-style seman-
tic role labeling (Palmer et al, 2005; Meyers et al,
2004; M`arquez et al, 2008), that approach strong
baselines, and are on par with prior state of the art
(Punyakanok et al, 2008).
2 Overview
Early work in frame-semantic analysis was pio-
neered by Gildea and Jurafsky (2002). Subsequent
work in this area focused on either the FrameNet
or PropBank frameworks, and research on the lat-
ter has been more popular. Since the CoNLL
2004-2005 shared tasks (Carreras and M`arquez,
1
There are exceptions, wherein the task has been modeled
using a pipeline of three classifiers that perform frame iden-
tification, a binary stage that classifies candidate arguments,
and argument identification on the filtered candidates (Baker
et al, 2007; Johansson and Nugues, 2007).
1448
John     bought    a   car   .
COMMERCE_BUY
buy.V
Buyer Goods
John     bought    a   car   .
buy.01
buy.V
A0 A1
Mary      sold        a   car   .
COMMERCE_BUY
sell.V
Seller Goods
Mary      sold        a   car   .
sell.01
sell.V
A0 A1
(a) (b)
Figure 1: Example sentences with frame-semantic analyses.
FrameNet annotation conventions are used in (a) while (b)
denotes PropBank conventions.
2004; Carreras and M`arquez, 2005) on PropBank
semantic role labeling (SRL), it has been treated
as an important NLP problem. However, research
has mostly focused on argument analysis, skipping
the frame disambiguation step, and its interaction
with argument identification.
2.1 Frame-Semantic Parsing
Closely related to SRL, frame-semantic parsing
consists of the resolution of predicate sense into
a frame, and the analysis of the frame?s argu-
ments. Work in this area exclusively uses the
FrameNet full text annotations. Johansson and
Nugues (2007) presented the best performing sys-
tem at SemEval 2007 (Baker et al, 2007), and Das
et al (2010) improved performance, and later set
the current state of the art on this task (Das et al,
2014). We briefly discuss FrameNet, and subse-
quently PropBank annotation conventions here.
FrameNet The FrameNet project (Baker et al,
1998) is a lexical database that contains informa-
tion about words and phrases (represented as lem-
mas conjoined with a coarse part-of-speech tag)
termed as lexical units, with a set of semantic
frames that they could evoke. For each frame,
there is a list of associated frame elements (or
roles, henceforth), that are also distinguished as
core or non-core.
2
Sentences are annotated us-
ing this universal frame inventory. For exam-
ple, consider the pair of sentences in Figure 1(a).
COMMERCE BUY is a frame that can be evoked by
morphological variants of the two example lexical
units buy.V and sell.V. Buyer, Seller and Goods are
some example roles for this frame.
2
Additional information such as finer distinction of the
coreness properties of roles, the relationship between frames,
and that of roles are also present, but we do not leverage that
information in this work.
PropBank The PropBank project (Palmer et al,
2005) is another popular resource related to se-
mantic role labeling. The PropBank corpus has
verbs annotated with sense frames and their ar-
guments. Like FrameNet, it also has a lexi-
cal database that stores type information about
verbs, in the form of sense frames and the possi-
ble semantic roles each frame could take. There
are modifier roles that are shared across verb
frames, somewhat similar to the non-core roles
in FrameNet. Figure 1(b) shows annotations for
two verbs ?bought? and ?sold?, with their lemmas
(akin to the lexical units in FrameNet) and their
verb frames buy.01 and sell.01. Generic core role
labels (of which there are seven, namely A0-A5 and
AA) for the verb frames are marked in the figure.
3
A key difference between the two annotation sys-
tems is that PropBank uses a local frame inven-
tory, where frames are predicate-specific. More-
over, role labels, although few in number, take spe-
cific meaning for each verb frame. Figure 1 high-
lights this difference: while both sell.V and buy.V
are members of the same frame in FrameNet, they
evoke different frames in PropBank. In spite of
this difference, nearly identical statistical models
could be employed for both frameworks.
Modeling In this paper, we model the frame-
semantic parsing problem in two stages: frame
identification and argument identification. As
mentioned in ?1, these correspond to a frame dis-
ambiguation stage,
4
and a stage that finds the var-
ious arguments that fulfill the frame?s semantic
roles within the sentence, respectively. This re-
sembles the framework of Das et al (2014), who
solely focus on FrameNet corpora, unlike this pa-
per. The novelty of this paper lies in the frame
identification stage (?3). Note that this two-stage
approach is unusual for the PropBank corpora
when compared to prior work, where the vast ma-
jority of published papers have not focused on the
verb frame disambiguation problem at all, only fo-
cusing on the role labeling stage (see the overview
paper of M`arquez et al (2008) for example).
3
NomBank (Meyers et al, 2004) is a similar resource for
nominal predicates, but we do not consider it in our experi-
ments.
4
For example in PropBank, the lexical unit buy.V has
three verb frames and in sentential context, we want to disam-
biguate its frame. (Although PropBank never formally uses
the term lexical unit, we adopt its usage from the frame se-
mantics literature.)
1449
2.2 Distributed Frame Identification
We present a model that takes word embeddings
as input and learns to identify semantic frames.
A word embedding is a distributed representa-
tion of meaning where each word is represented
as a vector in R
n
. Such representations allow a
model to share meaning between similar words,
and have been used to capture semantic, syntac-
tic and morphological content (Collobert and We-
ston, 2008; Turian et al, 2010, inter alia). We use
word embeddings to represent the syntactic con-
text of a particular predicate instance as a vector.
For example, consider the sentence ?He runs the
company.? The predicate runs has two syntac-
tic dependents ? a subject and direct object (but
no prepositional phrases or clausal complements).
We could represent the syntactic context of runs as
a vector with blocks for all the possible dependents
warranted by a syntactic parser; for example, we
could assume that positions 0 . . . n in the vector
correspond to the subject dependent, n+1 . . . 2n
correspond to the clausal complement dependent,
and so forth. Thus, the context is a vector in R
nk
with the embedding of He at the subject position,
the embedding of company in direct object posi-
tion and zeros everywhere else. Given input vec-
tors of this form for our training data, we learn a
matrix that maps this high dimensional and sparse
representation into a lower dimensional space. Si-
multaneously, the model learns an embedding for
all the possible labels (i.e. the frames in a given
lexicon). At inference time, the predicate-context
is mapped to the low dimensional space, and we
choose the nearest frame label as our classifica-
tion. We next describe this model in detail.
3 Frame Identification with Embeddings
We continue using the example sentence from
?2.2: ?He runs the company.? where we want to
disambiguate the frame of runs in context. First,
we extract the words in the syntactic context of
runs; next, we concatenate their word embeddings
as described in ?2.2 to create an initial vector space
representation. Subsequently, we learn a map-
ping from this initial representation into a low-
dimensional space; we also learn an embedding
for each possible frame label in the same low-
dimensional space. The goal of learning is to
make sure that the correct frame label is as close as
possible to the mapped context, while competing
frame labels are farther away.
Formally, let x represent the actual sentence
with a marked predicate, along with the associated
syntactic parse tree; let our initial representation
of the predicate context be g(x). Suppose that the
word embeddings we start with are of dimension
n. Then g is a function from a parsed sentence
x to R
nk
, where k is the number of possible syn-
tactic context types. For example g selects some
important positions relative to the predicate, and
reserves a block in its output space for the embed-
ding of words found at that position. Suppose g
considers clausal complements and direct objects.
Then g : X ? R
2n
and for the example sentence
it has zeros in positions 0 . . . n and the embedding
of the word company in positions n+1 . . . 2n.
g(x) = [0, . . . , 0, embedding of company].
Section 3.1 describes the context positions we use
in our experiments. Let the low dimensional space
we map to be R
m
and the learned mapping be M :
R
nk
? R
m
. The mapping M is a linear trans-
formation, and we learn it using the WSABIE algo-
rithm (Weston et al, 2011). WSABIE also learns an
embedding for each frame label (y, henceforth).
In our setting, this means that each frame corre-
sponds to a point in R
m
. If we have F possi-
ble frames we can store those parameters in an
F ?m matrix, one m-dimensional point for each
frame, which we will refer to as the linear map-
ping Y . Let the lexical unit (the lemma conjoined
with a coarse POS tag) for the marked predicate
be `. We denote the frames that associate with
` in the frame lexicon
5
and our training corpus
as F
`
. WSABIE performs gradient-based updates
on an objective that tries to minimize the distance
between M(g(x)) and the embedding of the cor-
rect label Y (y), while maintaining a large distance
between M(g(x)) and the other possible labels
Y (y?) in the confusion set F
`
. At disambiguation
time, we use a simple dot product similarity as our
distance metric, meaning that the model chooses
a label by computing the argmax
y
s(x, y) where
s(x, y) = M(g(x)) ?Y (y), where the argmax iter-
ates over the possible frames y ? F
`
if ` was seen
in the lexicon or the training data, or y ? F , if it
was unseen.
6
Model learning is performed using
the margin ranking loss function as described in
5
The frame lexicon stores the frames, corresponding se-
mantic roles and the lexical units associated with the frame.
6
This disambiguation scheme is similar to the one adopted
by Das et al (2014), but they use unlemmatized words to
define their confusion set.
1450
Figure 2: Context representation extraction for the
embedding model. Given a dependency parse (1)
the model extracts all words matching a set of paths
from the frame evoking predicate and its direct de-
pendents (2). The model computes a composed rep-
resentation of the predicate instance by using dis-
tributed vector representations for words (3) ? the
(red) vertical embedding vectors for each word are
concatenated into a long vector. Finally, we learn a
linear transformation function parametrized by the
context blocks (4).
Weston et al (2011), and in more detail in section
3.2.
Since WSABIE learns a single mapping from g(x)
to R
m
, parameters are shared between different
words and different frames. So for example ?He
runs the company? could help the model disam-
biguate ?He owns the company.? Moreover, since
g(x) relies on word embeddings rather than word
identities, information is shared between words.
For example ?He runs the company? could help
us to learn about ?She runs a corporation?.
3.1 Context Representation Extraction
In principle g(x) could be any feature function, but
we performed an initial investigation of two partic-
ular variants. In both variants, our representation
is a block vector where each block corresponds to
a syntactic position relative to the predicate, and
each block?s values correspond to the embedding
of the word at that position.
Direct Dependents The first context function we
considered corresponds to the examples in ?3. To
elaborate, the positions of interest are the labels of
the direct dependents of the predicate, so k is the
number of labels that the dependency parser can
produce. For example, if the label on the edge be-
tween runs and He is nsubj, we would put the em-
bedding of He in the block corresponding to nsubj.
If a label occurs multiple times, then the embed-
dings of the words below this label are averaged.
Unfortunately, using only the direct dependents
can miss a lot of useful information. For exam-
ple, topicalization can place discriminating infor-
mation farther from the predicate. Consider ?He
runs the company.? vs. ?It was the company that
he runs.? In the second sentence, the discrim-
inating word, company dominates the predicate
runs. Similarly, predicates in embedded clauses
may have a distant agent which cannot be captured
using direct dependents. Consider ?The athlete
ran the marathon.? vs. ?The athlete prepared him-
self for three months to run the marathon.? In the
second example, for the predicate run, the agent
The athlete is not a direct dependent, but is con-
nected via a longer dependency path.
Dependency Paths To capture more relevant
context, we developed a second context function
as follows. We scanned the training data for a
given task (either the PropBank or the FrameNet
domains) for the dependency paths that connected
the gold predicates to the gold semantic argu-
ments. This set of dependency paths were deemed
as possible positions in the initial vector space rep-
resentation. In addition, akin to the first context
function, we also added all dependency labels to
the context set. Thus for this context function, the
block cardinality k was the sum of the number of
scanned gold dependency path types and the num-
ber of dependency labels. Given a predicate in its
sentential context, we therefore extract only those
context words that appear in positions warranted
by the above set. See Figure 2 for an illustration
of this process.
We performed initial experiments using con-
text extracted from 1) direct dependents, 2) de-
pendency paths, and 3) both. For all our experi-
ments, setting 3) which concatenates the direct de-
pendents and dependency path always dominated
the other two, so we only report results for this
setting.
3.2 Learning
We model our objective function following We-
ston et al (2011), using a weighted approximate-
rank pairwise loss, learned with stochastic gradi-
ent descent. The mapping from g(x) to the low
dimensional space R
m
is a linear transformation,
so the model parameters to be learnt are the matrix
M ? R
nk?m
as well as the embedding of each
possible frame label, represented as another ma-
trix Y ? R
F?m
where there are F frames in total.
The training objective function minimizes:
?
x
?
y?
L
(
rank
y
(x)
)
max(0, ?+s(x, y)?s(x, y?)).
1451
where x, y are the training inputs and their cor-
responding correct frames, and y? are negative
frames, ? is the margin. Here, rank
y
(x) is the
rank of the positive frame y relative to all the neg-
ative frames:
rank
y
(x) =
?
y?
I(s(x, y) ? ? + s(x, y?)),
and L(?) converts the rank to a weight. Choos-
ing L(?) = C? for any positive constant C opti-
mizes the mean rank, whereas a weighting such as
L(?) =
?
?
i=1
1/i (adopted here) optimizes the
top of the ranked list, as described in (Usunier
et al, 2009). To train with such an objective,
stochastic gradient is employed. For speed the
computation of rank
y
(x) is then replaced with a
sampled approximation: sample N items y? until
a violation is found, i.e. max(0, ? + s(x, y?) ?
s(x, y))) > 0 and then approximate the rank with
(F ? 1)/N , see Weston et al (2011) for more
details on this procedure. For the choices of the
stochastic gradient learning rate, margin (?) and
dimensionality (m), please refer to ?5.4-?5.5.
Note that an alternative approach could learn
only the matrixM , and then use a k-nearest neigh-
bor classifier in R
m
, as in Weinberger and Saul
(2009). The advantage of learning an embedding
for the frame labels is that at inference time we
need to consider only the set of labels for classi-
fication rather than all training examples. Addi-
tionally, since we use a frame lexicon that gives
us the possible frames for a given predicate, we
usually only consider a handful of candidate la-
bels. If we used all training examples for a given
predicate for finding a nearest-neighbor match at
inference time, we would have to consider many
more candidates, making the process very slow.
4 Argument Identification
Here, we briefly describe the argument identifi-
cation model used in our frame-semantic parsing
experiments, post frame identification. Given x,
the sentence with a marked predicate, the argu-
ment identification model assumes that the pred-
icate frame y has been disambiguated. From a
frame lexicon, we look up the set of semantic roles
R
y
that associate with y. This set alo contains the
null role r
?
. From x, a rule-based candidate argu-
ment extraction algorithm extracts a set of spans
A that could potentially serve as the overt
7
argu-
7
By overtness, we mean the non-null instantiation of a
semantic role in a frame-semantic parse.
? starting word of a ? POS of the starting word of a
? ending word of a ? POS of the ending word of a
? head word of a ? POS of the head word of a
? bag of words in a ? bag of POS tags in a
? a bias feature ? voice of the predicate use
? word cluster of a?s head
? word cluster of a?s head conjoined with word cluster
of the predicate
?
? dependency path between a?s head and the predicate
? the set of dependency labels of the predicate?s children
? dependency path conjoined with the POS tag of a?s
head
? dependency path conjoined with the word cluster of
a?s head
? position of a with respect to the predicate (before, after,
overlap or identical)
? whether the subject of the predicate is missing (miss-
ingsubj)
? missingsubj, conjoined with the dependency path
? missingsubj, conjoined with the dependency path from
the verb dominating the predicate to a?s head
Table 1: Argument identification features. The span in con-
sideration is termed a. Every feature in this list has two ver-
sions, one conjoined with the given role r and the other con-
joined with both r and the frame y. The feature with a
?
su-
perscript is only conjoined with the role to reduce its sparsity.
mentsA
y
for y (see ?5.4-?5.5 for the details of the
candidate argument extraction algorithms).
Learning Given training data of the form
??x
(i)
, y
(i)
,M
(i)
??
N
i=1
, where,
M = {(r, a} : r ? R
y
, a ? A ?A
y
}, (1)
a set of tuples that associates each role r in R
y
with a span a according to the gold data. Note that
this mapping associates spans with the null role r
?
as well. We optimize the following log-likelihood
to train our model:
max
?
N
?
i=1
|M
(i)
|
?
j=1
log p?
(
(r, a)
j
|x, y,R
y
)
? C???
2
2
where p? is a log-linear model normalized over the
set R
y
, with features described in Table 1. We
set C = 1.0 and use L-BFGS (Liu and Nocedal,
1989) for training.
Inference Although our learning mechanism
uses a local log-linear model, we perform infer-
ence globally on a per-frame basis by applying
hard structural constraints. Following Das et al
(2014) and Punyakanok et al (2008) we use the
log-probability of the local classifiers as a score in
an integer linear program (ILP) to assign roles sub-
ject to hard constraints described in ?5.4 and ?5.5.
We use an off-the-shelf ILP solver for inference.
1452
5 Experiments
In this section, we present our experiments and
the results achieved. We evaluate our novel frame
identification approach in isolation and also con-
joined with argument identification resulting in
full frame-semantic structures; before presenting
our model?s performance we first focus on the
datasets, baselines and the experimental setup.
5.1 Data
We evaluate our models on both FrameNet- and
PropBank-style structures. For FrameNet, we use
the full-text annotations in the FrameNet 1.5 re-
lease
8
which was used by Das et al (2014, ?3.2).
We used the same test set as Das et al contain-
ing 23 documents with 4,458 predicates. Of the
remaining 55 documents, 16 documents were ran-
domly chosen for development.
9
For experiments with PropBank, we used the
Ontonotes corpus (Hovy et al, 2006), version 4.0,
and only made use of the Wall Street Journal doc-
uments; we used sections 2-21 for training, sec-
tion 24 for development and section 23 for testing.
This resembles the setup used by Punyakanok et
al. (2008). All the verb frame files in Ontonotes
were used for creating our frame lexicon.
5.2 Frame Identification Baselines
For comparison, we implemented a set of baseline
models, with varying feature configurations. The
baselines use a log-linear model that models the
following probability at training time:
p(y|x, `) =
e
??f(y,x,`)
?
y??F
`
e
??f(y?,x,`)
(2)
At test time, this model chooses the best frame as
argmax
y
? ? f(y, x, `) where argmax iterates over
the possible frames y ? F
`
if ` was seen in the
lexicon or the training data, or y ? F , if it was un-
seen, like the disambiguation scheme of ?3. We
train this model by maximizing L
2
regularized
log-likelihood, using L-BFGS; the regularization
constant was set to 0.1 in all experiments.
For comparison with our model from ?3, which
we call WSABIE EMBEDDING, we implemented two
baselines with the log-linear model. Both the
baselines use features very similar to the input rep-
resentations described in ?3.1. The first one com-
putes the direct dependents and dependency paths
8
See https://framenet.icsi.berkeley.edu.
9
These documents are listed in appendix A.
as described in ?3.1 but conjoins them with the
word identity rather than a word embedding. Ad-
ditionally, this model uses the un-conjoined words
as backoff features. This would be a standard NLP
approach for the frame identification problem, but
is surprisingly competitive with the state of the art.
We call this baseline LOG-LINEAR WORDS. The sec-
ond baseline, tries to decouple the WSABIE training
from the embedding input, and trains a log linear
model using the embeddings. So the second base-
line has the same input representation as WSABIE
EMBEDDING but uses a log-linear model instead of
WSABIE. We call this model LOG-LINEAR EMBED-
DING.
5.3 Common Experimental Setup
We process our PropBank and FrameNet training,
development and test corpora with a shift-reduce
dependency parser that uses the Stanford conven-
tions (de Marneffe and Manning, 2013) and uses
an arc-eager transition system with beam size of 8;
the parser and its features are described by Zhang
and Nivre (2011). Before parsing the data, it is
tagged with a POS tagger trained with a condi-
tional random field (Lafferty et al, 2001) with the
following emission features: word, the word clus-
ter, word suffixes of length 1, 2 and 3, capitaliza-
tion, whether it has a hyphen, digit and punctua-
tion. Beyond the bias transition feature, we have
two cluster features for the left and right words in
the transition. We use Brown clusters learned us-
ing the algorithm of Uszkoreit and Brants (2008)
on a large English newswire corpus for cluster fea-
tures. We use the same word clusters for the argu-
ment identification features in Table 1.
We learn the initial embedding representations
for our frame identification model (?3) using a
deep neural language model similar to the one pro-
posed by Bengio et al (2003). We use 3 hidden
layers each with 1024 neurons and learn a 128-
dimensional embedding from a large corpus con-
taining over 100 billion tokens. In order to speed
up learning, we use an unnormalized output layer
and a hinge-loss objective. The objective tries to
ensure that the correct word scores higher than a
random incorrect word, and we train with mini-
batch stochastic gradient descent.
5.4 Experimental Setup for FrameNet
Hyperparameters For our frame identification
model with embeddings, we search for the WSA-
BIE hyperparameters using the development data.
1453
SEMAFOR LEXICON FULL LEXICON
Development Data
Model All Ambiguous Rare All Ambiguous Rare
LOG-LINEAR WORDS 96.21 90.41 95.75 96.37 90.41 96.07
LOG-LINEAR EMBEDDING 96.06 90.56 95.38 96.19 90.49 95.70
WSABIE EMBEDDING (?3) 96.90 92.73 96.44 96.99 93.12 96.39
SEMAFOR LEXICON FULL LEXICON
Model All Ambiguous Rare Unseen All Ambiguous Rare
Test Data
Das et al (2014) supervised 82.97 69.27 80.97 23.08
Das et al (2014) best 83.60 69.19 82.31 42.67
LOG-LINEAR WORDS 84.71 70.97 81.70 27.27 87.44 70.97 87.10
LOG-LINEAR EMBEDDING 83.42 68.70 80.95 27.97 86.20 68.70 86.03
WSABIE EMBEDDING (?3) 86.58 73.67 85.04 44.76 88.73 73.67 89.38
Table 2: Frame identification results for FrameNet. See ?5.6.
SEMAFOR LEXICON FULL LEXICON
Model Precision Recall F
1
Precision Recall F
1
Development Data
LOG-LINEAR WORDS 89.43 75.98 82.16 89.41 76.05 82.19
WSABIE EMBEDDING (?3) 89.89 76.40 82.59 89.94 76.27 82.54
Test Data
Das et al supervised 67.81 60.68 64.05
Das et al best 68.33 61.14 64.54
LOG-LINEAR WORDS 71.16 63.56 67.15 73.35 65.27 69.08
WSABIE EMBEDDING (?3) 72.79 64.95 68.64 74.44 66.17 70.06
Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We
skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
We search for the stochastic gradient learning
rate in {0.0001, 0.001, 0.01}, the margin ? ?
{0.001, 0.01, 0.1, 1} and the dimensionality of the
final vector space m ? {256, 512}, to maximize
the frame identification accuracy of ambiguous
lexical units; by ambiguous, we imply lexical units
that appear in the training data or the lexicon with
more than one semantic frame. The underlined
values are the chosen hyperparameters used to an-
alyze the test data.
Argument Candidates The candidate argument
extraction method used for the FrameNet data, (as
mentioned in ?4) was adapted from the algorithm
of Xue and Palmer (2004) applied to dependency
trees. Since the original algorithm was designed
for verbs, we added a few extra rules to handle
non-verbal predicates: we added 1) the predicate
itself as a candidate argument, 2) the span ranging
from the sentence position to the right of the pred-
icate to the rightmost index of the subtree headed
by the predicate?s head; this helped capture cases
like ?a few months? (where few is the predicate and
months is the argument), and 3) the span ranging
from the leftmost index of the subtree headed by
the predicate?s head to the position immediately
before the predicate, for cases like ?your gift to
Goodwill? (where to is the predicate and your gift
is the argument).
10
10
Note that Das et al (2014) describe the state of the art
in FrameNet-based analysis, but their argument identifica-
tion strategy considered all possible dependency subtrees in
Frame Lexicon In our experimental setup, we
scanned the XML files in the ?frames? directory
of the FrameNet 1.5 release, which lists all the
frames, the corresponding roles and the associ-
ated lexical units, and created a frame lexicon to
be used in our frame and argument identification
models. We noted that this renders every lexical
unit as seen; in other words, at frame disambigua-
tion time on our test set, for all instances, we only
had to score the frames in F
`
for a predicate with
lexical unit ` (see ?3 and ?5.2). We call this setup
FULL LEXICON. While comparing with prior state
of the art on the same corpus, we noted that Das et
al. (2014) found several unseen predicates at test
time.
11
For fair comparison, we took the lexical
units for the predicates that Das et al considered
as seen, and constructed a lexicon with only those;
training instances, if any, for the unseen predicates
under Das et al?s setup were thrown out as well.
We call this setup SEMAFOR LEXICON.
12
We also
experimented on the set of unseen instances used
by Das et al
ILP constraints For FrameNet, we used three
ILP constraints during argument identification
(?4). 1) each span could have only one role, 2)
each core role could be present only once, and 3)
all overt arguments had to be non-overlapping.
a parse, resulting in a much larger search space.
11
Instead of using the frame files, Das et al built a frame
lexicon from FrameNet?s exemplars and the training corpus.
12
We got Das et al?s seen predicates from the authors.
1454
Model All Ambiguous Rare
LOG-LINEAR WORDS 94.21 90.54 93.33
LOG-LINEAR EMBEDDING 93.81 89.86 93.73
WSABIE EMBEDDING (?3) 94.79 91.52 92.55
Dev data ? ? Test data
Model All Ambiguous Rare
LOG-LINEAR WORDS 94.74 92.07 91.32
LOG-LINEAR EMBEDDING 94.04 90.95 90.97
WSABIE EMBEDDING (?3) 94.56 91.82 90.62
Table 4: Frame identification accuracy results for PropBank.
The model and the column names have the same semantics
as Table 2.
Model P R F
1
LOG-LINEAR WORDS 80.02 75.58 77.74
WSABIE EMBEDDING (?3) 80.06 75.74 77.84
Dev data ? ? Test data
Model P R F
1
LOG-LINEAR WORDS 81.55 77.83 79.65
WSABIE EMBEDDING (?3) 81.32 77.97 79.61
Table 5: Full frame-structure prediction results for Propbank.
This is a metric that takes into account frames and arguments
together. See ?5.7 for more details.
5.5 Experimental Setup for PropBank
Hyperparameters As in ?5.4, we made a hyper-
parameter sweep in the same space. The chosen
learning rate was 0.01, while the other values were
? = 0.01 and m = 512. Ambiguous lexical units
were used for this selection process.
Argument Candidates For PropBank we use
the algorithm of Xue and Palmer (2004) applied
to dependency trees.
Frame Lexicon For the PropBank experiments
we scanned the frame files for propositions in
Ontonotes 4.0, and stored possible core roles for
each verb frame. The lexical units were simply
the verb associating with the verb frames. There
were no unseen verbs at test time.
ILP constraints We used the constraints of Pun-
yakanok et al (2008).
5.6 FrameNet Results
Table 2 presents accuracy results on frame iden-
tification.
13
We present results on all predicates,
ambiguous predicates seen in the lexicon or the
training data, and rare ambiguous predicates that
appear ? 11 times in the training data. The WS-
ABIE EMBEDDING model from ?3 performs signif-
icantly better than the LOG-LINEAR WORDS base-
line, while LOG-LINEAR EMBEDDING underperforms
in every metric. For the SEMAFOR LEXICON setup,
we also compare with the state of the art from Das
13
We do not report partial frame accuracy that has been
reported by prior work.
Model P R F
1
LOG-LINEAR WORDS 77.29 71.50 74.28
WSABIE EMBEDDING (?3) 77.13 71.32 74.11
Dev data ? ? Test data
Model P R F
1
LOG-LINEAR WORDS 79.47 75.11 77.23
WSABIE EMBEDDING (?3) 79.36 75.04 77.14
Punyakanok et al Collins 75.92 71.45 73.62
Punyakanok et al Charniak 77.09 75.51 76.29
Punyakanok et al Combined 80.53 76.94 78.69
Table 6: Argument only evaluation (semantic role labeling
metrics) using the CoNLL 2005 shared task evaluation script
(Carreras and M`arquez, 2005). Results from Punyakanok et
al. (2008) are taken from Table 11 of that paper.
et al (2014), who used a semi-supervised learn-
ing method to improve upon a supervised latent-
variable log-linear model. For unseen predicates
from the Das et al system, we perform better as
well. Finally, for the FULL LEXICON setting, the ab-
solute accuracy numbers are even better for our
best model. Table 3 presents results on the full
frame-semantic parsing task (measured by a reim-
plementation of the SemEval 2007 shared task
evaluation script) when our argument identifica-
tion model (?4) is used after frame identification.
We notice similar trends as in Table 2, and our re-
sults outperform the previously published best re-
sults, setting a new state of the art.
5.7 PropBank Results
Table 4 shows frame identification results on the
PropBank data. On the development set, our best
model performs with the highest accuracy on all
and ambiguous predicates, but performs worse on
rare ambiguous predicates. On the test set, the
LOG-LINEAR WORDS baseline performs best by a
very narrow margin. See ?6 for a discussion.
Table 5 presents results where we measure pre-
cision, recall and F
1
for frames and arguments to-
gether; this strict metric penalizes arguments for
mismatched frames, like in Table 3. We see the
same trend as in Table 4. Finally, Table 6 presents
SRL results that measures argument performance
only, irrespective of the frame; we use the eval-
uation script from CoNLL 2005 (Carreras and
M`arquez, 2005). We note that with a better frame
identification model, our performance on SRL im-
proves in general. Here, too, the embedding model
barely misses the performance of the best baseline,
but we are at par and sometimes better than the sin-
gle parser setting of a state-of-the-art SRL system
(Punyakanok et al, 2008).
14
14
The last row of Table 6 refers to a system which used the
1455
6 Discussion
For FrameNet, the WSABIE EMBEDDING model we
propose strongly outperforms the baselines on all
metrics, and sets a new state of the art. We be-
lieve that the WSABIE EMBEDDING model performs
better than the LOG-LINEAR EMBEDDING baseline
(that uses the same input representation) because
the former setting allows examples with differ-
ent labels and confusion sets to share informa-
tion; this is due to the fact that all labels live in
the same label space, and a single projection ma-
trix is shared across the examples to map the input
features to this space. Consequently, the WSABIE
EMBEDDING model can share more information be-
tween different examples in the training data than
the LOG-LINEAR EMBEDDING model. Since the LOG-
LINEAR WORDS model always performs better than
the LOG-LINEAR EMBEDDING model, we conclude
that the primary benefit does not come from the
input embedding representation.
15
On the PropBank data, we see that the LOG-
LINEAR WORDS baseline has roughly the same per-
formance as our model on most metrics: slightly
better on the test data and slightly worse on the
development data. This can be partially explained
with the significantly larger training set size for
PropBank, making features based on words more
useful. Another important distinction between
PropBank and FrameNet is that the latter shares
frames between multiple lexical units. The ef-
fect of this is clearly observable from the ?Rare?
column in Table 4. WSABIE EMBEDDING performs
poorly in this setting while LOG-LINEAR EMBEDDING
performs well. Part of the explanation has to do
with the specifics of WSABIE training. Recall that
the WSABIE EMBEDDING model needs to estimate
the label location in R
m
for each frame. In other
words, it must estimate 512 parameters based on
at most 10 training examples. However, since the
input representation is shared across all frames,
every other training example from all the lexical
units affects the optimal estimate, since they all
modify the joint parameter matrixM . By contrast,
in the log-linear models each label has its own
set of parameters, and they interact only via the
normalization constant. The LOG-LINEAR WORDS
model does not have this entanglement, but cannot
share information between words. For PropBank,
combination of two syntactic parsers as input.
15
One could imagine training a WSABIE model with word
features, but we did not perform this experiment.
these drawbacks and benefits balance out and we
see similar performance for LOG-LINEAR WORDS
and LOG-LINEAR EMBEDDING. For FrameNet, esti-
mating the label embedding is not as much of a
problem because even if a lexical unit is rare, the
potential frames can be frequent. For example, we
might have seen the SENDING frame many times,
even though telex.V is a rare lexical unit.
In comparison to prior work on FrameNet, even
our baseline models outperform the previous state
of the art. A particularly interesting comparison is
between our LOG-LINEAR WORDS baseline and the
supervised model of Das et al (2014). They also
use a log-linear model, but they incorporate a la-
tent variable that uses WordNet (Fellbaum, 1998)
to get lexical-semantic relationships and smooths
over frames for ambiguous lexical units. It is
possible that this reduces the model?s power and
causes it to over-generalize. Another difference is
that when training the log-linear model, they nor-
malize over all frames, while we normalize over
the allowed frames for the current lexical unit.
This would tend to encourage their model to ex-
pend more of its modeling power to rule out pos-
sibilities that will be pruned out at test time.
7 Conclusion
We have presented a simple model that outper-
forms the prior state of the art on FrameNet-
style frame-semantic parsing, and performs at par
with one of the previous-best single-parser sys-
tems on PropBank SRL. Unlike Das et al (2014),
our model does not rely on heuristics to con-
struct a similarity graph and leverage WordNet;
hence, in principle it is generalizable to varying
domains, and to other languages. Finally, we pre-
sented results on PropBank-style semantic role la-
beling with a system that included the task of au-
tomatic verb frame identification, in tune with the
FrameNet literature; we believe that such a sys-
tem produces more interpretable output, both from
the perspective of human understanding as well as
downstream applications, than pipelines that are
oblivious to the verb frame, only focusing on ar-
gument analysis.
Acknowledgments
We thank Emily Pitler for comments on an early
draft, and the anonymous reviewers for their valu-
able feedback.
1456
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The berkeley framenet project. In Proceedings of
COLING-ACL.
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction.
In Proceedings of SemEval.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
X. Carreras and L. M`arquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling.
In Proceedings of CoNLL.
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 shared task: semantic role labeling. In
Proceedings of CoNLL.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of
ICML.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010.
Probabilistic frame-semantic parsing. In Proceed-
ings of NAACL-HLT.
D. Das, D. Chen, A. F. T. Martins, N. Schneider, and
N. A. Smith. 2014. Frame-semantic parsing. Com-
putational Linguistics, 40(1):9?56.
M.-C. de Marneffe and C. D. Manning, 2013. Stanford
typed dependencies manual.
C. Fellbaum, editor. 1998. WordNet: an electronic
lexical database.
C. J. Fillmore, C. R. Johnson, and M. R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3):235?250.
C. J. Fillmore. 1982. Frame Semantics. In Linguis-
tics in the Morning Calm, pages 111?137. Hanshin
Publishing Co., Seoul, South Korea.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90 In Pro-
ceedings of NAACL-HLT.
R. Johansson and P. Nugues. 2007. LTH: semantic
structure extraction using nonprojective dependency
trees. In Proceedings of SemEval.
A. Klementiev, I. Titov, and B. Bhattarai. 2012. In-
ducing crosslingual distributed representations of
words. In Proceedings of COLING.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
D. C. Liu and J. Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503 ? 528.
L. M`arquez, X. Carreras, K. C. Litkowski, and
S. Stevenson. 2008. Semantic role labeling: an in-
troduction to the special issue. Computational Lin-
guistics, 34(2):145?159.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In Pro-
ceedings of NAACL/HLT Workshop on Frontiers in
Corpus Annotation.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-
HLT.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2):257?287.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment distributions.
In Proceedings of EMNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word
representations: A simple and general method for
semi-supervised learning. In Proceedings of ACL,
Stroudsburg, PA, USA.
N. Usunier, D. Buffoni, and P. Gallinari. 2009. Rank-
ing with ordered weighted pairwise classification. In
ICML.
J. Uszkoreit and T. Brants. 2008. Distributed word
clustering for large scale class-based language mod-
eling in machine translation. In Proceedings of
ACL-HLT.
K. Q. Weinberger and L. K. Saul. 2009. Distance met-
ric learning for large margin nearest neighbor clas-
sification. Journal of Machine Learning Research,
10:207?244.
J. Weston, S. Bengio, and N. Usunier. 2011. Wsabie:
Scaling up to large vocabulary image annotation. In
Proceedings of IJCAI.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of EMNLP
2004.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Pro-
ceedings of ACL-HLT.
1457
Number Filename
dev-1 LUCorpus-v0.3 20000420 xin eng-NEW.xml
dev-2 NTI SouthAfrica Introduction.xml
dev-3 LUCorpus-v0.3 CNN AARONBROWN ENG 20051101 215800.partial-NEW.xml
dev-4 LUCorpus-v0.3 AFGP-2002-600045-Trans.xml
dev-5 PropBank TicketSplitting.xml
dev-6 Miscellaneous Hijack.xml
dev-7 LUCorpus-v0.3 artb 004 A1 E1 NEW.xml
dev-8 NTI WMDNews 042106.xml
dev-9 C-4 C-4Text.xml
dev-10 ANC EntrepreneurAsMadonna.xml
dev-11 NTI LibyaCountry1.xml
dev-12 NTI NorthKorea NuclearOverview.xml
dev-13 LUCorpus-v0.3 20000424 nyt-NEW.xml
dev-14 NTI WMDNews 062606.xml
dev-15 ANC 110CYL070.xml
dev-16 LUCorpus-v0.3 CNN ENG 20030614 173123.4-NEW-1.xml
Table 7: List of files used as development set for the FrameNet 1.5 corpus.
A Development Data
Table 7 features a list of the 16 randomly selected
documents from the FrameNet 1.5 corpus, which
we used for development. The resultant develop-
ment set consists of roughly 4,500 predicates. We
use the same test set as in Das et al (2014), con-
taining 23 documents and 4,458 predicates.
1458
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 115?120,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Enhanced Search with Wildcards and Morphological Inflections
in the Google Books Ngram Viewer
Jason Mann
??
David Zhang
??
Lu Yang
??
Dipanjan Das
?
Slav Petrov
?
?
Columbia University
?
USC
?
Cornell University
?
Google Inc.
Contact: dipanjand@google.com, slav@google.com
Abstract
We present a new version of the Google
Books Ngram Viewer, which plots the fre-
quency of words and phrases over the last
five centuries; its data encompasses 6%
of the world?s published books. The new
Viewer adds three features for more pow-
erful search: wildcards, morphological in-
flections, and capitalization. These addi-
tions allow the discovery of patterns that
were previously difficult to find and fur-
ther facilitate the study of linguistic trends
in printed text.
1 Introduction
The Google Books Ngram project facilitates the
analysis of cultural, social and linguistic trends
through five centuries of written text in eight
languages. The Ngram Corpus (Michel et al.,
2011; Lin et al., 2012) consists of words and
phrases (i.e., ngrams) and their usage frequency
over time.
1
The interactive Ngram Viewer
2
allows
users to retrieve and plot the frequency of mul-
tiple ngrams on a simple webpage. The Viewer
is widely popular and can be used to efficiently
explore and visualize patterns in the underlying
ngram data. For example, the ngram data has
been used to detect emotion trends in 20th cen-
tury books (Acerbi et al., 2013), to analyze text
focusing on market capitalism throughout the past
century (Schulz and Robinson, 2013), detect so-
cial and cultural impact of historical personalities
(Skiena and Ward, 2013), or to analyze the corre-
lation of economic crises with a literary ?misery
?
The majority of this work was carried out during an
internship at Google.
1
The Ngram Corpus is freely available for download at
http://books.google.com/ngrams/datasets.
2
See http://books.google.com/ngrams.
1930 1965 2000
Rel
ativ
e Fr
equ
enc
y
Query: "President Kennedy, President Reagan, President Nixon"
"President Kennedy"
"President Reagan"
"President Nixon"
Figure 1: Mention frequencies for three different American
presidents queried one-by-one.
index? reflected in printed text during crises peri-
ods (Bentley et al., 2014).
A limitation of the Viewer, however, is that all
the reasoning has to be done by the user, and
only individual, user-specified ngrams can be re-
trieved and plotted. For example, to compare
the popularity of different presidents, one needs
to come up with a list of presidents and then
search for them one-by-one. The result of the
query ?President Kennedy, President
Nixon, President Reagan? is shown in
Figure 1. To determine the most popular president,
one would need to search for all presidents, which
is cumbersome and should ideally be automated.
In this paper, we therefore present an updated
version of the Viewer that enhances its search
functionality. We introduce three new features
that automatically expand a given query and re-
trieve a collection of ngrams, to facilitate the dis-
covery of patterns in the underlying data. First,
users can replace one query term with a place-
holder symbol ?
*
? (wildcard, henceforth), which
will return the ten most frequent expansions of
the wildcard in the corpus for the specified year
range. Second, by adding a specific marker to
any word in a query (? INF?), ngrams with all
115
morphological inflections of that word will be re-
trieved. Finally, the new Viewer supports capi-
talization searches, which return all capitalization
variants of the query ngram. Figure 2 provides ex-
amples for these three new types of queries.
While it is fairly obvious how the above search
features can be implemented via brute-force com-
putation, supporting an interactive application
with low latency necessitates some precomputa-
tion. In particular, the wildcard search feature
poses some challenges because the most frequent
expansions depend on the selected year range
(consider the frequency with which presidents are
mentioned during different decades, for example).
To this end, we provide details of our system ar-
chitecture in ?2 and discuss how the new search
features are implemented in ?3. In addition, we
present an overhaul of the Ngram Viewer?s user
interface with interactive features that allow for
easier management of the increase in data points
returned.
Detailed analysis and interpretation of trends
uncovered with the new search interface is beyond
the scope of this paper. We highlight some in-
teresting use cases in ?4; many of the presented
queries were difficult (or impossible) to execute in
the previous versions of the system. We emphasize
that this demonstration updates only the Viewer,
providing tools for easier analysis of the underly-
ing corpora. The ngram corpora themselves are
not updated.
2 System Overview
We first briefly review the two editions of the
Ngram Corpus (Michel et al., 2011; Lin et al.,
2012) and then describe the extensions to the ar-
chitecture of the Viewer that are needed to support
the new search features.
2.1 The Ngram Corpus
The Google Books Ngram Corpus provides ngram
counts for eight different languages over more
than 500 years; additionally, the English corpus
is split further into British vs. American English
and Fiction to aid domain-specific analysis. This
corpus is a subset of all books digitized at Google
and represents more than 6% of all publicized texts
(Lin et al., 2012). Two editions of the corpus are
available: the first edition dates from 2009 and is
described in Michel et al. (2011); the second edi-
tion is from 2012 and is described in Lin et al.
1900 1950 2000
Re
lat
ive
 F
req
ue
nc
y
Query: "University of *"
University of California
University of Chicago
University of Wisconsin
University of Michigan
University of Pennsylvania
1950 1975 2000
Re
lat
ive
 F
req
ue
nc
y
Query: "book_INF a hotel"
book a hotel
booked a hotel
booking a hotel
books a hotel
1800 1900 2000
Re
lat
ive
 F
req
ue
nc
y
Query: "fitzgerald [case-insensitive]"
Fitzgerald
FitzGerald
FITZGERALD
Figure 2: In the new enhanced search features of the Ngram
Viewer, a single query is automatically expanded to retrieve
multiple related ngrams. From top to bottom, we show ex-
amples of the wildcard operator (?
*
?), the ? INF? marker that
results in morphological inflections, and the case insensitive
search functionality. Due to space considerations we show
only a subset of the results returned by the Ngram Viewer.
(2012). The new search features presented here
are available for both editions.
Michel et al. (2011) extract ngrams for each
page in isolation. More specifically, they use
whitespace tokenization and extract all ngrams up
to length five. These ngrams include ones that po-
tentially span sentence boundaries, but do not in-
clude ngrams that span across page breaks (even
when they are part of the same sentence). Lin
et al. (2012) on the other hand perform tokeniza-
tion, text normalization and segmentation into sen-
tences. They then add synthetic START and
END tokens to the beginning and end of the sen-
116
tences to enable the distinction of sentence me-
dial ngrams from those near sentence boundaries.
They also ensure that sentences that span across
page boundaries are included. Due to these dif-
ferences, as well as the availability of additional
book data, improvements to the optical character
recognition algorithms and metadata extraction for
dating the books, the ngrams counts from the two
editions are not the same.
The edition from Lin et al. (2012) additionally
includes syntactic ngrams. The corpus is tagged
using the universal part-of-speech (POS) tag set
of Petrov et al. (2012): NOUN (nouns), VERB
(verbs), ADJ (adjectives), ADV (adverbs), PRON
(pronouns), DET (determiners and articles), ADP
(prepositions and postpositions), CONJ (conjunc-
tions). Words can be disambiguated by their POS
tag by simply appending the tag to the word with
an underscore (e.g. book NOUN) and can also be
replaced by POS tags in the ngrams, see Lin et
al. (2012) for details. The corpus is parsed with
a dependency parser and head-modifier syntactic
relations between words in the same sentence are
extracted. Dependency relations are represented
as ?=>? in the corpus. Our new enhanced search
features for automatic expansions can also be ap-
plied to these syntactic ngrams. In fact, some of
the most interesting queries use expansions to au-
tomatically uncover related ngrams, while using
syntax to focus on particular patterns.
The Viewer supports the composition of ngram
frequencies via arithmetic operators. Addition (+),
subtraction (-) and division (/) of ngrams are car-
ried out on a per year basis, while multiplication
(
*
) is performed by a scalar that is applied to all
counts in the time series. Where ambiguous, the
wildcard operator takes precedence over the mul-
tiplication operator. Parentheses can be used to
disambiguate and to force the interpretation of a
mathematical operation.
2.2 Architecture
The Ngram Viewer provides a lightweight inter-
face to the underlying ngram corpora. In its basic
form, user requests are directed through the server
to a simple lookup table containing the raw ngrams
and their frequencies. This data flow is displayed
in the top part of Figure 3 and is maintained for
queries that do not involve the new expansion fea-
tures introduced in this work.
The expansion queries could in principle be
Raw Ngrams
?King James? :
{(1900, 234), 
(1901, 122), ?}
?Kinged James?: 
{(1900, 20), 
(1901, 15), ?}
?
Inflections
?King_INF?: 
{King, Kinged, 
Kings,
 ? }
Wildcards
?King *?:
{King James,
 King George,
 ? }
Capitalizations
?king james?: 
{king James, 
King James,
? }
Ngram 
Viewer
Server
User
new in this version
Ngram Viewer System Architecture
Figure 3: Overview of the Ngram Viewer architecture.
implemented by scanning the raw ngrams on
the fly and returning the matching subset: to
answer the query ?President
*
?, one would
need to obtain all bigrams starting with the word
President (there are 23,693) and extract the
most frequent ten. Given the large number of
ngrams (especially for larger n), such an approach
turns out to be too slow for an interactive appli-
cation. We therefore pre-compute intermediate re-
sults that can be used to more efficiently retrieve
the results for expansion queries. The intermedi-
ate results are stored in additional lookup tables
(shown at the bottom in Figure 3). When the user
executes an expansion search, the query is first
routed to the appropriate lookup table which stores
all possible expansions (including expansions that
might not appear in the corpus). These expanded
ngrams are then retrieved from the raw ngram ta-
ble, sorted by frequency and returned to he user.
We describe the intermediate results tables and
how they are generated in the next section.
Note that we only support one expansion oper-
ation per query ngram. This is needed in order to
avoid the combinatorial explosion that would re-
sult from mixing multiple expansion operators in
the same query.
3 New Features
The three new search features are implemented via
the same two-step approach. As shown in Fig-
ure 3, we add three new lookup tables that store
intermediate results needed for efficiently support-
117
1800 1900 2000
Rela
tive 
Freq
uenc
y
Query: "President *"
President ofPresident ?sPresident andPresident toPresident Roosevelt
1800 1900 2000
Rela
tive 
Freq
uenc
y
Query: "President *_NOUN, 1800-2000"
President Roosevelt_NOUNPresident Wilson_NOUNPresident Lincoln_NOUNPresident Johnson_NOUNPresident Truman_NOUN
1950 1975 2000
Rela
tive 
Freq
uenc
y
Query: "President *_NOUN, 1950-2000"
President Roosevelt_NOUNPresident Truman_NOUNPresident Kennedy_NOUNPresident Johnson_NOUNPresident Eisenhower_NOUN
Figure 4: Different wildcard queries for bigrams starting with President. Specification of a POS tag along with the wildcard
operator results in more specific results, and the results vary depending on the selected yaer range.
ing the new search types. In all cases the lookup
tables provide a set of possible expansions that are
then retrieved in the original raw ngram table. Be-
low we describe how these intermediate results are
generated and how they are used to retrieve the fi-
nal results.
3.1 Wildcards
Wildcards provide a convenient way to automat-
ically retrieve and explore related ngrams. Be-
cause of the large number of possibilities that can
fill a wildcard slot, returning anything but the top
few expansions is likely to be overwhelming. We
therefore return only the ten most frequent expan-
sions. Determining the most frequent expansions
is unfortunately computationally very expensive
because of the large number of ngrams; the query
?the
*
? for example has 2,353,960 expansions.
To avoid expensive on-the-fly computations,
we precompute the most frequent expansions for
all possible queries. The problem that arises
is that the ten most frequent expansions depend
on the selected year range. Consider the query
?President
*
?; we would like to be able get
the correct result for any year range. Since our
data spans more than 500 years, precomputing the
results for all year ranges is not a possibility. In-
stead, we compute the possible wildcard expan-
sions for each year. The top expansions for the
entire range are then taken from the union of top
expansions for each year. This set is at most of
size 10n (where n is the year range) and in practice
typically a lot smaller. Theoretically it is possible
for this approximation to miss an expansion that is
never among the top ten for a particular year, but
is cumulatively in the top ten for the entire range.
This would happen if there were many spikes in
the data, which is not the case.
To make the wildcard expansions more rele-
vant, we filter expansions that consist entirely of
punctuation symbols. To further narrow down
the expansions and focus on particular patterns,
we allow wildcards to be qualified via POS
tags. Figure 4 shows some example wildcard
queries involving bigrams that start with the word
?President.? See also Table 1 for some addi-
tional examples. Note that it is possible to replace
POS tags with wildcards (e.g., cook
*
) which
will find all POS tags that the query word can take.
3.2 Morphological Inflections
When comparing ngram frequencies (especially
across languages, but also for the same language),
it can be useful to examine and potentially aggre-
gate the frequencies of all inflected forms. This
can be accomplished by manually deriving all in-
flected forms and then using arithmetic operations
to aggregate their counts. Our new inflected form
search accomplishes this automatically. By ap-
pending the keyword INF to a word, a set of
ngrams with all inflected forms of the word will
be retrieved. To generate the inflected forms we
make use of Wiktionary
3
and supplement it with
automatically generated inflection tables based on
the approach of Durrett and DeNero (2013).
Because there are at most a few dozen inflected
forms for any given word, we can afford to sub-
stitute and retrieve all inflections of the marked
word, even the ones that are not grammatical in a
given ngram context. This has the advantage that
we only need to store inflected forms for individ-
ual words rather than entire ngrams. If a generated
ngram has no support in the corpus, we simply
omit it from the final set of results. We do not per-
form any additional filtering; as a result, an inflec-
tion search can produce many results, especially
for morphologically rich languages like Russian.
We have therefore updated the user interface to
better deal with many data lines (?4).
3
See http://www.wiktionary.org/. Because
Wiktionary is an evolving resource, results for a particular
query may change over time.
118
Query Possible Replacements
*
?s Theorem
Lagrange ?s Theorem, Gauss ?s Theorem,
Euler ?s Theorem, Pascal ?s Theorem
War=>
*
NOUN
War=>World NOUN, War=>Civil NOUN,
War=>Second NOUN, War=>Cold NOUN
lubov~ INF lubil, lublu, lubit, lubit~, lubila, lubimyi?, lubix~
book INF book, books, booked, booking
book INF NOUN book, books
cook
*
cook NOUN, cook VERB
the cook (case insensitive)
THE COOK, the cook, The Cook, the Cook, The cook
Table 1: Examples expansions for wildcard, inflection, and capitalization queries.
3.3 Capitalization
By aggregating different capitalizations of the
same word, one can normalize between sentence-
initial and sentence-medial occurrences of a given
word. A simple way to accomplish this is by
searching for a lowercased, capitalized and all
caps spelling of the query. This however can miss
CamelCase spelling and other capitalization vari-
ants (consider FitzGerald for example). It is
of course not feasible to try all case variants of ev-
ery letter in the query. Instead, we perform an of-
fline precomputation step in which we collect all
ngrams that map to the same lowercased string.
Due to scanning errors and spelling mistakes there
can be many extremely rare capitalization variants
for a given query. We therefore filter out all vari-
ants that have a cumulative count of less than 1%
of the most frequent variant for a given year range.
Capitalization searches are enabled by selecting a
case-insensitive check box on the new interface.
4 Use Cases
The three features introduced in this paper repre-
sent a major extension of the capabilities of the
Ngram Viewer. While the second edition of the
Ngram Corpus (Lin et al., 2012) introduced syn-
tactic ngrams, the functionality of the Viewer had
remained largely unchanged since its first launch
five years ago. Together, the updated Corpus and
Viewer enable a much more detailed analysis of
the underlying data. Below we provide some uses
cases highlighting the ways in which sophisticated
queries can be crafted. While the results produce
some intriguing patterns, we leave their analysis to
the experts.
Since we have made no modifications to the un-
derlying raw ngrams, all of the plots in this pa-
per could have also been generated with the pre-
vious version of the Viewer. They would, how-
ever, have required the user to manually generate
and issue all query terms. For example, Figure 1
shows manually created queries searching for spe-
cific presidents; contrarily, Figure 4 shows single
wildcard queries that automatically retrieve the ten
most frequently mentioned presidents and uncover
additional trends that would have required extra
work on behalf of the user.
The wildcard feature used on its own can be a
powerful tool for the analysis of top expansions
for a certain context. Although already useful on
its own, it becomes really powerful when com-
bined with POS tags. The user can attach an un-
derscore and POS tag to either a wildcard-based
or inflection-based query to specify that the ex-
pansions returned should be of a specific part of
speech. Compare the utility of the generic wild-
card and a search with a noun part-of-speech spec-
ification in a query examining president names,
?President
*
? vs. ?President
*
NOUN?
shown in Figure 4. The former gives a mixture
of prepositions, particles, and verbs along with
names of presidents, and because the latter spec-
ifies the noun tag, the top expansions turn out to
be names and more in line with the intention of
the search. Also, note in Figure 4 the difference in
expansions that searching over two different time
ranges provides. In Table 2, we compare the com-
bination of the wildcard feature with the existing
dependency link feature to highlight a comparison
of context across several languages.
It is worth noting that the newly introduced fea-
tures could result in many lines in the resulting
chart. Hence, we have updated the Viewer?s user
interface to better handle charts involving many
ngrams. The new interactive functionality allows
the user to highlight a line by hovering over it,
keep that focus by left clicking, and clear all fo-
cused lines by double clicking. A right click on
any of the expansions returned by an issued query
combines them into the year-wise sum total of all
the expansions. We added another feature to the
119
1700 1850 2000
Rela
tive
 Fre
que
ncy
Query: "light_INF"
"light"
"lights"
"lighted"
"lighter"
"lit"
"lighting"
"lightest"
1700 1850 2000
Rela
tive
 Fre
que
ncy
Query: "light_VERB_INF"
"light_VERB"
"lighted_VERB"
"lit_VERB"
"lighting_VERB"
"lights_VERB"
Figure 5: Comparison of specification of POS tag in wildcard search.
English American British
German French Russian Italian
Chinese
Spanish Hebrew
(All) English English (Simplified)
drinks drinks drinks trinkt boit p~t beve ? bebe dzy
water water water Bier (beer) vin (wine) on (he) vino (wine) ? (wine) agua (water) oii (wine)
wine wine wine Kaffee (coffee) sang (blood) qai? (tea) acqua (water) ? (tea) vino (wine) min (water)
milk coffee tea Wein (wine) eau (water) vodu (water) sangue (blood) ? (water) sangre (blood) d (the)
coffee beer blood Wasser (water) cafe (coffee) On (He) birra (beer) ?? (coffee) vaso (glass) qek (cup)
beer milk beer Tee (tea) verre (glass) vino (wine) caff?e (coffee) ? (person) cerveza (beer) dz (tea)
Table 2: Comparison of the top modifiers of the verb drinks, or its equivalent in translation, in all corpora, retrieved via
the query drinks VERB=>
*
NOUN and equivalents in the other languages. The modifiers can appear both in subject and in
object position because we have access only to unlabeled dependencies.
interface that creates static URLs maintaining all
the raw ngrams retrieved from any query. This pre-
vents statically linked charts from changing over
time, and allowing for backwards compatibility.
One of the primary benefits of the capitalization
feature is the combination of multiple searches
in one, which allows the user to compare case-
insensitive usages of two different phrases. An
alternative use is in Figure 2(c), where capitaliza-
tion search allows the immediate identification of
changing orthographic usage of a word or phrase;
in this case the figure shows the arrival of F. Scott
Fitzgerald in the early to mid 20th century, as well
as the rise in popularity of the CamelCase variety
of his surname at the turn of the 19th century.
Searches using inflections can be useful for the
same reasons as the capitalization feature, and also
be used to compare changes in spelling; it is par-
ticularly useful for the analysis of irregular verbs,
where the query can return both the regular and
irregular forms of a verb.
5 Conclusions
We have presented an update to the Ngram Viewer
that introduces new search features. Users can
now perform more powerful searches that auto-
matically uncover trends which were previously
difficult or impossible to extract. We look forward
to seeing what users of the Viewer will discover.
6 Acknowledgements
We would like to thank John DeNero, Jon Orwant,
Karl Moritz Hermann for many useful discussions.
References
A. Acerbi, V. Lampos, and R. A. Bentley. 2013. Ro-
bustness of emotion extraction from 20th century en-
glish books. In Proceedings of the IEEE Interna-
tional Conference on Big Data.
A. R. Bentley, A. Acerbi, P. Ormerod, and V. Lampos.
2014. Books average previous decade of economic
misery. PLOS One, 9(1).
G. Durrett and J. DeNero. 2013. Supervised learning
of complete morphological paradigms. In Proceed-
ings of NAACL-HLT.
Y. Lin, J.-B. Michel, E. L. Aiden, J. Orwant, W. Brock-
man, and S. Petrov. 2012. Syntactic annotations for
the Google Books Ngram Corpus. In Proceedings
of the ACL.
J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres,
M. K. Gray, The Google Books Team, J. P. Pick-
ett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant,
S. Pinker, M. A. Nowak, and E. Lieberman Aiden.
2011. Quantitative analysis of culture using millions
of digitized books. Science.
S. Petrov, D. Das, and R. McDonald. 2012. A univer-
sal part-of-speech tagset. In Proc. of LREC.
J. Schulz and L. Robinson. 2013. Shifting grounds and
evolving battlegrounds. American Journal of Cul-
tural Sociology, 1(3):373?402.
S. Skiena and C. Ward. 2013. Who?s Bigger?: Where
Historical Figures Really Rank. Cambridge Univer-
sity Press.
120
Transactions of the Association for Computational Linguistics, 1 (2013) 1?12. Action Editor: Sharon Goldwater.
Submitted 11/2012; Revised 1/2013; Published 3/2013. c?2013 Association for Computational Linguistics.
Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging
Oscar Ta?ckstro?m?? Dipanjan Das? Slav Petrov? Ryan McDonald? Joakim Nivre??
 Swedish Institute of Computer Science
?Department of Linguistics and Philology, Uppsala University
?Google Research, New York
oscar@sics.se
{dipanjand|slav|ryanmcd}@google.com
joakim.nivre@lingfil.uu.se
Abstract
We consider the construction of part-of-speech
taggers for resource-poor languages. Recently,
manually constructed tag dictionaries from
Wiktionary and dictionaries projected via bitext
have been used as type constraints to overcome
the scarcity of annotated data in this setting.
In this paper, we show that additional token
constraints can be projected from a resource-
rich source language to a resource-poor target
language via word-aligned bitext. We present
several models to this end; in particular a par-
tially observed conditional random field model,
where coupled token and type constraints pro-
vide a partial signal for training. Averaged
across eight previously studied Indo-European
languages, our model achieves a 25% relative
error reduction over the prior state of the art.
We further present successful results on seven
additional languages from different families,
empirically demonstrating the applicability of
coupled token and type constraints across a
diverse set of languages.
1 Introduction
Supervised part-of-speech (POS) taggers are avail-
able for more than twenty languages and achieve ac-
curacies of around 95% on in-domain data (Petrov et
al., 2012). Thanks to their efficiency and robustness,
supervised taggers are routinely employed in many
natural language processing applications, such as syn-
tactic and semantic parsing, named-entity recognition
and machine translation. Unfortunately, the resources
required to train supervised taggers are expensive to
create and unlikely to exist for the majority of written
?Work primarily carried out while at Google Research.
languages. The necessity of building NLP tools for
these resource-poor languages has been part of the
motivation for research on unsupervised learning of
POS taggers (Christodoulopoulos et al, 2010).
In this paper, we instead take a weakly supervised
approach towards this problem. Recently, learning
POS taggers with type-level tag dictionary constraints
has gained popularity. Tag dictionaries, noisily pro-
jected via word-aligned bitext, have bridged the gap
between purely unsupervised and fully supervised
taggers, resulting in an average accuracy of over 83%
on a benchmark of eight Indo-European languages
(Das and Petrov, 2011). Li et al (2012) further im-
proved upon this result by employing Wiktionary1 as
a tag dictionary source, resulting in the hitherto best
published result of almost 85% on the same setup.
Although the aforementioned weakly supervised
approaches have resulted in significant improvements
over fully unsupervised approaches, they have not
exploited the benefits of token-level cross-lingual
projection methods, which are possible with word-
aligned bitext between a target language of interest
and a resource-rich source language, such as English.
This is the setting we consider in this paper (?2).
While prior work has successfully considered both
token- and type-level projection across word-aligned
bitext for estimating the model parameters of genera-
tive tagging models (Yarowsky and Ngai, 2001; Xi
and Hwa, 2005, inter alia), a key observation under-
lying the present work is that token- and type-level
information offer different and complementary sig-
nals. On the one hand, high confidence token-level
projections offer precise constraints on a tag in a
particular context. On the other hand, manually cre-
1http://www.wiktionary.org/.
1
ated type-level dictionaries can have broad coverage
and do not suffer from word-alignment errors; they
can therefore be used to filter systematic as well as
random noise in token-level projections.
In order to reap these potential benefits, we pro-
pose a partially observed conditional random field
(CRF) model (Lafferty et al, 2001) that couples to-
ken and type constraints in order to guide learning
(?3). In essence, the model is given the freedom to
push probability mass towards hypotheses consistent
with both types of information. This approach is flex-
ible: we can use either noisy projected or manually
constructed dictionaries to generate type constraints;
furthermore, we can incorporate arbitrary features
over the input. In addition to standard (contextual)
lexical features and transition features, we observe
that adding features from a monolingual word cluster-
ing (Uszkoreit and Brants, 2008) can significantly im-
prove accuracy. While most of these features can also
be used in a generative feature-based hidden Markov
model (HMM) (Berg-Kirkpatrick et al, 2010), we
achieve the best accuracy with a globally normalized
discriminative CRF model.
To evaluate our approach, we present extensive
results on standard publicly available datasets for 15
languages: the eight Indo-European languages pre-
viously studied in this context by Das and Petrov
(2011) and Li et al (2012), and seven additional lan-
guages from different families, for which no compa-
rable study exists. In ?4 we compare various features,
constraints and model types. Our best model uses
type constraints derived from Wiktionary, together
with token constraints derived from high-confidence
word alignments. When averaged across the eight
languages studied by Das and Petrov (2011) and Li
et al (2012), we achieve an accuracy of 88.8%. This
is a 25% relative error reduction over the previous
state of the art. Averaged across all 15 languages,
our model obtains an accuracy of 84.5% compared to
78.5% obtained by a strong generative baseline. Fi-
nally, we provide an in depth analysis of the relative
contributions of the two types of constraints in ?5.
2 Coupling Token and Type Constraints
Type-level information has been amply used in
weakly supervised POS induction, either via pure
manually crafted tag dictionaries (Smith and Eisner,
2005; Ravi and Knight, 2009; Garrette and Baldridge,
2012), noisily projected tag dictionaries (Das and
Petrov, 2011) or through crowdsourced lexica, such
as Wiktionary (Li et al, 2012). At the other end
of the spectrum, there have been efforts that project
token-level information across word-aligned bitext
(Yarowsky and Ngai, 2001; Xi and Hwa, 2005). How-
ever, systems that combine both sources of informa-
tion in a single model have yet to be fully explored.
The following three subsections outline our overall
approach for coupling these two types of information
to build robust POS taggers that do not require any
direct supervision in the target language.
2.1 Token Constraints
For the majority of resource-poor languages, there
is at least some bitext with a resource-rich source
language; for simplicity, we choose English as our
source language in all experiments. It is then nat-
ural to consider using a supervised part-of-speech
tagger to predict part-of-speech tags for the English
side of the bitext. These predicted tags can subse-
quently be projected to the target side via automatic
word alignments. This approach was pioneered by
Yarowsky and Ngai (2001), who used the resulting
partial target annotation to estimate the parameters
of an HMM. However, due to the automatic nature
of the word alignments and the POS tags, there will
be significant noise in the projected tags. To conquer
this noise, they used very aggressive smoothing tech-
niques when training the HMM. Fossum and Abney
(2005) used similar token-level projections, but in-
stead combined projections from multiple source lan-
guages to filter out random projection noise as well
as the systematic noise arising from different source
language annotations and syntactic divergences.
2.2 Type Constraints
It is well known that given a tag dictionary, even if
it is incomplete, it is possible to learn accurate POS
taggers (Smith and Eisner, 2005; Goldberg et al,
2008; Ravi and Knight, 2009; Naseem et al, 2009).
While widely differing in the specific model struc-
ture and learning objective, all of these approaches
achieve excellent results. Unfortunately, they rely
on tag dictionaries extracted directly from the un-
derlying treebank data. Such dictionaries provide in
depth coverage of the test domain and also list all
2
	
 
     
   

 
   	
 
  	  	  

	

	
 



	



 

	    


Figure 1: Lattice representation of the inference search space Y(x) for an authentic sentence in Swedish (?The farming
products must be pure and must not contain any additives?), after pruning with Wiktionary type constraints. The
correct parts of speech are listed underneath each word. Bold nodes show projected token constraints y?. Underlined
text indicates incorrect tags. The coupled constraints lattice Y?(x, y?) consists of the bold nodes together with nodes for
words that are lacking token constraints; in this case, the coupled constraints lattice thus defines exactly one valid path.
inflected forms ? both of which are difficult to obtain
and unrealistic to expect for resource-poor languages.
In contrast, Das and Petrov (2011) automatically
create type-level tag dictionaries by aggregating over
projected token-level information extracted from bi-
text. To handle the noise in these automatic dictionar-
ies, they use label propagation on a similarity graph
to smooth (and also expand) the label distributions.
While their approach produces good results and is
applicable to resource-poor languages, it requires a
complex multi-stage training procedure including the
construction of a large distributional similarity graph.
Recently, Li et al (2012) presented a simple and
viable alternative: crowdsourced dictionaries from
Wiktionary. While noisy and sparse in nature, Wik-
tionary dictionaries are available for 170 languages.2
Furthermore, their quality and coverage is growing
continuously (Li et al, 2012). By incorporating type
constraints from Wiktionary into the feature-based
HMM of Berg-Kirkpatrick et al (2010), Li et al were
able to obtain the best published results in this setting,
surpassing the results of Das and Petrov (2011) on
eight Indo-European languages.
2.3 Coupled Constraints
Rather than relying exclusively on either token or
type constraints, we propose to complement the one
with the other during training. For each sentence in
our training set, a partially constrained lattice of tag
sequences is constructed as follows:
2http://meta.wikimedia.org/wiki/
Wiktionary ? October 2012.
1. For each token whose type is not in the tag dic-
tionary, we allow the entire tag set.
2. For each token whose type is in the tag dictio-
nary, we prune all tags not licensed by the dictio-
nary and mark the token as dictionary-pruned.
3. For each token that has a tag projected via a
high-confidence bidirectional word alignment:
if the projected tag is still present in the lattice,
then we prune every tag but the projected tag for
that token; if the projected tag is not present in
the lattice, which can only happen for dictionary-
pruned tokens, then we ignore the projected tag.
Figure 1 provides a running example. The lattice
shows tags permitted after constraining the words
to tags licensed by the dictionary (up until Step 2
from above). There is only a single token ?Jordbruk-
sprodukterna? (?the farming products?) not in the
dictionary; in this case the lattice permits the full
set of tags. With token-level projections (Step 3;
nodes with bold border in Figure 1), the lattice can
be further pruned. In most cases, the projected tag
is both correct and is in the dictionary-pruned lattice.
We thus successfully disambiguate such tokens and
shrink the search space substantially.
There are two cases we highlight in order to show
where our model can break. First, for the token
?Jordbruksprodukterna?, the erroneously projected
tag ADJ will eliminate all other tags from the lattice,
including the correct tag NOUN. Second, the token
?na?gra? (?any?) has a single dictionary entry PRON
and is missing the correct tag DET. In the case where
3
DET is the projected tag, we will not add it to the
lattice and simply ignore it. This is because we hy-
pothesize that the tag dictionary can be trusted more
than the tags projected via noisy word alignments. As
we will see in ?4, taking the union of tags performs
worse, which supports this hypothesis.
For generative models, such as HMMs (?3.1), we
need to define only one lattice. For our best gen-
erative model this is the coupled token- and type-
constrained lattice.3 At prediction time, in both the
discriminative and the generative cases, we find the
most likely label sequence using Viterbi decoding.
For discriminative models, such as CRFs (?3.2),
we need to define two lattices: one that the model
moves probability mass towards and another one
defining the overall search space (or partition func-
tion). In traditional supervised learning without a
dictionary, the former is a trivial lattice containing
the gold standard tag sequence and the latter is the
set of all possible tag sequences spanning the tokens.
With our best model, we will move mass towards the
coupled token- and type-constrained lattice, such that
the model can freely distribute mass across all paths
consistent with these constraints. The lattice defining
the partition function will be the full set of possible
tag sequences when no dictionary is used; when a
dictionary is used it will consist of all dictionary-
pruned tag sequences (sans Step 3 above; the full set
of possibilities shown in Figure 1 for our running
example).
Figures 2 and 3 provide statistics regarding the
supervision coverage and remaining ambiguity. Fig-
ure 2 shows that more than two thirds of all tokens in
our training data are in Wiktionary. However, there is
considerable variation between languages: Spanish
has the highest coverage with over 90%, while Turk-
ish, an agglutinative language with a vast number
of word forms, has less than 50% coverage. Fig-
ure 3 shows that there is substantial uncertainty left
after pruning with Wiktionary, since tokens are rarely
fully disambiguated: 1.3 tags per token are allowed
on average for types in Wiktionary.
Figure 2 further shows that high-confidence align-
ments are available for about half of the tokens for
most languages (Japanese is a notable exception with
3Other training methods exist as well, for example, con-
trastive estimation (Smith and Eisner, 2005).
0
25
50
75
100
avg bg cs da de el es fr it ja nl pt sl sv tr zh
Pe
rc
en
t o
f to
ke
ns
 c
ov
er
ed
Token
coverage Wiktionary Projected Projected+Filtered
Figure 2: Wiktionary and projection dictionary coverage.
Shown is the percentage of tokens in the target side of the
bitext that are covered by Wiktionary, that have a projected
tag, and that have a projected tag after intersecting the two.
0.0
0.5
1.0
1.5
avg bg cs da de el es fr it ja nl pt sl sv tr zh
Nu
mb
er 
of 
tag
s p
er 
tok
en
Figure 3: Average number of licensed tags per token on
the target side of the bitext, for types in Wiktionary.
less than 30% of the tokens covered). Intersecting the
Wiktionary tags and the projected tags (Step 2 and 3
above) filters out some of the potentially erroneous
tags, but preserves the majority of the projected tags;
the remaining, presumably more accurate projected
tags cover almost half of all tokens, greatly reducing
the search space that the learner needs to explore.
3 Models with Coupled Constraints
We now formally present how we couple token and
type constraints and how we use these coupled con-
straints to train probabilistic tagging models. Let
x = (x1x2 . . . x|x|) ? X denote a sentence, where
each token xi ? V is an instance of a word type from
the vocabulary V and let y = (y1y2 . . . y|x|) ? Y de-
note a tag sequence, where yi ? T is the tag assigned
to token xi and T denotes the set of all possible part-
of-speech tags. We denote the lattice of all admissible
tag sequences for the sentence x by Y(x). This is the
4
inference search space in which the tagger operates.
As we shall see, it is crucial to constrain the size of
this lattice in order to simplify learning when only
incomplete supervision is available.
A tag dictionary maps a word type xj ? V to
a set of admissible tags T (xj) ? T . For word
types not in the dictionary we allow the full set of
tags T (while possible, in this paper we do not at-
tempt to distinguish closed-class versus open-class
words). When provided with a tag dictionary, the
lattice of admissible tag sequences for a sentence x
is Y(x) = T (x1) ? T (x2) ? . . . ? T (x|x|). When
no tag dictionary is available, we simply have the full
lattice Y(x) = T |x|.
Let y? = (y?1y?2 . . . y?|x|) be the projected tags for
the sentence x. Note that {y?i} = ? for tokens without
a projected tag. Next, we define a piecewise operator
_ that couples y? and Y(x) with respect to every
sentence index, which results in a token- and type-
constrained lattice. The operator behaves as follows,
coherent with the high level description in ?2.3:
T? (xi, y?i) = y?i _ T (xi) =
{
{y?i} if y?i ? T (xi)
T (xi) otherwise .
We denote the token- and type-constrained lattice as
Y?(x, y?) = T? (x1, y?1)?T? (x2, y?2)?. . .?T? (x|x|, y?|x|).
Note that when token-level projections are not used,
the dictionary-pruned lattice and the lattice with cou-
pled constraints are identical, that is Y?(x, y?) = Y(x).
3.1 HMMs with Coupled Constraints
A first-order hidden Markov model (HMM) specifies
the joint distribution of a sentence x ? X and a
tag-sequence y ? Y(x) as:
p?(x, y) =
|x|?
i=1
p?(xi | yi)? ?? ?
emission
p?(yi | yi?1)? ?? ?
transition
.
We follow the recent trend of using a log-linear
parametrization of the emission and the transition
distributions, instead of a multinomial parametriza-
tion (Chen, 2003). This allows model parameters ?
to be shared across categorical events, which has
been shown to give superior performance (Berg-
Kirkpatrick et al, 2010). The categorical emission
and transition events are represented by feature vec-
tors ?(xi, yi) and ?(yi, yi?1). Each element of the
parameter vector ? corresponds to a particular fea-
ture; the component log-linear distributions are:
p?(xi | yi) =
exp
(
?>?(xi, yi)
)
?
x?i?V exp (?
>?(x?i, yi))
,
and
p?(yi | yi?1) =
exp
(
?>?(yi, yi?1)
)
?
y?i?T exp (?
>?(y?i, yi?1))
.
In maximum-likelihood estimation of the parameters,
we seek to maximize the likelihood of the observed
parts of the data. For this we need the joint marginal
distribution p?(x, Y?(x, y?)) of a sentence x, and its
coupled constraints lattice Y?(x, y?), which is obtained
by marginalizing over all consistent outputs:
p?(x, Y?(x, y?)) =
?
y?Y?(x,y?)
p?(x, y) .
If there are no projections and no tag dictionary, then
Y?(x, y?) = T |x|, and thus p?(x, Y?(x, y?)) = p?(x),
which reduces to fully unsupervised learning. The
`2-regularized marginal joint log-likelihood of the
constrained training data D = {(x(i), y?(i))}ni=1 is:
L(?;D) =
n?
i=1
log p?(x(i), Y?(x(i), y?(i)))?? ???22 .
(1)
We follow Berg-Kirkpatrick et al (2010) and take a
direct gradient approach for optimizing Eq. 1 with
L-BFGS (Liu and Nocedal, 1989). We set ? = 1 and
run 100 iterations of L-BFGS. One could also em-
ploy the Expectation-Maximization (EM) algorithm
(Dempster et al, 1977) to optimize this objective, al-
though the relative merits of EM versus direct gradi-
ent training for these models is still a topic of debate
(Berg-Kirkpatrick et al, 2010; Li et al, 2012).4 Note
that since the marginal likelihood is non-concave, we
are only guaranteed to find a local maximum of Eq. 1.
After estimating the model parameters ?, the tag-
sequence y? ? Y(x) for a sentence x ? X is pre-
dicted by choosing the one with maximal joint prob-
ability:
y? ? arg max
y?Y(x)
p?(x, y) .
4We trained the HMM with EM as well, but achieved better
results with direct gradient training and hence omit those results.
5
3.2 CRFs with Coupled Constraints
Whereas an HMM models the joint probability of
the input x ? X and output y ? Y(x), using locally
normalized component distributions, a conditional
random field (CRF) instead models the probability of
the output conditioned on the input as a globally nor-
malized log-linear distribution (Lafferty et al, 2001):
p?(y | x) =
exp
(
?>?(x, y)
)
?
y??Y(x) exp (?>?(x, y?))
,
where ? is a parameter vector. As for the HMM,
Y(x) is not necessarily the full space of possible
tag-sequences; specifically, for us, it is the dictionary-
pruned lattice without the token constraints.
With a first-order Markov assumption, the feature
function factors as:
?(x, y) =
|x|?
i=1
?(x, yi, yi?1) .
This model is more powerful than the HMM in that
it can use richer feature definitions, such as joint in-
put/transition features and features over a wider input
context. We model a marginal conditional probabil-
ity, given by the total probability of all tag sequences
consistent with the lattice Y?(x, y?):
p?(Y?(x, y?) | x) =
?
y?Y?(x,y?)
p?(y | x) .
The parameters of this constrained CRF are estimated
by maximizing the `2-regularized marginal condi-
tional log-likelihood of the constrained data (Riezler
et al, 2002):
L(?;D) =
n?
i=1
log p?(Y?(x(i), y?(i)) | x(i))? ????22 .
(2)
As with Eq. 1, we maximize Eq. 2 with 100 itera-
tions of L-BFGS and set ? = 1. In contrast to the
HMM, after estimating the model parameters ?, the
tag-sequence y? ? Y(x) for a sentence x ? X is
chosen as the sequence with the maximal conditional
probability:
y? ? arg max
y?Y(x)
p?(y | x) .
4 Empirical Study
We now present a detailed empirical study of the mod-
els proposed in the previous sections. In addition to
comparing with the state of the art in Das and Petrov
(2011) and Li et al (2012), we present models with
several combinations of token and type constraints,
additional features incorporating word clusters. Both
generative and discriminative models are explored.
4.1 Experimental Setup
Before delving into the experimental details, we
present our setup and datasets.
Languages. We evaluate on eight target languages
used in previous work (Das and Petrov, 2011; Li et
al., 2012) and on seven additional languages (see Ta-
ble 1). While the former eight languages all belong to
the Indo-European family, we broaden the coverage
to language families more distant from the source
language (for example, Chinese, Japanese and Turk-
ish). We use the treebanks from the CoNLL shared
tasks on dependency parsing (Buchholz and Marsi,
2006; Nivre et al, 2007) for evaluation.5 The two-
letter abbreviations from the ISO 639-1 standard are
used when referring to these languages in tables and
figures.
Tagset. In all cases, we map the language-specific
POS tags to universal POS tags using the mapping
of Petrov et al (2012).6 Since we use indirect super-
vision via projected tags or Wiktionary, the model
states induced by all models correspond directly to
POS tags, enabling us to compute tagging accuracy
without a greedy 1-to-1 or many-to-1 mapping.
Bitext. For all experiments, we use English as the
source language. Depending on availability, there
are between 1M and 5M parallel sentences for each
language. The majority of the parallel data is gath-
ered automatically from the web using the method
of Uszkoreit et al (2010). We further include data
from Europarl (Koehn, 2005) and from the UN par-
allel corpus (UN, 2006), for languages covered by
these corpora. The English side of the bitext is
POS tagged with a standard supervised CRF tagger,
trained on the Penn Treebank (Marcus et al, 1993),
with tags mapped to universal tags. The parallel sen-
5For French we use the treebank of Abeille? et al (2003).
6We use version 1.03 of the mappings available at http:
//code.google.com/p/universal-pos-tags/.
6
tences are word aligned with the aligner of DeNero
and Macherey (2011). Intersected high-confidence
alignments (confidence >0.95) are extracted and ag-
gregated into projected type-level dictionaries. For
purely practical reasons, the training data with token-
level projections is created by randomly sampling
target-side sentences with a total of 500K tokens.
Wiktionary. We use a snapshot of the Wiktionary
word definitions, and follow the heuristics of Li et
al. (2012) for creating the Wiktionary dictionary by
mapping the Wiktionary tags to universal POS tags.7
Features. For all models, we use only an identity
feature for tag-pair transitions. We use five features
that couple the current tag and the observed word
(analogous to the emission in an HMM): word iden-
tity, suffixes of up to length 3, and three indicator
features that fire when the word starts with a capital
letter, contains a hyphen or contains a digit. These are
the same features as those used by Das and Petrov
(2011). Finally, for some models we add a word
cluster feature that couples the current tag and the
word cluster identity of the word. These (monolin-
gual) word clusters are induced with the exchange
algorithm (Uszkoreit and Brants, 2008). We set the
number of clusters to 256 across all languages, as this
has previously been shown to produce robust results
for similar tasks (Turian et al, 2010; Ta?ckstro?m et
al., 2012). The clusters for each language are learned
on a large monolingual newswire corpus.
4.2 Models with Type Constraints
To examine the sole effect of type constraints, we
experiment with the HMM, drawing constraints from
three different dictionaries. Table 1 compares the per-
formance of our models with the best results of Das
and Petrov (2011, D&P) and Li et al (2012, LG&T).
As in previous work, training is done exclusively on
the training portion of each treebank, stripped of any
manual linguistic annotation.
We first use all of our parallel data to generate
projected tag dictionaries: the English POS tags are
projected across word alignments and aggregated to
tag distributions for each word type. As in Das and
Petrov (2011), the distributions are then filtered with
a threshold of 0.2 to remove noisy tags and to create
7The definitions were downloaded on August 31, 2012 from
http://toolserver.org/?enwikt/definitions/.
This snapshot is more recent than that used by Li et al
Prior work HMM with type constraints
Lang. D&P LG&T YHMMproj. YHMMwik. YHMMunion YHMMunion +C
bg ? ? 84.2 68.1 87.2 87.9
cs ? ? 75.4 70.2 75.4 79.2
da 83.2 83.3 87.7 82.0 78.4 89.5
de 82.8 85.8 86.6 85.1 80.0 88.3
el 82.5 79.2 83.3 83.8 86.0 83.2
es 84.2 86.4 83.9 83.7 88.3 87.3
fr ? ? 88.4 75.7 75.6 86.6
it 86.8 86.5 89.0 85.4 89.9 90.6
ja ? ? 45.2 76.9 74.4 73.7
nl 79.5 86.3 81.7 79.1 83.8 82.7
pt 87.9 84.5 86.7 79.0 83.8 90.4
sl ? ? 78.7 64.8 82.8 83.4
sv 80.5 86.1 80.6 85.9 85.9 86.7
tr ? ? 66.2 44.1 65.1 65.7
zh ? ? 59.2 73.9 63.2 73.0
avg (8) 83.4 84.8 84.9 83.0 84.5 87.3
avg ? ? 78.5 75.9 80.0 83.2
Table 1: Tagging accuracies for type-constrained HMM
models. D&P is the ?With LP? model in Table 2 of
Das and Petrov (2011), while LG&T is the ?SHMM-ME?
model in Table 2 of Li et al (2012). YHMMproj. , YHMMwik. and
YHMMunion are HMMs trained solely with type constraints
derived from the projected dictionary, Wiktionary and
the union of these dictionaries, respectively. YHMMunion +C is
equivalent to YHMMunion with additional cluster features. All
models are trained on the treebank of each language,
stripped of gold labels. Results are averaged over the
8 languages from Das and Petrov (2011), denoted avg (8),
as well as over the full set of 15 languages, denoted avg.
an unweighted tag dictionary. We call this model
YHMMproj. ; its average accuracy of 84.9% on the eight
languages is higher than the 83.4% of D&P and on
par with LG&T (84.8%).8 Our next model (YHMMwik. )
simply draws type constraints from Wiktionary. It
slightly underperforms LG&T (83.0%), presumably
because they used a second-order HMM. As a simple
extension to these two models, we take the union
of the projected dictionary and Wiktionary to con-
strain an HMM, which we name YHMMunion . This model
performs a little worse on the eight Indo-European
languages (84.5), but gives an improvement over the
projected dictionary when evaluated across all 15
languages (80.0% vs. 78.5%).
8Our model corresponds to the weaker, ?No LP? projection
of Das and Petrov (2011). We found that label propagation was
only beneficial when small amounts of bitext were available.
7
Token constraints HMM with coupled constraints CRF with coupled constraints
Lang. YHMMunion +C+L y?HMM+C+L y?CRF+C+L Y?HMMproj. +C+L Y?HMMwik. +C+L Y?HMMunion +C+L Y?CRFproj. +C+L Y?CRFwik. +C+L Y?CRFunion+C+L
bg 87.7 77.9 84.1 84.5 83.9 86.7 86.0 87.8 85.4
cs 78.3 65.4 74.9 74.8 81.1 76.9 74.7 80.3** 75.0
da 87.3 80.9 85.1 87.2 85.6 88.1 85.5 88.2* 86.0
de 87.7 81.4 83.3 85.0 89.3 86.7 84.4 90.5** 85.5
el 85.9 81.1 77.8 80.1 87.0 83.9 79.6 89.5** 79.7
es 89.1** 84.1 85.5 83.7 85.9 88.0 85.7 87.1 86.0
fr 88.4** 83.5 84.7 85.9 86.4 87.4 84.9 87.2 85.6
it 89.6 85.2 88.5 88.7 87.6 89.8 88.3 89.3 89.4
ja 72.8 47.6 54.2 43.2 76.1 70.5 44.9 81.0** 68.0
nl 83.1 78.4 82.4 82.3 84.2 83.2 83.1 85.9** 83.2
pt 89.1 84.7 87.0 86.6 88.7 88.0 87.9 91.0** 88.3
sl 82.4 69.8 78.2 78.5 81.8 80.1 79.7 82.3 80.0
sv 86.1 80.1 84.2 82.3 87.9 86.9 84.4 88.9** 85.5
tr 62.4 58.1 64.5 64.6 61.8 64.8 65.0 64.1** 65.2
zh 72.6 52.7 39.5 56.0 74.1 73.3 59.7 74.4** 73.4
avg (8) 87.2 82.0 84.2 84.5 87.0 86.8 84.9 88.8 85.4
avg 82.8 74.1 76.9 77.6 82.8 82.3 78.2 84.5 81.1
Table 2: Tagging accuracies for models with token constraints and coupled token and type constraints. All models use
cluster features (. . . +C) and are trained on large training sets each containing 500k tokens with (partial) token-level
projections (. . . +L). The best type-constrained model, trained on the larger datasets, YHMMunion +C+L, is included for
comparison. The remaining columns correspond to HMM and CRF models trained only with token constraints (y? . . .)
and with coupled token and type constraints (Y? . . .). The latter are trained using the projected dictionary (?proj.),
Wiktionary (?wik.) and the union of these dictionaries (?union), respectively. The search spaces of the models trained with
coupled constraints (Y? . . .) are each pruned with the respective tag dictionary used to derive the coupled constraints.
The observed difference between Y?CRFwik. +C+L and YHMMunion +C+L is statistically significant at p < 0.01 (**) and p < 0.015(*) according to a paired bootstrap test (Efron and Tibshirani, 1993). Significance was not assessed for avg or avg (8).
We next add monolingual cluster features to
the model with the union dictionary. This model,
YHMMunion +C, significantly outperforms all other type-
constrained models, demonstrating the utility of
word-cluster features.9 For further exploration, we
train the same model on the datasets containing 500K
tokens sampled from the target side of the parallel
data (YHMMunion +C+L); this is done to explore the effects
of large data during training. We find that training
on these datasets result in an average accuracy of
87.2% which is comparable to the 87.3% reported
for YHMMunion +C in Table 1. This shows that the different
source domain and amount of training data does not
influence the performance of the HMM significantly.
Finally, we train CRF models where we treat type
constraints as a partially observed lattice and use the
full unpruned lattice for computing the partition func-
9These are monolingual clusters. Bilingual clusters as intro-
duced in Ta?ckstro?m et al (2012) might bring additional benefits.
tion (?3.2). Due to space considerations, the results
of these experiments are not shown in table 1. We ob-
serve similar trends in these results, but on average,
accuracies are much lower compared to the type-
constrained HMM models; the CRF model with the
union dictionary along with cluster features achieves
an average accuracy of 79.3% when trained on same
data. This result is not unsurprising. First, the CRF?s
search space is fully unconstrained. Second, the dic-
tionary only provides a weak set of observation con-
straints, which do not provide sufficient information
to successfully train a discriminative model. How-
ever, as we will observe next, coupling the dictionary
constraints with token-level information solves this
problem.
4.3 Models with Token and Type Constraints
We now proceed to add token-level information,
focusing in particular on coupled token and type
8
constraints. Since it is not possible to generate
projected token constraints for our monolingual
treebanks, we train all models in this subsection
on the 500K-tokens datasets sampled from the bi-
text. As a baseline, we first train HMM and CRF
models that use only projected token constraints
(y?HMM+C+L and y?CRF+C+L). As shown in Table 2,
these models underperform the best type-level model
(YHMMunion +C+L),10 which confirms that projected to-
ken constraints are not reliable on their own. This
is in line with similar projection models previously
examined by Das and Petrov (2011).
We then study models with coupled token and type
constraints. These models use the same three dictio-
naries as used in ?4.2, but additionally couple the
derived type constraints with projected token con-
straints; see the caption of Table 2 for a list of these
models. Note that since we only allow projected tags
that are licensed by the dictionary (Step 3 of the trans-
fer, ?2.3), the actual token constraints used in these
models vary with the different dictionaries.
From Table 2, we see that coupled constraints are
superior to token constraints, when used both with
the HMM and the CRF. However, for the HMM, cou-
pled constraints do not provide any benefit over type
constraints alone, in particular when the projected
dictionary or the union dictionary is used to derive the
coupled constraints (Y?HMMproj. +C+L and Y?HMMunion +C+L).
We hypothesize that this is because these dictionar-
ies (in particular the former) have the same bias as
the token-level tag projections, so that the dictionary
is unable to correct the systematic errors in the pro-
jections (see ?2.1). Since the token constraints are
stronger than the type constraints in the coupled mod-
els, this bias may have a substantial impact. With
the Wiktionary dictionary, the difference between the
type-constrained and the coupled-constrained HMM
is negligible: YHMMunion +C+L and Y?HMMwik. +C+L both av-
erage at an accuracy of 82.8%.
The CRF model, on the other hand, is able to take
advantage of the complementary information in the
coupled constraints, provided that the dictionary is
able to filter out the systematic token-level errors.
With a dictionary derived from Wiktionary and pro-
jected token-level constraints, Y?CRFwik. +C+L performs
10To make the comparison fair vis-a-vis potential divergences
in training domains, we compare to the best type-constrained
model trained on the same 500K tokens training sets.
0 1 2 3
0
25
50
75
100
0 1 10 100 0 1 10 100 0 1 10 100 0 1 10 100
Number of token?level projections
Ta
gg
ing
 ac
cur
ac
y
Number of tags listed in Wiktionary
Figure 4: Relative influence of token and type constraints
on tagging accuracy in the Y?CRFwik. +C+L model. Word typesare categorized according to a) their number of Wiktionary
tags (0,1,2 or 3+ tags, with 0 representing no Wiktionary
entry; top-axis) and b) the number of times they are token-
constrained in the training set (divided into buckets of
0, 1-9, 10-99 and 100+ occurrences; x-axis). The boxes
summarize the accuracy distributions across languages
for each word type category as defined by a) and b). The
horizontal line in each box marks the median accuracy,
the top and bottom mark the first and third quantile, re-
spectively, while the whiskers mark the minimum and
maximum values of the accuracy distribution.
better than all the remaining models, with an average
accuracy of 88.8% across the eight Indo-European
languages available to D&P and LG&T. Averaged
over all 15 languages, its accuracy is 84.5%.
5 Further Analysis
In this section we provide a detailed analysis of the
impact of token versus type constraints and we study
the pruning and filtering mistakes resulting from in-
complete Wiktionary entries in detail. This analysis
is based on the training portion of each treebank.
5.1 Influence of Token and Type Constraints
The empirical success of the model trained with cou-
pled token and type constraints confirms that these
constraints indeed provide complementary signals.
Figure 4 provides a more detailed view of the rela-
tive benefits of each type of constraint. We observe
several interesting trends.
First, word types that occur with more token con-
straints during training are generally tagged more
accurately, regardless of whether these types occur
9
90.0
92.5
95.0
97.5
100.0
0 50 100 150 200 250
Number of corrected Wiktionary entries
Pr
un
ing
 ac
cur
ac
y
Figure 5: Average pruning accuracy (line) across lan-
guages (dots) as a function of the number of hypotheti-
cally corrected Wiktionary entries for the k most frequent
word types. For example, position 100 on the x-axis cor-
responds to manually correcting the entries for the 100
most frequent types, while position 0 corresponds to ex-
perimental conditions.
in Wiktionary. The most common scenario is for a
word type to have exactly one tag in Wiktionary and
to occur with this projected tag over 100 times in
the training set (facet 1, rightmost box). These com-
mon word types are typically tagged very accurately
across all languages.
Second, the word types that are ambiguous accord-
ing to Wiktionary (facets 2 and 3) are predominantly
frequent ones. The accuracy is typically lower for
these words compared to the unambiguous words.
However, as the number of projected token con-
straints is increased from zero to 100+ observations,
the ambiguous words are effectively disambiguated
by the token constraints. This shows the advantage
of intersecting token and type constraints.
Finally, projection generally helps for words that
are not in Wiktionary, although the accuracy for these
words never reach the accuracy of the words with
only one tag in Wiktionary. Interestingly, words that
occur with a projected tag constraint less than 100
times are tagged more accurately for types not in the
dictionary compared to ambiguous word types with
the same number of projected constraints. A possible
explanation for this is that the ambiguous words are
inherently more difficult to predict and that most of
the words that are not in Wiktionary are less common
words that tend to also be less ambiguous.
zh
tr
sv
sl
pt
nl
jait
fr
es
el
de
da
cs
bg
avg
0 25 50 75 100
Proportion of pruning errors
PRON
NOUN
DET
ADP
PRT
ADV
NUM
CONJ
ADJ
VERB
X
.
Figure 6: Prevalence of pruning mistakes per POS tag,
when pruning the inference search space with Wiktionary.
5.2 Wiktionary Pruning Mistakes
The error analysis by Li et al (2012) showed that the
tags licensed by Wiktionary are often valid. When
using Wiktionary to prune the search space of our
constrained models and to filter token-level projec-
tions, it is also important that correct tags are not
mistakenly pruned because they are missing from
Wiktionary. While the accuracy of filtering is more
difficult to study, due to the lack of a gold standard
tagging of the bitext, Figure 5 (position 0 on the x-
axis) shows that search space pruning errors are not
a major issue for most languages; on average the
pruning accuracy is almost 95%. However, for some
languages such as Chinese and Czech the correct tag
is pruned from the search space for nearly 10% of all
tokens. When using Wiktionary as a pruner, the upper
bound on accuracy for these languages is therefore
only around 90%. However, Figure 5 also shows that
with some manual effort we might be able to remedy
many of these errors. For example, by adding miss-
ing valid tags to the 250 most common word types in
the worst language, the minimum pruning accuracy
would rise above 95% from below 90%. If the same
was to be done for all of the studied languages, the
mean pruning accuracy would reach over 97%.
Figure 6 breaks down pruning errors resulting from
incorrect or incomplete Wiktionary entries across
the correct POS tags. From this we observe that,
for many languages, the pruning errors are highly
skewed towards specific tags. For example, for Czech
over 80% of the pruning errors are caused by mistak-
enly pruned pronouns.
10
6 Conclusions
We considered the problem of constructing multilin-
gual POS taggers for resource-poor languages. To
this end, we explored a number of different models
that combine token constraints with type constraints
from different sources. The best results were ob-
tained with a partially observed CRF model that ef-
fectively integrates these complementary constraints.
In an extensive empirical study, we showed that this
approach substantially improves on the state of the
art in this context. Our best model significantly out-
performed the second-best model on 10 out of 15
evaluated languages, when trained on identical data
sets, with an insignificant difference on 3 languages.
Compared to the prior state of the art (Li et al, 2012),
we observed a relative reduction in error by 25%,
averaged over the eight languages common to our
studies.
Acknowledgments
We thank Alexander Rush for help with the hyper-
graph framework that was used to implement our
models and Klaus Macherey for help with the bi-
text extraction. This work benefited from many dis-
cussions with Yoav Goldberg, Keith Hall, Kuzman
Ganchev and Hao Zhang. We also thank the editor
and the three anonymous reviewers for their valuable
feedback. The first author is grateful for the financial
support from the Swedish National Graduate School
of Language Technology (GSLT).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a Treebank for French. In A. Abeille?,
editor, Treebanks: Building and Using Parsed Corpora,
chapter 10. Kluwer.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?, John
DeNero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proceedings of NAACL-HLT.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Stanley F Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
Eurospeech.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin.
1977. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39.
John DeNero and Klaus Macherey. 2011. Model-based
aligner combination using dual decomposition. In Pro-
ceedings of ACL-HLT.
Brad Efron and Robert J. Tibshirani. 1993. An Introduc-
tion to the Bootstrap. Chapman & Hall, New York, NY,
USA.
Victoria Fossum and Steven Abney. 2005. Automatically
inducing a part-of-speech tagger by projecting from
multiple source languages across aligned corpora. In
Proceedings of IJCNLP.
Dan Garrette and Jason Baldridge. 2012. Type-supervised
hidden markov models for part-of-speech tagging with
incomplete tag dictionaries. In Proceedings of EMNLP-
CoNLL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings of ACL-HLT.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of ICML.
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP-CoNLL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2).
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. JAIR, 36.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of ACL-IJCNLP.
11
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, III, and Mark Johnson. 2002.
Parsing the wall street journal using a lexical-functional
grammar and discriminative estimation techniques. In
Proceedings of ACL.
Noah Smith and Jason Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data. In
Proceedings of ACL.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
UN. 2006. ODS UN parallel corpus.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of COLING.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-English languages.
In Proceedings of HLT-EMNLP.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL.
12
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 264?267,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SEMAFOR: Frame Argument Resolution with Log-Linear Models
Desai Chen Nathan Schneider Dipanjan Das Noah A. Smith
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
{desaic@andrew,dipanjan@cs,nschneid@cs,nasmith@cs}.cmu.edu
Abstract
This paper describes the SEMAFOR sys-
tem?s performance in the SemEval 2010
task on linking events and their partici-
pants in discourse. Our entry is based
upon SEMAFOR 1.0 (Das et al, 2010a),
a frame-semantic probabilistic parser built
from log-linear models. The extended sys-
tem models null instantiations, including
non-local argument reference. Performance
is evaluated on the task data with and with-
out gold-standard overt arguments. In both
settings, it fares the best of the submitted
systems with respect to recall and F
1
.
1 Introduction
The theory of frame semantics (Fillmore, 1982)
holds that meaning is largely structured by holis-
tic units of knowledge, called frames. Each frame
encodes a conventionalized gestalt event or sce-
nario, often with conceptual dependents (partic-
ipants, props, or attributes) filling roles to elab-
orate the specific instance of the frame. In the
FrameNet lexicon (Fillmore et al, 2003), each
frame defines core roles tightly coupled with
the particular meaning of the frame, as well as
more generic non-core roles (Ruppenhofer et al,
2006). Frames can be evoked with linguistic pred-
icates, known as lexical units (LUs); role fillers
can be expressed overtly and linked to the frame
via (morpho)syntactic constructions. However, a
great deal of conceptually-relevant content is left
unexpressed or is not explicitly linked to the frame
via linguistic conventions; rather, it is expected
that the listener will be able to infer the appro-
priate relationships pragmatically. Certain types
of implicit content and implicit reference are for-
malized in the theory of null instantiations (NIs)
(Fillmore, 1986; Ruppenhofer, 2005). A complete
frame-semantic analysis of text thus incorporates
covert and overt predicate-argument information.
In this paper, we describe a system for frame-
semantic analysis, evaluated on a semantic role
labeling task for explicit and implicit arguments
(?2). Extending the SEMAFOR 1.0 frame-
semantic parser (Das et al, 2010a; outlined in ?3),
we detect null instantiations via a simple two-stage
pipeline: the first stage predicts whether a given
role is null-instantiated, and the second stage (?4)
predicts how it is null-instantiated, if it is not overt.
We report performance on the SemEval 2010 test
set under the full-SRL and NI-only conditions.
2 Data
The SemEval 2007 task on frame-semantic pars-
ing (Baker et al, 2007) provided a small (about
50,000 words and 2,000 sentences) dataset of
news text, travel guides, and bureaucratic accounts
of weapons stockpiles. Sentences in this dataset
were fully annotated with frames and their argu-
ments. The SemEval 2010 task (Ruppenhofer et
al., 2010) adds annotated data in the fiction do-
main: parts of two Sherlock Holmes stories by
Arthur Conan Doyle. The SemEval 2010 train-
ing set consists of the SemEval 2007 data plus
one document from the new domain. This doc-
ument has about 7800 words in 438 sentences;
it has 1492 annotated frame instances, including
3169 (overt and null-instantiated) argument anno-
tations. The test set consists of two chapters from
another story: Chapter 13 contains about 4000
words, 249 sentences, and 791 frames; Chapter 14
contains about 5000 words, 276 sentences, and
941 frames (see also Table 3). Figure 1 shows
two annotated test sentences. All data released for
the 2010 task include part-of-speech tags, lemmas,
and phrase-structure trees from a parser, with head
annotations for constituents.
3 Argument identification
Our starting point is SEMAFOR 1.0 (Das et
al., 2010a), a discriminative probabilistic frame-
semantic parsing model that operates in three
stages: (a) rule-based target selection, (b) proba-
bilistic disambiguation that resolves each target to
a FrameNet frame, and (c) joint selection of text
spans to fill the roles of each target through a sec-
ond probabilistic model.
1
1
Das et al (2010a) report the performance of this system
on the complete SemEval 2007 task at 46.49% F
1
.
264
`` I         THINK   that I shall be in a position to     MAKE   the situation rather   more   CLEAR to you before long .
It has been an exceedingly DIFFICULT and most complicated  business .
DIFFICULTY
difficult.a
Degree
Activity
OPINION
think.v
CAUSATION
make.v
OBVIOUSNESS
clear.n
Experiencer
Cognizer Opinion
Actor Effect
Experiencer
Degree AttributePhenomenon
Figure 1. Two consecutive sentences
in the test set, with frame-semantic an-
notations. Shaded regions represent
frames: they include the target word in
the sentence, the corresponding frame
name and lexical unit, and arguments.
Horizontal bars mark gold argument
spans?white bars are gold annotations
and black bars show mistakes of our
NI-only system.
Chapter 13 Chapter 14
Training Data Prec. Rec. F
1
Prec. Rec. F
1
SemEval 2010 data (includes SemEval 2007 data) 0.69 0.50 0.58 0.66 0.48 0.56
SemEval 2007 data + 50% new, in-domain data 0.68 0.47 0.55 0.66 0.45 0.54
SemEval 2007 data only 0.67 0.41 0.50 0.64 0.40 0.50
Table 1. Overt
argument labeling
performance.
Stage (c), known as argument identification or
SRL, is most relevant here. In this step, the system
takes the target (frame-evoking) phrase t and cor-
responding frame type f predicted by the previous
stages, and independently fills each role of f with
a word or phrase from the sentence, or the sym-
bol OTHER to indicate that the role has no (local)
overt argument. Features used to inform this de-
cision include aspects of the syntactic dependency
parse (e.g. the path in the parse from the target
to the argument); voice; word overlap of the argu-
ment with respect to the target; and part-of-speech
tags within and around the argument. SEMAFOR
as described in (Das et al, 2010a) does not dis-
tinguish between different types of null instantia-
tions or find non-local referents. Given perfect
input to stage (c), the system achieved 68.5% F
1
on the SemEval 2007 data (exact match, evaluat-
ing overt arguments only). The only difference
in our use of SEMAFOR?s argument identification
module is in preprocessing the training data: we
use dependency parses transformed from the head-
augmented phrase-structure parses in the task data.
Table 1 shows the performance of our argument
identification model on this task?s test data. The
SRL systems compared in (Ruppenhofer et al,
2010) all achieved precision in the mid 60% range,
but SEMAFOR achieved substantially higher re-
call, F
1
, and label accuracy on this subtask. (The
table also shows how performance of our model
degrades when half or all of the new data are not
used for training; the 9% difference in recall sug-
gests the importance of in-domain training data.)
4 Null instantiation detection
In this subtask, which follows the argument iden-
tification subtask (?3), our system seeks to char-
acterize non-overt core roles given gold standard
local frame-argument annotations. Consider the
following passage from the test data:
?That?s lucky for him?in fact, it?s lucky for all
of you, since you are all on the wrong side of the
law in this matter. I am not sure that as a consci-
entious detective [
Authorities
my] first duty is not to
arrest [
Suspect
the whole household]. [
DNI
Charges
?]
The frame we are interested in, ARREST, has four
core roles, two of which (Authorities and Sus-
pect) have overt (local) arguments. The third core
role, Charges, is annotated as having anaphoric
or definite null instantiation (DNI). ?Definite?
means that the discourse implies a specific referent
that should be recoverable from context, without
marking that referent linguistically. Some DNIs in
the data are linked to phrases in syntactically non-
local positions, such as in another sentence (see
Figure 1). This one is not (though our model in-
correctly labels this matter from the previous sen-
tence as a DNI referent for this role). The fourth
core role, Offense, is not annotated as a null in-
stantiation because it belongs to the same CoreSet
as Charges?which is to say they are relevant in
a similar way to the frame as a whole (both pertain
to the rationale for the arrest) and only one is typ-
ically expressed.
2
We will use the term masked
to refer to any non-overt core role which does not
need to be specified as null-instantiated due to a
structural connection to another role in its frame.
The typology of NIs given in Ruppenhofer
(2005) and employed in the annotation distin-
guishes anaphoric/definite NIs from existential or
indefinite null instantiations (INIs). Rather than
having a specific referent accessible in the dis-
course, INIs are left vague or deemphasized, as in
2
If the FrameNet lexicon marks a pair of roles within a
frame as being in a CoreSet or Excludes relationship, then
filling one of them satisfies the requirement that the other be
(expressly or implicitly) present in the use of the frame.
265
Chapter 13 Chapter 14
Training Data Prec. Rec. F
1
Prec. Rec. F
1
N
I
-
o
n
l
y
SemEval 2010 new: 100% 0.40 0.64 0.50 0.53 0.60 0.56
SemEval 2010 new: 75% 0.66 0.37 0.50 0.70 0.37 0.48
SemEval 2010 new: 50% 0.73 0.38 0.51 0.75 0.35 0.48
Full All 0.35 0.55 0.43 0.56 0.49 0.52
Table 2. Performance on the
full task and the NI-only task.
The NI model was trained on the
new SemEval 2010 document, ?The
Tiger of San Pedro? (data from the
2007 task was excluded because
none of the null instantiations in that
data had annotated referents).
Predicted
overt DNI INI masked inc. total
G
o
l
d
overt 2068 (1630) 5 362 327 0 2762
DNI 64 12 (3) 182 90 0 348
INI 41 2 214 96 0 353
masked 73 0 240 1394 0 1707
inc. 12 2 55 2 0 71
total 2258 21 1053 1909 0 3688 correct
Table 3. Instantiation type confusion ma-
trix for the full model (argument identifi-
cation plus NI detection). Parenthesized
numbers count the predictions of the cor-
rect type which also predicted the same
(argument or referent) span. On the NI-
only task, our system has a similar distri-
bution of NI detection errors.
the thing(s) eaten in the sentence We ate.
The problem can be decomposed into two steps:
(a) classifying each null instantiation as definite,
indefinite, or masked; and (b) resolving the DNIs,
which entails finding referents in the non-local
context. Instead, our model makes a single NI pre-
diction for any role that received no local argument
(OTHER) in the argument identification phase (?3),
thereby combining classification and resolution.
3
4.1 Model
Our model for this subtask is analogous to the ar-
gument identification model: it chooses one from
among many possible fillers for each role. How-
ever, whereas the argument identification model
considers parse constituents as potential local
fillers (which might constitute an overt argument
within the sentence) along with a special category,
OTHER, here the set of candidate fillers consists of
phrases from outside the sentence, along with spe-
cial categories INI or MASKED. When selected, a
non-local phrase will be interpreted as a non-local
argument and labeled as a DNI referent.
These non-local candidate fillers are handled
differently from candidates within the sentence
considered in the argument identification model:
they are selected using more restrictive criteria,
and are associated with a different set of features.
Restricted search space for DNI referents. We
consider nouns, pronouns, and noun phrases from
the previous three sentences as candidate DNI ref-
erents. This narrows the search space considerably
to make learning tractable, but at a cost: many
gold DNI referents will not even be considered.
In the training data, there are about 250 DNI in-
stances with explicit referents; their distribution is
3
Investigation of separate modeling is left to future work.
chaotic.
4
Judging by the training data, our heuris-
tics thus limit oracle recall to about 20% of DNIs.
5
Modified feature set. Since it is not obvious how
to calculate a syntactic path between two words
in different sentences, we replaced dependency
path features with simpler features derived from
FrameNet?s lexicographic exemplar annotations.
For each candidate span, we use two types of fea-
tures to model the affinity between the head word
and the role. The first indicates whether the head
word is used as a filler for this role in at least
one of the lexicographic exemplars. The second
encodes the maximum distributional similarity to
any word heading a filler of that role in the ex-
emplars.
6
In practice, we found that these fea-
tures received negligible weight and had virtually
no effect on performance, possibly due to data
sparseness. An additional change in the feature
set is that ordering/distance features (Das et al,
2010b, p. 13) were replaced with a feature indicat-
ing the number of sentences away the candidate
is from the target.
7
Otherwise, the null identifica-
4
91 DNI referents are found no more than three sentences
prior; another 90 are in the same sentence as the target. 20
DNIs have referents which are not noun phrases. Six appear
after the sentence containing its frame target; 28 appear at
least 25 sentences prior. 60 have no referent.
5
Our system ignores DNIs with no referent or with a ref-
erent in the same sentence as the target. Experiments with
variants on these assumptions show that the larger the search
space (i.e. the more candidate DNI referents are under con-
sideration), the worse the trained model performs at distin-
guishing NIs from non-NIs (though DNI vs. INI precision
improves). This suggests that data sparseness is hindering
our system?s ability to learn useful generalizations about NIs.
6
Distributional similarity scores are obtained
from D. Lin?s Proximity-based Thesaurus (http:
//webdocs.cs.ualberta.ca/~lindek/
Downloads/sims.lsp.gz) and quantized into bi-
nary features for intervals: [0, .03), [.03, .06), [.06, .08),
[.08,?).
7
All of the new features are instantiated in three forms:
266
tion model uses the same features as the argument
identification model.
The theory of null instantiations holds that the
grammaticality of lexically-licensed NI for a role
in a given frame depends on the LU: for exam-
ple, the verbs buy and sell share the same frame
but differ as to whether the Buyer or Seller role
may be lexically null-instantiated. Our model?s
feature set is rich enough to capture this in a soft
way, with lexicalized features that fire, e.g., when
the Seller role is null-instantiated and the target
is buy. Moreover, (Ruppenhofer, 2005) hypoth-
esizes that each role has a strong preference for
one interpretation (INI or DNI) when it is lexically
null-instantiated, regardless of LU. This, too, is
modeled in our feature set. In theory these trends
should be learnable given sufficient data, though it
is doubtful that there are enough examples of null
instantiations in the currently available dataset for
this learning to take place.
4.2 Evaluation
We trained the model on the non-overt arguments
in the new SemEval 2010 training document,
which has 580 null instantiations?303 DNIs and
277 INIs.
8,9
Then we used the task scoring proce-
dure to evaluate the NI detection subtask in isola-
tion (given gold-standard overt arguments) as well
as the full task (when this module is combined in a
pipeline with argument identification). Results are
shown in Table 2.
10
Table 3 provides a breakdown of our sys-
tem?s predictions on the test data by instantiation
type: overt local arguments, DNIs, INIs, and the
MASKED category (marking the role as redundant
or irrelevant for the particular use of the frame,
given the other arguments). It also shows counts
for incorporated (?inc.?) roles, which are filled by
the frame-evoking target, e.g. clear in Figure 1.
11
This table shows that the system is reasonably ef-
fective at discriminating NIs from masked roles,
one specific to the frame and the role, one specific to the role
name only, and one to learn the overall bias of the data.
8
For feature engineering we held out the last 25% of sen-
tences from the new training document as development data,
retraining on the full training set for final evaluation.
9
We used Nils Reiter?s FrameNet API, version 0.4
(http://www.cl.uni-heidelberg.de/trac/
FrameNetAPI) in processing the data.
10
The other system participating in the NI-only subtask
had much lower NI recall of 8% (Ruppenhofer et al, 2010).
11
We do not predict any DNIs without referents or in-
corporated roles, though the evaluation script gives us credit
when we predict INI for these cases.
but DNI identification suffers from low recall and
INI identification from low precision. Data sparse-
ness is likely the biggest obstacle here. To put this
in perspective, there are over 20,000 training ex-
amples of overt arguments, but fewer than 600 ex-
amples of null instantiations, two thirds of which
do not have referents. Without an order of mag-
nitude more NI data (at least), it is unlikely that
a supervised learner could generalize well enough
to recognize on new data null instantiations of the
over 7000 roles in the lexicon.
5 Conclusion
We have described a system that implements a
clean probabilistic model of frame-semantic struc-
ture, considering overt arguments as well as var-
ious forms of null instantion of roles. The sys-
tem was evaluated on SemEval 2010 data, with
mixed success at detecting null instantiations. We
believe in-domain data sparseness is the predom-
inant factor limiting the robustness of our super-
vised model.
Acknowledgments
This work was supported by DARPA grant
NBCH-1080004 and computational resources
provided by Yahoo. We thank the task organizers for
providing data and conducting the evaluation, and two
reviewers for their comments.
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-2007
Task 19: Frame Semantic Structure Extraction. In Proc.
of SemEval.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010a.
Probabilistic frame-semantic parsing. In Proc. of NAACL-
HLT.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010b.
SEMAFOR 1.0: A probabilistic frame-semantic parser.
Technical Report CMU-LTI-10-001, Carnegie Mellon
University.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of Lexi-
cography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in the
Morning Calm, pages 111?137. Hanshin Publishing Co.,
Seoul, South Korea.
C. J. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proc. of Berkeley Linguistics Society, pages
95?107, Berkeley, CA.
J. Ruppenhofer, M. Ellsworth, M. R.L. Petruck, C. R. John-
son, and J. Scheffczyk. 2006. FrameNet II: extended the-
ory and practice.
J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker, and
M. Palmer. 2010. SemEval-2010 Task 10: Linking
Events and Their Participants in Discourse. In Proc. of
SemEval.
J. Ruppenhofer. 2005. Regularities in null instantiation.
267
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 209?217,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
An Exact Dual Decomposition Algorithm
for Shallow Semantic Parsing with Constraints
Dipanjan Das? Andre? F. T. Martins?? Noah A. Smith?
?Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{dipanjan,afm,nasmith}@cs.cmu.edu
Abstract
We present a novel technique for jointly predict-
ing semantic arguments for lexical predicates. The
task is to find the best matching between seman-
tic roles and sentential spans, subject to struc-
tural constraints that come from expert linguistic
knowledge (e.g., in the FrameNet lexicon). We
formulate this task as an integer linear program
(ILP); instead of using an off-the-shelf tool to
solve the ILP, we employ a dual decomposition
algorithm, which we adapt for exact decoding via
a branch-and-bound technique. Compared to a
baseline that makes local predictions, we achieve
better argument identification scores and avoid all
structural violations. Runtime is nine times faster
than a proprietary ILP solver.
1 Introduction
Semantic knowledge is often represented declara-
tively in resources created by linguistic experts. In
this work, we strive to exploit such knowledge in
a principled, unified, and intuitive way. An ex-
ample resource where a wide variety of knowledge
has been encoded over a long period of time is the
FrameNet lexicon (Fillmore et al, 2003),1 which
suggests an analysis based on frame semantics (Fill-
more, 1982). This resource defines hundreds of
semantic frames. Each frame represents a gestalt
event or scenario, and is associated with several se-
mantic roles, which serve as participants in the event
that the frame signifies (see Figure 1 for an exam-
ple). Along with storing the above data, FrameNet
also provides a hierarchy of relationships between
frames, and semantic relationships between pairs of
roles. In prior NLP research using FrameNet, these
interactions have been largely ignored, though they
1http://framenet.icsi.berkeley.edu
have the potential to improve the quality and consis-
tency of semantic analysis.
In this paper, we present an algorithm that finds
the full collection of arguments of a predicate given
its semantic frame. Although we work within the
conventions of FrameNet, our approach is general-
izable to other semantic role labeling (SRL) frame-
works. We model this argument identification task
as constrained optimization, where the constraints
come from expert knowledge encoded in a lexi-
con. Following prior work on PropBank-style SRL
(Kingsbury and Palmer, 2002) that dealt with simi-
lar constrained problems (Punyakanok et al, 2004;
Punyakanok et al, 2008, inter alia), we incorporate
this declarative knowledge in an integer linear pro-
gram (ILP).
Because general-purpose ILP solvers are propri-
etary and do not fully exploit the structure of the
problem, we turn to a class of optimization tech-
niques called dual decomposition (Komodakis et
al., 2007; Rush et al, 2010; Martins et al, 2011a).
We derive a modular, extensible, parallelizable ap-
proach in which semantic constraints map not just
to declarative components of the algorithm, but also
to procedural ones, in the form of ?workers.? While
dual decomposition algorithms only solve a relax-
ation of the original problem, we make a novel con-
tribution by wrapping the algorithm in a branch-and-
bound search procedure, resulting in exact solutions.
We experimentally find that our algorithm
achieves accuracy comparable to a state-of-the-art
system, while respecting all imposed linguistic con-
straints. In comparison to inexact beam search that
violates many of these constraints, our exact decoder
has less than twice the runtime; furthermore, it de-
codes nine times faster than CPLEX, a state-of-the-
art, proprietary, general-purpose exact ILP solver.
209
Austria , once expected to waltz smoothly into the European Union , is elbowing its partners , 
treading on toes and pogo-dancing in a most un-Viennese manner .
SELF_MOTION COLLABORATION
CONDUCTGoalManner Partner_1 Partner_2MannerAgentSelf_mover
Figure 1: An example sentence from the annotations released as part of FrameNet 1.5 with three predicates marked in
bold. Each predicate has its evoked semantic frame marked above it, in a distinct color. For each frame, its semantic
roles are shown in the same color, and the spans fulfilling the roles are underlined. For example, manner evokes the
CONDUCT frame, and has the Agent and Manner roles fulfilled by Austria and most un-Viennese respectively.
2 Collective Argument Identification
Here, we take a declarative approach to modeling
argument identification using an ILP and relate our
formulation to prior work in shallow semantic pars-
ing. We show how knowledge specified in a lin-
guistic resource can be used to derive the constraints
used in our ILP. Finally, we draw connections of our
specification to graphical models, a popular formal-
ism in AI, and describe how the constraints can be
treated as factors in a factor graph.
2.1 Declarative Specification
Let us denote a predicate by t and the semantic
frame it evokes within a sentence x by f . In this
work, we assume that the semantic frame f is given,
which is traditionally the case in controlled exper-
iments used to evaluate SRL systems (Ma`rquez et
al., 2008). Given the semantic frame of a predicate,
the semantic roles that might be filled are assumed
to be given by the lexicon (as in PropBank and
FrameNet). Let the set of roles associated with the
frame f be Rf . In sentence x, the set of candidate
spans of words that might fill each role is enumer-
ated, usually following an overgenerating heuristic;2
let this set of spans be St. We include the null span ?
in St; connecting it to a role r ? Rf denotes that the
role is not overt. Our approach assumes a scoring
function that gives a strength of association between
roles and candidate spans. For each role r ? Rf and
span s ? St, this score is parameterized as:
c(r, s) = ? ? h(t, f,x, r, s), (1)
where ? are model weights and h is a feature func-
tion that looks at the predicate t, the evoked frame
f , sentence x, and its syntactic analysis, along with
2Here, as in most SRL literature, role fillers are assumed to be
expressed as contiguous spans, though such an assumption is
easy to relax in our framework.
r and s. The SRL literature provides many feature
functions of this form and many ways to use ma-
chine learning to acquire ?. Our presented method
does not make any assumptions about the score ex-
cept that it has the form in Eq. 1.
We define a vector z of binary variables zr,s ?
{0, 1} for every role and span pair. We have that:
z ? {0, 1}d, where d = |Rf | ? |St|. zr,s = 1 means
that role r is filled by span s. Given the binary z vec-
tor, it is straightforward to recover the collection of
arguments by checking which components zr,s have
an assignment of 1; we use this strategy to find argu-
ments, as described in ?4.2 (strategies 4 and 6). The
joint argument identification task can be represented
as a constrained optimization problem:
maximize
?
r?Rf
?
s?St c(r, s)? zr,s
with respect to z ? {0, 1}d
such that Az ? b. (2)
The last line imposes constraints on the mapping be-
tween roles and spans; these are motivated on lin-
guistic grounds and are described next.3
Uniqueness: Each role r is filled by at most one
span in St. This constraint can be expressed by:
?r ? Rf ,
?
s?St zr,s = 1. (3)
There are O(|Rf |) such constraints. Note that since
St contains the null span ?, non-overt roles are also
captured using the above constraints. Such a con-
straint is used extensively in prior literature (Pun-
yakanok et al, 2008, ?3.4.1).
Overlap: SRL systems commonly constrain roles
to be filled by non-overlapping spans. For example,
Toutanova et al (2005) used dynamic programming
over a phrase structure tree to prevent overlaps be-
tween arguments, and Punyakanok et al (2008) used
3Note that equality constraints a ?z = b can be transformed into
double-side inequalities a ? z ? b and ?a ? z ? ?b.
210
constraints in an ILP to respect this requirement. In-
spired by the latter, we require that each input sen-
tence position of x be covered by at most one argu-
ment. For each role r ? Rf , we define:
Gr(i) = {s | s ? St, s covers position i in x}. (4)
We can define our overlap constraints in terms of Gr
as follows, for every sentence position i:
?i ? {1, . . . , |x|},
?
r?Rf
?
s?Gr(i) zr,s ? 1, (5)
This gives us O(|x|) constraints.
Pairwise ?Exclusions?: For many predicate
classes, there are pairs of roles forbidden to appear
together in the analysis of a single predicate token.
Consider the following two sentences:
A blackberry
Entity 1
resembles a loganberry
Entity 2
. (6)
Most berries
Entities
resemble each other. (7)
Consider the uninflected predicate resemble in
both sentences, evoking the same meaning. In exam-
ple 6, two roles, which we call Entity 1 and Entity 2
describe two entities that are similar to each other.
In the second sentence, a phrase fulfills a third role,
called Entities, that collectively denotes some ob-
jects that are similar. It is clear that the roles Entity 1
and Entities cannot be overt for the same predicate
at once, because the latter already captures the func-
tion of the former; a similar argument holds for the
Entity 2 and Entities roles. We call this phenomenon
the ?excludes? relationship. Let us define a set of
pairs fromRf that have this relationship:
Exclf = {(ri, rj) | ri and rj exclude each other}
Using the above set, we define the constraint:
?(ri, rj) ? Exclf , zri,? + zrj ,? ? 1 (8)
In English: if both roles are overt in a parse, this
constraint will be violated, and we will not respect
the ?excludes? relationship between the pair. If nei-
ther or only one of the roles is overt, the constraint
is satisfied. The total number of such constraints is
O(|Exclf |), which is the number of pairwise ?ex-
cludes? relationships of a given frame.
Pairwise ?Requirements?: The sentence in exam-
ple 6 illustrates another kind of constraint. The pred-
icate resemble cannot have only one of Entity 1 and
Entity 2 as roles in text. For example,
* A blackberry
Entity 1
resembles. (9)
Enforcing the overtness of two roles sharing this
?requires? relationship is straightforward. We define
the following set for a frame f :
Reqf = {(ri, rj) | ri and rj require each other}
This leads to constraints of the form
?(ri, rj) ? Reqf , zri,? ? zrj ,? = 0 (10)
If one role is overt (or absent), so must the other
be. A related constraint has been used previously
in the SRL literature, enforcing joint overtness re-
lationships between core arguments and referential
arguments (Punyakanok et al, 2008, ?3.4.1), which
are formally similar to the example above.4
Integer Linear Program and Relaxation: Plug-
ging the constraints in Eqs. 3, 5, 8 and 10 into the
last line of Eq. 2, we have the argument identifica-
tion problem expressed as an ILP, since the indica-
tor variables z are binary. In this paper, apart from
the ILP formulation, we will consider the follow-
ing relaxation of Eq. 2, which replaces the binary
constraint z ? {0, 1}d by a unit interval constraint
z ? [0, 1]d, yielding a linear program:
maximize
?
r?Rf
?
s?St c(r, s)? zr,s
with respect to z ? [0, 1]d
such that Az ? b. (11)
There are several LP and ILP solvers available,
and a great deal of effort has been spent by the
optimization community to devise efficient generic
solvers. An example is CPLEX, a state-of-the-art
solver for mixed integer programming that we em-
ploy as a baseline to solve the ILP in Eq. 2 as well
as its LP relaxation in Eq. 11. Like many of the best
implementations, CPLEX is proprietary.
4 We noticed in the annotated data, in some cases, the ?requires?
constraint is violated by the FrameNet annotators. This hap-
pens mostly when one of the required roles is absent in the
sentence containing the predicate, but is rather instantiated in
an earlier sentence; see Gerber and Chai (2010). We apply the
hard constraint in Eq. 10, though extending our algorithm to
seek arguments outside the sentence is straightforward (Chen
et al, 2010).
211
2.2 Linguistic Constraints from FrameNet
Although enforcing the four different sets of con-
straints above is intuitive from a general linguistic
perspective, we ground their use in definitive lin-
guistic information present in the FrameNet lexicon
(Fillmore et al, 2003). FrameNet, along with lists
of semantic frames, associated semantic roles, and
predicates that could evoke the frames, gives us a
small number of annotated sentences with frame-
semantic analysis. From the annotated data, we
gathered that only 3.6% of the time is a role instanti-
ated multiple times by different spans in a sentence.
This justifies the uniqueness constraint enforced by
Eq. 3. Use of such a constraint is also consistent
with prior work in frame-semantic parsing (Johans-
son and Nugues, 2007; Das et al, 2010a). Similarly,
we found that in the annotations, no arguments over-
lapped with each other for a given predicate. Hence,
the overlap constraints in Eq. 5 are also justified.
Our third and fourth sets of constraints, presented
in Eqs. 8 and 10, come from FrameNet, too; more-
over, they are explicitly mentioned in the lexicon.
Examples 6?7 are instances where the predicate re-
semble evokes the SIMILARITY frame, which is de-
fined in FrameNet as: ?Two or more distinct en-
tities, which may be concrete or abstract objects
or types, are characterized as being similar to each
other. Depending on figure/ground relations, the
entities may be expressed in two distinct frame el-
ements and constituents, Entity 1 and Entity 2, or
jointly as a single frame element and constituent,
Entities.?
For this frame, the lexicon lists several roles other
than the three roles we have already observed, such
as Dimension (the dimension along which the enti-
ties are similar), Differentiating fact (a fact that re-
veals how the concerned entities are similar or dif-
ferent), and so forth. Along with the roles, FrameNet
also declares the ?excludes? and ?requires? relation-
ships noted in our discussion in Section 2.1. The
case of the SIMILARITY frame is not unique; in Fig. 1,
the frame COLLABORATION, evoked by the predicate
partners, also has two roles Partner 1 and Partner 2
that share the ?requires? relationship. In fact, out
of 877 frames in FrameNet 1.5, the lexicon?s latest
edition, 204 frames have at least a pair of roles that
share the ?excludes? relationship, and 54 list at least
a pair of roles that share the ?requires? relationship.
2.3 Constraints as Factors in a Graphical Model
The LP in Eq. 11 can be represented as a maxi-
mum a posteriori (MAP) inference problem in an
undirected graphical model. In the factor graph,
each component of z corresponds to a binary vari-
able, and each instantiation of a constraint in
Eqs. 3, 5, 8 and 10 corresponds to a factor. Smith
and Eisner (2008) and Martins et al (2010) used
such a representation to impose constraints in a de-
pendency parsing problem; the latter discussed the
equivalence of linear programs and factor graphs for
representing discrete optimization problems. Each
of our constraints take standard factor forms we can
describe using the terminology of Smith and Eisner
(2008) and Martins et al (2010). The uniqueness
constraint in Eq. 3 corresponds to an XOR factor,
while the overlap constraint in Eq. 5 corresponds to
an ATMOSTONE factor. The constraints in Eq. 8
enforcing the ?excludes? relationship can be repre-
sented with an OR factor. Finally, each ?requires?
constraints in Eq. 10 is equivalent to an XORWITH-
OUTPUT factor.
In the following section, we describe how we ar-
rive at solutions for the LP in Eq. 11 using dual de-
composition, and how we adapt it to efficiently re-
cover the exact solution of the ILP (Eq. 2), without
the need of an off-the-shelf ILP solver.
3 ?Augmented? Dual Decomposition
Dual decomposition methods address complex op-
timization problems in the dual, by dividing them
into simple worker problems, which are repeat-
edly solved until a consensus is reached. The
most simple technique relies on the subgradient
algorithm (Komodakis et al, 2007; Rush et al,
2010); as an alternative, an augmented Lagrangian
technique was proposed by Martins et al (2011a,
2011b), which is more suitable when there are many
small components?commonly the case in declara-
tive constrained problems, such as the one at hand.
Here, we present a brief overview of the latter, which
is called Dual Decomposition with the Alternating
Direction Method of Multipliers (AD3).
Let us start by establishing some notation. Let
m ? {1, . . . ,M} index a factor, and denote by i(m)
212
the vector of indices of variables linked to that fac-
tor. (Recall that each factor represents the instantia-
tion of a constraint.) We introduce a new set of vari-
ables, u ? Rd, called the ?witness? vector. We split
the vector z into M overlapping pieces z1, . . . , zM ,
where each zm ? [0, 1]|i(m)|, and add M constraints
zm = ui(m) to impose that all the pieces must agree
with the witness (and therefore with each other).
Each of the M constraints described in ?2 can be
encoded with its own matrix Am and vector bm
(which jointly define A and b in Eq. 11). For conve-
nience, we denote by c ? Rd the score vector, whose
components are c(r, s), for each r ? Rf and s ? St
(Eq. 1), and define the following scores for the mth
subproblem:
cm(r, s) = ?(r, s)
?1c(r, s), ?(r, s) ? i(m), (12)
where ?(r, s) is the number of constraints that in-
volve role r and span s. Note that according to this
definition, c ? z =
?M
m=1 cm ? zm. We can rewrite
the LP in Eq. 11 in the following equivalent form:
maximize
M?
m=1
cm ? zm
with respect to u ? Rd, zm ? [0, 1]i(m), ?m
such that Amzm ? bm, ?m
zm = ui(m), ?m. (13)
We next augment the objective with a quadratic
penalty term ?2
?M
m=1 ?zm?ui(m)?
2 (for some ? >
0). This does not affect the solution of the problem,
since the equality constraints in the last line force
this penalty to vanish. However, as we will see, this
penalty will influence the workers and will lead to
faster consensus. Next, we introduce Lagrange mul-
tipliers ?m for those equality constraints, so that the
augmented Lagrangian function becomes:
L?(z,u,?) =
M?
m=1
(cm + ?m) ? zm ? ?m ? ui(m)
?
?
2
?zm ? ui(m)?
2. (14)
The AD3 algorithm seeks a saddle point of L? by
performing alternating maximization with respect to
z and u, followed by a gradient update of ?. The re-
sult is shown as Algorithm 1. Like dual decomposi-
tion approaches, it repeatedly performs a broadcast
operation (the zm-updates, which can be done in pa-
Algorithm 1 AD3 for Argument Identification
1: input:
? role-span matching scores c := ?c(r, s)?r,s,
? structural constraints ?Am,bm?Mm=1,
? penalty ? > 0
2: initialize u uniformly (i.e., u(r, s) = 0.5, ?r, s)
3: initialize each ?m = 0, ?m ? {1, . . . ,M}
4: initialize t? 1
5: repeat
6: for each m = 1, . . . ,M do
7: make a zm-update by finding the best scoring
analysis for the mth constraint, with penalties
for deviating from the consensus u:
zt+1m ? argmax
Amzm?bm
(cm+?m)?zm?
?
2
?zm?ui(m)?
2
8: end for
9: make a u-update by updating the consensus solu-
tion, averaging z1, . . . , zm:
ut+1(r, s)?
1
?(r, s)
?
m:(r,s)?i(m)
zt+1m (r, s)
10: make a ?-update:
?t+1m ? ?
t
m ? ?(z
(t+1)
m ? u
(t+1)
i(m) ), ?m
11: t? t+ 1
12: until convergence.
13: output: relaxed primal solution u? and dual solution
??. If u? is integer, it will encode an assignment of
spans to roles. Otherwise, it will provide an upper
bound of the true optimum.
-rallel, one constraint per ?worker?) and a gather op-
eration (the u- and ?-updates). Each u-operation
can be seen as an averaged voting which takes into
consideration each worker?s results.
Like in the subgradient method, the?-updates can
be regarded as price adjustments, which will affect
the next round of zm-updates. The only difference
with respect to the subgradient method (Rush et al,
2010) is that each subproblem involved in a zm-
update also has a quadratic penalty that penalizes de-
viations from the previous average voting; it is this
term that accelerates consensus and therefore con-
vergence. Martins et al (2011b) also provide stop-
ping criteria for the iterative updates using primal
and dual residuals that measure convergence; we re-
fer the reader to that paper for details.
A key attraction of this algorithm is all the com-
ponents of the declarative specification remain intact
213
in the procedural form. Each worker corresponds
exactly to one constraint in the ILP, which corre-
sponds to one linguistic constraint. There is no need
to work out when, during the procedure, each con-
straint might have an effect, as in beam search.
Solving the subproblems. In a different appli-
cation, Martins et al (2011b, ?4) showed how
to solve each zm-subproblem associated with the
XOR, XORWITHOUTPUT and OR factors in runtime
O(|i(m)| log |i(m)|). The only subproblem that re-
mains is that of the ATMOSTONE factor, to which
we now turn. The problem can be transformed into
that of projecting a point (a1, . . . , ak) onto the set
Sm =
{
zm ? [0, 1]|i(m)|
?
?
?|i(m)|
j=1 zm,j ? 1
}
.
This projection can be computed as follows:
1. Clip each aj into the interval [0, 1] (i.e., set
a?j = min{max{aj , 0}, 1}). If the result satisfies
?k
j=1 a
?
j ? 1, then return (a
?
1, . . . , a
?
k).
2. Otherwise project (a1, . . . , ak) onto the probabil-
ity simplex:
{
zm ? [0, 1]|i(m)|
?
?
?|i(m)|
j=1 zm,j = 1
}
.
This is precisely the XOR subproblem and can be
solved in time O(|i(m)| log |i(m)|).
Caching. As mentioned by Martins et al (2011b),
as the algorithm comes close to convergence, many
subproblems become unchanged and their solutions
can be cached. By caching the subproblems, we
managed to reduce runtime by about 60%.
Exact decoding. Finally, it is worth recalling that
AD3, like other dual decomposition algorithms,
solves a relaxation of the actual problem. Although
we have observed that the relaxation is often tight?
cf. ?4?this is not always the case. Specifically, a
fractional solution may be obtained, which is not in-
terpretable as an argument, and therefore it is de-
sirable to have a strategy to recover the exact solu-
tion. Two observations are noteworthy. First, the
optimal value of the relaxed problem (Eq. 11) pro-
vides an upper bound to the original problem (Eq. 2).
This is because Eq. 2 has the additional integer con-
straint on the variables. In particular, any feasible
dual point provides an upper bound to the original
problem?s optimal value. Second, during execution
of the AD3 algorithm, we always keep track of a se-
quence of feasible dual points. Therefore, each it-
eration constructs tighter and tighter upper bounds.
With this machinery, we have all that is necessary for
implementing a branch-and-bound search that finds
the exact solution of the ILP. The procedure works
recursively as follows:
1. Initialize L = ?? (our best value so far).
2. Run Algorithm 1. If the solution u? is integer, re-
turn u? and set L to the objective value. If along
the execution we obtain an upper bound less than
L, then Algorithm 1 can be safely stopped and
return ?infeasible??this is the bound part. Oth-
erwise (if u? is fractional) go to step 3.
3. Find the ?most fractional? component of u? (call
it u?j ) and branch: constrain uj = 0 and go to
step 2, eventually obtaining an integer solution u?0
or infeasibility; and then constrain uj = 1 and do
the same, obtaining u?1. Return the u
? ? {u?0,u
?
1}
that yields the largest objective value.
Although this procedure may have worst-case expo-
nential runtime, we found it empirically to rapidly
obtain the exact solution in all test cases.
4 Experiments and Results
4.1 Dataset, Preprocessing, and Learning
In our experiments, we use FrameNet 1.5, which
contains a lexicon of 877 frames and 1,068 role
labels, and 78 documents with multiple predicate-
argument annotations (a superset of the SemEval
shared task dataset; Baker et al, 2007). We used the
same split as Das and Smith (2011), with 55 doc-
uments for training (containing 19,582 frame anno-
tations) and 23 for testing (with 4,458 annotations).
We randomly selected 4,462 predicates in the train-
ing set as development data. The raw sentences in all
the training and test documents were preprocessed
using MXPOST (Ratnaparkhi, 1996) and the MST
dependency parser (McDonald et al, 2005).
The state-of-the-art system for this task is SE-
MAFOR, an open source tool (Das et al, 2010a)5
that provides a baseline benchmark for our new al-
gorithm. We use the components of SEMAFOR
as-is to define the features h and train the weights
? used in the scoring function c. We also use its
5http://www.ark.cs.cmu.edu/SEMAFOR
214
heuristic mechanism to find potential spans St for a
given predicate t. SEMAFOR learns weights using
`2-penalized log-likelihood; we augmented its dev
set-tuning procedure to tune both the regularization
strength and the AD3 penalty strength ?. We ini-
tialize ? = 0.1 and follow Martins et al (2011b)
in dynamically adjusting it. Note that we do not use
SEMAFOR?s automatic frame identification compo-
nent in our presented experiments, as we assume that
we have gold frames on each predicate. This lets us
compare the different argument identification meth-
ods in a controlled fashion.
4.2 Decoding Strategies
We compare the following algorithms:
1. Local: this is a na??ve argument identification
strategy that selects the best span for each role r,
according to the score function c(r, s). It ignores
all constraints except ?uniqueness.?
2. SEMAFOR: this strategy employs greedy beam
search to eliminate overlaps between predicted ar-
guments (Das et al, 2010b, Algorithm 1). Note
that it does not try to respect the ?excludes? and
?requires? constraints between pairs of roles. The
default size of the beam in SEMAFOR was a safe
10,000; this resulted in extremely slow decoding
times. We also tried beam sizes of 100 and 2
(the latter being the smallest size that achieves the
same F1 score on the dev set as beam width 100.)
3. CPLEX, LP: this uses CPLEX to solve the re-
laxed LP in Eq. 11. To handle fractional z, for
each role r, we choose the best span s?, such that
s? = argmaxs?Sr zr,s, solving ties arbitrarily.
4. CPLEX, exact: this tackles the actual ILP (Eq. 2)
with CPLEX.
5. AD3, LP: this is the counterpart of the LP version
of CPLEX, where the relaxed problem is solved
using AD3. We choose the spans for each role in
the same way as in strategy 3.
6. AD3, exact: this couples AD3 with branch-and-
bound search to get the exact integer solution.
4.3 Results
Table 1 shows performance of the different decoding
strategies on the test set. We report precision, recall,
and F1 scores.6 Since these scores do not penal-
6We use the evaluation script from SemEval 2007 shared task,
modified to evaluate only the argument identification output.
ize structural violations, we also report the number
of overlap, ?excludes,? and ?requires? constraints
that were violated in the test set. Finally, we tab-
ulate each setting?s decoding time in seconds on the
whole test set averaged over 5 runs.7 The Local
model is very fast but suffers degradation in pre-
cision and violates one constraint roughly per nine
predicates. SEMAFOR used a default beam size of
10,000, which is extremely slow; a faster version of
beam size 100 results in the same precision and re-
call values, but is 15 times faster. Beam size 2 results
in slightly worse precision and recall values, but is
even faster. All of these, however, result in many
constraint violations. Strategies involving CPLEX
and AD3 perform similarly to each other and SE-
MAFOR on precision and recall, but eliminate most
or all of the constraint violations. SEMAFOR with
beam size 2 is 11-16 times faster than the CPLEX
strategies, but is only twice as fast than AD3, and re-
sults in significantly more structural violations. The
exact algorithms are slower than the LP versions, but
compared to CPLEX, AD3 is significantly faster and
has a narrower gap between its exact and LP ver-
sions. We found that relaxation was tight 99.8% of
the time on the test examples.
The example in Fig. 1 is taken from our test set,
and shows an instance where two roles, Partner 1
and Partner 2 share the ?requires? relationship; for
this example, the beam search decoder misses the
Partner 2 role, which is a violation, while our AD3
decoder identifies both arguments correctly. Note
that beam search makes plenty of linguistic viola-
tions, but has precision and recall values that are
marginally better than AD3. We found that beam
search, when violating many ?requires? constraints,
often finds one role in the pair, which increases its
recall. AD3 is sometimes more conservative in such
cases, predicting neither role. A second issue, as
noted in footnote 4, is that the annotations some-
times violate these constraints. Overall, we found
it interesting that imposing the constraints did not
have much effect on standard measures of accuracy.
7We used a 64-bit machine with 2 2.6GHz dual-core CPUs (i.e.,
4 processors in all) with a total of 8GB of RAM. The work-
ers in AD3 were not parallelized, while CPLEX automatically
parallelized execution.
215
Violations
Method P R F1 Overlap Requires Excludes Time in Secs.
Local 67.69 59.76 63.48 441 45 15 1.26 ? 0.01
SEMAFOR (beam = 2) 70.18 59.54 64.42 0 49 0 2.74 ? 0.10
SEMAFOR (beam = 100) 70.43 59.64 64.59 0 50 1 29.00 ? 0.25
SEMAFOR (beam = 10000) 70.43 59.64 64.59 0 50 1 440.67 ? 5.53
CPLEX, LP 70.34 59.43 64.43 0 1 0 32.67 ? 1.29
CPLEX, exact 70.31 59.45 64.43 0 0 0 43.12 ? 1.26
AD3, LP 70.30 59.45 64.42 2 2 0 4.17 ? 0.01
AD3, exact 70.31 59.45 64.43 0 0 0 4.78 ? 0.04
Table 1: Comparison of decoding strategies in ?4.2. We evaluate in terms of precision, recall and F1 score on a test
set containing 4,458 predicates. We also compute the number of structural violations each model makes: number
of overlapping arguments and violations of the ?requires? and ?excludes? constraints of ?2. Finally decoding time
(without feature computation steps) on the whole test set is shown in the last column averaged over 5 runs.
5 Related Work
Semantic role labeling: Most SRL systems use
conventions from PropBank (Kingsbury and Palmer,
2002) and NomBank (Meyers et al, 2004), which
store information about verbal and nominal pred-
icates and corresponding symbolic and meaning-
specific semantic roles. A separate line of work,
including this paper, investigates SRL systems that
use FrameNet conventions; while less popular, these
systems, pioneered by Gildea and Jurafsky (2002),
consider predicates of a wider variety of syntactic
categories, use semantic frame abstractions, and em-
ploy explicit role labels. A common trait in prior
work has been the use of a two-stage model that
identifies arguments first, then labels them. They are
treated jointly here, unlike what has typically been
done in PropBank-style SRL (Ma`rquez et al, 2008).
Dual decomposition: Rush et al (2010) proposed
subgradient-based dual decomposition as a way of
combining models which are tractable individually,
but not jointly, by solving a relaxation of the origi-
nal problem. This was followed by work adopting
this method for syntax and translation (Koo et al,
2010; Auli and Lopez, 2011; DeNero and Macherey,
2011; Rush and Collins, 2011; Chang and Collins,
2011). Recently, Martins et al (2011b) showed that
the success of subgradient-based dual decomposi-
tion strongly relies on breaking down the original
problem into a ?good? decomposition, i.e., one with
few overlapping components. This leaves out many
declarative constrained problems, for which such a
good decomposition is not readily available. For
those, Martins et al (2011b) proposed the AD3 al-
gorithm, which retains the modularity of previous
methods, but can handle thousands of small over-
lapping components.
Exact decoding: This paper contributes an exact
branch-and-bound technique wrapped around AD3.
A related line of research is that of Rush and Collins
(2011), who proposed a tightening procedure for
dual decomposition, which can be seen as a cutting
plane method (another popular approach in combi-
natorial optimization).
6 Conclusion
We presented a novel algorithm for incorporating
declarative linguistic knowledge as constraints in
shallow semantic parsing. It outperforms a na??ve
baseline that is oblivious to the constraints. Further-
more, it is significantly faster than a decoder em-
ploying a state-of-the-art proprietary solver, and less
than twice as slow as beam search, which is inexact
and does not respect all linguistic constraints. Our
method is easily amenable to the inclusion of more
constraints, which would require minimal program-
ming effort. Our implementation of AD3 within
SEMAFOR will be publicly released at http://
www.ark.cs.cmu.edu/SEMAFOR.
Acknowledgments
We thank the three anonymous reviewers for their valu-
able feedback. This material is based upon work sup-
ported by NSF grant IIS-1054319, Google?s support
of the Wordly Knowledge Project, a FCT/ICTI grant
through the CMU-Portugal Program, and by Priberam,
through the Discooperio project, contract 2011/18501 of
the EU/FEDER program.
216
References
M. Auli and A. Lopez. 2011. A comparison of loopy be-
lief propagation and dual decomposition for integrated
ccg supertagging and parsing. In Proc. of ACL.
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction. In
Proc. of SemEval.
Y.-W. Chang and Michael Collins. 2011. Exact decoding
of Phrase-Based translation models through lagrangian
relaxation. In Proc. of EMNLP. Association for Com-
putational Linguistics.
D. Chen, N. Schneider, D. Das, and N. A. Smith.
2010. SEMAFOR: Frame argument resolution with
log-linear models. In Proc. of SemEval.
D. Das and N. A. Smith. 2011. Semi-supervised frame-
semantic parsing for unknown predicates. In Proc. of
ACL.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010a.
Probabilistic frame-semantic parsing. In Proc. of
NAACL-HLT.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010b.
SEMAFOR 1.0: a probabilistic frame-semantic parser.
Technical report, CMU-LTI-10-001.
J. DeNero and K. Macherey. 2011. Model-based aligner
combination using dual decomposition. In Proc. of
ACL.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame Semantics. In Linguistics in
the Morning Calm. Hanshin.
M. Gerber and J. Y. Chai. 2010. Beyond nombank: A
study of implicit arguments for nominal predicates. In
ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
R. Johansson and P. Nugues. 2007. LTH: semantic struc-
ture extraction using nonprojective dependency trees.
In Proc. of SemEval.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. of LREC.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In ICCV.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
L. Ma`rquez, X. Carreras, K. C. Litkowski, and S. Steven-
son. 2008. Semantic role labeling: an introduction to
the special issue. Computational Linguistics, 34(2).
A. F. T. Martins, N. A. Smith, E. P. Xing, M. A. T.
Figueiredo, and P. M. Q. Aguiar. 2010. Turbo parsers:
Dependency parsing by approximate variational infer-
ence. In EMNLP.
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2011a. An augmented
Lagrangian approach to constrained MAP inference.
In Proc. of ICML.
A F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011b. Dual decomposition with
many overlapping components. In Proc. of EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The
NomBank project: An interim report. In Proc. of
NAACL/HLT Workshop on Frontiers in Corpus Anno-
tation.
V. Punyakanok, D. Roth, W.-T. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear programming
inference. In Proc. of COLING.
V. Punyakanok, D. Roth, and W Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34:257?287.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
A. M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through lagrangian relax-
ation. In Proc. of ACL.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear programming
relaxations for natural language processing. In Pro-
ceedings of EMNLP.
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In EMNLP.
K. Toutanova, A. Haghighi, and C. Manning. 2005. Joint
learning improves semantic role labeling. In Proc. of
ACL.
217
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 213?222,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Distributed Asynchronous Online Learning
for Natural Language Processing
Kevin Gimpel Dipanjan Das Noah A. Smith
Language Technologies Institute
Carnegie Mellon Univeristy
Pittsburgh, PA 15213, USA
{kgimpel,dipanjan,nasmith}@cs.cmu.edu
Abstract
Recent speed-ups for training large-scale
models like those found in statistical NLP
exploit distributed computing (either on
multicore or ?cloud? architectures) and
rapidly converging online learning algo-
rithms. Here we aim to combine the two.
We focus on distributed, ?mini-batch?
learners that make frequent updates asyn-
chronously (Nedic et al, 2001; Langford
et al, 2009). We generalize existing asyn-
chronous algorithms and experiment ex-
tensively with structured prediction prob-
lems from NLP, including discriminative,
unsupervised, and non-convex learning
scenarios. Our results show asynchronous
learning can provide substantial speed-
ups compared to distributed and single-
processor mini-batch algorithms with no
signs of error arising from the approximate
nature of the technique.
1 Introduction
Modern statistical NLP models are notoriously
expensive to train, requiring the use of general-
purpose or specialized numerical optimization al-
gorithms (e.g., gradient and coordinate ascent al-
gorithms and variations on them like L-BFGS and
EM) that iterate over training data many times.
Two developments have led to major improve-
ments in training time for NLP models:
? online learning algorithms (LeCun et al, 1998;
Crammer and Singer, 2003; Liang and Klein,
2009), which update the parameters of a model
more frequently, processing only one or a small
number of training examples, called a ?mini-
batch,? between updates; and
? distributed computing, which divides training
data among multiple CPUs for faster processing
between updates (e.g., Clark and Curran, 2004).
Online algorithms offer fast convergence rates
and scalability to large datasets, but distributed
computing is a more natural fit for algorithms that
require a lot of computation?e.g., processing a
large batch of training examples?to be done be-
tween updates. Typically, distributed online learn-
ing has been done in a synchronous setting, mean-
ing that a mini-batch of data is divided among
multiple CPUs, and the model is updated when
they have all completed processing (Finkel et al,
2008). Each mini-batch is processed only after the
previous one has completed.
Synchronous frameworks are appealing in that
they simulate the same algorithms that work on
a single processor, but they have the drawback
that the benefits of parallelism are only obtainable
within one mini-batch iteration. Moreover, empir-
ical evaluations suggest that online methods only
converge faster than batch algorithms when using
very small mini-batches (Liang and Klein, 2009).
In this case, synchronous parallelization will not
offer much benefit.
In this paper, we focus our attention on asyn-
chronous algorithms that generalize those pre-
sented by Nedic et al (2001) and Langford et al
(2009). In these algorithms, multiple mini-batches
are processed simultaneously, each using poten-
tially different and typically stale parameters. The
key advantage of an asynchronous framework is
that it allows processors to remain in near-constant
use, preventing them from wasting cycles wait-
ing for other processors to complete their por-
tion of the current mini-batch. In this way, asyn-
chronous algorithms allow more frequent parame-
ter updates, which speeds convergence.
Our contributions are as follows:
? We describe a framework for distributed asyn-
chronous optimization (?5) similar to those de-
scribed by Nedic et al (2001) and Langford et
al. (2009), but permitting mini-batch learning.
The prior work contains convergence results for
asynchronous online stochastic gradient descent
213
for convex functions (discussed in brief in ?5.2).
? We report experiments on three structured NLP
tasks, including one problem that matches
the conditions for convergence (named entity
recognition; NER) and two that depart from the-
oretical foundations, namely the use of asyn-
chronous stepwise EM (Sato and Ishii, 2000;
Cappe? and Moulines, 2009; Liang and Klein,
2009) for both convex and non-convex opti-
mization.
? We directly compare asynchronous algorithms
with multiprocessor synchronous mini-batch al-
gorithms (e.g., Finkel et al, 2008) and tradi-
tional batch algorithms.
? We experiment with adding artificial delays to
simulate the effects of network or hardware traf-
fic that could cause updates to be made with ex-
tremely stale parameters.
? Our experimental settings include both indi-
vidual 4-processor machines as well as large
clusters of commodity machines implementing
the MapReduce programming model (Dean and
Ghemawat, 2004). We also explore effects of
mini-batch size.
Our main conclusion is that, when small mini-
batches work well, asynchronous algorithms of-
fer substantial speed-ups without introducing er-
ror. When large mini-batches work best, asyn-
chronous learning does not hurt.
2 Optimization Setting
We consider the problem of optimizing a function
f : Rd ? R with respect to its argument, denoted
? = ??1, ?2, . . . , ?d?. We assume that f is a sum
of n convex functions (hence f is also convex):1
f(?) =
?n
i=1 fi(?) (1)
We initially focus our attention on functions that
can be optimized using gradient or subgradient
methods. Log-likelihood for a probabilistic model
with fully observed training data (e.g., conditional
random fields; Lafferty et al, 2001) is one exam-
ple that frequently arises in NLP, where the fi(?)
each correspond to an individual training exam-
ple and the ? are log-linear feature weights. An-
other example is large-margin learning for struc-
tured prediction (Taskar et al, 2005; Tsochan-
1We use ?convex? to mean convex-up when minimizing
and convex-down, or concave, when maximizing.
taridis et al, 2005), which can be solved by sub-
gradient methods (Ratliff et al, 2006).
For concreteness, we discuss the architecture
in terms of gradient-based optimization, using the
following gradient descent update rule (for mini-
mization problems):2
?(t+1) ? ?(t) ? ?(t)g(?(t)) (2)
where ?(t) is the parameter vector on the tth iter-
ation, ?(t) is the step size on the tth iteration, and
g : Rd ? Rd is the vector function of first deriva-
tives of f with respect to ?:
g(?) =
?
?f
??1
(?), ?f??2 (?), . . . ,
?f
??d
(?)
?
(3)
We are interested in optimizing such functions
using distributed computing, by which we mean to
include any system containing multiple processors
that can communicate in order to perform a single
task. The set of processors can range from two
cores on a single machine to a MapReduce cluster
of thousands of machines.
Note our assumption that the computation re-
quired to optimize f with respect to ? is, essen-
tially, the gradient vector g(?(t)), which serves
as the descent direction. The key to distribut-
ing this computation is the fact that g(?(t)) =
?n
i=1 gi(?
(t)), where gi(?) denotes the gradient
of fi(?) with respect to ?. We now discuss several
ways to go about distributing such a problem, cul-
minating in the asynchronous mini-batch setting.
3 Distributed Batch Optimization
Given p processors plus a master processor, the
most straightforward way to optimize f is to par-
tition the fi so that for each i ? {1, 2, . . . , n},
gi is computed on exactly one ?slave? processor.
Let Ij denote the subset of examples assigned to
the jth slave processor (
?p
j=1 Ij = {1, . . . , n}
and j 6= j? ? Ij ? Ij? = ?). Processor j re-
ceives the examples in Ij along with the neces-
sary portions of ?(t) for calculating gIj (?
(t)) =
?
i?Ij
gi(?
(t)). The result of this calculation is
returned to the master processor, which calculates
g(?(t)) =
?
j gIj (?
(t)) and executes Eq. 2 (or
something more sophisticated that uses the same
information) to obtain a new parameter vector.
It is natural to divide the data so that each pro-
cessor is assigned approximately n/p of the train-
ing examples. Because of variance in the expense
2We use the term ?gradient? for simplicity, but subgradi-
ents are sufficient throughout.
214
of calculating the different gi, and because of un-
predictable variation among different processors?
speed (e.g., variation among nodes in a cluster,
or in demands made by other users), there can be
variation in the observed runtime of different pro-
cessors on their respective subsamples. Each it-
eration of calculating g will take as long as the
longest-running among the processors, whatever
the cause of that processor?s slowness. In comput-
ing environments where the load on processors is
beyond the control of the NLP researcher, this can
be a major bottleneck.
Nonetheless, this simple approach is widely
used in practice; approaches in which the gradient
computation is distributed via MapReduce have
recently been described in machine learning and
NLP (Chu et al, 2006; Dyer et al, 2008; Wolfe et
al., 2008). Mann et al (2009) compare this frame-
work to one in which each processor maintains a
separate parameter vector which is updated inde-
pendently of the others. At the end of learning, the
parameter vectors are averaged or a vote is taken
during prediction. A similar parameter-averaging
approach was taken by Chiang et al (2008) when
parallelizing MIRA (Crammer et al, 2006). In
this paper, we restrict our attention to distributed
frameworks which maintain and update a single
copy of the parameters ?. The use of multiple
parameter vectors is essentially orthogonal to the
framework we discuss here and we leave the inte-
gration of the two ideas for future exploration.
4 Distributed Synchronous Mini-Batch
Optimization
Distributed computing can speed up batch algo-
rithms, but we would like to transfer the well-
known speed-ups offered by online and mini-batch
algorithms to the distributed setting as well. The
simplest way to implement mini-batch stochastic
gradient descent (SGD) in a distributed computing
environment is to divide each mini-batch (rather
than the entire batch) among the processors that
are available and to update the parameters once the
gradient from the mini-batch has been computed.
Finkel et al (2008) used this approach to speed
up training of a log-linear model for parsing. The
interaction between the master processor and the
distributed computing environment is nearly iden-
tical to the distributed batch optimization scenario.
Where M (t) is the set of indices in the mini-batch
processed on iteration t, the update is:
?(t+1) ? ?(t) ? ?(t)
?
i?M(t) gi(?
(t)) (4)
The distributed synchronous framework can
provide speed-ups over a single-processor imple-
mentation of SGD, but inevitably some processors
will end up waiting for others to finish processing.
This is the same bottleneck faced by the batch ver-
sion in ?3. While the time for each mini-batch is
shorter than the time for a full batch, mini-batch
algorithms make far more updates and some pro-
cessor cycles will be wasted in computing each
one. Also, more mini-batches imply that more
time will be lost due to per-mini-batch overhead
(e.g., waiting for synchronization locks in shared-
memory systems, or sending data and ? to the pro-
cessors in systems without shared memory).
5 Distributed Asynchronous Mini-Batch
Optimization
An asynchronous framework may use multiple
processors more efficiently and minimize idle time
(Nedic et al, 2001; Langford et al, 2009). In this
setting, the master sends ? and a mini-batchMk to
each slave k. Once slave k finishes processing its
mini-batch and returns gMk(?), the master imme-
diately updates ? and sends a new mini-batch and
the new ? to the now-available slave k. As a result,
slaves stay occupied and never need to wait on oth-
ers to finish. However, nearly all gradient com-
ponents are computed using slightly stale parame-
ters that do not take into account the most recent
updates. Nedic et al (2001) proved that conver-
gence is still guaranteed under certain conditions,
and Langford et al (2009) obtained convergence
rate results. We describe these results in more de-
tail in ?5.2.
The update takes the following form:
?(t+1) ? ?(t) ? ?(t)
?
i?M(?(t)) gi(?
(?(t))) (5)
where ?(t) ? t is the start time of the mini-batch
used for the tth update. Since we started pro-
cessing the mini-batch at time ?(t) (using param-
eters ?(?(t))), we denote the mini-batch M (?(t)). If
?(t) = t, then Eq. 5 is identical to Eq. 4. That is,
t? ?(t) captures the ?staleness? of the parameters
used to compute the gradient for the tth update.
Asynchronous frameworks do introduce error
into the training procedure, but it is frequently
the case in NLP problems that only a small frac-
tion of parameters is needed for each mini-batch
215
Input: number of examples n, mini-batch size m,
random seed r
?` ? ?;
seedRandomNumberGenerator (r);
while converged (?) = false do
g ? 0;
for j ? 1 to m do
k ? Uniform({1, . . . , n});
g ? g + gk(?`);
end
acquireLock (?);
? ? updateParams (?, g);
?` ? ?;
releaseLock (?);
end
Algorithm 1: Procedure followed by each thread for multi-
core asynchronous mini-batch optimization. ? is the single
copy of the parameters shared by all threads. The conver-
gence criterion is left unspecified here.
of training examples. For example, for simple
word alignment models like IBM Model 1 (Brown
et al, 1993), only parameters corresponding to
words appearing in the particular subsample of
sentence pairs are needed. The error introduced
when making asynchronous updates should intu-
itively be less severe in these cases, where dif-
ferent mini-batches use small and mostly non-
overlapping subsets of ?.
5.1 Implementation
The algorithm sketched above is general enough
to be suitable for any distributed system, but when
using a system with shared memory (e.g., a single
multiprocessor machine) a more efficient imple-
mentation is possible. In particular, we can avoid
the master/slave architecture and simply start p
threads that each compute and execute updates in-
dependently, with a synchronization lock on ?. In
our single-machine experiments below, we use Al-
gorithm 1 for each thread. A different random seed
(r) is passed to each thread so that they do not all
process the same sequence of examples. At com-
pletion, the result is contained in ?.
5.2 Convergence Results
We now briefly summarize convergence results
from Nedic et al (2001) and Langford et al
(2009), which rely on the following assumptions:
(i) The function f is convex. (ii) The gradients
gi are bounded, i.e., there exists C > 0 such that
?gi(?
(t))? ? C. (iii) ? (unknown) D > 0 such
that t ? ?(t) < D. (iv) The stepsizes ?(t) satisfy
certain standard conditions.
In addition, Nedic et al require that all func-
tion components are used with the same asymp-
totic frequency (as t ? ?). Their results are
strongest when choosing function components in
each mini-batch using a ?cyclic? rule: select func-
tion fi for the kth time only after all functions have
been selected k ? 1 times. For a fixed step size
?, the sequence of function values f(?(t)) con-
verges to a region of the optimum that depends
on ?, the maximum norm of any gradient vector,
and the maximum delay for any mini-batch. For
a decreasing step size, convergence is guaranteed
to the optimum. When choosing components uni-
formly at random, convergence to the optimum is
again guaranteed using a decreasing step size for-
mula, but with slightly more stringent conditions
on the step size.
Langford et al (2009) present convergence rates
via regret bounds, which are linear in D. The con-
vergence rate of asynchronous stochastic gradient
descent is O(
?
TD), where T is the total number
of updates made. In addition to the situation in
which function components are chosen uniformly
at random, Langford et al provide results for sev-
eral other scenarios, including the case in which an
adversary supplies the training examples in what-
ever ordering he chooses.
Below we experiment with optimization of both
convex and non-convex functions, using fixed step
sizes and decreasing step size formulas, and con-
sider several values of D. Even when exploring
regions of the experimental space that are not yet
supported by theoretical results, asynchronous al-
gorithms perform well empirically in all settings.
5.3 Gradients and Expectation-Maximization
The theory applies when using first-order methods
to optimize convex functions. Though the function
it is optimizing is not usually convex, the EM algo-
rithm can be understood as a hillclimber that trans-
forms the gradient to keep ? feasible; it can also
be understood as a coordinate ascent algorithm.
Either way, the calculations during the E-step re-
semble g(?). Several online or mini-batch vari-
ants of the EM algorithm have been proposed, for
example incremental EM (Neal and Hinton, 1998)
and online EM (Sato and Ishii, 2000; Cappe? and
Moulines, 2009), and we follow Liang and Klein
(2009) in referring to this latter algorithm as step-
wise EM. Our experiments with asynchronous
minibatch updates include a case where the log-
likelihood f is convex and one where it is not.
216
task data n # params. eval. method convex?
?6.1 named entity
recognition (CRF;
Lafferty et al, 2001)
CoNLL 2003 English
(Tjong Kim Sang and De
Meulder, 2003)
14,987
sents.
1.3M F1 SGD yes
?6.2 word alignment (Model
1, both directions;
Brown et al, 1993)
NAACL 2003 parallel text
workshop (Mihalcea and
Pedersen, 2003)
300K
pairs
14.2M ?2
(E?F +
F?E)
AER EM yes
S6.3 unsupervised POS
(bigram HMM)
Penn Treebank ?1?21
(Marcus et al, 1993)
41,825
sents.
2,043,226 (Johnson,
2007)
EM no
Table 1: Our experiments consider three tasks.
0 2 4 6 8 10 12
84
86
88
90
Wall clock time (hours)
F1
 
 
Asynchronous (4 processors)Synchronous (4 processors)Single?processor
Figure 1: NER: Synchronous
mini-batch SGD converges faster
in F1 than the single-processor
version, and the asynchronous
version converges faster still. All
curves use a mini-batch size of 4.
6 Experiments
We performed experiments to measure speed-ups
obtainable through distributed online optimiza-
tion. Since we will be considering different opti-
mization algorithms and computing environments,
we will primarily be interested in the wall-clock
time required to obtain particular levels of perfor-
mance on metrics appropriate to each task. We
consider three tasks, detailed in Table 1.
For experiments on a single node, we used a
64-bit machine with two 2.6GHz dual-core CPUs
(i.e., 4 processors in all) with a total of 8GB of
RAM. This was a dedicated machine that was not
available for any other jobs. We also conducted
experiments using a cluster architecture running
Hadoop 0.20 (an implementation of MapReduce),
consisting of 400 machines, each having 2 quad-
core 1.86GHz CPUs with a total of 6GB of RAM.
6.1 Named Entity Recognition
Our NER CRF used a standard set of features, fol-
lowing Kazama and Torisawa (2007), along with
token shape features like those in Collins (2002)
and simple gazetteer features; a feature was in-
cluded if and only it occurred at least once in train-
ing data (total 1.3M).We used a diagonal Gaussian
prior with a variance of 1.0 for each weight.
We compared SGD on a single processor to dis-
tributed synchronous SGD and distributed asyn-
chronous SGD. For all experiments, we used a
fixed step size of 0.01 and chose each training ex-
ample for each mini-batch uniformly at random
from the full data set.3 We report performance by
3In preliminary experiments, we experimented with vari-
0 2 4 6 8 10
86
88
90
F1
 
 
Synchronous (4 processors)Synchronous (2 processors)Single?processor
0 2 4 6 8 10
86
88
90
Wall clock time (hours)
F1
 
 
Asynchronous (4 processors)
Asynchronous (2 processors)Single?processor
Figure 2: NER: (Top) Synchronous optimization improves
very little when moving from 2 to 4 processors due to the
need for load-balancing, leaving some processors idle for
stretches of time. (Bottom) Asynchronous optimization does
not require load balancing and therefore improves when mov-
ing from 2 to 4 processors because each processor is in near-
constant use. All curves use a mini-batch size of 4 and the
?Single-processor? curve is identical in the two plots.
plotting test-set accuracy against wall-time over
12 hours.4
Comparing Synchronous and Asynchronous
Algorithms Figure 1 shows our primary result
for the NER experiments. When using all four
available processors, the asynchronous algorithm
converges faster than the other two algorithms. Er-
ror due to stale parameters during gradient com-
putation does not appear to cause any more varia-
ous fixed step sizes and decreasing step size schedules, and
found a fixed step size to work best for all settings.
4Decoding was performed offline (so as not to affect mea-
surments) with models sampled every ten minutes.
217
tion in performance than experienced by the syn-
chronous mini-batch algorithm. Note that the dis-
tributed synchronous algorithm and the single-
processor algorithm make identical sequences of
parameter updates; the only difference is the
amount of time between each update. Since we
save models every ten minutes and not every ith
update, the curves have different shapes. The se-
quence of updates for the asynchronous algorithm,
on the other hand, actually depends on the vagaries
of the computational environment. Nonetheless,
the asynchronous algorithm using 4 processors has
nearly converged after only 2 hours, while the
single-processor algorithm requires 10?12 hours
to reach the same F1.
Varying the Number of Processors Figure 2
shows the improvement in convergence time by
using 4 vs. 2 processors for the synchronous (top)
and asynchronous (bottom) algorithms. The ad-
ditional two processors help the asynchronous al-
gorithm more than the synchronous one. This
highlights the key advantage of asynchronous al-
gorithms: it is easier to keep all processors in
constant use. Synchronous algorithms might be
improved through load-balancing; in our experi-
ments here, we simply assigned m/p examples to
each processor, wherem is the mini-batch size and
p is the number of processors. When m = p, as in
the 4-processor curve in the upper plot of Figure 2,
we assign a single example to each processor; this
is optimal in the sense that no other scheduling
strategy will process the mini-batch faster. There-
fore, the fact that the 2-processor and 4-processor
curves are so close suggests that the extra two pro-
cessors are not being fully exploited, indicating
that the optimal load balancing strategy for a small
mini-batch still leaves processors under-used due
to the synchronous nature of the updates.
The only bottleneck in the asynchronous algo-
rithm is the synchronization lock during updating,
required since there is only one copy of ?. For
CRFs with a few million weights, the update is
typically much faster than processing a mini-batch
of examples; furthermore, when using small mini-
batches, the update vector is typically sparse.5 For
all experimental results presented thus far, we used
a mini-batch size of 4. We experimented with ad-
5In a standard implementation, the sparsity of the update
will be nullified by regularization, but to improve efficiency
in practice the regularization penalty can be accumulated and
applied less frequently than every update.
0 2 4 6 8 10 1285
86
87
88
89
90
91
Wall clock time (hours)
F1
 
 
Asynchronous, no delay
Asynchronous, ? = 5
Single?processor, no delay
Asynchronous, ? = 10
Asynchronous, ? = 20
Figure 3: NER: Convergence curves when a delay is incurred
with probability 0.25 after each mini-batch is processed. The
delay durations (in seconds) are sampled fromN(?, (?/5)2),
for several means ?. Each mini-batch (size = 4) takes less
than a second to process, so if the delay is substantially longer
than the time required to process a mini-batch, the single-
node version converges faster. While curves with ? = 10 and
20 appear less smooth than the others, they are still heading
steadily toward convergence.
ditional mini-batch sizes of 1 and 8, but there was
very little difference in the resulting curves.
Artificial Delays We experimented with adding
artificial delays to the algorithm to explore how
much overhead would be tolerable before paral-
lelized computation becomes irrelevant. Figure 3
shows results when each processor sleeps with
0.25 probability for a duration of time between
computing the gradient on its mini-batch of data
and updating the parameters. The delay length is
chosen from a normal distribution with the means
(in seconds) shown and ? = ?/5 (truncated at
zero). Since only one quarter of the mini-batches
have an artificial delay, increasing ? increases the
average parameter ?staleness?, letting us see how
the asynchronous algorithm fares with extremely
stale parameters.
The average time required to compute the gradi-
ent for a mini-batch of 4 is 0.62 seconds. When the
average delay is 1.25 seconds (? = 5), twice the
average time for a mini-batch, the asynchronous
algorithm still converges faster than the single-
node algorithm. In addition, even with substan-
tial delays of 5?10 times the processing time for a
mini-batch, the asynchronous algorithm does not
fail but proceeds steadily toward convergence.
The practicality of using the asynchronous algo-
rithm depends on the average duration for a mini-
batch and the amount of expected additional over-
head. We attempted to run these experiments on
218
AER Time (h:m)
Single machine:
Asynch. stepwise EM 0.274 1:58
Synch. stepwise EM (4 proc.) 0.274 2:08
Synch. stepwise EM (1 proc.) 0.272 6:57
Batch EM 0.276 2:15
MapReduce:
Asynch. stepwise EM 0.281 5:41
Synch. stepwise EM 0.273 27:03
Batch EM 0.276 8:35
Table 2: Alignment error rates and wall time after 20 itera-
tions of EM for various settings. See text for details.
a large MapReduce cluster, but the overhead re-
quired for each MapReduce job was too large to
make this viable (30?60 seconds).
6.2 Word Alignment
We trained IBM Model 1 in both directions. To
align test data, we symmetrized both directional
Viterbi alignments using the ?grow-diag-final?
heuristic (Koehn et al, 2003). We evaluated our
models using alignment error rate (AER).
Experiments on a Single Machine We fol-
lowed Liang and Klein (2009) in using syn-
chronous (mini-batch) stepwise EM on a single
processor for this task. We used the same learning
rate formula (?(t) = (t+2)?q, with 0.5 < q ? 1).
We also used asynchronous stepwise EM by using
the same update rule, but gathered sufficient statis-
tics on 4 processors of a single machine in paral-
lel, analogous to our asynchronous method from
?5. Whenever a processor was done gathering the
expected counts for its mini-batch, it updated the
sufficient statistics vector and began work on the
next mini-batch.
We used the sparse update described by Liang
and Klein, which allows each thread to make
additive updates to the parameter vector and
to separately-maintained normalization constants
without needing to renormalize after each update.
When probabilities are needed during inference,
normalizers are divided out on-the-fly as needed.
We made 10 passes of asynchronous stepwise
EM to measure its sensitivity to q and the mini-
batch size m, using different values of these
hyperparameters (q ? {0.5, 0.7, 1.0}; m ?
{5000, 10000, 50000}), and selected values that
maximized log-likelihood (q = 0.7, m = 10000).
Experiments on MapReduce We implemented
the three techniques in a MapReduce framework.
We implemented batch EM on MapReduce by
converting each EM iteration into two MapRe-
duce jobs: one for the E-step and one for the M-
step.6 For the E-step, we divided our data into
24 map tasks, and computed expected counts for
the source-target parameters at each mapper. Next,
we summed up the expected counts in one reduce
task. For the M-step, we took the output from
the E-step, and in one reduce task, normalized
each source-target parameter by the total count for
the source word.7 To gather sufficient statistics
for synchronous stepwise EM, we used 6 mappers
and one reducer for a mini-batch of size 10000.
For the asynchronous version, we ran four parallel
asynchronous mini-batches, the sufficient statis-
tics being gathered using MapReduce again for
each mini-batch with 6 map tasks and one reducer.
Results Figure 4 shows log-likelihood for the
English?French direction during the first 80 min-
utes of optimization. Similar trends were observed
for the French?English direction as well as for
convergence in AER. Table 2 shows the AER at
the end of 20 iterations of EM for the same set-
tings.8 It takes around two hours to finish 20 iter-
ations of batch EM on a single machine, while it
takes more than 8 hours to do so on MapReduce.
This is because of the extra overhead of transfer-
ring ? from a master gateway machine to mappers,
from mappers to reducers, and from reducers back
to the master. Synchronous and asynchronous EM
suffer as well.
From Figure 4, we see that synchronous and
asynchronous stepwise EM converge at the same
rate when each is given 4 processors. The main
difference between this task and NER is the size
of the mini-batch used, so we experimented with
several values for the mini-batch size m. Fig-
ure 5 shows the results. As m decreases, a larger
fraction of time is spent updating parameters; this
slows observed convergence time even when us-
ing the sparse update rule. It can be seen that,
though synchronous and asynchronous stepwise
EM converge at the same rate with a large mini-
batch size (m = 10000), asynchronous stepwise
6The M-step could have been performed without MapRe-
duce by storing all the parameters in memory, but memory
restrictions on the gateway node of our cluster prevented this.
7For the reducer in the M-step, the source served as the
key, and the target appended by the parameter?s expected
count served as the value.
8Note that for wall time comparison, we sample models
every five minutes. The time taken to write these models
ranges from 30 seconds to a minute, thus artificially elon-
gating the total time for all iterations.
219
10 20 30 40 50 60 70 80
?40
?35
?30
?25
?20
Log
?Li
keli
hoo
d
 
 
Asynch. Stepwise EM (4 processors)
Synch. Stepwise EM (4 processors)
Synch. Stepwise EM (1 processor)
Batch EM (1 processor)
10 20 30 40 50 60 70 80
?40
?35
?30
?25
?20
Wall clock time (minutes)
Log
?Li
keli
hoo
d
 
 
Asynch. Stepwise EM (MapReduce)
Synch. Stepwise EM (MapReduce)
Batch EM (MapReduce)
Figure 4: English?French log-likelihood vs. wall clock time
in minutes on both a single machine (top) and on a large
MapReduce cluster (bottom), shown on separate plots for
clarity, though axis scales are identical. We show runs of
each setting for the first 80 minutes, although EM was run
for 20 passes through the data in all cases (Table 2). Fastest
convergence is obtained by synchronous and asynchronous
stepwise EM using 4 processors on a single node. While the
algorithms converge more slowly on MapReduce due to over-
head, the asynchronous algorithm converges the fastest. We
observed similar trends for the French?English direction.
EM converges faster as m decreases. With large
mini-batches, load-balancing becomes less impor-
tant as there will be less variation in per-mini-
batch observed runtime. These results suggest that
asynchronous mini-batch algorithms will be most
useful for learning problems in which small mini-
batches work best. Fortunately, however, we do
not see any problems stemming from approxima-
tion errors due to the use of asynchronous updates.
6.3 Unsupervised POS Tagging
Our unsupervised POS experiments use the same
task and approach of Liang and Klein (2009) and
so we fix hyperparameters for stepwise EM based
on their findings (learning rate ?(t) = (t+2)?0.7).
The asynchronous algorithm uses the same learn-
ing rate formula as the single-processor algorithm.
There is only a single t that is maintained and gets
incremented whenever any thread updates the pa-
rameters. Liang and Klein used a mini-batch size
of 3, but we instead use a mini-batch size of 4 to
better suit our 4-processor synchronous and asyn-
chronous architectures.
Like NER, we present results for unsupervised
tagging experiments on a single machine only, i.e.,
not using a MapReduce cluster. For tasks like POS
tagging that have been shown to work best with
small mini-batches (Liang and Klein, 2009), we
10 20 30 40 50 60 70 80
?35
?30
?25
?20
Wall clock time (minutes)
Log
?L
ike
liho
od
 
 
Asynch. (m = 10,000)
Synch. (m = 10,000)
Asynch. (m = 1,000)
Synch. (m = 1,000)
Asynch. (m = 100)
Synch. (m = 100)
Figure 5: English?French log-likelihood vs. wall clock time
in minutes for stepwise EM with 4 processors for various
mini-batch sizes (m). The benefits of asynchronous updat-
ing increase as m decreases.
did not conduct experiments with MapReduce due
to high overhead per mini-batch.
For initialization, we followed Liang and Klein
by initializing each parameter as ?i ? e1+ai ,
ai ? Uniform([0, 1]). We generated 5 random
models using this procedure and used each to ini-
tialize each algorithm. We additionally used 2
random seeds for choosing the ordering of exam-
ples,9 resulting in a total of 10 runs for each al-
gorithm. We ran each for six hours, saving mod-
els every five minutes. After training completed,
using each model we decoded the entire training
data using posterior decoding and computed the
log-likelihood. The results for 5 initial models and
two example orderings are shown in Figure 6. We
evaluated tagging performance using many-to-1
accuracy, which is obtained by mapping the HMM
states to gold standard POS tags so as to maximize
accuracy, where multiple states can be mapped to
the same tag. This is the metric used by Liang and
Klein (2009) and Johnson (2007), who report fig-
ures comparable to ours. The asynchronous algo-
rithm converges much faster than the single-node
algorithm, allowing a tagger to be trained from
the Penn Treebank in less than two hours using
a single machine. Furthermore, the 4-processor
synchronous algorithm improves only marginally
9We ensured that the examples processed in the sequence
of mini-batches were identical for the 1-processor and 4-
processor versions of synchronous stepwise EM, but the
asynchronous algorithm requires a different seed for each
processor and, furthermore, the actual order of examples pro-
cessed depends on wall times and cannot be controlled for.
Nonetheless, we paired a distinct set of seeds for the asyn-
chronous algorithm with each of the two seeds used for the
synchronous algorithms.
220
0 1 2 3 4 5 6
?7.5
?7
?6.5
?6 x 10
6
Log
?Li
keli
hoo
d
0 1 2 3 4 5 6
50
55
60
65
Wall clock time (hours)
Acc
urac
y (%
)
 
 
Asynch. Stepwise EM (4 processors)Synch. Stepwise EM (4 processors)Synch. Stepwise EM (1 processor)
Batch EM (1 processor)
0 1 2 3 4 5 6
?7.5
?7
?6.5
?6 x 10
6
Log
?Li
keli
hoo
d
0 1 2 3 4 5 6
50
55
60
65
Wall clock time (hours)
Acc
urac
y (%
)
 
 
Asynch. Stepwise EM (4 processors)Synch. Stepwise EM (4 processors)
Synch. Stepwise EM (1 processor)
Batch EM (1 processor)
Figure 6: POS: Asynchronous stepwise EM converges faster in log-likelihood and accuracy than the synchronous versions.
Curves are shown for each of 5 random initial models. One example ordering random seed is shown on the left, another on the
right. The accuracy curves for batch EM do not appear because the highest accuracy reached is only 40.7% after six hours.
over the 1-processor baseline.
The accuracy of the asynchronous curves of-
ten decreases slightly after peaking. We can sur-
mise from the log-likelihood plot that the drop
in accuracy is not due to the optimization be-
ing led astray, but probably rather due to the
complex relationship between likelihood and task-
specific evaluation metrics in unsupervised learn-
ing (Merialdo, 1994). In fact, when we exam-
ined the results of synchronous stepwise EM be-
tween 6 and 12 hours of execution, we found sim-
ilar drops in accuracy as likelihood continued to
improve. From Figure 6, we conclude that the
asynchronous algorithm has no harmful effect on
learned model?s accuracy beyond the choice to op-
timize log-likelihood.
While there are currently no theoretical conver-
gence results for asynchronous optimization algo-
rithms for non-convex functions, our results are
encouraging for the prospects of establishing con-
vergence results for this setting.
7 Discussion
Our best results were obtained by exploiting mul-
tiple processors on a single machine, while exper-
iments using a MapReduce cluster were plagued
by communication and framework overhead.
Since Moore?s Law predicts a continual in-
crease in the number of cores available on a sin-
gle machine but not necessarily an increase in the
speed of those cores, we believe that algorithms
that can effectively exploit multiple processors on
a single machine will be increasingly useful. Even
today, applications in NLP involving rich-feature
structured prediction, such as parsing and transla-
tion, typically use a large portion of memory for
storing pre-computed data structures, such as lex-
icons, feature name mappings, and feature caches.
Frequently these are large enough to prevent the
multiple cores on a single machine from being
used for multiple experiments, leaving some pro-
cessors unused. However, using multiple threads
in a single program allows these large data struc-
tures to be shared and allows the threads to make
use of the additional processors.
We found the overhead incurred by the MapRe-
duce programming model, as implemented in
Hadoop 0.20, to be substantial. Nonetheless,
we found that asynchronously running multiple
MapReduce calls at the same time, rather than
pooling all processors into a single MapReduce
call, improves observed convergence with negli-
gible effects on performance.
8 Conclusion
We have presented experiments using an asyn-
chronous framework for distributed mini-batch
optimization that show comparable performance
of trained models in significantly less time than
traditional techniques. Such algorithms keep pro-
cessors in constant use and relieve the programmer
from having to implement load-balancing schemes
for each new problem encountered. We expect
asynchronous learning algorithms to be broadly
applicable to training NLP models.
Acknowledgments The authors thank Qin Gao, Garth Gib-
son, Andre? Martins, Brendan O?Connor, Stephan Vogel, and
the reviewers for insightful comments. This work was sup-
ported by awards from IBM, Google, computing resources
from Yahoo, and NSF grants 0836431 and 0844507.
221
References
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
O. Cappe? and E. Moulines. 2009. Online EM algo-
rithm for latent data models. Journal of the Royal
Statistics Society: Series B (Statistical Methodol-
ogy), 71.
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In Proc. of EMNLP.
C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and
K. Olukotun. 2006. Map-Reduce for machine learn-
ing on multicore. In NIPS.
S. Clark and J.R. Curran. 2004. Log-linear models for
wide-coverage CCG parsing. In Proc. of EMNLP.
M. Collins. 2002. Ranking algorithms for named-
entity extraction: Boosting and the voted perceptron.
In Proc. of ACL.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
J. Dean and S. Ghemawat. 2004. MapReduce: Sim-
plified data processing on large clusters. In Sixth
Symposium on Operating System Design and Imple-
mentation.
C. Dyer, A. Cordova, A. Mont, and J. Lin. 2008. Fast,
easy, and cheap: Construction of statistical machine
translation models with MapReduce. In Proc. of the
Third Workshop on Statistical Machine Translation.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008.
Efficient, feature-based, conditional random field
parsing. In Proc. of ACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP-CoNLL.
J. Kazama and K. Torisawa. 2007. A new perceptron
algorithm for sequence labeling with non-local fea-
tures. In Proc. of EMNLP-CoNLL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML.
J. Langford, A. J. Smola, and M. Zinkevich. 2009.
Slow learners are fast. In NIPS.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278?2324.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL-HLT.
G. Mann, R. McDonald, M. Mohri, N. Silberman, and
D. Walker. 2009. Efficient large-scale distributed
training of conditional maximum entropy models.
In NIPS.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19:313?330.
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?172.
R. Mihalcea and T. Pedersen. 2003. An evaluation
exercise for word alignment. In HLT-NAACL 2003
Workshop: Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond.
R. Neal and G. E. Hinton. 1998. A view of the EM al-
gorithm that justifies incremental, sparse, and other
variants. In Learning in Graphical Models.
A. Nedic, D. P. Bertsekas, and V. S. Borkar. 2001.
Distributed asynchronous incremental subgradient
methods. In Proc. of the March 2000 Haifa Work-
shop: Inherently Parallel Algorithms in Feasibility
and Optimization and Their Applications.
N. Ratliff, J. Bagnell, and M. Zinkevich. 2006. Sub-
gradient methods for maximum margin structured
learning. In ICML Workshop on Learning in Struc-
tured Outputs Spaces.
M. Sato and S. Ishii. 2000. On-line EM algorithm for
the normalized Gaussian network. Neural Compu-
tation, 12(2).
B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin.
2005. Learning structured prediction models: A
large margin approach. In Proc. of ICML.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proc. of
CoNLL.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6:1453?1484.
J. Wolfe, A. Haghighi, and D. Klein. 2008. Fully
distributed EM for very large datasets. In Proc. of
ICML.
222
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 26?29,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Statistical Models for Frame-Semantic Parsing
Dipanjan Das
Google Inc.
76 9th Avenue,
New York, NY 10011
dipanjand@google.com
Abstract
We present a brief history and overview
of statistical methods in frame-semantic
parsing ? the automatic analysis of text us-
ing the theory of frame semantics. We dis-
cuss how the FrameNet lexicon and frame-
annotated datasets have been used by sta-
tistical NLP researchers to build usable,
state-of-the-art systems. We also focus on
future directions in frame-semantic pars-
ing research, and discuss NLP applications
that could benefit from this line of work.
1 Frame-Semantic Parsing
Frame-semantic parsing has been considered as
the task of automatically finding semantically
salient targets in text, disambiguating their se-
mantic frame representing an event and scenario
in discourse, and annotating arguments consisting
of words or phrases in text with various frame el-
ements (or roles). The FrameNet lexicon (Baker
et al., 1998), an ontology inspired by the theory
of frame semantics (Fillmore, 1982), serves as a
repository of semantic frames and their roles. Fig-
ure 1 depicts a sentence with three evoked frames
for the targets ?million?, ?created? and ?pushed?
with FrameNet frames and roles.
Automatic analysis of text using frame-
semantic structures can be traced back to the pi-
oneering work of Gildea and Jurafsky (2002). Al-
though their experimental setup relied on a prim-
itive version of FrameNet and only made use
of ?exemplars? or example usages of semantic
frames (containing one target per sentence) as op-
posed to a ?corpus? of sentences, it resulted in a
flurry of work in the area of automatic semantic
role labeling (M`arquez et al., 2008). However, the
focus of semantic role labeling (SRL) research has
mostly been on PropBank (Palmer et al., 2005)
conventions, where verbal targets could evoke a
?sense? frame, which is not shared across targets,
making the frame disambiguation setup different
from the representation in FrameNet. Further-
more, it is fair to say that early research on Prop-
Bank focused primarily on argument structure pre-
diction, and the interaction between frame and ar-
gument structure analysis has mostly been unad-
dressed (M`arquez et al., 2008). There are excep-
tions, where the verb frame has been taken into ac-
count during SRL (Meza-Ruiz and Riedel, 2009;
Watanabe et al., 2010). Moreoever, the CoNLL
2008 and 2009 shared tasks also include the verb
and noun frame identification task in their evalua-
tions, although the overall goal was to predict se-
mantic dependencies based on PropBank, and not
full argument spans (Surdeanu et al., 2008; Haji?c
et al., 2009).
The SemEval 2007 shared task (Baker et al.,
2007) attempted to revisit the frame-semantic
analysis task based on FrameNet. It introduced a
larger FrameNet lexicon (version 1.3), and also a
larger corpus with full-text annotations compared
to prior work, with multiple targets annotated per
sentence. The corpus allowed words and phrases
with noun, verb, adjective, adverb, number, deter-
miner, conjunction and preposition syntactic cat-
egories to serve as targets and evoke frames, un-
like any other single dataset; it also allowed targets
from different syntactic categories share frames,
and therefore roles. The repository of semantic
role types was also much richer than PropBank-
style lexicons, numbering in several hundreds.
Most systems participating in the task resorted
to a cascade of classifiers and rule-based modules:
identifying targets (a non-trivial subtask), disam-
biguating frames, identifying potential arguments,
and then labeling them with roles. The system
described by Johansson and Nugues (2007) per-
formed the best in this shared task. Next, we focus
on its performance, and subsequent improvements
made by the research community on this task.
26
Figure 1: A partial depiction of frame-semantic structures taken from Das et al. (2014). The words in bold correspond to targets,
which evoke semantic frames that are denoted in capital letters. Above each target is shown the corresponding lexical unit,
which is a lemma appended by a coarse part-of-speech tag. Every frame is shown in a distinct color; each frame?s arguments
are annotated with the same color, and are marked below the sentence, at different levels. For the CARDINAL NUMBERS frame,
?M? denotes the role Multiplier and ?E? denotes the role Entity.
P R F
1
SemEval?07 Data
(automatic targets)
Johansson and Nugues (2007) 51.59 35.44 42.01
Das et al. (2010) 58.08 38.76 46.49
FrameNet 1.5 Release Das et al. (2014) 68.33 61.14 64.54
(gold targets) Hermann et al. (2014) 72.79 64.95 68.64
Table 1: We show the current state of the art on the frame-semantic parsing task. The first section shows results on the
SemEval 2007 shared task. The best system in the task, presented by Johansson and Nugues (2007) was later outperformed
by SEMAFOR, a system described by Das et al. (2010). Both systems use a rule-based module to identify targets. On the
FrameNet 1.5 data, Das et al. (2014) presented additional semi-supervised experiments using gold targets, which was recently
outperformed by an approach presented by Hermann et al. (2014) that made use of distributed word representations.
2 Current State of the Art
Johansson and Nugues (2007) presented the sys-
tem that resulted in the best F
1
score on the Se-
mEval 2007 task of collectively identifying frame-
evoking targets, a disambiguated frame for each
target, and the set of role-labeled arguments for
each frame. The system contained a set of rule-
based heuristics to identify targets followed by a
cascade of three learned models as mentioned in
?1. Das et al. (2010) presented a tool called SE-
MAFOR,
1
which improved upon this system with
a similar framework for target identification, but
only used two probabilistic models, one for frame
identification, and one for predicting the argu-
ments. The frame identification subpart involved
a latent-variable log-linear model, which intended
to capture frames for unseen targets, many of
which appeared in the test data. Moreover, the fea-
ture sets in both the models were sufficiently dif-
ferent from prior work, resulting in improvements.
Table 1 shows results on the SemEval 2007 data
for these two systems.
The FrameNet project released more annota-
tions and a larger frame lexicon in 2010; Das et al.
(2014) used this dataset, and presented a variety of
experiments improving upon their prior work, set-
ting the new state of the art. A few salient aspects
1
See http://www.ark.cs.cmu.edu/SEMAFOR.
of this updated version of SEMAFOR involved
handling unseen targets using a graph-based semi-
supervised learning approach and improved infer-
ence using a dual decomposition algorithm. Sub-
sequently, Hermann et al. (2014) used a very simi-
lar framework but presented a novel method using
distributed word representations for better frame
identification, outperforming the aforementioned
update to SEMAFOR. Table 1 shows the perfor-
mance in terms of F
1
score for frames and ar-
guments given gold targets. Recent work on the
FrameNet corpora, including the aforementioned
two papers have used gold targets to measure the
performance of statistical methods because the
distribution of annotated targets in the data varied
significantly across documents and domains, mak-
ing it difficult to build a learnable system for target
identification.
The aforementioned papers focused on the
task of sentence-internal frame-semantic analysis.
There have been some investigation of finding im-
plicit arguments of frames that may be present in
other parts of a document, outside the sentential
context. Although there has not been extensive
research on this topic, a shared task at SemEval
2010 focused on this problem (Ruppenhofer et al.,
2010).
2
Moreover, there has been significant effort
2
Related work on the analysis of implicit arguments for
27
in developing unsupervised techniques for induc-
ing frame-semantic structures (Modi et al., 2012),
to induce FrameNet-like lexicons from weak su-
pervision, such as syntactic parses.
3 Applications
Shallow semantic analysis based on FrameNet
data has been recently utilized across various nat-
ural language processing applications with suc-
cess. These include the generation of meeting
summaries (Kleinbauer, 2012), the prediction of
stock price movement using (Xie et al., 2013), in-
ducing slots for domain-specific dialog systems
(Chen et al., 2013), stance classification in debates
(Hasan and Ng, 2013), modeling the clarity of stu-
dent essays (Persing and Ng, 2013) to name a few.
There is strong potential in using frame-
semantic structures in other applications such as
question answering and machine translation, as
demonstrated by prior work using PropBank-style
SRL annotations (Shen and Lapata, 2007; Liu and
Gildea, 2010).
4 Future Directions
Given the wide body of work in frame-semantic
analysis of text, and recent interest in using frame-
semantic parsers in NLP applications, the future
directions of research look exciting.
First and foremost, to improve the quality of au-
tomatic frame-semantic parsers, the coverage of
the FrameNet lexicon on free English text, and the
number of annotated targets needs to increase. For
example, the training dataset used for the state-of-
the-art system of Hermann et al. (2014) contains
only 4,458 labeled targets, which is approximately
40 times less than the number of annotated targets
in Ontonotes 4.0 (Hovy et al., 2006), a standard
NLP dataset, containing PropBank-style verb an-
notations. This comparison is important because
FrameNet covers many more syntactic categories
than the PropBank-style annotations, and features
more than 1,000 semantic role labels compared to
51 in Ontonotes, but severely lacks annotations. A
machine learned system would find it very hard
to generalize to new data given such data sparsity.
Increasing the quantity of such annotations re-
quires exhaustive inter-annotator agreement stud-
ies (which has been rare in FrameNet corpora gen-
eration) and the development of annotation guide-
nominal targets in NomBank (Meyers et al., 2004) has been
investigated recently (Gerber and Chai, 2012).
lines, such that these annotations can be produced
outside the FrameNet project.
Other than increasing the amount of labeled
data, there is a necessity of automatically aligning
predicate-level semantic knowledge present in re-
sources like FrameNet, PropBank, NomBank and
VerbNet (Schuler, 2005). These lexicons share a
lot of knowledge about predicates and current re-
sources like Ontonotes do align some of the infor-
mation, but a lot remains missing. For example,
alignment between these lexicons could be done
within a statistical model for frame-semantic pars-
ing, such that correlations between the coarse se-
mantic role labels in PropBank or NomBank and
the finer labels in FrameNet could be discovered
automatically.
Finally, the FrameNet data is an attractive test
bed for semi-supervised learning techniques be-
cause of data sparsity; distributed word represen-
tations, which often capture more semantic infor-
mation than surface-form features could be ex-
ploited in various aspects of the frame-semantic
parsing task.
Acknowledgments
The author thanks Desai Chen, Andr?e Martins,
Nathan Schneider and Noah Smith for numerous
discussions and several years of collaboration on
this topic at Carnegie Mellon. He is also grate-
ful to Kuzman Ganchev, Oscar T?ackstr?om, Karl
Moritz Hermann, Ryan McDonald, Slav Petrov,
Fernando Pereira and the greater Research at
Google community for helping research in this
area thrive over the past couple of years.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The Berkeley Framenet project. In Proceedings of
COLING-ACL.
C. F. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction.
In Proceedings of SemEval.
Y.-N. Chen, W. Y. Wang, and A. Rudnicky. 2013. Un-
supervised induction and filling of semantic slots for
spoken dialogue systems using frame-semantic pars-
ing. In Proceedings of ASRU.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010.
Probabilistic frame-semantic parsing. In Proceed-
ings of NAACL-HLT.
28
D. Das, D. Chen, A. F. T. Martins, N. Schneider, and
N. A. Smith. 2014. Frame-semantic parsing. Com-
putational Linguistics, 40(1):9?56.
C. J. Fillmore. 1982. Frame Semantics. In Linguis-
tics in the Morning Calm, pages 111?137. Hanshin
Publishing Co., Seoul, South Korea.
M. Gerber and J. Y. Chai. 2012. Semantic role labeling
of implicit arguments for nominal predicates. Com-
putational Linguistics, 38(4):755?798.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
J. Haji?c, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. M`arquez, A. Meyers, J. Nivre,
S. Pad?o, J.
?
St?ep?anek, P. Stra?n?ak, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of CoNLL.
K. S. Hasan and V. Ng. 2013. Frame semantics for
stance classification. In Proceedings of CoNLL.
K. M. Hermann, D. Das, J. Weston, and K. Ganchev.
2014. Semantic frame identification with distributed
word representations. In Proceedings of ACL.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solu-
tion. In Proceedings of NAACL-HLT.
R. Johansson and P. Nugues. 2007. LTH: semantic
structure extraction using nonprojective dependency
trees. In Proceedings of SemEval.
T. Kleinbauer. 2012. Generating automated meeting
summaries. Ph.D. thesis, Saarland University.
D. Liu and D. Gildea. 2010. Semantic role features for
machine translation. In Proceedings of COLING.
L. M`arquez, X. Carreras, K. C. Litkowski, and
S. Stevenson. 2008. Semantic role labeling: an in-
troduction to the special issue. Computational Lin-
guistics, 34(2):145?159.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In Pro-
ceedings of NAACL/HLT Workshop on Frontiers in
Corpus Annotation.
I. Meza-Ruiz and S. Riedel. 2009. Jointly identify-
ing predicates, arguments and senses using Markov
logic. In Proceedings of NAACL-HLT.
A. Modi, I. Titov, and A. Klementiev. 2012. Unsuper-
vised induction of frame-semantic representations.
In Proceedings of the NAACL-HLT Workshop on the
Induction of Linguistic Structure.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
I. Persing and V. Ng. 2013. Modeling thesis clarity in
student essays. In Proceedings of ACL.
J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker,
and M. Palmer. 2010. SemEval-2010 task 10: Link-
ing events and their participants in discourse. In
Proceedings of SemEval.
K. K. Schuler. 2005. Verbnet: A Broad-coverage,
Comprehensive Verb Lexicon. Ph.D. thesis, Univer-
sity of Pennsylvania.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez,
and J. Nivre. 2008. The CoNLL 2008 shared task
on joint parsing of syntactic and semantic dependen-
cies. In Proceedings of CoNLL.
Y. Watanabe, M. Asahara, and Y. Matsumoto. 2010. A
structured model for joint learning of argument roles
and predicate senses. In Proceedings of ACL.
B. Xie, R. J. Passonneau, L. Wu, and G. G. Creamer.
2013. Semantic frames to predict stock price move-
ment. In Proceedings of ACL.
29
