Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 64?67,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
64
65
66
67
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 808?813,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Using Paraphrases and Lexical Semantics to Improve the Accuracy and the
Robustness of Supervised Models in Situated Dialogue Systems
Claire Gardent
CNRS/LORIA, Nancy
claire.gardent@loria.fr
Lina M. Rojas Barahona
Universit? de Lorraine/LORIA, Nancy
lina.rojas@loria.fr
Abstract
This paper explores to what extent lemmati-
sation, lexical resources, distributional seman-
tics and paraphrases can increase the accuracy
of supervised models for dialogue manage-
ment. The results suggest that each of these
factors can help improve performance but that
the impact will vary depending on their com-
bination and on the evaluation mode.
1 Introduction
One strand of work in dialog research targets the
rapid prototyping of virtual humans capable of con-
ducting a conversation with humans in the context
of a virtual world. In particular, question answering
(QA) characters can respond to a restricted set of
topics after training on a set of dialogs whose utter-
ances are annotated with dialogue acts (Leuski and
Traum, 2008).
As argued in (Sagae et al, 2009), the size of the
training corpus is a major factor in allowing QA
characters that are both robust and accurate. In ad-
dition, the training corpus should arguably be of
good quality in that (i) it should contain the various
ways of expressing the same content (paraphrases)
and (ii) the data should not be skewed. In sum, the
ideal training data should be large (more data is
better data) ; balanced (similar amount of data for
each class targeted by the classifier) and varied (it
should encompass the largest possible number of
paraphrases and synonyms for the utterances of each
class).
In this paper, we explore different ways of im-
proving and complementing the training data of a
supervised QA character. We expand the size and
the quality (less skewed data) of the training corpus
using paraphrase generation techniques. We com-
pare the performance obtained on lemmatised vs.
non lemmatised data. And we investigate how vari-
ous resources (synonym dictionaries, WordNet, dis-
tributional neighbours) can be used to handle unseen
words at run time.
2 Related work
Previous work on improving robustness of super-
vised dialog systems includes detecting and han-
dling out of domain utterances for generating feed-
back (Lane et al, 2004) ; using domain-restricted
lexical semantics (Hardy et al, 2004) ; and work on
manual data expansion (DeVault et al, 2011). Our
work follows up on this research but provides a sys-
tematic investigation of how data expansion, lemma-
tisation and synonym handling impacts the perfor-
mance of a supervised QA engine.
3 Experimental Setup
We run our experiments on a dialog engine de-
veloped for a serious game called Mission Plastech-
nologie. In this game, the player must interact with
different virtual humans through a sequence of 12
subdialogs, each of them occurring in a different part
of the virtual world.
Training Data. The training corpus consists of
around 1250 Human-Human dialogues which were
manually annotated with dialog moves. As the fol-
lowing dialog excerpt illustrates, the dialogs are con-
ducted in French and each dialog turn is manu-
ally annotated using a set of 28 dialog acts. For
808
a more detailed presentation of the training corpus
and of the annotation scheme, the reader is referred
to (Rojas-Barahona et al, 2012a)
dialog : 01_dialogDirecteur-Tue Jun 14 11 :04 :23 2011
>M.Jasper : Bonjour, je suis M.Jasper le directeur. || greet
(Hello, I am the director, Mr. Jasper.)
>M.Jasper : Qu?est-ce que je peux faire pour vous ? || ask(task(X))
(What can I do for you ?)
>Lucas : je dois sauver mon oncle || first_step
(I must rescue my uncle)
>M.Jasper : Pour faire votre manette, il vous faut
des plans. Allez voir dans le bureau d??tudes,
ils devraient y ?tre. || inform(do(first_step))
(To build the joystick you will need the plans.
You will find them in the Designing Office.)
>M.Jasper : Bonne Chance ! || quit
(Good Luck !)
Dialog Systems For our experiments, we use a hy-
brid dialog system similar to that described in (Ro-
jas Barahona et al, 2012b; Rojas Barahona and
Gardent, 2012). This system combines a classifier
for interpreting the players utterances with an infor-
mation state dialog manager which selects an appro-
priate system response based on the dialog move as-
signed by the classifier to the user turn. The clas-
sifier is a logistic regression classifier 1 which was
trained for each subdialog in the game. The features
used for training are the set of content words which
are associated with a given dialog move and which
remain after TF*IDF 2 filtering. Note that in this ex-
periment, we do not use contextual features such as
the dialog acts labeling the previous turns. There are
two reasons for this. First, we want to focus on the
impact of synonym handling, paraphrasing and lem-
matisation on dialog management. Removing con-
textual features allows us to focus on how content
features (content words) can be improved by these
mechanisms. Second, when evaluating on the H-C
corpus (see below), contextual features are often in-
correct (because the system might incorrectly inter-
pret and thus label a user turn). Excluding contextual
features from training allows for a fair comparison
between the H-H and the H-C evaluation.
Test Data and Evaluation Metrics We use accu-
1. We used MALLET (McCallum, 2002) for the LR classi-
fier with L1 Regularisation.
2. TF*IDF = Term Frequency*Inverse Document Fre-
quency
racy (the number of correct classifications divided
by the number of instances in the testset) to mea-
sure performance and we carry out two types of
evaluation. On the one hand, we use 10-fold cross-
validation on the EmoSpeech corpus (H-H data). On
the other hand, we report accuracy on a corpus of
550 Human-Computer (H-C) dialogues obtained by
having 22 subjects play the game against the QA
character trained on the H-H corpus. As we shall see
below, performance decreases in this second evalua-
tion suggesting that subjects produce different turns
when playing with a computer than with a human
thereby inducing a weak out-of-domain effect and
negatively impacting classification. Evaluation on
the H-H corpus therefore gives a measure of how
well the techniques explored help improving the di-
alog engine when used in a real life setting.
Correspondingly, we use two different tests for
measuring statistical significance. In the H-H eval-
uation, significance is computed using the Wilcoxon
signed rank test because data are dependent and are
not assumed to be normally distributed. When build-
ing the testset we took care of not including para-
phrases of utterances in the training partition (for
each paraphrase generated automatically we keep
track of the original utterance), however utterances
in both datasets might be generated by the same sub-
ject, since a subject completed 12 distinct dialogues
during the game. Conversely, in the H-C evaluation,
training (H-H data) and test (H-C data) sets were
collected under different conditions with different
subjects therefore significance was computed using
the McNemar sign-test (Dietterich, 1998).
4 Paraphrases, Synonyms and
Lemmatisation
We explore three main ways of modifying the
content features used for classification : lemmatising
the training and the test data ; augmenting the train-
ing data with automatically acquired paraphrases ;
and substituting unknown words with synonyms at
run time.
Lemmatisation We use the French version of
Treetagger 3 to lemmatise both the training and the
test data. Lemmas without any filtering were used
3. http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/
809
to train classifiers. We then compare performance
with and without lemmatisation. As we shall see,
the lemma and the POS tag provided by TreeTag-
ger are also used to lookup synonym dictionaries and
EuroWordNet when using synonym handling at run
time.
Paraphrases : (DeVault et al, 2011) showed that
enriching the training corpus with manually added
paraphrases increases accuracy. Here we exploit au-
tomatically acquired paraphrases and use these not
only to increase the size of the training corpus but
also to better balance it 4. We proceed as follows.
First, we generated paraphrases using a pivot ma-
chine translation approach where each user utter-
ance in the training corpus (around 3610 utterances)
was translated into some target language and back
into French. Using six different languages (English,
Spanish, Italian, German, Chinese and Arabian),
we generated around 38000 paraphrases. We used
Google Translate API for translating.
Category Train Instances Balanced Instances
greet 24 86
help 20 82
yes 92 123
no 55 117
ack 73 135
other 27 89
quit 38 100
find_plans 115 146
job 26 88
staff 15 77
studies 20 82
security_policies 24 86
? 44.08 100.92
? ?32.68 ?23.32
TABLE 1: Skewed and Balanced Data on a sample sub-
dialog. The category with lowest number of paraphrases
is greet, with 62 paraphrases, hence lp = 62. All cat-
egories were increased by 62 except find_plans and
yes that were increased by half : 31.
Second, we eliminate from these paraphrases,
words that are likely to be incorrect lexical transla-
tions by removing words with low normalized term
4. The Emospeech data is highly skewed with some classes
being populated with many utterances and others with few.
Algorithm extendingDataWithParaphrases(trainingset ts)
1. Let c be the set of categories in ts.
2. ? be the mean of train instances per category
3. ? be the standard deviation of train instances per category
4. Let Npc be the number of paraphrases per category
5. Let lp ? min Npcj
6. Repeat
7. set i ? 0
8. Ninstci be the number of instances per category ci
9. di ? Ninstci ? ?
10. if di < ? then
11. Ninstci ? lp
12. else
13. Ninstci ?
lp
2
14. end if
15. set i?i+1
16. if i>?c? then
17. terminate
18. end
FIGURE 1: Algorithm for augmenting the training data
with paraphrases.
frequency (< 0.001) across translations i.e., lexical
translations given by few translations and/or transla-
tion systems. We then preprocessed the paraphrases
in the same way the utterances of the initial train-
ing corpus were preprocessed i.e., utterances were
unaccented, converted to lower-case and stop words
were removed, the remaining words were filtered
with TF*IDF. After preprocessing, duplicates were
removed.
Third, we added the paraphrases to the training
data seeking to improve the balance between dialog
moves per dialog, as shown in Figure 1. To this end,
we look for the category c with the lowest number
of paraphrases lp (line 5). We then compute the de-
viation di for each dialog move ci from the mean
? in the original training set (line 9). If the devia-
tion di is lower than the standard deviation then we
add lp number of paraphrases instances (line 11).
Conversely, if di is higher than the standard devia-
tion, we reduce the number of instances to be added
by half lp2 (line 13). Table 1 shows the original and
the extended training data for the third sub-dialog
in the Emospeech game. In this dialogue the player
is supposed to ask information about the joystick
plans (find_plans, which is the mandatory goal).
The categories cover mandatory and optional goals
and general dialogue acts, such as greetings, asking
for help, confirm and disconfirm, acknowledgment
and out of topic questions (i.e. other).
Substituting Synonyms for Unknown Words A
word is unknown, if it is a well-formed French
810
word 5 and if it does not appear in the training cor-
pus. Conversely, a word is known if it is not un-
known.
When an unknown word w is detected in a player
utterance at runtime, we search for a word w? which
occurs in the training data and is either a synonym of
w or a distributional neighbour. After disambigua-
tion, we substitute the unknown word for the syn-
onym.
To identify synonyms, we make use of two lexical
resources namely, the French version of EuroWord-
Net (EWN) (Vossen, 1998), which includes 92833
synonyms, hyperonyms and hyponyms pairs, and a
synonym lexicon for French (DIC) 6 which contains
38505 lemmas and 254149 synonym pairs. While
words are categorised into Noun, Verbs and Adjec-
tives in EWN, DIC contains no POS tag information.
To identify distributional neighbours, we con-
structed semantic word spaces for each subdialog
in the EmoSpeech corpus 7 using random indexing
(RI) 8 on the training corpus expanded with para-
phrases. Using the cosine measure as similarity met-
rics, we then retrieve for any unknown word w, the
word w? which is most similar to w and which ap-
pear in the training corpus.
For lexical disambiguation, two methods are com-
pared. We use the POS tag provided by TreeTagger.
In this case, disambiguation is syntactic only. Or we
pick the synonym with highest probability based on
a trigram language model trained on the H-H cor-
pus 9.
5 Results and Discussion
Table 2 summarises the results obtained in four
main configurations : (i) with and without para-
phrases ; (ii) with and without synonym handling ;
(iii) with and without lemmatisation ; and (iv) when
5. A word is determined to be a well-formed French word if
it occurs in the LEFFF dictionary, a large-scale morphological
and syntactic lexicon for French (Sagot, 2010)
6. DICOSYN (http ://elsap1.unicaen.fr/dicosyn.html).
7. We also used distributional semantics from the Gigaword
corpus but the results were poor probably because of the very
different text genre and domains between the the Gigaword and
the MP game.
8. Topics are Dialog acts while documents are utterances ;
we used the S-Space Package http://code.google.com/p/
airhead-research/wiki/RandomIndexing
9. We used SRILM (http://www.speech.sri.com/
projects/srilm)
combining lemmatisation with synonym handling.
We also compare the results obtained when evalu-
ating using 10-fold cross validation on the training
data (H-H dialogs) vs. evaluating the performance
of the system on H-C interactions.
Overall Impact The largest performance gain is
obtained by a combination of the three techniques
explored in this paper namely, data expansion, syn-
onym handling and lemmatisation (+8.9 points for
the cross-validation experiment and +2.3 for the H-
C evaluation).
Impact of Lexical Substitution at Run Time Be-
cause of space restrictions, we do not report here
the results obtained using lexical resources without
lemmatisation. However, we found that lexical re-
sources are only useful when combined with lemma-
tisation. This is unsurprising since synonym dictio-
naries and EuroWordNet only contain lemmas. In-
deed when distributional neighbours are used, lem-
matisation has little impact (e.g., 65.11% using dis-
tributional neighbours without lemmatisation on the
H-H corpus without paraphrases vs. 66.41% when
using lemmatisation).
Another important issue when searching for a
word synonym concerns lexical disambiguation : the
synonym used to replace an unknown word should
capture the meaning of that word in its given con-
text. We tried using a language model trained on the
training corpus to choose between synonym candi-
dates (i.e., selecting the synonym yielding the high-
est sentence probability when substituting that syn-
onym for the unknown word) but did not obtain a
significant improvement. In contrast, it is noticeable
that synonym handling has a higher impact when us-
ing EuroWordNet as a lexical resource. Since Eu-
roWordNet contain categorial information while the
synonym dictionaries we used do not, this suggests
that the categorial disambiguation provided by Tree-
Tagger helps identifying an appropriate synonym in
EuroWordNet.
Finally, it is clear that the lexical resources used
for this experiment are limited in coverage and qual-
ity. We observed in particular that some words which
are very frequent in the training data (and thus which
could be used to replace unknown words) do not oc-
cur in the synonym dictionaries. For instance when
using paraphrases and dictionaries (fourth row and
811
H Lemmatisation
H-H Orig. Lemmas +EWN +DIC +RI
Orig. 65.70%? 5.62 66.04%? 6.49 68.17%? 6.98 67.92%? 4.51 66.83%? 5.92
Parap. 70.89%? 6.45 74.31%? 4.78* 74.60%? 5.99* 73.07%? 7.71* 72.63%? 5.82*
H-C Orig. Lemmas +EWN +DIC +RI
Orig. 59.71%? 16.42 59.88%? 7.19 61.14%? 16.65 61.41%? 16.59 60.75%? 17.39
Parap. 59.82%? 15.53 59.48%? 14.02 61.70%? 14.09* 62.01%? 14.37* 61.16%? 14.41*
TABLE 2: Accuracy on the H-H and on the H-C corpus. The star denotes statistical significance with the Wilcoxon test
(p < 0.005) used for the HH corpus and the McNemar test (p < 0.005) for the HC corpus.
fourth column in Table 2) 50% of the unknown
words were solved, 17% were illformed and 33% re-
mained unsolved. To compensate this deficiency, we
tried combining the three lexical resources in vari-
ous ways (taking the union or combining them in a
pipeline using the first resource that would yield a
synonym). However the results did not improve and
even in some cases worsened due probably to the in-
sufficient lexical disambiguation. Interestingly, the
results show that paraphrases always improves syn-
onym handling presumably because it increases the
size of the known vocabulary thereby increasing the
possibility of finding a known synonym.
In sum, synonym handling helps most when (i)
words are lemmatised and (ii) unknown words can
be at least partially (i.e., using POS tag information)
disambiguated. Moreover since data expansion in-
creases the set of known words available as potential
synonyms for unknown words, combining synonym
handling with data expansion further improves ac-
curacy.
Impact of Lemmatisation When evaluating using
cross validation on the training corpus, lemmatisa-
tion increases accuracy by up to 3.42 points indi-
cating that unseen word forms negatively impact ac-
curacy. Noticeably however, lemmatisation has no
significant impact when evaluating on the H-C cor-
pus. This in turn suggests that the lower accuracy
obtained on the H-C corpus results not from unseen
word forms but from unseen lemmas.
Impact of Paraphrases On the H-H corpus, data
expansion has no significant impact when used
alone. However it yields an increase of up to 8.27
points and in fact, has a statistically significant im-
pact, for all configurations involving lemmatisation.
Thus, data expansion is best used in combination
with lemmatisation and their combination permits
creating better, more balanced and more general
training data. On the H-C corpus however, the im-
pact is negative or insignificant suggesting that the
decrease in performance on the H-C corpus is due to
content words that are new with respect to the train-
ing data i.e., content words for which neither a syn-
onym nor a lemma can be found in the expanded
training data.
Conclusion
While classifiers are routinely trained on dialog
data to model the dialog management process, the
impact of such basic factors as lemmatisation, au-
tomatic data expansion and synonym handling has
remained largely unexplored. The empirical eval-
uation described here suggests that each of these
factors can help improve performance but that the
impact will vary depending on their combination
and on the evaluation mode. Combining all three
techniques yields the best results. We conjecture
that there are two main reasons for this. First, syn-
onym handling is best used in combination with
POS tagging and lemmatisation because these sup-
ports partial lexical semantic disambiguation. Sec-
ond, data expansion permits expanding the set of
known words thereby increasing the possibility of
finding a known synonym to replace an unknown
word with.
Acknowledgments
This work was partially supported by the EU
funded Eurostar EmoSpeech project. We thank
Google for giving us access to the University Re-
search Program of Google Translate.
812
References
David DeVault, Anton Leuski, and Kenji Sagae. 2011.
Toward learning and evaluation of dialogue policies
with text examples. In 12th SIGdial Workshop on Dis-
course and Dialogue, Portland, OR, June.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Computation, 10 :1895?1923.
Hilda Hardy, Tomek Strzalkowski, Min Wu, Cristian
Ursu, Nick Webb, Alan W. Biermann, R. Bryce In-
ouye, and Ashley McKenzie. 2004. Data-driven
strategies for an automated dialogue system. In ACL,
pages 71?78.
Ian Richard Lane, Tatsuya Kawahara, and Shinichi Ueno.
2004. Example-based training of dialogue planning
incorporating user and situation models. In INTER-
SPEECH.
Anton Leuski and David Traum. 2008. A statistical ap-
proach for text processing in virtual humans. In Pro-
ceedings of the 26th Army Science Conference.
Andrew Kachites McCallum. 2002. Mallet : A ma-
chine learning for language toolkit. http ://mal-
let.cs.umass.edu.
Lina Maria Rojas Barahona and Claire Gardent. 2012.
What should I do now ? Supporting conversations in
a serious game. In SeineDial 2012 - 16th Workshop
on the Semantics and Pragmatics of Dialogue, Paris,
France. Jonathan Ginzburg (chair), Anne Abeill?, Mar-
got Colinet, Gregoire Winterstein.
Lina M. Rojas-Barahona, Alejandra Lorenzo, and Claire
Gardent. 2012a. Building and exploiting a corpus
of dialog interactions between french speaking virtual
and human agents. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Evalu-
ation.
Lina M. Rojas Barahona, Alejandra Lorenzo, and Claire
Gardent. 2012b. An end-to-end evaluation of two
situated dialog systems. In Proceedings of the 13th
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 10?19, Seoul, South Ko-
rea, July. Association for Computational Linguistics.
K. Sagae, G. Christian, D. DeVault, , and D.R. Traum.
2009. Towards natural language understanding of par-
tial speech recognition results in dialogue systems. In
Proceedings of Human Language Technologies : The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), Companion Volume : Short Papers, pages
53?56.
Beno?t Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for French. In 7th international conference on Lan-
guage Resources and Evaluation (LREC 2010), Val-
letta, Malta.
Piek Vossen, editor. 1998. EuroWordNet : a multilin-
gual database with lexical semantic networks. Kluwer
Academic Publishers, Norwell, MA, USA.
813
Using MMIL for the High Level Semantic Annotation of the
French MEDIA Dialogue Corpus.?
Lina Maria Rojas-Barahona
LORIA/INRIA, France
lina.rojas@loria.fr
Thierry Bazillon
Univ. Avignon, France
thierry.bazillon@univ-avignon.fr
Matthieu Quignard
LORIA/INRIA, France
matthieu.quignard@loria.fr
Fabrice Lefevre
Univ. Avignon, France
fabrice.lefevre@univ-avignon.fr
Abstract
The MultiModal Interface Language formalism (MMIL) has been selected as the High Level
Semantic (HLS) formalism for annotating the French MEDIA dialogue corpus. This corpus is com-
posed of human-machine dialogues in the domain of hotel reservation and tourist information. Utter-
ances in dialogues have been previously annotated with a concept-value flat semantics for studying
and evaluating spoken language understanding modules in dialogue systems. We are now interested
in investigating the use of more complex representations to improve the understanding capability.
The MMIL intermediate language is a high level semantic formalism that bears relevant linguistic
information, from syntax up to discourse. This representation should increase the expressivity of
the current annotation though at the expense of the annotation process complexity. In this paper we
present our first attempt in defining the annotation guidelines for the HLS annotation of the MEDIA
corpus and its effect on the annotation process itself, revealed by annotators? disagreements due to
the different levels of hierarchy and the granularity of the features defined in MMIL.
1 Introduction
MMIL is an ontology-oriented representation language that has been used in several natural language
processing (NLP) applications, Denis et al (2010). It permits the integration of divergent resources in
distributed systems as well as the representation of various levels of linguistic analysis. In this work we
are particularly interested in exploring the representation of these linguistic levels for analyzing utter-
ances in the context of human-machine interactions. To be able to evaluate the representation on a large
set of data the French MEDIA dialogue corpus is used, Bonneau-Maynard et al (2005). The MEDIA
corpus collects about 70 hours of spontaneous speech in the task of hotel room reservation and tourist
information. It has been created using a Wizard-of-Oz technique, as a consequence, the utterances are
made of many disfluencies, hesitations, false starts, truncations or fillers words (e.g., euh or ben). Thus,
the syntactic analysis is relevant for keeping valuable information for further processing (e.g., reference
resolution). The semantics describe fine grained predicates, arguments and features based on the domain
knowledge. Similarly, the possibility of link references for pragmatic analysis and the representation of
the illocutionary force of utterances are relevant to improve the understanding in NLP applications. We
selected MMIL for the semantic annotation because it supports the representation of all these features.
Although these features enrich the semantic annotation of utterances in the corpus, they also increase
the complexity of the annotation and compromise the agreement between annotators. The possibility
of representing different instantiations in MMIL has been the main cause of disagreement between an-
notators. On the one hand, linguists tend to annotate the surface form of the utterance. On the other
?This work is supported by the French Agence Nationale de la Recherche (ANR) and is part of the Project PORT-MEDIA
(www.port-media.org).
375
hand, application designers are more biased towards its canonical representation by keeping relevant
task oriented actions and features. The trade-off between these two lines of representation is significant
for building appropriately the annotation guidelines for the semantic annotation. The annotation would
keep the most valuable information in a multilevel representation for enhancing the understanding ca-
pability of NLP applications. In this paper we introduce briefly MMIL and we describe the annotation
methodology and the inter-annotation agreement.
2 The High Level Representation
MMIL permits the representation of communicative actions that are represented as components. A com-
ponent is a structure that gathers the communicative event and its propositional content. Components
are made up of two main types of entities: events, which are entities anchored in the time dimension,
and participants, which are entities not bounded by time. Entities are linked together by relations and
are described by sets of features (i.e. pairs of attribute-value), Denis et al (2010). Every component
has a unique communicative event with the illocutionary force represented by means of the dialogueAct
feature. The propositional content is represented as a main event with its arguments, which can be either
events or participants, linked to the communicative event by a relation propContent. In this represen-
tation, predicates are usually represented as events and predicate arguments are usually represented as
participants. Relations between participants and events usually describe the thematic roles.
French: "/1euh vous venez de dire que pre?ce?demment qu? il n? a y avait plus de chambres disponibles a` ces dates et maintenant
vous en avez/2 donc je voulais juste m? assurer qu? au Novotel vous avez bien une chambre double euh pour un couple avec un
enfant avec une baignoire dans la chambre euh il me il me faut un Parc ?a? proximite? et euh cent dix euros maximum la nuit
est-ce-que vous pouvez ve?rifier"
English:"/1um you just said earlier that there are not more rooms available on these dates and now there are/2 so I just
wanted to be sure that you have at the Novotel a double room for uh a couple with one child with a bath in the room uh I
need a park nearby and uh hundred and ten euros up at night is that you can check"
Figure 1: Example of a complex utterance of the MEDIA Corpus.
Speak
Inform
Comprendre
(Understand)
negative
Coordination
adversative
State
State
negative
Pe?riodeDe
Temps
(Time)
demonstrat.
Chambre
(Room)
disponible
propContent
patient
member
memberpatient
aPe?riodeRe?servation
Speak
RequestAck
State
Chambre
(Room)
indefinite
Hotel
Couple
location.
Relative
proche
(near)
parc
(park)
Enfant
(Child)
Prix
(Price)
inferieur
(lower)
110
euros
propContent
patient
aBe?ne?ficiaires
attribute
aLocalisation
aPrix
Figure 2: HLS as an abstraction of the meaning of the French utterance shown in Figure 1. Left: this component expresses the inform
of a misunderstanding of the first segment (?/1" in Figure 1). Right: this component is a request acknowledgment, representing the second
segment(?/2" in Figure 1). Note that events are exemplify by square boxes while participants are exemplify by ellipses.
376
Let us focus on the MMIL representation for a typical utterance of the MEDIA corpus, given in
Figure 1. In this utterance the user first announces an inconsistency, then asks for clarification. Thus,
two MMIL components with different communicative actions, inform and request acknowledgment, have
been used, as shown in Figure 2. The component on the left has a main event that describes the misun-
derstanding expressed in the first segment1 of the utterance. It is represented by the ontological concept
?Understand" and by the syntactic feature polarity with the negative value. It also contains a coordinated
entity mirroring an adversative coordination between two events, state. The event state represents the
status of something, therefore the negated state event can be understood as ?there are not more rooms
available on these dates" while the positive state represents ?now there are". The participants symbolize
the arguments ?rooms" and ?dates" respectively. The component on the right expresses the clarification
request of the second segment. It verifies the status of the hotel with the specific constraints.
3 The Annotation Methodology
In the process of defining the annotation guidelines, we elaborated a specification document that de-
scribes the representation of dialogue acts, events and exemplifies the high-level semantics. Moreover, it
delves into the methodology that might be applied for the automatic and manual annotation. Afterwards,
a linguist expert and a project designer were in charge of defining the annotation guidelines. For this
purpose, they annotated manually a subset of utterances which were supposed to be representative of
the most complex aspects of the HLS annotation, in terms of their semantic constituents. 330 utterances
were selected. They are all directly related to the reservation task (first two rows in Figure 4) and mostly
occurred in the first 3 turns of the dialogues when the user is describing his goal, defined as an overall
objective along with a set of constraints. Hereafter, we present the preliminary evaluation of the experts?
agreement on these utterances.
The annotation process has been supported by an annotation tool: ATool. It accesses two knowledge-
bases, one for the MMIL formalism and the other for the MEDIA domain. The latter is adapted from
the MEDIA evaluation campaign, Bonneau-Maynard et al (2006). ATool permits annotators to navi-
gate through utterances, while displaying the MMIL representation. Annotators can design the MMIL
components graphs, define the MMIL entities by associating features, values and segment. ATool will
suggest the possible features and values for the MMIL formalism and for the domain according to the
knowledge-bases ensuring the integrity of the constructed MMIL components in the annotation.
The MEDIA corpus is rich in expressions that evoke several communicative actions. Figure 4 shows
a few examples. For the purpose of the task, we are interested in the underlying meaning of sentences,
thus politeness and indirectness are discarded from the HLS representation. For this reason, in requests
the speaker is the patient, while the hearer is the agent (see Figure 4). Because when translating the
utterance into its deep instantiation, the speaker will benefit from the execution of the action, while the
hearer has the obligation to perform the action. All the expressions in the corpus that bear the seman-
tics of ?command for a reservation" (e.g., je veux re?server, je souhaite re?server, je voudrais faire une
re?servation, j?aimerais faire une re?servation, all equivalent to I would like to reserve), have been normal-
ized with the deep component shown in Figure 3, exemplifying unequivocally the user?s desire to request
for a reservation. The possible arguments and roles have been detailed in the domain knowledge-base.
As a consequence the knowledge-base defines relations between hotels, rooms, customers, prices, equip-
ments, services, locations and dates. Besides, the grammatical relations and features, such as coordina-
tion, have been defined in the MMIL knowledge-base. Coordination is indicated with the ?coordtype"
feature and it is used in cases of conjunction (je veux une chambre simple et deux chambres double, I
want a single room and two double bedrooms), disjunction (Paris ou en proche banlieue, in Paris or
suburbs) or adversation (en ville mais pas trop loin de la mer, in the city but not too far from the sea).
For annotating events we can find the main verb in the utterance and represent it as the main event
in MMIL by following a domain-specific classification of verbs, from which Figure 4 shows some
equivalences among dialogue acts and verbs. For each participant or event, several features can be
1Segments are sequence of words that are depicted as ?/i", where i is the number of the segment.
377
Speak
Request
Reserver
je arg0 argi
propContent
patient[0]
patient[1] patient[n]
Figure 3: Canonical representation of a booking request in
MEDIA.
D. Act EvType Examples Semantic Roles
Request Reserver re?server [la chambre] aObjetRe?serve?
re?server [pour le
troisie`me
week-end de novembre
une nuit]
aPe?riodeRe?servation
[a? Clermont-Ferrand] aLocalisation
[pour quatre chambres
doubles]
aObjetRe?serve?
Inform Inform [j?] ai des informations
supple?mentaires
agent
Request Inform [j?] aurais aim?l? avoir
exactement [les dates]
patient[0],
patient[1]
Request State [Il] est [?a? combien] patient, aPrix
Request Repeter pouvez-[vous] re?pe?ter agent
Inform Repeter [je] vais me re?pe?ter agent
Accept oui
Reject non
Figure 4: Some of the observed dialogue acts and main
events with their arguments in the corpus.
added. The most important of them are ?object type" (for participants) or ?event type" (for events),
which specify their ontological concepts. They may be re?server (reserve), h?tel (hotel), chambre (room),
pe?riodedetemps (time), ville (city), person, adulte (adult), enfant (child), localisationnomme?e (places),
among others. There are more specific features, for instance, the journey dates, hotel features (e.g.,
name, standing, services, etc). Some of these features have predefined values, such as the gender of an
object (either masculine or feminine). On the other side, features such as cardinality, have not predefined
values, in that case, the annotator has to manually indicate the correct value.
Obviously, the annotation task difficulty increases with the utterance?s complexity. The representa-
tion is rather tedious to define in elliptical utterances, such as multiple reservations, in which implicit
and explicit information must be taken under consideration. Furthermore, the MMIL formalism does
not support the association of discontinuous segments to entities, generating some imprecisions in the
HLS annotation. For instance, in je voudrais une chambre pour deux personnes euh simple (I would
like a room for two people uh simple),?une chambre" (a room) and ?simple" should be linked to an
unique participant, having as object type (?Room") and as type of room ?simple". However, given that
the speaker has not mentioned ?simple"right after ?chambre", there is a new element imbricated between
them: ?pour deux personnes". As a result, the annotator must integrate the subsegment ?pour deux per-
sonnes" in the ?Room" participant. Even though this subsegment is also associated to the ?Personne"
participant.
4 Results
When analyzing the sample of 330 utterances that were annotated, we found a perfect agreement be-
tween annotators in the detection of dialogue-acts, main events, as well as main arguments. In constrast,
when measuring fine-grained features inside components we found eight types of disagreement, namely
conjunctions, disjunctions, creation of participant for simple features, groups of features inside entities,
features of entities, values of features, relation names and relation among entities. The most frequent
cases concern the first two, which refer to coordination: conjunctions (20%) and disjunctions (5%). The
inter-annotator agreement for the coordinate entities was computed, obtaining the kappa measure, Car-
letta (1996), of 0.25 for conjunctions and 0.15 for disjunctions, meaning a fair and slight agreement
respectively. Although the other cases were less frequent, the inter-annotator agreement was even lower,
indicating no agreement.
In spite of the disagreement, when measuring the global similarity between the MMIL components
created by both annotators we found a high score of 98%. This metric measures the graph similarity
378
by computing the similarity between entities and relations, including the fine-grained features inside
entities. The speech-act, main-event and main arguments are in compliance with the specifications in
both annotations.
Case Annotator 1 Annotator 2
Conjunctions 68 56
Disjunctions 18 10
Part. for simple feats. 11 0
Grouping feats. 0 2
Case Discrepancy
Features 4
Features? values 5
Name of relations. 5
Relation among entities. 2
Figure 5: Left: the Table displays the number of utterances by annotator for the listed cases. Annotator 1, is the liguist expert, Annotator 2
is the project designer. Right: fhe Table shows the number of utterances with a completely discrepant annotation: different features for same
entities, different values for same features, different relation between same entities and entities related differently in a component.
These issues show that the disagreement cases were less frequent. So far, annotators have not being
so rigourous when segmenting the text inside features. Therefore, segmentation needs to be checked in
both annotations. After this experiment, we are defining the final certified annotation and deriving the
annotation guidelines formally.
5 Discussion
Defining the annotation guidelines for high level semantic representation is controversial. The multiple
features that can be represented in the selected MMIL formalism, as well as the multiple instantiations
offer different possibilities for representing the same utterance. In general representing spoken utterances
is cumbersome, because of the linguist phenomena present in spontaneous speech. As a consequence,
annotators have to deal not only with the explicit, but also with the implicit information, and in some
cases the representations might be subjective. For these reasons, we defined the standard for the annota-
tion, and based on it, we carried out an annotation experiment on a sample of 330 complex utterances,
directly related to the reservation task; involving two annotator profiles i.e., a linguist and a project
designer. Afterwards, we measured the similarity between the annotated MMIL components and the
inter-annotation agreement obtaining a 98% of similarity and only eight major cases of disagreement,
coordination discrepancy being the most frequent. Right now, we are refining the final annotation guide-
lines based on these results. This first experiment analyzes the most complex and numerous utterances
in the corpus covering reservation requests and affirmations. Subsequently, misunderstanding, questions
and clarifications will be analyzed following the same methodology. As a result, we will be able to
reduce the disagreement between annotators in order to produce the annotation of the whole MEDIA
corpus, which will be made freely available to the research community.
References
Bonneau-Maynard, H., C. Ayache, F. Bechet, A. Denis, A. Kuhn, F. Lefe`vre, D. Mostefa, M. Quignard, S. Ros-
set, C. Servan, , and J. Villaneau (2006). Results of the french evalda-media evaluation campaign for literal
understanding. In 5th International Conference on Language Resources and Evaluation (LREC2006).
Bonneau-Maynard, H., S. Rosset, C. Ayache, A. Kuhn, and D. Mostefa (2005). Semantic annotation of the french
media dialog corpus. In INTERSPEECH-2005, 3457-3460.
Carletta, J. (1996). Assessing agreement on classification tasks: the kappa statistic. Comput. Linguist. 22(2),
249?254.
Denis, A., L. M. Rojas-Barahona, and M. Quignard. (2010). Extending MMIL semantic representation: Ex-
periments in dialogue systems and semantic annotation of corpora. In proceedings of the Fifth Joint ISO-
ACL/SIGSEM Workshop on Interoperable Semantic Annotation (ISA-5), Hong Kong, January 2010.
379
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 332?334,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
An Incremental Architecture for the Semantic Annotation of Dialogue
Corpora with High-Level Structures. A case of study for the MEDIA corpus.?
Lina Maria Rojas-Barahona and Matthieu Quignard
LORIA/INRIA, France
{lina.rojas,matthieu.Quignard}@loria.fr
Abstract
The semantic annotation of dialogue cor-
pora permits building efficient language un-
derstanding applications for supporting en-
joyable and effective human-machine interac-
tions. Nevertheless, the annotation process
could be costly, time-consuming and compli-
cated, particularly the more expressive is the
semantic formalism. In this work, we propose
a bootstrapping architecture for the semantic
annotation of dialogue corpora with rich struc-
tures, based on Dependency Syntax and Frame
Semantics.
1 Introduction
We propose a cooperative architecture that incre-
mentally generates and improves the annotation of
the French MEDIA dialogue corpus with high-level
semantics (HLS), as a result of the cooperation of
several linguistic modules. MEDIA is a French cor-
pus that has collected about 70 hours of spontaneous
speech from the task of hotel room reservation. It
contains transcribed utterances1 that have been man-
ually segmented2 and annotated with a flat seman-
tics i.e.,concept-value pairs (Bonneau-Maynard et
al., 2005).
?This work is supported by the Agence Nationale de la
Recherche (ANR) in France and is part of the French Project
PORT-MEDIA.
1Utterances with ellipsis, disfluencies, false starts, reformu-
lations, repetitions and ungrammaticalities and special charac-
ters such as the symbol ?*? that indicates uncertainty due to
noise in the communication channel.
2The term Segment means sequence of words in utterances.
The HLS semantics, namely the MultiModal In-
terface Language formalism (MMIL) (Denis et al,
2010), augments the expressivity of the flat seman-
tics by representing communicative actions, predi-
cates, arguments and fine-grained features. Commu-
nicative actions are components built up from two
types of entity (i.e. events and participants), which
are linked together by relations and described by
sets of features (attribute-value pairs). It is possible
to identify in entities a set of main features, which
can be domain-specific. For the semantic annota-
tion, components are mapped to segments in utter-
ances. Figure 1 shows the canonical representation
of an utterance in the corpus in compliance with the
specifications for the annotation3.
2 The Architecture
The architecture (Figure 2) for the automatic anno-
tation has been formulated as a post-interpretation
process that takes place after the syntactic analysis
and semantic role labeling (SRL). Two linguistic re-
sources interact within the architecture, the corpus
and the Frames4. Four linguistic modules are in-
volved in the annotation: the Part-Of-Speech (POS)
tagger, the parsing, the semantic-role labeling (SRL)
and the HLS Builder. The common knowledge base
comprises two knowledge-bases (one for the domain
and the other for the HLS formalism) together with a
relational database management system (RDBMS).
The knowledge bases assure the coherence of the an-
3http://www.port-media.org/doku.php?id=
mmil_for_annotating_media
4Frames is the process in which the frames and frame ele-
ments (FE) are defined.
332
Speak
Request
Reserver
(Reserve)
Personne
(People)
Chambre
(Room)
indef.
je (I)
Ville
(City)
Niort
propContent
patient
aObjetRe?serve?
aBe?ne?ficiaires
aLocalisation
Entities Segment Features=Value
Communicative Act:Request je voudrais ... a` Niort
Main Event:Reserve faire une re?servation
Participant 1:Pronoun je
Participant 2:Chambre d? une chambre
une refType=indefinite
chambre objType=Chambre
Figure 1: HLS representation for the French utterance ?je voudrais
faire une re?servation d? une chambre pour une personne a` Niort? (So I
would like to make a reservation for a room for one person in Niort).
It shows a request to reserve: the communicative action is Request the
main event is Reserve. Note that the beneficiary and the patient are two
different roles, the beneficiary is the person, not necessarily the same
speaker, who will use the object reserved (e.g. rooms). The patient is
the speaker. The segmentation of the HLS Component is presented in
the Table, the component is mapped to the whole utterance. The fine-
grained segmentation of features is shown for the Participant 2.
notation while the database assures persistence and
data integrity. The database stores the corpus, the
frames, the results at each level of analysis, as well
as the progress in the annotation. The persistence
permits progressively optimizing the algorithms un-
til the desired annotation is obtained and integrated
into the corpus files. The corpus manager is in
charge of the resources management. Last but not
least, two annotation tools were built: one for the
SRL gold standard (web-based) and the other for the
HLS gold standard (standalone).
Syntactic Analysis. We decided to employ sta-
tistical approaches that could learn the irregularities
of spoken language: the French Tree-Tagger5 and
the dependency-based MALT-PARSER (Nivre et al,
2007). The parser has been trained with 1449 utter-
ances annotated according to the annotation guide-
lines described in (Cerisara and Gardent, 2009).
5http://www.ims.uni-stuttgart.de/
?schmid/
Figure 2: General Architecture for the HLS Annotation.
Definition of Frames. Frame Semantics, (Baker
et al, 1998) arranges common background knowl-
edge for situations by grouping verbal, nominal
causative and non-causative predicates. Neverthe-
less, paraphrases are more used in spoken language
than explicitly uttered nouns, adjectives or verbs for
referring to a situation (e.g.?ask?, ?request? or ?de-
mand?). Here we introduce the term: Frame Evok-
ing Phrase (FEP) for evoking frames and we in-
clude syntactic templates that mirror these phrases
in frames and frame elements (FE). Table 1 summa-
rizes the differences between PORT-MEDIA frames
and FrameNet (Baker et al, 1998).
FrameNet PORT-MEDIA
Frames
Lexical Units Lexical Units, POS tags and templates
MEDIA Flat Semantics
Frame Elements
Lexical Units, Phrase Type Lexical Units, POS tags, templates
and Grammatical Function and dependency relation
Semantic Type Semantic Type
and MEDIA flat semantics
Table 1: Static Characteristics of Frames in FrameNet
and in PORT-MEDIA.
Semantic Role Labeling. We built a rule-based
semantic role labeling for detecting frames and FE
(roles) by using dependency tree-template pattern
matchers that exploit the information already com-
pressed in frames. The SRL detects the bound-
aries of FEP and FE by measuring the syntactic and
semantic similarity between the utterance and the
frame.
HLS Builder.The HLS Builder is the last phase in
the annotation process: it is rule-based and it takes
utterances in the corpus with their flat semantics, de-
333
pendency trees and predicates-arguments and builds
the HLS representation (See Figure 1), according to
the specifications for the annotation and the knowl-
edge bases. The dialogue act and main event in
HLS components can be detected from the predi-
cates. Similarly, secondary events and participants
with their features can be detected from the roles and
the flat semantics.
3 Evaluation and Discussion
For evaluating the system we separately com-
puted the accuracy of its linguistic components.
The parser achieved a label attachment score
(LAS) (Nivre et al, 2007) of 86.16%, with a train-
ing set of 1097 utterances and a test-set of 100 utter-
ances. The SRL was evaluated with metrics adapted
from the CONLL 2005 evaluation (Carreras and
Ma`rquez, 2005) for supporting FEP and allowing
overlapped FEP for different frames. The LAS was
computed by comparing the semantic dependencies
of system?s and gold?s propositions6 and their seg-
ments. The gold standard comprises 115 utterances
annotated with the major frames in the domain:
Request, Reserve and Attributes. The F1-measure
computed for propositions with exactly the same
segments was 56.66%. When verifying whether the
segments contain the same syntactic governor, the
SRL achieves a better score: 71.30%. Finally, vary-
ing the number of excluded words in both segments7
yielded a constant increase of the F1-measure un-
til a maximum of 84.27%. The HLS annotation
was evaluated by measuring the similarity between
gold?s and system?s components with a gold stan-
dard of 330 complex utterances related to the reser-
vation task. When rigorously measuring the equal-
ity of components8, we obtained a F1-measure of
57.79%. Measuring equality of components with-
out being so rigorous with features? segmentation,
yielded a slightly higher score 63.31%. Finally,
when measuring equality of components by taking
6A proposition is a structure containing the predicate, their
arguments and the semantic relation between them.
7From 1 to n words not common in both segments.
8Two HLS components are equal if their entities and rela-
tions are equal. Two entities are equal if they have the same
segment and features (feature name and feature value) and if
these features are mapped to the same segments in the utter-
ance. Two relations are equal if they have the same source and
target entities as well as the same name
into account only the main features of entities, we
obtained a higher score: 70.65%.
We proposed an architecture for corpus manage-
ment that allows incremental updates over persistent
information until a more accurate semantic annota-
tion is obtained. The preliminary results show a gen-
eral agreement when defining the main features and
the main entities in HLS components and a disagree-
ment when segmenting fine-grained features. We
observed that the system tends to create new entities
when it detects repetitions or references in long ut-
terances. Defining a more precise segmentation pol-
icy in the manual annotation guidelines, augmenting
the training data for parsing, as well as integrating
reference resolution and disambiguation techniques,
will enhance the annotation process. An appealing
research direction would be to integrate and evaluate
machine learning components in the architecture.
References
He?le`ne Bonneau-Maynard and Matthieu Quignard and
Alexandre Denis. 2005. MEDIA: A semantically an-
notated corpus of task oriented dialogs in French. Lan-
guage Resources and Evaluation.
Alexandre Denis and Lina M. Rojas-Barahona and
Matthieu Quignard. 2010. Extending MMIL Seman-
tic Representation: Experiments in Dialogue Systems
and Semantic Annotation of Corpora. In: Proceedings
of the Fifth ISO-ACL/SIGSEM Workshop on Interoper-
able Semantic Annotation (ISA-5), Hong Kong.
Collin Baker and Charles Fillmore and John Lowe. 1998.
The Berkeley FrameNet Project. Proceedings of the
17th International Conference on Computational lin-
guistics, 86?90. Association for Computational Lin-
guistics.
Joakim Nivre and Johan Hall and Sandra Ku?bler and
Ryan McDonald and Jens Nilsson and Sebastian
Riedel and Deniz Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007. Prague, Czech Republic:915?932. Association
for Computational Linguistics.
Christophe Cerisara and Claire Gardent. 2009. Anal-
yse syntaxique du franc?ais parle?. Journe?e the?matique
ATALA Quels analyseurs syntaxiques pour le franc?ais.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. CONLL ?05: Proceedings of the Ninth Con-
ference on Computational Natural Language Learning.
152?164. Association for Computational Linguistics.
334
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 10?19,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
An End-to-End Evaluation of Two Situated Dialog Systems
Lina M. Rojas-Barahona
Inria, LORIA, UMR 7503
Villers-le`s-Nancy
F-54600, France
lina.rojas@loria.fr
Alejandra Lorenzo
Universite? de Lorraine
LORIA, UMR 7503
Vandoeuvre-le`s-Nancy
F-54500, France
alejandra.lorenzo@loria.fr
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy
F-54500, France
claire.gardent@loria.fr
Abstract
We present and evaluate two state-of-the art
dialogue systems developed to support dialog
with French speaking virtual characters in the
context of a serious game: one hybrid statis-
tical/symbolic and one purely statistical. We
conducted a quantitative evaluation where we
compare the accuracy of the interpreter and
of the dialog manager used by each system; a
user based evaluation based on 22 subjects us-
ing both the statistical and the hybrid system;
and a corpus based evaluation where we exam-
ine such criteria as dialog coherence, dialog
success, interpretation and generation errors in
the corpus of Human-System interactions col-
lected during the user-based evaluation. We
show that although the statistical approach is
slightly more robust, the hybrid strategy seems
to be better at guiding the player through the
game.
1 Introduction
In recent years, there has been much research on cre-
ating situated conversational characters i.e., virtual
characters (VCs) that look and act like humans but
inhabit a virtual environment (Gratch et al, 2002;
Hofs et al, 2010; Traum et al, 2007; Johnson et al,
2005; Traum et al, 2008; DeVault et al, 2011).
In this paper, we focus on French speaking, situ-
ated conversational agents who interact with virtual
characters in the context of a serious game designed
to promote careers in the plastic industry (The Mis-
sion Plastechnologie game or MP). We present and
compare two state-of-the art dialogue systems. The
first system (H) is a hybrid approach that com-
bines an information-state dialogue manager (Lars-
son and Traum, 2000) with a classifier for interpret-
ing the players? phrases. The second system (QA)
is a question/answering character model which pre-
dicts the system dialog move given a player?s ut-
terance (Leuski and Traum, 2008). Both systems
use a generation-by-selection strategy (Leuski et al,
2006; Gandhe and Traum, 2007) where the system?s
utterances are selected from a corpus of possible
outputs based on the dialog manager output. While
previous work focuses on relatively short dialogs in
a static setting, in our systems we consider long in-
teractions in which dialogs occur in a setting that
dynamically evolves as the game unfolds.
We evaluate the two dialog systems in the con-
text of the 3D game they were developed for and
seek to determine the degree to which a dialog sys-
tem is operational. To answer this question, we anal-
yse both systems with respect not only to quantita-
tive metrics such as accuracy but also to user- and
corpus-based metrics. User-based metrics are com-
puted based on a questionnaire the users filled in;
while corpus-based metrics are manually extracted
from the corpus of Player-VC interactions collected
during the user-based evaluation. As suggested by
evaluation frameworks such as PARADISE (Walker
et al, 1997) and SASSI (Hone and Graham, 2000),
we show that a multiview evaluation permits a better
assessment of how well the dialog system functions
?in the real world?. The metrics proposed assess di-
alog success and coherence, as well the costs of dia-
log components.
The paper is organized as follows. In Section 2,
10
we present the MP game, the dialogue strategies
used in the different dialogs and the dialog data used
for training. Section 3 presents the two dialog sys-
tems we compare. Section 4 presents the evaluation
schemes used to compare these two systems and dis-
cusses the results obtained. Section 5 concludes with
directions for further research.
2 Dialogues in the MP Game
We begin by describing the MP game, the dialogs in
the MP game, the strategies used to guide the hybrid
dialog manager and the data used for training.
2.1 The MP Game and Dialogs
The MP game is a multi-player quest where 3
teenagers seek to build a joystick in order to free
their uncle trapped in a video game 1. To build
this joystick, the player (who alternatively repre-
sents anyone of these three teenagers) must explore
the plastic factory and achieve 17 mandatory goals
(find the plans, get the appropriate mould, retrieve
some plastic from the storing shed, etc), as well
as 11 optional goals which, when reached, provide
them with extra information about the plastic indus-
try (and therefore increases their knowledge of it).
In total, the player can achieve up to 28 game
goals by conducting 12 separate dialogs in various
parts of the virtual world. Each of the 12 dialogs
in the MP game helps players to achieve the game
goals. The player interacts with the virtual charac-
ters to obtain information that helps her to achieve
these goals and, as a consequence, to increase her
score in the game. Table 1 summarises the game
goals and the contextual parameters (player?s role,
location in the virtual world, VCs present) associ-
ated with each dialog.
2.2 Dialog Data and Annotation
To train both classifiers, the one used by the hybrid
and the one used by the QA system, we collected
Human-Machine dialog data using a Wizard-of-Oz
setting and manually annotated each turn with a di-
alog move. The resulting corpus (called Emospeech
Corpus) and the annotation scheme (as well as the
inter-annotator agreement) used are described in de-
1The MP game was created by Artefacto, http://www.
artefacto.fr/index_ok.htm
tail (Rojas-Barahona et al, 2012). Briefly, the Emo-
speech Corpus comprises 1249 dialogs, 10454 utter-
ances and 168509 words. It contains 3609 player ut-
terances consisting of 31613 word tokens and 2969
word types, with approximately 100 conversations
for each dialog in the game. Turns were annotated
with dialog moves (Traum and Larsson, 2003) cap-
turing both domain knowledge (e.g., about the goals
set by the game) and the set of core communicative
acts.
2.3 Dialog Strategies
We identified four main dialog strategies underlying
the 12 MP dialogs and used these to define the plans
guiding the rule-based discourse management in the
hybrid system. These strategies can be seen as trans-
actions made up of conversational games (Carletta et
al., 1997).
Strategy 1. This strategy is used in the first di-
alog only and consists of a single Address Request
move by the VC followed by the player?s answer:
Lucas requests Ben to find the address of the Plas-
tic Enterprise that must be hidden somewhere in the
lab. Ben can accept, reject or ask for help. Lucas
answers accordingly and ends the conversation.
Strategy 2. Nine dialogues follow this strategy.
They include several (up to 5) requests for infor-
mation and the corresponding system/player?s ex-
change. Appendix A shows an example dialog fol-
lowing this strategy.
Strategy 3: This is a confirmation strategy where
the VC first checks that the player has already
achieved a given task, before informing her about
the next step (e.g. dialogs with Melissa in Table 1).
Strategy 4. This strategy, exemplified in Ap-
pendix B, is similar to strategy 2 but additionally
includes a negotiation step where the VC asks the
player for help.
3 Dialogue Systems
The game and the two dialog systems built were in-
tegrated as agents within the Open Agent Architec-
ture as shown in Figure 1. Both systems access a
database for starting the appropriate dialogs at the
appropriate place in the virtual world while simulta-
neously storing all interactions in the database.
11
Id VC Player Goals Location
1 Lucas Ben Find the address of the enterprise. Uncle?s place.
2 M.Jasper Lucas The manufacturing first step Enterprise reception
3 Samir Julie Find the plans of the joystick Designing Office
Optional: job, staff, studies, security policies
4 Samir Julie Find out what to do next Designing Office
Optional: jobs in the enterprise, staff in the enterprise
5 Melissa Lucas Find the mould, optional where are the moulds Plant
6 Melissa Lucas Find the right machine Plant
7 Melissa Lucas Confirm you have found the right mould and machine and Plant
find out what to do next
8 Operator Julie Knowing about the material space and about the job Material Space
Optional: find out what to do in the case of failure
helping to feed a machine with the right material
9 Serge Ben Perform quality tests. Laboratory Tests
Optional: VC?s job
10 Serge Ben Find out what to do next. Laboratory Tests
Optional: know what happens with broken items
11 Sophia Julie Find the electronic components, knowing about VC?s job Finishing
12 Sophia Lucas Finishing process Finishing
Optional: know about conditioning the product
Table 1: Description of the 12 dialogs in the MP Game.
Figure 1: General Architecture for the dialog system:
modules are implemented as agents within the Open
Agent Architecture.
3.1 The Hybrid Dialogue System
The hybrid system combines an interpreter; a rule
based, Information State Approach dialog manager;
a generator; and the game/dialog communication
components i.e., the OAA interface.
The Interpreter Module In the hybrid system,
the interpreter is a classifier trained on the anno-
tated data (cf. section 2.2), which maps the player?s
utterance to a dialog move. To build the classi-
fier we experimented with both SVM (Support Vec-
tor Machine) and LR (Logistic Regression) 2 us-
ing different sets of features: utterances were pre-
processed by removing stop words and converting
content words to unaccented lowercase. Afterwards,
we experiment with and without TF*IDF (term fre-
quency*Inverse Document Frequency) filtering and
context moves (from 0 to 4 previous dialog moves).
Since the game consist of a number of different
dialogs, two options naturally arise: we could either
train a single classifier on the whole dataset or train
one classifier per dialog. Hence the data sets (and the
number of categories to be learned) differ depending
on whether we trained one classifier on the whole
game data or a classifier for each dialog.
In addition, hand-written rules are used to correct
misclassification errors as follows. The best predic-
tion given by the classifier is matched against the
expected move determined by the last move stored
in the information-state (see below the description
of the dialog manager). In case of a mismatch, the
interpreter selects a move that is valid in the current
context and updates the information state with this
move. For instance, if the last move is a yes/no ques-
tion, confirm and disconfirm dialog acts are valid
moves; for propositional questions, Goali is a valid
2We used LIBSVM (Chang and Lin, 2001) and MALLET
(McCallum, 2002) for the LR classifier with L1 Regularisation.
12
dialog move 3; etc. Since the process is non deter-
ministic, this re-interpretation process may improve
the system response but it may also be incorrect. For
instance, in the following example re-interpretation
correctly overrides the dialog move output by the
classifier to a move asking the VC (Samir) for the
next goal to be achieved.
Samir: Vous avez d?autres questions pour moi?
Do you have other questions ?
Julie: comment cela fonctionne ?
How does it work?
Samir: Eh bien , il va falloir la fabriquer maintenant cette
manette . Allez voir Me?lissa.
Well, you have to build the joystick now. Go to see Melissa.
In contrast, the dialog below shows a case where
re-interpretation fails. Instead of re-interpreting the
player?s (Julie) input as a request for the next goal,
it outputs a request for information about the staff
thereby yielding an incoherent exchange.
Samir: D? autres questions ?
Other questions?
Julie: oui qu?est-ce que je peux faire avec ces bouts de papier ?
yes, what can I do with these pieces of paper ?
Samir: Et bien sachez qu?il y a de plus en plus des femmes dans
cette industrie ...
you should know there are more and more women in this indus-
try ...
The DialogManager We designed a plan for each
dialog strategy and extended Midiki (Burke et al,
2003) to support the OAA architecture and access
the relational database specifying the configurations
of the different dialogs in the game. Each time a new
dialog is launched, the information state is loaded
with the corresponding dialog-context (e.g., speak-
ers, list of goals to be discussed) and the plan mod-
eling the corresponding dialog strategy. To support
dialog management, we implemented a set of update
and selection rules for integrating players? moves,
handling the information-state and for preparing the
agenda according to the plan. More specifically, the
following rules are executed at runtime: Integration:
integrates dialog moves (e.g., questions, answers,
acknowledgments) in the information state (ques-
tions are listed in the Question Under Discussion,
3The system asks the player for the goal to be discussed:
ask(task(X)) and the player answers one goal in the situated
dialog: Goali.
answers change the Commond Ground, player an-
swers are integrated in response to VCs questions).
Manage Plan: searches the next action in the plan.
Refill Agenda: updates the agenda with the next ac-
tion and Selection: selects the next dialog move ac-
cording to the plan. Once the system move has been
selected, the Generator searches an appropriate ver-
balisation.
The Generator As mentioned above, the gener-
ator implements a generation-by-selection strategy.
Given the dialog move output by the dialog man-
ager, the generator selects any utterance in this cor-
pus that is labeled with this dialog move and with
the identifier of the current dialog.
In addition, two types of dialog moves are
given special treatment. The first two moves of
each dialog are systematically constrained to be
a welcome greeting followed by either a request
to pursue a goal (ask(Goali) or a proposal
to help (ask(task(X))). Furthermore, proposi-
tional questions (i.e., proposals by the system to
discuss additional topics) were annotated separately
with their respective dialog goals. For example,
Samir?s sentence: Are you interested in hearing
about my job, the people that work here or the se-
curity policies?, was annotated with the goals: job,
staff and security policies. For these dialog acts, the
generator checks the list of current missing goals so
as to retrieve an appropriate propositional question.
In this way, the system can coherently direct the
player by suggesting possible topics without using
vague and repetitive sentences such as Would you
like to know more?.
3.2 The QA System
The QA system combines a classifier that matches
players? turns to system dialog moves with the same
generation-by-selection algorithm used in the hybrid
system. Like the hybrid system, the QA dialog sys-
tem maintains a constant interaction with the game
to allow for the game coherence and continuity. In
particular, at the end of each dialog, the game goals
that have been reached through the dialog are re-
turned to the game server so that the game can pro-
ceed and be updated with the appropriate score.
For the QA system, we trained a Logistic Re-
gression (LR) classifier for each dialog in the game.
13
We applied the same preprocessing and extracted the
same features used for training the interpreter in the
hybrid system.
3.3 Discussion and Interaction Examples
As shown above, while the QA system simply
matches the player?s input to a system response, the
hybrid system has a much more elaborate dialog
management policy including re-interpretation and
the use of game and dialog history information,
to guide the dialog. As a result, the QA dialogs
are generally more spontaneous, giving the player
more initiative whereas the hybrid system enforces
a more System-Driven dialog strategy thereby guid-
ing the player through the game. The following two
examples illustrate this point. Sophia is the system
and Lucas the player. The game context provides
two goals which can be discussed: one mandatory
(To find out that finishing the manufacturing process
involves conditioning the product), and one optional
(To be told what conditioning the product means).
The examples show how the hybrid system proposes
both goals to the user while the QA engine provides
the same information fluently.
Hybrid system
Sophia: Vous avez fait du bon travail les enfants!
You did a great job kids!
Sophia: Bien. Que puis-je faire pour vous ?
What can I do for you?
Lucas: J?ai fini la manette
I?ve finished the joystick
Sophia: Normalement j?ai encore quelques e?tapes a` re?aliser
avant l?envoi au client. Vous voulez en savoir plus ?
Normally I still have to condition the product. Do you want to
know more about conditioning a product?
Lucas: Oui
Yes, ok
Sophia: Conditionner le produit veut dire l?emballer soigneuse-
ment pour pouvoir l?expe?dier au client. Aldo, le logisticien as-
sure qu?on ne manque pas de stock et de ge?rer les livraisons et
les expe?ditions.
Conditioning the product means packing it carefully in order to
ship it to the customer. Aldo,the logistics coordinator ensures
there is no shortage of stock and he is also in charge of the de-
liveries and shipments.
Sophia: Je vous laisse tester votre manette . Au revoir
I leave you test out your joystick. Goodbye
Question Answer
Sophia: he? bien c?est du bon travail tout c?a.
Good! That?s a great job kids!
Sophia: Alors , ou` en sommes nous?
So where are we?
Lucas:voila` nous avons utilise? les composants. Quelle est
l?e?tape suivante?
We used the components, what is the next step?
Sophia: On finis la fabrication par conditionner le produit.
We finish the fabrication by conditioning the product.
Lucas:Ok
Sophia: Conditioner le produit veut dire l?emballer soigneuse-
ment pour pouvoir l?expe?dier au client . Le logisticien assure
qu?on ne manque pas de stock et de ge?rer les livraisons et les
expe?ditions.
Conditioning the product means packing it carefully in order to
ship it to the customer. Aldo,the logistics coordinator ensures
there is no shortage of stock and he is also in charge of the de-
liveries and shipments.
Sophia: Et voila, je vous dis au revoir
Ok, I say good bye!
4 Evaluation
In evaluating the two systems, we seek to compare
their usability: Which system is best suited for use
by real users in the context of the MP serious game?
We also seek to better understand which module
causes which errors and why. To address these ques-
tions we conducted a quantitative evaluation where
we compare the accuracy of the interpreter and the
dialog manager integrated in each system; a user
based evaluation involving 22 subjects using both
the QA and the hybrid system; and a corpus based
evaluation where we examine such criteria as dialog
coherence, dialog success, interpretation and gener-
ation errors in the corpus of Human-System interac-
tions collected during the user-based evaluation.
4.1 Quantitative Evaluation
We begin by evaluating the accuracy of the inter-
preter and the dialog manager used by the hybrid and
the QA system respectively. These two classifiers
were trained on the Emospeech corpus mentioned
above and evaluated with 30-fold cross-validation.
Hybrid System As we mentioned in section 3.1,
since the game includes different dialogs, a natu-
ral question arise: whether to implement the inter-
14
preter with a single classifier for the whole dataset,
or using a different classifier for each dialog in the
game. To answer this question, we compared the
accuracy reached in each case. The details of these
experiments are described in (Rojas-Barahona et al,
2012). The highest accuracy is reported when using
a single classifier for the whole game, reaching an
accuracy of 90.26%, as opposed to 88.22% in aver-
age for each dialog. In both cases, the classifier used
is LR, with L1 regularisation and applying the tf*idf
filtering. However, although the classifier trained on
the whole dialog data has better accuracy (learning
a model per dialog often run into the sparse data is-
sue), we observed that, in practice, it often predicted
interpretations that were unrelated to the current di-
alog thereby introducing incoherent responses in di-
alogs. For instance, in the dialog below, the player
wants to know how waste is managed in the fac-
tory. The best prediction given by the interpreter is a
goal related to another dialog thereby creating a mis-
match with the DM expectations. Re-interpretation
then fails producing a system response that informs
the player of the next goal to be pursued in the game
instead of answering the player?s request.
Ben: Comment on ge`re les dechets ici?
How is the waste managed here ?
Serge: Allez voir Sophia pour qu?elle vous fournisse les com-
posants e?lectroniques ne?cessaires a` votre manette.
Go and see Sophia, she?ll give you the electronic components
you need for your joystick.
For the user based experiment, we therefore use
the LR models with one classifier per dialog.
QA System For evaluating the QA classifier, we
also compared results with or without tf*idf filter-
ing. The best results were obtained by the LR clas-
sifier for each dialog with tf*idf filtering yielding an
accuracy of 88.27% as shown in Table 2.
4.2 Preliminary User-Based Evaluation
The accuracy of the interpreter and the dialog man-
ager used by the hybrid and the QA system only
gives partial information on the usability of the di-
alog engine in a situated setting. We therefore con-
ducted a user-based evaluation which aims to assess
the following points: interpretation quality, overall
system quality, dialog clarity, game clarity and tim-
ing. We invited 22 subjects to play the game twice,
Id w/o Tf*Idf w Tf*Idf
1 83.33 82.93
2 93.55 91.8
3 72 80.95
4 80 82.47
5 95.24 93.98
6 97.56 97.5
7 97.5 97.44
8 70.59 76
9 92.77 91.14
10 85.53 86.49
11 83.51 87.5
12 94.12 91.04
Avg. 87.14 88.27
Table 2: Results of the LR classifier for mapping play-
ers? utterances to system moves, with content-words and
a context of four previous system moves, with and with-
out tf*idf filtering.
once with one system and once with the other. The
experiment is biased however in that the players al-
ways used the hybrid system first. This is because in
practice, the QA system often fail to provide novice
players with enough guidance to play the game. This
can be fixed by having the player first use the hybrid
system. Interestingly, the game guidance made pos-
sible by the Information State approach is effective
in guiding players through the game e.g., by propos-
ing new goals to be discussed at an appropriate point
in the dialog; and by taking dialog history into ac-
count.
After playing, each user completed the question-
naire shown in Table 3. For those criteria such as
dialog and game clarity, we do not report the scores
since these are clearly impacted by how many times
the player has played the game. Table 4 shows the
mean of the quantitative scores given by the 22 sub-
jects for interpretation, overall system quality and
timing. We computed a significance test between
the scores given by the subjects, using the Wilcoxon
signed-rank test4. As shown in the Table, for all
criteria, except Q.4, the QA performs significantly
(p < 0.01) better than the Hybrid system.
4The Wilcoxon signed-rank test is the non-parametric alter-
native to the paired t-test for correlated samples, applicable, e.g.
when dealing with measures which cannot be assumed to have
equal-interval scales, as is usual with user questionnaires.
15
Interpretation
Q.1 Did you have the feeling the virtual characters understood you? (very bad 1 ... 100 very good)
Overall System Quality
Q.2 Did you find the conversations coherent? (very bad 1 . . . 100 very good)
Q.3 Did you enjoy talking with the virtual characters? (very annoying 1 ... 100 very enjoyable)
Q.4 Would you prefer playing the game without conversations with virtual characters? (yes/no)
Q.5 What is your overall evaluation of the quality of the conversations? (very bad 1 . . . 100 very good)
Dialogue clarity
Q.6 How easy was it to understand what you were supposed to ask? (very difficult 1 ... 100 very easy)
Q.7 How clear was the information given by the virtual characters? (totally unclear 1 ... 100 very clear)
Q.8 How effective were the instructions at helping you complete the game? (not effective 1 ... 100 very effective)
Game clarity
Q.9 How easy was it to understand the game? (totally unclear 1 ... 100 very clear)
Timing
Q.10 Were the system responses too slow (1) / just at the right time (2) / too fast (3)
Table 3: Questionnaire filled by the subjects that played with both dialog systems.
Interpretation. Question Q.1 aims to captures the
user?s assessment of the dialog system ability to cor-
rectly interpret the player?s utterances. The QA sys-
tem scores 0.7 points higher than the Hybrid system
suggesting better question/answer coherence for this
system. One possible reason is that while the hybrid
system detects any incoherence and either tries to
fix it using re-interpretation (which as we saw some-
times yields an incoherent dialog) or make it explicit
(using a misunderstanding dialog act i.e., a request
for rephrasing), the QA system systematically pro-
vides a direct answer to the player?s input.
The relatively low scores assigned by the user
to the interpretation capabilities of the two systems
(57.36 and 64.55 respectively) show that the high
accuracy of the interpreter and the dialog manager
is not a sufficient criteria for assessing the usability
of a dialog system.
Timing. One important factor for the usability of
a system is of course real time runtimes. The eval-
uation shows that overall the speed of the QA sys-
tem was judged more adequate. Interestingly though
the difference between the two systems stems no so
much from cases where the hybrid approach is too
slow than from cases where it is too fast. These cases
are due to the fact that while the QA system always
issues one-turn answer, the rule based dialog based
approach used in the hybrid system often produce
two consecutive turns, one answering the player and
the other attempting to guide her towards the follow-
ing game goal.
In sum, although the QA system seems more ro-
bust and better at supporting coherent dialogs, the
hybrid system seems to be more effective at guiding
Question Hybrid QA
Interpr. Q.1 57.36 64.55 (*)
Sys Qual.
Q.2 57.78 60.68 (*)
Q.3 60.77 66.45 (*)
Q.4/no 86.37 81.82
Q.5 59.54 65.68 (*)
Avg. 66.12 68.66 (*)
Timing Q.10 2.25 2.05 (*)
Table 4: Mean of the quantitative scores given by 22 in-
dividuals. (*) denotes statistical significance at p < 0.01
(two-tailed significance level).
the player through the game.
4.3 Corpus-Based Evaluation
The User-Based evaluation resulted in the collection
of 298 dialogs (690 player and 1813 system turns)
with the Hybrid system and 261 dialogs (773 player
and 1411 system turns) with the QA system. To bet-
ter understand the causes of the scores derived from
the user-filled questionnaire, we performed manual
error analysis on this data focusing on dialog inco-
herences, dialog success, dialog management and
generation errors (reported in Table 5).
DM Errors The count of dialog management
(DM) errors is the ratio WRP of wrong system re-sponses on counts of player?s input. In essence this
metrics permits comparing the accuracy of the QA
dialog manager with that of the hybrid system. On
average there is no clear distinction between the two
systems.
16
Generation Errors The system response selected
by the generation component might be contextually
inappropriate for at least two reasons. First, it may
contain information which is unrelated to the current
context. Second, it might have been imprecisely or
incorrectly annotated. For instance, in the dialog
below, the annotation of the turn Yes, thanks. What
do you want me to do? did not indicate that the turn
included a Confirm dialog move. Selecting this turn
in the absence of a yes/no question resulted in a
contextually inappropriate system response.
SYSTEM: Bonjour les petits jeunes je suis le pre?parateur
matie?re.
Hello kids, I am the raw material responsible
SYSTEM: Oui merci. Vous me voulez quoi en fait ?
Yes, thanks. What do you want me to do?
PLAYER: je veux en savoir plus sur cet endroit.
I would like to know more about this place
As shown in Table 5, for both systems, there were
few generation errors.
Id %DM H. %DM. QA %Gen H. & QA
1 0.0 4.55 0.57
2 10.81 12.00 1.02
3 10.38 12.04 1.49
4 16.22 14.86 0.32
5 10.34 2.13 1.46
6 0.0 0.0 0.94
7 9.52 4.0 0.0
8 11.68 7.08 2.06
9 2.13 26.47 0.76
10 15.63 16.13 6.08
11 11.94 8.33 3.19
12 14.29 8.16 3.17
Avg. 9.41 9.65 1.76
Table 5: DM and generation errors detected in the hybrid
and the QA systems.
Unsuccessful Dialogs We counted as unsuccess-
ful those dialogs that were closed before discussing
the mandatory goals. The results are shown in Ta-
ble 6. Overall the QA system is more robust leading
to the mandatory goals being discussed in almost all
dialogs. One exception was dialog 8, where the sys-
tem went into a loop due to the player repeating the
same sequence of dialog moves. We fixed this by
Id %Uns. H. %Inco. H. %Uns. QA. %Inc. QA.
1 0 0.0 0.0 0.0
2 0 0.0 0.0 0.0
3 6.67 3.33 7.41 0.0
4 7.14 0.0 0.0 4.0
5 3.85 0.0 0.0 0.0
6 0.0 0.0 0.0 0.0
7 21.21 0.0 0.0 0.0
8 3.70 0.0 15.63 3.13
9 0.0 0.0 0.0 4.35
10 0.0 6.67 0.0 16.67
11 3.45 6.90 0.0 3.70
12 4.17 4.17 4.55 4.55
Avg. 4.89 1.76 4.47 3.03
Table 6: Overall dialog errors, the percentage of unsuc-
cessful dialogs
integrating a loop detection step in the QA dialog
manager. For the hybrid system, dialog 7, a dialog
involving the confirmation strategy (cf. section 2)
is the most problematic. In this case, the DM rules
used to handle this strategy are inappropriate in that
whenever the system fails to identify a contextually
appropriate response, it simply says so and quits the
dialog. The example illustrates the difficulty of de-
veloping a complete and coherent DM rule system.
Incoherent Dialogs We counted as incoherent, di-
alogs where most system answers were unrelated to
the player?s input. As shown in Table 6, despite
interpretation and generation imprecisions, most di-
alogs were globally coherent. They made sense ac-
cording to the game context: they were related to the
task to be solved by the player in the game, and the
generated instructions were correctly understood.
The hybrid system produces slightly less incoher-
ent dialogs probably because of its re-interpretation
mechanism which permits correcting contextually
invalid dialog moves.
5 Conclusion
We have presented a multi-view evaluation of two
system architectures for conversational agents situ-
ated in a serious game. Although the QA system
seems more robust and is easier to deploy, the hy-
brid dialog engine seems to fare better in terms of
game logic in that it guides the player more effec-
17
tively through the game. The evaluation shows the
importance of assessing not only the dialog engine
accuracy but also its usability in the setting it was
designed for. In future work, we plan to compute
a regression model of user satisfaction for applying
reinforcement learning and find the optimal strategy.
In addition, we plan to extend the comparison to
other domains such as language learning and com-
plex negociation dialogs.
6 Acknowledgments
The research presented in this paper was partially
supported by the Eurostar EmoSpeech project and
by the European Fund for Regional Development
within the framework of the INTERREG IV A Alle-
gro Project.
References
C. Burke, C. Doran, A. Gertner, A. Gregorowicz,
L. Harper, J. Korb, and D. Loehr. 2003. Dialogue
complexity with portability?: research directions for
the information state approach. In Proceedings of the
HLT-NAACL 2003 workshop on Research directions in
dialogue processing - Volume 7.
Jean Carletta, Stephen Isard, Gwyneth Doherty-Sneddon,
Amy Isard, Jacqueline C. Kowtko, and Anne H. An-
derson. 1997. The reliability of a dialogue struc-
ture coding scheme. Comput. Linguist., 23(1):13?31,
March.
Chih C. Chang and Chih J. Lin, 2001. LIBSVM: a library
for support vector machines.
David DeVault, Anton Leuski, and Kenji Sagae. 2011.
An evaluation of alternative strategies for implement-
ing dialogue policies using statistical classification
and hand-authored rules. In 5th International Joint
Conference on Natural Language Processing (IJCNLP
2011).
Sudeep Gandhe and David Traum. 2007. Creating spo-
ken dialogue characters from corpora without annota-
tions. In Proceedings of 8th Conference in the Annual
Series of Interspeech Events, pages 2201?2204.
Jonathan Gratch, Jeff Rickel, Elisabeth Andre?, Justine
Cassell, Eric Petajan, and Norman Badler. 2002. Cre-
ating interactive virtual humans: Some assembly re-
quired. IEEE Intelligent Systems, 17:54?63, July.
Dennis Hofs, Marie?t Theune, and Rieks Akker op den.
2010. Natural interaction with a virtual guide in a
virtual environment: A multimodal dialogue system.
Journal on Multimodal User Interfaces, 3(1-2):141?
153, March. Open Access.
Kate S. Hone and Robert Graham. 2000. Towards a
tool for the subjective assessment of speech system
interfaces (sassi). Nat. Lang. Eng., 6(3-4):287?303,
September.
W. L. Johnson, H. H. Vilhja?lmsson, and S. Marsella.
2005. Serious games for language learning: How
much game, how much AI? In Artificial Intelligence
in Education.
S. Larsson and D. Traum. 2000. Information state and di-
alogue management in the TRINDI dialogue move en-
gine toolkit. Natural Language Engineering, 6:323?
340.
Anton Leuski and David Traum. 2008. A statistical ap-
proach for text processing in virtual humans. In Pro-
ceedings of the 26th Army Science Conference.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proceedings of the 7th SIG-
DIAL Workshop on Discourse and Dialogue, pages
18?27.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Lina M. Rojas-Barahona, Alejandra Lorenzo, and Claire
Gardent. 2012. Building and exploiting a corpus of di-
alog interactions between french speaking virtual and
human agents. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(to be published).
David R. Traum and Staffan Larsson. 2003. The in-
formation state approach to dialogue management. In
Jan Kuppevelt, Ronnie W. Smith, and Nancy Ide, ed-
itors, Current and New Directions in Discourse and
Dialogue, volume 22 of Text, Speech and Language
Technology, pages 325?353. Springer Netherlands.
10.1007/978-94-010-0019-2 15.
David R. Traum, Antonio Roque, Anton Leuski, Panayi-
otis Georgiou, Jillian Gerten, Bilyana Martinovski,
Shrikanth Narayanan, Susan Robinson, and Ashish
Vaswani. 2007. Hassan: A virtual human for tac-
tical questioning. In Proceedings of the 8th SIGdial
Workshop on Discourse and Dialogue, page 71?74,
September.
David Traum, Stacy C. Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal vir-
tual agents. In Proceedings of the 8th international
conference on Intelligent Virtual Agents, IVA ?08,
pages 117?130, Berlin, Heidelberg. Springer-Verlag.
Marilyn A. Walker, Donald Hindle, Jeanne Fromer,
Giuseppe Di Fabbrizio, and Craig Mestel. 1997. Eval-
uating competing agent strategies for a voice email
agent. CoRR, cmp-lg/9706019.
18
A. Example of a dialog with the hybrid system.
Translation into English of a dialog between Samir
(the system) and Julie (the player) (Strategy 2), it
corresponds to the third dialog in Table 1
Samir: Hello my name is Samir, the product designer
Samir: What are you doing here young people?
Julie: we come to build the joystick of Professor Geekman
Samir: You are in the right place. The plans are in the closet
Samir: Before would you like to hear about my job, the studies
I did or my colleagues
Julie: Tell me about your job.
Samir: Well, for instance, is me who devise and design the
items we construct in this enterprise
Samir: Do you want to know more about my colleagues or
about the security measures applied here?
Julie: What do your colleagues do?
Samir: My job is universal! In fact it can please both girls that
boys...
Samir: In your opinion, what studies should be done to get
there
Julie: Designing studies?
Samir: This is a creative job. Several training can lead to the
job arts, industrial design, computer-aided design
Samir: And does the safety standards interest you?
Julie: That is important the safety yes, tell me more
Samir: You know, here you are in a strategic place.
The novelties, the products of tomorrow, is here that they are
born. They must be protected. For that reason we control the
entrance here.
Samir: Well, Good Luck. Bye!
B. Translation to English of a dialog with the
QA system. Dialog between the raw material opera-
tor (the system) and Julie (the player), it corresponds
to the 8th dialog in Table 1(Strategy 4).
Preparateur1: Hello I am the raw material responsible. I can
provide you a lot of information about this place.
Preparateur1: What can I do for you ?
Julie: What do you do here?
Preparateur1: My role is to ensure machines are supplied
with the right materials,in good colors. I turn on the drying
ovens and the whole supply system.
Julie: How does it work the supply system?
Preparateur1: Well when we get the pellet material, they are
wet ...
Julie: What does it happen when there is a problem?
Preparateur1: In that case, I call the technician, he is the best
for fixing everything
Julie:Thanks for the information, what should I do now?
Preparateur1: It is not complicated and you will learn things.
I have a new machine to supply but I can not find the right
material. Would you give me a hand?
Julie: yes
Preparateur1: You must supply this machine with a new
generation of plastic. This machine allows the manufacture of
plastic bags
Julie: ok
Preparateur1: Let?s go!
Julie: Ok, Let?s start!
Preparateur1: Great, Thanks!
Preparateur1: You are very kind, thank you.
19
Proceedings of the SIGDIAL 2013 Conference, pages 12?20,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Unsupervised structured semantic inference for spoken dialog reservation
tasks
Alejandra Lorenzo
Universite? de Lorraine
LORIA, UMR 7503
Nancy, France
alejandra.lorenzo@loria.fr
Lina M. Rojas-Barahona
LORIA, UMR 7503
Nancy, France
lina.rojas@loria.fr
Christophe Cerisara
LORIA, UMR 7503
Nancy, France
cerisara@loria.fr
Abstract
This work proposes a generative model
to infer latent semantic structures on top
of manual speech transcriptions in a spo-
ken dialog reservation task. The proposed
model is akin to a standard semantic role
labeling system, except that it is unsuper-
vised, it does not rely on any syntactic in-
formation and it exploits concepts derived
from a domain-specific ontology. The
semantic structure is obtained with un-
supervised Bayesian inference, using the
Metropolis-Hastings sampling algorithm.
It is evaluated both in terms of attachment
accuracy and purity-collocation for clus-
tering, and compared with strong baselines
on the French MEDIA spoken-dialog cor-
pus.
1 Introduction
Many concrete applications that involve human-
machine spoken dialogues exploit some hand-
crafted ontology that defines and relates the con-
cepts that are useful for the application. The main
challenge for the dialog manager used in the appli-
cation is then to interpret the user?s spoken input
in order to correctly answer the user?s expectations
and conduct a dialogue that shall be satisfactory
for the user. This whole process may be decom-
posed into the following stages:
? Automatic speech recognition, to transform
the acoustic signal into a sequence of words
(or sequences of word hypotheses);
? Spoken language understanding, to segment
and map these sequences of words into con-
cepts of the ontology;
? Semantic analysis, to relate these concepts
together and interpret the semantic of the user
input at the level of the utterance, or of the
speaker turn;
? Dialogue act recognition
? Dialogue planning
? Text generation
? ...
Note that the process sketched here often further
involves several other important steps that are used
internally within one or several of these broad
stages, for instance named entity recognition, co-
reference resolution, syntactic parsing, marcov de-
cision process, reinforcement learning, etc.
This work focuses mainly on the second and
third stages, since we assume that segmentation
is given and we want to discover the underly-
ing concepts and relations in the data. The third
stage is very important because it exhibits the la-
tent semantic structure hidden in the user utter-
ance: what is the object affected by a given pred-
icate ? What are the modifiers that may alter the
meaning of a predicate ? Without such a structure,
the system can hardly push understanding beyond
lexical semantics and reach fine-grained seman-
tic representations, which are thus often limited
to well-formed inputs and cannot handle sponta-
neous speech as considered here. But still, despite
its importance, most spoken dialog systems do not
make use of such structure.
We propose an approach here to address this
issue by directly inferring the semantic structure
from the flat sequence of concepts using the un-
supervised Bayesian learning framework. Hence,
the proposed model does not rely on any prede-
fined corpus annotated with semantic structure,
which makes it much more robust to spoken inputs
and adaptable to new domains than traditional su-
pervised approaches.
12
2 Related work
In recent years, an increasing number of works
have addressed robustness and adaptability issues
in most of standard Natural Language Processing
tasks with unsupervised or semi-supervised ma-
chine learning approaches. Unsupervised learn-
ing attempts to induce the annotations from large
amounts of unlabeled data. Several approaches
have recently been proposed in this context for the
semantic role labeling task. (Swier and Stevenson,
2004) were the first to introduce an unsupervised
semantic parser, followed by (Grenager and Man-
ning, 2006), (Lang and Lapata, 2010), (Lang and
Lapata, 2011b) and (Lang and Lapata, 2011a). Fi-
nally, (Titov and Klementiev, 2012), introduced
two new Bayesian models that achieve the best
current state-of-the-art results. However, all these
works use some kind of supervision (namely a
verb lexicon or a supervised syntactic system,
which is the case in most of the approaches).
(Abend et al, 2009) proposed an unsupervised
algorithm for argument identification that uses
a fully unsupervised syntactic parser and where
the only supervised annotation is part-of-speech
(POS) tagging.
Semi-supervised learning attempts to improve
the performance of unsupervised algorithms by
using both labeled and unlabeled data for train-
ing, where typically the amount of labeled data is
smaller. A variety of algorithms have been pro-
posed for semi-supervised learning1. In the con-
text of semantic role labeling, (He and Gildea,
2006) and (Lee et al, 2007) hence tested self-
training and co-training, while (Fu?rstenau and La-
pata, 2009) used a graph-alignment method to
semantic role labeling (SRL). Finally, in (De-
schacht and Moens, 2009) the authors present a
semi-supervised Latent Words Language Model,
which outperforms a state-of-the-art supervised
baseline. Although semi-supervised learning ap-
proaches minimize the manual effort involved,
they still require some amount of annotation. This
annotation is not always available, sometimes ex-
pensive to create and often domain specific. More-
over, these systems assume a specific role labeling
(e.g. PropBank, FrameNet or VerbNet) and are not
generally portable from one framework to another.
A number of works related to semantic infer-
ence have already been realized on the French
1We refer the reader to (Zhu, 2005) or (Pise and Kulkarni,
2008) for an overview on semi-supervised learning methods.
MEDIA corpus. Hence, dynamic Bayesian net-
works were proposed for semantic composition
in (Meurs et al, 2009), however their model re-
lies on manual semantic annotation (i.e. concept-
value pairs) and supervised training through the
definition of 70 rules. In (Huet and Lefe`vre, 2011;
Camelin et al, 2011) unsupervised models were
proposed that use stochastic alignment and Latent
Dirichlet Allocation respectively, but these mod-
els infer a flat concept-value semantic representa-
tion. Compared to these works, we rather propose
a purely unsupervised approach for structured se-
mantic Metropolis-Hastings inference with a gen-
erative model specifically designed for this task.
3 Proposed model
3.1 Principle
We consider a human-machine dialog, with the ob-
jective of automatically building a semantic struc-
ture on top of the user?s spoken utterances that
shall help the dialog system to interpret the user
inputs. This work focuses on inferring the seman-
tic structure, and it assumes that a segmentation of
users? utterances into concepts is given. More pre-
cisely, we exploit as input a manual segmentation
of each utterance into word segments, where each
segment represents a single concept that belongs
to MEDIA ontology (Denis et al, 2006) (see Fig-
ure 1).
Attributes
Price General
Park
Relative
Near
Restaurant
Location Person Time
Hotel Room
Object
Thing
Figure 1: Excerpt of MEDIA ontology
This ontology identifies the concepts that can
have arguments, and we thus use this informa-
tion to further distinguish between head segments
that can have arguments (noted Wh2 in Figure 3)
and argument segments that cannot govern another
concept (noted Wa). From these two classes of
2Wh actually represents one word in a segment composed
of Nh words, but by extension, we implicitly refer here to the
full segment.
13
segments and the words? inflected forms that com-
pose each segment we infer:
? A semantic structure composed of triplets
(Wa,Wh, A) where A is the type of argu-
ment, or, in other words, the type of semantic
relation between both segments;
? A semantic class Ct for the head segment
An example of the target structure we want to ob-
tain is shown in Figure 2.
Inference of these structure and classes is real-
ized with an unsupervised Bayesian model, i.e.,
without training the model on any corpus anno-
tated with such relations. Instead, the model is
trained on an unlabeled dialog corpus composed
of raw manual speech transcriptions, which have
also been manually segmented into utterances and
words? segments as described above. Training is
actually realized on this corpus using an approxi-
mate Bayesian inference algorithm that computes
the posterior distribution of the model?s param-
eters given the dataset. We have used for this
purpose the Metropolis-Hastings Markov Chain
Monte Carlo algorithm.
3.2 Bayesian model
Figure 3 shows the plate diagram of the proposed
model. The plate Nh (respectively Nw) that sur-
rounds a shaded node represents a single words?
segment of length Nh (respectively Nw). The
outer plate Nu indicates that the graphical model
shall be repeated for each of the Nu utterances in
the corpus.
Variable Description
Ct latent semantic type assigned to predicate t
Wh observed words in each head segment.
P (Wh|Ct) encodes lexical preferences for the
semantic inference
Ai latent semantic type assigned to the ith argu-
ment of predicate t
Rpi latent relative position assigned to the ith argu-
ment of predicate t
Wa observed words in each argument segment.
P (Wa|Ai) encodes lexical preferences for the
semantic inference
Table 1: Variables of the model
Each head word segment has a latent semantic
type Ct, and governs Na arguments. Each argu-
ment is represented by an argument words? seg-
ment, which has a latent semantic typeA. Each ar-
gument is further characterized by its relative po-
sition Rp with respect to its head segment. Rp
C1 ? ? ? Ct?1 Ct Ct+1 ? ? ? CNc
Wh
Nh
A
Wa
Nw
Rp
Na
Nu
Figure 3: Plate diagram of the proposed model.
Nu represents the number of utterances; Nh, the
number of words in a head segment; Nw, the num-
ber of words in an argument segment; and Na the
number of arguments assigned to predicate t.
can have 4 values, depending on whether the argu-
ment is linked to the closest (1) or another (2) ver-
bal3 head, or the closest (3) or another (4) nominal
head. Rp is derived from the argument-to-head
assignment, which is latent. So, Rp is also latent.
The sequence of Nc head segments in utterance u
is captured by the HMM shown on top of the plate
diagram, which models the temporal dependency
between successive ?semantic actions? of the user.
The variables of the model are explained in Ta-
ble 1.
The most important property of this model is
that the number of arguments Na is not known be-
forehand. In fact, every argument segment can be
governed by any of the Nc head segments in the
utterance, and it is the role of the inference pro-
cess to actually decide with which head it should
be linked. This is why the model performs struc-
tured inference.
Concretely, at any time during training, every
argument is governed by a single head. Then, in-
ference explores a new possible head attachment
for an argument Wa, which impacts the model as
follows:
? The number of arguments Na of the previous
head is decreased by one;
? The number of argumentsNa of the new head
is increased by one;
3Morphosyntactic classes are obtained with the Treetag-
ger
14
Je voudrais le prix en fait je euh une chambre pas che`re
I ?d like the price well in fact I uh a room not expensive
Reserve Room
Agent
Price Price
Booked object
Figure 2: Example of inferred semantic structure for a sentence in the MEDIA corpus. Traditional
dependency notations are used: the head segment points to the argument segment, where segments are
shown with boxes (arrows link segments, not words !). The semantic class assigned to each head segment
is shown in bold below the translated text.
? The relative position Rp of the argument is
recomputed based on its new head position;
? The argument typeA is also re sampled given
the new head type Ct.
This reassignment process, which is at the heart of
our inference algorithm, is illustrated in Figure 4.
3.3 Metropolis inference
Bayesian inference aims at computing the poste-
rior distribution of the model?s parameters, given
the observed data. We assume that all distributions
in our model are multinomial with uniform priors.
The parameters are thus:
P (Wh|Ct) ?M(?HCt)
Distribution of the
words for a given
head semantic class
P (Ct|Ct?1) ?M(?CCt?1)
Transition prob-
abilities between
semantic classes
P (Wa|A) ?M(?WA )
Distribution of the
words for a given
argument type
P (Rp|A) ?M(?RA)
Distrib. of the rel-
ative position of a
given argument to
its head given the
argument type
P (A|Ct) ?M(?ACt)
Distrib. of the ar-
gument types given
a head semantic
class
3.3.1 Inference algorithm
To perform inference, we have chosen a Markov
Chain Monte Carlo algorithm. As our model is
finite, parametric and identifiable, Doob?s theo-
rem guarantees the consistency of its posterior,
and thus the convergence of MCMC algorithms
towards the true posterior. Because changing the
head of one argument affects several variables si-
multaneously in the model, it is problematic to
use the basic Gibbs sampling algorithm. A block-
Gibbs sampling would have been possible, but this
would have increased the computational complex-
ity and we also wanted to keep as much flexibility
as possible in the jumps that could be realized in
the search space, in order to prevent slow-mixing
and avoid (nearly) non-ergodic Markov chains,
which are likely to occur in such structured infer-
ence problems.
We have thus chosen a Metropolis-Hastings
sampling algorithm, which allows us to design an
efficient proposal distribution that is adapted to our
task. The algorithm proceeds by first initializing
the variables with a random assignment of argu-
ments to one of the heads in the utterance, and a
uniform sampling of the class variables. Then, it
iterates through the following steps:
1. Sample uniformly one utterance u
2. Sample one jump following the proposal dis-
tribution detailed in Section 3.3.2.
3. Because the proposal is uniform, compute the
acceptance ratio between the model?s joint
probability at the proposed (noted with a ?)
and current states:
r = P (C
?,W ?h,W ?a, Rp?, A?)
P (C,Wh,Wa, Rp,A)
4. Accept the new sample with probability
min(1, r); while the sample is not accepted,
iterate from step 2.
15
Je voudrais le prix en fait je euh une chambre pas che`re
I ?d like the price well in fact I uh a room not expensive
Agent
Price Price
Booked object
Agent
Price
Booked object
Price
Figure 4: Illustration of the reassignment process following the expample presented in Figure 2. This
example illustrates the third Metropolis proposed move, which changes the head of argument ?le prix?:
arcs above the text represent the initial state, while arcs below the text represent the new proposed state.
5. When the sample is accepted, update the
multinomials accordingly and iterate from
step 1 until convergence.
This process is actually repeated for 2,000,000
iterations, and the sample that gives the largest
joint probability is chosen.
3.3.2 Metropolis proposal distribution
The proposal distribution is used to explore the
search space in an efficient way for the target
application. Each state in the search space is
uniquely defined by a value assignment to every
variable in the model, for every utterance in the
corpus. It corresponds to one possible sample of
all variables, or in other words, to the choice of
one possible semantic structure and class assign-
ment to all utterances in the corpus.
Given a current state in this search space, the
proposal distribution ?proposes? to jump to a
new state, which will then be evaluated by the
Metropolis algorithm. Our proposal samples a
new state in the following successive steps:
1. Sample uniformly one of the three possible
moves:
Move1: Change the semantic class of a head;
Move2: Change the argument type of an argu-
ment segment;
Move3: Change the assignment of an argument
to a new head;
2. If Move1 is chosen, sample uniformly one
head segment and one target semantic class;
3. If Move2 is chosen, sample uniformly one
argument segment and one target argument
type;
4. If Move3 is chosen, sample uniformly one
argument segment Wa and ?detach? it from
its current head. Then, sample uniformly one
target head segment W ?h, and reattach Wa to
its new head W ?h. Because the distribution of
argument types differ from one head class to
another, it would be interesting at this stage
to resample the argument type of Wa from
the new head class distribution. But in this
work, we resample the argument type from
the uniform distribution.
This proposal distribution Q(x ? x?) is re-
versible, i.e., Q(x? x?) > 0? Q(x? ? x) > 0.
We can show that it is further symmetric, i.e.,
Q(x ? x?) = Q(x? ? x), because the same
move is sampled to jump from x to x? than to jump
from x? to x, and because the proposal distribution
within each move is uniform.
4 Experimental validation
4.1 Experimental setup
The French MEDIA corpus collects about 70
hours of spontaneous speech (1258 dialogues,
46k utterances, 494.048 words and 4068 dis-
tinct words) for the task of hotel reservation
and tourist information (Bonneau-Maynard et al,
2005). Calls from 250 speakers to a simulated
reservation system (i.e. the Wizard-of-Oz) were
recorded and transcribed. Dialogues are full of
disfluencies, hesitations, false starts, truncations or
fillers words (e.g., euh or ben).
16
Gold Standard Annotation
Semantic Relation Frequency
Agent 320
Booked object 298
Location 285
Time 209
Coordination 134
Beneficiary 117
Price 108
Reference Location 66
Table 2: Most frequent semantic relations in the
gold annotation.
This corpus has been semantically annotated
as part of the French ANR project PORT-
MEDIA (Rojas-Barahona et al, 2011). We are
using a set of 330 utterances manually annotated
with gold semantic relations (i.e. High-Level Se-
mantics). This gold corpus gathers 653 head seg-
ments and 1555 argument segments, from which
around 20% are both arguments and heads, such
as une chambre in Figure 4. Table 2 shows the
semantic relations frequencies in the gold annota-
tion. 12 head segment types and 19 different argu-
ment segment types are defined in the gold anno-
tations. In the evaluation, we assume the number
of both classes is given. A possible extension of
the approach to automatically infer the number of
classes would be to use a non-parametric model,
but this is left for future work.
4.2 Evaluation metrics
The proposed method infers three types of seman-
tic information:
? The semantic relation between an argument
and its head;
? The argument type A
? The semantic class of the head Ct.
The three outcomes are evaluated as follows.
? The output structure is a forest of trees that
is similar to a partial syntactic dependency
structure. We thus use a classical unsuper-
vised dependency parsing metric, the Un-
labeled Attachment Score (UAS), which is
simply the accuracy of argument attachment:
an argument is correctly attached if and only
if its inferred head matches the gold head.
? Both argument and head classes correspond
to the outcome of a clustering process into
semantic classes, akin to the semantic classes
obtained in unsupervised semantic role la-
beling tasks. We then evaluate them with a
classical metric used to evaluate these classes
in unsupervised SRL (as done for instance
in (Lang and Lapata, 2011a) and (Titov and
Klementiev, 2012)): purity and collocation.
Purity measures the degree to which each clus-
ter contains instances that share the same gold
class, while collocation measures the degree to
which instances with the same gold class are as-
signed to a single cluster.
More formally, the purity of argument seg-
ments? (head segment?) clusters for the whole cor-
pus is computed as follows:
PU = 1N
?
i
max
j
|Gj ? Ci|
whereCi is the set of argument (head) segments
in the ith cluster found, Gj is the set of argument
(head) segments in the jth gold class, and N is
the number of gold argument (head) segment in-
stances. In a similar way, the collocation of argu-
ment segments? (head segment?) clusters is com-
puted as follows:
CO = 1N
?
j
max
i
|Gj ? Ci|
Finally the F1 measure is the harmonic mean of
the purity and collocation:
F1 = 2 ? CO ? PUCO + PU
4.3 Experimental results
We compare the proposed approach against two
baselines:
? An argument-head ?attachment? baseline,
which attaches each argument to the closest
head segment.
? A strong clustering baseline, which respec-
tively clusters the head and argument seg-
ments using a very effective topic model:
the Latent Dirichlet Allocation (LDA) ap-
proach (Blei et al, 2003).
17
Table 3 shows the UAS obtained for the pro-
posed model on the MEDIA corpus, while Table 4
shows the obtained Purity, Collocation and F1-
measure. In both cases, we compare the perfor-
mances of the proposed model with the respective
baseline. Our system outperforms both baselines
by a large margin.
System UAS
Closest attachment 68%
(?2%)
Proposed - UAS 74%
(?2%)
Table 3: Experimental results for UAS on the ME-
DIA database. The statistical confidence interval
at 95% with Gaussian approximation is reported.
System Purity Col. F-mes
LDA - Heads 51.7% 25.5% 34.2%
LDA - Args 31.7% 22.2% 26.1%
Proposed - Heads 78.7% 50.8% 61.8%
Proposed - Args 61.8% 53.3% 59.3%
Table 4: Experimental results on the MEDIA
database for purity, collocation and F1-measure.
4.3.1 Qualitative Evaluation
We further carried out a qualitative evaluation,
where we inspected the inferred clusters and com-
pared them with the baseline. Figures 7 and 8
show, for every head class Ct in each stacked col-
umn, the distribution of instances from all gold
clusters. Each column can also be viewed as a
graphical representation of the intersection of one
inferred class with all gold clusters. Figure 7 illus-
trates this for our model, and Figure 8 for LDA.
The same comparison for the argument types is
shown, respectively, in Figure 5 and Figure 6.
For head segment clusters, we can observe that
most inferred clusters contain many instances of
the Reservation type (in dark blue), both in the
LDA baseline and in the proposed system. The
main reason for that is that the corpus is very un-
balanced in favor of the Reservation class, while
we do not assume any prior knowledge about the
data and thus use a uniform prior. Still, every other
gold type that occurs with a reasonnably high
enough frequency, apart from two special types
that are discussed next, is well captured by one of
Figure 5: Distribution of the gold types (one per
color) into the clusters inferred by our system
(shown on the X-axis) for argument segments.
our inferred class: this is the case for ?Room? that
mainly intersects with our class 1, ?Place? with
our class 2 and ?Hotel? with our class 9.
Some examples of instances for each case are:
? Reservation: ?voudrais re?server?, ?aimerais
partir?, ?voudrais une *re?servation une
re?servation?, ?prends?, ?recherche? ,
?*de?sire de?sire?, ?il me faudrait?, ?opte?,
?aimerais s? il vous pla??t si c? est possible
avoir prendre?.
? Room: ?deux chambres pour un coup(le)
avec trois enfants avec bon standing?, ?trois
singles?, ?deux chambres de bon standing
a` peu pre`s niveau trois e?toiles?, ?trois dou-
bles?.
? Place: ?Paris?, ?a` Saintes?, ?a`
Charleville?, ?dans le dix huitie`me ar-
rondissement de Paris?.
? Hotel: ?un ho?tel deux e?toiles?, ?dans un
ho?tel beau standing?, ?un ho?tel formule un?,
?l? ho?t(el) le l? ho?tel?, ?un autre ho?tel dans
les me?mes conditions?, ?le Beaugency?, ?l?
autre?, ?au Novotel?, ?le premier?.
Two ?special? head segment types that are nei-
ther nicely captured by our system nor LDA are
Coordination and Inform, which are instead as-
signed to the clusters corresponding to the gold
segments that they coordinate or inform about.
For argument segments we also observed that
the inferred clusters are semantically related to the
gold types. We found, for instance, four clusters
18
Figure 6: Distribution of the gold types (one per
color) into the clusters inferred by the LDA base-
line (shown on the X-axis) for argument segments.
Figure 7: Distribution of the gold types (one per
color) into the clusters inferred by our system
(shown on the X-axis) for head segments.
(2, 5, 12 and 15) containing mainly ?Time? ar-
guments (?du premier au trois Novembre?, ?dix
nuit?, ?le festival du film?, ?au seize Novembre?,
etc.), two (3 and 14) dedicated to ?Location? argu-
ments (?a` Menton?, ?au festival lyrique de belle
euh Belle Ile En mer?, ?bastille?, ?sur le ville de
Paris?, ?parking prive??), one (10) for ?Price? ar-
guments (?pas plus de cent euros par personne?,
?un tarif infe?rieur a` quatre vingts euros?, ?pas
trop che`re?, ?a` cent vingt euros?, ?moins de cent*
cent euros?) etc.
Finally, as noted for the head segments, we can
observe that the most frequent gold types largely
intersect with several inferred clusters, for the
same reason: data is very unbalanced and we do
not assume any prior knowledge about the data
Figure 8: Distribution of the gold types (one per
color) into the clusters inferred by the LDA base-
line (shown on the X-axis) for head segments.
and thus use an uniform prior. Nevertheless, sev-
eral other important classes such as Event, Price
and Agent are well captured by our system.
5 Conclusions
This work proposes an unsupervised generative
model to infer latent semantic structures on top
of user spontaneous utterances. It relies on the
Metropolis-Hastings sampling algorithm to jointly
infer both the structure and semantic classes. It
is evaluated in the context of the French MEDIA
corpus for the hotel reservation task. Although the
system proposed in this work is evaluated on a spe-
cific spoken dialog reservation task, it actually re-
lies on a generic unsupervised structured inference
model and can thus be applied to many other struc-
tured inference tasks, as long as observed word
segments are given.
An interesting future direction of research
would be to modify this model so that it jointly
infers both the latent syntactic and semantic struc-
tures, which are known to be closely related but
still carry complementary information. We of
course also plan to evaluate the proposed model
with automatic speech transcriptions and concepts
decoding. Another advantage of the proposed
model is the possibility to build better Metropolis-
Hastings proposals, which may greatly improve
the convergence rate of the algorithm. In partic-
ular, we would like to investigate the use of some
non-uniform proposal distributions when reattach-
ing an argument to a new head, which shall im-
prove mixing.
19
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, ACL ?09,
pages 28?36, Stroudsburg, PA, USA. Association
for Computational Linguistics.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. the Journal of machine Learn-
ing research, 3:993?1022.
Helene Bonneau-Maynard, Sophie Rosset, Christelle
Ayache, Anne Kuhn, and Djamel Mostefa. 2005.
Semantic annotation of the french MEDIA dialog
corpus. In INTERSPEECH-2005, 3457-3460.
N. Camelin, B. Detienne, S. Huet, D. Quadri, and
F. Lefe`vre. 2011. Unsupervised concept annota-
tion using latent dirichlet alocation and segmental
methods. In EMNLP 1st Workshop on Unsupervised
Learning in NLP, Edinburgh (UK).
Alexandre Denis, Matthieu Quignard, and Guillaume
Pitel. 2006. A Deep-Parsing Approach to Natural
Language Understanding in Dialogue System: Re-
sults of a Corpus-Based Evaluation. In Proceedings
of the 5th international Conference on Language Re-
sources and Evaluation (LREC 2006) Proceedings
of Language Resources and Evaluation Conference,
pages 339?344, Genoa Italie.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In Proc. EMNLP, pages
21?29.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role label-
ing. In Proc. EMNLP, pages 11?20.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
S. He and H. Gildea. 2006. Self-training and Cotrain-
ing for Semantic Role Labeling: Primary Report.
Technical report, TR 891, University of Colorado at
Boulder.
Ste?phane Huet and Fabrice Lefe`vre. 2011. Unsuper-
vised alignment for segmental-based language un-
derstanding. In Proceedings of the First Workshop
on Unsupervised Learning in NLP, EMNLP ?11,
pages 97?104, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Joel Lang and Mirella Lapata. 2010. Unsuper-
vised induction of semantic roles. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 939?
947, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Joel Lang and Mirella Lapata. 2011a. Unsupervised
semantic role induction via split-merge clustering.
In Proc. ACL, pages 1117?1126.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
EMNLP, pages 1320?1331. Association for Com-
puter Linguistics.
Joo-Young Lee, Young-In Song, and Hae-Chang Rim.
2007. Investigation of weakly supervised learning
for semantic role labeling. In ALPIT, pages 165?
170. IEEE Computer Society.
Marie-Jean Meurs, Fabrice Lefe`vre, and Renato
de Mori. 2009. Spoken language interpretation: On
the use of dynamic bayesian networks for semantic
composition. In Proc. ICASSP, pages 4773?4776.
Nitin Namdeo Pise and Parag Kulkarni. 2008. A sur-
vey of semi-supervised learning methods. In Pro-
ceedings of the 2008 International Conference on
Computational Intelligence and Security - Volume
02, CIS ?08, pages 30?34, Washington, DC, USA.
IEEE Computer Society.
Lina Maria Rojas-Barahona, Thierry Bazillon,
Matthieu Quignard, and Fabrice Lefevre. 2011.
Using MMIL for the high level semantic annotation
of the french MEDIA dialogue corpus. In Pro-
ceedings of the Ninth International Conference on
Computational Semantics (IWCS 2011).
Robert S. Swier and Suzanne Stevenson. 2004. Un-
supervised Semantic Role Labelling. In EMNLP,
pages 95?102. Association for Computational Lin-
guistics.
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the Conference of the
European Chapter of the Association for Computa-
tional Linguistics, Avignon, France, April.
Xiaojin Zhu. 2005. Semi-Supervised Learning Liter-
ature Survey. Technical report, Computer Sciences,
University of Wisconsin-Madison.
20
Proceedings of the SIGDIAL 2013 Conference, pages 357?359,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Weakly and Strongly Constrained Dialogues for Language Learning
Claire Gardent
CNRS/LORIA, Nancy
claire.gardent@loria.fr
Alejandra Lorenzo
Universite? de Lorraine
LORIA, Nancy
alejandra.lorenzo@loria.fr
Laura Perez-Beltrachini
KRDB Research Centre
FUB, Bolzano
laura.perez@loria.fr
Lina Rojas-Barahona
Universite? de Lorraine
LORIA, Nancy
lina.rojas@loria.fr
Abstract
We present two dialogue systems for lan-
guage learning which both restrict the di-
alog to a specific domain thereby pro-
moting robustness and the learning of a
given vocabulary. The systems vary in how
much they constrain the learner?s answer :
one system places no other constrain on
the learner than that provided by the re-
stricted domain and the dialog context ; the
other provides the learner with an exercise
whose solution is the expected answer.
The first system uses supervised learning
for simulating a human tutor whilst the
second one uses natural language gener-
ation techniques to produce grammar ex-
ercises which guide the learner toward the
expected answer.
1 Introduction
Work on dialog based tutors for language learn-
ing includes both chatbot systems which maintain
a free flowing dialog with the learner (Shawar and
Atwell, 2007; Jia, 2004) and form-focused dia-
log systems which restrict the learner answer e.g.,
by providing her with an answer template to be
filled in for the dialog to continue (Wilske and
Wolska, 2011). While the former encourages lan-
guage practice with a virtual tutor and requires a
good knowledge of the language, the latter focuses
on linguistic forms and usually covers a more re-
stricted lexical field thereby being more amenable
to less advanced learners.
In these notes, we describe a dialog architecture
which (i) supports both free-flowing and form-
focused man/machine dialog ; and (ii) ensures that
in both cases, dialogs are restricted to a specific
lexical field. The free-flowing dialog system uses
supervised classification techniques to predict the
system dialog move based on the learner?s input
and does not explicitely constrain the learner?s an-
swer. In contrast, the dialog system for intermedi-
ate learners provides an exercise which the learner
must solve to construct her answer.
To restrict the dialog to a specific domain and
to improve system robustness, we make use of a
finite-state automaton (FSA) describing the range
of permissible interactions within a given domain.
This FSA serves to guide the collection of human-
human interactions necessary to train the classi-
fier ; to verify and if necessary to adjust the sys-
tem?s predicted answer ; and to support the gener-
ation of the exercise provided in the form-focused
dialog engine.
2 Finite State Automaton and Domain
Representation
To support focused dialog and promote robust-
ness, we make use of the FSA depicted in Fig-
ure 2. This FSA models a generic sequence of
(System Question, Learner Answer, System Re-
action) turns instantiating 4 main question/answer
patterns and two possible system reactions.
The system question systematically formulates
a question which explicitly lists a set of possible
answers.
The learner?s answer is then classified into four
main types. It selects one of the proposed answers ;
it chooses another alternative not mentioned in
the system?s question ; it indicates that the learner
does not know the correct answer or does not un-
derstand the question ; or it meets neither of these
categories.
Finally, for each answer, the system can con-
tinue the dialogue by making a ?confirmation?
which is used to reassure the learner in the free
dialogue system and to provide some feedback in
the guided dialogue system ; or just by ?inform-
ing? to the learner of the correct answer before
getting into the next question.
Given a specific domain, this generic FSA is
357
>S:Welcome to my house!, I?m the cat. Do you want
to play with me? || ask(ready)
>L:Ok. || answer(ready)
>S:Great! How are you doing? || ask(health)
>L: I?m doing very well, thanks!
|| answer(health, pref-option)
>S: Awesome! || confirm(health,pref-option)
>S: Come! I want to show you the kitchen!
|| inform(health)
>S:In the kitchen we prepare lunch. I prepare
soup very often. Do you prefer soup or salad?
|| ask(pref-meal)
>L:I prefer salad.
|| answer(pref-meal,pref-option)
>S:So do I! || confirm(pref-meal,pref-option)
>S:I also love cheese. || inform(pref-meal)
>S:Where do we usually keep the cheese?
|| ask(cheese where)
>L:In the fridge || answer(cheese where,pref-option)
>S:You are right || confirm(cheese where,pref-option)
>S:In the fridge || inform(cheese where)
...
>S:Thanks for this little chat! See you soon
|| quit
FIGURE 1 ? Example of a dialog (translated from
French to English)
instantiated differently for each question/answer
pair making up a dialog. In particular, in the cur-
rent demonstration system, it is instantiated to
model a dialog situated in the kitchen of a virtual
world. Figure 1 shows an example dialog.
3 Situated Dialogues for language
learning
Our dialog systems for language learning are in-
tegrated in a serious game called I-FLEG (Interac-
tive French Learning Game, (Amoia et al, 2012))
in which the learner can trigger grammar exercices
and interactive dialog sessions by clicking on the
objects present in the virtual world.
IFLEG integrates the two dialog systems for
language learning mentioned above namely, a
?free answer dialog system? where the learner an-
swer is guided only by the preceding dialog ex-
changes ; and a ?guided dialog system? which re-
stricts the set of permissible answers by providing
the learner with an exercise whose solution pro-
vides a possible answer given the current dialog
context.
3.1 Data collection
To provide the training data necessary to train
the free dialog system, we conducted a Wizard-
of-Oz experiment where language learners were
invited to engage in a conversation with the wiz-
ard, a French tutor. In these experiments, we fol-
lowed the methodology and used the tools for
data collection and annotation presented in (Rojas-
Barahona et al, 2012a). Given an FSA specifiying
a set of 5 questions the learner had to answer, the
wizard guided the learner through the dialog us-
ing this FSA. The resulting corpus consists of 52
dialogues and 1906 sentences.
3.2 Free answer Dialogue System
The free answer dialogue system simulates
the behavior of the wizard tutor by means of
a Logistic-Regression classifier, the FSA and
a generation-by-selection algorithm. The system
first uses the FSA to determine the next question
to be asked. Then for each question, the Logistic-
Regression classifier is used to map the learner an-
swer to a system dialog act. At this stage, the FSA
is used again, in two different ways. First, it is used
to ensure that the predicted system dialog act is
consistent with the states in the FSA. In case of a
mismatch, a valid dialog act is selected in the cur-
rent context. In particular, unpredicted ?preferred
options? and ?do not know? learner answers are
detected using keyword spotting methods. If the
classifier prediction conflicts with the prediction
made by key word spotting, it is ignored and the
FSA transition is prefereed.
Second, since the system has several consecu-
tive turns, and given that the classifier only pre-
dicts the next one, the FSA is used to determine
the following system dialog acts sequence. For
instance, if the predicted next system dialog act
was ?confirm?, according to the FSA the follow-
ing system dialog act is ?inform? and then eiher
the next question encoded in the FSA or ?quit?.
Training the simulator To train the classifier,
we labeled each learner sentence with the dialog
act caracterising the next system act. The features
used for trainig included context features (namely,
the four previous system dialogue acts) and the set
of content words present in the learner turns af-
ter filtering using tf*idf (Rojas Barahona et al,
2012b). Given the learner input and the current di-
alog context, the classifier predicts the next system
move.
Generation by Selection Given the system move
predicted by the dialog manager, the system turn
is produced by randomly selecting from the train-
ing corpus an utterance annotated with that dialog
move.
3.3 Guided dialogue system
Unlike the free answer dialogue, the guided di-
alogue strongly constrains the learner answer by
suggesting it in the form of a grammar exercise.
358
FIGURE 2 ? Finite-state automata that defines the different states in the dialog for each question Q X. S
defines the system, and P the learner.
In the guided dialogue system, the dialogue
paths contained in the training corpus are used to
decide on the next dialogue move. In a first step,
learner?s moves are labelled with the meaning rep-
resentation associated to them by the grammar un-
derlying the natural language generator used to
produce IFLEG grammar exercises. Given a se-
quence S/L contained in the training corpus with
S, a system turn and L the corresponding learner?s
turn, the system then constructs the exercise pro-
viding the learner?s answer using the methodology
described in (Perez-Beltrachini et al, 2012). First,
a sentence is generated from the meaning repre-
sentation of the learner answer. Next, the linguis-
tic information (syntactic tree, morpho-syntactic
information, lemmas) associated by the generator
with the generated sentence is used to build a shuf-
fle, a fill-in-the-blank or a transformation exercise.
Here is an example interaction produced by the
system :
S : Vous pre?fe?rez la soupe ou le fromage ? (Do you
prefer soup or salad ?)
Please answer using the following words : { je,
adorer, le, soupe }
This dialogue setting has several benefits. The
dialogue script provides a rich context for each
generated exercise item, learners are exposed to
example communicative interactions, and the sys-
tem can provide feedback by comparing the an-
swer entered by the learner against the expected
one.
4 Sample Dialogue
In this demo, the user will be able to interact
with both dialogue systems, situated in the kitchen
of a virtual world, and where the tutor prompts
the learner with questions about meals, drinks,
and various kitchen related activities such as floor
cleaning and food preferences.
References
M. Amoia, T. Bre?taudie`re, A. Denis, C. Gardent, and
L. Perez-Beltrachini. 2012. A Serious Game for Second
Language Acquisition in a Virtual Environment. Jour-
nal on Systemics, Cybernetics and Informatics (JSCI),
10(1) :24?34.
J. Jia. 2004. The study of the application of a web-based
chatbot system on the teaching of foreign languages. In
Society for Information Technology & Teacher Educa-
tion International Conference, volume 2004, pages 1201?
1207.
L. Perez-Beltrachini, C. Gardent, and G. Kruszewski. 2012.
Generating Grammar Exercises. In NAACL-HLT 7th
Workshop on Innovative Use of NLP for Building Educa-
tional Applications, Montreal, Canada, June.
L. M. Rojas-Barahona, A. Lorenzo, and C. Gardent. 2012a.
Building and exploiting a corpus of dialog interactions be-
tween french speaking virtual and human agents. In Pro-
ceedings of the 8th International Conference on Language
Resources and Evaluation.
L. M. Rojas Barahona, A. Lorenzo, and C. Gardent. 2012b.
An end-to-end evaluation of two situated dialog systems.
In Proceedings of the 13th Annual Meeting of the Special
Interest Group on Discourse and Dialogue, pages 10?19,
Seoul, South Korea, July. ACL.
B. Abu Shawar and E. Atwell. 2007. Chatbots : are they
really useful ? In LDV Forum, volume 22, pages 29?49.
S. Wilske and M. Wolska. 2011. Meaning versus form in
computer-assisted task-based language learning : A case
study on the german dative. JLCL, 26(1) :23?37.
359
