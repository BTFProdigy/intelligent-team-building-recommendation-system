Example-based Speech Intention Understanding and
Its Application to In-Car Spoken Dialogue System
Shigeki Matsubara? Shinichi Kimura? Nobuo Kawaguchi?
Yukiko Yamaguchi? and Yasuyoshi Inagaki?
?Information Technology Center, Nagoya University
?Graduate School of Engineering, Nagoya University
CIAIR, Nagoya University
Furo-cho, Chikusa-ku, Nagoya, 464-8601, Japan
matubara@itc.nagoya-u.ac.jp
Abstract
This paper proposes a method of speech inten-
tion understanding based on dialogue examples.
The method uses a spoken dialogue corpus with
intention tags to regard the intention of each in-
put utterance as that of the sentence to which it
is the most similar in the corpus. The degree of
similarity is calculated according to the degree
of correspondence in morphemes and dependen-
cies between sentences, and it is weighted by
the dialogue context information. An exper-
iment on inference of utterance intentions us-
ing a large-scale in-car spoken dialogue corpus
of CIAIR has shown 68.9% accuracy. Further-
more, we have developed a prototype system of
in-car spoken dialogue processing for a restau-
rant retrieval task based on our method, and
confirmed the feasiblity of the system.
1 Introduction
In order to interact with a user naturally and
smoothly, it is necessary for a spoken dialogue
system to understand the intentions of utter-
ances of the user exactly. As a method of speech
intention understanding, Kimura et al has pro-
posed a rule-based approach (Kimura et al,
1998). They have defined 52 kinds of utterance
intentions, and constructed rules for inferring
the intention from each utterance by taking ac-
count of the intentions of the last utterances, a
verb, an aspect of the input utterance, and so
on. The huge work for constructing the rules,
however, cannot help depending on a lot of
hands, and it is also difficult to modify the rules.
On the other hand, a technique for tagging di-
alogue acts has been proposed so far (Araki et
al., 2001). For the purpose of concretely deter-
mining the operations to be done by the system,
the intention to be inferred should be more de-
tailed than the level of dialogue act tags such as
?yes-no question? and ?wh question?.
This paper proposes a method of understand-
ing speeches intentions based on a lot of dia-
logue examples. The method uses the corpus in
which the utterance intention has given to each
sentence in advance. We have defined the ut-
terance intention tags by extending an annota-
tion scheme of dialogue act tags, called JDTAG
(JDRI, 2000), and arrived at 78 kinds of tags
presently. To detail an intention even on the
level peculiar to the task enables us to describe
the intention linking directly to operations of
the system.
In the technique for the intention inference,
the degree of similarity of each input utter-
ance with every sentence in a corpus is calcu-
lated. The calculation is based on the degree of
morphologic correspondence and that of depen-
dency correspondence. Furthermore, the degree
of similarity is weighted by using dialogue con-
text information. The intention of the utterance
to which the maximum score is given in the cor-
pus, will be accepted as that of the input utter-
ance. Our method using dialogue examples has
the advantage that it is not necessary to con-
struct rules for inferring the intention of every
utterance and that the system can also robustly
cope with the diversity of utterances.
An experiment on intention inference has
been made by using a large-scale corpus of spo-
ken dialogues. The experimental result, provid-
ing 68.9% accuracy, has shown our method to
be feasible and effective. Furthermore, we have
developed, based on our method, a prototype
system of in-car spoken dialogue processing for
a restaurant retrieval task, and confirmed the
feasiblity of the system.
Chikaku-ni chushajo-wa aru-ka-na
(Is there a parking lot nearby?)
Kono chikaku-ni firstfood aru?
(Is there a first food shop near here?
Mosburger-ga gozai-masu-ga
(Mosburger is near here.)
Spoken dialogue 
corpus with
intention tags
?????????
???????????
??????????
????????
???????????
?????????????
Dependency and morpheme analysis
System?s
speech
Intensions
probability
Calculation
of similarity
weighting
Utterance intension: ?parking lot question?
Context information
User?s
speech
Figure 1: Flow of the intention inference pro-
cessing
2 Outline of Example-based
Approach
Intentions of a speaker would appear in the vari-
ous types of phenomenon relevant to utterances,
such as phonemes, morphemes, keywords, sen-
tential structures, and contexts. An example-
based approach is expected to be effective for
developing the system which can respond to the
human?s complicated and diverse speeches. A
dialogue corpus, in which a tag showing an ut-
terance intention is given to each sentence, is
used for our approach. In the below, the outline
of our method is explained by using an inference
example.
Figure 1 shows the flow of our intention
inference processing for an input utterance
?Chikaku-ni chushajo-wa aru-ka-na ? (Is there
a parking lot nearby?)?. First, morphological
analysis and dependency analysis to the utter-
ance are carried out.
Then, the degree of similarity of each input
utterance with sentences in the corpus can be
calculated by using the degree of correspon-
dence since the information on both morphol-
ogy and dependency are given to all sentences
in the corpus in advance. In order to raise the
accuracy of the intention inference, moreover,
the context information is taken into consid-
eration. That is, according to the occurrence
probability of a sequence of intentions learned
from a dialogue corpus with the intention tags,
the degree of similarity with each utterance is
weighted based on the intentions of the last ut-
terances. Consequently, if the utterance whose
degree of similarity with the input utterance is
the maximum is ?sono chikaku-ni chushajo ari-
masu-ka? (Is there a parking lot near there?)?,
the intention of the input utterance is regarded
as ?parking lot question?.
3 Similarity and its Calculation
This section describes a technique for calculat-
ing the degree of similarity between sentences
using the information on both dependency and
morphology.
3.1 Degree of Similarity between
Sentences
In order to calculate the degree of similarity be-
tween two sentences, it can be considered to
make use of morphology and dependency infor-
mation. The calculation based on only mor-
phemes means that the similarity of only sur-
face words is taken into consideration, and thus
the result of similarity calculation may become
large even if they are not so similar from a struc-
tural point of view. On the other hand, the cal-
culation based on only dependency relations has
the problem that it is difficult to express the lex-
ical meanings for the whole sentence, in partic-
ular, in the case of spoken language. By using
both the information on morphology and de-
pendency, it can be expected to carry out more
reliable calculation.
Formula (1) defines the degree of similarity
between utterances as the convex combination
? of the degree of similarity on dependency, ?d,
and that on morpheme, ?m.
? = ??d + (1 ? ?)?m (1)
?d : the degree of similarity in dependency
?m: the degree of similarity in morphology
? : the weight coefficient (0 ? ? ? 1)
Section 3.2 and 3.3 explain ?d and ?m, re-
spectively.
3.2 Dependency Similarity
Generally speaking, a Japanese dependency re-
lation means the modification relation between
a bunsetsu and a bunsetsu. For example,
a spoken sentence ?kono chikaku-ni washoku-
no mise aru? (Is there a Japanese restau-
rant near here?)? consists of five bunsetsus of
?kono (here)?, ?chikaku-ni (near)?, ?washoku-
no (Japanese-style food)?, ?mise (a restau-
rant)?, ?aru (being)?, and there exist some de-
pendencies such that ?mise? modifies ?aru?. In
the case of this instance, the modifying bun-
setsu ?mise? and the modified bunsetsu ?aru?
are called dependent and head, respectively. It
is said that these two bunsetsus are in a depen-
dency relation. Likewise, ?kono?, ?chikaku-ni?
and ?washoku-no? modify ?chikaku-ni?, ?aru?
and ?mise?, respectively. In the following of this
paper, a dependency relation is expressed as the
order pair of bunsetsus like (mise, aru), (kono,
chikaku-ni).
A dependency relation expresses a part of
syntactic and semantic characteristics of the
sentence, and can be strongly in relation to the
intentional content. That is, it can be expected
that two utterances whose dependency relations
are similar each other have a high possibility
that the intentions are also so.
A formula (2) defines the degree of similar-
ity in Japanese dependency, ?D, between two
utterances SA and SB as the degree of corre-
spondence between them.
?d =
2CD
DA + DB
(2)
DA: the number of dependencies in SA
DB: the number of dependencies in SB
CD : the number of dependencies in corre-
spondence
Here, when the basic forms of independent
words in a head bunsetsu and in a dependent
bunsetsu correspond with each other, these de-
pendency relations are considered to be in cor-
respondence. For example, two dependencies
(chikaku-ni, aru) and (chikaku-ni ari-masu-ka)
correspond with each other because the inde-
pendent words of the head bunsetsu and the de-
pendent bunsetsu are ?chikaku? and ?aru?, re-
spectively. Moreover, each word class is given
to nouns and proper nouns characteristic of a
dialogue task. If a word which constitutes each
dependency belongs to the same class, these de-
pendencies are also considered to be in corre-
spondence.
3.3 Morpheme Similarity
A formula (3) defines the degree of similarity in
morpheme ?m between two sentences SA and
????????????? ??????????????
(Is there a Japanese restaurant near here?)
????
Japanese dependency
4 dependencies
common dependencies: 3
?d = 0.86
Japanese morpheme
common morphemes: 6
?m = 0.80
?
= 0.82
Degree of Similarity
If?= 0.4,
= 0.4*0.86+0.6*0.80
7 morphemes
User?s utterance unit: Si
????????????????????? ?????????? ????
(Is there a European restaurant nearby?)
Example of utterance: Se
3 dependencies 8 morphemes
Figure 2: Example of similarity calculation
SB.
?m =
2CM
MA + MB
(3)
MA: the number of morphemes in SA
MB: the number of morphemes in SB
CM : the number of morphemes in correspon-
dence
In our research, if a word class is given to
nouns and proper nouns characteristic of a di-
alogue task and two morphemes belong to the
same class, these morphemes are also consid-
ered to be in correspondence. In order to ex-
tract the intention of the sentence more simi-
lar as the whole sentence, not only independent
words and keywords but also all the morphemes
such as noun and particle are used for the cal-
culation on correspondence.
3.4 Calculation Example
Figure 2 shows an example of the calculation
of the degree of similarity between an input ut-
terance Si ?kono chikaku-ni washoku-no mise
aru? (Is there a Japanese restaurant near
here?)? and an example sentence in a corpus,
Se, ?chikaku-ni yoshoku-no mise ari-masu-ka (Is
there a European restaurant located nearby?)?,
when a weight coefficient ? = 0.4. The num-
ber of the dependencies of Si and Se is 4 and
3, respectively, and that of dependencies in cor-
respondence is 2, i.e., (chikaku, aru) and (mise,
aru). Moreover, since ?washoku (Japanese-style
food)? and ?yoshoku? (European-style food)
belong to the same word class, the dependencies
(washoku, aru) and (yoshoku, aru) also corre-
spond with each other. Therefore, the degree
of similarity in dependency ?d comes to 0.86
by the formula (2). Since the number of mor-
phemes of Si and Se are 7 and 8, respectively,
and that of morphemes in correspondence is 6,
i.e., ?chikaku?, ?ni?, ?no?, ?mise?, ?aru(i)? and
?wa(yo)shoku?. Therefore, ?m comes to 0.80
by a formula (3). As mentioned above, ? us-
ing both morphemes and dependencies comes
to 0.82 by a formula (1).
4 Utilizing Context Information
In many cases, the intention of a user?s utter-
ance occurs in dependence on the intentions of
the previous utterances of the user or those of
the person to which the user is speaking. There-
fore, an input utterance might also receive the
influence in the contents of the speeches before
it. For example, the user usually returns the
answer to it after the system makes a question,
and furthermore, may ask the system a ques-
tion after its response. Then, in our technique,
the degree of similarity ?, which has been ex-
plained in Section 3, is weighted based on the
intentions of the utterances until it results in a
user?s utterance. That is, we consider the oc-
currence of a utterance intention In at a certain
time n to be dependent on the intentions of the
last N ? 1 utterances. Then, the conditional
occurrence probability P (In|I
n?1
n?N+1) is defined
as a formula (4).
P (In|I
n?1
n?N+1) =
C(Inn?N+1)
C(In?1n?N+1)
(4)
Here, we write a sequence of utterance in-
tentions In?N+1 ? ? ?In as Inn?N+1, call it in-
tentions N-gram, and write the number of
appearances of them in a dialogue corpus as
C(Inn?N+1). Moreover, we call the conditional
occurrence probability of the formula (4), in-
tentions N-gram probability.
The weight assignment based on the inten-
tions sequences is accomplished by reducing the
value of the degree of similarity when the in-
tentions N-gram probability is smaller than a
threshold. That is, a formula (5) defines the de-
gree of similarity ? using the weight assignment
by intentions N-gram probability.
Search
Condition search
Parking search
Nearness question
Shop question
Business hours question
Distance question
Time question
Rank question
Menu price question
Number of car question
Parking price question Parking question
???intention tag
???dialogue act tag
???conditional tag
leafYes-no question Wh question ???
Unknown information
Unknown information
Figure 3: Decision tree of intention tag (a part)
? =
{
?? (P (In|I
n?1
n?N+1) ? ?)
? (otherwise)
(5)
?: weight coefficient (0 ? ? ? 1)
?: the degree of similarity
?: threshold
A typical example of the effect of using inten-
tions N-gram is shown below. For an input ut-
terance ?chikaku-ni chushajo-wa ari-masu-ka?
(Is there a parking lot located nearby?)?, the
degree of similarity with a utterance with a
tag ?parking lot question? which intends to
ask whether a parking lot is located around
the searched store, and a utterance with a tag
?parking lot search? which intends to search a
parking lot located nearby, becomes the maxi-
mum. However, if the input utterance has oc-
curred after the response intending that there
is no parking lot around the store, the system
can recognize its intention not to be ?parking
lot question? from the intentions N-gram prob-
abilities learned from the corpus, As a result,
the system can arrive at a correct utterance in-
tention ?parking lot search?.
5 Evaluation
In order to evaluate the effectiveness of our
method, we have made an experiment on ut-
terance intention inference.
5.1 Experimental Data
An in-car speech dialogue corpus which has
been constructed at CIAIR (Kawaguchi et al,
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
weight coefficient ?
a
c
c
u
r
a
c
y
recall precision
Figure 4: Relation between the weight coeffi-
cient ? and the accuracy (? = 0.3)
2001), was used as a corpus with intention tags,
and analyzed based on Japanese dependency
grammar (Matsubara et al, 2002). That is,
the intention tags were assigned manually into
all sentences in 412 dialogues about restaurant
search recorded on the corpus. The intentions
2-gram probability was learned from the sen-
tences of 174 dialogues in them. The standard
for assigning the intention tags was established
by extending the decision tree proposed as a di-
alogue tag scheme (JDRI, 2000). Consequently,
78 kinds of intention tags were prepared in all
(38 kinds are for driver utterances). The inten-
tion tag which should be given to each utter-
ance can be defined by following the extended
decision tree. A part of intention tags and the
sentence examples is shown in Table 1, and a
part of the decision tree for driver?s utterances
is done in Figure 3 1.
A word class database (Murao et al, 2001),
which has been constructed based on the cor-
pus, was used for calculating the rates of cor-
respondence in morphemes and dependencies.
Moreover, Chasen (Matsumoto et al, 99) was
used for the morphological analysis.
5.2 Experiment
5.2.1 Outline of Experiment
We have divided 1,609 driver?s utterances of
238 dialogues, which is not used for learning
the intentions 2-gram probability, into 10 pieces
equally, and evaluated by cross validation. That
is, the inference of the intentions of all 1,609 sen-
1In Figure 3, the description in condition branches is
omitted.
????
????
???
????
????
????
????
???
? ??? ??? ??? ??? ??? ??? ??? ??? ??? ?
????????????????????
?
?
?
?
?
?
?
?
???????????????????
?????????????????
Figure 5: Relation between the weight coeffi-
cient ? and the accuracy
tences was performed, and the recall and preci-
sion were calculated. The experiments based on
the following four methods of calculating the de-
gree of similarity were made, and their results
were compared.
1. Calculation using only morphemes
2. Calculation using only denpendencies
3. Calculation using both morphemes and
denpendencies (With changing the value of
the weight coefficient ?)
4. Calculation using intentions 2-gram prob-
abilities in addition to the condition of 3.
(With changing the value of the weight co-
efficient ? and as ? = 0)
5.2.2 Experimental Result
The experimental result is shown in Figure 4.
63.7% as the recall and 48.2% as the precision
were obtained by the inference based on the
above method 1 (i.e. ? = 0), and 62.6% and
58.6% were done in the method 2 (i.e. ? = 1.0).
On the other hand , in the experiment on the
method 3, the precision became the maximum
by ? = 0.2, providing 61.0%, and the recall by
? = 0.3 was 67.2%. The result shows our tech-
nique of using both information on morphology
and dependency to be effective.
When ? ? 0.3, the precision of the method
3 became lower than that of 1. This is because
the user speaks with driving a car (Kawaguchi
et al, 2000) and therefore there are much com-
paratively short utterances in the in-car speech
corpus. Since there is a few dependencies per
Table 1: Intention tags and their utterance examples
intention tag utterance example
search Is there a Japanese restaurant near here?
request Guide me to McDonald?s.
parking lot question Is there a parking lot?
distance question How far is it from here?
nearness question Which is near here?
restaurant menu question Are Chinese noodles on the menu?
Morphological & 
Intension
Intension
Action
Dependency analysis
Shop
information
database
Search
Response
inference
generation
In-car
spoken
dialogue
corpus with
intension tags Calculation
Intensions 2-gram
probabilityWeighting
Dictionary &
parsing rules
Intension-action
transfer rules
Context
stack
Decision
Analysis
Results
User?s Utterance
System?s utterance
Figure 6: Configuration of the prototype system
one utterance, a lot of sentences in the corpus
tend to have the maximum value in inference
using dependency information.
Next, the experimental result of the inference
using weight assignment by intentions 2-gram
probabilities, when considering as ? = 0.3, is
shown in Figure 5. At ? = 0.8, the maximum
values in both precision and recall were provided
(i.e., the precision is 68.9%). This shows our
technique of learning the context information
from the spoken dialogue corpus to be effective.
6 In-car Spoken Dialogue System
In order to confirm our technique for automat-
ically inferring the intentions of the user?s ut-
terances to be feasible and effective for task-
oriented spoken dialogue processing, a proto-
type system for restaurant retrieval has been
developed. This section describes the outline of
the system and its evaluation.
6.1 Implementation of the System
The configuration of the system is shown in Fig-
ure 6.
Table 2: Comparison between the results on in-
ferred intentions and those on given intentions
Inferred Given
Intentions num. rate num. rate
Correct 31 51.7% 42 70.0%
Partially corr. 5 8.3% 4 6.7%
Incorrect 7 11.7% 2 3.3%
No action 17 28.3% 12 20.0%
1. Morphological and dependency anal-
ysis: For the purpose of example-based
speech understanding, the morphological
and dependency analyses are given to each
user?s utterance by referring the dictionary
and parsing rules. Morphological analy-
sis is executed by Chasen (Matsumoto et
al., 99). Dependency parsing is done based
on a statistical approach (Matsubara et al,
2002).
2. Intentions inference: As section 3 and
4 explain, the intention of the user?s ut-
terance is inferred according to the degree
of similarity of it with each sentence in a
corpus, and the intentions 2-gram proba-
bilities.
3. Action: The transfer rules from the
user?s intentions to the system?s actions
have been made so that the system can
work as the user intends. We have al-
ready made the rules for all of 78 kinds
of intentions. The system decides the ac-
tions based on the rules, and executes
them. After that, it revises the context
stack. For example, if a user?s utterance
is ?kono chikaku-ni washoku-no mise ari-
masu-ka (Is there a Japanese restaurant
near here?)?, its intention is ?search?. In-
ferring it, the system retrieves the shop
information database by utilizing the key-
words such as ?washoku (Japanese restau-
rant)? and ?chikaku (near)?.
4. Response generation: The system re-
sponds based on templates which include
the name of shop, the number of shops, and
so on, as the slots.
6.2 Evaluation of the System
In order to confirm that by understanding the
user?s intention correctly the system can behave
appropriately, we have made an experiment on
the system. We used 1609 of driver?s utterances
in Section 5.2.1 as the learning data, and the
intentions 2-gram probabilities learned by 174
of dialogues in Section 5.1. Furthermore, 60 of
driver?s utterances which are not included in the
learning data were used for the test. We have
compared the results of the actions based on the
inferred intentions with those based on the given
correct intentions. The results have been classi-
fied into four groups: correct, partially correct,
incorrect, and no action.
The experimental result is shown in Table
2. The correct rate including partial correct-
ness provides 76.7% for the giving intentions
and 60.0% for the inferred intentions. We have
confirmed that the system could work appropri-
ately if correct intentions are inferred.
The causes that the system based on given
intentions did not behave appropriately for 14
utterances, have been investigated. 6 utterances
are due to the failure of keywords processing,
and 8 utterances are due to that they are out of
the system?s expectation. It is expected for the
improvement of the transfer rules to be effective
for the former. For the latter, it is considered
to turn the responses such as ?I cannot answer
the question. If the questions are about ? ? ?, I
can do that.?
7 Concluding Remarks
This paper has proposed the example-based
method for inferring speaker?s intention. The
intention of each input utterance is regarded as
that of the most similar utterance in the cor-
pus. The degree of similarity is calculated based
on the degrees of correspondence in both mor-
phemes and dependencies, taking account of the
effects of a sequence of the previous utterance?s
intentions. The experimental result using 1,609
driver?s utterances of CIAIR in-car speech cor-
pus has shown the feasibility of example-based
speech intention understanding. Furthermore,
we have developed a prototype system of in-car
spoken dialogue processing for a restaurant re-
trieval task based on our method.
Acknowledgement: The authors would like
to thank Dr. Hiroya Murao, Sanyo Electric Co.
LTD. for his helpful advice. This work is par-
tially supported by the Grand-in-Aid for COE
Research of the Ministry of Education, Sci-
ence, Sports and Culture, Japan. and Kayamori
Foundation of Information Science Advance-
ment.
References
Araki, M., Kimura, Y., Nishimoto, T. and Ni-
imi, Y.: Development of a Machine Learnable
Discourse Tagging Tool, Proc. of 2nd SIGdial
Workshop on Discourse and Dialogue, pp. 20?
25 (2001).
The Japanese Discouse Research Initiative
JDRI: Japanese Dialogue Corpus of Multi-
level Annotation, Proc. of 1st SIGdial Work-
shop on Discourse and Dialogue (2000).
Kawaguchi, N., Matsubara, S., Iwa, H., Ka-
jita, S, Takeda, K., Itakura, F. and Inagaki,
Y.: Construction of Speech Corpus in Mov-
ing Car Environment, Proc. of ICSLP-2000,
Vol. III, pp. 362?365 (2000).
Kawaguchi, N., Matsubara, S., Takeda, K.
and Itakura, F.: Multimedia Data Collec-
tion of In-car Speech Communication, Proc.
of Eurospeech-2001, pp. 2027?2030 (2001).
Kimura, H., Tokuhisa, M., Mera, K., Kai, K.
and Okada, N.: Comprehension of Intentions
and Planning for Response in Dialogue, Tech-
nical Report of IEICE, TL98-15, pp:25?32
(1998). (In Japanese)
Matsubara, S., Murase, T., Kawaguchi, N. and
Inagaki, Y.: Stochastic Dependency Parsing
of Spontaneous Japanese Spoken Language,
Proc. of COLING-2002 (2002).
Matsumoto, Y., Kitauchi, A., Yamashita, T.
and Hirano, Y.: Japanese Morphological
Analysis System Chasen version 2.0 Man-
ual, NAIST Techinical Report, NAIST-IS-
TR99009 (1999).
Murao, H., Kawaguchi, N., Matsubara, S. and
Inagaki, Y.: Example-based Query Genera-
tion for Spontaneous Speech, Proc. of ASRU-
2000 (2001).
Stochastic Dependency Parsing of
Spontaneous Japanese Spoken Language
Shigeki Matsubara? Takahisa Murase? Nobuo Kawaguchi?
and Yasuyoshi Inagaki?
?Information Technology Center/CIAIR, Nagoya University
?Graduate School of Engineering, Nagoya University
Furo-cho, Chikusa-ku, Nagoya, 464-8601, Japan
matubara@itc.nagoya-u.ac.jp
Abstract
This paper describes the characteristic features
of dependency structures of Japanese spoken
language by investigating a spoken dialogue cor-
pus, and proposes a stochastic approach to de-
pendency parsing. The method can robustly
cope with inversion phenomena and bunsetsus
which don?t have the head bunsetsu by relax-
ing the syntactic dependency constraints. The
method acquires in advance the probabilities
of dependencies from a spoken dialogue corpus
tagged with dependency structures, and pro-
vides the most plausible dependency structure
for each utterance on the basis of the probabili-
ties. An experiment on dependency parsing for
driver?s utterances in CIAIR in-car spoken dia-
logue corpus has been made. The experimental
result has shown our method to be effective for
robust parsing of spoken language.
1 Introduction
With the recent advances of the continuous
speech recognition technology, a considerable
number of studies have been made on spoken
dialogue systems. For the purpose of smooth
interaction with the user, it is necessary for the
system to understand the spontaneous speech.
Since spoken language includes a lot of gram-
matically ill-formed linguistic phenomena such
as fillers, hesitations and self-repairs, grammar-
oriented approaches are not necessarily suited
to spoken language processing. A technique for
robust parsing is thus strongly required.
This paper describes the characteristic fea-
tures of Japanese spoken language on the ba-
sis of investigating a large-scale spoken dialogue
corpus from the viewpoint of dependency, and
moreover, proposes a method of dependency
parsing by taking account of such the features.
The conventional methods of dependency pars-
ing have assumed the following three syntactic
constraints (Kurohashi and Nagao, 1994):
1. No dependency is directed from right to
left.
2. Dependencies don?t cross each other.
3. Each bunsetsu 1 , except the last one, de-
pends on only one bunsetsu.
As far as we have investigated the corpus, how-
ever, many spoken utterance do not satisfy
these constraints because of inversion phenom-
ena, bunsetsus which don?t have the head bun-
setsu, and so on. Therefore, our parsing method
relaxes the first and third ones among the above
three constraints, that is, permits the depen-
dency direction from right to left and the bun-
setsu which doesn?t depend on any bunsetsu.
The parsing results are expressed by partial de-
pendency structures.
The method acquires in advance the proba-
bilities of dependencies from a spoken dialogue
corpus tagged with dependency structures, and
provides the most plausible dependency struc-
ture for each utterance on the basis of the prob-
abilities. Several techniques for dependency
parsing based on stochastic approaches have
been proposed so far. Fujio and Matsumoto
have used the probability based on the fre-
quency of cooccurrence between two bunsetsus
for dependency parsing (Fujio and Matsumoto,
1998). Uchimoto et al have proposed a tech-
nique for learning the dependency probability
model based on a maximum entropy method
(Uchimoto et al, 1999). However, since these
1A bunsetsu is one of the linguistic units in Japanese,
and roughly corresponds to a basic phrase in English.
A bunsetsu consists of one independent word and more
than zero ancillary words. A dependency is a modifica-
tion relation between two bunsetsus.
techniques are for written language, whether
they are available for spoken language or not
is not clear. As the technique for stochas-
tic parsing of spoken language, Den has sug-
gested a new idea for detecting and parsing
self-repaired expressions, however, the phenom-
ena with which the framework can cope are re-
stricted (Den, 1995).
On the other hand, our method provides the
most plausible dependency structures for nat-
ural speeches by utilizing stochastic informa-
tion. In order to evaluate the effectiveness of our
method, an experiment on dependency parsing
has been made. In the experiment, all driver?s
utterances in 81 spoken dialogues of CIAIR in-
car speech dialogue corpus (Kawaguchi et al,
2001) have been used. The experimental result
has shown our method to be available for robust
parsing of spontaneously spoken language.
2 Linguistic Analysis of Spontaneous
Speech
We have investigated spontaneously spoken ut-
terances in an in-car speech dialogue corpus
which is constructed at the Center for Inte-
grated Acoustic Information Research (CIAIR),
Nagoya University (Kawaguchi et al, 2001) The
corpus contains speeches of dialogue between
drivers and navigators (humans, a Wizard of
OZ system, or a spoken dialogue system) and
their transcripts.
2.1 CIAIR In-car Speech Dialogue
Corpus
Data collection project of in-car speech dia-
logues at CIAIR has started in 1999 (Kawaguchi
et al, 2002). The project has developed a pri-
vate car, and been collecting a total of about
140 hours of multimodal data such as speeches,
images, locations and so on. These data would
be available for investigating in-car speech dia-
logues.
The speech files are transcribed into ASCII
text files by hand. The example of a tran-
script is shown in Figure 1. As an advance
analysis, discourse tags are assigned to fillers,
hesitations, and so on. Furthermore, each
speech is segmented into utterance units by
a pause, and the exact start time and end
time are provided for them. The environ-
mental information about sex (male/female),
speaker?s role (driver/navigator), dialogue task
Figure 1: Sample transcription of dialogue
speech
(navigation/information retrieval/...), noise
(noisy/clean) is provided for each utterance
unit.
In order to study the features of in-car dia-
logue speeches, we have investigated all driver?s
utterance units of 195 dialogues. The num-
ber per utterance unit of fillers, hesitations and
slips, are 0.34, 0.07, 0,04, respectively. The fact
that the frequencies are not less than those of
human-human conversations suggests the in-car
speech of the corpus to be spontaneous.
2.2 Dependency Structure of Spoken
Language
In order to characterize spontaneous dialogue
speeches from the viewpoint of dependency,
we have constructed a spoken language cor-
pus with dependency structures. Dependency
analyses have been provided by hand for all
driver?s utterance units in 81 spoken dialogues
of the in-car speech corpus. The specifications
of part-of-speeches and dependency grammars
are in accordance with those of Kyoto Corpus
(Kurohashi and Nagao, 1997), which is one of
Japanese text corpora. We have provided the
following criteria for the linguistic phenomena
peculiar to spoken language:
? There is no bunsetsu on which fillers and
hesitations depend. They forms depen-
dency structures independently.
? A bunsetsu whose head bunsetsu is omitted
doesn?t depend on any bunsetsu.
? The specification of part-of-speeches has
been provided for the phrases peculiar to
spoken language by adding lexical entries
to the dictionary.
? We have defined one conversational turn as
a unit of dependency parsing. The depen-
Table 1: Corpus data for dependency analysis
Dialogues 81
Utterance units 7,781
Conversational turns 6,078
Bunsetsus 24,993
Dependencies 11,789
Dependencies per unit 1.52
Dependencies per turn 1.94
dencies might be over two utterance units,
but be not hardly over two conversational
turns.
The outline of the corpus with dependency anal-
yses is shown in Table 1. There exist 11,789
dependencies for 24,993 bunsetsus 2. The av-
erage number of dependencies per turn is 1.94,
and is exceedingly less than that of written lan-
guage such as newspaper articles (about 10 de-
pendencies). This does not necessarily mean
that dependency parsing of spoken language is
easy than that of written language. It is also
required to specify the bunsetsu with no head
bunsetsu because every bunsetsu does not de-
pend on another bunsetsu. In fact, the bunset-
sus which don?t have the head bunsetsu occupy
52.8% of the whole.
Next, we investigated inversion phenomena
and dependencies over two utterance units. 320
inversions, 3.8% of all utterance turns and
about 0.04 times per turn, are in this data. This
fact means that the inversion phenomena can
not be ignored in spoken language processing.
About 86.5% of inversions appear at the last
bunsetsu. On the other hand, 73 dependen-
cies, providing 5.4% of 1,362 turns consisting
of more than two units, are over two utterance
units. Therefore, we can conclude that utter-
ance units are not always sufficient as parsing
units of spoken language.
3 Stochastic Dependency Parsing
Our method provides the most plausible depen-
dency analysis for each spoken language utter-
ance unit by relaxing syntactic constraints and
utilizing stochastic information acquired from a
large-scale spoken dialogue corpus. In this pa-
per, we define one turn as a parsing unit accord-
2The frequency of filler bunsetsus is 3,049.
0
1000
2000
3000
4000
5000
6000
7000
8000
-4 -2 0 2 4 6 8
Distance between bunsetsus
Number of dependencies
-3 -1 1 3 5 7
Figure 2: Distance between dependencies and
its frequencies
ing to the result of our investigation described
in Section 2.2
3.1 Dependency Structural Constraints
As Section 1 has already pointed out, most
conventional techniques for Japanese depen-
dency parsing have assumed three syntactic
constraints. Since the phenomena which are not
hardly in written language appear frequently in
spoken language, the actual dependency struc-
ture does not satisfy such the constraints. Our
method relaxes the constraints for the purpose
of robust dependency parsing. That is, our
method considers that the bunsetsus, which
don?t have the head bunsetsu, such as fillers
and hesitations, depend on themselves (relax-
ing the constraint that each bunsetsu depends
on another only one bunsetsu). Moreover, we
permit that a bunsetsu depends on its left-side
bunsetsu to cope with the inversion phenomena
(relaxing the constraint that dependencies are
directed from left to right) 3.
3.2 Utilizing Stochastic Information
Our method calculates the plausibility of the
dependency structure by utilizing the stochastic
information. The following attributes are used
for that:
? Basic forms of independent words of a de-
pendent bunsetsu b
i
and a head bunsetsu
3Since the phenomena that dependencies cross each
other is very few, the constraint is not relaxed.
Table 2: Examples of the types of dependencies
Dependent bunsetsu Type of dependency
denwa-ga (telephone) case particle ?ga?
mise-de (at a store) case particle ?de?
hayaku (early) continuous form
ookii (big) adnominal form
kaeru (can buy) adnominal form
chotto (briefly) adverb
b
j
: h
i
, h
j
? Part-of-speeches of independent words of a
dependent bunsetsu b
i
and a head bunsetsu
b
j
: t
i
, t
j
? Type of the dependency of a bunsetsu b
i
:
r
i
? Distance between bunsetsus b
i
and b
j
: d
ij
? Number of pauses between bunsetsus b
i
and b
j
: p
ij
? Location of a dependent bunsetsu b
i
: l
i
Here, if a dependent bunsetsu b
i
has an ancillary
word, the type of the dependencies of a bunsetsu
b
i
, r
i
, is the lexicon, part-of-speech and conju-
gated form of the word, and if not so, r
i
is the
part-of-speech and the conjugated form of the
last morpheme. Table 2 shows several exam-
ples of the types of dependencies. The location
of the dependent bunsetsu means whether it is
the last one of the turn or not. As Section 2 in-
dicates, the method uses the location attribute
for calculating the probability of the inversion,
because most inverse phenomena tend to appear
at the last of the turn.
The probability of the dependency between
bunsetsus are calculated using these attribute
values as follows:
P (i rel? j|B)
=
C(i ? j, h
i
, h
j
, t
i
, t
j
, r
i
)
C(h
i
, h
j
, t
i
, t
j
, r
i
)
(1)
?
C(i ? j, r
i
, d
ij
, p
ij
, l
i
)
C(r
i
, d
ij
, p
ij
, l
i
)
Here, C is a cooccurrence frequency function
and B is a sequence of bunsetsus (b
1
b
2
? ? ?b
n
).
In the formula (1), the first term of the right
hand side expresses the probability of cooccur-
rence between the independent words, and the
second term does that of the distance between
bunsetsus. The problem of data sparseness is re-
duced by considering these phenomena to be in-
dependent each other and separating the prob-
abilities into two terms. The probability that a
bunsetsu which doesn?t have the head bunsetsu
can also be calculated in formula (1) by con-
sidering such the bunsetsu to depend on itself
(i.e., i = j). The probability that a dependency
structure for a sequence of bunsetsus B is S can
be calculated from the dependency probabilities
between bunsetsus as follows.
P (S|B) =
n
?
i=1
P (i rel? j|B) (2)
For a sequence of bunsetsus, B, the method
identifies the dependency structure with
?argmax
S
P (S|B)? satisfying the following
conditions:
? Dependencies do not cross each other.
? Each bunsetsu doesn?t no more than one
head bunsetsu.
That is, our method considers the dependency
structure whose probability is maximum to be
the most plausible one.
3.3 Parsing Example
The parsing example of a user?s utterance
sentence including a filler ?eto?, a hesita-
tion ?so?, a inversion between ?nai-ka-na? and
?chikaku-ni?, and a pause, ?Eto konbini nai-ka-
na ?pause? so sono chikaku-ni (Is there a conve-
nience store near there?)? is as follows:
The sequence of bunsetsus of the sentence is
?[eto (well)],[konbini (convenience store)],[nai-
ka-na (Is there?)],?pause?, [so], [sono (there)],
[chikaku-ni (near)]?. The types of dependent
of bunsetsus and the dependency probabilities
between bunsetsus are shown in Table 2 and
3, respectively. Table 3 expresses that, for in-
stance, the probability that ?konbini? depends
on ?nai-ka-na? is 0.40. Moreover, the probabil-
ity of that ?eto? depends on ?eto? means that
the probability of that ?eto? does not depend
on any bunsetsu. Calculating the probability
of every possible structure according to Table
3, that of the dependency structure shown in
Figure 3 becomes the maximum.
Table 3: Dependency probabilities between bunsetsus
eto konbini nai-ka-na so soko-no chikaku-ni
eto (well) 1.00 0.00 0.00 0.00 0.00 0.00
konbini (convenience store) 0.00 0.01 0.40 0.00 0.00 0.00
nai-ka-na (Is there?) 0.00 0.00 0.88 0.00 0.00 0.00
so (hesitation) 0.00 0.00 0.00 1.00 0.00 0.00
soko-no (there) 0.00 0.02 0.00 0.00 0.00 0.75
chikaku-ni (near) 0.00 0.00 0.80 0.00 0.00 0.02
      eto        konbini          nai-kana                   so    soko-no  chikaku-ni<pose>
Figure 3: Dependency structure of ?eto konbini
nai-kana ?pose? so soko-no chikaku-ni?
Table 4: Experimental result of dependency
parsing
Item (a) (b) (a)+(b)
Precision 82.0% 88.5% 85.5%
Recall 64.3% 83.3% 73.8%
(a): The result for 241 bunsetsus with a head
(b): The result for 240 bunsetsus with no head
(a)+(b): The result for 481 bunsetsus
4 Parsing Experiment
In order to evaluate the effectiveness of our
method, an experiment on dependency pars-
ing has been made using a corpus of CIAIR
(Kawaguchi et al, 2001).
4.1 Outline of Experiment
We used the same data as that for our investiga-
tions in Section 2.2. That is, among all driver?s
utterance units of 81 dialogues, 100 turns were
used for the test data, and 5978 turns for the
learning data. The test data, the average bun-
setsus per turn is 4.81, consists of 481 depen-
dencies.
4.2 Experimental Result
The results of the parsing experiment are shown
partially in Figure 4. Table 4 shows the evalu-
ation. For the parsing accuracy, both precision
and recall are measured. 355 of 415 dependen-
cies extracted by our method are correct depen-
dencies, providing 85.5% for precision rate and
73.8% for recall rate. We have confirmed that
the parsing accuracy of our method for spoken
language is as high as that of another meth-
ods for written language (Fujio and Matsumoto,
1998; Uchimoto et al, 1999).
Our method correctly specified 200 of 240
bunsetsus which don?t have the head bunsetsu.
Most of them are fillers, hesitations and so on.
It became clear that it is effective to utilize the
dependency probabilities for identifying them.
5 Concluding Remarks
This paper has proposed a method for depen-
dency parsing of Japanese spoken language.
The method can execute the robust analysis by
relaxing syntactic constraints of Japanese and
utilizing stochastic information. An experiment
on CIAIR in-car spoken dialogue corpus has
shown our method to be effective for sponta-
neous speech understanding.
This experiment has been made on the as-
sumption that the speech recognition system
has a perfect performance. Since the tran-
script generated by a continuous speech recog-
nition system, however, might include a lot of
recognition errors, exceedingly robust parsing
technologies are strongly required. In order to
demonstrate our method to be practical for au-
tomatic speech transcription, an experiment us-
ing a continuous speech recognition system will
be done.
Acknowledgement: The authors would like
to thank all members of SLP Group in our lab-
oratory for their contribution to the construc-
tion of the Japanese spoken language corpus
with the dependency analysis. This work is par-
tially supported by the Grand-in-Aid for COE
Example of correct parsing for inversion
e-to nedan-wa ryoho oshiete-morae-masu-ka daitai
(Well, could you tell me both prices?) 
Example of incorrect parsing (1)
Hm.. chushajo-no aru aru-ka-naa
(Is there a caf? with a parking lot nearby ?) 
chikaku-ni kissaten-te
Example of incorrect parsing (2)
Hm.. ramen-ga
(Is there a caf? with a parking lot nearby ?) 
ikura-gurai-no arun-ka
correct result
incorrect result
right answer
Figure 4: The results of parsing experiment (a part)
Research of the Ministry of Education, Science,
Sports and Culture, Japan and Aritificial Intel-
ligence Research Promotion Foundation.
References
Den, Y.: A Unified Approach to Parsing Spoken
Natural Language, Proceedings of 3rd Natu-
ral Language Processing Pacific Rime Sympo-
sium (NLPRS?95), pp. 574?579 (1995).
Fujio, M. and Matsumoto, Y.: Japanese Depen-
dency Structure Analysis based on Lexical-
ized Statistics, Proceedings of 3rd Conference
on Empirical Method for Natural Language
Processing (EMNLP?98), pp. 87?96 (1998).
Kawaguchi, N., Matsubara, S., Takeda, K.,
and Itakura, F.: Multi-Dimensional Data Ac-
quisition for Integrated Acoustic Information
Research, Proceedings of 3rd International
Conference on Language Resources and Eval-
uation (LREC2002), pp. 2043?2046 (2002).
Kawaguchi, N., Matsubara, S., Takeda, K. and
Itakura, F.: Multimedia Data Collection of
In-car Speech Communication, Proceedings of
7th European Conference on Speech Commu-
nication and Technology (Eurospeech2001),
pp. 2027?2030 (2001).
Kurohashi, S. and Nagao, M.: Kyoto Univer-
sity Text Courpus Project, Proceedings of 3rd
Conference of Association for Natural Lan-
guage Processing, pages:115?118 (1997). (In
Japanese)
Kurohashi, S. and Nagao, M.: ?KN Parser:
Japanese Dependency/Case Structure Ana-
lyzer? Proceedings of Workshop on Sharable
Natural Language Resources, pages:48?95
(1994).
Matsumoto, Y., Kitauchi, A., Yamashita, T.
and Hirano, Y.: Japanese Morphological
Analysis System Chasen version 2.0 Man-
ual, NAIST Techinical Report, NAIST-IS-
TR99009 (1999).
Uchimoto, K., Sekine, S. and Isahara, K.:
Japanese Dependency Structure Analysis
based on Maximum Entropy Models, Pro-
ceedings of 9th European Chapter of the
Association for Computational Linguistics
(EACL?99), pp. 196?203 (1999).
Construction of Structurally Annotated Spoken Dialogue Corpus
Shingo Kato
Graduate School of Information Science,
Dept. of Information Engineering,
Nagoya University
Furo-cho, Chikusa-ku, Nagoya
gotyan@el.itc.nagoya-u.ac.jp
Shigeki Matsubara
Yukiko Yamaguchi
Nobuo Kawaguchi
Information Technology Center,
Nagoya University
Furo-cho, Chikusa-ku, Nagoya
Abstract
This paper describes the structural an-
notation of a spoken dialogue corpus.
By statistically dealing with the corpus,
the automatic acquisition of dialogue-
structural rules is achieved. The di-
alogue structure is expressed as a bi-
nary tree and 789 dialogues consist-
ing of 8150 utterances in the CIAIR
speech corpus are annotated. To eval-
uate the scalability of the corpus for
creating dialogue-structural rules, a di-
alogue parsing experiment was con-
ducted.
1 Introduction
With the improvement of speech processing tech-
nologies, spoken dialogue systems that appropri-
ately respond to a user?s spontaneous utterances
and cooperatively execute a dialogue are desired.
It is important for cooperative spoken dialogue
systems to understand the intentions of a user?s
utterances, the purpose of the dialogue, and its
achievement state (Litman, 1990). To solve this
issue, several approaches have been so far pro-
posed. One of them is an approach in which the
system expresses the knowledge of the dialogue
with a frame and executes the dialogue accord-
ing to that frame (Goddeau, 1996; Niimi, 2001;
Oku, 2004). However, it is difficult to make a
frame that totally defines the content of the dia-
logue. Additionally, there is a tendency for the
dialogue style to be greatly affected by the frame.
Figure 1: The data collection vehicle(DCV)
In this paper, we describe the construction of
a structurally annotated spoken dialogue corpus.
By statistically dealing with the corpus, we can
achieve the automatic acquisition of dialogue-
structural rules. We suppose that the system can
figure out the state of the dialogue through the in-
cremental building of the dialogue structure.
We use the CIAIR in-car spoken dialogue cor-
pus (Kawaguchi, 2004; Kawaguchi, 2005), and
describe the dialogue structure as a binary tree.
The tree expresses the purpose of partial dia-
logues and the relations between utterances or
partial dialogues. The speaker?s intention tags
were provided in the transcription of the corpus.
We annotated 789 dialogues consisting of 8150
utterances. Due to the advantages of the dialogue-
40
0022 - 01:37:398-01:41:513 F:D:I:C:
(F ) [FILLER:well] &(F )
	
	 [delicious] &
 [Udon] &
 [restaurant] &
Example-based Spoken Dialogue System using WOZ System Log
Hiroya MURAO *,**, Nobuo KAWAGUCHI **,? Shigeki MATSUBARA **,?
Yukiko YAMAGUCHI? Yasuyoshi INAGAKI?
* Digital Systems Development Center,SANYO Electric Co., Ltd.,
Hirakata-shi, Osaka, 573-8534 Japan
** Center for Integrated Acoustic Information Research,Nagoya University,
? Information Technology Center, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya-shi, 464-8603 Japan
? The Faculty of Information Science and Technology, Aichi Prefectural University,
Nagakute-cho, Aichi-gun, Aichi, 480-1198, Japan
murao@hr.hm.rd.sanyo.co.jp
Abstract
This paper proposes a new framework for
a spoken dialogue system based on dia-
logue examples between human subjects
and the Wizard of OZ (WOZ) system. Us-
ing this framework and a model of infor-
mation retrieval dialogue, a spoken dia-
logue system for retrieving shop informa-
tion while driving in a car has been de-
signed. The system refers to the dialogue
examples to find an example that is suit-
able for generating a query or a reply. The
authors have also constructed a large-scale
dialogue database using a WOZ system,
which enables efficient collection of dia-
logue examples.
1 Introduction
Against the background of ever-increasing comput-
ing power, techniques for constructing spoken di-
alogue systems using large-scale speech and text
corpora have become the target of much research
(Levin et al, 1998; Young, 2002). In prior re-
search, the authors have proposed a spoken-dialogue
control technique using dialogue examples with the
aim of performing flexible dialogue control dur-
ing information-retrieval dialogue and of achieving
speech understanding robust against speech recog-
nition errors (Murao et al, 2001). This technique
uses input speech data and supplementary informa-
tion corresponding to input speech such as retrieval
formulas (queries) to form ?examples? that decide
system action. A system using this technique can-
not run effectively, however, without a large volume
of example data. Traditionally, though, collecting
human-to-human dialogue data and manually pro-
viding such supplementary information for each in-
stance of input speech has required considerable la-
bor.
In this paper, we address this problem and pro-
pose a new technique for constructing an example-
based dialogue system using, as example data, the
dialogue performed between a human subject and a
pseudo-spoken-dialogue system based on the Wiz-
ard of OZ (WOZ) scheme. We also describe a
specific spoken dialogue system for information re-
trieval that we constructed using this technique.
2 Dialogue Processing Based on Examples
We first provide an overview of example-based dia-
logue processing that we previously proposed (Mu-
rao et al, 2001).
2.1 Model of information retrieval dialogue
Given a scenario in which a human operator
searches an information database and returns infor-
mation to a user, dialog between the operator and
user can be modeled as shown in Fig. 1. The ele-
ments of this model are described below.
1. Request The user tells the operator the con-
tents of an inquiry and demands reference.
2. Retrieval The operator receiving the user?s re-
quest generates a query after referencing do-
main knowledge and current dialogue context
Domain Knowledge
and
Dialogue Context
(1)Request
(4)Reply
(2)Retrieval
(3)Search
     Results
Search
Tool
Information
Database
Queries
OperatorUser
request to query
result
 to re
ply
Search
Results
Figure 1: Information flow of information retrieval
dialogue
and then processes the query indirectly by ma-
nipulating a search tool such as an ordinary
computer.
3. Search results The search tool generates
search results.
4. Reply The operator returns a reply to the user
based on search results and dialogue context.
Setting up information flow in this way allows
us to view operator behavior in the following way.
Specifically, the operator in Fig. 1 makes two deci-
sions in the process of advancing dialog.
Decision 1: Generate a query after listening to user
speech
Decision 2: Generate a reply after receiving search
results
Here, an experienced operator would use more
than just the superficial information obtained from
user speech. To generate a query or reply that
best suits the user?s need at that time, the opera-
tor would also make use of domain knowledge, di-
alogue context, and the search results themselves.
In other words, this kind of dialogue processing can
be viewed as a mapping operation from input infor-
mation such as user speech and domain knowledge
to output information such as a query. With this in
mind, we considered whether a ?decision? to guide
such dialogue could be automatically performed by
referring to actual examples of behavior manifested
by an experienced human operator. In short, we de-
cided to store a large volume of dialogue examples,
i.e., mapping information, and to determine output
information for certain input information on the ba-
sis of mapping information stored in similar dia-
logue examples.
2.2 Generation of queries and replies based on
examples
2.2.1 Structure of example data
The two ?decisions? performed during the time of
information retrieval dialogue between the user and
operator can be expressed as a mapping between the
following input and output information.
? Input/output information in the decision for
generating a query:
Input User speech and dialogue context
Output Query
? Input/output information in the decision for
generating a reply:
Input User speech, dialogue context, and
search results
Output Reply
It is therefore sufficient to save those items that
cover such input and output information. Specifi-
cally, a large number of example data can be col-
lected using the following information as elements
to construct an example database.
1. Text of user speech
2. Query
3. Reply text
4. Search results
5. Dialogue context (past speech, grounding in-
formation, conversational objects , etc.)
The following describes the procedure for gener-
ating a query or reply with respect to input speech
by referencing an example database.
2.2.2 Query generation process
From among the examples in the example
database, the system extracts the one most similar
to the input speech and the dialogue context at that
time. It then adjusts the query in that example to fit
the input speech and generates a new query.
2.2.3 Reply generation process
The system performs a search based on the gen-
erated query and receives search results. It then ex-
tracts the most similar example from the example
database with respect to input speech, the dialogue
context at that time, and the search results. Finally,
the system adjusts the reply in that example to fit the
current conditions and generates a new reply.
2.3 Problem points
Operating a dialogue system based on dialogue
examples requires the construction of an example
database as described above. Constructing a large-
scale example database, moreover, requires a large
volume of dialogue text in which supplementary in-
formation such as queries and search results has
been provided with respect to input speech.
Up to now, we have been constructing an exam-
ple database by first collecting human-to-human di-
alogue and converting speech to text and then as-
signing queries, search results, and the like to each
instance of input speech. This, however, is a labori-
ous process. In addition, example data constructed
on the basis of human-to-human dialogue data may
have features different from those of human-to-
dialogue-system dialogue data. In other words, we
cannot call the above approach an optimal method
for constructing example data.
3 Construction of an Example Database
using the WOZ System
We propose the Wizard of OZ (WOZ) system as
one means of efficiently collecting dialogue data
that includes supplementary information attached to
speech. Carrying on a dialogue using WOZ makes
it possible to collect the information needed for
constructing an example database while collecting
speech data.
3.1 WOZ system
When carrying on a dialogue using the WOZ sys-
tem, the user feels that he or she is talking to a com-
pletely mechanical system despite the fact that a hu-
man being is actually being used for some of the
elements making up the dialogue system. Collect-
ing dialogue data by WOZ should therefore result in
Touch Panel
and
Display
Information
Database
Query 
Generation
Part
Speech 
Output
Reply 
Generation
Part
  Query
Reply Text
Log 
Information
Tree Structured
Keywords
  Search 
Results
The Operator
Speech Input
User
The WoZ Software
Reply-Statement
Bigram
Search 
Execution
Part
Speech 
Symthesis
Figure 2: Configuration of Wizard of OZ system
data that is closer to dialogue that would occur be-
tween a human and a machine.
Collecting spoken dialogue data using the WOZ
system has actually been performed a number of
times in the past (MADCOW, 1992; Bertenstam et
al., 1995; Life et al, 1996; Eskenazi et al, 1999;
San-Segundo et al, 2001; Lemmela and Boda, 2002;
Yoma et al, 2002). The objective of those stud-
ies, however, was to collect, analyze, and evaluate
dialogue data between people and artificial objects,
and in many cases, only one of the artificial-object?s
functions was taken over by a human, for example,
the speech recognition function.
Our study, however, goes further than the above.
In particular, we create special software (called
WOZ software) that allows a human being to per-
form the functions of interpreting user speech, gen-
erating queries and executing searches, and generat-
ing replies. We then propose a framework that en-
ables the operator (wizard) to carry on a dialogue
with the user while operating this WOZ software so
that obtained data can be used later to perform di-
rect control of a dialogue system. Specifically, we
configure a pseudo-spoken-dialogue system (WOZ)
consisting of WOZ software and an operator, hold
information retrieval dialogue between this system
and human subjects, and save the queries ,search re-
sults and reply statements generated at this time as
log information. We then use this log information
and text-converted speech to construct an example
database that can be used for dialogue control.
3.2 System configuration
Figure 2 shows the entire configuration of the WOZ
system that we constructed. In this configuration,
keywords 
search 
results
type of 
keywords
control buttons
Figure 3: An example of display of Wizard of OZ system (1): Query generation part
text input
buttons
search 
results
type of 
keywords
control buttons 
&
standard phrases
Figure 4: An example of display of Wizard of OZ system (2): Reply generation part
the WOZ software, which was created using the
C++ language, runs on a personal computer under
Windows2000. It consists of a screen for generat-
ing queries (query part) and a screen for generating
replies (reply part). Figures 3 and 4 show sample
screens of these parts. This GUI adopts a touch-
panel system to facilitate operations ? an operator
only has to touch a button on one of these screens
to generate a query, search an information database,
generate a reply, or output synthesized speech.
WOZ software must feature high operability to
achieve natural dialogue between the WOZ system
and a human user. When designing WOZ software
on the basis of a human-to-human dialogue corpus
that we previously collected, we used the following
techniques to enable the system to operate in real
time while carrying on a dialogue with the user.
First, the query part arranges keywords in a tree
structure by search type so that appropriate key-
words can be selected at a touch to generate a query
and retrieve information quickly 1 . Search results
are displayed at the bottom of the screen in list form.
Second, the reply part displays text-input buttons
for generating replies and a list of search results.
The text-input buttons correspond to words, phrases,
and short standard sentences, and pushing them in
1Queries that deal with context in regard to input speech are
currently not defined for the sake of simplicity in software op-
eration.
?Hungry, but not enough time.
?You want to eat Chineese noodle.
?Search Japanese food restaurant.
Figure 5: Examples of prompting panels
an appropriate order generates a reply in text form.
The arrangement of these text-input buttons on the
screen is based on connection frequency between
text elements (reply-statement bigram) as previously
determined from the human-to-human dialogue cor-
pus mentioned above. In other words, each text-
input button represents a text entry having the high-
est frequency of following the immediately previous
text entry to the left, which makes for quick genera-
tion of a reply. Furthermore, to enable quick input,
the section of the screen displaying the search results
has been designed so that the name portion of each
result can be touched directly and automatically in-
cluded in the reply. The generated reply in text form
is finally output in voice form via the speech synthe-
sis section of the system.
Switching back and forth between the query and
reply parts can be performed as needed using a
switch button. The reply part also includes but-
tons for instantly generating words and short phrases
of confirmation and encouragement (e.g., ?yes,? ?I
see?) while the user is speaking to create as natural
a dialogue as possible.
3.3 Collecting dialogue data by the WOZ
system
We targeted shop-information retrieval while driv-
ing a car as an information-retrieval application
based on spoken dialogue, and collected dialogue
data between the WOZ system and human subjects
(Kawaguchi et al, 2002). This data was collected
within an automobile driven by subjects each of
whom acted as a user searching for information. A
personal computer running the WOZ software was
placed in the automobile with the ?wizard? sitting
in the back seat. All spoken dialogue was recorded
using another personal computer.
Data collection was performed according to the
following procedure for a duration of about five min-
Table 1: Collected WOZ data
Number of Speech length Speech Units
sessions (min.)User WOZ User WOZ
487 499 791 13,828 12,487
utes per subject.
? A prompting panel such as shown in Fig. 5 is
presented to the subject.
? The subject converses freely with WOZ based
on the prompting panel shown.
The wizard operates the WOZ system while lis-
tening to the subject, that is, the wizard performs an
appropriate search and returns a reply using speech
synthesis 2 .
Table 1 shows the scale of collected data.
3.4 Constructing an example database using
WOZ log information
WOZ software was designed to output detailed log
information. This information consists mainly of
the following items. All log information is recorded
with time stamps.
? Speaker ID (input by the wizard when initiating
a dialogue)
? Query generated for the input speech in ques-
tion
? Search results returned for the generated query
(number of hits and shop IDs)
? Text of reply generated by the operator (wiz-
ard)
A saved WOZ log can be used to efficiently con-
struct an example database by the following proce-
dure. To begin with, a written record of user speech
is made based on the voice recording of spoken di-
alog with time information added. Next, based on
2The wizard generates queries, performs searches, and gen-
erates replies to the extent possible for speech to which defined
queries can be applied. If a query cannot be generated, the wiz-
ard will not keep trying and will generate only an appropriate
response.
(Well, search convenience stores near here.)
(I found CIRCLE-K Makinohara store and SUNKUS Kamenoi store near here.)
Search results
Dialogue history
The most similar 
example 
for query generation
The most similar 
example 
for reply generation
Input text
(Result of speech
 recognition)
Reply
Figure 6: A view of example-based dialogue system
Table 2: Configuration of constructed example
database
Number of Number of
sessions examples
243 1,206
the time information in the log output by WOZ soft-
ware, a correspondence is established between user
speech and queries and between search results and
replies.
We constructed an example database using a por-
tion of dialogue data collected in the above manner.
Table 2 summarizes the data used for this purpose.
Query and search-result correspondences were es-
tablished for about 20% of all user speech excluding
speech outside of the task in question and speech
outside of query specifications.
4 Spoken Dialogue System using Dialogue
Examples
We here describe a dialogue system that runs using
the example database that we constructed (see (Mu-
rao et al, 2001) for details). The task is to search for
shop information while inside an automobile. This
system was implemented using the C++ language
under Windows2000. Figure 6 shows a screen shot
of this example-based dialogue system.
4.1 System configuration
The following describes the components of this sys-
tem with reference to Fig. 7.
Dialogue example database (DEDB): Consists of
data constructed from dialogue text and log in-
formation output from WOZ software. Dia-
logue text is subjected to morphological anal-
ysis 3, and words essential to advancing the di-
alogue (e.g., shop name, facility name, food
name) are assigned word class tags based on
classes given to these words beforehand ac-
cording to meaning.
Word Class Database (WCDB): Consists of
words essential to the task in question and
classes given to them according to meaning.
Word classes are determined empirically based
on dialogue within the dialogue corpus.
Shop Information Database (SIDB): Consists of
a collection of information on about 800 restau-
rants and shops in Nagoya, the same as that
used in the WOZ system.
Speech Recognition: Uses ?Japanese Dictation
Toolkit(Kawahara et al, 2000)?. The lan-
guage model was created from the previously
collected human-to-human dialogue corpus.
3Using ChaSen morphological-analysis software for the
Japanese language (Asahara and Matsumoto, 2000).
Speech
Input
    Speech
Recognition   Query Generation
Search
Speech
Output
Dialogue Example
Database
(DEDB)
Word class
Database
(WCDB)
Shop Information
Database
(SIDB)
 Speech
Synthesis    Reply Generation
Figure 7: Configuration of example-based dialogue
system
Query Generation: Extracts from the DEDB the
example closest to current input speech and
conditions, modifies the query in that example
according to current conditions, and outputs the
result.
Search execution: Accesses the SIDB using the
generated query and obtains search results.
Reply Generation: Extracts from the DEDB the
example closest to input speech and search re-
sults, modifies the reply in that example ac-
cording to current conditions, and outputs the
result.
Speech Synthesis: Outputs replies in voice form
using a Japanese TTS (Text To Speech) soft-
ware ?EleganTalk Ver. 2.1? by Sanyo Electric
Co., Ltd. .
4.2 Operation
The following describes system operation (see Fig.
8 for a specific operation example).
Step 1: Extracting similar example for query
For a speech recognition result, the system
extracts the most similar example from the
DEDB. The robustness of the similarity cal-
culation between the input utterance and the
utterance in the DEDB should be considered
against the speech recognition error. Therefore,
a keyword matching method using the word
class information is adopted. For a speech
recognition result combined with a morpholog-
ical analysis result, independent words and the
Input:   Etto, spaghetti no omise ni ikitai na.
            (I'd like to go to a spaghetti restaurant.)
Keywords: [10: spaghetti],[omise (shop)],[iku (go)]
 1st:  U: <10:Curry> no [omise] ni [iki]tain desu kedo            
(I'd like to go to a curry restaurant. )
         Q: search KEY=<10:curry>
 2nd: U: <10: Ramen(noodles)> wo <tabe> ni [iki] taina       
(I'd like to eat noodles.)
         Q: search KEY=<10:ramen>
 3rd: U: [10: Spaghetti] de <yu-mei> na <tokoro> ga iidesu 
            ( I prefer a popular resutaurant for spaghetti.)
         Q: search KEY=<10:spaghetti>
Step1: Extracting similar example for query
Step2: Query Modification
Query in the similar case:      search KEY=<10:curry>
   Matched keywords pair:       ( <10:curry> , <10:spaghetti> )
                 Output Query:      search KEY=<10:spaghetti>
Step3: Search
  Iutput Query:      search KEY=<10:spaghetti>
 Search Result:     RESULT=NONE
Input:   Etto, spaghetti no omise ni ikitai na.
(I'd like to go to a spaghetti restaurant.)
Keywords: [10: spaghetti],[omise (shop)],[iku (go)]
1st:   U:<10: Ramen(noodles)> wo <tabe> ni [iki] taina 
(I'd like to eat noodles.)
        Q: search KEY=<10:ramen>
        S:<10:Ramen(noodles)> no [omise] wa chikaku ni arimasen 
              ( There are no noodle restaurants near here.)
        A: RESULT=NONE
2nd:  U:<10:Curry> no [omise] ni [iki]tain desu kedo  
(I'd like to go to a curry restaurant. )
        Q: search KEY=<10:curry>
        S:Hai, Curry no omise wa 5-ken arimasu          
             (Well, I found 5 curry restaurants.)
        A: RESULT=5, ID1=120,..,ID5=565
Step4: Extracting similar example for reply
 Search Result:     RESULT=NONE
Similar cases
{Similar cases
Step5: Reply Modification
     Reply in the similar case: 
                   <10:Ramen(noodles)> no [omise] wa chikaku ni arimasen 
                       ( There are no noodle restaurants near here.)
       Matched keywords pair:  
                  ( <10:Ramen(noodles)> , <10:spaghetti> )
                      Output Reply:  
                  <10:spaghetti> no [omise] wa chikaku ni arimasen
                       ( There are no spaghetti restaurants near here.)
Figure 8: Example of query and reply generation
important words to which the word class tags
are assigned according to the information in
the WCDB are regarded as the keywords, and
their similarity is calculated as follows. For
each transcription of a user?s utterances in the
DEDB, the number of matched words and the
number of important words which belong to
the same word class are accumulated with the
correspondent weight and the result is treated
as the similarity. The utterance which marks
the highest similarity is regarded as the most
similar one.
Step2: Query Modification The query for the ex-
tracted example is modified with reference to
the input utterance. The modification is per-
formed by replacing the keywords in the refer-
ence query using word class information.
Step 3: Search The SIDB is searched by using the
modified query and a search result is obtained.
Step 4: Extracting similar example for reply
The system extracts the most similar example
from the DEDB, by taking account of not only
the similarity between the input utterance and
the utterance in examples but also that between
the number of items in the search result and
that in the examples. Here, a total similarity
score is computed by performing a weighted
summation of two values: the utterance sim-
ilarity score and the search-results similarity
score obtained from the difference between
the number of search results in an example
and that obtained in Step 3. The search-results
similarity score is computed as follows.
When the number of search results by mod-
ified query is 0: Give the highest score to
examples in the example database with 0 num-
ber of search results and the lowest score to all
other examples.
When the number of search results by mod-
ified query is 1 or more: Give the high-
est score to examples in the example database
with the same number of search results and an
increasingly lower score as difference in the
number of search results becomes larger (use
heuristics).
For example, if not even one search result could
be obtained by the modified query, examples in
the example database with not even one search
result constitute a match.
Step 5: Reply Modification The reply statement
for the extracted example is modified with ref-
erence to the input utterance. The modification
is performed by replacing the words in the ref-
erence reply statement by using word class in-
formation. Then a speech synthesis module is
used to produce a reply speech.
4.3 Adding, modification, and deletion of
example data
This system allows example data to be added, mod-
ified, and deleted. When a failed operation occurs
while carrying on a dialogue, for example, buttons
located at the bottom of the screen can be used to
modify existing example data, add new examples,
and delete unnecessary examples.
5 Conclusion
This paper has proposed an efficient technique for
collecting example data using the Wizard of OZ
(WOZ) system for the purpose of guiding spoken di-
alogue using dialogue examples. This technique has
the following effects.
? Knowledge buried in the WOZ system log
(conversions from input speech to query and
reply, etc.) can be used as dialogue system
knowledge.
? Because dialogue is collected using the WOZ
system, the examples so collected are close to
dialogue that would occur in an environment
with an actual dialogue system. In other words,
dialogue examples can be collected under con-
ditions close to human-to-machine dialogue.
? The labor involved in recording speech neces-
sary for construction of an example database
can be reduced.
In future research, we plan to evaluate dialogue-
processing performance and context processing us-
ing example databases constructed with the WOZ
system.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
tagger. In Proceedings of COLING 2000, July.
J. Bertenstam, M. Blomberg, R. Carlson, K. Elenius,
B. Granstrom, J. Gustafson, S. Hunnicutt, J. Hogberg,
R. Lindell, L. Neovius, A. de Serpa-Leitao, L. Nord,
and N. Strom. 1995. The waxholm application data-
base. In Proceedings of Eurospeech-95, volume 1,
pages 833?836.
Maxine Eskenazi, Alexander Rudnicky, Karin Gregory,
Paul Constantinides Robert Brennan, Christina Ben-
nett, and Jwan Allen. 1999. Data collection and pro-
cessing in the carnegie mellon communicator. In Pro-
ceedings of Eurospeech-99, volume 6, pages 2695?
2698.
Nobuo Kawaguchi, Shigeki Matsubara, Kazuya Takeda,
and Fumitada Itakura. 2002. Multi-dimensional
data acquisition for integrated acoustic information
research. In Proc. of 3rd International Language
Resources and Evaluation Conference (LREC-2002),
pages 2043?2046.
T. Kawahara, A. Lee, T. Kobayashi, K. Takeda, N. Mine-
matsu, S. Sagayama, K. Itou, A. Ito, M. Yamamoto,
A. Yamada, T. Utsuro, and K. Shikano. 2000. Free
software toolkit for japanese large vocabulary contin-
uous speech recognition. In Proceedings of ICSLP-
2000, volume 4, pages 476?479.
Saija-Maaria Lemmela and Peter Pal Boda. 2002. Effi-
cient combination of type-in and wizard-of-oz tests in
speech interface development process. In Proceedings
of ICSLP-2002, pages 1477?1480.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
1998. Using markov decision processes for learning
dialogue strategies. In Proceedings of ICASSP98, vol-
ume 1, pages 201?204.
A. Life, I. Salter, J.N. Temem, F. Bernard, S. Rosset,
S. Bennacef, and L. Lamel. 1996. Data collection
for the mask kiosk: Woz vs prototype system. In Pro-
ceedings of ICSLP-96, pages 1672?1675.
MADCOW. 1992. Multi-site data collection for a spo-
ken language corpus. In DARPA Speech and Natural
Language Workshop ?92.
Hiroya Murao, Nobuo Kawaguchi, Shigeki Matsubara,
and Yasuyoshi Inagaki. 2001. Example-based query
generation for spontaneous speech. In Proceedings of
2001 IEEE Workshop on Automatic Speech Recogni-
tion and Understanding (ASRU2001).
R. San-Segundo, J.M. Montero, J.M. Gutierrez, A. Gal-
lardo, J.D. Romeral, and J.M. Pardo. 2001. A
telephone-based railway information system for span-
ish: Development of a methodology for spoken dia-
logue design. In Proceedings of SIGdial-2001, pages
140?148.
Nestor Becerra Yoma, Angela Cortes, Mauricio Hormaz-
abal, and Enrique Lopez. 2002. Wizard of oz evalua-
tion of a dialogue with communicator system in chile.
In Proceedings of ICSLP-2002, pages 2701?2704.
Steve Young. 2002. Talking to machines (statistically
speaking). In Proceedings of ICSLP-2002, pages 9?
16.
