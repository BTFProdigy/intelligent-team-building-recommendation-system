Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 78?82,
Prague, June 2007. c?2007 Association for Computational Linguistics
Combining Lexical-Syntactic Information with Machine Learning for
Recognizing Textual Entailment
Arturo Montejo-Ra?ez, Jose Manuel Perea, Fernando Mart??nez-Santiago,
Miguel A?ngel Garc??a-Cumbreras, Maite Mart??n-Valdivia, Alfonso Uren?a-Lo?pez
Dpto. de Informa?tica, Universidad de Jae?n
Campus de las Lagunillas s/n, 23071 - Jae?n
{amontejo, jmperea, dofer, magc, maite, laurena}@ujaen.es
Abstract
This document contains the description of
the experiments carried out by SINAI group.
We have developed an approach based on
several lexical and syntactic measures inte-
grated by means of different machine learn-
ing models. More precisely, we have eval-
uated three features based on lexical sim-
ilarity and 11 features based on syntactic
tree comparison. In spite of the relatively
straightforward approach we have obtained
more than 60% for accuracy. Since this
is our first participation we think we have
reached a good result.
1 Approach description
We fill face the textual entailment recognition us-
ing Machine Learning methods, i.e. identifying fea-
tures that characterize the relation between hypothe-
sis and associated text and generating a model using
existing entailment judgements that will allow us to
provide a new entailment judgement agains unseen
pairs text-hypothesis. This approach can be split into
the two processes shown in Figures 1 and 2.
In a more formal way, given a text t and an hy-
pothesis h we want to define a function e which takes
these two elements as arguments and returns and an-
swer to the entailment question:
e(t, h) =
{ Y ES if h is entailed by t
NO otherwise (1)
Now the question is to find that ideal function
Figure 1: Training processes
Figure 2: Classification processes
e(t, h). We will approximate this function using a
binary classifier:
e?(t, h) = bc(f,m) (2)
where
bc is a binary classifier
f is a set of features
m is the learned model for the classifier
Therefore, it only remains to select a binary clas-
sifier and a feature extraction method. We have per-
formed two experiments with different choices for
both decisions. These two experiments are detailed
below.
78
1.1 Lexical similarity
This experiment approaches the textual entailment
task being based on the extraction of a set of lexical
measures that show the existing similarity between
the hypothesis-text pairs. Our approach is similar
to (Ferrandez et al, 2007) but we make matching
between similar words too while (Ferrandez et al,
2007) apply exact matching (see below).
The first step previous to the calculation of the
different measures is to preprocess the pairs using
the English stopwords list. Next we have used the
GATE1 architecture to obtain the stems of tokens.
Once obtained stems, we have applied four different
measures or techniques:
? Simple Matching: this technique consists of
calculating the semantic distance between each
stem of the hypothesis and text. If this dis-
tance exceeds a threshold, both stems are con-
sidered similar and the similarity weight value
increases in one. The accumulated weight is
normalized dividing it by the number of ele-
ments of the hypothesis. In this experiment we
have considered the threshold 0.5. The values
of semantic distance measure range from 0 to
1. In order to calculate the semantic distance
between two tokens (stems), we have tried sev-
eral measures based on WordNet (Alexander
Budanitsky and Graeme Hirst, 2001). Lin?s
similarity measure (Lin, 1998) was shown to
be best overall measures. It uses the notion of
information content and the same elements as
Jiang and Conrath?s approach (Jiang and Con-
rath, 1997) but in a different fashion:
simL(c1, c2) = 2? log p(lso(c1, c2))log p(c1) + log p(c2)
where c1 and c2 are synsets, lso(c1,c2) is
the information content of their lowest super-
ordinate (most specific common subsumer) and
p(c) is the probability of encountering an in-
stance of a synset c in some specific corpus
(Resnik, 1995). The Simple Matching tech-
nique is defined in the following equation:
SIMmatching =
?
i?H similarity(i)
|H|
1http://gate.ac.uk/
where H is the set that contains the elements of
the hypothesis and similarity(i) is defined like:
similarity(i) =
{ 1 if ?j ? TsimL(i, j) > 0.5
0 otherwise
? Binary Matching: this measure is the same
that the previous one but modifying the simi-
larity function:
similarity(i) =
{ 1 if ?j ? T i = j
0 otherwise
? Consecutive Subsequence Matching: this
technique relies on forming subsequences of
consecutive stems in the hypothesis and match-
ing them in the text. The minimal size of the
consecutive subsequences is two and the max-
imum is the maximum size of the hypothesis.
Every correct matching increases in one the fi-
nal weight. The sum of the obtained weights of
the matching between subsequences of a cer-
tain size or length is normalized by the number
of sets of consecutive subsequences of the hy-
pothesis created for this length. These weights
are accumulated and normalized by the size of
the hypothesis less one. The Consecutive Sub-
sequence Matching technique is defined in the
following equations:
CSSmatching =
?|H|
i=2 f(SHi)
|H| ? 1
where SHi is the set that contains the subse-
quences of the hypothesis with i size or length
and f(SHi) is defined like:
f(SHi) =
?
j?SHi matching(j)
|H| ? i+ 1
where
matching(i) =
{ 1 if ?k ? STi k = j
0 otherwise
where STi represents the set that contains the
subsequences with i size from text.
? Trigrams: this technique relies on forming tri-
grams of words in the hypothesis and match-
ing them in the text. A trigram is a group of
79
three words. If a hypothesis trigram matches in
text, then the similarity weight value increases
in one. The accumulated weight is normalized
dividing it by the number of trigrams of the hy-
pothesis.
1.2 Syntactic tree comparison
Some features have been extracted from pairs
hypothesis-text related to the syntactic information
that some parser can produce. The rationale be-
hind it consists in measuring the similarity between
the syntactic trees of both hypothesis and associated
text. To do that, terms appearing in both trees are
identified (we call this alignment) and then, graph
distances (number of nodes) between those terms in
both trees are compared, producing certain values as
result.
In our experiments, we have applied the
COLLINS (Collins, 1999) parser to generate the
syntactic tree of both pieces of text. In Figure 3 the
output of the syntactic parsing for a sample pair is
shown. This data is the result of the syntactical anal-
ysis performed by the mentioned parser. A graph
based view of the tree corresponding to the hypoth-
esis is drawn in Figure 4. This graph will help us to
understand how certain similarity measures are ob-
tained.
Figure 3: Syntactic trees of sample hypothesis and
its associated text
<t>
(TOP (S (LST (LS 0302) (. .)) (NP (JJ Next) (NN year))
(VP (VBZ is) (NP (NP (DT the) (JJ 50th) (NN anniversary))
(PP (IN of) (NP (NP (DT the) (NNP Normandy) (NN invasion)
(, ,)) (NP (NP (DT an)(NN event)) (SBAR (IN that) (S (VP
(MD would) (RB n?t) (VP (VB have) (VP (VBN been) (ADJP
(JJ possible)) (PP (IN without) (NP (NP (DT the) (NNP
Liberty) (NN ships.)) (SBAR (S (NP (DT The) (NNS
volunteers)) (VP (VBP hope) (S (VP (TO to) (VP (VB raise)
(NP (JJ enough) (NN money)) (S (VP (TO to) (VP (VB sail)
(NP (DT the) (NNP O?Brien)) (PP (TO to) (NP (NNP France)))
(PP (IN for)(NP (DT the) (JJ big) (NNP D-Day) (NN celebration)
(. .))))))))))))))))))))))))))
</t>
<h>
(TOP (S (NP (NP (CD 50th) (NNP Anniversary)) (PP (IN of)
(NP (NNP Normandy) (NNP Landings)))) (VP (VBZ lasts) (NP
(DT a) (NN year) (. .)))))
</h>
From the sample above, the terms normandy, year
and anniversary appear in both pieces of text. We
say that these terms are ?aligned?. Therefore, for
the three possible pairs of aligned terms we can com-
pute the distance, in nodes, to go from one term to
the other at each tree. Then, the difference of these
Figure 4: Syntact tree of sample hypothesis
distances is computed and some statistics are gener-
ated. We can summarize the process of computing
this differences in the algorithm detailed in Figure 6.
Figure 5: Tree comparison process
For instance, in the tree represented in Figure 4
we can see that we have to perform 5 steps to go
from node Anniversary to node Normandy. Since
there are no more possible occurrences of these two
terms, then the minimal distance between them is
5. This value is also measured on the tree corre-
80
sponding to the text, and the absolute difference be-
tween these two minimal distances is stored in order
to compute final feature weights consisting in basic
statistical values. The algorithm to obtain the distri-
bution of distance differences is detailed in Figure 6.
Figure 6: Extraction of features based on syntactic
distance
Input:
a syntactic tree of the hypothesis Sh
a syntactic tree of the text St
Output :
the set of distance differences
Dd = {ddij : ti, tj ? T}
Pseudo code:
T ? aligned terms between Sh and St
Dd ? ?
for i = 1..n do
for j = i+ 1..n do
disth ? minimal distance between
nodes ti and tj in Sh
distt ? minimal distance between
nodes ti and tj in St
ddij ? |disth ? distt|
Dd ? {ddij} ?Dd
end-for
end-for
The statistics generated from the resulting list of
distances differences Dd are the following:
1. The number of aligned terms (3 in the given
example).
2. The number of matched POS values of aligned
terms, that is, if the term appears with the same
POS label in both texts (in the example An-
niversary differs in the POS label assigned).
3. The number of unmatched POS labels of
aligned terms.
4. The average distance in nodes through the syn-
tactic tree to go from one aligned term to an-
other.
5. The minimal distance difference found.
Table 1: Results with TiMBL and BBR classifiers
(Exp5 is the only official result reported in this pa-
per).
Experiment Classifier Accuracy
Exp1 BBR 0.6475
Exp2 BBR 0.64625
Exp3 BBR 0.63875
Exp4 TiMBL 0.6062
Exp5 TiMBL 0.6037
Exp6 TiMBL 0.57
6. The maximal distance difference found.
7. The standard deviation of distance differences.
In a similar way, differences in the depth level of
nodes for aligned terms are also calculated. From
the example exposed the following values were
computed:
* Aligned 3
* MatchedPOS 2
* UnmatchedPOS 1
* AvgDistDiff 0.0392156863
* MinDistDiff 0.0000000000
* MaxDistDiff 0.0588235294
* StdevDistDiff 0.0277296777
* AvgDepthDiff 2.0000000000
* MinDepthDiff 1.0000000000
* MaxDepthDiff 3.0000000000
* StdevDepthDiff 0.8164965809
2 Experiments and results
The algorithms used as binary classifiers are two:
Bayesian Logistic Regression (BBR)2 and TiMBL
(Daelemans et al, 1998). Both algorithms have been
trained with the devel data provided by the organiza-
tion of the Pascal challange. As has been explained
in previous sections, a model is generated via the
supervised learning process. This model m is then
feed into the classification variant of the algorithm,
which will decide whether a new hypothesis sample
is entailed by the given text or not.
The experiments and results are shown in Table 1:
where:
? Exp1 uses four features: three lexical similari-
ties (SIMmatching + CSSmatching + Trigrams)
and Syntactic tree comparison.
2http://www.stat.rutgers.edu/?madigan/BBR/ [available at
March 27, 2007]
81
? Exp2 uses five features: four lexical similari-
ties (SIMmatching + CSSmatching + Trigrams
+ BINmatching) and Syntactic tree compari-
son.
? Exp3 uses only three lexical similarities
(SIMmatching + CSSmatching + Trigrams).
? Exp4 uses the four lexical similarities
(SIMmatching + CSSmatching + Trigrams +
BINmatching)
? Exp5 uses only three lexical similarities
(SIMmatching + CSSmatching + Trigrams).
? Exp6 uses four features: three lexical similari-
ties (SIMmatching + CSSmatching + Trigrams)
and Syntactic tree comparison.
As we expected, the best result we have obtained
is by means of the integration of the whole of the
features available. More surprising is the good result
obtained by using lexical features only, even better
than experiments based on syntactical features only.
On the other hand, we expected that the integration
of both sort of features improve significatively the
performance of the system, but the improvement re-
spect of lexical features is poor (less than 2%). .
Similar topics share similar vocabulary, but not sim-
ilar syntax at all. Thus, we think we should to inves-
tigate semantic features better than the syntactical
ones.
3 Conclusions and future work
In spite of the simplicity of the approach, we have
obtained remarkable results: each set of features has
reported to provide relevant information concerning
to the entailment judgement determination. On the
other hand, these two approaches can be merged into
one single system by using different features all to-
gether and feeding with them several binary classi-
fiers that could compose a voting system. We will
do that combining TiMBL, SVM and BBR.We ex-
pect to improve the performance of the entailment
recognizer by this integration.
Finally, we want to implement a hierarchical ar-
chitecture based on constraint satisfaction networks.
The constraints will be given by the set of avail-
able features and the maintenance of the integration
across the semantic interpretation process.
4 Acknowledgements
This work has been partially financed by the
TIMOM project (TIN2006-15265-C06-03) granted
by the Spanish Government Ministry of Science and
Technology and the RFC/PP2006/Id 514 granted by
the University of Jae?n.
References
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet: An experimental, application-
oriented evaluation of five measures.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 1998. Timbl: Tilburg memory
based learner, version 1.0, reference guide.
Oscar Ferrandez, Daniel Micolo, Rafael Mu noz, and
Manuel Palomar. 2007. Te?cnicas le?xico-sinta?cticas
para reconocimiento de inmplicacio?n textual. . Tec-
nolog??as de la Informaco?n Multilingu?e y Multimodal.
In press.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings of International Conference on
Research in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the 15th International
Conference on Machine Learning.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
Montreal.
82
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 402?407, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SINAI: Machine Learning and Emotion of the Crowd for Sentiment
Analysis in Microblogs
E. Mart??nez-Ca?mara
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
emcamara@ujaen.es
A. Montejo-Ra?ez
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
amontejo@ujaen.es
M. T. Mart??n-Valdivia
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
maite@ujaen.es
L. A. Uren?a-Lo?pez
SINAI research group
University of Jae?n
E-23071, Jae?n (Spain)
laurena@ujaen.es
Abstract
This paper describes the participation of
the SINAI research group in the 2013 edi-
tion of the International Workshop Se-
mEval. The SINAI research group has
submitted two systems, which cover the
two main approaches in the field of sen-
timent analysis: supervised and unsuper-
vised.
1 Introduction
In the last years, the sentiment analysis (SA) re-
search community wants to go one step further,
which consists in studying different texts that
usually can be found in commerce websites or
opinions websites. Currently, the users publish
their opinions through other platforms, being one
of the most important the microblogging plat-
form Twitter1. Thus, the SA research commu-
nity is focused on the study of opinions that users
publish through Twitter. This interest is shown in
several workshops focused on the study of SA in
Twitter:
1. RepLab 2012 at CLEF2 (Amigo? et al,
2012): Competition carried out within the
CLEF conference, where the participants
had to develop a system for measuring the
reputation of commercial brands.
1http://twitter.com
2http://limosine-project.eu/events/
replab2012
2. TASS 2012 at SEPLN3(Villena-Roma?n et
al., 2013): Satellite event of the SEPLN
2012 Conference to foster the research in
the field of SA in social media, specifically
focused on the Spanish language.
In this paper is described the participation of
the SINAI4 research group in the second task of
the 2013 edition of the International Workshop
SemEval (Wilson et al, 2013). We have submit-
ted two systems (constrained and unconstrained).
The constrained system follows a supervised ap-
proach, while the unconstrained system is based
on an unsupervised approach which used two lin-
guistic resources: the Sentiment Analysis Lexi-
con5 (Hu and Liu, 2004) andWeFeelFine6 (Kam-
var and Harris, 2011).
The paper is organized as follows: first we
present a description of the preparing data pro-
cess. Then the constrained system is outlined.
The participation overview finishes with the de-
scription of the unconstrained system.
2 Preparing data
The organizers provided two sets of data, one for
training and another for the development. The
data was concerned by a set of identification
number of tweets with their corresponding po-
larity label. We used the script provided by the
organizers to download the two sets of tweets.
3http://www.daedalus.es/TASS/
4http://sinai.ujaen.es
5http://www.cs.uic.edu/?liub/FBS/
opinion-lexicon-English.rar
6http://wefeelfine.org
402
The python script was no able to download all the
tweets. The training set was composed by 8,633
tweets and the development set by 1,053 tweets.
The data preparation is a step in the workflow
of most data mining tasks. Also, in Natural Lan-
guage Processing is usual the preparation of the
documents or the texts for their further process-
ing. Internet is usually the source of texts for SA
tasks, so the application of a specific processing
to those texts with the aim of extracting their po-
larity is recommended. The texts published in
Twitter have several issues that must be resolved
before processing them:
1. The linguistic style of tweets is usually in-
formal, with a intensive usage of abbrevia-
tions, idioms, and jargon.
2. The users do not care about the correct use
of grammar, which increases the difficulty
of carrying out a linguistic analysis.
3. Because the maximum length of a tweet is
140 characters, the users normally refer to
the same concept with a large variety of
short and irregular forms. This problems is
known as data sparsity, and it is a challenge
for the sentiment-topic task.
4. The lack of context, which makes difficult
to extract the semantics of these sort pieces
of text.
Before applying a cleaning process to the cor-
pus with the aim of overcoming the issues de-
scribed above, we have studied the different
kinds of marks, like emoticons, question and ex-
clamation marks or hashtags in the tweets.
Regarding the issues listed above and the
marks in the tweets, we have carried out a clean-
ing and a normalization process which imply the
following operations:
1. The uppercase characters have been ex-
changed by lowercase characters.
2. Links have been replaced by the token
? ULR ?.
3. Question and exclamation marks have been
switched to the tokens ? QUESTION ? and
? EXCLAMATION ? respectively.
4. Mentions7 have been exchanged by the to-
ken ? MENTION ?.
5. All the HTML tags have been removed.
6. The hashtags8 have been normalized with
the token ? HASHTAG ?.
7. Tokens that express laughing (hahaha,
hehehe...) have been normalized with the
token ? LAUGH ?.
8. Users usually write expressions or abbrevi-
ations for surprise phrases like omg. All
these kind of expressions are replaced by the
token ? SURPRISE ?.
9. Positive emoticons like :), ;) or :, have been
normalized with the token ? HAPPY ?.
10. Negative emoticons like :(, :?( or :-( have
been normalized with the token ? SAD ?.
11. Twitter users usually repeat letters to em-
phasize the idea that they want to express.
Therefore, all the words with a letter re-
peated more than two times have been re-
duced to only two instances. For exam-
ple, the word ?aaaamaaaaaziiiing? in tweet
111733236627025920 is transformed into
?aamaaziing?.
After applying a normalization process to the
training and development sets, we have used for
the constrained system and the unsconstrained
system a dataset of 9,686 tweets.
3 Constrained System
The guidelines of the task define a constrained
system as a system that only can use the train
data provided by the organizers. Due to this re-
striction we decided to follow a supervised ap-
proach. It is required to define a set of parame-
ters when the supervised method is the elected.
The first step is to choose the minimum unit of
information, i.e. what segments of text are con-
sidered as features. Pang et al (2002) assert that
7A twitter mention is a reference to another user which
has the pattern ?@user name?
8A hashtag is the way to refer a topic in Twitter, which
has the pattern ?#topic name?
403
Class Precision Recall F1-score
Positive 0.6983 0.6295 0.6621
Neutral 0.6591 0.8155 0.7290
Negative 0.5592 0.2710 0.3651
Average 0.6652
Table 1: Assessment with TF-IDF weighting scheme
opinions or reviews should be represented with
unigrams, but other work shows bigrams and tri-
grams outperformed the unigrams features (Dave
et al, 2003). Therefore, there is not agreement
in the SA research community about what is the
best choice, unigrams or n-grams. Before several
validations on the training set of the task we de-
cided to use unigrams as feature for the polarity
classification process. Thus, for the supervised
algorithm, we have represented each tweet as a
vector of unigrams.
The next decision was about the application
of a stemmer process and getting rid off the En-
glish stop words. We only have applied stemmer
process to the data because in previous works
(Mart??nez-Ca?mara et al, 2013a) we did not reach
good results removing the stop words in texts
from Twitter. Another topic of discussion in the
SA research community is the weighting scheme.
Pang et al (2002) weighted each unigram fol-
lowing a binary scheme. Also, in the most cited
survey about SA (Pang and Lee, 2008) the au-
thors indicated that the overall sentiment may not
usually be highlighted through repeated use of
the same terms. On the other hand, Mart??nez-
Ca?mara et al (2011) achieved the best results
using TF-IDF as weighting scheme. Due to the
lack of agreement in the SA research community
about the use of a specific weight scheme, we
have carried out several assessments with aim of
deciding the most suitable one for the task. The
machine learning algorithm selected for the eval-
uation was SVM. The results are shown in Tables
1 and 2.
The results achieved with the two weighting
schemes are very similar. Regarding the posi-
tive class, the binary weighting scheme obtains
better results than the TF-IDF one, so the pres-
ence of positive keywords is more useful than
Class Precision Recall F1-score
positive 0.7037 0.6335 0.6668
neutral 0.6506 0.8313 0.7299
negative 0.5890 0.2105 0.3112
Average 0.6654
Table 2: Assessment with a binary weighting scheme
the frequent occurrence of those keywords. For
the neutral class, regarding precision and F1-
score, the TF-IDF scheme outperformed the bi-
nary scheme, but the recall had a higher value
when the terms are weighted binary. The pre-
cision of the classification for the neutral class
is only 1.2% better than the case where TF-IDF
is used, while recall and the F1-score is better
when the weighting of the features is binary. Al-
though the negative class has a similar perfor-
mance to that of the positive one with the two
weighting schemes, we highlighted the high dif-
ference between the other two classes and the
negative. The difference is more evident in the
recall value, while the neutral class has a value
of 0.8313 (binary), the negative one has a value
of 0.2105 (binary). Therefore, due to the fact that
the binary weighting scheme achieved better re-
sults in average, we decided to use it in the final
system.
The last step in the configuration of a su-
pervised approach based on machine learning is
the selection of the algorithm. The algorithm
selected was Support Vector Machine (SVM)
(Cortes and Vapnik, 1995). Our decision is based
on the widely used SVM by the research com-
munity of SA. The first application of SVM for
SA was in (Pang et al, 2002) with good re-
sults. Since the publication of the previous work,
other researchers have used SVM, and some of
them are: (Zhang et al, 2009), (Pang and Lee,
2004) and (Jindal and Liu, 2006). Also, the al-
gorithm SVM has been used to classify the po-
larity over tweets (Go et al, 2009) (Zhang et al,
2011) (Jiang et al, 2011). A broader review of
the research about SA in Twitter can be found in
(Mart??nez-Ca?mara et al, 2013b). Furthermore,
our decision is supported by previous in-house
experimentation.
404
For the experimentation we have used the
framework for data mining RapidMiner9. In
RapidMiner there are several implementations
of SVM, among which we have selected Lib-
SVM10(Chang and Lin, 2011) with built-in de-
fault parametrization.
To sum up, the configuration of the SINAI
constrained system is:
1. Machine learning approach: Supervised
2. Features: Unigrams.
3. Weighted scheme: Binary. If the term is
presence the value is 1, 0 in other case.
4. Stemmer: Yes
5. Stopper: No
6. Algorithm: SVM.
The results reached during the development
period are shown in Table 2
4 Unconstrained System
Our unconstrained system follows a two level
categorization approach, determining whether
the tweet is subjective or not at a first stage, and,
for the subjective classified ones, whether the
tweet is positive or negative. Both classification
phases are fully based on knowledge resources.
A predefined list of affective words is used for
subjectivity detection, and a search process over
the collection of emotions generated from a web
resource is applied for final polarity classifica-
tion. Figure 1 shows a general diagram of the
system.
4.1 Step 1: determining subjectivity
The system based in WeFeelFine only catego-
rizes between positive and negative texts, so a
preliminary classification into subjective and ob-
jective (i.e. neutral) must be performed. To this
end, a lexical approach is followed: those tweets
containing at least one affective term from a list
of predefined ones are considered subjective. If
9http://rapid-i.com/
10http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
Figure 1: Unconstrained system general diagram
affective terms are not found, then the tweet is
directly labeled as neutral. This list is called Sen-
timent Analysis Lexicon (SAL), which is defined
in the work of Bing Liu (Hu and Liu, 2004). The
list has two differentiated groups: a list of posi-
tive terms (agile, enjoy, improving) and another
with negative ones (anger, refusing, unable...).
At this phase, the polarity is not considered, so
both lists are merged into a list of around 6,800
subjectivity terms.
4.2 Step 2: determining polarity
The WeFeelFine project (Kamvar and Harris,
2011) has been used as knowledge base for po-
larity classification following the approach pro-
posed by (Montejo-Ra?ez, 2013). WeFeelFine11
gathers affective texts from several blogs, cre-
ating a huge database of mood-related expres-
sions. Almost two millions ?feelings? are col-
lected and indexed by the system. It is possible
to retrieve related sentences and expressions by
using its API. In this way, we have obtained the
11http://wefeelfine.org
405
top 200 most frequent feelings. For each feeling,
about 1,500 sentences are include in a document
that represents such a feeling. Then, using the
Lucene12 search engine, these documents have
been indexed. In this way, we can use an incom-
ing tweet as query and retrieve a ranked list of
feelings, as shown in Figure 2.
Figure 2: Polarity classification
The ranked list with the top 100 feelings (i.e.
those feelings more related to the tweet) is taken
for computing the final polarity by a summation
of the manually assigned polarity of the feeling
weighted with the score value returned by the en-
gine, as shown in Equation 1.
p(t) = 1
|R|
?
r?R
RSVr ? lr (1)
where
p(t) is the polarity of tweet t
R is the list of retrieved feelings
lr is the polarity label of feeling r
RSVr is the Ranking Status Value of the feel-
ing determined by Lucene.
As we did with the constrained system, we
also assess the unconstrained system before ap-
plying the test data. The results reached during
the evaluation phase are shown in Table 3. It is
remarkable the fact that the precision value of the
unconstrained system is a bit higher than the one
12http://lucene.apache.org/
Class Precision Recall F1-score
positive 0.5004 0.6341 0.5593
neutral 0.6772 0.5416 0.6018
negative 0.3580 0.3456 0.3516
Average 0.5094
Table 3: Assessment of the unconstrained system
reached by the constrained configuration. Thus,
SAL is a good resource for subjective classifi-
cation tasks. The unconstrained system reached
worse results with positive and negative classes,
but it is an expected result because supervised
approaches usually obtain better results than the
unsupervised and knowledge based approaches.
However, the polarity classification has reached
acceptable results, so it encourage us to follow
improving the method based of the use of We-
FeelFine.
Acknowledgments
This work has been partially supported by a grant
from the Fondo Europeo de Desarrollo Regional
(FEDER), TEXT-COOL 2.0 project (TIN2009-
13391-C04-02) and ATTOS project (TIN2012-
38536-C03-0) from the Spanish Government.
Also, this paper is partially funded by the Eu-
ropean Commission under the Seventh (FP7
- 2007-2013) Framework Programme for Re-
search and Technological Development through
the FIRST project (FP7-287607). This publica-
tion reflects the views only of the authors, and
the Commission cannot be held responsible for
any use which may be made of the information
contained therein.
References
Enrique Amigo?, Adolfo Corujo, Julio Gonzalo, Edgar
Meij, and Md Rijke. 2012. Overview of replab
2012: Evaluating online reputation management
systems. In CLEF 2012 Labs and Workshop Note-
book Papers.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1?27:27, May.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20:273?297.
406
Kushal Dave, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: opinion
extraction and semantic classification of product
reviews. In Proceedings of the 12th international
conference on World Wide Web, WWW ?03, pages
519?528, New York, NY, USA. ACM.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervi-
sion. CS224N Project Report, Stanford, pages 1?
12.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
tenth ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ?04,
pages 168?177, New York, NY, USA. ACM.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
- Volume 1, HLT ?11, pages 151?160, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Nitin Jindal and Bing Liu. 2006. Identifying com-
parative sentences in text documents. In Proceed-
ings of the 29th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?06, pages 244?251, New
York, NY, USA. ACM.
Sepandar D. Kamvar and Jonathan Harris. 2011. We
feel fine and searching the emotional web. In Pro-
ceedings of the fourth ACM international confer-
ence on Web search and data mining, WSDM ?11,
pages 117?126, New York, NY, USA. ACM.
Eugenio Mart??nez-Ca?mara, M. Teresa Mart??n-
Valdivia, Jose? M. Perea-Ortega, and L. Al-
fonso Ure na Lo?pez. 2011. Opinion classification
techniques applied to a spanish corpus. Proce-
samiento de Lenguaje Natural, 47.
Eugenio Mart??nez-Ca?mara, M. Teresa Mart??n-
Valdivia, L. Alfonso Ure na Lo?pez, and Ruslan
Mitkov. 2013a. Detecting sentiment polarity in
spanish tweets. Information Systems Management,
In Press.
Eugenio Mart??nez-Ca?mara, M. Teresa Mart??n-
Valdivia, L. Alfonso Ure na Lo?pez, and Arturo
Montejo-Ra?ez. 2013b. Sentiment analysis
in twitter. Natural Language Engineering,
FirstView:1?28, 2.
Arturo Montejo-Ra?ez. 2013. Wefeelfine as resource
for unsupervised polarity classification. Proce-
samiento del Lenguaje Natural, 50:29?35.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification us-
ing machine learning techniques. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 79?86, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Julio Villena-Roma?n, Sara Lana-Serrano, Euge-
nio Mart??nez-Ca?mara, and Jose? Carlos Gonza?lez-
Cristo?bal. 2013. Tass - workshop on sentiment
analysis at sepln. Procesamiento del Lenguaje
Natural, 50(0).
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. SemEval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval ?13, June.
Changli Zhang, Daniel Zeng, Jiexun Li, Fei-Yue
Wang, and Wanli Zuo. 2009. Sentiment analy-
sis of chinese documents: From sentence to docu-
ment level. Journal of the American Society for In-
formation Science and Technology, 60(12):2474?
2487.
Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil,
Meichun Hsu, and Bing Liu. 2011. Combining
lexiconbased and learning-based methods for twit-
ter sentiment analysis. HP Laboratories, Technical
Report HPL-2011-89.
407
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 3?10,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Random Walk Weighting over SentiWordNet for
Sentiment Polarity Detection on Twitter
A. Montejo-Ra?ez, E. Mart??nez-Ca?mara, M. T. Mart??n-Valdivia, L. A. Uren?a-Lo?pez
University of Jae?n
E-23071, Jae?n (Spain)
{amontejo, emcamara, maite, laurena}@ujaen.es
Abstract
This paper presents a novel approach in Sen-
timent Polarity Detection on Twitter posts, by
extracting a vector of weighted nodes from the
graph of WordNet. These weights are used
on SentiWordNet to compute a final estima-
tion of the polarity. Therefore, the method
proposes a non-supervised solution that is
domain-independent. The evaluation over a
generated corpus of tweets shows that this
technique is promising.
1 Introduction
The birth of Web 2.0 supposed a breaking down of
the barrier between the consumers and producers of
information, i.e. the Web has changed from a static
container of information into a live environment in
which any user, in a very simple manner, can pub-
lish any type of information. This simplified means
of publication has led to the rise of several differ-
ent websites specialized in the publication of users
opinions. Some of the most well-known sites in-
clude Epinions1, RottenTomatoes2 and Muchocine3,
where users express their opinions or criticisms on a
wide range of topics. Opinions published on the In-
ternet are not limited to certain sites, but rather can
be found in a blog, forum, commercial website or
any other site allowing posts from visitors.
On of the most representative tools of the Web 2.0
are social networks, which allow millions of users
1http://epinions.com
2http://rottentomatoes.com
3http://muchocine.net
to publish any information in a simple way and to
share it with their network of contacts or ?friends?.
These social networks have also evolved and be-
come a continuous flow of information. A clear ex-
ample is the microblogging platform Twitter4. Twit-
ter publishes all kinds of information, disseminating
views on many different topics: politics, business,
economics and so on. Twitter users regularly pub-
lish their comments on a particular news item, a re-
cently purchased product or service, and ultimately
on everything that happens around them. This has
aroused the interest of the Natural Language Pro-
cessing (NLP) community, which has begun to study
the texts posted on Twitter, and more specifically re-
lated to Sentiment Analysis (SA) challenges.
In this manuscript we present a new approach to
resolve the scoring of posts according to the ex-
pressed positive or negative degree in the text. This
polarity detection problem is resolved by combin-
ing SentiWordNet scores with a random walk analy-
sis of the concepts found in the text over the Word-
Net graph. In order to validate our non-supervised
approach, several experiments have been performed
to analyze major issues in our method and to com-
pare it with other approaches like plain SentiWord-
Net scoring or machine learning solutions such as
Support Vector Machines in a supervised approach.
The paper is structured as follows: first, an introduc-
tion to the polarity detection problem is provided,
followed by the description of our approach. Then,
the experimental setup is given with a description of
the generated corpus and the results obtained. Fi-
nally, conclusions and further work are discussed.
4http://twitter.com
3
2 The polarity detection problem
In the literature related to the SA in long text a dis-
tinction is made between studies of texts where we
assume that the text is a opinion and therefore solely
need to calculate its polarity, and those in which be-
fore measuring polarity it is necessary to determine
whether the text is subjective or objective. A wide
study on SA can be found in (Pang and Lee, 2008),
(Liu, 2010) and (Tsytsarau and Palpanas, 2011).
Concerning the study of the polarity in Twitter, most
experiments assume that tweets5 are subjective. One
of the first studies on the classification of the polar-
ity in tweets was published in 2009 by (Go et al,
2009), in which the authors conducted a supervised
classification study of tweets in English.
Zhang et al (Zhang et al, 2011) proposed a hy-
brid method for the classification of the polarity in
Twitter, and they demonstrated the validity of their
method over an English corpus on Twitter. The clas-
sification is divided into two phases. The first one
consists on applying a lexicon-based method. In
the second one the authors used the SVM algorithm
to determine the polarity. For the machine learning
phase, it is needed a labelled corpus, so the purpose
of the lexicon-method is to tag the corpus. Thus, the
authors selected a set of subjective words from all
those available in English and added hash-tags with
a subjective meaning. After labelling the corpus, it
is used SVM for classifying new tweets.
In (Agarwal et al, 2011) a study was conducted
on a reduced corpus of tweets labelled manually.
The experiment tests different methods of polarity
classification and starts with a base case consisting
on the simple use of unigrams. Then a tree-based
model is generated. In a third step, several linguis-
tic features are extracted and finally a final model
learned as combination of the different models pro-
posed is computed. A common feature used both in
the tree-based model and in the feature-based one is
the polarity of the words appearing in each tweet. In
order to calculate this polarity the authors used DAL
dictionary (Whissell, 1989).
Most of the proposed systems for polarity detec-
tion compute a value of negativeness or positiveness.
Some of them even produce a neutrality value. We
will consider the following measurement of polar-
5The name of posts in Twitter.
ity (which is very common, indeed): a real value
in the interval [-1, 1] would be sufficient. Values
over zero would reflect a positive emotion expressed
in the tweet, while values below zero would rather
correspond to negative opinions. The closer to the
zero value a post is, the more its neutrality would
be. Therefore, a polarity detection system could be
represented as a function p on a text t such as:
p : RN ? R
so that p(t) ? [?1, 1]. We will define how to
compute this function, but before an explanation of
the techniques implied in such a computation is pro-
vided.
3 The approach: Random Walk and
SentiWordNet
3.1 The Random Walk algorithm
Personalized Page Rank vectors (PPVs) consists on
a ranked sequence of WordNet (Fellbaum, 1998)
synsets weighted according to a random walk algo-
rithm. Taking the graph of WordNet, where nodes
are synsets and axes are the different semantic re-
lations among them, and the terms contained in a
tweet, we can select those synsets that correspond to
the closest sense for each term and. Then, it starts
an iterative process so more nodes are selected if
they are not far from these ?seeds?. After a num-
ber of iterations or a convergence of the weights, a
final list of valued nodes can be retrieved. A simi-
lar approach has been used recently by (Ramage et
al., 2009) to compute text semantic similarity in rec-
ognizing textual entailment, and also as a solution
for word sense disambiguation (Agirre and Soroa,
2009). We have used the UKB software from this
last citation to generate the PPVs used in our system.
Random walk algorithms are inspired originally by
the Google PageRank algorithm (Page et al, 1999).
The idea behind it is to represent each tweet as a vec-
tor weighted synsets that are semantically close to
the terms included in the post. In some way, we are
expanding these sort texts by a set of disambiguated
concepts related to the terms included in the text.
As an example of a PPV,the text ?Overall, we?re
still having a hard time with it, mainly because we?re
not finding it in an early phase.? becomes the vector
of weighted synsets:
4
[02190088-a:0.0016, 12613907-n:0.0004,
01680996-a:0.0002, 00745831-a:0.0002, ...]
Here, the synset 02190088-a has a weight of
0.0016, for example.
3.2 SentiWordNet
SentiWordNet (Baccianella et al, 2008) is a lexi-
cal resource based on the well know WordNet (Fell-
baum, 1998). It provides additional information on
synsets related to sentiment orientation. A synset
is the basic item of information in WordNet and it
represents a ?concept? that is unambiguous. Most
of the relations over the lexical graph use synsets
as nodes (hyperonymy, synonymy, homonymy and
more). SentiWordNet returns from every synset a
set of three scores representing the notions of ?pos-
itivity?, ?negativity? and ?neutrality?. Therefore,
every concept in the graph is weighting accord-
ing to its subjectivity and polarity. The last ver-
sion of SentiWordNet (3.0) has been constructed
starting from manual annotations of previous ver-
sions, populating the whole graph by applying a ran-
dom walk algorithm. This resource has been used
by the opinion mining community, as it provides a
domain-independent resource to get certain informa-
tion about the degree of emotional charge of its con-
cepts (Denecke, 2008; Ogawa et al, 2011).
3.3 Computing the final estimation
As a combination of SentiWordNet scores with ran-
dom walk weights is wanted, it is important that
the final equation leads to comparable values. To
this end, the weights associated to synsets after the
random walk process are L1 normalized so vectors
of ?concepts? sum up the unit as maximum value.
The final polarity score is obtained by the product of
this vector with associated SentiWordNet vector of
scores, as expressed in equation 1.
p = r ? s|t| (1)
where p is the final score, r is the vector of
weighted synsets computed by the random walk al-
gorithm of the tweet text over WordNet, s is the vec-
tor of polarity scores from SentiWordNet, t is the
set of concepts derived from the tweet. The idea be-
hind it is to ?expand? the set of concepts with addi-
tional ones that are close in the WordNet graph, cor-
responding to those synset nodes which have been
activated during the random walk process. There-
fore, terms like dog and bite (both mainly neutral
in SentiWordNet) appearing in the same tweet could
eventually be expanded with a more emotional term
like hurt, which holds, in SentiWordNet, a negative
score of 0.75.
4 Experiments and results
Our experiments are focused in testing the validity
of applying this unsupervised approach compared to
a classical supervised one based on Support Vector
Machines (Joachims, 1998). To this end, the corpus
has been processed obtaining lemmas, as this is the
preferred input for the UKB software. The algorithm
takes the whole WordNet graph and performs a dis-
ambiguation process of the terms as a natural con-
sequence of applying random walk over the graph.
In this way, the synsets that are associated to these
terms are all of them initialized. Then, the iterative
process of the algorithm (similar to Page Rank but
optimized according to an stochastic solution) will
change these initial values and propagate weights to
closer synsets. An interesting effect of this process is
that we can actually obtain more concepts that those
contained in the tweet, as all the related ones will
also finalize with a certain value due to the propaga-
tion of weights across the graph. We believe that our
approach benefits from this effect, as texts in tweets
use to suffer from a very sort length, allowing us to
expand short posts.
Another concern is, therefore, the final size of the
PPV vector. If too many concepts are taken into ac-
count we may introduce noise in the understanding
of the latent semantic of the text. In order to study
this fact, different sizes of the vector have been ex-
plored and evaluated.
4.1 Our Twitter corpus
The analysis of the polarity on microblogging is a
very recent task, so there are few free resources
(Sas?a et al, 2010). Thus, we have collected our
own English corpus in order to accomplish the ex-
periments. The work of downloading tweets is not
nearly difficult due to the fact that Twitter offers two
kinds of API to those purposes. We have used the
5
Search API of Twitter6 for automatically accessing
tweets through a query. For a supervised polarity
study and to evaluate our approach, we need to gen-
erate a labelled corpus. We have built a corpus of
tweets written in English following the procedure
described in (Read, 2005) and (Go et al, 2009).
According to (Read, 2005), when authors of an
electronic communication use an emotion, they are
effectively marking up their own text with an emo-
tional state. The main feature of Twitter is that the
length of the messages must be 140 characters, so
the users have to express their opinions, thoughts,
and emotional states with few words. Therefore,
frequently users write ?smileys? in their tweets.
Thus, we have used positive emoticons to label pos-
itive tweets and negative emoticons to tag negative
tweets. The full list of emoticons that we have con-
sidered to label the retrieved tweets can be found in
Table 1. So, following (Go et al, 2009), the pre-
sumption in the construction of the corpus is that the
query ?:)? returns tweets with positive smileys, and
the query ?:(? retrieves negative emotions. We have
collected a set of 376,296 tweets (181,492 labelled
as positive tweets and 194,804 labelled as negative
tweets), which were published on Twitter?s public
message board from September 14th 2010 to March
19th 2011. Table 2 lists other characteristics of the
corpus.
On the other hand, the language used in Twit-
ter has some unique attributes, which have been re-
moved because they do not provide relevant infor-
mation for the polarity detection process. These spe-
cific features are:
1. Retweets: A retweet is the way to repeat a mes-
sage that users consider interesting. Retweets
can be done through the web interface using
the Retweet option, or as the old way writing
RT, the user name and the post to retweet. The
first way is not a problem because is the same
tweet, so the API only return it once, but old
way retweets are different tweets but with the
same content, so we removed them to avoid pit-
ting extra weight on any particular tweet.
2. Mentions: Other feature of Twitter is the so
called Mentions. When a user wants to refer
6https://dev.twitter.com/docs/api/1/get/search
Emoticons mapped to :)
(positive tweets)
:) : ) :-)
;) ;-) =)
? ? :-D :D
:d =D C:
Xd XD xD
Xd (x (=
?? ?o? ?u?
n n *-* *O*
*o* * *
Emoticons mapped to :(
(negative tweets)
:-( :( :((
: ( D: Dx
?n? :\ /:
):-/ :? =?[
: ( /T T TOT
; ;
Table 1: Emoticons considered as positives and negatives
to another one, he or she introduces a Mention.
A Mention is easily recognizable because all of
them start with the symbol ?@? followed by the
user name. We consider that this feature does
not provide any relevance information, so we
have removed the mentions in all the tweets.
3. Links: It is very common that tweets include
web directions. In our approach we do not ana-
lyze the documents that links those urls, so we
have eliminated them from all tweets.
4. Hash-tags: A hash-tag is the name of a topic
in Twitter. Anybody can begin a new topic by
typing the name of the topic preceded by the
symbol ?#?. For this work we do not classify
topics so we have neglected all the hash-tags.
Due to the fact that users usually write tweets
with a very casual language, it is necessary to pre-
process the raw tweets before feeding the sentiment
analyzer. For that purpose we have applied the fol-
lowing filters:
1. Remove new lines: Some users write tweets
in two or three different lines, so all newlines
symbols were removed.
2. Opposite emoticons: Twitter sometimes con-
siders positive or negative a tweet with smileys
6
Total
Positive tweets 181,492
Negative tweets 194,804 376,296
Unique users in positive
tweets
157,579
Unique users in negative
tweets
167,479 325,058
Words in positive tweets 418,234
Words in negative tweets 334,687 752,921
Average number of
words per positive tweet
9
Average number of
words per negative tweet
10
Table 2: Statistical description of the corpus.
that have opposite senses. For example:
@Harry Styles I have all day to try
get a tweet off you :) when are
you coming back to dublin i missed
you last time,I was in spain :(
The tweet has two parts one positive and the
other one negative, so the post cannot be con-
sidered as positive, but the search API returns
as a positive tweet because it has the positive
smiley ?:)?. We have removed this kind of
tweets in order to avoid ambiguity.
3. Emoticons with no clear sentiment: The
Twitter Search API considers some emoticons
like ?:P? or ?:PP? as negative. However, some
users do not type them to express a negative
sentiment. Thus, we have got rid of all tweets
with this kind of smileys (see Table 3).
Fuzzy emoticons :-P :P :PP \(
Table 3: Emoticons considered as fuzzy sentiments
4. Repeated letters: Users frequently repeat sev-
eral times letters of some words to emphasize
their messages. For example:
Blood drive todayyyy!!!!! :)
Everyone donateeeee!!
This can be a problem for the classification pro-
cess, because the same word with different rep-
etitions of the same letter would be considered
as a different word. Thus, we have normalized
all the repeated letters, and any letter occurring
more than two times in a word is replaced with
two occurrences. The example above would be
converted into:
blood drive todayy :) everyone
donatee!!
5. Laugh: There is not a unique manner to ex-
press laugh. Therefore, we have normalized
the way to write laugh. Table 4 lists the con-
versions.
Laugh Conversion
hahahaha... haha
hehehehe... hehe
hihihihi... hihi
hohohoho... hoho
huhuhuhu... huhu
Lol haha
Huashuashuas huas
muahahaha Buaha
buahahaha Buaha
Table 4: Normalization for expressions considered as
?Laugh?
Finally, although the emoticons have been used
to tag the positive and negative samples, the fi-
nal corpora does not include these emoticons.
In addition, all the punctuation characters have
been neglected in order to reduce the noise in
the data. Figure 1 shows the process to gener-
ate our Twitter corpus.
4.2 Results obtained
Our first experiment consisted on evaluating a super-
vised approach, like Support Vector Machines, us-
ing the well know vector space model to build the
vector of features. Each feature corresponds to the
TF.IDF weight of a lemma. Stop words have not
been removed and the minimal document frequency
required was two, that is, if the lemma is not present
in two o more tweets, then it is discarded as a di-
mension in the vectors. The SVM-Light7 software
was used to compute support vectors and to evaluate
them using a random leave-one-out strategy. From
7http://svmlight.joachims.org/
7
Figure 1: Corpus generation work-flow
a total of 376,284 valid samples 85,423 leave-one-
out evaluations were computed. This reported the
following measurements:
Precision Recall F1
0.6429 0.6147 0.6285
In our first implementation of our method, the fi-
nal polarity score is computed as described in equa-
tion 1. More precisely, it is the average of the prod-
uct between the difference of positive and negative
SentiWordNet scores, and the weight obtained with
the random walk algorithm, as unveiled in equa-
tion 2.
p =
?
?s?t rws ? (swn+s ? swn?s )
|t| (2)
Where s is a synset in the tweet t, rws is the
weight of the synset s after the random walk pro-
cess over WordNet, swn+s and swn?s ) are positive
and negative scores for the synset s retrieved from
SentiWordNet.
The results obtained are graphically shown in fig-
ures 2, 3 and 4 for precision, recall and F1 values
respectively. As can be noticed from the shapes
of the graphs, the size of the PPV vectors affects
the performance. Sizes above 10 presents an sta-
ble behavior, that is, considering a large number of
synsets does not improves the performance of the
system, but it gets worse neither. The WordNet
graph considered for the random walk algorithm in-
cludes antonyms relations, so we wanted to check
whether discarding these connections would affect
the system. From these graphs we can extract the
conclusion that antonyms relations are worth keep-
ing.
Figure 2: Precision values against PPV sizes
Figure 3: Recall values against PPV sizes
Comparing our best configuration to the SVM ap-
proach, the results are not better, but quite close (ta-
ble 5). Therefore, this unsupervised solution is an
interesting alternative to the supervised one.
8
Figure 4: F1 values against PPV sizes
Precision Recall F1
SVM 0.6429 0.6147 0.6285
RW?SWN 0.6259 0.6207 0.6233
Table 5: Approaches comparative table
5 Conclusions and further work
A new unsupervised approach to the polarity detec-
tion problem in Twitter posts has been proposed. By
combining a random walk algorithm that weights
synsets from the text with polarity scores provided
by SentiWordNet, it is possible to build a system
comparable to a SVM based supervised approach in
terms of performance. Our solution is a general ap-
proach that do not suffer from the disadvantages as-
sociated to supervised ones: need of a training cor-
pus and dependence on the domain where the model
was obtained.
Many issues remain open and they will drive our
future work. How to deal with negation is a ma-
jor concern, as the score from SentiWordNet should
be considered in a different way in the final com-
putation if the original term comes from a negated
phrase. Our ?golden rules? must be taken carefully,
because emoticons are a rough way to classify the
polarity of tweets. Actually, we are working in the
generation of a new corpus in the politics domain
that is now under a manual labeling process. An-
other step is to face certain flaws in the computation
of the final score. In this sense, we plan to study
the context of a tweet among the time line of tweets
from that user to identify publisher?s mood and ad-
just final scores. As an additional task, the process-
ing of original texts is important. The numerous
grammatical and spelling errors found in this fast
way of publication demand for a better sanitization
of the incoming data. An automatic spell checker is
under development.
As final conclusion, we believe that this first at-
tempt is very promising and that it has arose many
relevant questions on the subject of sentiment analy-
sis. More extensive research and experimentation is
being undertaken from the starting point introduced
in this paper.
Acknowledgments
This work has been partially supported by a grant
from the Fondo Europeo de Desarrollo Regional
(FEDER), TEXT-COOL 2.0 project (TIN2009-
13391-C04-02) from the Spanish Government. This
paper is partially funded by the European Commis-
sion under the Seventh (FP7 - 2007-2013) Frame-
work Programme for Research and Technologi-
cal Development through the FIRST project (FP7-
287607). This publication reflects the views only
of the author, and the Commission cannot be held
responsible for any use which may be made of the
information contained therein.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Language in Social Media (LSM 2011), pages 30?38,
Portland, Oregon, jun. Association for Computational
Linguistics.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In EACL
?09: Proceedings of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 33?41, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2008. Sentiwordnet 3.0 : An enhanced lexical
resource for sentiment analysis and opinion mining.
Proceedings of the Seventh conference on Interna-
tional Language Resources and Evaluation LREC10,
0:2200?2204.
K. Denecke. 2008. Using sentiwordnet for multilingual
sentiment analysis. In Data Engineering Workshop,
9
2008. ICDEW 2008. IEEE 24th International Confer-
ence on, pages 507 ?512, april.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
T. Joachims. 1998. Text categorization with support vec-
tor machines: learning with many relevant features. In
European Conference on Machine Learning (ECML).
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, 2nd ed.
Tatsuya Ogawa, Qiang Ma, and Masatoshi Yoshikawa.
2011. News Bias Analysis Based on Stakeholder Min-
ing. IEICE TRANSACTIONS ON INFORMATION
AND SYSTEMS, E94D(3):578?586, MAR.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford
University.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?
135.
Daniel Ramage, Anna N. Rafferty, and Christopher D.
Manning. 2009. Random walks for text seman-
tic similarity. In TextGraphs-4: Proceedings of the
2009 Workshop on Graph-based Methods for Natural
Language Processing, pages 23?31, Morristown, NJ,
USA. Association for Computational Linguistics.
Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL Stu-
dent Research Workshop, ACLstudent ?05, pages 43?
48, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Petrovic? Sas?a, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceed-
ings of the NAACL HLT 2010 Workshop on Compu-
tational Linguistics in a World of Social Media, WSA
?10, pages 25?26, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mikalai Tsytsarau and Themis Palpanas. 2011. Survey
on mining subjective data on the web. Data Mining
and Knowledge Discovery, pages 1?37, October.
C M Whissell, 1989. The dictionary of affect in lan-
guage, volume 4, pages 113?131. Academic Press.
Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Me-
ichun Hsu, and Bing Liu. 2011. Combining lexicon-
based and learning-based methods for twitter senti-
ment analysis. Technical Report HPL-2011-89, HP,
21/06/2011.
10
