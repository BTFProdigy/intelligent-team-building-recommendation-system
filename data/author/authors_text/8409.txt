An Integrated Architecture for Shallow and Deep Processing
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,
Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker and Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg 3
Saarbru?cken, Germany
whiteboard@dfki.de
Abstract
We present an architecture for the integra-
tion of shallow and deep NLP components
which is aimed at flexible combination
of different language technologies for a
range of practical current and future appli-
cations. In particular, we describe the inte-
gration of a high-level HPSG parsing sys-
tem with different high-performance shal-
low components, ranging from named en-
tity recognition to chunk parsing and shal-
low clause recognition. The NLP com-
ponents enrich a representation of natu-
ral language text with layers of new XML
meta-information using a single shared
data structure, called the text chart. We de-
scribe details of the integration methods,
and show how information extraction and
language checking applications for real-
world German text benefit from a deep
grammatical analysis.
1 Introduction
Over the last ten years or so, the trend in application-
oriented natural language processing (e.g., in the
area of term, information, and answer extraction)
has been to argue that for many purposes, shallow
natural language processing (SNLP) of texts can
provide sufficient information for highly accurate
and useful tasks to be carried out. Since the emer-
gence of shallow techniques and the proof of their
utility, the focus has been to exploit these technolo-
gies to the maximum, often ignoring certain com-
plex issues, e.g. those which are typically well han-
dled by deep NLP systems. Up to now, deep natural
language processing (DNLP) has not played a sig-
nificant role in the area of industrial NLP applica-
tions, since this technology often suffers from insuf-
ficient robustness and throughput, when confronted
with large quantities of unrestricted text.
Current information extractions (IE) systems
therefore do not attempt an exhaustive DNLP analy-
sis of all aspects of a text, but rather try to analyse or
?understand? only those text passages that contain
relevant information, thereby warranting speed and
robustness wrt. unrestricted NL text. What exactly
counts as relevant is explicitly defined by means
of highly detailed domain-specific lexical entries
and/or rules, which perform the required mappings
from NL utterances to corresponding domain knowl-
edge. However, this ?fine-tuning? wrt. a particular
application appears to be the major obstacle when
adapting a given shallow IE system to another do-
main or when dealing with the extraction of com-
plex ?scenario-based? relational structures. In fact,
(Appelt and Israel, 1997) have shown that the cur-
rent IE technology seems to have an upper perfor-
mance level of less than 60% in such cases. It seems
reasonable to assume that if a more accurate analy-
sis of structural linguistic relationships could be pro-
vided (e.g., grammatical functions, referential rela-
tionships), this barrier might be overcome. Actually,
the growing market needs in the wide area of intel-
ligent information management systems seem to re-
quest such a break-through.
In this paper we will argue that the quality of cur-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448.
                         Proceedings of the 40th Annual Meeting of the Association for
rent SNLP-based applications can be improved by
integrating DNLP on demand in a focussed manner,
and we will present a system that combines the fine-
grained anaysis provided by HPSG parsing with a
high-performance SNLP system into a generic and
flexible NLP architecture.
1.1 Integration Scenarios
Owing to the fact that deep and shallow technologies
are complementary in nature, integration is a non-
trivial task: while SNLP shows its strength in the
areas of efficiency and robustness, these aspects are
problematic for DNLP systems. On the other hand,
DNLP can deliver highly precise and fine-grained
linguistic analyses. The challenge for integration is
to combine these two paradigms according to their
virtues.
Probably the most straightforward way to inte-
grate the two is an architecture in which shallow and
deep components run in parallel, using the results of
DNLP, whenever available. While this kind of ap-
proach is certainly feasible for a real-time applica-
tion such as Verbmobil, it is not ideal for processing
large quantities of text: due to the difference in pro-
cessing speed, shallow and deep NLP soon run out
of sync. To compensate, one can imagine two possi-
ble remedies: either to optimize for precision, or for
speed. The drawback of the former strategy is that
the overall speed will equal the speed of the slow-
est component, whereas in case of the latter, DNLP
will almost always time out, such that overall preci-
sion will hardly be distinguishable from a shallow-
only system. What is thus called for is an integrated,
flexible architecture where components can play at
their strengths. Partial analyses from SNLP can be
used to identify relevant candidates for the focussed
use of DNLP, based on task or domain-specific crite-
ria. Furthermore, such an integrated approach opens
up the possibility to address the issue of robustness
by using shallow analyses (e.g., term recognition)
to increase the coverage of the deep parser, thereby
avoiding a duplication of efforts. Likewise, integra-
tion at the phrasal level can be used to guide the
deep parser towards the most likely syntactic anal-
ysis, leading, as it is hoped, to a considerable speed-
up.
shallow
NLP
components
NLP
deep
components internal repr.
layer
multi
chart
annot.
XML
external repr.
generic OOP
component
interface
WHAM
application
specification
input and
result
Figure 1: The WHITEBOARD architecture.
2 Architecture
The WHITEBOARD architecture defines a platform
that integrates the different NLP components by en-
riching an input document through XML annota-
tions. XML is used as a uniform way of represent-
ing and keeping all results of the various processing
components and to support a transparent software
infrastructure for LT-based applications. It is known
that interesting linguistic information ?especially
when considering DNLP? cannot efficiently be
represented within the basic XML markup frame-
work (?typed parentheses structure?), e.g., linguistic
phenomena like coreferences, ambiguous readings,
and discontinuous constituents. The WHITEBOARD
architecture employs a distributed multi-level repre-
sentation of different annotations. Instead of trans-
lating all complex structures into one XML docu-
ment, they are stored in different annotation layers
(possibly non-XML, e.g. feature structures). Hyper-
links and ?span? information together support effi-
cient access between layers. Linguistic information
of common interest (e.g. constituent structure ex-
tracted from HPSG feature structures) is available in
XML format with hyperlinks to full feature struc-
ture representations externally stored in correspond-
ing data files.
Fig. 1 gives an overview of the architecture of
the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and
a specification describing the components and con-
figuration options requested. The core WHAM en-
gine has an XML markup storage (external ?offline?
representation), and an internal ?online? multi-level
annotation chart (index-sequential access). Follow-
ing the trichotomy of NLP data representation mod-
els in (Cunningham et al, 1997), the XML markup
contains additive information, while the multi-level
chart contains positional and abstraction-based in-
formation, e.g., feature structures representing NLP
entities in a uniform, linguistically motivated form.
Applications and the integrated components ac-
cess the WHAM results through an object-oriented
programming (OOP) interface which is designed
as general as possible in order to abstract from
component-specific details (but preserving shallow
and deep paradigms). The interfaces of the actu-
ally integrated components form subclasses of the
generic interface. New components can be inte-
grated by implementing this interface and specifying
DTDs and/or transformation rules for the chart.
The OOP interface consists of iterators that walk
through the different annotation levels (e.g., token
spans, sentences), reference and seek operators that
allow to switch to corresponding annotations on a
different level (e.g., give all tokens of the current
sentence, or move to next named entity starting
from a given token position), and accessor meth-
ods that return the linguistic information contained
in the chart. Similarily, general methods support
navigating the type system and feature structures of
the DNLP components. The resulting output of the
WHAM can be accessed via the OOP interface or as
XML markup.
The WHAM interface operations are not only
used to implement NLP component-based applica-
tions, but also for the integration of deep and shallow
processing components itself.
2.1 Components
2.1.1 Shallow NL component
Shallow analysis is performed by SPPC, a rule-
based system which consists of a cascade of
weighted finite?state components responsible for
performing subsequent steps of the linguistic anal-
ysis, including: fine-grained tokenization, lexico-
morphological analysis, part-of-speech filtering,
named entity (NE) recognition, sentence bound-
ary detection, chunk and subclause recognition,
see (Piskorski and Neumann, 2000; Neumann and
Piskorski, 2002) for details. SPPC is capable of pro-
cessing vast amounts of textual data robustly and ef-
ficiently (ca. 30,000 words per second in standard
PC environment). We will briefly describe the SPPC
components which are currently integrated with the
deep components.
Each token identified by a tokenizer as a poten-
tial word form is morphologically analyzed. For
each token, its lexical information (list of valid read-
ings including stem, part-of-speech and inflection
information) is computed using a fullform lexicon
of about 700,000 entries that has been compiled out
from a stem lexicon of about 120,000 lemmas. Af-
ter morphological processing, POS disambiguation
rules are applied which compute a preferred read-
ing for each token, while the deep components can
back off to all readings. NE recognition is based on
simple pattern matching techniques. Proper names
(organizations, persons, locations), temporal expres-
sions and quantities can be recognized with an av-
erage precision of almost 96% and recall of 85%.
Furthermore, a NE?specific reference resolution is
performed through the use of a dynamic lexicon
which stores abbreviated variants of previously rec-
ognized named entities. Finally, the system splits
the text into sentences by applying only few, but
highly accurate contextual rules for filtering implau-
sible punctuation signs. These rules benefit directly
from NE recognition which already performs re-
stricted punctuation disambiguation.
2.1.2 Deep NL component
The HPSG Grammar is based on a large?scale
grammar for German (Mu?ller, 1999), which was
further developed in the VERBMOBIL project for
translation of spoken language (Mu?ller and Kasper,
2000). After VERBMOBIL the grammar was adapted
to the requirements of the LKB/PET system (Copes-
take, 1999), and to written text, i.e., extended with
constructions like free relative clauses that were ir-
relevant in the VERBMOBIL scenario.
The grammar consists of a rich hierarchy of
5,069 lexical and phrasal types. The core grammar
contains 23 rule schemata, 7 special verb move-
ment rules, and 17 domain specific rules. All rule
schemata are unary or binary branching. The lexicon
contains 38,549 stem entries, from which more than
70% were semi-automatically acquired from the an-
notated NEGRA corpus (Brants et al, 1999).
The grammar parses full sentences, but also other
kinds of maximal projections. In cases where no full
analysis of the input can be provided, analyses of
fragments are handed over to subsequent modules.
Such fragments consist of maximal projections or
single words.
The HPSG analysis system currently integrated
in the WHITEBOARD system is PET (Callmeier,
2000). Initially, PET was built to experiment
with different techniques and strategies to process
unification-based grammars. The resulting sys-
tem provides efficient implementations of the best
known techniques for unification and parsing.
As an experimental system, the original design
lacked open interfaces for flexible integration with
external components. For instance, in the beginning
of the WHITEBOARD project the system only ac-
cepted fullform lexica and string input. In collabora-
tion with Ulrich Callmeier the system was extended.
Instead of single word input, input items can now
be complex, overlapping and ambiguous, i.e. essen-
tially word graphs. We added dynamic creation of
atomic type symbols, e.g., to be able to add arbitrary
symbols to feature structures. With these enhance-
ments, it is possible to build flexible interfaces to
external components like morphology, tokenization,
named entity recognition, etc.
3 Integration
Morphology and POS The coupling between the
morphology delivered by SPPC and the input needed
for the German HPSG was easily established. The
morphological classes of German are mapped onto
HPSG types which expand to small feature struc-
tures representing the morphological information in
a compact way. A mapping to the output of SPPC
was automatically created by identifying the corre-
sponding output classes.
Currently, POS tagging is used in two ways. First,
lexicon entries that are marked as preferred by the
shallow component are assigned higher priority than
the rest. Thus, the probability of finding the cor-
rect reading early should increase without excluding
any reading. Second, if for an input item no entry is
found in the HPSG lexicon, we automatically create
a default entry, based on the part?of?speech of the
preferred reading. This increases robustness, while
avoiding increase in ambiguity.
Named Entity Recognition Writing HPSG gram-
mars for the whole range of NE expressions etc. is
a tedious and not very promising task. They typi-
cally vary across text sorts and domains, and would
require modularized subgrammars that can be easily
exchanged without interfering with the general core.
This can only be realized by using a type interface
where a class of named entities is encoded by a gen-
eral HPSG type which expands to a feature structure
used in parsing. We exploit such a type interface for
coupling shallow and deep processing. The classes
of named entities delivered by shallow processing
are mapped to HPSG types. However, some fine-
tuning is required whenever deep and shallow pro-
cessing differ in the amount of input material they
assign to a named entity.
An alternative strategy is used for complex syn-
tactic phrases containing NEs, e.g., PPs describ-
ing time spans etc. It is based on ideas from
Explanation?based Learning (EBL, see (Tadepalli
and Natarajan, 1996)) for natural language analy-
sis, where analysis trees are retrieved on the basis
of the surface string. In our case, the part-of-speech
sequence of NEs recognised by shallow analysis is
used to retrieve pre-built feature structures. These
structures are produced by extracting NEs from a
corpus and processing them directly by the deep
component. If a correct analysis is delivered, the
lexical parts of the analysis, which are specific for
the input item, are deleted. We obtain a sceletal
analysis which is underspecified with respect to the
concrete input items. The part-of-speech sequence
of the original input forms the access key for this
structure. In the application phase, the underspeci-
fied feature structure is retrieved and the empty slots
for the input items are filled on the basis of the con-
crete input.
The advantage of this approach lies in the more
elaborate semantics of the resulting feature struc-
tures for DNLP, while avoiding the necessity of
adding each and every single name to the HPSG lex-
icon. Instead, good coverage and high precision can
be achieved using prototypical entries.
Lexical Semantics When first applying the origi-
nal VERBMOBIL HPSG grammar to business news
articles, the result was that 78.49% of the miss-
ing lexical items were nouns (ignoring NEs). In
the integrated system, unknown nouns and NEs can
be recognized by SPPC, which determines morpho-
syntactic information. It is essential for the deep sys-
tem to associate nouns with their semantic sorts both
for semantics construction, and for providing se-
mantically based selectional restrictions to help con-
straining the search space during deep parsing. Ger-
maNet (Hamp and Feldweg, 1997) is a large lexical
database, where words are associated with POS in-
formation and semantic sorts, which are organized in
a fine-grained hierarchy. The HPSG lexicon, on the
other hand, is comparatively small and has a more
coarse-grained semantic classification.
To provide the missing sort information when re-
covering unknown noun entries via SPPC, a map-
ping from the GermaNet semantic classification to
the HPSG semantic classification (Siegel et al,
2001) is applied which has been automatically ac-
quired. The training material for this learning pro-
cess are those words that are both annotated with se-
mantic sorts in the HPSG lexicon and with synsets
of GermaNet. The learning algorithm computes a
mapping relevance measure for associating seman-
tic concepts in GermaNet with semantic sorts in the
HPSG lexicon. For evaluation, we examined a cor-
pus of 4664 nouns extracted from business news
that were not contained in the HPSG lexicon. 2312
of these were known in GermaNet, where they are
assigned 2811 senses. With the learned mapping,
the GermaNet senses were automatically mapped to
HPSG semantic sorts. The evaluation of the map-
ping accuracy yields promising results: In 76.52%
of the cases the computed sort with the highest rel-
evance probability was correct. In the remaining
20.70% of the cases, the correct sort was among the
first three sorts.
3.1 Integration on Phrasal Level
In the previous paragraphs we described strategies
for integration of shallow and deep processing where
the focus is on improving DNLP in the domain of
lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for
the integration of shallow and deep analysis at the
length cover- complete LP LR 0CB   2CB
age match
  40 100 80.4 93.4 92.9 92.1 98.9
 40 99.8 78.6 92.4 92.2 90.7 98.5
Training: 16,000 NEGRA sentences
Testing: 1,058 NEGRA sentences
Figure 2: Stochastic topological parsing: results
level of phrasal syntax by guiding the deep syntac-
tic parser towards a partial pre-partitioning of com-
plex sentences provided by shallow analysis sys-
tems. This strategy can reduce the search space, and
enhance parsing efficiency of DNLP.
Stochastic Topological Parsing The traditional
syntactic model of topological fields divides basic
clauses into distinct fields: so-called pre-, middle-
and post-fields, delimited by verbal or senten-
tial markers. This topological model of German
clause structure is underspecified or partial as to
non-sentential constituent boundaries, but provides
a linguistically well-motivated, and theory-neutral
macrostructure for complex sentences. Due to its
linguistic underpinning the topological model pro-
vides a pre-partitioning of complex sentences that is
(i) highly compatible with deep syntactic structures
and (ii) maximally effective to increase parsing ef-
ficiency. At the same time (iii) partiality regarding
the constituency of non-sentential material ensures
the important aspects of robustness, coverage, and
processing efficiency.
In (Becker and Frank, 2002) we present a corpus-
driven stochastic topological parser for German,
based on a topological restructuring of the NEGRA
corpus (Brants et al, 1999). For topological tree-
bank conversion we build on methods and results
in (Frank, 2001). The stochastic topological parser
follows the probabilistic model of non-lexicalised
PCFGs (Charniak, 1996). Due to abstraction from
constituency decisions at the sub-sentential level,
and the essentially POS-driven nature of topologi-
cal structure, this rather simple probabilistic model
yields surprisingly high figures of accuracy and cov-
erage (see Fig.2 and (Becker and Frank, 2002) for
more detail), while context-free parsing guarantees
efficient processing.
The next step is to elaborate a (partial) map-
ping of shallow topological and deep syntactic struc-
tures that is maximally effective for preference-gui-
Topological Structure:
CL-V2
VF-TOPIC LK-FIN MF RK-t
NN VVFIN ADV NN PREP NN VVFIN
[ 	 [ 
	 Peter] [ 
 i?t] [ 
 gerne Wu?rstchen mit Kartoffelsalat] [ Integrating Information Extraction and Automatic Hyperlinking 
 
Stephan Busemann, Witold'UR G \ VNL Hans-Ulrich Krieger,  
Jakub Piskorski, Ulrich Sch?fer, Hans Uszkoreit, Feiyu Xu 
German Research Center for Artificial Intelligence (DFKI GmbH) 
Stuhlsatzenhausweg 3, D-66123 Saarbr?cken, Germany 
sprout@dfki.de 
 
 
Abstract 
This paper presents a novel information sys-
tem integrating advanced information extrac-
tion technology and automatic hyper-linking. 
Extracted entities are mapped into a domain 
ontology that relates concepts to a selection of 
hyperlinks. For information extraction, we use 
SProUT, a generic platform for the develop-
ment and use of multilingual text processing 
components. By combining finite-state and 
unification-based formalisms, the grammar 
formalism used in SProUT offers both pro-
cessing efficiency and a high degree of decal-
rativeness. The ExtraLink demo system show-
cases the extraction of relevant concepts from 
German texts in the tourism domain, offering 
the direct connection to associated web docu-
ments on demand.   
1 Introduction 
The utilization of language technology for the 
creation of hyperlinks has a long history (e.g., 
Allen et al, 1993). Information extraction (IE) is a 
technology that can be applied to identifying both 
sources and targets of new hyperlinks. IE systems 
are becoming commercially viable in supporting 
diverse information discovery and management 
tasks. Similarly, automatic hyperlinking is a matu-
ring technology designed to interrelate pieces of 
information, using ontologies to define the rela-
tionships. With ExtraLink, we present a novel 
information system that integrates both technolo-
gies in order to reach at an improved level of 
informativeness and comfort. Extraction and link 
generation occur completely in the background. 
Entities identified by the IE system are mapped 
into a domain ontology that relates concepts to a 
structured selection of predefined hyperlinks, 
which can be directly visualized on demand using 
a standard web browser. This way, the user can, 
while reading a text, immediately link up textual 
information to the Internet or to any other docu-
ment base without accessing a search engine.  
The quality of the link targets is much higher 
than with standard search engines since, first of all, 
only domain-specific interpretations are sought, 
and second, the ontology provides additional 
structure, including related information. 
ExtraLink uses as its IE system SProUT, a gene-
ric multilingual shallow analysis platform, which 
currently provides linguistic processing resources 
for English, German, Italian, French, Spanish, 
Czech, Polish, Japanese, and Chinese (Becker et 
al., 2002). SProUT is used for tokenization, mor-
phological analysis, and named entity recognition 
in free texts. In Section 2 to 4, we describe innova-
tive features of SProUT. Section 5 gives details 
about the ExtraLink demonstrator. 
2 Integrating Typed Feature Structures 
and Finite State Machines 
The main motivation for developing SProUT 
comes from the need to have a system that (i) 
allows a flexible integration of different processing 
modules and (ii) to find a good trade-off between 
processing efficiency and linguistic expressive-
ness. On the one hand, very efficient finite state 
devices have been successfully applied to real-
world applications. On the other hand, unification-
based grammars (UBGs) are designed to capture 
fine-grained syntactic and semantic constraints, 
resulting in better descriptions of natural language 
phenomena. In contrast to finite state devices, 
unification-based grammars are also assumed to be 
more transparent and more easily modifiable. 
SProUT?s mission is to take the best from these 
two worlds, having a finite state machine that 
operates on typed feature structures (TFSs). I.e., 
transduction rules in SProUT do not rely on simple 
atomic symbols, but instead on TFSs, where the 
left-hand side of a rule is a regular expression over 
TFSs, representing the recognition pattern, and the 
right-hand side is a sequence of TFSs, specifying 
the output structure. Consequently, equality of 
atomic symbols is replaced by unifiability of TFSs 
and the output is constructed using TFS unification 
w.r.t. a type hierarchy. Such rules not only recog-
nize and classify patterns, but also extract frag-
ments embedded in the patterns and fill output 
templates with them. 
Standard finite state techniques such as minimi-
zation and determinization are no longer applicable 
here, due to the fact that edges in our automata are 
annotated by TFSs, instead of atomic symbols. 
However, not every outgoing edge in such an 
automaton must be analyzed, since TFS annota-
tions can be arranged under subsumption, and the 
failure of a general edge automatically causes the 
failure of several, more specialized edges, without 
applying the unifiability test. Such information can 
in fact be precompiled. This and other optimization 
techniques are described in (Krieger and Piskorski, 
2003). 
When compared to symbol-based finite state 
approaches, our method leads to smaller grammars 
and automata, which usually better approximate a 
given language.  
3 XTDL ? The Formalism in SProUT 
XTDL combines two well-known frameworks, 
viz., typed feature structures and regular ex-
pressions. XTDL is defined on top of TDL, a defi-
nition language for TFSs (Krieger and Sch?fer, 
1994) that is used as a descriptive device in several 
grammar systems (LKB, PAGE, PET).  
Apart from the integration into the rule 
definitions, we also employ TDL in SProUT for 
the establishment of a type hierarchy of linguistic 
entities. In the example definition below, the 
morph type inherits from sign and introduces three 
more morphologically motivated attributes with 
the corresponding typed values: 
morph := sign & [ POS  atom, STEM atom, INFL infl ]. 
A rule in XTDL is straightforwardly defined as 
a recognition pattern on the left-hand side, written 
as a regular expression, and an output description 
on the right-hand side. A named label serves as a 
handle to the rule. Regular expressions over TFSs 
describe sequential successions of linguistic signs. 
We provide a couple of standard operators. Con-
catenation is expressed by consecutive items. Dis-
junction, Kleene star, Kleene plus, and optionality 
are represented by the operators |, *, +, and ?, resp. 
{n} after an expression denotes an n-fold repetition. 
{m,n} repeats at least m times and at most n times. 
The XTDL grammar rule below may illustrate 
the syntax. It describes a sequence of morphologi-
cally analyzed tokens (of type morph). The first 
TFS matches one or zero items (?) with part-of-
speech Determiner. Then, zero or more Adjective 
items are matched (*). Finally, one or two Noun 
items ({1,2}) are consumed. The use of a variable 
(e.g., #1) in different places establishes a 
coreference between features. This example enfor-
ces agreement in case, number, and gender for the 
matched items. Eventually, the description on the 
RHS creates a feature structure of type phrase, 
where the category is coreferent with the category 
Noun of the right-most token(s), and the agreement 
features corefer to features of the morph tokens. 
 np :> 
   (morph & [ POS  Determiner, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )?  
   (morph & [ POS  Adjective, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] )*  
   (morph & [ POS  Noun & #4, 
 INFL  [CASE #1, NUM #2, GEN #3 ]] ){1,2} 
 -> phrase & [CAT #4, 
 AGR agr & [CASE #1, NUM #2, GEN #3 ]]. 
 
The choice of TDL has a couple of advantages. 
TFSs as such provide a rich descriptive language 
over linguistic structures and allow for a fine-
grained inspection of input items. They represent a 
generalization over pure atomic symbols. Unifia-
bility as a test criterion in a transition is a generali-
zation over symbol equality. Coreferences in 
feature structures express structural identity. Their 
properties are exploited in two ways. They provide 
a stronger expressiveness, since they create 
dynamic value assignments on the automaton 
transitions and thus exceed the strict locality of 
constraints in an atomic symbol approach. Further-
more, coreferences serve as a means of information 
transport into the output description on the RHS of 
the rule. Finally, the choice of feature structures as 
primary citizens of the information domain makes 
composition of modules very simple, since input 
and output are all of the same abstract data type.  
Functional (in contrast to regular) operators are 
a door to the outside world of SProUT.  They 
either serve as predicates, helping to locate 
complex tests that might cancel a rule application, 
or they construct new material, involving pieces of 
information from the LHS of a rule.  The sketch of 
a rule below transfers numerals into their 
corresponding digits using the functional operator 
normalize() that is defined externally. For instance, 
"one" is mapped onto "1", "two" onto "2", etc. 
 
  ?  numeral & [ SURFACE #surf, ... ] .?  -> 
  digit & [ ID #id, ... ],  where #id = normalize(#surf). 
4 The SProUT System  
The core of SProUT comprises of the following 
components: (i) a finite-state machine toolkit for 
building, combining, and optimizing finite-state 
devices; (ii) a flexible XML-based regular com-
piler for converting regular patterns into their cor-
responding compressed finite-state representation 
(Piskorski et al, 2002); (iii) a JTFS package which 
provides standard operations for constructing and 
manipulating TFSs; and (iv) an XTDL grammar 
interpreter. 
Currently, SProUT offers three online compo-
nents: a tokenizer, a gazetteer, and a morphological 
analyzer. The tokenizer maps character sequences 
to tokens and performs fine-grained token classifi-
cation. The gazetteer recognizes named entities 
based on static named entity lexica.  
The morphology unit provides lexical resources 
for English, German (equipped with online shallow 
compound recognition), French, Italian, and 
Spanish, which were compiled from the full form 
lexica of MMorph (Petitpierre and Russell, 1995). 
Considering Slavic languages, a component for 
Czech presented in (Haji?, 2001), and Morfeusz 
(Przepi?rkowski and Wolinski, 2003) for Polish. 
For Asian languages, we integrated Chasen 
(Asahara and Matsumoto, 2000) for Japanese and 
Shanxi (Liu, 2000) for Chinese.  
The XTDL-based grammar engineering plat-
form has been used to define grammars for 
English, German, French, Spanish, Chinese and 
Japanese allowing for named entity recognition 
and extraction. To guarantee a comparable 
coverage, and to ease evaluation, an extension of 
the MUC-7 standard for entities has been adopted.   
 
ne-person := enamex & [ TITLE list-of-strings, 
                          GIVEN_NAME list-of-strings, 
                          SURNAME list-of-strings, 
                                  P-POSITION list-of-strings, 
                          NAME-SUFFIX string, 
                                    DESCRIPTOR string ]. 
 
Given the expressiveness of XTDL expressions, 
MUC-7/MET-2 named entity types can be 
enhanced with more complex internal structures. 
For instance, a person name ne-person is defined 
as a subtype of enamex with the above structure. 
The named entity grammars can handle types 
such as person, location, organization, time point, 
time span (instead of date and time defined by 
MUC), percentage, and currency.  
The core system together with the grammars 
forms a basis for developing applications. SProUT 
is being used by several sites in both research and 
industrial contexts. 
A component for resolving coreferent named 
entities disambiguates and classifies incomplete 
named entities via dynamic lexicon search, e.g., 
Microsoft is coreferent with Microsoft corporation 
and is thus correctly classified as an organization. 
5 ExtraLink: Integrating Information 
Extraction and Automatic Hyperlinking  
A methodology for automatically enriching web 
documents with typed hyperlinks has been develo-
ped and applied to several domains, among them 
the domain of tourism information. A core compo-
nent is a domain ontology describing tourist sites 
in terms of sights, accommodations, restaurants, 
cultural events, etc. The ontology was specialized 
for major European tourism sites and regions (see 
Figure 1). It is associated with a large selection of  
 
 
 
Figure 1: Link Target Page (excerpt). The instance the 
web document is associated to (Isle of Capri) is shown 
on the left, together with neighboring concepts in the 
ontology, which the user can navigate through. 
 
link targets gathered, intellectually selected and 
continuously verified. Although language techno-
logy could also be employed to prime target 
selection, for most applications quality require-
ments demand the expertise of a domain specialist. 
In the case of the tourism domain, the selection 
was performed by a travel business professional. 
The system is equipped with an XML interface and 
accessible as a server. 
The ExtraLink GUI marks the relevant entities 
(usually locations) identified by SProUT (see 
second window on the left in Figure 2). Clicking 
on a marked expression causes a query related to 
the entity being shipped to the server. Coreferent 
concepts are handled as expanded queries. The 
server returns a set of links structured according to 
the ontology, which is presented in the ExtraLink 
GUI (Figure 2). The user can choose to visualize 
any link target in a new browser window that also 
shows the respective subsection of the ontology in 
an indented tree notation (see Figure 1).  
 
 
 
Figure 2: ExtraLink GUI. The links in the right-hand 
window are generated after clicking on the marked 
named entity for Lisbon (marked in dark). The bottom 
left window shows the SProUT result for ?Lissabon?. 
 
The ExtraLink demonstrator has been imple-
mented in Java and C++, and runs under both MS 
Windows and Linux. It is operational for German, 
but it can easily be extended to other languages 
covered by SProUT. This involves the adaptation 
of the mapping into the ontology and a multi-
lingual presentation of the ontology in the link 
target page. 
Acknowledgements 
Work on ExtraLink has been partially funded 
through grants by the German Ministry for 
Education, Science, Research and Technology 
(BMBF) to the project Whiteboard (contract 01 IW 
002), by the EC to the project Airforce (contract 
IST-12179), and by the state of the Saarland to the 
project SATOURN. We are indebted to Tim vor 
der Br?ck, Thierry Declerck, Adrian Raschip, and 
Christian Woldsen for their contributions to 
developing ExtraLink. 
References 
J. Allen, J. Davis, D. Krafft, D. Rus, and D. Subrama-
nian. Information agents for building hyperlinks. J. 
Mayfield and C. Nicholas: Proceedings of the Work-
shop on Intelligent Hypertext, 1993. 
M. Asahara and Y. Matsumoto. Extended models and 
tools for high-performance part-of-speech tagger. 
Proceedings of  COLING, 21-27, 2000. 
0 %HFNHU : 'UR G \ VNL +-U. Krieger, J. 
Piskorski, U. Sch?fer, F. Xu. SProUT?Shallow Pro-
cessing with Typed Feature Structures and Unifica-
tion. In Proceedings of  ICON, 2002. 
J. +DML? Disambiguation of rich inflection?compu-
tational morphology of Czech. Prague Karolinum, 
Charles University Press, 2001. 
H.-U. Krieger and U. Sch?fer. TDL?A Type Description 
Language for Constraint-Based Grammars. Procee-
dings of COLING, 893-899, 1994. 
H.-U. Krieger and J. Piskorski. Speed-up methods for 
complex annotated finite state grammars. DFKI 
Report, 2003. 
K. Liu. Research of automatic Chinese word segmen-
tation. Proceedings of ILT&CIP, 2001. 
D. Petitpierre and G. Russell. MMORPH?the Multext 
morphology program. Multext deliverable report 
2.3.1. ISSCO, University of Geneva, 1995. 
J. PiskRUVNL:'UR G \ VNL );X DQG2 6FKHUIA 
flexible XML-based regular compiler for creation 
and converting linguistic resources. Proceedings of 
LREC 2002, Las Palmas, Spain, 2002. 
A. Przepi?rkowski and M. Wolinski. The Unbearable 
Lightness of Tagging: A Case Study in Morphosyn-
tactic Tagging of Polish. Proceedings of the Work-
shop on Linguistically Interpreted Corpora, 2003. 
 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 584?591,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
 
A Seed-driven Bottom-up Machine Learning Framework  
for Extracting Relations of Various Complexity 
Feiyu Xu, Hans Uszkoreit and Hong Li 
Language Technology Lab, DFKI GmbH 
Stuhlsatzenhausweg 3, D-66123 Saarbruecken 
{feiyu,uszkoreit,hongli}@dfki.de  
 
Abstract 
A minimally supervised machine learning 
framework is described for extracting rela-
tions of various complexity. Bootstrapping 
starts from a small set of n-ary relation in-
stances as ?seeds?, in order to automati-
cally learn pattern rules from parsed data, 
which then can extract new instances of the 
relation and its projections. We propose a 
novel rule representation enabling the 
composition of n-ary relation rules on top 
of the rules for projections of the relation. 
The compositional approach to rule con-
struction is supported by a bottom-up pat-
tern extraction method. In comparison to 
other automatic approaches, our rules can-
not only localize relation arguments but 
also assign their exact target argument 
roles. The method is evaluated in two 
tasks: the extraction of Nobel Prize awards 
and management succession events. Perfor-
mance for the new Nobel Prize task is 
strong. For the management succession 
task the results compare favorably with 
those of existing pattern acquisition ap-
proaches.  
1 Introduction 
Information extraction (IE) has the task to discover 
n-tuples of relevant items (entities) belonging to an 
n-ary relation in natural language documents. One 
of the central goals of the ACE program1 is to de-
velop a more systematically grounded approach to 
IE starting from elementary entities, binary rela-
                                                 
1 http://projects.ldc.upenn.edu/ace/ 
tions to n-ary relations such as events. Current 
semi- or unsupervised approaches to automatic 
pattern acquisition are either limited to a certain 
linguistic representation (e.g., subject-verb-object), 
or only deal with binary relations, or cannot assign 
slot filler roles to the extracted arguments, or do 
not have good selection and filtering methods to 
handle the large number of tree patterns (Riloff, 
1996; Agichtein and Gravano, 2000; Yangarber, 
2003; Sudo et al, 2003; Greenwood and Stevenson, 
2006; Stevenson and Greenwood, 2006). Most of 
these approaches do not consider the linguistic in-
teraction between relations and their projections on 
k dimensional subspaces where 1?k<n, which is 
important for scalability and reusability of rules.  
Stevenson and Greenwood (2006) present a sys-
tematic investigation of the pattern representation 
models and point out that substructures of the lin-
guistic representation and the access to the embed-
ded structures are important for obtaining a good 
coverage of the pattern acquisition. However, all 
considered representation models (subject-verb-
object, chain model, linked chain model and sub-
tree model) are verb-centered. Relations embedded 
in non-verb constructions such as a compound 
noun cannot be discovered: 
(1)  the 2005  Nobel Peace Prize 
 
(1) describes a ternary relation referring to three 
properties of a prize: year, area and prize name. 
We also observe that the automatically acquired 
patterns in Riloff (1996), Yangarber (2003), Sudo 
et al (2003), Greenwood and Stevenson (2006) 
cannot be directly used as relation extraction rules 
because the relation-specific argument role infor-
mation is missing. E.g., in the management succes-
sion domain that concerns the identification of job 
changing events, a person can either move into a 
584
job (called Person_In) or leave a job (called Per-
son_Out). (2) is a simplified example of patterns 
extracted by these systems: 
(2) <subject: person> verb <object:organisation> 
 
In (2), there is no further specification of whether 
the person entity in the subject position is Per-
son_In or Person_Out.   
The ambitious goal of our approach is to provide 
a general framework for the extraction of relations 
and events with various complexity. Within this 
framework, the IE system learns extraction pat-
terns automatically and induces rules of various 
complexity systematically, starting from sample 
relation instances as seeds. The arity of the seed 
determines the complexity of extracted relations. 
The seed helps us to identify the explicit linguistic 
expressions containing mentionings of relation in-
stances or instances of their k-ary projections 
where 1?k<n. Because our seed samples are not 
linguistic patterns, the learning system is not re-
stricted to a particular linguistic representation and 
is therefore suitable for various linguistic analysis 
methods and representation formats. The pattern 
discovery is bottom-up and compositional, i.e., 
complex patterns can build on top of simple pat-
terns for projections.  
We propose a rule representation that supports 
this strategy. Therefore, our learning approach is 
seed-driven and bottom-up. Here we use depend-
ency trees as input for pattern extraction. We con-
sider only trees or their subtrees containing seed 
arguments. Therefore, our method is much more 
efficient than the subtree model of Sudo et al, 
(2003), where all subtrees containing verbs are 
taken into account. Our pattern rule ranking and 
filtering method considers two aspects of a pattern: 
its domain relevance and the trustworthiness of its 
origin. We tested our framework in two domains: 
Nobel Prize awards and management succession. 
Evaluations have been conducted to investigate the 
performance with respect to the seed parameters: 
the number of seeds and the influence of data size 
and its redundancy property.  The whole system 
has been evaluated for the two domains consider-
ing precision and recall. We utilize the evaluation 
strategy ?Ideal Matrix? of Agichtein and Gravano 
(2000) to deal with unannotated test data.   
The remainder of the paper is organised as fol-
lows: Section 2 provides an overview of the system 
architecture. Section 3 discusses the rule represen-
tation. In Section 4, a detailed description of the 
seed-driven bottom-up pattern acquisition is pre-
sented. Section 5 describes our experiments with 
pattern ranking, filtering and rule induction. Sec-
tion 6 presents the experiments and evaluations for 
the two application domains. Section 7 provides a 
conclusion and an outline of future work.   
2 System Architecture 
Given the framework, our system architecture 
can be depicted as follows: 
 
Figure 1. Architecture 
 
This architecture has been inspired by several 
existing seed-oriented minimally supervised ma-
chine learning systems, in particular by Snowball 
(Agichtein and Gravano, 2000) and ExDisco 
(Yangarber et al, 2000). We call our system 
DARE, standing for ?Domain Adaptive Relation 
Extraction based on Seeds?. DARE contains four 
major components: linguistic annotation, classifier, 
rule learning and relation extraction. The first com-
ponent only applies once, while the last three com-
ponents are integrated in a bootstrapping loop.  At 
each iteration, rules will be learned based on the 
seed and then new relation instances will be ex-
tracted by applying the learned rules. The new re-
lation instances are then used as seeds for the next 
iteration of the learning cycle.  The cycle termi-
nates when no new relations can be acquired. 
The linguistic annotation is responsible for en-
riching the natural language texts with linguistic 
information such as named entities and depend-
ency structures.  In our framework, the depth of the 
linguistic annotation can be varied depending on 
the domain and the available resources. 
The classifier has the task to deliver relevant 
paragraphs and sentences that contain seed ele-
ments. It has three subcomponents: document re-
585
trieval, paragraph retrieval and sentence retrieval. 
The document retrieval component utilizes the 
open source IR-system Lucene2. A translation step 
is built in to convert the seed into the proper IR 
query format. As explained in Xu et al (2006), we 
generate all possible lexical variants of the seed 
arguments to boost the retrieval coverage and for-
mulate a boolean query where the arguments are 
connected via conjunction and the lexical variants 
are associated via disjunction. However, the trans-
lation could be modified. The task of paragraph 
retrieval is to find text snippets from the relevant 
documents where the seed relation arguments co-
occur. Given the paragraphs, a sentence containing 
at least two arguments of a seed relation will be 
regarded as relevant. 
As mentioned above, the rule learning compo-
nent constitutes the core of our system. It identifies 
patterns from the annotated documents inducing 
extraction rules from the patterns, and validates 
them.  In section 4, we will give a detailed expla-
nation of this component.  
The relation extraction component applies the 
newly learned rules to the relevant documents and 
extracts relation instances. The validated relation 
instances will then be used as new seeds for the 
next iteration.  
3 DARE Rule Representation  
Our rule representation is designed to specify the 
location and the role of the arguments w.r.t. the 
target relation in a linguistic construction. In our 
framework, the rules should not be restricted to a 
particular linguistic representation and should be 
adaptable to various NLP tools on demand.  A 
DARE rule is allowed to call further DARE rules 
that extract a subset of the arguments. Let us step 
through some example rules for the prize award 
domain. One of the target relations in the domain is 
about a person who obtains a special prize in a cer-
tain area in a certain year, namely, a quaternary 
tuple, see (3). (4) is a domain relevant sentence.  
(3) <recipient, prize, area, year> 
(4) Mohamed ElBaradei won the 2005 Nobel 
Peace Prize on Friday for his efforts to limit the 
spread of atomic weapons. 
(5) is a rule that extracts a ternary projection in-
stance <prize, area, year>  from a  noun phrase 
                                                 
2 http://www.lucene.de 
compound, while (6) is a rule which triggers (5) in 
its object argument and extracts all four arguments. 
(5) and (6) are useful rules for  extracting argu-
ments from (4). 
(5)  
 
 (6) 
 
 
Next we provide a definition of a DARE rule: 
A DARE rule has three components  
1. rule name: ri; 
2. output: a set A containing the n arguments 
of the n-ary relation, labelled with their ar-
gument roles; 
3. rule body in AVM format containing: 
- specific linguistic labels or attributes 
(e.g., subject, object, head, mod), de-
rived from the linguistic analysis, e.g., 
dependency structures and the named en-
tity information 
- rule: its value is a DARE rule which ex-
tracts a subset of arguments of A  
The rule in (6) is a typical DARE rule. Its sub-
ject and object descriptions call appropriate DARE 
rules that extract a subset of the output relation 
arguments.  The advantages of this rule representa-
tion strategy are that (1) it supports the bottom-up 
rule composition; (2) it is expressive enough for 
the representation of rules of various complexity; 
(3) it reflects the precise linguistic relationship 
among the relation arguments and reduces the 
template merging task in the later phase; (4) the 
rules for the subset of arguments may be reused for 
other relation extraction tasks.  
The rule representation models for automatic or 
unsupervised pattern rule extraction discussed by 
586
Stevenson and Greenwood (2006) do not account 
for these considerations.  
4 Seed-driven Bottom-up Rule Learning  
Two main approaches to seed construction have 
been discussed in the literature: pattern-oriented 
(e.g., ExDisco) and semantics-oriented (e.g., 
Snowball) strategies. The pattern-oriented method 
suffers from poor coverage because it makes the IE 
task too dependent on one linguistic representation 
construction (e.g., subject-verb-object) and has 
moreover ignored the fact that semantic relations 
and events could be dispersed over different sub-
structures of the linguistic representation. In prac-
tice, several tuples extracted by different patterns 
can contribute to one complex relation instance.   
The semantics-oriented method uses relation in-
stances as seeds. It can easily be adapted to all re-
lation/event instances. The complexity of the target 
relation is not restricted by the expressiveness of 
the seed pattern representation. In Brin (1998) and 
Agichtein and Gravano (2000),  the semantics-
oriented methods have proved to be effective in 
learning patterns for some general binary relations 
such as booktitle-author and company-headquarter 
relations. In Xu et al (2006), the authors show that 
at least for the investigated task it is more effective 
to start with the most complex relation instance, 
namely, with an n-ary sample for the target n-ary 
relation as seed, because the seed arguments are 
often centred in a relevant textual snippet where 
the relation is mentioned.  Given the bottom-up 
extracted patterns, the task of the rule induction is 
to cluster and generalize the patterns. In compari-
son to the bottom-up rule induction strategy (Califf 
and Mooney, 2004), our method works also in a 
compositional way. For reasons of space this part 
of the work will be reported in Xu and Uszkoreit 
(forthcoming).  
4.1 Pattern Extraction 
Pattern extraction in DARE aims to find linguistic 
patterns which do not only trigger the relations but 
also locate the relation arguments. In DARE, the 
patterns can be extracted from a phrase, a clause or 
a sentence, depending on the location and the dis-
tribution of the seed relation arguments.   
 
Figure 2. Pattern extraction step 1 
 
Figure 3. Pattern extraction step 2 
 
Figures 2 and 3 depict the general steps of bot-
tom-up pattern extraction from a dependency tree t 
where three seed arguments arg1, arg2 and arg3 are 
located. All arguments are assigned their relation 
roles r1, r2 and r3. The pattern-relevant subtrees are 
trees in which seed arguments are embedded: t1, t2 
and t3. Their root nodes are n1, n2 and n3.  Figure 2 
shows the extraction of a unary pattern n2_r3_i, 
while Figure 3 illustrates the further extraction and 
construction of a binary pattern n1_r1_r2_j and a 
ternary pattern n3_r1_r2_r3_k. In practice, not all 
branches in the subtrees will be kept. In the follow-
ing, we give a general definition of our seed-driven 
bottom-up pattern extraction algorithm: 
input:  (i) relation = <r1, r2, ..., rn>: the target rela-
tion tuple with n argument roles. 
 T: a set of linguistic analysis trees anno-
tated with i seed relation arguments (1?i?n) 
output: P: a set of pattern instances which can ex-
tract i or a subset of i arguments.  
Pattern extraction: 
 for each tree t ?T 
587
Step 1: (depicted in Figure 2) 
1. replace all terminal nodes that are instanti-
ated with the seed arguments by new 
nodes. Label these new nodes with the 
seed argument roles and possibly the cor-
responding entity classes; 
2. identify the set of the lowest nonterminal 
nodes N1 in t that dominate only one ar-
gument (possibly among other nodes). 
3. substitute N1 by nodes labelled with the 
seed argument roles and their entity classes 
4. prune the subtrees dominated by N1 from t 
and add these subtrees into P. These sub-
trees are assigned the argument role infor-
mation and a unique id. 
Step2: For i=2 to n: (depicted in Figure 3) 
1. find the set of the lowest nodes N1 in t that 
dominate in addition to other children only 
i seed arguments; 
2. substitute N1 by nodes labelled with the i 
seed argument role combination informa-
tion (e.g., ri_rj) and with a unique id. 
3. prune the subtrees Ti dominated by Ni in t; 
4. add Ti to P together with the argument role 
combination information and the unique id  
With this approach, we can learn rules like (6) in 
a straightforward way. 
4.2 Rule Validation: Ranking and Filtering 
Our ranking strategy has incorporated the ideas 
proposed by Riloff (1996), Agichtein and Gravano 
(2000), Yangarber (2003) and Sudo et al (2003). 
We take two properties of a pattern into account: ? domain relevance: its distribution in the rele-
vant documents and irrelevant documents 
(documents in other domains); 
? trustworthiness of its origin: the relevance 
score of the seeds from which it is extracted.   
In Riloff (1996) and Sudo et al (2003), the rele-
vance of a pattern is mainly dependent on its oc-
currences in the relevant documents vs. the whole 
corpus.  Relevant patterns with lower frequencies 
cannot float to the top. It is known that some com-
plex patterns are relevant even if they have low 
occurrence rates. We propose a new method for 
calculating the domain relevance of a pattern. We 
assume that the domain relevance of a pattern is 
dependent on the relevance of the lexical terms 
(words or collocations) constructing the pattern, 
e.g., the domain relevance of (5) and (6) are de-
pendent on the terms ?prize? and ?win? respec-
tively. Given n different domains, the domain rele-
vance score (DR) of a term t in a domain di is: 
DR(t, di)= 
0, if df(t, di) =0; 
df(t,di)
N?D ?LOG(n?
df(t,di)
df(t,dj)
j=1
n?
), otherwise 
where 
? df(t, di): is the document frequency of a 
term t in the domain di  
? D: the number of the documents in di 
? N: the total number of the terms in di 
Here the domain relevance of a term is dependent 
both on its document frequency and its document 
frequency distribution in other domains. Terms 
mentioned by more documents within the domain 
than outside are more relevant (Xu et al, 2002).   
In the case of n=3 such different domains might 
be, e.g., management succession, book review or 
biomedical texts. Every domain corpus should ide-
ally have the same number of documents and simi-
lar average document size. In the calculation of the 
trustworthiness of the origin, we follow Agichtein 
and Gravano (2000) and Yangarber (2003). Thus, 
the relevance of a pattern is dependent on the rele-
vance of its terms and the score value of the most 
trustworthy seed from which it origins. Finally, the 
score of a pattern p is calculated as follows: 
score(p)= }:)(max{)(
0
SeedsssscoretDR
T
i
i ???
=
 
where    |T|> 0 and ti ? T 
? T: is the set of the terms occur in p; 
? Seeds: a set of seeds from which the pat-
tern is extracted; 
? score(s): is the score of the seed s; 
This relevance score is not dependent on the distri-
bution frequency of a pattern in the domain corpus. 
Therefore, patterns with lower frequency, in par-
ticular, some complex patterns, can be ranked 
higher when they contain relevant domain terms or 
come from reliable seeds. 
588
5 Top down Rule Application 
After the acquisition of pattern rules, the DARE 
system applies these rules to the linguistically an-
notated corpus. The rule selection strategy moves 
from complex to simple. It first matches the most 
complex pattern to the analyzed sentence in order 
to extract the maximal number of relation argu-
ments. According to the duality principle (Yangar-
ber 2001), the score of the new extracted relation 
instance S is dependent on the patterns from which 
it origins. Our score method is a simplified version 
of that defined by Agichtein and Gravano (2000): 
score(S)=1? (1? score(Pi )
i=0
P
? )  
where P={Pi} is the set of patterns that extract S. 
 
The extracted instances can be used as potential 
seeds for the further pattern extraction iteration, 
when their scores are validated.  The initial seeds 
obtain 1 as their score. 
6 Experiments and Evaluation 
 We apply our framework to two application do-
mains: Nobel Prize awards and management suc-
cession events.  Table 1 gives an overview of our 
test data sets. 
Data Set Name Doc Number Data Amount 
Nobel Prize A  (1999-2005) 2296 12,6 MB 
Nobel Prize B (1981-1998)  1032 5,8 MB 
MUC-6 199 1 MB 
Table1. Overview of Test Data Sets.  
For the Nobel Prize award scenario, we use two 
test data sets with different sizes: Nobel Prize A 
and Nobel Prize B. They are Nobel Prize related 
articles from New York Times, online BBC and 
CNN news reports.   The target relation for the ex-
periment is a quaternary relation as mentioned in 
(3), repeated here again: 
<recipient, prize, area, year> 
 Our test data is not annotated with target rela-
tion instances. However, the entire list of Nobel 
Prize award events is available for the evaluation 
from the Nobel Prize official website3. We use it as 
our reference relation database for building our 
Ideal table (Agichtein and Gravano, 2000).      
For the management succession scenario, we use 
the test data from MUC-6 (MUC-6, 1995) and de-
                                                 
3 http://nobelprize.org/ 
fine a simpler relation structure than the MUC-6 
scenario template with four arguments:  
<Person_In, Person_Out, Position, Organisation> 
In the following tables, we use PI for Person_In, 
PO for Person_Out, POS for Position and ORG for 
Organisation. In our experiments, we attempt to 
investigate the influence of the size of the seed and 
the size of the test data on the performance. All 
these documents are processed by named entity 
recognition (Drozdzynski et al, 2004) and depend-
ency parser MINIPAR (Lin, 1998).      
6.1 Nobel Prize Domain Evaluation 
For this domain, three test runs have been evalu-
ated, initialized by one randomly selected relation 
instance as seed each time.  In the first run, we use 
the largest test data set Nobel Prize A. In the sec-
ond and third runs, we have compared two random 
selected seed samples with 50% of the data each, 
namely Nobel Prize B. For data sets in this do-
main, we are faced with an evaluation challenge 
pointed out by DIPRE (Brin, 1998) and Snowball 
(Agichtein and Gravano, 2000), because there is no 
gold-standard evaluation corpus available. We 
have adapted the evaluation method suggested by 
Agichtein and Gravano, i.e., our system is success-
ful if we capture one mentioning of a Nobel Prize 
winner event through one instance of the relation 
tuple or its projections. We constructed two tables 
(named Ideal) reflecting an approximation of the 
maximal detectable relation instances: one for No-
bel Prize A and another for Nobel Prize B. The 
Ideal tables contain the Nobel Prize winners that 
co-occur with the word ?Nobel? in the test corpus. 
Then precision is the correctness of the extracted 
relation instances, while recall is the coverage of 
the extracted tuples that match with the Ideal table. 
In Table 2 we show the precision and recall of the 
three runs and their random seed sample: 
Recall Data 
Set 
Seed Preci-
sion total time interval 
Nobel 
Prize A
[Zewail, Ahmed H], 
nobel, chemistry,1999 
71,6% 50,7% 70,9% 
(1999-2005) 
Nobel 
Prize B
[Sen, Amartya], no-
bel, economics, 1998 
87,3% 31% 43% 
(1981-1998) 
Nobel 
Prize B
[Arias, Oscar],  
nobel, peace, 1987 
83,8% 32% 45% 
(1981-1998) 
Table 2. Precision, Recall against the Ideal Table  
The first experiment with the full test data has 
achieved much higher recall than the two experi-
ments with the set Nobel Prize B. The two experi-
ments with the Nobel Prize B corpus show similar 
589
performance. All three experiments have better 
recalls when taking only the relation instances dur-
ing the report years into account, because there are 
more mentionings during these years in the corpus.  
Figure (6) depicts the pattern learning and new 
seed extracting behavior during the iterations for 
the first experiment. Similar behaviours are ob-
served in the other two experiments.   
 
Figure 6. Experiment with Nobel Prize A  
6.2 Management Succession Domain 
The MUC-6 corpus is much smaller than the Nobel 
Prize corpus. Since the gold standard of the target 
relations is available, we use the standard IE preci-
sion and recall method. The total gold standard 
table contains 256 event instances, from which we 
randomly select seeds for our experiments. Table 3 
gives an overview of performance of the experi-
ments. Our tests vary between one seed, 20 seeds 
and 55 seeds. 
Initial Seed Nr.  Precision Recall 
A 12.6% 7.0% 1  
B 15.1% 21.8% 
20  48.4%  34.2% 
55  62.0% 48.0% 
Table 3. Results for various initial seed sets  
The first two one-seed tests achieved poor per-
formance. With 55 seeds, we can extract additional  
67 instances to obtain the half size of the instances 
occurring in the corpus. Table 4 show evaluations 
of the single arguments. B works a little better be-
cause the randomly selected single seed appears a 
better sample for finding the pattern for extracting 
PI argument.  
Arg precision 
(A) 
precision 
(B) 
Recall 
(A) 
Recall 
(B) 
PI 10.9% 15.1% 8.6% 34.4% 
PO 28.6% - 2.3% 2.3% 
ORG 25.6% 100% 2.6% 2.6% 
POS 11.2% 11.2% 5.5% 5.5% 
Table 4. Evaluation of one-seed tests (A and B) 
Table 5 shows the performance with 20 and 55 
seeds respectively. Both of them are better than the 
one-seed tests, while 55 seeds deliver the best per-
formance in average, in particular, the recall value. 
  
arg precision 
(20) 
precision 
(55) 
recall 
(20) 
recall 
(55) 
PI 84% 62.8% 27.9% 56.1% 
PO 41.2% 59% 34.2% 31.2% 
ORG 82.4% 58.2% 7.4% 20.2% 
POS 42% 64.8% 25.6% 30.6% 
Table 5. Evaluation of 20 and 55 seeds tests 
Our result with 20 seeds (precision of 48.4% and 
recall of 34.2%) is comparable with the best result 
reported by Greenwood and Stevenson (2006) with 
the linked chain model (precision of 0.434 and re-
call of 0.265). Since the latter model uses patterns 
as seeds, applying a similarity measure for pattern 
ranking, a fair comparison is not possible. Our re-
sult is not restricted to binary relations and our 
model also assigns the exact argument role to the 
Person role, i.e. Person_In or Person_Out.   
We have also evaluated the top 100 event-
independent binary relations such as Person-
Organisation and Position-Organisation. The preci-
sion of these by-product relations of our IE system 
is above 98%.  
7 Conclusion and Future Work 
Several parameters are relevant for the success 
of a seed-based bootstrapping approach to relation 
extraction. One of these is the arity of the relation.  
Another one is the locality of the relation instance 
in an average mentioning. A third one is the types 
of the relation arguments:  Are they  named entities 
in the classical sense? Are they lexically marked? 
Are there several arguments of the same type? 
Both tasks we explored involved extracting quater-
nary relations. The Nobel Prize domain shows bet-
ter lexical marking because of the prize name.  The 
management succession domain has two slots of 
the same NE type, i.e., persons. These differences 
are relevant for any relation extraction approach.   
The success of the bootstrapping approach cru-
cially depends on the nature of the training data 
base.  One of the most relevant properties of this 
data base is the ratio of documents to relation in-
stances. Several independent reports of an instance 
usually yield a higher number of patterns.   
The two tasks we used to investigate our method 
drastically differ in this respect.  The Nobel Prize 
590
domain we selected as a learning domain for gen-
eral award events since it exhibits a high degree of 
redundancy in reporting.  A Nobel Prize triggers 
more news reports than most other prizes.  The 
achieved results met our expectations.  With one 
randomly selected seed, we could finally extract 
most relevant events in some covered time interval. 
However, it turns out that it is not just the aver-
age number of reports per events that matters but 
also the distribution of reportings to events.  Since 
the Nobel prizes data exhibit a certain type of 
skewed distribution, the graph exhibits properties 
of scale-free graphs.  The distances between events 
are shortened to a few steps. Therefore, we can 
reach most events in a few iterations. The situation 
is different for the management succession task 
where the reports came from a single newspaper.  
The ratio of events to reports is close to one.  This 
lack of informational redundancy requires a higher 
number of seeds.  When we started the bootstrap-
ping with a single event, the results were rather 
poor.  Going up to twenty seeds, we still did not 
get the performance we obtain in the Nobel Prize 
task but our results compare favorably to the per-
formance of existing bootstrapping methods.  
The conclusion, we draw from the observed dif-
ference between the two tasks is simple:  We shall 
always try to find a highly redundant training data 
set.  If at all possible, the training data should ex-
hibit a skewed distribution of reports to events.  
Actually, such training data may be the only realis-
tic chance for reaching a large number of rare pat-
terns.  In future work we will try to exploit the web 
as training resource for acquiring patterns while 
using the parsed domain data as the source for ob-
taining new seeds in bootstrapping the rules before 
applying these to any other nonredundant docu-
ment base.  This is possible because our seed tu-
ples can be translated into simple IR queries and 
further linguistic processing is limited to the re-
trieved candidate documents.   
Acknowledgement 
The presented research was partially supported by a 
grant from the German Federal Ministry of Education 
and Research to the project Hylap (FKZ: 01IWF02) and 
EU?funding for the project RASCALLI. Our special 
thanks go to Doug Appelt and an anonymous reviewer 
for their thorough and highly valuable comments. 
 
References 
E. Agichtein and L. Gravano. 2000. Snowball: extract-
ing relations from large plain-text collections. In 
ACM 2000, pages 85?94, Texas, USA. 
S. Brin. Extracting patterns and relations from the 
World-Wide Web. In Proc. 1998 Int'l Workshop on 
the Web and Databases (WebDB '98), March 1998. 
M. E. Califf and R. J. Mooney. 2004. Bottom-Up Rela-
tional Learning of Pattern Matching Rules for Infor-
mation Extraction. Journal of Machine Learning Re-
search, MIT Press. 
W. Drozdzynski,  H.-U.Krieger, J.  Piskorski; U. Sch?-
fer, and F. Xu. 2004. Shallow Processing with Unifi-
cation and Typed Feature Structures ? Foundations 
and Applications. K?nstliche Intelligenz 1:17?23.  
M. A. Greenwood and M. Stevenson. 2006. Improving 
Semi-supervised Acquisition of Relation Extraction 
Patterns. In Proc. of the Workshop on Information 
Extraction Beyond  the Document, Australia.  
D. Lin. 1998. Dependency-based evaluation of  MINI-
PAR. In Workshop on the Evaluation of Parsing Sys-
tems, Granada, Spain. 
MUC. 1995. Proceedings of the Sixth Message Under-
standing Conference (MUC-6), Morgan Kaufmann. 
E. Riloff. 1996. Automatically Generating Extraction 
Patterns from Untagged Text. In Proc. of the Thir-
teenth National Conference on Articial Intelligence, 
pages 1044?1049, Portland, OR, August. 
M. Stevenson and Mark A. Greenwood. 2006. Compar-
ing Information Extraction Pattern Models. In Proc. 
of the Workshop on Information Extraction Beyond  
the Document, Sydney, Australia.  
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for 
Automatic IE Pattern Acquisition. In Proc. of ACL-
03, pages 224?231, Sapporo, Japan. 
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain 
Knowledge for Information Extraction. In Proc. of 
COLING 2000, Saarbr?cken, Germany. 
R. Yangarber. 2003. Counter-training in the Discovery 
of Semantic Patterns. In Proceedings of ACL-03, 
pages 343?350, Sapporo, Japan. 
F. Xu, D. Kurz, J. Piskorski and S. Schmeier. 2002. A 
Domain Adaptive Approach to Automatic Acquisition 
of Domain Relevant Terms and their Relations with 
Bootstrapping.  In Proc. of LREC 2002, May 2002.  
F. Xu, H. Uszkoreit and H. Li. 2006. Automatic Event 
and Relation Detection with Seeds of Varying Com-
plexity. In Proceedings of AAAI 2006 Workshop 
Event Extraction and Synthesis, Boston, July, 2006.  
591
Crosslingual Language Technologies for Knowledge Creation
and Knowledge Sharing
Hans Uszkoreit
DFKI Language Technology Lab
Stuhlsatzenhausweg 3
66123 Saarbruecken, Germany
uszkoreit@dfki.de
www.dfki.de/~hansu/
A large and fast growing part of corporate
knowledge is encoded in electronic texts.
Although digital information repositories are
becoming truly multimedial, human language
will remain the only medium for preserving and
sharing complex concepts, experiences and
ideas.  It is also the only medium suited for
expressing metainformation. For a human reader
a text has a rich structure, for a data processing
machine it is merely a string of symbols.
Classical information retrieval helps to sort and
find information in large libraries of documents
by matching strings of characters. Effective
information management is a building block of
modern knowledge management. However,
language technology can contribute much more
than methods for finding information.
A number of areas in which language
technologies can improve knowledge
management are described in Maybury (in this
volume). We will concentrate on examples in
which language technologies can facilitate the
creation of new knowledge from large volumes
of textual information and the sharing of
knowledge accross language boundaries.
1 Knowledge Sharing
One of the true challenges of KM is the
development and implementation of schemes
that make people share knowledge and use such
shared knowledge in critical situations.  Offering
incentives for the sharing of knowledge is not
sufficient. The valuable information needs to be
offered in situations where it is needed. It also
needs to be evaluated in such situations because
any effective incentive scheme might lead to
information overflow if the quality of the
provided information cannot be assessed.
Language technology can provide means for
associating shared knowledge with the relevant
decision situations by automatically linking it to
the critical elements within decision triggers, i.e,
electronic documents in the workflow that
demand and record a decision.
Together with some simple statistical methods
this method can also support a scheme for
evaluating shared information with a minimum
of additional effort.  The language technology
that can be applied for this purpose we call
automatic relational hyperlinking.  Relational
hyperlinks differ from the simple hyperlinks of
HTML in that they are composed out of a
number of named links that can be selected from
a menu.
Language technology is needed for identifying
and disambiguating the concepts in documents
that need to be linked. To this end, techniques
from information extraction are employed such
as named entity recognition. When automatic
hyperlinking associates information to decision
situations, an evaluation can be enforced without
an additional burden on the user.
Automatic hyperlinking can also be applied for
transforming information into knowledge-like
structures. By densely interconnecting
informational elements, three criteria are met
that distinguish knowledge from other forms of
information: immediate accessability, grounding
of pieces of knowledge and associative
structure. The important fourth criterion is the
suitability for inferencing, however in this
application scenario inferencing is not
performed by the machine but by the human
user of the service.
This method has been applied in the system
Hypercode of the DFKI LT Lab.  The original
purpose of this system which was developed for
a large German bank is to facilitate work with
legacy code. Hypercode provides dense
associative relational hyperlinking to program
code and documentation. By densely
interlinking code and documentation, the
knowledge encoded in the documentation
becomes much more accessible and usable. The
methods of Hypercode were also applied for
enriching a new WWW-based information
service of the Saarland State Government for
start-up companies.
2 Crosslingual Knowledge
Management
Globalization forces companies to become
multilingual.  The language of customer
interaction should be the preferred language of
the customer.  The language for knowledge
sharing should be preferred language of the
experts who voluntarily provide the knowledge.
On the other hand, the language of knowledge
sharing has to be a language that the potential
users of the information understand. The
languages of provider and users may differ.
Moreover, in a multinational enterprise there
may be user communities that extend across
several native languages.  Translation is costly
and may delay the exploitation of shared
knowledge.  Automatic translation offers alter-
native solutions.  Even the best machine trans-
lation systems cannot translate unseen texts
without grammatical or stylistic errors.
However, for the purpose of knowledge sharing
often a so called content translation or an
indicative translation will suffice. Such a
translation can be provided by existing
translation systems.  Factual errors can be
avoided by augmenting the general purpose
translation systems with specialized terminology
and transfer rules. We will exemplify the
utilization of specialized indicative machine
translation for multilingual expert groups by a
project for a large multinational automobile
manufacturer.
Finally we will provide an overview of other
crosslingual language technologies and their
potential for crosslingual knowledge manage-
ment. In this context, we will point to a number
of European R&D projects in which consortia
composed of academic and industrial partners
improve or adapt language technologies such as
information retrieval, information extraction and
summarization for knowledge management
applications in multilingual applications
scenarios.
References
Glushko, R. J. (1989): Transforming Text Into
Hypertext For a Compact Disc Encyclopedia,
In: Proceedings of CHI '89, ACM Press.
Jacobs, P. (1997) Text Interpretation: Extracting
Information. In: R.A. Cole, J. Mariani, H.
Uszkoreit, A.Zaenen, V. Zue (eds.): Survey of
the State of the Art in Human Language
Technology, Cambridge University Press and
Giardini.
Piskorski, J.  and G. Neumann (2000) An
Intelligent Text Extraction and Navigation
System. In : Proceedings of 6th International
Conference on Computer-Assisted Information
Retrieval (RIAO-2000), Paris.
Pustejovsky, J., B. Boguraev, M. Verhagen, P.
Buitelaar,. and M. Johnston (1997): Semantic
Indexing and Typed Hyperlinking. In:
Proceedings of the American Association for
Artical Intelligence Conference, Spring
Symposium, NLP for WWW.. Stanford
University, CA, 120-128.
   
Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 22?25,
Athens, Greece, 30 March, 2009. c?2009 Association for Computational Linguistics
 
Linguistics in Computational Linguistics: 
Observations and Predictions 
Hans Uszkoreit 
Language Technology Lab, DFKI GmbH 
Stuhlsatzenhausweg 3, D-66123 Saarbruecken 
uszkoreit@dfki.de  
 
Abstract 
As my title suggests, this position paper focuses 
on the relevance of linguistics in NLP instead of 
asking the inverse question. Although the ques-
tion about the role of computational linguistics 
in the study of language may theoretically be 
much more interesting than the selected topic, I 
feel that my choice is more appropriate for the 
purpose and context of this workshop. 
This position paper starts with some retrospec-
tive observations clarifying my view on the am-
bivalent and multi-facetted relationship between 
linguistics and computational linguistics as it 
has evolved from both applied and theoretical 
research on language processing. In four brief 
points I will then strongly advocate a strength-
ened relationship from which both sides benefit.   
First, I will observe that recent developments in 
both deep linguistic processing and statistical 
NLP suggest a certain plausible division of labor 
between the two paradigms. 
Second, I want to propose a systematic approach 
to research on hybrid systems which determines 
optimal combinations of the paradigms and con-
tinuously monitors the division of labor as both 
paradigm progress. Concrete examples illustrat-
ing the proposal are taken from our own re-
search. 
Third, I will argue that a central vision of 
computational linguistics is still alive, the dream 
of a formalized reusable linguistic knowledge 
source embodying the core competence of a 
language that can be utilized for wide range of 
applications.   
1 Introduction 
Computational linguistics did not organically 
grow out of linguistics as a new branch of mathe-
matical or applied linguistics. Although the term 
suggests the association with linguistics, in prac-
tice much of CL has rather been purely engineer-
ing-driven natural language processing. Even if 
computational linguistics has become a recognized 
subfield of linguistics, most of the action in CL 
does not address linguistic research questions.   
For most practitioners, the term was never more 
than a sexy sounding synonym for natural lan-
guage processing. Many others, however, fortu-
nately including many of the most creative and 
successful scientists in CL, shared the ambition of 
contributing to the scientific study of human lan-
guage.  
Already in the eighties Lauri Karttunen ob-
served that there is a coexistence and mutual fer-
tilization of applied computational linguistics and 
theoretical computational linguistics, and that the 
latter subarea can provide important insights into 
the structure and use of human language.   
When we look into the actual relationship be-
tween linguistics and CL, we can easily perceive a 
number of changes that have happened over time. 
We can distinguish five major paradigms in com-
putational linguistics, each of which has assigned a 
slightly different role to linguistic research. The 
first paradigm was the direct procedural implemen-
tation of language processing.  NLP systems of this 
paradigm were programs in languages such as 
FORTRAN, COBOL or assembler in which there 
was no systematic division between linguistic 
knowledge and processing. Linguistics was only 
important because it had educated some of the 
practitioners on relevant properties of human lan-
guage.  
The second paradigm was the development of 
specialized algorithms and methods for language 
22
processing. This paradigm includes for instance 
parsing algorithms, finite-state parsers, ATNs, 
RTNs and augmented phrase-structure grammars. 
Although we find a separation between linguistic 
knowledge and processing components, none of 
the developed methods were imports from linguis-
tics, nor were they adopted in linguistics. (A nota-
ble exception may have been two-level finite-state 
morphology which at least caused some discussion 
in linguistic morphology.) Nevertheless, some of 
the approaches required a certain level of linguistic 
sophistication.  
The third paradigm was the emergence of lin-
guistic formalisms. In the eighties a variety of new 
declarative grammatical formalisms such as HPSG, 
LFG, CCG, CUG had quite some influence on CL. 
These formal grammar models were accompanied 
by semantic formalisms such as DRT. A number of 
these formal models were tightly connected to lin-
guistic theories and therefore also taught in linguis-
tics curricula. Several attempts to turn current ver-
sions of the linguistic mainstream theory of 
GB/P&P/minimalism into such a declarative for-
malism were not very successful in NLP but still 
discussed and used in linguistic classrooms.  
When these linguistic formalisms failed to meet 
the performance criteria needed for realistic appli-
cations, most of applied computational linguistics 
research fell back on specialized methods for NLP 
such as finite state methods for information extrac-
tion. Other colleagues moved on to methods of the 
fourth paradigm in CL, i.e., statistical methods. 
Inspired by the rapid success of these statistical 
techniques, the new paradigm soon ruled most of 
NLP research. Not surprisingly, the distance be-
tween linguistics and mainstream CL increased, as 
researchers in most subareas researchers did not 
have to know much about language and linguistics 
in order to be successful in statistical NLP.  
Only when the success curve of statistical NLP 
started to flatten in several application areas, inter-
est in linguistic methods and knowledge sources 
reawakened. Hard core statistical NLP specialists 
consulted lexicons or tried to develop statistical 
models on phrase structures. Many statistical ap-
proaches now exploit structured linguistic descrip-
tions as obtained from treebanks and other linguis-
tically annotated corpora. 
In the meantime, proponents of linguistic meth-
ods had discovered the power of statistical models 
for overcoming some of the performance limita-
tions of deep NLP. Statistical models trained on 
treebanks have become the preferred method for 
solving the massive ambiguity problem of deep 
linguistic parsing. 
All these pragmatic mixes of statistical and lin-
guistic methods marked the birth of the fifth para-
digm in CL, the creative combination of statistical 
and non-statistical machine learning approaches 
with linguistic methods. 
2 Division of Labor between Linguistics 
and Statistics 
To illustrate my view on the complementary 
contributions of statistical and linguistic methods I 
want to start with three observations.  The first ob-
servation stems from parser evaluations. A CCG 
parser was successfully applied to the standard 
Wall Street Journal test data within the Penn Tree-
bank (refs).  Although the C&C parser did not 
quite get the same coverage as the best statistical 
systems, it produced very impressive results.  As 
Mark Steedman demonstrated in a talk at the Com-
putational Linguistics session of the 2008 Interna-
tional Congress of Linguists, the C&C parser 
moreover found many dependencies needed for 
semantic interpretation that are not even annotated 
in the Penn Treebank.  
Observation two stems from our work on hybrid 
machine translation. Within the EU project Euro-
Matrix we are organizing open evaluation cam-
paigns of MT systems by shared tasks whose re-
sults are reported in the annual WMT workshops.  
The first large campaign combining automatic and 
intellectual evaluation took place in 2008.  Partici-
pants could contribute translations of two test data 
sets for a range of language pairs. One test set was 
in a specific domain for which training data had 
been provided. The other test set contained news 
texts on a variety of topics. Although a training set 
of news texts had been provided as well, the cov-
ered domains exhibited much more diversity than 
the closed domain texts. It turned out that in gen-
eral the best systems for the closed domain task 
were statistical MT systems, whereas the open do-
main task was best solved by seasoned rule-based 
MT commercial products.  
23
A careful comparative study of errors made by 
some of the best SMT and RBMT systems re-
vealed that the errors of the two systems were 
largely complementary. As SMT can acquire fre-
quently used expressions from training data, the 
output generally appears rather fluent, at least for 
short sentences and short portions of sentences. 
SMT is also superior in lexical and phrasal disam-
biguation and the optimal lexical choice in the tar-
get language.  However, the translations exhibit 
many syntactic problems such as missing verbs or 
agreement violations, especially if the target lan-
guage has a complex morphology. RBMT systems, 
on the other hand, usually get the syntactic struc-
ture right?unless they fail in attachment ambigui-
ties?but on the word and phrase level they often do 
not select the correct or stylistically optimal trans-
lations.   
Today's machine learning methods for acquiring 
the statistical translation models from parallel texts 
fail on many syntactic phenomena that can be ana-
lyzed correctly by a linguistic grammar. Inducing a 
correct treatment of long distance phenomena such 
as topicalization or "easy"-adjectives, ellipsis and 
control phenomena from unannotated texts seems 
quite impossible. Learning complex rules from 
syntactically and semantically annoted texts may 
be possible if linguists have already understood 
and formalized the underlying analysis of the phe-
nomenon.  
The third observation comes from supervising 
work in grammar development and attempts to en-
large the coverage of existing grammars automati-
cally through the exploitation of corpus data.  
When he tried to extend the coverage of the ERG, 
Zhang Yi could show that almost all of the cover-
age gaps could be attributed to missing lexical 
knowledge. Even if the words in the unanalyzable 
sentence were all in the lexicon, usually some 
reading of words, i.e. their membership in some 
additional word class, was missing. The few re-
maining coverage deficits result from specific in-
frequent constructions not yet covered by the 
grammar plus missing treatments for a few notori-
ously tricky syntactic problems such as certain 
types of ellipsis. 
These three observations together with numer-
ous others strongly suggest the following insight. 
Every grammar of a human language consists of a 
small set of highly complex regularities and of a 
huge set of much less complex phenomena. The 
small set of highly complex phenomena occurs 
much more often than most of the phenomena of 
little complexity. This slanted distribution makes 
language learnable.  So far we have no automatic 
learning methods that could correctly induce the 
complex phenomena. It is highly questionable 
whether these regularities could ever be induced 
without full access to the syntax-semantics map-
pings that the human language learner exploits.  
On the other hand, the lexicon or simple selec-
tional restrictions can easily be learned because the 
complexity lies in the structure of the lexical 
classes and not in the simple mapping from words 
to these classes.  
3 Hybrid Systems Research 
In several areas of language processing, first ap-
proaches of designing hybrid systems containing 
both linguistic and statistical components have 
demonstrated promising results. 
However, much of this research is based on 
rather opportunistic selections. Readily available 
components are connected in a pure trial and error 
fashion.  In our hybrid MT research we are sys-
tematically searching for optimal combinations of 
the best statistical and the best rule-based systems 
for a given language pair. The approach is system-
atic, because we use a detailed error analysis by 
skilled linguists to find out which classes of 
phrases are usually better translated by the best 
statistical systems.  We then insert the translations 
for such kinds of phrases into the syntactic skele-
tons of the translated sentences provided by the 
rule-base system. One of the translations we sub-
mitted to this year?s EuroMatrix evaluation cam-
paign was obtained in this way. 
24
The technique to merge sentence parts from the 
two systems into one translation is only a crude 
first approximation of a truly hybrid processing 
system, i.e., a system in which the statistical phrase 
translation is fully integrated into the rule-based 
system. Our goal is to test the usefulness of statis-
tical methods in analysis, especially for disam-
biguation, in transfer, especially for selecting the 
best translation for words and smaller phrases and 
in generation, for the selection among paraphrases 
according to monolingual language models. 
Another systematic approach to hybrid systems 
design was investigated in the Norwegian LOGON 
project, in which deep linguistic processing by 
HPSG and LFG was complemented by statistical 
methods.  
Another example for a systematic approach to 
hybrid systems building is our work on an architec-
ture for the combination of components for the 
analysis of texts. The DFKI platform Heart-of-
Gold (HoG) was especially designed for this pur-
pose. In HoG several components can be combined 
in multiple ways.  All processing components write 
their analysis results into a multi-layer XML stand-
off annotation of the analyzed text. The actual in-
terface language is RMRS (Robust Minimal Re-
cursion Semantics, ref.) XML is just used as the 
syntactic carrier language for RMRS.  
4 Computational Models of Linguistic 
Competence 
Although the competence-performance distinc-
tion is a complex and highly controversial issue, 
the theoretical dichotomy is useful for the argu-
ment I want to make. When children acquire a lan-
guage, they first learn to comprehend and produce 
spoken utterances. Much later they learn to read 
and to write, and much later again they may learn 
how to sing and rhyme and how to summarize, 
translate and proofread texts.  
All of the acquired types of performance utilize 
their underlying linguistic competence. New types 
of performance are relatively easy to learn. The 
shared knowledge base ensures a useful level of 
consistency across the performance skills. Of 
course, each type of performance may use different 
parts of the shared competence. Certain types of 
performance may also extend the shared base into 
different directions.   
The child could not acquire the complex map-
ping between sound and meaning without having 
access to both spoken (and later also written) form 
and the corresponding semantics. Therefore the 
child cannot learn a language from a radio beside 
her crib, nor can the older child acquire Chinese by 
being locked up in a library of Chinese books. 
Thus the basic competence cannot be obtained out-
side performance or successful communication.  
The first approaches to linguistic computational 
grammars may have been too simplistic by not 
providing the connection between competence and 
performance needed for exploiting the competence 
base in realistic applications.  However, in gradu-
ally solving the problems of efficiency, robustness 
and coverage researchers have arrived at more so-
phisticated views of deep linguistic processing.  
After several decades of experience in working 
on competence and performance modeling for both 
generic grammatical resources and many special-
ized applications, I am fully convinced that the 
goal of a reusable shared competence model for 
every surviving language in our global digital in-
formation and communication structure is still a 
worthwhile and central goal of computational lin-
guistics. I am also certain that the goal will be ob-
tained in many steps. We already witness a reuse 
of large computational grammar resources such as 
the HPSG ERG, the LFG ParGram Grammar and 
the English CCG in many different applications. 
These applications are still experimental but when 
deep linguistic processing keeps improving in effi-
ciency, specificity (ability to select among read-
ings), robustness and coverage at current speed of 
progress, we will soon see first cases of real life 
applications.  
I am not able to predict the respective propor-
tions of the intellectually designed core compo-
nents, the components learned automatically from 
linguistically annotated data and the components 
automatically learned from unannotated data but I 
am convinced that the systematic search for the 
best combinations will be central to partially real-
izing the dream of computational linguistics still 
within our life times.  
If such solutions can be found and gradually im-
proved, the insights gained through this systematic 
investigation may certainly also have a strong im-
pact in the other direction, i.e. from computational 
linguistics into linguistics. 
25
An ontology of systematic relations for a shared grammar of Slavic 
Tania AVGUSTINOVA 
Computational Linguistics, Saarland University 
P. O. Box 151150 
Saarbriicken, Germany, D-66041 
avgustinova @coli.uui-sb.de 
Hans USZKOREIT 
Language Technology Lab, DFKI 
Computational Linguistics, Saarland Univc,'sity 
Saarbr(icken, Germany, D-66041 
uszkoreit@dfki.dc 
Abstract 
Sharing portions of grammars across languages greatly re- 
duces the costs of nutltilingual grammar engineering. Related 
languages hare a ntuch wider range of linguistic itff'ornuttio;t 
than typically assunwd in stamlard mttltilingttal gramtmtr 
atwhitectures. Taking grammatical relatedness eriously, we 
are particularly interested in designing lhtguistically motivated 
grammatical resottrces Jbr Slavic languages to be used itz 
applied and theoretical computational linguistics, ht order to 
gain the pelwpective of a language-family oriented gramntar 
desigtl, we consider {ttt arrtly of systematic relations that can 
hold between syntactical units. While the categorisation of 
primitive linguistic entities tends to be language-specilic or 
even constrttction-.~pecific, the relations holding between them 
allow viii'lofts degrees of absltztction. On the basis of Slavic 
&tta, we show how a domain ontology conceptualising molpho- 
syntaclic "buildiltg blocks" can serve as a basis r~" a shared 
grotlt;nar of Slavic. 
Introduction 
In applied computational linguistics, the need for 
developing and utilising operational notions of 
shared grammars tems fi'om multilingual grammar 
engineering. If considerable portions of existing 
grammars can be reused for the specification ot7 new 
grammars, development eftbrts can be greatly re- 
duced. A shared grammar also facilitates the diffi- 
cult task of lnaintaining consistency within and 
across the individual parallel grammars. In machine 
translation, the specification of a shared grammar 
can furthermore be exploited for simplifying the 
transfer process. 
Without much ado, computational linguists engaged 
in multilingual grammar development have always 
tried to reduce their labour by importing existing 
grammar components in a simple "copy-paste- 
modify" fashion. But there were also a number of 
systematic attempts to create and describe shared 
grammars that are convincingly documented in 
publications. \[Kam88\] demonstrates the concept for 
a relatively restricted domain, the grammatical 
description of simple nominal expressions in five 
languages. \[BOP88\] were able to exploit the gram- 
matical overlap of two Slavic languages, for the 
design of a lean transfer process in Russian to 
Czech machine translation. In multilingual applica- 
tion development within Microsoft research, gram- 
mar sharing has extensively been exploited - 
\[Pin96\], \[GLPR97\]. 
However, all these approaches are rather opportun- 
istic in the sense that existing grammatical descrip- 
tions based on existing grammar models were ex- 
plored. We went a step further and started grammar 
design with a notion of a shared grammar for a 
family of related languages. Pursuing the goal of 
designing linguistically motivated grammatical 
resources for Slavic languages to be used in com- 
putational linguistics, one is inevitably confronted 
with primary problems temming ti'om the t'act that 
different linguistic theories cut up grammars in 
quite different ways, and grammar formalisms differ 
in their degree of granularity. It cannot be expected, 
therefore, that the minimal differences between two 
languages or their shared elements form easily 
identifiable units in the available language-specific 
grammars. Therefore, an ontology conceptualising 
morphosyntactic "building blocks" would offer a 
solid basis for a shared grammar of Slavic in the 
sense of \[ASU99\]. Our use of the term ontology is 
fairly pragmatic, namely, as representing a formal 
shared conceptualisation f a particular domain of 
interest. It describes concepts relevant for the do- 
main, their relationships, as well as "axioms" about 
these concepts and relationships. Note that such a 
pragmatic approach does not presuppose any gen- 
eral all-encompassing ontology of language but 
rather "mini-ontologies" conceptualising the se- 
lected domain from various perspectives in a con- 
sistent way. The domain of interest in this project is 
the grammatical knowledge on Slavic morphosyn- 
tax contained in linguistic theories and linguistic 
descriptions. While the categorisation of primitive 
linguistic entities lends to be language-specific or
even construction-specific, the relations holding 
between them allow various degrees of abstraction. 
In order to gain the perspective of language-family 
oriented grammar design, we will consider the array 
of .s3,stematic relations that can hold between syn- 
tactically significant items. 
Systematic relations 
Systematic relations motivate shared patterns of 
variation cross-linguistically as well as across con- 
structions. In a constraint-based theory like HPSG, 
where the grammatical properties of linguistic enti- 
ties are typically revealed in complex taxonomies, 
nothing in the formal apparatus would actually 
exclude the possibility to organise also the relations 
holding in syntactic onstructions in a type hierar- 
chy. So, the type subsumption could be interpreted 
as modelling a continuum from general - and pre- 
sumably universal - systematic relations to more 
and still more specific instances of these relations 
resulting fi'om admissible cross-classifications. I In
I The two types of edges connecting types in otu g,'aphical 
representation f hierarchies - 'square' and 'direct' - are signifi- 
cant. The former indicate possible conjunction of types, and thus 
introduce various dimensions of multiple inhe,'itance. The latter 
indicate disjunction of types within the respective dimension of 
classification. 
98 
our view, two orthogonal types of systematic rela- 
tions have to be distinguished: syntagmatics and 
alignment, since they appear to be universally rele- 
vant for the well-for|nedlmss of utterances in any 
language (Hierarchy I). 2 Syntagmatic relations play 
a constitutive role in syntax by establishing instant 
COlmections between linguistic entities in various 
constructions. There is a covert, meaningful dime,> 
sion of structural syntagmatics, a +d a , l  ()Vel"t.,  , l lO l " -  
phosyntactic, form-oriented imension of combi- 
natorial syntagmatics. 
systematic relation I 
J 
I intonation 
syntagmatics alignnlent 
i \[ ~ ! i i"" 
combinatorial structural continuity directionality periphery 
overt covert , 
nlorp ~osyn actc ureaningM i 
1 ,icrnrchy I: Systcnmlic r?lntions: dimensions of classification 
With respect to the alignment relation, which is 
responsible for the actual lincat distributiot+ of syn- 
tactically relevant itch|s, we assun|e that at least the 
continuity o1' syntactic units, the directionality of 
the head (or, more gene,'ally, of a certain syntacti- 
cally significant entity) its well as the l)eril)hery of at 
syntactically determined omain a,'c relevant di- 
n|cnsions of classification (ltierarchy 2). 
alignmout 
! 
continuRy directionality periphery 
contitluous disconOnuous tlonX-X X-nonX lel~ @ht 
(nonhead-HEAD) (HEAD-tionheadl 
Hierarchy 2: Alignment 
The continuity of syntactic units can be realised its 
immediate constituency (i.e. of type continuous) or 
as long-distance constituency (i.e. o1' type discon- 
tinue++s). The directionality accounts for situations 
where, e.g., Ihe head either li)llows the dependent or 
precedes it. In turn, the i~eriphcry of a syntactically 
determined omain can be left or right. 
St ructura l  syntagmat ics  
The structural syutagn|atic relations between two 
syntactic units is classified along two primary di- 
mensions which we call centHcity and taxis 
(Hierarchy 3). The chosen terms should be under- 
{~ tr stood in the context of distin .uishin~, on the one 
hand, endocentric imd exocentric relations, and on 
the other hand, h37mtaxix and parataxis. The endo- 
centricity of a strt,ctural syntagmatic relation 
(between, e.g., ~ and \[~) presuptmses that one of the 
syntactic items involved in this relation (e.g., c0 
plays a pron|inent role. In contrast, the exocenlricity 
of a structural syntag,natic relation presupl)oses no 
assun+|ptions in this respect, hence, it can be viewed 
as the unmarked member of the centricity opposi- 
tion. The hypotaxis means that there is a depend- 
2 The intotmlional ol+ganisation of utterances by the way is 
anolher systematic relalion exhibiting ,his trait. A more thor- 
ough invcsligation of ,he in,onatiomd aspect would bc well 
beyond the morphosyntactic orientation of the present work. 
cncy of subol'di,mtion between the involved syntac- 
tic items, while the paramxis is net|tral in this re- 
spect and is regarded as the untnarked member of 
the taxis opposition. 
syntagmatics 
s t ruc tura l  
centlicity 
i 
taxis 
hypofnxis parataxis en(Jocenttlc exocenltlc 
I lierarchy 3: Cenhicity and taxis 
Consequently, if two linguistic entities belong to- 
gether fi'om the viewpoint of structural syntagtnat- 
its, they are involved in one of the following rela- 
tion types which arc obtained via+ admissible cross- 
classifications (Hierarchy 4). 
l,;ndocc||tric hypotaxis, or selection. The head- 
dependent co|fl'igu|'ation can be identified unambi- 
guously. Tim prominent element is also the domi- 
nating one in the subordination. 
Exoccntric hypotaxis, or modification. There is no 
l)t'olnincnt clone|on+ to unambigttously lake o\,oi" the 
role of a dolninating item in the subordination. Note 
that this is where (theory-sinusitis) linguistic con- 
ventions regarding the head-dependct+t configura- 
tion actually begin. 
Endocentric parataxis. There is a prominent elc- 
|nent in this relation, but no head-dependent con- 
figuration. 
Exocentric parataxis. In the relation holding be- 
tween lhc ilwolved linguistic entities there is neither 
a prominent element nor a head-dependent configu- 
ration. This is the tmlnarked case with respect to 
both ccnlricity and taxis. 
syntagmatics 
structural 
centricity taxis 
et\]docetltfic exocentfic hypctaxis parataxis 
selection modification endo-para exo-para 
" r, 
i licrarchy 4: Structural syntugmatics 
Due to the fact that there always is a principal or 
leading element in tile endocenlric relations, differ- 
ent linguistic theories typically agree on how to 
intel'pret these relations tructurally. But there is no 
consensus -- often even within the same linguistic 
theory - on the structural interpretation of the 
exocentric relations. So, additional factors are usu- 
ally taken into consideration as st, pporting the in- 
+,'eduction of particular conventions. The latter, 
however, arc not always linguistically motiwtted, 
29 
tile choice is sometimes arbitrary and often due to 
theory-sl~ecific technical reasons. 
Combinator ia l  syntagmat ics  
The combinatorial dimension in tile proposed tax- 
onomy (Hierarchy 5) largely corresponds, in our 
understanding, to what \[S&L95\] regard as mor- 
phological signalling of direct syntactic relations. 
syntagmatics 
I 
combinatorial 
. - - --  . . 
juxtaposition co-variation government 
Hicrm'chy  5: Combinator ia l  synt~mat ics  
Tile combinatorial syntagmatic relation of juxtapo- 
sition presupposes no overt morphological indica- 
tion. As to governmettt, i  is traditionally under- 
stood as the determination by one element of the 
inflectional form of the other, i.e. form government. 
Its classical instance is, of course, case government. 
In \[Cann93\] (p. 51) these morpllosyntactic relation 
is formulated for some construction involving y and 
(5 ill tile following way: y governs (5 if (i) varying the 
inflectional l'orm of (5 while keeping T constant leads 
to ungrammaticality, and (ii) varying the form o f t  
and keeping 8 constant makes no difference to 
grammaticality. The systematic co-variatioll o1' 
linguistic forms is typically rcalised as feature con- 
gruity, i.e. compatibility of values of identical 
grammatical categories of syntactically colnbined 
linguistic items. Ill our view, two general co- 
variation types must be distinguished (cf. Hierarchy 
6), namely, asymmetric and symmetric co-variation, 
with only the former actually corresponding to the 
traditional directional concept of agreement, e.g., 
\[Cor98\]. 
Cc-vatiatiorl 
asymmetric symmetric 
al 1~1 ('d ~I 
t__~ t__ J  
llierarchy 6: Morphosyntaclic co-variation 
As the term suggests, the asymmetry of co-variation 
presupposes a controller-target conl'iguration. This 
is to be contrasted with the synunetry of co- 
variation which is not interpretable in these terms. 
Symmetric o-variation, in essence, would presume 
redundancy as if both co-varying syntactic items 
were controllers and targets at the same time. 
Endocentric hypotaxis (selection) 
The endocentric hypotaxis corresponds to the tradi- 
tional notion of selection. Even though not directly 
observable, it underlies specific morphosyntactic 
realities interpreted in Hierarchy 7 as resulting from 
a cross-classification with the combinatorial syn- 
tagmatic relation types. 
The traditional notion of subcategorisation call thus 
be viewed as a selection that is realised via govern- 
mont. Two general oplions are usually available 
across languages lbr externalising the governed 
selection (i.e. subcategorisation) of nominal catego- 
ries in actual syntactic onstructions. 
syntagmatics 
combinatorial structural 
co-variatien centricity taxis 
asymmetric symmemc juxtaposition govemmem ot,ao exo nypo L)ara 
i 
seleclion 
\[ agreement I \[matching marking SL~bcalogonsatlon 
cress-referencing relational case 
object eliticisation 
Hierarchy 7: Selection 
A typical definition of Ihe firsl one call be found in 
\[Blake94\]: "Case in its most een|ral manifestation is
a system of marking dependent nouns for the type 
of relationship they bear to their beads". The rela- 
tional case explicating case government stands in 
opposition to tile so-called concordial case ob- 
served in ease-governed modification environments 
and presented in Hierarchy 8. The second option to 
externalise subcatcgorisation f nominal categories 
is the cross-referencing the syntactic function of the 
dependent at the head. It is actually confined to 
certain core grammatical relations and typically 
amounts to some kind of pronominal representation 
of these grammatical relations at tile head. As 
\[Blake94\] observes, the cross-referencing pronomi- 
nal elements erve as an alternative to case ill sig- 
nalling grammatical relations. In Slavic, there are 
two candidales for the second /ype of externalising 
a governed selection. On tile ()he hand, lh0 verb 
inl\]cction can possibly be interpreted as cross- 
referencing the sub.icct fnnetion, especially in Bul- 
garian where no relational case is realised on the 
dependent. On the other band, pronominal elitics 
can cross-reference the direct and the indirect object 
in Bulgarian verb complex, cf. \[Avg97a\], as well as 
the possessor relation in Bulgarian noun phrases, cf. 
\[Avg97b\]. Therefore, the systematic relation of 
object cliticisation can be viewed as a more specific 
instance of cross-referencing. In general, a nominal 
category representing a grammatical relation that is 
cross-referenced at the head selecting this nominal 
category need not be overtly realised. So, the cross- 
referenced noun phrase controlling the agreement 
can typically be omitted. 
In our view, the systematic relation of marking 
(Hierarchy 7) is an instance of juxtaposed selection, 
i.e. all endocentrie hypotaxis that is realised via 
juxtaposition. We promote a faMy extensive under- 
standing of this syntactic relation as involving vari- 
ous functional categories, including auxiliaries, 
particles, determiners, prepositions, conjunctions, 
etc. The notion of morphosyntactic marking is in- 
troduced in \[Avg97a\] for syntagmatic relations 
30 
holding in Bulgarian analyiic verb lbrms between 
the main verb as a syntactic (and semantic) head 
and the possibly nlultiplc auxiliary verbs ;is nlai+kers 
specifying it. The agreement between the verb and 
its subject or conlplcmcnt is interpreted in our tax- 
onotny as a selection real\]seal via asymmetric o- 
variation (agreement 1). It typically occurs in con\]- 
bination with the relation of subcatcgorisation (of. 
Table 1) which in languages mnploying relational 
case is realiscd as case assignment, while it\] those 
cn\]l+~loying cross-reforencing ;is a syntactic function 
identification. 
ent l iy  :i 
Ii(?ltll \] 
INDliX ~\] 
ii ,Nt,+xl,  
;';','&Ic,,+,+ I++l 
INI)I+X\[II 
Ilma ',wa,vor tzp I i II:AI)ICASE \ [ \ ]  I \[NI)EX \[1~ j 
........ ~B\] 
INI)F:X 
('oFuht 
INH NtIMItER\[I\]\] 
? GI~NI)IiR ~2 \] 
entity Ii 
l ,.,,,.b I INI:I,\[I~\] 
lthraval'-h,xe,u , 
IIEAI)\]I+I!X I:t)Ie.MANT III:ADICASF{2\] 
IN )  X\[\] 
"cliticized" i'ed~ 
('l l'llCS / III!AI)ZCASEI2\]\] 
" '  " \IINI)I!X~ \] 1'"') 
'clilicized" holm 
,++,< <:, 
\L tNDExLU 1/ 
copttla 
INFI.\[1 \] 
predicative - ilolltl 
INIII~X\[ NUMItI'R~I\] \] 
predicatire adjecfire 
INI'I NIIMBI!I{~j\] 
" {;I.:NDiiR ~\] / 
systenmilc rdi l t l ttns 
relational case 
agreement 1 with subject a 
subcategori.valion 
agreement 1 with sttbiect a
cro.vs-r@wem:ing 
agreement 1 with expcricnccr a 
cross-re./brencin,q, 
agree lne l l \ [  \] with contplol / /Ot l l  il 
cros.v-r@,rencing 
agreemcni 1with specifier a
SITvle langu'lges 
Russian, l'olish, Czech 
l lulgarian 
relatimud case Polish, Czech 
agreement 1 with subject "l 
,~ttbcaleL, or i ,va l ioH |/ulgal'ian 
agreement 1 with subject a 
Polish, Czech 
rehltio.al case 
agrcenlenl  1 with conipietncnt I} 
\] I ulgaritlll 
, s 'ubcategor i ,~a l io l l  
t tg rccn lent  \] ,,villi con lp lcntcnt  \]l 
Table 1: Subcategorisation a d agroenlent I 
entity a entity b 
\[ ,,,,i,,,,. I \[,,,,,.z, I INFI,\[li \] \[INFi,\[2}\] 
systematic rehttions 
marking 
malching 
s l lbg ;a legor i so I iO l l  
agreement 1 with complcnlcnt b 
Slavic hulgtulges 
lhtlgarian 
? (l~,ttssian, Czech, Polish) 
l{ussian, Czech, Polish 
Table 2: Marking and matdfillg 
Finally, what we call matching corresponds to a 
selection rcaliscd via symmetric co-variation. Its 
most typical instance can be found where there is 
compatibility in person, number or gender between 
(possibly multiple) auxiliaries and a main verb. 
Matching usually co-occut's with a 1nat+king rchition 
(cf. Table 2) which, as shown in Hierarchy 7, is 
interpreted its a juxlaposcd selection. 
Exocent r i c  hypotax is  (modi f icat ion)  
The exoccntric hypotaxis corresponds to the mtdi- 
tional notion of modification. It nnderlies specific 
morphosyntactic realities resulling from a cross- 
classification with combinatorial syntagmalic rela- 
tion types (cf. Hierarchy 8). In all of then\], we are 
confronted with a +'elation of subordination in 
which, however, there is no indisputable prominent 
clcmenL 
It+ general we assume that there is no "case agree- 
ment". Rather, the regular compatibility of case 
specifications between the itwolved syntaclic items 
is due to a modification relation realised in a gov-  
erned environment. Ix+ other words, we cat\] regard 
concordial case as a typical instance of a governed 
modification. The asymmetric co-variation real\]sing 
a modification relation can be called concord, but 
let us refer to it - for the sake of sitnplicity - as 
ctgreement 2. In the majority of Slavic languages, 
but obviously not in Bulgarian, this relation occurs 
in combination with concordial case (of. Table 3). 
Note that in our approach the treat+nun+ of nominal 
apposition would be parallel to that of the adjective- 
noun relations. 
syntaomatics 
combinatorial structural 
centricity co-vanarton 
asyRlll\]olec symmemc juxtaposition govemelenz endo 
taXiS 
exo nypo para 
l~odif~calion 
agreement 2 simile uxlaposed modificationtion governed modification \] 
tconcomt 
(case) adjunction secondary predicalion concord@ case 
predicative case adjunction 
I licrarchy 8: Modification 
The main difference between tl~e agt'eement 1
(Hierarchy 7) and the agreement 2 (or concord) 
discussed here amounts to tile fact that these co- 
variation relations exhibit different ccntricity. 
Cross-classifying cxoccntric hypotaxis with sym- 
metric co-vat\]alien results in what cat+ be called 
simile and is typically observed in comparative 
31 
constructions, provided appropriate categories are 
available. This systematic relation differs fl'om that 
of parallelism (distinguished in Hierarehy 10) in 
being hypotactic in nature, and thus, an actual in- 
stance of modification. Similarly to the asymmetric 
agreement 2 (concord), tim simile relation co-occurs 
with concordial case, cf. ex. 6. The systematic rela- 
tion of (case) adjunction is an instance of juxta- 
I)osed mod(fication. 
entity a entity 1) 
adjective noun 
\[CASI'\[~ \] IIEAIIlCASli \[~ 
\] 
systematic relations 
cmlcordial case 
ngreement 2 (concord) 
Shtvic hmguages 
Russian, Polish, Czech 
governed mod~'c'ation Bulgarian 
agreement 2 (concord) 
com'ordial case Russian, Polish, Czech 
agreement 2 (concord) 
governed mod(fication Bulgarian 
ngreement 2 (concord) 
Table 3: Concordial case and agreement 2 (concord) 
Interestingly, the well-known "instrumental" prob- 
lem - i.e. whether we are confi'onted with a com- 
plement or a free adjt, nc t -  narrows down in our 
approach to a fluctuation between adjunction 
(juxtaposed modification - Hierarchy 8) and sub- 
categorisation (governed selection - Hierarchy 7), 
with the crucial point being merely a different ccn- 
tricity interpretation. Also the secondaly predica- 
tion (referring, typically, to the relation holding 
between a verb and a secondary controlled predica- 
tive) is a subtype of juxtaposed modification, with 
the predicative case adjunction as a more specific 
instance. As to the relation holding between the 
secondary predicative and the subject or the object, 
it is an instance o1' control and presupposes co- 
reference. The latter two concepts realise an endo- 
ccntric parataxis and are introduced in Hierarchy 9. 
Endocentric parataxis 
In the endoccntric parataxis there is a prominent 
item but no subordination relation. This allows us to 
model concepts like co-reference, correlation, co- 
marking (illustrated by ex. 6) and control as natu- 
rally resulting from a cross-classification with the 
combinatorial syntagmatic relation types. 
If an endocentric parataxis is revealed by an asym- 
metric co-variation, this results in co-reference. This 
systematic relation is tbund in relativisation (i.e. 
between a nominal category and the relative pro- 
noun introducing a relative clause that modifies this 
nominal category), in resumption (i.e. between a 
nominal category and the pronominal element re- 
suming it in a different syntactic domain), and in 
binding (i.e. between a pronoun and its antecedent). 
When, however, an endocentric parataxis is re- 
vealed in a symmetric o-variation, we can speak of 
correlation. But in both instances of co-variation, 
we arc confronted with paMng indices (or restricted 
parameters) el' referential objects. What we propose 
to distinguish as co-marking corresponds to endo- 
centrie parataxis that is realised via juxtaposition. 
So, it contrasts with the systematic relation of 
marking (presented in Hierarchy 7) only along the 
taxis dimension of structural syntagmatics inasmuch 
as there is no subordination relation between the 
involved syntactic items. As to the systematic rela- 
tion of syntactic ontrol, it is registered in our tax- 
enemy as an cndocentrie parataxis resulting in a 
for,n government. In Bulgarian, it co-occurs with 
co-reference - cx. 3 and ex. 4. 
syntagmaties 
combinatorial srrucmra 
co-variation centricity taxis 
asymmernc symmetric luxraposliion government enao exo Ilypo \[;ara 
en~o-Dara 
co-referenoe correlallon co marking conlml 
rEativisalion resumption binding 
Ilierarchy 9: Endoccntric parataxis 
Exocentric parataxis 
The cxoeentric parataxis is the actual tnunarkcd 
case: there is neither a prominent dement nor a 
subordination relation belwcen the involved syntac- 
tic items. A cross-classification with combinatorial 
syntagmatic relation types allows us to encode fur- 
ther phenomena that are shown Hierarchy 10. 
syntagmatics 
t 
combinatorial structural 
\ t 
co-variation '\, centricity taxis 
\ 
asymmetric symmetric juxtaposibbn government endo exo hy#o para 
, \ 
,, , , 
exo-para ) 
\ 
agreement 3 \[ parallersm ' coordination co-dependence I (accord) I J \[ \] 
Hierarchy I0: Exocenlrie paralaxis 
The relation of co-dependence plays a crucial role 
in a number of constructions. It is an exoeentric 
parataxis that is realised via government, with a 
special requirement that all involved syntactic items 
have the same governor. In other words, these items 
are typically dependcnts of the same syntactic head. 
What we call agreement 3 (or accord) corresponds 
to an exocentric paralaxis that is rcaliscd via asym- 
metric co-variation. It regularly presupposes a co- 
32 
dependence relation (of. Table 4), and its most 
typical instance can be fotmd as a COlnpatibility in 
number or gender between the subject and the 
predicative in copular constructions. Another in- 
stance is lhe co-dependence relation holding be- 
lween a dependent of the primary predicate (i.e. the 
verb) and a secondary predicative in ex. 1 and cx. 2. 
When exocentric parataxis is externalised by a 
symmetric co-variation, we are confronted with 
parallelism. It co-occurs in ex. 5 with co- 
dependence. 
The coordinatioll relation is generally interprelable 
as an exocemric paratactic juxtaposition. 
mmn NUMBI{R~\[ 
INI)EX (;I!NI,ER\[2 l\] 
. . . . . . . .  l\] 
INI)FX \[GENDI{I{ ~\] 1\[ 
predicative noun 
predicative-adjective l 
INFI \[NI3MIIER0\]\] \[ 
........ 
INI)I:X \[NUMBH~ 
adjective 1 
~'stcma(ic relations 
co-d('/)glldetlce 
agreement 3 (accord) between subject a and 
complement b 
co-dependence 
agreement 3 (accord) between subject / 
object a and secondary predicalive I} 
~ s 
(Russian ?), V, ulgarian, 
Polish, Czech 
Russian, Bulgarian, t'olish, 
Czech 
Table 4: Co-dependence and a ;reement 3 (accold) 
Conclusion and prospects  
We have p,esented an approach of computational 
grammar design tlmt st, pports the notion of gram- 
mar sharing and, moreover, lends itself to the for- 
real linguistic description of individual anguages as 
well as language families. The basic building blocks 
of  such a grammar  were demonst ra ted  with the 
example  of  S lav ic  hmguagcs .  Grammars  of  this type 
can s t ra ight forward ly  be extended and employed in 
,q nt ln lber  o\[" deve lop lncnt  and rLiiiI.illle systems 
accommodating HPSG. Some of these systems have 
reached a parsing efficiency thai makes them suited 
for a wu'iety of applications, \]F()TU2000\[. Al- 
though the original motiwltion for the work calllO 
from applied research, the insights thai were gained 
on the differences between Slavic languages, led lo 
new results in comparative linguistic dcscrip/ion. 
We expecl that psycholinguistic research on bilin- 
gt, alism and second language acquisition will 
greatly benefit from opportunities of modelling 
shared grammatical knowledge. The insights gained 
by such models will in turn be useful for CALL 
applications and for the computational treatment of 
cross - language interference in gl'ananlar and style 
checking. 
References 
\[ASU99\] Avgustinova, T., W. Sktlt an(I 1|. Uszkoreit. Typologi- 
cal similarilies in ttPSG: a case sltldy orl Slavic verb dialhc- 
sis. In I'rzcpi6rkowski, A. and R. Borsley (cd.) Slavic in 
ItPSG. CSLI 1999:1-28 
Sample  mmlyses (relat ional 
ex. 1 (Russian) 'She turned out a healthy girl.' 
Ona relational case \[NOM\] 
3SG.F.NOM agreement 1 SG.I'\[ 
okazalas' 
turncd-SG. F 
zdorovoj 
healthy-SG.F.INST 
\[Avg97a\] Avguslinova, T. Word order and clilics in Bulgarian. 
Saarbri.icken l)isscrlalions in Computational IAnguislics and 
Imnguagc Technology, Volume 5, 1997 
\[Avg97b\] Avgustinova, T. Clustering clitics in Bulgarian nomi- 
nal constituents. In: Proceedings of FDSL-2, Potsdam 1997 
1BO1'881 Bemovfi, A., K. Oliwt and J. Panevovfi. Some prob- 
lems of machine lranslalion between closely related lan- 
guages. In: l'roceedings ofCO1ANG'88, Budapcsl 1988 
\[Blake94\] Blake, FL,I. Case. Cambridge Tcxlbooks in Linguis- 
tics. Cambridge University Press, 1994 
\[Cann931 Cann, R. l'atlems of  hcadedncss In: Corbcll, G., N. 
Fraser and S. McGlashan (ed.) tlcads in grammatical lhc- 
ory. Cambrktge Universily Iqvss, 1993.44-72 
\[Cor98\] Corl)cll, G. Agreement in Slavic. I'osition paper for Ihc 
workshol~ "Comparative Slavic Morphosyntax: The Slate of 
Ihe Art", Indiana University, 1998 
\[FOTU2000\] Flickingcr, I)., S. Ocpcn, J. Tsujii, and II. 
Uszkorci! (cd.).(in press) .Iournal of Nalural l~.lnguage t;,n- 
gincering. Special Issue on lffficicnt Processing with IfI'SG. 
Volume 6 (I). Cambridge, UK: Cambridge Univcrsily 
Press, 2000. 
\[GIJ'R97\] Gamon, M., .C. Lozano, .I. Pinkham and T. Reutler. 
I'raclical experience with grammar sharing in inullilingual 
NI,I'. Technical report MSR-TR-97-16, Redmond 1997 
\[Kam88\] Kanleyama, M. Alomizalion in grammar sharing. In: 
l'rocccdings of 261h Ammal Meeting of ACL, New York 
1988 
\[Pin96\] l'iukham, J. Grammar Sharing in I"mnch and linglish. 
In: l'roccedings of IANLP 1996 
\[S&1,95\] Schmidl, I'. and W. Lchfeldt. Kongruenz- Rektion - 
AdjunkliotL Systcmalischc trod historisehe Unlcrstlchtmgen 
zur allgcmcinen Morphosynlax und ztt den Wortfiigungcn 
(slovoso~%tat/ija) im Russischen. Specimina Philologiae 
S lavicae. Miinchen: Otto Sagner 1995 
char ts )  
co-dependence 
agreement 3 (accord) \[SG.F\] 
relational case \[INST\] 
agreement 1 ISG.FI 
concordial case \[INST\] 
agrccnlcnt 2 (concord) \[SG.F\] 
devo~koj. 
gM-SG.F.INST 
33 
ex. 2 (Russian) 'They ordered him to come washed.' 
O/ti relational case \[NOMI 
3PL.NOM lagrecmcnt I \[PL\] 
veleli 
ordercd-PL 
relational case \[DAT\] 
cx. 3 (Bulgarian) 'John saw Mary ill (reportedly).' 
Ivan-3SG.MIVan 
ja 
ACC.SG.F 
ex. 4 (Bulgarian) 'You would come disguised (reportedly).' 
elllll 
3SG.M.I)AT 
ot~iect cliticisation 
vidjal 
saw-3SG.M 
subcategorisation \[INF\] ...... 
control co-dependence 
agreement 3 (accord) \[SG.M\] 
prijti predicative case adjunction \[INST\] 
come-lNF 
cross-rel~rencing 
agreement I \[3SG.F\] 
uno,O,m. 
washcd-SG.M.lNST 
subcategorisation 
Maria 
Mary-SG.F 
Ti ...... subcatcgorisation 
2SG i agreement I \[2SG\] 
si marking 
AUX.2SG matching \[2SG.F\] 
gtjala 
AUX-SG.F 
ex. 5 (Polish) '1 consider him to be nice / to be a fool.' 
Uwa~am relational case IACC\] 
consider-lSG 
go 
3SG.M.ACC 
control 
co-rclizrence \[SG.F\] 
secondary predication 
control 
co-rcl)rcnce \[SG.FI 
bolua. 
ilI-SG.F 
control 
co-rcl~rence \[SG\] 
marking control 
matching \[2SG.F\] co-rcli.zrcnce \[SG.F\] 
i i 
da marking 
particle 
dojdeg 
come-2SG 
secondary predication 
maskiraua. 
disguiscd-SG.F 
relational case \[prepositional ACCI 
co-dependence 
parallelism \[SG.M / 3SG.M\] 
Za marking 
lbr 
milego / durnia. 
nicc-SG.M.ACC / lbol-3SG.M.ACC 
cx. 6 (Russian) 'I suffered lbr him as for a son.' 
l j,, re at on  caseINOMl : 
1SO Ilagreement 1 \[sG\] ............. : . . . . .  . . . .  
Z~ 
i2~r 
relational case \[prepositional ACC\] 
nego 
3SG.M.ACC 
stradala 
sufl?rcd-SG.F 
marking 
a@mction 
kak  
as 
CO=lllflrking 
concordial case \[ACC\] 
.... simile \[3SG.M\] 
mark n  
I 
Zll 
.~ylt l 
son-3SG.M.ACC 
34 
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 37?40,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Named Entity and Relation Identification System 
 
 
Tianfang Yao 
Department of Computer Science and 
Engineering 
Shanghai Jiao Tong University 
Shanghai, 200030, China 
yao-tf@cs.sjtu.edu.cn 
Hans Uszkoreit 
Department of Computational Linguistics and 
Phonetics 
Saarland University 
Saarbr?cken, 66041, Germany 
uszkoreit@coli.uni-sb.de 
 
  
 
Abstract 
In this interactive presentation, a Chinese 
named entity and relation identification 
system is demonstrated. The domain-
specific system has a three-stage pipeline 
architecture which includes word seg-
mentation and part-of-speech (POS) tag-
ging, named entity recognition, and 
named entity relation identitfication. The 
experimental results have shown that the 
average F-measure for word segmenta-
tion and POS tagging after correcting er-
rors achieves 92.86 and 90.01 separately. 
Moreover, the overall average F-measure 
for 6 kinds of name entities and 14 kinds 
of named entity relations is 83.08% and 
70.46% respectively. 
1 Introduction 
The investigation for Chinese information ex-
traction is one of the topics of the project COL-
LATE (DFKI, 2002) dedicated to building up the 
German Competence Center for Language Tech-
nology. The presented work aims at investigating 
automatic identification of Chinese named enti-
ties (NEs) and their relations in a specific domain.  
Information Extraction (IE) is an innovative 
language technology for accurately acquiring 
crucial information from documents. NE recog-
nition is a fundamental IE task, that detects some 
named constituents in sentences, for instance 
names of persons, places, organizations, dates, 
times, and so on. Based on NE recognition, the 
identification of Named Entity Relation (NER) 
can indicate the types of semantic relationships 
between identified NEs. e.g., relationships be-
tween person and employed organization; person 
and residing place; person and birthday; organi-
zation and seat, etc. The identified results for 
NEs and NERs can be provided as a resource for 
other application systems such as question-
answering system. Therefore, these two IE tasks 
are selected as our investigation emphases. 
Chinese has a very different structure from 
western languages. For example, it has a large 
character set involving more than 48,000 charac-
ters; there is no space between words in written 
texts; and Chinese words have fewer inflections, 
etc. In the past twenty years there have been sig-
nificant achievements in IE concerning western 
languages such as English. Comparing with that, 
the research on the relevant properties of Chinese 
for IE, especially for NER, is still insufficient. 
Our research focuses on domain-specific IE. 
We picked the sports domain, particularly, texts 
on soccer matches because the number and types 
of entities, relations and linguistic structures are 
representative for many applications. 
Based on the motivations above mentioned, 
our goals for the design and implementation of 
the prototype system called CHINERIS (Chinese 
Named Entity and Relation Identification System) 
are: 
? Establishing an IE computational model 
for Chinese web texts using hybrid tech-
nologies, which should to a great extent 
meet the requirements of IE for Chinese 
web texts; 
? Implementing a prototype system based 
on this IE computational model, which 
extracts information from Chinese web 
texts as accurately and quickly as possi-
ble; 
? Evaluating the performance of this sys-
tem in a specific domain. 
37
2 System Design 
In the model, the IE processing is divided into 
three stages: (i) word segmentation and part-of-
speech (POS) tagging; (ii) NE recognition; (iii) 
NER identification. Figure 1 demonstrates a Chi-
nese IE computational model comprised of these 
three stages. Each component in the system cor-
Figure 1. A three-s
responds to a stage. 
tage Chinese IE computa-
In general, the e first stage has 
co
e 
di
al., 2000). 
Th
ication performance for 
N
 defined a hierarchical tax-
on
mentation 
During the implementation, object-oriented de-
sign and programming methods are thoroughly 
tional model. 
 accuracy of th
nsiderable influence on the performance of the 
consequent two stages. It has been demonstrated 
by our experiments (Yao et al, 2002). In order to 
reduce unfavorable influence, we utilize a train-
able approach (Brill, 1995) to automatically gen-
erate effective rules, by which the first compo-
nent can repair different errors caused by word 
segmentation and POS tagging.  
At the second stage, there are two kinds of NE 
constructions to be processed (Yao et al, 2003). 
One is the NEs which involve trigger words; the 
other those without trigger words. For the former 
NEs, a shallow parsing mechanism, i.e., finite-
state cascades (FSC) (Abney, 1996) which are 
automatically constructed by sets of NE recogni-
tion rules, is adopted for reliably identifying dif-
ferent categories of NEs. For the latter NEs, 
however, some special strategies, such as the 
valence constraints of domain verbs, the con-
stituent analysis of NE candidates, the global 
context clues and the analysis for preposition 
objects etc., are designed for identifying them.  
After the recognition for NEs, NER identifica-
tion is performed in the last stage. Because of th
versity and complexity of NERs, at the same 
time, considering portability requirement in the 
identification, we suggest a novel supervised 
machine learning approach called positive and 
negative case-based learning (PNCBL) used in 
this stage (Yao and Uszkoreit, 2005).  
The learning in this approach is a variant of 
memory-based learning (Daelemans et 
e goal of that is to capture valuable informa-
tion from NER and non-NER patterns, which is 
implicated in different features. Because not all 
features we predefine are necessary for each 
NER or non-NER, we should select them by a 
reasonable measure mode. According to the se-
lection criterion we propose - self-similarity, 
which is a quantitative measure for the concen-
trative degree of the same kind of NERs or non-
NERs in the corresponding pattern library, the 
effective feature sets - General-Character Feature 
(GCF) sets for NERs and Individual-Character 
Feature (ICF) sets for non-NERs are built. More-
over, the GCF and ICF feature weighting serve 
as a proportion determination of feature?s degree 
of importance for identifying NERs against non-
NERs. Subsequently, identification thresholds 
can also be determined.  
Therefore, this approach pursues the im-
provement of the identif
ERs by simultaneously learning two opposite 
cases, automatically selecting effective multi-
level linguistic features from a predefined feature 
set for each NER and non-NER, and optimally 
making an identification tradeoff. Further, two 
other strategies, resolving relationship conflicts 
and inferring missing relationships, are also inte-
grated in this stage. 
Considering the actual requirements for do-
main knowledge, we
omy and constructed conceptual relationships 
among Object, Movement and Property concept 
categories under the taxonomy in a lexical sports 
ontology (Yao, 2005). Thus, this ontology can be 
used for the recognition of NEs with special con-
structions - without trigger words, the determina-
tion of NE boundaries, and the provision of fea-
ture values as well as the computation of the se-
mantic distance for two concepts during the iden-
tification of NERs. 
3 System Imple
Word Seg. and 
POS Tag. 
NE  
Recognition 
NER 
Identification 
Error Repair 
Resources 
Texts from Internet or Disk 
Word Seg. and 
POS Tag. 
Resources 
Texts with Word Seg. 
and POS Tags 
NER Identi-
fication  
Resources 
NE-Recognized Texts 
NE Recogni-
tion 
Resources
Lexical 
Ontology 
NER-Identified Texts 
38
used in the system development. In order to 
eriments for testing 
three components. Table 1 shows the experimen-
r f these compo-
avoid repeated development, we integrate other 
application system and resource, e.g., Modern 
Chinese Word Segmentation and POS Tagging 
System (Liu, 2000) and HowNet (Dong and 
Dong, 2000) into the system. Additionally, we 
utilize Prot?g?-2000 (version 1.9) (Stanford 
Medical Informatics, 2003) as a development 
environment for the implementation of lexical 
sports ontology. 
The prototype system CHINERIS has been 
implemented in Java. The system can automati-
cally identify 6 types of NEs1 and 14 types of 
NERs 2  in the sports domain. Furthermore, its 
run-time efficiency is acceptable and the system 
user interfaces are friendly. 
4 Testing and Evaluation 
We have finished three exp
tal esults for the performance o
nents.  
Stage Task (Total ) Ave. Rec. 
(Total) 
Ave. Pre. 
(Total) 
Ave. F-M
Word Seg. 95.08 90.74 92.86 
1st  
POS Tag. 92.39 87.75 90.01 
2nd N  E Ident. 83.38 82.79 83.08 
3rd  NER Ident. 78.50 63.92 70.46 
Table 1. Performance for the Sy  CHINERIS. 
In the first experiment, the training set consists 
of 94 texts including 3473 sentences collected 
                                                
stem
from the soccer matches of the Jie Fang Daily 
(http://www.jfdaily.com/) in 2001. During man-
ual error-correction, we adopted a double-person 
annotation method. After training, we obtain er-
ror repair rules. They can repair at least one error 
in the training corpus. The rules in the rule li-
brary are ranked according to the errors they cor-
rect. The testing set is a separate set that contains 
20 texts including 658 sentences. The texts in the 
 
entity identification which in-
cl
ing learning. They 
ha
1 Personal Name (PN); Date or Time (DT); Location Name 
(LN); Team Name (TN); Competition Title (CT); Personal 
Identity (PI). 
2 Person ? Team (PS_TM); Person ? Competition 
(PS_CP); Person ? City / Province / Country (PS_CPC); 
Person ? Identification (PS_ID); Home Team ? Visiting 
Team (HT_VT); Winning Team ? Losing Team (WT_LT); 
Draw Team ? Draw Team (DT_DT); Team ? Competi-
tion (TM_CP); Team ? City / Province / Country 
(TM_CPC); Identification ? Team (ID_TM); Competition 
? Date (CP_DA); Competition ? Time (CP_TI); Competi-
tion ? Location (CP_LOC); Location ? City / Province / 
Country (LOC_ CPC). 
testing set have been randomly chosen from the 
Jie Fang Daily from May 2002. In the testing, the 
usage of error repair rules with context con-
straints has priority over those without context 
constraints, and the usage of error repair rules for 
word segmentation has priority over those for 
POS tagging. Through experimental observation, 
this processing sequence can ensure that the rules 
repair many more errors. On the other hand, it 
can prevent new errors occurring during the re-
pair of existing errors. The results indicate that 
after the correction, the average F-measure of 
word segmentation has increased from 87.75 % 
to 92.86%; while that of POS tagging has even 
increased from 77.47% to 90.01%. That is to say, 
the performance of both processes has been dis-
tinctly enhanced. 
In the second experiment, we utilize the same 
testing set for the error repair component to 
check the named 
udes regular and special entity constructions. 
The rule sets provided for TN, CT, and PI recog-
nition have 35, 50, and 20 rules respectively. In 
lexical sports ontology, there are more than 350 
domain verbs used for the identification of TN 
with special constructions. Among six NEs, the 
average F-measure of DT, PI, and CT exceeds 
85%. Therefore, it specifies that the identifica-
tion performance of named entities after adding 
the special recognition strategies in this compo-
nent has reached a good level. 
In the third experiment, both pattern libraries 
are established in terms of the annotated texts 
and lexical sports ontology dur
ve 142 (534 NERs) and 98 (572 non-NERs) 
sentence groups respectively. To test the per-
formance of our approach, we randomly choose 
32 sentence groups from the Jie Fang Daily in 
2002 (these sentence groups are out of either 
NER or non-NER pattern library), which em-
body 117 different NER candidates. Table 1 
shows the total average recall, precision, and F-
measure for 14 different NERs by positive and 
negative case-based learning and identification. 
Among 14 types of NERs, the highest total aver-
age F-measure is 95.65 from the relation 
LOC_CPC and the lowest total average F-
measure is 34.09 from TM_CPC. The total aver-
age F-measure is 70.46. In addition, we also 
compared the performance between the total av-
erage recall, precision, and F-measure for all 
NERs only by positive and by positive and nega-
tive case-based learning and identification sepa-
rately. It shows the total average F-measure is 
enhanced from 63.61% to 70.46% as a whole, 
39
due to the adoption of both positive and negative 
cases.  
From the result, we also realize that the selec-
tion of relation features is critical. First, they 
should be selected from multiple linguistic levels, 
e.g
 
ap effective for Chinese named 
y cation in sports domain. 
wo
constraint 
sy
 
sit
Es, identi-
fic
 successful for the sample ap-
pl
 is a part of the COLLATE project un-
01B, which is supported 
ry for Education and Re-
search. 
g Workshop, pages 8-15. Prague, Czech Re-
 Transformation-Based Error-Driven 
W ans, A. Bosch, J. Zavrel, K. Van der Sloot, 
Netherlands. 
DF
Z. wNet. 
K.
T. . Ding, and G. Erbach. 2002. Correcting 
29-36. 
T. 
), 
T.
., morphology, syntax and semantics. Second, 
they should also embody the crucial information 
of Chinese language processing, such as word 
order, the context of words, and particles etc. 
Moreover, the proposed self-similarity is a rea-
sonable measure for selecting GCF and ICF for 
NERs and non-NERs identification respectively. 
5 Conclusion 
This three-stage IE prototype system CHINERIS
is propriate and 
entit  and relation identifi
In the first component, it is a beneficial explo-
ration to develop an error repairer which simul-
taneously enhances the performance of Chinese 
rd segmentation and POS tagging.  
In the second component, we theoretically ex-
tend the original definition of Finite State Auto-
mata (FSA), that is, we use complex 
mbols rather than atomic constraint symbols. 
With this extension, we improve the practicabil-
ity for the FSC mechanism. At the same time, the 
new issue for automatically constructing FSC 
also increases the flexibility of its maintenance. 
In order to improve the NE identification per-
formance, some special strategies for the identi-
fication of NEs without trigger words are added 
in this stage, which cannot be recognized by FSC.
In the third component, automatically select-
ing effectual multi-level linguistic features for 
each NER and non-NER and learning two oppo-
e types of cases simultaneously are two inno-
vative points in the PNCBL approach. 
The lexical sports ontology plays an important 
role in the identification of NEs and NERs, such 
as determination of the boundary of N
ation for NE with special constructons and 
calculation of similarity for the features (e.g. se-
mantic distance). 
The experimental results for the three compo-
nents in the prototype system show that the sys-
tem CHINERIS is
ication. 
Acknowledgement 
This work
der contract no. 01INA
by the German Minist
References 
S. Abney. 1996. Partial Parsing via Finite-State Cas-
cades. In Proceedings of the ESSLLI ?96 Robust 
Parsin
public. 
E. Brill. 1995.
Learning and Natural Language  Processing: A 
Case Study in Part of Speech Tagging. Computa-
tional Linguistics, 21(4): 543-565.  
. Daelem
and A. Vanden Bosch. 2000. TiMBL: Tilburg 
Memory Based Learner, Version 3.0, Reference 
Guide. Technical Report ILK-00-01, ILK, Tilburg 
University. Tilburg, The 
http://ilk.kub.nl/~ilk/papers/ilk0001.ps.gz. 
KI. 2002. COLLATE: Computational Linguistics 
and Language Technology for Real Life Applica-
tions. DFKI, Saarbr?cken, Germany. 
http://collate.dfki. de/. 
 Dong and Q. Dong. 2000. Ho
http://www.keenage.com/zhiwang/e_zhiwang.html. 
 Liu. 2000. Automatic Segmentation and Tagging 
for Chinese Text. The Commercial Press. Beijing, 
China. 
Stanford Medical Informatics. 2003. The Prot?g? On-
tology Editor and Knowledge Acquisition System. 
The School of Medicine, Stanford University. 
Stanford, USA. http://protege.stanford.edu/. 
 Yao, W
Word Segmentation and Part-of-Speech Tagging 
Errors for Chinese Named Entity Recognition. In G. 
Hommel and H. Sheng, editors, The Internet Chal-
lenge: Technology and Applications, pages 
Kluwer Academic Publishers. The Netherlands. 
Yao, W. Ding and G. Erbach. 2003. CHINERS: A 
Chinese Named Entity Recognition System for the 
Sports Domain. In: Proc. of the Second SIGHAN 
Workshop on Chinese Language Processing (ACL 
2003 Workshop), pages 55-62. Sapporo, Japan.  
T. Yao and H. Uszkoreit. 2005. A Novel Machine 
Learning Approach for the Identification of Named 
Entity Relations. In: Proc. of the Workshop on Fea-
ture Engineering for Machine Learning in Natural 
Language Processing (ACL 2005 Workshop
pages 1-8. Michigan, USA. 
 Yao. 2005. A Lexical Ontology for Chinese Infor-
mation Extraction. In M. Sun and Q. Chen, editors, 
Proc. of the 8th National Joint Symposium on 
Computational Linguistics (JSCL-2005), pages 
241-246. Nanjing, China. 
40
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 43?48,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Language Technology from a European Perspective 
 
 
Hans Uszkoreit, 
Valia Kordoni 
Vladislav Kubon Michael Rosner Sabine Kirchmeyer-
Andersen 
Dept. of Computational 
Linguistics 
UFAL MFF UK Dept. of Computer Sci-
ence and A.I. 
Dept. of Computational 
Linguistics 
Saarland University Charles University University of Malta Copenhagen Business 
School 
D-66041, Saarbruecken, 
Germany 
Prague, Czech Republic Msida, Malta Copenhagen, Denmark 
{uszkoreit, 
kordoni}@coli.uni-
sb.de 
vk@ufal.mff.cuni.cz mike.rosner 
@um.edu.mt 
ska.id@cbs.dk 
 
 
 
 
Abstract 
This paper describes the cooperation of 
four European Universities aiming at at-
tracting more students to European master 
studies in Language and Communication 
Technologies. The cooperation has been 
formally approved within the framework 
of the new European program ?Erasmus 
Mundus? as a Specific Support Action in 
2004. The consortium also aims at creat-
ing a sound basis for a joint master pro-
gram in the field of language technology 
and computer science. 
1 European higher education: Erasmus 
Mundus 
The Erasmus Mundus programme [1] is a co-
operation and mobility program in the field of 
higher education. It aims to enhance quality in 
European higher education and to promote inter-
cultural understanding through co-operation with 
non-EU countries. 
The program is intended to strengthen European 
co-operation and international links in higher edu-
cation by supporting high-quality European Mas-
ters Courses, by enabling students and visiting 
scholars from around the world to engage in post-
graduate study at European universities, as well as 
by encouraging the outgoing mobility of European 
students and scholars towards non-EU countries. 
The Erasmus Mundus program comprises four 
concrete actions: 
 
ACTION 1 - Erasmus Mundus Masters Courses: 
high-quality integrated courses at masters level 
offered by a consortium of at least three universi-
ties in at least three different European countries.  
 
ACTION 2 - Erasmus Mundus scholarships: a 
scholarship scheme for non-EU-country graduate 
students and scholars from the whole world. 
 
ACTION 3 - Partnerships: Erasmus Mundus Mas-
ters Courses selected under Action 1 also have the 
possibility of establishing partnerships with non-
EU-country higher education institutions.  
 
ACTION 4 - Enhancing attractiveness: projects 
aimed at enhancing the attractiveness of the Euro-
pean higher education.  
2 LATER 
One of the projects approved for funding (and the 
only one in the field of language technology) in the 
2004 call is called LATER ? Language Technol-
ogy Erasmus Mundus [2]. 
LATER falls under action 4 of the program and 
hence addresses the need to enhance the attractive-
ness of European higher education in Language 
43
Technology and Communication (LCT). This need 
will be met through dissemination of the combined 
LCT-related expertise in of a consortium of Uni-
versities whose members are as follows 
 
Saarland University in Saarbruecken (CoLi) 
The Department of Computational Linguistics 
and Phonetics (CoLi) of Saarland University (co-
ordinator) has an excellent international reputation 
for graduate training in Language Technologies, 
and for leading-edge basic research in this area. 
CoLi offers a new M.Sc. program in Language 
Science and Technology [3]. This is an active pro-
gram of basic, applied and cognitive research, 
which combines with state-of-the-art facilities to 
provide students with a rich and stimulating envi-
ronment for their research. Moreover, CoLi offers 
a European Ph.D. program in Language Technol-
ogy and Cognitive Systems. In the past 15 years, 
CoLi has provided postgraduate research training 
to 100 early-stage researchers [4]. 
 
Charles University, Prague (?FAL) 
The Institute of Formal and Applied Linguistics 
(?FAL) at the Faculty of Mathematics and Physics 
of the Charles University in Prague offers a five-
year master program in Computer Science with 
several specialized branches. One of the branches 
of this program is the masters in Computational 
and Formal Linguistics [7]. It focuses mainly on 
the following four topics: formal description of 
natural language, grammars and automata in lin-
guistics, methods of artificial intelligence in lin-
guistics, as well as methods of automatic natural 
language processing.  
 
University of Malta (UoM) 
The Department of Computer Science and Arti-
ficial Intelligence at the University of Malta, estab-
lished in 1993, teaches both Bachelors and Masters 
degree programs. The 4-year BSc. (Hons) scheme 
include several streams relevant to Language 
Technology including NLP and Computational 
Linguistics itself, Information Retrieval, Semantic 
Web, Internet and Agent technologies. The De-
partment also runs a, one-year research oriented 
M.Sc. program [10]. The areas of specialization 
include the development of computational tools, 
techniques and resources for Maltese, the only se-
mitic language to enjoy official EU status.  
 
Copenhagen Business School (CBS) 
The Department of Computational Linguistics 
is part of the Faculty of Modern Languages at the 
Copenhagen Business School. The Department is 
actively involved in research in the following four 
core fields: formal descriptions of the Danish lan-
guage, modeling of knowledge relevant for LSP, 
LSP databases, and Machine Translation. Embed-
ded in this context is the Master of Language Ad-
ministration (MLA) [9] that the Department of 
Computational Linguistics of the Copenhagen 
Business School offers in co-operation with the 
University of Southern Denmark in Roskilde  
3 Overall aims of the project 
The overall aim of the project is to export the 
common educational experience currently embod-
ied within existing Masters programs of the con-
sortium to scholars and students of non-EU 
countries. 
This aim will be realized by several different 
classes of activity under the rubrics of (i) work-
shops (ii) distance learning tools and (iii) coordina-
tion of a common Master program. We discuss 
these in the following sections. 
3.1 Workshops 
One of the most important types of activities of 
the project is organizing workshops and courses 
both for students from non-EU countries and for 
their teachers. The effect of these events is at least 
twofold ? the students from countries or regions 
which do not have an access to any higher degree 
education in LCT get a chance to broaden their 
perspective by listening to lectures of prominent 
scientists and lecturers. The courses will also help 
the consortium to establish better contacts with 
non-EU Universities, teachers, and students which 
will turn out to be invaluable when disseminating 
the common European Master program in Lan-
guage Technology discussed further below. 
Both ?FAL and CoLi have a long tradition in 
respect of offering such courses to students from 
the broadest possible range of countries. 
?FAL has devoted a huge effort in the past to 
raise funding for the organization, once or twice a 
year, of a series of lectures by prominent scientists 
and lecturers from all over the world. This series of 
lectures, the Vilem Mathesius courses [6], have 
become well-known, especially among the Central 
44
and East European students of computational and 
general linguistics.  
This year?s course, held in March under the aus-
pices of LATER, was able to support  the atten-
dance of 50 students from Russia, Ukraine, 
Albania, Bosnia, Serbia, Croatia and Georgia to 
lectures by prominent individuals including two 
ACL award winners. 
At CoLi, the Computational Linguistics Collo-
quium is also a traditional event attracting the at-
tention of both well-known lecturers and a number 
of master and postgraduate students from various 
countries. A second series of lectures in the frame 
of our project was held at the University of Saar-
landes in Saarbruecken in January. 
A third event, organized by the CBS, will take 
place in June. The first day consists of information 
seminar on content management and language 
technology to promote CBS? newly-launched In-
ternational Master of Language Administration, 
whilst the second will be devoted to diffusion of a 
various issues connected to the Erasmus Mundus 
course.  
Finally, a fourth event, in the form of a work-
shop with invited guest lecturers, is being organ-
ized at the University of Malta that will take place 
in September 2005. The theme of the workshop 
will be Machine Translation which is currently 
very topical given the newly-achieved official 
European status that the local language now enjoys.  
3.2 Coordination of Masters Programs 
A second important aim of the LATER project 
is the definition, coordination and implementation 
of an integrated European Masters Programme in 
LCT by creating a common basis that will appeal 
to both European and non-EU students. 
The rationale behind the creation of such a pro-
gramme is the assumption that LCT now occupies 
a central position in research and education in 
Europe, being a key enabling technology for nu-
merous applications related to the information so-
ciety, although the shortage of qualified 
researchers and developers is slowing down the 
speed of innovation in Europe. 
The proposed programme addresses this short-
age by creating a directed education and training 
opportunity for the next generation of LCT innova-
tors in that will in turn bring educational, social 
and economic benefits. Some specific aims of 
Erasmus Mundus are also addressed: European 
education in LCT will be promoted worldwide and 
its competitiveness increased, increasing at the 
same time the competitiveness of European IT in-
dustries, creating a multilingual information soci-
ety that is accessible for all, and turning the 
``information overload'' into a wealth of accessible 
and useful knowledge. 
3.3 Distance learning tools 
A third aim of LATER is the development of 
effective methods of hosting and integrating non-
EU students, for example by developing distance 
learning tools and joint distance education modules, 
in order to facilitate outreach by online dissemina-
tion of courses. An example of such modules, as 
well as for computer-based tools, is being devel-
oped on the basis of the virtual courses CoLi has 
developed in the last 3 years in the framework of 
the MiLCA project (Medienintensive Lehrmodule 
in der Computerlinguistik-Ausbildung1). 
We also plan to explore the use of collaboration 
technologies based on Sitescape [16], that have 
been developed at CBS for academic collaboration, 
for the management of certain aspects of the pro-
posed Masters programme.
The fruits of various initiatives already under 
way at UoM will be exploited and extended during 
the life of the proposed course. These include in-
teractive web based course delivery [13], just-in-
time support based on P2P architectures [14], 
XML-based frameworks for online courses [15], 
the latter being developed within as a part of the 
Mediterranean Virtual University (MVU) 
EUMEDIS project [17]. 
 
4 Integrated European LCT Masters 
Programme 
Whilst many agree with the above assessment of 
the importance of LCT, they disagree on the defi-
nition of ?integrated course?. Fortunately, we can 
turn to the comprehensive definition supplied by 
the EU call, the central element of which is ?a 
jointly developed curriculum or full recognition by 
the consortium of modules which are developed 
                                                          
1 for more see http://milca.sfs.uni-
tuebingen.de/index.html. 
45
and delivered separately, but make up a common 
standard Masters course.? 
Again, some turn away in horror at the notion of 
a standard curriculum in this area, the claim being 
that there is already enough standardization in the 
world, so why add to it? The point is, any pro-
gramme dealing with LCT has to address the fact 
that it is highly interdisciplinary, including, at the 
core, computer science, computational and theo-
retical linguistics, and mathematics, and at the pe-
riphery, a wide variety of other subjects including 
electrical engineering, psychology, cognitive sci-
ence artificial intelligence etc.  
With such a large number of disciplines in-
volved, it is practically impossible for a single 
University to excel in all of them. However if more 
than one University is involved, various kinds of 
curriculum sharing can be envisaged and so a 
much higher level of coverage becomes entirely 
achievable.  
Put another way, curriculum sharing, together 
with common admission and assessment proce-
dures envisaged, allows delivery of a complex 
course to be handled by what is effectively a ?su-
peruniversity?. 
4.1 Integration in practice 
To put this idea into practice we are proposing 
that students will get the chance to attend a two 
years? master program at two universities chosen 
from a larger consortium, which is currently being 
put together. It includes the four original partners 
of the LATER project and the following new part-
ners: University of Amsterdam (UvA) in the Neth-
erlands, Free University of Bolzano-Bozen (FUB) 
in Italy, the Universities of Nancy 1 and Nancy 2 
in France, Roskilde University in Denmark and 
Utrecht University in the Netherlands. 
Studying in multi-national groups at two uni-
versities in Europe, with English as instruction 
language, accompanied by language classes in an-
other European language, will contribute to the 
students' preparation for the increasing globaliza-
tion of science, commerce and industry. The 
course also will also prepare students for follow-up 
Ph.D. studies provided by the participating partners 
and others. 
The proposed programme follows the Bologna 
model for higher education in Europe and com-
prises 120 ECTS2 credits, 30 of which make up the 
Masters dissertation, and 90 of which are course-
work credits structured as follows: 
? Compulsory modules in Computer Science (28 
ECTS) 
? Compulsory modules in Language Technology 
(28 ECTS) 
? Advanced modules in Language Technology, 
Computational Linguistics and Computer Sci-
ence (34 ECTS) 
Coursework is distributed over three semesters, 
while the dissertation is supposed to be completed 
in the fourth semester  
It is important to underline that this structure 
permits a considerable degree of variation. First, a 
module might be ?implemented? by different set of 
courses at different Universities. Secondly, the ad-
vanced modules are electives, based on the specific 
strengths in research and teaching of individual 
partner institutions. There is no requirement that 
the advanced modules offered by different Univer-
sities should coincide. 
Let us now introduce individual modules in 
more detail. Parentheses indicate ECTS credits. 
Computer Science Modules 
The Computer Science Modules are as follows:  
 
? Logic, Computability and Complexity (? 9) 
Topics: Logic & inference; Computability the-
ory; Complexity theory; Discrete mathematics 
? Formal Languages and Algorithms (? 9) 
Topics: Formal grammars and languages hier-
archy; Parsing and compiler design; Search 
techniques and constraint resolution; Auto-
mated Learning 
? Data Structures, Data Organization and 
Processing (? 6) 
Topics: Algebraic data types; Relational data-
bases; Semi-structured data and XML; Informa-
tion retrieval; Digital libraries 
? Advanced Modules and Applications(? 6) 
Topics: Artificial Intelligence, Knowledge 
?epresentation, Automated Reasoning, 
Semantic Web, Neural Networks, Machine 
Learning etc. Students are expected to obtain at 
least 9 ECTS credits from each of the first two 
                                                          
2 European Credit Transfer System: a standard measure that is 
used in Europe for comparing the size of courses. 
46
modules and 6 ECTS credits from each of the 
remaining two modules.  
Language Technology Modules 
The Language Technology Modules are these: 
? Foundations of Language Technology (? 6) 
Topics: Statistical methods; Symbolic methods; 
Cognition; Corpus Linguistics; Text and 
speech; Foundations of Linguistics 
? Computational Syntax and Morphology (? 9) 
Topics: Finite state methods; Probabilistic ap-
proaches; Formal grammars; Tagging; Chunk-
ing; Parsing 
? Computational Semantics, Pragmatics and 
Discourse (? 6) 
Topics: Syntax-semantics interface; Semantic 
construction; Dialogue; Formal semantics 
? Advanced Modules and Applications 
(? 6) Topics: Machine Translation, Informa-
tion Retrieval, Speech Recognition, Question 
Answering, Psycholinguistics etc.. 
4.2 Main issues to be addressed  
Although it was not explicitly mentioned in the 
previous text, the integration of existing master 
programmes is done exclusively pair-wise. The 
students can?t study at three universities (although 
the rules of the Erasmus Mundus programme allow 
such triangular cooperation). The restrictions 
within our consortia go even further ? the students 
do not have a free choice of a combination of any 
two universities from within the consortium, they 
must choose one of the pairs offered by the consor-
tium. 
The reason for such a restriction is pretty simple 
- it turned out that although all members of the 
consortia in principle provide education both in 
Computer Science and in Computational Linguis-
tics, they differ in the balance between these two 
fields. Within the consortium, there are universities 
with a strong stress on a Computer Science courses, 
aiming at a complex education including the sound 
theoretical background in the field, while other 
universities offer a more practically oriented edu-
cational scheme, stressing the concepts attracting a 
wider audience, e.g. various types of web tech-
nologies, databases, data mining etc.  
As a result of this, each university participates in 
an average of four bilateral partnerships. We think 
that the fact that the consortium consists of univer-
sities which are not identical greatly increases the 
variety of options available. They have a chance to 
choose those universities which are best suited to 
their preferences whether these are in terms of sub-
ject area emphasis or geographical region.  
The preparation of the integrated Master pro-
gramme doesn?t stop at matching the universities 
and lectures offered. Erasmus Mundus is not just a 
cooperation, it is really a completely new scheme 
which must also address practical issues as grades, 
examination procedures, admission procedure, tui-
tion fees, defense of the thesis, local specialties 
existing at some partner universities etc.
The proposed Masters programme is something 
new. It is the first attempt to create a comprehen-
sive Masters degree in this subject area that con-
forms to all the legalistic requirements of each 
participating University. Students completing the 
course will possess a Masters degree delivered by 
two of the participant Universities. This is in con-
trast to the existing European Master in Language 
and Speech [11], which is implemented through a 
certification procedure that does not replace any 
legal degree that a student may obtain from a Uni-
versity.
 
5 Conclusion 
Although the process of establishing a new Euro-
pean Master programme in Language Technology 
was really very complicated, time consuming and 
painful, there are definitely already at this stage 
very positive results. 
In order to submit a proposal, our consortium 
has managed to overcome all formal and structural 
differences among all partners, it has found a rea-
sonable model of cooperation, it has developed a 
high-quality master programme open both to Euro-
pean and non-EU students. 
The wide variety of modules and topics offered 
combined with a relatively high degree of freedom 
of choice for students allows for individual pairs of 
partner universities to promote those courses and 
fields in which they excel. The students are of 
course offered individual guidance from consor-
tium members in order to allow them to identify 
that pair of universities which best suits their indi-
vidual needs and preferences 
47
The strategy we have chosen ? the initial coop-
eration of a smaller consortium in the LATER pro-
ject, promoting LTC education among the students 
from outside the EU and testing our ability both to 
offer a coordinated high-quality education and to 
attract a reasonable amount of interested students, 
has turned to be a sound one. It also helped to 
solve some issues in the larger consortium based 
on the experience from the smaller one. 
References  
[1] http://europa.eu.int/comm/education
/programmes/mundus/index_en.html 
(Erasmus Mundus web page) 
[2] http://europa.eu.int/comm/education
/programmes/mundus/projects/2004/47
.pdf (The description of the LATER pro-
ject) 
[3] http://www.coli.uni-
saarland.de/msc/ (the MSc website at 
the University of Saarlandes in Saar-
bruecken) 
[4] http://www.coli.uni-
saarland.de/kvv/ (courses at the Dept. 
of Computational Linguistics at the Uni-
versity of Saarlandes in Saarbruecken) 
[5] http://www.coli.uni-
saarland.de/courses/late2/ (the web 
page of the Language Technology II 
course in Saarbruecken) 
[6] http://ufal.mff.cuni.cz/vmc/vmc_ls2
0.html (the web page of the Vilem 
Mathesius Lecture Series) 
[7] http://www.mff.cuni.cz/toUTF8.en/st
udium/bcmgr/ok/i1b53.htm (the master 
programme in Mathematical Linguistics at 
the Charles University in Prague) 
[8] http://web.cbs.dk/stud_pro/clmdatau
k.shtml (the master program at the Co-
penhagen Business School) 
[9] http://uk.cbs.dk/mla (Master of Lan-
guage Administration at the Copenhagen 
Business School) 
[10] http://www.cs.um.edu.mt/rese
arch/pgEnquiries.html (the master 
program at the University of Malta) 
[11] http://www.cstr.ed.ac.uk/e
uromasters (European Masters in 
Language and Speech) 
[12] A.Burchardt, S. Walter and M. 
Pinkal. 2004. "MiLCA -- Distance Educa-
tion in Computational Linguistics". In 
Szucs, Andras and Bo, Ingeborg 
(eds.),  New Challenges and Partnerships 
in an Enlarged European Union ? Proc. 
2004 EDEN Conference, Budapest, pp. 
351-356. 
[13] Ellul, C., 2002, ?Just-in-Time Lec-
ture Delivery, Management and Student 
Support System?, BSc. Project report, 
Dept. CSAI, University of Malta. 
[14] Bezzina, R., 2002, ?Peer-to-Peer 
Just-in-Time Support for Curriculum based 
Learning?, BSc. Project report, Dept. 
CSAI, University of Malta. 
[15] Cachia, E., and Micallef, M., forth-
coming, ?A Universal XML/XSLT 
Framework for Online Courses?, Proc. In-
ternational Conference  on IT-Based 
Higher Education And Training (ITHET)?, 
Dominican Republic. 
[16] www.sitescape.com : SiteScape 
corporate website. 
[17] http://www.eumedis.net/en/project/
22: Mediterranean Virtual University 
(MVU) description. 
48
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 1?8,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Novel Machine Learning Approach for the Identification of 
Named Entity Relations 
 
 
 
Tianfang Yao Hans Uszkoreit 
Department of Computer Science and 
 Engineering 
Department of Computational Linguistics and 
Phonetics 
Shanghai Jiao Tong University Saarland University 
Shanghai, 200030, China Saarbruecken, 66041, Germany 
yao-tf@cs.sjtu.edu.cn uszkoreit@coli.uni-sb.de 
 
 
 
Abstract 
In this paper, a novel machine learning 
approach for the identification of named 
entity relations (NERs) called positive 
and negative case-based learning 
(PNCBL) is proposed. It pursues the im-
provement of the identification perform-
ance for NERs through simultaneously 
learning two opposite cases and auto-
matically selecting effective multi-level 
linguistic features for NERs and non-
NERs. This approach has been applied to 
the identification of domain-specific and 
cross-sentence NERs for Chinese texts. 
The experimental results have shown that 
the overall average recall, precision, and 
F-measure for 14 NERs are 78.50%, 
63.92% and 70.46% respectively. In addi-
tion, the above F-measure has been en-
hanced from 63.61% to 70.46% due to 
adoption of both positive and negative 
cases. 
1 Introduction 
The investigation for Chinese information extrac-
tion is one of the topics of the project COLLATE 
dedicated to building up the German Competence 
Center for Language Technology. After accom-
plishing the task concerning named entity (NE) 
identification, we go on studying identification 
issues for named entity relations (NERs). As an 
initial step, we define 14 different NERs based on 
six identified NEs in a sports domain based Chi-
nese named entity recognition system (Yao et al, 
2003). In order to learn NERs, we annotate the 
output texts from this system with XML. Mean-
while, the NER annotation is performed by an in-
teractive mode.  
The goal of the learning is to capture valuable 
information from NER and non-NER patterns, 
which is implicated in different features and helps 
us identify NERs and non-NERs. Generally speak-
ing, because not all features we predefine are im-
portant for each NER or non-NER, we should 
distinguish them by a reasonable measure mode. 
According to the selection criterion we propose - 
self-similarity, which is a quantitative measure for 
the concentrative degree of the same kind of NERs 
or non-NERs in the corresponding pattern library, 
the effective feature sets - general-character feature 
(GCF) sets for NERs and individual-character fea-
ture (ICF) sets for non-NERs are built. Moreover, 
the GCF and ICF feature weights serve as a pro 
portion determination of the features? degree of 
importance for identifying NERs against non-
NERs. Subsequently, identification thresholds can 
also be determined. 
 In the NER identification, we may be confronted 
with the problem that an NER candidate in a new 
case matches more than one positive case, or both 
positive and negative cases. In such situations, we 
have to employ a vote to decide which existing 
1
case environment is more similar to the new case. 
In addition, a number of special circumstances 
should be also considered, such as relation conflict 
and relation omission. 
2 Definition of Relations 
An NER may be a modifying / modified, dominat-
ing / dominated, combination, collocation or even 
cross-sentence constituent relationship between 
NEs. Considering the distribution of different 
kinds of NERs, we define 14 different NERs based 
on six identified NEs in the sports domain shown 
in Table 1. 
 
Table 1. NER Category 
 
In order to further indicate the positions of NEs 
in an NER, we define a general frame for the 
above NERs and give the following example using 
this description: 
 
Definition 1 (General Frame of NERs):  
NamedEntityRelation (NamedEntity1, Paragraph-
SentenceNamedEntityNo1; NamedEntity2, Para-
graphSentenceNamedEntityNo2) 
 
Example 1:  
 
?????1??????????????? 
The Guangdong Hongyuan Team defeated the Guangzhou 
Taiyangshen Team by 3: 0 in the guest field. 
 
In the sentence we observe that there exist two 
NERs. According to the general frame, the first 
NER description is HT_VT( ? ? ? ? ? ?
(Guangzhou Taiyangshen Team), 1-1-2; ????
?(Guangdong Hongyuan Team), 1-1-1) and the 
other is WT_LT( ? ? ? ? ? (Guangdong 
                                                          
1 The underlining of Chinese words means that an NE consists of these words.
Hongyuan Team), 1-1-1; ?????(Guangzhou 
Taiyangshen Team), 1-1-2). 
In this example, two NERs represent dominating 
/ dominated and collocation relationships sepa-
rately: namely, the first relation HT_VT gives the 
collocation relationship for the NE ?Guangdong 
Hongyuan Team? and the noun ?guest field?. This 
implies that ?Guangdong Hongyuan Team? is a 
guest team. Adversely, ?Guangzhou Taiyangshen 
Team? is a host team; the second relation WT_LT 
indicates dominating / dominated relationship be-
tween ?Guangdong Hongyuan Team? and 
?Guangzhou Taiyangshen Team? by the verb ?de-
feat?. Therefore, ?Guangdong Hongyuan Team? 
and ?Guangzhou Taiyangshen Team? are the win-
ning and losing team, respectively. 
NER Cate-
gory Explanation 
PS_TM The membership of a person in a sports team. 
PS_CP A person takes part in a sports competition. 
PS_CPC The origin location of a person. 
PS_ID A person and her / his position in a sports team or other occasions. 
HT_VT The home and visiting teams in a sports competition. 
WT_LT The winning and losing team name in a sports match. 
DT_DT The names of two teams which draw a match. 
TM_CP A team participates in a sports competition. 
TM_CPC It indicates where a sports team comes from. 
ID_TM The position of a person employed by a sports team. 
CP_DA The staged date for a sports competition. 
CP_TI The staged time for a sports competition. 
CP_LOC It gives the location where a sports match is held. 
LOC_ CPC The location ownership (LOC belongs to CPC). 
3 Positive and Negative Case-Based 
Learning 
The positive and negative case-based learning 
(PNCBL) belongs to supervised statistical learning 
methods (Nilsson, 1996). Actually, it is a variant of 
memory-based learning (Stanfill and Waltz, 1986; 
Daelemans, 1995; Daelemans et al, 2000). Unlike 
memory-based learning, PNCBL does not simply 
store cases in memory but transforms case forms 
into NER and non-NER patterns. Additionally, it 
stores not only positive cases, but also negative 
ones. Here, it should be clarified that the negative 
case we mean is a case in which two or more NEs 
do not stand in any relationships with each other, 
i.e, they bear non-relationships which are also in-
vestigated objects in which we are interested. 
During the learning, depending on the average 
similarity of features and the self-similarity of 
NERs (also non-NERs), the system automatically 
selects general or individual-character features 
(GCFs or ICFs) to construct a feature set. It also 
determines different feature weights and identifica-
tion thresholds for different NERs or non-NERs. 
Thus, the learning results provide an identification 
references for the forthcoming NER identification. 
3.1 Relation Features 
Relation features, by which we can effectively 
identify different NERs, are defined for capturing 
critical information of the Chinese language. Ac-
cording to the features, we can define NER / non-
2
NER patterns. The following essential factors mo-
tivate our definition for relation features: 
 
? The relation features should be selected 
from multiple linguistic levels, i.e.,  mor-
phology, grammar and semantics (Cardie, 
1996);  
? They can help us to identify NERs using 
positive and negative case-based machine 
learning as their information do not only 
deal with NERs but also with non-NERs; 
and 
? They should embody the crucial information 
of Chinese language processing (Dang et al, 
2002), such as word order, the context of 
words, and particles etc. 
 
There are a total of 13 relation features shown 
in Table 2, which are empirically defined accord-
ing to the above motivations. It should be ex-
plained that in order to distinguish feature names 
from element names of the NER / non-NER pat-
terns, we add a capital letter ?F? in the ending of 
feature names. In addition, a sentence group in 
the following definitions can contain one or mul-
tiple sentences. In other words, a sentence group 
must end with a stop, semicolon, colon, exclama-
tion mark, or question mark. 
 
Feature 
Category Explanation 
SGTF The type of a sentence group in which there exists a relation. 
NESPF The named entities of a relevant relation are located in the same sentence or different sentences. 
NEOF The order of the named entities of a relevant relation. 
NEVPF 
The relative position between the verbs and the named 
entities of a relevant relation. The verbs of a relevant 
relation mean that they occur in a sentence where the 
relation is embedded. 
NECF 
The context of named entities. The context only embod-
ies a word or a character preceding or following the 
current named entity. 
VSPF The verbs are located in the same sentence or different sentences in which there is a relevant relation. 
NEPPOF 
The relative order between parts-of-speech of particles 
and named entities. The particles occur within the 
sentences where the relation is embedded. 
NEPF The parts-of-speech of the named entities of a relevant relation. 
NECPF The parts-of-speech of the context for the named enti-ties associated with a relation. 
SPF The sequence of parts-of-speech for all sentence con-stituents within a relation range. 
VVF The valence expression of verbs in the sentence(s) where there is a relation embedded. 
NECTF The concepts of the named entities of a relevant relation from HowNet (Dong and Dong, 2000). 
VCTF The concepts of the verbs of a relevant relation from HowNet. 
 
Table 2. Feature Category 
 
In 13 features, three features (NECF, NECPF 
and NEPF) belong to morphological features, three 
features (NEOF, SPF and SGTF) are grammatical 
features, four features (NEPPOF, NESPF, NEVPF 
and VSPF) are associated with not only morphol-
ogy but also grammar, and three features (NECTF, 
VCTF and VVF) are semantic features.  
Every feature describes one or more properties 
of a relation. Through the feature similarity calcu-
lation, the quantitative similarity for two relations 
can be obtained, so that we can further determine 
whether a candidate relation is a real relation. 
Therefore, the feature definition plays an important 
role for the relation identification. For instance, 
NECF can capture the noun ?? (the guest field, 
it means that the guest team attends a competition 
in the host team?s residence.) and also determine 
that the closest NE by this noun is ????? 
(the Guangdong Hongyuan Team). On the other 
hand, NEOF can fix the sequence of two relation-
related NEs. Thus, another NE ?????? (the 
Guangzhou Taiyangshen Team) is determined. 
Therefore, these two features reflect the properties 
of the relation HT_VT. 
3.2 Relation and Non-Relation Patterns 
A relation pattern describes the relationships be-
tween an NER and its features. In other words, it 
depicts the linguistic environment in which NERs 
exist. 
 
Definition 2 (Relation Pattern): A relation pat-
tern (RP) is defined as a 14-tuple: RP = (NO, RE, 
SC, SGT, NE, NEC, VERB, PAR, NEP, NECP, SP, 
VV, NECT, VCT) where NO represents the num-
ber of a RP; RE is a finite set of relation expres-
sions; SC is a finite set for the words in the 
sentence group except for the words related to 
named entities; SGT is a sentence group type; NE 
is a finite set for named entities in the sentence 
group; NEC is a finite set that embodies the con-
text of named entities; VERB is a finite set that in-
cludes the sequence numbers of verbs and 
corresponding verbs; NEP is a finite set of named 
entities and their POS tags; NECP is a finite set 
which contains the POS tags of the context for 
named entities; SP is a finite set in which there are 
the sequence numbers as well as corresponding 
POS tags and named entity numbers in a sentence 
group; VV is a finite set comprehending the posi-
3
tion of verbs in a sentence and its valence con-
straints from Lexical Sports Ontology which is 
developed by us; NECT is a finite set that has the 
concepts of named entities in a sentence group; and 
VCT is a finite set which gives the concepts of 
verbs in a sentence group. 
 
Example 2: 
 
????????????????????????
????????????????????????
????????????????????????
?????? 
According to the news from Xinhua News Agency Beijing on 
March 26th: National Football Tournament (the First B 
League) today held five competitions of the second round, 
The Guangdong Hongyuan Team defeats the Guangzhou 
Taiyangshen Team by 3: 0 in the guest field, becoming the 
only team to win both matches, and temporarily occupying 
the first place of the entire competition. 
 
Relation Pattern: 
NO = 34; 
RE = {(CP_DA, NE1-3, NE1-2), (CP_TI, NE1-3, NE1-4), ?, 
(WT_LT, NE2-1, NE2-2)} 
SC = {(1, ?, according_to, Empty, AccordingTo), (2, ??
? , Xinhua/Xinhua_News_agency, Empty, institu-
tion/news/ProperName/China), ?, (42, ? , ., Empty, 
{punc})}; 
SGT = multi-sentences; 
NE = {(NE1-1, 3, LN, {(1, ??)}), (NE1-2, 4, Date, {(1, ?), 
(2, ?), (3, ??), (4, ?)}), ..., (NE2-2, 26, TN, {(1, ??), 
(2, ???), (3, ?)})}; 
NEC = {(NE1-1, ???,?), (NE1-2, ??, ?), ..., (NE2-2, 
??, ?) }; 
VERB = {(8, ??), (25, ??), ..., (39, ?)} 
PAR = {(1, ?), (9, ?), ..., (38, ?)}; 
NEP = {(NE1-1, {(1, N5)}), (NE1-2, {(1, M), (2, N), (3, M), 
(4, N)}), ..., (NE2-2, {(1, N5), (2, N), (3, N)})}; 
NECP = {(NE1-1, N, M), (NE1-2, N5, N), ?, (NE2-2, V, 
W)}; 
SP = {(1, P), (2, N), (3, NE1-1), ..., (42, W)}; 
VV = {(V_8, {Agent|fact/compete|CT, -Time|time|DT}), 
(V_25, {Agent|human/mass|TN, Patient|human/mass|TN}),..., 
(V_39, {Agent|human/sport|PN, Agent|human/mass|TN})}; 
NECT = {(NE1-1, place/capital/ProperName/China), (NE1-2, 
Empty+celestial/unit/time+Empty+ celestial/time/time/ 
morning), ?, (NE2-2, place/city/ProperName/China+ 
Empty+community/human/mass)}; 
VCT = {(V_8, GoForward/GoOn/Vgoingon), (V_25, de-
feat), ?, (V_39, reside/situated)} 
 
Analogous to the definition of the relation pat-
tern, a non-relation pattern is defined as follows:  
 
Definition 3 (Non-Relation Pattern): A non-
relation pattern (NRP) is also defined as a 14-tuple: 
NRP = (NO, NRE, SC, SGT, NE, NEC, VERB, 
PAR, NEP, NECP, SP, VV, NECT, VCT), where 
NRE is a finite set of non-relation expressions 
which specify the nonexistent relations in a sen-
tence group. The definitions of the other elements 
are the same as the ones in the relation pattern. For 
example, if we build an NRP for the above sen-
tence group in Example 2, the NRE is listed in the 
following: 
 
NRE = {(CP_LOC, NE1-3, NE1-1), (TM_CPC, NE2-1, 
NE1-1), ..., (DT_DT, NE2-1, NE2-2)} 
 
In this sentence group, the named entity (CT) ?
??????? (National Football Tournament 
(the First B League)) does not bear the relation 
CP_LOC to the named entity (LN) ?? (Beijing). 
This LN only indicates the release location of the 
news from Xinhua News Agency. 
As supporting means, the non-NER patterns also 
play an important role, because in the NER pattern 
library we collect sentence groups in which the 
NER exists. If a sentence group only includes non-
NERs, obviously, it is excluded from the NER pat-
tern library. Thus the impact of positive cases can-
not replace the impact of negative cases. With the 
help of non-NER patterns, we can remove misiden-
tified non-NERs and enhance the precision of NER 
identification. 
3.3 Similarity Calculation 
In the learning, the similarity calculation is a ker-
nel measure for feature selection.  
 
Definition 4 (Self-Similarity): The self-similarity 
of a kind of NERs or non-NERs in the correspond-
ing library can be used to measure the concentra-
tive degree of this kind of relations or non-relations. 
The value of the self-similarity is between 0 and 1. 
If the self-similarity value of a kind of relation or 
non-relation is close to 1, we can say that the con-
centrative degree of this kind of relation or non-
relation is very ?tight?. Conversely, the concentra-
tive degree of that is very ?loose?. 
The calculation of the self-similarity for the 
same kind of NERs is equal to the calculation for 
the average similarity of the corresponding relation 
features. Suppose R(i) is a defined NER in the 
NER set (1 ? i ? 14). The average similarity for 
this kind of NERs is defined as follows:  
 
                                               ? Sim (R(i)j, R(i)k)  
 1? j, k ? m; j ? k 
Simaverage(R(i)) =  ???????????                    (1)                                         
                                          Sumrelation_pair(R(i)j, R(i)k) 
 
where Sim (R(i)j, R(i)k) denotes the relation simi-
larity between the same kind of relations, R(i)j and 
4
R(i)k. 1 ? j, k ? m, j ? k; m is the total number of 
the relation R(i) in the NER pattern library. The 
calculation of Sim(R(i)j, R(i)k) depends on differ-
ent features. Sumrelation_pair(R(i)j, R(i)k) is the sum of 
calculated relation pair number. They can be calcu-
lated using the following formulas: 
 
                                   Sumf 
                               ? Sim (R(i)j, R(i)k) (ft)  
                                               t = 1 
Sim (R(i)j, R(i)k ) =  ???????????                (2)                                                            Sim                                   f(s)                                           Sumf
 
                                                                     1               m = 2 
      
Sumrelation_pair(R(i)j, R(i)k)  =              m !                              (3) 
                                                             ?????      m > 2                                         where R(i) is a defined relation in the NER set (1 ? 
i ? 14); n is the size of selected features, 1 ? s, t ? n; 
and 
                                                             (m-2) ! * 2 ! 
 
In the formula (2), ft is a feature in the feature 
set (1 ? t ? 13). Sumf is the total number of fea-
tures. The calculation formulas of Sim (R(i)j, R(i)k) 
(ft) depend on different features. For example, if ft 
is equal to NECF, Sim (R(i)j, R(i)k) (ft) is shown as 
follows: 
 
1  if all contexts of named  
entities for two relations 
                                                                are the same 
0.75 if only a  preceding or  
following context is not  
                                                                the same                                                         
Sim (R(i)
Sim (X(i)j, X(i)k) (NECF)  =       0.5      if two preceding and / or  
following contexts are 
                                                                not the same 
0.25     if three preceding and / or 
following contexts are 
                                               not the same 
0       if all contexts of named  
entities for two relations 
are not the same 
                                                                      (4) 
 
Notice that the similarity calculation for non-
NERs is the same as the above calculations.  
Before describing the learning algorithm, we 
want to define some fundamental conceptions re-
lated to the algorithm as follows: 
 
Definition 5 (General-Character Feature): If the 
average similarity value of a feature in a relation is 
greater than or equal to the self-similarity of this 
relation, it is called a General-Character Feature 
(GCF). This feature reflects a common characteris-
tic of this kind of relation. 
 
Definition 6 (Individual-Character Feature): An 
Individual-Character Feature (ICF) means its aver-
age similarity value in a relation is less than or 
equal to the self-similarity of this relation. This 
feature depicts an individual property of this kind 
of relation. 
 
Definition 7 (Feature Weight): The weight of a 
selected feature (GCF or ICF) denotes the impor-
tant degree of the feature in GCF or ICF set. It is 
used for the similarity calculation of relations or 
non-relations during relation identification.  
 
averagef(s)(R(i)) 
w(R(i)) = ?????????                                (5)                                         
                                        n 
                                       ? Simaveragef(t)(R(i)) 
                                      t = 1 
 
 
                                     ? Sim (R(i)j, R(i)k) (f(s)) 
                                      1? j, k ? m; j ? k 
Simaveragef(s)(R(i)) =  ???????????                 (6)                                        
                                              Sumrelation_pair(R(i)j, R(i)k) 
 
j, R(i)k) (f(s)) computes the feature simi-
larity of  the feature f(s) between same kinds of 
relations, R(i)j and R(i)k. 1 ? j, k ? m, j ? k; m is 
the total number of the relation R(i) in the NER 
pattern library. Sumrelation_pair(R(i)j, R(i)k) is the sum 
of calculated relation pair numbers, which can be 
calculated by the formula (3). 
 
Definition 8 (Identification Threshold): If a can-
didate relation is regarded as a relation in the rela-
tion pattern library, the identification threshold of 
this relation indicates the minimal similarity value 
between them. It is calculated by the average of the 
sum of average similarity values for selected fea-
tures: 
 
                                n 
                                        ? Simaveragef(t)(R(i)) 
                                       t  = 1                                         
            IdenThrh(R(i)) =  ????????                            (7) 
                                                        n              
       
where n is the size of selected features, 1 ? t ? n. 
Finally, the PNCBL algorithm is described as 
follows: 
1) Input annotated texts; 
2) Transform XML format of texts into internal 
data format; 
3) Build NER and non-NER patterns; 
4) Store both types of patterns in hash tables 
and construct indexes for them; 
5
5) Compute the average similarity for features 
and self-similarity for NERs and non-NERs; 
6) Select GCFs and ICFs for NERs and non-
NERs respectively; 
7) Calculate weights for selected features;  
8) Decide identification thresholds for every 
NER and non-NER;  
9) Store the above learning results.  
4 Relation Identification  
Our approach to NER identification is based on 
PNCBL, it can utilize the outcome of learning for 
further identifying NERs and removing non-NERs. 
4.1 Optimal Identification Tradeoff 
During the NER identification, the GCFs of NER 
candidates match those of all of the same kind of 
NERs in the NER pattern library. Likewise, the 
ICFs of NER candidates compare to those of non-
NERs in the non-NER pattern library. The comput-
ing formulas in this procedure are listed as follows: 
 
                
Sum(GCF)
i
Sim (R(i)can, R(i)j1 ) =  ? { wi (GCFk1) * Sim (R(i)can, R(i)j1 ) (GCFk1) }   
                     k1 = 1                
and                                                                     (8) 
                              
Sum(ICF)
i
Sim (R(i)can, NR(i)j2 ) =  ? { wi (ICFk2) * Sim (R(i)can, NR(i)j2 ) (ICFk2) }  
                        k2 = 1                
                                                                           (9) 
where R(i) represents the NERi, and NR(i) ex-
presses the non-NERi, 1? i ? 14. R(i)can is defined 
as a NERi candidate. R(i)j1 and NR(i)j2 are the j1-th 
NERi in the NER pattern library and the j2-th non-
NERi in the non-NER pattern library. 1 ? j1 ? Sum 
(R(i)) and 1 ? j2 ? Sum (NR(i)). Sum (R(i)) and 
Sum (NR(i)) are the total number of R(i) in the 
NER pattern library and that of NR(i) in non-NER 
pattern library respectively. wi (GCFk1) and wi 
(ICFk2) mean the weight of the k1-th GCF for the 
NERi and that of the k2-th ICF for the non-NERi. 
Sum (GCF)i and Sum (ICF)i are the total number of 
GCF for NERi and that of ICF for non-NERi sepa-
rately. 
In matching results, we find that sometimes the 
similarity values of a number of NERs or non-
NERs matched with NER candidates are all more 
than the identification threshold. Thus, we have to 
utilize a voting method to achieve an identification 
tradeoff in our approach. For an optimal tradeoff, 
we consider the final identification performance in 
two aspects: i.e., recall and precision. In order to 
enhance recall, as many correct NERs should be 
captured as possible; on the other hand, in order to 
increase precision, misidentified non-NERs should 
be removed as accurately as possible.  
 The voting refers to the similarity calculation 
results between an NER candidate and NER / non-
NER patterns. It pays special attention to circum-
stances in which both results are very close. If this 
happens, it exploits multiple calculation results to 
measure and arrive at a final decision. Additionally, 
notice that the impact of non-NER patterns is to 
restrict possible misidentified non-NERs. On the 
other hand, the voting assigns different thresholds 
to different NER candidates (e.g. HT_VT, WT_LT, 
and DT_DT or other NERs). Because the former 
three NERs have the same kind of NEs, the identi-
fication for these NERs is more difficult than for 
others. Thus, when voting, the corresponding 
threshold should be set more strictly. 
4.2 Resolving NER Conflicts 
In fact, although the voting is able to use similarity 
computing results for yielding an optimal tradeoff, 
there still remain some problems to be resolved. 
The relation conflict is one of the problems, which 
means that contradictory NERs occur in identifica-
tion results. For example:  
(i) The same kind of relations with different ar-
gument position: e.g., the relations HT_VT,  
 
HT_VT(ne1, no1; ne2, no2) and HT_VT(ne2, no2; ne1, no1) 
occur in an identification result at the same time. 
 
(ii)  The different kinds of relations with same or 
different argument positions: e.g., the relations 
WT_LT and DT_DT,  
 
WT_LT(ne1, no1; ne2, no2) and DT_DT(ne1, no1; ne2, no2) 
appear simultaneously in an identification result. 
 
The reason for a relation conflict lies in the si-
multaneous and successful matching of a pair of 
NER candidates whose NEs are the same kind. 
They do not compare and distinguish themselves 
further. Considering the impact of NER and non-
NER patterns, we organize the conditions to re-
move one of the relations, which has lower average 
similarity value with NER patterns or higher aver-
age similarity value with non-NER patterns.  
4.3 Inferring Missing NERs 
6
Due to a variety of reasons, some relations that 
should appear in an identification result may be 
missing. However, we can utilize some of the iden-
tified NERs to infer them. Of course, the prerequi-
site of the inference is that we suppose identified 
NERs are correct and non-contradictory. For all 
identified NERs, we should first examine whether 
they contain missing NERs. After determining the 
type of missing NERs, we may infer them - con-
taining the relation name and its arguments. For 
instance, in an identification result, two NERs are: 
 
PS_ID (ne1, no1; ne2, no2) and PS_TM (ne1, no1; ne3, no3) 
 
In the above NER expressions, ne1 is a personal 
name, ne2 is a personal identity, and ne3 is a team 
name, because if a person occupies a position, i.e., 
he / she has a corresponding identity in a sports 
team, that means the position or identity belongs to 
this sports team. Accordingly, we can infer the fol-
lowing NER: 
 
ID_TM (ne2, no2; ne3, no3) 
5 Experimental Results and Evaluation 
The main resources used for learning and identifi-
cation are NER and non-NER patterns. Before 
learning, the texts from the Jie Fang Daily2 in 2001 
were annotated based on the NE identification. 
During learning, both pattern libraries are estab-
lished in terms of the annotated texts and Lexical 
Sports Ontology. They have 142 (534 NERs) and 
98 (572 non-NERs) sentence groups, respectively.  
To test the performance of our approach, we 
randomly choose 32 sentence groups from the Jie 
Fang Daily in 2002, which embody 117 different 
NER candidates.  
For evaluating the effects of negative cases, we 
made two experiments. Table 3 shows the average 
and total average recall, precision, and F-measure 
for the identification of 14 NERs only by positive 
case-based learning. Table 4 demonstrates those by 
PNCBL. Comparing the experimental results, 
among 14 NERs, the F-measure values of the 
seven NERs (PS_ID, ID_TM, CP_TI, WT_LT, 
PS_CP, CP_DA, and DT_DT) in Table 4 are 
higher than those of corresponding NERs in Table 
3; the F-measure values of three NERs (LOC_CPC, 
TM_CP, and PS_CP) have no variation; but the F-
measure values of other four NERs (PS_TM, 
                                                          
2 This is a local newspaper in Shanghai, China.
CP_LOC, TM_CPC, and HT_VT) in Table 4 are 
lower than those of corresponding NERs in Table 3. 
This shows the performances for half of NERs are 
improved due to the adoption of both positive and 
negative cases. Moreover, the total average F-
measure is enhanced from 63.61% to 70.46% as a 
whole. 
 
Relation 
Type 
Average 
Recall 
Average 
Precision 
Average 
F-measure
LOC_CPC 100 91.67 95.65 
TM_CP 100 87.50 93.33 
PS_ID 100 84.62 91.67 
PS_TM 100 72.73 84.21 
CP_LOC 88.89 69.70 78.13 
ID_TM 90.91 66.67 76.93 
CP_TI 83.33 71.43 76.92 
PS_CP 60 75 66.67 
TM_CPC 100 42.50 59.65 
HT_VT 71.43 38.46 50 
WT_LT 80 30.77 44.45 
PS_CPC 33.33 66.67 44.44 
CP_DA 0 0 0 
DT_DT 0 0 0 
Total Ave. 71.99 56.98 63.61 
  
Table 3:  Identification Performance for 14 NERs 
only by Positive Case-Based Learning 
 
Relation 
Type 
Average 
Recall 
Average 
Precision 
Average 
F-measure
LOC_CPC 100 91.67 95.65 
TM_CP 100 87.50 93.33 
CP_TI 100 75 85.71 
PS_CPC 100 68.75 81.48 
ID_TM 90.91 68.19 77.93 
PS_ID 72.22 81.67 76.65 
CP_LOC 88.89 66.67 76.19 
PS_TM 80 65 71.72 
CP_DA 100 50 66.67 
DT_DT 66.67 66.67 66.67 
PS_CP 60 75 66.67 
WT_LT 60 37.50 46.15 
HT_VT 42.86 30 35.30 
TM_CPC 37.50 31.25 34.09 
Total Ave. 78.50 63.92 70.46 
 
Table 4:  Identification Performance  
for 14 NERs by PNCBL 
 
Finally, we have to acknowledge that it is diffi-
cult to compare the performance of our method to 
others because the experimental conditions and 
corpus domains of other NER identification efforts 
are quite different from ours. Nevertheless, we 
would like to use the performance of Chinese NER 
identification using memory-based learning (MBL) 
(Zhang and Zhou, 2000) for a comparison with our 
approach in Table 5. In the table, we select similar 
NERs in our domain to correspond to the three 
types of the relations (employee-of, product-of, and 
location-of). From the table we can deduce that the 
7
identification performance of relations for PNCBL 
is roughly comparable to that of the MBL. 
 
Method Relation Type Recall Precision F-measure
employee-of 75.60 92.30 83.12 
product-of 56.20 87.10 68.32 MBL&I 
location-of 67.20 75.60 71.15 
PS_TM 
PS_CP 
PS_ID 
80 
60 
72.22 
65 
75 
81.67 
71.72 
66.67 
76.65 
ID_TM 
TM_CP 
90.91 
100 
68.19 
87.50 
77.93 
93.33 PNCBL&I 
CP_LOC 
PS_CPC 
TM_CPC 
88.89 
100 
37.50 
66.67 
68.75 
31.25 
76.19 
81.48 
34.09 
 
Table 5:  Performances for Relation Identification  
(PNCBL&I vs. MBL&I) 
6 Conclusion 
In this paper, we propose a novel machine learning 
and identification approach PNCBL&I. This ap-
proach exhibits the following advantages: (i) The 
defined negative cases are used to improve the 
NER identification performance as compared to 
only using positive cases;  (ii) All of the tasks, 
building of NER and non-NER patterns, feature 
selection, feature weighting and identification 
threshold determination, are automatically com-
pleted. It is able to adapt the variation of NER and 
non-NER pattern library; (iii) The information 
provided by the relation features deals with multi-
ple linguistic levels, depicts both NER and non-
NER patterns, as well as satisfies the requirement 
of Chinese language processing; (iv) Self-
similarity is a reasonable measure for the concen-
trative degree of the same kind of NERs or non-
NERs, which can be used to select general-
character and individual-character features for 
NERs and non-NERs respectively; (v) The strate-
gies used for achieving an optimal NER identifica-
tion tradeoff, resolving NER conflicts, and 
inferring missing NERs can further improve the 
performance for NER identification; (vi) It can be 
applied to sentence groups containing multiple sen-
tences. Thus identified NERs are allowed to cross 
sentences boundaries.  
The experimental results have shown that the 
method is appropriate and effective for improving 
the identification performance of NERs in Chinese. 
 
Acknowledgement 
This work is a part of the COLLATE project 
under contract no. 01INA01B, which is supported 
by the German Ministry for Education and Re-
search. 
References 
C. Cardie. 1996. Automating Feature Set Selection for 
Case-Based Learning of Linguistic Knowledge. In 
Proc. of the Conference on Empirical Methods in 
Natural Language Processing. University of Pennsyl-
vania, Philadelphia, USA. 
W. Daelemans. 1995. Memory-based lexical acquisition 
and processing. In P. Steffens, editor, Machine 
Translations and the Lexicon, Lecture Notes in Arti-
ficial Intelligence, pages 85-98. Springer Verlag. 
Berlin, Germany. 
W. Daelemans, A. Bosch, J. Zavrel, K. Van der Sloot, 
and A. Vanden Bosch. 2000. TiMBL: Tilburg Mem-
ory Based Learner, Version 3.0, Reference Guide. 
Technical Report ILK-00-01, ILK, Tilburg Univer-
sity. Tilburg, The Netherlands. 
http://ilk.kub.nl/~ilk/papers/ilk0001.ps.gz. 
H. Dang, C. Chia, M. Palmer and F. Chiou. 2002. Sim-
ple Features for Chinese Word Sence Disambigua-
tion. In Proc. of the 19th International Conference on 
Computational Linguistics (COLING 2002), pages 
204-210. Taipei, Taiwan. 
Z. Dong and Q. Dong. 2000. HowNet. 
http://www.keenage.com/zhiwang/e_zhiwang.html. 
N. Nilsson. 1996. Introduction to Machine Learning: An 
Early Draft of a Proposed Textbook. Pages 175-188. 
http://robotics.stanford.edu/people/nilsson/mlbook.ht
ml. 
C. Stanfill and D. Waltz. 1986. Toward memory-based 
reasoning. Communications of the ACM, Vol.29, 
No.12, pages 1213-1228. 
T. Yao, W. Ding and G. Erbach. 2003. CHINERS: A 
Chinese Named Entity Recognition System for the 
Sports Domain. In: Proc. of the Second SIGHAN 
Workshop on Chinese Language Processing (ACL 
2003 Workshop), pages 55-62. Sapporo, Japan.  
Y. Zhang and J. Zhou. 2000. A trainable method for 
extracting Chinese entity names and their relations. 
In Proc. of the Second Chinese Language Processing 
Workshop (ACL 2000 Workshop), pages 66-72. 
Hongkong, China. 
8
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 1?8,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
Contextual phenomena and thematic relations in database QA dialogues:
results from a Wizard-of-Oz Experiment
Nu?ria Bertomeu, Hans Uszkoreit
Saarland University
Saarbru?cken, Germany
uszkoreit|bertomeu@coli.uni-sb.de
Anette Frank, Hans-Ulrich Krieger and Brigitte Jo?rg
German Research Center of Artificial Intelligence
Saarbru?cken, Germany
frank|krieger|joerg@dfki.de
Abstract
Considering data obtained from a corpus
of database QA dialogues, we address the
nature of the discourse structure needed
to resolve the several kinds of contextual
phenomena found in our corpus. We look
at the thematic relations holding between
questions and the preceding context and
discuss to which extent thematic related-
ness plays a role in discourse structure.
1 Introduction
As pointed out by several authors (Kato et al, 2004),
(Chai and Ron, 2004), the information needs of
users interacting with QA systems often go beyond
a single stand-alone question. Often users want to
research about a particular topic or event or solve
a specific task. In such interactions we can expect
that the individual user questions will be themati-
cally connected, giving the users the possibility of
reusing part of the context when formulating new
questions.
That users implicitly refer to and even omit ma-
terial which can be recovered from the context
has already been replicated in several Wizard-of-
Oz experiments simulating natural language inter-
faces to databases, (Carbonell, 1983), (Dahlba?ck
and Jo?nsson, 1989), the most frequent contextual
phenomena being ellipsis, anaphora and definite de-
scriptions.
A big challenge for interactive QA systems is,
thus, the resolution of contextual phenomena. In or-
der to be able to do so a system has to keep track of
the user?s focus of attention as the interaction pro-
ceeds. The attentional state at a given point in the
interaction is given by the discourse structure. An
open issue, however, is the nature of the discourse
structure model needed in a QA system. Ahrenberg
et al (1990) argue that the discourse structure in NL
interfaces is, given the limited set of actions to be
performed by the system and the user, simpler than
the one underlying human-human dialogue. Upon
Ahrenberg et al (1990) this is given by the discourse
goals, rather than the overall goals of the user, as is
the case in task-oriented dialogues, (Grosz and Sid-
ner, 1986). Following Ahrenberg et al (1990), the
QA discourse is structured in segments composed
by a pair of initiative-response units, like question-
answer, or question-assertion, in the absence of an
answer. A segment can be embedded in another seg-
ment if it is composed by a clarification request and
its corresponding answer. The local context of a
segment is given by the immediately preceding seg-
ment. Upon Ahrenberg et al (1990), the latter re-
liably limits up the search space for antecedents of
anaphoric devices and ellipsis. However, as we will
see, there are few cases where the antecedents of
contextual phenomena are to be found beyond the
immediately preceding segments. This suggests that
a more complex approach to discourse structure for
QA systems is needed.
In more recent studies of interactive QA special
attention has been paid to the thematic relatedness of
questions, (Chai and Ron, 2004), (Kato et al, 2004).
Chai and Ron (2004) propose a discourse model-
ing for QA interactions in which they keep track
of thematic transitions between questions. Although
1
the applications of tracking thematic transitions be-
tween questions have not been investigated in depth,
Sun and Chai (2006) report on an experiment which
shows that the use of a model of topic transitions
based on Centering Theory improves query expan-
sion for context questions. However, these previous
studies on the thematic relations between questions
are not based on collections of interactive data, but
on questions centered around a topic that were col-
lected in non-interactive environments. This means
that they do not consider the answers to the ques-
tions, to which following questions can be related.
This paper presents data on different kinds of con-
textual phenomena found in a corpus of written nat-
ural language QA exchanges between human users
and a human agent representing an interactive infor-
mation service. We address two issues: the kinds
and frequencies of thematic relations holding be-
tween the user questions and the preceding context,
on the one hand, and the location of antecedents for
the different contextual phenomena, on the other.
We also discuss the question whether thematic rela-
tions can contribute to determine discourse structure
and, thus, to the resolution of the contextual phe-
nomena.
In the next section we present our data collection
and the aspects of the annotation scheme which are
relevant to the current work. In section 3 we present
data regarding the overall thematic cohesion of the
QA sessions. In section 4 we report on data regard-
ing the co-occurrence of discourse phenomena and
thematic relations and the distance between the phe-
nomena and their antecedents. Finally, we discuss
our findings with regard to their relevance with re-
spect to the nature of discourse structure.
2 Corpus and methodology
2.1 Experimental set-up
In order to obtain a corpus of natural QA inter-
actions, we designed a Wizard-of-Oz experiment.
The experiment was set up in such a way that the
exchanges between users and information system
would be as representative as possible for the inter-
action between users and QA systems. We chose an
ontology database instead of a text based closed do-
main QA system, however, because in order to simu-
late a real system short time responses were needed.
30 subjects took part in the experiment, which
consisted in solving a task by querying LT-WORLD,
an ontology containing information about language
technology1, in English. The modality of interaction
was typing through a chat-like interface.
Three different tasks were designed: two of them
concentrated on information browsing, the other one
on information gathering. In the first task sub-
jects had to find three traineeships at three different
projects in three different institutions each on a dif-
ferent topic, and obtain some information about the
chosen projects, like a contact address, a descrip-
tion, etc. In the second task, subjects had to find
three conferences in the winter term and three con-
ferences in the summer term, each one on a differ-
ent topic and they had to obtain some information on
the chosen conferences such as deadline, place, date.
etc. Finally, the third task consisted of finding infor-
mation for writing a report on European language
technology in the last ten years. To this end, subjects
had to obtain quantitative information on patents, or-
ganizations, conferences, etc.
The Wizard was limited to very few types of re-
sponses. The main response was answering a ques-
tion. In addition, she would provide intermediate
information about the state of processing if the re-
trieval took too long. She could also make state-
ments about the contents of the database when it did
not contain the information asked for or when the
user appeared confused about the structure of the
domain. Finally, she could ask for clarification or
more specificity when the question could not be un-
derstood. Yet the Wizard was not allowed to take
the initiative by offering information that was not
explicitely asked for. Thus all actions of the Wiz-
ard were directly dependent on those of the user.
As a result we obtained a corpus of 33 logs (30
plus 3 pilot experiments) containing 125.534 words
in 2.534 turns, 1.174 of which are user turns.
2.2 Annotation scheme
The corpus received a multi-layer annotaton2 con-
sisting of five levels. The levels of turns and part-of-
speech were automatically annotated. The level of
turns records information about the speaker and time
1See http://www.lt-world.org.
2We employed the annotation tool MMAX2 developed at
EML Research, Heidelberg.
2
stamp. For the other levels - the questions level, the
utterances level, and the entities level - a specific an-
notation scheme was developed. For these, we only
explain the aspects relevant for the present study.
2.2.1 Questions
This level was conceived to keep track of the
questions asked by the user which correspond to
queries to the database. With the aim of annotating
thematic relatedness between questions we distin-
guished two main kinds of thematic relations: those
holding between a question and a previous ques-
tion, quest(ion)-to-quest(ion)-rel(ation), and those
holding between a question and a previous answer,
quest(ion)-to-answ(er)-rel(ation).
Quest-to-quest-rels can be of the following types:
? refinement if the current question asks for the
same type of entity as some previous question,
but the restricting conditions are different, ask-
ing, thus, for a subset, superset or disjoint set
of the same class.
(1) US: How many projects on language tech-
nologies are there right now?
US: How many have been done in the
past?
? theme-entity if the current question is about the
same entity as some previous question.
(2) US: Where will the conference take place?
US: What is the dead-line for applicants?
? theme-property if the current question asks for
the same property as the immediately preced-
ing question but for another entity.
(3) US: Dates of TALK project?
US: Dates of DEREKO?
? paraphrase if the question is the rephrasing of
some previous question.
? overlap if the content of a question is subsumed
by the content of some previous question.
We distinguish the following quest-to-answ-rels:
? refinement if the current question asks for a
subset of the entities given in the previous an-
swer.
(4) LT: 3810.
US: How many of them do research on
language technology?
? theme if the current question asks about an en-
tity first introduced in some previous answer.
(5) LT: Semaduct, ...
US: What language technology topics
does the Semaduct project investigate?
Although Chai and Jin (2004) only consider tran-
sitions among questions in dialogues about events,
most of our relations have a correspondence with
theirs. Refinement corresponds to their constraint
refinement, theme-property to their participant-shift,
and theme-entity to their topic exploration.
2.2.2 Utterances
Utterances are classified according to their
speech-act: question, answer, assertion, or request.
Our annotation of discourse structure is identical in
spirit to the one proposed by Ahrenberg et al (1990).
A segment is opened with a user question to the
database and is closed with its corresponding an-
swer or an assertion by the system. Clarification
requests and their corresponding answers form seg-
ments which are embedded in other segments. Re-
quests to wait and assertions about the processing of
a question are also embedded in the segment opened
by the question.
Fragmentary utterances are annotated at this level.
We distinguish between fragments with a full lin-
guistic source, fragments with a partial source,
and fragments showing a certain analogy with the
source. The first group corresponds to fragments
which are structurally identical to the source and
can, thus, be resolved by substitution or extension.
(6) US: Are there any projects on spell checking in
Europe in the year 2006?
US: And in the year 2005?
Fragments with a partial source implicitly refer to
some entity previously introduced, but some infer-
ence must be done in order to resolve them.
(7) US: How is the contact for that project?
US: Homepage?
3
The last group is formed by fragments which show
some kind of parallelism with the source but which
cannot be resolved by substitution.
(8) US: Which conferences are offered in this win-
ter term in the subject of English language?
US: Any conferences concerning linguistics in
general?
2.2.3 Reference
We distinguish the following types of reference
to entities: identity or co-reference, subset/superset
and bridging.
Co-reference occurs when two or more expres-
sions denote the same entity. Within this group we
found the following types of implicit co-referring
expressions which involve different degrees of ex-
plicitness: elided NPs, anaphoric and deictic pro-
nouns, deictic NPs, and co-referent definite NPs.
Elided NPs are optional arguments, that is, they
don?t need to be in the surface-form of the sentence,
but are present in the semantic interpretation. In (9)
there is an anaphoric pronoun and an elided NP both
referring to the conference Speech TEK West 2006.
(9) US: The Speech TEK West 2006, when does it
take place?
LT: 2006-03-30 - 2006-04-01.
US: Until when can I hand in a paper [ ]?
Bridging is a definite description which refers to
an entity related to some entity in the focus of at-
tention. The resolution of bridging requires some
inference to be done in order to establish the con-
nection between the two entities. In example (2) in
subsection 2.2.1 there is an occurrence of bridging,
where the dead-line is meant to be the dead-line of
the conference currently under discussion.
Finally, subset/superset reference takes place
when a linguistic expression denotes a subset or su-
perset of the set of entities denoted by some previ-
ous linguistic expression. Subset/superset reference
is sometimes expressed through two interesting con-
textual phenomena: nominal ellipsis3, also called se-
mantic ellipsis, and one-NPs4. Nominal ellipsis oc-
curs within an NP and it is namely the noun what
3Note, however, that nominal ellipsis does not necessarily
always denote a subset, but sometimes it can denote a disjoint
set, or just lexical material which is omitted.
4One-NPs are a very rare in our corpus, so we are not con-
sidering them in the present study.
is missing and must be recovered from the context.
Here follows an example:
(10) US: Show me the three most important.
3 Thematic follow-up
When looking at the thematic relatedness of the
questions it?s striking how well structured the in-
teractions are regarding thematic relatedness. From
1047 queries to the database, 948 (90.54%) follow-
up on some previous question or answer, or both.
Only 99 questions (9.46%) open a new topic. 725
questions (69.25% of the total, 76.48% of the con-
nected questions) are related to other questions, 332
(31.71% of the total, 35.02% of the connected ques-
tions) are related to answers, and 109 (10.41% of the
total, 11.49% of the connected questions) are con-
nected to both questions and answers. These num-
bers don?t say much about how well structured the
discourse is, since the questions could be far away
from the questions or answers they are related to.
However, this is very seldom the case. In 60% of
the cases where the questions are thematically con-
nected, they immediately follow the question they
are related to, that is, the two questions are consecu-
tive5. In 16.56% of the cases the questions immedi-
ately follow the answer they are related to. 74.58%
of the questions, thus, immediately follow up the
question or/and answer they are thematically related
to6.
Table 1 shows the distribution of occurrences
and distances in segments for each of the rela-
tions described in subsection 2.2.1. We found that
the most frequent question-to-question relation is
theme-entity, followed by the question-to-answer re-
lation theme. As you can see, for all the relations ex-
cept theme, most occurrences are between very close
standing questions or questions and answers, most
of them holding between consecutive questions or
questions and answers. The occurrences of the re-
lation theme, however, are distributed along a wide
range of distances, 29.70% holding between ques-
tions and answers that are 2 and 14 turns away from
5By consecutive we mean that there is no intervening query
to the database between the two questions. This doesn?t imply
that there aren?t several intervening utterances and turns.
69 questions are consecutive to the question and answer they
are related to, respectively, that?s why the total percentage of
related consecutive questions is not 76.56%.
4
REF. Q. THEME E. Q. THEME P. Q. PARA. Q. OVERL. Q. REF. A. THEME A.
TOTAL 74 338 107 174 29 29 303
(7.80%) (35.65%) (11.29%) (18.35%) (3.06%) (3.06%) (31.96%)
1 SEGM. 88.73% 81.65% 100% 60.92% 78.57% 83.34% 46.39%
2 SEGM. 5.63% 1.86% 0% 8.09% 21.43% 13.33% 10.20%
Table 1: Occurrences of the different thematic relations
REL. / PHEN. THEME E. Q. THEME P. Q. THEME A. REF. Q. REF. A. CONNECTED TOTAL
FRAGMENT 53 (54.08%) 17 (16.32%) 3 (3.06%) 21 (21.42%) 0 97 (85.08%) 114
BRIDGING 40 (74.07%) 0 3 (5.55%) 1 (1.85%) 0 54 (58.69%) 92
DEFINITE NP 26 (78.78%) 0 4 (12.21%) 2 (6.10%) 0 33 (66%) 50
DEICTIC NP 19 (51.35%) 0 13 (35.13%) 2 (5.40%) 1 (2.70%) 37 (78.72%) 47
ANAPHORIC PRON. 13 (39.39%) 2 (6.06%) 10 (30.30%) 0 5 (15.15%) 33 (39.75%) 83
DEICTIC PRON. 2 (75%) 0 1 (25%) 0 0 3 (25%) 12
ELIDED NP 9 (69.23%) 0 2 (15.38%) 0 0 13 (61.90%) 21
NOMINAL ELLIPSIS 0 1 (7.69%) 6 (46.15%) 1 (7.69%) 5 (38.46%) 13 (81.25%) 16
Table 2: Contextual phenomena and the thematic relations holding between the questions containing them
and the questions or answers containing the antecedents.
each other. This is because often several entities
are retrieved with a single query and addressed later
on separately, obtaining all the information needed
about each of them before turning to the next one.
We found also quite long distances for paraphrases,
which means that the user probably forgot that he
had asked that question, since he could have also
scrolled back.
These particular distributions of thematic rela-
tions seem to be dependent on the nature of the
tasks. We found some differences across tasks: the
information gathering task elicited more refinement,
while the information browsing tasks gave rise to
more theme relations. It is possible that in an in-
teraction around an event or topic we may find ad-
ditional kinds of thematic relations and different
distributions. We also observed different strategies
among the subjects. The most common was to ask
everything about an entity before turning to the next
one, but some subjects preferred to ask about the
value of a property for all the entities under discus-
sion before turning to the next property.
4 Contextual phenomena: distances and
thematic relatedness
There are 1113 user utterances in our corpus, 409 of
which exhibit some kind of discourse phenomenon,
i.e., they are context-dependent in some way. This
amounts to 36.16% of the user utterances, a pro-
portion which is in the middle of those found in the
several corpora analyzed by Dahlba?ck and Jo?nsson
(1989)7. The amount of context-dependent user ut-
terances, as Dahlba?ck and Jo?nsson (1989) already
pointed out, as well as the distribution of the dif-
ferent relations among questions and answers ex-
plained above, may be dependent on the nature of
the task attempted in the dialogue.
Table 2 shows the distribution of the most fre-
quent thematic relations holding between the ques-
tions containing the contextual phenomena consid-
ered in our study and the questions or answers con-
taining their antecedents. The rightmost column
shows the number of occurrences of each of the con-
textual phenomena described in subsection 2.2.3.
The second column on the right shows the number
of occurrences in which the antecedent is located
in some previous segment and the question contain-
ing the contextual phenomenon is related through a
thematic relation to the question or answer contain-
ing the antecedent. The percentages shown for each
phenomenon are out of the total number of its oc-
currences. The remaining columns show frequen-
7They found a high variance according to the kind of task
carried out in the different dialogues. Dialogues from tasks
where there was the possibility to order something contained
a higher number of context-dependent user initiatives, up to
54.62%, while information browsing dialogues contained a
smaller number of context-dependent user initiatives, 16.95%
being the lowest amount found.
5
cies of co-occurrence for each of the phenomena and
thematic relations. The percentages shown for each
phenomenon are out of the total number of its con-
nected occurrences.
For the majority of investigated phenomena we
observe that most questions exhibiting them stand
in a thematic relation to the question or answer con-
taining the antecedent. Although there may be sev-
eral intermediate turns, the related questions are al-
most always consecutive, that is, the segment con-
taining the contextual phenomenon immediately fol-
lows the segment containing the antecedent. In the
remainder of the cases, the contextual phenomenon
and its antecedent are usually in the same segment.
However, this is not the case for deictic and
anaphoric pronouns. In most cases their antecedents
are in the same segment and even in the same utter-
ance or just one utterance away. This suggests that
pronouns are produced in a more local context than
other phenomena and their antecedents are first to be
looked for in the current segment.
For almost all the phenomena the most frequent
relation holding between the question containing
them and the question or answer containing the an-
tecedent is the question-to-question relation theme-
entity, followed by the question-to-answer relation
theme. This is not surprising, since we refer back to
entities because we keep speaking about them.
However, fragments and nominal ellipsis show a
different distribution. Fragments are related to their
sources through the question-to-question relations
theme-property and refinement, as well. Regarding
the distribution of relations across the three differ-
ent types of fragments we distinguish in our study,
we find that the relations refinement and theme-
property only hold between fragments with a full
source and fragments of type analogy, and their re-
spective sources. On the other hand, practically all
fragments with a partial-source stand in a theme-
entity relation to their source. Questions containing
nominal ellipsis are mostly related to the preceding
answer both through the relations theme and refine-
ment.
4.1 Antecedents beyond the boundaries of the
immediately preceding segment
As we have seen, the antecedents of more implicit
co-referring expressions, like pronouns, are very of-
ten in the same segment as the expressions. The
antecedents of less explicit co-referring expressions,
like deictic and definite NPs, are mostly in the im-
mediately preceding segment, but also often in the
same segment. About 50% are 2 utterances away,
20% between 3 and 5, although we find distances up
to 41 utterances for definite NPs.
However, there is a small number (11) of cases in
which the antecedents are found across the bound-
aries of the immediately preceding segment. This
poses a challenge to systems since the context
needed for recovering these antecedent is not as lo-
cal. The following example is a case of split an-
tecedents. The antecedent of the elided NP is to be
found across the two immediately preceding ques-
tions. Moreover, as you can see, the Wizard is not
sure about how to interpret the missing argument,
which can be because of the split antecedents, but
also because of the amount of time passed, and/or
because one of the answers is still missing, that is,
more than one segment is open at the same time.
(11) US: Which are the webpages for European
Joint Conferences on Theory and Practice
of Software and International Conference on
Linguistic Evidence?
LT: Please wait... (waiting time)
US: Which are the webpages for International
Joint Conference on Neural Networks and
Translating and the Computer 27?
LT: http://www.complang.ac, ... (1st answer)
US: Up to which date is it possible to send a
paper, an abstract [ ]?
LT: http://uwb.edu/ijcnn05/, ... (2nd answer)
LT: For which conference?
US: For all of the conferences I got the web-
pages.
In the following example the antecedent of the
definite NP is also to be found beyond the bound-
aries of the immediately preceding segment.
(12) US: What is the homepage of the project?
LT: http://dip.semanticweb.org
USER: What is the email address of Christoph
Bussler?
LT: The database does not contain this informa-
tion.
US: Where does the project take place?
6
Here the user asks about the email address of a per-
son who was previously introduced in the discourse
as the coordinator of the project under discussion
and then keeps on referring to the project with a def-
inite NP. The intervening question is somehow re-
lated to the project, but not directly. There is a topic
shift, as defined by Chai and Jin (2004), where the
main topic becomes an entity related to the entity the
preceding question was about. However, this topic
shift is only at a very local level, since the dialogue
participants keep on speaking about the project, that
is, the topic at a more general level keeps on being
the same. We can speak here of thematic nesting,
since the second question is about an entity intro-
duced in relation to the entity in focus of attention
in the first question, and the third question is again
about the same entity as the first. The project has not
completely left the focus, but has remained in sec-
ondary focus during the second segment, to become
again the main focus in the third segment. It seems
that as long as the entity to which the focus of atten-
tion has shifted is related to the entity previously in
focus of attention, the latter still also remains within
the focus of attention.
5 Conclusions
The possibility of using contextual phenomena is
given by certain types of thematic relatedness - espe-
cially theme-entity and theme, for co-reference and
bridging, and refinement, theme-entity and theme-
property, for fragments -, and contiguity of ques-
tions. As we have seen, the immediately preced-
ing segment is in most cases the upper limit of the
search space for the last reference to the entity, or
the elided material in fragments. The directions of
the search for antecedents, however, can vary de-
pending on the phenomena, since for more implicit
referring expressions antecedents are usually to be
found in the same segment, while for less implicit
referring expressions they are to be found in the pre-
ceding one.
These data are in accordance with what Ahren-
berg et al (1990) predict in their model. Just to
consider the immediately preceding segment as the
upper limit of the search space for antecedents is
enough and, thus, no tracking of thematic relations
is needed to resolve discourse phenomena. How-
ever, there are occurrences of more explicit types
of co-reference expressions, where the antecedent
is beyond the immediately preceding segment. As
we have observed, in these cases the intervening
segment/s shift the focus of attention to an entity
(maybe provided in some previous answer) closely
related to the one in focus of attention in the pre-
ceding segment. It seems that as long as this rela-
tion exists, even if there are many segments in be-
tween8, the first entity remains in focus of attention
and can be referred to by an implicit deictic or defi-
nite NP without any additional retrieval cue. We can
speak of thematic nesting of segments, which seems
to be analogous to the intentional structure in task-
oriented dialogues as in (Grosz and Sidner, 1986),
also allowing for reference with implicit devices to
entities in the superordinate segments after the sub-
ordinated ones have been closed. It seems, thus, that
thematic structure, like the discourse goals, also im-
poses structure on the discourse.
These cases, although not numerous, suggest that
a more complex discourse structure is needed for
QA interactions than one simply based on the dis-
course goals. The local context is given by the dis-
course segments, which are determined by the dis-
course goals, but a less local context may encompass
several segments. As we have seen, reference with
implicit devices to entities in the less local context
is still possible. What seems to determine this less
local context is a unique theme, about which all the
segments encompassed by the context directly or in-
directly are. So, although it does not seem necessary
to track all the thematic transitions between the seg-
ments, it seems necessary to categorize the segments
as being about a particular more global theme.
In a system like the one we simulated, having spe-
cific tasks in mind and querying structured data, a
possible approach to model this extended context,
or focus of attention, would be in terms of frames.
Every time a new entity is addressed a new frame
is activated. The frame encompasses the entity it-
self and the properties holding of it and other enti-
ties, as well as those entities. This would already
allow us to successfully resolve bridging and frag-
ments with a partial source. If the focus of atten-
8We found up to five intervening segments, one of them be-
ing a subsegment.
7
tion then shifts to one of the related entities, the user
demanding particular information about it, then its
frame is activated, but the previous frame also re-
mains somehow active, although to a lesser degree.
As long as there is a connection between the enti-
ties being talked about and a frame is not explicitly
closed, by switching to speak about a different en-
tity of the same class, for example, frames remain
somehow active and implicit references will be ac-
commodated within the activation scope.
In principle, the closer the relation to the entity
currently in focus, the higher the degree of activation
of the related entities. Yet, there may be cases of
ambiguity, where only inferences about the goals of
the user may help to resolve the reference, as in (13):
(13) US: How is the contact for that project?
LT: daelem@uia.ua.ac.be
US: What is the institute?
LT: Centrum voor Nederlandse Taal en Spraak.
US: Homepage?
Here the property ?Homepage? could be asked about
the institution or the project, the institution being
more active. However, the Wizard interpreted it as
referring to the project without hesitation because
she knew that subjects were interested in projects,
not in organizations. In order to resolve the ambigu-
ity, we would need a system customized for tasks or
make inferences about the goals of the users based
on the kind of information they?ve been asking for.
Determining at which level of nesting some expres-
sion has to be interpreted may involve plan recogni-
tion.
However, for open domain systems not having a
knowledge-base with structured data it may be much
more difficult to keep track of the focus of attention
beyond the strictly local context. For other kinds
of interactions which don?t have such a structured
nature as our tasks, this may also be the case. For
example, in the information browsing tasks in (Kato
et al, 2004), there is not a global topic encompass-
ing the whole interaction, but the information needs
of the user are given by the information he is en-
countering as the interaction proceeds, that is, he is
browsing the information in a free way, without hav-
ing particular goals or particular pieces of informa-
tion he wants to obtain in mind. In such cases it
may be difficult to determine how long frames are
active if the nesting goes very far, as well as making
any inferences about the user?s plans. However, it
might also be the case, that in that kind of interac-
tions no implicit referring expressions are used be-
yond the segmental level, because there is no such
an extended context. In order to find out, a study
with interactive data should be carried out.
Acknowledgements
The research reported here has been conducted in
the projects QUETAL and COLLATE II funded by
the German Ministry for Education and Research,
grants no. 01IWC02 and 01INC02, respectively. We
are also grateful to Bonnie Webber for her helpful
comments on the contents of this paper.
References
Ahrenberg Lars, Dahlba?ck Nils and Arne Jo?nsson 1990.
Discourse representation and discourse management
for natural language interfaces. Proceeding of the
Second Nordic Conference on Text Comprehension in
Man and Machine, Ta?by, Sweeden, 1990.
Jaime G. Carbonell. 1983. Discourse pragmatics and
ellipsis resolution in task-oriented natural language
interfaces. Proceedings of the 21st annual meeting
on Association for Computational Linguistics, Cam-
bridge, Massachusetts, 1983
Chai Joyce Y. and Ron Jin. 2004. Discourse Status
for Context Questions. HLT-NAACL 2004 Workshop
on Pragmatics in Question Answering (HLT-NAACL
2004) Boston, MA, USA, May 3-7, 2004
Dahlba?ck Nils and Arne Jo?nsson. 1989. Empirical
Studies of Discourse Representations for Natural Lan-
guage Interfaces. Proceedings of the Fourth Confer-
ence of the European Chapter of the ACL (EACL?89),
Manchester.
Grosz Barbara and Candance Sidner. 1986. Attention,
Intention and the Structure of Discourse. Computa-
tional Linguistics 12(3): 175-204.
Kato Tsuneaki, Fukumoto Junichi, Masui Fumito and
Noriko Kando. 2004. Handling Information Access
Dialogue through QA Technologies - A novel chal-
lenge for open-domain question answering. HLT-
NAACL 2004 Workshop on Pragmatics in Question
Answering (HLT-NAACL 2004) Boston, MA, USA,
May 3-7, 2004
Sun Mingyu and Joycie J. Chai. 2006. Towards Intel-
ligent QA Interfaces: Discourse Processing for Con-
text Questions. International Conference on Intelligent
User Interfaces, Sydney, Australia, January 2006
8
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 70?74,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Translation Combination using Factored Word Substitution
Christian Federmann1, Silke Theison2, Andreas Eisele1,2, Hans Uszkoreit1,2,
Yu Chen2, Michael Jellinghaus2, Sabine Hunsicker2
1: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
2: Universita?t des Saarlandes, Saarbru?cken, Germany
{cfedermann,eisele,uszkoreit}@dfki.de, {sith,yuchen,micha,sabineh}@coli.uni-sb.de
Abstract
We present a word substitution approach
to combine the output of different machine
translation systems. Using part of speech
information, candidate words are deter-
mined among possible translation options,
which in turn are estimated through a pre-
computed word alignment. Automatic
substitution is guided by several decision
factors, including part of speech, local
context, and language model probabili-
ties. The combination of these factors
is defined after careful manual analysis
of their respective impact. The approach
is tested for the language pair German-
English, however the general technique it-
self is language independent.
1 Introduction
Despite remarkable progress in machine transla-
tion (MT) in the last decade, automatic translation
is still far away from satisfactory quality. Even the
most advanced MT technology as summarized by
(Lopez, 2008), including the best statistical, rule-
based and example-based systems, produces out-
put rife with errors. Those systems may employ
different algorithms or vary in the linguistic re-
sources they use which in turn leads to different
characteristic errors.
Besides continued research on improving MT
techniques, one line of research is dedicated to bet-
ter exploitation of existing methods for the com-
bination of their respective advantages (Macherey
and Och, 2007; Rosti et al, 2007a).
Current approaches for system combination in-
volve post-editing methods (Dugast et al, 2007;
Theison, 2007), re-ranking strategies, or shal-
low phrase substitution. The combination pro-
cedure applied for this pape tries to optimize
word-level translations within a ?trusted? sentence
frame selected due to the high quality of its syntac-
tic structure. The underlying idea of the approach
is the improvement of a given (original) translation
through the exploitation of additional translations
of the same text. This can be seen as a simplified
version of (Rosti et al, 2007b).
Considering our submission from the shared
translation task as the ?trusted? frame, we add
translations from four additional MT systems that
have been chosen based on their performance in
terms of automatic evaluation metrics. In total, the
combination system performs 1,691 substitutions,
i.e., an average of 0.67 substitutions per sentence.
2 Architecture
Our system combination approach computes a
combined translation from a given set of machine
translations. Below, we present a short overview
by describing the different steps in the derivation
of a combined translation.
Compute POS tags for translations. We apply
part-of-speech (POS) tagging to prepare the
selection of possible substitution candidates.
For the determination of POS tags we use the
Stuttgart TreeTagger (Schmid, 1994).
Create word alignment. The alignment between
source text and translations is needed to
identify translation options within the differ-
ent systems? translations. Word alignment
is computed using the GIZA++ toolkit (Och
and Ney, 2003), only one-to-one word align-
ments are employed.
Select substitution candidates. For the shared
task, we decide to substitute nouns, verbs
and adjectives based on the available POS
tags. Initially, any such source word is con-
sidered as a possible substitution candidate.
As we do not want to require substitution can-
70
didates to have exactly the same POS tag as
the source, we use groups of ?similar? tags.
Compute decision factors for candidates. We
define several decision factors to enable an
automatic ranking of translation options.
Details on these can be found in section 4.
Evaluate the decision factors and substitute.
Using the available decision factors we
compute the best translation and substitute.
The general combination approach is language
independent as it only requires a (statistical) POS
tagger and GIZA++ to compute the word align-
ments. More advanced linguistic resources are not
required. The addition of lexical resources to im-
prove the extracted word alignments has been con-
sidered, however the idea was then dropped as we
did not expect any short-term improvements.
3 System selection
Our system combination engine takes any given
number of translations and enables us to compute
a combined translation out of these. One of the
given system translations is chosen to provide the
?sentence skeleton?, i.e. the global structure of the
translation, thus representing the reference system.
All other systems can only contribute single words
for substitution to the combined translation, hence
serve as substitution sources.
3.1 Reference system
Following our research on hybrid translation try-
ing to combine the strengths of rule-based MT
with the virtues of statistical MT, we choose our
own (usaar) submission from the shared task to
provide the sentence frame for our combination
system. As this translation is based upon a rule-
based MT system, we expect the overall sentence
structure to be of a sufficiently high quality.
3.2 Substitution sources
For the implementation of our combination sys-
tem, we need resources of potential substitution
candidates. As sources for possible substitution,
we thus include the translation results of the fol-
lowing four systems:
? Google (google)1
1The Google submission was translated by the Google
MT production system offered within the Google Language
Tools as opposed to the qualitatively superior Google MT
research system.
? University of Karlsruhe (uka)
? University of Maryland (umd)
? University of Stuttgart (stuttgart)
The decision to select the output of these par-
ticular MT systems is based on their performance
in terms of different automatic evaluation metrics
obtained with the IQMT Framework by (Gime?nez
and Amigo?, 2006). This includes BLEU, BLEU1,
TER, NIST, METEOR, RG, MT06, and WMT08.
The results, listing only the three best systems per
metric, are given in table 1.
metric best three systems
BLEU1 google uka systran
0.599 0.593 0.582
BLEU google uka umd
0.232 0.231 0.223
TER umd rwth.c3 uka
0.350 0.335 0.332
NIST google umd uka
6.353 6.302 6.270
METEOR google uka stuttgart
0.558 0.555 0.548
RG umd uka google
0.527 0.525 0.520
MT06 umd google stuttgart
0.415 0.413 0.410
WMT08 stuttgart rbmt3 google
0.344 0.341 0.336
Table 1: Automatic evaluation results.
On grounds of these results we anticipate the
four above named translation engines to perform
best when being combined with our hybrid ma-
chine translation system. We restrict the substi-
tution sources to the four potentially best systems
in order to omit bad substitutions and to reduce
the computational complexity of the substitution
problem. It is possible to choose any other num-
ber of substitution sources.
4 Substitution
As mentioned above, we consider nouns, verbs
and adjectives as possible substitution candidates.
In order to allow for automatic decision making
amongst several translation options we define a set
of factors, detailed in the following. Furthermore,
we present some examples in order to illustrate the
use of the factors within the decision process.
71
4.1 Decision factors
The set of factors underlying the decision proce-
dure consists of the following:
A: Matching POS. This Boolean factor checks
whether the target word POS tag matches the
source word?s POS category. The factor com-
pares the source text to the reference trans-
lation as we want to preserve the sentential
structure of the latter.
B: Majority vote. For this factor, we compute
an ordered list of the different translation op-
tions, sorted by decreasing frequency. A con-
sensus between several systems may help to
identify the best translation.
Both the reference system and the Google
submission receive a +1 bonus, as they ap-
peared to offer better candidates in more
cases within the small data sample of our
manual analysis.
C: POS context. Further filtering is applied de-
termining the words? POS context. This is
especially important as we do not want to de-
grade the sentence structure maintained by
the translation output of the reference system.
In order to optimize this factor, we conduct
trials with the single word, the ?1 left, and
the +1 right context. To reduce complex-
ity, we shorten POS tags to a single character,
e.g. NN ? N or NPS ? N .
D: Language Model. We use an English lan-
guage model to score the different translation
options. As the combination system only re-
places single words within a bi-gram context,
we employ the bi-gram portion of the English
Gigaword language model.
The language model had been estimated us-
ing the SRILM toolkit (Stolcke, 2002).
4.2 Factor configurations
To determine the best possible combination of our
different factors, we define four potential factor
configurations and evaluate them manually on a
small set of sentences. The configurations differ
in the consideration of the POS context for factor
C (strict including ?1 left context versus relaxed
including no context) and in the usage of factor A
Matching POS (+A). Table 2 shows the settings of
factors A and C for the different configurations.
configuration Matching POS POS context
strict disabled ?1 left
strict+A enabled ?1 left
relaxed disabled single word
relaxed+A enabled single word
Table 2: Factor configurations for combination.
Our manual evaluation of the respective substi-
tution decisions taken by different factor combi-
nation is suggestive of the ?relaxed+A? configura-
tion to produce the best combination result. Thus,
this configuration is utilized to produce sound
combined translations for the complete data set.
4.3 Factored substitution
Having determined the configuration of the dif-
ferent factors, we compute those for the complete
data set, in order to apply the final substitution step
which will create the combined translation.
The factored substitution algorithm chooses
among the different translation options in the fol-
lowing way:
(a) Matching POS? If factor A is activated for
the current factor configuration (+A), sub-
stitution of the given translation options can
only be possible if the factor evaluates to
True. Otherwise the substitution candidate is
skipped.
(b) Majority vote winner? If the majority vote
yields a unique winner, this translation option
is taken as the final translation.
Using the +1 bonuses for both the reference
system and the Google submission we intro-
duce a slight bias that was motivated by man-
ual evaluation of the different systems? trans-
lation results.
(c) Language model. If several majority vote
winners can be determined, the one with the
best language model score is chosen.
Due to the nature of real numbers this step
always chooses a winning translation option
and thus the termination of the substitution
algorithm is well-defined.
Please note that, while factors A, B, and D are
explicitly used within the substitution algorithm,
factor C POS context is implicitly used only when
computing the possible translation options for a
given substitution candidate.
72
configuration substitutions ratio
strict 1,690 5.714%
strict+A 1,347 4.554%
relaxed 2,228 7.532%
relaxed+A 1,691 5.717%
Table 3: Substitutions for 29,579 candidates.
Interestingly we are able to obtain best results
without considering the ?1 left POS context, i.e.
only checking the POS tag of the single word
translation option for factor C.
4.4 Combination results
We compute system combinations for each of the
four factor configurations defined above. Table
3 displays how many substitutions are conducted
within each of these configurations.
The following examples illustrate the perfor-
mance of the substitution algorithm used to pro-
duce the combined translations.
?Einbruch?: the reference translation for ?Ein-
bruch? is ?collapse?, the substitution sources
propose ?slump? and ?drop?, but also ?col-
lapse?, all three, considering the context,
forming good translations. The majority vote
rules out the suggestions different to the ref-
erence translation due to the fact that 2 more
systems recommend ?collapse? as the correct
translation.
?Ru?ckgang?: the reference system translates this
word as ?drop? while all of the substitution
sources choose ?decline? as the correct trans-
lation. Since factor A evaluates to True, i.e.
the POS tags are of the same nature, ?de-
cline? is clearly selected as the best transla-
tion by factor B Majority vote and thus re-
places ?drop? in the final combined transla-
tion result.
?Tagesgescha?fte?: our reference system trans-
lates ?Tagesgescha?fte? with ?requirements?,
while two of the substitution systems indi-
cate ?business? to be a better translation. Due
to the +1 bonus for our reference translation
a tie between the two possible translations
emerges, leaving the decision to the language
model score, which is higher for ?business?.
4.5 Evaluation results
Table 4 shows the results of the manual evaluation
campaign carried out as part of the WMT09 shared
task. Randomly chosen sentences are presented
to the annotator, who then has to put them into
relative order. Note that each annotator is shown a
random subset of the sentences to be evaluated.
system relative rank data points
google -2.74 174
uka -3.00 217
umd -3.03 170
stuttgart -2.89 163
usaar -2.78 186
usaar-combo -2.91 164
Table 4: Relative ranking results from the WMT09
manual evalution campaign.
Interestingly, our combined system is not able
to outperform the baseline, i.e., additional data
did not improve translation results. However the
evaluation is rather intransparent since it does not
allow for a strict comparison between sentences.
5 Conclusion
Within the system described in this paper, we ap-
proach a hybrid translation technique combining
the output of different MT systems. Substituting
particular words within a well-structured transla-
tion frame equips us with considerably enhanced
translation output. We obtain promising results
providing substantiated proof that our approach is
going in the right direction.
Further steps in the future will include machine
learning methods to optimize the factor selection.
This was, due to limited amount of time and data,
not feasible thus far. We will also investigate the
potential of phrase-based substitution taking into
account multi-word alignments instead of just sin-
gle word mappings. Additionally, we would like
to continue work on the integration of lexical re-
sources to post-correct the word alignments ob-
tained by GIZA++ as this will directly improve the
overall system performance.
Acknowledgments
This work was supported by the EuroMatrix
project (IST-034291) which is funded by the
European Community under the Sixth Frame-
work Programme for Research and Technological
Development.
73
References
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
220?223, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
framework for automatic machine translation eval-
uation. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC?06).
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):1?49.
Wolfgang Macherey and Franz J. Och. 2007. An em-
pirical study on computing consensus translations
from multiple machine translation systems. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 986?995, Prague, Czech Republic,
June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007a. Combining outputs from multiple
machine translation systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 228?235, Rochester, New York, April.
Association for Computational Linguistics.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system
combination for machine translation. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 312?319,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, September.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In the 7th International
Conference on Spoken Language Processing (IC-
SLP) 2002, Denver, Colorado.
Silke Theison. 2007. Optimizing rule-based machine
translation output with the help of statistical meth-
ods. Master?s thesis, Saarland University, Computa-
tional Linguistics department.
74
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 42?46,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Combining Multi-Engine Translations with Moses
Yu Chen1, Michael Jellinghaus1, Andreas Eisele1,2,Yi Zhang1,2,
Sabine Hunsicker1, Silke Theison1, Christian Federmann2, Hans Uszkoreit1,2
1: Universita?t des Saarlandes, Saarbru?cken, Germany
2: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
{yuchen,micha,yzhang,sabineh,sith}@coli.uni-saarland.de
{eisele,cfedermann,uszkoreit}@dfki.de
Abstract
We present a simple method for generating
translations with the Moses toolkit (Koehn
et al, 2007) from existing hypotheses pro-
duced by other translation engines. As
the structures underlying these translation
engines are not known, an evaluation-
based strategy is applied to select sys-
tems for combination. The experiments
show promising improvements in terms of
BLEU.
1 Introduction
With the wealth of machine translation systems
available nowadays (many of them online and
for free), it makes increasing sense to investigate
clever ways of combining them. Obviously, the
main objective lies in finding out how to integrate
the respective advantages of different approaches:
Statistical machine translation (SMT) and rule-
based machine translation (RBMT) systems of-
ten have complementary characteristics. Previous
work on building hybrid systems includes, among
others, approaches using reranking, regeneration
with an SMT decoder (Eisele et al, 2008; Chen
et al, 2007), and confusion networks (Matusov et
al., 2006; Rosti et al, 2007; He et al, 2008).
The approach by (Eisele et al, 2008) aimed
specifically at filling lexical gaps in an SMT sys-
tem with information from a number of RBMT
systems. The output of the RBMT engines was
word-aligned with the input, yielding a total of
seven phrase tables which where simply concate-
nated to expand the phrase table constructed from
the training corpus. This approach differs from the
confusion network approaches mainly in that the
final hypotheses do not necessarily follow any of
the input translations as the skeleton. On the other
hand, it emphasizes that the additional translations
should be produced by RBMT systems with lexi-
cons that cannot be learned from the data.
The present work continues on the same track
as the paper mentioned above but implements a
number of important changes, most prominently
a relaxation of the restrictions on the number and
type of input systems. These differences are de-
scribed in more detail in Section 2. Section 3 ex-
plains the implementation of our system and Sec-
tion 4 its application in a number of experiments.
Finally, Section 5 concludes this paper with a sum-
mary and some thoughts on future work.
2 Integrating Multiple Systems of
Unknown Type and Quality
When comparing (Eisele et al, 2008) to the
present work, our proposal is more general in a
way that the requirement for knowledge about the
systems is minimum. The types and the identities
of the participated systems are assumed unknown.
Accordingly, we are not able to restrict ourselves
to a certain class of systems as (Eisele et al, 2008)
did. We rely on a standard phrase-based SMT
framework to extract the valuable pieces from the
system outputs. These extracted segments are also
used to improve an existing SMT system that we
have access to.
While (Eisele et al, 2008) included translations
from all of a fixed number of RBMT systems
and added one feature to the translation model for
each system, integrating all given system outputs
in this way in our case could expand the search
space tremendously. Meanwhile, we cannot rely
on the assumption that all candidate systems ac-
tually have the potential to improve our baseline.
This implies the need for a first step of system se-
lection where the best candidate systems are iden-
tified and a limited number of them is chosen to be
included in the combination. Our approach would
not work without a small set of tuning data being
available so that we can evaluate the systems for
later selection and adjust the weights of our sys-
tems. Such tuning data is included in this year?s
42
task.
In this paper, we use the Moses decoder to con-
struct translations from the given system outputs.
We mainly propose two slightly different ways:
One is to construct translation models solely from
the given translations and the other is to extend
an existing translation model with these additional
translations.
3 Implementation
Despite the fact that the output of current MT sys-
tems is usually not comparable in quality to hu-
man translations, the machine-generated transla-
tions are nevertheless ?parallel? to the input so
that it is straightforward to construct a translation
model from data of this kind. This is the spirit
behind our method for combining multiple trans-
lations.
3.1 Direct combination
Clearly, for the same source sentence, we expect
to have different translations from different trans-
lation systems, just like we would expect from hu-
man translators. Also, every system may have its
own advantages. We break these translations into
smaller units and hope to be able to select the best
ones and form them into a better translation.
One single translation of a few thousand sen-
tences is normally inadequate for building a re-
liable general-purpose SMT system (data sparse-
ness problem). However, in the system combina-
tion task, this is no longer an issue as the system
only needs to translate sentences within the data
set.
When more translation engines are available,
the size of this set becomes larger. Hence,
we collect translations from all available systems
and pair them with the corresponding input text,
thus forming a medium-sized ?hypothesis? cor-
pus. Our system starts processing this corpus
with a standard phrase-based SMT setup, using the
Moses toolkit (Koehn et al, 2007).
The hypothesis corpus is first tokenized and
lowercased. Then, we run GIZA++ (Och and
Ney, 2003) on the corpus to obtain word align-
ments in both directions. The phrases are extracted
from the intersection of the alignments with the
?grow? heuristics. In addition, we also generate
a reordering model with the default configuration
as included in the Moses toolkit. This ?hypothe-
sis? translation model can already be used by the
Moses decoder together with a language model to
perform translations over the corresponding sen-
tence set.
3.2 Integration into existing SMT system
Sometimes, the goal of system combination is not
only to produce a translation but also to improve
one of the systems. In this paper, we aim at incor-
porating the additional system outputs to improve
an out-of-domain SMT system trained on the Eu-
roparl corpus (Koehn, 2005). Our hope is that the
additional translation hypotheses could bring in
new phrases or, more generally, new information
that was not contained in the Europarl model. In
order to facilitate comparisons, we use in-domain
LMs for all setups.
We investigate two alternative ways of integrat-
ing the additional phrases into the existing SMT
system: One is to take the hypothesis translation
model described in Section 3.1, the other is to
construct system-specific models constructed with
only translations from one system at a time.
Although the Moses decoder is able to work
with two phrase tables at once (Koehn and
Schroeder, 2007), it is difficult to use this method
when there is more than one additional model.
The method requires tuning on at least six more
features, which expands the search space for the
translation task unnecessarily. We instead inte-
grate the translation models from multiple sources
by extending the phrase table. In contrast to the
prior approach presented in (Chen et al, 2007) and
(Eisele et al, 2008) which concatenates the phrase
tables and adds new features as system markers,
our extension method avoids duplicate entries in
the final combined table.
Given a set of hypothesis translation models
(derived from an arbitrary number of system out-
puts) and an original large translation model to be
improved, we first sort the models by quality (see
Section 3.3), always assigning the highest priority
to the original model. The additional phrase tables
are appended to the large model in sorted order
such that only phrase pairs that were never seen
before are included. Lastly, we add new features
(in the form of additional columns in the phrase ta-
ble) to the translation model to indicate each pair?s
origin.
3.3 System evaluation
Since both the system translations and the ref-
erence translations are available for the tuning
43
set, we first compare each output to the reference
translation using BLEU (Papineni et al, 2001)
and METEOR (Banerjee and Lavie, 2005) and a
combined scoring scheme provided by the ULC
toolkit (Gimenez and Marquez, 2008). In our ex-
periments, we selected a subset of 5 systems for
the combination, in most cases, based on BLEU.
On the other hand, some systems may be de-
signed in a way that they deliver interesting unique
translation segments. Therefore, we also measure
the similarity among system outputs as shown in
Table 2 in a given collection by calculating aver-
age similarity scores across every pair of outputs.
de-en fr-en es-en en-de en-fr en-es
Num. 20 23 28 15 16 9
Median 19.87 26.55 22.50 13.78 24.76 23.70
Range 16.37 17.06 9.74 4.75 11.05 13.94
Top 5 de-en fr-en es-en en-de en-fr en-es
Median 22.26 27.93 26.43 15.21 26.62 26.61
Range 4.31 4.76 5.71 1.71 0.68 5.56
Table 1: Statistics of system outputs? BLEU scores
The range of BLEU scores cannot indicate the
similarity of the systems. The direction with the
most systems submitted is Spanish-English but
their respective performances are very close to
each other. As for the selected subset, the English-
French systems have the most similar performance
in terms of BLEU scores. The French-English
translations have the largest range in BLEU but the
similarity in this group is not the lowest.
de-en fr-en es-en en-de en-fr en-es
All 34.09 46.48 61.83 31.74 44.95 38.11
Selected 36.65 56.16 56.06 33.92 52.78 57.25
Table 2: Similarity of the system outputs
Ideally, we should select systems with highest
quality scores and lowest similarity scores. For
German-English, we selected the three with the
highest METEOR scores and another two with
high METEOR scores but low similarity scores to
the first three. For the other language directions,
we chose five systems from different institutions
with the highest scores.
3.4 Language models
We use a standard n-gram language model for
each target language using the monolingual train-
ing data provided in the translation task. These
LMs are thus specific to the same domain as the
input texts. Moreover, we also generate ?hypoth-
esis? LMs solely based on the given system out-
puts, that is, LMs that model how the candidate
systems convey information in the target language.
These LMs do not require any additional training
data. Therefore, we do not require any training
data other than the given system outputs by using
the ?hypothesis? language model and the ?hypoth-
esis? translation model.
3.5 Tuning
After building the models, it is essential to tune
the SMT system to optimize the feature weights.
We use Minimal Error Rate Training (Och, 2003)
to maximize BLEU on the complete development
data. Unlike the standard tuning procedure, we do
not tune the final system directly. Instead, we ob-
tain the weights using models built from the tuning
portion of the system outputs.
For each combination variant, we first train
models on the provided outputs corresponding to
the tuning set. This system, called the tuning sys-
tem, is also tuned on the tuning set. The initial
weights of any additional features not included in
the standard setting are set to 0. We then adapt the
weights to the system built with translations cor-
responding to the test set. The procedure and the
settings for building this system must be identical
to that of the tuning system.
4 Experiments
The purpose of this exercise is to understand the
nature of the system combination task in prac-
tice. Therefore, we restrict ourselves to the train-
ing data and system translations provided by the
shared task. The types of the systems that pro-
duced the translations are assumed to be unknown.
We report results for six translation directions be-
tween four languages.
4.1 Data and baseline
We build an SMT system from release v4 of the
Europarl corpus (Koehn, 2005), following a stan-
dard routine using the Moses toolkit. The sys-
tem also includes 5-gram language models trained
on in-domain corpora of the respective target lan-
guages using SRILM (Stolcke, 2002).
The systems in this paper, including the base-
line, are all tuned on the same 501-sentence tuning
set. Note also that the provided n-best outputs are
excluded in our experiments.
44
4.2 Results
The experiments include three different setups for
direct system combination, involving only hypoth-
esis translation models. System S0, the baseline
for this group, uses a hypothesis translation model
built with all available system translations and a
hypothesis LM (also from the machine-generated
outputs). S1 differs from S0 in that the LM in S1 is
generated from a large news corpus. S2 consists of
translation models built with only the five selected
systems. The BLEU scores of these systems are
shown in Table 3.
de-en fr-en es-en en-de en-fr en-es
Top 1 21.16 30.91 28.54 14.96 26.55 27.84
Mean 17.29 23.78 21.39 12.76 22.96 21.43
S0 20.46 27.50 23.35 13.95 27.29 25.59
S1 21.76 28.05 25.49 15.16 27.70 26.09
S2 21.71 24.98 27.26 15.62 24.28 25.22
Table 3: BLEU scores of direct system combina-
tion
When all outputs are included, the combined
system can always produce translations better than
most of the systems. When only a hypothesis LM
is used, the BLEU scores are always higher than
the average BLEU scores of the outputs. It even
outperforms the top system for English-French.
This simple setup (S0) is certainly a feasible so-
lution when no additional data is available and no
system evaluation is possible. This approach ap-
pears to be more effective on typically difficult
language pairs that involve German.
As for the systems with normal language mod-
els, neither of the systems ensure better transla-
tions. The translation quality is not completely
determined by the number of included translations
and their quality. On the other hand, the output
set with higher diversity (Table 2) usually leads
to better combination results. This observation is
consistent with the results from the system inte-
gration experiments shown in Table 4.
de-en fr-en es-en en-de en-fr en-es
Bas 19.13 25.07 24.55 13.59 23.67 23.67
Med 17.99 24.56 20.70 13.19 24.19 22.12
All 21.40 28.00 27.75 15.21 27.20 26.41
Top5 21.70 26.01 28.53 15.52 27.87 27.92
Table 4: BLEU scores of integrated SMT systems
(Bas: Baseline, Med: Median)
There are two variants in our experiments on
system integration. All in Table 4 represents the
system that integrates the complete hypothesis
translation model with the Europarl model, while
Top 5 refers to the system that incorporates the five
system-specific models separately. Both setups re-
sult in an improvement over the baseline Europarl-
based SMT system. BLEU scores increase by up
to 4.25 points. The integrated SMT system some-
times produces translations better than the best
system (7 out of 12 cases).
5 Conclusion
This work uses the Moses toolkit to combine
translations from multiple engines in a simple way.
The experiments on six translation directions show
interesting results: The final translations are al-
ways better than the majority of the given systems,
while the combination performs better than the
best system in half the cases. A similar approach
was applied to improve an existing SMT system
which was built in a domain different from the test
task. We achieved improvements in all cases.
There are many possible future directions to
continue this work. As we have shown, the qual-
ity of the combined system is more related to the
diversity of the involved systems than to the num-
ber of the systems or their quality. Hand-picked
systems lead to better combinations than those se-
lected by BLEU scores. It would be interesting
to develop a more comprehensive system selection
strategy.
Acknowledgments
This work was supported by the EuroMatrix
project (IST-034291) which is funded by the
European Community under the Sixth Frame-
work Programme for Research and Technological
Development.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison.
2007. Multi-engine machine translation with an
open-source SMT decoder. In Proceedings of
45
WMT07, pages 193?196, Prague, Czech Republic,
June. Association for Computational Linguistics.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to integrate mul-
tiple rule-based machine translation engines into a
hybrid system. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 179?
182, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Jesus Gimenez and Lluis Marquez. 2008. A smor-
gasbord of features for automatic MT evaluation.
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 195?198, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 98?107, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbs. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of Annual meet-
ing of the Association for Computation Linguis-
tics (acl), demonstration session, pages 177?180,
Prague, Czech, June.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit 2005.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33?40, Trento, Italy, April.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Antti-Veikko I. Rosti, Spyridon Matsoukas, and
Richard M. Schwartz. 2007. Improved word-level
system combination for machine translation. In
ACL.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In the 7th International
Conference on Spoken Language Processing (IC-
SLP) 2002, Denver, Colorado.
46
Coling 2010: Poster Volume, pages 570?578,
Beijing, August 2010
Using Syntactic and Semantic based Relations for Dialogue Act
Recognition
Tina Klu?wer, Hans Uszkoreit, Feiyu Xu
Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (DFKI)
Projektbu?ro Berlin
{tina.kluewer,uszkoreit,feiyu}@dfki.de
Abstract
This paper presents a novel approach to
dialogue act recognition employing multi-
level information features. In addition to
features such as context information and
words in the utterances, the recognition
task utilizes syntactic and semantic rela-
tions acquired by information extraction
methods. These features are utilized by
a Bayesian network classifier for our dia-
logue act recognition. The evaluation re-
sults show a clear improvement from the
accuracy of the baseline (only with word
features) with 61.9% to an accuracy of
67.4% achieved by the extended feature
set.
1 Introduction
Dialogue act recognition is an essential task for
dialogue systems. Automatic dialogue act clas-
sification has received much attention in the past
years either as an independent task or as an em-
bedded component in dialogue systems. Various
methods have been tested on different corpora us-
ing several dialogue act classes and information
coming from the user input.
The work presented in this paper is part of a
dialogue system called KomParse (Klu?wer et al,
2010), which is an application of a NL dialogue
system combined with various question answering
technologies in a three-dimensional virtual world
named Twinity, a web-based online product of the
Berlin startup company Metaversum1. The Kom-
Parse NPCs provide various services through con-
1http://www.metaversum.com/
versation with game users such as selling pieces
of furniture to users via text based conversation.
The main task of the input interpretation com-
ponent of the agent is the detection of the dialogue
acts contained in the user utterances. This classi-
fication is done via a cue-based method with var-
ious features from multi-level knowledge sources
extracted from the incoming utterance considering
a small context of the previous dialogue.
In contrast to existing systems using mainly
lexical features, i.e. words, single markers such as
punctuation (Verbree et al, ) or combinations of
various features (Stolcke et al, 2000) for the dia-
logue act classification, the results of the interpre-
tation component presented in this paper are based
on syntactic and semantic relations. The system
first gathers linguistic information coming from
different levels of deep linguistic processing sim-
ilar to (Allen et al, 2007). The retrieved informa-
tion is used as input for an information extraction
component that delivers the relations embedded in
the actual utterance (Xu et al, 2007). These rela-
tions combined with additional features (a small
dialogue context and mood of the sentence) are
then utilized as features for the machine-learning
based recognition.
The classifier is trained on a corpus originating
from a Wizard-of-Oz experiment which was semi-
automatically annotated. It contains automatically
annotated syntactic relations namely, predicate ar-
gument structures, which were checked and cor-
rected manually afterwards. Furthermore these re-
lations are enriched by manual annotation with se-
mantic frame information from VerbNet to gain an
additional level of semantic richness. These two
representations of relations, the syntax-based re-
570
lations and the VerbNet semantic relations, were
used in separate training steps to detect how much
the classifier can benefit from either notations.
A systematic analysis of the data has been con-
ducted. It turns out that a comparatively small set
of syntactic relations cover most utterances, which
can moreover be expressed by an even smaller set
of semantic relations. Because of this observation
as well as the overall performance of the classifier
the interpretation is extended with an additional
rule based approach to ensure the robustness of
the system.
The paper is organized as follows: Section 2
provides an overview about existing dialogue act
recognition systems and the features they use for
classification.
Section 3 introduces the original data used as ba-
sis for the annotation and the classification task.
In Section 4 the annotation that provides the nec-
essary information for the dialogue act classifi-
cation and involves the relation extraction is de-
scribed in detail. The annotation is split into three
main steps: The annotation of dialogue informa-
tion (section 4.1), the integration of syntactic in-
formation (section 4.2) and finally the manual an-
notation of VerbNet predicate and role informa-
tion in section 4.3.
Section 5 presents the results of the actual classifi-
cation task using different feature sets and in Sec-
tion 6 the results and methods are summarized.
Finally, Section 7 provides a brief description of
the rule-based interpretation and presents an out-
look on future work.
2 Related Work
Dialogue Acts (DAs) represent the functional
level of a speaker?s utterance, such as a greeting,
a request or a statement. Dialogue acts are ver-
bal or nonverbal actions that incorporate partic-
ipant?s intentions originating from the theory of
Speech Acts by Searle and Austin (Searle, 1969).
They provide an abstraction from the original in-
put by detecting the intended action of an utter-
ance, which is not necessarily inferable from the
surface input (see the two requests in the follow-
ing example).
Can you show me a red car please?
Please show me a red car!
To detect the action included in an utterance,
different approaches have been suggested in re-
cent years which can be clustered into two main
classes: The first class uses AI planning methods
to detect the intention of the utterance based on
belief states of the communicating agents and the
world knowledge. These systems are often part of
an entire dialogue system e.g. in a conversational
agent which provides the necessary information
about current beliefs and goals of the conversa-
tion participants at runtime. One example is the
TRIPS system (Allen et al, 1996). Because of the
huge amount of reasoning, systems in this class
generally gather as much linguistic information as
possible.
The second class uses cues derived from the
actual utterance to detect the right dialogue act,
mostly using machine learning methods. This
class gained much attention due to less computa-
tional costs. The probabilistic classifications are
carried out via training on labeled examples of
dialogue acts described by different feature sets.
Frequently used cues for dialogue acts are lexi-
cal features such as the words of the utterance or
ngrams of words for example in (Verbree et al,
), (Zimmermann et al, 2005) or (Webb and Liu,
2008). Although the performance of the classi-
fication task is difficult to compare, because of
the variety of different corpora, dialogue act sets
and algorithms used, these approaches do pro-
vide considerably good results. For example (Ver-
bree et al, ) achieve accuracy values of 89% on
the ICSI Meeting Corpus containing 80.000 ut-
terances with a dialogue act set of 5 distinct di-
alogue act classes and amongst others the features
?ngrams of words? and ?ngrams of POS informa-
tion?.
Another group of systems utilizes acoustic fea-
tures derived from Automatic Speech Recognition
for automatic dialogue act tagging (Surendran and
Levow, 2006), context features like the preceding
dialogue act or ngrams of previous dialogue acts
(Keizer and Akker, 2006).
However grammatical and semantic informa-
tion is not that often incorporated into feature sets,
with the exception of single features such as the
571
Dialogue Act Meaning Frequency
REQUEST The utterance contains a wish or demand 449
REQUEST INFO The utterance contains a wish or demand regarding information 154
PROPOSE The utterance serves as suggestion or showing of an object 216
ACCEPT The utterance contains an affirmation 167
REJECT The utterance contains a rejection 88
PROVIDE INFO The utterance provides an information 156
ACKNOWLEDGE The utterance is a backchannelling 9
Table 1: The used Dialogue Act Set
type of verbs or arguments or the presence or ab-
sence of special operators e.g. wh-phrases (An-
dernach, 1996). (Keizer et al, 2002) use among
others linguistic features like sentence type for
classification with Bayesian networks. Although
(Jurafsky et al, 1998) already noticed a strong
correlation between selected dialogue acts and
special grammatical structures, approaches using
grammatical structure were not very succesful.
While grammatical and semantic features are
not often incorporated into dialogue act recogni-
tion, they are a commonly used in related fields
like automatic classification of rhetorical rela-
tions. For example (Sporleder and Lascarides,
2008) and (Lapata and Lascarides, 2004) extract
verbs as well as their temporal features derived
from parsing to infer sentence internal temporal
and rhetorical relations. Their best model for
analysing temporal relations between two clauses
achieves 70.7% accuracy. (Subba and Eugenio,
2009) also show a significant improvement of a
discourse relation classifier incorporating compo-
sitional semantics compared to a model without
semantic features. Their VerbNet based frame se-
mantics yield in a better result of 4.5%.
3 The Data
The data serving as the basis for the relation iden-
tification as well as the training corpus for the di-
alogue act classifier is taken from a Wizard-of-Oz
experiment (Bertomeu and Benz, 2009) in which
18 users furnish a virtual living room with the help
of a furniture sales agent. Users buy pieces of fur-
niture and room decoration from the agent by de-
scribing their demands and preferences in a text
chat. During the dialogue with the agent, the pre-
ferred objects are then selected and directly put to
the right location in the apartment. In the exper-
iments, users spent one hour each on furnishing
the living room by talking to a human wizard con-
trolling the virtual sales agent. The final corpus
consists of 18 dialogues containing 3,171 turns
with 4,313 utterances and 23,015 alpha-numerical
strings (words). The following example shows a
typical part of such a conversation:
USR.1: And do we have a little side table for the TV?
NPC.1: I could offer you another small table or a side-
board.
USR.2: Then I?ll take a sideboard that is similar to my
shelf.
NPC.2: Let me check if we have something like that.
Table 2: Example Conversation from the Wizard-
of-Oz Experiment
4 Annotation
The annotation of the corpus is carried out in sev-
eral steps.
4.1 Pragmatic Annotation
The first annotation step consists of annotating
discourse and pragmatic information including di-
alogue acts, projects according to (Clark, 1996),
sentence mood, the topic of the conversation and
an automatically retrieved information state for
every turn of the conversations. From the anno-
tated information the following elements were se-
lected as features in the final recognition system:
? The dialogue acts which carry the intentions
of the actual utterance as well as the last pre-
ceding dialogue act. The set used for anno-
tation is a domain specific set containing the
dialogue acts shown in table 1.
? The sentence mood. Sentence mood was
annotated with one of the following values:
declarative, imperative, interrogative.
572
? The topic of the utterance. The topic value
is coreferent with the currently discussed ob-
ject. Topic can consist of an object class
(e.g. sofa) or an special object instance
(sofa 1836). The topic of the directly pre-
ceding utterance was chosen as a feature too.
4.2 Annotation with Predicate Argument
Structure
The second annotation step, applied to the ut-
terance level of the input, automatically enriches
the annotation with predicate argument structures.
Each utterance is parsed with a predicate argu-
ment parser and annotated with syntactic relations
organized according to PropBank (Palmer et al,
2005) containing the following features: Predi-
cate, Subject, Objects, Negation, Modifiers, Cop-
ula Complements.
A single relation mainly consists of a predi-
cate and the belonging arguments. Verb modi-
fiers like attached PPs are classified as ?argM?
together with negation (?argM neg?) and modal
verbs (?argM modal?). Arguments are labeled
with numbers according to the found information
for the actual structure. PropBank is organized in
two layers, the first one being an underspecified
representation of a sentence with numbered argu-
ments, the second one containing fine-grained in-
formation about the semantic frames for the predi-
cate comparable to FrameNet (Baker et al, 1998).
While the information in the second layer is sta-
ble for each verb, the values of the numbered ar-
guments can change from verb to verb. While
for one verb the ?arg0? may refer to the subject
of the verb, another verb may encapsulate a di-
rect object behind the same notation ?arg0?. This
is very complicated to handle in a computational
setup, which needs continuous labeling for the
successive components. Therefore the arguments
were in general named as in PropBank but con-
sistently numbered by syntactic structure. This
means for example that the subject is always la-
beled as ?arg1?.
Consider the example ?Can you put posters or
pictures on the wall??. The syntactic relation will
yield in the following representation:
<predicate: put>
<ArgM_modal: can>
<Arg1: you>
<Arg2: posters or pictures>
<ArgM: on the wall>
Predicate Argument Structure Parser The
syntactic predicate argument structure that consti-
tutes the syntactic relations and serves as basis for
the VerbNet annotation, is automatically retrieved
by a rule-based predicate argument parser. The
rules utilized by the parser describe subtrees of de-
pendency structures in XML by means of relevant
grammatical functions. For detecting verbs with
two arguments in the input, for instance, a rule
can be written describing the dependency struc-
ture for a verb with a subject and an object. This
rule would then detect every occurrence of the
structure ?Verb-Subj-Obj? in a dependency tree.
This sample rule would express the following con-
straints: The matrix unit should be of the part of
speech ?Verb? , The structure belonging to this
verb must contain a ?nsubj? dependency and an
?obj? dependency.
The rules deliver raw predicate argument struc-
tures, in which the detected arguments and the
verb serve as hooks for further information lookup
in the input. If a verb fulfills all requirements
described by the rule, in a second step all modi-
ficational arguments existing in the structure are
recursively acquired. The same is done for
modal arguments as well as modifiers of the ar-
guments such as determiners, adjectives or em-
bedded prepositions. After the generation of the
main predicate argument structure from the gram-
matical functions, the last step inserts the content
values present in the actual input into the structure
to get the syntactic relations for the utterance.
Before the input can be parsed with the predi-
cate argument parser, some preprocessing steps of
the corpus are needed. These include:
Input Cleaning The input data coming from the
users contain many errors. Some string
substitutions as well as the external Google
spellchecker were applied to the input before
any further processing.
Segmentation For clausal separation we apply a
simple segmentation via heuristics based on
punctuation.
POS Tagging Then the input is processed by
573
the external part-of-speech tagger TreeTag-
ger (Schmid, 1994).
The embedded dependency parser is the Stan-
ford Dependency Parser (de Marneffe and Man-
ning, 2008), but other dependency parsers could
be employed instead. The predicate argument
parser is an standalone software and can be used
either as a system component or for batch process-
ing of a text corpus.
4.3 VerbNet Frame Annotation
The last step of annotation consists of the man-
ual annotation of semantic predicate classes and
semantic roles. Moreover, the automatically de-
termined syntactic relations are checked and cor-
rected if possible. VerbNet (Schuler, 2005) is uti-
lized as a source for semantic information. The
VerbNet role set consists of 21 general roles used
in all VerbNet classes. Examples of roles in
this general role set are ?agent?, ?patient? and
?theme?.
For the manual addition of the semantic frame
information a web-based annotation tool has been
developed. The annotation tool shows the utter-
ance which should be annotated in the context of
the dialogue including the information from the
preceding annotation steps. All VerbNet classes
containing the current predicate are listed as pos-
sibilities for the predicate classification together
with their syntactic frames. The annotators can se-
lect the appropriate predicate class and frame ac-
cording to the arguments found in the utterance.
If an argument is missing in the input that is re-
quired in the selected frame a null argument is
added to the structure. If the right predicate class
is existing, but the predicate is not yet a member
of the class, it is added to the VerbNet files. In
case the right predicate class is found but the fit-
ting frame is missing, the frame is added to the
VerbNet files. Thus during annotation 35 new
members have been added to the existing VerbNet
classes, 4 Frames and 4 new subclasses. Via these
modifications, a version of VerbNet has been de-
veloped that can be regarded as a domain-specific
VerbNet for the sales domain.
During the predicate classification, the annota-
tors also assign the appropriate semantic roles to
the arguments belonging to the selected predicate.
The semantic roles are taken from the selected
VerbNet frame.
From the annotated semantic structure, seman-
tic relations are inferred such as the one in the fol-
lowing example:
<predicate: put-3.1>
<agent: you>
<theme: posters or pictures>
<destination: on the wall>
5 Dialogue Act Recognition
Two datasets are derived from the corpus: The
dataset containing the utterances of the users
(CST) and one dataset containing the utterances
of the wizard (NPC), whereas the NPC corpus is
cleaned from the ?protocol sentences?. Protocol
sentences are canned sentences the wizard used
in every conversation, for example to initialize
the dialogue. For the experiments, the two sin-
gle datasets ?NPC? and ?CST? as well as a com-
bined dataset called ?ALL? are used. Unfortu-
nately from the original 4,313 utterances in total,
many utterances could not be used for the final ex-
periments. First, fragments are removed and only
the utterances found by the parser to contain a
valid predicate argument structure are used. After
protocol sentences are taken out too, a dataset of
1702 valid utterances remains. Moreover, 292 ut-
terances are annotated to contain no valid dialogue
act and are therefore not suitable for the recogni-
tion task. Of the remaining utterances, 171 predi-
cate argument structures were annotated as wrong
because of completely ungrammatical input. In
this way we arrive at a dataset of 804 instances for
the users and 435 for the wizard, summing up to
1239 instances in total.
The features used for dialogue act recognition
exploit the information extracted from the differ-
ent annotation steps:
? Context features: The last preceding dia-
logue act, equality between the last preced-
ing topic and the actual topic, sentence mood
? Syntactic relation features: Syntactic predi-
cate class, arguments, negation
? VerbNet semantic relation features: VerbNet
predicate class, VerbNet frame arguments,
negation
574
? Utterance features: The original utterances
without any modifications
Different sets of features for training and eval-
uation are generated from these:
DATASET Syn: All utterances of the specified
dataset described via syntactic relation and
context features.
DATASET VNSem: All utterances of the speci-
fied dataset described via VerbNet semantic
relations and context features.
DATASET Syn Only: All utterances of the
specified dataset only described via the
syntactic relations.
DATASET VNSem Only: All utterances of the
specified dataset only described via the Verb-
Net semantic relations.
DATASET Context Only: All utterances of the
specified dataset described via the context
features and negation without any informa-
tion regarding relations.
DATASET Utterances Context: The utterances
of the specified dataset as strings combined
with the whole set of context features without
further relation extraction results.
DATASET Utterances: Only the utterances of
the specified dataset as strings. This and the
last ?Utterances?-set serve as baselines.
Dialogue Act Recognition is carried out via
the Bayesian network classifier AOEDsr from the
WEKA toolkit. AODEsr augments AODE, an
algorithm averaging over all of a small space
of alternative naive-Bayes-like models that have
weaker independence assumptions than naive
Bayes, with Subsumption Resolution (Zheng and
Webb, 2006). Evaluation is performed using
crossfolded evaluation.
All results of the experiments are given in terms
of accuracy.
Results for the dataset ?All? comparing the syn-
tactic relations with VerbNet relations as well as
the pure utterances and context are shown in table
4.
Dataset Accuracy
All Syn 67.4%
All VNSem 66.8%
All Utterances Context 61.9%
All Utterances 48.1%
Table 4: Dialogue Act Classification Results for
the ?ALL? Datasets
The best result is achieved with the syntactic in-
formation, although the VerbNet information pro-
vides an abstraction over the predicate classifica-
tion. Both the set containing the VerbNet relations
as well as the syntactic relations are much better
than the set containing only the context and the
original utterances. The dataset containing only
the utterances could not reach 50%.
Although the experiments show much better re-
sults using the relations instead of the original ut-
terance, the overall accuracy is not very satisfying.
Several reasons for this phenomenon come into
consideration. While it can to a certain extend be
the fault of the classifying algorithm (see table 8
for some tests with a ROCCHIO based classifier),
the main reason might as well lie in the impre-
cise boundaries of the dialogue act classes: Sev-
eral categories are hard to distinguish even for a
human annotator as you can see from the wrongly
classified examples in table 3. Another possibil-
ity can be the comparatively small number of total
training instances.
For the NPC dataset the results are slightly bet-
ter and much better still for the set CST, which
is due to a smaller number (6) of dialogue acts:
The dialogue act ?PROPOSE?, which is the act
for showing an object or proposing a possibility,
was not used by any user, but only by the wizard.
Dataset Accuracy
CST Syn 73.1%
NPC Syn 68.5%
Table 5: Dialogue Act Classification Results for
Datasets ?CST? and ?NPC?
To find out if one sort of features is espe-
cially important for the classification we reorga-
575
Utterance Right Classification Classified As
What do you think about this one? request info propose
Let see what you have and where we can put it request info request
Table 3: Wrongly classified instances
nize the training sets to contain only the context
features without the relations (All Context Only)
on the one hand and only the relational informa-
tion without the context features on the other hand
(All Syn Only and All VNSem Only). Results
are shown in table 6.
Dataset Accuracy
All Context Only 56.6%
All VNSem Only 53.5%
All Syn Only 50.8%
Table 6: Dialogue Act Classification Results for
Context and Relation sets
Table 6 shows that the results are considerably
worse if only parts of the features are used. The
set with context feature performs 3,1% better than
the best set with the relations only. Furthermore
the VerbNet semantic relation set leads to nearly
3% better accuracy, which may mean that the ab-
straction of semantic predicates provides a better
mapping to dialogue acts after all if used without
further features which may be ranked more impor-
tant by the classifier.
Besides the experiments with the Bayesian net-
works, additional experiments are performed us-
ing a modified ROCCHIO algorithm similar to the
one in (Neumann and Schmeier, 2002). Three dif-
ferent datasets were tested (see table 7).
Dataset Accuracy
All Utterances 70.1%
All Utterances Context 73.2%
All Syn 74.4%
Table 8: Dialogue Act Classification Results using
the ROCCHIO Algorithm
Table 8 shows that the baseline dataset contain-
ing only the utterances already provides much bet-
ter results with the ROCCHIO algorithm, deliv-
ering 70.1% which is more than 10% more ac-
curacy compared to the 48.1% of the Bayesian
classifier. If tested together with the context fea-
tures the accuracy of the utterance dataset raises to
73.2% and, after including the relational informa-
tion, even to 74.4%. Thus, the results of this ROC-
CHIO experiment also prove that the employment
of the relation information leads to improved ac-
curacy of the classification.
6 Conclusion
This paper reports on a novel approach to auto-
matic dialogue act recognition using syntactic and
semantic relations as new features instead of the
traditional features such as ngrams of words.
Different feature sets are constructed via an
automatic annotation of syntactic predicate argu-
ment structures and a manual annotation of Verb-
Net frame information. On the basis of this infor-
mation, both the syntactic relations as well as the
semantic VerbNet-based relations included in the
utterances can be extracted and added to the fea-
ture sets for the recognition task. Besides the re-
lation information the employed features include
information from the dialogue context (e.g. the
last preceding dialogue act) and other features like
sentence mood.
The feature sets have been evaluated with a
Bayesian network classifier as well as a ROC-
CHIO algorithm. Both classifiers demonstrate the
benefits gained from the relations by exploiting
the additionally provided information. While the
difference between the best baseline feature set
and the best relation feature set in the Bayesian
network classifier yields a 5,5% boost in accuracy
(61.9% to 67.4%), the ROCCHIO setup exceeds
the boosted accuracy by another 1,5% , starting
from a higher baseline of 73.2%. Based on the
observed complexity of the classification task we
expect that the benefit of the relational informa-
576
Predicate Instances Example
see-30.1 59 I would like to see a table in front of the sofa
put-9.1 74 Can you put it in the corner?
reflexive appearance-48.1.2 80 Show me the red one
own-100 137 Do you have wooden chairs?
want-32.1 153 I would like some plants over here
Table 7: The Main Semantic Relations Found in the Data Sorted by Predicate
tion may turn out to be even more significant on
larger learning data.
7 Future Work
The results in section 5 show that the pure classifi-
cation cannot be used as interpretation component
in isolation, but additional methods have to be in-
corporated. In a preceding analysis of the data
it was found that certain predicates are very fre-
quently uttered by the users. In the syntactic pred-
icate scenario the total number of different predi-
cates is 80, whereas the semantic predicates build
up a total number of 66. The class containing the
predicates with one to ten occurrences constitutes
137 of 1239 instances. The remaining 1101 in-
stances are covered by only 21 different predicate
classes. These predicates together with their ar-
guments constitute a set of common domain re-
lations for the sales domain. The main domain
relations found are shown in table 7.
The figures suggest that the interpretation at
least for the domain relations can be established in
a robust manner, wherefore the agent?s interpreta-
tion component was extended to a hybrid module
including a robust rule based method. To derive
the necessary rules a rule generator was developed
and the rules covering the used feature set (includ-
ing the context features, sentence mood and the
syntactic relations) were automatically generated
from the given data.
Future work will focus on the evaluation of
these automatically derived rules on a recently
collected but not yet annotated dataset from a sec-
ond Wizard-of-Oz experiment, carried out in the
same furniture sales setting.
Additional experiments are planned for evalu-
ating the relation-based features in dialogue act
recognition on other corpora tagged with differ-
ent dialogue acts in order to test the overall per-
formance of our classification approach on more
transparent dialogue act sets.
Acknowledgements
The work described in this paper was partially
supported through the project ?KomParse? funded
by the ProFIT program of the Federal State of
Berlin, co-funded by the EFRE program of the
European Union. Additional support came from
the project TAKE, funded by the German Min-
istry for Education and Research (BMBF, FKZ:
01IW08003).
577
References
Allen, James F., Bradford W. Miller, Eric K. Ringger,
and Teresa Sikorski. 1996. A robust system for nat-
ural spoken dialogue. In Proceedings of ACL 1996.
Allen, James, Mehdi Manshadi, Myroslava Dzikovska,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In DeepLP ?07: Pro-
ceedings of the Workshop on Deep Linguistic Pro-
cessing, Morristown, NJ, USA.
Andernach, Toine. 1996. A machine learning ap-
proach to the classification of dialogue utterances.
CoRR, cmp-lg/9607022.
Baker, Collin F., Charles J. Fillmore, and John B.
Lowe. 1998. The berkeley framenet project. In
Proceedings of COLING 1998.
Bertomeu, Nuria and Anton Benz. 2009. Annotation
of joint projects and information states in human-
npc dialogues. In Proceedings of CILC-09, Murcia,
Spain.
Clark, H.H. 1996. Using Language. Cambridge Uni-
versity Press.
de Marneffe, Marie C. and Christopher D. Manning.
2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, Manchester, UK.
Jurafsky, Daniel, Elizabeth Shriberg, Barbara Fox, and
Traci Curl. 1998. Lexical, prosodic, and syntactic
cues for dialog acts.
Keizer, Simon and Rieks op den Akker. 2006.
Dialogue act recognition under uncertainty using
bayesian networks. Nat. Lang. Eng., 13(4).
Keizer, Simon, Rieks op den Akker, and Anton Nijholt.
2002. Dialogue act recognition with bayesian net-
works for dutch dialogues. In Proceedings of the
3rd SIGdial workshop on Discourse and dialogue,
Morristown, NJ, USA.
Klu?wer, Tina, Peter Adolphs, Feiyu Xu, Hans Uszko-
reit, and Xiwen Cheng. 2010. Talking npcs in a
virtual game world. In Proceedings of the System
Demonstrations Section at ACL 2010.
Lapata, Mirella and Alex Lascarides. 2004. Inferring
sentence-internal temporal relations. In Proceed-
ings of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 153?160.
Neumann, Gu?nter and Sven Schmeier. 2002. Shal-
low natural language technology and text mining.
Ku?nstliche Intelligenz. The German Artificial Intel-
ligence Journal.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Comput. Linguist., 31(1).
Schmid, Helmut. 1994. In Proceedings of the Inter-
national Conference on New Methods in Language
Processing.
Schuler, Karin Kipper. 2005. Verbnet: a broad-
coverage, comprehensive verb lexicon. Ph.D. the-
sis, Philadelphia, PA, USA.
Searle, John R. 1969. Speech acts : an essay in
the philosophy of language / John R. Searle. Cam-
bridge University Press, London.
Sporleder, Caroline and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetori-
cal relations: A critical assessment. Natural Lan-
guage Engineering, 14(3).
Stolcke, Andreas, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van, and Ess dykema
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26.
Subba, Rajen and Barbara Di Eugenio. 2009. An ef-
fective discourse parser that uses rich linguistic in-
formation. In NAACL ?09, Morristown, NJ, USA.
Surendran, Dinoj and Gina-Anne Levow. 2006. Di-
alog act tagging with support vector machines and
hidden markov models. In Interspeech.
Verbree, A.T., R.J. Rienks, and D.K.J. Heylen.
Dialogue-act tagging using smart feature selection:
results on multiple corpora. In Raorke, B., editor,
First International IEEE Workshop on Spoken Lan-
guage Technology SLT 2006.
Webb, Nick and Ting Liu. 2008. Investigating the
portability of corpus-derived cue phrases for dia-
logue act classification. In Proceedings of COLING
2008, Manchester, UK.
Xu, Feiyu, Hans Uszkoreit, and Hong Li. 2007. A
seed-driven bottom-up machine learning framework
for extracting relations of various complexity. In
Proceedings of ACl (07), Prague, Czech Republic.
Zheng, Fei and Geoffrey I. Webb. 2006. Efficient
lazy elimination for averaged one-dependence esti-
mators. In ICML, pages 1113?1120.
Zimmermann, Matthias, Yang Liu, Elizabeth Shriberg,
and Andreas Stolcke. 2005. Toward joint seg-
mentation and classification of dialog acts in mul-
tiparty meetings. In Proc. Multimodal Interaction
and Related Machine Learning Algorithms Work-
shop (MLMI05, page 187.
578
Coling 2010: Poster Volume, pages 1354?1362,
Beijing, August 2010
Boosting Relation Extraction with Limited Closed-World Knowledge
Feiyu Xu Hans Uszkoreit Sebastian Krause Hong Li
Language Technology Lab
German Research Center for Artificial Intelligence (DFKI GmbH)
{feiyu,uszkoreit,sebastian.krause,lihong}@dfki.de
Abstract
This paper presents a new approach to im-
proving relation extraction based on min-
imally supervised learning. By adding
some limited closed-world knowledge for
confidence estimation of learned rules to
the usual seed data, the precision of re-
lation extraction can be considerably im-
proved. Starting from an existing base-
line system we demonstrate that utilizing
limited closed world knowledge can ef-
fectively eliminate ?dangerous? or plainly
wrong rules during the bootstrapping pro-
cess. The new method improves the re-
liability of the confidence estimation and
the precision value of the extracted in-
stances. Although recall suffers to a cer-
tain degree depending on the domain and
the selected settings, the overall perfor-
mance measured by F-score considerably
improves. Finally we validate the adapt-
ability of the best ranking method to a new
domain and obtain promising results.
1 Introduction
Minimally supervised machine-learning ap-
proaches to learning rules or patterns for relation
extraction (RE) in a bootstrapping framework are
regarded as very effective methods for building
information extraction (IE) systems and for
adapting them to new domains (e. g., (Riloff,
1996), (Brin, 1998), (Agichtein and Gravano,
2000), (Yangarber, 2001), (Sudo et al, 2003),
(Jones, 2005), (Greenwood and Stevenson,
2006), (Agichtein, 2006), (Xu et al, 2007),
(Xu, 2007)). On the one hand, these approaches
show very promising results by utilizing minimal
domain knowledge as seeds. On the other hand,
they are all confronted with the same problem,
i.e., the acquisition of wrong rules because of
missing knowledge for their validation during
bootstrapping. Various approaches to confidence
estimation of learned rules have been proposed
as well as methods for identifying ?so-called?
negative rules for increasing the precision value
(e.g., (Brin, 1998), (Agichtein and Gravano,
2000), (Agichtein, 2006), (Yangarber, 2003),
(Pantel and Pennacchiotti, 2006), (Etzioni et al,
2005), (Xu et al, 2007) and (Uszkoreit et al,
2009)).
In this paper, we present a new approach to esti-
mating or ranking the confidence value of learned
rules by utilizing limited closed-world knowl-
edge. As many predecessors, our ranking method
is built on the ?Duality Principle? (e. g., (Brin,
1998), (Yangarber, 2001) and (Agichtein, 2006)).
We extend the validation method by an evalu-
ation of extracted instances against some lim-
ited closed-world knowledge, while also allowing
cases in which knowledge for informed decisions
is not available. In comparison to previous ap-
proaches to negative examples or negative rules
such as (Yangarber, 2003), (Etzioni et al, 2005)
and (Uszkoreit et al, 2009), we implicitly gener-
ate many negative examples by utilizing the pos-
itive examples in the closed-world portion of our
knowledge. Rules extracting wrong instances are
lowered in rank.
In (Xu et al, 2007) and (Xu, 2007), we develop
a generic framework for learning rules for rela-
tions of varying complexity, called DARE (Do-
main Adaptive Relation Extraction). Furthermore,
there is a systematic error analysis of the base-
1354
line system conducted in (Xu, 2007). We employ
our system both as a baseline reference and as a
platform for implementing and evaluating our new
method.
Our first experiments conducted on the same
data used in (Xu et al, 2007) demonstrate: 1) lim-
ited closed-world knowledge is very useful and ef-
fective for improving rule confidence estimation
and precision of relation extraction; 2) integration
of soft constraints boosts the confidence value of
the good and relevant rules, but without strongly
decreasing the recall value. In addition, we val-
idate our method on a new corpus of newspaper
texts about celebrities and obtain promising re-
sults.
The remainder of the paper is organized as fol-
lows: Section 2 explains the relevant related work.
Sections 3 and 4 describe DARE and our exten-
sions. Section 5 reports the experiments with
two ranking strategies and their results. Section
6 gives a summary and discusses future work.
2 Related Work
In the existing minimally supervised rule learning
systems for relation extraction based on bootstrap-
ping, they already employ various approaches to
confidence estimation of learned rules and differ-
ent methods for identification of so-called nega-
tive rules. For estimation of confidence/relevance
values of rules, most of the approaches follow
the so-called ?Duality Principle? as mentioned by
Brin (1998) and Yangarber (2001), namely, the
confidence value of learned rules is dependent
on the confidence value of their origins, which
can be documents or relation instances. For ex-
ample, Riloff (1996), Yangarber (2001), Sudo et
al. (2003) and Greenwood and Stevenson (2006)
use domain relevance of documents in which pat-
terns are discovered as well as the distribution fre-
quency of these patterns in those relevant docu-
ments as an indication of good patterns. Their
methods are aimed at detecting all patterns for
a specific domain, but those patterns cannot be
applied directly to a specific relation. In con-
trast, systems presented by Brin (1998), Agichtein
and Gravano (2000), Agichtein (2006), Pantel
and Pennacchiotti (2006) as well as our base-
line system (Xu et al, 2007) are designed to
learn rules for a specific relation. They start with
some relation instances as their so-called ?seman-
tic seeds? and detect rules from texts matching
with these instances. The new rules are applied
to new texts for extracting new instances. These
new instances in turn are utilized as new seeds.
All these systems calculate their rule confidence
based on the confidence values of the instances
from which they stem. In addition to the confi-
dence value of the seed instances, most of them
also consider frequency information and include
some heuristics for extra validation. For exam-
ple, Agichtein (2006) intellectually defines certain
constraints for evaluating the truth value of ex-
tracted instances. But it is not clear whether this
strategy can be adapted to new domains and other
relations. In (Xu et al, 2007) we make use of do-
main relevance values of terms occurring in rules.
This method is not applicable to general relations.
Parallel to confidence estimation strategies, the
learning of negative rules is useful for identifying
wrong rules straightforwardly. Yangarber (2003)
and Etzioni et al (2005) utilize the so-called
Counter-Training for detecting negative rules for
a specific domain or a specific class by learning
from multiple domains or classes at the same time.
Examples of one certain domain or class are re-
garded as negative examples for the other ones.
Bunescu and Mooney (2007) follow a classifi-
cation-based approach to RE. They use positive
and negative sentences of a target relation for a
SVM classifier. Uszkoreit et al (2009) exploit
negative examples as seeds for learning further
negative instances and negative rules. The dis-
advantage of the above four approaches is that
the selected negative domains or classes or neg-
ative instances cover only a subset of the neg-
ative domains/classes/relations of the target do-
main/class/relation.
3 DARE Baseline System
Our baseline system DARE is a minimally super-
vised learning system for relation extraction, ini-
tialized by so-called ?semantic seeds?, i.e., exam-
ples of the target relations, labelled with their se-
mantic roles. The system supports domain adap-
tation through a compositional rule representation
and a bottom-up rule discovery strategy. In this
1355
way, DARE can handle target relations of varying
arity. The following example is a relation instance
of the target relation from (Xu, 2007) concerning
Nobel Prize awards: <Mohamed ElBaradei, No-
bel, Peace, 2005>. The target relation contains
four arguments: WINNER, PRIZE NAME, PRIZE AREA
and YEAR. This example refers to an event men-
tioned in the sentence in example (1).
(1) Mohamed ElBaradei, won the 2005 Nobel
Prize for Peace on Friday because of ....
Figure 1 is a simplified dependency tree of ex-
ample (1). DARE utilizes a bottom-up rule dis-
covery strategy to extract rules from such depen-
dency trees. All sentences are processed with
named entity recognition and dependency parsing.
?win?subject
wwnnnn
n object
''PPP
PP
Winner ?Prize?lex-mod
ssggggg
ggggg
ggg
lex-mod  mod ''O
OOOO
Year Prize ?for?
pcomp-n 
Area
Figure 1: Dependency tree for example (1)
From the tree in Figure 1, DARE learns three
rules. The first rule is dominated by the prepo-
sition ?for?, extracting the argument PRIZE AREA
(Area). The second rule is dominated by the noun
?Prize?, extracting the arguments YEAR (Year) and
PRIZE NAME (Prize), and calling the first rule for
the argument PRIZE AREA (Area). The rule ?win-
ner prize area year 1? from Figure 2 extracts all
four arguments from the verb phrase dominated
by the verb ?win? and calls the second rule to
handle the arguments embedded in the linguistic
argument ?object?.
Rule name :: winner prize area year 1
Rule body ::?
????????????
head
?
?
pos verb
mode active
lex-form ?win?
?
?
daughters <
[
subject
[
head 1 Winner
]]
,
?
??object
?
?
rule year prize area 1 ::
< 4 Year, 2 Prize,
3 Area >
?
?
?
??>
?
????????????
Output :: < 1 Winner, 2 Prize, 3 Area, 4 Year >
Figure 2: DARE extraction rule.
We conduct a systematic error analysis based
on our experiments with the Nobel Prize award
data (Xu, 2007). The learned rules are divided
into four groups: good, useless, dangerous and
bad. The good rules are rules that only extract cor-
rect instances, while bad ones exclusively produce
wrong instances. Useless rules are those that do
not detect any new instances. Dangerous rules are
dangerous because they extract both correct and
wrong instances. Most good rules are rules with
high specificity, namely, extracting all or most ar-
guments of the target relation. The 14.7% extrac-
tion errors are from bad rules and dangerous rules.
Other errors are caused by wrong reported con-
tent, negative modality, parsing and named entity
recognition errors.
4 Our Approach: Boosting Relation Ex-
traction
4.1 Closed-World Knowledge: Modeling and
Construction
The error analysis of DARE confirms that the
identification of bad rules or dangerous rules is
important for the precision of an extraction sys-
tem. Using closed-world knowledge with large
numbers of implicit negative instances opens a
possibility to detect such rules directly. In our
work, closed-world knowledge for a target rela-
tion is the total set of positive relation instances
for entire relations or for some selected subsets
of individuals. For most real world applications,
closed-world knowledge can only be obtained for
relatively small subsets of individuals participat-
ing in the relevant relations. We store the closed-
world knowledge in a relational database, which
we dub ?closed-world knowledge database? (abbr.
cwDB). Thus, a cwDB for a target relation should
fill the following condition:
A cwDB must contain all correct relation
instances (insts) for an instantiation value
(argValue) of a selected relation argument
cwArg in the target relation.
Given R (the total set of relation instances of a
target relation), a cwDB is defined as follows:
cwDB={inst ? R : cwArg(inst) = argValue}.
An example of a cwDB is the set of all prize win-
ners of a specific prize area such as Peace, where
PRIZE AREA is the selected cwArg and argValue is
Peace. Note that the merger of two cwDBs, for
example with PRIZE AREAs Peace and Literature,
is again a cwDB (with two argValues in this case).
1356
4.2 Modified Learning Algorithm
In Algorithm 1, we present the modification of the
DARE algorithm (Xu, 2007). The basic idea of
DARE is that it takes some initial seeds as input
and learns relation extraction rules from sentences
in the textual corpus matching the seeds. Given
the learned rules, it extracts new instances from
the texts. The modified algorithm adds the val-
idate step to evaluate the new instances against
the closed-world knowledge cwDB. Based on the
evaluation result, both new instances and learned
rules are ranked with a confidence value.
INPUT: initial seeds
1 i? 0 (iteration of bootstrapping)
2 seeds ? initial seeds
3 all instances ? {}
4 while (seeds 6= {})
5 rulesi ? getRules(seeds)
6 instancesi ? getInstances(rulesi)
7 new instancesi ? instancesi ? all instances
8 validate(new instances i , cwDB)
9 rank(new instancesi)
10 rank(rulesi)
11 seeds ? new instancesi
12 all instances ? all instances + new instancesi
13 i? i+ 1
OUTPUT: all instances
Algorithm 1: Extended DARE
4.3 Validation against cwDB
Given a cwDB of a target relation and its argValue
of its selected argument cwArg, the validation of
an extracted instance (inst) against the cwDB is
defined as follows.
inst correct ? inst ? cwDB (1)
inst wrong ? inst 6? cwDB ?
cwArg(inst) = argValue
inst unknown ? ( inst 6? cwDB ?
cwArg(inst) 6= argValue )
? ( inst 6? cwDB ?
cwArg(inst) is unspecified )
4.4 Rule Confidence Ranking with cwDB
We develop two rule-ranking strategies for con-
fidence estimation, in order to investigate the
best way of integrating the closed-world knowl-
edge: (a) exclusive ranking: This ranking strat-
egy excludes every rule which extracts wrong in-
stances after their validation against the closed-
world knowledge; (b) soft ranking: This ranking
strategy is built on top of the duality principle and
takes specificity and the depth of learning into ac-
count.
Exclusive Ranking The exclusive ranking
method is a very naive ranking method which
estimates the confidence value of a learned rule
(e.g., rule) depending on the truth value of its
extracted instances (getInstances(rule)) against
a cwDB. Any rule with one wrong extraction
is regarded as a bad rule in this method. This
method works effectively in a special scenario
where the total list of the instances of the target
relation is available as the cwDB.
confidence(rule) =
{
1 if getInstances(rule) ? cwDB,
0 otherwise. (2)
Soft Ranking The soft ranking method works
in the spirit of the ?Duality Principle?, the con-
fidence value of rules is dependent on the truth
value of their extracted instances and on the seed
instances from which they stem. The confi-
dence value of the extracted instances is estimated
based on their validation against the cwDB or the
confidence value of their ancestor seed instances
from which their extraction rules stem. Further-
more, the specificity of the instances (percentage
of the filled arguments) and the learning depth
(iteration step of bootstrapping) are parameters
too. The definition of instance scoring, namely,
score(inst), is given as follows:
score(inst) =
?
?
?
? > 0 if validate(inst , cwDB) = correct,
0 if validate(inst , cwDB) = wrong,
UN inst if validate(inst , cwDB) = unknown.
(3)
As defined above, if a new instance is con-
firmed as correct by the cwDB, it will obtain a
positive value. In our experiment, we set ?=10
in order to boost the precision. In the case of un-
known about its truth value, the confidence value
of a new instance (inst) is dependent on the confi-
dence values of the seed instances (ancestor seeds)
from which its mother rules (Rinst ) stem. Below,
the scoring of the unknown case, namely, UN inst ,
is defined, where Rinst are rules that extract the
new instance inst , while Irule are instances from
which a rule inRinst is learned and ? is the speci-
ficity value of inst while ? is utilized to express
the noisy potential of each further iteration during
bootstrapping.
1357
UN inst =
?
rule?Rinst
(?
j?Irule score(j)
|Irule | ? ?
irule
)
|Rinst |
? ?
where
Rinst = getMotherRulesOf(inst),
Irule = getMotherInstancesOf(rule),
? = specificity,
? = 0.8,
irule = i-th iteration where rule occurs
(4)
Given the scoring of instance inst , the confidence
estimation of a rule is the average score of all
insts extracted by this rule:
confidence(rule) =
?
inst?I score(inst)
|I|
where I = getInstances(rule) (5)
5 Experiments
5.1 Corpora and Closed-World Knowledge
We conduct our experiments with two different
domains. We start with the Nobel Prize award do-
main reported in (Xu, 2007) and apply our method
to the same corpus, a collection from various on-
line newspapers. The target relation is the one
with the four arguments as mentioned in Sec-
tion 3. In this way, we can compare our results
with those reported in (Xu, 2007). Furthermore,
all Nobel Prize winners can be found from http:
//nobelprize.org, so it is easy to construct
a cwDB for Nobel Prize winners. We take the
PRIZE AREA as our selected argument for closing
sub-relations and construct various cwDBs with
the instantiation of this argument (e.g., all win-
ners of Nobel Peace Prize). The second domain
is about celebrities. Our text corpus is collected
from tabloid newspaper texts, containing 6850 ar-
ticles from the years 2001 and 2002. The target
relation is the marriage relationship between two
persons. We construct a cwDB of 289 persons in
which we have listed all their (ex-)spouses as well
as the time span of the marriage relation.
Table 1 summarizes the size of the corpus data
of the two domains.
Domain Space #Doc.
Nobel Prize 18,4 MB 3328
Celebrity Marr. 16,6 MB 6850
Table 1: Corpus data.
5.2 Nobel Prize Domain
We apply the extended DARE system to the Nobel
Prize corpus at first and conduct two rule rank-
ing strategies with different sizes of the cwDB.
We conduct all our experiments with the seed
<Guenter Grass, Nobel, Literature, 1999>. The
DARE-Baseline performance is shown in Table 2.
Precision Absolute Recall
Baseline 77.98% 89.01%
Table 2: DARE-Baseline Performance
Exclusive Ranking
Given the complete list of Nobel Laureates, we
can apply the exclusive ranking strategy to this do-
main. Our cwDB is the total list of Nobel Prize
winners. The wrong instances will not be used as
seed for the next iteration. Rules that extracted
at least one wrong instance are marked as bad, the
other rules as good. We utilize only the good rules
for relation extraction.
Prec. Rel. Recall Rel. F-Measure
100.00% 82.88% 90.64%
Table 3: Performance of Exclusive Ranking in
Nobel Prize award domain.
In comparison to the DARE baseline system,
given the same seed setup, this experiment results
in a precision boost from 77.98% to 100% (see
Table 3). This is not surprising since the cwDB
covers all relation instances for the target rela-
tion. Nevertheless, this experiment shows that the
closed-world knowledge approach is effective to
exclude bad rules. However, the recall decreases
and is only 82.88% of the one of the baseline sys-
tem. As we explain above, not all rules extracting
wrong instances are bad rules because wrong ex-
tractions can also be caused by other error sources
such as named entity recognition. Therefore, even
good rules can be excluded because of other er-
ror sources. The exclusive ranking strategy is use-
ful for application scenarios where people want to
learn rules for achieving 100% precision perfor-
mance and do not expect high recall. It is espe-
cially effective when a big cwDB is available.
Soft Ranking
This ranking strategy does not exclude any
rules and assigns a score to each rule based on
1358
the definition in Section 4.4. Rules which extract
correct instances, more specific relation instances
and stem from high-scored seed instances obtain
a better value than others. In our approach, the
specificity is dependent on the number of the ar-
guments in the extracted instances. For this do-
main, the most specific instances contain all four
arguments. In the following, we conduct two ex-
periments with two different sizes of the cwDB:
1) with the total list of winners (complete cwDB)
and 2) with only winners in one PRIZE AREA (lim-
ited cwDB).
1) Complete closed-world database Figure 3
displays the correlation between the score of rules
and their extraction precision performance. Each
point stands for a set of rules with the same
score and extraction precision. In this setup, the
higher the score, the higher the precision. Given
the scored rules, Figure 4 depicts precision, re-
call and F-Measure for different score thresholds.
For a given threshold j we take all rules with
score(rule) ? j and use the instances they ex-
tract. The recall value here is the relative recall
w. r. t. to the DARE baseline performance: i. e. the
number of correct extracted instances divided by
the number of correct instances extracted by the
DARE baseline system. The F-Measure value is
calculated by using the relative recall values, we
therefore refer to it as the relative F-Measure. If
the system takes all rules with score ? 7, the sys-
tem achieves the best relative F-Measure.
  0 1 2 3 4 5 6 7 8 9 100,00%10,00%
20,00%30,00%
40,00%50,00%
60,00%70,00%
80,00%90,00%
100,00%
Rule-Score
Correctne
ss of extra
cted insta
nces
Figure 3: Rule scores vs. precisions with the
complete closed-world database.
2) Limited closed-world database This experi-
ment investigates the system performance in cases
in which only a limited cwDB is available. This is
the typical situation for most real world RE appli-
cations. Therefore, this experiment is much more
  0 012 3 312 4 412 5 512 6 612 2 212 7 712 8 812 9 912 , ,12 3020%00R22%00R
70%00R72%00R80%00R
82%00R90%00R92%00R
,0%00R,2%00R300%00R
ule-ScSorCtrcnsr-ec  efsnSxeC e-sff  efsnSxeCadiesc?le
??lec?of?C?olC ?fed?-ole
Figure 4: Performance with the complete closed-
world database.
important than the previous one. We construct
a smaller database containing only Peace Nobel
Prize winners, which is about 1/8 of the previous
complete cwDB.
  0 1 2 3 4 5 6 7 8 9 100,00%10,00%
20,00%30,00%
40,00%50,00%
60,00%70,00%
80,00%90,00%
100,00%
Rule-Score
Correctne
ss of extra
cted insta
nces
Figure 5: Rule score vs. precision with the lim-
ited closed-world database
  0 012 3 312 4 412 5 512 6 612 2 212 7 712 8 812 9 912 , ,12 300%00R30%00R
40%00R50%00R60%00R
20%00R70%00R80%00R
90%00R,0%00R300%00R
ule-ScSorCtrcnsr-ec  efsnSxeC e-sff  efsnSxeCadiesc?le
??lec?of?C?olC ?fed?-ole
Figure 6: Performance with the limited closed-
world database
Figure 5 shows the correlation between the
score of the rules and their extraction precision.
Although the development curve here is not as
smooth as depicted in Figure 3, the higher scored
rules have better precision values than most of the
lower scored rules. However, we can observe that
some very good rules are scored low, located in
1359
Thresh. Good Dangerous Bad
Baseline 58.94% 26.49% 14.57%
1 64.96% 29.20% 5.84%
2 66.67% 27.91% 5.43%
3 69.23% 26.50% 4.27%
4 73.27% 23.76% 2.97%
5 76.00% 22.67% 1.33%
6 77.59% 20.69% 1.72%
7 77.50% 22.50% 0.00%
8 87.50% 12.50% 0.00%
9 85.71% 14.29% 0.00%
10 90.00% 10.00% 0.00%
Table 4: Quality analysis of rules with the limited
closed-world database
the left upper corner. The reason is that many of
their extracted instances are unknown, even if their
extracted instances are mostly correct.
As shown in Figure 6, even with the limited
cwDB, the precision values are comparable with
the complete cwDB (see Figure 4). However, the
recall value drops much earlier than with the com-
plete cwDB. With a threshold of score 4, the sys-
tem achieves the best modified F-Measure 92,21%
with an improvement of precision of about 11 per-
centage points compared to the DARE baseline
system (89.39% vs. 77.98%). These results show
that even with a limited cwDB this ranking system
can help to improve the precision without loosing
too much recall.
We take a closer look on the useful (actively ex-
tracting) rules and their extraction performance,
using the same rule classification as (Xu, 2007).
As shown in Table 4, more than one fourth of
the extraction rules created by the baseline system
are dangerous ones and almost 15% are plainly
wrong. Applying the rule scoring with the limited
cwDB increases the fraction of good rules to al-
most three quarters and nearly eliminates all bad
rules at threshold 4. By choosing higher thresh-
olds, surviving good rules raises to 90%. The total
remaining set of rules then only consists of rules
that at least partially extract correct instances.
5.3 Celebrity Domain
As presented above, the soft ranking method de-
livers very promising result. In order to val-
idate this ranking method, we choose an ad-
ditional domain and decide to learn marriage
relations among celebrities, where the target
relation consists of the following arguments:
[ NAME OF SPOUSE, NAME OF SPOUSE, YEAR].
The value of the marriage year is valid when
the year is within the marriage time interval. The
motivation of selecting this target relation is the
large number of possible relations between two
persons leading to dangerous or even bad rules.
For example, the rule in Figure 7 is a very dan-
gerous rule because ?meeting? events of two mar-
ried celebrities are often reported. A good confi-
dence estimation method is very useful for boost-
ing the good rules like the one in Figure 8. From
our text corpus we extract 37.000 sentences that
mention at least two persons. The cwDB con-
sists of sample relation instances, in which one
NAME OF SPOUSE is instantiated, i. e. we manu-
ally construct a database which contains all (ex-)
spouses of 289 celebrities.
head([SPOUSE<ne_person>]),
mod({head(("meet", VB)),
subj({head([SPOUSE<ne_person>])})})
Figure 7: A dangerous extraction rule example
head(("marry", VB)),
aux({head(("be", VB))}),
dep({head([SPOUSE<ne_person>]),
dep({head([DATE<point>])})}),
nsubj({head([SPOUSE<ne_person>])})
Figure 8: Example of a positive rule
Since a gold standard of mentions for this cor-
pus is not available, we manually validate 100 ran-
dom samples from each threshold group. This
evaluation gives us an opportunity to estimate the
effect of a cwDB in this domain. Table 5 presents
the performance of the rules with different thresh-
olds. The precision value of the baseline system
is very low. Threshold 3 slightly improves the
precision of the DARE baseline without damag-
ing recall too much. Step 4 excludes dangerous
rules such as the one in Figure 7 which drastically
boosts the precision. Unfortunately, the exclusion
of such general rules leads to the loss of many cor-
rect relation instances too, therefore, the immense
drop of recall from threshold 3 to 4 as well as from
threshold 4 to 5. Positive extraction rules such as
Figure 8 are quite highly scored. Because of the
large number of rules and instances, we start the
quality analysis of rules with score 3. As the table
indicates, the use of the rule scoring in this domain
clearly improves the quality of the created extrac-
tion rules. The error analysis shows that the ma-
jor error resource for this domain is wrong coref-
erence resolution or identity resolution. For ex-
1360
Thresh. # Instances Prec. Rel. Rec. Rel. F-Meas. # Rules Good Dangerous Bad
Baseline 25183 9.00% 100.00% 16.51% 12258
1 19806 7.00% 61.17% 12.56% 562
2 14542 9.00% 57.75% 15.57% 159
3 11259 15.00% 74.51% 24.97% 121 19.83% 33.88% 46.28%
4 788 65.00% 22.60% 33.54% 72 25.00% 27.78% 47.22%
5 195 67.00% 5.76% 10.62% 29 37.93% 17.24% 44.83%
6 115 84.00% 4.26% 8.11% 11 45.45% 27.27% 27.27%
7 55 89.09% 2.16% 4.22% 6 50.00% 33.33% 16.67%
8 9 77.78% 0.31% 0.62% 4 75.00% 0.00% 25.00%
9 5 60.00% 0.13% 0.26% 3 66.67% 0.00% 33.33%
10 5 60.00% 0.13% 0.26% 3 66.67% 0.00% 33.33%
Table 5: Soft ranking for the celebrity marriage domain with a limited cwDB.
ample, the inability to distinguish Prince Charles
(former husband of British princess Diana) from
Charles Spencer (her brother) is the reason that
DARE crosses the border between the marriage
and the sibling relation. In comparison to the
Nobel Prize award event, the marriage relation
between persons is often used as additional in-
formation to a person which is involved in a re-
ported event. Therefore, anaphoric references oc-
cur more often in their mentionings, as the exam-
ple relation in (3).
(3) ?My kids, I really don?t like them to
watch that much television,? said :::::Cruise, 40, whoadopted Isabella and Connor while ::he was mar-ried to second wife Nicole Kidman.
6 Summary
We propose a new way in which prior knowledge
about domains can be efficiently used as addi-
tional criteria for confidence estimation of learned
new rules or new instances in a minimally su-
pervised machine learning framework. By intro-
ducing rule scoring on the basis of available do-
main knowledge (the cwDB), rules can be eval-
uated during the bootstrapping process with re-
spect to their extraction precision. The results
are rather promising. The rule score threshold is
an easy way for users of an extraction system to
adjust the precision-recall-trade-off to their own
needs. The rule estimation method is also general
enough to extend to integration of common sense
knowledge. Although the relation instances in
the closed-world knowledge database can also be
used as seed in the beginning, the core idea of our
research work is to develop a general confidence
estimation strategy for discovered new informa-
tion. As discussed in (Xu, 2007) and (Uszkoreit
et al, 2009), the size of seed is not always rele-
vant for the learning and extraction performance,
in particular if the data corpus exhibits the small
world property. Using all instances in the cwDB
as seed, our experiments with the baseline system
yield worse precision performance than the modi-
fied DARE algorithm with only one seed instance.
This approach is quite general and easily adapt-
able to many domains; the only prerequisite is
the existence of a database with relation instances
from the target domain with a fulfilled closed-
world property on some relational argument. A
database of this kind should be easily obtainable
for many domains, e. g. by exploiting structured
and semi-structured information sources in the In-
ternet, such as YAGO (Suchanek et al (2007)) and
DBpedia (Bizer et al (2009)). Furthermore, in
some areas, such as Business Intelligence, there
is nearly complete knowledge already present for
past years, while the task is to extract informa-
tion only from recent news articles. Construct-
ing closed-worlds out of the present knowledge to
improve the learning of new information is there-
fore a straightforward approach. Even the manual
collection of suitable data might be a reasonable
choice since appropriate closed worlds could be
rather small if cwDBis chosen properly.
Acknowledgments
The work presented here has been partially sup-
ported through the prject KomParse by the ProFIT
program of the Federal State of Berlin which in
turn is co-funded by the EFRE program of the
European Union. It is additionally supported
through a grant to the project TAKE, funded by
the German Ministry for Education and Research
(BMBF, FKZ: 01IW08003).
1361
References
Agichtein, Eugene and Luis Gravano. 2000. Snow-
ball: extracting relations from large plain-text col-
lections. In DL ?00: Proceedings of the fifth ACM
conference on Digital libraries, pages 85?94, New
York, NY, USA. ACM.
Agichtein, Eugene. 2006. Confidence estimation
methods for partially supervised information extrac-
tion. In Proceedings of the Sixth SIAM International
Conference on Data Mining, Bethesda, MD, USA,
April. SIAM.
Bizer, Christian, Jens Lehmann, Georgi Kobilarov,
So?ren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia - a crys-
tallization point for the web of data. Journal of Web
Semantics, 7(3):154?165.
Brin, Sergey. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extending
Database Technology, EDBT?98.
Bunescu, Razvan C. and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics.
Etzioni, Oren, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial Intelligence,
165(1):91 ? 134.
Greenwood, Mark A. and Mark Stevenson. 2006. Im-
proving semi-supervised acquisition of relation ex-
traction patterns. In Proceedings of the Workshop
on Information Extraction Beyond The Document,
pages 29?35, Sydney, Australia, July. Association
for Computational Linguistics.
Jones, R. 2005. Learning to Extract Entities from La-
beled and Unlabeled Text. Ph.D. thesis, University
of Utah.
Pantel, Patrick and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia, July. The Association for Computer Lin-
guistics.
Riloff, Ellen. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings
of Thirteenth National Conference on Artificial In-
telligence (AAAI-96), pages 1044?1049. The AAAI
Press/MIT Press.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In 16th international World Wide Web con-
ference (WWW 2007), New York, NY, USA. ACM
Press.
Sudo, K., S. Sekine, and R. Grishman. 2003. An im-
proved extraction pattern representation model for
automatic IE pattern acquisition. Proceedings of
ACL 2003, pages 224?231.
Uszkoreit, Hans, Feiyu Xu, and Hong Li. 2009. Anal-
ysis and improvement of minimally supervised ma-
chine learning for relation extraction. In 14th In-
ternational Conference on Applications of Natural
Language to Information Systems. Springer.
Xu, Feiyu, Hans Uszkoreit, and Hong Li. 2007. A
seed-driven bottom-up machine learning framework
for extracting relations of various complexity. In
Proceedings of ACL 2007, 45th Annual Meeting
of the Association for Computational Linguistics,
Prague, Czech Republic, June.
Xu, Feiyu. 2007. Bootstrapping Relation Extraction
from Semantic Seeds. Phd-thesis, Saarland Univer-
sity.
Yangarber, Roman. 2001. Scenarion Customization
for Information Extraction. Dissertation, Depart-
ment of Computer Science, Graduate School of Arts
and Science, New York University, New York, USA.
Yangarber, Roman. 2003. Counter-training in dis-
covery of semantic patterns. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 343?350, Sapporo Con-
vention Center, Sapporo, Japan, July.
1362
Proceedings of the EACL 2009 Demonstrations Session, pages 13?16,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
GOSSIP GALORE
A Self-Learning Agent for Exchanging Pop Trivia
Xiwen Cheng, Peter Adolphs, Feiyu Xu, Hans Uszkoreit, Hong Li
DFKI GmbH, Language Technology Lab
Stuhlsatzenhausweg 3, D-66123 Saarbru?cken, Germany
{xiwen.cheng,peter.adolphs,feiyu,uszkoreit,lihong}@domain.com
Abstract
This paper describes a self-learning soft-
ware agent who collects and learns knowl-
edge from the web and also exchanges her
knowledge via dialogues with the users.
The agent is built on top of information
extraction, web mining, question answer-
ing and dialogue system technologies, and
users can freely formulate their questions
within the gossip domain and obtain the
answers in multiple ways: textual re-
sponse, graph-based visualization of the
related concepts and speech output.
1 Introduction
The system presented here is developed within the
project Responsive Artificial Situated Cognitive
Agents Living and Learning on the Internet (RAS-
CALLI) supported by the European Commission
Cognitive Systems Programme (IST-27596-2004).
The goal of the project is to develop and imple-
ment cognitively enhanced artificial agents, using
technologies in natural language processing, ques-
tion answering, web-based information extraction,
semantic web and interaction driven profiling with
cognitive modelling (Krenn, 2008).
This paper describes a conversational agent
?Gossip Galore?, an active self-learning system
that can learn, update and interpret information
from the web, and can make conversations with
users and provide answers to their questions in the
domain of celebrity gossip. In more detail, by
applying a minimally supervised relation extrac-
tion system (Xu et al, 2007; Xu et al, 2008), the
agent automatically collects the knowledge from
relevant websites, and also communicates with the
users using a question-answering engine via a 3D
graphic interface.
This paper is organized as follows. Section 2
gives an overview of the system architecture and
Figure 1: Gossip Galore responding to ?Tell me
something about Carla Bruni!?
presents the design and functionalities of the com-
ponents. Section 3 explains the system setup and
discusses implementation details, and finally Sec-
tion 4 draws conclusions.
2 System Overview
Figure 1 shows a use case of the system. Given a
query ?Tell me something about Carla Bruni?, the
application would trigger a series of background
actions and respond with: ?Here, have a look at
the personal profile of Carla Bruni?. Meanwhile,
the personal profile of Carla Bruni, would be dis-
played on the screen. The design of the interface
reflects the domain of celebrity gossip: the agent
is depicted as a young lady in 3D graphics, who
communicates with users. As an additional fea-
ture, users can access the dialogue memory of the
system, which simulates the human memory in di-
alogues. An example of the dialogue memory is
sketched in Figure 2.
As shown in Figure 3, the system consists of a
number of components. In principle, first, a user?s
query is linguistically analyzed, and then inter-
13
Dialogue
State
Dialogue
Memory
MM GeneratorResponseHandler
NE RecognizerSpellChecker Parser
Anaphora
Resolver
Knowledge
Base
Web
Miner
Input
Interpreter
Input
Analyzer
Relation
Extractor
Information
Wrapper
NL Generator
Conversational
Agent
Figure 3: Agent architecture and interaction of components
Figure 2: Representation of Social Network in Di-
alogue Memory
preted with respect to the context of the dialogue.
A Response Handler will then consult the knowl-
edge base pre-constructed by extracting relevant
information from the Web, and pass the answer, in
an abstract representation, to a Multimodal Gener-
ator, which realizes and presents the answer to the
user in multiple ways. The main components are
described in the following sections.
2.1 Knowledge Base
The knowledge base is automatically built by the
Web Miner. It contains knowledge regarding prop-
erties of persons or groups and their social rela-
tionships. The persons and groups that we concern
are celebrities in the entertainment industry (e.g.,
singers, bands, or movie stars) and their relatives
(e.g., partners) and friends. Typical properties of a
person include name, gender, birthday, etc., and
profiles of celebrities contain additional proper-
ties such as sexual orientation, home pages, stage
names, genres of their work, albums, and prizes.
Social relationships between the persons/groups
such as parent-child, partner, sibling, influenc-
ing/influenced and group-member, are also stored.
2.2 Web Miner
The Web Miner fetches relevant concepts and their
relations by means of two technologies: a) infor-
mation wrapping for exaction of personal profiles
from structured and semi-structured web content,
and b) a minimally supervised machine learning
method provided by DARE (Xu et al, 2007; Xu
et al, 2008) to acquire relations from free texts.
DARE learns linguistic patterns indicating the tar-
get semantic relations by taking some relation in-
stances as initial seed. For example, assume that
the following seed for a parent-child relationship
is given to the DARE system:
(1) Seed: ?Angelina Jolie, Shiloh Nouvel Jolie-Pitt,
daughter?
One sentence that matches the entities men-
tioned in the seed above could be (2), and from
which the DARE system can derive a linguistic
pattern as shown in 3.
(2) Matched sentence: Angelina Jolie and Brad Pitt
welcome their new daughter Shiloh Nouvel Jolie-Pitt.
(3) Extracted pattern: ?subject: celebrity? welcome
?mod: ?new daughter?? ?object: person?
Given the learned pattern, new instances of the
?parent-child? relationship can be automatically
discovered, e.g.:
(4) New acquired instances: ?Adam Sandler, Sunny
Madeline? ?Cynthia Rodriguez, Ella Alexander?
Given the discovered relations among the
celebrities and other people, the system constructs
a social network, which is the basis for providing
answers to users? questions regarding celebrities?
relationships. The network also serves as a re-
source for the active dialogue memory of the agent
as shown in Figure 2.
14
2.3 Input Analyzer and Input Interpreter
The Input Analyzer is designed as both domain
and dialogue context independent. It relies on sev-
eral linguistic analysis tools: 1) a spell checker, 2)
a named entity recognizer SProUT (Drozdzynski
et al, 2004), and 3) a syntactic parsing component
for which we currently employ a fuzzy paraphrase
matcher to approximate the output of a deep syn-
tactic/semantic parser.
In contrast to the Input Analyzer, the Input In-
terpreter analyzes the input with respect to the
context of the dialogue. It contains two major
components: 1) anaphoric resolution, which refers
pronouns to previously mentioned entities with the
help of the dialogue memory, and 2) domain clas-
sification, which determines whether the entities
contained in a user query can be found in the gos-
sip knowledge base (cf. ?Carla Bruni? vs. ?Nico-
las Sarkozy?) and whether the answer focus be-
longs to the domain (cf. ?stage name? vs ?body
guard?). For example, a simple factoid query such
as ?Who is Madonna?, an embedded questions
like ?I wonder who Madonna is?, and expressions
of requests and wishes such as ?I?m interested in
Madonna?, would share the same answer focus,
i.e., the ?personal profile? of ?Madonna?. In ad-
dition to the simple answer types such as ?person
name?, ?location? and ?date/time?, our system can
also deal with complex answer focus types such as
?personal profile?, ?social network? and ?relation
path?, as well as domain-relevant concepts such as
?party affiliation? or ?sexual orientation?.
Finally, the analysis of each query is associated
with a meaning representation, an answer focus
and an expected answer type.
2.4 Response Handler
This component executes the planned action based
on the properties of the answer focus and the en-
tities in a query. In cases where the answer focus
or the entities cannot be found in the knowledge
base, the system would still attempt to provide a
constructive answer. For instance, if a question
contains a domain-specific answer focus but en-
tities unknown to the knowledge base, the agent
will automatically look for alternative knowledge
resources, e.g., Wikipedia. For example, given
the question ?Tell me something about Nicolas
Sarkozy!?, the agent would attempt a Web search
and return the corresponding page on Wikipedia
about ?Nicolas Sarkozy?, even if the knowledge
base does not contain his information since he is a
politician rather than an entertainer.
In addition, specific strategies have been devel-
oped to deal with negative answers. For instance,
the agent would answer the question: When did
Madonna die?, with ?As far as I know, Madonna
is still alive.?, as it cannot find any information re-
garding Madonna?s death.
2.5 Multimodal Generator
The agent (i.e., the young lady in Figure 1) is
equipped with multimodal capabilities to inter-
act with users. It can show the results in tex-
tual and speech forms, using body gestures, fa-
cial expressions, and finally via multimedia out-
put to an embedded screen. We currently employ
template-based generators for producing both the
natural language utterances and the instructions to
the agent that controls the multimodal communi-
cation with the user.
2.6 Dialogue State
The responsibility of this component is to keep
track of the current state of the dialogue between a
user and the agent. It models the system?s expec-
tation of the user?s next action and the system?s re-
actions. For example, if a user misspelled a name
as in the question ?Who is Roby Williams??, the
system would answer with a clarification question:
?Did you mean Robbie Williams?? The user is
then expected to react to the question with either
?yes? or ?no?, which would not be interpretable in
other dialogue contexts where the user is expected
to ask a question. The fact that the system asks a
clarification question and expects a yes/no answer
as well as the repaired question are stored in the
Dialogue State component.
2.7 Dialogue Memory
This component aims to simulate the cognitive ca-
pacity of the memory of a human being: con-
struction of a short-time memory and activation
of long-time memory (our Knowledge Base). It
records the sequence of all entities mentioned dur-
ing the conversation and their respective target
foci. Simultaneously, it retrieves all the related in-
formation from the Knowledge Base. In figure 2,
the dialogue memory for the three questions ?Tell
me something about Carla Bruni.?, ?Can you tell
me some news about her??, ?How many kids does
Brad Pitt have?? is shown. Green and yellow bub-
bles are entities mentioned in the dialogue context,
15
where the yellow one is the last mentioned entity.
White bubbles indicate the newest records which
are acquired in the last process of online QA.
3 Implementation
The system uses a client-server architecture. The
server is responsible for accepting new connec-
tions, managing accounts, processing conversa-
tions and passing responses to the clients. All
the server-side functions are implemented in Java
1.6. We use Jetty as a web server to deliver mul-
timedia representations of an answer and to pro-
vide selected functionalities of the system as web
services to our partners. The knowledge base is
stored in a MySQL database whose size is 11MB,
and contains information of 38,758 persons in-
cluding 16,532 artists and 1,407 music groups. As
for the social connection data, there are 14,909
parent-child, 16,886 partner, 4,214 sibling, 308
influence/influenced and 9,657 group-member re-
lational pairs. The social network is visualized
in JGraph, and speech output is generated by the
open-source speech synthesis system OpenMary
(Schro?der and Hunecke, 2007).
There are two interfaces realizing the client-
side of the system: a 3D software application and
a web interface. The software application uses
a 3D computer game engine, and communicates
with the server by messages in an XML format
based on BML and SSML. In addition, we provide
a web interface1, implemented using HTML and
Javascript on the browser side, and Java Servlets
on the server side, offering the same core func-
tionality as the 3D client.
Both the server and the web client are platform
independent. The 3D client runs on Windows with
a dedicated 3D graphics card. The recommended
memory for the server is 1GB.
4 Conclusions
This paper describes a fully implemented software
application, which discovers and learns informa-
tion and knowledge from the Web, and communi-
cates with users and exchanges gossip trivia with
them. The system uses many novel technologies
in order to achieve the goal of vividly chatting and
interacting with the users in a fun way. The tech-
nologies include information extraction, question
answering, dialogue modeling, response planning
and multimodal presentation generation. Please
1http://rascalli.dfki.de/live/dialogue.page
refer to (Xu et al, 2009) for additional details
about the ?Gossip Galore? system.
The planned future extensions include the in-
tegration of deeper language processing methods
to discover more precise linguistic patterns. A
prime candidate for this extension is our own deep
syntactic/semantic parser. Another plan concerns
the required temporal aspects of relations together
with credibility checking. Finally, we plan to ex-
ploit the dialogue memory for moving more of the
dialogue initiative to the agent. In cases of miss-
ing or negative answers or in cases of pauses on
the user side, the agent can use the active parts
of the dialogue memory to propose additional rel-
evant information or to guide the user to fruitful
requests within the range of user?s interests.
References
Witold Drozdzynski, Hans-Ulrich Krieger, Jakub Piskorski,
Ulrich Scha?fer, and Feiyu Xu. 2004. Shallow processing
with unification and typed feature structures ? foundations
and applications. Ku?nstliche Intelligenz, 1:17?23.
Brigitte Krenn. 2008. Responsive artificial situated cognitive
agents living and learning on the internet, April. Poster
presented at CogSys 2008.
Marc Schro?der and Anna Hunecke. 2007. Mary tts partici-
pation in the Blizzard Challenge 2007. In Proceedings of
the Blizzard Challenge 2007, Bonn, Germany.
Feiyu Xu, Hans Uszkoreit, and Hong Li. 2007. A seed-
driven bottom-up machine learning framework for extract-
ing relations of various complexity. Proceedings of ACL-
2007, pages 584?591.
Feiyu Xu, Hans Uszkoreit, and Hong Li. 2008. Task driven
coreference resolution for relation extraction. In Proceed-
ings of ECAI 2008, Patras, Greece.
Feiyu Xu, Peter Adolphs, Hans Uszkoreit, Xiwen Cheng, and
Hong Li. 2009. Gossip galore: A conversational web
agent for collecting and sharing pop trivia. In Joaquim
Filipe, Ana Fred, and Bernadette Sharp (eds). Proceed-
ings of ICAART 2009, Porto, Portugal.
16
Proceedings of the ACL 2010 System Demonstrations, pages 36?41,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Talking NPCs in a Virtual Game World
Tina Klu?wer, Peter Adolphs, Feiyu Xu, Hans Uszkoreit, Xiwen Cheng
Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (DFKI)
Projektbu?ro Berlin
Alt-Moabit 91c
10559 Berlin
Germany
{tina.kluewer,peter.adolphs,feiyu,uszkoreit,xiwen.cheng}@dfki.de
Abstract
This paper describes the KomParse sys-
tem, a natural-language dialog system
in the three-dimensional virtual world
Twinity. In order to fulfill the various
communication demands between non-
player characters (NPCs) and users in
such an online virtual world, the system
realizes a flexible and hybrid approach
combining knowledge-intensive domain-
specific question answering, task-specific
and domain-specific dialog with robust
chatbot-like chitchat.
1 Introduction
In recent years multi-user online games in virtual
worlds such as Second Life or World of Warcraft
have attracted large user communities. Such vir-
tual online game worlds provide new social and
economic platforms for people to work and inter-
act in. Furthermore, virtual worlds open new per-
spectives for research in the social, behavioral, and
economic sciences, as well as in human-centered
computer science (Bainbridge, 2007). Depending
on the game type, non-player characters (NPCs)
are often essential for supporting the game plot,
for making the artificial world more vivid and ulti-
mately for making it more immersive. In addition,
NPCs are useful to populate new worlds by carry-
ing out jobs the user-led characters come in touch
with. The range of functions to be filled by NPCs
is currently still strongly restricted by their limited
capabilities in autonomous acting and communi-
cation. This shortcoming creates a strong need for
progress in areas such as AI and NLP, especially
their planning and dialog systems.
The KomParse system, described in this paper,
provides NPCs for a virtual online world named
Twinity, a product of the Berlin startup company
Metaversum1. The KomParse NPCs offer vari-
ous services through conversation with game users
using question-answering and dialog functional-
ity. The utilization of Semantic Web technology
with RDF-encoded generic and domain-specific
ontologies furthermore enables semantic search
and inference.
This paper is organized as follows: Section 2
presents the NPC modelling and explains the ap-
plication scenarios. Section 3 details the knowl-
edge representation and semantic inference in our
system. Section 4 explains the system architecture
and its key components. Section 5 describes the
KomParse dialog system. Section 7 gives a con-
clusion and closes off with our future work.
2 Application Scenario and NPC
Modelling
The online game Twinity extends the Second Life
idea by mirroring an urban part of the real world.
At the time of this writing, the simulated section of
reality already contains 3D models of the cities of
Berlin, Singapore and London and it keeps grow-
ing. Users can log into the virtual world, where
they can meet other users and communicate with
them using the integrated chat function or talk
to each other via Voice-over-IP. They can style
their virtual appearance, can rent or buy their own
flats and decorate them as to their preferences and
tastes.
Out of many types of NPCs useful for this appli-
cation such as pedestrians, city guides and person-
nel in stores, restaurants and bars, we start with
two specific characters: a female furniture sales
agent and a male bartender. The furniture seller
is designed for helping users furnish their virtual
apartments. Users can buy pieces of furniture and
room decoration from the NPC by describing their
demands and wishes in a text chat. During the di-
1http://www.metaversum.com/
36
Figure 1: The furniture sales NPC selling a sofa
alog with the NPC, the preferred objects are then
selected and directly put into a location in the
apartment, which can be further refined with the
user interfaces that Twinity provides.
In the second scenario, the bartender sells vir-
tual drinks. He can talk about cocktails with users,
but moreover, he can also entertain his guests by
providing trivia-type information about popular
celebrities and various relations among them.
We chose these two characters not only because
of their value for the Twinity application but also
for our research goals. They differ in many in-
teresting aspects. First of all, the furniture sales
agent is controlled by a complex task model in-
cluding ontology-driven and data-driven compo-
nents to guide the conversation. This agent also
possesses a much more fine-grained action model,
which allows several different actions to cover
the potential conversation situations for the sell-
ing task. The bartender agent on the other hand is
designed not to fulfill one strict task because his
clients do not follow a specific goal except order-
ing drinks. Our bartender has the role of a conver-
sation companion and is able to entertain clients
with his broad knowledge. Thus, he is allowed to
access to several knowledge bases and is able to
handle questions (and later conversations) about
a much larger domain called the ?gossip domain?
which enables conversation about pop stars, movie
actors and other celebrities as well as the relations
between these people. In order to achieve a high
robustness, we integrate a chatbot into the bar-
tender agent to catch chitchat utterances we cannot
handle.
Figure 2: Our bartender NPC in his bar in Twinity
3 Knowledge Representation and
Semantic Inference
Semantic Web technology is employed for mod-
elling the knowledge of the NPCs. The Resource
Description Format (RDF) serves as the base for
the actual encoding. An RDF statement is a binary
relation instance between two individuals, that is a
triple of a predicate and two arguments, called the
subject and the object, and written as subj pred obj
(e.g. f:Sofa Alatea f:hasMainColour
f:Burgundy).
All objects and properties the NPC can talk
about are modelled in this way. Therefore the
knowledge base has to reflect the physical prop-
erties of the virtual objects in Twinity as faithfully
as possible. For instance, specific pieces of furni-
ture are described by their main color, material or
style, whereas cocktails are characterized by their
ingredients, color, consistence and taste. Further-
more, references to the 3D models of the objects
are stored in order to create, find and remove such
objects in the virtual world.
The concepts and individuals of the particular
domain are structured and organized in domain-
specific ontologies. These ontologies are mod-
elled in the Web Ontology Language (OWL).
OWL allows us to define concept hierarchies, re-
lations between concepts, domains and ranges of
these relations, as well as specific relation in-
stances between instances of a concept. Our on-
tologies are defined by the freely available ontol-
ogy editor Prote?ge? 4.02. The advantage of using an
ontology for structuring the domain knowledge is
2http://protege.stanford.edu/, as accessed
27 Oct 2009
37
Twinity
Server
KomParse
Server
Twinity
Client
Conversational
AgentConversational
AgentConversational
Agent
Twinity
ClientTwinity
Client
Figure 3: Overall System Architecture ? Server/Client Architecture for NPC Control
the modular non-redundant encoding. When com-
bined with a reasoner, only a few statements about
an individual have to be asserted explicitely, while
the rest can be inferred from the ontology. We em-
ploy several ontologies, among which the follow-
ing are relevant for modelling the specific domains
of our NPCs:
? An extensive furniture ontology, created by
our project partner ZAS Berlin, defining
kinds of furniture, room parts, colors and
styles as well as the specific instances of fur-
niture in Twinity. This knowledge base con-
tains 95,258 triples, 123 furniture classes, 20
color classes, 243 color instances and various
classes defining styles and similar concepts.
? A cocktail ontology, defining 13 cocktail
classes with ingredients and tastes in 21,880
triples.
? A biographical ontology, the ?gossip on-
tology?, defining biographical and career-
specific concepts for people. This ontology is
accompanied by a huge database of celebri-
ties, which has been automatically acquired
from the Web and covers nearly 600,000 per-
sons and relations between these people like
family relationships, marriages and profes-
sional relations. (Adolphs et al, 2010)
The furniture ontology is the only knowledge
base for the furniture sales agent, whereas the bar-
tender NPC has access to both the cocktail as well
as the gossip knowledge base.
We use SwiftOwlim3 for storing and querying
the data. SwiftOwlim is a ?triple store?, a kind
of database which is specifically built for storing
and querying RDF data. It provides a forward-
chaining inference engine which evaluates the
domain definitions when loading the knowledge
repository, and makes implicit knowledge explicit
by asserting triples that must also hold true accord-
ing to the ontology. Once the reasoner is finished,
the triple store can be queried directly using the
RDF query language SPARQL.
3http://www.ontotext.com/owlim/
4 Overall System Architecture
Figure 3 shows the overall system architecture.
Twinity is a server/client application, in which the
server hosts the virtual world and coordinates the
user interactions. In order to use Twinity, users
have to download the Twinity client. The client
allows the user to control the physical represen-
tation of the user?s character in the virtual world,
also called the ?avatar?. Thus the client is respon-
sible for displaying the graphics, calculating the
effects of physical interactions, handling the user?s
input and synchronizing the 3D data and user ac-
tions with the Twinity server.
Each NPC comprises two major parts: whereas
its avatar is the physical appearance of the NPC in
the virtual world, the ?conversational agent? pro-
vides the actual control logic which controls the
avatar autonomously. It is in particular able to hold
a conversation with Twinity users in that it reacts
to a user?s presence, interprets user?s utterances in
dialog context and generates adequate responses.
The KomParse server is a multi-client, multi-
threaded server written in Java that hosts the con-
versational agents for the NPCs (section 5). The
NPC?s avatar, on the other hand, is realized by a
modified Twinity client. We utilize the Python in-
terface provided by the Twinity client to call our
own plugin which opens a bidirectional socket
connection to the KomParse server. The plugin is
started together with the Twinity client and serves
as a mediator between the Twinity server and the
KomParse server from then on (fig. 3). It sends all
in-game events relevant to our system to the server
and translates the commands sent by the server
into Twinity-specific actions.
The integration architecture allows us to be
maximally independent of the specific game plat-
form. Rather than using the particular program-
ming language and development environment of
the platform for realizing the conversational agent
or reimplementing a whole client/server proto-
col for connecting the avatar to the corresponding
agent, we use an interface tailored to the specific
needs of our system. Thus the KomParse system
38
can be naturally extended to other platforms since
only the avatar interfaces have to be adapted.
The integration architecture also has the advan-
tage that the necessary services can be easily dis-
tributed in a networked multi-platform environ-
ment. The Twinity clients require aMicrosoftWin-
dows machine with a 3D graphics card supporting
DirectX 9.0c or higher, 1 GB RAM and a CPU
core per instance. The KomParse server requires
roughly 1 GB RAM. The triple store is run as
a separate server process and is accessed by an
XML-RPC interface. Roughly 1.2 GB RAM are
required for loading our current knowledge base.
5 Conversational Agent: KomParse
Dialog System
Figure 4: Dialog System: Conversational Agent
The KomParse dialog system, the main func-
tionality of the conversational agent, consists of
the following three major components: input ana-
lyzer, dialog manager and output generator (fig.4).
The input analyzer is responsible for the lin-
guistic analysis of the user?s textual input includ-
ing preprocessing such as string cleaning, part-of-
speech tagging, named entity recognition, parsing
and semantic interpretation. It yields a semantic
representation which is the input for the dialog
manager.
The dialog manager takes the result of the input
analyzer and delivers an interpretation based on
the dialog context and the available knowledge. It
also controls the task conversation chain and han-
dles user requests. The dialog manager determines
the next system action based on the interpreted pa-
rameters.
The output generator realizes the action defined
by the dialog manager with its multimodal gener-
ation competence. The generated results can be
verbal, gestural or a combination of both.
As mentioned above, our dialog system has to
deal with two different scenarios. While the fo-
cal point of the bartender agent lies in the question
answering functionality, the furniture sales agent
is driven by a complex dialog task model based on
a dialog graph. Thus, the bartender agent relies
mainly on question answering technology, in that
it needs to understand questions and extract the
right answer from our knowledge bases, whereas
the sales agent has to accommodate various dialog
situations with respect to a sales scenario. It there-
fore has to understand the dialog acts intended
by the user and trigger the corresponding reac-
tions, such as presenting an object, memorizing
user preferences, negotiating further sales goals,
etc.
The task model for sales conversations is in-
spired by a corpus resulting from the annotation of
a Wizard-of-Oz experiment in the furniture sales
agent scenario carried out by our project partner at
ZAS (Bertomeu and Benz, 2009). In these exper-
iments, 18 users spent one hour each on furnish-
ing a virtual living room in a Twinity apartment by
talking to a human wizard controlling the virtual
sales agent. The final corpus consists of 18 di-
alogs containing 3,171 turns with 4,313 utterances
and 23,015 alpha-numerical strings (words). The
following example shows a typical part of such a
conversation:
USR.1: And do we have a little side table for the TV?
NPC.1: I could offer you another small table or a sideboard.
USR.2: Then I?ll take a sideboard thats similar to my shelf.
NPC.2: Let me check if we have something like that.
Table 1: Example Conversation from the Wizard-
of-Oz Experiment
The flow of the task-based conversation is con-
trolled by a data-driven finite-state model, which
is the backbone of the dialog manager. During
a sales conversation, objects and features of ob-
jects mentioned by the NPC and the user are ex-
tracted from the knowledge bases and added into
the underspecified graph nodes and egdes at run-
time. This strategy keeps the finite-state graph as
small as possible. Discussed objects and their fea-
tures are stored in a frame-based sub-component
named ?form?. The form contains entries which
correspond to ontological concepts in the furni-
39
ture ontology. During conversation, these entries
will be specified with the values of the properties
of the discussed objects. This frame-based ap-
proach increases the flexibility of the dialog man-
ager (McTear, 2002) and is particularly useful for
a task-driven dialog system. As long as the negoti-
ated object is not yet fully specified, the form rep-
resents the underspecified object description ac-
cording to the ontology concept. Every time the
user states a new preference or request, the form
is enriched with additional features until the set of
objects is small enough to be presented to the user
for final selection. Thus the actual flow of dia-
log according to the task model does not have to
be expressed by the graph but can be derived on
demand from the knowledge and handled by the
form which in turn activates the appropriate dia-
log subgraphs. This combination of graph-based
dialog models and form-based task modelling ef-
fectively accounts for the interaction of sequential
dialog strategies and the non-sequential nature of
complex dialog goals.
Given a resolved semantic representation, the
dialog manager triggers either a semantic search
in the knowledge bases to deliver factual answers
as needed in a gossip conversation or a further di-
alog response for example providing choices for
the user in a sales domain. The semantic search is
needed in both domains. In case that the semantic
representation can neither be resolved in the task
domain nor in the gossip domain, it gets passed to
the embedded A.L.I.C.E. chatbot that uses its own
understanding and generation components (Wal-
lace and Bush, 2001).
5.1 Semantic Representation
The input understanding of the system is imple-
mented as one single understanding pipeline.The
understanding pipeline delivers a semantic repre-
sentation which is the basis for the decision of the
dialog manager which action to perform next.
This semantic representation can be extracted
from the user input by our understanding com-
ponent via a robust hybrid approach: either via a
number of surface patterns containing regular ex-
pressions or via patterns reflecting the syntactic
analysis of a dependency parser (de Marneffe and
Manning, 2008).
The representation?s structure is inspired by our
knowledge representation design described in sec-
tion 3 as well as by predicate logic. The core of the
representation is a predicate-argument structure
limited to two arguments including message type
and the whole syntactic information found by the
analysis pipeline. The field ?Message Type? can
have one of the following values: wh-question,
yes/no-question, declarative. Predicates can often
be instantiated with the lemmatized matrix verb of
the successfully analysed piece of the input. If the
input contains a wh-question, the questioned fact
is marked as an unfilled argument slot. The gen-
eral structure can be simplified described as:
<PREDICATE, ARG1, ARG2, [message-type]>
The following examples show the structure used
for different input:
? ?Who is the boyfriend of Madonna??
<hasBoyfriend, Madonna, ?, [wh]>
? ?I want to buy a sofa.?
<buy, I, "a sofa", [declarative]>
5.2 Information Extraction
Both scenarios make use of state-of-the-art infor-
mation extraction approaches to extract the impor-
tant pieces from the user input. While the bar-
tender depends on relation extraction to detect the
fact or relation questioned by the user (Xu et al,
2007), the sales agent uses information extraction
methods to recognize user wishes and demands.
As a result, the questioned fact or the demanded
object feature equals the ontology structure con-
taining the knowledge needed to handle the user
input. The input ?Do you have any red couches??
for example needs to get processed by the system
in such a way that the information regarding the
sofa with red color is extracted.
This is done by the system in a data-driven way.
The input analysis first tries to find a demanded
object in the input via asking the ontology: Every
object which can be discussed in the scenario is
encoded in the sales agents knowledge base. This
can be seen as a Named Entity Recognition step.
In case of success, the system tries to detect one
of the possible relations of the object found in the
input. This is achieved by querying the ontology
about what kind of relations the identified object
can satisfy. Possible relations are encoded in the
class description of the given object. As a result
the system can detect a relation ?hasColour? for
the found object ?sofa? and the color value ?red?.
The found information gets inserted into the form
which gets more and more similar or if possible
equal to a search query via RDF.
40
Figure 5: Comparison of Input, Extracted Information and Knowledge Base
6 Conclusion and Future Work
The KomParse system demonstrates an attractive
application area for dialog systems that bears great
future potential. Natural language dialog with
NPCs is an important factor in making virtual
worlds more interesting, interactive and immer-
sive. Virtual worlds with conversing characters
will also find many additional applications in edu-
cation, marketing, and entertainment.
KomParse is an ambitious and nevertheless
pragmatic attempt to bring NLP into the world of
virtual games. We develop a new strategy to inte-
grate task models and domain ontologies into dia-
log models. This strategy is useful for task-driven
NPCs such as furniture sellers. With the chatty
bartender, a combination of task-specific dialog
and domain-specific question answering enables a
smart wide-domain off-task conversation. Since
the online game employs bubble-chat as a mode
of communication in addition to Voice-over-IP, we
are able to test our dialog system in a real-time
application without being hindered by imperfect
speech recognition.
The system presented here is still work in
progress. The next goals will include various eval-
uation steps. On the one hand we will focus on
single components like hybrid parsing of input ut-
terances and dialog interpretation in terms of pre-
cision and recall. On the other hand an evaluation
of the two different scenarios regarding the us-
ability are planned in experiments with end users.
Moreover we will integrate some opinion mining
and sentiment analysis functionality which can be
helpful to better detect and understand the user?s
preferences in the furniture sales agents scenario.
Acknowledgements
The project KomParse is funded by the ProFIT
programme of the Federal State of Berlin, co-
funded by the EFRE programme of the Euro-
pean Union. The research presented here is ad-
ditionally supported through a grant to the project
TAKE, funded by the German Ministry for Edu-
cation and Research (BMBF, FKZ: 01IW08003).
Many thanks go to our project partners at the Cen-
tre for General Linguistics (ZAS) in Berlin as well
as to the supporting company Metaversum.
References
Peter Adolphs, Xiwen Cheng, Tina Klu?wer, Hans
Uszkoreit, and Feiyu Xu. 2010. Question answering
biographic information and social network powered
by the semantic web. In Proceedings of LREC 2010,
Valletta, Malta.
William Sims Bainbridge. 2007. The scientific re-
search potential of virtual worlds. Science, 317.
Nuria Bertomeu and Anton Benz. 2009. Annotation of
joint projects and information states in human-npc
dialogues. In Proceedings of the First International
Conference on Corpus Linguistics (CILC-09), Mur-
cia, Spain.
Marie C. de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, Manchester, UK.
Michael F. McTear. 2002. Spoken dialogue tech-
nology: enabling the conversational user interface.
ACM Comput. Surv., 34(1).
Richard Wallace and Noel Bush. 2001. Artificial
intelligence markup language (aiml) version 1.0.1
(2001). Unpublished A.L.I.C.E. AI Foundation
Working Draft (rev 006).
Feiyu Xu, Hans Uszkoreit, and Hong Li. 2007. A
seed-driven bottom-up machine learning framework
for extracting relations of various complexity. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 584?
591, Prague, Czech Republic, June. Association for
Computational Linguistics.
41
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 242?252,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Capturing Paradigmatic and Syntagmatic Lexical Relations:
Towards Accurate Chinese Part-of-Speech Tagging
Weiwei Sun?? and Hans Uszkoreit?
?Institute of Computer Science and Technology, Peking University
?Saarbru?cken Graduate School of Computer Science
??Department of Computational Linguistics, Saarland University
??Language Technology Lab, DFKI GmbH
ws@pku.edu.cn, uszkoreit@dfki.de
Abstract
From the perspective of structural linguistics,
we explore paradigmatic and syntagmatic lex-
ical relations for Chinese POS tagging, an im-
portant and challenging task for Chinese lan-
guage processing. Paradigmatic lexical rela-
tions are explicitly captured by word cluster-
ing on large-scale unlabeled data and are used
to design new features to enhance a discrim-
inative tagger. Syntagmatic lexical relations
are implicitly captured by constituent pars-
ing and are utilized via system combination.
Experiments on the Penn Chinese Treebank
demonstrate the importance of both paradig-
matic and syntagmatic relations. Our linguis-
tically motivated approaches yield a relative
error reduction of 18% in total over a state-
of-the-art baseline.
1 Introduction
In grammar, a part-of-speech (POS) is a linguis-
tic category of words, which is generally defined
by the syntactic or morphological behavior of the
word in question. Automatically assigning POS tags
to words plays an important role in parsing, word
sense disambiguation, as well as many other NLP
applications. Many successful tagging algorithms
developed for English have been applied to many
other languages as well. In some cases, the meth-
ods work well without large modifications, such
as for German. But a number of augmentations
and changes become necessary when dealing with
highly inflected or agglutinative languages, as well
as analytic languages, of which Chinese is the focus
?This work is mainly finished when this author (correspond-
ing author) was in Saarland University and DFKI.
of this paper. The Chinese language is characterized
by the lack of formal devices such as morphological
tense and number that often provide important clues
for syntactic processing tasks. While state-of-the-
art tagging systems have achieved accuracies above
97% on English, Chinese POS tagging has proven to
be more challenging and obtained accuracies about
93-94% (Tseng et al, 2005b; Huang et al, 2007,
2009; Li et al, 2011).
It is generally accepted that Chinese POS tag-
ging often requires more sophisticated language pro-
cessing techniques that are capable of drawing in-
ferences from more subtle linguistic knowledge.
From a linguistic point of view, meaning arises from
the differences between linguistic units, including
words, phrases and so on, and these differences are
of two kinds: paradigmatic (concerning substitu-
tion) and syntagmatic (concerning positioning). The
distinction is a key one in structuralist semiotic anal-
ysis. Both paradigmatic and syntagmatic lexical re-
lations have a great impact on POS tagging, because
the value of a word is determined by the two rela-
tions. Our error analysis of a state-of-the-art Chinese
POS tagger shows that the lack of both paradigmatic
and syntagmatic lexical knowledge accounts for a
large part of tagging errors.
This paper is concerned with capturing paradig-
matic and syntagmatic lexical relations to advance
the state-of-the-art of Chinese POS tagging. First,
we employ unsupervised word clustering to explore
paradigmatic relations that are encoded in large-
scale unlabeled data. The word clusters are then ex-
plicitly utilized to design new features for POS tag-
ging. Second, we study the possible impact of syn-
tagmatic relations on POS tagging by comparatively
analyzing a (syntax-free) sequential tagging model
242
and a (syntax-based) chart parsing model. Inspired
by the analysis, we employ a full parser to implicitly
capture syntagmatic relations and propose a Boot-
strap Aggregating (Bagging) model to combine the
complementary strengths of a sequential tagger and
a parser.
We conduct experiments on the Penn Chinese
Treebank and Chinese Gigaword. We implement
a discriminative sequential classification model for
POS tagging which achieves the state-of-the-art ac-
curacy. Experiments show that this model are sig-
nificantly improved by word cluster features in ac-
curacy across a wide range of conditions. This con-
firms the importance of the paradigmatic relations.
We then present a comparative study of our tagger
and the Berkeley parser, and show that the combi-
nation of the two models can significantly improve
tagging accuracy. This demonstrates the importance
of the syntagmatic relations. Cluster-based features
and the Bagging model result in a relative error re-
duction of 18% in terms of the word classification
accuracy.
2 State-of-the-Art
2.1 Previous Work
Many algorithms have been applied to computation-
ally assigning POS labels to English words, includ-
ing hand-written rules, generative HMM tagging
and discriminative sequence labeling. Such meth-
ods have been applied to many other languages as
well. In some cases, the methods work well without
large modifications, such as German POS tagging.
But a number of augmentations and changes became
necessary when dealing with Chinese that has little,
if any, inflectional morphology. While state-of-the-
art tagging systems have achieved accuracies above
97% on English, Chinese POS tagging has proven
to be more challenging and obtains accuracies about
93-94% (Tseng et al, 2005b; Huang et al, 2007,
2009; Li et al, 2011).
Both discriminative and generative models have
been explored for Chinese POS tagging (Tseng
et al, 2005b; Huang et al, 2007, 2009). Tseng
et al (2005a) introduced a maximum entropy based
model, which includes morphological features for
unknown word recognition. Huang et al (2007) and
Huang et al (2009) mainly focused on the gener-
ative HMM models. To enhance a HMM model,
Huang et al (2007) proposed a re-ranking proce-
dure to include extra morphological and syntactic
features, while Huang et al (2009) proposed a la-
tent variable inducing model. Their evaluations on
the Chinese Treebank show that Chinese POS tag-
ging obtains an accuracy of about 93-94%.
2.2 Our Discriminative Sequential Model
According to the ACL Wiki, all state-of-the-art En-
glish POS taggers are based on discriminative se-
quence labeling models, including structure percep-
tron (Collins, 2002; Shen et al, 2007), maximum
entropy (Toutanova et al, 2003) and SVM (Gimnez
and Mrquez, 2004). A discriminative learner is easy
to be extended with arbitrary features and therefore
suitable to recognize more new words. Moreover, a
majority of the POS tags are locally dependent on
each other, so the Markov assumption can well cap-
tures the syntactic relations among words. Discrim-
inative learning is also an appropriate solution for
Chinese POS tagging, due to its flexibility to include
knowledge from multiple linguistic sources.
To deeply analyze the POS tagging problem for
Chinese, we implement a discriminative sequential
model. A first order linear-chain CRF model
is used to resolve the sequential classification
problem. We choose the CRF learning toolkit
wapiti1 (Lavergne et al, 2010) to train models.
In our experiments, we employ a feature set
which draws upon information sources such as
word forms and characters that constitute words.
To conveniently illustrate, we denote a word in
focus with a fixed window w?2w?1ww+1w+2,
where w is the current token. Our features includes:
Word unigrams: w?2, w?1, w, w+1, w+2;
Word bigrams: w?2 w?1, w?1 w, w w+1, w+1 w+2;
In order to better handle unknown words, we extract
morphological features: character n-gram prefixes and
suffixes for n up to 3.
2.3 Evaluation
2.3.1 Setting
Penn Chinese Treebank (CTB) (Xue et al, 2005)
is a popular data set to evaluate a number of Chinese
NLP tasks, including word segmentation (Sun and
1http://wapiti.limsi.fr/
243
Xu, 2011), POS tagging (Huang et al, 2007, 2009),
constituency parsing (Zhang and Clark, 2009; Wang
et al, 2006) and dependency parsing (Zhang and
Clark, 2008; Huang and Sagae, 2010; Li et al,
2011). In this paper, we use CTB 6.0 as the labeled
data for the study. The corpus was collected during
different time periods from different sources with a
diversity of topics. In order to obtain a representa-
tive split of data sets, we define the training, devel-
opment and test sets following two settings. To com-
pare our tagger with the state-of-the-art, we conduct
an experiment using the data setting of (Huang et al,
2009). For detailed analysis and evaluation, we con-
duct further experiments following the setting of the
CoNLL 2009 shared task. The setting is provided by
the principal organizer of the CTB project, and con-
siders many annotation details. This setting is more
robust for evaluating Chinese language processing
algorithms.
2.3.2 Overall Performance
Table 1 summarizes the per token classification
accuracy (Acc.) of our tagger and results reported in
(Huang et al, 2009). Huang et al (2009) introduced
a bigram HMM model with latent variables (Bigram
HMM-LA in the table) for Chinese tagging. Com-
pared to earlier work (Tseng et al, 2005a; Huang
et al, 2007), this model achieves the state-of-the-art
accuracy. Despite of simplicity, our discriminative
POS tagging model achieves a state-of-the-art per-
formance, even better.
System Acc.
Trigram HMM (Huang et al, 2009) 93.99%
Bigram HMM-LA (Huang et al, 2009) 94.53%
Our tagger 94.69%
Table 1: Tagging accuracies on the test data (setting 1).
2.4 Motivating Analysis
For the following experiments, we only report re-
sults on the development data of the CoNLL setting.
2.4.1 Correlating Tagging Accuracy with Word
Frequency
Table 2 summarizes the prediction accuracy on
the development data with respect to the word fre-
quency on the training data. To avoid overestimat-
ing the tagging accuracy, these statistics exclude all
punctuations. From this table, we can see that words
with low frequency, especially the out-of-vocabulary
(OOV) words, are hard to label. However, when a
word is very frequently used, its behavior is very
complicated and therefore hard to predict. A typi-
cal example of such words is the language-specific
function word ??.? This analysis suggests that a
main topic to enhance Chinese POS tagging is to
bridge the gap between the infrequent words and fre-
quent words.
Freq. Acc.
0 83.55%
1-5 89.31%
6-10 90.20%
11-100 94.88%
101-1000 96.26%
1001- 93.65%
Table 2: Tagging accuracies relative to word frequency.
2.4.2 Correlating Tagging Accuracy with Span
Length
A word projects its grammatical property to its
maximal projection and it syntactically governs all
words under the span of its maximal projection. The
words under the span of current token thus reflect
its syntactic behavior and good clues for POS tag-
ging. Table 3 shows the tagging accuracies relative
to the length of the spans. We can see that with the
increase of the number of words governed by the
token, the difficulty of its POS prediction increase.
This analysis suggests that syntagmatic lexical re-
lations plays a significant role in POS tagging, and
sometimes words located far from the current token
affect its tagging much.
Len. Acc.
1-2 93.79%
3-4 93.39%
5-6 92.19%
7- 94.18%
Table 3: Tagging accuracies relative to span length.
3 Capturing Paradigmatic Relations via
Word Clustering
To bridge the gap between high and low fre-
quency words, we employ word clustering to acquire
244
the knowledge about paradigmatic lexical relations
from large-scale texts. Our work is also inspired
by the successful application of word clustering to
named entity recognition (Miller et al, 2004) and
dependency parsing (Koo et al, 2008).
3.1 Word Clustering
Word clustering is a technique for partitioning sets
of words into subsets of syntactically or semanti-
cally similar words. It is a very useful technique
to capture paradigmatic or substitutional similarity
among words.
3.1.1 Clustering Algorithms
Various clustering techniques have been pro-
posed, some of which, for example, perform au-
tomatic word clustering optimizing a maximum-
likelihood criterion with iterative clustering algo-
rithms. In this paper, we focus on distributional
word clustering that is based on the assumption that
words that appear in similar contexts (especially
surrounding words) tend to have similar meanings.
They have been successfully applied to many NLP
problems, such as language modeling.
Brown Clustering Our first choice is the bottom-
up agglomerative word clustering algorithm of
(Brown et al, 1992) which derives a hierarchical
clustering of words from unlabeled data. This al-
gorithm generates a hard clustering ? each word be-
longs to exactly one cluster. The input to the algo-
rithm is sequences of words w1, ..., wn. Initially, the
algorithm starts with each word in its own cluster.
As long as there are at least two clusters left, the al-
gorithm merges the two clusters that maximizes the
quality of the resulting clustering. The quality is de-
fined based on a class-based bigram language model
as follows.
P (wi|w1, ...wi?1) ? p(C(wi)|C(wi?1))p(wi|C(wi))
where the function C maps a word w to its class
C(w). We use a publicly available package2 (Liang
et al, 2005) to train this model.
MKCLS Clustering We also do experiments by
using another popular clustering method based on
2http://cs.stanford.edu/?pliang/
software/brown-cluster-1.2.zip
the exchange algorithm (Kneser and Ney, 1993).
The objective function is maximizing the likelihood
?n
i=1 P (wi|w1, ..., wi?1) of the training data given
a partially class-based bigram model of the form
P (wi|w1, ...wi?1) ? p(C(wi)|wi?1)p(wi|C(wi))
We use the publicly available implementation MK-
CLS3 (Och, 1999) to train this model.
We choose to work with these two algorithms
considering their prior success in other NLP appli-
cations. However, we expect that our approach can
function with other clustering algorithms.
3.1.2 Data
Chinese Gigaword is a comprehensive archive
of newswire text data that has been acquired over
several years by the Linguistic Data Consortium
(LDC). The large-scale unlabeled data we use in
our experiments comes from the Chinese Gigaword
(LDC2005T14). We choose the Mandarin news text,
i.e. Xinhua newswire. This data covers all news
published by Xinhua News Agency (the largest news
agency in China) from 1991 to 2004, which contains
over 473 million characters.
3.1.3 Pre-processing: Word Segmentation
Different from English and other Western lan-
guages, Chinese is written without explicit word de-
limiters such as space characters. To find the basic
language units, i.e. words, segmentation is a neces-
sary pre-processing step for word clustering. Previ-
ous research shows that character-based segmenta-
tion models trained on labeled data are reasonably
accurate (Sun, 2010). Furthermore, as shown in
(Sun and Xu, 2011), appropriate string knowledge
acquired from large-scale unlabeled data can signif-
icantly enhance a supervised model, especially for
the prediction of out-of-vocabulary (OOV) words.
In this paper, we employ such supervised and semi-
supervised segmenters4 to process raw texts.
3.2 Improving Tagging with Cluster Features
Our discriminative sequential tagger is easy to be ex-
tended with arbitrary features and therefore suitable
to explore additional features derived from other
3http://code.google.com/p/giza-pp/
4http://www.coli.uni-saarland.de/?wsun/
ccws.tgz
245
sources. We propose to use of word clusters as sub-
stitutes for word forms to assist the POS tagger. We
are relying on the ability of the discriminative learn-
ing method to explore informative features, which
play a central role in boosting the tagging perfor-
mance. 5 clustering-based uni/bi-gram features are
added: w?1, w, w+1, w?1 w, w w+1.
3.3 Evaluation
Features Data Brown MKCLS
Baseline CoNLL 94.48%
+c100 +1991-1995(S) 94.77% 94.83%
+c500 +1991-1995(S) 94.84% 94.93%
+c1000 +1991-1995(S) - - 94.95%
+c100 +1991-1995(SS) 94.90% 94.97%
+c500 +1991-1995(SS) 94.94% 94.88%
+c1000 +1991-1995(SS) 94.89% 94.94%
+c100 +1991-2000(SS) 94.82% 94.93%
+c500 +1991-2000(SS) 94.92% 94.99%
+c1000 +1991-2000(SS) 94.90% 95.00%
+c100 +1991-2004(SS) - - 94.87%
+c500 +1991-2004(SS) - - 95.02%
+c1000 +1991-2004(SS) - - 94.97%
Table 4: Tagging accuracies with different features. S:
supervised segmentation; SS: semi-supervised segmenta-
tion.
Table 4 summarizes the tagging results on the de-
velopment data with different feature configurations.
In this table, the symbol ?+? in the Features col-
umn means current configuration contains both the
baseline features and new cluster-based features; the
number is the total number of the clusters; the sym-
bol ?+? in the Data column means which portion of
the Gigaword data is used to cluster words; the sym-
bol ?S? and ?SS? in parentheses denote (s)upervised
and (s)emi-(s)upervised word segmentation. For ex-
ample, ?+1991-2000(S)? means the data from 1991
to 2000 are processed by a supervised segmenter
and used for clustering. From this table, we can
clearly see the impact of word clustering features on
POS tagging. The new features lead to substantial
improvements over the strong supervised baseline.
Moreover, these increases are consistent regardless
of the clustering algorithms. Both clustering algo-
rithms contributes to the overall performance equiv-
alently. A natural strategy for extending current ex-
periments is to include both clustering results to-
gether, or to include more than one cluster granular-
ity. However, we find no further improvement. For
each clustering algorithm, there are not much dif-
ferences among different sizes of the total clustering
numbers. When a comparable amount of unlabeled
data (five years? data) is used, the further increase
of the unlabeled data for clustering does not lead to
much changes of the tagging performance.
3.4 Learning Curves
Size Baseline +Cluster
4.5K 90.10% 91.93%
9K 92.91% 93.94%
13.5K 93.88% 94.60%
18K 94.24% 94.77%
Table 5: Tagging accuracies relative to sizes of training
data. Size=#sentences in the training corpus.
We do additional experiments to evaluate the ef-
fect of the derived features as the amount of la-
beled training data is varied. We also use the
?+c500(MKCLS)+1991-2004(SS)? setting for these
experiments. Table 5 summarizes the accuracies of
the systems when trained on smaller portions of the
labeled data. We can see that the new features obtain
consistent gains regardless of the size of the training
set. The error is reduced significantly on all data
sets. In other words, the word cluster features can
significantly reduce the amount of labeled data re-
quired by the learning algorithm. The relative reduc-
tion is greatest when smaller amounts of the labeled
data are used, and the effect lessens as more labeled
data is added.
3.5 Analysis
Word clustering derives paradigmatic relational in-
formation from unlabeled data by grouping words
into different sets. As a result, the contribution of
word clustering to POS tagging is two-fold. On
the one hand, word clustering captures and abstracts
context information. This new linguistic knowledge
is thus helpful to better correlate a word in a cer-
tain context to its POS tag. On the other hand, the
clustering of the OOV words to some extent fights
the sparse data problem by correlating an OOV word
with in-vocabulary (IV) words through their classes.
To evaluate the two contributions of the word clus-
tering, we limit entries of the clustering lexicon to
only contain IV words, i.e. words appearing in
the training corpus. Using this constrained lexicon,
246
we train a new ?+c500(MKCLS)+1991-2004(SS)?
model and report its prediction power in Table 6.
The gap between the baseline and +IV clustering
models can be viewed as the contribution of the first
effect, while the gap between the +IV clustering and
+All clustering models can be viewed as the second
contribution. This result indicates that the improved
predictive power partially comes from the new in-
terpretation of a POS tag through a clustering, and
partially comes from its memory of OOV words that
appears in the unlabeled data.
Baseline +IV Clustering +All clustering
Acc. 94.48% 94.70%(?0.22) 95.02%(?0.32)
Table 6: Tagging accuracies with IV clustering.
Table 7 shows the recall of OOV words on the
development data set. Only the word types appear-
ing more than 10 times are reported. The recall of
all OOV words are improved, especially of proper
nouns (NR) and common verbs (VV). Another in-
teresting fact is that almost all of them are content
words. This table is also helpful to understand the
impact of the clustering information on the predic-
tion of OOV words.
4 Capturing Syntagmatic Relations via
Constituency Parsing
Syntactic analysis, especially the full and deep one,
reflects syntagmatic relations of words and phrases
of sentences. We present a series of empirical stud-
ies of the tagging results of our syntax-free sequen-
tial tagger and a syntax-based chart parser5, aiming
at illuminating more precisely the impact of infor-
mation about phrase-structures on POS tagging. The
analysis is helpful to understand the role of syntag-
matic lexical relations in POS prediction.
4.1 Comparing Tagging and PCFG-LA Parsing
The majority of the state-of-the-art constituent
parsers are based on generative PCFG learning, with
lexicalized (Collins, 2003; Charniak, 2000) or la-
tent annotation (PCFG-LA) (Matsuzaki et al, 2005;
Petrov et al, 2006; Petrov and Klein, 2007) refine-
ments. Compared to lexicalized parsers, the PCFG-
LA parsers leverages on an automatic procedure to
5Both the tagger and the parser are trained on the same por-
tion from CTB.
#Words Baseline +Clustering ?
AD 21 33.33% 42.86% <
CD 249 97.99% 98.39% <
JJ 86 3.49% 26.74% <
NN 1028 91.05% 91.34% <
NR 863 81.69% 88.76% <
NT 25 60.00% 68.00% <
VA 15 33.33% 53.33% <
VV 402 67.66% 72.39% <
Table 7: The tagging recall of OOV words.
learn refined grammars and are therefore more ro-
bust to parse non-English languages that are not well
studied. For Chinese, a PCFG-LA parser achieves
the state-of-the-art performance and defeat many
other types of parsers (Zhang and Clark, 2009). For
full parsing, the Berkeley parser6, an open source
implementation of the PCFG-LA model, is used for
experiments. Table 8 shows their overall and de-
tailed performance.
4.1.1 Content Words vs. Function Words
Table 8 gives a detailed comparison regarding dif-
ferent word types. For each type of word, we re-
port the accuracy of both solvers and compare the
difference. The majority of the words that are bet-
ter labeled by the tagger are content words, includ-
ing nouns(NN, NR, NT), numbers (CD, OD), pred-
icates (VA, VC, VE), adverbs (AD), nominal modi-
fiers (JJ), and so on. In contrast, most of the words
that are better predicted by the parser are function
words, including most particles (DEC, DEG, DER,
DEV, AS, MSP), prepositions (P, BA) and coordi-
nating conjunction (CC).
4.1.2 Open Classes vs. Close Classes
POS can be divided into two broad supercate-
gories: closed class types and open class types.
Open classes accept the addition of new morphemes
(words), through such processes as compounding,
derivation, inflection, coining, and borrowing. On
the other hand closed classes are those that have rel-
atively fixed membership. For example, nouns and
verbs are open classes because new nouns and verbs
are continually coined or borrowed from other lan-
guages, while DEC/DEG are two closed classes be-
cause only the function word ??? is assigned to
6http://code.google.com/p/
berkeleyparser/
247
Parser<Tagger Parser>Tagger
? AD 94.15<94.71 ? AS 98.54>98.44
? CD 94.66<97.52 ? BA 96.15>92.52
CS 91.12<92.12 ? CC 93.80>90.58
ETC 99.65<100.0 ? DEC 85.78>81.22
? JJ 81.35<84.65 ? DEG 88.94>85.96
LB 91.30<93.18 ? DER 80.95>77.42
LC 96.29<97.08 ? DEV 84.89>74.78
M 95.62<96.94 DT 98.28>98.05
? NN 93.56<94.95 ?MSP 91.30>90.14
? NR 89.84<95.07 ? P 96.26>94.56
? NT 96.70<97.26 VV 91.99>91.87
? OD 81.06<86.36
PN 98.10<98.15
SB 95.36<96.77
SP 61.70<68.89
? VA 81.27<84.25 Overall
? VC 95.91<97.67 Tagger: 94.48%
? VE 97.12<98.48 Parser: 93.69%
Table 8: Tagging accuracies of relative to word classes.
them. The discriminative model can conveniently
include many features, especially features related to
the word formation, which are important to predict
words of open classes. Table 9 summarizes the tag-
ging accuracies relative to IV and OOV words. On
the whole, the Berkeley parser processes IV words
slightly better than our tagger, but processes OOV
words significantly worse. The numbers in this ta-
ble clearly shows the main weakness of the Berkeley
parser is the the predictive power of the OOV words.
IV OOV
Tagger 95.22% 81.59%
Parser 95.38% 64.77%
Table 9: Tagging accuracies of the IV and OOV words.
4.1.3 Local Disambiguation vs. Global
Disambiguation
Closed class words are generally function words
that tend to occur frequently and often have struc-
turing uses in grammar. These words have little
lexical meaning or have ambiguous meaning, but
instead serve to express grammatical relationships
with other words within a sentence. They signal
the structural relationships that words have to one
another and are the glue that holds sentences to-
gether. Thus, they serve as important elements to the
structures of sentences. The disambiguation of these
words normally require more syntactic clues, which
is very hard and inappropriate for a sequential tagger
to capture. Based on global grammatical inference
of the whole sentence, the full parser is relatively
good at dealing with structure related ambiguities.
We conclude that discriminative sequential tag-
ging model can better capture local syntactic and
morphological information, while the full parser can
better capture global syntactic structural informa-
tion. The discriminative tagging model are limited
by the Markov assumption and inadequate to cor-
rectly label structure related words.
4.2 Enhancing POS Tagging via Bagging
The diversity analysis suggests that we may im-
prove parsing by simply combining the tagger and
the parser. Bootstrap aggregating (Bagging) is a ma-
chine learning ensemble meta-algorithm to improve
classification and regression models in terms of sta-
bility and classification accuracy (Breiman, 1996). It
also reduces variance and helps to avoid overfitting.
We introduce a Bagging model to integrate different
POS tagging models. In the training phase, given
a training set D of size n, our model generates m
new training sets Di of size 63.2%? n by sampling
examples from D without replacement. Namely no
example will be repeated in each Di. Each Di is
separately used to train a tagger and a parser. Us-
ing this strategy, we can get 2m weak solvers. In the
tagging phase, the 2m models outputs 2m tagging
results, each word is assigned one POS label. The
final tagging is the voting result of these 2m labels.
There may be equal number of different tags. In this
case, our system prefer the first label they met.
4.3 Evaluation
We evaluate our combination model on the same
data set used above. Figure 1 shows the influence
of m in the Bagging algorithm. Because each new
data set Di in bagging algorithm is generated by a
random procedure, the performance of all Bagging
experiments are not the same. To give a more sta-
ble evaluation, we repeat 5 experiments for each m
and show the averaged accuracy. We can see that
the Bagging model taking both sequential tagging
and chart parsing models as basic systems outper-
form the baseline systems and the Bagging model
taking either model in isolation as basic systems. An
248
 93
 93.5
 94
 94.5
 95
 95.5
 1  2  3  4  5  6  7  8  9  10
A
c
c
u
r
a
c
y
 
(
%
)
Number of sampling data sets m
TaggerParserTagger(WC)Tagger-BaggingParser-BaggingTagger+Parser-BaggingTagger(WC)-BaggingTagger(WC)+Parser-Bagging
Figure 1: Tagging accuracies of Bagging models.
Tagger-Bagging and Tagger(WC)-Bagging means that the
Bagging system built on the tagger with and without
word clusters. Parser-Bagging is named in the same way.
Tagger+Paser-Bagging and Tagger(WC)+Paser-Bagging
means that the Bagging systems are built on both tagger
and parser.
interesting phenomenon is that the Bagging method
can also improve the parsing model, but there is a
decrease while only combining taggers.
5 Combining Both
We have introduced two separate improvements for
Chinese POS tagging, which capture different types
of lexical relations. We therefore expect further im-
provement by combining both enhancements, since
their contributions to the task is different. We still
use a Bagging model to integrate the discriminative
tagger and the Berkeley parser. The only differ-
ence between current experiment and previous ex-
periment is that the sub-tagging models are trained
with help of word clustering features. Figure 1 also
shows the performance of the new Bagging model
on the development data set. We can see that the im-
provements that come from two ways, namely cap-
turing syntagmatic and paradigmatic relations, are
not much overlapping and the combination of them
gives more.
Table 10 shows the performance of different sys-
tems evaluated on the test data. The final result is
remarkable. The word clustering features and the
Bagging model result in a relative error reduction of
18% in terms of the classification accuracy. The sig-
nificant improvement of the POS tagging also help
successive language processing. Results in Table
Systems Acc.
Baseline 94.33%
Tagger(WC) 94.85%
Tagger+Parser(m = 15) 94.96%
Tagger(WC)+Parser(m = 15) 95.34%
Table 10: Tagging accuracies on the test data (CoNLL).
11 indicate that the parsing accuracy of the Berke-
ley parser can be simply improved by inputting the
Berkeley parser with the POS Bagging results. Al-
though the combination with a syntax-based tagger
is very effective, there are two weaknesses: (1) a
syntax-based model relies on linguistically rich syn-
tactic annotations that are not easy to acquire; (2)
a syntax-based model is computationally expensive
which causes efficiency difficulties.
Tagger LP LR F
Berkeley 82.71% 80.57% 81.63
Bagging(m = 15) 82.96% 81.44% 82.19
Table 11: Parsing accuracies on the test data. (CoNLL)
6 Conclusion
We hold a view of structuralist linguistics and study
the impact of paradigmatic and syntagmatic lexical
relations on Chinese POS tagging. First, we har-
vest word partition information from large-scale raw
texts to capture paradigmatic relations and use such
knowledge to enhance a supervised tagger via fea-
ture engineering. Second, we comparatively analyze
syntax-free and syntax-based models and employ a
Bagging model to integrate a sequential tagger and
a chart parser to capture syntagmatic relations that
have a great impact on non-local disambiguation.
Both enhancements significantly improve the state-
of-the-art of Chinese POS tagging. The final model
results in an error reduction of 18% over a state-of-
the-art baseline.
Acknowledgement
This work is mainly finished when the first author
was in Saarland University and DFKI. At that time,
this author was funded by DFKI and German Aca-
demic Exchange Service (DAAD). While working
in Peking University, the first author is supported
by NSFC (61170166) and National High-Tech R&D
Program (2012AA011101).
249
References
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural
language. Computational Linguistics, 18:467?
479. URL http://portal.acm.org/
citation.cfm?id=176313.176316.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the first con-
ference on North American chapter of the Associ-
ation for Computational Linguistics.
Michael Collins. 2002. Discriminative training
methods for hidden markov models: Theory
and experiments with perceptron algorithms. In
Proceedings of the 2002 Conference on Empir-
ical Methods in Natural Language Processing,
pages 1?8. Association for Computational Lin-
guistics. URL http://www.aclweb.org/
anthology/W02-1001.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Jes?s Gim?nez and Llu?s M?rquez. 2004.
Svmtool: A general pos tagger generator based
on support vector machines. In In Proceedings
of the 4th International Conference on Language
Resources and Evaluation, pages 43?46.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1077?1086. Association for Computational Lin-
guistics, Uppsala, Sweden. URL http://www.
aclweb.org/anthology/P10-1110.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm
part-of-speech tagger by latent annotation and
self-training. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics, Compan-
ion Volume: Short Papers, pages 213?216. As-
sociation for Computational Linguistics, Boulder,
Colorado. URL http://www.aclweb.org/
anthology/N/N09/N09-2054.
Zhongqiang Huang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and dis-
criminative reranking. In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-
CoNLL), pages 1093?1102. Association for
Computational Linguistics, Prague, Czech Re-
public. URL http://www.aclweb.org/
anthology/D/D07/D07-1117.
Reinhard Kneser and Hermann Ney. 1993. Im-
proved clustering techniques for class-based sta-
tistical language modeling. In In Proceedings of
the European Conference on Speech Communica-
tion and Technology (Eurospeech).
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency
parsing. In Proceedings of ACL-08: HLT,
pages 595?603. Association for Computa-
tional Linguistics, Columbus, Ohio. URL
http://www.aclweb.org/anthology/
P/P08/P08-1068.
Thomas Lavergne, Olivier Cappe?, and Franc?ois
Yvon. 2010. Practical very large scale CRFs.
pages 504?513. URL http://www.aclweb.
org/anthology/P10-1052.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting
Liu, Wenliang Chen, and Haizhou Li. 2011.
Joint models for Chinese pos tagging and depen-
dency parsing. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1180?1191. Association
for Computational Linguistics, Edinburgh, Scot-
land, UK. URL http://www.aclweb.org/
anthology/D11-1109.
Percy Liang, Michael Collins, and Percy Liang.
2005. Semi-supervised learning for natural lan-
guage. In Master?s thesis, MIT.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi
Tsujii. 2005. Probabilistic cfg with latent an-
notations. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, ACL ?05, pages 75?82. Associa-
tion for Computational Linguistics, Stroudsburg,
250
PA, USA. URL http://dx.doi.org/10.
3115/1219840.1219850.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 337?342. As-
sociation for Computational Linguistics, Boston,
Massachusetts, USA.
Franz Josef Och. 1999. An efficient method for
determining bilingual word classes. In Pro-
ceedings of the ninth conference on European
chapter of the Association for Computational
Linguistics, EACL ?99, pages 71?76. Associa-
tion for Computational Linguistics, Stroudsburg,
PA, USA. URL http://dx.doi.org/10.
3115/977035.977046.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
433?440. Association for Computational Linguis-
tics, Sydney, Australia.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for
Computational Linguistics; Proceedings of the
Main Conference, pages 404?411. Association
for Computational Linguistics, Rochester, New
York.
Libin Shen, Giorgio Satta, and Aravind Joshi.
2007. Guided learning for bidirectional sequence
classification. In Proceedings of the 45th An-
nual Meeting of the Association of Computa-
tional Linguistics, pages 760?767. Association
for Computational Linguistics, Prague, Czech Re-
public. URL http://www.aclweb.org/
anthology/P07-1096.
Weiwei Sun. 2010. Word-based and character-
based word segmentation models: Compari-
son and combination. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1211?1219.
Coling 2010 Organizing Committee, Beijing,
China. URL http://www.aclweb.org/
anthology/C10-2139.
Weiwei Sun and Jia Xu. 2011. Enhancing
Chinese word segmentation using unlabeled
data. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, pages 970?979. Association
for Computational Linguistics, Edinburgh, Scot-
land, UK. URL http://www.aclweb.org/
anthology/D11-1090.
Kristina Toutanova, Dan Klein, Christopher D.
Manning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages
173?180. Association for Computational Linguis-
tics, Stroudsburg, PA, USA. URL http://dx.
doi.org/10.3115/1073445.1073478.
Huihsin Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning.
2005a. A conditional random field word seg-
menter. In In Fourth SIGHAN Workshop on Chi-
nese Language Processing.
Huihsin Tseng, Daniel Jurafsky, and Christopher
Manning. 2005b. Morphological features help
pos tagging of unknown words across language
varieties. In The Fourth SIGHAN Workshop on
Chinese Language Processing.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006. A fast, accurate deterministic parser for
Chinese. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics
and 44th Annual Meeting of the Association for
Computational Linguistics, pages 425?432. As-
sociation for Computational Linguistics, Sydney,
Australia. URL http://www.aclweb.org/
anthology/P06-1054.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn Chinese treebank: Phrase
structure annotation of a large corpus. Natural
Language Engineering, 11(2):207?238.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
251
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
562?571. Association for Computational Linguis-
tics, Honolulu, Hawaii. URL http://www.
aclweb.org/anthology/D08-1059.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the Chinese treebank using a
global discriminative model. In Proceedings
of the 11th International Conference on Pars-
ing Technologies (IWPT?09), pages 162?171. As-
sociation for Computational Linguistics, Paris,
France. URL http://www.aclweb.org/
anthology/W09-3825.
252
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 198?202
Manchester, August 2008
Hybrid Learning of Dependency Structures from Heterogeneous
Linguistic Resources
Yi Zhang
Language Technology Lab
DFKI GmbH
yzhang@coli.uni-sb.de
Rui Wang
Computational Linguistics
Saarland University, Germany
rwang@coli.uni-sb.de
Hans Uszkoreit
Language Technology Lab
DFKI GmbH
uszkoreit@dfki.de
Abstract
In this paper we present our syntactic and
semantic dependency parsing system par-
ticipated in both closed and open compe-
titions of the CoNLL 2008 Shared Task.
By combining the outcome of two state-of-
the-art syntactic dependency parsers, we
achieved high accuracy in syntactic de-
pendencies (87.32%). With MRSes from
grammar-based HPSG parsers, we achieved
significant performance improvement on
semantic role labeling (from 71.31% to
71.89%), especially in the out-domain
evaluation (from 60.16% to 62.11%).
1 Introduction
The CoNLL 2008 shared task (Surdeanu et al,
2008) provides a unique chance of comparing dif-
ferent syntactic and semantic parsing techniques
in one unified open competition. Our contribution
in this joint exercise focuses on the combination
of different algorithms and resources, aiming not
only for state-of-the-art performance in the com-
petition, but also for the dissemination of the learnt
lessons to related sub-fields in computational lin-
guistics.
The so-called hybrid approach we take has two
folds of meaning. For syntactic dependency pars-
ing, we build our system based on state-of-the art
algorithms. Past CoNLL share task results have
shown that transition-based and graph-based algo-
rithms started from radically different ideas, yet
achieved largely comparable results. One of the
question we would like investigate is whether the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
combination of the two approach on the output
level leads to even better results.
For the semantic role labeling (SRL) task, we
would like to build a system that allows us to test
the contribution of different linguistic resources.
To our special interest is to examine the deep
linguistic parsing systems based on hand-crafted
grammars. During the past decades, various large
scale linguistic grammars have been built, some
of which achieved both broad coverage and high
precision. In combination with other advances
in deep linguistic processing, e.g. efficient pars-
ing algorithms, statistical disambiguation models
and robust processing techniques, several systems
have reached mature stage to be deployed in ap-
plications. Unfortunately, due to the difficulties
in cross-framework evaluation, fair comparison of
these systems with state-of-the-art data-driven sta-
tistical parsers is still hard to achieve. More impor-
tantly, it is not even clear whether deep linguistic
analysis is necessary at all for tasks such as shallow
semantic parsing (also known as SRL). Drawing
a conclusion on this latter point with experiments
using latest deep parsing techniques is one of our
objective.
The remainder of the paper is structure as fol-
lows. Section 2 introduces the overall system ar-
chitecture. Section 3 explains the voting mecha-
nism used in the syntactic parser. Section 4 de-
scribes in detail the semantic role labeling com-
ponent. Section 5 presents evaluation results and
error analysis. Section 6 concludes the paper.
2 System Architecture
As shown in Figure 1, our system is a two-stage
pipeline. For the syntactic dependencies, we apply
two state-of-the-art dependency parsers and com-
bined their results based on a voting model. For
198
Parse Selector
(MaltParser)Transition?based DepParser (MST Parser)Graph?based DepParser
Deep Linguistic Parser(ERG/PET)Predicate Identification
Argument Identification
Argument Classification
Predicated Classification
SemanticRoleLabeling
Syn.Dep.
MRS
SyntacticDependencyParsing
Figure 1: System Architecture
the semantic roles, we extracted features from the
previous stage, combined with deep parsing results
(in MRS), and use statistical classification models
to make predictions. In particular, the second part
can be further divided into four stages: predicate
identification (PI), argument identification (AI), ar-
gument classification (AC), and predicate classi-
fication (PC). Maximum entropy-based machine
learning techniques are used in both components
which we will see in detail in the following sec-
tions.
3 Syntactic Dependency Parsing
For obtaining syntactic dependencies, we have
combined the results of two state-of-the-art depen-
dency parsers: the MST parser (McDonald et al,
2005) and the MaltParser (Nivre et al, 2007).
The MST parser formalizes dependency parsing
as searching for maximum spanning trees (MSTs)
in directed graphs. A major advantage of their
framework is the ability to naturally and efficiently
model both projective and non-projective parses.
To learn these structures they used online large-
margin learning that empirically provides state-of-
the-art performance.
The MaltParser is a transition-based incremental
dependency parser, which is language-independent
and data-driven. It contains a deterministic algo-
rithm, which can be viewed as a variant of the ba-
sic shift-reduce algorithm. The learning method
they applied is support vector machine and experi-
mental evaluation confirms that the MaltParser can
achieve robust, efficient and accurate parsing for a
wide range of languages.
Since both their parsing algorithms and machine
learning methods are quite different, we decide to
take advantages of them. After a comparison be-
tween the results of the two parsers
1
, we find that,
1. The MST parser is better at the whole struc-
ture. In several sentences, the MaltParser was
wrong at the root node, but the MST parser is
correct.
2. The MaltParser is better at some dependency
labels (e.g. TMP, LOC, etc.).
These findings motivate us to do a voting based
on both outputs. The features considered in the
voting model are as follows:
? Dependency path: two categories of depen-
dency paths are considered as features: 1)
the POS-Dep-POS style and 2) the Dep-Dep
style. The former consists of part-of-speech
(POS) tags and dependency relations appear-
ing in turns; and the latter only contains de-
pendency relations. The maximum length of
the dependency path is three dependency re-
lations.
? Root attachments: the number of tokens at-
tached to the ROOT node by the parser in one
sentence
? Sentence length: the number of tokens in
each input sentence
? Projectivity: whether the parse is projective
or not
With these features, we apply a statistical model
to predict, for each sentence, we choose the pars-
ing result from which parser. The voted result will
be our syntactic dependency output and be passed
to the later stages.
4 Semantic Role Labeling
4.1 Overview
The semantic role labeling component of our sys-
tem is comprised of a pipeline model with four
1
In this experiment, we use second order features and the
projective decoder for the MST parser trained with 10 iter-
ations, and Arc-eager algorithm with a quadric polynomial
kernel for the MaltParser.
199
sub-components that performs predicate identi-
fication (PI), argument identification (AI), argu-
ment classification (AC) and predicate classifica-
tion (PC) respectively. The output in previous
steps are taken as input information to the follow-
ing stages. All these components are essentially
based on a maximum entropy statistical classifier,
although with different task-specific optimizations
and feature configurations in each step. Depending
on the available information from the input data
structure, the same architecture is used for both
closed and open challenge runs, with different fea-
ture types. Note that our system does not make use
of or predict SU chains.
Predicate Identification The component makes
binary prediction on each input token whether it
forms a predicate in the input sentence. This pre-
dictor precedes other components because it is a
relatively easy task (comparing to the following
components). Also, making this prediction early
helps to cut down the search space in the follow-
ing steps. Based on the observation on the training
data, we limit the PI predictor to only predict for
tokens with certain POS types (POSes marked as
predicates for at least 50 times in the training set).
This helps to significantly improve the system effi-
ciency in both training and prediction time without
sacrificing prediction accuracy.
It should be noted that the prediction of nominal
predicates are generally much more difficult (based
on CoNLL 2008 shared task annotation). The PI
model achieved 96.32 F-score on WSJ with verbal
predicates, but only 84.74 on nominal ones.
Argument Identification After PI, the argu-
ments to the predicted predicates are identified
with the AI component. Similar to the approach
taken in Hacioglu (2004), we use a statistical clas-
sifier to select from a set of candidate nodes in a
dependency tree. However, instead of selecting
from a set of neighboring nodes from the predicate
word
2
, we define the concept of argument path as
a chain of dependency relations from the predicate
to the argument in the dependency tree. For in-
stance, an argument path [??? | ???] indicates that
if the predicate is syntactically depending as ??? on
a node which has a ??? child, then the ??? node
2
Hacioglu (2004) defines a tree-structured family of a
predicate as a measure of locality. It is a set of dependency
relation nodes that consists of the predicate?s parent, chil-
dren, grandchildren, siblings, siblings? children and siblings?
grandchildren with respect to its dependency tree
(sibling to the predicate) is an argument candidate.
While Hacioglu (2004)?s approach focus mainly
on local arguments (with respect to the syntactic
dependencies), our approach is more suitable of
capturing long distance arguments from the pred-
icate. Another minor difference is that we allow
predicate word to be its own argument (which is
frequently the case for nominal predicates) with an
empty argument path [ | ].
The set of effective argument paths are obtained
from the training set, sorted and filtered according
to their frequencies, and used in testing to obtain
the candidate arguments. By setting a frequency
threshold, we are able to select the most useful
argument paths. The lower the threshold is, the
higher coverage one might get in finding candi-
date arguments, accompanied with a higher aver-
age candidate number per predicate and potentially
a more difficult task for the statistical classifier.
By experimenting with different frequency thresh-
olds on the training set, we established a frequency
threshold of 40, which guarantees candidate argu-
ment coverage of 95%, and on average 5.76 candi-
dates per predicate. Given that for the training set
each predicate takes on average 2.13 arguments,
the binary classifier will have relatively balanced
prediction classes.
Argument Classification For each identified ar-
gument, an argument label will be assigned during
the argument classification step. Unlike the binary
classifiers in previous two steps, AC uses a multi-
class classifier that predicts from the inventory of
argument labels. For efficiency reasons, we only
concern the most frequent 25 argument labels.
Predicate Classification The final step in the
SRL component labels the predicted predicate with
a predicate name. Due to the lack of lexical
resources in the closed competition, this step is
scheduled for the last, in order to benefit from the
predictions made in the previous steps. Unlike the
previous steps, the statistical model used in this
step is a ranking model. We obtained a list of can-
didate frames and corresponding rolesets from the
provided PropBank and NomBank data. Each pre-
dicted predicate is mapped onto the potential role-
sets it may take. When the frame for the predicate
word is missing from the list, or there is only one
candidate roleset for it, the predicate name is as-
signed deterministically (word stem concatenated
with ?.01? for frame missing predicates, the unam-
200
biguous roleset name when there is only one can-
didate). When there are more than one candidate
rolesets, a ranking model is trained to select the
most probable roleset for a given predicate given
the syntactic and semantic context.
4.2 Features
The feature types used in our SRL component are
summarized in Table 1, with the configurations of
our submitted ?closed? and ?open? runs marked.
Numerous different configurations with these fea-
ture types have been experimented on training and
development data. The results show that feature
types 1?14 are the best performing ones. Fea-
tures related to the siblings of the predicate only
introduce minor performance variation. We also
find the named entity labels does not lead to im-
mediate improvement of SRL performance. The
WordNet sense feature does achieve minor perfor-
mance increase on PI and PC, although the signif-
icant remains to be further examined. Based on
the pipeline model, we find it difficult to achieve
further improvement by incorporate more features
types from provided annotation. And the vari-
ance of SRL performance with different open fea-
tures is usually less than 1%. To clearly show the
contribution of extra external resources, these less
contributing features (siblings, named entity labels
and WordNet sense) are not used in our submitted
?open? runs.
MRSes as features to SRL As a novel point of
our SRL system, we incorporate parsing results
from a linguistic grammar-based parsing system
in our ?open? competition run. In this experi-
ment, we used English Resource Grammar (ERG,
Flickinger (2000)), a precision HPSG grammar for
English. For parsing, we used PET (Callmeier,
2001), an efficient HPSG parser, together with ex-
tended partial parsing module (Zhang et al, 2007)
for maximum robustness. The grammar is hand-
crafted and the disambiguation models are trained
on Redwoods treebanks. They present general lin-
guistic knowledge, and are not tuned for the spe-
cific domains in this competition.
While the syntactic analysis of the HPSG gram-
mar is largely different from the dependency anno-
tation used in this shared task, the semantic rep-
resentations do share a similar view on predicate-
argument structures. ERG uses as its semantic
representation the Minimal Recursion Semantics
(MRS, Copestake et al (2005)), a non-recursive flat
structure that is suitable for underspecifying scope
ambiguities. A predicate-argument backbone of
MRS can be extracted by identifying shared vari-
ables between elementary predications (??s).
In order to align the HPSG parser?s I/O with
CoNLL?s annotation, extensive mapping scripts
are developed to preprocess the texts, and extract
backbone from output MRSes. The unknown word
handling techniques (Zhang and Kordoni, 2005)
are used to overcome lexical gaps. Only the best
analysis is used for MRS extraction. Without par-
tial parsing, the ERG achieves around 70% of raw
coverage on the training data. When partial pars-
ing is used, almost all the sentences received ei-
ther full or partial analysis (except for several cases
where computational resources are exhausted), and
the SRL performance improves by ?0.5%.
5 Results
Among 20+ participant groups, our system ranked
seventh in the ?closed? competition, and first in
the ?open? challenge. The performance of the syn-
tactic and semantic components of our system are
summarized in Table 2.
In-Domain Out-Domain
Lab. Unlab. Lab. Unlab.
Syntactic Dep. 88.14% 90.78% 80.80% 86.12%
S
R
L
Closed 72.67% 82.68% 60.16% 76.98%
Open 73.08% 83.04% 62.11% 78.48%
Table 2: Labeled and unlabeled attachment scores
in syntactic dependency parsing and F1 score for
semantic role labeling.
The syntactic voting and semantic labeling parts
of our system are implemented in Java together
with a few Perl scripts. Using the open source
TADM for parameter estimation, our the voting
component take no more than 1 minute to train and
10 seconds to run (on WSJ testset). The SRL com-
ponent takes about 1 hour for training, and no more
than 30 seconds for labeling (WSJ testset).
Result analysis shows that the combination of
the two state-of-the-art parsers delivers good syn-
tactic dependencies (ranked 2nd). Error analysis
shows most of the errors are related to preposi-
tions. One category is the syntactic ambiguity of
pp-attachment, e.g. in ?when trading was halted
in Philip Morris?, ?in? can be attached to either
?trading? or ?halted?. The other category is the
LOC and TMP tags in phrases like ?at the end of
the day?, ?at the point of departure?, etc.
201
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
P
l
e
m
m
a
P
P
O
S
P
r
e
l
P
-
p
a
r
e
n
t
P
O
S
A
l
e
m
m
a
A
P
O
S
A
r
e
l
P
-
c
h
i
l
d
r
e
n
P
O
S
e
s
P
-
c
h
i
l
d
r
e
n
r
e
l
P
-
A
p
a
t
h
A
-
c
h
i
l
d
r
e
n
P
O
S
e
s
A
-
c
h
i
l
d
r
e
n
r
e
l
s
P
p
r
e
c
e
d
e
s
A
?
A
?
s
p
o
s
i
t
i
o
n
P
-
s
i
b
l
i
n
g
s
P
O
S
e
s
P
-
s
i
b
l
i
n
g
s
r
e
l
s
P
N
E
l
a
b
e
l
P
W
N
s
e
n
s
e
P
M
R
S
?
?
-
n
a
m
e
P
M
R
S
-
a
r
g
s
l
a
b
e
l
s
P
M
R
S
-
a
r
g
s
P
O
S
e
s
A
M
R
S
?
?
-
n
a
m
e
A
M
R
S
-
p
r
e
d
s
l
a
b
e
l
s
A
M
R
S
-
p
r
e
d
s
P
O
S
e
s
PI ? ? ? ? ? ? ? ?   
AI ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?      
AC ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?      
PC ? ? ? ? ? ? ? ?   
Table 1: Feature types used in semantic role labeling sub-components. Feature types marked with ? are
used in the ?closed? run; feature types marked with  are used in the ?open? run; feature types marked
with ? are used in both runs. P denotes predicate; A denotes semantic argument.
The results on semantic role labeling show,
sometimes, even with syntactic errors of
LOC/TMP tags, the semantic role labeler can
still predict AM-LOC/AM-TMP correctly, which
indicates the robustness of our hybrid approach.
By comparing our ?closed? and ?open? runs, the
MRS features do introduce a clear performance
improvement. The performance gain is even
more significant in out-domain test, showing that
the MRS features from ERG are indeed much less
domain dependent. Another example worth men-
tioning is that, in the sentence ?Scotty regarded the
ear and the grizzled hair around it with a moment
of interest?, it is extremely difficult to know that
?Scotty? is a semantic role of ?interest?.
Also, we are the only group that submitted runs
for both tracks, and achieved better performance
in open competition. Although the best ways of
integrating deep linguistic processing techniques
remain as an open question, the achieved results
at least show that hand-crafted grammars like ERG
do provide heterogeneous linguistic insights that
can potentially find their usage in data-driven NLP
tasks as such.
6 Conclusion
In this paper, we described our hybrid system
on both syntactic and semantic dependencies la-
beling. We built a voting model to combine
the results of two state-of-the-art syntactic depen-
dency parsers, and a pipeline model to combine
deep parsing results for SRL. The experimental re-
sults showed the advantages of our hybrid strat-
egy, especially on the cross-domain data set. Al-
though the optimal ways of combining deep pro-
cessing techniques remains to be explored, the
performance gain achieved by incorporating hand-
crafted grammar outputs shows a promising direc-
tion of study for both fields.
References
Callmeier, Ulrich. 2001. Efficient parsing with large-scale
unification grammars. Master?s thesis, Universit?at des
Saarlandes, Saarbr?ucken, Germany.
Copestake, Ann, Dan Flickinger, Carl J. Pollard, and Ivan A.
Sag. 2005. Minimal recursion semantics: an introduction.
Research on Language and Computation, 3(4):281?332.
Flickinger, Dan. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15?28.
Hacioglu, Kadri. 2004. Semantic role labeling using de-
pendency trees. In Proceedings of COLING 2004, pages
1273?1276, Geneva, Switzerland, Aug 23?Aug 27.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and Jan
Hajic. 2005. Non-Projective Dependency Parsing us-
ing Spanning Tree Algorithms. In Proceedings of HLT-
EMNLP 2005, pages 523?530, Vancouver, Canada.
Nivre, Joakim, Jens Nilsson, Johan Hall, Atanas Chanev,
G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov, and Er-
win Marsi. 2007. Maltparser: A language-independent
system for data-driven dependency parsing. Natural Lan-
guage Engineering, 13(1):1?41.
Surdeanu, Mihai, Richard Johansson, Adam Meyers, Llu??s
M`arquez, and Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic and semantic
dependencies. In Proceedings of the 12th Conference
on Computational Natural Language Learning (CoNLL-
2008), Manchester, UK.
Zhang, Yi and Valia Kordoni. 2005. A statistical approach
towards unknown word type prediction for deep grammars.
In Proceedings of the Australasian Language Technology
Workshop 2005, pages 24?31, Sydney, Australia.
Zhang, Yi, Valia Kordoni, and Erin Fitzgerald. 2007. Partial
parse selection for robust deep processing. In Proceed-
ings of ACL 2007 Workshop on Deep Linguistic Process-
ing, pages 128?135, Prague, Czech.
202
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 77?81,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Further Experiments with Shallow Hybrid MT Systems
Christian Federmann1, Andreas Eisele1, Hans Uszkoreit1,2,
Yu Chen1, Sabine Hunsicker1, Jia Xu1
1: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
2: Universita?t des Saarlandes, Saarbru?cken, Germany
{cfedermann,eisele,uszkoreit,yuchen,sabine.hunsicker,jia.xu}@dfki.de
Abstract
We describe our hybrid machine trans-
lation system which has been developed
for and used in the WMT10 shared task.
We compute translations from a rule-
based MT system and combine the re-
sulting translation ?templates? with par-
tial phrases from a state-of-the-art phrase-
based, statistical MT engine. Phrase sub-
stitution is guided by several decision
factors, a continuation of previous work
within our group. For the shared task,
we have computed translations for six lan-
guage pairs including English, German,
French and Spanish. Our experiments
have shown that our shallow substitu-
tion approach can effectively improve the
translation result from the RBMT system;
however it has also become clear that a
deeper integration is needed to further im-
prove translation quality.
1 Introduction
In recent years the quality of machine translation
(MT) output has improved greatly, although each
paradigm suffers from its own particular kind of
errors: statistical machine translation (SMT) of-
ten shows poor syntax, while rule-based engines
(RBMT) experience a lack in vocabulary. Hybrid
systems try to avoid these typical errors by com-
bining techniques from both paradigms in a most
useful manner.
In this paper we present the improved version of
the hybrid system we developed last year?s shared
task (Federmann et al, 2009). We take the out-
put from an RBMT engine as basis for our hybrid
translations and substitute noun phrases by trans-
lations from an SMT engine. Even though a gen-
eral increase in quality could be observed, our sys-
tem introduced errors of its own during the substi-
tution process. In an internal error analysis, these
degradations were classified as follows:
- the translation by the SMT engine is incorrect
- the structure degrades through substitution
(because of e.g. capitalization errors, double
prepositions, etc.)
- the phrase substitution goes astray (caused by
alignment problems, etc.)
Errors of the first class cannot be corrected, as
we have no way of knowing when the translation
by the SMT engine is incorrect. The other two
classes could be eliminated, however, by introduc-
ing additional steps for pre- and post-processing
as well as improving the hybrid algorithm itself.
Our current error analysis based on the results of
this year?s shared task does not show these types
of errors anymore.
Additionally, we extended our coverage to also
include the language pairs English?French and
English?Spanish in both directions as well as
English?German, compared to last year?s initial
experiments for German?English only. We were
able to achieve an increase in translation quality
for this language set, which shows that the substi-
tution method works for different language config-
urations.
2 Architecture
Our hybrid translation system takes translation
output from a) the Lucy RBMT system (Alonso
and Thurmair, 2003) and b) a Moses-based SMT
system (Koehn et al, 2007). We then identify
noun phrases inside the rule-based translation and
compute the most likely correspondences in the
statistical translation output. For these, we apply a
factored substitution method that decides whether
the original RBMT phrase should be kept or rather
be replaced by the Moses phrase. As this shallow
substitution process may introduce problems at
77
phrase boundaries, we afterwards perform several
post-processing steps to cleanup and finalize the
hybrid translation result. A schematic overview
of our hybrid system and its main components is
given in figure 1.
Figure 1: Schematic overview of the hybrid MT
system architecture.
2.1 Input to the Hybrid System
Lucy RBMT System We obtain the translation
as well as linguistic structures from the RBMT
system. An internal evaluation has shown that
these structures are usually of a high quality which
supports our initial decision to consider the RBMT
output as an appropriate ?template? for our hybrid
translation approach. The Lucy translation output
can include additional markup that allows to iden-
tify unknown words or other, local phenomena.
The Lucy system is a transfer-based MT system
that performs translation in three phases, namely
analysis, transfer, and generation. Intermediate
tree structures for each of the translation phases
can be extracted from the Lucy system to guide
the hybrid system. Sadly, only the 1-best path
through these three phases is given, so no alterna-
tive translation possibilities can be extracted from
the given data; a fact that clearly limits the poten-
tial for more deeply integrated hybrid translation
approaches. Nevertheless, the availability of the
1-best trees already allows to improve the transla-
tion quality of the RBMT system as we will show
in this paper.
Moses SMT System We used a state-of-the-art
Moses SMT system to create statistical phrase-
based translations of our input text. Moses has
been modified so that it returns the translation re-
sults together with the bidirectional word align-
ments between the source texts and the transla-
tions. Again, we make use of markup which helps
to identify unknown words as these will later guide
the factored substitution method. Both of the
translation models and the language models within
our SMT systems were only trained with lower-
cased and tokenized Europarl training data. The
system used sets of feature weights determined us-
ing data sets also from Europarl (test2008). In
addition, we used LDC gigaword corpus to train
large scale n-gram language models to be used in
our hybrid system. We tokenized the source texts
using the standard tokenizers available from the
shared task website. The SMT translations are re-
cased before being fed into the hybrid system to-
gether with the word alignment information.The
hybrid system can easily be adapted to support
other statistical translation engines. If the align-
ment information is not available, a suitable align-
ment tool would be necessary to compute it as the
alignment is a key requirement for the hybrid sys-
tem.
2.2 Aligning RBMT and SMT Output
We compute alignment in several components of
the hybrid system, namely:
source-text-to-tree: we first find an alignment
between the source text and the correspond-
ing analysis tree(s). As Lucy tends to sub-
divide large sentences into several smaller
units, it sometimes becomes necessary to
align more than one tree structure to a given
source sentence.
analysis-transfer-generation: for each of the
analysis trees, we re-construct the path from
its tree nodes, via the transfer tree, and their
corresponding generation tree nodes.
tree-to-target-text: similarly to the first align-
ment process, we find a mapping between
generation tree nodes and the actual transla-
tion output of the RBMT system.
source-text-to-tokenized: as the Lucy RBMT
system works on non-tokenized input text
and our Moses system takes tokenized input,
78
we need to align the source text to its tok-
enized form.
Given the aforementioned alignments, we can then
correlate phrases from the rule-based translation
with their counterparts from the statistical trans-
lation, both on source or target side. As our
hybrid approach relies on the identification of
such phrase pairs, the computation of the different
alignments is critical to obtain good combination
performance.
Please note that all these tree-based alignments
can be computed with a very high accuracy. How-
ever, due to the nature of statistical word align-
ment, the same does not hold for the alignment
obtained from the Moses system. If the alignment
process has produced erroneous phrase tables, it is
very likely that Lucy phrases and their ?aligned?
SMT matches simply will not fit. Or put the other
way round: the better the underlying SMT word
alignment, the greater the potential of the hybrid
substitution approach.
2.3 Factored Substitution
Given the results of the alignment process, we can
then identify ?interesting? phrases for substitution.
Following our experimental setup from last year?s
shared task, we again decided to focus on noun
phrases as these seem to be best-suited for in-place
swapping of phrases. Our initial assumption is that
SMT phrases are better on a lexical level, hence
we aim to replace Lucy?s noun phrases by their
Moses counterparts.
Still, we want to perform the substitution in a
controlled manner in order to avoid problems or
non-matching insertions. For this, we have (man-
ually) derived a set of factors that are checked for
each of the phrase pairs that are processed. The
factors are described briefly below:
identical? simply checks whether two candidate
phrases are identical.
too complex? a Lucy phrase is ?too complex?
to substitute if it contains more than 2
embedded noun phrases.
many-to-one? this factor checks if a Lucy phrase
containing more than one word is mapped to
a Moses phrase with only one token.
contains pronoun? checks if the Lucy phrase
contains a pronoun.
contains verb? checks if the Lucy phrase con-
tains a verb.
unknown? checks whether one of the phrases is
marked as ?unknown?.
length mismatch computes the number of words
for both phrases and checks if the absolute
difference is too large.
language model computes language model
scores for both phrases and checks which is
more likely according to the LM.
All of these factors have been designed and ad-
justed during an internal development phase using
data from previous shared tasks.
2.4 Post-processing Steps
After the hybrid translation has been computed,
we perform several post-processing steps to clean
up and finalize the result:
cleanup first, we perform basic cleanup opera-
tions such as whitespace normalization, cap-
italizing the first word in each sentence, etc.
multi-words then, we take care of proper han-
dling of multi-word expressions. Using the
tree structures from the RBMT system we
eliminate superfluous whitespace and join
multi-words, even if they were separated in
the SMT phrase.
prepositions finally, we give prepositions a spe-
cial treatment. Experience from last year?s
shared task had shown that things like double
prepositions contributed to a large extent to
the amount of avoidable errors. We tried to
circumvent this class of error by identifying
the correct prepositions; erroneous preposi-
tions are removed.
3 Hybrid Translation Analysis
We evaluated the intermediate outputs using
BLEU (Papineni et al, 2001) against human refer-
ences as in table 3. The BLEU score is calculated
in lower case after the text tokenization. The trans-
lation systems compared are Moses, Lucy, Google
and our hybrid system with different configura-
tions:
Hybrid: we use the language model with case
information and substitute some NPs in Lucy
outputs by Moses outputs.
Hybrid LLM: same as Hybrid but we use a
larger language model.
79
Table 1: Intermediate results of BLEU[%] scores for WMT10 shared task.
System de?en en?de fr?en en?fr es?en en?es
Moses 18.32 12.66 22.26 20.06 24.28 24.72
Lucy 16.85 12.38 18.49 17.61 21.09 20.85
Google 25.64 18.51 28.53 28.70 32.77 32.20
Hybrid 17.29 13.05 18.92 19.58 22.53 23.55
Hybrid LLM 17.37 13.73 18.93 19.76 22.61 23.66
Hybrid SG 17.43 14.40 19.67 20.55 24.37 24.99
Hybrid NCLM 17.38 14.42 19.56 20.55 24.41 24.92
Hybrid SG: same as Hybrid but the NP substitu-
tions are based on Google output instead of
Moses translations.
Hybrid NCLM: same as Hybrid but we use the
language model without case information.
We participated in the translation evaluation in
six language pairs: German to English (de?en),
English to German (en?de), French to English
(fr?en), English to French (en?fr), Spanish to
English (es?en) and English to Spanish (en?es).
As shown in table 3, the Moses translation sys-
tem achieves better results overall than the Lucy
system does. Google?s system outperforms other
systems in all language pairs. The hybrid transla-
tion as described in section 2 improves the Lucy
translation quality with a BLEU score up to 2.7%
absolutely.
As we apply a larger language model or a lan-
guage model without case information, the trans-
lation performance can be improved further. One
major problem in the hybrid translation is that the
Moses outputs are still not good enough to replace
the Lucy outputs, therefore we experimented on
a hybrid translation of Google and Lucy systems
and substitute some unrelaible NP translations by
the Google?s translations. The results in the line
of ?Hybrid SG? shows that the hybrid translation
quality can be enhanced if the translation system
where we select substitutions is better.
4 Internal Evaluation of Results
In the analysis of the remaining issues, the fol-
lowing main sources of problems can be distin-
guished:
- Lucy?s output contains structural errors that
cannot be fixed by the chosen approach.
- Lucy results contain errors that could have
been corrected by alternative expressions
from SMT, but the constraints in our system
were too restrictive to let that happen.
- The SMT engine we use generates subopti-
mal results that find their way into the hybrid
result.
- SMT results that are good are incorporated
into the hybrid results in a wrong way.
We have inspected a part of the results and classi-
fied the problems according to these criteria. As
this work is still ongoing, it is too early to report
numerical results for the relative frequencies of the
different causes of the error. However, we can
already see that three of these four cases appear
frequently enough to justify further attention. We
observed several cases in which the parser in the
Lucy system was confused by unknown expres-
sions and delivered results that could have been
significantly improved by a more robust parsing
approach. We also encountered several cases in
which an expression from SMT was used although
the original Lucy output would have been better.
Also we still observe problems finding to correct
correspondences between Lucy output and SMT
output, which leads to situations where material is
inserted in the wrong place, which can lead to the
loss of content words in the output.
5 Conclusion and Outlook
In our contribution to the shared task we have ap-
plied the hybrid architecture from (Federmann et
al., 2009) to six language pairs. We have identi-
fied and fixed many of the problems we had ob-
served last year, and we think that, in addition to
the increased coverage in laguage pairs, the overall
quality has been significantly increased.
However, in the last section we characterized
three main sources of problems that will require
further attention. We will address these problems
in the near future in the following way:
80
1. We will investigate in more detail the align-
ment issue that leads to occasional loss of
content words, and we expect that a careful
inspection and correction of the code will in
all likelihood give us a good remedy.
2. The problem of picking expressions from the
SMT output that appear more probable to the
language model although they are inferior to
the original expression from the RBMT sys-
tem is more difficult to fix. We will try to find
better thresholds and biases that can at least
reduce the number of cases in which this type
of degradation happen.
3. Finally, we will also address the robustness
issue that leads to suboptimal structures from
the RBMT engine caused by parsing failures.
Our close collaboration with Lucy enables us to
address these issues in a very effective way via the
inspection and classification of intermediate struc-
tures and, if these structures indicate parsing prob-
lems, the generation of variants of the input sen-
tence that facilitate correct parsing.
Acknowledgments
This work was supported by the EuroMatrixPlus
project (IST-231720) which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme. The authors want to thank Michael
Jellinghaus and Bastian Simon for help with the
inspection of intermediate results and classifica-
tion of errors.
References
Juan A. Alonso and Gregor Thurmair. 2003. The Com-
prendium Translator system. In Proc. of the Ninth
MT Summit.
Christian Federmann, Silke Theison, Andreas Eisele,
Hans Uszkoreit, Yu Chen, Michael Jellinghaus, and
Sabine Hunsicker. 2009. Translation combination
using factored word substitution. In Proceedings of
the Fourth Workshop on Statistical Machine Transla
tion, pages 70?74, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL Demo and Poster Sessions, pages 177?
180, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report
RC22176(W0109-022), IBM.
81
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 485?489,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
DFKI Hybrid Machine Translation System for WMT 2011
- On the Integration of SMT and RBMT
Jia Xu and Hans Uszkoreit and Casey Kennington and David Vilar and Xiaojun Zhang
DFKI GmbH, Language Technology Lab
Stuhlsatzenhausweg 3
D-66123 Saarbru?cken Germany
{Jia.Xu,uszkoreit,David.Vilar}@dfki.de, {bakuzen,xiaojun.zhang.iiken}@gmail.com
Abstract
We present the DFKI hybrid translation sys-
tem at the WMT workshop 2011. Three SMT
and two RBMT systems are combined at the
level of the final translation output. The trans-
lation results show that our hybrid system sig-
nificantly outperformed individual systems by
exploring strengths of both rule-based and sta-
tistical translations.
1 Introduction
Machine translation (MT), in particular the statisti-
cal approach to it, has undergone incremental im-
provements in recent years. While rule-based ma-
chine translation (RBMT) maintains competitive-
ness in human evaluations. Combining the advan-
tages of both approaches have been investigated by
many researchers such as (Eisele et al, 2008).
Nonetheless, significant improvements over statis-
tical approaches still remain to be shown. In this
paper, we present the DFKI hybrid system in the
WMT workshop 2011. Our system is different from
the system of the last year (Federmann et al, 2010),
which is based on the shallow phrase substitution.
In this work, two rule-based translation systems are
applied. In addition, three statistical machine trans-
lation systems are built, including a phrase-based,
a hierarchical phrase-based and a syntax-based sys-
tem. Instead of combining with rules or post-editing,
we perform system combination on the final transla-
tion hypotheses. We applied the CMU open toolkit
(Heafield and Lavie, 2010) among numerous com-
bination methods such as (Matusov, 2009), (Sim et
al., 2007) and (He et al, 2008). The final transla-
tion output outperforms each individual output sig-
nificantly.
2 Individual translation systems
2.1 Phrase-based system
We use the IBM model 1 and 4 (Brown et al, 1993)
and Hidden-Markov model (HMM) (Vogel et al,
1996) to train the word alignment using the mgiza
toolkit1. We applied the EMS in Moses (Koehn et
al., 2007) to build up the phrase-based translation
system. Features in the log-linear model include
translation models in two directions, a language
model, a distortion model and a sentence length
penalty. A dynamic programming beam search al-
gorithm is used to generate the translation hypoth-
esis with maximum probability. We applied a 5-
gram mixture language model with each sub-model
trained on one fifth of the monolingual corpus with
Kneser-Ney smoothing using SRILM toolkit (Stol-
cke, 2002). We did not perform any tuning, because
it hurts the evaluation performance in our experi-
ments.
2.2 Syntax-based system
To capture the syntactic structure, we also built a
tree-based system using the same configuration of
EMS in Moses (Koehn et al, 2007). Tree-based
models operate on so-called grammar rules, which
include variables in the mapping rules. To increase
the diversity of models in combination, the lan-
guage model in each individual translation system
is trained differently. For the tree-based system,
we applied a 4-gram language model with Kneser-
Ney smoothing using SRILM toolkit (Stolcke, 2002)
trained on the whole monolingual corpus. The
test2007 news part is applied to tune the feature
weights using mert, because the tuning on test2007
1http://geek.kyloo.net/software/doku.php/mgiza:overview
485
improves the translation performance more than the
tuning on test2008 in a small-scale experiment for
the tree-based system.
2.3 Hierarchical phrase-based system
For the hierarchical system, we used the open source
hierarchical phrased-based system Jane, developed
at RWTH and free for non-commercial use (Vi-
lar et al, 2010). This approach is an extension
of the phrase-based approach, where the phrases
are allowed to have gaps (Chiang, 2007). In this
way long-range dependencies and reorderings can
be modeled in a consistent statistical framework.
The system uses a fairly standard setup, trained
using the bilingual data provided by the organizers,
word aligned using the mgiza. Two 5-gram language
models were used during decoding: one trained on
the monolingual part of the bilingual training data,
and a larger one trained on the additional news data.
Decoding was carried out using the cube pruning al-
gorithm. The tuning is performed on test2008 with-
out further experiments.
2.4 Rule-based systems
We applied two rule-based translation systems, the
Lucy system (Lucy, 2011) and the Linguatec sys-
tem (Aleksic? and Thurmair, 2011). The Lucy sys-
tem is a recent offspring of METAL. The Linguatec
system is a modular system consisting of grammar,
lexicon and morphological analyzers based on logic
programming using slot grammar.
3 Hybrid translation
A hybrid approach combining rule-based and sta-
tistical machine translation is usually investigated
with an in-box integration, such as multi-way trans-
lation (Eisele et al, 2008), post-editing (Ueffing et
al., 2008) or noun phrase substitution (Federmann
et al, 2010). However, significant improvements
over state-of-the-art statistical machine translation
are still expected. In the meanwhile system combi-
nation methods for instance described in (Matusov,
2009), (Sim et al, 2007) and (He et al, 2008) are
mostly evaluated to combine statistical translation
systems, rule-based systems are not considered. In
this work, we integrate the rule-based and statistical
machine translation system on the level of the final
PBT Syntax
PBT-2010 18.32
Max80words 20.65 21.10
Max100words 20.78
+Compound 21.52 22.13
+Newparallel 21.77
Table 1: Translation performance BLEU[%] on
phrase/syntax-based system using various settings eval-
uated on test10.
translation hypothesis and treat the rule-based sys-
tem anonymously as an individual system. In this
way an black-box integration is allowed using the
current system combination techniques.
We applied the CMU open toolkit (Heafield
and Lavie, 2010) MEMT, a package by Kenneth
Heafield to combine the translation hypotheses. The
language model is trained on the target side of the
parallel training corpus using SRILM (Stolcke,
2002). We used only the Europarl part to train lan-
guage models for tuning and all target side of paral-
lel data to train language models for decoding. The
beam size is set to 80, and 300 nbest is considered.
4 Translation experiments
4.1 MT Setup
The parallel training corpus consists of 1.8
million German-English parallel sentences from
Europarl-v6 (Koehn, MT Summit 2005) and news-
commentary with 48 million tokenized German
words and 54 million tokenized English words re-
spectively. The monolingual training corpus con-
tains the target side of the parallel training cor-
pus and the additional monolingual language model
training data downloaded from (SMT, 2011). We
did not apply the large-scale Gigaword corpus, be-
cause it does not significantly reduce the perplexity
of our language model but raises the computational
requirement heavily.
4.2 Single systems
For each individual translation system, different
configurations are experimented to achieve a higher
translation quality. We take phrase- and syntax-
based translation system as examples. Table 1
presents official submission result on DE-EN by
486
PBT+Syntax 20.37
PBT+Syntax+HPBT 20.78
PBT+HPBT+Linguatec+Lucy 20.27
PBT+Syntax+HPBT+Linguatec+Lucy 20.81
Table 2: Translation performance BLEU[%] on test2011
using hybrid system tuned on test10 with various settings
(DE-EN).
DFKI in 2010. In 2010?s translation system only
Europarl parallel corpus was applied, and the trans-
lation output was evaluated as 18.32% in the BLEU
score. In 2011, we added the News Commentary
parallel corpus and trained the language model on all
monolingual data provided by (SMT, 2011) except
for Gigaword. As shown in Table 1, if we increase
the maximum sentence length of the training cor-
pus from 80 to 100, the BLEU score increases from
20.65% to 20.78%. In the error analysis, we found
that many OOVs come from the compound words
in German. Therefore, we applied the compound
splitting for both German and English by activating
the corrensponding settings in the EMS in Moses.
This leads to a further improvement of nearly 1%
in the BLEU score. As we add the new parallel
corpus provided on the homepage of SMT work-
shop in 2011 (SMT, 2011) to the corpus in 2010,
a slight improvement can be achieved. Within one
year, the score for the DFKI PBT system DE-EN has
improved by nearly 3.5% absolute and 20% relative
BLEU score points, as shown in Table 1.
In the phrase-based translation, the tuning was not
applied, because it improves the results on the held-
out data but hurts the results on the evaluation set.
In our observation, the decrease is in the range of
0.01% to 1% in the BLEU score. However tun-
ing does help for the Tree-based system. Therefore
we applied the test2007 to optimize the parameters,
which enhanced the BLEU score from 17.52% to
21.10%. The compound splitting also improves the
syntax system, with about 1% in the BLEU score.
We did not add the new parallel corpus into the train-
ing for syntax system due to its larger computational
requirement than that of the phrase-based system.
Test10 Test08 Test11
Hybrid-2010 17.43
PBT 21.77 20.70 20.40
Syntax 22.13 20.50 20.49
HPBT 19.21 18.26 17.06
Linguatec 16.59 16.07 15.97
Lucy 16.57 16.66 16.68
Hybrid-2011 23.88 21.13 21.25
Table 3: Translation performance BLEU[%] on three test
sets using different translation systems in 2011 submis-
sion (DE-EN).
Test10 Test11
Hybrid-2010 14.42
PBT 15.46 14.05
Linguatec 14.92 12.92
Lucy 13.77 13.0
Hybrid-2011 15.55 15.83
Table 4: Translation performance BLEU[%] on two test
sets using different translation systems in 2011 submis-
sion (EN-DE).
4.3 Hybrid system
We applied test10 as the held-out data to tune
the German-English and English-German transla-
tion systems. For experiments, we applied a small-
scaled 4-gram language model trained only on the
target side of the Europarl parallel training data. As
shown in Table 2, different combinations are per-
formed on the hypotheses generated from single sys-
tems. We first combined the PBT with syntax sys-
tem, then together with the HPBT system. The
translation result in the BLEU score performs best
when we combine all three statistical machine trans-
lation systems and two rule-based systems together.
4.4 Evaluation results
For the decoding during the WMT evaluation, we
applied a larger 4-gram language model trained on
the target side of all parallel training corpus. As
shown in Table 3, in last year?s evaluation the DFKI
hybrid translation result was evaluated as 17.34% in
the BLEU score. In 2011, among all the transla-
tion systems, the syntax system performs the best
on test10 and test11, while the PBT performs the
487
SRC Diese Verordnung wurde vom Gesundheitsministerium in diesem Jahr einigermassen gemildert - die Ku?hlschrankpflicht fiel weg.
REF It was mitigated by the Ministry of Health this year - the obligation to have a refrigerator has been removed.
PBT This regulation by the Ministry of Health in this year - somewhat mitigated the fridge duty fell away.
Syntax This regulation was somewhat mitigated by the Ministry of Health this year - the refrigerator duty fell away.
HPBT This regulation was by the Ministry of Health in reasonably Dokvadze this year - the Ku?hlschrankpflicht fell away.
Linguatec This ordinance was soothed to some extent by the brazilian ministry of health this year, the refrigerator duty was discontinued.
Lucy This regulation was quite moderated by the Department of Health, Education and Welfare this year - the refrigerator duty was omitted.
Hybrid This regulation was somewhat mitigated by the Ministry of Health this year - the fridge duty fell away.
SRC Die Deregulierung und Bakalas ehemalige Bergarbeiterwohnungen sind ein brisantes Thema.
REF Deregulation and Bakala ?s former mining flats are local hot topic.
PBT The deregulation and Bakalas former miners? homes are a sensitive issue.
Syntax The deregulation and Bakalas former miners? homes are a sensitive issue.
HPBT The deregulation and Bakalas former Bergarbeiterwohnungen are a hot topic.
Linguatec Former miner flats are an explosive topic the deregulation and Bakalas.
HPBT The deregulation and Bakalas former miner apartments are an explosive topic.
Hybrid The deregulation and Bakalas former miners? apartments are a sensitive issue.
Table 5: Examples of translation output by the different systems.
best on test08. The rule-based sytems, Linguatec
and Lucy are expected to have a higher score in the
human evaluation than in the automatic evaluation.
Furthermore, as we can see from Table 3, there is
still room to improve the Jane system, with better
modeling, configurations or even higher-order lan-
guage model. Using the hybrid system we success-
fully improved the translation result to 23.88% on
test10. The hybrid system outperforms the best sin-
gle system by 0.43% and 0.76% in the BLEU score
on the test08 and test11, respectively.
For the translation from English to German, the
translation result of last year?s submission was eval-
uated as 14.42% in the BLEU score, as shown in Ta-
ble 4. In this year, the phrase-based translation result
is 15.46% in the BLEU score. We only set up one
statistical translation system due to time limitation.
With the respect of the BLEU score, phrase-based
translation outperforms rule-based translations. Be-
tween rule-based translation systems, Linguatec per-
forms better on the test10 (14.92%) and Lucy per-
forms better on the test11 (13.0%). Combining three
translation hypotheses leads to a smaller improve-
ment (from 15.46% to 15.55%) on the test10 and a
greater improvement (from 14.05% to 15.83%) on
the test11 in the BLEU score over the single best
translation system. Comparing to last year?s trans-
lation output, the improvement is over one percent
absolutely (from 14.42% to 15.55%) in the BLEU
score on the test10.
4.5 Output examples
Table 5 shows two translation examples from the
MT output of the test2011. We list the source sen-
tence in German and its reference translation as
well as the translation results generated by different
translation systems. As can be seen from Table 5,
the translation quality of source sentences is greatly
improved using the hybrid system over the single in-
dividual systems. Translations of words and word
orderings are more appropriate by the hybrid sys-
tem.
5 Conclusion and future work
We presented the DFKI hybrid translation system
submitted in the WMT workshop 2011. The hy-
brid translation is performed on the final translation
output by individual systems, including a phrase-
based system, a syntax-based system, a hierarchical
phrase-based system and two rule-based systems.
Combining the results from statistical and rule-
based systems significantly improved the translation
performance over the single-best system, which is
shown by the automatic evaluation scores and the
output examples. Despite of the encouraging results,
there is still room to improve our system, such as the
tuning in the phrase-based translation and a better
language model in the combination.
488
References
Vera Aleksic? and Gregor Thurmair. 2011. Personal
translator at wmt2011 - a rule-based mt system with
hybrid components. In Proceedings of WMT work-
shop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Andreas Eisele, Christian Federmann, Hans Uszkoreit,
Herve? Saint-Amand, Martin Kay, Michael Jellinghaus,
Sabine Hunsicker, Teresa Herrmann, and Yu Chen.
2008. Hybrid architectures for multi-engine machine
translation. In Proceedings of Translating and the
Computer 30, pages ASLIB, ASLIB/IMI, London,
United Kingdom, November.
Christian Federmann, Andreas Eisele, Hans Uszkoreit,
Yu Chen, Sabine Hunsicker, and Jia Xu. 2010. Fur-
ther experiments with shallow hybrid mt systems. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 237?
248, Uppsala, Sweden. John Benjamins.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of EMNLP,
October.
Kenneth Heafield and Alon Lavie. 2010. Voting on n-
grams for machine translation system combination. In
Proc. Ninth Conference of the Association for Machine
Translation in the Americas, Denver, Colorado, Octo-
ber.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL.
Philipp Koehn. MT Summit 2005. Europarl: A parallel
corpus for statistical machine translation.
Lucy. 2011. Home page of software lucy and services.
http://www.lucysoftware.com.
Evgeny Matusov. 2009. Combining Natural Language
Processing Systems to Improve Machine Translation
of Speech. Ph.D. thesis, Department of Electrical
and Computer Engineering, Johns Hopkins University,
Baltimore, MD.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and
P. C. Woodland. 2007. Consensus network decoding
for statistical machine translation system combination.
In IN IEEE INT. CONF. ON ACOUSTICS, SPEECH,
AND SIGNAL PROCESSING.
SMT. 2011. Sixth workshop on statistical machine trans-
lation home page. http://www.statmt.org/wmt11/.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference On Spoken Language Processing, pages
901?904, Denver, Colorado, September.
Nicola Ueffing, Jens Stephan, Evgeny Matusov, Lo ic
Dugast, George F. Foster, Roland Kuhn, Jean Senel-
lart, and Jin Yang. 2008. Tighter integration of rule-
based and statistical mt in serial system combination.
In Proceedings of COLING 2008, pages 913?920.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open Source Hierarchical Trans-
lation, Extended with Reordering and Lexicon Mod-
els. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, pages
262?270, Uppsala, Sweden, July.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING ?96: The 16th Int. Conf. on Com-
putational Linguistics, pages 836?841, Copenhagen,
Denmark, August.
489
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, page 1,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
Analytical Approaches to Combining MT Technologies 
 
Hans Uszkoreit 
 
Dept. of Computational Linguistics 
and Phonetics 
Saarland University Saarbr?cken 
& 
German Research Center for Artificial 
Intelligence (DFKI) 
DFKI Language Technology Lab 
Germany 
Hans.Uszkoreit@dfki.de 
  
 
Abstract 
The talk will report on recent and ongoing work dedicated to analytical methods for a systematic 
combination of observed strengths of translation technologies. The focus will be on different ways of 
exploiting existing data on MT output and performance measures for system combination and for 
gaining insights on strengths and weaknesses of existing technologies. 
1
