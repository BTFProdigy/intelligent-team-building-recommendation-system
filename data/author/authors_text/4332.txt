243
244
245
246
Cascaded Regular Grammars over XML Documents

Kiril Simov, Milen Kouylekov, Alexander Simov
CLaRK Programme & BulTreeBank Project
http://www.BulTreeBank.org
Linguistic Modelling Laboratory - CLPPI, Bulgarian Academy of Sciences
Acad. G.Bonchev Str. 25A, 1113 Soa, Bulgaria
Tel: (+3592) 979 28 25, (+3592) 979 38 12, Fax: (+3592) 70 72 73
kivs@bultreebank.org, mkouylekov@dir.bg, adis 78@dir.bg
Abstract
The basic mechanism of CLaRK for linguistic pro-
cessing of text corpora is the cascade regular gram-
mar processor. The main challenge to the grammars
in question is how to apply them on XML encoding
of the linguistic information. The system oers a so-
lution using an XPath language for constructing the
input word to the grammar and an XML encoding
of the categories of the recognized words.
1 Introduction
This paper describes a mechanism for denition and
application of cascaded regular grammars over XML
documents. The main problems are how to dene
the input words for a grammar and how to incorpo-
rate back in the document the grammar categories
returned by the rules of the grammar. The presented
solutions are implemented within the CLaRK Sys-
tem { an XML-based System for Corpora Develop-
ment (Simov et. al., 2001).
The main goal behind the design of the system
is the minimization of human intervention during
the creation of corpora. Creation of corpora is still
an important task for the majority of languages like
Bulgarian, where the invested eort in such devel-
opment is very modest in comparison with more
intensively studied languages like English, German
and French. We consider the corpora creation task
as editing, manipulation, searching and transform-
ing of documents. Some of these tasks will be done
for a single document or a set of documents, others
will be done on a part of a document. Besides e?-
ciency of the corresponding processing in each state
of the work, the most important investment is the
human labor. Thus, in our view, the design of the
system has to be directed to minimization of the hu-
man work. For document management, storing and

This work is funded by the Volkswagen Stiftung, Federal
Republic of Germany under the Programme \Cooperation
with Natural and Engineering Scientists in Central and East-
ern Europe" contract I/76 887. The authors are grateful to
Tylman Ule from SfS, University of Tubingen, Germany and
the three anonymous reviewers for their comments on the ear-
lier drafts of the paper. Needless to say, all remaining errors
in the text are ours.
querying we have chosen the XML technology be-
cause of its popularity and its ease for understand-
ing. The XML technology becomes a part of our
lives and a predominant language for data descrip-
tion and exchange on the Internet. Moreover, a lot
of already developed standards for corpus descrip-
tions like (XCES, 2001) and (TEI, 2001) are already
adapted to the XML requirements. The core of the
CLaRK System is an XML Editor which is the main
interface to the system. With the help of the editor
the user can create, edit or browse XML documents.
To facilitate the corpus management, we enlarge the
XML inventory with facilities that support linguistic
work. We added the following basic language pro-
cessing modules: a tokenizer with a module that sup-
ports a hierarchy of token types, a nite-state engine
that supports the writing of cascaded regular gram-
mars and facilities for regular pattern search, the
XPath query language which is able to support nav-
igation over the whole set of mark-up of a document,
mechanisms for imposing constraints over XML doc-
uments which are applicable in the context of some
events. We envisage several usages of our system:
1. Corpora markup. Here users work with the
XML tools of the system in order to mark-
up texts with respect to an XML DTD. This
task usually requires an enormous human ef-
fort and comprises both the mark-up itself and
its validation afterwards. Using the available
grammar resources such as morphological ana-
lyzers or partial parsing, the system can state
local constraints reecting the characteristics of
a particular kind of texts or mark-up. One ex-
ample of such constraints can be as follows: a
PP according to a DTD can have as parent an
NP or VP, but if the left sister is a VP then the
only possible parent is VP. The system can use
such kind of constraints in order to support the
user and minimize his/her work.
2. Dictionary compilation for human users. The
system supports the creation of the actual lexi-
cal entries whose structure is dened via a DTD.
The XML tools will be used also for corpus in-
vestigation that provides appropriate examples
of the word usage in the available corpora. The
constraints incorporated in the system are used
for writing a grammar of the sub-languages of
the denitions of the lexical items, for imposing
constraints over elements of lexical entries and
the dictionary as a whole.
3. Corpora investigation. The CLaRK System of-
fers a set of tools for searching over tokens and
mark-up in XML corpora, including cascaded
grammars, XPath language. The combinations
between these tools are used for tasks such as:
extraction of elements from a corpus - for exam-
ple, extraction of all NPs in the corpus; concor-
dance - for example, give me all NPs in contexts,
ordered by a user's dened set of criteria.
The structure of the paper is as follows: in the
next section we give a short introduction to the
main technologies on which the CLaRK System is
built. These are: XML technology; Cascaded regu-
lar grammars; Unicode-based tokenizers; and Con-
straints over XML documents. The third section
describes the denition of cascaded regular gram-
mars. The fourth section presents an approach for
applying cascaded regular grammars over XML doc-
uments. The last section concludes the paper.
2 The technologies behind the
CLaRK System
CLaRK is an XML-based software system for cor-
pora development implemented in JAVA. It incor-
porates several technologies:
 XML technology;
 Unicode;
 Regular Grammars (they are presented in the
next sections);
 Constraints over XML Documents.
2.1 XML Technology
The XML technology is at the heart of the CLaRK
system. It is implemented as a set of utilities for
structuring, manipulation and management of data.
We have chosen the XML technology because of its
popularity, its ease of understanding and its already
wide use in description of linguistic information. Be-
sides the XML language (see (XML, 2000)) proces-
sor itself, we have implemented an XPath language
(see (XPath, 1999)) engine for navigation in docu-
ments and an XSLT language (see (XSLT, 1999))
engine for transformation of XML documents. The
documents in the system are represented as DOM
Level1 trees (see (DOM, 1998)). We started with ba-
sic facilities for creation, editing, storing and query-
ing of XML documents and developed further this
inventory towards a powerful system for processing
not only single XML documents but an integrated
set of documents and constraints over them. The
main goal of this development is to allow the user to
add the desirable semantics to the XML documents.
In the implementation of cascaded regular gram-
mars within the CLaRK System, a crucial role plays
the XPath language. XPath is a powerful language
for selecting elements from an XML document. The
XPath engine considers each XML document as a
tree where the nodes of the tree represent the ele-
ments of the document, the document's most outer
tag is the root of the tree and the children of a
node represent the content of the corresponding el-
ement. The content nodes can be element nodes or
text nodes. Attributes and their values of each ele-
ment are represented additionally to the tree.
The XPath language uses the tree-based termi-
nology to point to some direction within the tree.
One of the basic notions of the XPath language is
the so called context node, i.e. a chosen node in the
tree. Each expression in the XPath language is eval-
uated with respect to some context node. The nodes
in the tree are categorized with respect to the con-
text node as follows: the nodes immediately under
the context node are called children of the context
node; all nodes under the context node are called
descendant nodes of the context node; the node im-
mediately above the context node is called parent of
the context node; all nodes that are above the con-
text node are called ancestor nodes of the context
node; the nodes that have the same parent as the
context node are called sibling nodes of the context
node; siblings of the context node are divided into
two types: preceding siblings and following siblings,
depending on their order with respect to the context
node in the content of their parent - if the sibling is
before the context node, then it is a preceding sib-
ling, otherwise it is a following sibling. Attribute
nodes are added with the context node as attribute
nodes and they are not children, descendant, parent
or ancestor nodes of the context node. The context
node with respect to itself is called self node.
2.2 Tokenization
XML considers the content of each text element a
whole string that is usually unacceptable for cor-
pus processing. For this reason it is required for the
wordforms, punctuation and other tokens in the text
to be distinguished. In order to solve this problem,
the CLaRK system supports a user-dened hierar-
chy of tokenizers. At the very basic level the user can
dene a tokenizer in terms of a set of token types.
In this basic tokenizer each token type is dened by
a set of UNICODE symbols. Above this basic level
tokenizers the user can dene other tokenizers for
which the token types are dened as regular expres-
sions over the tokens of some other tokenizer, the so
called parent tokenizer. For each tokenizer an alpha-
betical order over the token types is dened. This
order is used for operations like the comparison be-
tween two tokens, sorting and similar.
2.3 Constraints on XML documents
Several mechanisms for imposing constraints over
XML documents are available. The constraints can-
not be stated by the standard XML technology
(even by the means of XML Schema (XML Schema,
2000)). The following types of constraints are imple-
mented in CLaRK: 1) nite-state constraints - addi-
tional constraints over the content of given elements
based on a document context; 2) number restriction
constraints - cardinality constraints over the content
of a document; 3) value constraints - restriction of
the possible content or parent of an element in a
document based on a context. The constraints are
used in two modes: checking the validity of a docu-
ment regarding a set of constraints; supporting the
linguist in his/her work during the process of cor-
pus building. The rst mode allows the creation of
constraints for the validation of a corpus according
to given requirements. The second mode helps the
underlying strategy of minimization of the human
labor.
The general syntax of the constraints in the
CLaRK system is the following:
(Selector, Condition, Event, Action)
where the selector denes to which node(s) in the
document the constraint is applicable; the condition
denes the state of the document at the time when
the constraint is applied. The condition is stated as
an XPath expression which is evaluated with respect
to each node selected by the selector. If the evalu-
ation of the condition is a non-empty list of nodes
then the constraints are applied; the event denes
some conditions of the system when this constraint
is checked for application. Such events can be: the
selection of a menu item, the pressing of key short-
cut, some editing command as enter a child or a
parent and similar; the action denes the way of the
actual application of the constraint.
Here we present constraints of type "Some Chil-
dren". This kind of constraints deal with the content
of some elements. They determine the existence of
certain values within the content of these elements.
A value can be a token or an XML mark-up and the
actual value for an element can be determined by
the context. Thus a constraint of this kind works in
the following way: rst it determines to which ele-
ments in the document it is applicable, then for each
such element in turn it determines which values are
allowed and checks whether in the content of the el-
ement some of these values are presented as a token
or an XML mark-up. If there is such a value, then
the constraint chooses the next element. If there is
no such a value, then the constraint oers to the
user a possibility to choose one of the allowed values
for this element and the selected value is added to
the content as a rst child. Additionally, there is a
mechanism for ltering of the appropriate values on
the basis of the context of the element.
3 Cascaded Regular Grammars
The CLaRK System is equipped with a nite-state
engine which is used for several tasks in the system
such as validity check for XML documents, tokeniz-
ers, search and cascaded regular grammar. In this
and the next section we present the use of this en-
gine for cascaded regular grammars over XML doc-
uments along the lines described in (Abney, 1996).
The general idea underlying cascaded regular gram-
mars is that there is a set of regular grammars. The
grammars in the set are in particular order. The in-
put of a given grammar in the set is either the input
string if the grammar is rst in the order or the out-
put string of the previous grammar. Another specic
feature of the cascaded grammars is that each gram-
mar tries to recognize only a particular category in
the string but not the whole string. The parts of
the input word that are not recognized by the gram-
mar are copied to the output word. Before going
into detail of how to apply grammars in the CLaRK
System some basic notions about regular expressions
are given.
Regular grammars standardly are formalized as a
set of rewriting rules of the following kinds
A -> b C
A -> B
A -> b
where A, B, C stand for non-terminal symbols and
b stands for terminal symbols. Such grammars are
applied over a word of terminal symbols in order to
parse it to a special goal symbol S. Each language
accepted by such a grammar is called regular. Using
such a formalization one could situated the regular
languages within the families of other languages like
context free languages, context sensitive languages
and so on. In practice this formalization is rarely
used. Other formal devices for dealing with regular
languages are regular expressions and nite-state au-
tomata with the well know correspondence between
them. Although regular grammars are not expres-
sive enough in order to be a good model of nat-
ural languages they are widely used in NLP. They
are used in modelling of inectional morphology (see
(Koskenniemi, 1983)), tokenization and Named En-
tity recognition (Grover et. al., 2000), and many oth-
ers.
In our work we modify the denition of regular
grammars along the lines of (Abney, 1996). We use
rewriting rules of the following kind:
C -> R
where R is a regular expression and C is a category
of the words recognized by R. We can think of C as
a name of the language recognized by R.
A regular grammar is a set of rules such that the
regular expressions of the rules recognize pairwise
disjoint languages. The disjointness condition is nec-
essary in order the grammar to assign a unique cat-
egory to each word recognized by it.
A regular grammar works over a word of letters,
called input word. The grammar scans the input
word from left to right trying to nd the rst sub-
word such that it belongs to the language of the
regular expression presented by some of the rules of
the grammar. If there is no such word starting with
the rst letter in the input word, then the grammar
outputs the rst letter of the word and prolongs the
scanning with the second letter. When the gram-
mar recognizes a sub-word then it outputs the cate-
gory of the corresponding rule and prolongs the scan-
ning with the letter after the recognized sub-word.
The grammar works deterministically over the input
word. The result of the application of the grammar
is a copy of the input word in which the recognized
sub-words are substituted with the categories of the
grammar. The result word is called output word of
the grammar. In this respect such kind of regular
grammars could be considered a kind of nite-state
transducers.
An additional requirement suggested by (Abney,
1996) is the so-called longest match, which is a way
to choose one of the possible analyses for a grammar.
The longest match strategy requires that the recog-
nized sub-words from left to right have the longest
length possible. Thus the segmentation of the in-
put word starts from the left and tries to nd the
rst longest sub-words that can be recognized by
the grammar and so on to the end of the word.
An example of such a regular grammar is the
grammar  
1
for recognition of dates in the format
dd.mm.yyyy (10.11.2002) dened by the following
rule:
Date ->
( (0,(1|2|3|4|5|6|7|8|9)) |
((1|2),(0|1|2|3|4|5|6|7|8|9)) |
(3,(0|1))
)
,
.
,
((0,(1|2|3|4|5|6|7|8|9))|(1,(0|1|2)))
,
.
,
(((1|2|3|4|5|6|7|8|9),
(0|1|2|3|4|5|6|7|8|9)*))
Application of this grammar on the following input
word
The feast is from 12.03.2002 to 15.03.2002.
will produce the output word
The feast is from Date to Date.
A cascaded regular grammar (Abney, 1996) is a
sequence of regular grammars dened in such a way
that the rst grammar works over the input word
and produces an output word, the second grammar
works over the output word of the rst grammar,
produces a new output word and so on.
As one example of a cascaded regular grammar let
us consider the sequence of  
1
as dened above and
the grammar  
2
dened by the following rule:
Period -> from, Date, to, Date
Application of the grammar  
2
on output of the
grammar  
1
:
The feast is from Date to Date.
will produce the following output word
The feast is Period.
In the next section we describe how cascaded reg-
ular grammars can be applied to XML documents.
4 Cascaded Regular Grammars over
XML Documents
The application of the regular grammars to XML
documents is connected with the following problems:
 how to treat the XML document as an input
word for a regular grammar;
 how should the returned grammar category be
incorporated into the XML document; and
 what kind of `letters' to be used in the regu-
lar expressions so that they correspond to the
`letters' in the XML document.
The solutions to these problems are described in
the next paragraphs.
First of all, we accept that each grammar works
on the content of an element in an XML document.
The content of each XML element
1
is either a se-
quence of XML elements, or text, or both (MIXED
content). Thus, our rst task is to dene how to
turn the content of an XML element into an input
word of a grammar. We consider the two basic cases
- when the content is text and when it is a sequence
of elements.
When the content of the element to which the
grammar will be applied is a text we have two
choices:
1
Excluding the EMPTY elements on which regular gram-
mar cannot be applied.
1. we can accept that the 'letters' of the grammars
are the codes of the symbols in the encoding of
the text; or
2. we can segment the text in meaningful non-
overlapping chunks (in usual terminology to-
kens) and treat them as 'letters' of the gram-
mars.
We have adopted here the second approach. Each
text content of an element is rst tokenized by a to-
kenizer
2
and is then used as an input for grammars.
Additionally, each token receives a unique type. For
instance, the content of the following element
<s>
John loves Mary who is in love with Peter
</s>
can be segmented as follows:
"John" CAPITALFIRSTWORD
" " SPACE
"loves" WORD
" " SPACE
"Mary" CAPITALFIRSTWORD
" " SPACE
"who" WORD
" " SPACE
"is" WORD
" " SPACE
"in" WORD
" " SPACE
"love" WORD
" " SPACE
"with" WORD
" " SPACE
"Peter" CAPITALFIRSTWORD
Here on each line in double quotes one token from
the text followed by its token type is presented.
Therefore when a text is considered an input word
for a grammar, it is represented as a sequence of to-
kens. How can we refer now to the tokens in the
regular expressions in the grammars? The most
simple way is by tokens. We decided to go further
and to enlarge the means for describing tokens with
the so called token descriptions which correspond to
the letter descriptions in the above section on reg-
ular expressions. In the token descriptions we use
strings (sequences of characters), wildcard symbols
# for zero or more symbols, @ for zero or one sym-
bol, and token categories. Each token description
matches exactly one token in the input word.
We divide the token descriptions into two types
- those that are interpreted directly as tokens and
2
There are built-in tokenizers which are always available
and there is also a mechanism for dening new tokenizers by
the user.
others that are interpreted as token types rst and
then as tokens belonging to these token types.
The rst kind of token descriptions is represented
as a string enclosed in double quotes. The string is
interpreted as one token with respect to the current
tokenizer. If the string does not contain a wildcard
symbol then it represents exactly one token. If the
string contains the wildcard symbol # then it denotes
an innite set of tokens depending on the symbols
that are replaced by #. This is not a problem in
the system because the token description is always
matched by a token in the input word. The other
wildcard symbol is treated in a similar way, but zero
or one symbol is put in its place. One token descrip-
tion may contain more than one wildcard symbol.
Examples:
"Peter" as a token description could be matched
only by the last token in the above example.
"lov#" could be matched by the tokens "loves"
and "love"
"lov@" is matched only by "love"
"@" is matched by the token corresponding to the
intervals " "
"#h#" is matched by "John", "who", and "with"
"#" is matched by any of the tokens including the
spaces.
The second kind of token description is repre-
sented by the dollar sign $ followed by a string. The
string is interpreted as either a token type or a set
of token types if it contains wildcard symbols. Then
the type of the token in the input word is matched
by the token types denoted by the string. If the to-
ken type of the token in the text is denoted by the
token description, then the token is matched to the
token description.
Examples:
$WORD is matched to "loves", "who", "is", "in",
"love", "with"
$CAP# is matched to "John", "Mary" and "Peter"
$#WORD is matched to "John", "loves", "Mary",
"who", "is", "in", "love", "with", and "Peter"
$# is matched to any of the tokens including the
spaces.
Now we turn to the case when the content of a
given element is a sequence of elements. For instance
the above sentence can be represented as:
<s>
<N>John</N><V>loves</V><N>Mary</N>
<Pron>who</Pron><V>is</V><P>in</P>
<N>love</N><P>with</P><N>Peter</N>
</s>
At rst sight the natural choice for the input word
is the sequences of the tags of the elements: <N>
<V> <N> <Pron> <V> <P> <N> <P> <N>, but when
the encoding of the grammatical features is more
sophisticated (see below):
<s>
<w g="N">John</w>
<w g="V">loves</w>
<w g="N">Mary</w>
<w g="Pron">who</w>
<w g="V">is</w>
<w g="P">in</w>
<w g="N">love</w>
<w g="P">with</w>
<w g="N">Peter</w>
</s>
then the sequence of tags is simply <w> <w> ...,
which is not acceptable as an input word. In order to
solve this problem we substitute each element with a
sequence of values. This sequence is determined by
an XPath expression that is evaluated taking the ele-
ment node as context node. The sequence dened by
an XPath expression is called element value. Thus
each element in the content of the element is replaced
by a sequence of text segments of appropriate types
as it is explained below.
For the above example a possible element value
for tag w could be dened by the XPath expression:
\attribute::g". This XPath expression returns the
value of the attribute g for each element with tag
w. Therefore a grammar working on the content of
the above sentence will receive as an input word the
sequence: "<" "N" ">" "<" "V" ">" "<" "N" ">" "<"
"Pron" ">" "<" "V" ">" "<" "P" ">" "<" "N" ">"
"<" "P" ">" "<" "N" ">". The angle brackets "<"
">" determine the boundaries of the element value
for each of the elements.
Besides such text values, by using of XPath ex-
pressions one can point to arbitrary nodes in the
document, so that the element value is determined
dierently. In fact, an XPath expression can be eval-
uated as a list of nodes and then the element value
will be a sequence of values.
For example, if the element values for the above
elements are dened by the following XPath ex-
pression: \text() j attribute::g"
3
, then the input
word will be the sequence: "<" "John" "N" ">" "<"
"loves" "V" ">" "<" "Mary" "N" ">" "<" "who"
"Pron" ">" "<" "is" "V" ">" "<" "in" "P" ">" "<"
"love" "N" ">" "<" "with" "P" ">" "<" "Peter"
"N" ">".
Using predicates in the XPath expressions one
can determine the element values on the basis
of the context. For example, if in the above
case we want to use the grammatical features
for verbs, nouns and pronouns, but the actual
words for prepositions, we can modify the XPath
expression for the element value in the follow-
ing way: \text()[../attribute::g="P"] j at-
tribute::g[not(../attribute::g="P")]". In this
3
The meaning of this XPath is point to the text child of
the element and the value of the attribute g.
case the XPath expression will select the textual con-
tent of the element if the value of the attribute g for
this element has value "P". If the value of the at-
tribute g for the element is not "P", then the XPath
expression will select the value of the attribute g.
The input word then is: "<" "N" ">" "<" "V" ">"
"<" "N" ">" "<" "Pron" ">" "<" "V" ">" "<" "in"
">" "<" "N" ">" "<" "with" ">" "<" "N" ">".
The element value is a representation of the im-
portant information for an element. One can con-
sider it conceptual information about the element.
From another point of view it can be seen as a tex-
tual representation of the element tree structure.
At the moment in the CLaRK system the nodes
selected by an XPath expression are processed in the
following way:
1. if the returned node is an element node, then
the tag of the node is returned with the addi-
tional information conrming that this is a tag;
2. if the returned node is an attribute node, then
the value of the attribute is:
(a) tokenized by an appropriate tokenizer if the
attribute value is declared as CDATA in
the DTD;
(b) text if the value of the attribute is declared
as an enumerated list or ID.;
3. if the returned node is a text node, then the
text is tokenized by an appropriate tokenizer.
Within the regular expressions we use the angle
brackets in order to denote the boundaries of the
element values. Inside the angle brackets we could
write a regular expression of arbitrary complexity in
round brackets. As letters in these regular expres-
sions we use again token descriptions for the values
of textual elements and the values of attributes. For
tag descriptions we use strings which are neither en-
closed in double quotes nor preceded by a dollar sign.
We can use wildcard symbols in the tag name. Thus
<p> is matched with a tag p;
<@> is matched with all tags with length one.
<#> is matched with all tags.
The last problem when applying grammars to
XML documents is how to incorporate the category
assigned to a given rule. In general we can accept
that the category has to be encoded as XML mark-
up in the document and that this mark-up could be
very dierent depending on the DTD we use. For in-
stance, let us have a simple tagger (example is based
on (Abney, 1996)):
Det -> "the"|"a"
N -> "telescope"|"garden"|"boy"
Adj -> "slow"|"quick"|"lazy"
V -> "walks"|"see"|"sees"|"saw"
Prep -> "above"|"with"|"in"
Then one possibility for representing the cate-
gories as XML mark-up is by tags around the recog-
nized words:
the boy with the telescope
becomes
<Det>the</Det><N>boy</N>
<Prep>with</Prep>
<Det>the</Det><N>telescope</N>
This encoding is straightforward but not very con-
venient when the given wordform is homonymous
like:
V -> "move"
N -> "move"
In order to avoid such cases we decided that the
category for each rule in the CLaRK System is a cus-
tom mark-up that substitutes the recognized word.
Since in most cases we would also like to save the
recognized word, we use the variable \w for the rec-
ognized word. For instance, the above example will
be:
<Det>\w</Det> -> "the"|"a"
<N>\w</N> -> "telescope"|"garden"|"boy"
<Adj>\w</Adj> -> "slow"|"quick"|"lazy"
<V>\w</V> -> "walks"|"see"|"sees"|"saw"
<Prep>\w</Prep> -> "above"|"with"|"in"
The mark-up dening the category can be as com-
plicated as necessary. The variable \w can be re-
peated as many times as necessary (it can also be
omitted). For instance, for "move" the rule could
be:
<w aa="V;N">\w</w> -> "move"
Let us give now one examples in which element
values are used. For instance, the following grammar
recognizes prepositional phrases:
<PP>\w</PP> -> <"P"><"N#">
Generally it says that a prepositional phrase consists
of an element with element value "P" followed by an
element with element value which matches the to-
ken description "N#". Usage of the token description
"N#" ensures that the rule will work also for the case
when a preposition is followed by an element with el-
ement value "NP". The application of this grammar
on the above example with appropriate denition of
element values will result in the following document:
<s>
<w g="N">John</w>
<w g="V">loves</w>
<w g="N">Mary</w>
<w g="Pron">who</w>
<w g="V">is</w>
<PP>
<w g="P">in</w>
<w g="N">love</w>
</PP>
<PP>
<w g="P">with</w>
<w g="N">Peter</w>
</PP>
</s>
Here is another example (based on a grammar
developed by Petya Osenova for Bulgarian noun
phrases) which demonstrates a more complicated
regular expressions inside element descriptions:
<np aa="NPsn">\w</np> ->
<("An#"|"Pd@@@sn")>,<("Pneo-sn"|"Pfeo-sn")>
Here "An#" matches all morphosyntactic tags for
adjectives of neuter gender, "Pd@@@sn" matches all
morphosyntactic tags for demonstrative pronouns of
neuter gender, singular , "Pneo-sn" is a morphosyn-
tactic tag for the negative pronoun, neuter gender,
singular, and "Pfeo-sn" is a morphosyntactic tag
for the indenite pronoun, neuter gender, singular.
This rule recognizes as a noun phrase each sequence
of two elements where the rst element has an ele-
ment value corresponding to an adjective or demon-
strative pronoun with appropriate grammatical fea-
tures, followed by an element with element value cor-
responding to a negative or an indenite pronoun.
Notice the attribute aa of the category of the rule.
It represents the information that the resulting noun
phrase is singular, neuter gender. Let us now sup-
pose that the next grammar is for determination of
prepositional phrases is dened as follows:
<pp>\w</pp> -> <"R"><"N#">
where "R" is the morphosyntactic tag for preposi-
tions. Let us trace the application of the two gram-
mars one after another on the following XML ele-
ment:
<text>
<w aa="R">s</w>
<w aa="Ansd">golyamoto</w>
<w aa="Pneo-sn">nisto</w>
</text>
First, we dened the element value for the ele-
ments with tag w by the XPath expression: \at-
tribute::aa". Then the cascaded regular gram-
mar processor calculated the input word for the
rst grammar: "<" "R" ">" "<" "Ansd" ">" "<"
"Pneo-sn" ">". Then the rst grammar is applied
on this input words and it recognizes the last two
elements as a noun phrase. This results in two ac-
tions: rst, the markup of the rule is incorporated
into the original XML document:
<text>
<w aa="R">s</w>
<np aa="NPsn">
<w aa="Ansd">golyamoto</w>
<w aa="Pneo-sn">nisto</w>
</np>
</text>
Second, the element value for the new element <np>
is calculated and it is substituted in the input word
of the rst grammar and in this way the input word
for the second grammar is constructed: "<" "R" ">"
"<" "NPsn" ">". Then the second grammar is ap-
plied on this word and the result is incorporated in
the XML document:
<text>
<pp>
<w aa="R">s</w>
<np aa="NPsn">
<w aa="Ansd">golyamoto</w>
<w aa="Pneo-sn">nisto</w>
</np>
</pp>
</text>
Because the cascaded grammar only consists of this
two grammars the input word for the second gram-
mar is not modied, but simply deleted.
5 Conclusion
In this paper we presented an approach to appli-
cation of cascaded regular grammars within XML.
This mechanism is implemented within the CLaRK
System (Simov et. al., 2001). The general idea is
that an XML document could be considered as a
\blackboard" on which dierent grammars work.
Each grammar is applied on the content of some of
the elements in the XML document. The content of
each of these elements is converted into input words.
If the content is text then it is converted into a se-
quence of tokens. If the content is a sequence of
elements, then each element is substituted by an el-
ement value. Each element value is dened by an
XPath expression. The XPath engine selects the ap-
propriate pieces of information which determine the
element value. The element value can be considered
as the category of the element, or as textual repre-
sentation of the tree structure of the element and its
context. The result of the grammar is incorporated
in the XML document as XML markup.
In fact in the CLaRK System are implemented
more tools for modication of an XML document,
such as: constraints, sort, remove, transformations.
These tools can write some information, reorder it
or delete it. The user can order the applications of
the dierent tools in order to achieve the necessary
processing. We call this possibility cascaded pro-
cessing after the cascaded regular grammars. In
this case we can order not just dierent grammars
but also other types of tools.
References
Steve Abney. 1996. Partial Parsing via Finite-State
Cascades. In: Proceedings of the ESSLLI'96 Ro-
bust Parsing Workshop. Prague, Czech Republic.
DOM. 1998. Document Object Model (DOM) Level
1. Specication Version 1.0. W3C Recommenda-
tion. http://www.w3.org/TR/1998/REC-DOM--
Level-1-19981001
Claire Grover, Colin Matheson, Andrei Mikheev and
Marc Moens. 2000. LT TTT - A Flexible To-
kenisation Tool. In: Proceedings of Second Inter-
national Conference on Language Resources and
Evaluation (LREC 2000).
K. Koskenniemi. 1983. Two-level Model for Morpho-
logical Analysis. In: Proceedings of IJCAI-83 ,
pages: 683-685, Karlsruhe, Germany.
Kiril Simov, Zdravko Peev, Milen Kouylekov, Ale-
xander Simov, Marin Dimitrov, Atanas Kiryakov.
2001. CLaRK - an XML-based System for Corpora
Development. In: Proc. of the Corpus Linguistics
2001 Conference, pages: 558-560. Lancaster, UK.
Text Encoding Initiative. 1997. Guidelines for Elec-
tronic Text Encoding and Interchange. Sperberg-
McQueen C.M., Burnard L (eds).
Corpus Encoding Standard. 2001. XCES: Corpus
Encoding Standard for XML. Vassar College, New
York, USA.
http://www.cs.vassar.edu/XCES/
XML. 2000. Extensible Markup Language (XML) 1.0
(Second Edition). W3C Recommendation.
http://www.w3.org/TR/REC-xml
XML Schema. 2001. XML Schema Part 1: Struc-
tures. W3C Recommendation.
http://www.w3.org/TR/xmlschema-1/
XPath. 1999. XML Path Lamguage (XPath) version
1.0. W3C Recommendation.
http://www.w3.org/TR/xpath
XSLT. 1999. XSL Transformations (XSLT) version
1.0. W3C Recommendation.
http://www.w3.org/TR/xslt
Coling 2008: Companion volume ? Posters and Demonstrations, pages 173?176
Manchester, August 2008
Entailment-based Question Answering  
for Structured Data 
Bogdan Sacaleanu?, Constantin Orasan?, Christian Spurk?, Shiyan 
Ou?, Oscar Ferrandez?, Milen Kouylekov? and Matteo Negri?  
?LT-Lab, DFKI GmbH / Saarbr?cken, Germany 
?RIILP, University of Wolverhampton / Wolverhampton, UK 
?Fondazione Bruno Kessler (FBK) / Trento, Italy 
?University of Alicante / Alicante, Spain 
 
 
Abstract  
This paper describes a Question Answer-
ing system which retrieves answers from 
structured data regarding cinemas and 
movies. The system represents the first 
prototype of a multilingual and multi-
modal QA system for the domain of tour-
ism. Based on specially designed domain 
ontology and using Textual Entailment as 
a means for semantic inference, the sys-
tem can be used in both monolingual and 
cross-language settings with slight ad-
justments for new input languages. 
1 Introduction 
Question Answering over structured data has 
been traditionally addressed through a deep 
analysis of the question in order to reconstruct a 
logical form, which is then translated in the query 
language of the target data (Androutsopoulos et 
al, 1995, Popescu et al 2003). This approach im-
plies a complex mapping between linguistic ob-
jects (e.g. lexical items, syntactic structures) and 
against data objects (e.g. concepts and relations 
in a knowledge base). Unfortunately, such a 
mapping requires extensive manual work, which 
in many cases represents a bottleneck preventing 
the realization of large scale and portable natural 
language interfaces to structured data.  
This paper presents the first prototype of a 
question answering system which can answer 
questions in several languages about movies and 
cinema using a multilingual ontology and textual 
                                                 
 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
entailment. The remainder of the paper is struc-
tured as follows: Section 2 presents the concept 
of entailment-based question answering; Section 
3 describes our prototype which implements this 
concept; A brief evaluation is presented in Sec-
tion 4, followed by conclusions in Section 5. 
2 Entailment-based QA 
Recently Textual Entailment (TE) has been pro-
posed as a unifying framework for applied se-
mantics (Dagan and Glickman, 2004), where the 
need for an explicit representation of a mapping 
between linguistic objects and data objects can 
be, at least partially, bypassed through the defini-
tion of semantic inferences at a textual level. In 
this framework, a text (T) is said to entail a hy-
pothesis (H) if the meaning of H can be derived 
from the meaning of T. 
On the basis of the TE framework, the QA 
problem can be recast as an entailment problem, 
where the text (T) is the question (or its affirma-
tive version) and the hypothesis (H) is a rela-
tional answer pattern, which is associated to in-
structions for retrieving the answer to the input 
question. In this framework, given a question Q 
and a set of relational answer patterns P, a QA 
system needs to select those patterns in P that are 
entailed by Q. Instructions associated to answer 
patterns may be viewed as high precision proce-
dures for answer extraction, which are dependent 
on the specific source which is asked for. In case 
of QA over structured data, instructions could be 
queries to a database; whilst in case of QA on the 
Web, an instruction could be the URL of a Web 
page containing the answer to a question or some 
form of IR query to a search engine. 
Therefore, the underlying idea of an entail-
ment-based QA system is to match the user?s re-
quest to a set of predefined question patterns in 
order to get some kind of analysis for the request. 
173
As an example consider the question ?Where 
can I watch the movie ?Dreamgirls? next Satur-
day?? and the predefined question patterns: 
? Which movies are currently running in 
[CINEMA]?  EAT = [MOVIE] 
? Where can I watch the movie [MOVIE] 
on [WEEKDAY]?  EAT = [CINEMA] 
? Where can I see [MOVIE]? 
 EAT = [CINEMA] 
In the example, each of the patterns contains 
placeholders for relevant named entities and has 
an expected answer type (EAT) associated with 
it. The entailment-based QA system should re-
turn that pattern (2) is entailed by the question 
and as a result the retrieval instructions associ-
ated to it will be used to answer the question. 
3 Description of system 
Our question answering system implements the 
concept of entailment-based question answering 
described in the previous section. The overall 
structure of our system is presented in Figure 1.  
Given a question asked by a user of the system 
in a known location, the QA planner forwards it 
to the Instance Annotator in order to find any 
concepts that might be related to the targeted 
domain (i.e. cinema, city, movie). The result is 
then analyzed by the Relation Matcher, which on 
the basis of entailment can either select the most 
appropriate interpretation of the question and im-
plicitly its associated procedure of answering the 
question, or decide that the user request is out-of-
coverage if no such interpretation is available. 
The cross-linguality of our system and, to a 
certain extent, the interaction between its compo-
nents is ensured by a domain ontology which is 
used for all four languages involved in the pro-
ject: English, German, Italian and Spanish, and 
its modules (Ou et al, 2008). Concepts from the 
ontology are used to annotate the user questions 
as well as data from which the answer is ex-
tracted. In the current stage of the project, the 
answers are contained in databases obtained from 
content provides or built from structured web 
pages. As a result, the information in the database 
tables was annotated with concepts from the on-
tology and then converted into an RDF graph to 
Figure 1. System Architecture 
174
facilitate retrieval using SPARQL query lan-
guage (Prud'hommeaux and Seaborne, 2006). 
Question patterns corresponding to one or several 
ontological relations were produced after ques-
tions for users were collected and used in the en-
tailment module. The question patterns used by 
the system are very similar to those presented in 
the previous section and contain placeholders for 
the actual entities that are expected to appear in a 
question. 
The SPARQL query associated with a pattern 
selected for a user question is used to retrieve the 
answers from the knowledge base and prepare for 
presentation. Given that our system is not limited 
to returning only textual information, further 
processing can be applied to the retrieved data. 
For example, for proximity questions the list of 
answers consists of cinema names and their GPS-
coordinates, which are used by the Answer Sort-
ing component to reorder the list of answers on 
the basis of their distance to the user?s location. 
Besides presenting the possible answers to a 
given question, the system can offer additional 
information based on the answer?s type: 
? a map for answers that are location 
names, 
? a route description for answers that are 
cinema names, 
? a video-trailer for answers that are movie 
names and 
? an image for answers that are person 
names. 
Due to the fact that a common semantics is 
shared by all four languages by way of a domain 
ontology, the system can be used not only in a 
monolingual setting, but also in a cross-language 
setting. This corresponds to a user-scenario 
where a tourist asks for information in their own 
language in a foreign location (i.e. English 
speaker in Italy). The only difference between 
monolingual and cross-language scenarios is that 
in the cross-language setting, the QA Core sub-
system (Figure 1) selects a Find Entailed Rela-
tion component according to the user input?s lan-
guage. This is due to the entailment algorithms 
that tend to use language specific resources in 
order to attain high accuracy results of matching 
the user request with one of the lexicalized rela-
tions (patterns). It is only the entailment compo-
nent that has to be provided in order to adapt the 
system to new input languages, once the lexical-
ized relations have been translated either manual 
or automatically. 
Both the Instance Annotator and the Answer 
Retriever are language independent, but location 
dependent (Figure 2). The Answer Retriever de-
pends on the location since it is querying data 
found at that place (i.e. Italy), while the Instance 
Annotator looks up instances of the data in the 
user?s question (i.e. annotates an English ques-
tion). They are language independent since they 
are working with data abstractions like SPARQL 
queries (Answer Retriever) or work at character 
level and do not consider language specific as-
pects, like words, in their look-up process (In-
stance Annotator). 
The current version of the system1 is designed 
according to the SOA (Service Oriented Archi-
tecture) and is implemented as point-to-point in-
tegrated web services. Any of the system?s com-
ponents can be substituted by alternative imple-
mentations with no need for further changes as 
long as the functionality remains the same. 
                                                 
1
 http://attila.dfki.uni-sb.de:8282/ QallMe_Proto-
type_WEB_Update/faces/Page6.jsp 
Figure 2. Cross-language Setting 
175
4 Evaluation 
A preliminary evaluation of the first prototype 
was carried out on randomly selected questions 
from a benchmark specifically designed for the 
project. This benchmark was developed to con-
tain questions about various aspects from the 
domain of tourism and for this reason we filtered 
out questions not relevant to cinema or movies. 
The evaluation of the system did not assess 
whether it can extract the correct answer. Instead, 
it measured to what extent the system can select 
the right SPARQL pattern. The explanation for 
this can be found in the fact that once a correct 
question pattern is selected, the extraction of the 
answer requires only retrieval of the answer from 
the database. Moreover, it should be pointed out 
that the main purpose of this preliminary evalua-
tion was to test the interaction between compo-
nents and indicate potential problems, and it was 
less about their performances.  
Table 1 summarises the results of the evalua-
tion. The number of questions used in the evalua-
tion is different from one language to another. 
This can be explained by the fact that for each 
language a number of questions (in general 500) 
was randomly selected from the benchmark and 
only the ones which referred to cinema or movies 
were selected. The column Questions indicates 
the number of questions assessed. The Correct 
column indicates for how many questions a cor-
rect SPARQL was generated. The Wrong column 
corresponds to the number of questions where a 
wrong or incomplete SPARQL was generated. 
This number also includes cases where no 
SPARQL was generated due to lack of corre-
sponding answer pattern. 
 
 Questions Correct Wrong 
English 167 74 (44.31%) 93 (55.68%) 
German 214 120 (56.04%) 94 (43.92%) 
Spanish 58 50 (86.20%) 8 (13.79%) 
Italian 99 46 (46.46%) 53 (53.53%) 
Table 1: Evaluation results 
As can be seen, the results are very different 
from one language to another. This can be ex-
plained by the fact that different entailment en-
gines are used for each language. In addition, 
even though the benchmark was built using a 
common set of guidelines, the complexity of 
questions varies from one language to another. 
For this reason, for some questions it is more dif-
ficult to find the correct pattern than for others.  
Analysis of the results revealed that one of the 
easiest ways to improve the performance of the 
system is to increase the number of patterns. Cur-
rently the average number of patterns per lan-
guage is 42. Improvement of the entailment en-
gines is another direction which needs to be pur-
sued. Most of the partners involved in the project 
have more powerful entailment engines than 
those integrated in the prototype which were 
ranked highly in RTE competitions. Unfortu-
nately, many of these engines cannot be used di-
rectly in our system due to their slow speed. Our 
system is supposed to give users results in real 
time which imposes some constraints on the 
amount of processing that can be done. 
5 Conclusions 
This paper presented the first prototype of an 
entailment-based QA system, which can answer 
questions about movies and cinema. The use of a 
domain ontology ensures that the system is cross-
language and can be extended to new languages 
with slight adjustments at the entailment engine. 
The system is implemented as a set of web ser-
vices and along a Service Oriented Architecture. 
6 Acknowledgements 
This work is supported by the EU-funded pro-
ject QALL-ME (FP6 IST-033860).  
References 
Androutsopoulos, I. and G.D. Ritchie and P. Thanisch. 
1995. Natural Language Interfaces to Databases -- 
An Introduction, Journal of Natural Language En-
gineering, vol.1, no.1, Cambridge University Press. 
Popescu Ana-Marie, Oren Etzioni, and Henry Kautz. 
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the confer-
ence on Intelligent User Interfaces. 
Dagan Ido and Oren Glickman. 2004. Probabilistic 
textual entailment: Generic applied modeling of 
language variability. In PASCAL Workshop on 
Learning Methods for Text Understanding and 
Mining, Grenoble. 
Ou Shiyan, Viktor Pekar, Constantin Orasan, Chris-
tian Spurk, Matteo Negri. 2008. Development and 
alignment of a domain-specific ontology for ques-
tion answering. In Proceedings of the 6th Edition of 
the Language Resources and Evaluation Confer-
ence (LREC-08).  
Prud'hommeaux Eric, Andy Seaborne (eds.). 2006. 
SPARQL Query Language for RDF. RDF Data Ac-
cess Working Group. 
176
Investigating a Generic Paraphrase-based Approach
for Relation Extraction
Lorenza Romano
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
romano@itc.it
Milen Kouylekov
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
kouylekov@itc.it
Idan Szpektor
Department of Computer Science
Bar Ilan University
Ramat Gan, 52900, Israel
szpekti@cs.biu.ac.il
Ido Dagan
Department of Computer Science
Bar Ilan University
Ramat Gan, 52900, Israel
dagan@cs.biu.ac.il
Alberto Lavelli
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
lavelli@itc.it
Abstract
Unsupervised paraphrase acquisition has
been an active research field in recent
years, but its effective coverage and per-
formance have rarely been evaluated. We
propose a generic paraphrase-based ap-
proach for Relation Extraction (RE), aim-
ing at a dual goal: obtaining an applicative
evaluation scheme for paraphrase acquisi-
tion and obtaining a generic and largely
unsupervised configuration for RE.We an-
alyze the potential of our approach and
evaluate an implemented prototype of it
using an RE dataset. Our findings reveal a
high potential for unsupervised paraphrase
acquisition. We also identify the need for
novel robust models for matching para-
phrases in texts, which should address syn-
tactic complexity and variability.
1 Introduction
A crucial challenge for semantic NLP applica-
tions is recognizing the many different ways for
expressing the same information. This seman-
tic variability phenomenon was addressed within
specific applications, such as question answering,
information extraction and information retrieval.
Recently, the problem was investigated within
generic application-independent paradigms, such
as paraphrasing and textual entailment.
Eventually, it would be most appealing to apply
generic models for semantic variability to concrete
applications. This paper investigates the applica-
bility of a generic ?paraphrase-based? approach to
the Relation Extraction (RE) task, using an avail-
able RE dataset of protein interactions. RE is
highly suitable for such investigation since its goal
is to exactly identify all the different variations in
which a target semantic relation can be expressed.
Taking this route sets up a dual goal: (a) from
the generic paraphrasing perspective - an objective
evaluation of paraphrase acquisition performance
on a concrete application dataset, as well as identi-
fying the additional mechanisms needed to match
paraphrases in texts; (b) from the RE perspective -
investigating the feasibility and performance of a
generic paraphrase-based approach for RE.
Our configuration assumes a set of entailing
templates (non-symmetric ?paraphrases?) for the
target relation. For example, for the target rela-
tion ?X interact with Y? we would assume a set of
entailing templates as in Tables 3 and 7. In addi-
tion, we require a syntactic matching module that
identifies template instances in text.
First, we manually analyzed the protein-
interaction dataset and identified all cases in which
protein interaction is expressed by an entailing
template. This set a very high idealized upper
bound for the recall of the paraphrase-based ap-
proach for this dataset. Yet, obtaining high cover-
age in practice would require effective paraphrase
acquisition and lexical-syntactic template match-
ing. Next, we implemented a prototype that uti-
lizes a state-of-the-art method for learning en-
tailment relations from the web (Szpektor et al,
2004), the Minipar dependency parser (Lin, 1998)
and a syntactic matching module. As expected,
the performance of the implemented system was
much lower than the ideal upper bound, yet ob-
taining quite reasonable practical results given its
unsupervised nature.
The contributions of our investigation follow
409
the dual goal set above. To the best of our knowl-
edge, this is the first comprehensive evaluation
that measures directly the performance of unsuper-
vised paraphrase acquisition relative to a standard
application dataset. It is also the first evaluation of
a generic paraphrase-based approach for the stan-
dard RE setting. Our findings are encouraging for
both goals, particularly relative to their early ma-
turity level, and reveal constructive evidence for
the remaining room for improvement.
2 Background
2.1 Unsupervised Information Extraction
Information Extraction (IE) and its subfield Rela-
tion Extraction (RE) are traditionally performed
in a supervised manner, identifying the different
ways to express a specific information or relation.
Given that annotated data is expensive to produce,
unsupervised or weakly supervised methods have
been proposed for IE and RE.
Yangarber et al (2000) and Stevenson and
Greenwood (2005) define methods for automatic
acquisition of predicate-argument structures that
are similar to a set of seed relations, which rep-
resent a specific scenario. Yangarber et al (2000)
approach was evaluated in two ways: (1) manually
mapping the discovered patterns into an IE system
and running a full MUC-style evaluation; (2) using
the learned patterns to perform document filtering
at the scenario level. Stevenson and Greenwood
(2005) evaluated their method through document
and sentence filtering at the scenario level.
Sudo et al (2003) extract dependency subtrees
within relevant documents as IE patterns. The goal
of the algorithm is event extraction, though perfor-
mance is measured by counting argument entities
rather than counting events directly.
Hasegawa et al (2004) performs unsupervised
hierarchical clustering over a simple set of fea-
tures. The algorithm does not extract entity pairs
for a given relation from a set of documents but
rather classifies all relations in a large corpus. This
approach is more similar to text mining tasks than
to classic IE problems.
To conclude, several unsupervised approaches
learn relevant IE templates for a complete sce-
nario, but without identifying their relevance to
each specific relation within the scenario. Accord-
ingly, the evaluations of these works either did not
address the direct applicability for RE or evaluated
it only after further manual postprocessing.
2.2 Paraphrases and Entailment Rules
A generic model for language variability is us-
ing paraphrases, text expressions that roughly con-
vey the same meaning. Various methods for auto-
matic paraphrase acquisition have been suggested
recently, ranging from finding equivalent lexical
elements to learning rather complex paraphrases
at the sentence level1.
More relevant for RE are ?atomic? paraphrases
between templates, text fragments containing vari-
ables, e.g. ?X buy Y ? X purchase Y?. Under a
syntactic representation, a template is a parsed text
fragment, e.g. ?X subj? interact mod? with pcomp?n? Y?
(based on the syntactic dependency relations of
the Minipar parser). The parses include part-of-
speech tags, which we omit for clarity.
Dagan and Glickman (2004) suggested that a
somewhat more general notion than paraphrasing
is that of entailment relations. These are direc-
tional relations between two templates, where the
meaning of one can be entailed from the meaning
of the other, e.g. ?X bind to Y? X interact with Y?.
For RE, when searching for a target relation, it is
sufficient to identify an entailing template since it
implies that the target relation holds as well. Un-
der this notion, paraphrases are bidirectional en-
tailment relations.
Several methods extract atomic paraphrases by
exhaustively processing local corpora (Lin and
Pantel, 2001; Shinyama et al, 2002). Learn-
ing from a local corpus is bounded by the cor-
pus scope, which is usually domain specific (both
works above processed news domain corpora). To
cover a broader range of domains several works
utilized the Web, while requiring several manu-
ally provided examples for each input relation,
e.g. (Ravichandran and Hovy, 2002). Taking a
step further, the TEASE algorithm (Szpektor et al,
2004) provides a completely unsupervised method
for acquiring entailment relations from the Web
for a given input relation (see Section 5.1).
Most of these works did not evaluate their re-
sults in terms of application coverage. Lin and
Pantel (2001) compared their results to human-
generated paraphrases. Shinyama et al (2002)
measured the coverage of their learning algorithm
relative to the paraphrases present in a given cor-
pus. Szpektor et al (2004) measured ?yield?, the
number of correct rules learned for an input re-
1See the 3rd IWP workshop for a sample of recent works
on paraphrasing (http://nlp.nagaokaut.ac.jp/IWP2005/).
410
lation. Ravichandran and Hovy (2002) evaluated
the performance of a QA system that is based
solely on paraphrases, an approach resembling
ours. However, they measured performance using
Mean Reciprocal Rank, which does not reveal the
actual coverage of the learned paraphrases.
3 Assumed Configuration for RE
Phenomenon Example
Passive form ?Y is activated by X?
Apposition ?X activates its companion, Y?
Conjunction ?X activates prot3 and Y?
Set ?X activates two proteins, Y and Z?
Relative clause ?X, which activates Y?
Coordination ?X binds and activates Y?
Transparent head ?X activates a fragment of Y?
Co-reference ?X is a kinase, though it activates Y?
Table 1: Syntactic variability phenomena, demon-
strated for the normalized template ?X activate Y?.
The general configuration assumed in this pa-
per for RE is based on two main elements: a list
of lexical-syntactic templates which entail the re-
lation of interest and a syntactic matcher which
identifies the template occurrences in sentences.
The set of entailing templates may be collected ei-
ther manually or automatically. We propose this
configuration both as an algorithm for RE and as
an evaluation scheme for paraphrase acquisition.
The role of the syntactic matcher is to iden-
tify the different syntactic variations in which tem-
plates occur in sentences. Table 1 presents a list
of generic syntactic phenomena that are known in
the literature to relate to linguistic variability. A
phenomenon which deserves a few words of ex-
planation is the ?transparent head noun? (Grish-
man et al, 1986; Fillmore et al, 2002). A trans-
parent noun N1 typically occurs in constructs of
the form ?N1 preposition N2? for which the syn-
tactic relation involving N1, which is the head of
the NP, applies to N2, the modifier. In the example
in Table 1, ?fragment? is the transparent head noun
while the relation ?activate? applies to Y as object.
4 Manual Data Analysis
4.1 Protein Interaction Dataset
Bunescu et al (2005) proposed a set of tasks re-
garding protein name and protein interaction ex-
traction, for which they manually tagged about
200 Medline abstracts previously known to con-
tain human protein interactions (a binary symmet-
ric relation). Here we consider their RE task of
extracting interacting protein pairs, given that the
correct protein names have already been identi-
fied. All protein names are annotated in the given
gold standard dataset, which includes 1147 anno-
tated interacting protein pairs. Protein names are
rather complex, and according to the annotation
adopted by Bunescu et al (2005) can be substrings
of other protein names (e.g., <prot> <prot>
GITR </prot> ligand </prot>). In such
cases, we considered only the longest names and
protein pairs involving them. We also ignored all
reflexive pairs, in which one protein is marked
as interacting with itself. Altogether, 1052 inter-
actions remained. All protein names were trans-
formed into symbols of the type ProtN , where N
is a number, which facilitates parsing.
For development purposes, we randomly split
the abstracts into a 60% development set (575 in-
teractions) and a 40% test set (477 interactions).
4.2 Dataset analysis
In order to analyze the potential of our approach,
two of the authors manually annotated the 575 in-
teracting protein pairs in the development set. For
each pair the annotators annotated whether it can
be identified using only template-based matching,
assuming an ideal implementation of the configu-
ration of Section 3. If it can, the normalized form
of the template connecting the two proteins was
annotated as well. The normalized template form
is based on the active form of the verb, stripped
of the syntactic phenomena listed in Table 1. Ad-
ditionally, the relevant syntactic phenomena from
Table 1 were annotated for each template instance.
Table 2 provides several example annotations.
A Kappa value of 0.85 (nearly perfect agree-
ment) was measured for the agreement between
the two annotators, regarding whether a protein
pair can be identified using the template-based
method. Additionally, the annotators agreed on
96% of the normalized templates that should be
used for the matching. Finally, the annotators
agreed on at least 96% of the cases for each syn-
tactic phenomenon except transparent heads, for
which they agreed on 91% of the cases. This high
level of agreement indicates both that template-
based matching is a well defined task and that nor-
malized template form and its syntactic variations
are well defined notions.
Several interesting statistics arise from the an-
411
Sentence Annotation
We have crystallized a complex between human FGF1 and
a two-domain extracellular fragment of human FGFR2.
? template: ?complex between X and Y?
? transparent head: ?fragment of X?
CD30 and its counter-receptor CD30 ligand (CD30L) are
members of the TNF-receptor / TNFalpha superfamily and
function to regulate lymphocyte survival and differentiation.
? template: ?X?s counter-receptor Y?
? apposition
? co-reference
iCdi1, a human G1 and S phase protein phosphatase that
associates with Cdk2.
? template: ?X associate with Y?
? relative clause
Table 2: Examples of annotations of interacting protein pairs. The annotation describes the normalized
template and the different syntactic phenomena identified.
Template f Template f Template f
X interact with Y 28 interaction of X with Y 12 X Y interaction 5
X bind to Y 22 X associate with Y 11 X interaction with Y 4
X Y complex 17 X activate Y 6 association of X with Y 4
interaction between X and Y 16 binding of X to Y 5 X?s association with Y 3
X bind Y 14 X form complex with Y 5 X be agonist for Y 3
Table 3: The 15 most frequent templates and their instance count (f ) in the development set.
notation. First, 93% of the interacting protein pairs
(537/575) can be potentially identified using the
template-based approach, if the relevant templates
are provided. This is a very promising finding,
suggesting that the template-based approach may
provide most of the requested information. We
term these 537 pairs as template-based pairs. The
remaining pairs are usually expressed by complex
inference or at a discourse level.
Phenomenon % Phenomenon %
transparent head 34 relative clause 8
apposition 24 co-reference 7
conjunction 24 coordination 7
set 13 passive form 2
Table 4: Occurrence percentage of each syntactic
phenomenon within template-based pairs (537).
Second, for 66% of the template-based pairs
at least one syntactic phenomenon was annotated.
Table 4 contains the occurrence percentage of each
phenomenon in the development set. These results
show the need for a powerful syntactic matcher on
top of high performance template acquisition, in
order to correctly match a template in a sentence.
Third, 175 different normalized templates were
identified. For each template we counted its tem-
plate instances, the number of times the tem-
plate occurred, counting only occurrences that ex-
press an interaction of a protein pair. In total,
we counted 341 template instances for all 175
templates. Interestingly, 50% of the template in-
stances (184/341) are instances of the 21 most fre-
quent templates. This shows that, though protein
interaction can be expressed in many ways, writ-
ers tend to choose from among just a few common
expressions. Table 3 presents the most frequent
templates. Table 5 presents the minimal number
of templates required to obtain the range of differ-
ent recall levels.
Furthermore, we grouped template variants
that are based on morphological derivations (e.g.
?X interact with Y? and ?X Y interaction?)
and found that 4 groups, ?X interact with Y?,
?X bind to Y?, ?X associate with Y? and ?X com-
plex with Y?, together with their morphological
derivations, cover 45% of the template instances.
This shows the need to handle generic lexical-
syntactic phenomena, and particularly morpholog-
ical based variations, separately from the acquisi-
tion of normalized lexical syntactic templates.
To conclude, this analysis indicates that the
template-based approach provides very high cov-
erage for this RE dataset, and a small number of
normalized templates already provides significant
recall. However, it is important to (a) develop
a model for morphological-based template vari-
ations (e.g. as encoded in Nomlex (Macleod et
al., )), and (b) apply accurate parsing and develop
syntactic matching models to recognize the rather
412
complex variations of template instantiations in
text. Finally, we note that our particular figures
are specific to this dataset and the biological ab-
stracts domain. However, the annotation and anal-
ysis methodologies are general and are suggested
as highly effective tools for further research.
R(%) # templates R(%) # templates
10 2 60 39
20 4 70 73
30 6 80 107
40 11 90 141
50 21 100 175
Table 5: The number of most frequent templates
necessary to reach different recall levels within the
341 template instances.
5 Implemented Prototype
This section describes our initial implementation
of the approach in Section 3.
5.1 TEASE
The TEASE algorithm (Szpektor et al, 2004) is
an unsupervised method for acquiring entailment
relations from the Web for a given input template.
In this paper we use TEASE for entailment rela-
tion acquisition since it processes an input tem-
plate in a completely unsupervised manner and
due to its broad domain coverage obtained from
the Web. The reported percentage of correct out-
put templates for TEASE is 44%.
The TEASE algorithm consists of 3 steps,
demonstrated in Table 6. TEASE first retrieves
from the Web sentences containing the input tem-
plate. From these sentences it extracts variable in-
stantiations, termed anchor-sets, which are identi-
fied as being characteristic for the input template
based on statistical criteria (first column in Ta-
ble 6). Characteristic anchor-sets are assumed to
uniquely identify a specific event or fact. Thus,
any template that appears with such an anchor-set
is assumed to have an entailment relationship with
the input template. Next, TEASE retrieves from
the Web a corpus S of sentences that contain the
characteristic anchor-sets (second column), hop-
ing to find occurrences of these anchor-sets within
templates other than the original input template.
Finally, TEASE parses S and extracts templates
that are assumed to entail or be entailed by the
input template. Such templates are identified as
maximal most general sub-graphs that contain the
anchor sets? positions (third column in Table 6).
Each learned template is ranked by number of oc-
currences in S.
5.2 Transformation-based Graph Matcher
In order to identify instances of entailing templates
in sentences we developed a syntactic matcher that
is based on transformations rules. The matcher
processes a sentence in 3 steps: 1) parsing the sen-
tence with the Minipar parser, obtaining a depen-
dency graph2; 2) matching each template against
the sentence dependency graph; 3) extracting can-
didate term pairs that match the template variables.
A template is considered directly matched in a
sentence if it appears as a sub-graph in the sen-
tence dependency graph, with its variables instan-
tiated. To further address the syntactic phenomena
listed in Table 1 we created a set of hand-crafted
parser-dependent transformation rules, which ac-
count for the different ways in which syntactic
relationships may be realized in a sentence. A
transformation rule maps the left hand side of the
rule, which strictly matches a sub-graph of the
given template, to the right hand side of the rule,
which strictly matches a sub-graph of the sentence
graph. If a rule matches, the template sub-graph is
mapped accordingly into the sentence graph.
For example, to match the syntactic tem-
plate ?X(N) subj? activate(V) obj? Y(N)? (POS
tags are in parentheses) in the sentence ?Prot1
detected and activated Prot2? (see Figure 1) we
should handle the coordination phenomenon.
The matcher uses the transformation rule
?Var1(V) ? and(U)mod? Word(V) conj? Var1(V)?
to overcome the syntactic differences. In this
example Var1 matches the verb ?activate?, Word
matches the verb ?detect? and the syntactic rela-
tions for Word are mapped to the ones for Var1.
Thus, we can infer that the subject and object
relations of ?detect? are also related to ?activate?.
6 Experiments
6.1 Experimental Settings
To acquire a set of entailing templates we first ex-
ecuted TEASE on the input template ?X subj? in-
teract mod? with pcomp?n? Y?, which corresponds to
the ?default? expression of the protein interaction
2We chose a dependency parser as it captures directly the
relations between words; we use Minipar due to its speed.
413
Extracted Anchor-set Sentence containing Anchor-set Learned Template
X=?chemokines?,
Y=?specific receptors?
Chemokines bind to specific receptors on the target
cells
X subj? bind mod?
to
pcomp?n
? Y
X=?Smad3?, Y=?Smad4? Smad3 / Smad4 complexes translocate to the nucleus X Y nn? complex
Table 6: TEASE output at different steps of the algorithm for ?X subj? interact mod? with pcomp?n? Y?.
1. X bind to Y 7. X Y complex 13. X interaction with Y
2. X activate Y 8. X recognize Y 14. X trap Y
3. X stimulate Y 9. X block Y 15. X recruit Y
4. X couple to Y 10. X binding to Y 16. X associate with Y
5. interaction between X and Y 11. X Y interaction 17. X be linked to Y
6. X become trapped in Y 12. X attach to Y 18. X target Y
Table 7: The top 18 correct templates learned by TEASE for ?X interact with Y?.
detect(V )
subjwwppp
pp
pp
pp
pp
conj

mod
''NN
NN
NN
NN
NN
N
obj // Prot2(N)
Prot1(N) activate(V ) and(U)
Figure 1: The dependency parse graph of the sen-
tence ?Prot1 detected and activated Prot2?.
relation. TEASE learned 118 templates for this
relation. Table 7 lists the top 18 learned templates
that we considered as correct (out of the top 30
templates in TEASE output). We then extracted
interacting protein pair candidates by applying the
syntactic matcher to the 119 templates (the 118
learned plus the input template). Candidate pairs
that do not consist of two proteins, as tagged in the
input dataset, were filtered out (see Section 4.1;
recall that our experiments were applied to the
dataset of protein interactions, which isolates the
RE task from the protein name recognition task).
In a subsequent experiment we iteratively ex-
ecuted TEASE on the 5 top-ranked learned tem-
plates to acquire additional relevant templates. In
total, we obtained 1233 templates that were likely
to imply the original input relation. The syntactic
matcher was then reapplied to extract candidate in-
teracting protein pairs using all 1233 templates.
We used the development set to tune a small
set of 10 generic hand-crafted transformation rules
that handle different syntactic variations. To han-
dle transparent head nouns, which is the only phe-
nomenon that demonstrates domain dependence,
we extracted a set of the 5 most frequent trans-
parent head patterns in the development set, e.g.
?fragment of X?.
In order to compare (roughly) our performance
with supervised methods applied to this dataset, as
summarized in (Bunescu et al, 2005), we adopted
their recall and precision measurement. Their
scheme counts over distinct protein pairs per ab-
stract, which yields 283 interacting pairs in our test
set and 418 in the development set.
6.2 Manual Analysis of TEASE Recall
experiment pairs instances
input 39% 37%
input + iterative 49% 48%
input + iterative + morph 63% 62%
Table 8: The potential recall of TEASE in terms of
distinct pairs (out of 418) and coverage of template
instances (out of 341) in the development set.
Before evaluating the system as a whole we
wanted to manually assess in isolation the cover-
age of TEASE output relative to all template in-
stances that were manually annotated in the devel-
opment set. We considered a template as covered
if there is a TEASE output template that is equal
to the manually annotated template or differs from
it only by the syntactic phenomena described in
Section 3 or due to some parsing errors. Count-
ing these matches, we calculated the number of
template instances and distinct interacting protein
pairs that are covered by TEASE output.
Table 8 presents the results of our analysis. The
414
1st line shows the coverage of the 119 templates
learned by TEASE for the input template ?X inter-
act with Y?. It is interesting to note that, though we
aim to learn relevant templates for the specific do-
main, TEASE learned relevant templates also by
finding anchor-sets of different domains that use
the same jargon, such as particle physics.
We next analyzed the contribution of the itera-
tive learning for the additional 5 templates to recall
(2nd line in Table 8). With the additional learned
templates, recall increased by about 25%, showing
the importance of using the iterative steps.
Finally, when allowing matching between a
TEASE template and a manually annotated tem-
plate, even if one is based on a morphologi-
cal derivation of the other (3rd line in Table 8),
TEASE recall increased further by about 30%.
We conclude that the potential recall of the cur-
rent version of TEASE on the protein interaction
dataset is about 60%. This indicates that signif-
icant coverage can be obtained using completely
unsupervised learning from the web, as performed
by TEASE. However, the upper bound for our cur-
rent implemented system is only about 50% be-
cause our syntactic matching does not handle mor-
phological derivations.
6.3 System Results
experiment recall precision F1
input 0.18 0.62 0.28
input + iterative 0.29 0.42 0.34
Table 9: System results on the test set.
Table 9 presents our system results for the test
set, corresponding to the first two experiments in
Table 8. The recall achieved by our current imple-
mentation is notably worse than the upper bound
of the manual analysis because of two general set-
backs of the current syntactic matcher: 1) parsing
errors; 2) limited transformation rule coverage.
First, the texts from the biology domain pre-
sented quite a challenge for the Minipar parser.
For example, in the sentences containing the
phrase ?X bind specifically to Y? the parser consis-
tently attaches the PP ?to? to ?specifically? instead
of to ?bind?. Thus, the template ?X bind to Y? can-
not be directly matched.
Second, we manually created a small number of
transformation rules that handle various syntactic
phenomena, since we aimed at generic domain in-
dependent rules. The most difficult phenomenon
to model with transformation rules is transparent
heads. For example, in ?the dimerization of Prot1
interacts with Prot2?, the transparent head ?dimer-
ization of X? is domain dependent. Transforma-
tion rules that handle such examples are difficult
to acquire, unless a domain specific learning ap-
proach (either supervised or unsupervised) is used.
Finally, we did not handle co-reference resolution
in the current implementation.
Bunescu et al (2005) and Bunescu and Mooney
(2005) approached the protein interaction RE task
using both handcrafted rules and several super-
vised Machine Learning techniques, which uti-
lize about 180 manually annotated abstracts for
training. Our results are not directly comparable
with theirs because they adopted 10-fold cross-
validation, while we had to divide the dataset into
a development and a test set, but a rough compari-
son is possible. For the same 30% recall, the rule-
based method achieved precision of 62% and the
best supervised learning algorithm achieved preci-
sion of 73%. Comparing to these supervised and
domain-specific rule-based approaches our system
is noticeably weaker, yet provides useful results
given that we supply very little domain specific in-
formation and acquire the paraphrasing templates
in a fully unsupervised manner. Still, the match-
ing models need considerable additional research
in order to achieve the potential performance sug-
gested by TEASE.
7 Conclusions and Future Work
We have presented a paraphrase-based approach
for relation extraction (RE), and an implemented
system, that rely solely on unsupervised para-
phrase acquisition and generic syntactic template
matching. Two targets were investigated: (a) a
mostly unsupervised, domain independent, con-
figuration for RE, and (b) an evaluation scheme
for paraphrase acquisition, providing a first evalu-
ation of its realistic coverage. Our approach differs
from previous unsupervised IE methods in that we
identify instances of a specific relation while prior
methods identified template relevance only at the
general scenario level.
We manually analyzed the potential of our ap-
proach on a dataset annotated with protein in-
teractions. The analysis shows that 93% of the
interacting protein pairs can be potentially iden-
tified with the template-based approach. Addi-
415
tionally, we manually assessed the coverage of
the TEASE acquisition algorithm and found that
63% of the distinct pairs can be potentially rec-
ognized with the learned templates, assuming an
ideal matcher, indicating a significant potential re-
call for completely unsupervised paraphrase ac-
quisition. Finally, we evaluated our current system
performance and found it weaker than supervised
RE methods, being far from fulfilling the poten-
tial indicated in our manual analyses due to insuf-
ficient syntactic matching. But, even our current
performance may be considered useful given the
very small amount of domain-specific information
used by the system.
Most importantly, we believe that our analysis
and evaluation methodologies for an RE dataset
provide an excellent benchmark for unsupervised
learning of paraphrases and entailment rules. In
the long run, we plan to develop and improve our
acquisition and matching algorithms, in order to
realize the observed potential of the paraphrase-
based approach. Notably, our findings point to the
need to learn generic morphological and syntactic
variations in template matching, an area which has
rarely been addressed till now.
Acknowledgements
This work was developed under the collaboration
ITC-irst/University of Haifa. Lorenza Romano
has been supported by the ONTOTEXT project,
funded by the Autonomous Province of Trento un-
der the FUP-2004 research program.
References
Razvan Bunescu and Raymond J. Mooney. 2005. Sub-
sequence kernels for relation extraction. In Proceed-
ings of the 19th Conference on Neural Information
Processing Systems, Vancouver, British Columbia.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155. Special Issue on Sum-
marization and Information Extraction from Medi-
cal Documents.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling of
language variability. In Proceedings of the PAS-
CAL Workshop on Learning Methods for Text Un-
derstanding and Mining, Grenoble, France.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. Seeing arguments through transparent struc-
tures. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC 2002), pages 787?791, Las Palmas, Spain.
Ralph Grishman, Lynette Hirschman, and Ngo Thanh
Nhan. 1986. Discovery procedures for sublanguage
selectional patterns: Initial experiments. Computa-
tional Linguistics, 12(3).
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discoverying relations among named
entities from large corpora. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2004), Barcelona, Spain.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation on
MINIPAR. In Proceedings of LREC-98 Workshop
on Evaluation of Parsing Systems, Granada, Spain.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. Nomlex: A lexi-
con of nominalizations. In Proceedings of the 8th
International Congress of the European Association
for Lexicography, Liege, Belgium.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a Question Answering
system. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), Philadelphia, PA.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase ac-
quisition from news articles. In Proceedings of
the Human Language Technology Conference (HLT
2002), San Diego, CA.
Mark Stevenson and Mark A. Greenwood. 2005. A
semantic approach to IE pattern induction. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), Ann
Arbor, Michigan.
K. Sudo, S. Sekine, and R. Grishman. 2003. An im-
proved extraction pattern representation model for
automatic IE pattern acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2003), Sapporo, Japan.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisi-
tion of entailment relations. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2004), Barcelona,
Spain.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
Proceedings of the 18th International Conference on
Computational Linguistics, Saarbruecken, Germany.
416
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 90?94, Dublin, Ireland, August 23-29 2014.
RDF Triple Stores and a Custom SPARQL Front-End for
Indexing and Searching (Very) Large Semantic Networks
Milen Kouylekov
?
and Stephan Oepen
??
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
{milen |oe}@ifi.uio.no
Abstract
With growing interest in the creation and search of linguistic annotations that form general graphs
(in contrast to formally simpler, rooted trees), there also is an increased need for infrastructures
that support the exploration of such representations, for example logical-form meaning represen-
tations or semantic dependency graphs. In this work, we lean heavily on semantic technologies
and in particular the data model of the Resource Description Framework (RDF) to represent,
store, and efficiently query very large collections of text annotated with graph-structured repre-
sentations of sentence meaning. Our full infrastructure is available under open-source licensing,
and through this system demonstration we hope to receive feedback on the general approach,
explore its application to additional types of meaning representation, and attract new users and
possibly co-developers.
1 Motivation: The Problem
Much work in the creation and use of language resources has focused on tree-shaped data structures,
1
as are commonly used for the encoding of, for example, syntactic or discourse annotations. Conversely,
there has been less focus on supporting general graphs until recently, but there is growing interest in
graph-structured representations, for example to annotate and process natural language semantics. In
this work, we demonstrate how semantic technologies, and in particular the data model of the Resource
Description Framework (RDF) can be put to use for efficient indexing and search in (very) large-scale
collections of semantic graphs.
We develop a mapping to RDF graphs for a variety of semantic representations, ranging from un-
derspecified logical-form meaning representations to ?pure? bi-lexical semantic dependency graphs, as
exemplified in Figures 1 and 2 below, respectively. Against this uniform data model, we populate off-
the-shelf RDF triple stores with semantic networks comprising between tens of thousands and tens of
millions of analyzed sentences. To lower the technological barrier to exploration of our triple stores, we
implement a compact ?designer? query language for semantic graphs through on-the-fly expansion into
SPARQL. In sum, the combination of standard RDF technologies and specialized query and visualization
interfaces yields a versatile and highly scalable infrastructure for search (and in principle limited forms
of reasoning) over diverse types of graph-structured representations of sentence meaning.
In our view, there is little scientific innovation in this work, but our approach rather demonstrates sub-
stantial design and engineering creativity. Our semantic search infrastructure is built from the combina-
tion of industrial-grade standard technologies (Apache Jena, Lucene, and Tomcat) with an open-source
application for, among others, format conversion, query processing, and visualization implemented in
Java. Thus, the complete tool chain is available freely and across platforms. Its application to additional
types of meaning representation (and possibly other graph-structured layers of linguistic analysis) should
be relatively straightforward, and we thus believe that our infrastructure can be of immediate value to
both providers and consumers of large-scale linguistic annotations that transcend tree structures.
This work is licenced under a Creative Commons Attribution 4.0 International License; page numbers and the proceedings
footer are added by the organizers. http://creativecommons.org/licenses/by/4.0/
1
Formally, trees are a restricted form of graphs, where every node is reachable from a distinguished root node by exactly
one directed path.
90
? h
1
,
h
4
:_a_q(x
6
, h
7
, h
5
), h
8
:_similar_a_to(e
9
, x
6
), h
8
:comp(e
11
, e
9
, ), h
8
:_technique_n_1(x
6
),
h
2
:_almost_a_1(e
12
, h
13
), h
14
:_impossible_a_for(e
3
, h
15
, i
16
),
h
17
:_apply_v_to(e
18
, i
19
, x
6
, x
20
), h
21
:udef_q(x
20
, h
22
, h
23
), h
24
:_other_a_1(e
25
, x
20
), h
24
:_crop_n_1(x
20
),
h
24
:_such+as_p(e
26
, x
20
, x
27
), h
40
:implicit_conj(x
27
, x
33
, x
38
),
h
31
:udef_q(x
33
, h
32
, h
34
), h
35
:_cotton_n_1(x
33
), h
46
:_and_c(x
38
, x
43
, x
47
),
h
41
:udef_q(x
43
, h
42
, h
44
), h
45
:_soybean_u_unknown(x
43
), h
48
:udef_q(x
47
, h
49
, h
50
), h
51
:_rice_n_1(x
47
)
{ h
49
=
q
h
51
, h
42
=
q
h
45
, h
32
=
q
h
35
, h
22
=
q
h
24
, h
15
=
q
h
17
, h
13
=
q
h
14
, h
7
=
q
h
8
, h
1
=
q
h
2
} ?
Figure 1: Example logical form meaning representation (MRS; taken from DeepBank).
2 Technology: Core Components
Our system architecture comprises two core components, viz. (a) the RDF repository, a database storing
semantic networks in RDF triple form, and (b) the Web application, an interface for interactive search
and visualization over the RDF repository.
Representing Semantic Graphs in RDF The RDF data model is based on statements about resources
in the form of subject?predicate?object triples. The subject denotes the resource, and the predicate
denotes traits or aspects of the resource, thus expressing a relationship between the subject and the
object. A database that can store such expression and evaluate queries to them is called a triple store.
In Kouylekov and Oepen (2014), we describe the conversion of different types of semantic struc-
tures into RDF graphs. To date, we have addressed three types of meaning representations, viz. (in de-
creasing complexity) (a) scope-underspecified logical formulas in Minimal Recursion Semantics (MRS;
Copestake et al., 2005); (b) variable-free Elementary Dependency Structures (EDS; Oepen and L?nning,
2006); and (c) bi-lexical dependency graphs as used in Task 8 at SemEval 2014 on Broad-Coverage
Semantic Dependency Parsing (SDP; Oepen et al., 2014; Ivanova et al., 2012). For all three formats, we
draw on (a) gold-standard annotations from DeepBank (Flickinger et al., 2012), a re-annotation of the
venerable Penn Treebank WSJ Corpus (Marcus et al., 1993); and on (b) much larger collections of auto-
matically generated analyses over the full English Wikipedia from the WikiWoods Treecache (Flickinger
et al., 2010).
To store MRS, EDS, and SDP structures, we created small ontologies for each type of representation,
building on a common core of shared ontology elements. In a nutshell, the EDS and SDP ontologies
provide a generic representation of directed graphs with (potentially complex) node and edge labels;
the dependencies proper, i.e. labeled arcs of the graph, are encoded as RDF object properties. The
MRS ontology, on the other hand, distinguishes different types of nodes, corresponding to full predica-
tions vs. individual logical variables vs. hierarchically organized sub-properties of variables. Mapping
the (medium-complexity) EDS graphs from DeepBank and WikiWoods onto RDF, for example, yields
around 12 million and 4.3 billion triples, respectively (for the semantic dependencies of about 37 thou-
sand and 48 million sentences in the two resources).
Web Application The core of our Web application is a search engine that executes SPARQL queries
against the RDF repository. SPARQL is an RDF query language to search triple stores, allowing one to
retrieve and manipulate RDF data. It is fully standardized and considered one of the key technologies
of the Semantic Web. A SPARQL query can consist of triple patterns, conjunctions, disjunctions, and
optional filters and functions. The query processor searches for sets of triples that match the patterns
expressed in the query, binding variables in the query to the corresponding parts of each triple.
A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.
top
ARG2
ARG3 ARG1
ARG2 _and_cARG1 mweARG1
BV
ARG1 conjARG1
Figure 2: Example bi-lexical semantic dependencies (SDP; taken from DeepBank).
91
PREFIX sdp:<http://wesearch.delph-in.net/rdf/sdp#>
PREFIX dm:<http://wesearch.delph-in.net/rdf/sdp/dm#>
select ?graph
where {
GRAPH ?graph {
?101 sdp:form "quarterly"^^xsd:string .
?x dm:lemma "result"^^xsd:string .
{
?100 dm:pos "vbp"^^xsd:string
UNION ?100 dm:pos "vbg"^^xsd:string
UNION ...
}
?101 dm:arg1 ?x .
{
?100 dm:arg1 ?x UNION ?100 dm:arg2 ?x
UNION ?100 dm:arg3 ?x UNION ?100 dm:arg4
}
FILTER
((!bound(?101) || !bound(?100) || ?101 != ?100)
&& (!bound(?101) || !bound(?x) || ?101 != ?x)
&& (!bound(?100) || !bound(?x) || ?100 != ?x))
}
}
GROUP BY ?graph
ORDER BY ?graph
Figure 3: Core of the auto-generated SPARQL query corresponding to our running example.
Our infrastructure supports the definition of families of ?meta? query languages, to address semantic
structures in a form that is more compact and much better adapted to the specific target format than
SPARQL. An example of such a ?designer? language is the WeSearch Query Language (WQL), which
was used in the context of the SemEval 2014 SDP task.
2
By way of informal introduction, consider the
following example query:
(1) /v
*
[ARG
*
x]
quarterly[ARG1 x]
x:+result
This example is comprised of three predications, one per line. The following characters have operator
status: ?/? (slash), ?
*
? (asterisk), ?[? and ?]? (left and right square bracket), ?:? (colon), and ?+? (plus
sign). This is a near-complete list of operator characters in WQL. Each predication can be composed of
(i) an identifier, followed by a colon if present; (ii) a form pattern; (iii) a lemma pattern, prefixed by a
plus sign, if present; (iv) a part-of-speech (PoS) pattern, prefixed by a slash, if present; and (v) a list of
arguments, enclosed in square brackets, if present. Patterns can make use of Lucene-style wildcards, with
the asterisk matching any number of characters, and a question mark (???) to match a single character.
Thus, our example query searches for a verbal predicate (any PoS tag starting with ?v?), that takes
any form of the lemma ?result? as its argument (in the range ARG
1
. . . ARG
n
), where this argument is
further required to be the ARG
1
of a node labeled ?quaterly?.
The auto-generated SPARQL expression that corresponds to this example query is shown in Figure
3. The query generator replaces the wildcarded PoS pattern by the union of all matching tags (that start
with ?v?, e.g. ??100 dm:pos "vbp" UNION ...? Likewise, the underspecified argument relation
of this predication is replaced by the union of all possible argument types. Finally, the query processor
ensures a one-to-one correspondence between query elements and matching graph elements, i.e. multiple
distinct query components cannot match against the same target (graph component), or vice versa. This
is accomplished in SPARQL through the filter expressions towards the end of the generated query.
2
See http://wesearch.delph-in.net/sdp/ for an on-line demonstration and additional documentation.
92
Figure 4: Screenshot of the interactive search interface, querying (semantic) object control structures.
Figure 4 shows a screenshot from the SemEval SDP user interface, demonstrating how WQL facilitates
concise (and reasonably transparent) search for semantic ?object? control, i.e. a configuration involving
two predicates sharing an argument in a specific assignment of roles.
Within the capabilities of the SPARQL back-end, different dialects of the meta query language can be
implemented in a modular fashion, for example distinguishing different types of nodes and introducing
additional node properties, as in the more complex MRS universe. Our query front-end transforms ?meta?
queries into equivalent SPARQL expressions, and the search interface allows users to inspect the result
of this transformation (and matching results), to possibly refine the search incrementally either at the
?meta? query layer or directly in SPARQL.
3 Demonstration: Indexing and Search
Our proposed interactive demonstration will seek to highlight (a) the flexibility of our infrastructure, i.e.
walk through a series of queries of increasing complexity against different target formats; (b) its scala-
bility, by comparing response times for different types of queries and different target formats over the
large DeepBank and the vast WikiWoods indexes; and (c) the ease of ?behind the scenes? functionality,
showing how additional semantic annotations in various formats can be ingested into the index. As part
of this latter aspect of the demonstration, we will optionally discuss how we apply string-level index-
ing (in Apache Lucene) and basic frequency statistics in query interpretation and optimization, which
jointly with parallelization over ?striped? RDF triple stores can yield greatly reduced response times for
common types of queries to the WikiWoods index. We envision that parts of the demonstration can be
organized in an audience-driven manner, for example taking as input the (possibly informal) charac-
terization of a semantic configuration, collectively transforming it into a query against our DeepBank
or WikiWoods stores, observing linguistic or technical properties of matching results, and refining the
search incrementally.
Our software infrastructure is entirely open-source and (increasingly) modularized and parameterized
to facilitate adaptation to additional types of annotation. Please see the project web page for licensing
and access information, as well as for pointers to a variety of existing on-line demonstrations:




	
http://wesearch.delph-in.net/
93
References
Copestake, A., Flickinger, D., Pollard, C., and Sag, I. A. (2005). Minimal Recursion Semantics. An
introduction. Research on Language and Computation, 3(4), 281 ? 332.
Flickinger, D., Oepen, S., and Ytrest?l, G. (2010). WikiWoods. Syntacto-semantic annotation for En-
glish Wikipedia. In Proceedings of the 7th International Conference on Language Resources and
Evaluation. Valletta, Malta.
Flickinger, D., Zhang, Y., and Kordoni, V. (2012). DeepBank. A dynamically annotated treebank of the
Wall Street Journal. In Proceedings of the 11th International Workshop on Treebanks and Linguistic
Theories (p. 85 ? 96). Lisbon, Portugal: Edi??es Colibri.
Ivanova, A., Oepen, S., ?vrelid, L., and Flickinger, D. (2012). Who did what to whom? A contrastive
study of syntacto-semantic dependencies. In Proceedings of the Sixth Linguistic Annotation Workshop
(p. 2 ? 11). Jeju, Republic of Korea.
Kouylekov, M., and Oepen, S. (2014). Semantic technologies for querying linguistic annotations. An
experiment focusing on graph-structured data. In Proceedings of the 9th International Conference on
Language Resources and Evaluation. Reykjavik, Iceland.
Marcus, M., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a large annotated corpora of
English: The Penn Treebank. Computational Linguistics, 19, 313 ? 330.
Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D., Flickinger, D., Haji
?
c, J., . . . Zhang, Y. (2014). SemEval
2014 Task 8. Broad-coverage semantic dependency parsing. In Proceedings of the 8th International
Workshop on Semantic Evaluation. Dublin, Ireland.
Oepen, S., and L?nning, J. T. (2006). Discriminant-based MRS banking. In Proceedings of the 5th
International Conference on Language Resources and Evaluation (p. 1250 ? 1255). Genoa, Italy.
94
Proceedings of the ACL 2010 System Demonstrations, pages 42?47,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
An Open-Source Package for Recognizing Textual Entailment
Milen Kouylekov and Matteo Negri
FBK - Fondazione Bruno Kessler
Via Sommarive 18, 38100 Povo (TN), Italy
[kouylekov,negri]@fbk.eu
Abstract
This paper presents a general-purpose
open source package for recognizing Tex-
tual Entailment. The system implements a
collection of algorithms, providing a con-
figurable framework to quickly set up a
working environment to experiment with
the RTE task. Fast prototyping of new
solutions is also allowed by the possibil-
ity to extend its modular architecture. We
present the tool as a useful resource to ap-
proach the Textual Entailment problem, as
an instrument for didactic purposes, and as
an opportunity to create a collaborative en-
vironment to promote research in the field.
1 Introduction
Textual Entailment (TE) has been proposed as
a unifying generic framework for modeling lan-
guage variability and semantic inference in dif-
ferent Natural Language Processing (NLP) tasks.
The Recognizing Textual Entailment (RTE) task
(Dagan and Glickman, 2007) consists in deciding,
given two text fragments (respectively called Text
- T, and Hypothesis - H), whether the meaning of
H can be inferred from the meaning of T, as in:
T: ?Yahoo acquired Overture?
H: ?Yahoo owns Overture?
The RTE problem is relevant for many different
areas of text processing research, since it repre-
sents the core of the semantic-oriented inferences
involved in a variety of practical NLP applications
including Question Answering, Information Re-
trieval, Information Extraction, Document Sum-
marization, and Machine Translation. However, in
spite of the great potential of integrating RTE into
complex NLP architectures, little has been done
to actually move from the controlled scenario pro-
posed by the RTE evaluation campaigns1 to more
practical applications. On one side, current RTE
technology might not be mature enough to provide
reliable components for such integration. Due to
the intrinsic complexity of the problem, in fact,
state of the art results still show large room for im-
provement. On the other side, the lack of available
tools makes experimentation with the task, and the
fast prototyping of new solutions, particularly dif-
ficult. To the best of our knowledge, the broad
literature describing RTE systems is not accompa-
nied with a corresponding effort on making these
systems open-source, or at least freely available.
We believe that RTE research would significantly
benefit from such availability, since it would allow
to quickly set up a working environment for ex-
periments, encourage participation of newcomers,
and eventually promote state of the art advances.
The main contribution of this paper is to present
the latest release of EDITS (Edit Distance Textual
Entailment Suite), a freely available, open source
software package for recognizing Textual Entail-
ment. The system has been designed following
three basic requirements:
Modularity. System architecture is such that the
overall processing task is broken up into major
modules. Modules can be composed through a
configuration file, and extended as plug-ins ac-
cording to individual requirements. System?s
workflow, the behavior of the basic components,
and their IO formats are described in a compre-
hensive documentation available upon download.
Flexibility. The system is general-purpose, and
suited for any TE corpus provided in a simple
XML format. In addition, both language depen-
dent and language independent configurations are
allowed by algorithms that manipulate different
representations of the input data.
1TAC RTE Challenge: http://www.nist.gov/tac
EVALITA TE task: http://evalita.itc.it
42
Figure 1: Entailment Engine, main components
and workflow
Adaptability. Modules can be tuned over train-
ing data to optimize performance along several di-
mensions (e.g. overall Accuracy, Precision/Recall
trade-off on YES and NO entailment judgements).
In addition, an optimization component based on
genetic algorithms is available to automatically set
parameters starting from a basic configuration.
EDITS is open source, and available under
GNU Lesser General Public Licence (LGPL). The
tool is implemented in Java, it runs on Unix-based
Operating Systems, and has been tested on MAC
OSX, Linux, and Sun Solaris. The latest release
of the package can be downloaded from http:
//edits.fbk.eu.
2 System Overview
The EDITS package allows to:
? Create an Entailment Engine (Figure 1) by
defining its basic components (i.e. algo-
rithms, cost schemes, rules, and optimizers);
? Train such Entailment Engine over an anno-
tated RTE corpus (containing T-H pairs anno-
tated in terms of entailment) to learn aModel;
? Use the Entailment Engine and the Model to
assign an entailment judgement and a confi-
dence score to each pair of an un-annotated
test corpus.
EDITS implements a distance-based framework
which assumes that the probability of an entail-
ment relation between a given T-H pair is inversely
proportional to the distance between T and H (i.e.
the higher the distance, the lower is the probability
of entailment). Within this framework the system
implements and harmonizes different approaches
to distance computation, providing both edit dis-
tance algorithms, and similarity algorithms (see
Section 3.1). Each algorithm returns a normalized
distance score (a number between 0 and 1). At a
training stage, distance scores calculated over an-
notated T-H pairs are used to estimate a threshold
that best separates positive from negative exam-
ples. The threshold, which is stored in a Model, is
used at a test stage to assign an entailment judge-
ment and a confidence score to each test pair.
In the creation of a distance Entailment Engine,
algorithms are combined with cost schemes (see
Section 3.2) that can be optimized to determine
their behaviour (see Section 3.3), and optional ex-
ternal knowledge represented as rules (see Section
3.4). Besides the definition of a single Entailment
Engine, a unique feature of EDITS is that it al-
lows for the combination of multiple Entailment
Engines in different ways (see Section 4.4).
Pre-defined basic components are already pro-
vided with EDITS, allowing to create a variety of
entailment engines. Fast prototyping of new solu-
tions is also allowed by the possibility to extend
the modular architecture of the system with new
algorithms, cost schemes, rules, or plug-ins to new
language processing components.
3 Basic Components
This section overviews the main components of
a distance Entailment Engine, namely: i) algo-
rithms, iii) cost schemes, iii) the cost optimizer,
and iv) entailment/contradiction rules.
3.1 Algorithms
Algorithms are used to compute a distance score
between T-H pairs.
EDITS provides a set of predefined algorithms,
including edit distance algorithms, and similar-
ity algorithms adapted to the proposed distance
framework. The choice of the available algorithms
is motivated by their large use documented in RTE
literature2.
Edit distance algorithms cast the RTE task as
the problem of mapping the whole content of H
into the content of T. Mappings are performed
as sequences of editing operations (i.e. insertion,
deletion, substitution of text portions) needed to
transform T into H, where each edit operation has
a cost associated with it. The distance algorithms
available in the current release of the system are:
2Detailed descriptions of all the systems participating in
the TAC RTE Challenge are available at http://www.
nist.gov/tac/publications
43
? Token Edit Distance: a token-based version
of the Levenshtein distance algorithm, with
edit operations defined over sequences of to-
kens of T and H;
? Tree Edit Distance: an implementation of the
algorithm described in (Zhang and Shasha,
1990), with edit operations defined over sin-
gle nodes of a syntactic representation of T
and H.
Similarity algorithms are adapted to the ED-
ITS distance framework by transforming measures
of the lexical/semantic similarity between T and H
into distance measures. These algorithms are also
adapted to use the three edit operations to support
overlap calculation, and define term weights. For
instance, substitutable terms in T and H can be
treated as equal, and non-overlapping terms can be
weighted proportionally to their insertion/deletion
costs. Five similarity algorithms are available,
namely:
? Word Overlap: computes an overall (dis-
tance) score as the proportion of common
words in T and H;
? Jaro-Winkler distance: a similarity algorithm
between strings, adapted to similarity on
words;
? Cosine Similarity: a common vector-based
similarity measure;
? Longest Common Subsequence: searches the
longest possible sequence of words appearing
both in T and H in the same order, normaliz-
ing its length by the length of H;
? Jaccard Coefficient: confronts the intersec-
tion of words in T and H to their union.
3.2 Cost Schemes
Cost schemes are used to define the cost of each
edit operation.
Cost schemes are defined as XML files that ex-
plicitly associate a cost (a positive real number) to
each edit operation applied to elements of T and
H. Elements, referred to as A and B, can be of dif-
ferent types, depending on the algorithm used. For
instance, Tree Edit Distance will manipulate nodes
in a dependency tree representation, whereas To-
ken Edit Distance and similarity algorithms will
manipulate words. Figure 2 shows an example of
<scheme>
<insertion><cost>10</cost></insertion>
<deletion><cost>10</cost></deletion>
<substitution>
<condition>(equals A B)</condition>
<cost>0</cost>
</substitution>
<substitution>
<condition>(not (equals A B))</condition>
<cost>20</cost>
</substitution>
</scheme>
Figure 2: Example of XML Cost Scheme
cost scheme, where edit operation costs are de-
fined as follows:
Insertion(B)=10 - inserting an element B from H
to T, no matter what B is, always costs 10;
Deletion(A)=10 - deleting an element A from T,
no matter what A is, always costs 10;
substitution(A,B)=0 if A=B - substituting A with
B costs 0 if A and B are equal;
substitution(A,B)=20 if A !=B - substituting A
with B costs 20 if A and B are different.
In the distance-based framework adopted by
EDITS, the interaction between algorithms and
cost schemes plays a central role. Given a T-H
pair, in fact, the distance score returned by an al-
gorithm directly depends on the cost of the opera-
tions applied to transform T into H (edit distance
algorithms), or on the cost of mapping words in
H with words in T (similarity algorithms). Such
interaction determines the overall behaviour of an
Entailment Engine, since distance scores returned
by the same algorithm with different cost schemes
can be considerably different. This allows users to
define (and optimize, as explained in Section 3.3)
the cost schemes that best suit the RTE data they
want to model3.
EDITS provides two predefined cost schemes:
? Simple Cost Scheme - the one shown in Fig-
ure 2, setting fixed costs for each edit opera-
tion.
? IDF Cost Scheme - insertion and deletion
costs for a word w are set to the inverse doc-
ument frequency of w (IDF(w)). The sub-
stitution cost is set to 0 if a word w1 from
T and a word w2 from H are the same, and
IDF(w1)+IDF(w2) otherwise.
3For instance, when dealing with T-H pairs composed by
texts that are much longer than the hypotheses (as in the RTE5
Campaign), setting low deletion costs avoids penalization to
short Hs fully contained in the Ts.
44
In the creation of new cost schemes, users can
express edit operation costs, and conditions over
the A and B elements, using a meta-language
based on a lisp-like syntax (e.g. (+ (IDF A) (IDF
B)), (not (equals A B))). The system also provides
functions to access data stored in hash files. For
example, the IDF Cost Scheme accesses the IDF
values of the most frequent 100K English words
(calculated on the Brown Corpus) stored in a file
distributed with the system. Users can create new
hash files to collect statistics about words in other
languages, or other information to be used inside
the cost scheme.
3.3 Cost Optimizer
A cost optimizer is used to adapt cost schemes (ei-
ther those provided with the system, or new ones
defined by the user) to specific datasets.
The optimizer is based on cost adaptation
through genetic algorithms, as proposed in
(Mehdad, 2009). To this aim, cost schemes can
be parametrized by externalizing as parameters the
edit operations costs. The optimizer iterates over
training data using different values of these param-
eters until on optimal set is found (i.e. the one that
best performs on the training set).
3.4 Rules
Rules are used to provide the Entailment Engine
with knowledge (e.g. lexical, syntactic, semantic)
about the probability of entailment or contradic-
tion between elements of T and H. Rules are in-
voked by cost schemes to influence the cost of sub-
stitutions between elements of T and H. Typically,
the cost of the substitution between two elements
A and B is inversely proportional to the probability
that A entails B.
Rules are stored in XML files called Rule
Repositories, with the format shown in Figure 3.
Each rule consists of three parts: i) a left-hand
side, ii) a right-hand side, iii) a probability that
the left-hand side entails (or contradicts) the right-
hand side.
EDITS provides three predefined sets of lexical
entailment rules acquired from lexical resources
widely used in RTE: WordNet4, Lin?s word sim-
ilarity dictionaries5, and VerbOcean6.
4http://wordnet.princeton.edu
5http://webdocs.cs.ualberta.ca/ lindek/downloads.htm
6http://demo.patrickpantel.com/Content/verbocean
<rule entailment="ENTAILMENT">
<t>acquire</t>
<h>own</h>
<probability>0.95</probability>
</rule>
<rule entailment="CONTRADICTION">
<t>beautiful</t>
<h>ugly</h>
<probability>0.88</probability>
</rule>
Figure 3: Example of XML Rule Repository
4 Using the System
This section provides basic information about the
use of EDITS, which can be run with commands
in a Unix Shell. A complete guide to all the pa-
rameters of the main script is available as HTML
documentation downloadable with the package.
4.1 Input
The input of the system is an entailment corpus
represented in the EDITS Text Annotation Format
(ETAF), a simple XML internal annotation for-
mat. ETAF is used to represent both the input T-H
pairs, and the entailment and contradiction rules.
ETAF allows to represent texts at two different
levels: i) as sequences of tokens with their asso-
ciated morpho-syntactic properties, or ii) as syn-
tactic trees with structural relations among nodes.
Plug-ins for several widely used annotation
tools (including TreeTagger, Stanford Parser, and
OpenNLP) can be downloaded from the system?s
website. Users can also extend EDITS by imple-
menting plug-ins to convert the output of other an-
notation tools in ETAF.
Publicly available RTE corpora (RTE 1-3, and
EVALITA 2009), annotated in ETAF at both the
annotation levels, are delivered together with the
system to be used as first experimental datasets.
4.2 Configuration
The creation of an Entailment Engine is done by
defining its basic components (algorithms, cost
schemes, optimizer, and rules) through an XML
configuration file. The configuration file is divided
in modules, each having a set of options. The fol-
lowing XML fragment represents a simple exam-
ple of configuration file:
<module alias="distance">
<module alias="tree"/>
<module alias="xml">
<option name="scheme-file"
45
value="IDF_Scheme.xml"/>
</module>
<module alias="pso"/>
</module>
This configuration defines a distance Entailment
Engine that combines Tree Edit Distance as a core
distance algorithm, and the predefined IDF Cost
Scheme that will be optimized on training data
with the Particle Swarm Optimization algorithm
(?pso?) as in (Mehdad, 2009). Adding external
knowledge to an entailment engine can be done by
extending the configuration file with a reference to
a rules file (e.g. ?rules.xml?) as follows:
<module alias="rules">
<option name="rules-file"
value="rules.xml"/>
</module>
4.3 Training and Test
Given a configuration file and an RTE corpus an-
notated in ETAF, the user can run the training
procedure to learn a model. At this stage, ED-
ITS allows to tune performance along several di-
mensions (e.g. overall Accuracy, Precision/Recall
trade-off on YES and/or NO entailment judge-
ments). By default the system maximizes the over-
all accuracy (distinction between YES and NO
pairs). The output of the training phase is a model:
a zip file that contains the learned threshold, the
configuration file, the cost scheme, and the en-
tailment/contradiction rules used to calculate the
threshold. The explicit availability of all this in-
formation in the model allows users to share, repli-
cate and modify experiments7.
Given a model and an un-annotated RTE corpus
as input, the test procedure produces a file con-
taining for each pair: i) the decision of the system
(YES, NO), ii) the confidence of the decision, iii)
the entailment score, iv) the sequence of edit oper-
ations made to calculate the entailment score.
4.4 Combining Engines
A relevant feature of EDITS is the possibility to
combine multiple Entailment Engines into a sin-
gle one. This can be done by grouping their def-
initions as sub-modules in the configuration file.
EDITS allows users to define customized combi-
nation strategies, or to use two predefined com-
bination modalities provided with the package,
7Our policy is to publish online the models we use for par-
ticipation in the RTE Challenges. We encourage other users
of EDITS to do the same, thus creating a collaborative envi-
ronment, allow new users to quickly modify working config-
urations, and replicate results.
Figure 4: Combined Entailment Engines
namely: i) Linear Combination, and ii) Classi-
fier Combination. The two modalities combine in
different ways the entailment scores produced by
multiple independent engines, and return a final
decision for each T-H pair.
Linear Combination returns an overall entail-
ment score as the weighted sum of the entailment
scores returned by each engine:
scorecombination =
n?
i=0
scorei ? weighti (1)
In this formula, weighti is an ad-hoc weight
parameter for each entailment engine. Optimal
weight parameters can be determined using the
same optimization strategy used to optimize the
cost schemes, as described in Section 3.3.
Classifier Combination is similar to the ap-
proach proposed in (Malakasiotis and Androut-
sopoulos, 2007), and is based on using the entail-
ment scores returned by each engine as features to
train a classifier (see Figure 4). To this aim, ED-
ITS provides a plug-in that uses the Weka8 ma-
chine learning workbench as a core. By default
the plug-in uses an SVM classifier, but other Weka
algorithms can be specified as options in the con-
figuration file.
The following configuration file describes a
combination of two engines (i.e. one based on
Tree Edit Distance, the other based on Cosine
Similarity), used to train a classifier with Weka9.
<module alias="weka">
<module alias="distance">
<module alias="tree"/>
<module alias="xml">
<option name="scheme-file"
value="IDF_Scheme.xml"/>
</module>
</module>
8http://www.cs.waikato.ac.nz/ml/weka
9A linear combination can be easily obtained by changing
the alias of the highest-level module (?weka?) into ?linear?.
46
<module alias="distance">
<module alias="cosine"/>
<module alias="IDF_Scheme.xml"/>
</module>
</module>
5 Experiments with EDITS
To give an idea of the potentialities of the ED-
ITS package in terms of flexibility and adaptabil-
ity, this section reports some results achieved in
RTE-related tasks by previous versions of the tool.
The system has been tested in different scenarios,
ranging from the evaluation of standalone systems
within task-specific RTE Challenges, to their inte-
gration in more complex architectures.
As regards the RTE Challenges, in the last
years EDITS has been used to participate both in
the PASCAL/TAC RTE Campaigns for the En-
glish language (Mehdad et al, 2009), and in the
EVALITA RTE task for Italian (Cabrio et al,
2009). In the last RTE-5 Campaign the result
achieved in the traditional ?2-way Main task?
(60.17% Accuracy) roughly corresponds to the
performance of the average participating systems
(60.36%). In the ?Search? task (which consists in
finding all the sentences that entail a given H in
a given set of documents about a topic) the same
configuration achieved an F1 of 33.44%, rank-
ing 3rd out of eight participants (average score
29.17% F1). In the EVALITA 2009 RTE task,
EDITS ranked first with an overall 71.0% Accu-
racy. To promote the use of EDITS and ease ex-
perimentation, the complete models used to pro-
duce each submitted run can be downloaded with
the system. An improved model obtained with the
current release of EDITS, and trained over RTE-5
data (61.83% Accuracy on the ?2-way Main task?
test set), is also available upon download.
As regards application-oriented integrations,
EDITS has been successfully used as a core com-
ponent in a Restricted-Domain Question Answer-
ing system within the EU-Funded QALL-ME
Project10. Within this project, an entailment-based
approach to Relation Extraction has been defined
as the task of checking for the existence of en-
tailment relations between an input question (the
text in RTE parlance), and a set of textual realiza-
tions of domain-specific binary relations (the hy-
potheses in RTE parlance). In recognizing 14 re-
lations relevant in the CINEMA domain present in
a collection of spoken English requests, the system
10http://qallme.fbk.eu
achieved an F1 of 72.9%, allowing to return cor-
rect answers to 83% of 400 test questions (Negri
and Kouylekov, 2009).
6 Conclusion
We have presented the first open source package
for recognizing Textual Entailment. The system
offers a modular, flexible, and adaptable working
environment to experiment with the task. In addi-
tion, the availability of pre-defined system config-
urations, tested in the past Evaluation Campaigns,
represents a first contribution to set up a collabo-
rative environment, and promote advances in RTE
research. Current activities are focusing on the de-
velopment of a Graphical User Interface, to further
simplify the use of the system.
Acknowledgments
The research leading to these results has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der Grant Agreement n. 248531 (CoSyne project).
References
Prodromos Malakasiotis and Ion Androutsopoulos
2007. Learning Textual Entailment using SVMs and
String Similarity Measures. Proc. of the ACL ?07
Workshop on Textual Entailment and Paraphrasing.
Ido Dagan and 0ren Glickman 2004. Probabilistic
Textual Entailment: Generic Applied Modeling of
Language Variability. Proc. of the PASCAL Work-
shop on Learning Methods for Text Understanding
and Mining.
Kaizhong Zhang and Dennis Shasha 1990. Fast Al-
gorithm for the Unit Cost Editing Distance Between
Trees. Journal of Algorithms. vol.11.
Yashar Mehdad 2009. Automatic Cost Estimation for
Tree Edit Distance Using Particle Swarm Optimiza-
tion. Proc. of ACL-IJCNLP 2009.
Matteo Negri and Milen Kouylekov 2009. Question
Answering over Structured Data: an Entailment-
Based Approach to Question Analysis. Proc. of
RANLP-2009.
Elena Cabrio, Yashar Mehdad, Matteo Negri, Milen
Kouylekov, and Bernardo Magnini 2009. Rec-
ognizing Textual Entailment for Italian EDITS @
EVALITA 2009 Proc. of EVALITA 2009.
Yashar Mehdad, Matteo Negri, Elena Cabrio, Milen
Kouylekov, and Bernardo Magnini 2009. Recogniz-
ing Textual Entailment for English EDITS @ TAC
2009 To appear in Proceedings of TAC 2009.
47
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 202?205,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
FBK NK: a WordNet-based System
for Multi-Way Classification of Semantic Relations
Matteo Negri and Milen Kouylekov
FBK-Irst
Trento, Italy
{negri,kouylekov}@fbk.eu
Abstract
We describe a WordNet-based system for
the extraction of semantic relations be-
tween pairs of nominals appearing in
English texts. The system adopts a
lightweight approach, based on training
a Bayesian Network classifier using large
sets of binary features. Our features con-
sider: i) the context surrounding the an-
notated nominals, and ii) different types
of knowledge extracted from WordNet, in-
cluding direct and explicit relations be-
tween the annotated nominals, and more
general and implicit evidence (e.g. seman-
tic boundary collocations). The system
achieved a Macro-averaged F1 of 68.02%
on the ?Multi-Way Classification of Se-
mantic Relations Between Pairs of Nom-
inals? task (Task #8) at SemEval-2010.
1 Introduction
The ?Multi-Way Classification of Semantic Re-
lations Between Pairs of Nominals? task at
SemEval-2010 (Hendrickx et al, 2010) consists
in: i) selecting from an inventory of nine possi-
ble relations the one that most likely holds be-
tween two annotated nominals appearing in the in-
put sentence, and ii) specifying the order of the
nominals as the arguments of the relation. In con-
trast with the semantic relations classification task
(Task #4) at SemEval-2007 (Girju et al, 2007),
which treated each semantic relation separately as
a single two-class (positive vs. negative) classifi-
cation task, this year?s edition of the challenge pre-
sented participating systems with a more difficult
and realistic multi-way setup, where the relation
Other can also be assigned if none of the nine re-
lations is suitable for a given sentence. Examples
of the possible markable relations are reported in
Table 1
1
.
The objective of our experiments with the pro-
posed task is to develop a Relation Extraction sys-
tem based on shallow linguistic processing, taking
the most from available thesauri and ontologies.
As a first step in this direction, our submitted runs
have been obtained by processing the input sen-
tences only to lemmatize their terms, and by using
WordNet as the sole source of knowledge.
Similar to other approaches (Moldovan and
Badulescu, 2009; Beamer et al, 2009), our sys-
tem makes use of semantic boundaries extracted
from the WordNet IS-A backbone. Such bound-
aries (i.e. divisions in the WordNet hierarchy
that best generalize over the training examples)
are used to define pairs of high-level synsets with
high correlation with specific relations. For in-
stance, <microorganism#1, happening#1> and
<writing#1, consequence#1> are extracted from
the training data as valid high-level collocations
respectively for the relations Cause-Effect and
Message-Topic. Besides exploiting the Word-
Net IS-A hierarchy, the system also uses the
holo-/meronymy relations, and information de-
rived from the WordNet glosses to capture specific
relations such as Member-Collection and Product-
Producer. In addition, the context surrounding
the annotated nominals is represented as a bag-of-
words/synonyms to enhance the relation extraction
process. Several experiments have been carried
out encoding all the information as large sets of
binary features (up to ?6200) to train a Bayesian
Network classifier available in the Weka
2
toolkit.
To capture both the relations and the order of
1
In the first example the order of the nominals is
(<e2>,<e1>), while in the others is (<e1>,<e2>)
2
http://www.cs.waikato.ac.nz/ml/weka/
202
1 Cause-Effect(e2,e1) A person infected with a particular <e1>flu</e1> <e2>virus</e2> strain develops an
antibody against that virus.
2 Instrument-Agency(e1,e2) The <e1>river</e1> once powered a <e2>grist mill</e2>.
3 Product-Producer(e1,e2) The <e1>honey</e1><e2>bee</e2> is the third insect genome published by scientists,
after a lab workhorse, the fruit fly, and a health menace, the mosquito.
4 Content-Container(e1,e2) I emptied the <e1>wine</e1> <e2>bottle</e2> into my glass and toasted my friends.
5 Entity-Origin(e1,e2) <e1>This book</e1>is from the 17th <e2>century</e2>.
6 Entity-Destination(e1,e2) <e1>Suspects</e1> were handed over to the <e2>police station</e2>.
7 Component-Whole(e1,e2) <e1>Headlights</e1> are considered as the eyes of the <e2>vehicle</e2>.
8 Member-
Collection(e1,e2)
Mary looked back and whispered: ?I know every <e1>tree</e1> in this
<e2>forest</e2>, every scent?.
9 Message-Topic(e1,e2) Here we offer a selection of our favourite <e1>books</e1> on military
<e2>history</e2>.
Table 1: SemEval-2010 Task #8 semantic relations.
their arguments, training sentences having oppo-
site argument directions for the same relation have
been handled separately, and assigned to different
classes (thus obtaining 18 classes for the nine tar-
get relations, plus one for the Other relation).
The following sections overview our experi-
ments, describing the features used by the sys-
tem (Section 2), and the submitted runs with the
achieved results (Section 3). A concluding discus-
sion on the results is provided in Section 4.
2 Features used
The system uses two types of boolean features:
WordNet features, and context features.
2.1 WordNet features
WordNet features consider different types of
knowledge extracted from WordNet 3.0.
Semantic boundary collocations. Collocations
of high-level synsets featuring a high correlation
with specific relations are acquired from the train-
ing set using a bottom-up approach. Starting from
the nominals annotated in the training sentences
(<e1> and<e2>), the WordNet IS-A backbone is
climbed to collect all their ancestors. Then, all the
ancestors? collocations occurring at least n times
for at most m relations are retained, and treated
as boolean features (set to 1 for a given sentence
if its annotated nominals appear among their hy-
ponyms). The n and m parameters are optimized
on the training set.
Holo-/meronymy relations. These boolean fea-
tures are set to 1 every time a pair of annotated
nominals in a sentence is directly connected by
holo-/meronyny relations. They are particularly
appropriate to capture the Component-Whole and
Member-Collection relations, as in the 8th exam-
ple in Table 1 (where tree#1 is an holonym of
forest#1). Due to time constraints, we did not
explore the possibility to generalize these fea-
tures considering transitive closures of the nomi-
nals? hypo-/hypernyms. This possibility could al-
low to handle sentences like ?A <e1>herd</e1>
is a large group of <e2>animals</e2>.? Here,
though herd#1 and animal#1 are not directly con-
nected by the meronymy relation, all the herd#1
meronyms have animal#1 as a common ancestor.
Glosses. Given a pair of annotated nominals
<e1>,<e2>, these features are set to 1 every time
either <e1> appears in the gloss of <e2>, or
vice-versa. They are intended to support the dis-
covery of relations in the case of consecutive nom-
inals (e.g. honey#1 and bee#1 in the 3rd example
in Table 1), where contextual information does not
provide sufficient clues to make a choice. In our
experiments we extracted features from both tok-
enized and lemmatized words (both nominals, and
gloss words). Also in this case, due to time con-
straints we did not explore the possibility to gener-
alize the feature considering the nominals? hypo-
/hypernyms. This possibility could allow to handle
sentences like examples 1 and 4 in Table 1. For
instance in example 4, the gloss of ?bottle? con-
tains two hypernyms of wine#1, namely drink#3
and liquid#1, that could successfully trigger the
Content-Container relation.
Synonyms. While the previous features operate
with the annotated nominals, WordNet synonyms
are used to generalize the other terms in the sen-
tence, allowing to extract different types of con-
textual features (see the next Section).
2.2 Context features
Besides the annotated nominals, also specific
words (and word combinations) appearing in the
surrounding context often contribute to trigger the
203
target relations. Distributional evidence is cap-
tured by considering word contexts before, be-
tween, and after the annotated nominals. To this
aim, we experimented with windows of different
size, containing words that occur in the training
set a variable number of times. Both the parame-
ters (i.e. the size of the windows, and the number
of occurrences) are optimized on training data. In
our experiments we extracted contextual features
from lemmatized sentences.
3 Submitted runs and results
Our participation to the SemEval-2010 Task
#8 consisted in four runs, with the best one
(FBK NK-RES1) achieving a Macro-averaged F1
of 68.02% on the test data. For this submis-
sion, the overall training and test running times are
about 12?30? and 1?30? respectively, on an Intel
Core2 Quad 2.66GHz with 4GB RAM.
FBK NK-RES1. This run has been obtained
adopting a conservative approach, trying to min-
imize the risk of overfitting the training data. The
features used can be summarized as follows:
? Semantic boundary collocations: all the col-
locations of <e1> and <e2> ancestors oc-
curring at least 10 times in the training set (m
param.), for at most 3 relations (n param.);
? Holo-/meronymy relations between the anno-
tated nominals;
? Glosses: handled at the level of tokens;
? Context features: left, between, and right
context windows of size 3-ALL-3 words re-
spectively. Number of occurrences: 25 (left),
10 (between), 25 (right).
On the training set, the Bayesian Network classi-
fier (trained with 2239 features, and evaluated with
10-fold cross-validation) achieves an Accuracy of
65.62% (5249 correctly classified instances out of
8000), and a Macro F1 of 78.15%.
FBK NK-RES2. Similar to the first run, but:
? Semantic boundary collocations: m=9, n=3;
? Glosses: handled at the level of lemmas;
? Context features: left, between, and right
context windows of size 4-ALL-1 words re-
spectively (occurrences: 25-10-25).
Run 1000 2000 4000 8000
FBK NK-RES1 55.71 64.06 67.80 68.02
FBK NK-RES2 54.27 63.68 67.08 67.48
FBK NK-RES3 54.25 62.73 66.11 66.90
FBK NK-RES4 44.11 58.85 63.06 65.84
Table 2: Test results (Macro-averaged F1) using
different amounts of training sentences.
Based on the observation of system?s behaviour on
the training data, the objectives of this run were to:
i) add more collocations as features, ii) increase
the importance of terms appearing in the left con-
text, iii) reduce the importance of terms appearing
in the right context, and iv) increase the possibil-
ity of matching the nominals with gloss terms by
considering their respective lemmas. On the train-
ing set, the classifier (trained with 2998 features)
achieves 66.92% Accuracy (5353 correctly classi-
fied instances), and a Macro F1 of 79.56%.
FBK NK-RES3. Similar to the second run, but
considering the synonyms of the most frequent
sense of the words between <e1> and <e2>.
The goal of this run was to generalize the con-
text between nominals, by considering word lem-
mas. On the training set, the classifier (trained
with 2998 features) achieves an Accuracy of
64.94% (5195 correctly classified instances), and
a Macro F1 of 77.38%.
FBK NK-RES4. Similar to the second run, but
considering semantic boundary collocations oc-
curring at least 7 times in the training set (m
param.), for at most 3 relations (n param.).
The goal of this run was to further increase the
number of collocations used as features. On the
training set, the classifier (trained with 6233 fea-
tures) achieves achieves 68.12% Accuracy (5449
correct classifications), and 82.24% Macro F1.
As regards the results on the test set, Table 2 re-
ports the scores achieved by each run using differ-
ent portions of the training set (1000, 2000, 4000,
8000 examples), while Figure 1 shows the learn-
ing curves for each relation of our best run.
4 Discussion and conclusion
As can be seen from Table 2, the results contra-
dict our expectations about the effectiveness of our
less conservative configurations and, in particular,
about the utility of using larger amounts of se-
mantic boundary collocations. The performance
204
Figure 1: Learning curves on the test set
(FBK NK-RES1).
decrease from Run2 to Run4
3
clearly indicates an
overfitting problem. Though suitable to model the
training data, the additional collocations were not
encountered in the test set. This caused a bias to-
wards the Other relation, which reduced the over-
all performance of the system.
Regarding our best run, Figure 1 shows dif-
ferent system?s behaviours with the different tar-
get relations. For some of them (e.g. Entity-
Destination, Cause-Effect) better results are mo-
tivated by the fact that they are often triggered
by frequent unambiguous word patterns (e.g.
?<e1>has been moved to a <e2>?, ?<e1>
causes <e2>?). Such relations are effectively
handled by the context features which, in contrast,
are inadequate for those expressed with high lex-
ical variability. This is particularly evident with
the Other relation, for which the acquired context
features poorly discriminate positive from nega-
tive examples even on the training set.
For some relations additional evidence is suc-
cessfully brought by the WordNet features. For
instance, the good results for Member-Collection
demonstrate the usefulness of the holo-/meronymy
features.
As regards semantic boundary collocations, to
check their effectiveness we performed a post-hoc
analysis of those used in our best run. Such anal-
ysis was done in two ways: i) by counting the
number of collocations acquired on the training
set for each relation R
i
, and ii) by calculating the
ambiguity of each R
i
?s collocation on the train-
3
The only difference between Run2 and Run4 is the addi-
tion of around 4000 semantic boundary collocations, which
lead to an overall 2.4% F1 performance decrease. The de-
crease mainly comes in terms of Recall (from 65.91% in
Run2 to 63.35% in Run4).
ing set (i.e. the average number of other relations
activated by the collocation). The analysis re-
vealed that the top performing relations (Member-
Collection, Entity-Destination, Cause-Effect, and
Content-Container) are those for which we ac-
quired lots of unambiguous collocations. These
findings also explain the poor performance on the
Instrument-Agency and the Other relation. For
Instrument-Agency we extracted the lowest num-
ber of collocations, which were also the most am-
biguous ones. For the Other relation the high am-
biguity of the collocations extracted is not com-
pensated by their huge number (around 50% of the
total collocations acquired).
In conclusion, considering i) the level of pro-
cessing required (only lemmatization), ii) the fact
that WordNet is used as the sole source of knowl-
edge, and iii) the many possible solutions left
unexplored due to time constraints, our results
demonstrate the validity of our approach, de-
spite its simplicity. Future research will focus
on a better use of semantic boundary colloca-
tions, on more refined ways to extract knowledge
from WordNet, and on integrating other knowl-
edge sources (e.g. SUMO, YAGO, Cyc).
Acknowledgments
The research leading to these results has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der Grant Agreement n. 248531 (CoSyne project).
References
B. Beamer, A. Rozovskaya, and R. Girju 2008. Au-
tomatic Semantic Relation Extraction with Multiple
Boundary Generation. Proceedings of The National
Conference on Artificial Intelligence (AAAI).
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret 2007. SemEval-2007 task 04:
Classification of semantic relations between nomi-
nals. Proceedings of the 4th Semantic Evaluation
Workshop (SemEval-2007).
I. Hendrickx et al 2010. SemEval-2010 Task 8: Multi-
Way Classification of Semantic Relations Between
Pairs of Nominals Proceedings of the 5th SIGLEX
Workshop on Semantic Evaluation.
D. Moldovan, A. Badulescu 2005. A Semantic Scatter-
ing Model for the Automatic Interpretation of Gen-
itives. Proceedings of The Human Language Tech-
nology Conference (HLT).
205
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 696?700,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
CELI: An Experiment with Cross Language Textual Entailment
Milen Kouylekov
Celi S.R.L.
via San Quintino 31
Torino, Italy
kouylekov@celi.it
Luca Dini
Celi S.R.L.
via San Quintino 31
Torino, Italy
dini@celi.it
Alessio Bosca
Celi S.R.L.
via San Quintino 31
Torino, Italy
bosca@celi.it
Marco Trevisan
Celi S.R.L.
via San Quintino 31
Torino, Italy
trevisan@celi.it
Abstract
This paper presents CELI?s participation in the
SemEval Cross-lingual Textual Entailment for
Content Synchronization task.
1 Introduction
The Cross-Lingual Textual Entailment task (CLTE)
is a new task that addresses textual entailment (TE)
(Bentivogli et. al., 2011), targeting the cross-
lingual content synchronization scenario proposed
in (Mehdad et. al., 2011) and (Negri et. al., 2011).
The task has interesting application scenarios that
can be investigated. Some of them are content syn-
chronization and cross language query alignment.
The task is defined by the organizers as follows:
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgments:
? Bidirectional: the two fragments entail each
other (semantic equivalence)
? Forward: unidirectional entailment from T1 to
T2
? Backward: unidirectional entailment from T2
to T1
? No Entailment: there is no entailment between
T1 and T2
In this task, both T1 and T2 are assumed to be
TRUE statements; hence in the dataset there are no
contradictory pairs.
Example for Spanish English pairs:
? bidirectional
Mozart naci en la ciudad de Salzburgo
Mozart was born in Salzburg.
? forward
Mozart naci en la ciudad de Salzburgo
Mozart was born on the 27th January 1756 in
Salzburg.
? backward
Mozart naci el 27 de enero de 1756 en
Salzburgo
Mozart was born in 1756 in the city of Salzburg
? no entailment
Mozart naci el 27 de enero de 1756 en
Salzburgo
Mozart was born to Leopold and Anna Maria
Pertl Mozart.
2 Our Approach to CLTE
In our participation in the 2012 SemEval Cross-
lingual Textual Entailment for Content Synchroniza-
tion task (Negri et. al., 2012) we have developed
an approach based on cross-language text similarity.
We have modified our cross-language query similar-
ity system TLike to handle longer texts.
Our approach is based on four main resources:
? A system for Natural Language Processing able
to perform for each relevant language basic
tasks such as part of speech disambiguation,
lemmatization and named entity recognition.
? A set of word based bilingual translation mod-
ules.
696
? A semantic component able to associate a se-
mantic vectorial representation to words.
? We use Wikipedia as multilingual corpus.
NLP modules are described in (Bosca and Dini,
2008), and will be no further detailed here.
Word-based translation modules are composed by
a bilingual lexicon look-up component coupled with
a vector based translation filter, such as the one de-
scribed in (Curtoni and Dini, 2008). In the context of
the present experiments, such a filters has been de-
activated, which means that for any input word the
component will return the set of all possible transla-
tions. For unavailable pairs, we make use of trian-
gular translation (Kraaij, 2003).
As for the semantic component we experimented
with a corpus-based distributional approach capable
of detecting the interrelation between different terms
in a corpus; the strategy we adopted is similar to La-
tent Semantic Analysis (Deerwester et. al., 1990)
although it uses a less expensive computational solu-
tion based on the Random Projection algorithm (Lin
et. al., 2003) and (Bingham et. al., 2001). Different
works debate on similar issues: (Turney, 2001) uses
LSA in order to solve synonymy detection questions
from the well-known TOEFL test while the method
presented by (Inkpen, 2001) or by (Baroni and Bisi,
2001) proposes the use of the Web as a corpus to
compute mutual information scores between candi-
date terms.
More technically, Random Indexing exploits an
algebraic model in order to represent the seman-
tics of terms in a Nth dimensional space (a vector
of length N); approaches falling into this category,
actually create a Terms By Contexts matrix where
each cell represents the degree of memberships of a
given term to the different contexts. The algorithm
assigns a random signature to each context (a highly
sparse vector of length N , with few, randomly cho-
sen, non-zero elements) and then generates the vec-
tor space model by performing a statistical analysis
of the documents in the domain corpus and by ac-
cumulating on terms rows all the signatures of the
contexts where terms appear.
According to this approach if two different terms
have a similar meaning they should appear in similar
contexts (within the same documents or surrounded
by the same words), resulting into close coordinates
in the so generated semantic space.
In our case study semantic vectors have been gen-
erated taking as corpus the set of metadata available
via the CACAO project (Cacao Project, 2007) fed-
eration (about 6 millions records). After processing
for each word in the corpus we have:
? A vector of float from 0 to 1 representing its
contextual meaning;
? A set of neighbors terms selected among the
terms with a higher semantic similarity, calcu-
lated as cosine distance among vectors.
We use Wikipedia as a corpus for calculating
word statistics in different languages. We have in-
dexed using Lucene1 the English, Italian, French,
German, Spanish distributions of the resource.
The basic idea behind our algorithm is to detect
the probability for two texts to be one a translation
of the other. In the simple case we expect that if all
the words in text TS have a translation in text TT and
if TS and TT have the same number of terms, then
TS and TT are entailed. Things are of course more
complex than this, due to the following facts:
? The presence of compound words make the
constraints on cardinality of search terms not
feasible (e.g. the Italian Carta di Credito vs.
the German KreditCarte).
? One or more words in TS could be absent from
translation dictionaries.
? One or more words in TS could be present
in the translation dictionaries, but contextually
correct translation might be missing.
? There might be items which do not need to be
translated, notably Named Entities.
The first point, compounding, is only partially
an obstacle. NLP technology developed during
CACAO Project, which adopted translation dictio-
naries, deals with compound words both in terms
of identification and translation. Thus the Italian
?Carta di Credito? would be recognized and cor-
rectly translated into ?KreditCarte?. So, in an ideal
1http://lucene.apache.org
697
word, the cardinality principle could be considered
strict. In reality, however, there are many com-
pounding phenomena which are not covered by our
dictionaries, and this forces us to consider that a mis-
match in text term cardinality decrease the probabil-
ity that the two translations are translation of each
other, without necessarily setting it to zero.
Concerning the second aspect, the absence of
source (T1) words in translation dictionaries, it is
dealt with by accessing the semantic repository de-
scribed in the previous section. We first obtain the
list of neighbor terms for the untranslatable source
word. This list is likely to contain many words that
have one or more translations. For each translation,
again, we consult our semantic repository and we
obtain its semantic vector.
Finally, we compose all vectors of all available
translations and we search in the target text (T2) for
the word whose semantic vector best matches the
composed one (cosine distance). Of course we can-
not assume that the best matching vector is a transla-
tion of the original word, but we can use the distance
between the two vectors as a further weight for de-
ciding whether the two texts are translations one of
the other.
There are of course cases when the source word
is correctly missing in the source dictionary. This
is typically the case of most named entities, such
as geographical and person names. These entities
should be appropriately recognized and searched as
exact matches in the target text, thus by-passing any
dictionary look-up and any semantic based match-
ing. Notice that the recognition of named entities
it is not just a matter of generalizing the statement
according to which ?if something is not in the dic-
tionaries, it is a named entity?. Indeed there are well
known cases where the named entity is homograph
with common words (e.g. the French author ?La
Fontaine?), and in these cases we must detect them
in order to avoid the rejection of likely translation
pairs. In other words we must avoid that the two
texts ?La fontaine fables? and ?La Fontaine fav-
ole? are rejected as translation pairs, just by virtue
of the fact that ?La fontaine? is treated as a com-
mon word, thus generating the Italian translation?La
fontana?. Fortunately CACAO disposes of a quite
accurate subsystem for recognizing named entities
in texts, mixing standard NLP technologies with sta-
tistical processing and other corpus-oriented heuris-
tics.
We concentrated our work on handling cases
where two texts are candidates to be mutual trans-
lations, but one or more words receive a translation
which is not contained in the target text. Typically
these cases are a symptom of a non-optimal quality
in translation dictionaries: the lexicographer prob-
ably did not consider some translation candidate.
To address this problem we have created a solution
based on a weighting scheme. For each word of the
source language we assign a weight that reflects its
importance to the semantic interpretation of the text.
We define a matchweight of a word using the for-
mula represented in Figure 2.In this formula wis is
a word from the source text, wkt is a word from the
target text, w is a word in the source language and
trans is a boolean function that searches in the dic-
tionary for translations between two words.
The matchweight is relevant to the matching of a
translation of a word from the source with one of
the words of the target. If the system finds a direct
correspondence the weight is 0. If the match was
made using random indexing the weight is inverse
to the cosine similarity between the vectors.
In order to make an approximation of the signif-
icance of the word to the meaning of the phrase we
have used as its cost the inverse document frequency
(IDF) of the word calculated using Wikipedia as a
corpus. IDF is a most popular measure (a measure
commonly used in Information Retrieval) for calcu-
lating the importance of a word to a text. If N is the
number of documents in a text collection and Nw is
the number of documents of the collection that con-
tain w then the IDF of w is given by the formula:
weight(wis) = idf(w) = log(
N
Nw
) (2)
Using the matchweight and weight we define the
matchscore of a source target pair as:
matchscore(Ts, Tt) =
?
matchweigth(wis)
?
weight(wis)
(3)
If all the words of the source text have a transla-
tion in the target text the score is 0. If none is found
the score is 1. We have calculated the scores for each
698
matchweight(wis) =
?
??
??
0 ?wkt trans(wis) = wkt
w ? (wis) ? (1? d) ?w &wkt distance(wis, w) = d&trans(w) = wkt
w ? (wis) otherwise
(1)
Figure 1: Match Weight of a Word
pair taking t1 as a source and t2 as a target and vice
versa.
3 Systems
We have submitted four runs in the SemEval CLTE
challenge. We used the NaiveBayse algorithm im-
plemented in Mallet2 to create a classifier that will
produce the output for each of the four categories
Forward , Backward , Bidirectional and No Entail-
ment.
System 1 As our first system we have created a
binary classifier in the classical RTE (Bentivogli et.
al., 2011) classification (YES & NO) for each direc-
tion Forward and Backward. We assigned the Bidi-
rectional category if both classifiers returned YES.
As features the classifiers used only the match scores
obtained for the corresponding direction as one and
only numeric feature.
System 2 For the second system we trained a clas-
sifier using all four categories as output. Apart of the
scores obtained matching the texts in both directions
we have included also a set of eight simple surface
measures. Some of these are:
? The length of the two texts.
? The number of common words without transla-
tions.
? The cosine similarity between the tokens of the
two texts without translation.
? Levenshtein distance between the texts.
System 3 For the third system we trained a classi-
fier using all four categories as output. We used as
features scores obtained matching the texts in both
directions without the surface features used in the
System 2.
2http://mallet.cs.umass.edu/
System 4 In the last system we trained a classifier
using all four categories as output. We used as fea-
tures the simple surface measures used in System 2.
The results obtained are shown in Table 1.
4 Analysis
Analyzing the results of our participation we have
reached several important conclusions.
The dataset provided by the organizers presented
a significant challenge for our system which was
adapted from a query similarity approach. The re-
sults obtained demonstrate that only a similarity
based approach will not provide good results for this
task. This fact is also confirmed by the poor perfor-
mance of the simple similarity measures by them-
selves (System 4) and by their contribution to the
combined run (System 2).
The poor performance of our system can be par-
tially explained also by the small dimensions of the
cross-language dictionaries we used. Expanding
them with more words and phrases can potentially
increase our results.
The classifier with four categories clearly outper-
forms the two directional one (System 1 vs. System
3).
Overall we are not satisfied with our experi-
ment. A radically new approach is needed to address
the problem of Cross-Language Textual Entailment,
which our similarity based system could not model
correctly.
In the future we intend to integrate our approach
in our RTE open source system EDITS (Kouylekov
et. al., 2011) (Kouylekov and Negri, 2010) available
at http://edits.sf.net.
Acknowledgments
This work has been partially supported by the
ECfunded project Galateas (CIP-ICT PSP-2009-3-
250430).
699
SPA-ENG ITA-ENG FRA-ENG DEU-ENG
System 1 0.276 0.278 0.278 0.280
System 2 0.336 0.336 0.300 0.352
System 3 0.322 0.334 0.298 0.350
System 4 0.268 0.280 0.280 0.274
Table 1: Results obtained.
References
Baroni M., Bisi S. 2004. Using cooccurrence statistics
and the web to discover synonyms in technical lan-
guage In Proceedings of LREC 2004
Bentivogli L., Clark P., Dagan I., Dang H, Giampic-
colo D. 2011. The Seventh PASCAL Recognizing
Textual Entailment Challenge In Proceedings of TAC
2011
Bingham E., Mannila H. 2001. Random projection in
dimensionality reduction: Applications to image and
text data. In Knowledge Discovery and Data Mining,
ACM Press pages 245250
Bosca A., Dini L. 2008. Query expansion via library
classification system. In CLEF 2008. Springer Verlag,
LNCS
Cacao Project CACAO - project supported by the eCon-
tentplus Programme of the European Commission.
http://www.cacaoproject.eu/
Curtoni P., Dini L. 2006. Celi participation at clef 2006
Cross language delegated search. In CLEF2006 Work-
ing notes.
Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K.,
Harshman R. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science 41 391407
Inkpen D. 2007. A statistical model for near-synonym
choice. ACM Trans. Speech Language Processing
4(1)
Kraaij W. 2003. Exploring transitive translation meth-
ods. In Vries, A.P.D., ed.: Proceedings of DIR 2003.
Kouylekov M., Negri M. An Open-Source Package for
Recognizing Textual Entailment. 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010) ,Uppsala, Sweden. July 11-16, 2010
Kouylekov M., Bosca A., Dini L. 2011. EDITS 3.0 at
RTE-7. Proceedings of the Seventh Recognizing Tex-
tual Entailment Challenge (2011).
Lin J., Gunopulos D. 2003. Dimensionality reduction
by random projection and latent semantic indexing. In
proceedings of the Text Mining Workshop, at the 3rd
SIAM International Conference on Data Mining.
Mehdad Y.,Negri M., Federico M.. 2011. Using Paral-
lel Corpora for Cross-lingual Textual Entailment. In
Proceedings of ACL-HLT 2011.
Negri M., Bentivogli L., Mehdad Y., Giampiccolo D.,
Marchetti A. 2011. Divide and Conquer: Crowd-
sourcing the Creation of Cross-Lingual Textual Entail-
ment Corpora. In Proceedings of EMNLP 2011.
Negri M., Marchetti A., Mehdad Y., Bentivogli L., Gi-
ampiccolo D. Semeval-2012 Task 8: Cross-lingual
Textual Entailment for Content Synchronization. In
Proceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012). 2012.
Turney P.D. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl. In EMCL 01: Proceedings of
the 12th European Conference on Machine Learning,
London, UK, Springer-Verlag pages 491502
700
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 592?597, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Celi: EDITS and Generic Text Pair Classification
Milen Kouylekov
Celi S.R.L.
via San Quintino 31
Torino,10121, Italy
kouylekov@celi.it
Luca Dini
Celi S.R.L.
via San Quintino 31
Torino,10121, Italy
dini@celi.it
Alessio Bosca
Celi S.R.L.
via San Quintino 31
Torino,10121, Italy
alessio.bosca@celi.it
Marco Trevisan
Celi S.R.L.
via San Quintino 31
Torino, Italy
trevisan@celi.it
Abstract
This paper presents CELI?s participation in the
SemEval The Joint Student Response Anal-
ysis and 8th Recognizing Textual Entailment
Challenge (Task7) and Cross-lingual Textual
Entailment for Content Synchronization task
(Task 8).
1 Introduction
Recognizing an existing relation between two text
fragments received a significant interest as NLP task
in the recent years. A lot of the approaches were
focused in the filed of Textual Entailment(TE). TE
has been proposed as as a comprehensive frame-
work for applied semantics (Dagan and Glickman,
2004), where the need for an explicit mapping be-
tween linguistic objects can be, at least partially,
bypassed through the definition of semantic infer-
ences at the textual level. In the TE framework, a
text (T ) is said to entail the hypothesis (H) if the
meaning of H can be derived from the meaning of
T . Initially defined as binary relation between texts
(YES/NO there is an entailment or there is not) the
TE evolved in the third RTE3 (Giampiccolo et al,
2007) challenge into a set of three relations between
texts: ENTAILMENT, CONTRADICTION and
UNKNOWN. These relations are interpreted as fol-
lows:
? ENTAILMENT - The T entails the H .
? CONTRADICTION - The H contradicts the T
? UNKNOWN - There is no semantic connection
between T and H .
With more and more applications available for
recognizing textual entailment the researches fo-
cused their efforts in finding practical applications
for the developed systems. Thus the Cross-Lingual
Textual Entailment task (CLTE) was created using
textual entailment (TE) to define cross-lingual con-
tent synchronization scenario proposed in (Mehdad
et. al., 2011), (Negri et. al., 2011) (Negri et. al.,
2012). The task is defined by the organizers as fol-
lows: Given a pair of topically related text fragments
(T1 and T2) in different languages, the CLTE task
consists of automatically annotating it with one of
the following entailment judgments:
? Bidirectional: the two fragments entail each
other (semantic equivalence)
? Forward: unidirectional entailment from T1 to
T2
? Backward: unidirectional entailment from T2
to T1
? No Entailment: there is no entailment between
T1 and T2
The textual entailment competition also evolved.
In this year SEMEVAL The Joint Student Response
Analysis and 8th Recognizing Textual Entailment
Challenge - JRSA-RTE8 (Task7) the textual entail-
ment was defined in three subtasks:
5-way task , where the system is required to clas-
sify the student answer according to one of the fol-
lowing judgments:
? Correct, if the student answer is a complete and
correct paraphrase of the reference answer;
592
? Partially correct incomplete, if the student an-
swer is a partially correct answer containing
some but not all information from the reference
answer;
? Contradictory, if the student answer explicitly
contradicts the reference answer;
? Irrelevant, if the student answer is ?irrelevant?,
talking about domain content but not providing
the necessary information;
? Non domain, if the student answer expresses a
request for help, frustration or lack of domain
knowledge - e.g., ?I don?t know?, ?as the book
says?, ?you are stupid?.
3-way task , where the system is required to clas-
sify the student answer according to one of the fol-
lowing judgments:
? correct
? contradictory
? incorrect, conflating the categories of par-
tially correct incomplete, irrelevant or
non domain in the 5-way classification
2-way task , where the system is required to clas-
sify the student answer according to one of the fol-
lowing judgments:
? correct
? incorrect, conflating the categories of contra-
dictory and incorrect in the 3-way classifica-
tion.
Following the overall trend, we have decided to
convert our system for recognizing textual entail-
ment EDITS from a simple YES/NO recognition
system into a generic system capable of recognizing
multiple semantic relationships between two texts.
EDITS (Kouylekov and Negri, 2010) and
(Kouylekov et. al., 2011) is an open source pack-
age for recognizing textual entailment, which offers
a modular, flexible, and adaptable working environ-
ment to experiment with the RTE task over different
datasets. The package allows to: i) create an en-
tailment engine by defining its basic components ii)
train such entailment engine over an annotated RTE
corpus to learn a model; and iii) use the entailment
engine and the model to assign an entailment judg-
ments and a confidence score to each pair of an un-
annotated test corpus.
We define the recognition of semantic relations
between two texts as a classification task. In this
task the system takes as an input two texts and clas-
sifies them in one of a set of predefined relations.
We have modified EDITS in order to handle the so
defined task.
Having this in mind we have participated in
JRSA-RTE8 (task 7) and CLTE2 (task 8) with the
same approach. We have merged EDITS with some
features from the TLike system described in our last
participation in CLTE (Kouylekov et. al., 2011). For
each of the tasks we have created a specialized com-
ponents that are integrated in EDITS as one of the
system?s modules.
2 EDITS and Generic Text Pair
Classification
As in the previous versions, the core of EDITS im-
plements a distance-based framework. Within this
framework the system implements and harmonizes
different approaches to distance computation be-
tween texts, providing both edit distance algorithms,
and similarity algorithms. Each algorithm returns
a normalized distance score (a number between 0
and 1). Each algorithm algorithm depends on two
generic modules defined by the system?s user:
? Matcher - a module that is used to align text
fragments. This module uses semantic tech-
niques and entailment rules to find equivalent
textfragments.
? Weight Calculator - a module that is used to
give weight to text fragments. The weights are
used to determine the importance of a text por-
tion to the overall meaning of the text.
In the previous versions of the system at the train-
ing stage, distance scores calculated over annotated
T-H pairs are used to estimate a threshold that best
separates positive (YES) from negative (NO) exam-
ples. The calculated threshold was used at a test
stage to assign an entailment judgment and a con-
fidence score to each test pair. In the new version
593
of the system we used a machine learning classifier
to classify the T-H pairs in the appropriate category.
The overall architecture of the system is shown in
Figure 1.
The new architecture is divided in two sets of
modules: Machine Learning and Edit Distance. In
the Edit Distance set various distance algorithms are
used to calculate the distance between the two texts.
Each of these algorithms have a custom matcher and
weight calculator. The distances calculated by each
of these algorithms are used as features for the clas-
sifiers of the Machine Leaning modules. The ma-
chine learning modules are structured in two levels:
? Binary Classifiers - for each semantic relation
we create a binary classifier that distinguishes
between the members of the relation and the
members of the other relations. For example:
For 3way task (Task 7) the system created 3 bi-
nary classifiers one for each relation.
? Classifier - a module that makes final decision
for the text pair taking the output (decision and
confidence) of the binary classifiers as an input.
We have experimented with other configurations
of the machine leaning modules and selected this
one as the best performing on the available datasets
of the previous RTE competitions. In the version
of EDITS avalble online other configurations of the
machine leaning modules will be available using the
flexibility of the system configuration.
We have used the algorithms implemented in
WEKA (Hall et al, 2009) for the classification mod-
ules. The binary modules use SMO algorithm. The
top classifier uses NaiveBayes.
The input to the system is a corpus of text pairs
each classified with one semantic relation. We have
used the format of the previous RTE competitions
in order to be compliant. The goal of the system is
to create classifier that is capable of recognizing the
correct relation for an un-annotated pair of texts.
The new version of EDITS package allows to:
? Create an Classifier by defining its basic com-
ponents (i.e. algorithms, matchers, and weight
calculators);
? Train such Classifier over an annotated corpus
(containing T-H pairs annotated in terms of en-
tailment) to learn a Model;
? Use the Classifier and the Model to assign an
entailment judgment and a confidence score to
each pair of an un-annotated test corpus.
3 Resources
Like our participation in the 2012 SemEval Cross-
lingual Textual Entailment for Content Synchroniza-
tion task (Kouylekov et. al., 2011), our approach is
based on four main resources:
? A system for Natural Language Processing able
to perform for each relevant language basic
tasks such as part of speech disambiguation,
lemmatization and named entity recognition.
? A set of word based bilingual translation mod-
ules.(Employed only for Task 8)
? A semantic component able to associate a se-
mantic vectorial representation to words.
? We use Wikipedia as multilingual corpus.
NLP modules are described in (Bosca and Dini,
2008), and will be no further detailed here.
Word-based translation modules are composed by
a bilingual lexicon look-up component coupled with
a vector based translation filter, such as the one de-
scribed in (Curtoni and Dini, 2008). In the context of
the present experiments, such a filters has been de-
activated, which means that for any input word the
component will return the set of all possible transla-
tions. For unavailable pairs, we make use of trian-
gular translation (Kraaij, 2003).
As for the semantic component we experimented
with a corpus-based distributional approach capable
of detecting the interrelation between different terms
in a corpus; the strategy we adopted is similar to La-
tent Semantic Analysis (Deerwester et. al., 1990)
although it uses a less expensive computational solu-
tion based on the Random Projection algorithm (Lin
et. al., 2003) and (Bingham et. al., 2001). Different
works debate on similar issues: (Turney, 2001) uses
LSA in order to solve synonymy detection questions
from the well-known TOEFL test while the method
presented by (Inkpen, 2001) or by (Baroni and Bisi,
2001) proposes the use of the Web as a corpus to
594
Figure 1: EDITS Architecture
compute mutual information scores between candi-
date terms.
We use Wikipedia as a corpus for calculating
word statistics in different languages. We have in-
dexed using Lucene1 the English, Italian, French,
German, Spanish distributions of the resource.
The semantic component and the translation2
modules are used as core components in the matcher
module. IDF calculated on Wikipedia is used as
weight for the words by the weight calculator model.
4 JRSA-RTE8
In the JRSA-RTE8 we consider the reference an-
swers as T (text) and the student answer as H (hy-
pothesis). As the reference answers are often more
than one, we considered as input to the machine
learning algorithms the distance between the student
answer and the closest reference answer. We define
the closest reference answer as the reference answer
with minimum distance according to the distance al-
gorithm.
1http://lucene.apache.org
2Translation module is used only for Task 8.
4.1 Systems
We have submitted two runs in the SemEval JRSA-
RTE8 challenge (Task 7). The systems were exe-
cuted on each of the sub tasks of the main task.
System 1 The distance algorithm used in the first
system is Word Overlap. The algorithm tries to find
the words of a source text between the words of the
target text. We have created two features for each
binary classifier: 1) Feature 1 - word overlap of H
into T (words of H are matched by the words in T;
2) Feature 2 - word overlap T into H (Words of T are
matched by the words in H).
System 2 In the second system the we have used
only Feature 1.
We have created separate models for the Beatle
dataset and the sciEntsBank dataset. The results ob-
tained are shown in Table 1.
4.2 Analysis
The results obtained are in line with our previous
participations in the RTE challenges (Kouylekov et.
al., 2011). Of course as we described before in our
papers (Kouylekov et. al., 2011) the potential of the
edit distance algorithm is limited. Still it provides a
595
Task Beatle Q Beatle A sciEntsBank Q sciEntsBank A sciEntsBank D
2way
run 1 0.6400 0.6570 0.5930 0.6280 0.6160
run 2 0.4620 0.4480 0.5560 0.5930 0.5710
3way
run 1 0.5510 0.4950 0.5240 0.5780 0.5490
run 2 0.4150 0.4400 0.4390 0.5030 0.4770
5way
run 1 0.4830 0.4470 0.4130 0.4340 0.4170
run 2 0.3850 0.4320 0.2330 0.2370 0.2540
Table 1: Task 7 Results obtained. (Accuracy)
good performance and provides a solid potential for
some close domain tasks as described in (Negri and
Kouylekov, 2009). We were quite content with the
new machine learning based core. The selected con-
figuration performed in an acceptable manner. The
results obtained were in line with the cross accuracy
obtained by our system on the training set which
shows that it is not susceptible to over-training.
5 CLTE
5.1 Systems
We have submitted two runs in the CLTE task (Task
8).
System 1 The distance algorithm used in the first
system is Word Overlap as we did for task 7. We
have created two features for each binary classifier:
1) Feature 1 - word overlap of H into T (words of H
are matched by the words in T; 2) Feature 2 - word
overlap T into H (Words of T are matched by the
words in H).
System 2 In the second system we have made a
slight modification of the matcher that handled num-
bers.
The matcher module for this task used the transla-
tion modules defined in Section 3. We have created
a model for each language pair.
The results obtained are shown in Table 2.
5.2 Analysis
The results obtained are quite disappointing. Our
system obtained on the test set of the last CLTE com-
petition (CLTE1) quite satisfactory results (clte1-
test). All the results obtained for this competition
are near or above the medium of the best systems.
Our algorithm did not show signs of over-training
(the accuracy of the system on the test and on the
training of CLTE1 were almost equal). Having this
in mind we expected to obtain scores at least in the
margins of 0.45 to 0.5. This does not happen ac-
cording us due to the fact that this year dataset has
characteristics quite different than the last year. To
test this hypothesis we have trained our system on
half of the dataset (clte2-half-training) ,given for test
this year, and test it on the rest (clte-half-test). The
results obtained demonstrate that the dataset given
is more difficult for our system than the last years
one. The results also prove that our system is prob-
ably too conservative when learning from examples.
If the test set is similar to the training it performs
in consistent manner on both, otherwise it demon-
strates severe over-training problems.
6 Conclusions
In this paper we have presented a generic system for
text pair classification. This system was evaluated
on task 7 and task 8 of Semeval 2013 and obtained
satisfactory results. The new machine learning mod-
ule of the system needs improvement and we plan to
focus our future efforts in it.
We plan to release the newly developed system as
version 4 of the open source package EDITS avail-
able at http://edits.sf.net.
Acknowledgments
This work has been partially supported by the
ECfunded project Galateas (CIP-ICT PSP-2009-3-
250430).
596
Run Spanish Italian French German
run1 0.34 0.324 0.346 0.349
run2 0.342 0.324 0.34 0.349
clte2-half-training 0.41 0.43 0.40 0.44
clte2-half-test 0.43 0.44 0.41 0.43
clte1-test 0.52 0.51 0.54 0.55
Table 2: Task 8. Results obtained. (Accuracy)
References
Baroni M., Bisi S. 2004. Using cooccurrence statistics
and the web to discover synonyms in technical lan-
guage In Proceedings of LREC 2004
Bentivogli L., Clark P., Dagan I., Dang H, Giampic-
colo D. 2011. The Seventh PASCAL Recognizing
Textual Entailment Challenge In Proceedings of TAC
2011
Bingham E., Mannila H. 2001. Random projection in
dimensionality reduction: Applications to image and
text data. In Knowledge Discovery and Data Mining,
ACM Press pages 245250
Bosca A., Dini L. 2008. Query expansion via library
classification system. In CLEF 2008. Springer Verlag,
LNCS
Curtoni P., Dini L. 2006. Celi participation at clef 2006
Cross language delegated search. In CLEF2006 Work-
ing notes.
Dagan I. and Glickman O. 2004. Probabilistic Textual
Entailment: Generic Applied Modeling of Language
Variability. Learning Methods for Text Understanding
and Mining Workshop.
Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K.,
Harshman R. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science 41 391407
Giampiccolo; Bernardo Magnini; Ido Dagan; Bill Dolan.
2007. The Third PASCAL Recognizing Textual
Entailment Challenge. Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing. June 2007, Prague, Czech Republic
Hall M., Frank E., Holmes G., Pfahringer B., Reute-
mann P., Witten I. 2009 The WEKA Data Mining
Software: An Update; SIGKDD Explorations, Vol-
ume 11, Issue 1.
Inkpen D. 2007. A statistical model for near-synonym
choice. ACM Trans. Speech Language Processing
4(1)
Kouylekov M., Negri M. An Open-Source Package for
Recognizing Textual Entailment. 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010) ,Uppsala, Sweden. July 11-16, 2010
Kouylekov M., Bosca A., Dini L. 2011. EDITS 3.0 at
RTE-7. Proceedings of the Seventh Recognizing Tex-
tual Entailment Challenge (2011).
Kouylekov M., Bosca A., Dini L., Trevisan M. 2012.
CELI: An Experiment with Cross Language Textual
Entailment. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012).
Kouylekov M., Mehdad Y. and Negri M. 2011 Is it Worth
Submitting this Run? Assess your RTE System with a
Good Sparring Partner Proceedings of the TextInfer
2011 Workshop on Textual Entailment
Kraaij W. 2003. Exploring transitive translation meth-
ods. In Vries, A.P.D., ed.: Proceedings of DIR 2003.
Lin J., Gunopulos D. 2003. Dimensionality reduction
by random projection and latent semantic indexing. In
proceedings of the Text Mining Workshop, at the 3rd
SIAM International Conference on Data Mining.
Mehdad Y.,Negri M., Federico M.. 2011. Using Paral-
lel Corpora for Cross-lingual Textual Entailment. In
Proceedings of ACL-HLT 2011.
Negri M., Bentivogli L., Mehdad Y., Giampiccolo D.,
Marchetti A. 2011. Divide and Conquer: Crowd-
sourcing the Creation of Cross-Lingual Textual Entail-
ment Corpora. In Proceedings of EMNLP 2011.
Negri M., Kouylekov M., 2009 Question Answer-
ing over Structured Data: an Entailment-Based Ap-
proach to Question Analysis. RANLP 2009 - Re-
cent Advances in Natural Language Processing, 2009
Borovets, Bulgaria
Negri M., Marchetti A., Mehdad Y., Bentivogli L., Gi-
ampiccolo D. Semeval-2012 Task 8: Cross-lingual
Textual Entailment for Content Synchronization. In
Proceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012). 2012.
Turney P.D. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl. In EMCL 01: Proceedings of
the 12th European Conference on Machine Learning,
London, UK, Springer-Verlag pages 491502
597
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 699?703,
Dublin, Ireland, August 23-24, 2014.
UIO-Lien: Entailment Recognition using Minimal Recursion Semantics
Elisabeth Lien
Department of Informatics
University of Oslo, Norway
elien@ifi.uio.no
Milen Kouylekov
Department of Informatics
University of Oslo, Norway
milen@ifi.uio.no
Abstract
In this paper we present our participa-
tion in the Semeval 2014 task ?Evalu-
ation of compositional distributional se-
mantic models on full sentences through
semantic relatedness and textual entail-
ment?. Our results demonstrate that us-
ing generic tools for semantic analysis is a
viable option for a system that recognizes
textual entailment. The invested effort in
developing such tools allows us to build
systems for reasoning that do not require
training.
1 Introduction
Recognizing textual entailment (RTE) has been a
popular area of research in the last years. It has
appeared in a variety of evaluation campaigns as
both monolingual and multilingual tasks. A wide
variety of techniques based on different levels of
text interpretation has been used, e.g., lexical dis-
tance, dependency parsing and semantic role la-
beling (Androutsopoulos and Malakasiotis, 2010).
Our approach uses a semantic representation
formalism called Minimal Recursion Semantics
(MRS), which, to our knowledge, has not been
used extensively in entailment decision systems.
Notable examples of systems that use MRS are
Wotzlaw and Coote (2013), and Bergmair (2010).
In Wotzlaw and Coote (2013), the authors present
an entailment recognition system which combines
high-coverage syntactic and semantic text analysis
with logical inference supported by relevant back-
ground knowledge. MRS is used as an interme-
diate format in transforming the results of the lin-
guistic analysis into representations used for log-
ical reasoning. The approach in Bergmair (2010)
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
uses the syllogism as an approximation of natural
language reasoning. MRS is used as a step in the
translation of natural language sentences into logi-
cal formulae that are suitable for processing. Both
works describe approaches that can be adapted
to RTE, but no empirical evaluation is included
to demonstrate the potential of the proposed ap-
proaches.
In contrast to these approaches, our system
bases entailment decision directly on the MRS
representations. Graph alignment over MRS rep-
resentations forms the basis for entailment recog-
nition. If key nodes in the hypothesis MRS can be
aligned to nodes in the text MRS, this is treated as
an indicator of entailment.
This paper represents our first attempt to evalu-
ate a system based on logical-form semantic rep-
resentations in a RTE competition. Using a state-
of-the-art semantic analysis component, we have
created a generic rule-based system for recogniz-
ing textual entailment that obtains competitive re-
sults on a real evaluation dataset. Our approach
does not require training. We confront it with
a strong baseline provided by the EDITS system
(Kouylekov et al., 2011).
In Section 2 we describe the computational se-
mantics framework that forms the basis of our ap-
proach. Section 3 details our entailment system,
and in Section 4 we analyze our results from the
task evaluation.
2 Minimal Recursion Semantics
Minimal Recursion Semantics (MRS) (Copestake
et al., 2005) is a framework for computational se-
mantics which provides expressive representations
with a clear interface with syntax. MRS allows
underspecification of scope, in order to capture the
different readings of a sentence with a single MRS
representation. We use the MRS analyses that are
produced by the HPSG English Resource Gram-
mar (ERG) (Flickinger, 2000).
699
The core of an MRS representation is a mul-
tiset of relations, called elementary predications
(EPs). An EP represents a single lexeme, or gen-
eral grammatical features. Each EP has a predi-
cate symbol, and a label (also called handle) that
identifies the EPs position within the MRS struc-
ture. Each EP contains a list of numbered argu-
ments: ARG0, ARG1, etc., whose values are scopal
or non-scopal variables. The ARG0 value is called
the EP?s distinguished variable, and denotes an
event or state, or an entity.
Finally, an MRS has a set of handle constraints
which describe how the scopal arguments of the
EPs can be equated with EP labels. A constraint
h
i
=
q
h
j
denotes equality modulo quantifier inser-
tion. EPs are directly and indirectly linked through
handle constraints and variable sharing, and the re-
sulting MRS forms a connected graph.
In Figure 1, we see an MRS for the sentence
A woman is cutting a potato. The topmost EP,
cut v 1, has a list of three argument-value pairs:
its distinguished variable e
3
denotes an event, and
the variables x
6
and x
9
refer to the entities filling
the agent and patient roles in the verb event. x
6
and x
9
are in turn the distinguished variables of
the EPs that represent a woman and a potato, re-
spectively.
3 System Description
In the following, T
sent
and H
sent
refer to the text
and hypothesis sentence, and T
mrs
and H
mrs
to
their MRS representations.
The core of our system is a rule based compo-
nent, which bases entailment decision on graph
alignment over MRS structures. An earlier ver-
sion of the system is described in Lien (2014).
The earlier version was developed on the data set
from the SemEval-2010 shared task Parser Eval-
uation using Textual Entailment (PETE) (Yuret et
al., 2010). Using no external linguistic resources,
the system output positive entailment decisions for
sentence pairs where core nodes of the H
mrs
could
be aligned to nodes in T
mrs
according to a set of
heuristic matching rules. The system we present
in this paper extends the earlier version by adding
support for contradiction recognition, and by us-
ing lexical relations from WordNet.
For our participation in the entailment recogni-
tion task, first, we did an analysis of the SICK trial
data. In the ENTAILMENT pairs, H
sent
is a para-
phrase over the whole or part of the text sentence.
The changes from T
sent
to H
sent
can be syntactic
(e.g., active-passive conversion), lexical (e.g., syn-
onymy, hyponymy-hypernymy, multiword expres-
sions replaced by single word), or T
sent
contains
some element that does not appear in H
sent
(e.g.,
T
sent
is a conjunction and H
sent
one of its con-
juncts, a modifier in T
sent
is left out of H
sent
). In
the CONTRADICTION category, the sentences of
a pair are also basically the same or paraphrases,
and a negation or a pair of antonymous expres-
sions create the contradiction. The NEUTRAL
pairs often have a high degree of word overlap, but
H
sent
cannot be inferred from T
sent
. Our system
accounts for many of these characteristics.
The system bases its decision on the results of
two procedures: a) an event relation match which
searches for an alignment between the MRSs, and
b) a contradiction cue check. After running these
procedures, the system outputs
1. ENTAILMENT, if the event relation match-
ing procedure found an alignment, and no
contradiction cues were found,
2. CONTRADICTION, if contradiction cues
were found,
3. NEUTRAL, if neither of the above condi-
tions are met.
The event relation matching procedure extends
the one developed in Lien (2014) to account for
the greater lexical variation in the SICK data. The
procedure selects all the EPs in T
mrs
and H
mrs
that have an event variable as their ARG0?we call
them event relations. These event relations mainly
represent verbs, verb conjunctions, adjectives, and
prepositions. For each event relation H
event
in the
hypothesis the procedure tries to find a matching
relation T
event
among the text event relations. We
say that H
event
matches T
event
if:
1. they represent the same lexeme with the
same part-of-speech, or if both are verbs and
H
event
is a synonym or hypernym of T
event
,
and
2. all their arguments match. Two event rela-
tion arguments in the same argument position
match if:
? they are the same or synonymous, or the
H
event
argument is a hypernym of the
T
event
argument, or
700
?h
1
,
h
4
: a q?0:1?(ARG0 x
6
, RSTR h
7
, BODY h
5
),
h
8
: woman n 1?2:7?(ARG0 x
6
),
h
2
: cut v 1?11:18?(ARG0 e
3
, ARG1 x
6
, ARG2 x
9
),
h
10
: a q?19:20?(ARG0 x
9
, RSTR h
12
, BODY h
11
),
h
13
: potato n 1?21:28?(ARG0 x
9
)
{h
12
=
q
h
13
, h
7
=
q
h
8
, h
1
=
q
h
2
} ?
Figure 1: MRS for A woman is cutting a potato (pair 4661, SICK trial data).
? the argument in T
event
represents a noun
phrase and the argument in H
event
is an
underspecified pronoun like somebody,
or
? the argument in T
event
is either a sco-
pal relation or a conjunction relation,
and one of its arguments matches that of
H
event
, or
? the argument in H
event
is not expressed
(i.e., it matches the T
event
argument by
default)
The matching procedure does not search for
more than one alignment between the event rela-
tions of H
mrs
and T
mrs
.
The contradiction cue procedure checks
whether the MRS pairs contain relations express-
ing negation. The quantifier no q rel negates
an entity (e.g., no man), whereas neg rel
denotes sentence negation. If a negation relation
appears in one but not the other MRS, we treat
this as an indicator of CONTRADICTION.
Example: Figure 1 shows the MRS analysis of
the hypothesis in the entailment pair A woman
is slicing a potato ? A woman is cutting a
potato. There is only one event relation in H
mrs
:
cut v 1. T
mrs
is an equivalent structure with
one event relation slice v 1. Using Word-
Net, the system finds that cut v 1 is a hyper-
nym of slice v 1. Then, the system compares
the ARG1 and ARG2 values of the event relations.
The arguments match since they are the same re-
lations. There are no contradiction cues in either
of the MRSs, so the system correctly outputs EN-
TAILMENT.
If we look at the rule based component?s output
(Table 1) for the 481 of the 500 SICK trial sen-
tence pairs for which the ERG produced MRSs,
we get a picture of how well it covers the phenom-
ena in the data set:
Of the 134 ENTAILMENT pairs, 59 were para-
phrases where the variation was relatively limited
gold ENT gold CON gold NEU
sys ENT 59 0 1
sys CON 0 51 14
sys NEU 75 22 259
Table 1: Output for the system on SICK trial data.
and could be captured by looking for synonyms,
hyponyms, and treating the hypothesis as a sub-
graph of the text. The simple contradiction cue
check, which looks for negation relations, covered
51 of 73 CONTRADICTION pairs.
75 ENTAILMENT and 22 CONTRADICTION
pairs were not captured by the matching and con-
tradiction cue procedures. Almost 30% of the
ENTAILMENT pairs had word pairs whose lex-
ical relationship was not recognized using Word-
Net (e.g.: playing a guitar? strumming a guitar).
In the other pairs there were alternations between
simple and more complex noun phrases (protec-
tive gear ? gear used for protection), change of
part-of-speech from T
sent
to H
sent
for the same
meaning entities (It is raining on a walking man?
A man is walking in the rain); some pairs required
reasoning, and in some cases H
sent
contained in-
formation not present in T
sent
. In some cases, en-
tailment recognition fails because the MRS analy-
sis is not correct (e.g., misrepresentation of passive
constructions).
The contradiction cue check did not look for
antonymous words and expressions, and this ac-
counts for almost half of the missing CONTRA-
DICTION pairs. The rest contained negation,
but were misclassified either because an incorrect
MRS analysis was chosen by the parser or because
synonymous words within the scope of the nega-
tion were not recognized.
EDITS We used a backoff-system for the pairs
when the rule-based system fails to produce re-
701
System 1 2 3 4 5
Rules Only Rules Only Combined Combined Edits
Training 76.13 75.4 76.62 76.62 74.78
Test 77.0 76.35 77.12 77.14 74.79
Table 2: Submitted system accuracy on training and test set.
sults. Our choice was EDITS
1
as it provides
a strong baseline system for recognizing textual
entailment (Kouylekov et al., 2011). EDITS
(Kouylekov and Negri, 2010) is an open source
package which offers a modular, flexible, and
adaptable working environment for experimenting
with the RTE task over different datasets. The
package allows to: i) create an entailment engine
by defining its basic components; ii) train this
entailment engine over an annotated RTE corpus
to learn a model and iii) use the entailment en-
gine and the model to assign an entailment judg-
ment and a confidence score to each pair of an un-
annotated test corpus.
We used two strategies for combining the rule-
based system with EDITS: Our first strategy was
to let the rule-based system classify those sentence
pairs for which the ERG could produce MRSs, and
use EDITS for the pairs were we did not have
MRSs (or processing failed due to errors in the
MRSs) . The second strategy was to mix the out-
put from both systems when they disagree. In this
case we took the ENTAILMENT decisions from
the rule-based, and EDITS contributes with CON-
TRADICTION and NEUTRAL.
4 Analysis
We have submitted the results obtained from five
system configurations. The first four used the rule-
based system as the core. The fifth was a system
obtained by training EDITS on the training set.
We use the fifth system as a strong baseline. In
the few cases in which the rule-based system did
not produce result (2% of the test set pairs) EDITS
judgments were used in the submission. In System
1 and System 2 we have used the first combination
strategy described in the end of section 3. In Sys-
tem 4 and System 5 the entailment decisions are a
combination of the results from the rule-based sys-
tem and EDITS as described in the second strategy
in the same section. The rule-based component
in System 1 and System 3 has more fine-grained
1
http://edits.sf.net
Precision Recall F-Measure
Contradiction 0.8422 0.7264 0.78
Entailment 0.9719 0.4158 0.5825
Neutral 0.7241 0.9595 0.8254
Table 3: Performance of System 1.
negation rules so that no q rel is not treated as
a contradiction cue in different contexts (e.g., No
woman runs does not contradict A woman sings).
Table 2 shows the results for the five submitted
systems.
The results demonstrate that the rule-based sys-
tem can be used as a general system for recogniz-
ing textual entailment. It surpasses with 3 points
of accuracy EDITS, which is an established strong
baseline system. We are quite content with the re-
sults obtained as we did not use the training dataset
to create the rules, but only the trial dataset. The
combination of the two systems brings a slight im-
provement.
Overall the rule-based system is quite precise
as demonstrated in Table 3. The numbers in the
table correspond to System 1 but are comparable
to the other rule-based systems 2, 3 and 4. The
system achieves an excellent precision on the en-
tailment and contradiction relations. It is almost
always correct when assigning the entailment rela-
tion. And it also obtains a decent recall, correctly
assigning almost half of the entailment pairs. On
the contradiction relation the system also obtained
a decent result, capturing most of the negation
cases.
5 Conclusions
Using a state-of-the-art semantic analysis compo-
nent, we have created a generic rule-based sys-
tem for recognizing textual entailment that obtains
competitive results on a real evaluation dataset.
An advantage of our approach is that it does not
require training. The precision of the approach
makes it an excellent candidate for a system that
uses textual entailment as the core of an intelligent
search engine.
702
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual Entail-
ment Methods. J. Artif. Intell. Res. (JAIR), 38:135?
187.
Richard Bergmair. 2010. Monte Carlo Semantics: Ro-
bust Inference and Logical Pattern Processing with
Natural Language Text. Ph.D. thesis, University of
Cambridge.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal Recursion Semantics:
An Introduction. Research on Language & Compu-
tation, 3(2):281?332.
Dan Flickinger. 2000. On Building a More Effcient
Grammar by Exploiting Types. Natural Language
Engineering, 6(1):15?28.
Milen Kouylekov and Matteo Negri. 2010. An
Open-Source Package for Recognizing Textual En-
tailment. In 48th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2010) ,Up-
psala, Sweden, pages 42?47.
Milen Kouylekov, Yashar Mehdad, and Matteo Negri.
2011. Is it Worth Submitting this Run? Assess your
RTE System with a Good Sparring Partner. In Pro-
ceedings of the TextInfer 2011 Workshop on Textual
Entailment, Edinburgh Scotland, pages 30?34.
Elisabeth Lien. 2014. Using Minimal Recursion Se-
mantics for Entailment Recognition. In Proceed-
ings of the Student Research Workshop at the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 76?84,
Gothenburg, Sweden, April.
Andreas Wotzlaw and Ravi Coote. 2013. A Logic-
based Approach for Recognizing Textual Entailment
Supported by Ontological Background Knowledge.
CoRR, abs/1310.4938.
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
SemEval-2010 Task 12: Parser Evaluation using
Textual Entailments. In Proceedings of the 5th
International Workshop on Semantic Evaluation,
pages 51?56.
703
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 30?34,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Is it Worth Submitting this Run?
Assess your RTE System with a Good Sparring Partner
Milen Kouylekov
CELI s.r.l.
Turin, Italy
kouylekov@celi.it
Yashar Mehdad
FBK-irst and University of Trento
Trento, Italy
mehdad@fbk.eu
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Abstract
We address two issues related to the devel-
opment of systems for Recognizing Textual
Entailment. The first is the impossibility to
capitalize on lessons learned over the different
datasets available, due to the changing nature
of traditional RTE evaluation settings. The
second is the lack of simple ways to assess
the results achieved by our system on a given
training corpus, and figure out its real potential
on unseen test data. Our contribution is the ex-
tension of an open-source RTE package with
an automatic way to explore the large search
space of possible configurations, in order to
select the most promising one over a given
dataset. From the developers? point of view,
the efficiency and ease of use of the system,
together with the good results achieved on all
previous RTE datasets, represent a useful sup-
port, providing an immediate term of compar-
ison to position the results of their approach.
1 Introduction
Research on textual entailment (TE) has received a
strong boost by the Recognizing Textual Entailment
(RTE) Challenges, organized yearly to gather the
community around a shared evaluation framework.
Within such framework, besides the intrinsic diffi-
culties of the task (i.e. deciding, given a set of Text-
Hypothesis pairs, if the hypotheses can be inferred
from the meaning of the texts), the development of
RTE systems has to confront with a number of ad-
ditional problems and uncertainty factors. First of
all, since RTE systems are usually based on com-
plex architectures that integrate a variety of tools and
resources, it is per se very difficult to tune them and
define the optimal configuration given a new dataset.
In general, when participating to the evaluation chal-
lenges there?s no warranty that the submitted runs
are those obtained with the best possible configura-
tion allowed by the system. Second, the evaluation
settings change along the years. Variations in the
length of the texts, the origin of the pairs, the bal-
ance between positive and negative examples, and
the type of entailment decisions allowed, reflect the
need to move from easier and more artificial settings
to more complex and natural ones. However, in con-
trast with other more stable tasks in terms of eval-
uation settings and metrics (e.g. machine transla-
tion), such changes make it difficult to capitalize on
the experience obtained by participants throughout
the years. Third, looking at RTE-related literature
and the outcomes of the six campaigns organised so
far, the conclusions that can be drawn are often con-
troversial. For instance, it is not clear whether the
availability of larger amounts of training data corre-
lates with better performance (Hickl et al, 2006) or
not (Zanzotto et al, 2007; Hickl and Bensley, 2007),
even within the same evaluation setting. In addi-
tion, ablation tests carried out in recent editions of
the challenge do not allow for definite conclusions
about the actual usefulness of tools and resources,
even the most popular ones (Bentivogli et al, 2009).
Finally, the best performing systems often have dif-
ferent natures from one year to another, showing al-
ternations of deep (Hickl and Bensley, 2007; Tatu
and Moldovan, 2007) and shallow approaches (Jia
et al, 2010) ranked at the top positions. In light
of these considerations, it would be useful for sys-
30
tems developers to have: i) automatic ways to sup-
port systems? tuning at a training stage, and ii) reli-
able terms of comparison to validate their hypothe-
ses, and position the results of their work before sub-
mitting runs for evaluation. In this paper we address
these needs by extending an open-source RTE pack-
age (EDITS1) with a mechanism that automatizes
the selection of the most promising configuration
over a training dataset. We prove the effectiveness
of such extension showing that it allows not only to
achieve good performance on all the available RTE
Challenge datasets, but also to improve the official
results, achieved with the same system, through ad
hoc configurations manually defined by the devel-
opers team. Our contribution is twofold. On one
side, in the spirit of the collaborative nature of open
source projects, we extend an existing tool with a
useful functionality that was still missing. On the
other side, we provide a good ?sparring partner? for
system developers, to be used as a fast and free term
of comparison to position the results of their work.
2 ?Coping? with configurability
EDITS (Kouylekov and Negri, 2010) is an open
source RTE package, which offers a modular, flex-
ible, and adaptable working environment to experi-
ment with the RTE task over different datasets. The
package allows to: i) create an entailment engine
by defining its basic components (i.e. algorithms,
cost schemes, rules, and optimizers); ii) train such
entailment engine over an annotated RTE corpus to
learn a model; and iii) use the entailment engine and
the model to assign an entailment judgement and a
confidence score to each pair of an un-annotated test
corpus. A key feature of EDITS is represented by its
high configurability, allowed by the availability of
different algorithms, the possibility to integrate dif-
ferent sets of lexical entailment/contradiction rules,
and the variety of parameters for performance opti-
mization (see also Mehdad, 2009). Although config-
urability is per se an important aspect (especially for
an open-source and general purpose system), there
is another side of the coin. In principle, in order to
select the most promising configuration over a given
development set, one should exhaustively run a huge
number of training/evaluation routines. Such num-
1http://edits.fbk.eu/
ber corresponds to the total number of configura-
tions allowed by the system, which result from the
possible combinations of parameter settings. When
dealing with enlarging dataset sizes, and the tight
time constraints usually posed by the evaluation
campaigns, this problem becomes particularly chal-
lenging, as developers are hardly able to run exhaus-
tive training/evaluation routines. As recently shown
by the EDITS developers team, such situation re-
sults in running a limited number of experiments
with the most ?reasonable? configurations, which
consequently might not lead to the optimal solution
(Kouylekov et al, 2010).
The need of a mechanism to automatically ob-
tain the most promising solution on one side, and
the constraints posed by the evaluation campaigns
on the other side, arise the necessity to optimize
this procedure. Along this direction, the objective
is good a trade-off between exhaustive experimen-
tation with all possible configurations (unfeasible),
and educated guessing (unreliable). The remainder
of this section tackles this issue introducing an op-
timization strategy based on genetic algorithms, and
describing its adaptation to extend EDITS with the
new functionality.
2.1 Genetic algorithm
Genetic algorithms (GA) are well suited to effi-
ciently deal with large search spaces, and have been
recently applied with success to a variety of opti-
mization problems and specific NLP tasks (Figueroa
and Neumann, 2008; Otto and Riff, 2004; Aycinena
et al, 2003). GA are a direct stochastic method for
global search and optimization, which mimics natu-
ral evolution. To this aim, they work with a popu-
lation of individuals, representing possible solutions
to the given task. Traditionally, solutions are rep-
resented in binary as strings of 0s and 1s, but other
encodings (e.g. sequences of real values) are possi-
ble. The evolution usually starts from a population
of randomly generated individuals, and at each gen-
eration selects the best-suited individuals based on
a fitness function (which measures the optimality of
the solution obtained by the individual). Such selec-
tion is then followed by modifications of the selected
individuals obtained by recombining (crossover) and
performing random changes (mutation) to form a
new population, which will be used in the next iter-
31
ation. Finally, the algorithm is terminated when the
maximum number of generations, or a satisfactory
fitness level has been reached for the population.
2.2 EDITS-GA
Our extension to the EDITS package, EDITS-GA,
consists in an iterative process that starts with an
initial population of randomly generated configura-
tions. After a training phase with the generated con-
figurations, the process is evaluated by means of the
fitness function, which is manually defined by the
user2. This measure is used by the genetic algo-
rithm to iteratively build new populations of config-
urations, which are trained and evaluated. This pro-
cess can be seen as the combination of: i) a micro
training/evaluation routine for each generated con-
figuration of the entailment engine; and ii) a macro
evolutionary cycle, as illustrated in Figure 1. The
fitness function is an important factor for the evalu-
ation and the evolution of the generated configura-
tions, as it drives the evolutionary process by deter-
mining the best-suited individuals used to generate
new populations. The procedure to estimate and op-
timize the best configuration applying the GA, can
be summarized as follows.
(1) Initialization: generate a random initial popula-
tion (i.e. a set of configurations).
(2) Selection:
2a. The fitness function (accuracy, or F-measure)
is evaluated for each individual in the population.
2b. The individuals are selected according to their
fitness function value.
(3) Reproduction: generate a new population of
configurations from the selected one, through ge-
netic operators (cross-over and mutation).
(4) Iteration: repeat the Selection and Reproduction
until Termination.
(5) Termination: end if the maximum number of
iterations has been reached, or the population has
converged towards a particular solution.
In order to extend EDITS with genetic algo-
rithms, we used a GA implementation available in
the JGAP tool3. In our settings, each individual con-
tains a sequence of boolean parameters correspond-
2For instance, working on the RTE Challenge ?Main? task
data, the fitness function would be the accuracy for RTE1 to
RTE5, and the F-measure for RTE6.
3http://jgap.sourceforge.net/
Figure 1: EDITS-GA framework.
ing to the activation/de-activation of the system?s
basic components (algorithms, cost schemes, rules,
and optimizers). The configurations corresponding
to such individuals constitute the populations itera-
tively evaluated by EDITS-GA on a given dataset.
3 Experiments
Our experiments were carried out over the datasets
used in the six editions of the RTE Challenge
(?Main? task data from RTE1 to RTE6). For each
dataset we obtained the best model by training
EDITS-GA over the development set, and evaluat-
ing the resulting model on the test pairs. To this
aim, the optimization process is iterated over all
the available algorithms in order to select the best
combination of parameters. As termination crite-
rion, we set to 20 the maximum number of itera-
tions. To increase efficiency, we extended EDITS
to pre-process each dataset using the tokenizer and
stemmer available in Lucene4. This pre-processing
phase is automatically activated when the EDITS-
GA has to process non-annotated datasets. How-
ever, we also annotated the RTE corpora with the
Stanford parser plugin (downloadable from the ED-
ITS websitein order to run the syntax-based algo-
rithms available (e.g. tree edit distance). The num-
ber of boolean parameters used to generate the con-
figurations is 18. In light of this figure, it becomes
evident that the number of possible configurations
is too large (218=262,144) for an exhaustive train-
ing/evaluation routine over each dataset5. However,
4http://lucene.apache.org/
5In an exploratory experiment we measured in around 4
days the time required to train EDITS, with all possible con-
32
# Systems Best Lowest Average EDITS (rank) EDITS-GA (rank) % Impr. Comp. Time
RTE1 15 0.586 0.495 0.544 0.559 (8) 0.5787 (3) +3.52% 8m 24s
RTE2 23 0.7538 0.5288 0.5977 0.605 (6) 0.6225 (5) +2.89% 9m 8s
RTE3 26 0.8 0.4963 0.6237 - 0.6875 (4) - 9m
RTE4 26 0.746 0.516 0.5935 0.57 (17) 0.595 (10) +4.38% 30m 54s
RTE5 20 0.735 0.5 0.6141 0.6017 (14) 0.6233 (9) +3.58% 8m 23s
RTE6 18 0.4801 0.116 0.323 0.4471 (4) 0.4673 (3) +4.51% 1h 54m 20s
Table 1: RTE results (acc. for RTE1-RTE5, F-meas. for RTE6). For each participant, only the best run is considered.
with an average of 5 reproductions on each iteration,
EDITS-GA makes an average of 100 configurations
for each algorithm. Thanks to EDITS-GA, the aver-
age number of evaluated configurations for a single
dataset is reduced to around 4006.
Our results are summarized in Table 1, showing
the total number of participating systems in each
RTE Challenge, together with the highest, lowest,
and average scores they achieved. Moreover, the of-
ficial results obtained by EDITS are compared with
the performance achieved with EDITS-GA on the
same data. We can observe that, for all datasets,
the results achieved by EDITS-GA significantly im-
prove (up to 4.51%) the official EDITS results. It?s
also worth mentioning that such scores are always
higher than the average ones obtained by partici-
pants. This confirms that EDITS-GA can be poten-
tially used by RTE systems developers as a strong
term of comparison to assess the capabilities of
their own system. Since time is a crucial factor for
RTE systems, it is important to remark that EDITS-
GA allows to converge on a promising configura-
tion quite efficiently. As can be seen in Table 1,
the whole process takes around 9 minutes7 for the
smaller datasets (RTE1 to RTE5), and less than 2
hours for a very large dataset (RTE6). Such time
analysis further proves the effectiveness of the ex-
tended EDITS-GA framework. For the sake of com-
pleteness we gave a look at the differences between
the ?educated guessing? done by the EDITS de-
velopers for the official RTE submissions, and the
?optimal? configuration automatically selected by
EDITS-GA. Surprisingly, in some cases, even a mi-
nor difference in the selected parameters leads to
figurations, over small datasets (RTE1 to RTE5).
6With these settings, training EDITS-GA over small datasets
(RTE1 to RTE5) takes about 9 minutes each.
7All time figures are calculated on an Intel(R) Xeon(R),
CPU X3440 @ 2.53GHz, 8 cores with 8 GB RAM.
significant gaps in the results. For instance, in RTE6
dataset, the ?guessed? configuration (Kouylekov et
al., 2010) was based on the lexical overlap algo-
rithm, setting the cost of replacing H terms with-
out an equivalent in T to the minimal Levenshtein
distance between such words and any word in T.
EDITS-GA estimated, as a more promising solution,
a combination of lexical overlap with a different cost
scheme (based on the IDF of the terms in T). In ad-
dition, in contrast with the ?guessed? configuration,
stop-words filtering was selected as an option, even-
tually leading to a 4.51% improvement over the of-
ficial RTE6 result.
4 Conclusion
?Is it worth submitting this run??,?How good is my
system??. These are the typical concerns of system
developers approaching the submission deadline of
an RTE evaluation campaign. We addressed these is-
sues by extending an open-source RTE system with
a functionality that allows to select the most promis-
ing configuration over an annotated training set. Our
contribution provides developers with a good ?spar-
ring partner? (a free and immediate term of compar-
ison) to position the results of their approach. Ex-
perimental results prove the effectiveness of the pro-
posed extension, showing that it allows to: i) achieve
good performance on all the available RTE datasets,
and ii) improve the official results, achieved with the
same system, through ad hoc configurations manu-
ally defined by the developers team.
Acknowledgments
This work has been partially supported by the EC-
funded projects CoSyne (FP7-ICT-4-24853), and
Galateas (CIP-ICT PSP-2009-3-250430).
33
References
Margaret Aycinena, Mykel J. Kochenderfer, and David
Carl Mulford. 2003. An Evolutionary Approach to
Natural Language Grammar Induction. Stanford CS
224N Natural Language Processing.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, Bernardo Magnini. 2009. The Fifth
PASCAL Recognizing Textual Entailment Challenge.
Proceedings of the TAC 2009 Workshop.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proceedings of the PASCAL Workshop of
Learning Methods for Text Understanding and Min-
ing.
Alejandro G. Figueroa and Gu?nter Neumann. 2008. Ge-
netic Algorithms for Data-driven Web Question An-
swering. Evolutionary Computation 16(1) (2008) pp.
89-125.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing Textual Entailment with LCCs Groundhog Sys-
tem. Proceedings of the Second PASCAL Challenges
Workshop.
Andrew Hickl and Jeremy Bensley. 2007. A discourse
commitment-based framework for recognizing textual
entailment. Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM Participation
at TAC 2010 RTE and Summarization Track. Proceed-
ings of the Sixth Recognizing Textual Entailment Chal-
lenge.
Milen Kouylekov and Matteo Negri. 2010. An Open-
source Package for Recognizing Textual Entailment.
Proceedings of ACL 2010 Demo session.
Milen Kouylekov, Yashar Mehdad, Matteo Negri, and
Elena Cabrio. 2010. FBK Participation in RTE6:
Main and KBP Validation Task. Proceedings of the
Sixth Recognizing Textual Entailment Challenge.
Yashar Mehdad 2009. Automatic Cost Estimation for
Tree Edit Distance Using Particle Swarm Optimiza-
tion. Proceedings of ACL-IJCNLP 2009.
Eridan Otto and Mar??a Cristina Riff 2004. Towards an
efficient evolutionary decoding algorithm for statisti-
cal machine translation. LNAI, 2972:438447..
Marta Tatu and Dan Moldovan. 2007. COGEX at RTE3.
Proceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Fabio Massimo Zanzotto, Marco Pennacchiotti and
Alessandro Moschitti. 2007. Shallow Semantics in
Fast Textual Entailment Rule Learners. Proceedings
of the Third Recognizing Textual Entailment Chal-
lenge.
34
