Mining the Web for Relations between Digital Devices using a Probabilistic
Maximum Margin Model
Oksana Yakhnenko
Iowa State University
Ames, IA, 50010
oksayakh@cs.iastate.edu
Barbara Rosario
Intel Research
Santa Clara, CA, 95054
barbara.rosario@intel.com
Abstract
Searching and reading the Web is one of the
principal methods used to seek out infor-
mation to resolve problems about technol-
ogy in general and digital devices in partic-
ular. This paper addresses the problem of
text mining in the digital devices domain. In
particular, we address the task of detecting
semantic relations between digital devices in
the text of Web pages. We use a Na??ve Bayes
model trained to maximize the margin and
compare its performance with several other
comparable methods. We construct a novel
dataset which consists of segments of text
extracted from the Web, where each segment
contains pairs of devices. We also propose
a novel, inexpensive and very effective way
of getting people to label text data using a
Web service, the Mechanical Turk. Our re-
sults show that the maximum margin model
consistently outperforms the other methods.
1 Introduction
In the digital home domain, home networks
are moving beyond the common infrastructure
of routers and wireless access points to include
application-oriented devices like network attached
storage, Internet telephones (VOIP), digital video
recorders (e.g., Tivo), media players, entertainment
PCs, home automation, and networked photo print-
ers. There is an ongoing challenge associated with
domestic network design, technology education, de-
vice setup, repair, and tuning. In this digital home
setting, searching the Web is one of the principle
methods used to seek out information and to resolve
problems about technology in general and about dig-
ital devices in particular (Bly et al, 2006).
This paper addresses the problem of automatic
text mining in the digital networks domain. Under-
standing the relations between entities in natural lan-
guage sentences is a crucial step toward the goal of
text mining. We address the task of identifying and
extracting the sentences from Web pages which ex-
pressed a relation between two given digital devices
in contrast to sentences in which these devices co-
occur.
As an example, consider a user who is looking
for information on digital video recorders (DVR),
in particular, on how she can use a DVR with a
PC. This user will not be satisfied with finding Web
pages that simply mention these devices (such as
the many products catalogs or shopping sites), but
rather, the user is interested in retrieving and read-
ing only the Web pages in which a specific relation
between the two devices is expressed. The user is
interested to learn that, for example, ?Any modern
Windows PC can be used for DVR duty? or that it is
possible to transfer data from a DVR to a PC (?You
can simply take out the HD from the DVR, hook it up
to the PC, and copy the videos over to the PC?).1
The specific task addressed in this paper is the fol-
lowing: given a pair of devices, search the Web and
extract only the sentences in which the devices are
actually involved in an activity or a relation in the
retrieved Web pages.
Note that we do not attempt to identify the type
1In italic are real sentences extracted from Web pages.
273
of relationship between devices but rather we clas-
sify sentences into whether the relation or activity
is present or not, and thus we frame the problem as
a binary text classification problem.2 We propose
a directed maximum margin probabilistic model to
solve this classification task. Maximum margin
probabilistic models have received a lot of atten-
tion in the machine learning and natural language
processing literature. These models are trained to
maximize the smallest difference between the proba-
bilities of the true class and the best alternative class.
Approaches such as maximum margin Markov net-
works (M3N) (Taskar et al, 2003) have been con-
sidered in prediction problems in which the goal is
to assign a label to each word in the sentence or a
document (such as part of speech tagging). It has
also been shown that training of Bayesian networks
by maximizing the margin can result in better per-
formance than M3N in a flat-table structured domain
(simulated and UCI repository datasets) and a struc-
tured prediction problem (protein secondary struc-
ture) (Guo et al, 2005). Given this background,
we draw our attention to the application of maxi-
mum margin probabilistic models to a text classifi-
cation task. We consider a directed model, where
the parameters represent a probability distribution
for words in each class (maximum margin equiv-
alent of a Na??ve Bayes). We evaluate the maxi-
mum margin model and compare its performance
with the equivalent joint likelihood model (Na??ve
Bayes), conditional likelihood model (logistic re-
gression) and support vector machines (SVM) on the
relationship extraction task described above, as well
as several other classification methods. Our results
show that the maximum margin Na??ve Bayes outper-
forms the other methods in terms of classification
accuracy. To train such a model, manually labeled
data is required, which is usually slow and expensive
to acquire. To address this, we propose a novel, inex-
pensive and very effective way of getting people to
label text data using the Mechanical Turk, an Ama-
zon website3 where people earn ?micro-money? for
2Classifying or clustering the relation types would involve
the tricky task of defining the possible semantic relations be-
tween devices as well as relations. We plan of addressing this
in the future work, however, we believe that such binary distinc-
tion is already quite useful for many tasks in this domain.
3Available at http://www.mturk.com
completing tasks which are simple for humans to ac-
complish.
The paper is organized as follows: in Section 2
we discuss related work. In Section 3 we review
joint likelihood and conditional likelihood models
and maximum margin Na??ve Bayes. In Section 4
we describe the collection of the training sentences,
and how Mechanical Turk was used to construct the
labels for the data. Section 5 introduces the exper-
imental setup and presents performance results for
each of the algorithms. We analyze Na??ve Bayes,
maximum margin Na??ve Bayes and logistic regres-
sion in terms of the learned probability distributions
in Section 6. Section 7 concludes with discussion.
2 Related work
2.1 Relation extraction
There has been a spate of work on relation extrac-
tion in recent years. However, many papers actually
address the task of role extraction: (usually two) en-
tities are identified and the relationship is implied
by the co-occurrence of these entities or by some
linguistic expression (Agichtein and Gravano, 2000;
Zelenko et al, 2003).
Several papers propose the use of machine learn-
ing models and probabilistic models for relation ex-
traction: Na??ve Bayes for the relation subcellular-
location in the bio-medical domain (Craven, 1999)
or for person-affiliation and organization-location
(Zelenko et al, 2003). Rosario and Hearst (2005)
have used a more complicated dynamic graphical
model to identify interaction types between proteins
and to simultaneously extract the proteins.
2.2 Maximum margin models
Probabilistic graphical models and different ap-
proaches to training them have received a lot of at-
tention in application to natural language process-
ing. McCallum and Nigam (1998) showed that
Na??ve Bayes can be a very accurate model for text
categorization.
Since probabilistic graphical models represent
joint probability distributions whereas classification
focuses on the conditional probability, there has
been debate regarding the objective that should be
maximized in order to train these models. Ng and
Jordan (2001) have compared a joint likelihood
274
model (Na??ve Bayes) and its discriminative coun-
terpart (logistic regression), and they have shown
that while for large number of examples logistic re-
gression has a lower error rate, Na??ve Bayes often
outperforms logistic regression for smaller data sets.
However, Klein and Manning (2002) showed that
for natural language and text processing tasks, con-
ditional models are usually better than joint likeli-
hood models. Yakhnenko et al (2005) also showed
that conditional models suffer from overfitting in
text and sequence structured domains.
In recent years, the interest in learning parameters
of probabilistic models by maximizing the proba-
bilistic margin has developed. Taskar et al (2003)
have solved the problem of learning Markov net-
works (undirected graphs) by maximizing the mar-
gin. Their work has focused on likelihood based
structured classification where the goal is to assign
a class to each word in the sentence or a document.
Guo et al (2005) have proposed a solution to learn-
ing parameters of the maximum margin Bayesian
Networks.
Surprisingly, little has been done in applying
probabilistic models trained to maximize the mar-
gin to simple classification tasks (to the best of
our knowledge). Therefore, since the Na??ve Bayes
model has been shown to be a successful algorithm
for many text classification tasks (McCallum and
Nigam, 1998) we suggest learning the parameters
of Na??ve Bayes model to maximize the probabilis-
tic margin. We apply the Na??ve Bayes model trained
to maximize the margin to a relation extraction task.
3 Joint and conditional likelihood models
and maximum margin
We now describe the background in probabilistic
models as well as different approaches to parame-
ter estimation for probabilistic models. In particular,
we describe Na??ve Bayes, logistic regression (analo-
gous to conditionally trained Na??ve Bayes) and then
introduce Na??ve Bayes trained to maximize the mar-
gin.
First, we introduce some notation. Let D be a
corpus that consists of training examples. Let T be
the size of D. We represent each example with a
tuple ?s, c? where s is a sentence or a document, and
c is a label from a set of all possible labels, c ? C =
{c
1
...cm}. Let D=
{?
si, ci
?}
where superscript 1 ?
i ? T is the index of the document in the corpus, and
ci is the label of example si. Let V be vocabulary of
D, so that every document s consists of elements
of V . We will use sj to denote a word from s in
position j, where 1 ? j ? length(s).
3.1 Generative and discriminative Na??ve Bayes
models
A probabilistic model assigns to each instance s
a joint probability of the instance and the class
P (s, c). If the probability distribution is known,
then a new instance snew can be classified by giv-
ing it a label which has the highest probability:
c = arg max
ck?C
P (ck|snew) (1)
Joint likelihood models learn the parameters by
maximizing the probability of an example and its
class, P (s, c). Na??ve Bayes multinomial, for in-
stance, assumes that all words in the sentence are
independent given the class, and computes this prob-
ability as P (c)?length(s)j=1 P (sj |c). Each of P (sj |c)
and P (c) are estimated from the training data using
relative frequency estimates. From here on we will
refer to joint likelihood Na??ve Bayes multinomial as
NB-JL.
Since the conditional probability is needed for the
classification task, it has been suggested to solve the
maximization problem and train the model so that
the choice of the parameters maximizes P (c|s) di-
rectly. One can use a joint likelihood model to ob-
tain joint probability distribution P (s, c) and then
use the definition of conditional probability to get
P (c|s) = P (s, c)/?ck?C P (s, ck). The solutions
that maximize this objective function are searched
for by using gradient ascent methods. Logistic re-
gression is a conditional model that assumes the in-
dependence of features given the class, and it is a
conditional counterpart to NB-JL (Ng and Jordan,
2001).
We will now introduce a probabilistic maximum
margin objective and describe a maximum margin
model that is analogous to Na??ve Bayes and logistic
regression.
275
3.2 Maximum margin training of Na??ve Bayes
models
The basic idea behind maximum margin models is to
choose model parameters that for each example will
make the probability of the true class and the exam-
ple as high as possible while making the probability
of the nearest alternative class as low as possible.
Formally, the maximum margin objective is
? =
T
min
i=1
min
c 6=ci
P (ci|si)
P (c|si) =
T
min
i=1
min
c 6=ci
P (si, ci)
P (si, c) (2)
Here P (s, c) is modeled by a generative model, and
parameter learning is reduced to solving a convex
optimization problem (Guo et al, 2005).
In order for the example to be classified correctly,
the probability of the true class given the example
has to be higher than the probability of getting the
wrong class or
?i = log p(ci|si) ? log p(cj |si) > 0 (3)
where j 6= i and ci is the true label of example si.
The larger the margin ?i is, the more confidence we
have in the prediction.
We consider a Na??ve Bayes model trained to
maximize the margin and refer to this model as
MMNB. Using exponential family notation, let
P (sj |c) = ewsj |c . The likelihood is P (s, c) =
ewc
?len(s)
j=1 e
wsj |c
. Then the log-likelihood
logP (s, c) = wc+
len(s)
?
j=1
count(sj)wsj |c = w??(s, c)
(4)
where w is the weight vector for all the parame-
ters that need to be learned, and ?(s, c) is the vector
of counts of words associated with each parameter
?(s, c) = (...count(sjc)....) in s for class c.
The general formulation for Bayesian networks
was given in Guo et al, and we adapt their formu-
lation for training a Na??ve Bayes model. The para-
meters are learned by solving a convex optimization
problem. If the margin ? is the smallest log-ratio,
then ? needs to be maximized, where the constraint
is that for each instance the log-ratio of the proba-
bility of predicting the instance correctly and pre-
dicting it incorrectly is at least ?. Such formulation
also allows for the use of slack variables ? so that the
classifier ?gives up? on the examples that are diffi-
cult to classify.
minimize ?,w,?
1
?2 + B
T
?
i=1
?i
subject to w(?(i, ci) ? ?(i, c)) ? ??(ci, c) ? ?i
and
?
si?V
ewsi,c ? 1?c ? C
and ? ? 0
This problem is convex in the variables ?,w, ?. B is
a regularization parameter, and ?(ci, c) = 1 if ci 6= c
and 0 otherwise. The inequality constraint for prob-
abilities is needed to preserve convexity of the prob-
lem, and in the case of Na??ve Bayes, the probability
distribution over the parameters (the equality con-
straint) can be easily obtained by renormalizing the
learned parameters.
The minimization problem is somewhat similar to
?
2
-norm support vector machine with a soft margin
(Cristianini and Shawe-Taylor, 2000). The first con-
straint imposes that for each example the log of the
ratio between the example under the true class and
the example under some alternative class is greater
than the margin allowing for some slack. The sec-
ond constraint enforces that the parameters do not
get very large and that the probabilities sum to less
than 1 to maintain valid probability distribution (the
inequality constraint is required to preserve convex-
ity, and the probability distribution can be obtained
after training by renormalization).
Following Guo et al (2005), we find parame-
ters using a log-barrier method (Boyd and Vanden-
berghe, 2004), the sum of the logarithms of con-
straints are subtracted from the objective and scaled
by a parameter ?. The problem is solved sequen-
tially using a fixed ? and gradually lowering ? to 0.
The solution for a fixed ? is obtained using (typ-
ically) a second order method to guarantee faster
convergence. This solution is then used as the ini-
tial parameter values for the next ?. In our imple-
mentation we used a limited memory quasi-Newton
method (Nocedal and Liu, 1989).
276
4 Data and labels
4.1 The problem of labeling data
One major problem of natural language processing
is the sparsity of data; to accurately learn a linguis-
tic model, one needs to label a large amount of text,
which is usually an expensive requirement. For in-
formation extraction, the labeling process is particu-
larly difficult and time consuming. Moreover, in dif-
ferent applications one needs different labeled data
for each domain. We propose a creative way of con-
vincing many people to label data quickly and at
low cost to us by using the Mechanical Turk. Sim-
ilarly, Luis von Ahn (2006) creates very successful
and compelling computer games in such a way that
while playing, people provide labels for images on
the Web.
4.2 Collecting data and label agreement
analysis
To collect the data, we identified 58 pairs of dig-
ital devices, as well as their synonyms (for exam-
ple, computer, laptop, PC, desktop, etc), and differ-
ent manufacturers for a given device (for example
Toshiba, Dell, IBM, etc). The devices alone were
used to construct the query (for example ?computer,
camera?, as well as a combination of manufacturer
and devices (for example ?dell laptop, cannon cam-
era?). Each of these pairs was used as a query in
Google, and the sentences that contain both devices
were extracted resulting in a total of 3624 sentences.
We use the word ?sentence? when referring to the
examples, however we note that not all text excerpts
are sentences, some are chunks of text data.
To label the data we used the Mechanical Turk
(MTurk), a Web service that allows you to create
and post a task for humans to solve; typical tasks are
labeling pictures, choosing the best among several
photographs, writing product descriptions, proof-
reading and transcribing podcasts. After the task is
completed the requesters can then review the sub-
missions and reject them if the results are poor.
We created a total of 121 unique surveys consist-
ing of 30 questions. Each question consisted of one
of the extracted statements with the devices high-
lighted in red. The task for the labeler was to choose
between ?Yes?, if the statement contained a relation
between the devices, ?No? if it did not, or ?not ap-
worker3
worker1 worker2 yes no n/a
yes yes 1091 237 23
no 226 281 22
n/a 19 18 6
no yes 217 199 8
no 186 870 56
n/a 14 39 8
n/a yes 17 13 5
no 6 32 6
n/a 4 12 9
Table 1: Summary of the labels assigned by the MT workers
to all the sentences.
plicable? if the text extract was not a sentence, or if
the query words were not used as different devices
(as for noun compounds such as computer stereo).4
Each survey was assigned to 3 distinct workers, thus
having 3 possible labels for all 3624 sentences.5
We used Fleiss?s kappa (Fleiss, 1971) (a general-
ization of kappa statistic which takes into account
multiple raters and measures inter-rater reliability)
in order to determine the degree of agreement and
to determine whether the agreement was accidental.
Kappa statistics is a number between 0 and 1 where
0 is random agreement, and 1 is perfect agreement.
In order to compute kappa statistic, since the com-
putation requires that the raters are the same for
each survey, we mapped workers into ?worker1?,
?worker2?, ?worker3? with ?worker1? being the
first worker to complete each of the 121 surveys,
?worker2? the second, and so on. The responses are
summarized in Table 1.
The overall Fleiss?s kappa was 0.416, and there-
fore, it can be concluded that the agreement between
the workers was not accidental.
We had perfect agreement for 49% of all sen-
tences, 5% received all three labels (these examples
were discarded) and for the remaining 46% two la-
4This dataset, including all the
MTurk?s workers responses is available at
http://www.cs.iastate.edu/?oksayakh/relation data.html
5The requirement for the workers to be different was im-
posed by the MTurk system, which checks their Amazon iden-
tity; however, this still allows for the same person who has mul-
tiple identities to complete the same task more than once.
6The kappa coefficients for categories ?Yes? and ?No? were
0.45 and 0.41 respectively (moderate agreement) and for cate-
gory ?not applicable? was 0.15 (slight agreement).
277
bels were assigned (the majority vote was used to
determine the final label). For these cases, we no-
ticed that some of the labels were wrong (however
in most cases the majority vote results in the correct
label) but other sentences were ambiguous and ei-
ther label could be right. To assign the final label we
used majority vote, and we discarded sentences for
which ?not applicable? was the majority label.
We rewarded the users with between 15 and 30
cents per survey (resulting in less than a cent for a
text segment) and we were able to obtain labels for
3594 text segments for under $70. It also took any-
where between a few minutes to a half-hour from
the time the survey was made available until it was
completed by all three users. We find Mechanical
Turk to be a quite interesting, inexpensive, fairly ac-
curate and fast way to obtain labeled data for natural
language processing tasks.
We used this data to evaluate the classification
models as described in the next section.
5 Experimental setup and results
The words were stemmed, and the data was
smoothed by mapping all the words that appeared
only once to a unique token smoothing token (re-
sulting in a total of approximately 2,800 words
in the vocabulary). We performed 10-fold cross-
validation, with smoothed test data where all the un-
seen words in the test data were mapped to the token
smoothing token. We used the exact same data in
the folds for all four algorithms ? MMNB, NB-JL,
logistic regression and SVM. Since MMNB, SVM,
and logistic regression allows for regularization, we
used tuning to find the optimal performance of the
models. At each fold we withheld 30% of the train-
ing data for validation purposes (thus resulting in 3
disjoint sets at each fold). The model was trained
on the resulting 70% of the training data for differ-
ent values of the regularization parameters, and the
value which yielded the highest accuracy on the val-
idation set was used to train the model that was eval-
uated on the test set.
As a baseline, we consider a classifier which as-
signs the most frequent label (?Yes?); such a classi-
fier results in 53% accuracy.
Table 2 summarizes the performance of MMNB
and other algorithms as determined by 10-fold cross-
Algorithm Accuracy
MMNB 80.23%
SVM-RBF 76.49%
NB-JL 75.62%
Perceptron 74.04%
SVM-2 72.72%
SVM-3 71.54%
DT 70.76%
LR 69.95%
SVM-1 69.94%
Baseline 53.8%
Table 2: Classification accuracies as determined by 10-
fold cross-validation. SVM-1 uses linear kernel, SVM-2 uses
quadratic kernel, SVM-3 uses cubic kernel, SVM-RBF uses
RBF kernel with parameter ? = 0.1. The Decision Tree (DT)
uses binary splits. LR is logistic regression.
validation with tuning data. We compared the accu-
racies of the maximum margin model with the accu-
racy of generative Na??ve Bayes, logistic regression
and SVM as shown in Table 2. The MMNB has the
highest accuracy followed by NB-JL and then SVM
with RBF kernel. Even after tuning, logistic regres-
sion did not reach the performance of MMNB and
NB-JL.
Since MMNB is trained to maximize the mar-
gin, we compared it with the Support Vector Ma-
chine (linear maximum margin classifier). Counts
of words were used as features (resulting in the
bag of words representation7). We ran our experi-
ments with linear, quadratic, cubic and RBF kernels.
SVM was tuned using the validation set similarly to
MMNB. We also experimented with Perceptron and
Decision Tree using binary splits with reduced error-
pruning, which are methods commonly used for text
classification (due to lack of space, we will not de-
scribe these methods and their applications, but refer
the reader to Manning and Schu?tze (1999)). Among
all the known methods, the maximum margin Na??ve
Bayes is the algorithm with the highest accuracy,
suggesting that it is a competitive algorithm in re-
lation extraction and text classification tasks.
7This representation allows for additional or alternative fea-
tures such as k-grams of words, whether the words are capital-
ized, where on the page the sentence was located, etc. Evalu-
ating MMNB and other methods with additional features is of
interest in the future
278
6 Analysis of behavior of Na??ve Bayes,
maximum margin Na??ve Bayes and
logistic regression
We analyzed the behavior of the parameters of the
probabilistic models (Na??ve Bayes, MMNB and lo-
gistic regression) on the training data. For each ex-
ample in the training data we computed the probabil-
ity P (c = noRelation|s) using the parameters from
the model, and examined the probabilities assigned
to examples from both classes. We show these plots
in Figure 1.
Figure 1: Probability distribution of P (c = noRelation|s)
learned by the Na??ve Bayes (upper left), logistic regression (up-
per right) and maximum margin Na??ve Bayes(lower). In gray
are class-conditional probabilities assigned to positive exam-
ples, and in black are class-conditional probabilities assigned
to negative examples.
As we see, the logistic regression discriminates
between the majority of the examples by assigning
extreme probabilities (0 and 1). However, there are
some examples which are extremely borderline, and
thus it does not generalize well on the test set. On the
other had, Na??ve Bayes does not have such ?sharp?
discrimination. Maximum margin Na??ve Bayes has
?sharper? discrimination than Na??ve Bayes, however
the discrimination is smoother than for logistic re-
gression. The examples which are more difficult to
classify have probabilities that are more spread out
(away from 0.5), as opposed to the case of logistic
regression, which assigns these difficult examples to
probability close to 0.5. This suggests that maxi-
mum margin Na??ve Bayes, possibly has a better gen-
eralization ability than both logistic regression and
Na??ve Bayes, however to make such a claim addi-
tional experiments are needed.
7 Conclusions
The contribution of this paper is threefold. First, we
addressed the important problem of identifying the
presence of semantic relations between entities in
text, focusing on the digital domain. We presented
some encouraging results; it remains to be seen
however, how this would transfer to better results in
an information retrieval task. Secondly, we consid-
ered a probabilistic model trained to maximize the
margin, that achieved the highest accuracy for this
task, suggesting that it could be a competitive algo-
rithm for relation extraction and text classification
in general. However in order to fully evaluate the
MMNB method for relation classification it needs
to be applied to other classification and or relation
prediction tasks. We also empirically analyzed the
behavior of the parameters learned by maximum
margin model and showed that the parameters allow
for better generalization power than Na??ve Bayes or
logistic regression models. Finally, we suggested an
inexpensive way of getting people to label text data
via Mechanical Turk.
Acknowledgment The authors would like to
thank the reviewers for their feedback and com-
ments; William Schilit for invaluable insight and
help and for first suggesting using the MTurk to
gather labeled data; David McDonald for help with
developing survey instructions; and numerous MT
workers for providing the labels.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of Digital Libraries.
Sara Bly, William Schilit, David McDonald, Barbara
Rosario, and Ylian Saint-Hilaire. 2006. Broken ex-
pectations in the digital home. In Proceedings of Com-
puter Human Interaction (CHI).
Stephen Boyd and Lieven Vandenberghe. 2004. Convex
Optimization. Cambridge University Press.
Mark Craven. 1999. Learning to extract relations from
Medline. In AAAI-99 Workshop.
279
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Yuhong Guo, Dana Wilkinson, and Dale Schuurmans.
2005. Maximum margin bayesian networks. In Pro-
ceedings of the 21th Annual Conference on Uncer-
tainty in Artificial Intelligence (UAI-05), page 233.
Dan Klein and Christopher Manning. 2002. Conditional
structure versus conditional estimation in nlp models.
In Empirical Methods in Natural Language Process-
ing (EMNLP).
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, June.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive bayes text classifi-
cation. In AAAI-98 Workshop on Learning for Text
Categorization.
Andrew Y. Ng and Michael I. Jordan. 2001. On dis-
criminative vs. generative classifiers: A comparison of
logistic regression and naive bayes. In Proceedings of
Neural Information Processing Systems (NIPS), pages
841?848.
Jorge Nocedal and Dong C. Liu. 1989. On the limited
memory method for large scale optimization. Mathe-
matical Programming, 3(45):503?528.
Barbara Rosario and Marti Hearst. 2005. Multi-way re-
lation classification: Application to protein-protein in-
teractions. In Empirical Methods in Natural Language
Processing (EMNLP).
Benjamin Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In Proceedings
of Neural Information Processing Systems (NIPS).
Luis von Ahn. 2006. Games with a purpose. Computer,
39(6):92?94.
Oksana Yakhnenko, Adrian Silvescu, and Vasant
Honavar. 2005. Discriminatively trained markov
model for sequence classification. In Proceedings of
International Conference on Data Mining (ICDM).
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP).
280
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1366?1371,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Connecting Language and Knowledge Bases with Embedding Models
for Relation Extraction
Jason Weston
Google
111 8th avenue
New York, NY, USA
jweston@google.com
Antoine Bordes
Heudiasyc
UT de Compi?gne
& CNRS
Compi?gne, France
bordesan@utc.fr
Oksana Yakhnenko
Google
111 8th avenue
New York, NY, USA
oksana@google.com
Nicolas Usunier
Heudiasyc
UT de Compi?gne
& CNRS
Compi?gne, France
nusunier@utc.fr
Abstract
This paper proposes a novel approach for rela-
tion extraction from free text which is trained
to jointly use information from the text and
from existing knowledge. Our model is based
on scoring functions that operate by learning
low-dimensional embeddings of words, enti-
ties and relationships from a knowledge base.
We empirically show on New York Times ar-
ticles aligned with Freebase relations that our
approach is able to efficiently use the extra in-
formation provided by a large subset of Free-
base data (4M entities, 23k relationships) to
improve over methods that rely on text fea-
tures alone.
1 Introduction
Information extraction (IE) aims at generating struc-
tured data from free text in order to populate Knowl-
edge Bases (KBs). Hence, one is given an incom-
plete KB composed of a set of triples of the form
(h, r , t); h is the left-hand side entity (or head), t
the right-hand side entity (or tail) and r the relation-
ship linking them. An example from the Freebase
KB1 is (/m/2d3rf ,<director-of>, /m/3/324), where
/m/2d3rf refers to the director ?Alfred Hitchcock"
and /m/3/324 to the movie ?The Birds".
This paper focuses on the problem of learning to
perform relation extraction (RE) under weak super-
vision from a KB. RE is sub-task of IE that consid-
ers that entities have already been detected by a dif-
ferent process, such as a named-entity recognizer.
RE then aims at assigning to a relation mention m
1www.freebase.com
(i.e. a sequence of text which states that some rela-
tion is true) the corresponding relationship from the
KB, given a pair of extracted entities (h, t) as con-
text. For example, given the triple (/m/2d3rf ,?wrote
and directed", /m/3/324), a system should predict
<director-of>. The task is said to be weakly super-
vised because for each pair of entities (h, t) detected
in the text, all relation mentions m associated with
them are labeled with all the relationships connect-
ing h and t in the KB, whether they are actually ex-
pressed by m or not.
Our key contribution is a novel model that em-
ploys not only weakly labeled text mention data, as
most approaches do, but also leverages triples from
the known KB. The model thus learns the plausi-
bility of new (h, r , t) triples by generalizing from
the KB, even though this triple is not present. A
ranking-based embedding framework is used to train
our model. Thereby, relation mentions, entities and
relationships are all embedded into a common low-
dimensional vector space, where scores are com-
puted. We show that our method can successfully
take into account information from a large-scale KB
(Freebase: 4M entities, 23k relationships) to im-
prove over systems that are only using text features.
This paper is organized as follows: Section 2
presents related work, Section 3 introduces our
model and its main influences, and experimental re-
sults are displayed in Section 4.
2 Previous Work
Learning under weak supervision is common in nat-
ural language processing, especially for tasks where
the annotation costs are significant such as in se-
1366
mantic parsing (Kate and Mooney, 2007; Liang et
al., 2009; Bordes et al, 2010; Matuszek et al,
2012). This is also naturally used in IE, since it
allows to train large-scale systems without requir-
ing to label numerous texts. The idea was intro-
duced by (Craven et al, 1999), which matched the
Yeast Protein Database with PubMed abstracts. It
was also used to train open extractors based on
Wikipedia infoboxes and corresponding sentences
(Wu and Weld, 2007; Wu and Weld, 2010). Large-
scale open IE projects (e.g. (Banko et al, 2007))
also rely on weak supervision, since they learn mod-
els from a seed KB in order to extend it.
Weak supervision is also a popular option for RE:
Mintz et al (2009) used Freebase to train weakly su-
pervised relational extractors on Wikipedia, an ap-
proach generalized by the multi-instance learning
frameworks (Riedel et al, 2010; Hoffmann et al,
2011; Surdeanu et al, 2012). All these works only
use textual information to perform extraction.
Lao et al (2012) proposed the first work aiming to
perform RE employing both KB data and text, using
a rule-based random walk method. Recently, Riedel
et al (2013) proposed another joint approach based
on collaborative filtering for learning entity embed-
dings. This approach connects text with Freebase
by learning shared embeddings of entities through
weak supervision, in contrast to our method where
no joint learning is performed. We do not compare
to these two approaches since they use two different
evaluation protocols that greatly differ from those
used in all aforementioned previous works. Never-
theless, our method is easier to integrate into exist-
ing systems than those, since KB data is used via
the addition of a scoring term, which is trained sepa-
rately beforehand (with no shared embeddings). Be-
sides, we demonstrate in our experimental section
that our system can handle a large number of rela-
tionships, significantly larger than that presented in
(Lao et al, 2012; Riedel et al, 2013).
3 Embedding-based Framework
Our work concerns energy-based methods, which
learn low-dimensional vector representations (em-
beddings) of atomic symbols (words, entities, re-
lationships, etc.). In this framework, we learn two
models: one for predicting relationships given re-
lation mentions and another one to encode the in-
teractions among entities and relationships from the
KB. The joint action of both models in prediction
allows us to use the connection between the KB and
text to perform relation extraction. One could also
share parameters between models (via shared em-
beddings), but this is not implemented in this work.
This approach is inspired by previous work designed
to connect words and Wordnet (Bordes et al, 2012).
Both submodels end up learning vector embed-
dings of symbols, either for entities or relationships
in the KB, or for each word/feature of the vocabulary
(denoted V). The set of entities and relationships in
the KB are denoted by E and R, and nv , ne and nr
denote the size of V , E and R respectively. Given
a triple (h, r , t) the embeddings of the entities and
the relationship (vectors in Rk ) are denoted with the
same letter, in boldface characters (i.e. h, r, t).
3.1 Connecting Text and Relationships
The first part of the framework concerns the learn-
ing of a function Sm2r (m, r), based on embeddings,
that is designed to score the similarity of a relation
mention m and a relationship r .
Our scoring approach is inspired by previous
work for connecting word labels and images (We-
ston et al, 2010), which we adapted, replacing im-
ages by mentions and word labels by relationships.
Intuitively, it consists of first projecting words and
features into the embedding space and then comput-
ing a similarity measure (the dot product in this pa-
per) between this projection and a relationship em-
bedding. The scoring function is then:
Sm2r (m, r) = f(m)>r
with f a function mapping words and features into
Rk , f(m) = W>?(m). W is the matrix of Rnv?k
containing all word embeddings w, ?(m) is the
(sparse) binary representation of m (? Rnv ) indi-
cating absence or presence of words/features, and
r ? Rk is the embedding of the relationship r .
This approach can be easily applied at test time to
score (mention, relationship) pairs. Since this type
of learning problem is weakly supervised, Bordes et
al. (2010) showed that a convenient way to train it
is by using a ranking loss. Hence, given a data set
D = {(mi , ri ), i = 1, ... , |D|} consisting of (men-
tion, relationship) training pairs, one could learn the
1367
embeddings using constraints of the form:
?i , ?r ? 6= ri , f(mi )>ri > 1 + f(mi )>r? , (1)
where 1 is the margin. That is, we want the re-
lation that (weakly) labels a given mention to be
scored higher than other relation by a margin of 1.
Then, given any mention m one can predict the cor-
responding relationship r?(m) with:
r?(m) = arg max
r ??R
Sm2r (m, r ?) = arg max
r ??R
(
f(m)>r?
)
.
Learning Sm2r (?) under constraints (1) is well
suited when one is interested in building a per-
mention prediction system. However, performance
metrics of relation extraction are sometimes mea-
sured using precision recall curves aggregated for
all mentions concerning the same pair of entities,
as in (Riedel et al, 2010). In that case the scores
across predictions for different mentions need to be
calibrated so that the most confident ones have the
higher scores. This can be better encoded with con-
straints of the following form:
?i , j , ?r ? 6= ri , rj , f(mi )>ri > 1 + f(mj)>r? .
In this setup, scores of pairs observed in the training
set should be larger than that of any other prediction
across all mentions. In practice, we use ?soft? rank-
ing constraints (optimizing the hinge loss), i.e. we
minimize:
?i , j , ?r ? 6= ri , rj , max(0, 1?f(mi )>ri +f(mj)>r?).
Finally, we also enforce a (hard) constraint on the
norms of the columns of W and r, i.e. ?i , ||Wi ||2 ?
1 and ?j , ||rj ||2 ? 1. Training is carried out by
Stochastic Gradient Descent (SGD), updating W
and r at each step, following (Weston et al, 2010;
Bordes et al, 2013). That is, at the start of training
the parameters to be learnt (the nv ? k word/feature
embeddings in W and the nr ? k relation embed-
dings r ) are initialized to random weights. We ini-
tialize each k-dimensional embedding vector ran-
domly with mean 0, standard deviation 1k . Then, we
iterate the following steps to train them:
1. Select at random a positive training pair
(mi , ri ).
2. Select at random a secondary training pair
(mj , rj), used to calibrate the scores.
3. Select at random a negative relation r ? such that
r ? 6= ri and r ? 6= rj .
4. Make a stochastic gradient step to minimize
max(0, 1? f(mi )>ri + f(mj)>r?).
5. Enforce the constraint that each embedding
vector is normalized, i.e. if ||Wi ||2 > 1 then
Wi ?Wi/||Wi ||2.
3.2 Encoding Structured Data of KBs
Using only weakly labeled text mentions for train-
ing ignores much of the prior knowledge we can
leverage from a large KB such as Freebase. In or-
der to connect this relational data with our model,
we propose to encode its information into entity and
relationship embeddings. This allows us to build a
model which can score the plausibility of new en-
tity relationship triples which are missing from Free-
base. Several models have been recently developed
for that purpose (e.g. in (Nickel et al, 2011; Bor-
des et al, 2011; Bordes et al, 2012)): we chose in
this work to follow the approach of (Bordes et al,
2013), which is simple, flexible and has shown very
promising results on Freebase data.
Given a training set S = {(hi , ri , ti ), i =
1, ... , |S|} of relations extracted from the KB, this
model learns vector embeddings of the entities and
of the relationships using the idea that the func-
tional relation induced by the r -labeled arcs of the
KB should correspond to a translation of the em-
beddings. That is, given a k-dimensional embed-
ding of the left-hand side (head) entity, adding the
k-dimensional embedding of a given relation should
yield the point (or close to the point) of the k-
dimensional embedding of the right-hand side (tail)
entity. Hence, this method enforces that h + r ? t
when (h, r , t) holds, while h + r should be far away
from t otherwise. The model thus gives the follow-
ing score for the plausibility of a relation:
Skb(h, r , t) = ?||h + r ? t||22 .
A ranking loss is also used for training Skb. The
ranking objective is designed to assign higher scores
1368
to existing relations versus any other possibility:
?i ,?h? 6= hi , Skb(hi , ri , ti ) ? 1 + Skb(h?, ri , ti ),
?i ,?r ? 6= ri , Skb(hi , ri , ti ) ? 1 + Skb(hi , r ?, ti ),
?i ,?t ? 6= ti , Skb(hi , ri , ti ) ? 1 + Skb(hi , ri , t ?).
That is, for each known triple (h, r , t), if we re-
placed the (i) head, (ii) relation or (iii) tail with some
other possibility, the modified triple should have a
lower score (i.e. be less plausible) than the original
triple. The three sets of constraints defined above
encode the three types of modification. As in Sec-
tion 3.1 we use soft constraints via the hinge loss,
enforce constraints on the norm of embeddings, i.e.
?h,r ,t , ||h||2 ? 1, ||r ||2 ? 1, ||t||2 ? 1, and training
is performed using SGD, as in (Bordes et al, 2013).
At test time, one may again need to calibrate the
scores Skb across entity pairs. We propose a sim-
ple approach: we convert the scores by ranking all
relationshipsR by Skb and instead output:
S?kb(h, r , t)=?
(?
r ? 6=r
?(Skb(h, r , t)>Skb(h, r
?, t))
)
,
i.e. a function of the rank of r . We chose the simpli-
fied model ?(x) = 1 if x < ? and 0 otherwise; ?(?)
is the Kronecker function.
3.3 Implementation for Relation Extraction
Our framework can be used for relation extraction
in the following way. First, for each pair of entities
(h, t) that appear in the test set, all the correspond-
ing mentionsMh,t in the test set are collected and a
prediction is performed with:
r?h,t = argmax
r?R
?
m?Mh,t
Sm2r (m, r) .
The predicted relationship can either be a valid re-
lationship or NA ? a marker that means that there is
no relation between h and t (NA is added to R dur-
ing training and is treated like other relationships).
If r?h,t is a relationship, a composite score is defined:
Sm2r+kb(h, r?h,t , t)=
?
m?Mh,t
Sm2r (m, r?h,t)+S?kb(h, r?h,t , t)
That is, only the top scoring non-NA predictions are
re-scored. Hence, our final composite model favors
predictions that agree with both the mentions and the
KB. If r?h,t is NA, the score is unchanged.
4 Experiments
We use the training and test data, evaluation frame-
work and baselines from (Riedel et al, 2010; Hoff-
mann et al, 2011; Surdeanu et al, 2012).
NYT+FB This dataset, developed by (Riedel et
al., 2010), aligns Freebase relations with the New
York Times corpus. Entities were found using the
Stanford named entity tagger (Finkel et al, 2005),
and were matched to their name in Freebase. For
each mention, sentence level features are extracted
which include part of speech, named entity and de-
pendency tree path properties. Unlike some of the
previous methods, we do not use features that aggre-
gate properties across multiple mentions. We kept
the 100,000 most frequent features.There are 52 pos-
sible relationships and 121,034 training mentions of
which most are labeled as no relation (labeled ?NA?)
? there are 4700 Freebase relations mentioned in the
training set, and 1950 in the test set.
Freebase Freebase is a large-scale KB that has
around 80M entities, 23k relationships and 1.2B re-
lations. We used a subset restricted to the top 4M
entities for scalability reasons ? where top is defined
as the ones with the largest number of relations to
other entities. We used all the 23k possible relation-
ships in Freebase. To make a realistic setting, we
did not choose the entity set using the NYT+FB data
set, so it may not overlap completely. For that rea-
son, we needed to keep the set rather large. Keeping
the top 4M entities gives an overlap of 80% with the
entities in the NYT+FB test set. Most importantly,
we then removed all the entity pairs present in the
NYT+FB test set from Freebase, i.e. all relations
they are involved in independent of the relationship.
This ensures that we cannot just memorize the true
relations for an entity pair ? we have to learn to gen-
eralize from other entities and relations.
As the NYT+FB dataset was built on an earlier
version of Freebase we also had to translate the dep-
recated relationships into their new variants (e.g.
?/p/business/company/place_founded ? ? ?/orga-
nization/organization/place_founded?) to make the
two datasets link (then, the 52 relationships in
NYT+FB are now a subset of the 23k from Free-
base). We then trained the Skb model on the remain-
ing triples.
1369
recall
prec
ision
0 0.1 0.20
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Wsabie M2R+FBMIMLREHoffmannWsabie M2RRiedelMintz
recall
prec
ision
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10.4
0.5
0.6
0.7
0.8
0.9 Wsabie M2R+FBMIMLREHoffmannWsabie M2RRiedelMintz
Figure 1: Top: Aggregate extraction precision/recall
curves for a variety of methods. Bottom: the
same plot zoomed to the recall [0-0.1] region.
WsabieM2R is our method trained only on mentions,
WsabieM2R+FB uses Freebase annotations as well.
Modeling Following (Bordes et al, 2013) we set
the embedding dimension k to 50. The learning rate
for SGD was selected using a validation set: we ob-
tained 0.001 for Sm2r , and 0.1 for Skb. For the cal-
ibration of S?kb, ? = 10 (note, here we are ranking
all 23k Freebase relationships). Training Sm2r took
5 minutes, whilst training Skb took 2 days due to the
large scale of the data set.
Results Figure 1 displays the aggregate precision
/ recall curves of our approach WSABIEM2R+FB
which uses the combination of Sm2r + Skb, as well
as WSABIEM2R , which only uses Sm2r , and existing
state-of-the-art approaches: HOFFMANN (Hoffmann
et al, 2011)2, MIMLRE (Surdeanu et al, 2012).
RIEDEL (Riedel et al, 2010) and MINTZ (Mintz et
al., 2009).
WSABIEM2R is comparable to, but slightly worse
than, the MIMLRE and HOFFMANN methods, possi-
bly due to its simplified assumptions (e.g. predict-
ing a single relationship per entity pair). However,
the addition of extra knowledge from other Freebase
entities in WSABIEM2R+FB provides superior per-
formance to all other methods, by a wide margin, at
least between 0 and 0.1 recall (see bottom plot).
Performance of WSABIEM2R and
WSABIEM2R+FB for recall > 0.1 degrades rapidly,
faster than that of other methods. This is also
caused by the simplifications of WSABIEM2R that
prevent it from reaching high precision when the
recall is greater than 0.1. We recall that Freebase
data is not used to detect relationships i.e. to
discriminate between NA and the rest, but only to
select the best relationship in case of detection.
That is WSABIEM2R+FB only improves precision,
not recall, so both versions of Wsabie are similar
w.r.t. recall. This could be improved by borrowing
ideas from HOFFMANN (Hoffmann et al, 2011) or
MIMLRE (Surdeanu et al, 2012) for dealing with
the multi-label case. Our approach, which uses
Freebase to increase precision, is general and could
improve any other method.
5 Conclusion
In this paper we described a framework for leverag-
ing large scale knowledge bases to improve relation
extraction by training not only on (mention, relation-
ship) pairs but using all other KB triples as well. We
empirically showed that it allows to significantly im-
prove precision on extracted relations. Our model-
ing approach is general and should apply to other
settings, e.g. for the task of entity linking.
Acknowledgments
This work was carried out in the framework of
the Labex MS2T (ANR-11-IDEX-0004-02), and
funded by the French National Agency for Research
(EVEREST-12-JS02-005-01).
2There is an error in the plot from (Hoffmann et al, 2011),
which we have corrected. The authors acknowledged this issue.
1370
References
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In IJCAI, vol-
ume 7, pages 2670?2676.
Antoine Bordes, Nicolas Usunier, and Jason Weston.
2010. Label ranking under ambiguous supervision for
learning semantic correspondences. In Proceedings of
the 27th International Conference on Machine Learn-
ing (ICML-10), pages 103?110.
Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Proc. of the 25th Conf.
on Artif. Intel. (AAAI).
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words and
meaning representations for open-text semantic pars-
ing. In Proc. of the 15th Intern. Conf. on Artif. Intel.
and Stat., volume 22, pages 127?135. JMLR.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,
Jason Weston, and Oksana Yakhnenko. 2013. Trans-
lating embeddings for modeling multi-relational data.
In Advances in Neural Information Processing Sys-
tems (NIPS 26).
Mark Craven, Johan Kumlien, et al 1999. Constructing
biological knowledge bases by extracting information
from text sources. In ISMB, volume 1999, pages 77?
86.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363?370.
Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, vol-
ume 1, pages 541?550.
Rohit J Kate and Raymond J Mooney. 2007. Learning
language semantics from ambiguous supervision. In
AAAI, volume 7, pages 895?900.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1017?
1026. Association for Computational Linguistics.
Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1-Volume 1, pages 91?99.
Association for Computational Linguistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume 2,
pages 1003?1011. Association for Computational Lin-
guistics.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings of
the 28th International Conference on Machine Learn-
ing (ICML-11), pages 809?816.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowledge
Discovery in Databases, pages 148?163. Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT, pages 74?84.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 455?465. Associa-
tion for Computational Linguistics.
Jason Weston, Samy Bengio, and Nicolas Usunier. 2010.
Large scale image annotation: learning to rank with
joint word-image embeddings. Machine learning,
81(1):21?35.
Fei Wu and Daniel S Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the sixteenth
ACM conference on Conference on information and
knowledge management, pages 41?50. ACM.
Fei Wu and Daniel S Weld. 2010. Open information ex-
traction using wikipedia. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 118?127. Association for Computa-
tional Linguistics.
1371
