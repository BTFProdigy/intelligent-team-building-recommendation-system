Coling 2008: Companion volume ? Posters and Demonstrations, pages 19?22
Manchester, August 2008
Phrasal Segmentation Models for Statistical Machine Translation
Graeme Blackwood, Adri
`
a de Gispert, William Byrne
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
Trumpington Street, Cambridge, CB2 1PZ, U.K.
{gwb24|ad465|wjb31}@cam.ac.uk
Abstract
Phrasal segmentation models define a
mapping from the words of a sentence
to sequences of translatable phrases. We
discuss the estimation of these models
from large quantities of monolingual train-
ing text and describe their realization as
weighted finite state transducers for incor-
poration into phrase-based statistical ma-
chine translation systems. Results are re-
ported on the NIST Arabic-English trans-
lation tasks showing significant comple-
mentary gains in BLEU score with large
5-gram and 6-gram language models.
1 Introduction
In phrase-based statistical machine transla-
tion (Koehn et al, 2003) phrases extracted from
word-aligned parallel data are the fundamental
unit of translation. Each phrase is a sequence
of contiguous translatable words and there is no
explicit model of syntax or structure.
Our focus is the process by which a string of
words is segmented as a sequence of such phrases.
Ideally, the segmentation process captures two as-
pects of natural language. Firstly, segmentations
should reflect the underlying grammatical sentence
structure. Secondly, common sequences of words
should be grouped as phrases in order to preserve
context and respect collocations. Although these
aspects of translation are not evaluated explicitly,
phrases have been found very useful in transla-
tion. They have the advantage that, within phrases,
words appear as they were found in fluent text.
However, reordering of phrases in translation can
lead to disfluencies. By defining a distribution over
possible segmentations, we hope to address such
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
disfluencies. A strength of our approach is that it
exploits abundantly available monolingual corpora
that are usually only used for training word lan-
guage models.
Most prior work on phrase-based statistical lan-
guage models concerns the problem of identifying
useful phrasal units. In (Ries et al, 1996) an iter-
ative algorithm selectively merges pairs of words
as phrases with the goal of minimising perplex-
ity. Several criteria including word pair frequen-
cies, unigram and bigram log likelihoods, and a
correlation coefficient related to mutual informa-
tion are compared in (Kuo and Reichl, 1999). The
main difference between these approaches and the
work described here is that we already have a defi-
nition of the phrases of interest (i.e. the phrases of
the phrase table extracted from parallel text) and
we focus instead on estimating a distribution over
the set of possible alternative segmentations of the
sentence.
2 Phrasal Segmentation Models
Under the generative model of phrase-based statis-
tical machine translation, a source sentence s
I
1
gen-
erates sequences u
K
1
= u
1
, . . . , u
K
of source lan-
guage phrases that are to be translated. Sentences
cannot be segmented into phrases arbitrarily: the
space of possible segmentations is constrained by
the contents of the phrase table which consists of
phrases found with translations in the parallel text.
We start initially with a distribution in which seg-
mentations assume the following dependencies:
P (u
K
1
,K|s
I
1
) = P (u
K
1
|K, s
I
1
)P (K|I). (1)
The distribution over the number of phrases K is
chosen to be uniform, i.e. P (K|I) = 1/I, K ?
{1, 2, . . . , I}, and all segmentations are considered
equally likely. The probability of a particular seg-
mentation is therefore
P (u
K
1
|K, s
I
1
) =
{
C(K, s
I
1
) if u
K
1
= s
I
1
0 otherwise
(2)
19
where C(K, s
I
1
) is chosen to ensure normalisation
and the phrases u
1
, . . . , u
K
are found in the phrase
table. This simple model of segmentation has been
found useful in practice (Kumar et al, 2006).
Our goal is to improve upon the uniform seg-
mentation of equation (2) by estimating the phrasal
segmentation model parameters from naturally oc-
curing phrase sequences in a large monolingual
training corpus. An order-n phrasal segmentation
model assigns a probability to a phrase sequence
u
K
1
according to
P (u
K
1
|K, s
I
1
) =
K
?
k=1
P (u
k
|u
k?1
1
,K, s
I
1
) ?
{
C(K, s
I
1
)
?
K
k=1
P (u
k
|u
k?1
k?n+1
) if u
K
1
= s
I
1
0 otherwise
(3)
where the approximation is due to the Markov as-
sumption that only the most recent n ? 1 phrases
are useful when predicting the next phrase. Again,
each u
k
must be a phrase with a known transla-
tion. For a fixed sentence s
I
1
, the normalisation
term C(K, s
I
1
) can be calculated. In translation,
however, calculating this quantity becomes harder
since the s
I
1
are not fixed. We therefore ignore
the normalisation and use the unnormalised like-
lihoods as scores.
2.1 Parameter Estimation
We focus on first-order phrasal segmentation mod-
els. Although we have experimented with higher-
order models we have not yet found them to yield
improved translation.
Let f(u
k?1
, u
k
) be the frequency of occurrence
of a string of words w
j
i
in a very large training
corpus that can be split at position x such that
i < x ? j and the substrings w
x?1
i
and w
j
x
match
precisely the words of two phrases u
k?1
and u
k
in
the phrase table. The maximum likelihood proba-
bility estimate for phrase bigrams is then their rel-
ative frequency:
?
P (u
k
|u
k?1
) =
f(u
k?1
, u
k
)
f(u
k?1
)
. (4)
These maximum likelihood estimates are dis-
counted and smoothed with context-dependent
backoff such that
P (u
k
|u
k?1
) =
{
?(u
k?1
, u
k
)
?
P (u
k
|u
k?1
) if f(u
k?1
, u
k
) > 0
?(u
k?1
)P (u
k
) otherwise
(5)
where ?(u
k?1
, u
k
) discounts the maximum like-
lihood estimates and the context-specific backoff
weights ?(u
k?1
) are chosen to ensure normalisa-
tion.
3 The Transducer Translation Model
The Transducer Translation Model (TTM) (Kumar
and Byrne, 2005; Kumar et al, 2006) is a gener-
ative model of translation that applies a series of
transformations specified by conditional probabil-
ity distributions and encoded as Weighted Finite
State Transducers (Mohri et al, 2002).
The generation of a target language sentence
t
J
1
starts with the generation of a source lan-
guage sentence s
I
1
by the source language model
P
G
(s
I
1
). Next, the source language sentence is
segmented according to the uniform phrasal seg-
mentation model distribution P
W
(u
K
1
,K|s
I
1
) of
equation (2). The phrase translation and reorder-
ing model P
?
(v
R
1
|u
K
1
) generates the reordered se-
quence of target language phrases v
R
1
. Finally,
the reordered target language phrases are trans-
formed to word sequences t
J
1
under the target
segmentation model P
?
(t
J
1
|v
R
1
). These compo-
nent distributions together form a joint distribu-
tion over the source and target language sentences
and their possible intermediate phrase sequences
as P (t
J
1
, v
R
1
, u
K
1
, s
I
1
).
In translation under the generative model, we
start with the target sentence t
J
1
in the foreign lan-
guage and search for the best source sentence s?
I
1
.
Encoding each distribution as a WFST leads to a
model of translation as a series of compositions
L = G ?W ? ? ? ? ? T (6)
in which T is an acceptor for the target language
sentence and L is the word lattice of translations
obtained during decoding. The most likely trans-
lation s?
I
1
is the path in L with least cost.
The above approach generates a word lattice L
under the unweighted phrasal segmentation model
of equation (2). In the initial experiments reported
here, we apply the weighted phrasal segmentation
model via lattice rescoring. We take the word lat-
tice L and compose it with the unweighted trans-
ducer W to obtain a lattice of phrases L ?W ; this
lattice contains phrase sequences and translation
scores consistent with the initial translation. We
also extract the complete list of phrases relevant to
each translation.
20
We then wish to apply the phrasal segmentation
model distribution of equation (3) to this phrase
lattice. The conditional probabilities and backoff
structure defined in equation (5) can be encoded
as a weighted finite state acceptor (Allauzen et al,
2003). In this acceptor, ?, states encode histories
and arcs define the bigram and backed-off unigram
phrase probabilities. We note that the raw counts
of equation (4) are collected prior to translation
and the first-order probabilities are estimated only
for phrases found in the lattice.
The phrasal segmentation model is composed
with the phrase lattice and projected on the in-
put to obtain the rescored word lattice L
?
=
(L ?W ) ??. The most likely translation after ap-
plying the phrasal segmentation model is found as
the path in L
?
with least cost. Apart from likeli-
hood pruning when generating the original word
lattice, the model scores are included correctly in
translation search.
4 System Development
We describe experiments on the NIST Arabic-
English machine translation task and apply phrasal
segmentation models in lattice rescoring.
The development set mt02-05-tune is formed
from the odd numbered sentences of the NIST
MT02?MT05 evaluation sets; the even numbered
sentences form the validation set mt02-05-test.
Test performance is evaluated using the NIST sub-
sets from the MT06 evaluation: mt06-nist-nw for
newswire data and mt06-nist-ng for newsgroup
data. Results are also reported for the MT08 evalu-
ation. Each set contains four references and BLEU
scores are computed for lower-case translations.
The uniformly segmented TTM baseline system
is trained using all of the available Arabic-English
data for the NIST MT08 evaluation
1
. In first-pass
translation, decoding proceeds with a 4-gram lan-
guage model estimated over the parallel text and a
965 million word subset of monolingual data from
the English Gigaword Third Edition. Minimum
error training (Och, 2003) under BLEU optimises
the decoder feature weights using the development
set mt02-05-tune. In the second pass, 5-gram and
6-gram zero-cutoff stupid-backoff (Brants et al,
2007) language models estimated using 4.7 billion
words of English newswire text are used to gener-
ate lattices for phrasal segmentation model rescor-
ing. The phrasal segmentation model parameters
1
http://www.nist.gov/speech/tests/mt/2008/
mt02-05-tune mt02-05-test
TTM+MET 48.9 48.6
+6g 51.9 51.7
+6g+PSM 52.7 52.7
Table 2: BLEU scores for phrasal segmentation
model rescoring of 6-gram rescored lattices.
are trained using a 1.8 billion word subset of the
same monolingual training data used to build the
second-pass word language model. A phrasal seg-
mentation model scale factor and phrase insertion
penalty are tuned using the development set.
5 Results and Analysis
First-pass TTM translation lattices generated with
a uniform segmentation obtain baseline BLEU
scores of 48.9 for mt02-05-tune and 48.6 for
mt02-05-test. In our experiments we demon-
strate that phrasal segmentation models continue
to improve translation even for second-pass lat-
tices rescored with very large zero-cutoff higher-
order language models. Table 1 shows phrasal seg-
mentation model rescoring of 5-gram lattices. The
phrasal segmentation models consistently improve
the BLEU score: +1.1 for both the development
and validation sets, and +1.4 and +0.4 for the in-
domain newswire and out-of-domain newsgroup
test sets. Rescoring MT08 gives gains of +0.9 on
mt08-nist-nw and +0.3 on mt08-nist-ng.
For a limited quantity of training data it is not
always possible to improve translation quality sim-
ply by increasing the order of the language model.
Comparing tables 1 and 2 shows that the gains in
moving from a 5-gram to a 6-gram are small. Even
setting aside the practical difficulty of estimating
and applying such higher-order language models,
it is doubtful that further gains could be had simply
by increasing the order. That the phrasal segmenta-
tion models continue to improve upon the 6-gram
lattice scores suggests they capture more than just
a longer context and that they are complementary
to word-based language models.
The role of the phrase insertion penalty is to
encourage longer phrases in translation. Table 3
shows the effect of tuning this parameter. The
upper part of the table shows the BLEU score,
brevity penalty and individual n-gram precisions.
The lower part shows the total number of words
in the output, the number of words translated as
a phrase of the specified length, and the average
number of words per phrase. When the insertion
21
mt02-05-tune mt02-05-test mt06-nist-nw mt06-nist-ng mt08-nist-nw mt08-nist-ng
TTM+MET 48.9 48.6 46.1 35.2 48.4 33.7
+5g 51.5 51.5 48.4 36.7 49.1 36.4
+5g+PSM 52.6 52.6 49.8 37.1 50.0 36.7
Table 1: BLEU scores for phrasal segmentation model rescoring of 5-gram rescored lattices.
PIP -4.0 -2.0 0.0 2.0 4.0
BLEU 48.6 50.1 51.1 49.9 48.7
BP 0.000 0.000 0.000 -0.034 -0.072
1g 82.0 83.7 84.9 85.7 86.2
2g 57.3 58.9 59.9 60.5 61.1
3g 40.8 42.2 43.1 43.6 44.2
4g 29.1 30.3 31.1 31.5 32.0
words 70550 66964 63505 60847 58676
1 58840 46936 25040 15439 11744
2 7606 12388 18890 19978 18886
3 2691 4890 11532 13920 14295
4 860 1820 5016 6940 8008
5 240 450 1820 2860 3500
6+ 313 480 1207 1710 2243
w/p 1.10 1.21 1.58 1.86 2.02
Table 3: Effect of phrase insertion penalty (PIP)
on BLEU score, brevity penalty (BP), individual
n-gram precisions, phrase length distribution, and
average words per phrase (w/p) for mt02-05-tune.
penalty is too low, single word phrases dominate
the output and any benefits from longer context or
phrase-internal fluency are lost. As the phrase in-
sertion penalty increases, there are large gains in
precision at each order and many longer phrases
appear in the output. At the optimal phrase in-
sertion penalty, the average phrase length is 1.58
words and over 60% of the translation output is
generated from multi-word phrases.
6 Discussion
We have defined a simple model of the phrasal seg-
mentation process for phrase-based SMT and esti-
mated the model parameters from naturally occur-
ring phrase sequence examples in a large training
corpus. Applying first-order models to the NIST
Arabic-English machine translation task, we have
demonstrated complementary improved transla-
tion quality through exploitation of the same abun-
dantly available monolingual data used for training
regular word-based language models.
Comparing the in-domain newswire and out-
of-domain newsgroup test set performance shows
the importance of choosing appropriate data for
training the phrasal segmentation model param-
eters. When in-domain data is of limited avail-
ability, count mixing or other adaptation strategies
may lead to improved performance.
Acknowledgements
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-06-
C-0022.
References
Allauzen, Cyril, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 557?564.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on EMNLP and CoNLL,
pages 858?867.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference for Computational
Linguistics on Human Language Technology, pages
48?54, Morristown, NJ, USA.
Kumar, Shankar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of the conference on HLT
and EMNLP, pages 161?168.
Kumar, Shankar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer translation
template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Kuo, Hong-Kwang Jeff and Wolfgang Reichl. 1999.
Phrase-based language models for speech recogni-
tion. In Sixth European Conference on Speech Com-
munication and Technology, pages 1595?1598.
Mohri, Mehryar, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language,
volume 16, pages 69?88.
Och, Franz Josef. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 160?167, Morristown, NJ, USA.
Ries, Klaus, Finn Dag Bu, and Alex Waibel. 1996.
Class phrase models for language modeling. In Pro-
ceedings of the 4th International Conference on Spo-
ken Language Processing.
22
Proceedings of the Third Workshop on Statistical Machine Translation, pages 131?134,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
European Language Translation with Weighted Finite State Transducers:
The CUED MT System for the 2008 ACL Workshop on SMT
Graeme Blackwood, Adria` de Gispert, Jamie Brunning, William Byrne
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
Trumpington Street, Cambridge, CB2 1PZ, U.K.
{gwb24|ad465|jjjb2|wjb31}@cam.ac.uk
Abstract
We describe the Cambridge University En-
gineering Department phrase-based statisti-
cal machine translation system for Spanish-
English and French-English translation in the
ACL 2008 Third Workshop on Statistical Ma-
chine Translation Shared Task. The CUED
system follows a generative model of trans-
lation and is implemented by composition of
component models realised as Weighted Fi-
nite State Transducers, without the use of a
special-purpose decoder. Details of system
tuning for both Europarl and News translation
tasks are provided.
1 Introduction
The Cambridge University Engineering Department
statistical machine translation system follows the
Transducer Translation Model (Kumar and Byrne,
2005; Kumar et al, 2006), a phrase-based generative
model of translation that applies a series of transfor-
mations specified by conditional probability distri-
butions and encoded as Weighted Finite State Trans-
ducers (Mohri et al, 2002).
The main advantages of this approach are its mod-
ularity, which facilitates the development and eval-
uation of each component individually, and its im-
plementation simplicity which allows us to focus on
modeling issues rather than complex decoding and
search algorithms. In addition, no special-purpose
decoder is required since standard WFST operations
can be used to obtain the 1-best translation or a lat-
tice of alternative hypotheses. Finally, the system
architecture readily extends to speech translation, in
which input ASR lattices can be translated in the
same way as for text (Mathias and Byrne, 2006).
This paper reviews the first participation of CUED
in the ACL Workshop on Statistical Machine Trans-
lation in 2008. It is organised as follows. Firstly,
section 2 describes the system architecture and its
main components. Section 3 gives details of the de-
velopment work conducted for this shared task and
results are reported and discussed in section 4. Fi-
nally, in section 5 we summarise our participation in
the task and outline directions for future work.
2 The Transducer Translation Model
Under the Transducer Translation Model, the gen-
eration of a target language sentence tJ1 starts with
the generation of a source language sentence sI1 by
the source language model PG(sI1). Next, the source
language sentence is segmented into phrases accord-
ing to the unweighted uniform phrasal segmenta-
tion model PW (uK1 ,K|sI1). This source phrase se-
quence generates a reordered target language phrase
sequence according to the phrase translation and re-
ordering model PR(xK1 |uK1 ). Next, target language
phrases are inserted into this sequence according to
the insertion model P?(vR1 |xK1 , uK1 ). Finally, the
sequence of reordered and inserted target language
phrases are transformed to word sequences tJ1 under
the target phrasal segmentation model P?(tJ1 |vR1 ).
These component distributions together form a joint
distribution over the source and target language sen-
tences and their possible intermediate phrase se-
quences as P (tJ1 , vR1 , xK1 , uK1 , sI1).
In translation under the generative model, we start
with the target sentence tJ1 in the foreign language
131
and search for the best source sentence s?I1. Encod-
ing each distribution as a WFST leads to a model of
translation as the series of compositions
L = G ? W ? R ? ? ?? ? T (1)
in which T is an acceptor for the target language
sentence and L is the word lattice of translations ob-
tained during decoding. The most likely translation
s?I1 is the path in L with least cost.
2.1 TTM Reordering Model
The TTM reordering model associates a jump se-
quence with each phrase pair. For the experi-
ments described in this paper, the jump sequence
is restricted such that only adjacent phrases can be
swapped; this is the MJ1 reordering model of (Ku-
mar and Byrne, 2005). Although the reordering
probability for each pair of phrases could be esti-
mated from word-aligned parallel data, we here as-
sume a uniform reordering probability p tuned as de-
scribed in section 3.1. Figure 1 shows how the MJ1
reordering model for a pair of phrases x1 and x2 is
implemented as a WFST.
0 1
x : x
x2 : x1
x1 : x2
p / b=+1
1 / b=?1
1?p / b=0
Figure 1: The uniform MJ1 reordering transducer.
3 System Development
CUED participated in two of the WMT shared task
tracks: French?English and Spanish?English. For
both tracks, primary and contrast systems were sub-
mitted. The primary submission was restricted
to only the parallel and language model data dis-
tributed for the shared task. The contrast submission
incorporates large additional quantities of English
monolingual training text for building the second-
pass language model described in section 3.2.
Table 1 summarises the parallel training data, in-
cluding the total number of sentences, total num-
ber of words, and lower-cased vocabulary size. The
Spanish and French parallel texts each contain ap-
proximately 5% News Commentary data; the rest
is Europarl data. Various single-reference develop-
ment and test sets were provided for each of the
tracks. However, the 2008 evaluation included a new
News task, for which no corresponding development
set was available.
sentences words vocab
FR 39.9M 124k
EN
1.33M 36.4M 106k
ES 38.2M 140k
EN 1.30M 35.7M 106k
Table 1: Parallel corpora statistics.
All of the training and system tuning was per-
formed using lower-cased data. Word alignments
were generated using GIZA++ (Och and Ney, 2003)
over a stemmed version of the parallel text. Stems
for each language were obtained using the Snowball
stemmer1. After unioning the Viterbi alignments,
the stems were replaced with their original words,
and phrase-pairs of up to five foreign words in length
were extracted in the usual fashion (Koehn et al,
2003).
3.1 System Tuning
Minimum error training (Och, 2003) under
BLEU (Papineni et al, 2001) was used to optimise
the feature weights of the decoder with respect
to the dev2006 development set. The following
features are optimized:
? Language model scale factor
? Word and phrase insertion penalties
? Reordering scale factor
? Insertion scale factor
? Translation model scale factor: u-to-v
? Translation model scale factor: v-to-u
? Three phrase pair count features
The phrase-pair count features track whether each
phrase-pair occurred once, twice, or more than twice
1Available at http://snowball.tartarus.org
132
in the parallel text (Bender et al, 2007). All de-
coding and minimum error training operations are
performed with WFSTs and implemented using the
OpenFST libraries (Allauzen et al, 2007).
3.2 English Language Models
Separate language models are used when translating
the Europarl and News sets. The models are esti-
mated using SRILM (Stolcke, 2002) and converted
to WFSTs for use in TTM translation. We use the of-
fline approximation in which failure transitions are
replaced with epsilons (Allauzen et al, 2003).
The Europarl language model is a Kneser-
Ney (Kneser and Ney, 1995) smoothed default-
cutoff 5-gram back-off language model estimated
over the concatenation of the Europarl and News
language model training data. The News language
model is created by optimising the interpolation
weights of two component models with respect to
the News Commentary development sets since we
believe these more closely match the newstest2008
domain. The optimised interpolation weights were
0.44 for the Europarl corpus and 0.56 for the much
smaller News Commentary corpus. For our contrast
submission, we rescore the first-pass translation lat-
tices with a large zero-cutoff stupid-backoff (Brants
et al, 2007) language model estimated over approx-
imately five billion words of newswire text.
4 Results and Discussion
Table 2 reports lower-cased BLEU scores for the
French?English and Spanish?English Europarl
and News translation tasks. The NIST scores are
also provided in parentheses. The row labelled
?TTM+MET? shows results obtained after TTM
translation and minimum error training, i.e. our pri-
mary submission constrained to use only the data
distributed for the task. The row labelled ?+5gram?
shows translation results obtained after rescoring
with the large zero-cutoff 5-gram language model
described in section 3.2. Since this includes addi-
tional language model data, it represents the CUED
contrast submission.
Translation quality for the ES?EN task is
slightly higher than that of FR?EN. For Europarl
translation, most of the additional English language
model training data incorporated into the 5-gram
rescoring step is out-of-domain and so does not sub-
stantially improve the scores. Rescoring yields an
average gain of just +0.5 BLEU points.
Translation quality is significantly lower in both
language pairs for the new news2008 set. Two fac-
tors may account for this. The first is the change
in domain and the fact that no training or devel-
opment set was available for the News translation
task. Secondly, the use of a much freer translation
in the single News reference, which makes it dif-
ficult to obtain a good BLEU score. However, the
second-pass 5-gram language model rescoring gains
are larger than those observed in the Europarl sets,
with approximately +1.7 BLEU points for each lan-
guage pair. The additional in-domain newswire data
clearly helps to improve translation quality.
Finally, we use a simple 3-gram casing model
trained on the true-case workshop distributed
language model data, and apply the SRILM
disambig tool to restore true-case for our final
submissions. With respect to the lower-cased scores,
true-casing drops around 1.0 BLEU in the Europarl
task, and around 1.7 BLEU in the News Commen-
tary and News tasks.
5 Summary
We have reviewed the Cambridge University Engi-
neering Department first participation in the work-
shop on machine translation using a phrase-based
SMT system implemented with a simple WFST ar-
chitecture. Results are largely competitive with the
state-of-the-art in this task.
Future work will examine whether further im-
provements can be obtained by incorporating addi-
tional features into MET, such as the word-to-word
Model 1 scores or phrasal segmentation models. The
MJ1 reordering model could also be extended to al-
low for longer-span phrase movement. Minimum
Bayes Risk decoding, which has been applied suc-
cessfully in other tasks, could also be included.
The difference in the gains from 5-gram lattice
rescoring suggests that, particularly for Europarl
translation, it is important to ensure the language
model data is in-domain. Some form of count mix-
ing or alternative language model adaptation tech-
niques may prove useful for unconstrained Europarl
translation.
133
Task dev2006 devtest2006 test2007 test2008 newstest2008
FR?EN TTM+MET 31.92 (7.650) 32.51 (7.719) 32.94 (7.805) 32.83 (7.799) 19.58 (6.108)
+5gram 32.51 (7.744) 32.96 (7.797) 33.33 (7.880) 33.03 (7.856) 21.22 (6.311)
ES?EN TTM+MET 33.11 (7.799) 32.25 (7.649) 32.90 (7.766) 33.11 (7.859) 20.99 (6.308)
+5gram 33.30 (7.835) 32.96 (7.740) 33.55 (7.857) 33.47 (7.893) 22.83 (6.513)
Table 2: Translation results for the Europarl and News tasks for various dev sets and the 2008 test sets.
Acknowledgements
This work was supported in part under the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of the 41st Meeting of
the Association for Computational Linguistics, pages
557?564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFST: a
general and efficient weighted finite-state transducer
library. In Proceedings of the 9th International Con-
ference on Implementation and Application of Au-
tomata, pages 11?23. Springer.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language trans-
lation system. In Proceedings of the 2007 Automatic
Speech Understanding Workshop, pages 396?401.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858?867.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech, and
Signal Processing, pages 181?184.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference for Computational Lin-
guistics on Human Language Technology, pages 48?
54, Morristown, NJ, USA.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 161?168.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lambert Mathias and William Byrne. 2006. Statistical
phrase-based speech translation. In 2006 IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language, vol-
ume 16, pages 69?88.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Meeting of the Association for Computational
Linguistics, pages 160?167, Morristown, NJ, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Meeting of the Association for Computational
Linguistics, pages 311?318, Morristown, NJ, USA.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
134
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 71?79,
Beijing, August 2010
Fluency Constraints for Minimum Bayes-Risk Decoding
of Statistical Machine Translation Lattices
Graeme Blackwood and Adria` de Gispert and William Byrne
Machine Intelligence Laboratory, Department of Engineering, Cambridge University
{gwb24|ad465|wjb31}@cam.ac.uk
Abstract
A novel and robust approach to improv-
ing statistical machine translation fluency
is developed within a minimum Bayes-
risk decoding framework. By segment-
ing translation lattices according to con-
fidence measures over the maximum like-
lihood translation hypothesis we are able
to focus on regions with potential transla-
tion errors. Hypothesis space constraints
based on monolingual coverage are ap-
plied to the low confidence regions to im-
prove overall translation fluency.
1 Introduction and Motivation
Translation quality is often described in terms of
fluency and adequacy. Fluency reflects the ?na-
tiveness? of the translation while adequacy indi-
cates how well a translation captures the meaning
of the original text (Ma and Cieri, 2006).
From a purely utilitarian view, adequacy should
be more important than fluency. But fluency and
adequacy are subjective and not easy to tease apart
(Callison-Burch et al, 2009; Vilar et al, 2007).
There is a human tendency to rate less fluent trans-
lations as less adequate. One explanation is that
errors in grammar cause readers to be more crit-
ical. A related phenomenon is that the nature of
translation errors changes as fluency improves so
that any errors in fluent translations must be rel-
atively subtle. It is therefore not enough to fo-
cus solely on adequacy. SMT systems must also
be fluent if they are to be accepted and trusted.
It is possible that the reliance on automatic met-
rics may have led SMT researchers to pay insuffi-
cient attention to fluency: BLEU (Papineni et al,
2002), TER (Snover et al, 2006), and METEOR
(Lavie and Denkowski, 2009) show broad corre-
lation with human rankings of MT quality, but are
incapable of fine distinctions between fluency and
adequacy.
There is concern that the fluency of current
SMT is inadequate (Knight, 2007b). SMT is ro-
bust, in that a translation is nearly always pro-
duced. But unlike translators who should be
skilled in at least one of the languages, SMT sys-
tems are limited in both source and target lan-
guage competence. Fluency and accuracy there-
fore tend to suffer together as translation quality
degrades. This should not be the case. Ideally, an
SMT system should never be any less fluent than
the best stochastic text generation system avail-
able in the target language (Oberlander and Brew,
2000). What is needed is a good way to enhance
the fluency of SMT hypotheses.
The maximum likelihood (ML) formulation
(Brown et al, 1990) of translation of source lan-
guage sentence F to target language sentence E?
E? = argmax
E
P (F |E)P (E) (1)
makes it clear why improving SMT fluency is a
difficult modelling problem. The language model
P (E), the closest thing to a ?fluency component?
in the original formulation, only affects candidates
likely under the translation model P (F |E). Given
the weakness of current translation models this is
a severe limitation. It often happens that SMT sys-
tems assign P (F |E?) = 0 to a correct reference
translation E? of F (see the discussion in Section
9). The problem is that in ML decoding the lan-
guage model can only encourage the production
of fluent translations; it cannot easily enforce con-
straints on fluency or introduce new hypotheses.
In Hiero (Chiang, 2007) and syntax-based SMT
(Knight and Graehl, 2005; Knight, 2007a), the
primary role of syntax is to drive the translation
process. Translations produced by these systems
respect the syntax of their translation models, but
71
this does not force them to be grammatical in the
way that a typical human sentence is grammati-
cal; they produce many translations which are not
fluent. The problem is robustness. Generating
fluent translations demands a tightly constraining
target language grammar but such a grammar is at
odds with broad-coverage parsing needed for ro-
bust translation.
We have described two problems in transla-
tion fluency: (1) SMT may fail to generate flu-
ent hypotheses and there is no simple way to in-
troduce them into the search; (2) SMT produces
many translations which are not fluent but enforc-
ing constraints to improve fluency can hurt robust-
ness. Both problems are rooted in the ML decod-
ing framework in which robustness and fluency
are conflicting objectives.
We propose a novel framework to improve the
fluency of any SMT system, whether syntactic or
phrase-based. We will perform Minimum Bayes-
risk search (Kumar and Byrne, 2004) over a space
of fluent hypotheses H:
E?MBR = argmin
E??H
?
E?E
L(E,E?)P (E|F ) (2)
In this approach the MBR evidence space E is
generated by an SMT system as a k-best list or lat-
tice. The system runs in its best possible config-
uration, ensuring both translation robustness and
good baselines. Rather than decoding in the out-
put of the baseline SMT system, translations will
be sought among a collection of fluent sentences
that are close to the top SMT hypotheses as deter-
mined by the loss function L(E,E?).
Decoupling the MBR hypothesis space from
first-pass translation offers great flexibility. Hy-
potheses in H may be arbitrarily constrained ac-
cording to lexical, syntactic, semantic, or other
considerations, with no effect on translation ro-
bustness. This is because constraints on fluency
do not affect the production of the evidence space
by the baseline system. Robustness and fluency
are no longer conflicting objectives. This frame-
work also allows the MBR hypothesis space to be
augmented with hypotheses produced by an NLG
system, although this is beyond the scope of the
present paper.
This paper focuses on searching out fluent
strings amongst the vast number of hypotheses en-
coded in SMT lattices. Oracle BLEU scores com-
puted over k-best lists (Och et al, 2004) show
that many high quality hypotheses are produced
by first-pass SMT decoding. We propose reducing
the difficulty of enhancing the fluency of complete
hypotheses by first identifying regions of high-
confidence in the ML translations and using these
to guide the fluency refinement process. This has
two advantages: (1) we keep portions of the base-
line hypotheses that we trust and search for alter-
natives elsewhere, and (2) the task is made much
easier since the fluency of sentence fragments can
be refined in context.
In what follows, we use posterior probabilities
over SMT lattices to identify useful subsequences
in the ML translations (Sections 2 & 3). These
subsequences drive the segmentation and transfor-
mation of lattices into smaller subproblems (Sec-
tions 4 & 5). Subproblems are mined for fluent
strings (Section 6), resulting in improved transla-
tion fluency (Sections 7 & 8). Our results show
that, when guided by the careful selection of sub-
problems, fluency can be improved with no real
degradation of the BLEU score.
2 Lattice MBR Decoding
The formulation of the MBR decoder in Equation
(2) separates the hypothesis space from the evi-
dence space. We apply the linearised lattice MBR
decision rule (Tromble et al, 2008)
E?LMBR = argmax
E??H
{
?0|E?|+
?
u?N
?u#u(E?)p(u|E)
}
,
(3)
whereH is the hypothesis space, E is the evidence
space, N is the set of all n-grams in H (typically,
n = 1 . . . 4), and ? are constants estimated on
held-out data. The quantity p(u|E) is the path pos-
terior probability of n-gram u
p(u|E) =
?
E?Eu
P (E|F ), (4)
where Eu = {E ? E : #u(E) > 0} is the sub-
set of paths containing n-gram u at least once.
The path posterior probabilities p(u|E) of Equa-
tion (4) can be efficiently calculated (Blackwood
et al, 2010) using general purpose WFST opera-
tions (Mohri et al, 2002).
72
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Posterior probability threshold ?
A
v
e
r
a
g
e
 
p
e
r
?
s
e
n
t
e
n
c
e
 
p
r
e
c
is
io
n 
p n
,
?
 
 
1?gram
2?gram
3?gram
4?gram
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
5
10
15
20
25
30
Posterior probability threshold ?
A
v
e
r
a
g
e
 
p
e
r
?
s
e
n
t
e
n
c
e
 
n
?
g
r
a
m
 
c
o
u
n
t
s
 
 
1?grams
2?grams
3?grams
4?grams
Figure 1: Average n-gram precisions (left) and counts (right) for 2075 sentences of NIST
Arabic?English ML translations at a range of posterior probability thresholds 0 ? ? ? 1. The left
plot shows at ? = 0 the n-gram precisions used in the BLEU score of the ML baseline system.
3 Posterior Probability Confidence
Measures
In the formulation of Equations (3) and (4) the
path posterior n-gram probabilities play a crucial
role. MBR decoding under the linear approxima-
tion to BLEU is driven mainly by the presence
of high posterior n-grams in the lattice; the low
posterior n-grams contribute relatively little to the
MBR decision criterion. Here we investigate the
predictive power of these statistics. We will show
that the n-gram posterior is a good predictor as to
whether or not an n-gram is to be found in a set of
reference translations.
Let Nn denote the set of n-grams of order n
in the ML hypothesis E?, and let Rn denote the
set of n-grams of order n in the union of the ref-
erences. For confidence threshold ?, let Nn,? =
{u?Nn : p(u|E) ? ?} denote the n-grams inNn
with posterior probability greater than or equal to
?, where p(u|E) is computed using Equation (4).
This is equivalent to identifying all substrings of
length n in the translation hypotheses for which
the system assigns a posterior probability of ? or
higher. The precision at order n for threshold ? is
the proportion of n-grams in Nn,? also present in
the references:
Pn,? =
|Rn ? Nn,?|
|Nn,?|
(5)
The left plot in Figure 1 shows average per-
sentence n-gram precisions Pn,? at orders 1. . .4
for an Arabic?English translation task at a range
of thresholds 0 ? ? ? 1. Sentence start and end
tokens are ignored when computing unigram pre-
cisions. We note that precision at all orders im-
proves as the threshold ? increases. This confirms
that these intrinsic measures of translation confi-
dence have strong predictive power.
The right-hand side of the figure shows the av-
erage number of n-grams per sentence for the
same range of ?. We see that for high ?, there are
few n-grams with p(u|E) ? ?; this is as expected.
However, even at a high threshold of ? = 0.9
there are still on average three 4-grams per sen-
tence with posterior probabilities that exceed ?.
Even at this very high confidence level, high pos-
terior n-grams occur frequently enough that we
can expect them to be useful.
These precision results motivate our use of path
posterior n-gram probabilities as a confidence
measure. We assign confidence p(E?ji |E) to sub-
sequences E?i . . . E?j of the ML hypothesis.
Prior work focuses on word-level confidence
extracted from k-best lists and lattices (Ueffing
and Ney, 2007), while Zens and Ney (2006)
rescore k-best lists with n-gram posterior proba-
bilities. Similar experiments with a slightly dif-
ferent motivation are reported by DeNero et al
(2009); they show that expected n-gram counts in
a lattice can be used to predict which n-grams ap-
pear in the references.
4 Lattice Segmentation
We have shown that current SMT systems, al-
though flawed, can identify with confidence par-
73
the newspaper ? constitution ? quoted brigadier abdullah krishan , the chief of police in karak governorate ( 521 km
south @-@ west of amman ) as saying that the seizure took place after police received information that there were
attempts by the group to sell for more than $ 100 thousand dollars , the police rushed to the arrest in possession .
0 11/313.69
2
3
3
7/4.3574
4
23/0.068359
53580
63
7
23/0.0097656
81300
93/4.6934
104/5.7402
11
5/4.7529
12
23
38
1300/2.9102
13
23
14
3580/2.248
15
1300
18
3580
16
4/4.0488
1723/2.5654
1300
19
23/1.2598
3/0.66016
2023
23/1.5156
32
3
231300
21
3/2.8027
225/6.2256
4423
24
1300
23
4/2.5117 25
23
30
23
2623
29
3580
27
3/2.1895
2810/3.9199
1300
1300
3/1.1123
1300
23
33
4/4.4336
31
1300
3
4/1.6416
23
3580
3/2.1895
3
365614
34
3580
353580
375614
4/1.8594
23
39
23
415614
425614
4312316
5512316
575614
56
5614
69
12316
49
12316
50
12/2.9502
52
7359
53
12272/1.9209
51
42/1.04
4512/2.9502
46
42/1.3105
47
7359
48
12272/1.9209
54
8615/6.5205
58
12316
40
12/2.9502
42/1.3105
7359
12272/1.9209
5942
60
7359
618615/3.6523
62
12272/0.66504
112
58332/5.0586
63
4/0.5752
11036/6.5117
64
309/6.1533
65
755/6.3652
66
58332
68
58332
67
4/0.12109
7359
12272/1.9209
42/1.04
707359
7142
58332
72
4/0.5752
58332
1234/0.12109
7359
12272/0.66504
12316
58332/3.2891
747359
758615/3.9463
76
12272/0.91699
78
58332
774/0.69727
79
12
58332
80
4/0.52637
4
58332/4.6484
132
3
81
515/4.1318
82
755/2.9941
58332
83
515
84
5
4
85
3/4.2812
86
309/3.6953 87
515/3.8184
88
755/3.9072
4
755/3.9072
89
3/4.2812
90309/3.6953
91
515/3.8184
58332/3.1895
755/4.7812
3
515/3.9678
58332
7359
12272/0.91699
3
755/2.9941
3
94
12
10565
12272/1.9209
42/1.3105
73
7359
4/0.5752
58332
9358332
92
4/0.69727
58332/0.086914
12
58332
4/0.52637
4
755/3.9072
3/4.2812
309/3.6953
96515/3.8184
58332/4.7764
3
755/3.123
95
515/4.2607
97
10565
58332/3.1895
3
515/3.9678
139755
985
99
5
100
5
101
309
755
102
515/1.4756
103
515
104
755/1.0029
309/2.4023
105
5
106
5
755
755/1.0029
107
515
108
5
4
755/3.9072
515/3.8184 109
309/3.6953
3
755/3.123
111
5
5
58332
4/1.5498
113
3
114
309
115
3
116
95850/0.21289
138
9
1175
5
4/2.9111
95850/0.44141
118
95850309
1193/0.63672
120
95850/1.582
121
309
5
95850/0.44141
309
3/0.63672
515
122
3
95850/0.75391
124
309
9
95850/1.751
95850
125
8716
1263/0.24902
127
95850
309/0.16016
128
3
1298716
95850/0.39355
130
309
1311008
9
95850/0.91602
309
58332/0.84375
95850/0.12402
133
9
134
15/2.5664
153
16
309
95850/0.9082
16
95850/0.12402
9
136309
95850/0.074219
141
3
14213267
15713267
1358716
1505
143
16/6.207
1378716
2035/4.6826
15/2.5664
16
144
13267/5.418
16
1405
146
8716
1475108
14895850
13267
1495108
145
309
9
95850/6.04
5
168
8/2.209
151
368
154368
1703
171
12
1761001/1.9658
15/1.5693
16
13267/3.3818
12 1591001
160
5
152 156
5 1627375
1635108
155 5
161
7375/4.4355
166
7375
181
15
16715
8/4.0586
14226/5.498
169
368
158 16412 1721001
177
5
1657375/5.1436
1807375
17315
174
15
18320
175
20
12
1001/2.3379
5
4/0.94141 20
4/0.9873
20 1791976
368
1861976 189
13
1843/4.5869
16/6.084
178
15
4/0.9873
20
1821976/5.3818 18513
1923
1874854
1883
13
190714
1914854
196
336
195714
193
31/1.3125
197
57/1.459
1984854
205
185
204336
199
4/5.2324
200
185/1.6523
201626
202236
714
31/1.3125
57/1.459
194
63/6.7617
206
185
207
3
20813/3.3818
20924/1.8857
626/0.54102
210
90/0.18164
211
309/1.875
212505
203
626
214
505
215
185
217309
216
3/0.79297
213336
218
309
220309
219
90/3.8145
2213
222
505
22331
225505
224
90/1.124
226
90
227185
288
90
309
3/0.79297
228
24/5.6748
626/4.7695
229
90/4.7461
233
505
232
90/4.0713
231
309
230
90/3.8027
505
505
90/2.3574
234
57
235
90
236
90
237505
239
90
23831
240
5/4.6641
2418
2423
2978
243
505
24431
505
90/3.7266
2455
8/0.95117
266
57/1.7461
24790
24631
24850
5
8
5/2.1689
8
249
3/3.0986
250
505
25113
2523
2533
255
309
254
90/4.8203
25690
257505
2583
278
50
259
505
26013
261505
309 26213
263
83
264309
265309
505
57
267
8
8
2685/2.1689
269309
270
13
271
83
272
3029/2.8291
273
8
27483
275
29
337
63/1.9453
27613
27713
279
3
3083
2803
2814713
28283 63
28329/0.66406
28429
3
29
2853029
3563029
28683
83
2873029/2.8857
505
289309
290309
29113
63/0.8457
29
2923029
293
194
36217
29
63/1.5664
29529
296
63/0.29004
298
13
29913
30083
30117
302
87
303
3
30410/0.21484
306
194
307194
30983
31083 29
63/1.7354
3
305140/5.4287
312
10/0.21484
313
17
294 3
10/0.65234
140/6.0684
314
140
315
1188/5.5186
316
140
317
1188/3.1943
318
5
31987
32087
321309
63
29/1.043
63
140
1188/2.3926
323
3
32410/0.125
326
8
3255/0.27246
327
11/2.5967
328
1188/0.80762
329140
330
5
331
11/5.2559
332
1188/2.5459
333
140
334
1188
33517
33617
31113 32283 29/0.15527
63
400
140
1188/2.4033
338140
341
3/2.9023
342
1188/6.0127
343
3996
339
1188
340
2710/4.4717
401
3
344
5/4.9404
345
8/1.1172
346
11
347
3848/3.9785
348
8
2710/4.2734
349
1188
350
3
3848/3.3076
402
11
351
8/0.44629
8
352
11/2.667
354
8
353
4/0.53027
3
355
10/0.125
3
10/0.20996
11/2.8037
357
5
358
8/2.1064
359
1188/1.2041
360
5
3611188/2.4062
4/1.6289
365
8
363
3/5.5244
364
5/3.4756
366
11/1.3096
367
1579/6.1328
368
3848/3.5664
8
369
5/1.5518
370
11/1.7031
3711579
3848 374
11
372
3/3.1328
373
9/3.7803
375
81/4.0898
376
1188/2.4893
377
1704/4.7871
378
2710/5.6816
379
1579
1579
3848/0.19922
3/0.019531
380
3996
3996/4.1592
3821579
381
3/2.3301
383
1649/4.8389
384
3848/1.6953
385
11
3996
3512/3.4209
3996/2.5566
4/3.2705
388
8
3863/3.7451
387
5/2.584
389
11/3.0176
390
1579/1.8506
391
1649/5.4414
392
3848/0.073242
393
1579
3/2.3301
395
1579
3963848/1.6953
3943996
1579/0.65039
397
3
399
3996
398
8
140
17
1188
11
8/0.44629
1579/6.5
3848/6.1123
3996
406
3/5.6807
403
1579
1579/1.1162
3848/1.3164
404
3
405
1649/2.5039
3996/4.21391649/4.8936
407
3
408
1579/0.054688
409
3848/1.75
410
11
411
11
3
1579
412
5
413
9/2.7051
414
11/2.5723
415
1188/3.8848
416
1704/4.084
81
421
38/4.04
422
143/4.9678
423
188/6.5078
417
1188
4182710/0.94434
419
569
420
775/0.1084
714
163
775/2.3652
425
11
424
9/0.68457
426
20/4.0977
427
130/1.0742
4281704/0.60352
429
3431/4.0811
430
8/0.40527
431
21
432
9
4331704/1.2285
9/3.7803
438
11
439
1704/4.7871
775/5.4775
443
11441
5/4.5166
442
9/1.3047
444
20/5.0928
445
130/3.2822
446
1704/1.4395
440
1579
447
1704
81/3.7598
449
11
448
9/2.5488
450
130/3.375
451
1704/1.54
452
8246/3.0615
81
3848/6.1123
3996
3/5.6807
453
1579
1579/2.0078
3848/2.208
454
3
3/0.21191 3848/1.9619
455
1579
775/4.3457
5/3.6152
11
456
9/1.7539
457
1704/1.4248
458
1704
81/5.1875
11
459
9/4.9961
4601704/4.542
461
5
11
462
1579
463
3996
9/3.7803
81/5.3926
1704/4.7871
20/6.6523
464
11
1188
3996
8/1. 172
11
1704/1.0518
465
11
3848/2.8662
4661579
1704
467
1579
468
1579
5/4.5166
9/1.3047
20/5.0928
1704/1.4395
469
11
470
130/3.2822
11
9/2.5488
1704/1.54
471
130/3.375
472
8246/3.0615
81
473
38/3.3545
474
143/3.3359
81
475
38/4.7412
476
143/4.5361
81/0.95898
542
1188
477
2710/2.8506
478
569/0.63574
479
775
81
38/1.4326
481
1704
480
9/0.081055
21
775/1.7617
9/0.25488
20/3.3711
1704
482
11/1.3154
483
130/0.80176
11
1704/2.5254
484
9/0.31934
485
11
48611
487
561
488
74
489
188/1.9863
490
3250/0.032227
491
3745/1.8066
492
4816/0.54199
493
143
494
143
81
38/1.3213
497
3/3.3994
496
775
495
569/0.53906
498
3431
81
4993/1.8145
50038/1.2832
501
143/0.49512
502
3745/0.70996
503
8
504
21/0.38281
505
8
81
3/1.2666
50638/0.38281
81
507
3/1.6318
775/0.014648
508
569
8
4345
435
9/4.2754
436
11/3.8105
437
1188/3.6299
81/2.2656
1188
2710/2.8506
775
81/0.10449
509
38
130/0.4707
511
1704
510
9/0.081055
81
38/4.04
512
143/4.9678
21
513
5
5149/1.4326
515
11/0.9668
516
1704/2.2139
81
38/2.5791
3/5.5254
519
143/3.0596
775/0.79395
81
517
1704/0.10059
775/1.0488
518
569
520
3431
81/1.75
522
38
521
3/2.3906
523
143/0.81934
524
8
525
21/1.2314
526
8
81
38/3.6436
528
775
527
569/0.38184
81/0.98145
38
3/0.7793
21/1.4082
529
8
81
38/1.166
530
3/0.90234
531
143/0.37305
532
561/0.8584
5/1.2695
9/0.99316
11
533
1704/1.4629
3848/3.7578
534
1579
536
11
535
9/1.3047
537
1704/1.7061
569
775/0.43359
539
21
5388/0.4082
21
5408/0.33887
775
569/0.38184
21/0.69336
541
8
1188
11
9/1.3047
130/3.2822
1704/1.4395
11
1704/1.54
5
54311
81
38/4.04
188/6.5078
143/4.9678
38
5/2.417
9/1.4287
544
11
545
1704/2.7559
5
11/0.92969
20/4.6992
5
9/1.4326
11/0.9668
1704/2.2139
546
130/5.1309
81
38/2.5791
143/3.0596
81/1.75
38
143/0.81934
38
81
143/0.37305
547
561
74/0.45117
3250
4816/0.50977
548
561
3250
4816/0.50977
549
74/0.71582
775/2.4541
20/3.7178
1704/0.69238
3431/3.6172
550
9
551
11/0.39551
552
130/0.37207
553
9
55411
555
11
556
8
775
81
38/0.22363
3/2.1543
81
38/1.2832
143/0.49512
3745/0.70996
557
3/0.82227
775/0.99414
569
81/0.38965
558
10
81
559
38/1.5928
560
5/2.4922
561
3745
562
188
563
1191
565
1191
564
143/0.625
566143
567
1191/0.20703
1191
568
1191
57111
57011
572
561
8
81/0.56348
573
561
574
561
188/1.9551
3250
3745/1.7744
4816/0.50977
575143
81
577
38/0.38281
576
3/2.0371
81
578
3/1.6318
81/0.09082
38
579
561
580
561
581
11
582
561
21/0.38281
8
775
569/0.53906
74
3250/0.032227
4816/0.54199
775/2.917
81
583
11/3.3545
584569/3.9336
585
1704/2.4365
775
586
569/0.63574
81/0.10449
3/3.4795
38
21/0.51562
587
8
5888
589
11
188/2.8662
3250
3745/0.82031
4816/1.4219
590
74/0.55664
5918
593561
592
561
3250
3745/0.82031
4816/1.4219
81/1.4834
3/3.4424
38
81
594
3/0.89746
38
596
11
595
11
81/0.71484
38
597
561
3250
4816/0.50977
3745
598
5/0.037109
599
4816/0.43164
21
8/0.6377
5/2.0967
9/1.4287
11
1704/2.7559
81
38/2.8457
775/1.0488
600569
21/0.96484
6018
81
3/1.5
81
3/2.3506
38/0.38281
81
38/0.15137
81
38/0.38281
81
38/4.5273
81
38/1.9385
21
8/0.6377
38
602
3745
3745
188
603
81/0.34082
775/0.014648
610
569
81/0.25586
10
81
3/2.1406
38/1.5928
81
38/0.38281
81/0.56348
611
561
612
2339
613
561
614
188
615
143
6161191
617
3356
6193356
6181191
6201191
621
3356
622
3356
5691191
623
3356
81/0.28613
10
81
3/3.3311
38/1.5928
5
6245
625
4816/0.8916
3745
5/1.5254
6261191
628
561
627
561
629561
630
3745
631
5
10
5/4.0293
3745
81/0.56543
38
632
11
21/1.7168
633
8
634
11
81/1.1543
38
81/1.4834
38
81/0.25586
635
10
636
38/3.7988
188
637
3250/1.4678 638
3745/0.75
81/0.84473
38
5
639
3745
640
561
81
10/0.13965
81
3/1.7939
38/1.5928
5
4816/0.048828
641
188
642
1191
64311
81/1.2168
38
775
604569/0.53906
81
38/1.6504
3/3.9385
81
38/1.2832
3745/0.70996
606
143/0.49512
605
3/1.5205
607
561/3.8506
608
3250/3.5908
609
4593/3.8887
1191/3.2842
143
163
644
11
188/1.9551
3745/1.7744
646
3250
647
4816/0.50977
81/0.56348
645
561
3745
5/0.037109
648143
5
649
11
5
4816/0.8916
6509/1.6494
737
5
5
3745/0.52344
652
1191
651
143/2.3555
653
1191
654
3356
655
1032
656
3356
657
1032
6583356
659
1032
6601032
661
1032
662
2295/6.8135
663
188
1191/0.25391
664
143
6653356
5
6664816/1.8076
5/3.1465
3745
5
667
3745/1.3447
1191/3.7744
143
668
188
81
10/1.3828
81/1.2715
38
81/0.25586
669
10
670
2339
671
561
1191/0.5625 672
143
143/1.8193
1191
1191/2.8828
143
4816/2.0596
5
3745/1.3447
1191
6733356
81/0.25586
674
10
81/0.28613
10
38/3.8291
3745/2.3125
5
4816/0.8916
9/1.6494
1191
143/0.625
675
4376/3.2324
1191
676
143/2.5205
677
4376/2.5254
678
1191
81/0.52148
10
6794593
680
81/2.3428
681116/4.3838
682
188
6843356
6831191
6853356
686
1032
687
4
689
4
688
3/2.4316
690
71/1.3447
692
4
691
3/1.8115
693
4
3/1.8115
694
4
4
71/3.583
3/1.8115
695
4
696
6/6.2324
697
7/6.3906
698
9/5.1299
699
24/3.6904
700
77/4.2246
701
119/3.668
702
1032
143/1.918
1191
7031191
704
1032
143
1191/0.83887
143
1191
143/1.918
705
2339
5
706
9/4.5615
740
3745/1.1836
5
3745/0.57617
707
1191
7081032
709
2339
710
1032
7111191
712
1032
7133356
715
5
7173356
7181032
3/3.6035
71/2.5166
4
4
3/1.8115
7193
720
309/4.5137
721
9331/2.3145
309/2.1367
723
9331
724
22194/2.8037
722309
7259331
3
9331/2.3145
726
309
309/2.1367
9331
22194/2.8037
727
3/2.9238
3
71/4.9521
524/5.0078
119/4.9316
3
309/4.5137
9331/2.3145
22194/5.1182
728
24/5.543
48 7299331
730
3
731
9331
732
22194/1.9717
22194/1.3242
733
9331
734
3
735
3
736
3356
3/1.8115
4
5
3745/1.5127
738
4593
1191/1.6035
143
739
3356
4
5
3745/1.4502
741
4
742
3356 3/0.46582
743
4
744
1032
1191
143/0.66602
716
163
1191
143/1.6924
8
1191
143/1.5361
3/2.4316
71/1.3447
4
4
71/3.583
3/1.8115
119/3.668
745
24/3.6904
746
77/4.2246
747
309
749
9331
748
1899/1.582
750
8
751309/4.4062
7529331
8
309/4.4062
753
3/4.7422
754
8
7558
757
9331
756
1899/4.4609
758309
22194/0.54492
759
9331
760
8
761
309
762
8
3/0.42383
8
763
8
764
309
765
309
71/1.3447
8044
766
5
4
854
3
767
9331/0.080078
4
524/0.36816
3
9331/0.080078
4
9331
9331
1899/4.3027
803
9331
768
2925/6.7461
770
8
7692925
309/1.9824
771
3
772
8
7738
309
309
309/2.1914
7743
776
8
7752925
777
9331
778
8
779
3
780
9331
309/0.55273
3
309/0.96387
3
781
8/1.3115
78221215
9331
9331
783
4376
784
309
785
8
7878
786
1716
7888
1716/2.3975 789
3
790
2318/1.6875
791
309
792
2318
795
3
793309
794
8
1716/2.4541
3
7962318/1.7441
797
8
309/1.1377
3
798
309
799
8
8002318
8018
802
1032
3
805
9
8072318
8092318
808
1716/0.81934
810
7/0.6084
811
161
8/3.6729
8121716
813
11070/2.5137
161
11070/3.251
8162318
815
1716/0.81934
814
124/3.165
817
5763/4.4824
818
7235/5.5088
819
11744/5.4629
1716
11070/2.5137
820
2318
7/0.6084
823161
821
3/4.7285
879
4/3.4551
822
9/0.92676
3
2318/1.7441
1716
3
2318/1.7441
824
7
826
2318
825
3/2.9131
827
4
9331
828
3005
1716/2.4541
3
2318/1.7441
806
45/5.8838
829
2318
161
830
7/0.77637
833
9
832
7/0.30078
831
9
57/3.4434
834
9
835
109/2.0273
8363005/0.50586
837
9
9
7/4.2412
838
161/4.2617
839
9
161
7/0.98242
845
9/0.92676
9
843
7/0.30078
842
4/3.3135
4/5.543
9 841
7/4.8145
8405
844
9
9
9/0.125
846
7
9
848
4/3.5527
8497/2.3818
880
3005
881
9
45/2.6279
847
3005
85021215
9/0.92676
852
7/0.6084
853
161
8512318
3
5
8846/0.25
7/0.55762
9
9
45/4.8506
3005
45/2.6006
3005
57/2.3096
9
855
3/1.5703
856
5/0.34961
857
3005/0.019531
45
858
3005/0.84277859
63
5/2.6631
6
45/0.20508
860
3005
9
45/1.249
3005
861
45
45/0.48828
8629
21/3.7686
45/5.5586
57/2.3096
9
109/4.7979
3/3.0703
5/1.8926
3005/0.019531
864
63/5.4697
7/3.0781
863
9
865
3005
3005
9
6
5/0.94043
6
7/0.5791
9
3005/2.1006
866
9
86799
868
7/0.30078
9
109/2.0273
9
869
7/2.3818
8822
870
3005
3
5/3.1211
6
6
8835/0.24609
871
9
5/0.89355
6
872
2318
45
873
3005/0.84277
45
874
3005/2.4316
875
9
5/0.99414
6
45
45
3005/1.2275
45
3005/1.5234
9
3005/0.019531
9
5/2.0693
6
161/2.6025 6
8763005
877
7
5/0.24609
6
5
878
3005
5/0.28711
6
3/2.0576
57/1.9053
9
5
6/0.25098
H1 H2 H3 H4 H5 H6 H7 H8 H9
0 11
23
3
7
423
5
3580
63
7
23 8
1300
9
3
10
4
115
1223
13
1300
23
143580
25
1300
15
4
1623
17
3580
1300
18
23
3
1923
23
203
21
3
22
5 23
1300 24
23
23
4
2623
27
23
28
3
2910
30
3580
31
23
1300
1300
3
1300
321300
3
23
33
4
345614
3 35
3580
363580
37
5614
385614
23
4
3923
4012316
3
3580
41
5614
425614
43
12316
4412316
45
5614
46
12
47
42
48
7359
49
12272
50
12316
51
12316
52
12
53
42
64
7359
12272
12
42
7359
12272
548615
55
12316
56
42
7359
12272
57
8615
63
58332
58332
58
4
36
58332
4
42
7359
12272
7359
5942
7359
12272
58332
6012
7359
12272
42
12272
7359
58332
61
8615 62
12
58332
7359
12272
10565
5833212
10565
0 14 23 3755 45 5309 69 0
13
2
95850
38716
52035
8716
45 95850
0 116 213267 35108 4368 512 61001 75 87375 915 1020 111976 1213 133 144854
0 131
2
57
3
714
44
5
185
6236
7336
10185
3
8336
185
9185
3
0 1309 2505 390 413
0
183
2
3029
329
63
4
29
53029
6
194
717
887
9
3
1010
11
140
1217
13
140
14
1188
15140
16
1188
17
5
18
3
1910
205
21
8
2211
23
1188
24
140
255
2611
27
1188
28140
29
1188
30
140
1188
31
140
32
1188
332710
343
35
1188
36
3996
373
38
5
39
8
40
11
41
3848
42
8
2710
43
1188
443
3848
45
8
46
11
8
47
11
48
4
49
8
11
505
51
8
52
1188
535
54
1188
4
55
3
56
5
57
8
58
11
59
1579
603848
8
61
5
62
11
631579
3848
64
3
65
9
66
11
67
81
68
1188
69
1704
70
2710
711579
1579
3848
3
72
3996
3996
73
3
74
1579
751649
76
3848
77
11
3996
3996
3512
4
78
3
79
5
808
81
11
82
1579
83
1649
84
3848
851579
86
3996
3
87
1579
883848
1579
89
3
90
8
91
3996
1188
3996
8
11
1188
8
11
92
1579
1579
3848
93
3
94
1649
1579
3848
3996
95
3
3996
1649
96
3
97
1579
98
3848
99
11
100
11
3
1579
101
5
102
9
103
11
1041188
105
1704
1061188
1072710
108569
109
775
81
110
38
111
143
112
188
113
163775
9
114
11
115
20
116
130
117
1704
118
3431
1198
120
21
121
9
122
1704
5
123
9
124
11
125
1188
9
126
11
127
1704
128
1579
775
129
5
130
9
131
11
132
20
133
130
134
1704
1351704
81
9
136
11
137
130
138
1704
1398246
81
140
1579
1579
3848
3
3848
3996
3
3
3848
141
1579
775
5
11
9
1704
1704
81
11
1704
142
9
1435
11
9
11
130
1704
11
1704
144
1579
3996
9
81
1704
20
145
11
1704
146
11
3848
147
1579
1704
148
1579
149
1579
5
9
20
1704
150
11
151
130
9
11
1704
152
130
153
8246
81
154
38
155
143
81
156
38
157
143
81
158
1188
159
2710
569
160
775
81
38
1619
162
1704
21
775
9
20
1704
163
11
164
130
11
1704
165
9
166
11
167
11
168
561
169
74
170
188
171
3250
172
3745
173
4816
143
174
143
81
38
175
3
176
3431
81
177
3
178
38
179
143
180
3745
181
8
182
21
8
81
38
3
81
183
3
775
184
569
8
775
81
185
38
130
9
186
1704
81
38
143
21
187
5
188
9
189
11
190
1704
775
81
191
1704
775
192
569
81
38
3
193
143
3431
81
38
194
3
195
143
8
196
21
8
81
38
81
38
3
21
8
81
38
197
3
198
143
199
561
5
9
11
1704
11
9
1704
775
569
1188
5
81
38
188
143
38
5
9
11
200
1704
5
11
20
130
5
9
11
1704
81
38
143
81
38
143
38
81
143
201
561
74
3250
4816
202
561
3250
4816
203
74
775
20
1704
3431
11
2049
205
130
9
11
775
206
8
81
38
3
81
38
143
3745
207
3
775
569
81
208
10
81
38
209
5
2103745
188
211
1191
212
143
213
1191
1191
214
143
1191
2151191
216
561
8
81
217
561
218
561
188
3250
3745
4816
143
81
3
21938
81
3
220
561
22111
222
561
21
8
775
81
569
11
1704
775
223
569
81
3
38
8
21
8
22411
188
3250
3745
4816
225
74
226
561
3250
3745
4816
81
227
3
228
561
3250
4816
5
4816
3745
8
21
229
3745
3745
188
230
81
775
231
569
81
3250
38
3745
561
232
3
233
143
234
4593
81
38
81
235
561
236
2339
237
188
238
143
266
3356
239
1191
240
3356
2411191
2423356
5
4816
243
5
5
3745
244
561
5
10
5
3745
245
11
81
38
246
10
3250
3745
188
5
247
561
5
4816
1191
143
163
248
11
249
561
188
3745
250
3250
251
4816
5
48165
2529
253
5 1191
143
254
1191
267
1032
268
3356
255
1032
3356
256
1032
2572295
188
5
3745
81
25810
259
2339
4816
5
3745
81
38
10
4816
3745
5
9
143
1191
4376
1191
4376
2604593
81
188
261
116
3356
4
4
4
9
1032
262
2339
3745
5
263
9
5
8
3745
5
2644593 2655
4376
0 13 2309 39331 48
0
1
3
2
45
30
1716
32318
5763
7235
4
124
51716
6
2318
7
11070
811744
92318
35
9
10
3
11
4
12
7
13
9
14
161
155
4
9
16
7
9
174
18
7
9
7
9
7
9
19
3005
20
9
57
21
9
22
109
37
3005
45
3005
9
23
4
24
7
2545
45
9
7
26
9
63
21
45
57
3
9
109
3005
27
5
3005
28
6
45
45
29
3005
63
5
6
7
9
9
3005
312318
45
32
3005
3
332
6
34
5
36
7
5
161
6
3
57
9
433 1 4 1 6 1 6860 1 76
Figure 2: ML translation E?, word lattice E , and decomposition as a sequence of four string and five
sublattice regions H1 . . .H9 using n-gram posterior probability threshold p(u|E)?0.8.
tial hypotheses that can be trusted. We wish to
constrain MBR decoding to include these trusted
partial hypotheses but allow decoding to consider
alternatives in regions of low confidence. In this
way we aim to improve the best possible output of
the best available systems.
We use the path posterior n-gram probabilities
of Equation (4) to segment lattice E into regions of
high and low confidence. As shown in the exam-
ple of Figure 2, the lattice segmentation process
is performed relative to the ML hypothesis E?, i.e.
relative to the best path through E .
For confidence threshold ?, we find all 4-grams
u = E?i, . . . , E?i+3 in the ML translation for which
p(u|E) > ?. We then segment E? into regions
of high and low confidence where the high confi-
dence regions are identified by consecutive, over-
lapping high confidence 4-grams. The high confi-
dence regions are contiguous strings of words for
which there is consensus amongst the translations
in the lattice. If we trust the path posterior n-gram
probabilities, any hypothesised translation should
include these high confidence substrings. This ap-
proach differs from simple posterior-based prun-
ing in that we discard paths, rather than words
or n-grams, which are not consistent with high-
confidence regions of the ML hypothesis.
The hypothesis string E? is in this way seg-
mented into R alternating subsequences of high
and low confidence. The segment boundaries are
ir and jr so that E?jrir is either a high confidence
or a low confidence subsequence. Each subse-
quence is associated with an unweighted subspace
Hr; this subspace has the form of a string for high
confidence regions and the form of a lattice for
low confidence regions.
If the rth segment is a high confidence region
then Hr accepts only the string E?jrir . If the rth
segment is a region of low confidence, then Hr
is built to accept relevant substrings from E . It is
constructed as follows. The rth low confidence
region E?jrir has a high confidence left context e?r?1
and a high confidence right context e?r+1 formed
from subsequences of the ML translation hypoth-
esis E? as
e?r?1 = E?jr?1ir?1 , e?r+1 = E?
jr+1
ir+1
Note that when r = 1 the left context e?r?1 is the
empty string and when r = R the right context
e?r+1 is the empty string. We build a transducer
74
Tr for the regular expression /. ? e?r?1(.?)e?r+1. ?
/\1/.1 Composition with E yieldsHr = E?Tr , so
that Hr contains all the reasonable alternatives to
E?jrir in E consistent with the high confidence left
and right contexts e?r?1 and e?r+1. IfHr is aligned
to a high confidence subsequence of E?, we call
it a string region since it contains a single path;
if it is aligned to a low confidence region it is a
lattice and we call it a sublattice region. The se-
ries of high and low confidence subspace regions
H1, . . . ,HR defines the lattice segmentation.
5 Hypothesis Space Construction
We now describe a general framework for improv-
ing the fluency of the MBR hypothesis space.
The segmentation of the lattice described in
Section 4 considerably simplifies the problem of
improving the fluency of its hypotheses since each
region of low confidence may be considered in-
dependently. The low confidence regions can be
transformed one-by-one and then reassembled to
form a new MBR hypothesis space.
In order to transform the hypothesis region Hr
it is important to know the context in which it oc-
curs, i.e. the sequences of words that form its pre-
fix and suffix. Some transformations might need
only a short context; others may need a sentence-
level context, i.e. the full sequence of ML words
E?jr?11 and E?Nir+1 to the left and right of the region
Hr that is to be transformed.
To put this formally, each low confidence sub-
lattice region is transformed by the application of
some function ?:
Hr ? ?(E?jr?11 , Hr, E?Nir+1) (6)
The hypothesis space is then constructed from the
concatenation of high confidence string and trans-
formed low confidence sublattice regions
H = E ?
?
1?r?R
Hr (7)
The composition with the original lattice E dis-
cards any new hypotheses that might be created
via the unconstrained concatenation of strings
from theHr. It may be that in some circumstances
1In this notation parentheses indicate string matches so
that /. ? y(a?)w. ? /\1/ applied to xyaaawzz yields aaa.
the introduction of new paths is good, but in what
follows we test the ability to improve fluency by
searching among existing hypotheses, and this en-
sures that nothing new is introduced.
Size of the Hypothesis Space If no new hy-
potheses are introduced by the operations ?, the
size of the hypothesis space H is determined by
the posterior probability threshold ?. Only the
ML hypothesis remains at ? = 0, since all its
subsequences are of high confidence, i.e. can be
covered by n-grams with non-zero path posterior
probability. At the other extreme, for ? = 1, it
follows that H = E and no paths are removed,
since any string regions created are formed from
subsequences that occur on every path in E .
We can therefore use ? to tighten or relax
constraints on the LMBR hypothesis space. At
? = 0, LMBR returns only the ML hypothesis;
at ? = 1, LMBR is done over the full transla-
tion lattice. This is shown in Table 1, where the
BLEU score approaches the BLEU score of un-
constrained LMBR as ? increases.
Note also that the size of the resulting hypoth-
esis space is the product of the number of se-
quences in the sublattice regions. For Figure 2 at
? = 0.8, this product is ?5.4 billion hypotheses.
Even for fairly aggressive constraints on the hy-
pothesis space, many hypotheses remain.
6 Monolingual Coverage Constraints
This section describes one implementation of the
transformation function ? that we will show leads
to improved fluency of machine translation out-
put. This transformation is based on n-gram cov-
erage in a large target language text collection:
where possible, we filter the sublattice regions
so that they contain only long-span n-grams ob-
served in the text. Our motivation is that large
monolingual text collections are good guides to
fluency. If a hypothesis is composed entirely of
previously seen high order n-grams, it is likely to
be fluent and should be favoured.
Initial attempts to identify fluent hypotheses in
sublattice regions by ranking according to n-gram
LM scores were ineffective. Figure 3 shows the
difficulties. We see that both the 4-gram Kneser-
Ney and 5-gram stupid-backoff language models
75
LM Translation hypothesis E and n-gram orders used by the LM to score each word Score
4g <s>1 the2 reactor3 produces3 plutonium2 needed2 to3 manufacture4 atomic3 bomb2 .3 </s>4 -22.59
<s>1 the2 reactor3 produces3 plutonium2 needed2 to3 manufacture4 the4 atomic2 bomb3 .4 </s>4 -23.61
5g <s>1 the2 reactor3 produces4 plutonium5 needed3 to3 manufacture4 atomic5 bomb2 .3 </s>4 -16.04
<s>1 the2 reactor3 produces4 plutonium5 needed3 to3 manufacture4 the4 atomic4 bomb5 .4 </s>5 -17.96
Figure 3: Scores and n-gram orders for hypotheses using 4-gram Kneser-Ney and 5-gram stupid-
backoff (estimated from 1.1B and 6.6B tokens, resp.) LMs. Low confidence regions are in italics.
favour the shorter but disfluent hypothesis; nor-
malising by length was not effective. However,
the stupid-backoff LM has better coverage and the
backing-off behaviour is a clue to the presence
of disfluency. Similar cues have been observed
in ASR analysis (Chase, 1997). The shorter hy-
pothesis backs off to a bigram for ?atomic bomb?,
whereas the longer hypothesis covers the same
words with 4-grams and 5-grams. We therefore
disregard the language model scores and focus on
n-gram coverage. This is an example where ro-
bustness and fluency are at odds. The n-gram
models are robust, but often favour less fluent hy-
potheses.
Let S denote the set of all n-grams in the mono-
lingual training data. To identify partial hypothe-
ses in sublattice regions that have complete mono-
lingual coverage at the maximum order n, we
build a coverage acceptor Cn with a similar form
to the WFST representation of an n-gram backoff
language model (Allauzen et al, 2003). Cn as-
signs a penalty to every n-gram not found in S .
In Cn word arcs have no cost and backoff arcs are
assigned a fixed cost of 1. Firstly, arcs from the
start state are added for each unigram w ? N1:
w
w/0?
Then for n-grams u ? S ? {?ni=2 Ni}, where
u = wn1 consisting of history h = wn?11 and target
word wn, arcs are added
wn/0h h+
where h+ = wn?12 if u has order n and h+ = wn1
if u has order less than n. Backoff arcs are added
for each u as
?/1h h?
where h? = wn?12 if u has order > 2, and bi-
grams backoff to the null history start state ?.
For each sublattice region Hr, we wish to pe-
nalise each path proportionally to the number of
its n-grams not found in the monolingual text col-
lection S . We wish to do this in context, so that
we include the effect of the neighbouring high
confidence regions Hr?1 and Hr+1. Given that
we are counting n-grams at order n we form the
left context machine Lr which accepts the last
n ? 1 words in Hr?1; similarly, Rr accepts the
first n ? 1 words of Hr+1. The concatenation
Xr = Lr?Hr?Rr represents the partial transla-
tion hypotheses inHr padded with n?1 words of
left and right context from the neighbouring high
confidence regions. Composing Xr ? Cn assigns
each partial hypothesis a cost equal to the number
of times it was necessary to back off to lower order
n-grams while reading the string. Partial hypothe-
ses with cost 0 did not back off at all and contain
only maximum order n-grams.
In the following experiments, we look at each
Xn ? Cn and if there are paths with cost 0, only
these are kept and all others discarded. We intro-
duce this as a constraint on the hypothesis space
which we will evaluate for improvement on flu-
ency. Here the transformation function ? returns
Hr as Xr ?Cn after pruning. If Xr ?Cn has no zero
cost paths, the transformation function ? returns
Hr as we find it, since there is not enough mono-
lingual coverage to guide the selection of fluent
hypotheses. After applying monolingual coverage
constraints to each region, the modified hypothe-
sis space used for MBR search is formed by con-
catenation using Equation (7).
We note that Cn is a simplistic NLG system. It
generates strings by concatenating n-grams found
in S . We do not allow it to run ?open loop? in these
experiments, but instead use it to find the strings
in Xr with good n-gram coverage.
7 LMBR Over Segmented Lattices
The effect of fluency constraints on LMBR de-
coding is evaluated in the context of the NIST
Arabic?English MT task. The set tune consists
76
ML ... view , especially with the open chinese economy to the world and ...
+LMBR ... view , especially with the open chinese economy to the world and ...
+LMBR+CC ... view , especially with the opening of the chinese economy to the world and ...
ML ... revision of the constitution of the japanese public , which dates back ...
+LMBR ... revision of the constitution of the japanese public , which dates back ...
+LMBR+CC ... revision of the constitution of japan , which dates back ...
Figure 4: Improved fluency through the application of monolingual coverage constraints to the hypoth-
esis space in MBR decoding of NIST MT 08 Arabic?English newswire lattices.
of the odd numbered sentences of the MT02?
MT05 testsets; the even numbered sentences form
test. MT08 performance on nw08 (newswire) and
ng08 (newsgroup) data is also reported.
First-pass translation is performed using HiFST
(Iglesias et al, 2009), a hierarchical phrase-based
decoder. The first-pass LM is a modified Kneser-
Ney (Kneser and Ney, 1995) 4-gram estimated
over the English side of the parallel text and an
881M word subset of the English GigaWord 3rd
Edition. Prior to LMBR, the first-pass lattices are
rescored with zero-cutoff stupid-backoff 5-gram
language models (Brants et al, 2007) estimated
over more than 6B words of English text. The
LMBR factors ?0, . . . , ?4 are set as in Tromble et
al. (2008) using unigram precision p = 0.85 and
recall ratio r = 0.74.
The effect of performing LMBR over the seg-
mented hypothesis space is shown in Table 1. The
hypothesis subspaces Hr are constructed at var-
ious confidence thresholds as described in Sec-
tion 4 with H formed via Equation (7); no cover-
age constraints are applied yet. Constraining the
search space using ? = 0.6 leads to little degra-
dation in LMBR performance under BLEU. This
shows lattice segmentation works as intended.
We next investigate the effect of monolingual
coverage constraints on BLEU. We build accep-
tors Cn as described in Section 6 with S con-
sisting of all n-grams in the English GigaWord.
At ? = 0.6 we found 181 sentences with sub-
lattices Hr spanned by maximum order n-grams
from S , i.e. for which Xr ? Cn have paths with
cost 0; these are filtered as described. LMBR
over these coverage-constrained sublattices is de-
noted LMBR+CC. On nw08 the BLEU score for
LMBR+CC is 52.0 which is +0.7 over the ML de-
coder and only -0.2 BLEU below unconstrained
LMBR decoding. Done in this way, constraining
hypotheses to have 5-grams from the GigaWord
tune test nw08 ng08
ML 54.2 53.8 51.3 36.3
?
0.0 54.2 53.8 51.3 36.3
0.2 54.3 53.8 51.3 36.3
0.4 54.6 54.2 51.6 36.7
0.6 54.9 54.4 52.1 36.6
0.8 54.9 54.4 52.1 36.6
1.0 54.9 54.4 52.2 36.7
LMBR 54.9 54.4 52.2 36.8
Table 1: BLEU scores for ML hypotheses and
LMBR decoding inH over 0 ? ? ? 1.
has little impact on BLEU.
At this value of ?, 116 of the 813 nw08 sen-
tences have a low confidence region (1) com-
pletely covered by 5-grams, and (2) within which
the ML hypothesis and the LMBR+CC hypothe-
sis differ. It is these regions which we will inspect
for improved fluency.
8 Human Fluency Evaluation
We asked 17 native speakers to judge the fluency
of sentence fragments from nw08. We compared
hypotheses from the ML and the LMBR+CC de-
coders. Each fragment consisted of the partial
translation hypothesis from a low confidence re-
gion together with its left and right high confi-
dence contexts (examples given in Figure 4). For
each sample, judges were asked: ?Could this frag-
ment occur in a fluent sentence??
The results are shown in Table 2. Most of the
time, the ML and LMBR+CC sentence fragments
were both judged to be fluent; it often happened
that they differed by only a single noun or verb
substitution which didn?t affect fluency. In a small
number of cases, both ML and LMBR+CC were
judged to be disfluent. We are most interested in
the ?off-diagonal? cases. In cases when one sys-
tem was judged to be fluent and the other was not,
LMBR+CC was preferred about twice as often as
the ML baseline (26.9% to 9.7%). In other words,
the monolingual fluency constraints were judged
77
LMBR+CC
Fluent Not Fluent
ML Fluent 1175 (59.6%) 192 (9.7%)Not Fluent 530 (26.9%) 75 (3.8%)
Table 2: Partial hypothesis fluency judgements.
to have improved the fluency of the low confi-
dence region more than twice as often as a fluent
hypothesis was made disfluent.
Some examples of improved fluency are shown
in Figure 4. Although both the ML and un-
constrained LMBR hypotheses might satisfy ad-
equacy, they lack the fluency of the LMBR+CC
hypotheses generated using monolingual fluency
constraints.
9 Summary and Discussion
We have described a general framework for im-
proving SMT fluency. Decoupling the hypothesis
space from the evidence space allows for much
greater flexibility in lattice MBR search.
We have shown that high path posterior proba-
bility n-grams in the ML translation can be used to
guide the segmentation of a lattice into regions of
high and low confidence. Segmenting the lattice
simplifies the process of refining the hypothesis
space since low confidence regions can be refined
in the context of their high confidence neighbours.
This can be done independently before reassem-
bling the refined regions. Lattice segmentation
facilitates the application of post-processing and
rescoring techniques targeted to address particu-
lar deficiencies in ML decoding.
The techniques we presented are related to con-
sensus decoding and system combination for SMT
(Matusov et al, 2006; Sim et al, 2007), and to
segmental MBR for automatic speech recognition
(Goel et al, 2004). Mohit et al (2009) describe
an alternative approach to improving specific por-
tions of translation hypotheses. They use an SVM
classifier to identify a single phrase in each source
language sentence that is ?difficult to translate?;
such phrases are then translated using an adapted
language model estimated from parallel data. In
contrast to their approach, our approach is able
to exploit large collections of monolingual data to
refine multiple low confidence regions using pos-
terior probabilities obtained from a high-quality
evidence space of first-pass translations.
Testset Sentences Reachability
tune 2075 15%
test 2040 14%
nw08 813 11%
ng08 547 9%
Table 3: Arabic?English reference reachability.
We applied hypothesis space constraints based
on monolingual coverage to low confidence re-
gions resulting in improved fluency with no real
degradation in BLEU score relative to uncon-
strained LMBR decoding. This approach is lim-
ited by the coverage of sublattices using monolin-
gual text. We expect this to improve with larger
text collections or in tightly focused scenarios
where in-domain text is less diverse.
However, fluency will be best improved by inte-
grating more sophisticated natural language gen-
eration. NLG systems capable of generating sen-
tence fragments in context can be incorporated di-
rectly into this framework. If the MBR hypothe-
sis spaceH contains a generated hypothesis E? for
which P (F |E?) = 0, E? could still be produced as
a translation, since it can be ?voted for? by nearby
hypotheses produced by the underlying system.
Table 3 shows the proportion of NIST testset
sentences that can be aligned to any of the ref-
erence translations using our high quality base-
line hierarchical decoder with a powerful gram-
mar. The low level of reachability suggests that
NLG may be required to achieve high levels of
translation quality and fluency. Other rescoring
approaches (Kumar et al, 2009; Li et al, 2009)
may also benefit from NLG when the baseline is
incapable of generating the reference.
We note that our approach could also be used to
improve the fluency of ASR, OCR and other lan-
guage processing tasks where the goal is to pro-
duce fluent natural language output.
Acknowledgments
We would like to thank Matt Gibson and the
human judges who participated in the evalua-
tion. This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022 and the European Union Seventh
Framework Programme (FP7-ICT-2009-4) under
Grant Agreement No. 247762.
78
References
Allauzen, Cyril, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of ACL 2003.
Blackwood, Graeme, Adria` de Gispert, and William Byrne.
2010. Efficient path counting transducers for minimum
Bayes-risk decoding of statistical machine translation lat-
tices. In Proceedings of ACL 2010.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in ma-
chine translation. In Proceedings of the EMNLP 2007.
Brown, Peter F., John Cocke, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, Fredrick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin. 1990. A sta-
tistical approach to machine translation. Computational
Linguistics, 16(2):79?85.
Callison-Burch, Chris, Philipp Koehn, Christof Monz, and
Josh Schroeder. 2009. Findings of the 2009 Workshop
on Statistical Machine Translation. In WMT 2009.
Chase, Lin Lawrance. 1997. Error-responsive feed-
back mechanisms for speech recognizers, Ph.D. Thesis,
Carnegie Mellon University.
Chiang, David. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
DeNero, John, David Chiang, and Kevin Knight. 2009. Fast
consensus decoding over translation forests. In Proceed-
ings of ACL-IJCNLP 2009.
Goel, V., S. Kumar, and W. Byrne. 2004. Segmental mini-
mum Bayes-risk decoding for automatic speech recogni-
tion. IEEE Transactions on Speech and Audio Process-
ing, 12:234?249.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo R. Banga, and
William Byrne. 2009. Hierarchical phrase-based trans-
lation with weighted finite state transducers. In Proceed-
ings of the 2009 Annual Conference of the NAACL.
Kneser, R. and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech, and
Signal Processing.
Knight, K and J Graehl. 2005. An overview of probabilis-
tic tree transducers for natural language processing. In
Proceedings of CICLING 2005.
Knight, K. 2007a. Capturing practical natural language
transformations. Machine Translation, 21(2).
Knight, Kevin. 2007b. Automatic language translation gen-
eration help needs badly. In MT Summit XI Workshop on
Using Corpora for NLG: Keynote Address.
Kumar, Shankar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine translation. In
NAACL 2004.
Kumar, Shankar, Wolfgang Macherey, Chris Dyer, and Franz
Och. 2009. Efficient minimum error rate training and
minimum bayes-risk decoding for translation hypergraphs
and lattices. In Proceedings of ACL-IJCNLP 2009.
Lavie, Alon and Michael J. Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine trans-
lation. Machine Translation Journal.
Li, Zhifei, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine translation.
In Proceedings of ACL-IJCNLP 2009.
Ma, Xiaoyi and Christopher Cieri. 2006. Corpus support for
machine translation at LDC. In LREC 2006.
Matusov, Evgeny, Nicola Ueffing, and Hermann Ney. 2006.
Computing consensus translation from multiple machine
translation systems using enhanced hypotheses align-
ment. In 11th Conference of the EACL.
Mohit, B., F. Liberato, and R. Hwa. 2009. Language model
adaptation for difficult-to-translate phrases. In Proceed-
ings of the 13th Annual Conference of the EAMT.
Mohri, Mehryar, Fernando Pereira, and Michael Riley. 2002.
Weighted finite-state transducers in speech recognition.
In CSL, volume 16, pages 69?88.
Oberlander, Jon and Chris Brew. 2000. Stochastic text gen-
eration. In Philosophical Transactions of the Royal Soci-
ety.
Och, F., D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, and D. Radev. 2004. A smorgasbord of features
for statistical machine translation. In Proceedings of the
HLT Conference of the NAACL.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a method for automatic evaluation of
machine translation. In Proceedings of ACL 2002.
Sim, K.-C., W. Byrne, M. Gales, H. Sahbi, and P.C. Wood-
land. 2007. Consensus network decoding for statisti-
cal machine translation system combination. In ICASSP
2007.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, , and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Pro-
ceedings of AMTA.
Tromble, Roy, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum Bayes-risk decoding
for statistical machine translation. In Proceedings of the
2008 Conference on EMNLP.
Ueffing, Nicola and Hermann Ney. 2007. Word-level confi-
dence estimation for machine translation. Computational
Linguistics, 33(1):9?40.
Vilar, D, G Leusch, H Ney, and R Banchs. 2007. Human
evaluation of machine translation through binary system
comparisons. In Proceedings of WMT 2007.
Zens, Richard and Hermann Ney. 2006. N -gram posterior
probabilities for statistical machine translation. In Pro-
ceedings of WMT 2006.
79
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 736?746,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Syntax-Based Word Ordering Incorporating a Large-Scale Language
Model
Yue Zhang
University of Cambridge
Computer Laboratory
yz360@cam.ac.uk
Graeme Blackwood
University of Cambridge
Engineering Department
gwb24@eng.cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Abstract
A fundamental problem in text generation
is word ordering. Word ordering is a com-
putationally difficult problem, which can
be constrained to some extent for particu-
lar applications, for example by using syn-
chronous grammars for statistical machine
translation. There have been some recent
attempts at the unconstrained problem of
generating a sentence from a multi-set of
input words (Wan et al 2009; Zhang and
Clark, 2011). By using CCG and learn-
ing guided search, Zhang and Clark re-
ported the highest scores on this task. One
limitation of their system is the absence
of an N-gram language model, which has
been used by text generation systems to
improve fluency. We take the Zhang and
Clark system as the baseline, and incor-
porate an N-gram model by applying on-
line large-margin training. Our system sig-
nificantly improved on the baseline by 3.7
BLEU points.
1 Introduction
One fundamental problem in text generation is
word ordering, which can be abstractly formu-
lated as finding a grammatical order for a multi-
set of words. The word ordering problem can also
include word choice, where only a subset of the
input words are used to produce the output.
Word ordering is a difficult problem. Finding
the best permutation for a set of words accord-
ing to a bigram language model, for example, is
NP-hard, which can be proved by linear reduction
from the traveling salesman problem. In prac-
tice, exploring the whole search space of permu-
tations is often prevented by adding constraints.
In phrase-based machine translation (Koehn et al
2003; Koehn et al 2007), a distortion limit is
used to constrain the position of output phrases.
In syntax-based machine translation systems such
as Wu (1997) and Chiang (2007), synchronous
grammars limit the search space so that poly-
nomial time inference is feasible. In fluency
improvement (Blackwood et al 2010), parts of
translation hypotheses identified as having high
local confidence are held fixed, so that word or-
dering elsewhere is strictly local.
Some recent work attempts to address the fun-
damental word ordering task directly, using syn-
tactic models and heuristic search. Wan et al
(2009) uses a dependency grammar to solve word
ordering, and Zhang and Clark (2011) uses CCG
(Steedman, 2000) for word ordering and word
choice. The use of syntax models makes their
search problems harder than word permutation us-
ing an N -gram language model only. Both meth-
ods apply heuristic search. Zhang and Clark de-
veloped a bottom-up best-first algorithm to build
output syntax trees from input words, where
search is guided by learning for both efficiency
and accuracy. The framework is flexible in allow-
ing a large range of constraints to be added for
particular tasks.
We extend the work of Zhang and Clark (2011)
(Z&C) in two ways. First, we apply online large-
margin training to guide search. Compared to the
perceptron algorithm on ?constituent level fea-
tures? by Z&C, our training algorithm is theo-
retically more elegant (see Section 3) and con-
verges more smoothly empirically (see Section 5).
Using online large-margin training not only im-
proves the output quality, but also allows the in-
corporation of an N -gram language-model into
736
the system. N -gram models have been used as a
standard component in statistical machine trans-
lation, but have not been applied to the syntac-
tic model of Z&C. Intuitively, an N -gram model
can improve local fluency when added to a syntax
model. Our experiments show that a four-gram
model trained using the English GigaWord cor-
pus gave improvements when added to the syntax-
based baseline system.
The contributions of this paper are as follows.
First, we improve on the performance of the Z&C
system for the challenging task of the general
word ordering problem. Second, we develop a
novel method for incorporating a large-scale lan-
guage model into a syntax-based generation sys-
tem. Finally, we analyse large-margin training in
the context of learning-guided best-first search,
offering a novel solution to this computationally
hard problem.
2 The statistical model and decoding
algorithm
We take Z&C as our baseline system. Given
a multi-set of input words, the baseline system
builds a CCG derivation by choosing and ordering
words from the input set. The scoring model is
trained using CCGBank (Hockenmaier and Steed-
man, 2007), and best-first decoding is applied. We
apply the same decoding framework in this paper,
but apply an improved training process, and incor-
porate an N -gram language model into the syntax
model. In this section, we describe and discuss
the baseline statistical model and decoding frame-
work, motivating our extensions.
2.1 Combinatory Categorial Grammar
CCG, and parsing with CCG, has been described
elsewhere (Clark and Curran, 2007; Hockenmaier
and Steedman, 2002); here we provide only a
short description.
CCG (Steedman, 2000) is a lexicalized gram-
mar formalism, which associates each word in a
sentence with a lexical category. There is a small
number of basic lexical categories, such as noun
(N), noun phrase (NP), and prepositional phrase
(PP). Complex lexical categories are formed re-
cursively from basic categories and slashes, which
indicate the directions of arguments. The CCG
grammar used by our system is read off the deriva-
tions in CCGbank, following Hockenmaier and
Steedman (2002), meaning that the CCG combina-
tory rules are encoded as rule instances, together
with a number of additional rules which deal with
punctuation and type-changing. Given a sentence,
its CCG derivation can be produced by first assign-
ing a lexical category to each word, and then re-
cursively applying CCG rules bottom-up.
2.2 The decoding algorithm
In the decoding algorithm, a hypothesis is an
edge, which corresponds to a sub-tree in a CCG
derivation. Edges are built bottom-up, starting
from leaf edges, which are generated by assigning
all possible lexical categories to each input word.
Each leaf edge corresponds to an input word with
a particular lexical category. Two existing edges
can be combined if there exists a CCG rule which
combines their category labels, and if they do not
contain the same input word more times than its
total count in the input. The resulting edge is as-
signed a category label according to the combi-
natory rule, and covers the concatenated surface
strings of the two sub-edges in their order or com-
bination. New edges can also be generated by ap-
plying unary rules to a single existing edge. Start-
ing from the leaf edges, the bottom-up process is
repeated until a goal edge is found, and its surface
string is taken as the output.
This derivation-building process is reminiscent
of a bottom-up CCG parser in the edge combina-
tion mechanism. However, it is fundamentally
different from a bottom-up parser. Since, for
the generation problem, the order of two edges
in their combination is flexible, the search prob-
lem is much harder than that of a parser. With
no input order specified, no efficient dynamic-
programming algorithm is available, and less con-
textual information is available for disambigua-
tion due to the lack of an input string.
In order to combat the large search space, best-
first search is applied, where candidate hypothe-
ses are ordered by their scores, and kept in an
agenda, and a limited number of accepted hy-
potheses are recorded in a chart. Here the chart
is essentially a set of beams, each of which con-
tains the highest scored edges covering a particu-
lar number of words. Initially, all leaf edges are
generated and scored, before they are put onto the
agenda. During each step in the decoding process,
the top edge from the agenda is expanded. If it is
a goal edge, it is returned as the output, and the
737
Algorithm 1 The decoding algorithm.
a? INITAGENDA( )
c? INITCHART( )
while not TIMEOUT( ) do
new? []
e? POPBEST(a)
if GOALTEST(e) then
return e
end if
for e? ? UNARY(e, grammar) do
APPEND(new, e)
end for
for e? ? c do
if CANCOMBINE(e, e?) then
e?? BINARY(e, e?, grammar)
APPEND(new, e?)
end if
if CANCOMBINE(e?, e) then
e?? BINARY(e?, e, grammar)
APPEND(new, e?)
end if
end for
for e? ? new do
ADD(a, e?)
end for
ADD(c, e)
end while
decoding finishes. Otherwise it is extended with
unary rules, and combined with existing edges in
the chart using binary rules to produce new edges.
The resulting edges are scored and put onto the
agenda, while the original edge is put onto the
chart. The process repeats until a goal edge is
found, or a timeout limit is reached. In the latter
case, a default output is produced using existing
edges in the chart.
Pseudocode for the decoder is shown as Algo-
rithm 1. Again it is reminiscent of a best-first
parser (Caraballo and Charniak, 1998) in the use
of an agenda and a chart, but is fundamentally dif-
ferent due to the fact that there is no input order.
2.3 Statistical model and feature templates
The baseline system uses a linear model to score
hypotheses. For an edge e, its score is defined as:
f(e) = ?(e) ? ?,
where ?(e) represents the feature vector of e and
? is the parameter vector of the model.
During decoding, feature vectors are computed
incrementally. When an edge is constructed, its
score is computed from the scores of its sub-edges
and the incrementally added structure:
f(e) = ?(e) ? ?
=
(
(
?
es?e
?(es)
)
+ ?(e)
)
? ?
=
(
?
es?e
?(es) ? ?
)
+ ?(e) ? ?
=
(
?
es?e
f(es)
)
+ ?(e) ? ?
In the equation, es ? e represents a sub-edge of
e. Leaf edges do not have any sub-edges. Unary-
branching edges have one sub-edge, and binary-
branching edges have two sub-edges. The fea-
ture vector ?(e) represents the incremental struc-
ture when e is constructed over its sub-edges.
It is called the ?constituent-level feature vector?
by Z&C. For leaf edges, ?(e) includes informa-
tion about the lexical category label; for unary-
branching edges, ?(e) includes information from
the unary rule; for binary-branching edges, ?(e)
includes information from the binary rule, and ad-
ditionally the token, POS and lexical category bi-
grams and trigrams that result from the surface
string concatenation of its sub-edges. The score
f(e) is therefore the sum of f(es) (for all es ? e)
plus ?(e) ??. The feature templates we use are the
same as those in the baseline system.
An important aspect of the scoring model is that
edges with different sizes are compared with each
other during decoding. Edges with different sizes
can have different numbers of features, which can
make the training of a discriminative model more
difficult. For example, a leaf edge with one word
can be compared with an edge over the entire in-
put. One way of reducing the effect of the size dif-
ference is to include the size of the edge as part of
feature definitions, which can improve the compa-
rability of edges of different sizes by reducing the
number of features they have in common. Such
features are applied by Z&C, and we make use of
them here. Even with such features, the question
of whether edges with different sizes are linearly
separable is an empirical one.
3 Training
The efficiency of the decoding algorithm is de-
pendent on the statistical model, since the best-
738
first search is guided to a solution by the model,
and a good model will lead to a solution being
found more quickly. In the ideal situation for the
best-first decoding algorithm, the model is perfect
and the score of any gold-standard edge is higher
than the score of any non-gold-standard edge. As
a result, the top edge on the agenda is always a
gold-standard edge, and therefore all edges on the
chart are gold-standard before the gold-standard
goal edge is found. In this oracle procedure, the
minimum number of edges is expanded, and the
output is correct. The best-first decoder is perfect
in not only accuracy, but also speed. In practice
this ideal situation is rarely met, but it determines
the goal of the training algorithm: to produce the
perfect model and hence decoder.
If we take gold-standard edges as positive ex-
amples, and non-gold-standard edges as negative
examples, the goal of the training problem can be
viewed as finding a large separating margin be-
tween the scores of positive and negative exam-
ples. However, it is infeasible to generate the full
space of negative examples, which is factorial in
the size of input. Like Z&C, we apply online
learning, and generate negative examples based
on the decoding algorithm.
Our training algorithm is shown as Algo-
rithm 2. The algorithm is based on the decoder,
where an agenda is used as a priority queue of
edges to be expanded, and a set of accepted edges
is kept in a chart. Similar to the decoding algo-
rithm, the agenda is intialized using all possible
leaf edges. During each step, the top of the agenda
e is popped. If it is a gold-standard edge, it is ex-
panded in exactly the same way as the decoder,
with the newly generated edges being put onto
the agenda, and e being inserted into the chart.
If e is not a gold-standard edge, we take it as a
negative example e?, and take the lowest scored
gold-standard edge on the agenda e+ as a positive
example, in order to make an udpate to the model
parameter vector ?. Our parameter update algo-
rithm is different from the baseline perceptron al-
gorithm, as will be discussed later. After updating
the parameters, the scores of agenda edges above
and including e?, together with all chart edges,
are updated, and e? is discarded before the start
of the next processing step. By not putting any
non-gold-standard edges onto the chart, the train-
ing speed is much faster; on the other hand a wide
range of negative examples is pruned. We leave
Algorithm 2 The training algorithm.
a? INITAGENDA( )
c? INITCHART( )
while not TIMEOUT( ) do
new? []
e? POPBEST(a)
if GOLDSTANDARD(e) and GOALTEST(e)
then return e
end if
if not GOLDSTANDARD(e) then
e?? e
e+?MINGOLD(a)
UPDATEPARAMETERS(e+ , e?)
RECOMPUTESCORES(a, c)
continue
end if
for e? ? UNARY(e, grammar) do
APPEND(new, e)
end for
for e? ? c do
if CANCOMBINE(e, e?) then
e?? BINARY(e, e?, grammar)
APPEND(new, e?)
end if
if CANCOMBINE(e?, e) then
e?? BINARY(e?, e, grammar)
APPEND(new, e?)
end if
end for
for e? ? new do
ADD(a, e?)
end for
ADD(c, e)
end while
for further work possible alternative methods to
generate more negative examples during training.
Another way of viewing the training process is
that it pushes gold-standard edges towards the top
of the agenda, and crucially pushes them above
non-gold-standard edges. This is the view de-
scribed by Z&C. Given a positive example e+ and
a negative example e?, they use the perceptron
algorithm to penalize the score for ?(e?) and re-
ward the score of ?(e+), but do not update pa-
rameters for the sub-edges of e+ and e?. An argu-
ment for not penalizing the sub-edge scores for e?
is that the sub-edges must be gold-standard edges
(since the training process is constructed so that
only gold-standard edges are expanded). From
739
the perspective of correctness, it is unnecessary
to find a margin between the sub-edges of e+ and
those of e?, since both are gold-standard edges.
However, since the score of an edge not only
represents its correctness, but also affects its pri-
ority on the agenda, promoting the sub-edge of
e+ can lead to ?easier? edges being constructed
before ?harder? ones (i.e. those that are less
likely to be correct), and therefore improve the
output accuracy. This perspective has been ob-
served by other works of learning-guided-search
(Shen et al 2007; Shen and Joshi, 2008; Gold-
berg and Elhadad, 2010). Intuitively, the score
difference between easy gold-standard and harder
gold-standard edges should not be as great as the
difference between gold-standard and non-gold-
standard edges. The perceptron update cannot
provide such control of separation, because the
amount of update is fixed to 1.
As described earlier, we treat parameter update
as finding a separation between correct and incor-
rect edges, in which the global feature vectors ?,
rather than ?, are considered. Given a positive ex-
ample e+ and a negative example e?, we make a
minimum update so that the score of e+ is higher
than that of e? with some margin:
? ? argmin
??
? ????0 ?, s.t.?(e+)????(e?)?? ? 1
where ?0 and ? denote the parameter vectors be-
fore and after the udpate, respectively. The up-
date is similar to the update of online large-margin
learning algorithms such as 1-best MIRA (Cram-
mer et al 2006), and has a closed-form solution:
? ? ?0+
f(e?)? f(e+) + 1
? ?(e+)? ?(e?) ?2
(
?(e+)??(e?)
)
In this update, the global feature vectors ?(e+)
and ?(e?) are used. Unlike Z&C, the scores
of sub-edges of e+ and e? are also udpated, so
that the sub-edges of e? are less prioritized than
those of e+. We show empirically that this train-
ing algorithm significantly outperforms the per-
ceptron training of the baseline system in Sec-
tion 5. An advantage of our new training algo-
rithm is that it enables the accommodation of a
separately trained N -gram model into the system.
4 Incorporating an N-gram language
model
Since the seminal work of the IBM models
(Brown et al 1993), N -gram language models
have been used as a standard component in statis-
tical machine translation systems to control out-
put fluency. For the syntax-based generation sys-
tem, the incorporation of an N -gram language
model can potentially improve the local fluency
of output sequences. In addition, the N -gram
language model can be trained separately using
a large amount of data, while the syntax-based
model requires manual annotation for training.
The standard method for the combination of
a syntax model and an N -gram model is linear
interpolation. We incorporate fourgram, trigram
and bigram scores into our syntax model, so that
the score of an edge e becomes:
F (e) = f(e) + g(e)
= f(e) + ? ? gfour(e) + ? ? gtri(e) + ? ? gbi(e),
where f is the syntax model score, and g is the
N -gram model score. g consists of three com-
ponents, gfour, gtri and gbi, representing the log-
probabilities of fourgrams, trigrams and bigrams
from the language model, respectively. ?, ? and
? are the corresponding weights.
During decoding, F (e) is computed incremen-
tally. Again, denoting the sub-edges of e as es,
F (e) = f(e) + g(e)
=
(
?
es?e
F (es)
)
+ ?(e)? + g?(e)
Here g?(e) = ? ?g?four(e)+? ?g?tri(e)+? ?g?bi(e)
is the sum of log-probabilities of the new N -
grams resulting from the construction of e. For
leaf edges and unary-branching edges, no new N -
grams result from their construction (i.e. g? = 0).
For a binary-branching edge, new N -grams result
from the surface-string concatenation of its sub-
edges. The sum of log-probabilities of the new
fourgrams, trigrams and bigrams contribute to g?
with weights ?, ? and ?, respectively.
For training, there are at least three methods to
tune ?, ?, ? and ?. One simple method is to train
the syntax model ? independently, and select ?,
?, and ? empirically from a range of candidate
values according to development tests. We call
this method test-time interpolation. An alterna-
tive is to select ?, ? and ? first, initializing the
vector ? as all zeroes, and then run the training
algorithm for ? taking into account the N -gram
language model. In this process, g is considered
when finding a separation between positive and
740
negative examples; the training algorithm finds a
value of ? that best suits the precomputed ?, ?
and ? values, together with the N -gram language
model. We call this method g-precomputed in-
terpolation. Yet another method is to initialize ?,
?, ? and ? as all zeroes, and run the training al-
gorithm taking into account the N -gram language
model. We call this method g-free interpolation.
The incorporation of an N -gram language
model into the syntax-based generation system is
weakly analogous to N -gram model insertion for
syntax-based statistical machine translation sys-
tems, both of which apply a score from the N -
gram model component in a derivation-building
process. As discussed earlier, polynomial-time
decoding is typically feasible for syntax-based
machine translation systems without an N -gram
language model, due to constraints from the
grammar. In these cases, incorporation of N -
gram language models can significantly increase
the complexity of a dynamic-programming de-
coder (Bar-Hillel et al 1961). Efficient search
has been achieved using chart pruning (Chiang,
2007) and iterative numerical approaches to con-
strained optimization (Rush and Collins, 2011).
In contrast, the incorporation of an N -gram lan-
guage model into our decoder is more straightfor-
ward, and does not add to its asymptotic complex-
ity, due to the heuristic nature of the decoder.
5 Experiments
We use sections 2?21 of CCGBank to train our
syntax model, section 00 for development and
section 23 for the final test. Derivations from
CCGBank are transformed into inputs by turn-
ing their surface strings into multi-sets of words.
Following Z&C, we treat base noun phrases (i.e.
NPs that do not recursively contain other NPs) as
atomic units for the input. Output sequences are
compared with the original sentences to evaluate
their quality. We follow previous work and use
the BLEU metric (Papineni et al 2002) to com-
pare outputs with references.
Z&C use two methods to construct leaf edges.
The first is to assign lexical categories according
to a dictionary. There are 26.8 lexical categories
for each word on average using this method, cor-
responding to 26.8 leaf edges. The other method
is to use a pre-processing step ? a CCG supertag-
ger (Clark and Curran, 2007) ? to prune can-
didate lexical categories according to the gold-
CCGBank Sentences Tokens
training 39,604 929,552
development 1,913 45,422
GigaWord v4 Sentences Tokens
AFP 30,363,052 684,910,697
XIN 15,982,098 340,666,976
Table 1: Number of sentences and tokens by language
model source.
standard sequence, assuming that for some prob-
lems the ambiguities can be reduced (e.g. when
the input is already partly correctly ordered).
Z&C use different probability cutoff levels (the
? parameter in the supertagger) to control the
pruning. Here we focus mainly on the dictionary
method, which leaves lexical category disam-
biguation entirely to the generation system. For
comparison, we also perform experiments with
lexical category pruning. We chose ? = 0.0001,
which leaves 5.4 leaf edges per word on average.
We used the SRILM Toolkit (Stolcke, 2002)
to build a true-case 4-gram language model es-
timated over the CCGBank training and develop-
ment data and a large additional collection of flu-
ent sentences in the Agence France-Presse (AFP)
and Xinhua News Agency (XIN) subsets of the
English GigaWord Fourth Edition (Parker et al
2009), a total of over 1 billion tokens. The Gi-
gaWord data was first pre-processed to replicate
the CCGBank tokenization. The total number
of sentences and tokens in each LM component
is shown in Table 1. The language model vo-
cabulary consists of the 46,574 words that oc-
cur in the concatenation of the CCGBank train-
ing, development, and test sets. The LM proba-
bilities are estimated using modified Kneser-Ney
smoothing (Kneser and Ney, 1995) with interpo-
lation of lower n-gram orders.
5.1 Development experiments
A set of development test results without lexical
category pruning (i.e. using the full dictionary) is
shown in Table 2. We train the baseline system
and our systems under various settings for 10 iter-
ations, and measure the output BLEU scores after
each iteration. The timeout value for each sen-
tence is set to 5 seconds. The highest score (max
BLEU) and averaged score (avg. BLEU) of each
system over the 10 training iterations are shown
in the table.
741
Method max BLEU avg. BLEU
baseline 38.47 37.36
margin 41.20 39.70
margin +LM (g-precomputed) 41.50 40.84
margin +LM (? = 0, ? = 0, ? = 0) 40.83 ?
margin +LM (? = 0.08, ? = 0.016, ? = 0.004) 38.99 ?
margin +LM (? = 0.4, ? = 0.08, ? = 0.02) 36.17 ?
margin +LM (? = 0.8, ? = 0.16, ? = 0.04) 34.74 ?
Table 2: Development experiments without lexical category pruning.
The first three rows represent the baseline sys-
tem, our largin-margin training system (margin),
and our system with the N -gram model incorpo-
rated using g-precomputed interpolation. For in-
terpolation we manually chose ? = 0.8, ? = 0.16
and ? = 0.04, respectively. These values could
be optimized by development experiments with
alternative configurations, which may lead to fur-
ther improvements. Our system with large-margin
training gives higher BLEU scores than the base-
line system consistently over all iterations. The
N -gram model led to further improvements.
The last four rows in the table show results
of our system with the N -gram model added us-
ing test-time interpolation. The syntax model is
trained with the optimal number of iterations, and
different ?, ?, and ? values are used to integrate
the language model. Compared with the system
using no N -gram model (margin), test-time inter-
polation did not improve the accuracies.
The row with ?, ?, ? = 0 represents our system
with the N -gram model loaded, and the scores
gfour , gtri and gbi computed for each N -gram
during decoding, but the scores of edges are com-
puted without using N -gram probabilities. The
scoring model is the same as the syntax model
(margin), but the results are lower than the row
?margin?, because computing N -gram probabil-
ities made the system slower, exploring less hy-
potheses under the same timeout setting.1
The comparison between g-precomputed inter-
polation and test-time interpolation shows that the
system gives better scores when the syntax model
takes into consideration the N -gram model during
1More decoding time could be given to the slower N -
gram system, but we use 5 seconds as the timeout setting
for all the experiments, giving the methods with the N -gram
language model a slight disadvantage, as shown by the two
rows ?margin? and ?margin +LM (?, ?, ? = 0).
 37
 38
 39
 40
 41
 42
 43
 44
 45
 1  2  3  4  5  6  7  8  9  10
BL
EU
training iteration
baseline
margin
margin +LM
Figure 1: Development experiments with lexical cate-
gory pruning (? = 0.0001).
training. One question that arises is whether g-
free interpolation will outperform g-precomputed
interpolation. g-free interpolation offers the free-
dom of ?, ? and ? during training, and can poten-
tially reach a better combination of the parameter
values. However, the training algorithm failed to
converge with g-free interpolation. One possible
explanation is that real-valued features from the
language model made our large-margin training
harder. Another possible reason is that our train-
ing process with heavy pruning does not accom-
modate this complex model.
Figure 1 shows a set of development experi-
ments with lexical category pruning (with the su-
pertagger parameter ? = 0.0001). The scores
of the three different systems are calculated by
varying the number of training iterations. The
large-margin training system (margin) gave con-
sistently better scores than the baseline system,
and adding a language model (margin +LM) im-
proves the scores further.
Table 3 shows some manually chosen examples
for which our system gave significant improve-
ments over the baseline. For most other sentences
the improvements are not as obvious. For each
742
baseline margin margin +LM
as a nonexecutive director Pierre Vinken
, 61 years old , will join the board . 29
Nov.
61 years old , the board will join as a
nonexecutive director Nov. 29 , Pierre
Vinken .
as a nonexecutive director Pierre Vinken
, 61 years old , will join the board Nov.
29 .
Lorillard nor smokers were aware of the
Kent cigarettes of any research on the
workers who studied the researchers
of any research who studied Neither the
workers were aware of smokers on the
Kent cigarettes nor the researchers
Neither Lorillard nor any research on the
workers who studied the Kent cigarettes
were aware of smokers of the researchers
.
you But 35 years ago have to recognize
that these events took place .
recognize But you took place that these
events have to 35 years ago .
But you have to recognize that these
events took place 35 years ago .
investors to pour cash into money funds
continue in Despite yields recent declines
Despite investors , yields continue to
pour into money funds recent declines in
cash .
Despite investors , recent declines in
yields continue to pour cash into money
funds .
yielding The top money funds are cur-
rently well over 9 % .
The top money funds currently are yield-
ing well over 9 % .
The top money funds are yielding well
over 9 % currently .
where A buffet breakfast , held in the mu-
seum was food and drinks to . everyday
visitors banned
everyday visitors are banned to where
A buffet breakfast was held , food and
drinks in the museum .
A buffet breakfast , everyday visitors are
banned to where food and drinks was
held in the museum .
A Commonwealth Edison spokesman
said an administrative nightmare would
be tracking down the past 3 12 years that
the two million customers have . whose
changed
tracking A Commonwealth Edison
spokesman said that the two million cus-
tomers whose addresses have changed
down during the past 3 12 years would
be an administrative nightmare .
an administrative nightmare whose ad-
dresses would be tracking down A Com-
monwealth Edison spokesman said that
the two million customers have changed
during the past 3 12 years .
The $ 2.5 billion Byron 1 plant , Ill. , was
completed . near Rockford in 1985
The $ 2.5 billion Byron 1 plant was near
completed in Rockford , Ill. , 1985 .
The $ 2.5 billion Byron 1 plant near
Rockford , Ill. , was completed in 1985 .
will ( During its centennial year , The
Wall Street Journal report events of the
past century that stand as milestones of
American business history . )
as The Wall Street Journal ( During its
centennial year , milestones stand of
American business history that will re-
port events of the past century . )
During its centennial year events will re-
port , The Wall Street Journal that stand
as milestones of American business his-
tory ( of the past century ) .
Table 3: Some chosen examples with significant improvements (supertagger parameter ? = 0.0001).
method, the examples are chosen from the devel-
opment output with lexical category pruning, af-
ter the optimal number of training iterations, with
the timeout set to 5s. We also tried manually se-
lecting examples without lexical category prun-
ing, but the improvements were not as obvious,
partly because the overall fluency was lower for
all the three systems.
Table 4 shows a set of examples chosen ran-
domly from the development test outputs of our
system with the N -gram model. The optimal
number of training iterations is used, and a time-
out of 1 minute is used in addition to the 5s time-
out for comparison. With more time to decode
each input, the system gave a BLEU score of
44.61, higher than 41.50 with the 5s timout.
While some of the outputs we examined are
reasonably fluent, most are to some extent frag-
mentary.2 In general, the system outputs are
still far below human fluency. Some samples are
2Part of the reason for some fragmentary outputs is the
default output mechanism: partial derivations from the chart
are greedily put together when timeout occurs before a goal
hypothesis is found.
syntactically grammatical, but are semantically
anomalous. For example, person names are often
confused with company names, verbs often take
unrelated subjects and objects. The problem is
much more severe for long sentences, which have
more ambiguities. For specific tasks, extra infor-
mation (such as the source text for machine trans-
lation) can be available to reduce ambiguities.
6 Final results
The final results of our system without lexical cat-
egory pruning are shown in Table 5. Row ?W09
CLE? and ?W09 AB? show the results of the
maximum spanning tree and assignment-based al-
gorithms of Wan et al(2009); rows ?margin?
and ?margin +LM? show the results of our large-
margin training system and our system with the
N -gram model. All these results are directly com-
parable since we do not use any lexical category
pruning for this set of results. For each of our
systems, we fix the number of training iterations
according to development test scores. Consis-
tent with the development experiments, our sys-
743
timeout = 5s timeout = 1m
drooled the cars and drivers , like Fortune 500 executives . over
the race
After schoolboys drooled over the cars and drivers , the race
like Fortune 500 executives .
One big reason : thin margins . One big reason : thin margins .
You or accountants look around ... and at an eye blinks . pro-
fessional ballplayers
blinks nobody You or accountants look around ... and at an eye
. professional ballplayers
most disturbing And of it , are educators , not students , for the
wrongdoing is who .
And blamed for the wrongdoing , educators , not students who
are disturbing , much of it is most .
defeat coaching aids the purpose of which is , He and other
critics say can to . standardized tests learning progress
gauge coaching aids learning progress can and other critics say
the purpose of which is to defeat , standardized tests .
The federal government of government debt because Congress
has lifted the ceiling on U.S. savings bonds suspended sales
The federal government suspended sales of government debt
because Congress has n?t lifted the ceiling on U.S. savings
bonds .
Table 4: Some examples chosen at random from development test outputs without lexical category pruning.
System BLEU
W09 CLE 26.8
W09 AB 33.7
Z&C11 40.1
margin 42.5
margin +LM 43.8
Table 5: Test results without lexical category pruning.
System BLEU
Z&C11 43.2
margin 44.7
margin +LM 46.1
Table 6: Test results with lexical category pruning (su-
pertagger parameter ? = 0.0001).
tem outperforms the baseline methods. The acu-
racies are significantly higher when the N -gram
model is incorporated.
Table 6 compares our system with Z&C using
lexical category pruning (? = 0.0001) and a 5s
timeout for fair comparison. The results are sim-
ilar to Table 5: our large-margin training systems
outperforms the baseline by 1.5 BLEU points, and
adding the N -gram model gave a further 1.4 point
improvement. The scores could be significantly
increased by using a larger timeout, as shown in
our earlier development experiments.
7 Related Work
There is a recent line of research on text-to-
text generation, which studies the linearization of
dependency structures (Barzilay and McKeown,
2005; Filippova and Strube, 2007; Filippova and
Strube, 2009; Bohnet et al 2010; Guo et al
2011). Unlike our system, and Wan et al(2009),
input dependencies provide additional informa-
tion to these systems. Although the search space
can be constrained by the assumption of projec-
tivity, permutation of modifiers of the same head
word makes exact inference for tree lineariza-
tion intractable. The above systems typically ap-
ply approximate inference, such as beam-search.
While syntax-based features are commonly used
by these systems for linearization, Filippova and
Strube (2009) apply a trigram model to control
local fluency within constituents. A dependency-
based N-gram model has also been shown effec-
tive for the linearization task (Guo et al 2011).
The best-first inference and timeout mechanism
of our system is similar to that of White (2004), a
surface realizer from logical forms using CCG.
8 Conclusion
We studied the problem of word-ordering using
a syntactic model and allowing permutation. We
took the model of Zhang and Clark (2011) as the
baseline, and extended it with online large-margin
training and an N -gram language model. These
extentions led to improvements in the BLEU eval-
uation. Analyzing the generated sentences sug-
gests that, while highly fluent outputs can be pro-
duced for short sentences (? 10 words), the sys-
tem fluency in general is still way below human
standard. Future work remains to apply the sys-
tem as a component for specific text generation
tasks, for example machine translation.
Acknowledgements
Yue Zhang and Stephen Clark are supported by the Eu-
ropean Union Seventh Framework Programme (FP7-
ICT-2009-4) under grant agreement no. 247762.
744
References
Yehoshua Bar-Hillel, M. Perles, and E. Shamir. 1961.
On formal properties of simple phrase structure
grammars. Zeitschrift fu?r Phonetik, Sprachwis-
senschaft und Kommunikationsforschung, 14:143?
172. Reprinted in Y. Bar-Hillel. (1964). Language
and Information: Selected Essays on their Theory
and Application, Addison-Wesley 1964, 116?150.
Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence fusion for multidocument news summariza-
tion. Computational Linguistics, 31(3):297?328.
Graeme Blackwood, Adria` de Gispert, and William
Byrne. 2010. Fluency constraints for minimum
Bayes-risk decoding of statistical machine trans-
lation lattices. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(Coling 2010), pages 71?79, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia
Burga. 2010. Broad coverage multilingual deep
sentence generation with a stochastic multi-level re-
alizer. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 98?106, Beijing, China, August. Col-
ing 2010 Organizing Committee.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Sharon A. Caraballo and Eugene Charniak. 1998.
New figures of merit for best-first probabilistic chart
parsing. Comput. Linguist., 24:275?298, June.
David Chiang. 2007. Hierarchical Phrase-
based Translation. Computational Linguistics,
33(2):201?228.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Katja Filippova and Michael Strube. 2007. Gener-
ating constituent order in german clauses. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 320?
327, Prague, Czech Republic, June. Association for
Computational Linguistics.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in english: Improving language model
based approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 225?228, Boulder, Colorado,
June. Association for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742?750, Los Angeles, California,
June. Association for Computational Linguistics.
Yuqing Guo, Deirdre Hogan, and Josef van Genabith.
2011. Dcu at generation challenges 2011 surface
realisation track. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 227?229,
Nancy, France, September. Association for Compu-
tational Linguistics.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combi-
natory Categorial Grammar. In Proceedings of the
40th Meeting of the ACL, pages 335?342, Philadel-
phia, PA.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In International
Conference on Acoustics, Speech, and Signal Pro-
cessing, 1995. ICASSP-95, volume 1, pages 181?
184.
Philip Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of NAACL/HLT, Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition, Linguistic Data Consortium.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
745
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
72?82, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 495?504, Honolulu, Hawaii, Octo-
ber. Association for Computational Linguistics.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of ACL, pages 760?767,
Prague, Czech Republic, June.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, Mass.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, pages 901?904.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 852?860, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Michael White. 2004. Reining in CCG chart realiza-
tion. In Proc. INLG-04, pages 182?191.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Yue Zhang and Stephen Clark. 2011. Syntax-
based grammaticality improvement using CCG and
guided search. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1147?1157, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
746
Hierarchical Phrase-Based Translation with
Weighted Finite-State Transducers and
Shallow-n Grammars
Adria` de Gispert?
University of Cambridge
Gonzalo Iglesias??
University of Vigo
Graeme Blackwood?
University of Cambridge
Eduardo R. Banga??
University of Vigo
William Byrne?
University of Cambridge
In this article we describe HiFST, a lattice-based decoder for hierarchical phrase-based translation
and alignment. The decoder is implemented with standard Weighted Finite-State Transducer
(WFST) operations as an alternative to the well-known cube pruning procedure. We find that
the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in
fewer search errors, better parameter optimization, and improved translation performance. The
direct generation of translation lattices in the target language can improve subsequent rescoring
procedures, yielding further gains when applying long-span language models and Minimum
Bayes Risk decoding. We also provide insights as to how to control the size of the search space
defined by hierarchical rules. We show that shallow-n grammars, low-level rule catenation,
and other search constraints can help to match the power of the translation system to specific
language pairs.
1. Introduction
Hierarchical phrase-based translation (Chiang 2005) is one of the current promising
approaches to statistical machine translation (SMT). Hiero SMT systems are based
on probabilistic synchronous context-free grammars (SCFGs) whose translation rules
? University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K.
E-mail: {ad465,gwb24,wjb31}@eng.cam.ac.uk.
?? University of Vigo, Department of Signal Processing and Communications, 36310 Vigo, Spain.
E-mail: {giglesia,erbanga}@gts.tsc.uvigo.es.
Submission received: 30 October 2009; revised submission received: 24 February 2010; accepted for
publication: 10 April 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
can be extracted automatically from word-aligned parallel text. These grammars can
produce a very rich space of candidate translations and, relative to simpler phrase-
based systems (Koehn, Och, and Marcu 2003), the power of Hiero is most evident in
translation between dissimilar languages, such as English and Chinese (Chiang 2005,
2007). Hiero is able to learn and apply complex patterns in movement and translation
that are not possible with simpler systems. Hiero can also be used to good effect on
?simpler? problems, such as translation between English and Spanish (Iglesias et al
2009c), even though there is not the same need for the full complexity of movement and
translation. If gains in using Hiero are small, however, the computational and modeling
complexity involved are difficult to justify. Such concerns would vanish if there were
reliable methods to match Hiero complexity for specific translation problems. Loosely
put, it would be a good thing if the complexity of a system was somehow proportional
to the improvement in translation quality the system delivers.
Another notable current trend in SMT is system combination. Minimum Bayes
Risk decoding is widely used to rescore and improve hypotheses produced by indi-
vidual systems (Kumar and Byrne 2004; Tromble et al 2008; de Gispert et al 2009),
and more aggressive system combination techniques which synthesize entirely new
hypotheses from those of contributing systems can give even greater translation im-
provements (Rosti et al 2007; Sim et al 2007). It is now commonplace to note that even
the best available individual SMT system can be significantly improved upon by such
techniques. This puts a burden on the underlying SMT systems which is somewhat
unusual in NLP. The requirement is not merely to produce a single hypothesis that
is as good as possible. Ideally, the SMT systems should generate large collections of
candidate hypotheses that are simultaneously diverse and of good quality.
Relative to these concerns, previously published descriptions of Hiero have noted
certain limitations. Spurious ambiguity (Chiang 2005) was described as
a situation where the decoder produces many derivations that are distinct yet have the
same model feature vectors and give the same translation. This can result in n-best lists
with very few different translations which is problematic for the minimum-error-rate
training algorithm ...
This is due in part to the cube pruning procedure (Chiang 2007), which enumerates all
distinct hypotheses to a fixed depth by means of k-best hypothesis lists. If enumeration
was not necessary, or if the lists could be arbitrarily deep, there might still be many
duplicate derivations, but at least the hypothesis space would not be impoverished.
Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu
1997; Setiawan et al 2009). For our purposes we say that overgeneration occurs when
different derivations based on the same set of rules give rise to different translations.
An example is given in Figure 1.
This process is not necessarily a bad thing in that it allows new translations to be
synthesized from rules extracted from training data; a strong target language model,
such as a high order n-gram, is typically relied upon to discard unsuitable hypotheses.
Overgeneration does complicate translation, however, in that many hypotheses are
introduced only to be subsequently discarded. The situation is further complicated by
search errors. Any search procedure which relies on pruning during search is at risk of
search errors and the risk is made worse if the grammars tend to introduce many similar
scoring hypotheses. In particular we have found that cube pruning is very prone to
search errors, that is, the hypotheses produced by cube pruning are not the top scoring
hypotheses which should be found under the Hiero grammar (Iglesias et al 2009b).
506
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 1
Example of multiple translation sequences from a simple grammar fragment showing variability
in reordering in translation of the source sequence abc.
These limitations are clearly related to each other. Moreover, they become more
problematic as the amount of parallel text grows. As the number of rules in the grammar
increases, the grammars become more expressive, but the ability to search them does not
improve. This leads to a widening gap between the expressive power of the grammar
and the ability to search it to find good and diverse hypotheses.
In this article we describe the following two refinements to Hiero which are in-
tended to address some of the limitations in its original formulation.
Lattice-based hierarchical translation We describe how the cube pruning procedure
can be replaced by standard operations with Weighted Finite State Transducers
(WFSTs) so that Hiero uses translation lattices rather than n-best lists in search.
We find that keeping partial translation hypotheses in lattice form greatly reduces
search errors. In some instances it is possible to perform translation without
any pruning at all so that search errors are completely eliminated. Consistent
with the observation by Chiang (2005), this leads to improvements in minimum
error rate training. Furthermore, the direct generation of translation lattices can
improve gains from subsequent language model and Minimum Bayes Risk (MBR)
rescoring.
Shallow-n grammars and additional nonterminal categories Nonterminals can be in-
corporated into hierarchical translation rules for the purpose of tuning the size
of the Hiero search space for individual language pairs. Shallow-n grammars are
described and shown to control the level of rule nesting, low-level rule catenation,
and the minimum and maximum spans of individual translation rules. In trans-
lation experiments we find that a shallow-1 grammar (one level of rule nesting)
is sufficiently expressive for Arabic-to-English translation, but that a shallow-3
grammar is required in Chinese-to-English translation to match the performance
of a full Hiero system that allows arbitrary rule nesting. These nonterminals are
introduced to control the Hiero search space and do not require estimation from
annotated?or parsed?parallel text, as can be required by translation systems
based on linguistically motivated grammars. We use this approach as the basis
of a general approach to SMT modeling. To control overgeneration, we revisit
the synchronous context-free grammar defined by hierarchical rules and take a
shallow-1 grammar as a starting point. We then increase the complexity of the
rules until the desired translation quality is found.
507
Computational Linguistics Volume 36, Number 3
With these refinements we find that hierarchical phrase-based translation can be effi-
ciently carried out with no (or minimal) search errors in large-data tasks and can achieve
state-of-the-art translation performance.
There are many benefits to formulating Hiero translation in terms of WFSTs. Fol-
lowing the manner in which Knight and Al-Onaizan (1998), Kumar, Deng, and Byrne
(2006), and Graehl, Knight, and May (2008) elucidate other machine translation models,
we can use WFST operations to make the operations of the Hiero decoder very clear. The
simplicity of the analysis makes it possible to focus on the underlying grammars and
avoid the complexities of heuristic search procedures. Once the decoder is formulated,
implementation is mostly straightforward using standard WFST techniques developed
for language processing (Mohri, Pereira, and Riley 2002). What difficulties arise are due
to using finite state techniques with grammars which are not themselves finite state.
We will show, however, that the basic operations which need to be performed, such as
extracting sufficient statistics for minimum error rate training, can be done relatively
easily and naturally.
1.1 Overview
In Section 2 we describe HiFST, which is a hierarchical phrase-based translation system
based on the OpenFST WFST libraries (Allauzen et al 2007). We describe how trans-
lation lattices can be generated over the Cocke-Younger-Kasami (CYK) grid used for
parsing under Hiero. We also review some modeling issues needed for practical trans-
lation, such as the efficient handling of source language deletions and the extraction of
statistics for minimum error rate training. This requires running HiFST in ?alignment
mode? (Section 2.3) to find all the rule derivations that generate a given set of translation
hypotheses.
In Section 3 we investigate parameters that control the size and nature of the
hierarchical phrase-based search space as defined by hierarchical translation rules. To
efficiently explore the largest possible space and avoid pruning in search, we introduce
ways to easily adapt the grammar to the reordering needs of each language pair. We
describe the use of additional nonterminal categories to limit the degree of rule nesting,
and can directly control the minimum or maximum span each translation rule can cover.
In Section 4 we report detailed translation results for Arabic-to-English and
Chinese-to-English, and review translation results for Spanish-to-English and Finnish-
to-English translation. In these experiments we contrast the performance of lattice-based
and cube pruning hierarchical decoding and we measure the impact on processing
time and translation performance due to changes in search parameters and grammar
configurations. We demonstrate that it is easy and feasible to compute the marginal
instead of the Viterbi probabilities when using WFSTs, and that this yields gains in
translation performance. And finally, we show that lattice-based translation performs
significantly better than k-best lists for the task of combining translation hypotheses
generated from alternative morphological segmentations of the data via lattice-based
MBR decoding.
2. Hierarchical Translation and Alignment with WFSTs
Hierarchical phrase-based rules define a synchronous context-free grammar (CFG) and
a particular search space of translation candidates. Table 1 shows the type of rules in-
cluded in a standard hierarchical phrase-based grammar, where T denotes the terminals
(words) and ? is a bijective function that relates the source and target nonterminals of
508
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 1
Rules contained in the standard hierarchical grammar.
standard hierarchical grammar
S??X,X? glue rule 1
S??S X,S X? glue rule 2
X???,?,?? , ?,? ? {X ? T}+ hiero rules
each rule (Chiang 2007). This function is defined if there are at least two nonterminals,
and for clarity of presentation may be omitted in general rule discussions. When ?,? ?
{T}+, that is, the rule contains no nonterminals, the rule is a simple lexical phrase pair.
The HiFST translation system is based on a variant of the CYK algorithm closely
related to CYK+ (Chappelier and Rajman 1998). Parsing follows the description of
Chiang (2005, 2007); it maintains back-pointers and employs hypothesis recombination
without pruning. The underlying model is a probabilisitic synchronous CFG consisting
of a set R = {Rr} of rules Rr : Nr ? ??r,?r? / pr, with ?glue? rules, S ? ?X,X? and
S ? ?S X,S X?. N denotes the set of nonterminal categories (examples are given in
Section 3), and pr denotes the rule probability, typically transformed to a cost cr; unless
otherwise noted we use the tropical semiring, so cr = ? log pr. T denotes the terminals
(words), and the grammar builds parses based on strings ?,? ? {N ? T}+. Each cell
in the CYK grid is specified by a nonterminal symbol and position in the CYK grid:
(N, x, y), which spans sx+y?1x on the source sentence.
In effect, the source language sentence is parsed using a CFG with rules N ? ?. The
generation of translations is a second step that follows parsing. For this second step, we
describe a method to construct word lattices with all possible translations that can be
produced by the hierarchical rules. Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In each cell (N, x, y) in the CYK grid, we
build a target language word lattice L(N, x, y). This lattice contains every translation of
sx+y?1x from every derivation headed by N. These lattices also contain the translation
scores on their arc weights.
The ultimate objective is the word lattice L(S, 1, J) which corresponds to all the
analyses that cover the source sentence sJ1. Once this is built, we can apply a target lan-
guage model to L(S, 1, J) to obtain the final target language translation lattice (Allauzen,
Mohri, and Roark 2003).
2.1 Lattice Construction over the CYK Grid
In each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), that
is, for r ? R(N, x, y), the rule N ? ??r,?r? was used in at least one derivation involving
that cell.
For each rule Rr, r ? R(N, x, y), we build a lattice L(N, x, y, r). This lattice is derived
from the target side of the rule ?r by concatenating lattices corresponding to the ele-
ments of ?r = ?r1...?
r
|?r|. If an ?
r
i is a terminal, creating its lattice is straightforward. If
?ri is a nonterminal, it refers to a cell (N
?, x?, y?) lower in the grid identified by the back-
pointer BP(N, x, y, r, i); in this case, the lattice used is L(N?, x?, y?). Taken together,
L(N, x, y, r) =
?
i=1..|?r|
L(N, x, y, r, i) (1)
509
Computational Linguistics Volume 36, Number 3
Figure 2
Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3.
The grid is represented here in two dimensions (x, y). In practice only the first column accepts
both nonterminals (S,X). For this reason it is divided into two subcolumns.
L(N, x, y, r, i) =
{
A(?i) if ?i ? T
L(N?, x?, y?) otherwise (2)
where A(t), t ? T returns a single-arc acceptor which accepts only the symbol t. The
lattice L(N, x, y) is then built as the union of lattices corresponding to the rules in
R(N, x, y):
L(N, x, y) =
?
r?R(N,x,y)
L(N, x, y, r) (3)
Lattice union and concatenation are performed using the? and?WFST operations,
respectively, as described by Allauzen et al (2007). If a rule Rr has a cost cr, it is applied
to the exit state of the lattice L(N, x, y, r) prior to the operation of Equation (3).
2.1.1 An Example of Phrase-based Translation. Figure 2 illustrates this process for a three-
word source sentence s1s2s3 under monotonic phrase-based translation. The left-hand
side shows the state of the CYK grid after parsing using the rules R1 to R5. These include
three standard phrases, that is, rules with only terminals (R1, R2, R3), and the glue rules
(R4, R5). Arrows represent back-pointers to lower-level cells. We are interested in the
uppermost S cell (S, 1, 3), as it represents the search space of translation hypotheses
covering the whole source sentence. Two rules (R4, R5) are in this cell, so the lattice
L(S, 1, 3) will be obtained by the union of the two lattices found by the back-pointers of
these two rules. This process is explicitly derived in the right-hand side of Figure 2.
2.1.2 An Example of Hierarchical Translation. Figure 3 shows a hierarchical scenario for
the same sentence. Three rules, R6,R7,R8, are added to the example of Figure 2, thus
providing two additional derivations. This makes use of sublattices already produced
in the creation of L(S, 1, 3, 5) and L(X, 1, 3, 1) in Figure 2; these are shown within {}.
510
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 3
Translation as in Figure 2 but with additional rules R6,R7,R8. Lattices previously derived appear
within {}.
2.2 A Procedure for Lattice Construction
Figure 4 presents an algorithm to build the lattice for every cell. The algorithm uses
memoization: If a lattice for a requested cell already exists, it is returned (line 2);
otherwise it is constructed via Equations (1)?(3). For every rule, each element of the
target side (lines 3,4) is checked as terminal or nonterminal (Equation (2)). If it is a
terminal element (line 5), a simple acceptor is built. If it is a nonterminal (line 6), the
lattice associated to its back-pointer is returned (lines 7 and 8). The complete lattice
L(N, x, y, r) for each rule is built by Equation (1) (line 9). The lattice L(N, x, y) for this
cell is then found by union of all the component rules (line 10, Equation (3)); this lattice
is then reduced by standard WFST operations (lines 11, 12, 13). It is important at this
point to remove any epsilon arcs which may have been introduced by the various WFST
union, concatenation, and replacement operations (Allauzen et al 2007).
We now address several important aspects of efficient implementation.
Figure 4
Recursive lattice construction from a CYK grid.
511
Computational Linguistics Volume 36, Number 3
Figure 5
Delayed translation WFST with derivations from Figures 2 and 3 before (left) and after
minimization (right).
2.2.1 Delayed Translation. Equation (2) leads to the recursive construction of lattices in
upper levels of the grid through the union and concatenation of lattices from lower
levels. If Equations (1) and (3) are actually carried out over fully expanded word lattices,
the memory required by the upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as pointers to the low-level lattices. This
effectively builds a skeleton for the desired lattice and delays the creation of the final
word lattice until a single replacement operation is carried out in the top cell (S, 1, J).
To make this exact, we define a function g(N, x, y) which returns a unique tag for each
lattice in each cell, and use it to redefine Equation (2). With the back-pointer (N?, x?, y?) =
BP(N, x, y, r, i), these special arcs are introduced as
L(N, x, y, r, i) =
{
A(?i) if ?i ? T
A(g(N?, x?, y?)) otherwise (4)
The resulting lattices L(N, x, y) are a mix of target language words and lattice
pointers (Figure 5, left). Each still represents the entire search space of all translation
hypotheses covering the span, however. Importantly, operations on these lattices?such
as lossless size reduction via determinization and minimization (Mohri, Pereira, and
Riley 2002)?can still be performed. Owing to the existence of multiple hierarchical rules
which share the same low-level dependencies, these operations can greatly reduce the
size of the skeleton lattice; Figure 5 shows the effect on the translation example. This
process is carried out for the lattice at every cell, even at the lowest level where there
are only sequences of word terminals. As stated, size reductions can be significant. Not
all redundancy is removed, however, because duplicate paths may arise through the
concatenation and union of sublattices with different spans.
At the upper-most cell, the lattice L(S, 1, J) contains pointers to lower-level lattices.
A single FST replace operation (Allauzen et al 2007) recursively substitutes all pointers
by their lower-level lattices until no pointers are left, thus producing the complete
target word lattice for the whole source sentence. The use of the lattice pointer arc was
inspired by the ?lazy evaluation? techniques developed by Mohri, Pereira, and Riley
(2000), closely related to Recursive Transition Networks (Woods 1970; Mohri 1997). Its
implementation uses the infrastructure provided by the OpenFST libraries for delayed
composition.
2.2.2 Top-level Pruning and Search Pruning. The final translation lattice L(S, 1, J) can
be quite large after the pointer arcs are expanded. We therefore apply a word-based
512
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 6
Transducers for filtering up to one (left) or two (right) consecutive deletions.
language model via WFST composition (Allauzen et al 2007) and perform likelihood-
based pruning based on the combined translation and language model scores. We call
this top-level pruning because it is performed over the topmost lattice.
Pruning can also be performed on the sublattices in each cell during search. One
simple strategy is to monitor the number of states in the determinized lattices L(N, x, y).
If this number is above a threshold, we expand any pointer arcs and apply a word-based
language model via composition. The resulting lattice is then reduced by likelihood-
based pruning, after which the LM scores are removed. These pruning strategies can be
very selective, for example allowing the pruning threshold to depend on the height of
the cell in the grid. In this way the risk of search errors can be controlled.
The same n-gram language model can be used for top-level pruning and search
pruning, although different WFST realizations are required. For top-level pruning, a
standard implementation as described by Allauzen et al (2007) is appropriate. For
search pruning, the WFST must allow for incomplete language model histories, because
many sublattice paths are incomplete translation hypotheses which do not begin with
a sentence-start marker. The language model acceptor is constructed so that initial
substrings of length less than the language model order are assigned no weight under
the language model.
2.2.3 Constrained Source Word Deletion. As a practical matter it can be useful to allow SMT
systems to delete some source words rather than to enforce their translation. Deletions
can be allowed in Hiero by including in the grammar a set of special deletion rules
of the type: X??s,NULL?. Unconstrained application of these rules can lead to overly
large and complex search spaces, however. We therefore limit the number of consecutive
source word deletions as we explore each cell of the CYK grid. This is done by standard
composition with an unweighted transducer that maps any word to itself, and up to k
NULL tokens to  arcs. In Figure 6 this simple transducer for k = 1 and k = 2 is drawn.
Composition of the lattice in each cell with this transducer filters out all translations
with more than k consecutive deleted words.
2.3 Hierarchical Phrase-Based Alignment with WFSTs
We now describe a method to apply our decoder in alignment mode. The objective in
alignment is to recover all the derivations which can produce a given translation. We do
this rather than keep track of the rules used during translation, because we find it faster
and more efficient first to generate translations and then, by running the system as an
aligner with a constrained target space, to extract all the relevant derivations with their
costs. As will be discussed in Section 2.3.1, this is useful for minimum error training,
where the contribution of each feature to the overall hypothesis cost is required for
system optimization.
513
Computational Linguistics Volume 36, Number 3
Figure 7
Transducer encoding simultaneously rule derivations R2R1R3R4 and R1R5R6, and the translation
t5t8. The input sentence is s1s2s3 and the grammar considered here contains the following rules:
R1: S??X,X?, R2: S??S X,S X? , R3: X??s1,t5?, R4: X??s2 s3,t8?, R5: X??s1 X s3,X t8? and R6:
X??s2,t5?.
Conceptually, we would like to create a transducer that represents the mapping
from all possible rule derivations to all possible translations, and then compose this
transducer with an acceptor for the translations of interest. Creating this transducer
which maps derivations to translations is not feasible for large translation grammars,
so we instead keep track of rules as they are used to generate a particular translation
output. We introduce two modifications into lattice construction over the CYK grid
described in Section 2.2:
1. In each cell transducers are constructed which map rule sequences to the
target language translation sequences they produce. In each transducer the
output strings are all possible translations of the source sentence span
covered by that cell; the input strings are all the rule derivations that
generate those translations. The rule derivations are expressed as
sequences of rule indices r given the set of rules R = {Rr}.
2. As these transducers are built they are composed with acceptors for
subsequences of the reference translations so that any translations not
present in the given set of reference translations are removed. In effect, this
replaces the general target language model used in translation with an
unweighted acceptor which accepts only specific sentences.
For alignment, Equations (1) and (2) are redefined as
L(N, x, y, r) = AT(r,)
?
i=1..|?r|
L(N, x, y, r, i) (5)
L(N, x, y, r, i) =
{
AT(,?i) if ?i ? T
L(N?, x?, y?) otherwise (6)
where AT(r, t), Rr ? R, t ? T returns a single-arc transducer which accepts the symbol
r in the input language (rule indices) and the symbol t in the output language (target
words). The weight assigned to each arc is the same in alignment as in translation. With
these definitions the goal lattice L(S, 1, J) is now a transducer with rule indices in the
input symbols and target words in the output symbols. A simple example is given
in Figure 7 where two rule derivations for the translation t5t8 are represented by the
transducer.
As we are only interested in those rule derivations that generate the given target
references, we can discard non-desired translations via standard FST composition of
514
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 8
Construction of a substring acceptor. An acceptor for the strings t1t2t4 and t3t4 (left) and its
substring acceptor (right). In alignment the substring acceptor can be used to filter out undesired
partial translations via standard FST composition operations.
the lattice transducer with the given reference acceptor. In principle, this would be done
in the uppermost cell of the CYK, once the complete source sentence has been covered.
However, keeping track of all possible rule derivations and all possible translations until
the last cell may not be computationally feasible for many sentences. It is more desirable
to carry out this filtering composition in lower-level cells while constructing the lattice
over the CYK grid so as to avoid storing an increasing number of undesired translations
and derivations in the lattice. The lattice in each cell should contain translations formed
only from substrings of the references.
To achieve this we build an unweighted substring acceptor that accepts all sub-
strings of each target reference string. For instance, given the reference string t1t2 . . .
tJ, we build an acceptor for all substrings ti . . . tj, where 1 ? i ? j ? J. This is applied
to lattices in all cells (x, y) that do not span the whole sentence. In the uppermost
cell we compose with the reference acceptor which accepts only complete reference
strings. Given a lattice of target references, the unweighted substring acceptor is
built as:
1. change all non-initial states into final states
2. add one initial state and add  arcs from it to all other states
Figure 8 shows an example of a substring acceptor for the two references t1t2t4 and
t3t4. The substring acceptor also accepts an empty string, accounting for those rules
that delete source words, which in other words translate into NULL. In some instances
the final composition with the reference acceptor might return an empty lattice. If this
happens there is no rule sequence in the grammar that can generate the given source
and target sentences simultaneously.
We have described the use of transducers to encode mappings from rule deriva-
tions to translations. These transducers are somewhat impoverished relative to parse
trees and parse forests, which are more commonly used to encode this relationship. It
is easy to map from a parse tree to one of these transducers but the reverse essentially
requires re-parsing to recreate the tree structure. The structures of the parse trees asso-
ciated with a translation are not needed by many algorithms, however. In particular,
parameter optimization by MERT requires only the rules involved in translation. Our
approach keeps only what is needed by such algorithms. This approach also has prac-
tical advantages such as the ability to align directly to k-best lists or lattices of reference
translations.
515
Computational Linguistics Volume 36, Number 3
Figure 9
One arc from a rule acceptor that assigns a vector of K feature weights to each rule (top) and the
result of composition with the transducer of Figure 7 (after weight-pushing) (bottom). The
components of the final K-dimensional weight vector agree with the feature weights of the rule
sequence, for example ck = c2,k + c1,k + c3,k + c4,k for k = 1 . . .K.
2.3.1 Extracting Feature Values from Alignments. As described by Chiang (2007), scores are
associated with hierarchical translation rules through a factoring into features within a
log-linear model (Och and Ney 2002). We assume that we have a collection of K features
and that the cost cr of each rule Rr is cr =
?K
k=1 ?kc
r,k, where cr,k is the value of the kth
feature value for the rth rule and ?k is the weight assigned to the kth feature for all rules.
For a parse which makes use of the rules Rr1 . . .RrN , its cost
?N
n=1 c
rn can therefore
be written as
?K
k=1 ?k
?N
n=1 c
rn,k. The quantity
?N
n=1 c
rn,k is the contribution by the kth
feature to the overall translation score for that parse. These are the quantities which
need to be extracted from alignment lattices for use in procedures such as minimum
error rate training for estimation of the feature weights ?k.
The procedure described in Section 2.3 produces alignment lattices with scores
consistent with the total parse score. Further steps must be taken to factor this over-
all score to identify the contribution due to individual features or translation rules.
We introduce a rule acceptor which accepts sequences of rule indices, such as the
input sequences of the alignment transducer, and assigns weights in the form of
K-dimensional vectors. Each component of the weight vector corresponds to the feature
value for that rule. Arcs have the form 0
Rr/wr?? 0 where wr = [cr,1, . . . , cr,K]. An example
of composition with this rule acceptor is given in Figure 9 to illustrate how feature scores
are mapped to components of the weight vector. The same operations can be applied to
the (unweighted) alignment transducer on a much larger scale to extract the statistics
needed for minimum error rate training.
We typically apply this procedure in the tropical semiring (Viterbi likelihoods), so
that only the best rule derivation that generated each translation candidate is taken
into account when extracting feature contributions for MERT. However, given the
alignment transducer L, this could also be performed in the log semiring (marginal
likelihoods), taking into account the feature contributions from all rule derivations, for
each translation candidate. This would be adequate if the translation system also car-
ried out decoding in the log semiring, an experiment which is partially explored in
Section 4.4.
We note that the contribution of the language model to the overall translation score
cannot be calculated in this scheme, since the language model score cannot be factored
in terms of rules. To obtain the language model contribution, we simply carry out
WFST composition (Allauzen et al 2007) between an unweighted acceptor of the target
516
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 10
Hierarchical translation grammar example and two parse trees with different levels of rule
nesting for the input sentence s1s2s3s4.
sentences and the n-gram language model used in translation. After determinization,
the cost of each path in the acceptor is then the desired LM feature contribution.
3. Shallow-n Translation Grammars: Translation Search Space Refinements
In this section we describe shallow-n grammars in order to reduce Hiero overgeneration
and adapt the grammar complexity to specific language pairs; the ultimate goal is to de-
fine the most constrained grammar that is capable of generating the desired movement
and translation, so that decoding can be performed without search errors.
Hiero can provide varying degrees of complexity in movement and translation.
Consider the example shown in Figure 10, which shows a hierarchical grammar defined
by six rules. For the input sentence s1s2s3s4, there are two possible parse trees as shown;
the rule derivations for each tree are R1R4R3R5 and R2R1R3R5R6. Along with each tree
is shown the translation generated and the phrase-level alignment. Comparing the two
trees and alignments, the leftmost tree makes use of more reordering when translating
from source to target through the nested application of the hierarchical rules R3 and R4.
For some language pairs this level of reordering may be required in translation, but for
other language pairs it may lead to overgeneration of unwanted hypotheses. Suppose
the grammar in this example is modified as follows:
1. A nonterminal X0 is introduced into hierarchical translation rules
R3:X??X0 s3,t5 X0?
R4:X??X0 s4,t3 X0?
2. Rules for lexical phrases are applied to X0
R5:X0??s1 s2,t1 t2?
R6:X0??s4,t7?
These modifications exclude parses in which hierarchical translation rules generate
other hierarchical rules, except at the 0th level which generate lexical phrases. Con-
sequently the left most tree of Figure 10 cannot be generated and t5t1t2t7 is the only
allowable translation of s1s2s3s4. We call this a ?shallow-1? grammar: The maximum
517
Computational Linguistics Volume 36, Number 3
degree of rule nesting allowed is 1 and only the glue rule can be applied above this
level.
The use of additional nonterminal categories is an elegant way to easily control
important aspects that can have a strong impact on the search space. A shallow-n
translation grammar can be formally defined as:
1. the usual nonterminal S
2. a set of nonterminals {X0, . . . ,XN}
3. two glue rules: S ? ?XN,XN? and S ? ?S XN,S XN?
4. hierarchical translation rules for levels n = 1, . . . ,N:
R: Xn???,?,?? , ?,? ? {{Xn?1} ? T}+
with the requirement that ? and ? contain at least one Xn?1
5. translation rules which generate lexical phrases:
R: X0???,?? , ?,? ? T+
3.1 Avoiding Some Spurious Ambiguity
The added requirement in condition (4) in the definition of shallow-n grammars is
included to avoid some instances in which multiple parses lead to the same translation.
It is not absolutely necessary but it can be added without any loss in representational
capability. To see the effect of this constraint, consider the following example with a
source sentence s1 s2 and a shallow-1 grammar defined by these four rules:
R1: S??X1,X1?
R2: X1??s1 s2,t2 t1?
R3: X1??s1 X0,X0 t1?
R4: X0??s2,t2?
There are two derivations R1R2 and R1R3R4 which yield identical translations. However
R2 would not be allowed under the constraint introduced here because it does not
rewrite an X1 to an X0.
3.2 Structured Long-Distance Movement
The basic formulation of shallow-n grammars allows only the upper-level nonterminal
category S to act within the glue rule. This can prevent some useful long-distance
movement, as might be needed to translate Arabic sentences in Verb-Subject-Object
order into English. It often happens that the initial Arabic verb requires long-distance
movement, but the subject which follows can be translated in monotonic order. For
instance, consider the following Romanized Arabic sentence:
TAlb AlwzrA? AlmjtmEyn Alywm fy dm$q <lY ...
(CALLED) (the ministers) (gathered) (today) (in Damascus) (FOR) ...
where the verb ?TAlb? must be translated into English so that it follows the translations
of the five subsequent Arabic words ?AlwzrA? AlmjtmEyn Alywm fy dm$q?, which
518
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
are themselves translated monotonically. A shallow-1 grammar cannot generate this
movement except in the relatively unlikely case that the five words following the verb
can be translated as a single phrase.
A more powerful approach is to define grammars which allow low-level rules to
form movable groups of phrases. Additional nonterminals {Mk} are introduced to allow
successive generation of k nonterminals XN?1 in monotonic order for both languages,
where K1 ? k ? K2. These act in the same manner as the glue rule does in the uppermost
level. Applying Mk nonterminals at the N?1 level allows one hierarchical rule to perform
a long-distance movement over the tree headed by Mk.
We further refine the definition of shallow-n grammars by specifying the allowable
values of k for the successive productions of nonterminals XN?1. There are many pos-
sible ways to formulate and constrain these grammars. If K2 = 1, then the grammar
is equivalent to the previous definition of shallow-n grammars, because monotonic
production is only allowed by the glue rule of level N. If K1 = 1 and K2 > 1, then the
search space defined by the grammar is greater than the standard shallow-n grammar
as it includes structured long-distance movement. Finally, if K1 > 1 then the search
space is different from standard shallow-n as the n level is only used for long-distance
movement.
Introduction of Mk nonterminals redefines shallow-n grammars as:
1. the usual nonterminal S
2. a set of nonterminals {X0, . . . ,XN}
3. a set of nonterminals {MK1 , . . . ,MK2} for K1 = 1, 2; K1 ? K2
4. two glue rules: S ? ?XN,XN? and S ? ?S XN,S XN?
5. hierarchical translation rules for level N:
R: XN???,?,?? , ?,? ? {{MK1 , . . . ,MK2} ? T}+
with the requirement that ? and ? contain at least one Mk
6. hierarchical translation rules for levels n = 1, . . . ,N ? 1:
R: Xn???,?,?? , ?,? ? {{Xn?1} ? T}+
with the requirement that ? and ? contain at least one Xn?1
7. translation rules which generate lexical phrases:
R: X0???,?? , ?,? ? T+
8. rules which generate k nonterminals XN?1:
if K1 == 2 :
R: Mk??XN?1 Mk?1,XN?1 Mk?1? , for k = 3, . . . ,K2
R: M2??XN?1 XN?1,XN?1 XN?1,I?
if K1 == 1 :
R: Mk??XN?1 Mk?1,XN?1 Mk?1? , for k = 2, . . . ,K2
R: M1??XN?1,XN?1?
where I denotes the identity function that enforces monotonocity in the nonterminals.
For example, with a shallow-1 grammar, M3 leads to the monotonic production of three
nonterminals X0, which leads to the production of three lexical phrase pairs; these can be
moved with a hierarchical rule of level 1. This is graphically represented by the leftmost
tree in Figure 11. With a shallow-2 grammar, M2 leads to the monotonic production of
519
Computational Linguistics Volume 36, Number 3
Figure 11
Movement allowed by two grammars: shallow-1, with K1 = 1, K2 = 3 (left), and shallow-2, with
K1 = 1, K2 = 3 (right). Both grammars allow movement of the bracketed term as a unit.
Shallow-1 requires that translation within the object moved be monotonic whereas shallow-2
allows up to two levels of reordering.
two nonterminals X1, a movement represented by the rightmost tree in Figure 11. This
movement cannot be achieved with a shallow-1 grammar.
3.3 Minimum and Maximum Rule Span
It is useful to define two parameters which further control the application of hierarchical
translation rules in generating the search space. Parameters hmax and hmin specify
the maximum and minimum height at which any hierarchical translation rule can be
applied in the CYK grid. In other words, a hierarchical rule can only be applied in cell
(x, y) if hmin? y ?hmax. Note that these parameters can also be set independently for
each nonterminal category.
3.4 Verb Movement Grammars for Arabic-to-English Translation
Following the discussion which motivated this section, we wish to model movement of
Arabic verbs when translating into English. We add to the shallow-n grammars a verb
restriction so that the hierarchical translation rules (5) apply only if the source language
string ? contains a verb. This encourages translations in which the Arabic verb is moved
at the uppermost level N.
3.5 Grammars Used for SMT Experiments
We now define the hierarchical grammars for the translation experiments which we
describe next.
Shallow-1,2,3 : Shallow-n grammars for n = 1, 2, 3. These grammars do not incorporate
any monotonicity constraints, that is K1 = K2 = 1.
Shallow-1, K1 = 1, K2 = 3 : hierarchical rules with one nonterminal can reorder a
monotonic production of up to three target language phrases of level 0.
520
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Shallow-1, K1 = 1, K2 = 3, vo : hierarchical rules with one nonterminal can reorder a
monotonic catenation of up to three target language phrases of level 0, but only if
one of the source terminals is tagged as a verb.
Shallow-2, K1 = 2,K2 = 3, vo : two levels of reordering with monotonic production
of up to three target language phrases of level 1, but only if one of the source
terminals is tagged as a verb.
4. Translation Experiments
In this section we report on hierarchical phrase-based translation experiments with
WFSTs. We focus mainly on the NIST Arabic-to-English and Chinese-to-English trans-
lation tasks; some results for other language pairs are summarized in Section 4.6.
Translation performance is evaluated using the BLEU score (Papineni et al 2001) as
implemented for the NIST 2009 evaluation.1 The experiments are organized as follows:
- Lattice-based and cube pruning hierarchical decoding (Section 4.2)
- Grammar configurations and search parameters and their effect on
translation performance and processing time (Section 4.3)
- Marginalization over translation derivations (Section 4.4)
- Combining translation lattices obtained from alternative morphological
decompositions of the input (Section 4.5)
4.1 Experimental Framework
For Arabic-to-English translation we use all allowed parallel corpora in the NIST MT08
(and MT09) Arabic Constrained Data track (?150M words per language). In addition to
reporting results on the MT08 set itself, we make use of a development set mt02-05-tune
formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation
sets; the even numbered sentences form a validation set mt02-05-test. The mt02-05-tune
set has 2,075 sentences.
For Chinese-to-English translation we use all available parallel text for the GALE
2008 evaluation;2 this is approximately 250M words per language. We report translation
results on the NIST MT08 set, a development set tune-nw, and a validation set test-nw.
These tuning and test sets contain translations produced by the GALE program and
portions of the newswire sections of MT02 through MT05. The tune-nw set has 1,755
sentences, and test-nw set is similar.
The parallel texts for both language pairs are aligned using MTTK (Deng and Byrne
2008). We extract hierarchical rules from the aligned parallel texts using the constraints
developed by Chiang (2007). We further filter the extracted rules by count and pattern
as described by Iglesias et al(2009a). The following features are extracted from the
parallel data and used to assign scores to translation rules: source-to-target and target-
to-source phrase translation models, word and rule penalties, number of usages of the
glue rule, source-to-target and target-to-source lexical models, and three rule count
features inspired by Bender et al (2007).
1 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
2 See http://projects.ldc.upenn.edu/gale/data/catalog.html
521
Computational Linguistics Volume 36, Number 3
We use two types of language model in translation. In first-pass translation we use
4-gram language models estimated over the English side of the parallel text (for each
language pair) and a 965 million word subset of monolingual data from the English
Gigaword Third Edition (LDC2007T07). These are the language models used if pruning
is needed during search. The main language model is a zero-cutoff stupid-backoff
(Brants et al 2007) 5-gram language model, estimated using 6.6B words of English text
from the English Gigaword corpus. These language models are converted to WFSTs
as needed (Allauzen, Mohri, and Roark 2003); failure transitions are used for correct
application of back-off weights. In tuning the systems, standard MERT (Och 2003)
iterative parameter estimation under IBM BLEU is performed on the development sets.
4.2 Contrast between HiFST and Cube Pruning
We contrast two hierarchical phrase-based decoders. The first decoder, HCP, is a k-best
decoder using cube pruning following the description by Chiang (2007); in our im-
plementation, these k-best lists contain only unique hypotheses (Iglesias et al 2009a),
which are obtained by extracting the 10,000 best candidates from each cell (including
the language model cost), using a priority queue to explore the cross-product of the
k-best lists from the cells pointed by nonterminals. We find that deeper k-best lists
(i.e., k = 100, 000) results in impractical decoding times and that fixed k-best list depths
yields better performance than use of a likelihood threshold parameter. The second
decoder, HiFST, is a lattice-based decoder implemented with WFSTs as described earlier.
Hypotheses are generated after determinization under the tropical semiring so that
scores assigned to hypotheses arise from a single minimum cost/maximum likelihood
derivation.
Translation proceeds as follows. After Hiero translation with optimized feature
weights and the first-pass language model, hypotheses are written to disk. For HCP we
save translations as 10,000-best lists, whereas HiFST generates word lattices. The first-
pass results are then rescored with the main 5-gram language model. In this operation
the first-pass language model scores are removed before the main language model
scores are applied. We then perform MBR rescoring. For the n-best lists we rescore
the top 1,000 hypotheses using the negative sentence-level BLEU score as the loss
function (Kumar and Byrne 2004); we have found that using a deeper k-best list is
impractically slow. For the HiFST lattices we use lattice-based MBR search procedures
described by Tromble et al (2008) in an implementation based on standard WFST
operations (Allauzen et al 2007).
4.2.1 Shallow-1 Arabic-to-English Translation. We translate Arabic-to-English with
shallow-1 hierarchical decoding: as described in Section 3, nonterminals are allowed
only to generate target language phrases. Table 2 shows results for mt02-05-tune, mt02-
05-test, and mt08. In this experiment we use MERT to find optimized parameters for
HCP and we use these parameter values in HiFST as well. This allows for a close
comparison of decoder behavior, independent of parameter optimization.
In these experiments, the first-pass translation quality of the two systems (Table 2
a vs. b) is nearly identical. The most notable difference in the first-pass behavior of the
decoders is their memory use. For example, for an input sentence of 105 words, HCP
uses 1.2Gb memory whereas HiFST takes only 900Mb under the same conditions. To
run HCP successfully requires cube pruning with the first-pass 4-gram language model.
By contrast, HiFST requires no pruning during lattice construction and the first pass
language model is not applied until the lattice is fully built at the upper most cell of the
522
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 2
Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) after first-pass
decoding and subsequent rescoring steps. Decoding time reported for mt02-05-tune is in seconds
per word. Both systems are optimized using MERT over the k-best lists generated by HCP.
decoder time mt02-05-tune mt02-05-test mt08
a HCP 1.1 52.5 51.9 42.8
+5g - 53.4 52.9 43.5
+5g+MBR - 53.6 53.0 43.6
b HiFST 0.5 52.5 51.9 42.8
+5g - 53.6 53.2 43.9
+5g+LMBR - 54.3 53.7 44.8
CYK grid. For this grammar, HiFST is able to produce exact translations without any
search errors.
Search Errors Because both decoders are constrained to use exactly the same features,
we can compare their search errors on a sentence-by-sentence basis. A search error is
assigned to one of the decoders if the other has found a hypothesis with lower cost. For
mt02-05-tune, we find that in 18.5% of the sentences HiFST finds a hypothesis with lower
cost than HCP. In contrast, HCP never finds any hypothesis with lower cost for any
sentence. This is as expected: The HiFST decoder requires no pruning prior to applying
the first-pass language model, so search is exact.
Lattice/k-best Quality It is clear from the results that the lattices produced by HiFST
yield better rescoring results than the k-best lists produced by HCP. This is the case for
both 5-gram language model rescoring and MBR. In MT08 rescoring, HCP k-best lists
yield an improvement of 0.8 BLEU relative to the first-pass baseline, whereas rescoring
HiFST lattices yield an improvement of 2.0 BLEU. The advantage of maintaining a large
search space in lattice form during decoding is clear. The use of k-best lists in HCP limits
the gains from subsequent rescoring procedures.
Translation Speed HCP requires an average of 1.1 seconds per input word. HiFST
cuts this time by half, producing output at a rate of 0.5 seconds per word. It proves
much more efficient to process compact lattices containing many hypotheses rather than
independently processing each distinct hypothesis in k-best form.
4.2.2 Fully Hierarchical Chinese-to-English Translation. We translate Chinese-to-English
with full hierarchical decoding: nonterminals are allowed to generate other hierarchical
rules in recursion.
We apply the constraint hmax=10 for nonterminal category X, as described in Sec-
tion 2.2.2, so that only glue rules are allowed at upper levels of the CYK grid; this is
applied in both HCP and HiFST.
In HiFST any lattice in the CYK grid is pruned if it covers at least three source words
and contains more than 10,000 states. The log-likelihood pruning threshold relative to
the best path in the sublattices is 9.0.
Improved Optimization and Generalization Table 3 shows results for tune-nw, test-nw,
and mt08. The first two rows show results for HCP when using MERT parameters
optimized over k-best lists produced by HCP (row a) and by HiFST (row b); in the latter
case, we are tuning HCP parameters over the hypothesis list generated by HiFST. When
measured over test-nw this gives a 0.3 BLEU improvement. HCP benefits from tuning
523
Computational Linguistics Volume 36, Number 3
Table 3
Contrastive Chinese-to-English translation results (lower-cased IBM BLEU) after first-pass
decoding and subsequent rescoring steps. The MERT k-best column indicates which decoder
generated the k-best lists used in MERT optimization. The mt08 set contains 691 sentences of
newswire and 666 sentences of Web text.
decoder MERT k-best tune-nw test-nw mt08
a HCP HCP 32.8 33.1 ?
b HCP 32.9 33.4 28.2
+5g HiFST 33.4 33.8 28.7
+5g+MBR 33.6 34.0 28.9
c HiFST 33.1 33.4 28.1
+5g HiFST 33.8 34.3 29.0
+5g+LMBR 34.5 34.9 30.2
over the HiFST hypotheses and we conclude that using the k-best list obtained by the
HiFST decoder yields better parameters in optimization.
Search Errors Measured over the tune-nw development set, HiFST finds a hypothesis
with lower cost in 48.4% of the sentences. In contrast, HCP never finds any hypothesis
with a lower cost for any sentence, indicating that the described pruning strategy for
HiFST is much broader than that of HCP. Note that HCP search errors are more frequent
for this language pair. This is due to the larger search space required for full hierarchical
translation; the larger the search space, the more likely it is that search errors will be
introduced by the cube pruning algorithm.
Lattice/k-best Quality Large LMs and MBR both benefit from the richer space of
translation hypotheses contained in the lattices. Relative to the first-pass baseline in
MT08, rescoring HiFST lattices yields a gain of 2.1 BLEU, compared to a gain of 0.7
BLEU with HCP k-best lists.
4.2.3 Reliability of n-gram Posterior Distributions. MBR decoding under linear BLEU
(Tromble et al 2008) is driven mainly by the presence of high posterior n-grams in
the lattice; the low posterior n-grams have poor discriminatory power. In the following
experiment, we show that high posterior n-grams are more likely to be found in the
references, and that using the full evidence space of the lattice is much better than even
very large k-best lists for computing posterior probabilities. Let Ni = {w1, . . .,w|Ni|}
denote the set of n-grams of order i in the first-pass translation 1-best, and let Ri =
{w1, . . .,w|Ri|} denote the set of n-grams of order i in the union of the references.
For confidence threshold ?, let Ni,? = {w ? Ni : p(w|L) ? ?} denote the set of all
n-grams in Ni with posterior probability greater than or equal to ?, where p(w|L) is
the posterior probability of the n-gram w, that is, the sum of the posterior probabilities
of all translations containing at least one occurrence of w. The precision at order i for
threshold ? is the proportion of n-grams in Ni,? found in the references:
pi,? =
|Ri ?Ni,?|
|Ni,?|
. (7)
The average per-sentence 4-gram precision at a range of posterior probability thresholds
? is shown in Figure 12. The posterior probabilities are computed using either the full
524
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 12
4-gram precisions for Arabic-to-English mt02-05-tune first-pass 1-best translations computed
using the full evidence space of the lattice and k-best lists of various sizes.
lattice L or a k-best list of the specified size. The 4-gram precision of the 1-best trans-
lations is approximately 0.35. At higher values of ?, the reference precision increases
considerably. Expanding the k-best list size from 1,000 to 10,000 hypotheses only slightly
improves the precision but much higher precisions are observed when the full evidence
space of the lattice is used. The improved precision results from more accurate estimates
of n-gram posterior probabilites and emphasizes once more the advantage of lattice-
based decoding and rescoring techniques.
4.3 Grammar Configurations and Search Parameters
We report translation performance and decoding speed as we vary hierarchical gram-
mar depth and the constraints on low-level rule concatenation (see Section 3). Unless
otherwise noted, hmin = 1 and hmax = 10 throughout (except for the ?S? nonterminal
category, where these constraints are not relevant).
4.3.1 Grammars for Arabic-to-English Translation. Table 4 reports Arabic-to-English trans-
lation results using the alternative grammar configurations described in Section 3.5.
Results are shown in first-pass decoding (HiFST rows), and in rescoring with a larger
5-gram language model for the most promising configurations (+5g rows). Decoding
time is reported for first-pass decoding only; rescoring time is negligible by comparison.
As shown in the upper part of Table 4, translation under a shallow-2 grammar does
not improve relative to a shallow-1 grammar, although decoding is much slower. This
indicates that the additional hypotheses generated when allowing a hierarchical depth
of two are not useful in Arabic-to-English translation. By contrast the shallow gram-
mars that allow long-distance movement for verbs only (shallow-1+K1,K2 = 1, 3, vo and
shallow-2+K1,K2 = 2, 3, vo), perform slightly better than shallow-1 grammar at a similar
decoding time. Performance differences increase when the larger 5-gram is applied
525
Computational Linguistics Volume 36, Number 3
Table 4
Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) with various
grammar configurations. Decoding time reported in seconds per word for mt02-05-tune.
grammar time mt02-05-tune mt02-05-test mt08
HiFST shallow-1 0.8 52.7 52.0 42.9
+K1,K2 = 1, 3 1.3 52.6 51.9 42.8
+K1,K2 = 1, 3, vo 0.9 52.7 52.1 42.9
shallow-2 4.2 52.7 51.9 42.6
+K1,K2 = 2, 3, vo 1.8 52.8 52.2 43.0
+5g shallow-1 - 53.9 53.4 44.9
+K1,K2 = 1, 3, vo - 54.1 53.6 45.0
shallow-2
+K1,K2 = 2, 3, vo
- 54.2 53.8 45.0
(Table 4, bottom). This is expected given that these grammars add valid translation
candidates to the search space with similar costs; a language model is needed to select
the good hypotheses among all those introduced.
4.3.2 Grammars for Chinese-to-English Translation. Table 5 shows contrastive results in
Chinese-to-English translation for full hierarchical and shallow-n (n = 1, 2, 3) gram-
mars.3 Unlike Arabic-to-English translation, Chinese-to-English translation improves
as the hierarchical depth of the grammar is increased (i.e., for larger n). Decoding time
also increases significantly. The shallow-1 grammar constraints which worked well for
Arabic-to-English translation are clearly inadequate for this task; performance degrades
by approximately 1.0 BLEU relative to the full hierarchical grammar.
However, we find that translation under the shallow-3 grammar yields performance
nearly as good as that of the full hiero grammars; translation times are shorter and
yield degradations of only 0.1 to 0.3 BLEU. Translation can be made significantly faster
by constraining the shallow-3 search space with hmin = 9, 5, 2 for X2,X1, and X0, respec-
tively; translation speed is reduced from 10.8 sec/word to 3.8 sec/word at a degradation
of 0.2 to 0.3 BLEU relative to full Hiero.
Shallow-3 grammars describe a restricted search-space but appear to have expressive
power in Chinese-to-English translation which is very similar to that of a full Hiero
grammar. Each cell (x, y) is represented by a bigger set of nonterminals; this allows for
more effective pruning strategies during lattice construction. We note also that hmax
values greater than 10 yield little improvement. As shown in the five bottom rows of
Table 5, differences between grammar configurations tend to carry through after 5-gram
rescoring. In summary, a shallow-3 grammar and filtering with hmin = 9, 5, 2 lead to a
0.4 degradation in BLEU relative to full Hiero. As a final contrast, the mixed-case NIST
BLEU-4 for the HiFST system on mt08 is 28.6. This result is obtained under the same
evaluation conditions as the official NIST MT08 Constrained Training Track.4
3 We note that the scores in full hiero row do not match those of row c in Table 3 which were obtained with
a slightly simplified version of HiFST and optimized according to the 2008 NIST implementation of IBM
BLEU; here we use the 2009 implementation by NIST.
4 Full MT08 results are available at
www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
526
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 5
Contrastive Chinese-to-English translation results (lower-cased IBM BLEU ) with various
grammar configurations and search parameters. Decoding time is reported in sec/word for
tune-nw.
grammar time tune-nw test-nw mt08 (nw)
HiFST shallow-1 0.7 33.6 33.4 32.6
shallow-2 5.9 33.8 34.2 32.7
+hmin=5 5.6 33.8 34.1 32.9
+hmin=7 4.0 33.8 34.3 33.0
shallow-3 8.8 34.0 34.3 33.0
+hmin=7 7.7 34.0 34.4 33.1
+hmin=9 5.9 33.9 34.3 33.1
+hmin=9,5,2 3.8 34.0 34.3 33.0
+hmin=9,5,2+hmax=11 6.1 33.8 34.4 33.0
+hmin=9,5,2+hmax=13 9.8 34.0 34.4 33.1
full hiero 10.8 34.0 34.4 33.3
+5g shallow-1 - 34.1 34.5 33.4
shallow-2 - 34.3 35.1 34.0
shallow-3 - 34.6 35.2 34.4
+hmin=9,5,2 - 34.5 34.8 34.2
full hiero - 34.5 35.2 34.6
4.4 Marginalization Over Translation Derivations
As has been discussed earlier, the translation model in hierarchical phrase-based ma-
chine translation allows for multiple derivations of a target language sentence. Each
derivation corresponds to a particular combination of hierarchical rules and it has been
argued that the correct approach to translation is to accumulate translation probability
by summing over the scores of all derivations (Blunsom, Cohn, and Osborne 2008).
Computing this sum for each of the many translation candidates explored during de-
coding is computationally difficult, however. For this reason the translation probability
is commonly computed using the Viterbi max-derivation approximation. This is the
approach taken in the previous sections in which translations scores were accumulated
under the tropical semiring.
The use of WFSTs allows the sum over alternative derivations of a target string
to be computed efficiently. HiFST generates a translation lattice realized as a weighted
transducer with output labels encoding words and input labels encoding the sequence
of rules corresponding to a particular derivation, and the cost of each path in the lattice
is the negative log probability of the derivation that generated the hypothesis.
Determinization applies the ? operator to all paths with the same word se-
quence (Mohri 1997). When applied in the log semiring, this operator computes the
sum of two paths with the same word sequence as x ? y = ?log(e?x + e?y) so that the
probabilities of alternative derivations can be summed.
Currently this operation is only performed in the top cell of the hierarchical decoder
so it is still an approximation to the true translation probability. Computing the true
translation probability would require the same operation to be repeated in every cell
during decoding, which is very time consuming. Note that the translation lattice was
generated with a language model and so the language model costs must be removed
527
Computational Linguistics Volume 36, Number 3
Table 6
Arabic-to-English results (lower-cased IBM BLEU) when determinizing the lattice at the
upper-most CYK cell with alternative semirings.
semiring mt02-05-tune mt02-05-test mt08
tropical HiFST 52.8 52.2 43.0
+5g 54.2 53.8 44.9
+5g+LMBR 55.0 54.6 45.5
log HiFST 53.1 52.6 43.2
+5g 54.6 54.2 45.2
+5g+LMBR 55.0 54.6 45.5
before determinization to ensure that only the derivation probabilities are included
in the sum. After determinization, the language model is reapplied and the 1-best
translation hypothesis can be extracted from the logarc determinized lattices.
Table 6 compares translation results obtained using the tropical semiring (Viterbi
likelihoods) and the log semiring (marginal likelihoods). First-pass translation shows
small gains in all sets: +0.3 and +0.4 BLEU for mt02-05-tune and mt02-05-test, and +0.2
for mt08. These gains show that the sum over alternative derivations can be easily
obtained in HiFST simply by changing semiring and that these alternative derivations
are beneficial to translation. The gains carry through to the large language model 5-gram
rescoring stage but after LMBR the final BLEU scores are unchanged. The hypotheses
selected by LMBR are in almost all cases exactly the same regardless of the choice of
semiring. This may be due to the fact that our current marginalization procedure is only
an approximation to the true marginal likelihoods, since the log semiring determiniza-
tion operation is applied only in the uppermost cell of the CYK grid and MERT training
is performed using regular Viterbi likelihoods.
We note that a close study of the interaction between LMBR and marginalization
over derivations is beyond the scope of this paper. Our purpose here is to show how
easily these operations can be done using WFSTs.
4.5 Combining Lattices Obtained from Alternative Morphological Decompositions
It has been shown that MBR decoding is a very effective way of combining translation
hypotheses obtained from alternative morphological decompositions of the same source
data. In particular, de Gispert et al (2009) show gains for Arabic-to-English and Finnish-
to-English when taking k-best lists obtained from two morphological decompositions
of the source language. Here we extend this approach to the case of translation lat-
tices and experiment with more than two alternative decompositions. We will show
that working with translation lattices gives significant improvements relative to k-best
lists.
In lattice-based MBR system combination, first-pass decoding results in a set of I
distinct translation lattices L(i), i = 1. . .I for each foreign sentence, with each lattice
produced by translating one of the alternative morphological decompositions. The
evidence space for MBR decoding is formed as the union of these lattices L =
?I
i=1 L(i).
The posterior probability of n-gram w in the union of lattices is computed as a simple
528
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 7
Arabic-to-English results (lower-cased IBM BLEU) when using alternative Arabic
decompositions, and their combination with k-best-based and lattice-based MBR.
configuration mt02-05-tune mt02-05-test mt08
a HiFST+5g 54.2 53.8 44.9
b HiFST+5g 53.8 53.6 45.0
c HiFST+5g 54.1 53.8 44.7
a+b +MBR 55.1 54.7 46.1
+LMBR 55.7 55.4 46.7
a+c +MBR 55.4 54.9 46.5
+LMBR 56.0 55.9 46.9
a+b+c +MBR 55.3 54.9 46.5
+LMBR 56.0 55.7 47.3
linear interpolation of the posterior probabilities according to the evidence space of each
individual lattice so that
p(w|L) =
I
?
i=1
?i pi(w|L(i) ), (8)
where the interpolation parameters 0 ? ?i ? 1 such that
?I
i=1 ?i = 1 specify the weight
associated with each system in the combination and are optimized with respect to
the tuning set. The system-specific posteriors required for the interpolation are com-
puted as
pi(w|L(i) ) =
?
E?L(i)w
Pi(E|F), (9)
where Pi(E|F) is the posterior probability of translation E given source sentence F and
the sum is taken over the subset L(i)w = {E ? L(i) : #w(E) > 0} of the lattice L(i) containing
paths with at least one occurrence of the n-gram w. These posterior probabilities are
used in MBR decoding under the linear approximation to the BLEU score described
in Tromble et al (2008). We find that for system combination, decoding often produces
output that is slightly shorter than required. A fixed per-word factor optimized on the
tuning set is applied when computing the gain and this results in output with improved
BLEU score and reduced brevity penalty.
Table 7 shows translation results in Arabic-to-English using three alternative mor-
phological decompositions of the Arabic text (upper rows a, b, and c). For each decom-
position an independent set of hierarchical rules is obtained from the respective parallel
corpus alignments. The decompositions were generated by the MADA toolkit (Habash
and Rambow 2005) with two alternative tokenization schemes, and by the Sakhr Arabic
Morphological Tagger, developed by Sakhr Software in Egypt.
The following rows show the results when combining with MBR the translation
hypotheses obtained from two or three decompositions. The table also shows a contrast
529
Computational Linguistics Volume 36, Number 3
Figure 13
Average per-sentence 4-gram reference precisions for Arabic-to-English mt02-05-tune
single-system MBR 1-best translations and the 1-best obtained through MBR system
combination.
between decoding the joint k-best lists (rows named MBR, with k = 1, 000) and decod-
ing the unioned translation lattices (rows named LMBR). In line with the findings of
de Gispert et al (2009), we find significant gains from combining k-best lists with respect
to using any one segmentation alone. Interestingly, here we find further gains when
applying lattice-based MBR instead of a k-best approach, obtaining consistent gains of
0.6?0.8 BLEU across all sets.
The results reported in Table 7 are very competitive. The mixed-case NIST BLEU-4
score for a+b+c LMBR system in MT08 is 44.9. This result is obtained under the same
evaluation conditions as the official NIST MT08 Constrained Training Track.5 For MT09,
the mixed-case BLEU-4 is 48.3, which ranks first in the Arabic-to-English NIST 2009
Constrained Data Track.6
4.5.1 System Combination and Reference Precision. We have demonstrated that MBR de-
coding of multiple lattices generated from alternative morphological segmentations
leads to significant improvements in BLEU score. We now show that one reason for
the improved performance is that lattice combination leads to better n-gram posterior
probability estimates. To combine two equally weighted lattices L(1) and L(2), the in-
terpolation weights are ?1 = ?2 = 12 ; Equation (8) simplifies as p(w|L) = 12 (p1(w|L(1) )+
p2(w|L(2))). Figure 13 plots average per-sentence reference precisions for the 4-grams in
the MBR 1-best of systems a and b and their combination (labeled a+b) at a range of
posterior probability thresholds 0 ? ? ? 1. Systems a and b have similar precisions at
all values of ?, confirming that the optimal interpolation weights for this combination
should be equal. The precision obtained using n-gram posterior probabilities computed
5 Full MT08 results are available at
www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
6 Full MT09 results are available at www.itl.nist.gov/iad/mig/tests/mt/2009/ResultsRelease
530
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
from the combined lattices is higher than that of the individual systems. A higher
proportion of the n-grams assigned high posterior probability under the interpolated
distribution are found in the references and this is one of the reasons for the large gains
in BLEU in lattice-based MBR system combination.
4.6 European Language Translation
The HiFST described here has also been found to achieve competitive performance for
other language pairs, such as Spanish-to-English and Finnish-to-English.
For Spanish-to-English we carried out experiments on the shared task of the ACL
2008 Workshop on Statistical Machine Translation (Callison-Burch et al 2008) based on
the Europarl corpus. For the official test2008 evaluation set we obtain a BLEU score of
34.2 using a shallow-1 grammar. Similarly to the Arabic case, deeper grammars are not
found to improve scores for this task.
In Finnish-to-English, we conducted translation experiments based on the Europarl
corpus using 3,000 sentences from the Q4/2000 period for testing with a single ref-
erence. In this case, the shallow-1 grammar obtained a BLEU score of 28.0, whereas
the full hierarchical grammar only achieved 27.6. This is further evidence that full-
hierarchical grammars are not appropriate in all instances. In this case we suspect that
the use of Finnish words without morphological decomposition leads to data sparsity
problems and complicates the task of learning complex translation rules. The lack of
a large English language model suitable for this domain may also make it harder to
select the right hypothesis when the translation grammar produces many more English
alternatives.
5. Conclusions
We have described two linked investigations into hierarchical phrase-based translation.
We investigate the use of weighted finite state transducers rather than k-best lists to
represent the space of translation hypotheses. We describe a lattice-based Hiero de-
coder, with which we find reductions in search errors, better parameter optimization,
and improved translation performance. Relative to these reductions in search errors,
direct generation of target language translation lattices also leads to further translation
improvements through subsequent rescoring steps, such as MBR decoding and the
application of large n-gram language models. These steps can be carried out easily via
standard WFST operations.
As part of the machinery needed for our experiments we develop WFST procedures
for alignment and feature extraction so that statistics needed for system optimization
can be easily obtained and represented as transducers. In particular, we make use of
a lattice-based representation of sequences of rule applications, which proves useful
for minimum error rate training. In all instances we find that using lattices as compact
representations of translation hypotheses offers clear modeling advantages.
We also investigate refinements in translation search space through shallow-n gram-
mars, structured long-distance movement, and constrained word deletion. We find
that these techniques can be used to fit the complexity of Hiero translation systems to
individual language pairs. In translation from Arabic into English, shallow grammars
make it possible to explore the entire search space and to do so more quickly but with the
same translation quality as the full Hiero grammar. Even in complex translation tasks,
such as Chinese to English, we find significant speed improvements with minimal loss
531
Computational Linguistics Volume 36, Number 3
in performance using these methods. We take the view that it is better to perform exact
search of a constrained space than to risk search errors in translation.
We note finally that Chiang introduced Hiero as a model ?based on a synchronous
CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns
1968).? We have taken this formulation as a starting point for the development of
novel realizations of Hiero. Our motivation has mainly been practical in that we seek
improved translation quality and efficiency through better models and algorithms.
Our approach suggests close links between Hiero and Recursive Transition Net-
works (Woods 1970; Mohri 1997). Although this connection is beyond the scope of this
paper, we do note that Hiero translation requires keeping track of two grammars, one
based on the Hiero translation rules and the other based on n-gram language model
probabilities. These two grammars have very different dependencies which suggests
that a full implementation of Hiero translation such as we have addressed does not
have a simple expression as an RTN.
Acknowledgments
This work was supported in part by the
GALE program of the Defense Advanced
Research Projects Agency, Contract No.
HR0011- 06-C-0022, and in part by the
Spanish government and the ERDF under
projects TEC2006-13694-C03-03 and
TEC2009-14094-C04-04.
References
Allauzen, Cyril, Mehryar Mohri, and Brian
Roark. 2003. Generalized algorithms for
constructing statistical language models.
In Proceedings of ACL, pages 557?564,
Sapporo.
Allauzen, Cyril, Michael Riley, Johan
Schalkwyk, Wojciech Skut, and Mehryar
Mohri. 2007. OpenFst: A general and
efficient weighted finite-state transducer
library. In Proceedings of CIAA,
pages 11?23, Prague.
Bender, Oliver, Evgeny Matusov, Stefan
Hahn, Sasa Hasan, Shahram Khadivi,
and Hermann Ney. 2007. The RWTH
Arabic-to-English spoken language
translation system. In Proceedings of
ASRU, pages 396?401, Kyoto.
Blunsom, Phil, Trevor Cohn, and Miles
Osborne. 2008. A discriminative latent
variable model for statistical machine
translation. In Proceedings of ACL-HLT,
pages 200?208, Columbus, OH.
Brants, Thorsten, Ashok C. Popat, Peng Xu,
Franz J. Och, and Jeffrey Dean. 2007.
Large language models in machine
translation. In Proceedings of EMNLP-ACL,
pages 858?867, Prague.
Callison-Burch, Chris, Cameron Fordyce,
Philipp Koehn, Christof Monz, and Josh
Schroeder. 2008. Further meta-evaluation
of machine translation. In Proceedings
of the ACL Workshop on Statistical
Machine Translation, pages 70?106,
Columbus, OH.
Chappelier, Jean-Ce?dric and Martin Rajman.
1998. A generalized CYK algorithm for
parsing stochastic CFG. In Proceedings of
TAPD, pages 133?137, Paris.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of ACL,
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
de Gispert, Adria`, Sami Virpioja, Mikko
Kurimo, and William Byrne. 2009.
Minimum Bayes-Risk combination of
translation hypotheses from alternative
morphological decompositions. In
Proceedings of HLT/NAACL, Companion
Volume: Short Papers, pages 73?76,
Boulder, CO.
Deng, Yonggang and William Byrne. 2008.
HMM word and phrase alignment for
statistical machine translation. IEEE
Transactions on Audio, Speech, and Language
Processing, 16(3):494?507.
Graehl, Jonathan, Kevin Knight, and
Jonathan May. 2008. Training tree
transducers. Computational Linguistics,
34(3):391?427.
Habash, Nizar and Owen Rambow. 2005.
Arabic tokenization, part-of-speech
tagging and morphological
disambiguation in one fell swoop. In
Proceedings of the ACL, pages 573?580,
Ann Arbor, MI.
Iglesias, Gonzalo, Adria` de Gispert,
Eduardo R. Banga, and William Byrne.
2009a. Rule filtering by pattern for
532
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
efficient hierarchical translation. In
Proceedings of the EACL, pages 380?388,
Athens.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo
R. Banga, and William Byrne. 2009b.
Hierarchical phrase-based translation with
weighted finite state transducers. In
Proceedings of HLT/NAACL, pages 433?441,
Boulder, CO.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo
R. Banga, and William Byrne. 2009c. The
HiFST system for the Europarl
Spanish-to-English task. In Proceedings of
SEPLN, pages 207?214, Donosti.
Knight, Kevin and Yaser Al-Onaizan. 1998.
Translation with finite-state devices. In
Proceedings of the Third Conference of the
AMTA on Machine Translation and the
Information Soup, pages 421?437,
Langhorne, PA.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL,
pages 127?133, Edmonton.
Kumar, Shankar and William Byrne. 2004.
Minimum Bayes-risk decoding for
statistical machine translation. In
Proceedings of HLT-NAACL, pages 169?176,
Boston, MA.
Kumar, Shankar, Yonggang Deng, and
William Byrne. 2006. A weighted finite
state transducer translation template
model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lewis, P. M., II, and R. E. Stearns. 1968.
Syntax-directed transduction. Journal of the
ACM, 15(3):465?488.
Mohri, Mehryar. 1997. Finite-state
transducers in language and speech
processing. Computational Linguistics,
23:269?311.
Mohri, Mehryar, Fernando Pereira, and
Michael Riley. 2000. The design principles
of a weighted finite-state transducer
library. Theoretical Computer Science,
231:17?32.
Mohri, Mehryar, Fernando Pereira, and
Michael Riley. 2002. Weighted finite-state
transducers in speech recognition.
Computer Speech and Language, 16:69?88.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL, pages 160?167,
Sapporo.
Och, Franz Josef and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the ACL,
pages 295?302, Philadelphia, PA.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. BLEU: A
method for automatic evaluation of
machine translation. In Proceedings of ACL,
pages 311?318, Toulouse.
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing
Xiang, Spyros Matsoukas, Richard
Schwartz, and Bonnie Dorr. 2007.
Combining outputs from multiple
machine translation systems. In
Proceedings of HLT-NAACL, pages 228?235,
Rochester, NY.
Setiawan, Hendra, Min Yen Kan, Haizhou Li,
and Philip Resnik. 2009. Topological
ordering of function words in hierarchical
phrase-based translation. In Proceedings
of the ACL-IJCNLP, pages 324?332,
Singapore.
Sim, Khe Chai, William Byrne, Mark Gales,
Hichem Sahbi, and Phil Woodland. 2007.
Consensus network decoding for statistical
machine translation system combination.
In Proceedings of ICASSP, pages 105?108,
Honolulu, HI.
Tromble, Roy, Shankar Kumar, Franz J. Och,
and Wolfgang Macherey. 2008. Lattice
Minimum Bayes-Risk decoding for
statistical machine translation. In
Proceedings of EMNLP, pages 620?629,
Honolulu, HI.
Varile, Giovanni B. and Peter Lau. 1988.
Eurotra practical experience with a
multilingual machine translation system
under development. In Proceedings of the
Second Conference on Applied Natural
Language Processing, pages 160?167,
Austin, TX.
Woods, W. A. 1970. Transition network
grammars for natural language analysis.
Communications of the ACM,
13(10):591?606.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
533

Proceedings of the ACL 2010 Conference Short Papers, pages 27?32,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding
of Statistical Machine Translation Lattices
Graeme Blackwood, Adria` de Gispert, William Byrne
Machine Intelligence Laboratory
Cambridge University Engineering Department
Trumpington Street, CB2 1PZ, U.K.
{gwb24|ad465|wjb31}@cam.ac.uk
Abstract
This paper presents an efficient imple-
mentation of linearised lattice minimum
Bayes-risk decoding using weighted finite
state transducers. We introduce transduc-
ers to efficiently count lattice paths con-
taining n-grams and use these to gather
the required statistics. We show that these
procedures can be implemented exactly
through simple transformations of word
sequences to sequences of n-grams. This
yields a novel implementation of lattice
minimum Bayes-risk decoding which is
fast and exact even for very large lattices.
1 Introduction
This paper focuses on an exact implementation
of the linearised form of lattice minimum Bayes-
risk (LMBR) decoding using general purpose
weighted finite state transducer (WFST) opera-
tions1. The LMBR decision rule in Tromble et al
(2008) has the form
E? = argmax
E??E
{
?0|E?|+
?
u?N
?u#u(E?)p(u|E)
}
(1)
where E is a lattice of translation hypotheses, N
is the set of all n-grams in the lattice (typically,
n = 1 . . . 4), and the parameters ? are constants
estimated on held-out data. The quantity p(u|E)
we refer to as the path posterior probability of the
n-gram u. This particular posterior is defined as
p(u|E) = p(Eu|E) =
?
E?Eu
P (E|F ), (2)
where Eu = {E ? E : #u(E) > 0} is the sub-
set of lattice paths containing the n-gram u at least
1We omit an introduction to WFSTs for space reasons.
See Mohri et al (2008) for details of the general purpose
WFST operations used in this paper.
once. It is the efficient computation of these path
posterior n-gram probabilities that is the primary
focus of this paper. We will show how general
purpose WFST algorithms can be employed to ef-
ficiently compute p(u|E) for all u ? N .
Tromble et al (2008) use Equation (1) as an
approximation to the general form of statistical
machine translation MBR decoder (Kumar and
Byrne, 2004):
E? = argmin
E??E
?
E?E
L(E,E?)P (E|F ) (3)
The approximation replaces the sum over all paths
in the lattice by a sum over lattice n-grams. Even
though a lattice may have many n-grams, it is
possible to extract and enumerate them exactly
whereas this is often impossible for individual
paths. Therefore, while the Tromble et al (2008)
linearisation of the gain function in the decision
rule is an approximation, Equation (1) can be com-
puted exactly even over very large lattices. The
challenge is to do so efficiently.
If the quantity p(u|E) had the form of a condi-
tional expected count
c(u|E) =
?
E?E
#u(E)P (E|F ), (4)
it could be computed efficiently using counting
transducers (Allauzen et al, 2003). The statis-
tic c(u|E) counts the number of times an n-gram
occurs on each path, accumulating the weighted
count over all paths. By contrast, what is needed
by the approximation in Equation (1) is to iden-
tify all paths containing an n-gram and accumulate
their probabilities. The accumulation of probabil-
ities at the path level, rather than the n-gram level,
makes the exact computation of p(u|E) hard.
Tromble et al (2008) approach this problem by
building a separate word sequence acceptor for
each n-gram in N and intersecting this acceptor
27
with the lattice to discard all paths that do not con-
tain the n-gram; they then sum the probabilities of
all paths in the filtered lattice. We refer to this as
the sequential method, since p(u|E) is calculated
separately for each u in sequence.
Allauzen et al (2010) introduce a transducer
for simultaneous calculation of p(u|E) for all un-
igrams u ? N1 in a lattice. This transducer is
effective for finding path posterior probabilities of
unigrams because there are relatively few unique
unigrams in the lattice. As we will show, however,
it is less efficient for higher-order n-grams.
Allauzen et al (2010) use exact statistics for
the unigram path posterior probabilities in Equa-
tion (1), but use the conditional expected counts
of Equation (4) for higher-order n-grams. Their
hybrid MBR decoder has the form
E? = argmax
E??E
{
?0|E?|
+
?
u?N :1?|u|?k
?u#u(E?)p(u|E)
+
?
u?N :k<|u|?4
?u#u(E?)c(u|E)
}
, (5)
where k determines the range of n-gram orders
at which the path posterior probabilities p(u|E)
of Equation (2) and conditional expected counts
c(u|E) of Equation (4) are used to compute the
expected gain. For k < 4, Equation (5) is thus
an approximation to the approximation. In many
cases it will be perfectly fine, depending on how
closely p(u|E) and c(u|E) agree for higher-order
n-grams. Experimentally, Allauzen et al (2010)
find this approximation works well at k = 1 for
MBR decoding of statistical machine translation
lattices. However, there may be scenarios in which
p(u|E) and c(u|E) differ so that Equation (5) is no
longer useful in place of the original Tromble et
al. (2008) approximation.
In the following sections, we present an efficient
method for simultaneous calculation of p(u|E) for
n-grams of a fixed order. While other fast MBR
approximations are possible (Kumar et al, 2009),
we show how the exact path posterior probabilities
can be calculated and applied in the implementa-
tion of Equation (1) for efficient MBR decoding
over lattices.
2 N-gram Mapping Transducer
We make use of a trick to count higher-order n-
grams. We build transducer ?n to map word se-
quences to n-gram sequences of order n. ?n has a
similar form to the WFST implementation of an n-
gram language model (Allauzen et al, 2003). ?n
includes for each n-gram u = wn1 arcs of the form:
wn-11 wn2wn:u
The n-gram lattice of order n is called En and is
found by composing E ??n, projecting on the out-
put, removing ?-arcs, determinizing, and minimis-
ing. The construction of En is fast even for large
lattices and is memory efficient. En itself may
have more states than E due to the association of
distinct n-gram histories with states. However, the
counting transducer for unigrams is simpler than
the corresponding counting transducer for higher-
order n-grams. As a result, counting unigrams in
En is easier than counting n-grams in E .
3 Efficient Path Counting
Associated with each En we have a transducer ?n
which can be used to calculate the path posterior
probabilities p(u|E) for all u ? Nn. In Figures
1 and 2 we give two possible forms2 of ?n that
can be used to compute path posterior probabilities
over n-grams u1,2 ? Nn for some n. No modifica-
tion to the ?-arc matching mechanism is required
even in counting higher-order n-grams since all n-
grams are represented as individual symbols after
application of the mapping transducer ?n.
Transducer ?Ln is used by Allauzen et al (2010)
to compute the exact unigram contribution to the
conditional expected gain in Equation (5). For ex-
ample, in counting paths that contain u1, ?Ln re-
tains the first occurrence of u1 and maps every
other symbol to ?. This ensures that in any path
containing a given u, only the first u is counted,
avoiding multiple counting of paths.
We introduce an alternative path counting trans-
ducer ?Rn that effectively deletes all symbols ex-
cept the last occurrence of u on any path by en-
suring that any paths in composition which count
earlier instances of u do not end in a final state.
Multiple counting is avoided by counting only the
last occurrence of each symbol u on a path.
We note that initial ?:? arcs in ?Ln effectively
create |Nn| copies of En in composition while
searching for the first occurrence of each u. Com-
2The special composition symbol ? matches any arc; ?
matches any arc other than those with an explicit transition.
See the OpenFst documentation: http://openfst.org
28
01
2
3
u1:u1
u2:u2?:?
?:?
?:?
?:?
?:?
Figure 1: Path counting transducer ?Ln matching
first (left-most) occurrence of each u ? Nn.
0
1
3
2
4
u1:u1
u2:u2
u1:?
u2:?
?:?
?:?
?:?
Figure 2: Path counting transducer ?Rn matching
last (right-most) occurrence of each u ? Nn.
posing with ?Rn creates a single copy of En while
searching for the last occurrence of u; we find this
to be much more efficient for large Nn.
Path posterior probabilities are calculated over
each En by composing with ?n in the log semir-
ing, projecting on the output, removing ?-arcs, de-
terminizing, minimising, and pushing weights to
the initial state (Allauzen et al, 2010). Using ei-
ther ?Ln or ?Rn , the resulting counts acceptor is Xn.
It has a compact form with one arc from the start
state for each ui ? Nn:
0 iui/- log p(ui|E)
3.1 Efficient Path Posterior Calculation
Although Xn has a convenient and elegant form,
it can be difficult to build for large Nn because
the composition En ? ?n results in millions of
states and arcs. The log semiring ?-removal and
determinization required to sum the probabilities
of paths labelled with each u can be slow.
However, if we use the proposed ?Rn , then each
path in En ? ?Rn has only one non-? output la-
bel u and all paths leading to a given final state
share the same u. A modified forward algorithm
can be used to calculate p(u|E) without the costly
?-removal and determinization. The modification
simply requires keeping track of which symbol
u is encountered along each path to a final state.
More than one final state may gather probabilities
for the same u; to compute p(u|E) these proba-
bilities are added. The forward algorithm requires
that En??Rn be topologically sorted; although sort-
ing can be slow, it is still quicker than log semiring
?-removal and determinization.
The statistics gathered by the forward algo-
rithm could also be gathered under the expectation
semiring (Eisner, 2002) with suitably defined fea-
tures. We take the view that the full complexity of
that approach is not needed here, since only one
symbol is introduced per path and per exit state.
Unlike En ??Rn , the composition En ??Ln does
not segregate paths by u such that there is a di-
rect association between final states and symbols.
The forward algorithm does not readily yield the
per-symbol probabilities, although an arc weight
vector indexed by symbols could be used to cor-
rectly aggregate the required statistics (Riley et al,
2009). For large Nn this would be memory in-
tensive. The association between final states and
symbols could also be found by label pushing, but
we find this slow for large En ??n.
4 Efficient Decoder Implementation
In contrast to Equation (5), we use the exact values
of p(u|E) for all u ? Nn at orders n = 1 . . . 4 to
compute
E? = argmin
E??E
{
?0|E?|+
4
?
n=1
gn(E,E?)
}
, (6)
where gn(E,E?) =
?
u?Nn ?u#u(E
?)p(u|E) us-
ing the exact path posterior probabilities at each
order. We make acceptors ?n such that E ? ?n
assigns order n partial gain gn(E,E?) to all paths
E ? E . ?n is derived from ?n directly by assign-
ing arc weight ?u?p(u|E) to arcs with output label
u and then projecting on the input labels. For each
n-gram u = wn1 in Nn arcs of ?n have the form:
wn-11 wn2wn/?u ? p(u|E)
To apply ?0 we make a copy of E , called E0,
with fixed weight ?0 on all arcs. The decoder is
formed as the composition E0 ??1 ??2 ??3 ??4
and E? is extracted as the maximum cost string.
5 Lattice Generation for LMBR
Lattice MBR decoding performance and effi-
ciency is evaluated in the context of the NIST
29
mt0205tune mt0205test mt08nw mt08ng
ML 54.2 53.8 51.4 36.3
k
0 52.6 52.3 49.8 34.5
1 54.8 54.4 52.2 36.6
2 54.9 54.5 52.4 36.8
3 54.9 54.5 52.4 36.8
LMBR 55.0 54.6 52.4 36.8
Table 1: BLEU scores for Arabic?English maximum likelihood translation (ML), MBR decoding using
the hybrid decision rule of Equation (5) at 0 ? k ? 3, and regular linearised lattice MBR (LMBR).
mt0205tune mt0205test mt08nw mt08ng
Posteriors
sequential 3160 3306 2090 3791
?Ln 6880 7387 4201 8796
?Rn 1746 1789 1182 2787
Decoding sequential 4340 4530 2225 4104
?n 284 319 118 197
Total
sequential 7711 8065 4437 8085
?Ln 7458 8075 4495 9199
?Rn 2321 2348 1468 3149
Table 2: Time in seconds required for path posterior n-gram probability calculation and LMBR decoding
using sequential method and left-most (?Ln) or right-most (?Rn ) counting transducer implementations.
Arabic?English machine translation task3. The
development set mt0205tune is formed from the
odd numbered sentences of the NIST MT02?
MT05 testsets; the even numbered sentences form
the validation set mt0205test. Performance on
NIST MT08 newswire (mt08nw) and newsgroup
(mt08ng) data is also reported.
First-pass translation is performed using HiFST
(Iglesias et al, 2009), a hierarchical phrase-based
decoder. Word alignments are generated using
MTTK (Deng and Byrne, 2008) over 150M words
of parallel text for the constrained NIST MT08
Arabic?English track. In decoding, a Shallow-
1 grammar with a single level of rule nesting is
used and no pruning is performed in generating
first-pass lattices (Iglesias et al, 2009).
The first-pass language model is a modified
Kneser-Ney (Kneser and Ney, 1995) 4-gram esti-
mated over the English parallel text and an 881M
word subset of the GigaWord Third Edition (Graff
et al, 2007). Prior to LMBR, the lattices are
rescored with large stupid-backoff 5-gram lan-
guage models (Brants et al, 2007) estimated over
more than 6 billion words of English text.
The n-gram factors ?0, . . . , ?4 are set according
to Tromble et al (2008) using unigram precision
3http://www.itl.nist.gov/iad/mig/tests/mt
p = 0.85 and average recall ratio r = 0.74. Our
translation decoder and MBR procedures are im-
plemented using OpenFst (Allauzen et al, 2007).
6 LMBR Speed and Performance
Lattice MBR decoding performance is shown in
Table 1. Compared to the maximum likelihood
translation hypotheses (row ML), LMBR gives
gains of +0.8 to +1.0 BLEU for newswire data and
+0.5 BLEU for newsgroup data (row LMBR).
The other rows of Table 1 show the performance
of LMBR decoding using the hybrid decision rule
of Equation (5) for 0 ? k ? 3. When the condi-
tional expected counts c(u|E) are used at all orders
(i.e. k = 0), the hybrid decoder BLEU scores are
considerably lower than even the ML scores. This
poor performance is because there are many un-
igrams u for which c(u|E) is much greater than
p(u|E). The consensus translation maximising the
conditional expected gain is then dominated by
unigram matches, significantly degrading LMBR
decoding performance. Table 1 shows that for
these lattices the hybrid decision rule is an ac-
curate approximation to Equation (1) only when
k ? 2 and the exact contribution to the gain func-
tion is computed using the path posterior probabil-
ities at orders n = 1 and n = 2.
30
We now analyse the efficiency of lattice MBR
decoding using the exact path posterior probabil-
ities of Equation (2) at all orders. We note that
the sequential method and both simultaneous im-
plementations using path counting transducers ?Ln
and ?Rn yield the same hypotheses (allowing for
numerical accuracy); they differ only in speed and
memory usage.
Posteriors Efficiency Computation times for
the steps in LMBR are given in Table 2. In calcu-
lating path posterior n-gram probabilities p(u|E),
we find that the use of ?Ln is more than twice
as slow as the sequential method. This is due to
the difficulty of counting higher-order n-grams in
large lattices. ?Ln is effective for counting uni-
grams, however, since there are far fewer of them.
Using ?Rn is almost twice as fast as the sequential
method. This speed difference is due to the sim-
ple forward algorithm. We also observe that for
higher-order n, the composition En ? ?Rn requires
less memory and produces a smaller machine than
En ? ?Ln . It is easier to count paths by the final
occurrence of a symbol than by the first.
Decoding Efficiency Decoding times are signif-
icantly faster using ?n than the sequential method;
average decoding time is around 0.1 seconds per
sentence. The total time required for lattice MBR
is dominated by the calculation of the path pos-
terior n-gram probabilities, and this is a func-
tion of the number of n-grams in the lattice |N |.
For each sentence in mt0205tune, Figure 3 plots
the total LMBR time for the sequential method
(marked ?o?) and for probabilities computed using
?Rn (marked ?+?). This compares the two tech-
niques on a sentence-by-sentence basis. As |N |
grows, the simultaneous path counting transducer
is found to be much more efficient.
7 Conclusion
We have described an efficient and exact imple-
mentation of the linear approximation to LMBR
using general WFST operations. A simple trans-
ducer was used to map words to sequences of n-
grams in order to simplify the extraction of higher-
order statistics. We presented a counting trans-
ducer ?Rn that extracts the statistics required for
all n-grams of order n in a single composition and
allows path posterior probabilities to be computed
efficiently using a modified forward procedure.
We take the view that even approximate search
0 1000 2000 3000 4000 5000 6000
0
10
20
30
40
50
60
70
 
 
sequential
simultaneous ?Rn
to
ta
lt
im
e
(se
co
n
ds
)
lattice n-grams
Figure 3: Total time in seconds versus |N |.
criteria should be implemented exactly where pos-
sible, so that it is clear exactly what the system is
doing. For machine translation lattices, conflat-
ing the values of p(u|E) and c(u|E) for higher-
order n-grams might not be a serious problem, but
in other scenarios ? especially where symbol se-
quences are repeated multiple times on the same
path ? it may be a poor approximation.
We note that since much of the time in calcula-
tion is spent dealing with ?-arcs that are ultimately
removed, an optimised composition algorithm that
skips over such redundant structure may lead to
further improvements in time efficiency.
Acknowledgments
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 557?564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: a
general and efficient weighted finite-state transducer
library. In Proceedings of the 9th International Con-
ference on Implementation and Application of Au-
tomata, pages 11?23. Springer.
Cyril Allauzen, Shankar Kumar, Wolfgang Macherey,
Mehryar Mohri, and Michael Riley. 2010. Expected
31
sequence similarity maximization. In Human Lan-
guage Technologies 2010: The 11th Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, Los Angeles,
California, June.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 858?867.
Yonggang Deng and William Byrne. 2008. HMM
word and phrase alignment for statistical machine
translation. IEEE Transactions on Audio, Speech,
and Language Processing, 16(3):494?507.
Jason Eisner. 2002. Parameter estimation for prob-
abilistic finite-state transducers. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 1?8, Philadel-
phia, July.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edition.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Hierarchical phrase-
based translation with weighted finite state trans-
ducers. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 433?441, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech,
and Signal Processing, pages 181?184.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2004 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 169?176.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the Association for Computational Linguistics and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 163?
171, Suntec, Singapore, August. Association for
Computational Linguistics.
M. Mohri, F.C.N. Pereira, and M. Riley. 2008. Speech
recognition with weighted finite-state transducers.
Handbook on Speech Processing and Speech Com-
munication.
Michael Riley, Cyril Allauzen, and Martin Jansche.
2009. OpenFst: An Open-Source, Weighted Finite-
State Transducer Library and its Applications to
Speech and Language. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Tutorial Abstracts, pages 9?10, Boulder, Col-
orado, May. Association for Computational Linguis-
tics.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
620?629, Honolulu, Hawaii, October. Association
for Computational Linguistics.
32
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 155?160,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The CUED HiFST System for the WMT10 Translation Shared Task
Juan Pino Gonzalo Iglesias?1 Adria` de Gispert
Graeme Blackwood Jamie Brunning William Byrne
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
{jmp84,gi212,ad465,gwb24,jjjb2,wjb31}@eng.cam.ac.uk
? Department of Signal Processing and Communications, University of Vigo, Vigo, Spain
Abstract
This paper describes the Cambridge Uni-
versity Engineering Department submis-
sion to the Fifth Workshop on Statistical
Machine Translation. We report results for
the French-English and Spanish-English
shared translation tasks in both directions.
The CUED system is based on HiFST, a
hierarchical phrase-based decoder imple-
mented using weighted finite-state trans-
ducers. In the French-English task, we
investigate the use of context-dependent
alignment models. We also show that
lattice minimum Bayes-risk decoding is
an effective framework for multi-source
translation, leading to large gains in BLEU
score.
1 Introduction
This paper describes the Cambridge University
Engineering Department (CUED) system submis-
sion to the ACL 2010 Fifth Workshop on Statis-
tical Machine Translation (WMT10). Our trans-
lation system is HiFST (Iglesias et al, 2009a), a
hierarchical phrase-based decoder that generates
translation lattices directly. Decoding is guided
by a CYK parser based on a synchronous context-
free grammar induced from automatic word align-
ments (Chiang, 2007). The decoder is imple-
mented with Weighted Finite State Transducers
(WFSTs) using standard operations available in
the OpenFst libraries (Allauzen et al, 2007). The
use of WFSTs allows fast and efficient exploration
of a vast translation search space, avoiding search
errors in decoding. It also allows better integration
with other steps in our translation pipeline such as
5-gram language model (LM) rescoring and lattice
minimum Bayes-risk (LMBR) decoding.
1Now a member of the Department of Engineering, Uni-
versity of Cambridge, Cambridge, CB2 1PZ, U.K.
# Sentences # Tokens # Types
(A)Europarl+News-Commentary
FR 1.7 M 52.4M 139.7kEN 47.6M 121.6k
(B)Europarl+News-Commentary+UN
FR 8.7 M 277.9M 421.0kEN 241.4M 482.1k
(C)Europarl+News-Commentary+UN+Giga
FR 30.2 M 962.4M 2.4MEN 815.3M 2.7M
Table 1: Parallel data sets used for French-to-
English experiments.
We participated in the French-English and
Spanish-English translation shared tasks in each
translation direction. This paper describes the de-
velopment of these systems. Additionally, we re-
port multi-source translation experiments that lead
to very large gains in BLEU score.
The paper is organised as follows. Section 2
describes each step in the development of our sys-
tem for submission, from pre-processing to post-
processing. Section 3 presents and discusses re-
sults and Section 4 describes an additional experi-
ment on multi-source translation.
2 System Development
We built three French-English and two Spanish-
English systems, trained on different portions of
the parallel data sets available for this shared task.
Statistics for the different parallel sets are sum-
marised in Tables 1 and 2. No additional parallel
data was used. As will be shown, the largest paral-
lel corpus gave the best results in French, but this
was not the case in Spanish.
2.1 Pre-processing
The data was minimally cleaned by replacing
HTML-related metatags by their corresponding
155
# Sentences # Tokens # Types
(A) Europarl + News-Commentary
SP 1.7M 49.4M 167.2kEN 47.0M 122.7k
(B) Europarl + News-Commentary + UN
SP 6.5M 205.6M 420.8kEN 192.0M 402.8k
Table 2: Parallel data sets used for Spanish-to-
English experiments.
UTF8 token (e.g., replacing ?&amp? by ?&?) as
this interacts with tokenization. Data was then to-
kenized and lowercased, so mixed case is added as
post-processing.
2.2 Alignments
Parallel data was aligned using the MTTK toolkit
(Deng and Byrne, 2005). In the English-to-French
and English-to-Spanish directions, we trained
a word-to-phrase HMM model with maximum
phrase length of 2. In the French to English and
Spanish to English directions, we trained a word-
to-phrase HMM Model with a bigram translation
table and maximum phrase length of 4.
We also trained context-dependent alignment
models (Brunning et al, 2009) for the French-
English medium-size (B) dataset. The context of
a word is based on its part-of-speech and the part-
of-speech tags of the surrounding words. These
tags were obtained by applying the TnT Tagger
(Brants, 2000) for English and the TreeTagger
(Schmid, 1994) for French. Decision tree clus-
tering based on optimisation of the EM auxiliary
function was used to group contexts that trans-
late similarly. Unfortunately, time constraints pre-
vented us from training context-dependent models
for the larger (C) dataset.
2.3 Language Model
For each target language, we used the SRILM
Toolkit (Stolcke, 2002) to estimate separate 4-
gram LMs with Kneser-Ney smoothing (Kneser
and Ney, 1995), for each of the corpora listed in
Tables 3, 4 and 5. The LM vocabulary was ad-
justed to the parallel data set used. The compo-
nent models of each language pair were then in-
terpolated to form a single LM for use in first-pass
translation decoding. For French-to-English trans-
lation, the interpolation weights were optimised
for perplexity on a development set.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 246.4M
CNA 1.3M 34.8M
LTW 12.9M 298.7M
XIN 16.0M 352.5M
AFP 30.4M 710.6M
APW 62.1M 1268.6M
NYT 73.6M 1622.5M
Giga 21.4M 573.8M
News 48.7M 1128.4M
Total 275.4M 6236.4M
Table 3: English monolingual training corpora.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 282.8
AFP 25.2M 696.0M
APW 12.7M 300.6M
News 15.2M 373.5M
Giga 21.4M 684.4M
Total 83.5 M 2337.3M
Table 4: French monolingual training corpora.
Corpus # Sentences # Tokens
NC + News 4.0M 110.8M
EU + Gigaword (5g) 249.4M 1351.5M
Total 253.4 M 1462.3M
Table 5: Spanish monolingual training corpora.
The Spanish-English first pass LM was trained
directly on the NC+News portion of monolingual
data, as we did not find improvements by using
Europarl. The second pass rescoring LM used all
available data.
2.4 Grammar Extraction and Decoding
After unioning the Viterbi alignments, phrase-
based rules of up to five source words in length
were extracted, hierarchical rules with up to two
non-contiguous non-terminals in the source side
were then extracted applying the restrictions de-
scribed in (Chiang, 2007). For Spanish-English
and French-English tasks, we used a shallow-1
grammar where hierarchical rules are allowed to
be applied only once on top of phrase-based rules.
This has been shown to perform as well as a
fully hierarchical grammar for a Europarl Spanish-
English task (Iglesias et al, 2009b).
For translation, we used the HiFST de-
156
coder (Iglesias et al, 2009a). HiFST is a hierarchi-
cal decoder that builds target word lattices guided
by a probabilistic synchronous context-free gram-
mar. Assuming N to be the set of non-terminals
and T the set of terminals or words, then we can
define the grammar as a set R = {Rr} of rules
Rr : N ? ??r,?r? / pr, where N ? N; and
?, ? ? {N ? T}+.
HiFST translates in three steps. The first step
is a variant of the CYK algorithm (Chappelier and
Rajman, 1998), in which we apply hypothesis re-
combination without pruning. Only the source
language sentence is parsed using the correspond-
ing source-side context-free grammar with rules
N ? ?. Each cell in the CYK grid is specified
by a non-terminal symbol and position: (N,x, y),
spanning sx+y?1x on the source sentence s1...sJ .
For the second step, we use a recursive algo-
rithm to construct word lattices with all possi-
ble translations produced by the hierarchical rules.
Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In
each cell (N,x, y) of the CYK grid, we build a
target language word lattice L(N,x, y) containing
every translation of sx+y?1x from every derivation
headed by N . For efficiency, this lattice can use
pointers to lattices on other cells of the grid.
In the third step, we apply the word-based LM
via standard WFST composition with failure tran-
sitions, and perform likelihood-based pruning (Al-
lauzen et al, 2007) based on the combined trans-
lation and LM scores.
As explained before, we are using shallow-1 hi-
erarchical grammars (de Gispert et al, 2010) in
our experiments for WMT2010. One very inter-
esting aspect is that HiFST is able to build ex-
act search spaces with this model, i.e. there is no
pruning in search that may lead to spurious under-
generation errors.
2.5 Parameter Optimisation
Minimum error rate training (MERT) (Och, 2003)
under the BLEU score (Papineni et al, 2001) opti-
mises the weights of the following decoder fea-
tures with respect to the newstest2008 develop-
ment set: target LM, number of usages of the
glue rule, word and rule insertion penalties, word
deletion scale factor, source-to-target and target-
to-source translation models, source-to-target and
target-to-source lexical models, and three binary
rule count features inspired by Bender et al (2007)
indicating whether a rule occurs once, twice, or
more than twice in the parallel training data.
2.6 Lattice Rescoring
One of the advantages of HiFST is direct gener-
ation of large translation lattices encoding many
alternative translation hypotheses. These first-pass
lattices are rescored with second-pass higher-order
LMs prior to LMBR.
2.6.1 5-gram LM Lattice Rescoring
We build sentence-specific, zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram LMs esti-
mated over approximately 6.2 billion words for
English, 2.3 billion words for French, and 1.4 bil-
lion words for Spanish. For the English-French
task, the second-pass LM training data is the same
monolingual data used for the first-pass LMs (as
summarised in Tables 3, 4). The Spanish second-
pass 5-gram LM includes an additional 1.4 billion
words of monolingual data from the Spanish Giga-
Word Second Edition (Mendonca et al, 2009) and
Europarl, which were not included in the first-pass
LM (see Table 5).
2.6.2 LMBR Decoding
Minimum Bayes-risk (MBR) decoding (Kumar
and Byrne, 2004) over the full evidence space
of the 5-gram rescored lattices was applied to
select the translation hypothesis that maximises
the conditional expected gain under the linearised
sentence-level BLEU score (Tromble et al, 2008;
Blackwood and Byrne, 2010). The unigram preci-
sion p and average recall ratio r were set as de-
scribed in Tromble et al (2008) using the new-
stest2008 development set.
2.7 Hypothesis Combination
Linearised lattice minimum Bayes-risk decoding
(Tromble et al, 2008) can also be used as an ef-
fective framework for multiple lattice combination
(de Gispert et al, 2010). For the French-English
language pair, we used LMBR to combine transla-
tion lattices produced by systems trained on alter-
native data sets.
2.8 Post-processing
For both Spanish-English and French-English sys-
tems, the recasing procedure was performed with
the SRILM toolkit. For the Spanish-English sys-
tem, we created models from the GigaWord set
corresponding to each system output language.
157
Task Configuration newstest2008 newstest2009 newstest2010
FR ? EN
HiFST (A) 23.4 26.4 ?
HiFST (B) 24.0 27.3 ?
HiFST (B)CD 24.2 27.6 28.0
+5g+LMBR 24.6 28.4 28.9
HiFST (C) 24.7 28.4 28.5
+5g+LMBR 25.3 29.1 29.3
LMBR (B)CD+(C) 25.6 29.3 29.6
EN ? FR
HiFST (A) 22.5 24.2 ?
HiFST (B) 23.4 24.8 ?
HiFST (B)CD 23.3 24.8 26.7
+5g+LMBR 23.7 25.3 27.1
HiFST (C) 23.6 25.6 27.4
+5g+LMBR 23.9 25.8 27.8
LMBR (B)CD+(C) 24.2 26.1 28.2
Table 6: Translation Results for the French-English (FR-EN) language pair, shown in single-reference
lowercase IBM BLEU. Bold results correspond to submitted systems.
For the French-English system, the English model
was trained using the monolingual News corpus
and the target side of the News-Commentary cor-
pus, whereas the French model was trained using
all available constrained French data.
English, Spanish and French outputs were also
detokenized before submission. In French, words
separated by apostrophes were joined.
3 Results and Discussion
French?English Language Pair
Results are reported in Table 6. We can see
that using more parallel data consistently improves
performance. In the French-to-English direction,
the system HiFST (B) improves over HiFST (A)
by +0.9 BLEU and HiFST (C) improves over
HiFST (B) by +1.1 BLEU on the newstest2009
development set prior to any rescoring. The
same trend can be observed in the English-to-
French direction (+0.6 BLEU and +0.8 BLEU im-
provement). The use of context dependent align-
ment models gives a small improvement in the
French-to-English direction: system (B)CD im-
proves by +0.3 BLEU over system (B) on new-
stest2009. In the English-to-French direction,
there is no improvement nor degradation in per-
formance. 5-gram and LMBR rescoring also give
consistent improvement throughout the datasets.
Finally, combination between the medium-size
system (B)CD and the full-size system (C) gives
further small gains in BLEU over LMBR on each
individual system.
Spanish?English Language Pair
Results are reported in Table 7. We report experi-
mental results on two systems. The HiFST(A) sys-
tem is built on the Europarl + News-Commentary
training set. Systems HiFST (B),(B2) and (B3)
use UN data in different ways. System (B) simply
uses all the data for the standard rule extraction
procedure. System HiFST (B2) includes UN data
to build alignment models and therefore reinforce
alignments obtained from smaller dataset (A), but
extracts rules only from dataset (A). HiFST (B3)
combines hierarchical phrases extracted for sys-
tem (A) with phrases extracted from system (B).
Unfortunately, these three larger data strategies
lead to degradation over using only the smaller
dataset (A). For this reason, our best systems only
use the Euparl + News-Commentary parallel data.
This is surprising given that additional data was
helpful for the French-English task. Solving this
issue is left for future work.
4 Multi-Source Translation Experiments
Multi-source translation (Och and Ney, 2001;
Schroeder et al, 2009) is possible whenever mul-
tiple translations of the source language input sen-
tence are available. The motivation for multi-
source translation is that some of the ambiguity
that must be resolved in translating between one
pair of languages may not be present in a differ-
ent pair. In the following experiments, multiple
LMBR is applied for the first time to the task of
multi-source translation.
158
Task Configuration newstest2008 newstest2009 newstest2010
SP ? EN
HiFST (A) 24.6 26.0 29.1
+5g+LMBR 25.4 27.0 30.5
HiFST (B) 23.7 25.4 ?
HiFST (B2) 24.3 25.7 ?
HiFST (B3) 24.2 25.6 ?
EN ? SP HiFST (A) 23.9 24.5 28.0
+5g+LMBR 24.7 25.5 29.1
Table 7: Translation Results for the Spanish-English (SP-EN) language pair, shown in lowercase IBM
BLEU. Bold results correspond to submitted systems.
Configuration newstest2008 newstest2009 newstest2010
FR?EN HiFST+5g 24.8 28.5 28.8
+LMBR 25.3 29.0 29.2
ES?EN HiFST+5g 25.2 26.8 30.1
+LMBR 25.4 26.9 30.3
FR?EN + ES?EN LMBR 27.2 30.4 32.0
Table 8: Lowercase IBM BLEU for single-system LMBR and multiple LMBR multi-source translation
of French (FR) and Spanish (ES) into English (EN).
Separate second-pass 5-gram rescored lattices
EFR and EES are generated for each test set sen-
tence using the French-to-English and Spanish-to-
English HiFST translation systems. The MBR hy-
pothesis space is formed as the union of these lat-
tices. In a similar manner to MBR decoding over
multiple k-best lists in de Gispert et al (2009),
the path posterior probability of each n-gram u re-
quired for linearised LMBR is computed as a lin-
ear interpolation of the posterior probabilities ac-
cording to each individual lattice so that p(u|E) =
?FR p(u|EFR) + ?ES p(u|EES), where p(u|E) is the
sum of the posterior probabilities of all paths con-
taining the n-gram u. The interpolation weights
?FR + ?ES = 1 are optimised for BLEU score on
the development set newstest2008.
The results of single-system and multi-source
LMBR decoding are shown in Table 8. The opti-
mised interpolation weights were ?FR = 0.55 and
?ES = 0.45. Single-system LMBR gives relatively
small gains on these test sets. Much larger gains
are obtained through multi-source MBR combina-
tion. Compared to the best of the single-system 5-
gram rescored lattices, the BLEU score improves
by +2.0 for newstest2008, +1.9 for newstest2009,
and +1.9 for newstest2010. For scoring with re-
spect to a single reference, these are very large
gains indeed.
5 Summary
We have described the CUED submission to
WMT10 using HiFST, a hierarchical phrase-based
translation system. Results are very competitive in
terms of automatic metric for both English-French
and English-Spanish tasks in both directions. In
the French-English task, we have seen that the UN
and Giga additional parallel data are helpful. It
is surprising that UN data did not help for the
Spanish-English language pair.
Future work includes investigating this issue,
developing detokenization tailored to each output
language and applying context dependent align-
ment models to larger parallel datasets.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022. Gonzalo Iglesias was sup-
ported by the Spanish Government research grant
BES-2007-15956 (projects TEC2006-13694-C03-
03 and TEC2009-14094-C04-04).
159
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood and William Byrne. 2010. Ef-
ficient Path Counting Transducers for Minimum
Bayes-Risk Decoding of Statistical Machine Trans-
lation Lattices (to appear). In Proceedings of the
ACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of ANLP, pages 224?
231, April.
Jamie Brunning, Adria` de Gispert, and William Byrne.
2009. Context-dependent alignment models for
statistical machine translation. In Proceedings of
HLT/NAACL, pages 110?118.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alterna-
tive Morphological Decompositions. In Proceed-
ings of HLT/NAACL, Companion Volume: Short Pa-
pers, pages 73?76.
Adria` de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite state transducers and shallow-n grammars (to
appear). In Computational Linguistics.
Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. In Proceedings of HLT/EMNLP, pages
169?176.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-
based translation with weighted finite state transduc-
ers. In Proceedings of NAACL, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. The HiFST System for
the EuroParl Spanish-to-English Task. In Proceed-
ings of SEPLN, pages 207?214.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of ICASSP, volume 1, pages 181?184.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Angelo Mendonca, David Graff, and Denise DiPersio.
2009. Spanish Gigaword Second Edition, Linguistic
Data Consortium.
Franz Josef Och and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Machine Translation
Summit 2001, pages 253?258.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12. Manchester, UK.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word Lattices for Multi-Source Translation.
In Proceedings of EACL, pages 719?727.
Andreas Stolcke. 2002. SRILM?An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP,
volume 3, pages 901?904.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk decoding for statistical machine trans-
lation. In Proceedings of EMNLP, pages 620?629.
160
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 116?125,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Lattice-Based Minimum Error Rate Training using Weighted Finite-State
Transducers with Tropical Polynomial Weights
Aurelien Waite? Graeme Blackwood?
? Department of Engineering, University of Cambridge, Trumpington Street, CB2 1PZ, U.K.
{aaw35|wjb31}@cam.ac.uk
? IBM T.J. Watson Research, Yorktown Heights, NY-10598
blackwood@us.ibm.com
William Byrne?
Abstract
Minimum Error Rate Training (MERT) is a
method for training the parameters of a log-
linear model. One advantage of this method
of training is that it can use the large num-
ber of hypotheses encoded in a translation lat-
tice as training data. We demonstrate that the
MERT line optimisation can be modelled as
computing the shortest distance in a weighted
finite-state transducer using a tropical polyno-
mial semiring.
1 Introduction
Minimum Error Rate Training (MERT) (Och, 2003)
is an iterative procedure for training a log-linear sta-
tistical machine translation (SMT) model (Och and
Ney, 2002). MERT optimises model parameters
directly against a criterion based on an automated
translation quality metric, such as BLEU (Papineni
et al, 2002). Koehn (2010) provides a full descrip-
tion of the SMT task and MERT.
MERT uses a line optimisation procedure (Press
et al, 2002) to identify a range of points along a line
in parameter space that maximise an objective func-
tion based on the BLEU score. A key property of the
line optimisation is that it can consider a large set of
hypotheses encoded as a weighted directed acyclic
graph (Macherey et al, 2008), which is called a lat-
tice. The line optimisation procedure can also be ap-
plied to a hypergraph representation of the hypothe-
ses (Kumar et al, 2009).
?The work reported in this paper was carried out while the
author was at the University of Cambridge.
It has been noted that line optimisation over a lat-
tice can be implemented as a semiring of sets of lin-
ear functions (Dyer et al, 2010). Sokolov and Yvon
(2011) provide a formal description of such a semir-
ing, which they denote the MERT semiring. The dif-
ference between the various algorithms derives from
the differences in their formulation and implemen-
tation, but not in the objective they attempt to opti-
mise.
Instead of an algebra defined in terms of trans-
formations of sets of linear functions, we propose
an alternative formulation using the tropical polyno-
mial semiring (Speyer and Sturmfels, 2009). This
semiring provides a concise formalism for describ-
ing line optimisation, an intuitive explanation of the
MERT shortest distance, and draws on techniques
in the currently active field of Tropical Geometry
(Richter-Gebert et al, 2005) 1.
We begin with a review of the line optimisation
procedure, lattice-based MERT, and the weighted
finite-state transducer formulation in Section 2. In
Section 3, we introduce our novel formulation
of lattice-based MERT using tropical polynomial
weights. Section 4 compares the performance of our
approach with k-best and lattice-based MERT.
2 Minimum Error Rate Training
Following Och and Ney (2002), we assume that
we are given a tuning set of parallel sentences
{(r1, f1), ..., (rS , fS)}, where rs is the reference
translation of the source sentence fs. We also as-
sume that sets of hypotheses Cs = {es,1, ..., es,K}
1An associated technical report contains an extended discus-
sion of our approach (Waite et al, 2011)
116
are available for each source sentence fs.
Under the log-linear model formulation with fea-
ture functions hM1 and model parameters ?M1 , the
most probable translation in a set Cs is selected as
e?(fs;?M1 ) = argmax
e?Cs
{ M?
m=1
?mhm(e, fs)
}
. (1)
With an error function of the form E(rS1 , eS1 ) =?S
s=1E(rs, es), MERT attempts to find model pa-
rameters to minimise the following objective:
??M1 = argmin
?M1
{ S?
s=1
E(rs, e?(fs;?M1 ))
}
. (2)
Note that for MERT the hypotheses set Cs is
a k-best list of explicitly enumerated hypotheses,
whereas lattice-based MERT uses a larger space.
2.1 Line Optimisation
Although the objective function in Eq. (2) cannot be
solved analytically, the line optimisation procedure
of Och (2003) can be used to find an approxima-
tion of the optimal model parameters. Rather than
evaluating the decision rule in Eq. (1) over all pos-
sible points in parameter space, the line optimisa-
tion considers a subset of points defined by the line
?M1 +?dM1 , where ?M1 corresponds to an initial point
in parameter space and dM1 is the direction along
which to optimise. Eq. (1) can be rewritten as:
e?(fs; ?) = argmax
e?Cs
{
(?M1 + ?dM1 )ThM1 (e, f s)
}
= argmax
e?Cs
{?
m
?mhm(e, f s)
? ?? ?
a(e,fs)
+?
?
m
dmhm(e, f s)
? ?? ?
b(e,fs)
}
= argmax
e?Cs
{a(e, f s) + ?b(e, f s)? ?? ?
`e(?)
} (3)
This decision rule shows that each hypothesis
e ? Cs is associated with a linear function of ?:
`e(?) = a(e, f s) + ?b(e, f s), where a(e, f s) is the
y-intercept and b(e, f s) is the gradient. The opti-
misation problem is further simplified by defining a
subspace over which optimisation is performed. The
subspace is found by considering a form of the func-
tion in Eq. (3) defined with a range of real numbers
(Macherey et al, 2008; Och, 2003):
Env(f) = max
e?C
{a(e, f) + ?b(e, f)? ?? ?
`e(?)
: ? ? R} (4)
?
Env(fs; ?)
`e1
`e2 `e3
`e4
?
E(rs, e?(fs; ?))
e4
e3
e1
?1 ?2
Figure 1: An upper envelope and projected error. Note
that the upper envelope is completely defined by hypothe-
ses e4, e3, and e1, together with the intersection points ?1
and ?2 (after Macherey et al (2008), Fig. 1).
For any value of ? the linear functions `e(?) associ-
ated with Cs take (up to) K values. The function in
Eq. (4) defines the ?upper envelope? of these values
over all ?. The upper envelope has the form of a con-
tinuous piecewise linear function in ?. The piece-
wise linear function can be compactly described by
the linear functions which form line segments and
the values of ? at which they intersect. The example
in the upper part of Figure 1 shows how the upper
envelope associated with a set of four hypotheses
can be represented by three associated linear func-
tions and two values of ?. The first step of line op-
timisation is to compute this compact representation
of the upper envelope.
Macherey et al (2008) use methods from com-
putational geometry to compute the upper envelope.
The SweepLine algorithm (Bentley and Ottmann,
1979) computes the upper envelope from a set of lin-
ear functions with a complexity of O(K log(K)).
Computing the upper envelope reduces the run-
time cost of line optimisation as the error function
need only be evaluated for the subset of hypotheses
in Cs that contribute to the upper envelope. These
errors are projected onto intervals of ?, as shown in
the lower part of Figure 1, so that Eq. (2) can be
readily solved.
2.2 Incorporation of Line Optimisation into
MERT
The previous algorithm finds the upper envelope
along a particular direction in parameter space over
117
a hypothesis set Cs. The line optimisation algorithm
is then embedded within a general optimisation pro-
cedure. A common approach to MERT is to select
the directions using Powell?s method (Press et al,
2002). A line optimisation is performed on each co-
ordinate axis. The axis giving the largest decrease
in error is replaced with a vector between the initial
parameters and the optimised parameters. Powell?s
method halts when there is no decrease in error.
Instead of using Powell?s method, the Downhill
Simplex algorithm (Press et al, 2002) can be used
to explore the criterion in Eq. (2). This is done by
defining a simplex in parameter space. Directions
where the error count decreases can be identified by
considering the change in error count at the points
of the simplex. This has been applied to parameter
searching over k-best lists (Zens et al, 2007).
Both Powell?s method and the Downhill Simplex
algorithms are approaches based on heuristics to se-
lect lines ?M1 + ?dM1 . It is difficult to find theoret-
ically sound reasons why one approach is superior.
Therefore Cer et al (2008) instead choose the di-
rection vectors dM1 at random. They report that this
method can find parameters that are as good as the
parameters produced by more complex algorithms.
2.3 Lattice Line Optimisation
Macherey et al (2008) describe a procedure for con-
ducting line optimisation directly over a word lattice
encoding the hypotheses in Cs. Each lattice edge is
labelled with a word e and has a weight defined by
the vector of word specific feature function values
hM1 (e, f) so that the weight of a path in the lattice
is found by summing over the word specific feature
function values on that path. Given a line through
parameter space, the goal is to extract from a lattice
its upper envelope and the associated hypotheses.
Their algorithm proceeds node by node through
the lattice. Suppose that for a state q the upper enve-
lope is known for all the partial hypotheses on all
paths leading to q. The upper envelope defines a
set of functions {`e?1(?), ..., `e?N (?)} over the partial
hypotheses e?n. Two operations propagate the upper
envelope to other lattice nodes.
We refer to the first operation as the ?extend? op-
eration. Consider a single edge from state q to state
q?. This edge defines a linear function associated
with a single word `e(?). A path following this edge
transforms all the partial hypotheses leading to q by
concatenating the word e. The upper envelope as-
sociated with the edge from q to q? is changed by
adding `e(?) to the set of linear functions. The in-
tersection points are not changed by this operation.
The second operation is a union. Suppose q?
has another incoming edge from a state q?? where
q 6= q??. There are now two upper envelopes rep-
resenting two sets of linear functions. The first up-
per envelope is associated with the paths from the
initial state to state q? via the state q. Similarly the
second upper envelope is associated with paths from
the initial state to state q? via the state q??. The upper
envelope that is associated with all paths from the
initial state to state q? via both q and q?? is the union
of the two sets of linear functions. This union is no
longer a compact representation of the upper enve-
lope as there may be functions which never achieve
a maximum for any value of ?. The SweepLine al-
gorithm (Bentley and Ottmann, 1979) is applied to
the union to discard redundant linear functions and
their associated hypotheses (Macherey et al, 2008).
The union and extend operations are applied to
states in topological order until the final state is
reached. The upper envelope computed at the final
state compactly encodes all the hypotheses that max-
imise Eq. (1) along the line ?M1 + ?dM1 . Macherey?s
theorem (Macherey et al, 2008) states that an upper
bound for the number of linear functions in the up-
per envelope at the final state is equal to the number
of edges in the lattice.
2.4 Line Optimisation using WFSTs
Formally, a weighted finite-state transducer (WFST)
T = (?,?, Q, I, F,E, ?, ?) over a semiring
(K,?,?, 0?, 1?) is defined by an input alphabet ?, an
output alphabet ?, a set of states Q, a set of initial
states I ? Q, a set of final states F ? Q, a set
of weighted transitions E, an initial state weight as-
signment ? : I ? K, and a final state weight assign-
ment ? : F ? K (Mohri et al, 2008). The weighted
transitions of T form the setE ? Q?????K?Q,
where each transition includes a source state fromQ,
input symbol from ?, output symbol from ?, cost
from the weight set K, and target state from Q.
For each state q ? Q, let E[q] denote the set of
edges leaving state q. For each transition e ? E[q],
let p[e] denote its source state, n[e] its target state,
118
and w[e] its weight. Let pi = e1 ? ? ? eK denote a
path in T from state p[e1] to state n[eK ], so that
n[ek?1] = p[ek] for k = 2, . . . ,K. The weight as-
sociated by T to path pi is the generalised product ?
of the weights of the individual transitions:
w[pi] =
K?
k=1
w[ek] = w[e1]? ? ? ? ? w[eK ] (5)
If P(q) denotes the set of all paths in T start-
ing from an initial state in I and ending in state q,
then the shortest distance d[q] is defined as the gen-
eralised sum ? of the weights of all paths leading to
q (Mohri, 2002):
d[q] = ?pi?P(q)w[pi] (6)
For some semirings, such as the tropical semir-
ing, the shortest distance is the weight of the short-
est path. For other semirings, the shortest distance
is associated with multiple paths (Mohri, 2002); for
these semirings there are shortest distances but need
not any be shortest paths. That will be the case in
what follows. However, the shortest distance algo-
rithms rely only on general properties of semirings,
and once the semiring is specified, the general short-
est distance algorithms can be directly employed.
Sokolov and Yvon (2011) define the MERT
semiring based on operations described in the pre-
vious section. The extend operation is used for the
generalised product ?. The union operation fol-
lowed by an application of the SweepLine algorithm
becomes the generalised sum ?. The word lattice
is then transformed for an initial parameter ?M1 and
direction dM1 . The weight of edge is mapped from
a word specific feature function hM1 (e, f) to a word
specific linear function `e(?). The weight of each
path is the generalised product ? of the word spe-
cific feature linear functions. The upper envelope is
the shortest distance of all the paths in the WFST.
3 The Tropical Polynomial Semiring
In this section we introduce the tropical polynomial
semiring (Speyer and Sturmfels, 2009) as a replace-
ment for the MERT semiring (Sokolov and Yvon,
2011). We then provide a full description and a
worked example of our MERT algorithm.
3.1 Tropical Polynomials
A polynomial is a linear combination of a finite
number of non-zero monomials. A monomial con-
sists of a real valued coefficient multiplied by one or
more variables, and these variables may have expo-
nents that are non-negative integers. In this section
we limit ourselves to a description of a polynomial
in a single variable. A polynomial function is de-
fined by evaluating a polynomial:
f(?) = an?n + an?1?n?1 + ? ? ?+ a2?2 + a1?+ a0
A useful property of these polynomials is that they
form a ring2 (Cox et al, 2007) and therefore are can-
didates for use as weights in WFSTs.
Speyer and Sturmfels (2009) apply the defini-
tion of a classical polynomial to the formulation of
a tropical polynomial. The tropical semiring uses
summation for the generalised product ? and a min
operation for the generalised sum ?. In this form,
let ? be a variable that represents an element in the
tropical semiring weight set R ? {??,+?}. We
can write a monomial of ? raised to an integer expo-
nent as
?i = ? ? ? ? ? ? ?? ?? ?
i
where i is a non-negative integer. The monomial
can also have a constant coefficient: a? ?i, a ? R.
We can define a function that evaluates a tropical
monomial for a particular value of ?. For example,
the tropical monomial a? ?i is evaluated as:
f(?) = a? ?i = a+ i?
This shows that a tropical monomial is a linear
function with the coefficient a as its y-intercept and
the integer exponent i as its gradient. A tropical
polynomial is the generalised sum of tropical mono-
mials where the generalised sum is evaluated using
the min operation. For example:
f(?) = (a? ?i)? (b? ?j) = min(a+ i?, b+ j?)
Evaluating tropical polynomials in classical arith-
metic gives the minimum of a finite collection of
linear functions.
Tropical polynomials can also be multiplied by a
monomial to form another tropical polynomial. For
example:
f(?) = [(a? ?i)? (b? ?j)]? (c? ?k)
= [(a+ c)? ?i+k]? [(b+ c)? ?j+k]
= min((a+ c) + (i+ k)?, (b+ c) + (j + k)?)
2A ring is a semiring that includes negation.
119
Our re-formulation of Eq. (4) negates the feature
function weights and replaces the argmax by an
argmin. This allows us to keep the usual formu-
lation of tropical polynomials in terms of the min
operation when converting Eq. (4) to a tropical rep-
resentation. What remains to be addressed is the role
of integer exponents in the tropical polynomial.
3.2 Integer Realisations for Tropical
Monomials
In the previous section we noted that the function
defined by the upper envelope in Eq. (4) is simi-
lar to the function represented by a tropical poly-
nomial. A significant difference is that the formal
definition of a polynomial only allows integer expo-
nents, whereas the gradients in Eq. (4) are real num-
bers. The upper envelope therefore encodes a larger
set of model parameters than a tropical polynomial.
To create an equivalence between the upper enve-
lope and tropical polynomials we can approximate
the linear functions {`e(?) = a(e, f s)+? ? b(e, f s)}
that compose segments of the upper envelope. We
define a?(e, f s) = [a(e, f s) ? 10n]int and b?(e, f s) =
[b(e, f s)?10n]int where [x]int denotes the integer part
of x. The approximation to `e(?) is:
`e(?) ? ?`e(?) =
a?(e, f s)
10n + ? ?
b?(e, f s)
10n (7)
The result of this operation is to approximate
the y-intercept and gradient of `e(?) to n decimal
places. We can now represent the linear function
?`e(?) as the tropical monomial?a?(e, fs)???b?(e,fs).
Note that a?(e, fs) and b?(e, fs) are negated since trop-
ical polynomials define the lower envelope as op-
posed to the upper envelope defined by Eq. (4).
The linear function represented by the tropical
monomial is a scaled version of `e(?), but the up-
per envelope is unchanged (to the accuracy allowed
by n). If for a particular value of ?, `ei(?) > `ej (?),
then ?`ei(?) > ?`ej (?). Similarly, the boundary
points are unchanged: if `ei(?) = `ej (?), then
?`ei(?) = ?`ej (?). Setting n to a very large value re-
moves numerical differences between the upper en-
velope and the tropical polynomial representation,
as shown by the identical results in Table 1.
Using a scaled version of `e(?) as the basis for a
tropical monomial may cause negative exponents to
be created. Following Speyer and Sturmfels (2009),
?
f(?)
0
a? ?i
(a? ?i)? (b? ?j)? (c? ?k)
b? ?j
c? ?k
Figure 2: Redundant terms in a tropical polynomial. In
this case (a??i)?(b??j)?(c??k) = (a??i)?(c??k).
we widen the definition of a tropical polynomial to
allow for these negative exponents.
3.3 Canonical Form of a Tropical Polynomial
We noted in Section 2.1 that linear functions induced
by some hypotheses do not contribute to the upper
envelope and can be discarded. Terms in a tropi-
cal polynomial can have similar behaviour. Figure
2 plots the lines associated with the three terms of
the example polynomial function f(?) = (a??i)?
(b??j)?(c??k). We note that the piecewise linear
function can also be described with the polynomial
f(?) = (a??i)?(c??k). The latter representation
is simpler but equivalent.
Having multiple representations of the same poly-
nomial causes problems when implementing the
shortest distance algorithm defined by Mohri (2002).
This algorithm performs an equality test between
values in the semiring used to weight the WFST. The
behaviour of the equality test is ambiguous when
there are multiple polynomial representations of the
same piecewise linear function. We therefore re-
quire a canonical form of a tropical polynomial so
that a single polynomial represents a single function.
We define the canonical form of a tropical polyno-
mial to be the tropical polynomial that contains only
the monomial terms necessary to describe the piece-
wise linear function it represents.
We remove redundant terms from a tropical poly-
nomial after computing the generalised sum. For a
tropical polynomial of one variable we can take ad-
vantage of the equivalence with Lattice MERT and
compute the canonical form using the SweepLine al-
gorithm (Bentley and Ottmann, 1979). Each term
120
corresponds to a linear function; linear functions
that do not contribute to the upper envelope are dis-
carded. Only monomials which correspond to the
remaining linear functions are kept in the canonical
form. The canonical form of a tropical polynomial
thus corresponds to a unique and minimal represen-
tation of the upper envelope.
3.4 Relationship to the Tropical Semiring
Tropical monomial weights can be transformed into
regular tropical weights by evaluating the tropical
monomial for a specific value of ?. For example, a
tropical polynomial evaluated at ? = 1 corresponds
to the tropical weight:
f(1) = ?a?(e, fs)? 1?b?(e,fs) = ?a?(e, fs)? b?(e, fs)
Each monomial term in the tropical polynomial
shortest distance represents a linear function. The
intersection points of these linear functions define
intervals of ? (as in Fig. 1). This suggests an alter-
nate explanation for what the shortest distance com-
puted using the tropical polynomial semiring rep-
resents. Conceptually, there is a continuum of lat-
tices which have identical edges and vertices but
with varying, real-valued edge weights determined
by values of ? ? R, so that each lattice in the contin-
uum is indexed by ?. The tropical polynomial short-
est distance agrees with the shortest distance through
each lattice in the continuum.
Our alternate explanation is consistent with the
Theorem of Macherey (Section 2.3), as there could
never be more paths than edges in the lattice. There-
fore the upper bound for the number of monomial
terms in the tropical polynomial shortest distance is
the number of edges in the input lattice.
We can use the mapping to the tropical semiring
to compute the error surface. Let us assume we have
n + 1 intervals separated by n interval boundaries.
We use the midpoint of each interval to transform the
lattice of tropical monomial weights into a lattice of
tropical weights. The sequence of words that label
the shortest path through the transformed lattice is
the MAP hypothesis for the interval. The shortest
path can be extracted using the WFST shortest path
algorithm (Mohri and Riley, 2002). As a technical
matter, the midpoints of the first interval [??, ?1)
and last interval [?n,?) are not defined. We there-
fore evaluate the tropical polynomial at ? = ?1 ? 1
and ? = ?n + 1 to find the MAP hypothesis in the
first and last intervals, respectively.
3.5 The TGMERT Algorithm
We now describe an alternative algorithm to Lat-
tice MERT that is formulated using the tropical
polynomial shortest distance in one variable. We
call the algorithm TGMERT, for Tropical Geome-
try MERT. As input to this procedure we use a word
lattice weighted with word specific feature functions
hM1 (e, f), a starting point ?M1 , and a direction dM1 in
parameter space.
1. Convert the word specific feature functions
hM1 (e, f) to a linear function `e(?) using ?M1
and dM1 , as in Eq. (3).
2. Convert `e(?) to ?`e(?) by approximating y-
intercepts and gradients to n decimal places, as
in Eq. (7).
3. Convert ?`e(?) in Eq. (7) to the tropical mono-
mial ?a?(e, fs)? ??b?(e,fs).
4. Compute the WFST shortest distance to the exit
states (Mohri, 2002) with generalised sum ?
and generalised product ? defined by the trop-
ical polynomial semiring. The resulting trop-
ical polynomial represents the upper envelope
of the lattice.
5. Compute the intersection points of the linear
functions corresponding to the monomial terms
of the tropical polynomial shortest distance.
These intersection points define intervals of ?
in which the MAP hypothesis does not change.
6. Using the midpoint of each interval convert the
tropical monomial?a?(e, fs)???b?(e,fs) to a reg-
ular tropical weight. Find the MAP hypothesis
for this interval by extracting the shortest path
using the WFST shortest path algorithm (Mohri
and Riley, 2002).
3.6 TGMERT Worked Example
This section presents a worked example showing
how we can use the TGMERT algorithm to compute
the upper envelope of a lattice. We start with a three
state lattice with a two dimensional feature vector
shown in the upper part of Figure 3.
We want to optimise the parameters along a line
in two-dimensional feature space. Suppose the ini-
tial parameters are ?21 = [0.7, 0.4] and the direction
121
0 1 2
z/[?0.2, 0.7]?
x/[?1.4, 0.3]?
y/[?0.9,?0.8]?
z/[?0.2,?0.6]?
0 1 2
z/?14? ??29
x/86? ?27
y/95? ?67
z/38? ?36
Figure 3: The upper part is a translation lattice with 2-
dimensional log feature vector weights hM1 (e, f) where
M = 2. The lower part is the lattice from the upper part
with weights transformed into tropical monomials.
is d21 = [0.3, 0.5]. Step 1 of the TGMERT algorithm
(Section 3.5) maps each edge weight to a word spe-
cific linear function. For example, the weight of the
edge labelled ?x? between states 0 and 1 is trans-
formed as follows:
`e(?) =
2?
m=1
?mhM1 (e, f)
? ?? ?
a(e,f)
+?
2?
m=1
dmhM1 (e,fs)
? ?? ?
b(e,f)
= 0.7 ? ?1.4 + 0.4 ? 0.3? ?? ?
a(e,f)
+? ? 0.3 ? ?1.4 + 0.5 ? 0.3? ?? ?
b(e,f)
= ?0.86? 0.27?
Step 2 of the TGMERT algorithm converts the
word specific linear functions into tropical mono-
mial weights. Since all y-intercepts and gradients
have a precision of two decimal places, we scale the
linear functions `e(?) by 102 and negate them to cre-
ate tropical monomials (Step 3). The edge labelled
?x? now has the monomial weight of 86? ?27. The
transformed lattice with weights mapped to the trop-
ical polynomial semiring is shown in the lower part
of Figure 3.
We can now compute the shortest distance
(Mohri, 2002) from the transformed example lattice
with tropical monomial weights. There are three
unique paths through the lattice corresponding to
three distinct hypotheses. The weights associated
with these hypotheses are:
?14? ??29 ? 38? ?36 = 24? ?7 z z
86? ?27 ? 38? ?36 = 122? ?63 x z
95? ?67 ? 38? ?36 = 133? ?103 y z
0 1 2
z/-2.4
x/75.2
y/68.2
z/23.6
0 1 2
z/55.6
x/21.2
y/-65.8
z/-48.4
Figure 4: The lattice in the lower part of Figure 3 trans-
formed to regular tropical weights: ? = ?0.4 (top) and
? = ?1.4 (bottom).
The shortest distance from initial to final state is
the generalised sum of the path weights: (24??7)?
(133? ?103). The monomial term 122? ?63 corre-
sponding to ?x z? can be dropped because it is not
part of the canonical form of the polynomial (Sec-
tion 3.3). The shortest distance to the exit state can
be represented as the minimum of two linear func-
tions: min(24 + 7?, 133 + 103?).
We now wish to find the hypotheses that define
the error surface by performing Steps 5 and 6 of the
TGMERT algorithm. These two linear functions de-
fine two intervals of ?. The linear functions intersect
at ? ? ?1.4; at this value of ? the MAP hypothesis
changes. Two lattices with regular tropical weights
are created using ? = ?0.4 and ? = ?2.4. These
are shown in Figure 4. For the lattice shown in the
upper part the value for the edge labelled ?x? is com-
puted as 86??0.427 = 86 + 0.4 ? 27 = 75.2.
When ? = ?0.4 the lattice in the upper part in
Figure 4 shows that the shortest path is associated
with the hypothesis ?z z?, which is the MAP hy-
pothesis for the range ? < 1.4. The lattice in the
lower part of Figure 4 shows that when ? = ?2.4
the shortest path is associated with the hypothesis
?y z?, which is the MAP hypothesis when ? > 1.4.
3.7 TGMERT Implementation
TGMERT is implemented using the OpenFst Toolkit
(Allauzen et al, 2007). A weight class is added
for tropical polynomials which maintains them in
canonical form. The ? and ? operations are im-
plemented for piece-wise linear functions, with the
SweepLine algorithm included as discussed.
122
Iteration Arabic-to-English
MERT LMERT TGMERT
Tune Test Tune Test Tune Test
1 36.2 36.2 36.242.1 40.9 39.7 38.9 39.7 38.9
2 42.0 44.5 44.545.1 43.2 45.8 44.3 45.8 44.3
3 44.545.5 44.1
4 45.645.7 44.0
Iteration Chinese-to-English
MERT LMERT TGMERT
Tune Test Tune Test Tune Test
1 19.5 19.5 19.525.3 16.7 29.3 22.6 29.3 22.6
2 16.4 22.5 22.518.9 23.9 31.4 32.1 31.4 32.1
3 23.6 31.6 31.628.2 29.1 32.2 32.5 32.2 32.5
4 29.2 32.2 32.231.3 31.5 32.2 32.5 32.2 32.5
5 31.331.8 32.1
6 32.132.4 32.3
7 32.432.4 32.3
Table 1: GALE AR?EN and ZH?EN BLEU scores
by MERT iteration. BLEU scores at the initial and final
points of each iteration are shown for the Tune sets.
4 Experiments
We compare feature weight optimisation using k-
best MERT (Och, 2003), lattice MERT (Macherey
et al, 2008), and tropical geometry MERT. We refer
to these as MERT, LMERT, and TGMERT, resp.
We investigate MERT performance in the context
of the Arabic-to-English GALE P4 and Chinese-
to-English GALE P3 evaluations3. For Arabic-to-
English translation, word alignments are generated
over around 9M sentences of GALE P4 parallel text.
Following de Gispert et al (2010b), word align-
ments for Chinese-to-English translation are trained
from a subset of 2M sentences of GALE P3 paral-
lel text. Hierarchical rules are extracted from align-
ments using the constraints described in (Chiang,
2007) with additional count and pattern filters (Igle-
3See http://projects.ldc.upenn.edu/gale/data/catalog.html
sias et al, 2009b). We use a hierarchical phrase-
based decoder (Iglesias et al, 2009a; de Gispert et
al., 2010a) which directly generates word lattices
from recursive translation networks without any in-
termediate hypergraph representation (Iglesias et al,
2011). The LMERT and TGMERT optimisation al-
gorithms are particularly suitable for this realisation
of hiero in that the lattice representation avoids the
need to use the hypergraph formulation of MERT
given by Kumar et al (2009).
MERT optimises the weights of the following fea-
tures: target language model, source-to-target and
target-to-source translation models, word and rule
penalties, number of usages of the glue rule, word
deletion scale factor, source-to-target and target-to-
source lexical models, and three count-based fea-
tures that track the frequency of rules in the parallel
data (Bender et al, 2007). In both Arabic-to-English
and Chinese-to-English experiments all MERT im-
plementations start from a flat feature weight initial-
ization. At each iteration new lattices and k-best lists
are generated from the best parameters at the previ-
ous iteration, and each subsequent iteration includes
100 hypotheses from the previous iteration. For
Arabic-to-English we consider an additional twenty
random starting parameters at every iteration. All
translation scores are reported for the IBM imple-
mentation of BLEU using case-insensitive match-
ing. We report BLEU scores for the Tune set at the
start and end of each iteration.
The results for Arabic-to-English and Chinese-
to-English are shown in Table 1. Both TGMERT
and LMERT converge to a small gain over MERT
in fewer iterations, consistent with previous re-
ports (Macherey et al, 2008).
5 Discussion
We have described a lattice-based line optimisation
algorithm which can be incorporated into MERT
for parameter tuning of SMT systems and systems
based on log-linear models. Our approach recasts
the optimisation procedure used in MERT in terms
of Tropical Geometry; given this formulation imple-
mentation is relatively straightforward using stan-
dard WFST operations and algorithms.
123
References
C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and
M. Mohri. 2007. OpenFst: A general and efficient
weighted finite-state transducer library. In Proceed-
ings of the Ninth International Conference on Imple-
mentation and Application of Automata, pages 11?23.
O. Bender, E. Matusov, S. Hahn, S. Hasan, S. Khadivi,
and H. Ney. 2007. The RWTH Arabic-to-English spo-
ken language translation system. In Automatic Speech
Recognition Understanding, pages 396 ?401.
J.L. Bentley and T.A. Ottmann. 1979. Algorithms for
reporting and counting geometric intersections. Com-
puters, IEEE Transactions on, C-28(9):643 ?647.
Daniel Cer, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Regularization and search for minimum
error rate training. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 26?34.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
David A. Cox, John Little, and Donal O?Shea. 2007.
Ideals, Varieties, and Algorithms: An Introduction to
Computational Algebraic Geometry and Commutative
Algebra, 3/e (Undergraduate Texts in Mathematics).
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010a. Hier-
archical phrase-based translation with weighted finite-
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3):505?533.
Adria` de Gispert, Juan Pino, and William Byrne. 2010b.
Hierarchical phrase-based translation grammars ex-
tracted from alignment posterior probabilities. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 545?554.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7?12, July.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-based
translation with weighted finite state transducers. In
Proceedings of HLT: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, pages
380?388.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adria`
de Gispert, and Michael Riley. 2011. Hierarchical
phrase-based translation representations. In Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 1373?1383. As-
sociation for Computational Linguistics.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 725?734.
Mehryar Mohri and Michael Riley. 2002. An efficient
algorithm for the n-best-strings problem. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing 2002.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Ri-
ley. 2008. Speech recognition with weighted finite-
state transducers. Handbook on Speech Processing
and Speech Communication.
Mehryar Mohri. 2002. Semiring frameworks and algo-
rithms for shortest-distance problems. J. Autom. Lang.
Comb., 7(3):321?350.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
K. A. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318.
W. H. Press, W. T. Vetterling, S. A. Teukolsky, and B. P.
Flannery. 2002. Numerical Recipes in C++: the art
of scientific computing. Cambridge University Press.
J. Richter-Gebert, B. Sturmfels, and T. Theobald. 2005.
First steps in tropical geometry. In Idempotent mathe-
matics and mathematical physics.
Artem Sokolov and Franc?ois Yvon. 2011. Minimum er-
ror rate training semiring. In Proceedings of the Euro-
pean Association for Machine Translation.
124
David Speyer and Bernd Sturmfels. 2009. Tropical
mathematics. Mathematics Magazine.
Aurelien Waite, Graeme Blackwood, and William Byrne.
2011. Lattice-based minimum error rate training using
weighted finite-state transducers with tropical polyno-
mial weights. Technical report, Department of Engi-
neering, University of Cambridge.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 524?532.
125
