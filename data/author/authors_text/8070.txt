139
140
141
142
An Evaluation Exercise for Romanian Word Sense Disambiguation
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX, USA
rada@cs.unt.edu
Vivi Na?stase
School of Computer Science
University of Ottawa
Ottawa, ON, Canada
vnastase@site.uottawa.ca
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA, USA
timc@isi.edu
Doina Ta?tar
Department of Computer Science
Babes?-Bolyai University
Cluj-Napoca, Romania
dtatar@ubb.ro
Dan Tufis?
Romanian Academy Center
for Artificial Intelligence
Bucharest, Romania
tufis@racai.ro
Florentina Hristea
Department of Computer Science
University of Bucharest
Bucharest, Romania
fhristea@mailbox.ro
Abstract
This paper presents the task definition, resources,
participating systems, and comparative results for a
Romanian Word Sense Disambiguation task, which
was organized as part of the SENSEVAL-3 evaluation
exercise. Five teams with a total of seven systems
were drawn to this task.
1 Introduction
SENSEVAL is an evaluation exercise of the lat-
est word-sense disambiguation (WSD) systems. It
serves as a forum that brings together researchers in
WSD and domains that use WSD for various tasks.
It allows researchers to discuss modifications that
improve the performance of their systems, and an-
alyze combinations that are optimal.
Since the first edition of the SENSEVAL competi-
tions, a number of languages were added to the orig-
inal set of tasks. Having the WSD task prepared for
several languages provides the opportunity to test
the generality of WSD systems, and to detect dif-
ferences with respect to word senses in various lan-
guages.
This year we have proposed a Romanian WSD
task. Five teams with a total of seven systems have
tackled this task. We present in this paper the data
used and how it was obtained, and the performance
of the participating systems.
2 Open Mind Word Expert
The sense annotated corpus required for this task
was built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002), adapted to Roma-
nian1.
To overcome the current lack of sense tagged
data and the limitations imposed by the creation of
such data using trained lexicographers, the Open
Mind Word Expert system enables the collection
of semantically annotated corpora over the Web.
Sense tagged examples are collected using a Web-
based application that allows contributors to anno-
tate words with their meanings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, who are asked to
select the most appropriate sense for the target word
in each sentence. The selection is made using check-
boxes, which list all possible senses of the current
target word, plus two additional choices, ?unclear?
and ?none of the above.? Although users are en-
couraged to select only one meaning per word, the
selection of two or more senses is also possible. The
results of the classification submitted by other users
are not presented to avoid artificial biases.
3 Sense inventory
For the Romanian WSD task, we have chosen a set
of words from three parts of speech - nouns, verbs
and adjectives. Table 1 presents the number of
words under each part of speech, and the average
number of senses for each class.
The senses were (manually) extracted from a Ro-
manian dictionary (Dict?ionarul EXplicativ al limbii
roma?ne - DEX (Coteanu et al, 1975)). These senses
1Romanian Open Mind Word Expert can be accessed at
http://teach-computers.org/word-expert/romanian
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Number Avg senses Avg senses
Class words (fine) (coarse)
Nouns 25 8.92 4.92
Verbs 9 8.7 4.6
Adjectives 5 9 4
Total 39 8.875 4.725
Table 1: Sense inventory
and their dictionary definitions were incorporated in
the Open Mind Word Expert. For each annotation
task, the contributors could choose from this list of
39 words. For each chosen word, the system dis-
plays the associated senses, together with their def-
initions, and a short (1-4 words) description of the
sense. After the user gets familiarized with these
senses, the system displays each example sentence,
and the list of senses together with their short de-
scription, to facilitate the tagging process.
For the coarse grained WSD task, we had the op-
tion of using the grouping provided by the dictio-
nary. A manual analysis however showed that some
of the senses in the same group are quite distinguish-
able, while others that were separated were very
similar.
For example, for the word circulatie (roughly, cir-
culation). The following two senses are grouped in
the dictionary:
2a. movement, travel along a communication
line/way
2b. movement of the sap in plants or the cytoplasm
inside cells
Sense 2a fits better with sense 1 of circulation:
1. the event of moving about
while sense 2b fits better with sense 3:
3. movement or flow of a liquid, gas, etc. within a
circuit or pipe.
To obtain a better grouping, a linguist clustered
the similar senses for each word in our list of forty.
The average number of senses for each class is al-
most halved.
Notice that Romanian is a language that uses dia-
critics, and the the presence of diacritics may be cru-
cial for distinguishing between words. For example
peste without diacritics may mean fish or over. In
choosing the list of words for the Romanian WSD
task, we have tried to avoid such situations. Al-
though some of the words in the list do have dia-
critics, omitting them does not introduce new ambi-
guities.
4 Corpus
Examples are extracted from the ROCO corpus, a
400 million words corpus consisting of a collection
of Romanian newspapers collected on the Web over
a three years period (1999-2002).
The corpus was tokenized and part-of-speech
tagged using RACAI?s tools (Tufis, 1999). The to-
kenizer recognizes and adequately segments various
constructs: clitics, dates, abbreviations, multiword
expressions, proper nouns, etc. The tagging fol-
lowed the tiered tagging approach with the hidden
layer of tagging being taken care of by Thorsten
Brants? TNT (Brants, 2000). The upper level of
the tiered tagger removed from the assigned tags all
the attributes irrelevant for this WSD exercise. The
estimated accuracy of the part-of-speech tagging is
around 98%.
5 Sense Tagged Data
While several sense annotation schemes have been
previously proposed, including single or dual anno-
tations, or the ?tag until two agree? scheme used dur-
ing SENSEVAL-2, we decided to use a new scheme
and collect four tags per item, which allowed us
to conduct and compare inter-annotator agreement
evaluations for two-, three-, and four-way agree-
ment. The agreement rates are listed in Table 3.
The two-way agreement is very high ? above 90%
? and these are the items that we used to build the
annotated data set. Not surprisingly, four-way agree-
ment is reached for a significantly smaller number of
cases. While these items with four-way agreement
were not explicitly used in the current evaluation,
we believe that this represents a ?platinum standard?
data set with no precedent in the WSD research com-
munity, which may turn useful for a range of future
experiments (for bootstrapping, in particular).
Agreement type Total (%)
TOTAL ITEMS 11,532 100%
At least two agree 10,890 94.43%
At least three agree 8,192 71.03%
At least four agree 4,812 41.72%
Table 3: Inter-agreement rates for two-, three-, and
four-way agreement
Table 2 lists the target words selected for this task,
together with their most common English transla-
tions. For each word, we also list the number of
senses, as defined in the DEX sense inventory (col-
locations included), and the number of annotated ex-
amples made available to task participants.
Word Main English senses senses Train Test Word Main English senses senses Train Test
translation (fine) (coarse) size size translation (fine) (coarse) size size
NOUNS
ac needle 16 7 127 65 accent accent 5 3 172 87
actiune action 10 7 261 128 canal channel 6 5 134 66
circuit circuit 7 5 200 101 circulatie circulation 9 3 221 114
coroana crown 15 11 252 126 delfin doplhin 5 4 31 15
demonstratie demonstration 6 3 229 115 eruptie eruption 2 2 54 27
geniu genius 5 3 106 54 nucleu nucleus 7 5 64 33
opozitie opposition 12 7 266 134 perie brush 5 3 46 24
pictura painting 5 2 221 111 platforma platform 11 8 226 116
port port 7 3 219 108 problema problem 6 4 262 131
proces process 11 3 166 82 reactie reaction 7 6 261 131
stil style 14 4 199 101 timbru stamp 7 3 231 116
tip type 7 4 263 131 val wave 15 9 242 121
valoare value 23 9 251 125
VERBS
cistiga win 5 4 227 115 citi read 10 4 259 130
cobori descend 11 6 252 128 conduce drive 7 6 265 134
creste grow 14 6 209 103 desena draw 3 3 54 27
desface untie 11 5 115 58 fierbe boil 11 4 83 43
indulci sweeten 7 4 19 10
ADJECTIVES
incet slow 6 3 224 113 natural natural 12 5 242 123
neted smooth 7 3 34 17 oficial official 5 3 185 96
simplu simple 15 6 153 82
Table 2: Target words in the SENSEVAL-3 Romanian Lexical Sample task
Team System name Reference (this volume)
Babes-Bolyai University, Cluj-Napoca (1) ubb nbc ro (Csomai, 2004)
Babes-Bolyai University, Cluj-Napoca (2) UBB (Serban and Tatar, 2004)
Swarthmore College swat-romanian (Wicentowski et al, 2004a)
Swarthmore College / Hong Kong Polytechnic University swat-hk-romanian (Wicentowski et al, 2004b)
Hong Kong University of Science and Technology romanian-swat hk-bo
University of Maryland, College Park UMD SST6 (Cabezas et al, 2004)
University of Minnesota, Duluth Duluth-RomLex (Pedersen, 2004)
Table 4: Teams participating in the SENSEVAL-3 Romanian Word Sense Disambiguation task
In addition to sense annotated examples, partici-
pants have been also provided with a large number
of unlabeled examples. However, among all partici-
pating systems, only one system ? described in (Ser-
ban and Ta?tar 2004) ? attempted to integrate this ad-
ditional unlabeled data set into the learning process.
6 Participating Systems
Five teams participated in this word sense disam-
biguation task. Table 4 lists the names of the par-
ticipating systems, the corresponding institutions,
and references to papers in this volume that provide
detailed descriptions of the systems and additional
analysis of their results.
There were no restrictions placed on the number
of submissions each team could make. A total num-
ber of seven submissions was received for this task.
Table 5 shows all the submissions for each team, and
gives a brief description of their approaches.
7 Results and Discussion
Table 6 lists the results obtained by all participating
systems, and the baseline obtained using the ?most
frequent sense? (MFS) heuristic. The table lists pre-
cision and recall figures for both fine grained and
coarse grained scoring.
The performance of all systems is significantly
higher than the baseline, with the best system per-
forming at 72.7% (77.1%) for fine grained (coarse
grained) scoring, which represents a 35% (38%) er-
ror reduction with respect to the baseline.
The best system (romanian-swat hk-bo) relies on
a Maximum Entropy classifier with boosting, using
local context (neighboring words, lemmas, and their
part of speech), as well as bag-of-words features for
surrounding words.
Not surprisingly, several of the top perform-
ing systems are based on combinations of multi-
ple sclassifiers, which shows once again that voting
System Description
romanian-swat hk-bo Supervised learning using Maximum Entropy with boosting, using bag-of-words
and n-grams around the head word as features
swat-hk-romanian The swat-romanian and romanian-swat hk-bo systems combined with majority voting.
Duluth-RLSS An ensemble approach that takes a vote among three bagged decision trees,
based on unigrams, bigrams and co-occurrence features
swat-romanian Three classifiers: cosine similarity clustering, decision list, and Naive Bayes,
using bag-of-words and n-grams around the head word as features
combined with a majority voting scheme.
UMD SST6 Supervised learning using Support Vector Machines, using contextual features.
ubb nbc ro Supervised learning using a Naive Bayes learning scheme, and features extracted
using a bag-of-words approach.
UBB A k-NN memory-based learning approach, with bag-of-words features.
Table 5: Short description of the systems participating in the SENSEVAL-3 Romanian Word Sense Disam-
biguation task. All systems are supervised.
Fine grained Coarse grained
System P R P R
romanian-swat hk-bo 72.7% 72.7% 77.1% 77.1%
swat-hk-romanian 72.4% 72.4% 76.1% 76.1%
Duluth-RLSS 71.4% 71.4% 75.2% 75.2%
swat-romanian 71.0% 71.0% 74.9% 74.9%
UMD SST6 70.7% 70.7% 74.6% 74.6%
ubb nbc ro 71.0% 68.2% 75.0% 72.0%
UBB 67.1% 67.1% 72.2% 72.2%
Baseline (MFS) 58.4% 58.4% 62.9% 62.9%
Table 6: System results on the Romanian Word Sense Disambiguation task.
schemes that combine several learning algorithms
outperform the accuracy of individual classifiers.
8 Conclusion
A Romanian Word Sense Disambiguation task
was organized as part of the SENSEVAL-3 eval-
uation exercise. In this paper, we presented
the task definition, and resources involved, and
shortly described the participating systems. The
task drew the participation of five teams, and in-
cluded seven different systems. The sense an-
notated data used in this exercise is available
online from http://www.senseval.org and
http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the
Romanian Open Mind Word Expert project, mak-
ing this task possible. Special thanks to Bog-
dan Harhata, from the Institute of Linguistics Cluj-
Napoca, for building a coarse grained sense map.
We are also grateful to all the participants in this
task, for their hard work and involvement in this
evaluation exercise. Without them, all these com-
parative analyses would not be possible.
References
T. Brants. 2000. Tnt - a statistical part-of-speech
tagger. In Proceedings of the 6th Applied NLP
Conference, ANLP-2000, Seattle, WA, May.
T. Chklovski and R. Mihalcea. 2002. Building a
sense tagged corpus with Open Mind Word Ex-
pert. In Proceedings of the Workshop on ?Word
Sense Disambiguation: Recent Successes and Fu-
ture Directions?, ACL 2002, Philadelphia, July.
I. Coteanu, L. Seche, M. Seche, A. Burnei,
E. Ciobanu, E. Contras?, Z. Cret?a, V. Hristea,
L. Mares?, E. St??ngaciu, Z. S?tefa?nescu, T. T?ugulea,
I. Vulpescu, and T. Hristea. 1975. Dict?ionarul
Explicativ al Limbii Roma?ne. Editura Academiei
Republicii Socialiste Roma?nia.
D. Tufis. 1999. Tiered tagging and combined classi-
fiers. In Text, Speech and Dialogue, Lecture Notes
in Artificial Intelligence.
UBB system at Senseval3
Gabriela Serban
Department of Computer Science
University ?Babes-Bolyai?
Romania
gabis@cs.ubbcluj.ro
Doina Tatar
Department of Computer Science
University ?Babes-Bolyai?
Romania
dtatar@cs.ubbcluj.ro
Abstract
It is known that whenever a system?s actions
depend on the meaning of the text being pro-
cessed, disambiguation is beneficial or even nec-
essary. The contest Senseval is an international
frame where the research in this important field
is validated in an hierarchical manner. In this
paper we present our system participating for
the first time at Senseval 3 contest on WSD,
contest developed in March-April 2004. We
present also our intentions on improving our
system, intentions occurred from the study of
results.
1 Introduction
Word Sense Disambiguation (WSD) is the pro-
cess of identifying the correct meanings of words
in particular contexts (Manning and Schutze,
1999). It is only an intermediate task in NLP,
like POS tagging or parsing. Examples of final
tasks are Machine Translation, Information Ex-
traction or Dialogue systems. WSD has been a
research area in NLP for almost the beginning
of this field due to the phenomenon of polysemy
that means multiple related meanings with a
single word (Widdows, 2003). The most im-
portant robust methods in WSD are: machine
learning methods and dictionary based meth-
ods. While for English exist some machine read-
able dictionaries, the most known being Word-
Net (Christiane Fellbaum, 1998), for Romanian
until now does not exist any. Therefore for our
application we used the machine learning ap-
proach.
2 Machine learning approach in
WSD
Our system falls in the supervised learning ap-
proach category. It was trained to learn a clas-
sifier that can be used to assign a yet unseen ex-
ample to one or two of a fixed number of senses.
We had a trained corpus (a number of annotated
contexts), from where the system learned the
classifier, and a test corpus which the system
will annotate.
In our system we used the Vector Space
Model: a context c was represented as a vec-
tor ~c of some features which we will present bel-
low. By a context we mean the same definition
as in Senseval denotation: the content between
?context? and ?/context?.
The notations used to explain our method are
(Manning and Schutze, 1999):
? w - the word to be disambiguate;
? s1, ? ? ? , sNs the senses for w;
? c1, ? ? ? , cNc the contexts for w;
? v1, ? ? ? , vNf the features selected.
As we treated each word w to be disam-
biguated separately, let us explain the method
for a single word. The features selected was
the set of ALL words used in the trained corpus
(nouns, verbs, prepositions, etc) , so we used the
cooccurrence paradigm (Dagan, Lee and Pereira
, 1994).
The vector of a context c of the target word
w is defined as:
? ~c = (w1, ? ? ? , w|W |) where wi is the number
of occurences of the word vi in the context
c and vi is a word from the entire trained
corpus of | W | words.
The similarity between two contexts ca, cb is
the normalised cosine between the vectors ~ca
and ~cb (Jurafsky and Martin, 2000):
cos(~ca, ~cb) =
?m
j=1 wa,j ? wb,j??m
j=1 w2a,j ?
?m
j=1 w2b,j
and sim(~ca, ~cb) = cos(~ca, ~cb).
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
The number wi is the weight of the feature
vi. This can be the frequency of the feature vi
(term frequency or tf), or ?inverse document
frequency ?, denoted by idf . In our system we
considered all the words from the entire corpus,
so both these aspects are satisfied.
3 k-NN or memory based learning
At training time, our k-NN model memorizes all
the contexts in the training set by their associ-
ated features. Later, when proceeds a new con-
text cnew, the classifier first selects k contexts
in the training set that are closest to cnew, then
pick the best sense (senses) for cnew (Jackson
and Moulinier, 2002).
? TRAINING: Calculate ~c for each context c.
? TEST: Calculate
Step1.
A = {~c | sim( ~cnew,~c) ismaxim, | A |= k}
that means A is the set of the k nearest
neighbors contexts of ~cnew.
Step2.
Score(cnew, sj) =
?
~ci?A
(sim( ~cnew, ~ci)? aij)
where aij is 1 if ~ci has the sense sj and aij
is 0 otherwise.
Step3. Finally,
s? = argmaxjScore(cnew, sj).
We used the value of k set to 3 after some
experimental verifications.
A major problem with supervised approaches
is the need for a large sense tagged training set.
The bootstrapping methods use a small number
of contexts labeled with senses having a high
degree of confidence.
These labeled contexts are used as seeds to
train an initial classifier. This is then used to
extract a larger training set from the remain-
ing untagged contexts. Repeating this process,
the number of training contexts grows and the
number of untagged contexts reduces. We will
stop when the remaining unannotated corpus is
empty or any new context can?t be annotated.
In (Tatar and Serban, 2001), (Serban and Tatar,
2003) we presented an algorithm which falls in
this category. The algorithm is based on the two
principles of Yarowsky (Resnik and Yarowsky,
1999):
? One sense per discourse: the sense of a tar-
get word is highly consistent within a given
discourse (document);
? One sense per collocation: the contextual
features ( nearby words) provide strong
clues to the sense of a target word.
Also, for each iteration, the algorithm uses
a NBC classifier. We intend to present a sec-
ond system based on this algorithm at the next
Senseval contest.
4 Implementation details
Our disambiguation system is written in JDK
1.4.
In order to improve the performance of the
disambiguation algorithm, we made the follow-
ing refinements in the above k-NN algorithm.
First one is to substitute the lack of an efficient
tool for stemming words in Romanian.
1. We defined a relation between words as ? :
W ? W , where W is the set of words. If
w1 ? W and w2 ? W are two words, we
say that (w1, w2) ? ? if w1 and w2 have
the same grammatical root. Therefore, if
w is a word and C is a context, we say that
w occurs in C iff exists a word w2 ? C
so that (w,w2) ? ?. In other words, we
replaced the stemming step with collecting
all the words with the same root in a single
class. This collection is made considering
the rules for romanian morphology;
2. The step 3 of the algorithm for choosing
the appropriate sense (senses) of a poly-
semic word w in a given context C (in
fact the sense that maximizes the set S =
{Score(C, sj) | j = 1, ? ? ?Ns} of scores for
C) is divided in three sub-steps:
? If there is a single sense s that maxi-
mizes S, then s is reported as the ap-
propriate sense for C;
? If there are two senses s1 and s2 that
maximize S, then s1 and s2 are re-
ported as the appropriate senses for C;
? Consider that Max1 and Max2 are
the first two maximum values from S
where (Max1 > Max2). If Max1 is
obtained for a sense s1 and if Max2 is
obtained for a sense s2 and if
Max1?Max2 ? P
where P = Max1?Min(Ns?1) and Min is the
minimum score from S, then s1 and s2
are reported as the appropriate senses
for C.
Experimentally, we proved that the above im-
provements grow the precision of the disam-
biguation process.
5 Conclusions after the evaluation
Coarse-grained score for our system UBB using
key ?EVAL/RomanianLS.test.key? was:
precision: 0.722 (2555.00 correct of 3541.00
attempted)
recall: 0.722 (2555.00 correct of 3541.00 in
total)
attempted: 100.00
Fine-grained score was:
precision: 0.671 (2376.50 correct of 3541.00
attempted)
recall: 0.671 (2376.50 correct of 3541.00 in
total)
attempted: 100.00
Considering as baseline procedure the major-
ity sense (all contexts are solved with the most
frequent sense in the training corpus), for the
word nucleu (noun) is obtained a precision of
0,78 while our procedure obtained 0,81. Also,
for the word desena (verb) the baseline proce-
dure of the majority sense obtains precision 0,81
while our procedure obtained 0,85.
At this stage our system has not as a goal to
label with U (unknown) a context, every time
choosing one or two from the best scored senses.
Annotating with the label U is one of our com-
ing improving. This can be done simply by
adding as a new sense for each word the sense
U . A simple experiment reported a number of
right annotated contexts.
Another direction to improve our system is
to exploit better the senses as they are done in
training corpus: our system simply consider the
first sense.
References
I. Dagan, L. Lee and F. C. N. Pereira. 1994.
Similarity-based Estimation of Word Cooc-
curences Probabilities. Meeting of the Asso-
ciation for Computational Linguistics, 272?
278.
Christiane Fellbaum. 1998. WordNet: An elec-
tronic lexical database. The MIT Press.
P. Jackson and I. Moulinier. 2002. Natural
Language Processing for Online Applications.
John Benjamin Publ. Company.
D. Jurafsky and J. Martin. 2000. Speech and
language processing. Prentice-Hall, NJ.
C. Manning and H. Schutze. 1999. Foundation
of statistical natural language processing. The
MIT Press.
Ruslan Mitkov,editor. 2002 The Oxford Hand-
book of Computational Linguistics. Oxford
University Press.
P. Resnik and D. Yarowsky. 1999. Distinguish-
ing Systems and Distinguishing sense: new
evaluation methods for WSD. Natural Lan-
guage Engineering, 5(2):113-134.
G. Serban and D. Tatar. 2003. Word Sense Dis-
ambiguation for Untagged Corpus: Applica-
tion to Romanian Language. CICLing-2003,
LNCS 2588, 270?275.
D. Tatar and G. Serban. 2001. A new algorithm
for WSD. Studia Univ. ?Babes-Bolyai?, In-
formatica, 2 99?108.
D. Widdows. 2003. A mathematical model for
context and word meaning. Fourth Interna-
tional Conference on Modeling and using con-
text, Stanford, California, June 23-25.
Top-Down Cohesion
Segmentation in
Summarization
Doina Tatar
Andreea Diana Mihis
Gabriela Serban
University "Babes?-Bolyai" Cluj-Napoca (Romania)
email: dtatar@cs.ubbcluj.ro
Abstract
The paper proposes a new method of linear text segmentation based on
lexical cohesion of a text. Namely, first a single chain of disambiguated
words in a text is established, then the rips of this single chain are consid-
ered as boundaries for the segments of the cohesion text structure (Cohe-
sion TextTiling or CTT). The summaries of arbitrarily length are obtained
by extraction using three different methods applied to the obtained seg-
ments. The informativeness of the obtained summaries is compared with
the informativeness of the pair summaries of the same length obtained us-
ing an earlier method of logical segmentation by text entailment (Logical
TextTiling or LTT). Some experiments about CTT and LTT methods are
carried out for four ?classical" texts in summarization literature showing
that the quality of the summarization using cohesion segmentation (CTT)
is better than the quality using logical segmentation (LTT).
389
390 Tatar, Mihis, and Serban
1 Introduction
Text summarization has become the subject of an intense research in the last years
and it is still an emerging field (Orasan, 2006; Radev et al, 2002; Hovy, 2003; Mani,
2001). The research is done in the extracts (which we are treating in this paper) and
abstracts areas. The most important task of summarization is to identify the most
informative (salient) parts of a text comparatively with the rest. A good segmentation
of a text could help in this identification (Boguraev and Neff, 2000; Barzilay and
Elhadad, 1999; Reynar, 1998).
This paper proposes a new method of linear text segmentation based on lexical
cohesion of a text. Namely, first a single chain of disambiguated words in a text is
established, then the rips of this chain are considered. These rips are boundaries of the
segments in the cohesion structure of the text. Due to some similarities with TextTiling
algorithm for topic shifts detection of Hearst (1997), the method is called Cohesion
TextTiling (CTT).
The paper is structured as follows: in Section 2 we present the problem of Word
Sense Disambiguation by a chain algorithm and the derived CTT method. In Sec-
tion 3, some notions about textual entailment and logical segmentation of a text by
LTT method are discussed. Summarization by different methods after segmentation
is the topic of Section 4. The parallel application of CTT and LTT methods to four
"classical" texts in summarization literature, two narrative and two newspapers, and
some statistics of the results are presented in Section 5. We finish the article with
conclusions and possible further work directions.
2 A top-down cohesion segmentation method
2.1 Lexical chains
A lexical chain is a sequence of words such that the meaning of each word from the se-
quence can be obtained unambiguously from the meaning of the rest of words (Morris
and Hirst, 1991; Barzilay and Elhadad, 1999; Harabagiu and Moldovan, 1997; Silber
and McCoy, 2002; Stokes, 2004). The map of all lexical chains of a text provides a
representation of the lexical cohesive structure of the text. Usually a lexical chain is
obtained in a bottom-up fashion, by taking each candidate word of a text, and finding
an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or
WordNet (Barzilay and Elhadad, 1999). If it is found, the word is inserted with the
appropriate sense in the current chain, and the senses of the other words in the chain
are updated. If no relation is found, then a new chain is initiated.
Our method approaches the construction of lexical chains in a reverse order: we
first disambiguate the whole text and then construct the lexical chains which cover as
much as possible the text.
2.2 CHAD algorithm
It is known that in the last years many researchers studied the possibility to globally
disambiguate a text. In Tatar et al (2007) is presented CHAD algorithm, a Lesk?s
type algorithm based on WordNet, that doesn?t require syntactic analysis and syntac-
tic parsing. As usually for a Lesk?s type algorithm, it starts from the idea that a word?s
dictionary definition is a good indicator for the senses of this word and uses the defi-
Top-down Cohesion Segmentation in Summarization 391
nition in the dictionary directly. The base of the algorithm is the disambiguation of a
triplet of words, using Dice?s overlap or Jaccard?s measures. Shortly, CHAD begins
with the disambiguation of a triplet w1w2w3 and then adds to the right the following
words to be disambiguated. Hence it disambiguates at a time a new triplet, where first
two words are already associated with the best senses and the disambiguation of the
third word depends on the disambiguation of these first two words.
Due to the brevity of definitions in WordNet (WN), the first sense in WN for a word
wi (WN 1st sense) must be associated in some cases in a "forced" way. The forced
condition represents the situation that any sense of wi is related with the senses of the
words wi?2 and wi?1. Thus the forced condition signals that a lexical chain stops,
and, perhaps, a new one begins.
Comparing the precision obtained with CHAD and the precision obtained by the
WN 1st sense algorithm for 10 files of Brown corpus (Tatar et al, 2007) we obtained
the result: for 7 files the difference was greater or equal to 0.04 (favorable to WN 1st),
and for 3 files was lower. For example, in the worst case (Brown 01 file), the precisions
obtained by CHAD are: 0.625 for Dice?s measure, 0.627 for Overlap measure, 0.638
for Jaccard?s measure while the precision obtained by WN 1st sense is 0.688. Let us
remark that CHAD is used to mark the discontinuity in cohesion, while WN 1st sense
algorithm is unable to do this.
2.3 CHAD and lexical chains
The CHAD algorithm shows what words in a sentence are unrelated as senses with
the previously words: these are the words which receive a "forced" first WN sense.
Of course, these are regarded differently from the words which receive a "justified"
first WN sense. Scoring each sentence of a text by the number of "forced" to first WN
sense words in this sentence, we will provide a representation of the lexical cohesive
structure of the text. If F is this number, then the valleys (the local minima) in the
graph representing the function 1/F will represent the boundaries between lexical
chains (see Figure 2).
Lexical chains could serve further as a basis for an algorithm of segmentation. As
our method of determination of lexical chains is linear, the corresponding segmenta-
tion is also linear. The obtained segments could be used effectively in summarization.
In this respect, our method of summarization falls in the discourse-based category.
In contrast with other theories about discourse segmentation, as Rhetorical Struc-
ture Theory (RST) of Mann and Thompson (1988), attentional/intentional structure
of Grosz and Sidner (1986) or parsed RST tree of Marcu (1997), our CTT method
(and also, as presented below, our LTT method) supposes a linear segmentation (ver-
sus hierarchical segmentation) which results in an advantage from a computational
viewpoint.
3 Segmentation by Logical TextTiling
3.1 Text entailment
Text entailment is an autonomous field of Natural Language Processing and it rep-
resents the subject of some recent Pascal Challenges. As is established in an earlier
paper (Tatar et al, 2007), a text T entails an hypothesis H, denoted by T ? H, iff H
is less informative than T . A method to prove T ? H which relies on this definition
392 Tatar, Mihis, and Serban
consists in the verification of the relation: sim(T,H)T ? sim(T,H)H . Here sim(T,H)T
and sim(T,H)H are text-to-text similarities introduced in Corley and Mihalcea (2005).
The method used by our tool for Text entailment verification calculates the similarity
between T and H by cosine, thus the above relation becomes cos(T,H)T ? cos(T,H)H
(Tatar et al, 2007).
3.2 Logical segmentation
Tatar et al (2008) present a method named logical segmentation because the score of
a sentence is the number of sentences of the text which are entailed by it. Representing
the scores of sentences as a graph, we obtain a structure which indicates how the most
important sentences alternate with ones less important and which organizes the text
according to its logical content. Simply, a valley (a local minimum) in the obtained
logical structure of the text is a boundary between two logical segments (see Figure 1).
The method is called Logical TextTiling (LTT), due to some similarities with the
TextTiling algorithm for topic shifts detection (Hearst, 1997). The drawback of LTT,
that the number of the segments is fixed for a given text (as it results from its logical
structure), is eliminated by a method to dynamically correlate the number of the logi-
cal segments obtained by LTT with the required length of the summary. Let us remark
that LTT does not require a predicate-argument analysis. The only semantic structure
processing required is the Text Entailment verification.
4 Summarization by segmentation
4.1 Scoring the segments
An algorithm of segmentation has usually the following function:
INPUT: a list of sentences S1, ...,Sn and a list of scores score(S1), ...,score(Sn);
OUTPUT: a list of segments Seg1, ...,SegN .
Given a set of N segments (obtained by CTT or LTT) we need a criterion to select
those sentences from a segment which will be introduced in the summary. Thus,
after the score of a sentence is calculated, we calculate a score of a segment. The final
score, Score f inal, of a sentence is weighted by the score of the segment which contains
it. The summary is generated by selecting from each segment a number of sentences
proportional with the score of the segment. The method has some advantages when a
desired level of granularity of summarization is imposed.
The summarization algorithm with Arbitrarily Length of the summary (AL) is the
following:
INPUT: The segments Seg1, ...SegN, the length of summary X (as parameter),
Score f inal(Si) for each sentence Si;
OUTPUT: A summary SUM of length X, where from each segment Seg j are selected
NSenSeg j sentences. The method of selecting the sentences is given by defini-
tions Sum1,Sum2,Sum3 (Section 4.2).
Remark: A number of segments Seg j may have NSenSeg j > 1. If X < N then a
number of segments Seg j must have NSenSeg j = 0
Top-down Cohesion Segmentation in Summarization 393
In Section 5 (Experiments) the variant of summarization algorithm as above is de-
noted as Var1. In the variant Var2 a second choice of computing the score of a
segment is considered. Namely, the score is not normalized, and it is equal with the
sum of its sentences scores, without been divided to the segment length. The draw-
back of Var1 is that in some cases a very long segment can contain some sentences
with a high score and many sentences with a very low score, the final score of this
segment will be a small one and those important sentences will not be included in the
final summary. The drawback of Var2 is that of increased importance of the length
of the segment in some cases. Thus, the score of a short segment with high sentences
scores will be less then one of a long segment with small sentences scores, and again
some important sentences will be lost.
4.2 Strategies for summary calculus
The method of extracting sentences from the segments is decisive for the quality of
the summary. The deletion of an arbitrary amount of source material between two
sentences which are adjacent in the summary has the potential of losing essential in-
formation. We propose and compare some simple strategies for including sentences
in the summary:
? Our first strategy is to include in the summary the first sentence from each seg-
ment, as this is of special importance for a segment. The corresponding sum-
mary will be denoted by Sum1.
? The second way is that for each segment the sentence(s) with a maximal score
are considered the most important for this segment, and hence they are included
in the summary. The corresponding summary is denoted by Sum2.
? The third way of reasoning is that from each segment the most informative
sentence(s) (the least similar) relative to the previously selected sentences are
picked up. The corresponding summary is denoted by Sum3.
5 Experiments
In our experiments for CTT method each sentence is scored as following: Score(Si) =
1
nuwi where nuwi is the number of words "forced" to get the first WN sense in the
sentence Si. If nuwi = 0 then Score(Si) = 2. The graph of the logical structure for the
text Hirst is presented in Figure 1 while the graph for the cohesion structure for the
same text is presented in Figure 2.
We have applied CTT and LTT methods of segmentation and summarization to
four texts denoted in the following by: Hirst (Morris and Hirst, 1991), Koan (Richie,
1991), Tucker1 (Tucker, 1999) and Tucker2 (Tucker, 1999).1 The denotations are as
following: LSi for LTT with Sumi method, CSi for CTT with Sumi method. Also an
ideal summary (IdS) has been constructed by taking the majority occurrences of the
sentences in all LSi and CSi summaries. IdS is the last raw of the table. For the text
Tucker1 the summaries with five sentences obtained by us and the summary obtained
by the author with CLASP (Tucker, 1999) are presented in Table 1.
1All these texts are shown on-line at http://www.cs.ubbcluj.ro/~dtatar/nlp/ (first entries).
394 Tatar, Mihis, and Serban
Logical structure of Hirst text
0
11
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41
Positions of sentences
Figure 1: The logical structure of the text Hirst
Cohesion structure of the Hirst text
0
2.25
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41
Positions of sentences
Figure 2: The cohesion structure of the text Hirst
Table 1: The summaries with the length 5 for the text Tucker1 compared with the
author?s summary
Method 5 sent Tucker1
LS1 1,16,17,34,35 6,8,16,23,34
LS2 1,16,17,34,35
LS3 1,32,34,43,44
CS1 1,23,31,34,40
CS2 8,23,31,35,43
CS3 1,9,27,34,39
IdS 1,16,17,34,35
Top-down Cohesion Segmentation in Summarization 395
Table 2: The average informativeness of CTT and LTT summaries for all texts
Method Var1 Var2 Var1 + Var2
LS1 0.606538745 0.603946109 0.605242427
LS2 0.580429322 0.577647764 0.579038543
LS3 0.594914426 0.600104854 0.59750964
CS1 0.607369111 0.603053171 0.605211141
CS2 0.592993154 0.589675201 0.591334178
CS3 0.631625044 0.594702506 0.613163775
average 0.602311633 0.594854934 0.598583284
LT Taverage 0.593960831 0.593899576 0.593930203
CT Taverage 0.6106624 0.5958102 0.6032363
5.1 Evaluation of the summarization
There is no an unique formal method to evaluate the quality of a summary. In this
paper we use as a measure of the quality of a summary, the similarity (calculated
as cosine) between the summarized (initial) text and the summaries obtained with
different methods. We call this similarity "the informativeness".
The informativeness of the different types of summaries Sum1,Sum2,Sum3 (see
Section 4.2) and of different lengths (5, 6 and 10) is calculated for each text. Then, the
average informativeness for all four texts is calculated. A view with the these average
results of informativeness, calculated with different methods, in variants Var1 and
Var2, is given in the Table 2.
Let us remark that for obtaining summaries with different lengths, after a first seg-
mentation with CTT and LTT methods the algorithm AL from Section 4.1 is applied.
Table 2 displays the results announced in the abstract: the quality of CTT sum-
maries is better than the quality of the LTT summaries from the point of view of
informativeness.
5.2 Implementation details
The methods presented in this paper are fully implemented: we used our own systems
of Text Entailment verification, Word Sense Disambiguation, top-down lexical chains
determination, LTT and CTT segmentation, summarization with Sumi and AL meth-
ods. The programs are realized in Java and C++. WordNet (Miller, 1995) is used by
our system of Word Sense Disambiguation.
6 Conclusion and further work
This paper shows that the text segmentation by lexical chains and by text entailment
relation between sentences are good bases for obtaining highly accurate summaries.
Moreover, our method replaces the usually bottom-up lexical chain construction with
a top-down one, where first a single chain of disambiguated words is established and
then it is divided in a sequence of many shorter lexical chains. The segmentation of
text follows the sequence of lexical chains. Our methods of summarization control the
length of the summaries by a process of scoring the segments. Thus, more material is
extracted from the strongest segments.
396 Tatar, Mihis, and Serban
The evaluation indicates acceptable performance when informativeness of sum-
maries is considered. However, our methods have the potential to be improved: in
CTT method we correspond a segment to a lexical chain. We intend to improve our
scoring method of a segment by considering some recent method of scoring lexical
chains (Ercan and Cicekli, 2008). Also, we intend to study how anaphora resolution
could improve the lexical chains and the segmentation. We further intend to apply
the presented methods to the corpus of texts DUC2002 and to evaluate them with the
standard ROUGE method (for our experiments we didn?t have the necessary human
made summaries).
Acknowledgments
This work has been supported by PN2 Grant TD 400/2007.
References
Barzilay, R. and M. Elhadad (1999). Using lexical chains for Text summarization. In
J. Mani and M. Maybury (Eds.), Advances in Automated Text Summarization. MIT
Press.
Boguraev, B. and M. Neff (2000). Lexical Cohesion, Discourse Segmentation and
Document Summarization. In Proceedings of the 33rd Hawaii International Con-
ference on System Sciences.
Corley, C. and R. Mihalcea (2005). Measuring the semantic similarity of texts. In
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence
and Entailment, Ann Arbor.
Ercan, G. and I. Cicekli (2008). Lexical cohesion based topic modeling for summa-
rization. In Proceedings of the Cicling 2008, pp. 582?592.
Harabagiu, S. and D. Moldovan (1997). TextNet ? a textbased intelligent system.
Natural Language Engineering 3(2), 171?190.
Hearst, M. (1997). Texttiling: Segmenting text into multi-paragraph subtopic pas-
sages. Computational Linguistics 23(1), 33?64.
Hovy, E. (2003). Text summarization. In R. Mitkov (Ed.), The Oxford Handbook of
Computational Linguistics. Oxford University Press.
Mani, I. (2001). Automatic summarization. John Benjamins.
Marcu, D. (1997). From discourse structure to text summaries. In Proceedings of the
ACL/EACL ?97 Workshop on Intelligent Scalable Text Summarization, pp. 82?88.
Miller, G. (1995). WordNet: a lexical database for english. Comm. of the
ACM 38(11), 39?41.
Morris, J. and G. Hirst (1991). Lexical cohesion computed by thesaural relations as
an indicator of the structure of text. Computational Linguistics 17(1), 21?48.
Top-down Cohesion Segmentation in Summarization 397
Orasan, C. (2006). Comparative evaluation of modular automatic summarization sys-
tems using CAST. Ph. D. thesis, University of Wolverhampton.
Radev, D., E. Hovy, and K. McKeown (2002). Introduction to the special issues on
summarization. Computational Linguistics 28, 399?408.
Reynar, J. (1998). Topic Segmentation: algorithms and applications. Ph. D. thesis,
Univ. of Penn.
Richie, D. (1991). The koan. In Z. Inklings (Ed.), Some Stories, Fables, Parables and
Sermons, pp. 25?27.
Silber, H. and K. McCoy (2002). Efficiently computed lexical chains, as an inter-
mediate representation for automatic text summarization. Computational Linguis-
tics 28(4), 487?496.
Stokes, N. (2004). Applications of Lexical Cohesion Analysis in the Topic Detection
and Tracking Domain. Ph. D. thesis, National University of Ireland, Dublin.
Tatar, D., A. M. G. Serban, and R. Mihalcea (2007). Text entailment as directional
relation. In Proceedings of CALP07 Workshop at RANLP2007, Borovets, Bulgaria,
pp. 53?58.
Tatar, D., A. Mihis, and D. Lupsa (2008). Text entailment for logical segmentation
and summarization. In 13th International Conference on Applications of Natural
Language to Information Systems, pp. 233?244.
Tatar, D., G. Serban, A. Mihis, M. Lupea, D. Lupsa, and M. Frentiu (2007). A chain
dictionary method for wsd and applications. In Proceedings of the International
Conference on Knowledge Engineering,Principles and Techniques, pp. 41?49.
Tucker, R. (1999). Automatic summarising and the CLASP system. Ph. D. thesis,
University of Cambridge.
