First Joint Conference on Lexical and Computational Semantics (*SEM), pages 449?453,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Soft Cardinality: A Parameterized Similarity Function for Text Comparison
Sergio Jimenez
Universidad Nacional
de Colombia, Bogota,
Ciudad Universitaria
edificio 453, oficina 220
sgjimenezv@unal.edu.co
Claudia Becerra
Universidad Nacional
de Colombia, Bogota
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz,
Av. Mendiz?bal, Col.
Nueva Industrial Vallejo,
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
We present an approach for the construction of text
similarity functions using a parameterized resem-
blance coefficient in combination with a softened
cardinality function called soft cardinality. Our ap-
proach provides a consistent and recursive model,
varying levels of granularity from sentences to char-
acters. Therefore, our model was used to compare
sentences divided into words, and in turn, words di-
vided into q-grams of characters. Experimentally,
we observed that a performance correlation func-
tion in a space defined by all parameters was rel-
atively smooth and had a single maximum achiev-
able by ?hill climbing.? Our approach used only sur-
face text information, a stop-word remover, and a
stemmer to tackle the semantic text similarity task
6 at SEMEVAL 2012. The proposed method ranked
3rd (average), 5th (normalized correlation), and 15th
(aggregated correlation) among 89 systems submit-
ted by 31 teams.
1 Introduction
Similarity is the intrinsic ability of humans and some
animals to balance commonalities and differences when
comparing objects that are not identical. Although there
is no direct evidence of how this process works in liv-
ing organisms, some models have been proposed from
the cognitive perspective (Sj?berg, 1972; Tversky, 1977;
Navarro and Lee, 2004). On the other hand, several simi-
larity models have been proposed in mathematics, statis-
tics, and computer science among other fields. Particu-
larly in AI, similarity measures play an important role in
the construction of intelligent systems that are required
to exhibit behavior similar to humans. For instance, in
the field of natural language processing, text similarity
functions provide estimates of the human similarity judg-
ments related to language. In this paper, we combine el-
ements from the perspective of cognitive psychology and
computer science to propose a model for building simi-
larity functions suitable for the task of semantic text sim-
ilarity.
We identify four main families of text similarity func-
tions: i) resemblance coefficients based on sets (e.g. Jac-
card?s (1901) and Dice?s (1945) coefficients) ii) functions
in metric spaces (e.g. cosine tf-idf similarity (Salton et
al., 1975)); iii) the edit distance family of measures (e.g.
Levenstein (1966) distance, LCS (Hirschberg, 1977));
and iv) hybrid approaches ((Monge and Elkan, 1996; Co-
hen et al, 2003; Corley and Mihalcea, 2005; Jimenez et
al., 2010)). All of these measures use a subdivision of
the texts in different granularity levels, such as q-grams
of words, words, q-grams of characters, syllables, and
characters. Among hybrid approaches, Monge-Elkan?s
measure and soft cardinality methods are recursive and
can be used to build similarity functions at any arbitrary
range of granularity. For instance, it is possible to con-
struct a similarity function to compare sentences based
on a function that compares words, which in turn can be
constructed based on a function that compares bigrams of
characters. Furthermore, hybrid approaches can integrate
similarity functions that are not based on the representa-
tion of the surface of text, such as semantic relatedness
measures (Pedersen et al, 2004).
Text similarity measures can be static or adaptive
whether they are binary functions using only surface in-
formation of the two texts, or are functions that suit
to a wider set of texts. For instance, measures using
tf-idf weights adapt their results to the set of texts in
which those weights were obtained. Other approaches
learn parameters of the similarity function from a set of
texts to optimize a particular task. For instance, Ris-
tad and Yianilos (1998) and Bikenko and Mooney (2003)
learned the costs of edit operations for all characters for
an edit-distance function in a name-matching task. Other
machine-learning approaches have also been proposed to
build adaptive measures in name-matching (Bilenko and
449
Mooney, 2003) and textual-entailment tasks.
However, those machine-learning-based methods for
adaptive similarity suffer from sparseness and the ?curse
of dimensionality?. For example, the method of Ristad
and Yianilos learns n2 + 2n parameters, where n is the
size of the character set. Similarly, dimensionality in the
method of Bilenko and Mooney is the size of the data
set vocabulary. This issue is addressed primarily through
machine-learning algorithms, which reduce the dimen-
sionality of the problem regularizating to achieve enough
generalization to get an acceptable performance differ-
ence between training and test data. Although machine-
learning solutions have proven effective for many appli-
cations, the principle of Occam?s razor suggests that it
should be preferable to have a model that explains the
data with a smaller number of significant parameters. In
this paper, we seek a simpler adaptive similarity model
with few meaningful parameters.
Our proposed similarity model starts with a
cardinality-based resemblance coefficient (i.e. Dice?s
coefficient 2|A?B|/|A|+|B|) and generalizes it to model
the effect of asymmetric selection of the referent. This
effect is a human factor discovered by Tversky (1977)
that affects judgments of similarity, i.e. humans tends
to select the more prominent stimulus as the referent
and the less salient stimulus as the object. Some of
Tversky?s examples are ?the son resembles the father?
rather than ?the father resembles the son?, ?an ellipse is
like a circle? not ?a circle is like an ellipse?, and ?North
Korea is like Red China? rather than ?Red China is like
North Korea?. Generally speaking, ?the variant is more
similar to the prototype than vice versa?. In the previous
example, stimulus salience is associated with the promi-
nence of the country; for text comparison we associate
word salience with tf-idf weights. At the text level, we
associate salience with a combination of word-salience,
inter-word similarity, and text length provided by soft
cardinality. Experimentally, we observed that this effect
also occurs when comparing texts, but not necessarily
in the same direction suggested by Tversky. We used
this effect to improve the performance of our similarity
model. In addition, we proposed a parameter that biases
the function to generate greater or lower similarity
scores.
Finally, in our model we used a soft cardinality func-
tion (Jimenez et al, 2010) instead of the classical set car-
dinality. Just as classical cardinality counts the number
of elements which are not identical in a set, soft cardi-
nality uses an auxiliary inter-element similarity function
to make a soft count. For instance, the soft cardinality of
a set with two very similar (but not identical) elements
should be a real number closer to 1.0 instead of 2.0.
The rest of the paper is organized as follows. In Sec-
tion 2 we briefly present soft cardinality. In Section 3 the
proposed parameterized similarity model is presented. In
Section 4 experimental validation is provided using 8 data
sets annotated with human similarity judgments from the
?Semantic-Text-Similarity? task at SEMEVAL-2012. Fi-
nally, a brief discussion is provided in Section 5 and con-
clusions are presented in Section 6.
2 Soft Cardinality
Let A =
{
a1, a2, . . . , a|A|
}
and B =
{
b1, b2, . . . , b|B|
}
be two sets being compared. When each element of ai
or bj has an associated weight wai or wbj the problem
of comparing those sets becomes a weighted similarity
problem. This means that such model has to take into
account not only the commonalities and diferences, but
also their weights. Also, if an (|A ? B|) ? (|A ? B|)
similarity matrix S is available, the problem becomes a
weighted soft similarity problem because the common-
ality between A and B has to be computed not only
with identical elements, but also with elements with a
degree of similarity. The values of S can be obtained
from an auxiliary similarity function sim(a, b) that sat-
isfies at least non-negativity (?a, b, sim(a, b) ? 0) and
reflexivity (?a, sim(a, a) = 1). Other postulates such as
symmetry (?a, b, sim(a, b) = sim(b, a)) and triangle in-
equality1 (?a, b, c, sim(a, c) ? sim(a, b) + sim(b, c)?
1) are not strictly necessary.
Jimenez et al (2010) proposed a set-based weighted
soft-similarity model using resemblance coefficients and
the soft cardinality function instead of classical set car-
dinality. The idea of calculating the soft cardinality is
to treat elements ai in set the A as sets themselves and
to treat inter-element similarities as the intersections be-
tween the elements sim(ai, aj) = |ai ? aj |. Therefore,
the soft cardinality of set A becomes |A|
?
=
?
?
?
?|A|
i=1ai
?
?
?.
Since it is not feasible to calculate this union, they pro-
posed the following weighted approximation using |ai| =
wai :
|A|
?
sim '
|A|?
i
wai
?
?
|A|?
j
sim(ai, aj)
p
?
?
?1
(1)
Parameter p ? 0 in eq.1 controls the ?softeness? of
the cardinality, taking p = 1 its no-effect value and leav-
ing element similarities unchanged for the calculation of
soft cardinality. When p is large, all sim(?, ?) results
lower than 1 are transformed into a number approaching
0. As a result, the soft cardinality behaves like the clas-
sical cardinality, returning the addition of all the weights
of the elements, i.e |A|
?
sim '
?|A|
i wai . When p is close
to 0, all sim(?.?) results are transformed approaching
1triangle inequality postulate for similarity is derived from its coun-
terpart for dissimilarity (distance) distance(a, b) = 1? sim(a, b).
450
into a number approaching 1, making the soft cardinal-
ity returns the average of the weights of the elements, i.e.
|A|
?
sim '
1
|A|
?|A|
i wai . Jimenez et al used p = 2 and
idf weights in the same name-matching task proposed by
Cohen et al (Cohen et al, 2003).
3 A Parameterized Similarity Model
As we mentioned above, Tvesky proposed that humans
tends to select more salient stimulus as referent and less
salient stimulus as object when comparing two objects A
and B. Based on the idea of Tvesrky, the similarity be-
tween two objects can be measured as the ratio between
the salience of commonalities and the salience of the less
salient object. Drawing an analogy between objects as
sets and salience as the cardinality of a set, the salience
of commonalities is |A ? B|, and the salience of the less
salient object is min(|A|, |B|). This ratio is known as the
overlap coefficient Overlap(A,B) = |A?B|min(|A|,|B|) . How-
ever, whether |A| < |B| or whether |A|  |B|, the sim-
ilarity obtained by Overlap(A,B) is the same. Hence,
we propose to model the selecction of the referent using
a parameter ? that makes a weighted average between
min(|A|, |B|) and max(|A|, |B|), controling the degree
to which the asymmetric referent-selection effect is con-
sidered in the similarity measure.
SIM(A,B) =
|A ?B|+ bias
?max (|A|, |B|) + (1? ?)min (|A|, |B|)
(2)
The parameter ? controls the degree to which the
asymmetric referent-selection effect is considered in the
similarity measure. Its no-effect value is ? = 0.5, so
the eq.2 becomes the Dice coefficient. Moreover, when
? = 0 the eq.2 becomes the overlap coefficient, other-
wise when ? = 1 the opposite effect is modeled.
In addition, we introduced a bias parameter in eq. 2
that increases the commonalities of each object pair by
the same amount, and so it measures the degree to which
all of the objects have commonalities among each other.
Clearly, the non-effect value for the bias parameter is 0.
Besides, the bias parameter has the effect of biasing
SIM(A,B) by considering any pair ?A,B? more sim-
ilar if bias > 0 and their cardinalities are small. Con-
versely, the similarity between pairs with large cardinal-
ities is promoted if bias < 0. However, as higher values
of biasmay result in similarity scores outside the interval
[0, 1], additional post-procesing to limit the similarities in
this interval may be required.
The proposed parameterized text similarity measure is
constructed by combining the proposed resemblance co-
efficient in eq.2 and the soft cardinality in eq.1. The
resulting measure has three parameters: ?, bias, and p.
Weights wai can be idf weights. This measure takes two
? Asymetric referent selection at text level
bias Bias parameter at text level
p Soft cardinality exponent at word level
wai Element weights at word level
q1, q2 q1-grams or [q1 : q2]spectra word division
?sim Asymetric referent selection at q-gram level
biassim Bias parameter q-gram level
Table 1: Parameters of the proposed similarity model
texts represented as sets of words and returns their simi-
larity. The auxiliary similarity function sim(a, b) neces-
sary for calculating the soft cardinality is another param-
eter of the model. This auxiliary function is any function
that can compare two words and return a similarity score
in [0, 1].
To build this sim(a, b) function, we chose to reuse the
eq.2 but representing words as sets of q-grams or ranges
of q-grams of different sizes, i.e. [q1 : q2] spectra. Q-
grams are consecutive overlapped substrings of size q.
For instance, the word ?saturday? divided into trigrams
is {/sa, sat, atu, tur, urd, rda, day, ay.}. The character
?.? is a padding character added to differenciate q-grams
at the begining or end of the string. A [2 : 4]spectra
is the combined representation of a word using ?in this
example? bigrams, trigrams and quadgrams (Jimenez and
Gelbukh, 2011). The cardinality function for sim() was
the classical set cardinality. Clearly, the soft cardinal-
ity could be used again if an auxiliary similarity func-
tion for character comparison and a q-gram weighting
mechanism are provided to allow another level of recur-
sion. Therefore, the parameters of sim(a, b) are: ?sim,
biassim. Finally, the entire set of parameters of the pro-
posed similarity model is shown in Table 1.
4 Experimental Setup and Results
The aim of these experiments is to observe the behavior
of the parameters of our similarity model and verify if the
hypothesis that motivated these parameters can be con-
firmed experimentally. The experimental data are 8 data
sets (3 for training and 5 for test) proposed in the ?Seman-
tic Text Similarity? task at SEMEVAL-2012. Each data
set consist of a set of pairs of text annotated with human-
similarity judgments on a scale of 0 to 5. Each similarity
judgment is the average of the judgments provided by 5
human judges. For a comprehensible description of the
task see(Agirre et al, 2012).
For the experiments, all data sets were pre-processed
by converting to lowercase characters, English stop-
words removal and stemming using Porter stemmer
(Porter, 1980). The performance measure used for all ex-
periments was the Pearson correlation r.
451
4.1 Model Parameters
In order to make an initial exploration of the parame-
ters in Table 1, we set q1 = 2 (i.e. bigrams) and used
wai = idf(ai). For other parameters, we started with all
the non-effect values, i.e. ? = 0.5, bias = 0, p = 1,
?sim = 0.5 and biassim = 0. Plots in Figure 1 show
the Pearson correlation measured in each of the data sets.
For each graph, the non-effect configuration was used and
each parameter varies in the range indicated in each hor-
izontal axis. For best viewing, the non-effect values on
each graph are represented by a vertical line.
In this exploration of the parameters it was noted that
each parameter defines a function for the performance
measure that is smooth and with an unique global maxi-
mum. Therefore, we assumed that the join performance
function in the space defined by the 5 parameters also
had the same properties. The parameters for each data set
shown in Table 2 were found using a simple hill-climbing
algorithm. Different q-gram and spectra configurations
were tested manually.
5 Discussion
It is possible to observe from the results in Figure 1 and
Table 2 that the behavior of the parameters is similar in
pairs of data sets that have training and test parts. This
behavior is evident in both MSRvid and MSRpar data
sets, but it is less evident in SMTeuroparl. Furthermore,
the optimal parameters for training data sets MSRvid and
MSRpar were similar to those of their test data sets. In
conclusion, the proposed set of parameters provides a set
of features that characterize a data set for the text similar-
ity task.
Regarding the effect of asymmetry in referent selecc-
tion proposed by Tvesrky, it was observed that ?at text
level? the MSRvid data sets were the only ones that sup-
ported this hypothesis (? = 0.32, 0.42). The remaining
data sets showed the opposite effect (? > 0.5). That is,
annotators chose the most salient document (the longer)
as the referent when a pair of texts is being compared.
The Table 2 also shows that the optimal parameters
for all data sets were different from the no-effect values
combination. This result can also be seen in Figure 1,
where curves crossed the vertical line of no-effect value
?in most of the cases? in values different to the optimum.
Clearly, the proposed set of parameters is useful for ad-
justing the similarity function for a particular data set and
task.
6 Conclusions
We have proposed a new parameterized similarity func-
tion for text comparison and a method for finding the op-
timal values of the parameter set when training data is
available. In addition, the parameter ?, which was moti-
vated by the similarity model of Tversky, proved effective
in obtaining better performance, but we could not con-
firm the Tvesky?s hypothesis that humans tends to select
the object (text) with less stimulus salience (text length)
as the referent. This result might have occurred because
either the stimulus salience is not properly represented by
the length of the text, or Tversky?s hypothesis cannot be
extended to text comparison.
The proposed similarity function proved effective in
the task of ?Semantic Text Similarity? in SEMEVAL
2012. Our method obtained the third best average cor-
relation on the 5 test data sets. This result is remarkable
because our method only used data from the surface of
the texts, a stop-word remover, and a stemmer, which can
be even be considered as a baseline method.
Acknowledgments
This research was funded by the Systems and Industrial
Engineering Department, the Office of Student Welfare
of the National University of Colombia, Bogot?, and
throught a grant from the Colombian Department for
Science, Technology and Innovation Colciencias, proj.
110152128465. The second author recognizes the sup-
port from Mexican Government (SNI, COFAA-IPN, SIP
20113295, CONACYT 50206-H) and CONACYT?DST
India (proj. ?Answer Validation through Textual Entail-
ment?).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-Agirre
Aitor. 2012. SemEval-2012 task 6: A pilot on semantic
textual similarity. In Proc. of the 6th International Workshop
on Semantic Evaluation (SemEval 2012), in conjunction with
the First Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012)., Montreal,Canada.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive du-
plicate detection using learnable string similarity measures.
In Proc. of the ninth ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, pages 39?48,
Washington, D.C. ACM.
William W Cohen, Pradeep Ravikumar, and Stephen E Fien-
berg. 2003. A comparison of string distance metrics for
Name-Matching tasks. In Proc. of the IJCAI2003 Workshop
on Information Integration on the Web II Web03.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proc. of the ACL Workshop
on Empirical Modeling of Semantic Equivalence and Entail-
ment, Stroudsburg, PA.
Lee R. Dice. 1945. Measures of the amount of ecologic associ-
ation between species. Ecology, pages 297?302.
Daniel S. Hirschberg. 1977. Algorithms for the longest com-
mon subsequence problem. J. ACM, 24(4):664?675.
452
0.6
0.7
0.8
correlati
on
p
0.3
0.4
0.5
0 1 2 3 4 5 6 7 8
Pearson
correlati
on
?
0 0.5 1 1.5
bias
-15 -5 5 15
?sim
-0.5 0.5 1.5
biassim
-4 -2 0 2 4
MSRvid(tr) MSRvid(te) MSRpar(tr) MSRpar(te) SMTeur(tr) SMTeur(te) OnWN SMTnews no effect
-5 -3 -1 1 3 5
Figure 1: Exploring similarity model parameters around their no-effect values (tr=training, te=test)
Parameters correl. Official Results
Data set [q1 : q2] ? bias p ?sim biassim r SoftCard Best
MSRpar.training [4] 0.62 1.14 0.77 -0.04 -0.38 0.6598 n/a n/a
MSR.par.test [4] 0.60 1.02 0.9 -0.02 -0.4 0.6335 0.64051 0.7343
MSRvid.training [1:4] 0.42 -0.80 2.28 0.18 0.08 0.8323 n/a n/a
MSRvid.test [1:4] 0.32 -0.80 1.88 1.08 0.08 0.8579 0.8562 0.8803
SMTeuroparl.training [2:4] 0.74 -0.06 0.91 1.88 2.90 0.6193 n/a n/a
SMTeuroparl.test [2:4] 0.84 -0.16 0.71 1.78 3.00 0.5178 0.51522 0.5666
OnWN.test [2:5] 0.88 -0.62 1.36 -0.02 -0.70 0.7202 0.71091 0.7273
SMTnews.test [1:4] 0.88 0.88 1.57 0.80 3.21 0.5344 0.48331 0.6085
1Result obtained using Jaro-Winkler (Winkler, 1990) measure as sim(a, b) function between words.
2Result obtained using generalized Monge-Elkan measure p = 4, no stop-words removal and no term weights
(Jimenez et al, 2009).
Table 2: Results with optimized parameters and official SEMEVAL 2012 results
Paul Jaccard. 1901. Etude comparative de la distribution florare
dans une portion des alpes et des jura. Bulletin de la Soci?t?
Vaudoise des Sciences Naturelles, pages 547?579.
Sergio Jimenez and Alexander Gelbukh. 2011. SC spectra: a
linear-time soft cardinality approximation for text compari-
son. In Proc. of the 10th international conference on Artifi-
cial Intelligence, MICAI?11, Puebla, Mexico.
Sergio Jimenez, Claudia Becerra, Alexander Gelbukh, and
Fabio Gonzalez. 2009. Generalized Monge-Elkan method
for approximate text string comparison. In Computational
Linguistics and Intelligent Text Processing, volume 5449 of
LNCS, pages 559?570.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gelbukh.
2010. Text comparison using soft cardinality. In String Pro-
cessing and Information Retrieval, volume 6393 of LNCS,
pages 297?302.
Vladimir I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Alvaro E. Monge and Charles Elkan. 1996. The field matching
problem: Algorithms and applications. In Proc. KDD-96,
pages 267?270, Portland, OR.
Daniel Navarro and Michael D. Lee. 2004. Common and dis-
tinctive features in stimulis representation: A modified ver-
sion of the contrast model. Psychonomic Bulletin & Review,
11:961?974.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. WordNet::Similarity: measuring the relatedness of
concepts. In Proc. HLT-NAACL?Demonstration Papers,
Stroudsburg, PA.
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 3(14):130?137.
Eric S. Ristad and Peter N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(5):522?532.
Gerard Salton, A. Wong, and C.S. Yang. 1975. A vector space
model for automatic indexing. Com. ACM, 18(11):613?620.
L. Sj?berg. 1972. A cognitive theory of similarity. G?teborg
Psychological Reports.
Amos Tversky. 1977. Features of similarity. Psychological
Review, 84(4):327?352.
William E. Winkler. 1990. String comparator metrics and en-
hanced decision rules in the Fellegi-Sunter model of record
linkage. In Proc. of the Section on Survey Research Methods.
453
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 684?688,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Soft Cardinality + ML: Learning Adaptive Similarity Functions
for Cross-lingual Textual Entailment
Sergio Jimenez
Universidad Nacional
de Colombia, Bogota,
Ciudad Universitaria
edificio 453, oficina 220
sgjimenezv@unal.edu.co
Claudia Becerra
Universidad Nacional
de Colombia, Bogota
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz,
Av. Mendiz?bal, Col.
Nueva Industrial Vallejo,
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
This paper presents a novel approach for building
adaptive similarity functions based on cardinality us-
ing machine learning. Unlike current approaches
that build feature sets using similarity scores, we
have developed these feature sets with the cardinal-
ities of the commonalities and differences between
pairs of objects being compared. This approach al-
lows the machine-learning algorithm to obtain an
asymmetric similarity function suitable for direc-
tional judgments. Besides using the classic set cardi-
nality, we used soft cardinality to allow flexibility in
the comparison between words. Our approach used
only the information from the surface of the text,
a stop-word remover and a stemmer to address the
cross-lingual textual entailment task 8 at SEMEVAL
2012. We have the third best result among the 29
systems submitted by 10 teams. Additionally, this
paper presents better results compared with the best
official score.
1 Introduction
Adaptive similarity functions are those functions that, be-
yond using the information of two objects being com-
pared, use information from a broader set of objects
(Bilenko and Mooney, 2003). Therefore, the same sim-
ilarity function may return different results for the same
pair of objects, depending on the context of where the
objects are. Adaptability is intended to improve the per-
formance of the similarity function in relation to the task
in question associated with the entire set of objects. For
example, adaptiveness improves relevance of documents
retrieved for a query in an information retrieval task for a
particular document collection.
In text applications there are mainly three methods
to provide adaptiveness to similarity functions: term
weighting, adjustment or learning the parameters of the
similarity function, and machine learning. Term weight-
ing is a common practice that assigns a degree of im-
portance to each occurrence of a term in a text collec-
tion (Salton and Buckley, 1988; Lan et al, 2005). Sec-
ondly, if a similarity function has parameters, these can
be adjusted or learned to adapt to a particular data set.
Depending on the size of the search space defined by
these parameters, they can be adjusted either manually
or using a technique of AI. For instance, Jimenez et
al. manually adjusted a single parameter in the gener-
alized measure of Monge-Elkan (1996) (Jimenez et al,
2009) and Ristrad and Yanilios (1998) learned the costs
of editing operations between particular characters for
the Levenshtein distance (1966) using HMMs. Thirdly,
the machine-learning approach aims to learn a similar-
ity function based on a vector representation of texts us-
ing a subset of texts for training and a learning func-
tion (Bilenko and Mooney, 2003). The three methods
of adaptability can also be used in a variety of combina-
tions, e.g. term weighting in combination with machine
learning (Debole and Sebastiani, 2003; Lan et al, 2005).
Finally, to achieve adaptability, other approaches use data
sets considerably larger, such as large corpora or the Web,
e.g. distributional similarity (Lee, 1999).
In the machine-learning approach, a vector representa-
tion of texts is used in conjunction with an algorithm of
classification or regression (Alpaydin, 2004). Each vec-
tor of features ?f1, f2, . . . , fm? is associated to each pair
?Ti, Tj? of texts. Thus, Bilenko et al (2003) proposed a
set of features indexed by the data set vocabulary, simi-
lar to Zanzotto et al, (2009) who used fragments of parse
trees. However, a more common approach is to select as
features the scores of different similarity functions. Using
these features, the machine-learning algorithm discovers
the relative importance of each feature and a combina-
tion mechanism that maximizes the alignment of the final
result with a gold standard for the particular task.
In this paper, we propose a novel approach to extract
feature sets for a machine-learning algorithm using car-
684
dinalities rather than scores of similarity functions. For
instance, instead of using as a feature the score obtained
by the Dice?s coefficient (i.e. 2?|Ti?Tj |/|Ti|+|Tj |), we use
|Ti|, |Tj | and |Ti ? Tj | as features. The rationale behind
this idea is that despite the similarity scores being suitable
for learning a combined function of similarity, they hide
the information imbalance between the original pair of
texts. Our hypothesis is that the information coded in this
imbalance could provide the machine-learning algorithm
with better information to generate a combined similar-
ity score. For instance, consider these pairs of texts: ?
?The beach house is white.?, ?The house was completely
empty.? ? and ? ?The house?, ?The beach house was com-
pletely empty and isolated? ?. Both pairs have the same
similarity score using the Dice coefficient, but it is evi-
dent that the latter has an imbalance of information lost in
that single score. This imbalance of information is even
more important if the task requires to identify directional
similarities, such as ?T1 is more similar to T2, than T2 is
to T1?.
However, unlike the similarity functions, which are
numerous, there is only one set cardinality. This issue
can be addressed using the soft cardinality proposed by
Jimenez et al (2010), which uses an auxiliary function of
similarity between elements to make a soft count of the
elements in a set. For instance, the classic cardinality of
the set A = { ?Sunday?, ?Saturday? } is |A| = 2; and the
soft cardinality of the same set, using a normalized edit-
distance as auxiliary similarity function, is |A|
?
sim = 1.23
because of the commonalities between both words. Fur-
thermore, soft cardinality allows weighting of elements
giving it additional capacity to adapt.
We used the proposed approach to participate in the
cross-lingual textual-entailment task 8 at SEMEVAL
2012. The task was to recognize bidirectional, forward,
backward or lack of entailment in pairs of texts written
in five languages. We built a system based on the pro-
posed method and the use of surface information of the
text, a stop-word remover and a stemmer. Our system
achieved the third best result in official classification and,
after some debugging, we are reporting better results than
the best official scores.
This paper is structured as follows. Section 2 briefly
describes soft cardinality and other cardinalities for text
applications. Section 3 presents the proposed method.
Experimental validation is presented in Section 4. A brief
discussion is presented in Section 5. Finally, conclusions
are drawn in Section 6.
2 Cardinalities for text
Cardinality is a measure of counting the number of el-
ements in a set. The cardinality of classical set theory
represents the number of non-repeated elements in a set.
However, this cardinality is rigid because it counts in the
same manner very similar or highly differentiated ele-
ments. In text applications, text can be modeled as a
set of words and a desirable cardinality function should
take into account the similarities between words. In this
section, we present some methods to soften the classical
concept of cardinality.
2.1 Lemmatizer Cardinality
The simplest approach is to use a stemmer that collapses
words with common roots in a single lemma. Consider
the sentence: ?I loved, I am loving and I will love you?.
The plain word counting of this sentence is 10 words. The
classical cardinality collapses the three occurrences of the
pronoun ?I? giving a count of 8. However, a lemmatizer
such as Porter?s stemmer (1980) also collapses the words
?loved?, ?loving? and ?love? in a single lemma ?love? for
a count of 6. Thus, when a text is lemmatized, it induces
a relaxation of the classical cardinality of a text. In ad-
dition, to provide corpus adaptability, a weighted version
of this cardinality can add weights associated with each
word occurrence instead of adding 1 for each word (e.g.
tf-idf).
2.2 LCS cardinality
Longest common subsequence (LCS) length is a measure
of the commonalities between two texts, unlike set in-
tersection, taking into account the order. Therefore, a
cardinality function of a pair of texts A and B could
be |A ? B| = len(LCS(A,B)), |A| = len(A) and
|B| = len(B). Functions len(?) and LCS(?, ?) calcu-
late length and LCS respectively, either in character or
word granularity.
2.3 Soft Cardinality
Soft cardinality is a function that uses an auxiliary simi-
larity function to make a soft count of the elements (i.e.
words) in a set (i.e. text) (Jimenez et al, 2010). The aux-
iliary similarity function can be any measure or metric
that returns scores in the interval [0, 1], with 0 being the
lowest degree of similarity and 1 the highest (i.e. identi-
cal words). Clearly, if the auxiliary similarity function is
a rigid comparator that returns 1 for identical words and
0 otherwise, the soft cardinality becomes the classic set
cardinality.
The soft cardinality of a set A = {a1, a2, . . . , a|A|}
can be calculated by the following expression: |A|
?
sim '
?|A|
i wai
(?|A|
j sim(ai, aj)
p
)?1
. Where sim(?, ?) is
the auxiliary similarity function for approximate word
comparison, wai are weights associated with each word
ai, and p is a tuning parameter that controls the degree
of smoothness of the cardinality, i.e. if 0 ? p all ele-
ments in a set are considered identical and if p?? soft
cardinality becomes classic cardinality.
685
2.4 Dot-product VSM ?Cardinality?
Resemblance coefficients are cardinality-based simi-
larity functions. For instance, the Dice coefficient
is the ratio between the cardinality of the intersec-
tion divided by the arithmetic mean of individual
cardinalities:2?|A?B|/|A|+|B|. The cosine coefficient is
similar but instead of using the arithmetic mean it uses
the geometric mean: |A?B|/
?
|A|?
?
|B|. Furthermore, the
cosine similarity is a well known metric used in the vec-
tor space model (VSM) proposed by Salton et al (1975)
cosine(A,B) =
?
wai?wbi??
w2ai?
??
w2bi
. Clearly, this expres-
sion can be compared with the cosine coefficient inter-
preting the dot-product operation in the cosine similar-
ity as a cardinality. Thus, the obtained cardinalities are:
|A ? B|vsm =
?
wpai ? w
p
bi
, |A|vsm =
?
w2pai and
|B|vsm =
?
w2pbi . The exponent p controls the effect
of weighting providing no effect if 0? p or emphasising
the weights if p > 0. In a similar application, Gonza-
lez and Caicedo (2011) used p = 0.5 and normalization
justified by the quantum information retrieval theory.
3 Learning Similarity Functions from
Cardinalities
Different similarity measures use different knowledge,
identify different types of commonalities, and compare
objects with different granularity. In many of the auto-
matic text-processing applications, the qualities of sev-
eral similarity functions may be required to achieve the
final task. The combination of similarity scores with a
machine-learning algorithm to obtain a unified effect for
a particular task is a common practice (Bilenko et al,
2003; Malakasiotis and Androutsopoulos, 2007; Malaka-
siotis, 2009). For each pair of texts for comparison, there
is provided a vector representation based on multiple sim-
ilarity scores as a set of features. In addition, a class at-
tribute is associated with each vector which contains the
objective of the task or the gold standard to be learned by
the machine-learning algorithm.
However, the similarity scores conceal important in-
formation when the task requires dealing with directional
problems, i.e. whenever the order of comparing each pair
of texts is related with the class attribute. For instance,
textual entailment is a directional task since it is neces-
sary to recognize whether the first text entails the second
text or vice versa. This problem can be addressed us-
ing asymmetric similarity functions and including scores
for sim(A,B) and sim(B,A) in the resulting vector for
each pair ?A,B?. Nevertheless, the similarity measures
that are more commonly used are symmetric, e.g. edit-
distance (Levenshtein, 1966), LCS (Hirschberg, 1977),
cosine similarity, and many of the current semantic re-
latedness measures (Pedersen et al, 2004). Although,
there are asymmetric measures such as the Monge-Elkan
measure (1996) and the measure proposed by Corley and
Mihalcea (Corley and Mihalcea, 2005), they are outnum-
bered by the symmetric measures. Clearly, this situation
restricts the use of the machine learning as a method of
combination for directional problems.
Alternatively, we propose the construction of a vector
for each pair of texts using cardinalities instead of sim-
ilarity scores. Moreover, using cardinalities rather than
similarity scores allows the machine-learning algorithm
to discover patterns to cope with directional tasks.
Basically, we propose to use a set with six features for
each cardinality function: |A|, |B|, |A ? B|, |A ? B|,
|A?B| and |B ?A|.
4 Experimental Setup
4.1 Cross-lingual Textual Entailment (CLTE) Task
This task consist of recognizing in a pair of topically re-
lated text fragments T1 and T2 in different languages, one
of the following possible entailment relations: i) bidi-
rectional T1 ? T2 ? T1 ? T2, i.e. semantic equiv-
alence; ii) forward T1 ? T2 ? T1 : T2; iii) back-
ward T1 ; T2 ? T1 ? T2; and iv) no entailment
T1 ; T2 ? T1 : T2. Besides, both T1 and T2 are as-
sumed to be true statements; hence contradictory pairs
are not allowed.
Data sets consist of a collection of 1,000 text pairs
(500 for training and 500 for testing) each one labeled
with one of the possible entailment types. Four balanced
data sets were provided using the following language
pairs: German-English (deu-eng), French-English (fra-
eng), Italian-English (ita-eng) and Spanish-English (spa-
eng). The evaluation measure for experiments was accu-
racy, i.e. the ratio of correctly predicted pairs by the total
number of predictions. For a comprehensive description
of the task see (Negri et al, 2012).
4.2 Experiments
Given that each pair of texts ?T1, T2? are in different lan-
guages, a pair of translations ?T t1 , T
t
2? were provided us-
ing Google Translate service. Thus, each one of the text
pairs ?T1, T t2? and ?T
t
1 , T2? were in the same language.
Then, all produced pairs were pre-processed by remov-
ing stop-words in their respective languages. Finally, all
texts were lemmatized using Porter?s stemmer (1980) for
English and Snowball stemmers for other languages us-
ing an implementation provided by the NLTK (Loper and
Bird, 2002).
Then, different set of features were generated using
similarity scores or cardinalities. While each symmet-
ric similarity function generates 2 features i)sim(T1, T t2)
and ii)sim(T t1 , T2), asymmetric functions generate two
additional features iii)sim(T t2 , T1) and iv)sim(T2, T
t
1).
686
On the other hand, each cardinality function generates
12 features: i) |T1|, ii) |T t2 |, iii) |T1 ? T
t
2 |, iv) |T1 ? T
t
2 |,
v) |T1 ? T t2 |, vi) |T
t
2 ? T1|, vii) |T
t
1 |, viii) |T2|, ix)
|T t1 ? T2|, x) |T
t
1 ? T2|, xi) |T
t
1 ? T2|, and xii) |T2 ? T
t
1 |.
Various combinations of cardinalities, symmetric and
asymmetric functions were used to generate the follow-
ing feature sets:
Sym.simScores: scores of the following symmetric
similarity functions: Jaccard, Dice, and cosine coef-
ficients using classical cardinality and soft cardinality
(edit-distance as auxiliar sim. function). In addition, co-
sine similarity, softTFIDF (Cohen et al, 2003) and edit-
distance (total 18 features).
Asym.LCS.sim: scores of the following asymmetric
similarity functions: sim(T1, T2) = lcs(T1,T2)/len(T1)
and sim(T1, T2) = lcs(T1,T2)/len(T2) at character level (4
features).
Classic.card: cardinalities using classical set cardinal-
ity (12 features).
Dot.card.w: dot-product cardinality using idf weights
as described in Section 2.4, using p = 1 (12 features).
LCS.card: LCS cardinality at word-level using idf
weights as described in Section 2.1 (12 features).
SimScores: combined features sets from
Sym.SimScores, Asym.LCS.sim and the general-
ized Monge-Elkan measure (Jimenez et al, 2009) using
p = 1, 2, 3 (30 features).
Dot.card.w.0.5: same as Dot.card.w using p = 0.5.
Classic.card.w: classical cardinality using idf weights
(12 features).
Soft.card.w: soft cardinality using idf weights as de-
scribed in Section 2.3 using p = 1, 2, 3, 4, 5 (60 features).
The machine-learning classification algorithm for all
feature sets was SVM (Cortes and Vapnik, 1995) with the
complexity parameter C = 1.5 and a linear polynomial
kernel. All experiments were conducted using WEKA
(Hall et al, 2009).
4.3 Results
In Semeval 2012 exercise, participants were given a par-
ticular subdivision into training and test subsets for each
data set. For official results, participants received only the
gold-standard labels for the subset of training, and accu-
racies of each system in the test subset was measured by
the organizers. In Table 1, the results for that particular
division are shown. At the bottom of that table, the of-
ficial results for the first three systems are shown. Our
system, ?3rd.Softcard? was configured using soft cardi-
nality with edit-distance as auxiliary similarity function
and p = 2. Erroneously, at the time of the submission,
all texts in the 5 languages were lemmatized using an En-
glish stemmer and stop-words in all languages were ag-
gregated into a single set before the withdrawal. In spite
of these bugs, our system was the third best score.
FEATURES SPA ITA FRA DEU avg.
Sym.simScores 0.404 0.410 0.410 0.410 0.409
Asym.LCS.sim 0.490 0.492 0.482 0.474 0.485
Classic.card 0.560 0.534 0.570 0.542 0.552
Dot.card.w 0.562 0.568 0.550 0.548 0.557
LCS.card 0.606 0.566 0.568 0.558 0.575
SimScores 0.600 0.562 0.568 0.572 0.576
Dot.card.w.0.5 0.584 0.574 0.586 0.572 0.579
Classic.card.w 0.584 0.576 0.588 0.590 0.585
Soft.card.w 0.598 0.602 0.624 0.604 0.607
SEMEVAL 2012 OFFICIAL RESULTS
1st.HDU.run2 0.632 0.562 0.570 0.552 0.579
2nd.HDU.run1 0.630 0.554 0.564 0.558 0.577
3rd.Softcard 0.552 0.566 0.570 0.550 0.560
Table 1: Accuracy results for Semeval2012 task 8
Soft.card.w 60.174(1.917)% imprv. Sign.
Sym.simScore 39.802(1.783)% 51.2% <0.001
Asym.LCS.sim 48.669(1.820)% 23.6% <0.001
Classic.card 55.278(2.422)% 8.9% 0.010
Dot.card.w 54.906(2.024)% 9.6% 0.004
LCS.card 55.131(2.471) % 9.1% 0.015
SimScores 56.889(2.412) % 5.8% 0.124
Dot.card.w.0.5 57.114(2.141)% 5.4% 0.059
Classic.card.w 56.708(2.008)% 6.1% 0.017
Table 2: Average accuracy comparison vs. Soft.card.w in 100
runs
To compare our approach of using feature sets based
on soft cardinality versus other approaches, we gener-
ated 100 random training-test subdivisions (50%-50%) of
each data set. The average results were compared and
tested statistically with the paired T-tested corrected test.
Results, deviations, the percentage of improvement, and
its significance in comparison with the Soft.card.w sys-
tem are shown in Table2.
5 Discusion
Results in Table 2 show that our hypothesis that fea-
ture sets obtained from cardinalities should outperform
features sets obtained from similarity scores was de-
mostrated when compared versus similarity functions al-
ternatively symmetrical or asymetrical. However, when
our approach is compared with a feature set obtained by
combining symmetric and asymmetric functions, we ob-
tained an improvement of 5.8% but only with a signif-
icance of 0.124. Regarding soft cardinality compared
to alternative cardinalities, soft cardinality outperformed
others in all cases with significance <0.059.
687
6 Conclusions
We have proposed a new method to compose feature sets
using cardinalities rather than similarity scores. Our ap-
proach proved to be effective for directional text compar-
ison tasks such as textual entailment. Furthermore, the
soft cardinality function proved to be the best for obtain-
ing such sets of features.
Acknowledgments
This research was funded by the Systems and Industrial
Engineering Department, the Office of Student Welfare
of the National University of Colombia, Bogot?, and
throught a grant from the Colombian Department for
Science, Technology and Innovation Colciencias, proj.
110152128465. The second author recognizes the sup-
port from Mexican Government (SNI, COFAA-IPN, SIP
20113295, CONACYT 50206-H) and CONACYT?DST
India (proj. ?Answer Validation through Textual Entail-
ment?).
References
Ethem Alpaydin. 2004. Introduction to Machine Learning.
MIT press.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive du-
plicate detection using learnable string similarity measures.
In Proc. of the ninth ACM SIGKDD international conference
on Knowledge discovery and data mining, Washington, D.C.
Mikhail Bilenko, Raymond Mooney, William Cohen, Pradeep
Ravikumar, and Stephen Fienberg. 2003. Adaptive name
matching in information integration. IEEE Intelligent Sys-
tems, 18(5):16?23.
William W Cohen, Pradeep Ravikumar, and Stephen E Fien-
berg. 2003. A comparison of string distance metrics for
Name-Matching tasks. In Proc. of the IJCAI2003 Workshop
on Information Integration on the Web II Web03.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proceedings of the ACL Work-
shop on Empirical Modeling of Semantic Equivalence and
Entailment, Stroudsburg, PA.
Corinna Cortes and Vladimir N. Vapnik. 1995. Support-Vector
networks. Machine Learning, 20(3):273?297.
Franca Debole and Fabrizio Sebastiani. 2003. Supervised term
weighting for automated text categorization. In Proc. of the
2003 ACM symposium on applied computing, New York,
NY.
Fabio A. Gonzalez and Juan C. Caicedo. 2011. Quantum la-
tent semantic analysis. In Proc. of the Third international
conference on Advances in information retrieval theory.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software: An
update. SIGKDD Explorations, 11(1):10?18.
Daniel S. Hirschberg. 1977. Algorithms for the longest com-
mon subsequence problem. J. ACM, 24(4):664?675.
Sergio Jimenez, Claudia Becerra, Alexander Gelbukh, and
Fabio Gonzalez. 2009. Generalized Monge-Elkan method
for approximate text string comparison. In Computational
Linguistics and Intelligent Text Processing, volume 5449 of
LNCS, pages 559?570.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gelbukh.
2010. Text comparison using soft cardinality. In String Pro-
cessing and Information Retrieval, volume 6393 of LNCS,
pages 297?302.
Man Lan, Chew-Lim Tan, Hwee-Boon Low, and Sam-Yuan
Sung. 2005. A comprehensive comparative study on term
weighting schemes for text categorization with support vec-
tor machines. In Special interest tracks and posters of the
14th international conference on World Wide Web, New
York, NY.
Lillian Lee. 1999. Measures of distributional similarity. In
Proc. of the 37th annual meeting of the Association for Com-
putational Linguistics on Computational Linguistics, Col-
lege Park, Maryland.
Vladimir I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Edward Loper and Steven Bird. 2002. NLTK: the natural lan-
guage toolkit. In In Proceedings of the ACL Workshop on
Effective Tools andMethodologies for Teaching Natural Lan-
guage Processing and Computational Linguistics, Philadel-
phia, PA.
Prodromos Malakasiotis and Ion Androutsopoulos. 2007.
Learning textual entailment using SVMs and string similarity
measures. In Proc. of the ACL-PASCALWorkshop on Textual
Entailment and Paraphrasing, Stroudsburg, PA.
Prodromos Malakasiotis. 2009. Paraphrase recognition using
machine learning to combine similarity measures. In Proc. of
the ACL-IJCNLP 2009 Student Research Workshop, Strouds-
burg, PA.
Alvaro E. Monge and Charles Elkan. 1996. The field matching
problem: Algorithms and applications. In Proc. KDD-96,
Portland, OR.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad, Luisa
Bentivogli, and Danilo Giampiccolo. 2012. 2012. semeval-
2012 task 8: Cross-lingual textual entailment for content syn-
chronization. In In Proc. of the 6th International Workshop
on Semantic Evaluation (SemEval 2012), Montreal, Canada.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. WordNet::Similarity: measuring the relatedness of
concepts. In Proc. HLT-NAACL?Demonstration Papers,
Stroudsburg, PA.
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 3(14):130?137.
Eric S. Ristad and Peter N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(5):522?532.
Gerard Salton and Christopher Buckley. 1988. Term-weighting
approaches in automatic text retrieval. Information Process-
ing & Management, 24(5):513?523.
Gerard Salton, Andrew K. C. Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing. Com-
mun. ACM, 18(11):613?620.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessan-
dro Moschitti. 2009. A machine learning approach to tex-
tual entailment recognition. Natural Language Engineering,
15(Special Issue 04):551?582.
688
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 194?201, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
SOFTCARDINALITY-CORE: Improving Text Overlap with
Distributional Measures for Semantic Textual Similarity
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, esq. Av. Mendiz?bal,
Col. Nueva Industrial Vallejo,
CP 07738, DF, M?xico
www.gelbukh.com
Abstract
Soft cardinality has been shown to be a very
strong text-overlapping baseline for the task of
measuring semantic textual similarity (STS),
obtaining 3rd place in SemEval-2012. At
*SEM-2013 shared task, beside the plain text-
overlapping approach, we tested within soft
cardinality two distributional word-similarity
functions derived from the ukWack corpus.
Unfortunately, we combined these measures
with other features using regression, obtain-
ing positions 18th, 22nd and 23rd among the
90 participants systems in the official rank-
ing. Already after the release of the gold stan-
dard annotations of the test data, we observed
that using only the similarity measures with-
out combining them with other features would
have obtained positions 6th, 7th and 8th; more-
over, an arithmetic average of these similarity
measures would have been 4th(mean=0.5747).
This paper describes both the 3 systems as
they were submitted and the similarity mea-
sures that would obtained those better results.
1 Introduction
The task of textual semantic similarity (STS) con-
sists in providing a similarity function on pairs of
texts that correlates with human judgments. Such
a function has many practical applications in NLP
tasks (e.g. summarization, question answering, tex-
tual entailment, paraphrasing, machine translation
evaluation, among others), which makes this task
particularly important. Numerous efforts have been
devoted to this task (Lee et al, 2005; Mihalcea et al,
2006) and major evaluation campaigns have been
held at SemEval-2012 (Agirre et al, 2012) and in
*SEM-2013 (Agirre et al, 2013).
The experimental setup of STS in 2012 consisted
of three data sets, roughly divided in 50% for train-
ing and for testing, which contained text pairs manu-
ally annotated as a gold standard. Furthermore, two
data sets were provided for surprise testing. The
measure of performance was the average of the cor-
relations per data set weighted by the number of
pairs in each data set (mean). The best performing
systems were UKP (B?r et al, 2012) mean=0.6773,
TakeLab (?aric et al, 2012) mean=0.6753 and soft
cardinality (Jimenez et al, 2012) mean=0.6708.
UKP and TakeLab systems used a large number of
resources (see (Agirre et al, 2012)) such as dictio-
naries, a distributional thesaurus, monolingual cor-
pora, Wikipedia, WordNet, distributional similar-
ity measures, KB similarity, POS tagger, machine
learning and others. Unlike those systems, the soft
cardinality approach used mainly text overlapping
and conventional text preprocessing such as remov-
ing of stop words, stemming and idf term weighting.
This shows that the additional gain in performance
from using external resources is small and that the
soft cardinality approach is a very challenging base-
line for the STS task. Soft cardinality has been
previously shown (Jimenez and Gelbukh, 2012) to
be also a good baseline for other applications such
as information retrieval, entity matching, paraphrase
detection and recognizing textual entailment.
Soft cardinality approach to constructing similar-
ity functions (Jimenez et al, 2010) consists in using
any cardinality-based resemblance coefficient (such
as Jaccard or Dice) but substituting the classical set
194
cardinality with a softened counting function called
soft cardinality. For example, the soft cardinality of
a set containing three very similar elements is close
to (though larger than) 1, while for three very dif-
ferent elements it is close to (though less than) 3.
To use the soft cardinality with texts, they are repre-
sented as sets of words, and a word-similarity func-
tion is used for the soft counting of the words. For
the sake of completeness, we give a brief overview
of the soft-cardinality method in Section 3.
The resemblance coefficient used in our participa-
tion is a modified version of Tversky?s ratio model
(Tversky, 1977). Apart from the two parameters of
this coefficient, a new parameter was included and
functions max and min were used to make it sym-
metrical. The rationale for this new coefficient is
given in Section 2.
Three word similarity features used in our sys-
tems are described in Section 4. The one is a mea-
sure of character q-gram overlapping, which reuses
the coefficient proposed in Section 2; this measure is
described in subsection 4.1. The other two ones are
distributional measures obtained from the ukWack
corpus (Baroni et al, 2009), which is a collection of
web-crawled documents containing about 1.9 billion
words in English. The second measure is, again, a
reuse of the coefficient specified in Section 2, but us-
ing instead sets of occurrences (and co-occurrences)
of words in sentences in the ukWack corpus; this
measure is described in subsection 4.2. Finally, the
third one, which is a normalized version of point-
wise mutual information (PMI), is described in sub-
section 4.3.
The parameters of the three text-similarity func-
tions derived from the combination of the proposed
coefficient of resemblance (Section 2), the soft car-
dinality (Section 3) and the three word-similarity
measures (Section 4) were adjusted to maximize the
correlation with the 2012 STS gold standard data.
At this point, these soft-cardinality similarity func-
tions can provide predictions for the test data. How-
ever, we decided to test the approach of learning a
resemblance function from the training data instead
of using a preset resemblance coefficient. Basically,
most resemblance coefficients are ternary functions
F (x, y, z) where x = |A|, y = |B| and z = |A?B|:
e.g. Dice coefficient is F (x, y, z) = 2z/x+y and Jac-
card is F (x, y, z) = z/x+y?z. Thus, this function
can be learned using a regression model, providing
cardinalities x, y and z as features and the gold stan-
dard value as the target function. The results ob-
tained for the text-similarity functions and the re-
gression approach are presented in Section 7.
Unfortunately, when using a regressor trained
with 2012 STS data and tested with 2013 surprise
data we observed that the results worsened rather
than improved. A short explanation of this is over-
fitting. A more detailed discussion of this, together
with an assessment of the performance gain obtained
by the use of distributional measures is provided in
Section 8.
Finally, in Section 9 the conclusions of our partic-
ipation in this evaluation campaign are presented.
2 Symmetrical Tversky?s Ratio Model
In the field of mathematical psychology Tversky
proposed the ratio model (TRM) (Tversky, 1977)
motivated by the imbalance that humans have on
the selection of the referent to compare things. This
model is a parameterized resemblance coefficient to
compare two sets A and B given by the following
expression:
trm(A,B) =
|A ?B|
?|A \B|+ ?|B \A|+ |A ?B|
,
Having ?, ? ? 0. The numerator represents the
commonality between A and B, and the denomina-
tor represents the referent for comparison. Parame-
ters ? and ? represent the preference in the selection
of A or B as referent. Tversky associated the set
cardinality, to the stimuli of the objects being com-
pared. Let us consider a Tversky?s example of the
70s: A is North Corea, B is red China and stimuli
is the prominence of the country. When subjects as-
sessed the similarity between A and B, they tended
to select the country with less prominence as ref-
erent. Tversky observed that ? was larger than ?
when subjects compared countries, symbols, texts
and sounds. Our motivation is to use this model by
adjusting the parameters ? and ? for better modeling
human similarity judgments for short texts.
However, this is not a symmetric model and the
parameters ? and ?, have the dual interpretation of
modeling the asymmetry in the referent selection,
while controlling the balance between |A ? B| and
195
|A?B|+ |B ?A| as well. The following reformu-
lation, called symmetric TRM (strm), is intended to
address these issues:
strm(A,B) =
c
? (?a+ (1? ?) b) + c
, (1)
a = min(|A ? B|, |B ? A|), b = max(|A ?
B|, |B ? A|) and c = |A ? B| + bias. In strm, ?
models only the balance between the differences in
the cardinalities of A and B, and ? models the bal-
ance between |A?B| and |A?B|+|B?A|. Further-
more, the use of functions min and max makes the
measure to be symmetric. Although the motivation
for the bias parameter is empirical, we believe that
this reduces the effect of the common features that
are frequent and therefore less informative, e.g. stop
words. Note that for ? = 0.5,? = 1 and bias = 0,
strm is equivalent to Dice?s coefficient. Similarity,
for ? = 0.5,? = 2 and bias = 0, strm is equivalent
to the Jaccard?s coefficient.
3 Soft Cardinality
The cardinality of a set is its number of elements. By
definition, the sets do not allow repeated elements,
so if a collection of elements contains repetitions its
cardinality is the number of different elements. The
classical set cardinality does not take into account
similar elements, i.e. only the identical elements
in a collection counted once. The soft cardinality
(Jimenez et al, 2010) considers not only identical
elements but also similar using an auxiliary similar-
ity function sim, which compares pairs of elements.
This cardinality can be calculated for a collection of
elements A with the following expression:
|A|? =
n?
i=1
wi
?
?
n?
j=1
sim(ai, aj)p
?
?
?1
(2)
A ={a1, a2, . . . , an}; wi ? 0; p ? 0; 1 >
sim(x, y) ? 0, x 6= y; and sim(x, x) = 1. The
parameter p controls the degree of "softness" of
the cardinality. This formulation has the property
of reproducing classical cardinality when p is large
and/or when sim is a rigid function that returns 1
only for identical elements and 0 otherwise. The co-
efficients wi are the weights associated with each el-
ement. In text applications elements ai are words
and weights wi represent the importance or infor-
mative character of each word (e.g. idf weights).
The apostrophe is used to differentiate soft cardinal-
ity from the classic set cardinality.
4 Word Similarity
Analogous to the STS, the word similarity is the task
of measuring the relationship of a couple of words
in a way correlated with human judgments. Since
when Rubenstein and Goodenough (1965) provided
the first data set, this task has been addressed pri-
marily through semantic networks (Resnik, 1999;
Pedersen et al, 2004) and distributional measures
(Agirre et al, 2009). However, other simpler ap-
proaches such as edit-distance (Levenshtein, 1966)
and stemming (Porter, 1980) can also be used. For
instance, the former identifies the similarity between
"song" and "sing", and later that between "sing" and
"singing". This section presents three approaches
for word similarity that can be plugged into the soft
cardinality expression in eq. 2.
4.1 Q-grams similarity
Q-grams are the collection of consecutive-
overlapped sub-strings of length q obtained
from the character string in a word. For instance,
the 2-grams (bi-grams) and 3-grams (trigrams) rep-
resentation of the word ?sing? are {?#s?, ?si?, ?in?,
?ng?, ?g#?} and {?#si?, ?sin?, ?ing?, ?ng#?} respec-
tively. The character ?#? is a padding character that
distinguishes q-grams at the beginning and ending
of a word. If the number of characters in a word is
greater or equal than q its representation in q-grams
is the word itself (e.g. the 6-grams in ?sing? are
{?sing?}). Moreover, the 1-grams (unigrams) and
0-grams representations of ?sing? are {?s?, ?i?, ?n?,
?g?} and {?sing?}. A word can also be represented
by combining multiple representations of q-grams.
For instance, the combined representation of ?sing?
using 0-grams, unigrams, and bi-grams is {?sing?,
?s?, ?i?, ?n?, ?g?, ?#s?, ?si?, ?in?, ?ng?, ?g#?}, denoted
by [0:2]-grams. In practice a range [q1 : q2] of
q-grams can be used having 0 ? q1 < q2.
The proposed word-similarity function (named
qgrams) first represents a pair of words using
[q1 : q2]-grams and then compares them reusing
the strm coefficient (eq.1). The parameters of the
196
qgrams function are q1, q2, ?qgrams, ?qgrams, and
biasqgrams. These parameters are sub-scripted to
distinguish them from their counterparts at the text-
similarity functions.
4.2 Context-Set Distributional Similarity
The hypothesis of this measure is that the co-
occurrence of two words in a sentence is a hint of
the possible relationship between them. Let us de-
fine sf(t) as the sentence frequency of a word t in
a corpus. The sentence frequency is equivalent to
the well known document frequency but uses sen-
tences instead of documents. Similarly sf(tA ? tB)
is the number of sentences where words tA and tB
co-occur. The idea is to compute a similarity func-
tion between tA and tB representing them as A and
B, which are sets of the sentences where tA and tB
occur. Similarly, A?B is the set of sentences where
both words co-occur. The required cardinalities can
be obtained from the sentence frequencies by: |A| =
sf(tA); |B| = sf(tB) and |A ? B| = sf(tA ? tB).
These cardinalities are combined reusing again the
strm coefficient (eq. 1) to obtain a word-similarity
function. The parameters of this function, which we
refer to it as csds, are ?csds, ?csds and biascsds.
4.3 Normalized Point-wise Mutual Information
The pointwise mutual information (PMI) is a mea-
sure of relationship between two random variables.
PMI is calculated by the following expression:
pmi(tA, tB) = log2
(
P (tA ? tB)
P (tA) ? P (tB)
)
PMI has been used to measure the relatedness of
pairs of words using the number of the hits returned
by a search engine (Turney, 2001; Bollegala et al,
2007). However, PMI cannot be used directly as
sim function in eq.2. The alternative is to normal-
ize it dividing it by log2(P (tA ? tB)) obtaining a
value in the [1,?1] interval. This measure returns
1 for complete co-occurrence, 0 for independence
and -1 for ?never? co-occurring. Given that the re-
sults in the interval (0,-1] are not relevant, the final
normalized-trimmed expression is:
npmi(tA, tB) = max
[
pmi(tA, tB)
log2(P (tA ? tB))
, 0
]
(3)
The probabilities required by PMI can be obtained
by MLE using sentence frequencies in a large cor-
pus: P (tA) ?
sf(tA)
S , P (tB) ?
sf(tB)
S ,and P (tA ?
tB) ?
sf(tA?tB)
S . Where S is the total number of
sentences in the corpus.
5 Text-similarity Functions
The ?building blocks? proposed in sections 2,
3 and 4, are assembled to build three text-
similarity functions, namely STSqgrams, STScsds
and STSnpmi. The first component is the strm re-
semblance coefficient (eq. 1), which takes as argu-
ments a pair of texts represented as bags of words
with importance weights associated with each word.
In the following subsection 5.1 a detailed descrip-
tion of the procedure for obtaining such weighted
bag-of-words is provided.
The strm coefficient is enhanced by replac-
ing the classical cardinality by the soft cardinality,
which exploits two resources: importance weights
associated with each word (weights wi) and pair-
wise comparisons among words (sim). Unlike
STSqgrams measure, STScsds and STSnpmi mea-
sures require statistics from a large corpus. A brief
description of the used corpus and the method for
obtaining such statistics is described in subsection
5.2. Finally, the three proposed text-similarity func-
tions contain free parameters that need to be ad-
justed. The method used to get those parameters is
described in subsection 5.3.
5.1 Preprocessing and Term Weighting
All training and test texts were preprocessed with
the following sequence of actions: i) text strings
were tokenized, ii) uppercase characters are con-
verted into lower-cased equivalents, iii) stop-words
were removed, iv) punctuation marks were removed,
and v) words were stemmed using Porter?s algorithm
(1980). Then each stemmed word was weighted
with idf (Jones, 2004) calculated using the entire
collection of texts.
5.2 Sentence Frequencies from Corpus
The sentence frequencies sf(t) and sf(tA ? tB) re-
quired by csds and npmi word-similarity func-
tions were obtained from the ukWack corpus (Ba-
roni et al, 2009). This corpus has roughly 1.9 bil-
197
lion words, 87.8 millions of sentences and 2.7 mil-
lions of documents. The corpus was iterated sen-
tence by sentence with the same preprocessing that
was described in the previous section, looking for
all occurrences of words and word pairs from the
full training and test texts. The target words were
stored in a trie, making the entire corpus iteration
took about 90 minutes in a laptop with 4GB and a
1.3Ghz processor.
5.3 Parameter optimization
The three proposed text-similarity functions have
several parameters: p exponent in the soft car-
dinality; ?, ?, and bias in strm coefficient;
their sub-scripted versions in qgrams and csds
word-similarity functions; and finally q1and q2 for
qgrams function. Parameter sets for each of the
three text-similarity functions were optimized us-
ing the full STS-SemEval-2012 data. The function
to maximize was the correlation between similar-
ity scores against the gold standard in the training
data. The set of parameters for each similarity func-
tion were optimized using a greedy hill-climbing ap-
proach by using steps of 0.01 for all parameters ex-
cept q1 and q2 that used 1 as step. The initial values
were p = 1, ? = 0.5, ? = 1, bias = 0, q1 = 2 and
q2 = 3. All parameters were optimized until im-
provement in the function to maximize was below
0.0001. The obtained values are :
STSqgrams p = 1.32,? = 0.52, ? = 0.64, bias =
?0.45, q1 = 0, q2 = 2, ?qgrams = 0.95,
?qgrams = 1.44, biasqgrams = ?0.44.
STScsds p = 0.5, ? = 0.63, ? = 0.69, bias =
?2.05, ?csds = 1.34, ?csds = 2.57, biascsds =
?1.22 .
STSnpmi p = 6.17,? = 0.83, ? = 0.64, bias =
?2.11.
6 Regression for STS
The use of regression is motivated by the follow-
ing experiment. First, a synthetic data set with
1,000 instances was generated with the following
three features: |A| = RandomBetween(1, 100),
|B| = RandomBetween(1, 100) and |A ? B| =
RandomBetween(0,min[|A|, |B|]). Secondly, a
#1 STSsim #11 |A?B|
?/|A|?
#2 |A|? #12 |A?B|?/|B|?
#3 |B|? #13 |A|? ? |B|?
#4 |A ?B|? #14 |A?B|?/|A?B|?
#5 |A ?B|? #15 2?|A?B|?/|A|?+|B|?
#6 |A \B|? #16 |A?B|/min[|A|,|B|]
#7 |B \A|? #17 |A?B|?/max[|A|?,|B|?]
#8 |A ?B ?A ?B|? #18 |A?B|?/
?
|A|??|B|?
#9 |A?B|?/|A|? #19 |A?B|
?+|A|?+|B|?
2?|A|??|B|?
#10 |B?A|?/|B|? #20 gold standard
Table 1: Feature set for regression
linear regressor was trained using the Dice?s coef-
ficient (i.e. 2|A ? B|/|A| + |B|) as target function.
The Pearson correlation obtained using 4-fold cross-
validation as method of evaluation was r = 0.93.
Besides, a Reduced Error Pruning (REP) tree (Wit-
ten and Frank, 2005) boosted with 30 iterations of
Bagging (Breiman, 1996) was used instead of the
linear regressor obtaining r = 0.99. We concluded
that a particular resemblance coefficient can be ac-
curately approximated using a nonlinear regression
algorithm and training data.
This approach can be used for replacing the strm
coefficient by a similarity function learned from STS
training data. The three features used in the previ-
ous experiment were extended to a total of 19 (see
table 1) plus the gold standard as target. The feature
#1 is the score of the corresponding text-similarity
function described in the previous section. Three
sets of features were constructed, each with 19 fea-
tures using the soft cardinality in combination with
the word-similarity functions qgrams, csds and
npmi. Let us name these feature sets as fs:qgrams,
fs:csds and fs:npmi. The submission labeled run1
was obtained using the feature set fs:qgrams (19 fea-
tures). The submission labeled run2 was obtained
using the aggregation of fs:qgrams and fs:csds (19?
2 = 38 features). Finally, run3 was the aggregation
of fs:grams, fs:csds and fs:npmi (19 ? 3 = 57 fea-
tures).
7 Results in *SEM 2013 Shared Task
In this section three groups of systems are described
by using the functions and models proposed in the
previous sections. The first group (and simplest)
198
Data set STSqgrams STScsds STSnpmi average
headlines 0.7625 0.7243 0.7379 0.7562
OnWN 0.7022 0.7050 0.6832 0.7063
FNWM 0.2704 0.3713 0.4215 0.3940
SMT 0.3151 0.3325 0.3408 0.3402
mean 0.5570 0.5592 0.5653 0.5747
rank 8 7 6 4
Table 2: Unofficial results using text-similarity functions
Data set run1 run2 run3
headlines 0.7591 0.7632 0.7640
OnWN 0.7159 0.7239 0.7485
FNWM 0.2806 0.3679 0.3487
SMT 0.2820 0.2786 0.2952
mean 0.5491 0.5586 0.5690
rank 14 8 4
Table 3: Unofficial results using linear regression
of systems consist in using the scores of the three
text-similarity functions STSqgrams, STScsds and
STSnpmi. Table 2 shows the unofficial results of
these three systems. The bottom row shows the posi-
tions that these systems would have obtained if they
had been submitted to the *SEM shared task 2013.
The last column shows the results of a system that
combines the scores of three measures on a single
score calculating the arithmetic mean. This is the
best performing system obtained with the methods
described in this paper.
Tables 3 and 4 show unofficial and official re-
sults of the method described in section 6 using
linear regression and Bagging (30 iterations)+REP
tree respectively. These results were obtained using
WEKA (Hall et al, 2009).
8 Discussion
Contrary to the observation we made in training
data, the methods that used regression to predict the
gold standard performed poorly compared with the
text similarity functions proposed in Section 5. That
is, the results in Table 2 overcome those in Tables 3
and 4. Also in training data, Bagging+REP tree sur-
passed linear regression, but, as can be seen in tables
3 and 4 the opposite happened in test data. This is
a clear symptom of overfitting. However, the OnWN
Data set run1 run2 run3
headlines 0.6410 0.6713 0.6603
OnWN 0.7360 0.7412 0.7401
FNWM 0.3442 0.3838 0.3347
SMT 0.3035 0.2981 0.2900
mean 0.5273 0.5402 0.5294
rank 23 18 22
Table 4: Official results of the submitted runs to STS
*SEM 2013 shared task using Bagging + REP tree for
regression
data set was an exception, which obtained the best
results using linear regression. OnWN was the only
one among the 2013 data sets that was not a sur-
prise data set. Probably the 5.97% relative improve-
ment obtained in run3 by the linear regression versus
the best result in Table 2 may be justified owing to
some patterns discovered by the linear regressor in
the OnWN?2012 training data which are projected
on the OnWN?2013 test data.
It is worth noting that in all three sets of results,
the lowest mean was consistently obtained by the
text-overlapping methods, namely STSqgrams and
run1. The relative improvement in mean due to
the use of distributional measures against the text-
overlapping methods was 3.18%, 3.62% and 2.45%
in each set of results (see Tables 2, 3 and 4). In
FNWM data set, the biggest improvements achieved
55.88%, 31.11% and 11.50% respectively in the
three groups of results, followed by SMT data set.
Both in FNWN data set as in SMT, the texts are sys-
tematically longer than those found in OnWN and
headlines. This result suggests that the improvement
due to distributional measures is more significant in
longer texts than in the shorter ones.
Lastly, it is also important to notice that
the STSqgrams text-similarity function obtained
mean = 0.5570, which proved again to be a very
strong text-overlapping baseline for the STS task.
9 Conclusions
We participated in the CORE-STS shared task in
*SEM 2013 with satisfactory results obtaining po-
sitions 18th, 22nd, and 23rd in the official ranking.
Our systems were based on a new parameterized
resemblance coefficient derived from the Tversky?s
199
ratio model in combination with the soft cardinal-
ity. The three proposed text-similarity functions
used q-grams overlapping and distributional mea-
sures obtained from the ukWack corpus. These text-
similarity functions would have been attained posi-
tions 6th, 7th and 8th in the official ranking, besides
a simple average of them would have reached the
4thplace. Another important conclusion was that the
plain text-overlapping method was consistently im-
proved by the incremental use of the proposed distri-
butional measures. This result was most noticeable
in long texts.
In conclusion, the proposed text-similarity func-
tions proved to be competitive despite their simplic-
ity and the few resources used.
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The Section 2 was proposed during the first author?s
internship at Microsoft Research in 2012. The third
author recognizes the support from Mexican Gov-
ernment (SNI, COFAA-IPN, SIP 20131702, CONA-
CYT 50206-H) and CONACYT?DST India (proj.
122030 ?Answer Validation through Textual Entail-
ment?). Entailment?).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, NAACL ?09,
pages 19?27, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre Aitor. 2012. SemEval-2012 task 6: A pilot on
semantic textual similarity. In Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval@*SEM 2012), Montreal,Canada. Association
for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. Atlanta, Georgia, USA. Association
for Computational Linguistics.
Daniel B?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: computing semantic textual
similarity by combining multiple content similarity
measures. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval *SEM
2012), Montreal, Canada. Association for Computa-
tional Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language resources and evaluation,
43(3):209?226.
Danushka Bollegala, Yutaka Matsuto, and Mitsuru
Ishizuka. 2007. Measuring semantic similarity be-
tween words using web search engines. In Proceed-
ings of the 16th international conference on World
Wide Web, WWW ?07, pages 757?766, New York,
NY, USA. ACM.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Sergio Jimenez and Alexander Gelbukh. 2012. Baselines
for natural language processing tasks. Appl. Comput.
Math., 11(2):180?199.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardinality.
In Edgar Chavez and Stefano Lonardi, editors, String
Processing and Information Retrieval, volume 6393 of
LNCS, pages 297?302. Springer, Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality: A parameterized sim-
ilarity function for text comparison. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval *SEM 2012), Montreal, Canada.
Karen Sp?rck Jones. 2004. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 60(5):493?502, October.
Michael D Lee, B.M. Pincombe, and Matthew Welsh.
2005. An empirical evaluation of models of text docu-
ment similarity. IN COGSCI2005, pages 1254?1259.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
200
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In In AAAI?06, pages 775?
780.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity: measuring the re-
latedness of concepts. In Proceedings HLT-NAACL?
Demonstration Papers, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
Phillip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95?130.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627?633, October.
Frane ?aric, Goran Glava?, Mladen Karan, Jan ?najder,
and Bojana Dalbelo Ba?ic. 2012. TakeLab: systems
for measuring semantic text similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval *SEM 2012), Montreal, Canada. As-
sociation for Computational Linguistics.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Luc De Raedt and
Peter Flach, editors, Machine Learning: ECML 2001,
number 2167 in Lecture Notes in Computer Science,
pages 491?502. Springer Berlin Heidelberg, January.
Amos Tversky. 1977. Features of similarity. Psycholog-
ical Review, 84(4):327?352, July.
I.H. Witten and E. Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann Publishers Inc., San Francisto, CA, 2nd
edition.
201
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 34?38, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SOFTCARDINALITY: Learning to Identify Directional
Cross-Lingual Entailment from Cardinalities and SMT
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, Av. Mendiz?bal,
Col. Nueva Industrial Vallejo
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
In this paper we describe our system submit-
ted for evaluation in the CLTE-SemEval-2013
task, which achieved the best results in two
of the four data sets, and finished third in av-
erage. This system consists of a SVM clas-
sifier with features extracted from texts (and
their translations SMT) based on a cardinality
function. Such function was the soft cardinal-
ity. Furthermore, this system was simplified
by providing a single model for the 4 pairs
of languages obtaining better (unofficial) re-
sults than separate models for each language
pair. We also evaluated the use of additional
circular-pivoting translations achieving results
6.14% above the best official results.
1 Introduction
The Cross-Lingual Textual Entailment (CLTE) task
consists in determining the type of directional en-
tailment (i.e. forward, backward, bidirectional or
no-entailment) between a pair of texts T1 and T2,
each one written in different languages (Negri et al,
2013). The texts and reference annotations for this
task were obtained through crowdsourcing applied
to simpler sub-tasks (Negri et al, 2011). CLTE has
as main applications content synchronization and
aggregation in different languages (Mehdad et al,
2012; Duh et al, 2013). We participated in the first
evaluation of this task in 2012 (Negri et al, 2012),
achieving third place on average among 29 partici-
pating systems (Jimenez et al, 2012).
Since in the CLTE task text pairs are in different
languages, in our system, all comparisons made be-
tween two texts imply that one of them was written
by a human and the other is a translation provided by
statistical machine translation (SMT). Our approach
is based on an SVM classifier (Cortes and Vapnik,
1995) whose features were cardinalities combined
with similarity scores. That system was motivated
by the fact that most text similarity functions are
symmetric, e.g. Edit Distance (Levenshtein, 1966),
longest common sub-sequence (Hirschberg, 1977),
Jaro-Winkler similarity (Winkler, 1990), cosine sim-
ilarity (Salton et al, 1975). Thus, the use of these
functions as only resource seems counter-intuitive
since CLTE task is asymmetric for the forward and
backward entailment classes.
Moreover, cardinality is the central component of
the resemblance coefficients such as Jaccard, Dice,
overlap, etc. For instance, if T1 and T2 are texts
represented as bag of words, it is only necessary to
know the cardinalities |T1|, |T2| and |T1 ? T2| to ob-
tain a similarity score using a resemblance coeffi-
cient such as the Dice?s coefficient (i.e. 2 ? |T1 ?
T2|/(|T1| + |T2|)). Therefore, the idea is to use the
individual cardinalities to enrich a set of features ex-
tracted from texts.
Cardinality gives a rough idea of the amount of
information in a collection of elements (i.e. words)
providing the number of different elements therein.
That is, in a collection of elements whose majority
are repetitions contains less information than a col-
lection whose elements are mostly different. How-
ever, the classical sets cardinality is a rigid mea-
sure as do not take account the degree of similarity
among the elements. Unlike the sets cardinality, soft
cardinality (Jimenez et al, 2010) uses the similari-
ties among the elements providing a more flexible
34
measurement of the amount of information in a col-
lection. In the 2012 CLTE evaluation campaign, it
was noted that the soft cardinality overcame classi-
cal cardinality in the task at hand. All the models
used in our participation and proposed in this paper
are based on the soft cardinality. A brief descrip-
tion of the soft cardinality is presented in Section 2,
along with a description of the functions used to pro-
vide the similarities between words. Besides, the set
of features that are derived from all pairs of texts and
their cardinalities are presented in Section 3.
Section 4 provides a detailed description for each
of the 4 models (one for each language pair) used
to get the predictions submitted for evaluation. In
Section 5 a simplified-multilingual model is tested
with several word-similarity functions and circular-
pivoting translations.
In sections 6 and 7 a brief discussion of the results
and conclusions of our participation in this evalua-
tion campaign are presented.
2 Soft Cardinality
The soft cardinality (Jimenez et al, 2010) of a col-
lection of words T is calculated with the following
expression:
|T |? =
n?
i=1
wi
?
?
n?
j=1
sim(ti, tj)p
?
?
?1
(1)
Having T ={t1, t2, . . . , tn}; wi ? 0; p ? 0; 1 >
sim(x, y) ? 0, x 6= y; and sim(x, x) = 1. The
parameter p controls the degree of "softness" of the
cardinality (the larger the ?harder?). The coefficients
wi are weights associated with each word (or term)
t, which can represent the importance or informative
character of each word (e.g. idf weights). The func-
tion sim is a word-similarity function. Three such
functions are considered in this paper:
Q-grams: each word ai is represented as a col-
lection of character q-grams (Kukich, 1992). In-
stead of single length q-grams, a combination of
a range of lengths q1 to q2 was used. Next,
a couple of words are compared with the fol-
lowing resemblance coefficient: sim(ti, tj) =
|ti?tj |+bias
??max(|ti|,|tj |)+(1??)?min(|ti|,|tj |)
. The parameters of
this word-similarity function are q1, q2, ? and bias.
Group 1: basic cardinalities
#1 |T1|? #4 |T1 ? T2|?
#2 |T2|? #5 |T1 ? T2|?
#3 |T1 ? T2|? #6 |T2 ? T1|?
Group 2: asymmetrical ratios
#7 |T1?T2|
?/|T1|? #8 |T1?T2|
?/|T2|?
Group 3: similarity and arithmetical* scores
#9 |T1?T2|
?/|T1?T2|? #10
2?|T1?T2|
?
|T1|?+|T2|?
#11 |T1?T2|
?/
?
|T1|??|T2|? #12
|T!?T2|
?
min[|T1|?,|T2|?]
#13 |T1?T2|
?+|T1|
?+|T2|
?
2?|T1|??|T2|?
#14* |T1|? ? |T2|?
Table 1: Set of features derived from texts T1 and T2
Edit-Distance: a similarity score for a pair of
words can be obtained from their Edit Distance
(Levenshtein, 1966) by normalizing and converting
distance to similarity with the following expression:
sim(ti, tj) = 1?
EditDistance(ti,tj)
max[len(ti),len(tj)]
.
Jaro-Winkler: this measure is based on the Jaro
(1989) similarity, which is given by this expression
Jaro(ti, tj) = 13
(
c
len(ti)
+ clen(tj) +
c?m
c
)
, where c
is the number of characters in common within a slid-
ing window of length max[len(ti),len(tj)]2 ?1. To avoid
division by 0, when c = 0 then Jaro(ti, tj) = 0. The
number of transpositions m is obtained sorting the
common characters according to their occurrence
in each of the words and counting the number of
non-matching characters. Winkler (1990) proposed
an extension to this measure taking into account
the common prefix length l through this expression:
sim(ti, tj) = Jaro(ti, tj) + l10 (1? Jaro(ti, tj)).
3 Features from Cardinalities
For a pair of texts T1 and T2 represented as bags
of words three basic soft cardinalities can be cal-
culated: |T1|?, |T2|? and |T1 ? T2|?. The soft car-
dinality of their union is calculated using the con-
catenation of T1 and T2. More additional features
can be derived from these three basic features, e.g.
|T1?T2|? = |T1|?+|T2|??|T1?T2|? and |T1?T2|? =
|T1|?? |T1 ? T2|?. The complete set of features clas-
sified into three groups are shown in Table 1.
4 Submitted Runs Description
The data for the 2013 CLTE task consists of 4 data
sets (spa-eng, ita-eng, fra-eng and deu-eng) each
35
Data set q1 q2 ? bias
deu-eng 2 2 0.5 0.0
fra-eng 2 3 0.5 0.0
ita-eng 2 4 0.6 0.0
spa-eng 1 3 0.5 0.1
Table 2: Parameters of the q-grams word-similarity func-
tion for each language pair
with 1,000 pairs of texts for training and 500 for
testing. For each pair of texts T1 and T2 written
in two different languages, two translations are pro-
vided using the Google?s translator1. Thus, T t1 is a
translation of T1 into the language of T2 and T t2 is
a translation of T2 into the language of T1. Using
these pivoting translations, two pairs of texts can be
compared: T1 with T t2 and T
t
1 with T2.
Then all training and testing texts and their trans-
lations were pre-processed with the following se-
quence of actions: i) text strings were tokenized,
ii) uppercase characters are converted into lower-
case equivalents, iii) stop words were removed, iv)
punctuation marks were removed, and v) words were
stemmed using the Snowball2 multilingual stem-
mers provided by the NLTK Toolkit (Loper and
Bird, 2002). Then every stemmed word is tagged
with its idf weight (Jones, 2004) calculated with the
complete collection of texts and translations in the
same language.
Five instances of the soft cardinality are provided
using 1, 2, 3, 4 and 5 as values of the parameter
p. Therefore, the total number of features for each
pair of texts is the multiplication of the number of
features in the feature set (i.e. 14, see Table 1) by
the number of soft cardinality functions (5) and by 2,
corresponding to the two pairs of comparable texts.
That is, 14? 5? 2 = 140 features.
The sim function used was q-grams, whose pa-
rameters were adjusted for each language pair.
These parameters, which are shown in Table 2, were
obtained by manual exploration using the training
data.
Four vector data sets for training (one for each
language pair) were built by extracting the 140 fea-
tures from the 1,000 training instances and using
1https://translate.google.com
2http://snowball.tartarus.org
ECNUCS-team?s system
spa-eng ita-eng fra-eng deu-eng average
run4 0.422 0.416 0.436 0.452 0.432
run3 0.408 0.426 0.458 0.432 0.431
SOFTCARDINALITY-team?s system
spa-eng ita-eng fra-eng deu-eng average
run1 0.434 0.454 0.416 0.414 0.430
run2 0.432 0.448 0.426 0.402 0.427
Table 3: Official results for our system and the top per-
forming system ECNUCS (accuracies)
their gold-standard annotations as class attribute.
Predictions for the 500 test cases were obtained
through a SVM classifier trained with each data set.
For the submitted run1, this SVM classifier used a
linear kernel with its complexity parameter set to its
default value C = 1. For the run2, this parameter
was adjusted for each pair of languages with the fol-
lowing values: Cspa?eng = 2.0, Cita?eng = 1.5,
Cfra?eng = 2.3 and Cdeu?eng = 2.0. The imple-
mentation of the SVM used is that which is available
in WEKA v.3.6.9 (SMO) (Hall et al, 2009). Official
results for run1, run2 and best accuracies obtained
among all participant systems are shown in Table 3.
5 A Single Multilingual Model
This section presents the results of our additional ex-
periments in search for a simplified model and in
turn to respond to the following questions: i) Can
one simplified-multilingual model overcome the ap-
proach presented in Section 4? ii) Does using addi-
tional circular-pivoting translations improve perfor-
mance? and iii) Do other word-similarity functions
work better than the q-grams measure?
First, it is important to note that the approach
described in Section 4 used only patterns discov-
ered in cardinalities. This means, that no language-
dependent features was used, with the exception of
the stemmers. Therefore, we wonder whether the
patterns discovered in a pair of languages can be use-
ful in other language pairs. To answer this question,
a single prediction model was built by aggregating
instances from each of the vector data sets into one
data set with 4,000 training instances. Afterward,
this model was used to provide predictions for the
2,000 test cases.
36
Moreover, customization for each pair of lan-
guages in the word-similarity function, which is
show in Table 2, was set on the following unique set
of parameters: q1 = 1, q2 = 3, ? = 0.5, bias = 0.0.
Thus, the words are compared using q-grams and
the Dice coefficient. In addition to the measure of
q-grams, two "off-the-shelf" measures were used as
nonparametric alternatives, namely: Edit Distance
(Levenshtein, 1966) and the Jaro-Winkler similarity
(Winkler, 1990).
In another attempt to simplify this model, we
evaluated the predictive ability of each of the three
groups of features shown in Table 1. The combi-
nation of groups 2 and 3, consistently obtained bet-
ter results when the evaluation with 10 fold cross-
validation was used in the training data. This result
was consistent with the simple training versus test
data evaluation. The sum of all previous simplifica-
tions significantly reduced the number of parameters
and features in comparison with the model described
in Section 4. That is, only one SVM and 4 parame-
ters, namely: ?, bias, q1 and q2.
Besides, the additional use of circular-pivoting
translations was tested. In the original model, for
every pair of texts (T1, T2) their pivot translations
(T t1 , T
t
2) were provided allowing the calculation of
|T1 ? T t2| and |T
t
1 ? T2|. Translations T
t
1 and T
t
2 can
also be translated back to their original languages
obtaining T tt1 and T
tt
2 . These additional transla-
tions in turn allows the calculation of |T tt1 ? T
t
2|
and |T t1 ? T
tt
2 |. This procedure can be repeated
again to obtain T ttt1 and T
ttt
2 , which in turn provides
|T1 ? T ttt2 |, |T
ttt
1 ? T2|, |T
tt
1 ? T
ttt
2 | and |T
ttt
1 ? T
tt
2 |.
The original feature set is denoted as t. The extended
feature sets using double-pivoting translations and
triple-pivot translations are denoted respectively as
tt and ttt.
The results obtained with this simplified model
using single, double and triple pivot translations are
shown in Table 4. The first column indicates the
word-similarity function used by the soft cardinal-
ity and the second column indicates the number of
pivoting translations.
6 Discussion
In spite of the customization of the parameter C in
the run2, the run1 obtained better results than run2
Soft C. #t spa-e ita-e fra-e deu-e avg.
Ed.Dist. t 0.444 0.450 0.440 0.410 0.436
Ed.Dist. tt 0.452 0.464 0.434 0.432 0.446
Ed.Dist. ttt 0.464 0.468 0.440 0.424 0.449
Jaro-W. t 0.422 0.450 0.426 0.406 0.426
Jaro-W. tt 0.430 0.456 0.444 0.400 0.433
Jaro-W. ttt 0.426 0.458 0.430 0.430 0.436
q-grams t 0.428 0.456 0.456 0.432 0.443
q-grams tt 0.436 0.478 0.444 0.430 0.447
q-grams ttt 0.452 0.474 0.464 0.442 0.458
Table 4: Single-multilingual model results (accuracies)
(see Table 3). This result indicates that the simpler
model produced better predictions in unseen data.
It is also important to note that two of the three
multilingual systems proposed in Section 5 achieved
higher scores than the best official results (see rows
containing ?t? in Table 4). This indicates that the
proposed simplified model is able to discover pat-
terns in the cardinalities of a pair of languages and
project them into the other language pairs.
Regarding the use of additional circular-pivoting
translations, Table 4 shows that t was overcome on
average by tt and tt by ttt in all cases of the three
sets of results. The relative improvement obtained
by comparing t versus ttt for each group was 3.0% in
Edit Distance, 2.3% for Jaro-Winkler and 3.4% for
the q-gram measure. This same trend holds roughly
for each language pair.
7 Conclusions
We described the SOFTCARDINALITY system
that participated in the SemEval CLTE evaluation
campaign in 2013, obtaining the best results in data
sets spa-eng and ita-eng, and achieving the third
place on average. This result was obtained using
separate models for each language pair. It was also
concluded that a single-multilingual model outper-
forms that approach. Besides, we found that the
use of additional pivoting translations provide bet-
ter results. Finally, the measure based on q-grams of
characters, used within the soft cardinality, resulted
to be the best option among other measures of word
similarity. In conclusion, the soft cardinality method
used in combination with SMT and SVM classifiers
is a competitive method for the CLTE task.
37
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The third author recognizes the support from Mexi-
can Government (SNI, COFAA-IPN, SIP 20131702,
CONACYT 50206-H) and CONACYT?DST India
(proj. 122030 ?Answer Validation through Textual
Entailment?).
References
Corinna Cortes and Vladimir N. Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Kevin Duh, Ching-Man Au Yeung, Tomoharu Iwata, and
Masaaki Nagata. 2013. Managing information dispar-
ity in multilingual document collections. ACM Trans.
Speech Lang. Process., 10(1):1:1?1:28, March.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Daniel S. Hirschberg. 1977. Algorithms for the longest
common subsequence problem. J. ACM, 24(4):664?
675, October.
M.A. Jaro. 1989. Advances in record-linkage methodol-
ogy as applied to matching the 1985 census of tampa,
florida. Journal of the American Statistical Associa-
tion, pages 414?420, June.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardinality.
In Edgar Chavez and Stefano Lonardi, editors, String
Processing and Information Retrieval, volume 6393 of
LNCS, pages 297?302. Springer, Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality+ ML: learning adaptive
similarity functions for cross-lingual textual entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval, *SEM 2012),
Montreal, Canada. ACL.
Karen Sp?rck Jones. 2004. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 60(5):493?502, October.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys,
24:377?439, December.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia. Association for Computa-
tional Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting semantic equivalence and informa-
tion disparity in cross-lingual documents. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume 2,
ACL ?12, page 120?124, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, page 670?679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
2012. semeval-2012 task 8: Cross-lingual textual en-
tailment for content synchronization. In Proceedings
of the 6th International Workshop on Semantic Evalu-
ation (SemEval 2012), Montreal, Canada.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
and Luisa Bentivogli. 2013. Semeval-2013 task
8: Cross-lingual textual entailment for content syn-
chronization. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Gerard Salton, Andrew K. C. Wong, and Chung-Shu
Yang. 1975. A vector space model for automatic in-
dexing. Commun. ACM, 18(11):613?620.
William E. Winkler. 1990. String comparator metrics
and enhanced decision rules in the fellegi-sunter model
of record linkage. In Proceedings of the Section on
Survey Research Methods, pages 354?359. American
Statistical Association.
38
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 114?117, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UNAL: Discriminating between Literal and Figurative
Phrasal Usage Using Distributional Statistics and POS tags
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, Av. Mendiz?bal,
Col. Nueva Industrial Vallejo
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
In this paper we describe the system used to
participate in the sub task 5b in the Phrasal Se-
mantics challenge (task 5) in SemEval 2013.
This sub task consists in discriminating lit-
eral and figurative usage of phrases with
compositional and non-compositional mean-
ings in context. The proposed approach is
based on part-of-speech tags, stylistic features
and distributional statistics gathered from the
same development-training-test text collec-
tion. The system obtained a relative improve-
ment in accuracy against the most-frequent-
class baseline of 49.8% in the ?unseen con-
texts? (LexSample) setting and 8.5% in ?un-
seen phrases? (AllWords).
1 Introduction
The Phrasal Semantics task-5b in SemEval 2013
consisted in the discrimination of literal of figura-
tive usage of phrases in context (Korkontzelos et al,
2013). For instance, the occurrence in a text of the
phrase ?a piece of cake? can be used whether to re-
fer to something that is pretty easy or to an actual
piece of cake. The motivation for this task is that
such discrimination could improve the quality and
performance of other tasks like machine translation
and information retrieval.
This problem has been studied in the past. Lin
(1999) observed that the distributional characteris-
tics of the literal and figurative usage are different.
Katz and Giesbrecht (2006) showed that the similar-
ities among contexts are correlated with their literal
or figurative usage. Birke and Sarkar (2006) clus-
tered literal and figurative contexts using a word-
sense-disambiguation approach. Fazly et al (2009)
showed that literal and figurative usages are related
to particular syntactical forms. Sporleder and Li
(2009) showed that for a particular phrase the con-
texts of its literal usages are more cohesive than
those of its figurative usages. Inspired by these
works and in a new observation, we proposed a set or
features based on cohesiveness, syntax and stylom-
etry (Section 2), which are used to train a machine
learning classifier.
The cohesiveness between a phrase an its context
can be measured aggregating the relatedness of the
context words against the target phrase. This cohe-
siveness should be high for phrases used literally.
Conversely, figurative usages can occur in a large
variety of contexts implying low cohesiveness. For
instance, the cohesiveness of the phrase ?a piece of
cake? against context words such as ?coffee?, ?birth-
day? and ?bakery? should be high. The distribu-
tional measures used to obtain the needed related-
ness scores and the proposed measures of cohesive-
ness are presented in subsection 2.1.
Moreover, we observed a stylistic trend in the
training data set. That is, figurative usage tends to
occur later in the document in comparison with the
literal usage. Consequently, a small set of features
that exploits this particular observation is proposed
in subsection 2.2.
Fazly et al (2009) showed that idiomatic phrases
composed of a verb and a noun (e.g. ?break a leg?)
differ from their literal usages in the use of some
syntactic structures. For instance, idiomatic phrases
are less flexible in the use of determiners, pluraliza-
114
tion and passivization. In order to capture that no-
tion in a simple way, a set of features form a part-
of-speech tagger was included in the feature set (see
subsection 2.3).
In Section, additional details of the proposed sys-
tem are provided jointly with the obtained official
results. Finally, in sections 4 and 5 a brief discus-
sion of the results and some concluding remarks are
presented.
2 Features
Each instance of the training and test sets consist of a
short document d where one or more occurrences of
its target phase pd are annotated. For each particular
phrase p, several instances are provided correspond-
ing to literal or figurative usages. In this section, the
set of features that was extracted from each instance
to provide a vectorial representation is presented.
2.1 Cohesiveness Features
Let?s start with some definitions borrowed from the
information retrieval field: D is a collection of doc-
uments, df(w) is the number of documents in D
where the word w occurs (document frequency),
df(w ? pd) is the number of documents where w
and a target phrase pd co-occur, tf(w, d) is the num-
ber of occurrences of w in a document d ? D (term
frequency), and idf(w) = log2
df(w)
|D| is the inverse
document frequency of w (Jones, 2004).
A simple distributional measure of relatedness be-
tween w and p can be obtained with the following
ratio:
R(w, p) =
df(w ? pd)
df(w)
(1)
Pointwise mutual information (PMI) (Church and
Hanks, 1990) is another distributional measure that
can be used for measuring the relatedness of w and
p. The probabilities needed for its calculation can be
obtained by maximum likelihood estimation (MLE):
P (w) ? df(w)|D| , P (pd) ?
df(pd)
|D| and P (w ? pd) ?
df(w?pd)
|D| .
Thus, PMI is given by this expression:
PMI(w, pd) = log2
(
P (w ? pd)
P (w) ? P (pd)
)
(2)
F1:
?
w?d? R(w, pd)
F2:
?
w?d? tf(w, d)
F3:
?
w?d? idf(w)
F4:
?
w?d? PMI(w, pd)
F5:
?
w?d? NPMI(w, pd)
F6:
?
w?d? (tf(w,d) ? R(w, pd))
F7:
?
w?d? (idf(w) ? R(w, pd))
F8
?
w?d? (R(w, pd) ? PMI(w, pd))
F9:
?
w?d? (R(w, pd) ?NPMI(w, pd))
F10:
?
w?d? (tf(w, d) ? idf(w))
F11:
?
w?d? (tf(w, pd) ? PMI(w, pd))
F12:
?
w?d? (tf(w, pd) ?NPMI(w, pd))
F13:
?
w?d? (idf(w) ? PMI(w, pd))
F14:
?
w?d? (idf(w) ?NPMI(w, pd))
F15:
?
w?d? (PMI(w, pd) ?NPMI(w, pd))
F16:
?
w?d? (tf(w, d) ? idf(w) ? R(w,pd))
F17:
?
w?d? (tf(w, d) ? R(w, pd) ? PMI(w, pd))
F18:
?
w?d? (tf(w, d) ? R(w, pd) ?NPMI(w, pd))
F19:
?
w?d? (tf(w, d) ? idf(w) ? PMI(w,pd))
F20:
?
w?d? (tf(w, d) ? idf(w) ?NPMI(w,pd))
Table 1: Cohesiveness features
Furthermore, the scores obtained through eq. 2
can be normalized in the interval [+2,0] with the fol-
lowing expression:
NPMI(w, pd) =
PMI(w, pd)
? log2(P (w ? pd))
+ 1 (3)
A measure of the cohesiveness between a docu-
ment d against its target phrase pd, can be obtained
by aggregating the pairwise relatedness scores be-
tween all the words in d and pd. For instance, us-
ing eq. 1 that measure is
?
w?d? R(w, pd), where d
?
is the set of different words in d. The equations 1,
2 and 3 can be used as weights associated to each
word, which can also be combined among them and
with tf and idf weights. Such weight combinations
produce measures that can be used as cohesiveness
features for a document. The set of 20 features ob-
tained using this approach is shown in Table 1.
2.2 Stylistic Features
The set of stylistic features related to the document
length, vocabulary size and relative position of the
occurrence of the target phrase in a document is
shown in Table 2.
115
F21: Relative position of pd in d
F22: Document length in characters
F23: Document length in tokens
F24: Number of different words
Table 2: Stylistic features
2.3 Syntactic Features
The features F25 to F67 correspond to the set of 43
part-of-speech tags of the NLTK English POS tag-
ger (Loper and Bird, 2002). Each feature contains
the frequency of occurrence of each POS-tag in a
document d.
3 Experimental Setup and Results
The data provided for this task consists of two data
sets LexSample and AllWords, which are divided
into development, training and test sets. Neverthe-
less, we considered a single training set aggregat-
ing the development and training parts from both
data sets for a total of 3,230 instances. Each train-
ing instance has a class label whether ?literally? or
?figuratively? depending on the usage or the tar-
get phrase. Similarly, the aggregated test set con-
tains 1,112 instances, but with unknown values in
the class attribute.
Firstly, the syntactic features for each text were
obtained using the POS tagger included in the NLTK
v.2.0.4 (Loper and Bird, 2002). Secondly, all texts
were preprocessed by tokenizing, lowecasing, stop-
word removing, punctuation removing and stem-
ming using the Porter?s algorithm (1980). This pre-
processed version of the texts was used to obtain the
remaining cohesiveness and stylistic features. The
resulting vectorial data set was used to produce the
predictions labeled ?UNAL.RUN1? through a Lo-
gistic classifier (Cessie and Houwelingen, 1992).
The implementation used for this classifier was the
included in WEKA v.3.6.9 (Hall et al, 2009). The
accuracies obtained by the different feature groups
in the training set using 10-fold cross validation are
shown in Table 3. The last column shows the per-
centage of relative improvement of different feature
sets combinations from the most frequent class base-
line to our best system using all features.
The predictions labeled ?UNAL.RUN2? were ob-
tained with the same vectorial data set but adding
Features Accuracy % improv.
All features 0.7272 100.0%
Cohesiveness+Syntactic 0.7034 87.1%
Cohesiveness 0.6833 76.2%
Syntactic 0.6229 43.5%
Stylistic 0.5492 3.5%
Baseline MFC 0.5427 0.0%
Table 3: Results by group of features in the training set
using 10-fold cross validation
System LexSample AllWords Both
UNAL.RUN1 0.7222 0.6680 0.6970
UNAL.RUN2 0.7542 0.6448 0.7032
Baseline MFC 0.5034 0.6158 0.5558
Best SemEval?13 0.7795 0.6680 0.7276
# test instances 594 518 1,112
Table 4: Official results in the test set (accuracy)
as a nominal feature the target phrase of each in-
stance. The official results obtained by both sub-
mitted runs are shown in Table 4. Note that official
results in the test set are reported separately for the
data sets LexSample and AllWords. The LexSample
test set contains instances whose target phrases were
seen in the training set (i.e. unseen contexts). Un-
like LexSample, AllWords contains instances whose
target phrases were unseen in the training set (i.e.
unseen phrases).
4 Discussion
As it was expected, the results obtained in the ?un-
seen context? setting were consistently better than
in ?unseen phrases?. This result suggests that the
discrimination of literal and figurative usage heavily
depends on particular idiomatic phrases. This can
also be confirmed by the best accuracy obtained by
RUN2 compared with RUN1 in LexSample. Clearly,
the classifier used in RUN2 exploited the identifica-
tion of the phrase to leverage a priori information
about the phrase such as the most frequent usage.
Another factor that could undermine the results in
the ?unseen phrases? setting is the low number of in-
stances per phrase in the AllWords test set, roughly a
third in comparison with LexSample. Given that the
effectiveness of the cohesiveness features depends
116
on the number of documents where the idiomatic
phrase occurs, the predictions for this test set relied
mainly on the less effective features, namely syn-
tactic and stylistic features (see Table 3). However,
this problem could be alleviated obtaining the distri-
butional statistics from a large corpus with enough
occurrences of the unseen phrases.
Besides it is important to note, that in spite of the
low individual contribution of the stylistic features
to the overall accuracy (3.5%), when these are com-
bined with the remaining features they provide an
improvement of 12.9% (see Table 3).
5 Conclusions
We participated in the Phrasal Semantics sub task 5b
in SemEval 2013. Our system proved the effective-
ness of the use of cohesiveness, stylistic and syn-
tactic features for discriminating literal from figura-
tive usage of idiomatic phrases. The most-frequent-
class baseline was overcame by 49.8% in the ?un-
seen contexts? setting (LexSample) and 8.5% in ?un-
seen phrases? (AllWords).
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The third author recognizes the support from Mexi-
can Government (SNI, COFAA-IPN, SIP 20131702,
CONACYT 50206-H) and CONACYT?DST India
(proj. 122030 ?Answer Validation through Textual
Entailment?).
References
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of nonlit-
eral language. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics, Trento, Italy.
S. Le Cessie and J. C. Van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied Statistics,
41(1):191.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Comput. Linguist., 16(1):22?29, March.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identifica-
tion of idiomatic expressions. Comput. Linguist.,
35(1):61?103, March.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Karen Sp?rck Jones. 2004. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 60(5):493?502, October.
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
?06, pages 12?19, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ioannis Korkontzelos, Torsten Zesch, Fabio Massimo
Zanzotto, and Chris Biemann. 2013. SemEval-2013
task 5: Evaluating phrasal semantics. In Proceedings
of the 7th International Workshop on Semantic Evalu-
ation (SemEval 2013).
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, ACL ?99,
page 317?324, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia. Association for Computa-
tional Linguistics.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. In Proceedings of the 12th Conference of
the European Chapter of the Association for Computa-
tional Linguistics, EACL ?09, page 754?762, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
117
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 280?284, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SOFTCARDINALITY: Hierarchical Text Overlap
for Student Response Analysis
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, Av. Mendiz?bal,
Col. Nueva Industrial Vallejo
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
In this paper we describe our system used to
participate in the Student-Response-Analysis
task-7 at SemEval 2013. This system is based
on text overlap through the soft cardinality and
a new mechanism for weight propagation. Al-
though there are several official performance
measures, taking into account the overall ac-
curacy throughout the two availabe data sets
(Beetle and SciEntsBank), our system ranked
first in the 2 way classification task and sec-
ond in the others. Furthermore, our sys-
tem performs particularly well with ?unseen-
domains? instances, which was the more chal-
lenging test set. This paper also describes an-
other system that integrates this method with
the lexical-overlap baseline provided by the
task organizers obtaining better results than
the best official results. We concluded that the
soft cardinality method is a very competitive
baseline for the automatic evaluation of stu-
dent responses.
1 Introduction
The Student-Response-Analysis (SRA) task consist
in provide assessments of the correctness of student
answers (A), considering their corresponding ques-
tions (Q) and reference answers (RA) (Dzikovska
et al, 2012). SRA is the task-7 in the SemEval
2013 evaluation campaign (Dzikovska et al, 2013).
The method used in our participation was basically
text overlap based on the soft cardinality (Jimenez
et al, 2010) plus a machine learning classifier. This
method did not use any information external to the
data sets except for a stemmer and a list of stop
words.
The soft cardinality is a general model for object
comparison that has been tested at text applications.
Particularly, this text overlap approach has provided
strong baselines for several applications, i.e. entity
resolution (Jimenez et al, 2010), semantic textual
similarity (Jimenez et al, 2012a), cross-lingual tex-
tual entailment (Jimenez et al, 2012b), information
retrieval, textual entailment and paraphrase detec-
tion (Jimenez and Gelbukh, 2012). A brief descrip-
tion of the soft cardinality is presented in the next
section.
The data for SRA consist of two data sets Bee-
tle (5,199 instances) and SciEntsBank (10,804 in-
stances) divided into training and test sets (76%-
24% for Beetle and 46%-54% SciEntsBank). In ad-
dition, the test part of Beetle data set was divided
into two test sets: ?unseen answers? (35%) and ?un-
seen questions? (65%). Similarity, SciEntsBank test
part is divided into ?unseen answers? (9%), ?unseen
questions? (13%) and ?unseen domains? (78%). All
texts are in English.
The challenge consists in predicting for each in-
stance triple (Q, A, RA) an assessment of correct-
ness for the student?s answer. Three levels of detail
are considered for this assessment: 2 way (correct
and incorrect), 3 way (correct, contradictory and in-
correct) and 5 way (correct, incomplete, contradic-
tory, irrelevant and non-in-the-domain).
Section 3 presents the method used for the extrac-
tion of features from texts using the soft cardinal-
ity to provide a vector representation. In Section 4,
the details of the system used to produce our predic-
280
tions are presented. Besides, in that section a system
that integrates our system with the lexical-overlap
baseline proposed by the task organizers is also pre-
sented. This combined system was motivated by the
observation that our system performed well in the
SciEntsBank data set but poorly in Beetle in compar-
ison with the lexical-overlap baseline. The results
obtained by both systems are also presented in that
section.
Finally in Section 5 the conclusions of our partic-
ipation in this evaluation campaign are presented.
2 Soft Cardinality
The soft cardinality (Jimenez et al, 2010) of a col-
lection of elements S is calculated with the follow-
ing expression:
|S|? =
n?
i=1
wi ?
?
?
n?
j=1
sim(si, sj)p
?
?
?1
(1)
Having S ={s1, s2, . . . , sn}; wi ? 0; p ? 0;
1 > sim(x, y) ? 0, x 6= y; and sim(x, x) = 1.
The parameter p controls the degree of "softness"
of the cardinality (the larger the ?harder?). In fact,
when p ? ? the soft cardinality is equivalent to
classical set cardinality. The default value for this
parameter is p = 1. The coefficients wi are weights
associated with each element, which can represent
the importance or informative character of each ele-
ment. The function sim is a similarity function that
compares pairs of elements in the collection S.
3 Features from Cardinalities
It is commonly accepted that it is possible to make
a fair comparison of two objects if they are of the
same nature. If the objects are instances of a com-
positional hierarchy, they should belong to the same
class to be comparable. Clearly, a house is compa-
rable with another house, a wall with another wall
and a brick with another brick, but walls and bricks
are not comparable (at least not directly). Similarly,
in text applications documents should be compared
with documents, sentences with sentences, words
with words, and so on.
However, a comparison measure between a sen-
tence and a document can be obtained with different
approaches. First, using the information retrieval ap-
proach, the document is considered like a very long
sentence and the comparison is then straight for-
ward. Another approach is to make pairwise com-
parisons between the sentence and each sentence in
the document. Then, the similarity scores of these
comparisons can be aggregated in a single score
using average, max or min functions. These ap-
proaches have issues, the former ignores the sen-
tence subdivision of the document and the later ig-
nores the similarities among the sentences in the
document.
In the task at hand, each instance is composed of
a question Q, a student answer A, which are sen-
tences, and a collection of reference answers RA,
which could be considered as a multi-sentence doc-
ument. The soft cardinality can be used to provide
values for |Q|?, |A|?, |RA|?, |Q?A|?, |A?RA|? and
|Q?RA|?. The intersections that involve RA require
a special treatment to tackle the aforementioned is-
sues.
Let?s start defining a word-similarity function.
Two words (or terms) t1 and t2 can be compared di-
viding them into character q-grams (Kukich, 1992).
The representation in q-grams of ti can be denoted
as t[q]i . Similarly, a combined representation us-
ing a range of q-grams of different length can be
denoted as t[q1:q2]i . For instance, if t1 =?home?
then t[2:3]1 ={?ho?,?om?,?me?,?hom?,?ome?}. Thus,
t[q1:q2]1 and t
[q1:q2]
2 representations can be com-
pared using the Dice?s coefficient to build a word-
similarity function:
simwords(t1, t2) =
2 ?
?
?
?t[q1:q2]1 ? t
[q1:q2]
2
?
?
?
?
?
?t[q1:q2]1
?
?
?+
?
?
?t[q1:q2]1
?
?
?
(2)
Note that in eq. 2 the classical set cardinality was
used, i.e |x| means classical cardinality and |x|? soft
cardinality.
The function simwords can be plugged in eq.1 to
obtain the soft cardinality of a sentence S (using uni-
tary weights wi = 1 and p = 1):
|S|? =
|S|?
i=1
?
?
|S|?
j=1
simword(ti, tj)
?
?
?1
(3)
281
|X| |Y | |X ? Y |
BF1: |Q|? BF2: |A|? BF3: |Q ?A|?
BF2: |A|? BF4: |RA|?? BF5: |RA ?A|??
BF1: |Q|? BF4: |RA|?? BF6: |RA ?Q|??
Table 1: Basic feature set
Where ti are the words in the sentence S .
The sentence-soft-cardinality function can be
used to build a sentence-similarity function to com-
pare two sentences S1 and S2 using again the Dice?s
coefficient:
simsent.(S1, S2) =
2 ? (|S1|? + |S2|? ? |S1 ? S2|?)
|S1|+ |S2|
(4)
In this formulation S1?S2 is the concatenation of
both sentences.
The eq. 4 can be plugged again into eq. 1 to obtain
the soft cardinality of a ?document? RA, which is a
collection of sentences RA = {S1, S2. . . . , S|RA|}:
|RA|?? =
|RA|?
i=1
|Si|
? ?
?
?
|RA|?
j=1
sim(Si, Sj)
?
?
?1
(5)
Note that the soft cardinalities of the sentences
|Si|? were re-used as importance weights wi in eq.
1. These weights are propagations of the unitary
weights assigned to the words, which in turn were
aggregated by the soft cardinality at sentence level
(eq. 3). This soft cardinality is denoted with double
apostrophe because is a function recursively based
in the single-apostrophized soft cardinality.
The proposed soft cardinality expressions are
used to obtain the basic feature set presented in Ta-
ble 1. The soft cardinalities of |Q|?, |A|? and |Q?A|?
are calculated with eq. 3. The soft cardinalities
|RA|??, |RA?A|?? and |RA?Q|?? are calculated with
eq. 5. Recall that Q ? A is the concatenation of the
question and answer sentences. Similarly, RA ? A
and RA ?Q are the collection of reference answers
adding A xor Q .
Starting from the basic feature set, an extended
set, showed in Table 2, can be obtained from each
one of the three rows in Table 1. Recall that |X ?
Y | = |X|+ |Y |?|X?Y | and |X \Y | = |X|?|X?
EF1: |X ? Y | EF2: |X \ Y |
EF3: |Y \X| EF4: |X?Y ||X|
EF5:
|X?Y |
|Y | EF6:
|X?Y |
|X?Y |
EF7:
2?|X?Y |
|X|+|Y | EF8:
|X?Y |?
|X|?|Y |
EF9:
|X?Y |
min(|X|,|Y |) EF10:
|X?Y |
max(|X|,|Y |)
EF11:
|X?Y |?(|X|+|Y |)
2?|X|?|Y | EF12: |X ? Y | ? |X ? Y |
Table 2: Extended feature set
Y |. Consequently, the total number of features is 6
basic features plus 12 extended features multiplied
by 3, i.e. 42 features.
4 Systems Description
4.1 Submitted System
First, each text in the SRA data was preprocessed by
tokenizing, lowercasing, stop-words1 removing and
stemming with the Porter?s algorithm (Porter, 1980).
Second, each stemmed word t was represented in
q-grams: t[3:4] for Beetle and t[4] for SciEntsBank.
These representations obtained the best accuracies
in the training data sets.
Two vector data sets were obtained extracting the
42 features?described in Section 3?for each instance
in Beetle and SciEntsBank separately. Then, three
classification models (2 way, 3way and 5 way) were
learned from the training partitions on each vector
data set using a J48 graft tree (Webb, 1999). All
6 resulting classification models were boosted with
15 iterations of bagging (Breiman, 1996). The used
implementation of this classifier was that included
in WEKA v.3.6.9 (Hall et al, 2009). The results
obtained by this system are shown in Table 3 in the
rows labeled with ?Soft Cardinality-run1?.
4.2 An Improved System
At the time when the official results were released,
we observed that our submitted system performed
pretty well in SciEntsBank but poorly in Beetle.
Moreover, the lexical-overlap baseline outperformed
our system in Beetle. Firstly, we decided to include
in our feature set the 8 features of the lexical over-
lap baseline described by Dzikovska et al (2012)
1those provided by nltk.org
282
Beetle SciEntsBank
Task System UA1 UQ2 All UA1 UQ2 UD3 All All Rank
2 way
Soft Cardinality-unofficial 0.797 0.725 0.750 0.717 0.733 0.726 0.726 0.730 -
Soft Cardinality-run1 0.781 0.667 0.707 0.724 0.745 0.711 0.716 0.715 1
ETS-run1 0.811 0.741 0.765 0.722 0.711 0.698 0.702 0.713 2
CU-run1 0.786 0.718 0.742 0.656 0.674 0.693 0.687 0.697 3
Lexical overlap baseline 0.797 0.740 0.760 0.661 0.674 0.676 0.674 0.690 6
3 way
Soft Cardinality-unofficial 0.608 0.532 0.559 0.656 0.671 0.646 0.650 0.634 -
ETS-run1 0.633 0.551 0.580 0.626 0.663 0.632 0.635 0.625 1
Soft Cardinality-run1 0.624 0.453 0.513 0.659 0.652 0.637 0.641 0.618 2
CoMeT-run1 0.731 0.518 0.592 0.713 0.546 0.579 0.587 0.588 3
Lexical overlap baseline 0.595 0.512 0.541 0.556 0.540 0.577 0.570 0.565 8
5way
Soft Cardinality-unofficial 0.572 0.476 0.510 0.552 0.520 0.534 0.534 0.530 -
ETS-run1 0.574 0.560 0.565 0.543 0.532 0.501 0.509 0.519 1
Soft Cardinality-run1 0.576 0.451 0.495 0.544 0.525 0.512 0.517 0.513 2
ETS-run2 0.715 0.621 0.654 0.631 0.401 0.476 0.481 0.512 3
Lexical overlap baseline 0.519 0.480 0.494 0.437 0.413 0.415 0.417 0.430 11
Total number of test instances 439 819 1,258 540 733 4,562 5,835 7,093
TEST SETS: unseen answers1, unseen questions2, unseen domains3.
Table 3: Official results for the top-3 performing systems (among 15), the lexical overlap baseline in the SRA task
SemEval 2013 and unofficial results of the soft cardinality system combined with the lexical overlap (in italics).
Performance measure used: overall accuracy.
(see Text::Similarity::Overlaps2 package for more
details).
Secondly, the lexical overlap baseline aggregates
the pairwise scores between each reference answer
and the student answer by taking the maximum
value of the pairwise scores. So, we decided to use
this aggregation mechanism instead of the aggrega-
tion proposed through eq. 3.
Thirdly, only at that time we realized that, unlike
Beetle, in SciEntsBank all instances have only one
reference answer. Consequently, the only effect of
eq. 5 in SciEntsBank was in the calculation of |RA?
A|?? (and |RA?Q|??) by |X?Y |?? = |X|
?+|Y |?
1+simsent.(X,Y )
.
As a result, this transformation induced a boosting
effect in X?Y making |X?Y |?? ? |X?Y |? for any
X , Y . We decided to use this intersection-boosting
effect not only in RA ? A, RA ? Q, but in Q ?
A. This intersecton boosting effect works similarly
to the Lesk?s measure (Lesk, 1986) included in the
lexical overlap baseline.
The individual effect in the performance of each
2http://search.cpan.org/dist/Text-
Similarity/lib/Text/Similarity/Overlaps.pm
of the previous decisions was positive in all three
cases. The results obtained using an improved
system that implemented those three decisions are
shown in Table 3?in italics. This system would have
obtained the best general overall accuracy in the of-
ficial ranking.
5 Conclusions
We participated in the Student-Response-Analysis
task-7 in SemEval 2013 with a text overlap system
based on the soft cardinality. This system obtained
places 1st (2 way task) and 2nd (3 way and 5 way)
considering the overall accuracy across all data sets
and test sets. Particularly, our system was the best
in the largest and more challenging test set, namely
?unseen domains?. Moreover, we integrated the lex-
ical overlap baseline to our system obtaining even
better results.
As a conclusion, the text overlap method based on
the soft cardinality is very challenging base line for
the SRA task.
283
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The third author recognizes the support from Mexi-
can Government (SNI, COFAA-IPN, SIP 20131702,
CONACYT 50206-H) and CONACYT?DST India
(proj. 122030 ?Answer Validation through Textual
Entailment?).
References
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback for
explanation questions: a dataset and baselines. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACL
HLT ?12, page 200?210, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Myroslava O. Dzikovska, Rodney D. Nielsen, Chris
Brew, Claudia Leacock, Danilo Giampiccolo, Luisa
Bentivogli, Peter Clark, Ido Dagan, and Hoa Trang
Dang. 2013. SemEval-2013 task 7: The joint stu-
dent response analysis and 8th recognizing textual en-
tailment challenge. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013), in conjunction with the Second Joint Confer-
ence on Lexical and Computational Semantcis (*SEM
2013), Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Sergio Jimenez and Alexander Gelbukh. 2012. Baselines
for natural language processing tasks. Appl. Comput.
Math., 11(2):180?199.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardinality.
In Edgar Chavez and Stefano Lonardi, editors, String
Processing and Information Retrieval, volume 6393 of
LNCS, pages 297?302. Springer, Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized simi-
larity function for text comparison. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval, *SEM 2012), Montreal, Canada.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: learning adaptive
similarity functions for cross-lingual textual entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval, *SEM 2012),
Montreal, Canada. ACL.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys,
24:377?439, December.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ?86, page 24?26, New York, NY,
USA. ACM.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
Geoffrey I. Webb. 1999. Decision tree grafting from the
all-tests-but-one partition. In Proceedings of the 16th
international joint conference on Artificial intelligence
- Volume 2, IJCAI?99, pages 702?707, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
284
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 424?427,
Dublin, Ireland, August 23-24, 2014.
MindLab-UNAL: Comparing Metamap and T-mapper for Medical
Concept Extraction in SemEval 2014 Task 7
Alejandro Riveros, Maria De-Arteaga,
Fabio A. Gonz
?
alez and Sergio Jimenez
Universidad Nacional de Colombia
Ciudad Universitaria
Bogot?a, Colombia
[lariverosc,mdeg,fagonzalezo,
sgjimenezv]@unal.edu.co
Henning M?uller
Univ. of Applied Sciences
Western Switzerland, HES-SO
Sierre, Switzerland
henning.mueller@hevs.ch
Abstract
This paper describes our participation in
task 7 of SemEval 2014, which focuses
on analysis of clinical text. The task is
divided into two parts: recognizing men-
tions of concepts that belong to the UMLS
(Unified Medical Language System) se-
mantic group disorders, and mapping each
disorder to a unique UMLS CUI (Concept
Unique Identifier), if possible. For identi-
fying and mapping disorders belonging to
the UMLS meta thesaurus, we explore two
tools: Metamap and T-mapper. Addition-
ally, a Named Entity Recognition system,
based on a maximum entropy model, was
implemented to identify other disorders.
1 Introduction
Clinical texts are unstructured data that, when pro-
cessed properly, can be of great value. Extracting
key information from these documents can make
medical notes more suitable for automatic pro-
cessing. It can also help diagnose patients, struc-
ture their medical histories and optimize other
clinical procedures and research.
The task of identifying mentions to medical
concepts in free text and mapping these mentions
to a knowledge base was recently proposed in
ShARe/CLEF eHealth Evaluation Lab 2013, at-
tracting the attention of several research groups
worldwide (Pradhan et al., 2013). The task 7 in
SemEval 2014 (Pradhan et al., 2014) elaborates
in that previous effort focusing on the recognition
and normalization of named entity mentions be-
longing to the UMLS semantic group disorders.
The paper is organized as follows: in section 2
we briefly present the data, section 3 contains the
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
description of the methods and tools used in our
system. Later, on sections 4 and 5 we provide the
details of the three submitted runs and expose the
official results. Finally, sections 6 and 7 include
discussions on variations that could be done to im-
prove performance and conclusions to be drawn
from our participation in the task.
2 Data Description
The training data for SemEval 2014 Task 7 con-
sists of the ShARe (Shared Annotation Resource)
corpus, which contains clinical notes from MIMIC
II database (Multiparameter Intelligent Monito?
ring in Intensive Care). The data were manually
annotated for disorder mentions, normalized to a
UMLS Concept Unique Identifier when possible,
and marked as CUI-less otherwise.
Four types of reports where found in the cor-
pus: 61 discharge summaries, 54 ECG reports, 42
ECHO reports and 42 radiology reports, for a to-
tal of 199 training documents, each containing se?
veral disorder mentions.
3 Methods Used
3.1 Named-Entity Recognition
Using the Java libraries Apache OpenNLP
1
and
Maxent
2
, a maximum entropy model was im-
plemented for Named Entity Recognition (NER).
Two types of classifiers were built: the first one
using the library?s default configuration, and a se?
cond one including additional features. The de-
fault model includes the following attributes: tar-
get word, two words of context at the left of the
target word, two words of context at the right of
the target word, type of token for target word (cap-
italized word, number, hyphen, commas, etc.), and
type of token for words in the context.
1
http://opennlp.apache.org
2
http://maxent.sourceforge.net/about.html
424
For the enhanced model, we included n-grams
at character level extracted from the target word,
going from two to five characters.
OpenNLP uses the BIO tagging scheme, which
marks each token as either beginning a chunk,
continuing it, or not in a chunk, therefore, this
model cannot identify discontinuous terms. Given
this, we excluded discontinuous term annotations
from the training data, and trained the model with
the resulting corpus.
During the experiments, we also considered
POS (Part of Speech) tags obtained with the
OpenNLP library, POS tags obtained with the
Stanford Java library and the number of charac-
ters in each token. However, we decided not to
include any of these because accuracy decreased
when using them.
3.2 Weirdness Measure
According to preliminary experiments, the cho-
sen enhanced NER method exhibited low preci-
sion, i.e. a high number of false positives. To
deal with this problem we calculated a measure for
the specificity of a candidate named entity with re-
spect to a specialized corpus, this quantity is based
on the weirdness (Ahmad et al., 1999) of the can-
didate words. Having a general corpus C
g
and a
specialized corpus C
s
, where w
g
and w
s
refer to
the number of occurrences of a word w in each
corpus and t
s
and t
g
to the total count of words in
each corpus, the weirdness of a word is defined as
follows:
Weirdness(w) =
w
s
t
s
/
w
g
t
g
Those words that are common to any domain
will very likely have a low weirdness score, while
those with a high weirdness score indicate w is not
used in the general corpus as much as in the spe-
cialized one, meaning it probably corresponds to
specialized vocabulary.
Using around 1000 books from the Guttenberg
Project as the general corpus, and the terms in
UMLS as the specialized corpus, we applied the
weirdness measure to those words that, according
to the NER model, are disorders. By keeping only
those with high weirdness measures, we prevent
our system from tagging words that are not even
medical vocabulary, thus reducing the amount of
false positives.
3.3 Metamap
For identifying and mapping disorders included
in the UMLS meta thesaurus to its corresponding
CUI, we explored two tools. Both of them find
candidates in the document and give the possible
CUIs for each; in both cases, we selected the CUI
that belongs to the UMLS semantic group disor-
ders, as specified in the task description.
The first tool we explored is Metamap. For
processing the documents, we use the following
Metamap features: allow concept gap and word
sense disambiguation.
After processing a document, the results were
filtered, keeping only those tags that were mapped
to a CUI that belongs to one of the following
UMLS semantic types: congenital abnormality,
acquired abnormality, injury or poisoning, patho-
logic function, disease or syndrome, mental or
behavioral dysfunction, cell or molecular dys-
function, experimental model of disease, anato?
mical abnormality, neoplastic process, and signs
or symptoms.
3.4 T-mapper
As an alternative to Metamap we experimented
with T-mapper
3
, an annotation tool developed at
MindLab
4
that works in languages different than
English and with any knowledge source (i.e. not
only UMLS). The method implemented by T-
mapper is inspired by the one in Metamap, with
some modifications. The method works as fol-
lows:
1. Indexing and vocabulary generation: an in-
verted index and other data structures are
built to perform fast lookups over the dictio-
nary and the vocabulary list in C
g
and C
s
.
2. Sentence detection and tokenization: the in-
put text is divided into sentences and then
each sentence is divided into tokens using a
whitespace as separator.
3. Spelling correction: to deal with noise and
simple morphological variations, each token
that does not match a word within the voca?
bulary is replaced by the most frequent word
among the most similar words found above a
threshold of 0.75. The similarity is computed
using a normalized score based on the Leven-
sthein distance.
3
https://github.com/lariverosc/tmapper
4
http://mindlaboratory.org/
425
4. Candidate generation and scoring: a subset
that contains all the terms that match at least
one of the words in the sentence is gene?
rated, the terms contained in this set are
called candidates. Once this subset is built,
each of the candidate terms is scored using
a simplified version of Metamap?s scoring
function (Aronson, 2001). In comparison, T-
mapper?s function uses only variation, cov-
erage and cohesiveness as criteria, excluding
centrality, since it is language dependant.
5. Candidate selection and disambiguation: the
score computed in the previous step is used
to choose the candidates that will be used as
mappings. Ambiguity can occur because of
two reasons: a tie in the scores or by over-
lapping over the sentence tokens. In the first
case, the Lin?s measure (Lin, 1998) is used
as disambiguation criteria between the can-
didates and the previous detected concepts.
In the second case, the most concrete term is
chosen according to the UMLS hierarchy.
4 System Submissions
The team submitted three runs. The run 0 was
intended as a baseline; run 1 used Metamap for
UMLS concept mapping and run 2 did this using
T-mapper. Both run 1 and run 2 used the enhanced
features for NER and applied the weirdness mea-
sure.
For run 0, the documents were processed with
Metamap and those concepts mapped to a CUI
belonging to one of the desired UMLS seman-
tic types were chosen. Parallel to this, the do?
cument was tagged using the default NER model.
Finally, results were merged, preferring Metamap
mapping outputs in the cases where a concept was
mapped by both tools (in an ideal scenario, all
terms mapped by Metamap would have also been
mapped by the NER model).
Run 1 differs from run 0 in two steps of the pro-
cess: the NER model included the enhanced fea-
tures described previously and its output was fil-
tered, keeping only those concepts whose weird-
ness measure exceeds 0.7. For multiword concepts
the weirdness of each word was aggregated.
Finally, run 2 was equal to run 1, with the di?
fference that T-mapper was used to map concepts
to the UMLS meta thesaurus.
Rank Run Strict P Strict R Strict F
1 best 0.843 0.786 0.813
31 2 0.561 0.534 0.547
32 1 0.578 0.515 0.545
37 0 0.321 0.565 0.409
Table 1: Official results for task A obtained by the
best system and our runs (ranked by exact acc.)
Rank Run Strict Accuracy
1 best 0.741
19 2 0.461
21 0 0.435
24 1 0.411
Table 2: Official results for task B obtained by the
best system and our runs (ranked by exact acc.)
5 Results
For both task A and B, run 2 produced the best
performance among our systems. In Table 1 the re-
sults of the three runs are presented, together with
the information of the system with the best perfor-
mance among all participating teams (labeled as
best). The position in the ranking is from a total
of 43 submitted systems. Table 2 shows analogous
results for Task B, where 37 systems were submit-
ted.
Even though the official ranking is based on the
strict accuracy, which only considers a tag to be
correct if it matches exactly both the first and last
characters, a relaxed accuracy is also provided by
the organizers. This second scoring measure con-
siders a tag to be correct if it has an overlap with
the actual one. Tables 3 and 4 show these results.
In both tables 1 and 3, P stands for Precision, R
for Recall, and F for F-score. The ranking is based
on the F-score.
6 Discussion
The system that gave the best results for both tasks
was the one based on T-mapper. Certain features
Rank Run Relax P Relax R Relax F
1 best 0.916 0.907 0.911
35 2 0.769 0.677 0.720
37 1 0.777 0.654 0.710
40 0 0.439 0.725 0.547
Table 3: Official results for task A obtained by the
best system and our runs (ranked by relaxed acc.)
426
Rank Run Relaxed Accuracy
1 best 0.928
11 2 0.863
19 0 0.797
21 1 0.771
Table 4: Official results for task B obtained by the
best system and our runs (ranked by relaxed acc.)
of this tool make this finding particularly inte?
resting: it works for any language and ontology,
and it is considerably faster than Metamap. While
Metamap took 581 minutes to tag 133 documents,
T-mapper only required 96 minutes (133 is the
number of documents in the test set).
One aspect that might have damaged the per-
formance of our system is the fact that, unlike
most of the teams, we did not use the develop-
ment data for training. However, there are still
a number of changes that could be made, which
would very likely improve the accuracy of our sys-
tem. First, the tokenizer used for the NER model
and for T-mapper were too simple. Separation was
done based on blank spaces, therefore slashes, cer-
tain punctuation marks and hyphens might not be
treated properly.
In addition to this, the spell checker used by T-
mapper also needs to be improved. Currently, it
gives a ranked list of options for each word that
should be replaced, and automatically chooses the
first one in the ranking. However, the best match
is often the second or third in the list. Changing
the criteria used to choose the replacement, taking
into account word sense disambiguation, would
enhance the accuracy of T-mapper.
The weirdness measure is also something that
should be reconsidered, since it would be inte?
resting to use a metric that responds better to un-
seen terms. And in case this was still the chosen
measure, other training corpora could work better,
since an ontology might lack words that are cur-
rently used in a medical context but do not have
a CUI, and it also fails to give a notion of which
words are more frequently used than others. It is
not easy, however, to replace UMLS as corpus,
since it is not easy to compete with its size and
richness.
Finally, the OpenNLP NER system does not
recognize discontinuous terms. Therefore, no
CUI-less term with a gap can currently be iden-
tified by the system. For this reason, the NER
method should be changed to one that allows this
type of mentions to be present in texts.
For Task B, it is very interesting to see the di?
fference between the strict and relaxed evaluation
rankings. We go from being in position 19 to being
in position 11. This might be partially explained
by some of the flaws previously mentioned; in par-
ticular, the weak tokenizer and the incapability to
identify CUI-less terms with gaps.
7 Conclusion
We participated with three runs in the Semeval
2014 task for analysis of clinical texts. Even
though the performance of our runs indicates they
still need to be enhanced in order to be com?
petitive in this specific task, the performance of
the run based on T-mapper compared to that of the
ones that use Metamap proves that T-mapper is a
viable alternative for mapping concepts to clinical
terminologies. Moreover, T-mapper should also be
considered for cases in which Metamap cannot be
used: languages other than English and terminolo-
gies other than UMLS.
References
Khurshid Ahmad, Lee Gillam, Lena Tostevin, et al.
1999. University of surrey participation in trec8:
Weirdness indexing for logical document extrapola-
tion and retrieval (wilder). In TREC.
Alan R Aronson. 2001. Effective mapping of biomed-
ical text to the umls metathesaurus: the metamap
program. In Proceedings of the AMIA Symposium,
page 17. American Medical Informatics Associa-
tion.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In ICML, volume 98, pages 296?
304.
Sameer Pradhan, Noemie Elhadad, Brett R. South,
David Martinez, Lee Chistensen, Amy Vogel, Hanna
Suominen, Wendy W. Chapman, and Guergana
Savova. 2013. Task 1: ShARe/CLEF eHealth eval-
uation lab 2013. In Online Working Notes of the
CLEF 2013 Evaluation Labs and Workshop, Valen-
cia, Spain, September.
Sameer Pradhan, Noemie Elhadad, Wendy W. Chap-
man, and Guergana Savova. 2014. Semeval-2014
task : Analysis of clinical text. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland, August.
427
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 448?453,
Dublin, Ireland, August 23-24, 2014.
NTNU: Measuring Semantic Similarity with
Sublexical Feature Representations and Soft Cardinality
Andr
?
e Lynum, Partha Pakray, Bj
?
orn Gamb
?
ack Sergio Jimenez
{andrely,parthap,gamback}@idi.ntnu.no sgjimenezv@unal.edu.co
Norwegian University of Science and Technology Universidad Nacional de Colombia
Trondheim, Norway Bogot?a, Colombia
Abstract
The paper describes the approaches taken
by the NTNU team to the SemEval 2014
Semantic Textual Similarity shared task.
The solutions combine measures based
on lexical soft cardinality and character
n-gram feature representations with lexi-
cal distance metrics from TakeLab?s base-
line system. The final NTNU system is
based on bagged support vector machine
regression over the datasets from previous
shared tasks and shows highly competi-
tive performance, being the best system on
three of the datasets and third best overall
(on weighted mean over all six datasets).
1 Introduction
The Semantic Textual Similarity (STS) shared task
aims at providing a unified framework for evaluat-
ing textual semantic similarity, ranging from ex-
act semantic equivalence to completely unrelated
texts. This is represented by the prediction of
a similarity score between two sentences, drawn
from a particular category of text, which ranges
from 0 (different topics) to 5 (exactly equivalent)
through six grades of semantic similarity (Agirre
et al., 2013). This paper describes the NTNU
submission to the SemEval 2014 STS shared task
(Task 10). The approach is based on the lexical
and distributional features of the baseline Take-
Lab system from the 2012 shared task (
?
Sari?c et al.,
2012), but improves on it in three ways: by adding
two new categories of features and by using a bag-
ging regression model to predict similarity scores.
The new feature categories added are based on
soft cardinality and character n-grams, described
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails:http://creativecommons.org/licenses/by/4.0/
in Section 2. The parameters of the two cate-
gories are optimised over several corpora and the
features are combined through support vector re-
gression (Section 3) to create the actual systems
(Section 4). As Section 5 shows, the new mea-
sures give the baseline system a substantial boost,
leading to very competitive results in the shared
task evaluation.
2 Feature Generation Methods
The methods used for creating new features utilise
soft cardinality and character n-grams. Soft cardi-
nality (Jimenez et al., 2010) was used successfully
for the STS task in previous SemEval editions
(Jimenez et al., 2012a; Jimenez et al., 2013a).
The NTNU systems utilise an ensemble of such 18
measures, based only on surface text information,
which were extracted using soft cardinality with
different similarity functions, as further described
in Section 2.1.
Section 2.2 then introduces the similarity mea-
sures based on character n-gram feature represen-
tations, which proved themselves as the strongest
features in the STS 2013 task (Marsi et al., 2013).
The measures used here replace character n-gram
features with cluster frequencies or vector val-
ues based on the n-gram collocational structure
learned in an unsupervised manner from text data.
A variety of n-gram feature representations were
trained on subsets of Wikipedia and the best per-
forming ones were used for the new measures,
which are based on cosine similarity between the
document vectors derived from each sentence in a
given pair.
2.1 Soft Cardinality Measures
Soft cardinality resembles classical set cardinality
as it is a method for counting the number of ele-
ments in a set, but differs from it in that similarities
among elements are being considered for the ?soft
counting?. The soft cardinality of a set of words
448
A = {a
1
, a
2
, .., a
|A|
} (a sentence) is defined by:
|A|
sim
=
|A|
?
i=1
w
a
i
?
|A|
j=1
sim(a
i
, a
j
)
p
(1)
Where p is a parameter that controls the cardinal-
ity?s softness (p?s default value is 1) and w
a
i
are
weights for each word, obtained through inverse
document frequency (idf ) weighting. sim(a
i
, a
j
)
is a similarity function that compares two words
a
i
and a
j
using the symmetrized Tversky?s index
(Tversky, 1977; Jimenez et al., 2013a) represent-
ing them as sets of 3-grams of characters. That
is, a
i
= {a
i,1
, a
i,2
, ..., a
i,|a
i
|
} where a
i,n
is the n
th
character trigram in the word a
i
in A. Thus, the
proposed word-to-word similarity is given by:
sim(a
i
, a
j
)=
|c|
?(?|a
min
|+(1??)|a
max
|)+|c|
(2)
?
?
?
?
?
|c| = |a
i
? a
j
|+ bias
sim
|a
min
| = min {|a
i
\ a
j
|, |a
j
\ a
i
|}
|a
max
| = max {|a
i
\ a
j
|, |a
j
\ a
i
|}
The sim function is equivalent to the Dice?s co-
efficient if the three parameters are given their de-
fault values, namely ? = 0.5, ? = 1 and bias = 0.
The soft cardinalities of any pair of sentencesA,
B andA?B can be obtained using Eq. 1. The soft
cardinality of the intersection is approximated by
|A?B|
sim
= |A|
sim
+|B|
sim
?|A?B|
sim
. These
four basic soft cardinalities are algebraically re-
combined to produce an extended set of 18 fea-
tures as shown in Table 1. The featureSTS
sim
is a
parameterized similarity function built by reusing
at word level the symmetrized Tversky?s index
(Eq. 2), whose parameters are tuned from training
data (as further described in Subsection 3.2).
Although this method is based purely on string
matching, the soft cardinality has been shown to
be a very strong baseline for semantic textual com-
parison. The word-to-word similarity sim in Eq. 1
could be replaced by other similarity functions
based on semantic networks or any distributional
representation making this method able to capture
more complex semantic relations among words.
2.2 Sublexical Feature Representations
We have created a set of similarity measures based
on induced representations of character n-grams.
The measures are based on similarity between
STS
sim
(|A|?|A?B|)
/|A|
|A|
(|A|?|A?B|)
/|A?B|
|B|
|B|
/|A?B|
|A ?B|
(|B|?|A?B|)
/|B|
|A ?B|
(|B|?|A?B|)
/|A?B|
|A| ? |A ?B|
|A?B|
/|A|
|B| ? |A ?B|
|A?B|
/|B|
|A ?B| ? |A ?B|
|A?B|
/|A?B|
|A|
/|A?B|
(|A?B|?|A?B|)
/|A?B|
NB: in this table only, | ? | is short for | ? |
sim
Table 1: Soft cardinality features.
document vectors, here the centroid of the individ-
ual term vector representations, which are trained
on character n-grams rather than full words. The
vector representations are induced in an unsuper-
vised manner from large unannotated corpora us-
ing word clustering, topic learning and word rep-
resentation learning methods.
In this paper, three different methods have
been used for creating the character n-gram fea-
ture representations: Brown Clusters (Brown et
al., 1992), Latent Semantic Indexing (LSI) topics
(Deerwester et al., 1990), and log linear skip-gram
models (Mikolov et al., 2013). The Brown clusters
were trained using the implementation by Liang
(2005), while the LSI topic vectors and log linear
skip-gram representations were trained using the
Gensim topic modelling framework (
?
Reh?u?rek and
Sojka, 2010). In addition, tf-idf (Term-Frequency
Inverse Document Frequency) weighting was used
when training LSI topic models. We used a cosine
distance measure between document vectors con-
sisting of the centroid of the term representation
vectors. For Brown clusters, the normalized term
frequency vectors were used with the cluster IDs
instead of the terms themselves. For LSI topic rep-
resentations, the tf-idf weighted topic mixture for
each term was used as the term representation. For
the log linear skip-grams, the word representations
were extracted from the model weight matrix.
3 Feature and Parameter Optimisation
The extracted features and the parameters for the
two methods described in the previous section
were optimised over several sets of training data.
As no training data was explicitly provided for the
STS evaluation campaign this year, we used dif-
ferent training sets from past campaigns and from
Wikipedia for the new test sets.
449
Test set Training set
deft-forum
MSRvid 2012 train and test +
OnWN 2012 and 2013 test
deft-news MSRvid 2012 train + test
headlines headlines 2013 test
images MSRvid 2012 train + test
OnWN OnWN 2012 and 2013 test
tweet-news
SMTeuroparl 2012 test +
SMTnews 2012 test
Table 2: Training-test set pairs.
3.1 Training Data and Pre-processing
The training-test sets pairs used for optimising the
parameters of the soft cardinality methods were
selected from the STS 2012 and STS 2013 task,
as shown in Table 2. The character n-gram repre-
sentation vectors were trained in an unsupervised
manner on two subsets of Wikipedia consisting,
respectively, of the first 12 million words (10
8
characters, hence referred to as Wiki8) and of 125
million words (10
9
characters; Wiki9).
First, however, the training data had to be pre-
processed. Thus, before extracting the idf weights
and the soft cardinality features, all the texts
shown in Table 2 were passed through the follow-
ing four pre-processing steps:
(i) tokenization and stop-word removal (pro-
vided by NLTK, Bird et al. (2009)),
1
(ii) conversion to lowercase characters,
(iii) punctuation and special character removal
(e.g., ?.?, ?;?, ?$?, ?&?), and
(iv) Porter stemming.
Character n-grams including whitespace were
generated from the Wikipedia texts, which in con-
trast only were pre-processed in a 3-step chain:
(i) removal of punctuation and extra whites-
pace,
(ii) replacing numbers with their single digit
word (?one?, ?two?, etc.), and
(iii) lowercasing all text.
1
http://www.nltk.org/
Data ? ? bias p ?
?
?
?
bias
?
deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63
deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02
headlines 0.36 -0.29 4.17 0.85 -4.50 0.43 0.19
images 1.12 -1.11 0.93 0.64 -0.98 0.50 0.11
OnWN 0.53 -0.53 1.01 1.00 -4.89 0.52 0.46
tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45
Table 3: Optimal parameters used for each dataset.
3.2 Soft Cardinality Parameter Optimisation
The first feature in Table 1, STS
sim
, was used to
optimise the four parameters ?, ?, bias, and p in
the following way. First, we built a text similarity
function reusing Eq. 2 for comparing two sets of
words (instead of two sets of character 3-grams)
and replacing the classic cardinality |?| by the soft
cardinality | ? |
sim
from Eq. 1. This text similarity
function adds three parameters (?
?
, ?
?
, and bias
?
)
to the initial four parameter set (?, ?, bias, and p).
Second, these seven parameters were set to their
default values and the scores obtained from this
function for each pair of sentences were compared
to the gold standards in the training data using
Pearson?s correlation. The parameter search space
was then explored iteratively using hill-climbing
until reaching optimal Pearson?s correlation. The
criterion for assignment of training-test set pairs
was by closeness of average character length. The
optimal training parameters are shown in Table 3.
3.3 Parameters for N-gram Feature Training
The character n-gram feature representation vec-
tors were trained while varying the parameters of
n-gram size, cluster size, and term frequency cut-
offs for all models. For the log linear skip-gram
models, our intuition is that a larger skip-gram
context is needed than the 5 or 10 wide skip-grams
used to train word-based representations due to the
smaller term vocabulary and dependency between
adjacent n-grams, so instead we trained models us-
ing skip-gram widths of 25 or 50 terms. Term fre-
quency cut-offs were set to limit the model size,
but also potentially serve as a regularization on
the resulting measure. In detail, the following sub-
lexical representation measures are used:
? Log linear skip-gram representations of char-
acter 3- and 4-grams of size 1000 and 2000,
respectively. Trained on the Wiki8 corpus us-
ing a skip gram window of size 25 and 50,
and frequency cut-off of 5.
450
? Brown clusters with size 1024 of character 4-
grams using a frequency cut-off of 20.
? Brown clusters of character 3-, 4- and 5-
grams with cluster sizes of resp. 1024, 2048
and 1024. The representations are trained on
the Wiki9 corpus with successively increas-
ing frequency cut-offs of 20, 320 and 1200.
? LSI topic vectors based on character 4-grams
of size 2000. Trained on the Wiki8 corpus
using a frequency cut-off of 5.
? LSI topic vectors based on character 4-grams
of size 1000. Trained on the Wiki9 corpus
using a frequency cut-off of 80.
3.4 Similarity Score Regression
The final sentence pair similarity score is predicted
by a Support Vector Regression (SVR) model with
a Radial Basis (RBF) kernel (Vapnik et al., 1997).
The model is trained on all the test data for the
2013 STS shared task combined with all the trial
and test data of the 2012 STS shared task.
The combined dataset hence consists of about
7,500 sentence pairs from nine different text cat-
egories: five sets from the annotated data sup-
plied to STS 2012, based on Microsoft Research
Paraphrase and Video description corpora (MSR-
par and MSvid), statistical machine translation
system output (SMTeuroparl and SMTnews), and
sense mappings between OntoNotes and WordNet
(OnWN); and four sets from the STS 2013 test
data: headlines (news headlines), SMT, OnWN,
and FNWM (mappings of sense definitions from
FrameNet and WordNet).
The SVR model was trained as a bagged classi-
fier, that is, for each run, 100 regression models
were trained with 80% of the samples and fea-
tures of the original training set drawn with re-
placement. The outputs of all models were then
averaged into a final prediction. This bagged train-
ing procedure adds extra regularization, which can
reduce the instability of prediction accuracy be-
tween different test data categories.
The prediction pipeline was implemented with
the Scikit-learn software framework (Pedregosa et
al., 2011), and the SVR models were trained with
the implementation?s default parameters: cost
penalty (C) 1.0, margin () 0.1, and RBF precision
(?) 1/|featurecount|.
We were unable to improve the performance
over these defaults by cross validation parameter
search unless the models were trained for specific
text categories. Consequently no parameter opti-
mization was performed during training of the fi-
nal systems.
4 Submitted Systems
The three submitted systems consist of one us-
ing only the soft cardinality features described in
Section 3.2 (NTNU-run1), one system using a
baseline set of lexical measures and WordNet aug-
mented similarity in addition to the new sublexical
representation measures (NTNU-run2), and one
(NTNU-run3) which combines the output from
the other two systems by taking the mean of the
two sets of predictions. NTNU-run3 thus repre-
sents a combination of the measures and methods
introduced by NTNU-run1 and NTNU-run2.
In addition to the sublexical feature measures
described in Section 3.3, NTNU-run2 uses the fol-
lowing baseline features adapted from the Take-
Lab 2012 system submission (
?
Sari?c et al., 2012).
? Simple lexical features: Relative document
length differences, number overlap, case
overlap, and stock symbol named entity
recognition.
? Lemma and word n-gram overlap of orders 1-
3, frequency weighted lemma and word over-
lap, and WordNet augmented overlap.
? Cosine similarity between the summed word
representation vectors from each sentence us-
ing LSI models based on large corpora with
or without frequency weighting.
The specific measures used in the submitted
systems were found by training the regression
model on the STS 2012 shared task data and eval-
uating on the STS 2013 test data. We used a step-
wise forward feature selection method by compar-
ing mean (but unweighted) correlation on the four
test categories in order to identify the subset of
measures to include in the final system.
The system composes a feature set of similar-
ity scores from these 20 baseline measures and the
nine sublexical representation measures, and uses
these to train a bagged SVM regressor as described
in Section 3.4 in order to predict the final semantic
similarity score for new sentence pairs.
451
NTNU-run1 NTNU-run2 NTNU-run3 Best
Dataset r rank r rank r rank r
deft-forum 0.4369 16 0.5084 2 0.5305 1 0.5305
deft-news 0.7138 14 0.7656 6 0.7813 2 0.7850
headlines 0.7219 17 0.7525 13 0.7837 1 0.7837
images 0.8000 9 0.8129 4 0.8343 1 0.8343
OnWN 0.8348 7 0.7767 20 0.8502 4 0.8745
tweet-news 0.4109 33 0.7921 1 0.6755 13 0.7921
mean 0.6531 20 0.7347 4 0.7426 2 0.7429
weighted mean 0.6631 21 0.7491 4 0.7549 3 0.7610
Table 4: Final evaluation results for the submitted systems.
5 Results and Discussion
The final evaluation results for the three submit-
ted systems are shown in Table 4, where the right-
most column (?Best?) for comparison displays the
performance figures obtained by any of the 38 sys-
tems on each dataset.
The systems using sublexical representation
based measures show competitive performance,
ranking third and fourth among the submitted sys-
tems with a weighted mean correlation of ?0.75.
They also produced the best result in four out of
the six text categories in the evaluation dataset,
with NTNU-run3 being the #1 system on deft-
forum, headlines and images, #2 on deft-news, and
#4 on OnWN. It would thus have been the clear
winner if it had not been for its sub-par perfor-
mance on the tweet-news dataset, which on the
other hand is the category NTNU-run2 was the
best of all systems on.
The system based solely on soft cardinality fea-
tures, NTNU-run1, displays more modest perfor-
mance ranking at 21
st
place (of the in total 38 sub-
mitted systems) with ?0.66 correlation. This is a
bit surprising, since this method for obtaining fea-
tures from pairs of texts was used successfully in
other SemEval tasks such as cross-lingual textual
entailment (Jimenez et al., 2012b) and student re-
sponse analysis (Jimenez et al., 2013b). Similarly,
Croce et al. (2012) used soft cardinality represent-
ing text as a bag of dependencies (syntactic soft
cardinality) obtaining the best results in the typed-
similarity task (Croce et al., 2013).
From our results it can be noted that for most
categories the sublexical representation measures
show strong performance in NTNU-run2, with a
significantly better result for the combined sys-
tem NTNU-run3. This indicates that while the soft
cardinality features are weaker predictors overall,
they are complimentary to the sublexical and lex-
ical features of NTNU-run2. It is also indicative
that this is not the case for the tweet-news cate-
gory, where the text is more ?free form? and less
normative, so it would be expected that sublexical
approaches should have stronger performance.
Acknowledgements
This work was made possible with the support
from Department of Computer and Information
Science, Norwegian University of Science and
Technology.
Partha Pakray was 2013?2014 supported by an
ERCIM Alain Bensoussan Fellowship.
The NTNU systems are partly based on code
made available by the Text Analysis and Knowl-
edge Engineering Laboratory, Department of
Electronics, Microelectronics, Computer and In-
telligent Systems, Faculty of Electrical Engineer-
ing and Computing, University of Zagreb.
452
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, pages 32?43, Atlanta, Georgia, USA,
June.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Danilo Croce, Valerio Storch, P. Annesi, and Roberto
Basili. 2012. Distributional compositional seman-
tics and text similarity. In 2012 IEEE Sixth Interna-
tional Conference on Semantic Computing (ICSC),
pages 242?249, September.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. UNITOR-CORE TYPED: Combining text
similarity and semantic filters through SV regres-
sion. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 59?65, At-
lanta, Georgia, USA, June.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardi-
nality. In Edgar Chavez and Stefano Lonardi, ed-
itors, String Processing and Information Retrieval,
volume 6393 of LNCS, pages 297?302. Springer,
Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized
similarity function for text comparison. In Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation (SemEval 2012), Montr?eal, Canada,
7-8 June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: Learning adap-
tive similarity functions for cross-lingual textual en-
tailment. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
Montr?eal, Canada, 7-8 June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013a. SOFTCARDINALITY-CORE: Im-
proving text overlap with distributional measures for
semantic textual similarity. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Con-
ference and the Shared Task: Semantic Textual Sim-
ilarity, Atlanta, Georgia, USA, June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013b. SOFTCARDINALITY: Hierarchical
text overlap for student response analysis. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, Atlanta, Georgia, USA, June.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Ph.D. thesis, Massachusetts Institute
of Technology.
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov,
Bj?orn Gamb?ack, and Andr?e Lynum. 2013. NTNU-
CORE: Combining strong features for semantic sim-
ilarity. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 66?73, At-
lanta, Georgia, USA, June.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45?
50, Valletta, Malta, May. ELRA.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84(4):327?352, July.
Vladimir Vapnik, Steven E. Golowich, and Alex
Smola. 1997. Support vector method for function
approximation, regression estimation, and signal
processing. In Michael C. Mozer, Michael I. Jordan,
and Thomas Petsche, editors, Advances in Neural
Information Processing Systems, volume 9, pages
281?287. MIT Press, Cambridge, Massachusetts.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. TakeLab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
453
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 732?742,
Dublin, Ireland, August 23-24, 2014.
UNAL-NLP: Combining Soft Cardinality Features for Semantic
Textual Similarity, Relatedness and Entailment
Sergio Jimenez, George Due
?
nas,
and Julia Baquero
Universidad Nacional de Colombia
Ciudad Universitaria, edificio 453,
oficina 114, Bogot?a, Colombia
[sgjimenezv,geduenasl,
jmbaquerov]@unal.edu.co
Alexander Gelbukh
Center for Computing Research (CIC),
Instituto Polit?ecnico Nacional (IPN),
Av. Juan Dios B?atiz, Av. Mendiz?abal,
Col. Nueva Industrial Vallejo,
Mexico City, Mexico
www.gelbukh.com
Abstract
This paper describes our participation in
the SemEval-2014 tasks 1, 3 and 10. We
used an uniform approach for addressing
all the tasks using the soft cardinality for
extracting features from text pairs, and
machine learning for predicting the gold
standards. Our submitted systems ranked
among the top systems in all the task and
sub-tasks in which we participated. These
results confirm the results obtained in pre-
vious SemEval campaigns suggesting that
the soft cardinality is a simple and useful
tool for addressing a wide range of natural
language processing problems.
1 Introduction
The semantic textual similarity is a core prob-
lem in the computational linguistic field. Con-
sequently, the previous evaluation campaigns of
this task in SemEval have attracted the attention
of many research groups worldwide (Agirre et al.,
2012; Agirre et al., 2013).This year, 3 tasks related
to this problem have been proposed exploring dif-
ferent facets such as semantic relatedness, entail-
ment , multilingualism, lack of training data and
imbalance in the amount of information.
The soft cardinality (Jimenez et al., 2010) is a
simple concept that generalizes the classical set
cardinality by considering the similarities among
the elements in a collection for a more intuitive
quantification of the number of elements in that
collection. This approach can be applied to text
applications representing texts as collections of
words and providing a similarity function that
compares two words. Varying this word-to-word
similarity function the soft cardinality can reflect
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
notions of syntactic similarity, semantic related-
ness, among others. We (and others) have used
this approach to address with success the semantic
textual similarity and other tasks in previous Se-
mEval editions (Jimenez et al., 2012b; Jimenez et
al., 2012a; Jimenez et al., 2013a; Jimenez et al.,
2013b; Jimenez et al., 2013c; Croce et al., 2013).
In this paper we describe our participating sys-
tems in the SemEval-2014 tasks 1, 3, and 10,
which used the soft cardinality as core approach.
2 Features from Soft Cardinalities
The cardinality of a collection of elements is the
counting of non-repeated elements in it. This def-
inition is intrinsically associated with the notion
of set, which is a collection of non-repeated ele-
ments.Thus, the cardinality of a collection or set
A is denoted as |A|. Clearly, the cardinality of a
collection with repeated elements treats groups of
identical elements as a single instance contribut-
ing only with a unit (1) to the element counting.
Jimenez et al. (2010) proposed the soft cardinal-
ity that uses a notion of similarity among elements
for grouping not only identical elements but simi-
lar too. That notion of similarity among elements
is provided by a similarity function that compares
two elements a
i
and a
j
and returns a score in [0,1]
interval, having sim(a
i
, a
i
) = 1. Although, it
is not necessary that sim fulfills another metric
properties aside of identity, symmetry is also de-
sirable. Thus, the soft cardinality of a collection
A, whose elements a
1
, a
2
, . . . , a
|A|
are compara-
ble with a similarity function sim(a
i
, a
j
), is de-
noted as |A|
sim
. This soft cardinality is given by
the following expression:
|A|
sim
=
|A|
?
i=1
w
a
i
?
|A|
j=1
sim(a
i
, a
j
)
p
(1)
It is trivial to see that |A| = |A|
sim
either if
p ? ? or when the function sim is a crisp com-
732
Basic Derived
|A| |A ?B| = |A|+ |B| ? |A ?B|
|B| |A4B| = |A ?B| ? |A ?B||
|A ?B| |A \B| = |A| ? |A ?B|
|B \A| = |B| ? |A ?B|
Table 1: The 7 basic and derived cardinalities for
two sets comparison.
parator, i.e. one that returns 1 for identical ele-
ments and 0 otherwise. This property shows that
the soft cardinality generalizes the classical cardi-
nality and that the parameter p controls its degree
of ?softness?, whose default value is 1. The values
w
a
i
are optional ?importance? weights associated
with each element a
i
, by default those weights can
be assigned to 1.
For the tasks at hand, we represent each short
text (lets say A) as a collection of words a
i
and
the sim function can be any operator that com-
pares pairs of words. The motivation for using the
soft cardinality is that the sim function can reflect
any dimension of word similarity (e.g. syntactic,
semantic) and the soft cardinality projects that no-
tion at sentence level. For instance, if sim pro-
vides the degree of semantic relatedness between
two words using WordNet, two texts A and B
could be compared by computing |A|
sim
, |B|
sim
and |A?B|
sim
. Given that A?B could be empty,
the soft cardinality of the intersection must be ap-
proximated by |A ? B|
sim
? |A|
sim
+ |B|
sim
?
|A ? B|
sim
instead of being computed directly
from A ? B using equation 1. Using that approx-
imation, the commonality (intersection) between
A and B is induced by the pair-wise similarities
provided by sim among the words in A and B.
Since more than a century when Jaccard (1901)
proposed his well-known index, the classical set
cardinality has been used to build similarity func-
tions for set comparison. Any binary-cardinality-
based similarity function is an algebraic combina-
tion of |A|, |B| and either |A ? B| or |A ? B|
(e.g. Jaccard, Dice, Tversky, overlap and cosine
indexes). These three cardinalities describes un-
ambiguously all the regions in the Venn?s diagram
when comparing two sets. Thus, in this scenario 4
possible cardinalities can be derived from these 3
basic cardinalities, see Table 1. Clearly, the same
set of cardinalities can be obtained for the soft car-
dinality.
When training data is available, which is the
# Feature expression
1
|A|
/|A?B|
2
|A|?|A?B|
/|A|
3
|A|?|A?B|
/|A?B|
4
|B|
/|A?B|
5
|B|?|A?B|
/|B|
6
|B|?|A?B|
/|A?B|
7
|A?B|
/|A|
8
|A?B|
/|B|
9
|A?B|
/|A?B|
10
|A?B|?|A?B|
/|A?B|
Table 2: Extended set of 10 rational features.
case for tasks 1, 3 and 10 in SemEval 2014, it
is possible to think that instead of using an ad-
hoc expression (e.g. Jaccard, Dice) the similar-
ity function can be obtained using the cardinalities
in Table 1 as features for a machine-learning re-
gression algorithm. Our hypothesis is that such
learnt function should predict in a more accurate
way the gold standard variable than any other ad-
hoc function. However, these cardinality features
are intrinsically correlated with the length of the
texts where they were obtained. This correlation
makes that the performance of the learnt similar-
ity function could be dependent of the length of
the texts. For instance, if the function was trained
using long texts it is plausible to think that this
function would be more effective when tested with
long texts than with shorter ones. Having this in
mind, an extended set of rational features is pro-
posed, whose values are standardized in [0,1] in-
terval aiming to reduce the effect of the length of
the texts. These features are presented in Table 2.
The soft cardinality has proven to overcome
the classic cardinality in the semantic textual
similarity (STS) task in previous SemEval cam-
paigns (Jimenez et al., 2012b; Jimenez et al.,
2013a). Even using a simplistic function sim
based on q-grams of characters, the soft cardinal-
ity method ranked third among 89 participating
systems (Agirre et al., 2012). Thus, our participat-
ing systems in the SemEval 2014 campaign were
based on the previously described set of 17 fea-
tures, obtained from the soft cardinality with dif-
ferent sim functions for comparing pairs of words.
Each sim function produced a different set of fea-
tures, which were combined with a regression al-
gorithm for similarity and relatedness tasks. Sim-
ilarly, a classification algorithm was used for the
733
entailment task.
3 Systems Description
In this section the different feature sets used for
each submitted system to the different task and
subtask are described. Besides, the data used for
training, parameters and other preprocessing de-
tails are described for each system.
3.1 Task 1: Textual Relatedness and
Entailment
The task 1 is based on the SICK (Sentences
Involving Compositional Knowledge) data set
(Marelli et al., 2014), which contains nearly
10,000 pairs of sentences manually labeled by re-
latedness and entailment. The relatedness gold la-
bels range from 1 to 5, having 1 the minimum level
of relatedness between the texts and 5 for the max-
imum. The entailment labels have three categori-
cal values: neutral, contradiction and entailment.
The two sub tasks consist of predicting the related-
ness and entailment gold standards using approxi-
mately the 50% of the text pairs as training and the
other part as test bed.
Our overall approach consists in extracting 4
different sets of features using the method pre-
sented in section 2 and training a machine learn-
ing algorithm for predicting the gold standard la-
bels in the test data. Each feature set is described
in the following 4 subsections and the subsection
3.1.6 provides details of the used combination of
features, machine learning algorithm and prepro-
cessing details.
3.1.1 String-Matching Features
First, all texts in the SICK data set where prepro-
cessed by lower casing, tokenizing and stop-word
removal (using the NLTK
1
). Then each word was
reduced to its stem using the Porter?s algorithm
(Porter, 1980) and a idf weight (Jones, 2004) was
associated to each stem (w
a
i
weights in eq. 1) us-
ing the very SICK data set as document collec-
tion. Next, for each instance in the data, which
is composed of two texts A and B, the 17 fea-
tures listed in Tables 1 and 2 where extracted using
eq.1. The used word-to-word similarity function
sim decomposes each word in bags of 3-grams
of characters, which are compared using the sym-
metrical Tversky?s index (Tversky, 1977; Jimenez
et al., 2013a). Thus, the similarity between two
1
http://www.nltk.org/
pairs of words w
1
and w
2
, represented each one as
a collection of 3-grams of characters, is given by
the following expression:
sim(w
1
, w
2
) =
|c|
?(?|w
min
|+ (1? ?)|w
max
|) + |c|
(2)
|c| = |w
1
? w
2
|+ bias
sim
,
|w
min
| = min[|w
1
\ w
2
|, |w
2
\ w
1
|],
|w
max
| = max[|w
1
\ w
2
|, |w
2
\ w
1
|].
The values used for the parameters were ? =
1.9, ? = 2.36, bias = ?0.97, and p = 0.39
(where p corresponds to eq.1). The motivation and
justification for these parameters can be found in
(Jimenez et al., 2013a). These values were ob-
tained by building a text similarity function us-
ing the Dice?s coefficient and the soft cardinali-
ties plugging eq.2 in eq.1. Next, this text similar-
ity function is evaluated in the 5,000 training text
pairs and the obtained scores are compared against
the relatedness gold-standard using the Pearson?s
correlation.
w
a
i
are not training parameters, but they are
weights associated with the words. These weights
could have been obtained from a larger corpus,
but we use the training texts to obtain them. This
process is repeated iteratively exploring the search
space defined by these 4 parameters using a hill-
climbing approach until a maximum correlation is
reached. We observe that the optimal values of the
parameters p, ?, ?, and bias vary considerably be-
tween the data sets and for the different sim func-
tions of word-to-word similarity. We do not yet
understand from which factors of the data and the
sim functions depend on these parameters. This
issue will be the objective of further research.
Henceforth, the set of 17 string-based features
described in this subsection will be referred as
SM.
3.1.2 ESA Features
For this set of features we used the idea proposed
by Gabrilovich and Markovitch (2007) of enrich-
ing the representation of a text by representing
each word by its textual definition in a knowl-
edge base, i.e. explicit semantic analysis (ESA).
For that, we used as knowledge base the synset?s
textual definitions provided by WordNet. First,
in order to determine the textual definition asso-
ciated to each word, the texts were tagged using
734
the maximum entropy POS tagger included in the
NLTK. Next, the adapted Lesk algorithm (Baner-
jee and Pedersen, 2002) for word sense disam-
biguation was applied in the texts disambiguating
one word at the time. The software package used
for this disambiguation process was pywsd
2
. The
arguments needed for the disambiguation of each
word are the POS tag of the target word and the
entire sentence as context. Once all the words are
disambiguated with their corresponding WordNet
synsets, each word is replaced by all the words in
their textual definition jointly with the same word
and its lemma. The final result of this stage is that
each text in the data set is replaced by a longer
text including the original text and some related
words. The motivation of this procedure is that the
extended versions of each pair of texts have more
chance of sharing common words that the original
texts.
The extended versions of these texts were used
to obtain another 17 features with the same proce-
dure described in the previous subsection (3.1.1).
This feature subset will henceforth be referred as
ESA.
3.1.3 Features for each part-of-speech
category
This set of features is motivated by the idea pro-
posed by Corley and Mihalcea (2005) of group-
ing words by their POS category before being
compared for semantic textual similarity. Our ap-
proach consist in provide a version of each text
pair in the data set for each POS category in-
cluding only the words belonging to that cate-
gory. For instance, the pair of texts {?A beauti-
ful girl is playing tennis?, ?A nice and handsome
boy is playing football?} produce new pairs such
as: {?beautiful?, ?nice handsome?} for the ADJ
tag, {?girl tennis?, ?boy football?} for NOUN and
{?is playing?, ?is playing?} for VERB.
Again, the POS tags were provided by the
NLTK?s max entropy tagger. The 28 POS cate-
gories were simplified to 9 categories in order to
avoid an excessive number of features and hence
sparseness; the used mapping is shown in Table 3.
Next, for each one of the 9 new POS categories a
set of 17 features (SM) is extracted reusing again
the method proposed in subsection 3.1.1. The only
difference with the method described in that sub-
section is that the stop-words were not removed
2
https://github.com/alvations/pywsd
Reduced tag set NLTK?s POS tag set
ADJ JJ,JJR,JJS
NOUN NN,NNP,NNPS,NNS
ADV RB,RBR,RBS,WRB
VERB VB,VBD,VBG,VBN,VBP,VBZ
PRO WP,WP$,PRP,PRP$
PREP RP,IN
DET PDT,DT,WDT
EX EX
CC CC
Table 3: Mapping reduction of the POS tag set.
and the stemming process was not performed. The
motivation for generating this feature sets by POS
category is that the machine learning algorithms
could weight differently each category. The intu-
ition behind this is that it is reasonable that cat-
egories such as VERB and NOUN could play a
more important role for the task at hand than oth-
ers such as ADJ or PREP. Using these categorized
features, such discrimination among POS cate-
gories can be discovered from the training data.
Finally, the total number of features in this set is
153 (17 features? 9 POS categories). This feature
set will be referred as POS.
3.1.4 Features From Dependencies
The syntactic soft cardinality (Croce et al., 2012;
Croce et al., 2013) extend the soft cardinality
approach by representing texts as bags of de-
pendencies instead of bags of words. Each de-
pendency is a 3-tuple composed of two syntac-
tically related words and the type of their rela-
tionship. For instance, the sentence ?The boy
plays football? can be represented with 3 depen-
dencies: [det,?boy?,?The?], [subj,?plays?,?boy?]
and [obj,?plays?,?football?]. Clearly, this repre-
sentation distinguish pairs of texts such as {?The
dog bites a boy?,?The boy bites a dog?}, which
are indistinguishable when they are represented as
bags of words. This representation can be obtained
automatically using the Stanford Parser (De Marn-
effe et al., 2006), which in addition provides a de-
pendency identifying the root word in a sentence.
We used the version 3.3.1
3
of that parser to obtain
such representation.
Once the texts are represented as bags of de-
pendencies, it is necessary to provide a similar-
ity function between two dependency tuples in or-
3
http://nlp.stanford.edu/software/lex-parser.shtml
735
der to use the soft cardinality (eq. 1) and hence
to obtain the 17 cardinality features in Tables 1
and 2. Such function can be obtained using the
sim function (eq. 2) for comparing the first and
second words between the dependencies and even
the labels of the dependency types. Let?s consider
two dependencies tuples d = [d
dep
, d
w
1
, d
w
2
] and
p = [p
dep
, p
w
1
, p
w
2
] where d
dep
and p
dep
are the
labels of the dependency type; d
w
1
and p
w
1
are
the first words on each dependency tuple; and d
w
2
and p
w
2
are the second words. The similarity func-
tion for comparing two dependency tuples can be a
linear combination of the sim scores between the
corresponding elements of the dependency tuples
by the following expression:
sim
dep
(d, p) =
?sim(d
dep
, p
dep
) + ?sim(d
w
1
, p
w
2
) + ?sim(d
w
2
, p
w
2
)
Although, it is unusual to compare the depen-
dencies? type labels d
dep
and p
dep
with a similar-
ity function designed for words, we observed ex-
perimentally that this approach yield better overall
performance in the relatedness task in comparison
with a simple crisp comparison. The optimal val-
ues for the parameters ? = ?3, ? = 10 and ? = 3
were determined with the same methodology used
in subsection 3.1.1 for determining ?, ? and bias.
Clearly, the fact that ? > ? means that the first
words in the dependency tuples plays a more im-
portant role than the second ones for the task at
hand. However, the fact that ? < 0 is counter intu-
itive because it means that the lower the similarity
between the dependency type labels is, the larger
the similarity between the two dependencies. Up
to date we have been unable to find a plausible ex-
planation for this phenomenon. This set of 17 fea-
tures will be referred hereinafter as DEP.
3.1.5 Additional Features
In addition to the feature sets based in soft car-
dinality, we designed some features aimed to ad-
dress linguistic phenomena such as antonymy, hy-
pernymy and negation.
Antonymy: Consider the following text pair
from the test data {?A man is emptying a container
made of plastic?,?A man is filling a container
made of plastic? }, which is labeled as a contra-
diction with a relatedness score of 3.91. Clearly,
these labels are explained by the antonymy rela-
tion between ?emptying? and ?filling?. Given that
none of the features presented above address this
issue, a list of 11,028 pairs of antonym words was
gathered from several web sites (see Table 4) and
from the antonymy relationships in WordNet, in
order to detect these cases. That list was used to
count the number of occurrences of pairs antonym
words between pairs of texts and in each one of
the texts. Thus, for any pair of texts A and B (rep-
resented as sets of words), three features (referred
henceforth as ANT) were extracted:
antonym AB Counts the number of occurrences
of pairs of antonyms in A ? B (Cartesian
product) or in B ?A .
antonym AA Counts the number of occurrences
of pairs of antonyms in A?A.
antonym BB Counts the number of occurrences
of pairs of antonyms in B ?B.
Hypernymy: Consider the following text pair
from the test data {?A man is sitting comfortably
at a table?,?A person is sitting comfortably at the
table? }, which is labeled as an entailment with
a relatedness score of 3.96. In this case, the en-
tailment is based on the hypernymy between ?per-
son? and ?man?. In order to capture this linguis-
tic factor 3 features similar to the previously de-
scribed antonym features were proposed. First,
word sense disambiguation was performed (as de-
scribed in subsection 3.1.2) for obtaining a synset
label for each word. Secondly, we build a bi-
nary function hyp(ss
1
, ss
2
) that takes two Word-
Net synsets as arguments and returns 1 if ss
1
is
a hypernym of ss
2
with a maximum depth in the
WordNet?s is-a hierarchy of 6 steps, and 0 oth-
erwise. This hypernymy function was build us-
ing the WordNet interface provided by the NLTK.
Next, based on that synset-to-synset function, a
text-to-text function that captures the degree or hy-
pernymy in a text or in a pair of texts was build us-
ing the Monge-Elkan measure (Monge and Elkan,
1996). Thus, for two texts A and B represented
as sets of synset labels, the following expression
measures their degree of hypernymy:
HY P (A,B) =
1
|A|
|A|
?
i=1
|B|
max
j=1
hyp(a
i
, b
j
)
Using the function HY P (?, ?), 3 features are
extracted from each pair of text (referred hence-
forth as HYP):
hypernym AB from HY P (A,B)
736
http://www.myenglishpages.com/site php files/vocabulary-lesson-opposites-adjectives.php
http://www.allaboutspace.com/wordlist/opposites.shtml
http://www.michigan-proficiency-exams.com/antonym-list.html
http://examples.yourdictionary.com/examples-of-antonyms.html
http://www.synonyms-antonyms.com/antonyms.html
http://englishwilleasy.com/word-must-know/vocabulary/vocabulary-list-by-opposites-or-antonyms/
http://www.meridianschools.org/staff/districtcurriculum/moreresources/languagearts/all grades/antonyms.doc
http://mrsbrower.weebly.com/uploads/1/3/2/4/13243672/antonymlist.pdf
https://foxhugh.wordpress.com/word-lists/list-of-antonyms/
http://www.paulnoll.com/Books/Clear-English/English-antonyms-1.html
http://wordnet.princeton.edu/wordnet/download/
Table 4: URLs used for the list of 11,028 antonym pairs (accessed on March 20, 2014).
hypernym AA from HY P (A,A)
hypernym BB from HY P (B,B)
Negation: Negations play an important role in
the task at hand. For instance, consider this pair
of texts {?A person is rinsing a steak with wa-
ter?,?A man is not rinsing a large steak?} labeled
as a contradiction. In that example the negation of
the verb ?rising? is the main factor of contradic-
tion. In order to capture this linguistic feature we
build a simple function that detects the occurrence
of a verb negation if the text contains one of the
following words: ?not?, ?n?t?, ?nor?, ?null?, ?nei-
ther?, ?either?, ?barely?, ?scarcely? and ?hardly?.
Similarly, noun negation is detected looking for
the words: ?no?, ?none?, ?nobody?, ?nowhere?,
?nothing? and ?never?. Thus, for two texts A and
B, 4 features are extracted (referred henceforth as
NEG):
verb neg A if verb negation is detected in A
verb neg B if verb negation is detected in B
noun neg A if noun negation is detected in A
noun neg B if noun negation is detected in B
3.1.6 Submitted Runs and Results
RUN1 (PRIMARY) This system produced pre-
dictions by extracting all the features described
previously (SM, ESA, POS, DEP, ANT,
HYP and NEG) from all the texts in the SICK
data set. Next, two machine learning models were
obtained (WEKA (Hall et al., 2009) was used
for that) using the training part of SICK, one for
regression (relatedness) and another for classifi-
cation (entailment). The regression model was
a reduced-error pruning tree (REPtree) (Quin-
lan, 1987) boosted with 20 iterations of bagging
(Breiman, 1996). The classification model was a
J48Graft tree also boosted with 20 bagging itera-
tions. These two models produced the predictions
for the test part of SICK.
RUN2 This system is similar to the one used
in RUN1, but it used only the feature sets SM and
NEG. Another difference is that a linear regres-
sion was used instead of the REPtree and no bag-
ging was performed.
RUN3 The same as RUN1, but again, linear
regression was used instead of the REPtree and no
bagging was performed.
RUN4 The same as RUN2, but the models
were boosted with 20 iterations of bagging.
RUN5 The same as RUN3, but 30 iterations of
bagging were used instead of 20.
The official results obtained by these systems
(prefixed UNAL-NLP) are shown in Table 5
jointly with those obtained by other 3 top sys-
tems among the 18 participating systems. Our
primary run (RUN1) obtained pretty competitive
results ranking 3th and 4th in the entailment and
relatedness tasks. The RUN4 obtained a remark-
able performance (it would be ranked 6th for en-
tailment and 8th for relatedness) in spite of the
fact that is a system purely based on string match-
ing. The comparison of our runs 1, 3 and 5, which
mainly differs by the use of bagging, shows that
this boosting method provides considerable im-
provements. In fact, comparing RUN3 (all fea-
tures, no bagging) and RUN4 (SM and NEG fea-
ture sets boosted with bagging), they performed
similarly in spite of the considerable larger num-
ber of features used in RUN3. Besides, the RUN5
slightly outperformed our primary run (RUN1) us-
737
Entailment Relatedness
system accuracy official rank Pearson Spearman MSE official rank
UNAL-NLP run1 (primary) 83.05% 3rd/18 0.8043 0.7458 0.3593 4th/17
UNAL-NLP run2 79.81% - 0.7482 0.7033 0.4487 -
UNAL-NLP run3 80.15% - 0.7747 0.7286 0.4081 -
UNAL-NLP run4 80.21% - 0.7662 0.7142 0.4210 -
UNAL-NLP run5 83.24% - 0.8070 0.7489 0.3550 -
ECNU run1 83.64% 2nd/18 0.8280 0.7689 0.3250 1st/17
Stanford run5 74.49% 12th/18 0.8272 0.7559 0.3230 2nd/17
Illinois-LH run1 84.58% 1st/18 0.7993 0.7538 0.3692 5th/17
Table 5: Results for task 1.
ing 10 additional iterations of bagging.
3.1.7 Error Analysis
Our primary run for the task 1 failed in 835 pairs of
sentences out of 4,927 in the entailment subtask.
We wanted to understand in why our system failed
in these 835 instances, so we classified manually
these instances in 4 error categories (each instance
could be assigned to several categories).
Paraphrase not detected (NP): exam-
ple={?Two groups of people are playing football?,
?Two teams are competing in a football match?},
gold standard=entailment, prediction=neutral,
number of occurrences= 420 (50.3%). The system
failed to detect the paraphrase between ?groups of
people? and ?teams?.
Negation not detected (NN) : exam-
ple={?There is no one playing the guitar?,
?Someone is playing the guitar?}, gold stan-
dard=contradiction, prediction=neutral, number
of occurrences=94 (11.3%). The system failed to
detect that the contradiction is due to the negation
in the first text.
False similarity between words (NSS) : ex-
ample={?Two dogs are playing by a tree?,
?Two dogs are sleeping by a tree?}, gold stan-
dard=neutral, prediction=entailment, number of
occurrences=413 (49.5%). The only difference
between these 2 sentences is the gerund ?playing?
vs. ?sleeping?, which the system erroneously con-
sidered as similar.
Antonym not detected (NA): exam-
ple={?Three children are running down hill?,
?Three children are running up hill?}, gold
standard=contradiction, prediction=entailment,
number of occurrences=40 (4.8%). The only
difference between these 2 sentences is the
words ?down? vs. ?up?. In spite that this pair
of antonyms was included in the antonym list,
Error category NP NN NSS NA
NP 420 5 125 0
NN - 94 1 0
NSS - - 413 22
NA - - - 40
Table 6: Co-ocurrences of types of errors in RUN1
(task1).
the system failed to distinguish the contradiction
between the texts.
The matrix in Table 6 reports the number of
co-occurrences of error categories in the 835 in-
stances erroneously classified.
3.2 Task 3: Cross-level Semantic Similarity
The SemEval 2014 task 3 (cross-level semantic
similarity) (Jurgens et al., 2014) proposed the se-
mantic textual similarity task but across differ-
ent textual levels, namely paragraph-to-sentence,
sentence-to-phrase, phrase-to-word and word-to-
sense. As usual, the goal is to predict the gold sim-
ilarity scores for each pair of texts. For each one
of these cross-level comparison types there were
proposed a separated training and test data sets.
Basically, we addressed this task using the set of
features SM presented in subsection 3.1.1 in com-
bination with a text expansion approach similar to
the method presented in subsection 3.1.2.
3.2.1 Paragraph-to-sentence and
Sentence-to-phrase
For these two cross-level comparison types we
extracted the SM feature set using the pro-
vided texts. The model parameters obtained for
paragraph-to-sentence were ? = 0.1, ? = 1.75,
bias = ?1.35, p = 1.55; and for sentence-to-
phrase were ? = 0.68, ? = 0.92, bias = ?0.92,
p = 2.49.
738
The system for the RUN2 used the SM fea-
ture set and a machine learning model build with
the provided training data for generating the simi-
larity score predictions for the test data. For the
paragraph-to-sentence data set the model was a
REPtree for regression boosted with 40 bagging it-
erations. Similarly, the model for the sentence-to-
phrase data set was a linear regressor also boosted
with 40 bagging iterations.
Unlike RUN2, RUN1 does not make use of any
machine learning algorithm. Instead, we used the
only the basic cardinalities (see Table 1) from the
SM feature set in combination with an ad-hoc re-
semblance coefficient, i.e. the Dice?s coefficient
2|A?B|
/|A|+|B| for the paragraph-to-sentence data
set. In turn, for sentence-to-phrase the overlap co-
efficient, i.e.
|A?B|
/min[|A|,|B|], was used.
3.2.2 Phrase-to-word and Word-to-sense
Before applying the same procedure used in the
previous subsection, the texts in the phrase-to-
word and word-to-sense data sets were expanded
with a similar approach to that was used in subsec-
tion 3.1.2.
Phrase-to-word expansion: First, the ?word?
was expanded finding its corresponding WordNet
synset using the adapted Lesk?s algorithm provid-
ing as context the ?phrase?. Then, once the word?s
synset is obtained, the ?word? text is extended
with the textual definition of the synset. Simi-
larity, this procedure is repeated for each word in
the ?phase? obtaining and extended version of the
phrase. Finally, these two texts are used for ex-
tracting the SM feature set. The model param-
eters were ? = 0.8, ? = 1.9, bias = ?0.8,
p = 1.5.
Word-to-sense expansion: First, the ?sense?
(i.e. synset) is replaced by its textual definition
and its lemma. At this point the pair word-sense
becomes a pair word-sentence. Then, the synset
of the ?word? is obtained performing the adapted
Lesk?s algorithm. Next, the ?word? is extended
with textual definition of the synset. Finally, these
two texts are used for extracting the SM feature
set obtaining the following model parameters were
? = 0.59, ? = 0.9, bias = ?0.89, p = 3.91.
3.2.3 Results
The official results obtained by the two submitted
runs jointly with other 3 top systems are shown in
Table 7. Our submissions (prefixed with UNAL-
NLP) ranked 3rd and 5th among 38 participating
test data train data
OnWN (en) OnWN 2012/2013 test
headlines (en) headlines 2013 test
images (en) MSRvid 2012 train and test
deft-news (en) MSRpar 2013 train and test
deft-forum (en)
MSRvid 2012 train and test
OnWN 2012/2013 test
tweet-news (en)
SMTeuroparl 2012 test
SMTnews 2012 test
Wikipedia (es) SMTeuroparl 2012 train
news (es) SMTeuroparl 2012 train
Table 8: Training data used for the STS-2014 data
sets (task 10).
systems, showing that the SM (string-matching)
feature set is effective for the prediction of sim-
ilarity scores. Particularly, in the paragraph-to-
sentence data set, which has the longest text,
RUN2 obtained the best official score. In contrast,
the scores obtained for the phrase-to-word and
word-to-sense data sets were considerably lower
in comparison with the top system, but still com-
petitive against most of the other participating sys-
tems.
3.3 Task 10: Multilingual Semantic
Similarity
The SemEval-2014 task 10 (multilingual seman-
tic similarity) (Agirre et al., 2014) is the sequel of
the semantic textual similarity (STS) evaluations
at SemEval in the past two years (Agirre et al.,
2012; Agirre et al., 2013). This year 6 test data
sets were proposed in English and 2 data sets in
Spanish. Similarly to the 2013 campaign, there is
not explicit training data for each data set. Conse-
quently, different data sets from the previous STS
evaluations were selected to be used as training
data for the new data sets. The selection criterion
was the average character length and type of the
texts. The Table 8 shows the training data used for
each test data set.
3.3.1 English Subtask
The RUN1 for the English data sets was produced
with a parameterized similarity function based on
the SM feature set and the symmetrized Tversky?s
index (Tversky, 1977; Jimenez et al., 2013a). For
a detailed description of this function and its pa-
rameters, please refer to the STS
sim
feature in
the system description paper of the NTNU team
(Lynum et al., 2014). The parameters used in that
739
System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank
SimCompass run1 0.811 0.742 0.415 0.356 1st/38
ECNU run1 0.834 0.771 0.315 0.269 2nd/38
UNAL-NLP run2 0.837 0.738 0.274 0.256 3rd/38
SemantiKLUE run1 0.817 0.754 0.215 0.314 4th/38
UNAL-NLP run1 0.817 0.739 0.252 0.249 5th/38
Table 7: Official results for task 3 (Pearson?s correlation).
Data ? ? bias p ?
?
?
?
bias
?
OnWN 0.53 -0.53 1.01 1.00 -4.89 0.52 0.46
headlines 0.36 -0.29 4.17 0.85 -4.50 0.43 0.19
images 1.12 -1.11 0.93 0.64 -0.98 0.50 0.11
deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02
deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63
tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45
Table 9: Optimal parameters used for task 10 in
English.
function are reported in Table 9. Unlike subsec-
tion 3.1.1 where the Dice?s coefficient was used as
the text similarity function, here the symmetrical
Tversky?s index (eq. 2) was reused generating the
three additional parameters marked with apostro-
phe (?
?
, ?
?
and bias
?
).
For the RUN2 the SM feature set was extracted
from all the data sets in English (en) listed in Table
8. Then, a REPtree (Quinlan, 1987) boosted with
50 bagging iterations (Breiman, 1996) was trained
using the training data sets selected for each test
data set. Finally, these machine learning models
produced the similarity score predictions for each
test data set.
The RUN3 was identical to the RUN2 but in-
cluded additional feature sets apart from SM,
namely: ESA, POS and WN. The WN feature
set is the same as SM, but replacing the word-to-
word similarity function in eq. 2 by the path mea-
sure from the WordNet::Similarity package (Ped-
ersen et al., 2004).
3.3.2 Spanish Subtask
The Spanish system was based entirely in the SM
feature set with some small changes for adapt-
ing the system to Spanish. Basically, the list of
English stop-words was replaced by the Spanish
stop-words provided by the NLTK. In addition,
the Porter stemmer was replaced by its Spanish
equivalent, i.e. the Snowball stemmer for Span-
ish. The RUN1 is equivalent to the RUN1 for the
data set run1 run2 run3
deft-forum 0.5043 0.3826 0.4607
deft-news 0.7205 0.7305 0.7216
headlines 0.7616 0.7645 0.7605
images 0.8071 0.7706 0.7782
OnWN 0.7823 0.8268 0.8426
tweet-news 0.6145 0.4028 0.6583
mean (en) 0.7113 0.6573 0.7209
official rank (en) 12th/38 22th/38 9th/38
Wikipedia 0.7804 0.7566 0.6894
news 0.8154 0.7829 0.7965
mean (es) 0.8013 0.7723 0.7533
official rank (es) 3rd/22 9th/22 12th/22
Table 10: Official results for the task 10 (Pearson?s
correlation).
English subtask described in the previous subsec-
tion. The parameters used for the text similarity
function were ? = 1.16, ? = 1.08, bias = 0.02,
p = 1.02, ?
?
= 1.54, ?
?
= 0.08 and bias
?
= 1.37.
The description and meaning of these parameters
can be found in (Lynum et al., 2014) associated to
the STS
sim
feature.
The RUN2 was obtained using the SM feature
set and a linear regressor for generating the simi-
larity score predictions. Similarity, RUN3 used the
same feature set SM in combination with a REP-
tree boosted with 30 bagging iterations.
3.3.3 Results
The results for the 3 submitted runs correspond-
ing to the 2 sub tasks (English and Spanish) are
shown in Table 10. It is important to note that
the RUN1 for the Wikipedia data set in Spanish
was the top system among 22 participating sys-
tems. This result is remarkable given that this sys-
tem was trained with a data set in English showing
the domain adaptation ability of the soft cardinal-
ity approach.
740
4 Conclusions
We participated in the SemEval-2014 task 1, 3 and
10 with an uniform approach based on soft cardi-
nality features, obtaining pretty satisfactory results
in all data sets, tasks and sub tasks. This approach
has been used since SemEval-2012 in all versions
of the following tasks: semantic textual similar-
ity (Jimenez et al., 2012b; Jimenez et al., 2013a),
typed similarity (Croce et al., 2013), cross-lingual
textual entailment (Jimenez et al., 2012a; Jimenez
et al., 2013c), student response analysis (Jimenez
et al., 2013b), and multilingual semantic textual
similarity (Lynum et al., 2014). In the majority
of the cases, the systems based on soft cardinality,
built by us and other teams, have been among the
top systems. Given the uniformity of the approach,
the consistency of the results, the few computa-
tional resources required and the overall concep-
tual simplicity, the soft cardinality is established
as a useful tool for a wide spectrum of applications
in natural language processing.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre Aitor. 2012. SemEval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval@*SEM 2012), Montreal,Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity, including a pilot
on typed-similarity. Atlanta, Georgia, USA.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Weibe. 2014. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using WordNet. In Computational linguis-
tics and intelligent text processing, page 136?145.
Springer.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings
of the ACL Workshop on Empirical Modeling of Se-
mantic Equivalence and Entailment, EMSEE ?05,
page 13?18, Stroudsburg, PA, USA.
Danilo Croce, Valerio Storch, P. Annesi, and Roberto
Basili. 2012. Distributional compositional seman-
tics and text similarity. In 2012 IEEE Sixth Interna-
tional Conference on Semantic Computing (ICSC),
pages 242?249, September.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. UNITOR-CORE TYPED: Combining text
similarity and semantic filters through SV regres-
sion. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: SemanticTextual Similarity, page 59, Atlanta,
Georgia, USA.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, page 449?454,
Genoa, Italy, May.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, IJCAI?07, page 1606?1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bern-
hard Pfahringer. 2009. The WEKA data min-
ing software: An update. SIGKDD Explorations,
11(1):10?18.
Paul Jaccard. 1901. Etude comparative de la distribu-
tion florare dans une portion des alpes et des jura.
Bulletin de la Soci?et?e Vaudoise des Sciences Na-
turelles, pages 547?579.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardi-
nality. In Edgar Chavez and Stefano Lonardi, ed-
itors, String Processing and Information Retrieval,
volume 6393 of LNCS, pages 297?302. Springer,
Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized
similarity function for text comparison. In SemEval
2012, Montreal, Canada.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: Learning adap-
tive similarity functions for cross-lingual textual en-
tailment. In SemEval 2012, Montreal, Canada.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013a. SOFTCARDINALITY-CORE: Im-
proving text overlap with distributional measures for
semantic textual similarity. In *SEM 2013, Atlanta,
Georgia, USA, June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013b. SOFTCARDINALITY: Hierarchical
text overlap for student response analysis. In Se-
mEval 2013, Atlanta, Georgia, USA, June.
741
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013c. SOFTCARDINALITY: Learning
to identify directional cross-lingual entailment from
cardinalities and SMT. In SemEval 2013, Atlanta,
Georgia, USA, June.
Karen Sp?arck Jones. 2004. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 60(5):493?502, October.
David Jurgens, Mohammad T. Pilehvar, and Roberto
Navigli. 2014. SemEval-2014 task 3: Cross-
level semantic similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinalty. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Marco Marelli, Stefano Menini, Marco Baroni, Lucia
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC, Reykjavik, Iceland, May.
Alvaro E. Monge and Charles Elkan. 1996. The field
matching problem: Algorithms and applications. In
Proceeding of the 2nd International Conference on
Knowledge Discovery and Data Mining (KDD-96),
pages 267?270, Portland, OR.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::similarity: measuring the
relatedness of concepts. In Proceedings HLT-
NAACL?Demonstration Papers, Stroudsburg, PA,
USA.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
J. Ross Quinlan. 1987. Simplifying decision
trees. International journal of man-machine studies,
27(3):221?234.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84(4):327?352, July.
742
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 743?747,
Dublin, Ireland, August 23-24, 2014.
UNAL-NLP: Cross-Lingual Phrase Sense Disambiguation with
Syntactic Dependency Trees
Emilio Silva-Schlenker
Departamento de Ling??stica
Universidad Nacional de Colombia
Departamento de Ingenier?a de Sistemas
Universidad de los Andes,
Bogot? D.C., Colombia
esilvas@unal.edu.co
Sergio Jimenez and Julia Baquero
Universidad Nacional de Colombia,
Bogot? D.C., Colombia
sgjimenezv@unal.edu.co
jmbauqerov@unal.edu.co
Abstract
In this paper we describe our participa-
tion in the SemEval 2014, Task 5, con-
sisting of the construction of a translation
assistance system that translates L1 frag-
ments, written in L2 context, to their cor-
rect L2 translation. Our approach con-
sists of a bilingual parallel corpus, a sys-
tem of syntactic features extraction and a
statistical memory-based classification al-
gorithm. Our system ranked 4th and 6th
among the 10 participating systems that
used the English-Spanish data set.
1 Introduction
An L2 writing assistant is a tool intended for lan-
guage learners who need to improve their writing
skills. This tool lets them write a text in L2, but fall
back to their native L1 whenever they are not sure
about a certain word or expression. In these cases,
the assistant automatically translates this text for
them (van Gompel et al., 2014).
Although at first glance this may be seen as
a classification problem, it might be better ful-
filled by a cross-lingual word sense disambigua-
tion (WSD) approach, which takes context into
account by means of contextual features used in
a machine learning setting. The main differences
between this and previous approaches to cross-
lingual WSD are the bilingual nature of the input
sentences (see section 2.3) and the annotation of
target phrases, rather than single words.
The remainder of this article is organized as fol-
lows. Section 2 describes the proposed method.
A description of the system we submitted, the ob-
tained results and an error analysis are discussed
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
in section 3. In section 4 we present a brief dis-
cussion about the results. Finally, in section 5 we
make some concluding remarks.
2 Method Description
The core of the proposed system uses techniques
from memory-based classification to find the most
appropriate translation of a target phrase in a
given context. It receives an input as in (1) and
yields an output as in (2).
(1) No creo que ella is coming.
(2) No creo que ella venga.
It does so on the basis of a syntactic selec-
tion of context features, a large bilingual parallel
corpus and a classifier built using the Tilburg
Memory-Based Learner, TiMBL (Daelemans et
al., 2010).
The proposed system consists of several stages.
First, a large bilingual corpus is aligned at word
and phrase level. Next, an index is built by each
phrase in the L1 side of the corpus to retrieve ef-
ficiently the occurrences of a particular L1 phrase
in the aligned corpus along with their translations
and contexts in L2 (subsection 2.1). Second, the
relevant contexts for each L1 phrase in the test set
(example sentences) are retrieved from the corpus
and a set of syntactic features are extracted from
each sentence (subsection 2.2). Third, a special
two-stage process is used to extract the same fea-
tures from the sentences in the test set to deal with
the fact that these sentences were written in two
languages (subsection 2.3). Finally, each target
phrase is translated using the IBL algorithm and
the translations were incorporated in the original
test sentences (subsection 2.4).
743
Input sentence Parallel example sentences
No creo que las necesidades
afectivas de las personas est?n
necesariamente linked al
matrimonio.
He said Boyd already linked
him to Brendan.
Dijo que Boyd ya le hab?a rela-
cionado con Brendan.
The three things are inextrica-
bly linked, and I have the for-
mula right here.
Las tres cosas est?n es-
trechamente vinculadas, y
tengo la f?rmula aqu?.
Table 1: An input sentence and 2 example sentences from Linguee.com.
2.1 Parallel Corpus Selection and
Preparation
As no training corpus was given prior to develop-
ing this system, finding and processing the most
suitable corpus for this task was paramount. As
the purpose of this system is to help language stu-
dents, the corpus needs to account for simple yet
correct everyday speech.
In an initial stage of development we opted
to use the 70-million sentences OpenSubtitles.org
corpus compiled by the Opus Project (Tiedemann,
2012), which includes many informal everyday ut-
terances, at the expense of a less accurate transla-
tion quality
1
. Although the use of this training cor-
pus yielded over 95% of recall on the trial corpus
given by the task organizers, only 80% of the trial
sentences had enough (>100) training examples in
order to produce a quality translation. To solve this
issue, an ad-hoc corpus compilation mechanism
was created by using the Linguee.com. Thus, a
set of parallel example sentences is retrieved from
Linguee.com by querying all the L1 target phrases
from the evaluation data (see an example in Table
1).
The corpus preparation procedure consisted of
several steps. The first step was to clean the cor-
pus with the Moses cleaning script (Koehn et al.,
2007). Next, the corpus was tokenized and PoS-
tagged using FreeLing (Padr? and Stanilovsky,
2012) (HMM tagger was used). After that, the
corpus was word-aligned using Giza++ (Och and
Ney, 2003) over Moses (Koehn et al., 2007). The
resulting alignment was then combined with the
tagged version of the corpus. Finally, a phrase in-
dex was built using a SMT phrase extraction algo-
rithm (Ling et al., 2010) including for each phrase
pointers to all its occurrences in the corpus for fur-
ther retrieval.
1
The EPPS corpus (Lambert et al., 2005) was very useful
as a training corpus in the developing stages of this system.
It was however not used in the final system training.
2.2 Syntactic Feature Extraction
The syntactic tags feature is a novel feature we are
introducing for the CLWSD problem (Lefever and
Hoste, 2013). They are linearizations of syntactic
dependency trees. These trees were built by Freel-
ing?s Txala Parser (Lloberes et al., 2010) and were
introduced as individual tags in a sentence analy-
sis by parsing the tree and mapping its leaves with
their corresponding order in the source sentence.
Then, each leaf?s label and parent number was ex-
tracted. For the root, the special parent tag ?S? was
used.
The WSD literature commonly distinguishes
between local and global context features (Mar-
tinez and Agirre, 2001). The former are extracted
from the neighboring words and the latter are ex-
tracted from words of the whole context provided
using some heuristic to select relevant. Unlike
global features, the relevance of the surrounding
words is not put into question or are weighted
by the degree of relevance according to their po-
sition in the sentence and lexicographic distance
from the target phrase (van Gompel, 2010). There
is a linguistic explanation as to why surrounding
words play a significant role in determining the
target?s translation. Often, these words have a di-
rect dependency relation with the target. Indeed,
physical closeness is an approximation of syntac-
tic relatedness. What we propose in this paper is
that the relevance of the context words for deter-
mining a correct translation is proportional to their
syntactic relatedness to the target, rather than their
physical closeness in the sentence. Unlike Mar-
tinez et al. (2002), what we propose here is to use
syntax as a feature selector, rather than as a feature
itself.
Instead of defining a local and a global set of
relevant words, we selected a single set of relevant
words according to their syntactic relation to the
target phrase. This set consisted of all the children
of the target words, and the parents of the main
target words. The main target words are the subset
744
0 1 2 3 4 5 6
Forms Las tres cosas est?n estrechamente vinculadas .
Lemmas el 3 cosa estar estrechamente vincular .
PoS Tags DA0FP0 Z NCFP000 VAIP3P0 RG VMP00PF Fp
Syn Tags espec:1 espec:2 subj:3 co-v:7 espec:5 att:3 ?:7
Table 2: Tagging of the sentence ?Las tres cosas est?n estrechamente vinculadas.?
of words with the highest number of (nested) chil-
dren within the target phrase. Table 3 features the
rules used for selecting the relevant words.
This Feature Extraction method uses the depen-
dency labels as a means of selecting only rele-
vant examples. Take for instance the example sen-
tences in Table 1. Given that the target word is an
attribute, the subject is included as a relevant fea-
ture, as per the last rule in Table 3. Any example
sentence in which there is no subject as the sib-
ling of the target word (as is the case for the first
example sentence in Table 1) will have an empty
feature, which increases its likelihood of not being
included in the training set of this sentence.
2.3 Test Data Pre-processing
The test data for this task is composed of bilin-
gual input sentences, making it impossible to ob-
tain a correct tagging or parsing. To overcome this
issue, a two-stage process wherein the first stage
obtains translations for the L1 portions was per-
formed. These plausible translations are obtained
by TiMBL using as features the neighboring words
of the target phrases. Once the sentences are in a
single language (L2) they are tagged and parsed
syntactically. Finally, the second stage consists in
applying the same feature selection algorithm pro-
posed in subsection 2.2.
2.4 Translation Selection
The processing of each sentence consists of sev-
eral steps. In the first step, the L1 target phrase
is searched for in the phrase index Given an L1
phrase, a binary search algorithm iterates through
the phrase index and returns an array of point-
ers
2
to the corpus. Then, a multi-threaded subrou-
tine reads the word-aligned bilingual corpus and
extracts all the referenced sentences. Thus, for
each input sentence, a set of example bilingual
word-aligned sentences is extracted from the cor-
pus. Relevant features are extracted according to
2
Given that line breaks are just regular characters, what is
actually referenced in the phrase index are byte offsets.
a syntactic analysis as explained in subsection 2.2,
and written to text files in the C4.5 format. The
features extracted from the example sentences, as
well as the L2 translations of the target phrases
in each sentence, are used as the training set for
TiMBL, while the features extracted from the in-
put sentence are used as its (singleton) test set.
The L2 translations of each target phrase in the
example sentences are used as the classes for the
training set, in order to turn a bilingual disam-
biguation problem into a machine learning clas-
sification problem. TiMBL learns how to classify
the training feature vectors into their correspond-
ing classes and then predicts the class for the test
set feature vector, i.e. its most likely translation
using an IBL algorithm (Aha et al., 1991), which
is a variation of the k-nearest neighbor classifier.
3 System Submissions
We submitted three result sets for the English-
Spanish language pair. Two of them were submit-
ted for the ?Best? evaluation type, and the other
one was submitted for the ?out-of-five? evaluation
type. The difference between these two evaluation
types is that out-of-five evaluation expects up to
five different translations for every target phrase,
while ?best? only accepts one. The evaluation met-
rics include accuracy and recall, and also a word-
based special type of accuracy, which takes into
account partially correct translations.
Of the two runs submitted in the ?Best? evalu-
ation type, Run1-best (see table 4) used our pro-
posed syntactic feature extraction method, while
Run2-best used a regular 2-word window around
the target phrase. For the Run1-oof we combined
the two methods mentioned above with different
values of k.
3.1 Results
The test data consisted in 500 sentences written in
Spanish, with target English phrases. The official
results obtained by our runs are shown in Table 4.
Our control run, Run2-best, yielded slightly
745
Case Rule Example
One of the target words is a
subject.
Include any sibling which is an
auxiliary or modal verb.
Our cat quiere comerse la en-
salada.
The parent of one of the main
target words is a coordinative
conjunction.
Include its closest sibling. No quer?a ni eat, ni dormir.
The parent of one of the main
target words is a relative pro-
noun.
Include its grandparent. No creo que ella is coming.
One of the target words is an
attribute.
Include any sibling which is
subject.
Mis t?as est?n very tired.
Table 3: Relevant word selection rules.
better results than our experimental run, Run1-
best. This means that our method of syntactic fea-
tures extraction did not improve translation qual-
ity.
3.2 Error Analysis
By analyzing our results, we detected three groups
of recurrent errors. The first group of errors is re-
lated to verb morphology, in which a single En-
glish verbal form corresponds to many Spanish
verbal forms. In these cases, our system often out-
puts an infinitive form or a past participle instead
of a finite verb.
The second group of errors we detected com-
prises incomplete translations. In these cases, a
single word in English has a multiword Spanish
translation, but our system often outputs a single-
word translation.
The third group of errors are related to English
words with multiple possible parts of speech, as
?flood?, which can be a noun but also a verb. Our
system tends to output nouns instead of verbs and
vice versa.
4 Discussion
There are two main reasons as to why the syntac-
tic feature extraction method did not work. The
first reason is related to the nature of the task; the
second is related to the scope of the method.
The fact that this task involved analyzing sen-
tences partly written in two languages made syn-
tactic analysis extremely difficult as dependencies
span all over the bilingual sentence. The best solu-
tion we found for this was to divide the operation
of the system in two stages, where the first one did
not involve syntactic dependencies and provided a
working translation, and the second one used this
first translation to perform a syntactic analysis and
then rerun the classification step. This, however,
favored error propagation. Although translation
quality did improve between the two stages, there
were many cases in which a bad initial translation
involved a bad syntactic analysis, which in turn re-
sulted in a bad final translation.
A more sophisticated version of his method was
initially developed for the English-Spanish lan-
guage pair and involved several language-specific
rules. However, we decided to make this method
language-independent, so we simplified it to its ac-
tual version. This simplified version uses syntactic
dependencies as feature selectors, but the features
themselves are regular lemma/PoS combinations,
which is not always the best feature choice.
5 Conclusion
Syntactic dependency relations are an important
means of analyzing the internal structure of a sen-
tence and can successfully be used to improve the
feature selection process in WSD. However, syn-
tactic parsing is far away from optimal in Spanish,
a fortiori if it involves sentences written in two lan-
guages. For this kind of task, perhaps a statistical
language model of L2 would have yielded better
results.
Acknowledgments
We would like to specially thank Professor Sil-
via Takahashi of Universidad de los Andes for her
continued advice and support. We would also like
to thank Pedro Rodr?guez for his development of
the Linguee crawler, Mar?a De-Arteaga, Alejandro
Riveros and David Hoyos for their useful sugges-
tions in the conception and development of this
project, and the rest of the UNAL-NLP team for
746
Run Recall Accuracy Word Accuracy Rank (runs) Rank (systems)
Run1-best 0.993 0.721 0.794 5 2
Run2-best 0.993 0.733 0.809 4 2
Run1-oof 0.993 0.823 0.880 6 3
Table 4: Official results.
their interest and encouragement. Many thanks to
Jay C. Soper for proof-reading this article.
References
David W. Aha, Dennis Kibler, and Marc K. Albert.
1991. Instance-based learning algorithms. Machine
Learning, 6:37?66.
Walter Daelemans, Jakub Zavrel, Ko Van der Sloot,
and Antal Van den Bosch. 2010. Timbl: Tilburg
memory-based learner. reference guide. ILK Re-
search Group, Tilburg University.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, and
Richard Zens. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, page 177?180.
Patrik Lambert, Adri? Gispert, Rafael Banchs, and
Jos? B. Mari?o. 2005. Guidelines for word align-
ment evaluation and manual alignment. Language
Resources and Evaluation, 39(4):267?285, Decem-
ber.
Els Lefever and V?ronique Hoste. 2013. Semeval-
2013 task 10: Cross-lingual word sense disambigua-
tion. In Second joint conference on lexical and com-
putational semantics, volume 2, page 158?166.
Wang Ling, Tiago Lu?s, Jo?o Gra?a, Lu?sa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT?10:
International Workshop on Spoken Language Trans-
lation, page 313?320.
Marina Lloberes, Irene Castell?n, and Llu?s Padr?.
2010. Spanish FreeLing dependency grammar. In
LREC, volume 10, page 693?699.
David Martinez and Eneko Agirre. 2001. Deci-
sion lists for english and basque. In The Proceed-
ings of the Second International Workshop on Eval-
uating Word Sense Disambiguation Systems, page
115?118.
David Mart?nez, Eneko Agirre, and Llu?s M?rquez.
2002. Syntactic features for high precision word
sense disambiguation. In Proceedings of the
19th international conference on Computational
linguistics-Volume 1, page 1?7.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Llu?s Padr? and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence, pages 2473?2479, Istambul, Turkey, May.
J?rg Tiedemann. 2012. Parallel data, tools and in-
terfaces in OPUS. In Proceedings of the Lan-
guage Resources and Evaluation Conference, page
2214?2218, Istambul, Turkey, May.
Maarten van Gompel, Iris Hendrickx, Antal van den
Bosh, Els Lefever, and V?ronique Hoste. 2014.
Semeval-2014 task 5: L2 writing assistant. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland,
August.
Maarten van Gompel. 2010. UvT-WSD1: a cross-
lingual word sense disambiguation system. In Pro-
ceedings of the 5th international workshop on se-
mantic evaluation, page 238?241, Uppsala, Sweden.
747
