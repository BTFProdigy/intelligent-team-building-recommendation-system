Proceedings of NAACL HLT 2007, Companion Volume, pages 149?152,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Agenda-Based User Simulation for
Bootstrapping a POMDP Dialogue System
Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye and Steve Young
Cambridge University Engineering Department
Trumpington Street, Cambridge, CB21PZ, United Kingdom
{js532, brmt2, kw278, hy216, sjy}@eng.cam.ac.uk
Abstract
This paper investigates the problem of boot-
strapping a statistical dialogue manager with-
out access to training data and proposes a new
probabilistic agenda-based method for simu-
lating user behaviour. In experiments with a
statistical POMDP dialogue system, the simu-
lator was realistic enough to successfully test
the prototype system and train a dialogue pol-
icy. An extensive study with human subjects
showed that the learned policy was highly com-
petitive, with task completion rates above 90%.
1 Background and Introduction
1.1 Bootstrapping Statistical Dialogue Managers
One of the key advantages of a statistical approach to Dia-
logue Manager (DM) design is the ability to formalise de-
sign criteria as objective reward functions and to learn an
optimal dialogue policy from real dialogue data. In cases
where a system is designed from scratch, however, it is
often the case that no suitable in-domain data is available
for training the DM. Collecting dialogue data without a
working prototype is problematic, leaving the developer
with a classic chicken-and-egg problem.
Wizard-of-Oz (WoZ) experiments can be carried out to
record dialogues, but they are often time-consuming and
the recorded data may show characteristics of human-
human conversation rather than typical human-computer
dialogue. Alternatively, human-computer dialogues can
be recorded with a handcrafted DM prototype but neither
of these two methods enables the system designer to test
the implementation of the statistical DM and the learn-
ing algorithm. Moreover, the size of the recorded corpus
(typically  103 dialogues) usually falls short of the re-
quirements for training a statistical DM (typically 104
dialogues).
1.2 User Simulation-Based Training
In recent years, a number of research groups have inves-
tigated the use of a two-stage simulation-based setup. A
statistical user model is first trained on a limited amount
of dialogue data and the model is then used to simulate
dialogues with the interactively learning DM (see Schatz-
mann et al (2006) for a literature review).
The simulation-based approach assumes the presence
of a small corpus of suitably annotated in-domain dia-
logues or out-of-domain dialogues with a matching dia-
logue format (Lemon et al, 2006). In cases when no such
data is available, handcrafted values can be assigned to
the model parameters given that the model is sufficiently
simple (Levin et al, 2000; Pietquin and Dutoit, 2005) but
the performance of dialogue policies learned this way has
not been evaluated using real users.
1.3 Paper Outline
This paper presents a new probabilistic method for simu-
lating user behaviour based on a compact representation
of the user goal and a stack-like user agenda. The model
provides an elegant way of encoding the relevant dialogue
history from a user?s point of view and has a very small
parameter set so that manually chosen priors can be used
to bootstrap the DM training and testing process.
In experiments presented in this paper, the agenda-
based simulator was used to train a statistical POMDP-
based (Young, 2006; Young et al, 2007) DM. Even with-
out any training of its model parameters, the agenda-
based simulator was able to produce dialogue behaviour
realistic enough to train a competitive dialogue policy.
An extensive study1 with 40 human subjects showed that
task completion with the learned policy was above 90%
despite a mix of native and non-native speakers.
1This research was partly funded by the EU FP6 TALK
Project. The system evaluation was conducted in collabora-
tion with O. Lemon, K. Georgila and J. Henderson at Edinburgh
University and their work is gratefully acknowledged.
149
2 Agenda-Based Simulation
2.1 User Simulation at a Semantic Level
Human-machine dialogue can be formalised on a seman-
tic level as a sequence of state transitions and dialogue
acts2. At any time t, the user is in a state S, takes ac-
tion a
u
, transitions into the intermediate state S?, receives
machine action a
m
, and transitions into the next state S??
where the cycle restarts.
S ? a
u
? S
?
? a
m
? S
??
? ? ? ? (1)
Assuming a Markovian state representation, user be-
haviour can be decomposed into three models: P (a
u
|S)
for action selection, P (S?|a
u
, S) for the state transition
into S?, and P (S??|a
m
, S
?
) for the transition into S??.
2.2 Goal- and Agenda-Based State Representation
Inspired by agenda-based methods to dialogue manage-
ment (Wei and Rudnicky, 1999) the approach described
here factors the user state into an agenda A and a goal G.
S = (A,G) and G = (C,R) (2)
During the course of the dialogue, the goal G ensures that
the user behaves in a consistent, goal-directed manner.
G consists of constraints C which specify the required
venue, eg. a centrally located bar serving beer, and re-
quests R which specify the desired pieces of information,
eg. the name, address and phone number (cf. Fig. 1).
The user agenda A is a stack-like structure containing
the pending user dialogue acts that are needed to elicit the
information specified in the goal. At the start of the dia-
logue a new goal is randomly generated using the system
database and the agenda is initially populated by convert-
ing all goal constraints into inform acts and all goal re-
quests into request acts. A bye act is added at the bottom
of the agenda to close the dialogue.
As the dialogue progresses the agenda and goal are dy-
namically updated and acts are selected from the top of
the agenda to form user acts a
u
. In response to incom-
ing machine acts a
m
, new user acts are pushed onto the
agenda and no longer relevant ones are removed. The
agenda thus serves as a convenient way of tracking the
progress of the dialogue as well as encoding the rele-
vant dialogue history. Acts can also be temporarily stored
when actions of higher priority need to be issued first,
hence providing the simulator with a simple model of
user memory3.
2In this paper, the terms dialogue act and dialogue action
are used interchangeably. The notation act(a=x, b=y,...) is used
to represent a dialogue act of a given type act (such as inform
or request with items a = x, b = y, etc.
3See Fig. 1, turns 1-3, for an example: System turn 1 ?What
pricerange?? triggers the user act inform(pricerange=cheap)
to be pushed onto the agenda but it is not executed until turn 3
because negate(drinks=beer) is issued first.
C
0
=
[
type = bar
drinks = beer
area = central
]
R
0
=
[
name =
addr =
phone =
]
Sys 0 Hello, how may I help you?
A
1
=
?
?
?
?
?
?
?
inform(type = bar)
inform(drinks = beer)
inform(area = central)
request(name)
request(addr)
request(phone)
bye()
?
?
?
?
?
?
?
Usr 1 I?m looking for a nice bar serving beer.
Sys 1 Ok, a wine bar. What pricerange?
A
2
=
?
?
?
?
?
?
?
negate(drinks = beer)
inform(pricerange = cheap)
inform(area = central)
request(name)
request(addr)
request(phone)
bye()
?
?
?
?
?
?
?
Usr 2 No, beer please!
Sys 2 A bar serving beer, correct?
A
3
=
?
?
?
?
?
?
?
affirm()
inform(pricerange = cheap)
inform(area = central)
request(name)
request(addr)
request(phone)
bye()
?
?
?
?
?
?
?
Usr 3 Yeah something cheap in the town centre.
Sys 3 Murphy?s on Main Square serves cheap beer.
A
4
=
[
request(phone)
bye()
]
Usr 4 Ok, and what?s the phone number?
Sys 4 The number is 796 69 94.
A
5
=
[
bye()
]
Usr 5 Thanks, goodbye!
Figure 1: Sample dialogue and agenda sequence
2.3 User Act Selection
At any time during the dialogue, the updated agenda of
length N contains all dialogue acts the user intends to
convey to the system. Since the agenda is ordered ac-
cording to priority, with A[N ] denoting the top and A[1]
denoting the bottom item, selecting the next user act sim-
plifies to popping n items off the top of the stack. Hence,
letting a
u
[i] denote the ith item in the user act a
u
a
u
[i] := A[N ? n + i] ?i ? [1..n], 1 ? n ? N. (3)
Using A[N?n+1..N ] as a Matlab-like shorthand nota-
150
tion for the top n items on A, the action selection model
becomes a Dirac delta function
P (a
u
|S) = P (a
u
|A,G) = ?(a
u
, A[N? n+1..N ]) (4)
where the random variable n corresponds to the level
of initiative taken by the simulated user. In a statistical
model the probability distribution over integer values for
n should be conditioned on A and learned from dialogue
data. For the purposes of bootstrapping the system, n can
be assumed independent of A and any distribution P (n)
that places the majority of its probability mass on small
values of n can be used.
2.4 State Transition Model
The factorisation of S into A and G can now be ap-
plied to the state transition models P (S?|a
u
, S) and
P (S
??
|a
m
, S
?
). Letting A? denote the agenda after select-
ing a
u
(as explained in the previous subsection) and using
N
?
= N ? n to denote the size of A?, we have
A
?
[i] := A[i] ?i ? [1..N
?
]. (5)
Using this definition of A? and assuming that the goal
remains constant when the user executes a
u
, the first state
transition depending on a
u
simplifies to
P (S
?
|a
u
, S) = P (A
?
, G
?
|a
u
, A,G)
= ?(A
?
, A[1..N
?
])?(G
?
, G). (6)
Using S = (A,G), the chain rule of probability, and rea-
sonable conditional independence assumptions, the sec-
ond state transition based on a
m
can be decomposed into
goal update and agenda update modules:
P (S
??
|a
m
, S
?
)
= P (A
??
|a
m
, A
?
, G
??
)
? ?? ?
agenda update
P (G
??
|a
m
, G
?
)
? ?? ?
goal update
. (7)
When no restrictions are placed on A?? and G??, the space
of possible state transitions is vast. The model parame-
ter set is too large to be handcrafted and even substantial
amounts of training data would be insufficient to obtain
reliable estimates. It can however be assumed that A?? is
derived from A? and that G?? is derived from G? and that
in each case the transition entails only a limited number
of well-defined atomic operations.
2.5 Agenda Update Model for System Acts
The agenda transition from A? to A?? can be viewed as a
sequence of push-operations in which dialogue acts are
added to the top of the agenda. In a second ?clean-up?
step, duplicate dialogue acts, null() acts, and unnecessary
request() acts for already filled goal request slots must
be removed but this is a deterministic procedure so that it
can be excluded in the following derivation for simplicity.
Considering only the push-operations, the items 1 to N ?
at the bottom of the agenda remain fixed and the update
model can be rewritten as follows:
P (A
??
|a
m
, A
?
, G
??
)
= P (A
??
[1..N
??
]|a
m
, A
?
[1..N
?
], G
??
) (8)
= P (A
??
[N
?
+1..N
??
]|a
m
, G
??
)
? ?(A
??
[1..N
?
], A
?
[1..N
?
]). (9)
The first term on the RHS of Eq. 9 can now be further
simplified by assuming that every dialogue act item in
a
m
triggers one push-operation. This assumption can be
made without loss of generality, because it is possible to
push a null() act (which is later removed) or to push an
act with more than one item. The advantage of this as-
sumption is that the known number M of items in a
m
now determines the number of push-operations. Hence
N
??
= N
?
+ M and
P (A
??
[N
?
+1..N
??
]|a
m
, G
??
)
= P (A
??
[N
?
+1..N
?
+M ]|a
m
[1..M ], G
??
) (10)
=
M
?
i=1
P (A
??
[N
?
+i]|a
m
[i], G
??
) (11)
The expression in Eq. 11 shows that each item a
m
[i] in
the system act triggers one push operation, and that this
operation is conditioned on the goal. This model is now
simple enough to be handcrafted using heuristics. For ex-
ample, the model parameters can be set so that when the
item x=y in a
m
[i] violates the constraints in G??, one of
the following is pushed onto A??: negate(), inform(x=z),
deny(x=y, x=z), etc.
2.6 Goal Update Model for System Acts
The goal update model P (G??|a
m
, G
?
) describes how the
user constraints C ? and requests R? change with a given
machine action a
m
. Assuming that R?? is conditionally
independent of C? given C ?? it can be shown that
P (G
??
|a
m
, G
?
)
= P (R
??
|a
m
, R
?
, C
??
)P (C
??
|a
m
, R
?
, C
?
). (12)
To restrict the space of transitions from R? to R?? it
can be assumed that the request slots are independent and
each slot (eg. addr,phone,etc.) is either filled using infor-
mation in a
m
or left unchanged. Using R[k] to denote the
k?th request slot, we approximate that the value of R??[k]
only depends on its value at the previous time step, the
value provided by a
m
, and M(a
m
, C
??
) which indicates
a match or mismatch between the information given in
a
m
and the goal constraints.
P (R
??
|a
m
, R
?
, C
??
)
=
?
k
P (R
??
[k]|a
m
, R
?
[k],M(a
m
, C
??
)). (13)
151
To simplify P (C ??|a
m
, R
?
, C
?
) we assume that C ?? is
derived from C ? by either adding a new constraint, set-
ting an existing constraint slot to a different value (eg.
drinks=dontcare), or by simply changing nothing. The
choice of transition does not need to be conditioned on
the full space of possible a
m
, R
? and C ?. Instead it can
be conditioned on simple boolean flags such as ?Does a
m
ask for a slot in the constraint set??, ?Does a
m
signal that
no item in the database matches the given constraints??,
etc. The model parameter set is then sufficiently small for
handcrafted values to be assigned to the probabilities.
3 Evaluation
3.1 Training the HIS Dialogue Manager
The Hidden Information State (HIS) model is the first
trainable and scalable implementation of a statistical
spoken dialog system based on the Partially-Observable
Markov Decision Process (POMDP) model of dialogue
(Young, 2006; Young et al, 2007; Williams and Young,
2007). POMDPs extend the standard Markov-Decision-
Process model by maintaining a belief space, i.e. a proba-
bility distribution over dialogue states, and hence provide
an explicit model of the uncertainty present in human-
machine communication.
The HIS model uses a grid-based discretisation of the
continuous state space and online -greedy policy iter-
ation. Fig. 2 shows a typical training run over 60,000
simulated dialogues, starting with a random policy. User
goals are randomly generated and an (arbitrary) reward
function assigning 20 points for successful completion
and -1 for every dialogue turn is used. As can be seen, di-
alogue performance (defined as the average reward over
1000 dialogues) converges after roughly 25,000 iterations
and asymptotes to a value of approx. 14 points.
Figure 2: Training a POMDP system
3.2 Experimental Evaluation and Results
A prototype HIS dialogue system with a learned policy
was built for the Tourist Information Domain and exten-
sively evaluated with 40 human subjects including native
and non-native speakers. A total of 160 dialogues with
21667 words was recorded and the average Word-Error-
Rate was 29.8%. Task scenarios involved finding a spe-
cific bar, hotel or restaurant in a fictitious town (eg. the
address of a cheap, Chinese restaurant in the west).
The performance of the system was measured based
on the recommendation of a correct venue, i.e. a venue
matching all constraints specified in the given task (all
tasks were designed to have a solution). Based on this
definition, 145 out of 160 dialogues (90.6%) were com-
pleted successfully, and the average number of turns to
completion was 5.59 (if no correct venue was offered the
full number of turns was counted).
4 Summary and Future Work
This paper has investigated a new agenda-based user sim-
ulation technique for bootstrapping a statistical dialogue
manager without access to training data. Evaluation re-
sults show that, even with manually set model parame-
ters, the simulator produces dialogue behaviour realistic
enough for training and testing a prototype system. While
the results demonstrate that the learned policy works well
for real users, it is not necessarily optimal. The next step
is hence to use the recorded data to train the simulator,
and to then retrain the DM policy.
References
O. Lemon, K. Georgila, and J. Henderson. 2006. Evaluating
Effectiveness and Portability of Reinforcement Learned Di-
alogue Strategies with real users: the TALK TownInfo Eval-
uation. In Proc. of IEEE/ACL SLT, Palm Beach, Aruba.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A Stochastic
Model of Human-Machine Interaction for Learning Dialog
Strategies. IEEE Trans. on Speech and Audio Processing,
8(1):11?23.
O. Pietquin and T. Dutoit. 2005. A probabilistic framework for
dialog simulation and optimal strategy learning. IEEE Trans.
on Speech and Audio Processing, Special Issue on Data Min-
ing of Speech, Audio and Dialog.
J. Schatzmann, K. Weilhammer, M.N. Stuttle, and S. Young.
2006. A Survey of Statistical User Simulation Tech-
niques for Reinforcement-Learning of Dialogue Manage-
ment Strategies. Knowledge Engineering Review, 21(2):97?
126.
X Wei and AI Rudnicky. 1999. An agenda-based dialog man-
agement architecture for spoken language systems. In Proc.
of IEEE ASRU. Seattle, WA.
J. Williams and S. Young. 2007. Partially Observable Markov
Decision Processes for Spoken Dialog Systems. Computer
Speech and Language, 21(2):231?422.
S. Young, J. Schatzmann, K. Weilhammer, and H. Ye. 2007.
The Hidden Information State Approach to Dialog Manage-
ment. In Proc. of ICASSP, Honolulu, Hawaii.
S. Young. 2006. Using POMDPs for Dialog Management. In
Proc. of IEEE/ACL SLT, Palm Beach, Aruba.
152
NAACL HLT Demonstration Program, pages 27?28,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
The Hidden Information State Dialogue Manager:
A Real-World POMDP-Based System
Steve Young, Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye
Cambridge University Engineering Department
Trumpington Street, Cambridge, CB21PZ, United Kingdom
{sjy, js532, brmt2, kw278, hy216}@eng.cam.ac.uk
Abstract
The Hidden Information State (HIS)
Dialogue System is the first trainable
and scalable implementation of a spoken
dialog system based on the Partially-
Observable Markov-Decision-Process
(POMDP) model of dialogue. The system
responds to n-best output from the speech
recogniser, maintains multiple concurrent
dialogue state hypotheses, and provides
a visual display showing how competing
hypotheses are ranked. The demo is
a prototype application for the Tourist
Information Domain and achieved a task
completion rate of over 90% in a recent
user study.
1 Partially Observable Markov Decision
Processes for Dialogue Systems
Recent work on statistical models for spoken di-
alogue systems has argued that Partially Observ-
able Markov Decision Processes (POMDPs) provide
a principled mathematical framework for modeling
the uncertainty inherent in human-machine dialogue
(Williams, 2006; Young, 2006; Williams and Young,
2007). Briefly speaking, POMDPs extend the tra-
ditional fully-observable Markov Decision Process
(MDP) framework by maintaining a belief state, ie.
a probability distribution over dialogue states. This
enables the dialogue manager to avoid and recover
from recognition errors by sharing and shifting prob-
ability mass between multiple hypotheses of the cur-
rent dialogue state. The framework also naturally
incorporates n-best lists of multiple recognition hy-
potheses coming from the speech recogniser.
Due to the vast number of possible dialogue states
and policies, the use of POMDPs in practical dia-
logue systems is far from straightforward. The size
of the belief state scales linearly with the number of
dialogue states and belief state updates at every turn
during a dialogue require all state probabilities to be
recomputed. This is too computationally intensive
to be practical with current technology. Worse than
that, the complexity involved in policy optimisation
grows exponentially with the number of states and
system actions and neither exact nor approximate al-
gorithms exist that provide a tractable solution for
systems with thousands of states.
2 The Hidden Information State (HIS)
Dialogue Manager
The Hidden Information State (HIS) dialogue man-
ager presented in this demonstration is the first train-
able and scalable dialogue system based on the
POMDP model. As described in (Young, 2006;
Young et al, 2007) it partitions the state space using
a tree-based representation of user goals so that only
a small set of partition beliefs needs to be updated
at every turn. In order to make policy optimisation
tractable, a much reduced summary space is main-
tained in addition to the master state space. Policies
are optimised in summary space and the selected
summary actions are then mapped back to master
space to form system actions. Apart from some very
simple ontology definitions, the dialog manager has
no application dependent heuristics.
The system uses a grid-based discretisation of the
27
Figure 1: The HIS Demo System is a Tourist Infor-
mation application for a fictitious town
state space and online -greedy policy optimisation.
While this offers the potential for online adaptation
with real users at a later stage, a simulated user is
needed to bootstrap the training process. A novel
agenda-based simulation technique was used for this
step, as described in (Schatzmann et al, 2007).
3 The HIS Demo System
The HIS demo system is a prototype application for
the Tourist Information domain. Users are assumed
to be visiting a fictitious town called ?Jasonville?
(see Fig. 1) and need to find a suitable hotel, bar
or restaurant subject to certain constraints. Exam-
ples of task scenarios are ?finding a cheap Chinese
restaurant near the post office in the centre of town?
or ?a wine bar with Jazz music on the riverside?.
Once a venue is found, users may request further in-
formation such as the phone number or the address.
At run-time, the system provides a visual display
(see Fig. 2) which shows how competing dialogue
state hypotheses are being ranked. This allows de-
velopers to gain a better understanding of the inter-
nal operation of the system.
4 Demo System Performance
In a recent user study the demo system was evalu-
ated by 40 human subjects. In total, 160 dialogues
were recorded with an average Word-Error-Rate of
29.8%. The performance of the system was mea-
sured based on the recommendation of a correct
venue and achieved a task completion rate of 90.6%
with an average number of 5.59 dialogue turns to
completion (Thomson et al, 2007).
Figure 2: A system screenshot showing the ranking
of competing dialogue state hypotheses
The results demonstrate that POMDPs facilitate
design and implementation of spoken dialogue sys-
tems, and that the implementation used in the HIS
dialogue manager can be scaled to handle real world
tasks. The user study results also show that a
simulated user can be successfully used to train a
POMDP dialogue policy that performs well in ex-
periments with real users.
5 Accompanying materials
The demo system and related materials are accessi-
ble online at our website
http://mi.eng.cam.ac.uk/research/dialogue/.
References
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-Based User Simulation for Boot-
strapping a POMDP Dialogue System. In Proceedings of
HLT/NAACL, Rochester, NY.
B. Thomson, J. Schatzmann, K. Weilhammer, H. Ye, and
S. Young. 2007. Training a real-world POMDP-based Di-
alogue System. In Proceedings of Bridging the Gap: Aca-
demic and Industrial Research in Dialog Technology, Work-
shop at HLT/NAACL, Rochester, NY.
J. D. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Systems.
Computer Speech and Language, 21(2):231?422.
J. D. Williams. 2006. Partially Observable Markov Decision
Processes for Spoken Dialogue Management. Ph.D. thesis,
University of Cambridge.
S. Young, J. Schatzmann, K. Weilhammer, and H. Ye. 2007.
The Hidden Information State Approach to Dialog Manage-
ment. In Proc. of ICASSP (forthcoming), Honolulu, Hawaii.
S. Young. 2006. Using POMDPs for Dialog Management. In
Proc. of IEEE/ACL SLT, Palm Beach, Aruba.
28
Using Wizard-of-Oz simulations to bootstrap Reinforcement-Learning-
based dialog management systems 
Jason D. Williams Steve Young 
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, United Kingdom 
{jdw30,sjy}@eng.cam.ac.uk 
 
Abstract 
This paper describes a method for ?boot-
strapping? a Reinforcement Learning-
based dialog manager using a Wizard-of-
Oz trial.  The state space and action set 
are discovered through the annotation, 
and an initial policy is generated using a 
Supervised Learning algorithm.  The 
method is tested and shown to create an 
initial policy which performs significantly 
better and with less effort than a hand-
crafted policy, and can be generated using 
a small number of dialogs. 
1 Introduction and motivation 
Recent work has successfully applied Rein-
forcement Learning (RL) to learning dialog strat-
egy from experience, typically formulating the 
problem as a Markov Decision Process (MDP).  
(Walker et al, 1998; Singh et al, 2002; Levin et 
al., 2000). Despite successes, several open 
questions remain, especially the issue of how to 
create (or ?bootstrap?) the initial system prior to 
data becoming available from on-line operation.  
This paper proceeds as follows.  Section 2 out-
lines the core elements of an MDP and issues re-
lated to applying an MDP to dialog management.  
Sections 3 and 4 detail a method for addressing 
these issues, and the procedure used to test the 
method, respectively.  Sections 5-7 present the re-
sults, a discussion, and conclusions, respectively.  
2 Background 
An MDP is composed of a state space, an action 
set, and a policy which maps each state to one ac-
tion.  Introducing a reward function allows us to 
create or refine the policy using RL.  (Sutton and 
Barto, 1998).  
When the MDP framework is applied to dialog 
management, the state space is usually constructed 
from vector components including information 
state, dialog history, recognition confidence, data-
base status, etc.  In most of the work to date both 
the state space and action set are hand selected, in 
part to ensure a limited state space, and to ensure 
training can proceed using a tractable number of 
dialogs.  However, hand selection becomes im-
practical as system size increases, and automatic 
generation/selection of these elements is currently 
an open problem, closely related to the problem of 
exponential state space size.  
3 A method for bootstrapping RL-based 
systems 
Here we propose a method for ?bootstrapping? an 
MDP-based system; specifically, we address the 
choice of the state representation and action set, 
and the creation of an initial policy. 
3.1 Step 1: Conduct Wizard-of-Oz dialogs 
The method commences with ?talking wizard? 
interactions in which either the wizard?s voice is 
disguised, or a Text-to-speech engine is used. We 
choose human/wizard rather than human/human 
dialogs as people behave differently toward (what 
they perceive to be) machines and other people as 
discussed in J?nsson and Dahlbick, 1988 and also 
validated in Moore and Browning, 1992. The dia-
log, including wizard?s interaction with back-end 
data sources is recorded and transcribed. 
3.2 Step 2: Exclude out-of-domain turns 
The wizard will likely handle a broader set of re-
quests than the system will ultimately be able to 
cover; thus some turns must be excluded.  Step 2 
begins by formulating a list of tasks which are to 
be included in the final system?s repertoire; turns 
dealing with tasks outside this repertoire are la-
beled out-of-domain (OOD) and excluded. 
This step takes an approach which is analogous 
to, but more simplistic than ?Dialogue Distilling? 
(Larsson et al, 2000) which changes, adds and re-
moves portions of turns or whole turns.  Here rules 
simply stipulate whether to keep a whole turn. 
3.3 Step 3: Enumerate action set and state 
space 
Next, the in-domain turns are annotated with dia-
log acts.  Based on these, an action set is enumer-
ated, and a set of state parameters and their 
possible values to form a vector describing the 
state space is determined, including: 
? Information state (e.g., departure-city, arri-
val-city) from the user and database. 
? The confidence/confirmation status of in-
formation state variables. 
? Expressed user goal and/or system goal. 
? Low-level turn information (e.g., yes/no re-
sponses, backchannel, ?thank you?, etc.). 
? Status of database interactions (e.g., when a 
form can be submitted or has been returned). 
A variety of dialog-act tagging taxonomies ex-
ist in the literature.  Here we avoid a tagging sys-
tem that relies on a stack or other recursive 
structure (for example, a goal or game stack) as it 
is not immediately clear how to represent a recur-
sive structure in a state space.  
In practice, many information state components 
are much less important than their corresponding 
confirmation status, and can be removed. 
Even with this reduction, the state space will be 
massive ? probably too large to ever visit all states.  
We propose using a parameterized value function -
- i.e., a value function that shares parameters 
across states (including states previously unob-
served).  One special case of this is state tying, in 
which a group of states share the same value func-
tion; an alternative is to use a Supervised Learning 
algorithm to estimate a value function. 
3.4 Step 4: Form an initial policy 
For each turn in the corpus, a vector is created rep-
resenting the current dialog state plus the subse-
quent wizard action.  Taking the action as the class 
variable, Supervised Learning (SL) is used to build 
a classifier which functions as the initial policy. 
Depending on the type of SL algorithm used, it 
may be possible to produce a prioritized list of ac-
tions rather than a single classification; in this case, 
this list can form an initial list of actions permitted 
in a given state. 
As noted by Levin et al (2000), supervised 
learning is not appropriate for optimizing dialog 
strategy because of the temporal/environmental 
nature of dialog.  Here we do not assert that the 
SL-learned policy will be optimal ? simply that it 
can be easily created, that it will be significantly 
better than random guessing, and better and 
cheaper to produce than creating a cursory hand-
crafted strategy. 
3.5 Limitations of the method 
This method has several obvious limitations: 
? Because a talking, perfect-hearing wizard is 
used, no/little account is taken of the recog-
nition errors to be expected with automated 
speech recognition (ASR).   
? Excluding too much in Step 2 may exclude 
actions or state components which would 
have ultimately produced a superior system. 
4 Experimental design 
The proposed approach has been tested using the 
Autoroute corpus of 166 dialogs, in which a talk-
ing wizard answered questions about driving direc-
tions in the UK (Moore and Browning, 1992).   
A small set of in-domain tasks was enumerated 
(e.g., gathering route details, outputting summary 
information about a route, disambiguation of place 
names, etc.), and turns which did not deal with 
these tasks were labeled OOD and excluded.  The 
latter included gathering the caller?s name and lo-
cation (?UserID?), the most common OOD type. 
The corpus was annotated using an XML 
schema to provide the following: 
? 15 information components were created 
(e.g., from, to, time, car-type).  
? Each information component was given a 
status: C (Confirmed), U (Unconfirmed), 
and NULL (Not known).   
? Up to 5 routes may be under discussion at 
once ? the state tracked the route under dis-
cussion (RUD), total number of routes (TR), 
and all information and status components 
for each route.  
? A component called flow tracked single-
turn dialog flow information from the caller 
(e.g., yes, no, thank-you, silence).  
? A component called goal tracked the (most 
recent) goal expressed by the user (e.g., 
plan-route, how-far).  Goal is empty 
unless explicitly set by the caller, and only 
one goal is tracked at a time.  No attempt is 
made to indicate if/when a goal has been 
satisfied.  
33 action types were identified, some of which 
take information slots as parameters (e.g., wh-
question, implicit-confirmation) . 
The corpus gave no indication of database in-
teractions other than what can be inferred from the 
dialog transcripts.  One common wizard action 
asked the caller to ?please wait? when the wizard 
was waiting for a database response.  To account 
for this, we provided an additional state component 
which indicated whether the database was working 
called db-request, which was set to true 
whenever the action taken was please-wait 
and false otherwise.  Other less common database 
interactions occurred when town names were am-
biguous or not found, and no attempt was made to 
incorporate this information into the state represen-
tation. 
The state space was constructed using only the 
status of the information slots (not the values); of 
the 15, 4 were occasionally expressed (e.g., day of 
the week) but not used to complete the transaction 
and were therefore excluded from the state space.   
Two turns of wizard action history were also in-
corporated.  This formulation of the state space 
leads to approximately 1033 distinct states. 
For evaluation of the method, a hand-crafted 
policy of 30 rules mapping states to actions was 
created by inspecting the dialogs.1 
5 Results 
Table 1 shows in-domain vs. out-of-domain wizard 
and caller turns.  Figures 1 through 4 show counts 
of flow values, goal values, action values, and state 
                                                          
1 It was not clear in what situations some of the actions should 
be used, so some (rare) actions were not covered by the rules. 
components, respectively.  The most common ac-
tion type was ?please-wait? (14.6% of actions). 
 
Turn 
type 
Total In  
domain 
OOD: 
User ID 
OOD: 
Other 
Wiz-
ard 
3155 
(100%)
2410 
(76.4%) 
594 
(18.8%) 
151 
(4.8%) 
Caller 2466 
(100%)
1713 
(69.5%) 
561 
(22.7%) 
192 
(7.8%) 
Table 1: In-domain and Out-of-domain (OOD) turns  
 
Criteria States Visits 
Visited only 
once 
1182  
(85.7%) 
1182  
(45.9%) 
Visited more 
than once 
without a con-
flict 
96  
(7.0%) 
353  
(13.7%) 
Visited more 
than once with 
conflict 
101 
(7.3%) 
1041  
(40.3%) 
TOTAL 1379  
(100%) 
2576  
(100%) 
Table 2: ?Conflicts? by state and visits 
 
Estimated action probabilities  Visits 
p(action taken | state) > p(any 
other action | state) 
774 (74.3%) 
p(action taken | state) = p(one 
or more other actions | state) > 
p(all remaining actions | state) 
119 (11.4%) 
p(action taken | state) < 
p(another action | state) 
148 (14.2%) 
TOTAL 1041 (100%) 
Table 3: Estimated probabilities in ?conflict? states 
 
Engine Class Precision 
Action-type only 72.7% jBNC 
Action-type & parameters 66.7% 
Action-type only 79.1% C4.5 
Action-type & parameters 72.9% 
Action-type only 58.4% Hand- 
craft Action-type & parameters 53.9% 
Table 4: Results from SL training and evaluation 
 
In some cases, the wizard took different actions 
in the same state; we labeled this situation a ?con-
flict.? Table 2 shows the number of distinct states 
that were encountered and, for states visited more 
than once, whether conflicting actions were se-
lected.  Of states with conflicts, Table 3 shows 
probabilities estimated from the corpus.  
The interaction data was then submitted to 2 SL 
pattern classifiers ? c4.5 using decision-trees 
(Quinlan, 1992) and jBNC using Na?ve Bayesians 
(Sacha, 2003).  Table 4 shows both algorithms? 10-
fold cross validation classification error rates 
classifying (1) the action type, and (2) the action 
type with parameters, as well as the results for the 
hand-crafted policy. 
Figure 5 show the 10-fold cross validation clas-
sification error rates for varying amounts of train-
ing data for the two SL algorithms classifying 
action-type and parameters. 
6 Discussion 
The majority of the data collected was ?usable?: 
although 26.7% of turns were excluded, 20.5% of 
these were due to a well-defined task not under 
study here (user identification), and only 6.1% fell 
outside of designated tasks.  That said, it may be 
desirable to impose a minimum threshold on how 
many times a flow, goal, or action must be ob-
served before adding it to the state space or action 
set given the ?long tails? of these elements.   
0
50
100
150
200
1 2 3 4 5 6 7 8 9 10 11 12
Flow component ID
D
ia
lo
gs
 c
on
ta
in
in
g 
Fl
ow
 ID
 
Figure 1: Dialogs containing flow components 
 
0
2
4
6
8
10
12
14
16
1 2 3 4 5 6 7 8 9 10 11 12 13
Goal component ID
D
ia
lo
gs
 c
on
ta
in
in
g 
G
oa
l
 
Figure 2: Dialogs containing goal components 
About half of the turns took place in states 
which were visited only once.  This confirms that 
massive amounts of data would be needed to ob-
serve all valid dialog states, and suggests dialogs 
do not confine themselves to familiar states. 
Within a given state, the wizard?s behavior is 
stochastic, occasionally deviating from an other-
wise static policy.  Some of this behavior results 
from database information not included in the cor-
pus and state space; in other cases, the wizard is 
making apparently random choices. 
 
0
50
100
150
200
1 5 9 13 17 21 25 29 33
Action ID
D
ia
lo
gs
 c
on
ta
in
in
g 
A
ct
io
n
 
Figure 3: Dialogs containing action types 
 
0
50
100
150
200
1 3 5 7 9 11 13
Component ID
D
ia
lo
gs
 c
on
ta
in
in
g
co
m
po
ne
nt
 
Figure 4: Dialogs containing information components 
 
Figure 5 implies that a relatively small number 
of dialogs (several hundred turns, or about 30-40 
dialogs) contain the vast majority of information 
relevant to SL algorithms ? less than expected.  
Correctly predicting the wizard?s action in 72.9% 
of turns is significantly better than the 58.4% cor-
rect prediction rate from the handcrafted policy. 
When a caller allows the system to retain initia-
tive, the policy learned by the c4.5 algorithm han-
dled enquiries about single trips perfectly.  Policy 
errors start to occur as the user takes more initia-
tive, entering less well observed states. 
Hand examination of a small number of mis-
classified actions indicate that about half of the 
actions were ?reasonable? ? e.g., including an extra 
item in a confirmation.  Hand examination also 
confirmed that the wizard?s non-deterministic be-
havior and lack of database information resulted in 
misclassifications. 
Other sources of mis-classifications derived 
primarily from under-account of the user?s goal 
and other deficiencies in the expressiveness of the 
state space. 
7 Conclusion & future work 
This work has proposed a method for determining 
many of the basic elements of a RL-based spoken 
dialog system with minimal input from dialog de-
signers using a ?talking wizard.?  The viability of 
the model has been tested with an existing corpus 
and shown to perform significantly better than a 
hand-crafted policy and with less effort to create. 
Future research will explore refining this ap-
proach vis-?-vis user goal, applying this method to 
actual RL-based systems and finding suitable 
methods for parameterized value functions 
References 
A. J?nsson and N. Dahlbick.  1988.  Talking to A Com-
puter is Not Like Talking To Your Best Friend. 
Proceedings of the Scandinavian Conference on 
ceedings of the Scandinavian Conference on 
Artificial Intelligence '88, pp. 53-68. 
Staffan Larsson, Arne J?nsson and Lena Santamarta.  
2000.  Using the process of distilling dialogues to 
understand dialogue systems. ICSLP 2000, Beijing. 
Ester Levin, Roberto Pieraccini and Wieland Eckert.  
2000.  A Stochastic Model of Human-Machine Inter-
action for Learning Dialogue Structures.  IEEE 
Trans on Speech and Audio Processing 8(1):11-23. 
R. K. Moore and S. R. Browning.  1992. Results of an 
exercise to collect ?genuine? spoken enquiries using 
Wizard of Oz techniques. Proc. of the Inst. of Acous-
tics. 
Ross Quinlan.  1992.  C4.5 Release 8. (Software pack-
age).  http://www.cse.unsw.edu.au/~quinlan/ 
Jarek P. Sacha.  2003.  jBNC version 1.0.  (Software 
package).  http://sourceforge.net/projects/jbnc/. 
Satinder Singh, Diane Litman, Michael Kearns, Marilyn 
Walker. 2002.  Optimizing Dialogue Management 
with Reinforcement Learning: Experiments with the 
NJFun System.  Journal of Artificial Intelligence Re-
search, vol 16, 105-133. 
Richard S. Sutton and Andrew G. Barto.  1998.  Rein-
forcement Learning: an Introduction.  The MIT 
Press, Cambridge, Massachusetts, USA. 
Marilyn A. Walker, Jeanne C. Fromer, Shrikanth Nara-
yanan.  1998.  Learning Optimal Dialogue Strate-
gies: A Case Study of a Spoken Dialogue Agent for 
Email.  Proc. 36th Annual Meeting of the ACM and 
17th Int?l Conf. on Comp. Linguistics, 1345--1352. 
 
 
20.0%
30.0%
40.0%
50.0%
60.0%
70.0%
80.0%
0 500 1000 1500 2000 2500
Training examples (dialog turns)
Cl
as
sif
ic
at
io
n 
er
ro
rs
 (%
)
c4.5
Naive Bayes
 
Figure 5: Classification errors vs. training samples for action-type & parameters 
Robustness Issues in a Data-Driven Spoken Language Understanding
System
Yulan He and Steve Young
Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, England
{yh213, sjy}@eng.cam.ac.uk
Abstract
Robustness is a key requirement in spoken lan-
guage understanding (SLU) systems. Human
speech is often ungrammatical and ill-formed,
and there will frequently be a mismatch be-
tween training and test data. This paper dis-
cusses robustness and adaptation issues in a
statistically-based SLU system which is en-
tirely data-driven. To test robustness, the sys-
tem has been tested on data from the Air Travel
Information Service (ATIS) domain which has
been artificially corrupted with varying levels
of additive noise. Although the speech recog-
nition performance degraded steadily, the sys-
tem did not fail catastrophically. Indeed, the
rate at which the end-to-end performance of
the complete system degraded was significantly
slower than that of the actual recognition com-
ponent. In a second set of experiments, the
ability to rapidly adapt the core understanding
component of the system to a different appli-
cation within the same broad domain has been
tested. Using only a small amount of training
data, experiments have shown that a semantic
parser based on the Hidden Vector State (HVS)
model originally trained on the ATIS corpus
can be straightforwardly adapted to the some-
what different DARPA Communicator task us-
ing standard adaptation algorithms. The paper
concludes by suggesting that the results pre-
sented provide initial support to the claim that
an SLU system which is statistically-based and
trained entirely from data is intrinsically robust
and can be readily adapted to new applications.
1 Introduction
Spoken language is highly variable as different people
use different words and sentence structures to convey the
same meaning. Also, many utterances are grammatically-
incorrect or ill-formed. It thus remains an open issue as to
how to provide robustness for large populations of non-
expert users in spoken dialogue systems. The key compo-
nent of a spoken language understanding (SLU) system is
the semantic parser, which translates the users? utterances
into semantic representations. Traditionally, most seman-
tic parser systems have been built using hand-crafted se-
mantic grammar rules and so-called robust parsing (Ward
and Issar, 1996; Seneff, 1992; Dowding et al, 1994) is
used to handle the ill-formed user input in which word
patterns corresponding to semantic tokens are used to fill
slots in different semantic frames in parallel. The frame
with the highest score then yields the selected semantic
representation.
Formally speaking, the robustness of language (recog-
nition, parsing, etc.) is a measure of the ability of hu-
man speakers to communicate despite incomplete infor-
mation, ambiguity, and the constant element of surprise
(Briscoe, 1996). In this paper, two aspects of SLU sys-
tem performance are investigated: noise robustness and
adaptability to different applications. For the former, we
expect that an SLU system should maintain acceptable
performance when given noisy input speech data. This
requires, the understanding components of the SLU sys-
tem to be able to correctly interpret the meaning of an
utterance even when faced with recognition errors. For
the latter, the SLU system should be readily adaptable to
a different application using a relatively small set (e.g.
less than 100) of adaptation utterances.
The rest of the paper is organized as follows. An
overview of our data-driven SLU system is outlined in
section 2. Experimental results on performance under a
range of SNRs are then presented in section 3. Section 4
discusses adaptation of the HVS model to new applica-
tions. Finally, section 5 concludes the paper.
2 System Overview
Spoken language understanding (SLU) aims to interpret
the meanings of users? utterances and respond reason-
ably to what users have said. A typical architecture of
an SLU system is given in Fig. 1, which consists of a
speech recognizer, a semantic parser, and a dialog act de-
coder. Within a statistical framework, the SLU problem
can be factored into three stages. First the speech recog-
nizer recognizes the underlying word string W from each
input acoustic signal A, i.e.
W? = argmax
W
P (W |A) = argmax
W
P (A|W )P (W ) (1)
then the semantic parser maps the recognized word string
W? into a set of semantic concepts C
C? = argmax
C
P (C|W? ) (2)
and finally the dialogue act decoder infers the user?s dia-
log acts or goals by solving
G?u = argmax
Gu
P (Gu|C?) (3)
Dialog Act
Decoder
Semantic
Parser
Speech
Recognizer
Acoustic Signal Words Concepts User?s Dialog Acts
PSfrag replacements
A W C Gu
Figure 1: Typical structure of a spoken language under-
standing system.
The sequential decoding described above is suboptimal
since the solution at each stage depends on an exact so-
lution to the previous stage. To reduce the effect of this
approximation, a word lattice or N -best word hypotheses
can be retained instead of the single best string W? as the
output of the speech recognizer. The semantic parse re-
sults may then be incorporated with the output from the
speech recognizer to rescore the N -best list as below.
C?, W? ? argmax
C,W?LN
P (A|W )P (W )P (C|W )
? argmax
C,W?LN
P (A|W )P (W )?P (C|W )? (4)
where P (A|W ) is the acoustic probability from the
first pass, P (W ) is the language modelling likelihood,
P (C|W ) is the semantic parse score, LN denotes the N -
best list, ? is a semantic parse scale factor, and ? is a
grammar scale factor.
In the system described in this paper, each of these
stages is modelled separately. We use a standard HTK-
based (HTK, 2004) Hidden Markov Model (HMM) rec-
ognizer for recognition, the Hidden Vector State (HVS)
model for semantic parsing (He and Young, 2003b), and
Tree-Augmented Naive Bayes networks (TAN) (Fried-
man et al, 1997) for dialog act decoding.
The speech recognizer comprises 14 mixture Gaus-
sian HMM state-clustered cross-word triphones aug-
mented by using heteroscedastic linear discriminant anal-
ysis (HLDA) (Kumar, 1997). Incremental speaker adap-
tation based on the maximum likelihood linear regression
(MLLR) method (Gales and Woodland, 1996) was per-
formed during the test with updating being performed in
batches of five utterances per speaker.
The Hidden Vector State (HVS) model (He and Young,
2003b) is a hierarchical semantic parser which associates
each state of a push-down automata with the state of a
HMM. State transitions are factored into separate stack
pop and push operations and then constrained to give
a tractable search space. The result is a model which
is complex enough to capture hierarchical structure but
which can be trained automatically from unannotated
data.
CITY DATE
SS
SE
TOLOC ON
RETURN
SS
DUMMY
SS DUMMY
SS SS
Dallassent_start I want to return to
RETURN
SS
RETURN
TOLOC
CITY
SS
RETURN
TOLOC
RETURN
RETURN
SS
ON
DATE
Thursday sent_end
SE
SS
ON
on
Figure 2: Example of a parse tree and its vector state
equivalent.
Let each state at time t be denoted by a vector of Dt
semantic concept labels (tags) ct = [ct[1], ct[2], ..ct[Dt]]
where ct[1] is the preterminal concept and ct[Dt] is the
root concept (SS in Figure 2). Given a word sequence
W , concept vector sequence C and a sequence of stack
pop operations N , the joint probability of P (W,C, N)
can be decomposed as
P (W,C, N) =
T
?
t=1
P (nt|ct?1) ?
P (ct[1]|ct[2 ? ? ?Dt]) ? P (wt|ct) (5)
where ct at word position t is a vector of Dt semantic
concept labels (tags), nt is the vector stack shift operation
and takes values in the range 0, ? ? ? , Dt?1 where Dt?1 is
the stack size at word position t ? 1, and ct[1] = cwt is
the new preterminal semantic tag assigned to word wt at
word position t.
Thus, the HVS model consists of three types of proba-
bilistic move:
1. popping semantic tags off the stack;
2. pushing a pre-terminal semantic tag onto the stack;
3. generating the next word.
The dialog act decoder was implemented using the
Tree-Augmented Naive Bayes (TAN) algorithm (Fried-
man et al, 1997), which is an extension of Naive Bayes
Networks. One TAN was used for each dialogue act or
goal Gu, the semantic concepts Ci which serve as input
to its corresponding TAN were selected based on the mu-
tual information (MI) between the goal and the concept.
Naive Bayes networks assume all the concepts are con-
ditionally independent given the value of the goal. TAN
networks relax this independence assumption by adding
dependencies between concepts based on the conditional
mutual information (CMI) between concepts given the
goal. The goal prior probability P (Gu) and the condi-
tional probability of each semantic concept Ci given the
goal Gu, P (Ci|Gu) are learned from the training data.
Dialogue act detection is done by picking the goal with
the highest posterior probability of Gu given the particu-
lar instance of concepts C1 ? ? ?Cn, P (Gu|C1 ? ? ?Cn).
3 Noise Robustness
The ATIS corpus which contains air travel information
data (Dahl et al, 1994) has been chosen for the SLU sys-
tem development and evaluation. ATIS was developed
in the DARPA sponsored spoken language understanding
programme conducted from 1990 to 1995 and it provides
a convenient and well-documented standard for measur-
ing the end-to-end performance of an SLU system. How-
ever, since the ATIS corpus contains only clean speech,
corrupted test data has been generated by adding samples
of background noise to the clean test data at the waveform
level.
3.1 Experimental Setup
The experimental setup used to evaluate the SLU system
was similar to that described in (He and Young, 2003a).
As mentioned in section 2, the SLU system consists of
three main components, a standard HTK-based HMM
recognizer, the HVS semantic parser, and the TAN dia-
logue act (DA) decoder. Each of the three major compo-
nents are trained separately. The acoustic speech signal
in the ATIS training data is modelled by extracting 39
features every 10ms: 12 cepstra, energy, and their first
and second derivatives. This data is then used to train the
speaker-independent, continuous speech recognizer. The
HVS semantic parser is trained on the unannotated utter-
ances using EM constrained by the domain-specific lex-
ical class information and the dominance relations built
into the abstract annotations (He and Young, 2003b). In
the case of ATIS, the lexical classes can be extracted au-
tomatically from the relational database, whilst abstract
semantic annotations for each utterance are automatically
derived from the accompanying SQL queries of the train-
ing utterances. The dialogue act decoder is trained using
the main topics or goals and the key semantic concepts
extracted automatically from the reference SQL queries
Performance is measured at both the component and
the system level. For the former, the recognizer is eval-
uated by word error rate, the parser by concept slot re-
trieval rate using an F-measure metric (Goel and Byrne,
1999), and the dialog act decoder by detection rate. The
overall system performance is measured using the stan-
dard NIST ?query answer? rate.
In the expriments reported here, car noise from the
NOISEX-92 (Varga et al, 1992) database was added to
the ATIS-3 NOV93 and DEC94 test sets. In order to ob-
tain different SNRs, the noise was scaled accordingly be-
fore adding to the speech signal.
3.2 Experimental Results
Robust spoken language understanding components
should be able to compensate for the weakness of the
speech recognizer. That is, ideally they should be capable
of generating the correct meaning of an utterance even
if it is recognized wrongly by a speech recognizer. At
minimum, the performance of the understanding compo-
nents should degrade gracefully as recognition accuracy
degrades.
Figure 3 gives the system performance on the cor-
rupted test data with additive noise ranging from 25dB to
10dB SNR. The label ?clean? in the X-axis denotes the
original clean speech data without additive noise. Note
that the recognition results on the corrupted test data
were obtained directly using the original clean speech
HMM models without retraining for the noisy condi-
tions. The upper portion of Figure 3 shows the end-to-
end performance in terms of query answer error rate for
the NOV93 and DEC94 test sets. For easy reference,
WER is also shown. The individual component perfor-
mance, F-measure for the HVS semantic parser and di-
alogue act (DA) detection accuracy for the DA decoder,
are illustrated in the lower portion of Figure 3. For each
test set, the performance on the rescored word hypothe-
ses is given as well. This incorporates the semantic parse
scores into the acoustic and language modelling likeli-
hoods to rescore the 25-best word lists from the speech
recognizer.
It can be observed that the system gives fairly stable
performance at high SNRs and then the recognition accu-
racy degrades rapidly in the presence of increasing noise.
At 20dB SNR, the WER for the NOV93 test set increases
by 1.6 times relative to clean whilst the query answer
error rate increases by only 1.3 times. On decreasing
the SNR to 15dB, the system performance degrades sig-
nificantly. The WER increases by 3.1 times relative to
clean but the query answer error rate increases by only
1.7 times. Similar figures were obtained for the DEC94
test set.
The above suggests that the end-to-end performance
measured in terms of answer error rate degrades more
slowly compared to the recognizer WER as the noise
level increases. This demonstrates that the statistically-
based understanding components of the SLU system, the
semantic parser and the dialogue act decoder, are rela-
tively robust to degrading recognition performance.
Regarding the individual component performance, the
dialogue act detection accuracy appears to be less sensi-
tive to decreasing SNR. This is probably a consequence
of the fact that the Bayesian networks are set up to re-
spond to only the presence or absence of semantic con-
cepts or slots, regardless of the actual values assigned to
them. In another words, the performance of the dialogue
act decoder is not affected by the mis-recognition of indi-
vidual words, but only by a failure to detect the presence
of a semantic concept. It can also be observed from Fig-
ure 3 that the F-measure needs to be better than 85% in
order to achieve acceptable end-to-end performance.
4 Adaptation to New Applications
Statistical model adaptation techniques are widely used
to reduce the mismatch between training and test or to
adapt a well-trained model to a novel domain. Com-
monly used techniques can be classified into two cat-
egories, Bayesian adaptation which uses a maximum a
posteriori (MAP) probability criteria (Gauvain and Lee,
1994) and transformation-based approaches such as max-
imum likelihood linear regression (MLLR) (Gales and
Woodland, 1996), which uses a maximum likelihood
(ML) criteria. In recent years, MAP adaptation has been
successfully applied to n-gram language models (Bac-
chiani and Roark, 2003) and lexicalized PCFG models
(Roark and Bacchiani, 2003). Luo et al have proposed
transformation-based approaches based on the Markov
transform (Luo et al, 1999) and the Householder trans-
form (Luo, 2000), to adapt statistical parsers. However,
the optimisation processes for the latter are complex and
it is not clear how general they are.
Since MAP adaptation is straightforward and has been
applied successfully to PCFG parsers, it has been selected
for investigation in this paper. Since one of the special
forms of MAP adaptation is interpolation between the in-
domain and out-of-domain models, it is natural to also
consider the use of non-linear interpolation and hence this
has been studied as well 1.
1Experiments using linear interpolation have also been con-
ducted but it was found that the results are worse than those
4.1 MAP Adaptation
Bayesian adaptation reestimates model parameters di-
rectly using adaptation data. It can be implemented via
maximum a posteriori (MAP) estimation. Assuming that
model parameters are denoted by ?, then given observa-
tion samples Y , the MAP estimate is obtained as
?MAP = argmax
?
P (?|Y ) = argmax
?
P (Y |?)P (?)
(6)
where P (Y |?) is the likelihood of the adaptation data Y
and model parameters ? are random vectors described by
their probabilistic mass function (pmf) P (?), also called
the prior distribution.
In the case of HVS model adaptation, the objective is to
estimate probabilities of discrete distributions over vector
state stack shift operations and output word generation.
Assuming that they can be modelled under the multino-
mial distribution, for mathematical tractability, the con-
jugate prior, the Dirichlet density, is normally used. As-
sume a parser model P (W,C) for a word sequence W
and semantic concept sequence C exists with J compo-
nent distributions Pj each of dimension K, then given
some adaptation data Wl, the MAP estimate of the kth
component of Pj , P?j(k), is
P?j(k) =
?j
?j + ?
P?j(k) +
?
?j + ?
Pj(k) (7)
where ?j =
?K
k=1 ?j(k) in which ?j(k) is defined as the
total count of the events associated with the kth compo-
nent of Pj summed across the decoding of all adaptation
utterances Wl, ? is the prior weighting parameter, Pj(k)
is the probability of the original unadapted model, and
P?j(k) is the empirical distribution of the adaptation data,
which is defined as
P?j(k) =
?j(k)
?K
i=1 ?j(i)
(8)
As discussed in section 2, the HVS model consists of
three types of probabilistic move. The MAP adaptation
technique can be applied to the HVS model by adapting
each of these three component distributions individually.
4.2 Log-Linear Interpolation
Log-linear interpolation has been applied to language
model adaptation and has been shown to be equivalent
to a constrained minimum Kullback-Leibler distance op-
timisation problem(Klakow, 1998).
Following the notation introduced in section 4.1, where
Pj(k) is the probability of the original unadapted model,
and P?j(k) is the empirical distribution of the adaptation
obtained using MAP adaptation or log-linear interpolation.
clean 25dB 20dB 15dB 10dB
3.5
8.5
13.5
18.5
23.5
28.5
33.5
38.5
43.5
Speech to Noise Ratio ? SNR (NOV93 Test Set)
Sp
ok
en
 L
an
gu
ag
e 
Un
de
rs
ta
nd
in
g 
Er
ro
r R
at
e 
(%
)
WER                        
WER with Rescoring         
Answer Error               
Answer Error with Rescoring
(a) NOV93 End-to-End Performance
clean 25dB 20dB 15dB 10dB
2.5
7.5
12.5
17.5
22.5
27.5
32.5
Speech to Noise Ratio ? SNR (DEC94 Test Set)
Sp
ok
en
 L
an
gu
ag
e 
Un
de
rs
ta
nd
in
g 
Er
ro
r R
at
e 
(%
)
WER                        
WER with Rescoring         
Answer Error               
Answer Error with Rescoring
(c) DEC94 End-to-End Performance
clean 25dB 20dB 15dB 10dB
0.7
0.75
0.8
0.85
0.9
0.95
Speech to Noise Ratio ? SNR (NOV93 Test Set)
F?
m
ea
su
re
 a
nd
 D
A 
De
te
ct
io
n 
Ac
cu
ra
cy
F?measure                           
F?measure with Rescoring            
DA Detection Accuracy               
DA Detection Accuracy with Rescoring
(b) NOV93 Component Performance
clean 25dB 20dB 15dB 10dB
0.82
0.84
0.86
0.88
0.9
0.92
Speech to Noise Ratio ? SNR (DEC94 Test Set)
F?
m
ea
su
re
 a
nd
 D
A 
De
te
ct
io
n 
Ac
cu
ra
cy
F?measure                           
F?measure with Rescoring            
DA Detection Accuracy               
DA Detection Accuracy with Rescoring
(d) DEC94 Component Performance
Figure 3: SLU system performance vs SNR.
data, denote the final adapted model probability as P?j(k).
It is assumed that the Kullback-Leibler distance of the
adapted model to the unadapted and empirically deter-
mined model is
D(P?j(k) ? Pj(k)) = d1 (9)
D(P?j(k) ? P?j(k)) = d2 (10)
Given an additional model probability P?j(k) whose
distance to P?j(k) should be kept small, and introducing
Lagrange multipliers ??1 and ?
?
2 to ensure that constraints
9 and 10 are satistifed, yields
D = D(P?j(k) ? P?j(k))+?
?
1(D(P?j(k) ? Pj(k))?d1)
+ ??2(D(P?j(k) ? P?j(k)) ? d2) (11)
Minimizing D with respect to P?j(k) yields the required
distribution.
With some manipulation and redefinition of the La-
grange Multipliers, it can be shown that
P?j(k) =
1
Z?
Pj(k)?1 P?j(k)?2 (12)
where P?j(k) has been assumed to be a uniform distribu-
tion which is then absorbed into the normalization term
Z?.
The computation of Z? is very expensive and can usu-
ally be dropped without significant loss in performance
(Martin et al, 2000). For the other parameters, ?1 and
?2, the generalized iterative scaling algorithm or the sim-
plex method can be employed to estimate their optimal
settings.
4.3 Experiments
To test the portability of the statistical parser, the initial
experiments reported here are focussed on assessing the
adaptability of the HVS model when it is tested in a do-
main which covers broadly similar concepts, but com-
prises rather different speaking styles. To this end, the
flight information subset of the DARPA Communicator
Travel task has been used as the target domain (CUD-
ata, 2004). By limiting the test in this way, we ensure
that the dimensionalities of the HVS model parameters
remain the same and no new semantic concepts are intro-
duced by the adaptation training data.
The baseline HVS parser was trained on the ATIS
corpus using 4978 utterances selected from the context-
independent (Class A) training data in the ATIS-2 and
ATIS-3 corpora. The vocabulary size of the ATIS training
corpus is 611 and there are altogether 110 semantic con-
cepts defined. The parser model was then adapted using
utterances relating to flight reservation from the DARPA
Communicator data. Although the latter bears similari-
ties to the ATIS data, it contains utterances of a different
style and is often more complex. For example, Commu-
nicator contains utterances on multiple flight legs, infor-
mation which is not available in ATIS.
To compare the adapted ATIS parser with an in-domain
Communicator parser, a HVS model was trained from
scratch using 10682 Communicator training utterances.
The vocabulary size of the in-domain Communicator
training data is 505 and a total of 99 semantic concepts
have been defined. For all tests, a set of 1017 Communi-
cator test utterances was used.
Table 1 lists the recall, precision, and F-measure re-
sults obtained when tested on the 1017 utterance DARPA
Communicator test set. The baseline is the unadapted
HVS parser trained on the ATIS corpus only. The in-
domain results are obtained using the HVS parser trained
solely on the 10682 DARPA training data. The other rows
of the table give the parser performance using MAP and
log-linear interpolation based adaptation of the baseline
model using 50 randomly selected adaptation utterances.
System Recall Precision F-measure
Baseline 79.81% 87.14% 83.31%
In-domain 87.18% 91.89% 89.47%
MAP 86.74% 91.07% 88.85%
Log-Linear 86.25% 92.35% 89.20%
Table 1: Performance comparison of adaptation using
MAP or log-linear interpolation.
Since we do not yet have a reference database for the
DARPA Communicator task, it is not possible to conduct
the end-to-end performance evaluation as in section 3.
However, the experimental results in section 3.2 indi-
cate that the F-measure needs to exceed 85% to give ac-
ceptable end-to-end performance (see Figure 3). There-
fore, it can be inferred from Table 1 that the unadapted
ATIS parser model would perform very badly in the new
Communicator application whereas the adapted models
would give performance close to that of a fully trained
in-domain model.
Figure 4 shows the parser performance versus the num-
ber of adaptation utterances used. It can be observed that
when there are only a few adaptation utterances, MAP
adaptation performs significantly better than log-linear
interpolation. However above 25 adaptation utterances,
the converse is true. The parser performance saturates
when the number of adaptation utterances reaches 50 for
both techniques and the best performance overall is given
by the parser adapted using log-linear interpolation. The
performance of both models however degrades when the
number of adaptation utterances exceeds 100, possibly
due to model overtraining. For this particular application,
we conclude that just 50 adaptation utterances would be
sufficient to adapt the baseline model to give comparable
results to the in-domain Communicator model.
0 25 50 75 100 125 150
0.82
0.84
0.86
0.88
0.9
Adaptation Training Utterance Number
F?
m
ea
su
re
MAP       
Log?Linear
Figure 4: F-measure vs amount of adaptation training
data.
5 Conclusions
The spoken language understanding (SLU) system dis-
cussed in this paper is entirely statistically based. The
recogniser uses a HMM-based acoustic model and an n-
gram language model, the semantic parser uses a hid-
den vector state model and the dialogue act decoder uses
Bayesian networks. The system is trained entirely from
data and there are no heuristic rules. One of the major
claims motivating the design of this type of system is
that its fully-statistical framework makes it intrinsically
robust and readily adaptable to new applications. The
aim of this paper has been to investigate this claim ex-
perimentally via two sets of experiments using a system
trained on the ATIS corpus.
In the first set of experiments, the acoustic test data
was corrupted with varying levels of additive car noise.
The end-to-end system performance was then measured
along with the individual component performances. It
was found that although the addition of noise had a sub-
stantial effect on the word error rate, its relative influ-
ence on both the semantic parser slot/value retrieval rate
and the dialogue act detection accuracy was somewhat
less. Overall, the end-to-end error rate degraded rela-
tively more slowly than word error rate and perhaps most
importantly of all, there was no catastrophic failure point
at which the system effectively stops working, a situation
not uncommon in current rule-based systems.
In the second set of experiments, the ability of the se-
mantic decoder component to be adapted to another ap-
plication was investigated. In order, to limit the issues to
parameter mismatch problems, the new application cho-
sen (Communicator) covered essentially the same set of
concepts but was a rather different corpus with different
user speaking styles and different syntactic forms. Over-
all, we found that moving a system trained on ATIS to
this new application resulted in a 6% absolute drop in F-
measure on concept accuracy (i.e. a 62% relative increase
in parser error) and by extrapolation with the results in
the ATIS domain, we infer that this would make the non-
adapted system essentially unusable in the new applica-
tion. However, when adaptation was applied using only
50 adaptation sentences, the loss of concept accuracy was
mostly restored. Specifically, using log-linear adapta-
tion, the out-of-domain F-measure of 83.3% was restored
to 89.2% which is close to the in-domain F-measure of
89.5%.
Although these tests are preliminary and are based on
off-line corpora, the results do give positive support to
the initial claim made for statistically-based spoken lan-
guage systems, i.e. that they are robust and they are read-
ily adaptable to new or changing applications.
Acknowledgements
The authors would like to thank Mark Gales for providing
the software to generate the corrupted speech data with
additive noise.
References
M. Bacchiani and B. Roark. 2003. Unsupervised lan-
guage model adaptation. In Proc. of the IEEE Intl.
Conf. on Acoustics, Speech and Signal Processing,
Hong Kong, Apr.
T. Briscoe. 1996. Robust parsing. In R. Cole, J. Mariani,
H. Uszkoreit, A. Zaenen, and V. Zue, editors, Survey
of the State of the Art of Human Language Technology,
chapter 3.7. Cambridge University Press, Cambridge,
England.
CUData, 2004. DARPA Communicator Travel
Data. University of Colorado at Boulder.
http://communicator.colorado.edu/phoenix.
D.A. Dahl, M. Bates, M. Brown, K. Hunicke-Smith,
D. Pallett, C. Pao, A. Rudnicky, and L. Shriberg. 1994.
Expanding the scope of the ATIS task: the ATIS-3
corpus. In ARPA Human Language Technology Work-
shop, Princeton, NJ, Mar.
J. Dowding, R. Moore, F. Andry, and D. Moran.
1994. Interleaving syntax and semantics in an efficient
bottom-up parser. In Proc. of the 32nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 110?116, Las Cruces, New Maxico, June.
N. Friedman, D. Geiger, and M. Goldszmidt. 1997.
Bayesian network classifiers. Machine Learning,
29(2):131?163.
M.J. Gales and P.C. Woodland. 1996. Mean and variance
adaptation within the MLLR framework. Computer
Speech and Language, 10:249?264, Oct.
J.L. Gauvain and C.-H. Lee. 1994. Maximum a poste-
riori estimation for multivariate Gaussian mixture ob-
servations of Markov chains. IEEE Trans. on Speech
and Audio Processing, 2(2):291?298.
V. Goel and W. Byrne. 1999. Task dependent loss
functions in speech recognition: Application to named
entity extraction. In ESCA ETRW Workshop on Ac-
cessing Information from Spoken Audio, pages 49?53,
Cambridge, UK.
Yulan He and Steve Young. 2003a. A data-driven spoken
language understanding system. In IEEE Automatic
Speech Recognition and Understanding Workshop, St.
Thomas, U.S. Virgin Islands, Dec.
Yulan He and Steve Young. 2003b. Hidden vector state
model for hierarchical semantic parsing. In Proc. of
the IEEE Intl. Conf. on Acoustics, Speech and Signal
Processing, Hong Kong, Apr.
HTK, 2004. Hidden Markov Model Toolkit (HTK)
3.2. Cambridge University Engineering Department.
http://htk.eng.cam.ac.uk/.
D. Klakow. 1998. Log-linear interpolation of language
models. In Proc. of Intl. Conf. on Spoken Language
Processing, Sydney, Australia, Nov.
N. Kumar. 1997. Investigation of Silicon Auditory Mod-
els and Generalization of Linear Discriminant analysis
for Improved Speech Recognition. Ph.D. thesis, Johns
Hopkins University, Baltimore MD.
X. Luo, S. Roukos, and T. Ward. 1999. Unsupervised
adaptation of statistical parsers based on Markov trans-
form. In IEEE Automatic Speech Recognition and Un-
derstanding Workshop, Keystone, Colorado, Dec.
X. Luo. 2000. Parser adaptation via householder trans-
form. In Proc. of the IEEE Intl. Conf. on Acoustics,
Speech and Signal Processing, Istanbul, Turkey, June.
S. Martin, A. Kellner, and T. Portele. 2000. Interpolation
of stochastic grammar and word bigram models in nat-
ural language understanding. In Proc. of Intl. Conf. on
Spoken Language Processing, Beijing, China, Oct.
B. Roark and M. Bacchiani. 2003. Supervised and unsu-
pervised PCFG adaptation to novel domains. In Pro-
ceedings of the joint meeting of the North American
Chapter of the Association for Computational Linguis-
tics and the Human Language Technology Conference
(HLT-NAACL 2003), Edmonton, Canada, May.
S. Seneff. 1992. Robust parsing for spoken language
systems. In Proc. of the IEEE Intl. Conf. on Acoustics,
Speech and Signal Processing, San Francisco.
A.P. Varga, H.J.M. Steeneken, M. Tomlinson, and
D. Jones. 1992. The NOISEX-92 study on the ef-
fect of additive noise on automatic speech recognition.
Technical report, DRA Speech Research Unit.
W. Ward and S. Issar. 1996. Recent improvements in the
CMU spoken language understanding system. In Proc.
of the ARPA Human Language Technology Workshop,
pages 213?216. Morgan Kaufman Publishers, Inc.
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 9?16,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
9
10
11
12
13
14
15
16
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112?119,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Training and Evaluation of the HIS POMDP Dialogue System in Noise
M. Gas?ic?, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, K. Yu, S. Young
Machine Intelligence Laboratory
Engineering Department
Cambridge University
United Kingdom
Abstract
This paper investigates the claim that a di-
alogue manager modelled as a Partially Ob-
servable Markov Decision Process (POMDP)
can achieve improved robustness to noise
compared to conventional state-based dia-
logue managers. Using the Hidden Infor-
mation State (HIS) POMDP dialogue man-
ager as an exemplar, and an MDP-based dia-
logue manager as a baseline, evaluation results
are presented for both simulated and real dia-
logues in a Tourist Information Domain. The
results on the simulated data show that the
inherent ability to model uncertainty, allows
the POMDP model to exploit alternative hy-
potheses from the speech understanding sys-
tem. The results obtained from a user trial
show that the HIS system with a trained policy
performed significantly better than the MDP
baseline.
1 Introduction
Conventional spoken dialogue systems operate by
finding the most likely interpretation of each user
input, updating some internal representation of the
dialogue state and then outputting an appropriate re-
sponse. Error tolerance depends on using confidence
thresholds and where they fail, the dialogue manager
must resort to quite complex recovery procedures.
Such a system has no explicit mechanisms for rep-
resenting the inevitable uncertainties associated with
speech understanding or the ambiguities which natu-
rally arise in interpreting a user?s intentions. The re-
sult is a system that is inherently fragile, especially
in noisy conditions or where the user is unsure of
how to use the system.
It has been suggested that Partially Observable
Markov Decision Processes (POMDPs) offer a nat-
ural framework for building spoken dialogue sys-
tems which can both model these uncertainties
and support policies which are robust to their ef-
fects (Young, 2002; Williams and Young, 2007a).
The key idea of the POMDP is that the underlying
dialogue state is hidden and dialogue management
policies must therefore be based not on a single state
estimate but on a distribution over all states.
Whilst POMDPs are attractive theoretically, in
practice, they are notoriously intractable for any-
thing other than small state/action spaces. Hence,
practical examples of their use were initially re-
stricted to very simple domains (Roy et al, 2000;
Zhang et al, 2001). More recently, however, a num-
ber of techniques have been suggested which do al-
low POMDPs to be scaled to handle real world tasks.
The two generic mechanisms which facilitate this
scaling are factoring the state space and perform-
ing policy optimisation in a reduced summary state
space (Williams and Young, 2007a; Williams and
Young, 2007b).
Based on these ideas, a number of real-world
POMDP-based systems have recently emerged. The
most complex entity which must be represented in
the state space is the user?s goal. In the Bayesian
Update of Dialogue State (BUDS) system, the user?s
goal is further factored into conditionally indepen-
dent slots. The resulting system is then modelled
as a dynamic Bayesian network (Thomson et al,
2008). A similar approach is also developed in
112
(Bui et al, 2007a; Bui et al, 2007b). An alterna-
tive approach taken in the Hidden Information State
(HIS) system is to retain a complete representation
of the user?s goal, but partition states into equiva-
lence classes and prune away very low probability
partitions (Young et al, 2007; Thomson et al, 2007;
Williams and Young, 2007b).
Whichever approach is taken, a key issue in a real
POMDP-based dialogue system is its ability to be
robust to noise and that is the issue that is addressed
in this paper. Using the HIS system as an exem-
plar, evaluation results are presented for a real-world
tourist information task using both simulated and
real users. The results show that a POMDP system
can learn noise robust policies and that N-best out-
puts from the speech understanding component can
be exploited to further improve robustness.
The paper is structured as follows. Firstly, in Sec-
tion 2 a brief overview of the HIS system is given.
Then in Section 3, various POMDP training regimes
are described and evaluated using a simulated user at
differing noise levels. Section 4 then presents results
from a trial in which users conducted various tasks
over a range of noise levels. Finally, in Section 5,
we discuss our results and present our conclusions.
2 The HIS System
2.1 Basic Principles
A POMDP-based dialogue system is shown in Fig-
ure 1 where sm denotes the (unobserved or hidden)
machine state which is factored into three compo-
nents: the last user act au, the user?s goal su and
the dialogue history sd. Since sm is unknown, at
each time-step the system computes a belief state
such that the probability of being in state sm given
belief state b is b(sm). Based on this current belief
state b, the machine selects an action am, receives
a reward r(sm, am), and transitions to a new (un-
observed) state s?m, where s?m depends only on sm
and am. The machine then receives an observation
o? consisting of an N-best list of hypothesised user
actions. Finally, the belief distribution b is updated
based on o? and am as follows:
b?(s?m) = kP (o?|s?m, am)
?
sm?Sm
P (s?m|am, sm)b(sm)
(1)
where k is a normalisation constant (Kaelbling et al,
1998). The first term on the RHS of (1) is called the
observation model and the term inside the summa-
tion is called the transition model. Maintaining this
belief state as the dialogue evolves is called belief
monitoring.
Speech
Understanding
Speech
Generation
User
au
am
~
su
am
Belief
Estimator
Dialog
Policy
sd
b(     )sm
sm = <au,s u,s d>
au
1
.
.

au
N
Figure 1: Abstract view of a POMDP-based spoken dia-
logue system
At each time step t, the machine receives a reward
r(bt, am,t) based on the current belief state bt and the
selected action am,t. Each action am,t is determined
by a policy ?(bt) and building a POMDP system in-
volves finding the policy ?? which maximises the
discounted sum R of the rewards
R =
??
t=0
?tr(bt, am,t) (2)
where ?t is a discount coefficient.
2.2 Probability Models
In the HIS system, user goals are partitioned and
initially, all states su ? Su are regarded as being
equally likely and they are placed in a single par-
tition p0. As the dialogue progresses, user inputs
result in changing beliefs and this root partition is
repeatedly split into smaller partitions. This split-
ting is binary, i.e. p ? {p?, p ? p?} with probability
P (p?|p). By replacing sm by its factors (su, au, sd)
and making reasonable independence assumptions,
it can be shown (Young et al, 2007) that in parti-
113
tioned form (1) becomes
b?(p?, a?u, s?d) = k ? P (o?|a?u)
? ?? ?
observation
model
P (a?u|p?, am)
? ?? ?
user action
model
?
?
sd
P (s?d|p?, a?u, sd, am)
? ?? ?
dialogue
model
P (p?|p)b(p, sd)
? ?? ?
partition
splitting
(3)
where p is the parent of p?.
In this equation, the observation model is approx-
imated by the normalised distribution of confidence
measures output by the speech recognition system.
The user action model allows the observation prob-
ability that is conditioned on a?u to be scaled by the
probability that the user would speak a?u given the
partition p? and the last system prompt am. In the
current implementation of the HIS system, user dia-
logue acts take the form act(a = v) where act is the
dialogue type, a is an attribute and v is its value [for
example, request(food=Chinese)]. The user action
model is then approximated by
P (a?u|p?, am) ? P (T (a?u)|T (am))P (M(a?u)|p?)
(4)
where T (?) denotes the type of the dialogue act
and M(?) denotes whether or not the dialogue act
matches the current partition p?. The dialogue
model is a deterministic encoding based on a simple
grounding model. It yields probability one when the
updated dialogue hypothesis (i.e., a specific combi-
nation of p?, a?u, sd and am) is consistent with the
history and zero otherwise.
2.3 Policy Representation
Policy representation in POMDP-systems is non-
trivial since each action depends on a complex prob-
ability distribution. One of the simplest approaches
to dealing with this problem is to discretise the state
space and then associate an action with each dis-
crete grid point. To reduce quantisation errors, the
HIS model first maps belief distributions into a re-
duced summary space before quantising. This sum-
mary space consists of the probability of the top
two hypotheses plus some status variables and the
user act type associated with the top distribution.
Quantisation is then performed using a simple dis-
tance metric to find the nearest grid point. Ac-
tions in summary space refer specifically to the top
two hypotheses, and unlike actions in master space,
they are limited to a small finite set: greet, ask, ex-
plicit confirm, implicit confirm, select confirm, of-
fer, inform, find alternative, query more, goodbye.
A simple heuristic is then used to map the selected
next system action back into the full master belief
space.
1
Observation
From
User
Ontology Rules
2
N
ua
ma
~
From
System
1
1
2
2
2
1
ds
2
ds
1
ds
2
ds
3
ds
1
up
2
up
3
up
POMDP
Policy
2h
3h
4h
5h
1h







~
a u
~
a u
~
a u
~
a u
~
a u
~
a u
~
a u
Belief
State
Application Database
Action
Refinement
(heuristic)
ma
^
Strategic
Action
Specific
Action
Map to
Summary
Space
ma
b
b^
Summary Space
Figure 2: Overview of the HIS system dialogue cycle
The dialogue manager is able to support nega-
tions, denials and requests for alternatives. When the
selected summary action is to offer the user a venue,
the summary-to-master space mapping heuristics
will normally offer a venue consistent with the most
likely user goal hypothesis. If this hypothesis is then
rejected its belief is substantially reduced and it will
no longer be the top-ranking hypothesis. If the next
system action is to make an alternative offer, then
the new top-ranking hypothesis may not be appro-
priate. For example, if an expensive French restau-
rant near the river had been offered and the user asks
for one nearer the centre of town, any alternative of-
fered should still include the user?s confirmed de-
sire for an expensive French restaurant. To ensure
this, all of the grounded features from the rejected
hypothesis are extracted and all user goal hypothe-
ses are scanned starting at the most likely until an
alternative is found that matches the grounded fea-
tures. For the current turn only, the summary-to-
master space heuristics then treat this hypothesis as
if it was the top-ranking one. If the system then of-
fers a venue based on this hypothesis, and the user
accepts it, then, since system outputs are appended
to user inputs for the purpose of belief updating, the
114
alternative hypothesis will move to the top, or near
the top, of the ranked hypothesis list. The dialogue
then typically continues with its focus on the newly
offered alternative venue.
2.4 Summary of Operation
To summarise, the overall processing performed by
the HIS system in a single dialogue turn (i.e. one cy-
cle of system output and user response) is as shown
in Figure 2. Each user utterance is decoded into an
N-best list of dialogue acts. Each incoming act plus
the previous system act are matched against the for-
est of user goals and partitions are split as needed.
Each user act au is then duplicated and bound to
each partition p. Each partition will also have a
set of dialogue histories sd associated with it. The
combination of each p, au and updated sd forms a
new dialogue hypothesis hk whose beliefs are eval-
uated using (3). Once all dialogue hypotheses have
been evaluated and any duplicates merged, the mas-
ter belief state b is mapped into summary space b?
and the nearest policy belief point is found. The as-
sociated summary space machine action a?m is then
heuristically mapped back to master space and the
machine?s actual response am is output. The cycle
then repeats until the user?s goal is satisfied.
3 Training and Evaluation with a
Simulated User
3.1 Policy optimisation
Policy optimisation is performed in the discrete
summary space described in the previous section us-
ing on-line batch ?-greedy policy iteration. Given
an existing policy ?, dialogs are executed and ma-
chine actions generated according to ? except that
with probability ? a random action is generated. The
system maintains a set of belief points {b?i}. At each
turn in training, the nearest stored belief point b?k to
b? is located using a distance measure. If the distance
is greater than some threshold, b? is added to the set
of stored belief points. The sequence of points b?k
traversed in each dialogue is stored in a list. As-
sociated with each b?i is a function Q(b?i, a?m) whose
value is the expected total reward obtained by choos-
ing summary action a?m from state b?i. At the end
of each dialogue, the total reward is calculated and
added to an accumulator for each point in the list,
discounted by ? at each step. On completion of a
batch of dialogs, the Q values are updated accord-
ing to the accumulated rewards, and the policy up-
dated by choosing the action which maximises each
Q value. The whole process is then repeated until
the policy stabilises.
In our experiments, ? was fixed at 0.1 and ? was
fixed at 0.95. The reward function used attempted
to encourage short successful dialogues by assign-
ing +20 for a successful dialogue and ?1 for each
dialogue turn.
3.2 User Simulation
To train a policy, a user simulator is used to gen-
erate responses to system actions. It has two main
components: a User Goal and a User Agenda. At
the start of each dialogue, the goal is randomly
initialised with requests such as ?name?, ?addr?,
?phone? and constraints such as ?type=restaurant?,
?food=Chinese?, etc. The agenda stores the di-
alogue acts needed to elicit this information in a
stack-like structure which enables it to temporarily
store actions when another action of higher priority
needs to be issued first. This enables the simulator
to refer to previous dialogue turns at a later point. To
generate a wide spread of realistic dialogs, the sim-
ulator reacts wherever possible with varying levels
of patience and arbitrariness. In addition, the sim-
ulator will relax its constraints when its initial goal
cannot be satisfied. This allows the dialogue man-
ager to learn negotiation-type dialogues where only
an approximate solution to the user?s goal exists.
Speech understanding errors are simulated at the di-
alogue act level using confusion matrices trained on
labelled dialogue data (Schatzmann et al, 2007).
3.3 Training and Evaluation
When training a system to operate robustly in noisy
conditions, a variety of strategies are possible. For
example, the system can be trained only on noise-
free interactions, it can be trained on increasing lev-
els of noise or it can be trained on a high noise level
from the outset. A related issue concerns the gener-
ation of grid points and the number of training itera-
tions to perform. For example, allowing a very large
number of points leads to poor performance due to
over-fitting of the training data. Conversely, having
too few point leads to poor performance due to a lack
115
of discrimination in its dialogue strategies.
After some experimentation, the following train-
ing schedule was adopted. Training starts in a
noise free environment using a small number of grid
points and it continues until the performance of the
policy levels off. The resulting policy is then taken
as an initial policy for the next stage where the noise
level is increased, the number of grid points is ex-
panded and the number of iterations is increased.
This process is repeated until the highest noise level
is reached. This approach was motivated by the ob-
servation that a key factor in effective reinforcement
learning is the balance between exploration and ex-
ploitation. In POMDP policy optimisation which
uses dynamically allocated grid points, maintaining
this balance is crucial. In our case, the noise intro-
duced by the simulator is used as an implicit mech-
anism for increasing the exploration. Each time ex-
ploration is increased, the areas of state-space that
will be visited will also increase and hence the num-
ber of available grid points must also be increased.
At the same time, the number of iterations must be
increased to ensure that all points are visited a suf-
ficient number of times. In practice we found that
around 750 to 1000 grid points was sufficient and
the total number of simulated dialogues needed for
training was around 100,000.
A second issue when training in noisy conditions
is whether to train on just the 1-best output from the
simulator or train on the N-best outputs. A limit-
ing factor here is that the computation required for
N-best training is significantly increased since the
rate of partition generation in the HIS model in-
creases exponentially with N. In preliminary tests,
it was found that when training with 1-best outputs,
there was little difference between policies trained
entirely in no noise and policies trained on increas-
ing noise as described above. However, policies
trained on 2-best using the incremental strategy did
exhibit increased robustness to noise. To illustrate
this, Figures 3 and 4 show the average dialogue suc-
cess rates and rewards for 3 different policies, all
trained on 2-best: a hand-crafted policy (hdc), a pol-
icy trained on noise-free conditions (noise free) and
a policy trained using the incremental scheme de-
scribed above (increm). Each policy was tested us-
ing 2-best output from the simulator across a range
of error rates. In addition, the noise-free policy was
also tested on 1-best output.
Figure 3: Average simulated dialogue success rate as a
function of error rate for a hand-crafted (hdc), noise-free
and incrementally trained (increm) policy.
Figure 4: Average simulated dialogue reward as a func-
tion of error rate for a hand-crafted (hdc), noise-free and
incrementally trained (increm) policy.
As can be seen, both the trained policies improve
significantly on the hand-crafted policies. Further-
more, although the average rewards are all broadly
similar, the success rate of the incrementally trained
policy is significantly better at higher error rates.
Hence, this latter policy was selected for the user
trial described next.
4 Evaluation via a User Trial
The HIS-POMDP policy (HIS-TRA) that was incre-
mentally trained on the simulated user using 2-best
lists was tested in a user trial together with a hand-
crafted HIS-POMDP policy (HIS-HDC). The strat-
egy used by the latter was to first check the most
likely hypothesis. If it contains sufficient grounded
116
keys to match 1 to 3 database entities, then offer is
selected. If any part of the hypothesis is inconsis-
tent or the user has explicitly asked for another sug-
gestion, then find alternative action is selected. If
the user has asked for information about an offered
entity then inform is selected. Otherwise, an un-
grounded component of the top hypothesis is identi-
fied and depending on the belief, one of the confirm
actions is selected.
In addition, an MDP-based dialogue manager de-
veloped for earlier trials (Schatzmann, 2008) was
also tested. Since considerable effort has been put in
optimising this system, it serves as a strong baseline
for comparison. Again, both a trained policy (MDP-
TRA) and a hand-crafted policy (MDP-HDC) were
tested.
4.1 System setup and confidence scoring
The dialogue system consisted of an ATK-based
speech recogniser, a Phoenix-based semantic parser,
the dialogue manager and a diphone based speech
synthesiser. The semantic parser uses simple phrasal
grammar rules to extract the dialogue act type and a
list of attribute/value pairs from each utterance.
In a POMDP-based dialogue system, accurate
belief-updating is very sensitive to the confidence
scores assigned to each user dialogue act. Ideally
these should provide a measure of the probability of
the decoded act given the true user act. In the evalu-
ation system, the recogniser generates a 10-best list
of hypotheses at each turn along with a compact con-
fusion network which is used to compute the infer-
ence evidence for each hypothesis. The latter is de-
fined as the sum of the log-likelihoods of each arc
in the confusion network and when exponentiated
and renormalised this gives a simple estimate of the
probability of each hypothesised utterance. Each ut-
terance in the 10-best list is passed to the semantic
parser. Equivalent dialogue acts output by the parser
are then grouped together and the dialogue act for
each group is then assigned the sum of the sentence-
level probabilities as its confidence score.
4.2 Trial setup
For the trial itself, 36 subjects were recruited (all
British native speakers, 18 male, 18 female). Each
subject was asked to imagine himself to be a tourist
in a fictitious town called Jasonville and try to find
particular hotels, bars, or restaurants in that town.
Each subject was asked to complete a set of pre-
defined tasks where each task involved finding the
name of a venue satisfying a set of constraints such
as food type is Chinese, price-range is cheap, etc.,
and getting the value of one or more additional at-
tributes of that venue such as the address or the
phone number.
For each task, subjects were given a scenario to
read and were then asked to solve the task via a di-
alogue with the system. The tasks set could either
have one solution, several solutions, or no solution
at all in the database. In cases where a subject found
that there was no matching venue for the given task,
he/she was allowed to try and find an alternative
venue by relaxing one or more of the constraints.
In addition, subjects had to perform each task at
one of three possible noise levels. These levels cor-
respond to signal/noise ratios (SNRs) of 35.3 dB
(low noise), 10.2 dB (medium noise), or 3.3 dB
(high noise). The noise was artificially generated
and mixed with the microphone signal, in addition
it was fed into the subject?s headphones so that they
were aware of the noisy conditions.
An instructor was present at all times to indicate
to the subject which task description to follow, and
to start the right system with the appropriate noise-
level. Each subject performed an equal number of
tasks for each system (3 tasks), noise level (6 tasks)
and solution type (6 tasks for each of the types 0, 1,
or multiple solutions). Also, each subject performed
one task for all combinations of system and noise
level. Overall, each combination of system, noise
level, and solution type was used in an equal number
of dialogues.
4.3 Results
In Table 1, some general statistics of the corpus re-
sulting from the trial are given. The semantic error
rate is based on substitutions, insertions and dele-
tions errors on semantic items. When tested after the
trial on the transcribed user utterances, the semantic
error rate was 4.1% whereas the semantic error rate
on the ASR input was 25.2%. This means that 84%
of the error rate was due to the ASR.
Tables 2 and 3 present success rates (Succ.) and
average performance scores (Perf.), comparing the
two HIS dialogue managers with the two MDP base-
117
Number of dialogues 432
Number of dialogue turns 3972
Number of words (transcriptions) 18239
Words per utterance 4.58
Word Error Rate 32.9
Semantic Error Rate 25.2
Semantic Error Rate transcriptions 4.1
Table 1: General corpus statistics.
line systems. For the success rates, also the stan-
dard deviation (std.dev) is given, assuming a bino-
mial distribution. The success rate is the percentage
of successfully completed dialogues. A task is con-
sidered to be fully completed when the user is able to
find the venue he is looking for and get al the addi-
tional information he asked for; if the task has no so-
lution and the system indicates to the user no venue
could be found, this also counts as full completion.
A task is considered to be partially completed when
only the correct venue has been given. The results on
partial completion are given in Table 2, and the re-
sults on full completion in Table 3. To mirror the re-
ward function used in training, the performance for
each dialogue is computed by assigning a reward of
20 points for full completion and subtracting 1 point
for the number of turns up until a successful recom-
mendation (i.e., partial completion).
Partial Task Completion statistics
System Succ. (std.dev) #turns Perf.
MDP-HDC 68.52 (4.83) 4.80 8.91
MDP-TRA 70.37 (4.75) 4.75 9.32
HIS-HDC 74.07 (4.55) 7.04 7.78
HIS-TRA 84.26 (3.78) 4.63 12.22
Table 2: Success rates and performance results on partial
completion.
Full Task Completion statistics
System Succ. (std.dev) #turns Perf.
MDP-HDC 64.81 (4.96) 5.86 7.10
MDP-TRA 65.74 (4.93) 6.18 6.97
HIS-HDC 63.89 (4.99) 8.57 4.20
HIS-TRA 78.70 (4.25) 6.36 9.38
Table 3: Success rates and performance results on full
completion.
The results show that the trained HIS dialogue
manager significantly outperforms both MDP based
dialogue managers. For success rate on partial com-
pletion, both HIS systems perform better than the
MDP systems.
4.3.1 Subjective Results
In the user trial, the subjects were also asked for
a subjective judgement of the systems. After com-
pleting each task, the subjects were asked whether
they had found the information they were looking
for (yes/no). They were also asked to give a score
on a scale from 1 to 5 (best) on how natural/intuitive
they thought the dialogue was. Table 4 shows the
results for the 4 systems used. The performance of
the HIS systems is similar to the MDP systems, with
a slightly higher success rate for the trained one and
a slightly lower score for the handcrafted one.
System Succ. Rate (std.dev) Score
MDP-HDC 78 (4.30) 3.52
MDP-TRA 78 (4.30) 3.42
HIS-HDC 71 (4.72) 3.05
HIS-TRA 83 (3.90) 3.41
Table 4: Subjective performance results from the user
trial.
5 Conclusions
This paper has described recent work in training a
POMDP-based dialogue manager to exploit the ad-
ditional information available from a speech under-
standing system which can generate ranked lists of
hypotheses. Following a brief overview of the Hid-
den Information State dialogue manager and pol-
icy optimisation using a user simulator, results have
been given for both simulated user and real user di-
alogues conducted at a variety of noise levels.
The user simulation results have shown that al-
though the rewards are similar, training with 2-best
rather than 1-best outputs from the user simulator
yields better success rates at high noise levels. In
view of this result, we would have liked to inves-
tigate training on longer N-best lists, but currently
computational constraints prevent this. We hope in
the future to address this issue by developing more
efficient state partitioning strategies for the HIS sys-
tem.
118
The overall results on real data collected from the
user trial clearly indicate increased robustness by the
HIS system. We would have liked to be able to
plot performance and success scores as a function
of noise level or speech understanding error rate,
but there is great variability in these kinds of com-
plex real-world dialogues and it transpired that the
trial data was insufficient to enable any statistically
meaningful presentation of this form. We estimate
that we need at least an order of magnitude more
trial data to properly investigate the behaviour of
such systems as a function of noise level. The trial
described here, including transcription and analysis
consumed about 30 man-days of effort. Increasing
this by a factor of 10 or more is not therefore an
option for us, and clearly an alternative approach is
needed.
We have also reported results of subjective suc-
cess rate and opinion scores based on data obtained
from subjects after each trial. The results were only
weakly correlated with the measured performance
and success rates. We believe that this is partly due
to confusion as to what constituted success in the
minds of the subjects. This suggests that for subjec-
tive results to be meaningful, measurements such as
these will only be really useful if made on live sys-
tems where users have a real rather than imagined
information need. The use of live systems would
also alleviate the data sparsity problem noted earlier.
Finally and in conclusion, we believe that despite
the difficulties noted above, the results reported in
this paper represent a first step towards establish-
ing the POMDP as a viable framework for develop-
ing spoken dialogue systems which are significantly
more robust to noisy operating conditions than con-
ventional state-based systems.
Acknowledgements
This research was partly funded by the UK EPSRC
under grant agreement EP/F013930/1 and by the
EU FP7 Programme under grant agreement 216594
(CLASSIC project: www.classic-project.org).
References
TH Bui, M Poel, A Nijholt, and J Zwiers. 2007a. A
tractable DDN-POMDP Approach to Affective Dia-
logue Modeling for General Probabilistic Frame-based
Dialogue Systems. In Proc 5th Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
pages 34?57.
TH Bui, B van Schooten, and D Hofs. 2007b. Practi-
cal dialogue manager development using POMDPs .
In 8th SIGdial Workshop on Discourse and Dialogue,
Antwerp.
LP Kaelbling, ML Littman, and AR Cassandra. 1998.
Planning and Acting in Partially Observable Stochastic
Domains. Artificial Intelligence, 101:99?134.
N Roy, J Pineau, and S Thrun. 2000. Spoken Dialogue
Management Using Probabilistic Reasoning. In Proc
ACL.
J Schatzmann, B Thomson, and SJ Young. 2007. Error
Simulation for Training Statistical Dialogue Systems.
In ASRU 07, Kyoto, Japan.
J Schatzmann. 2008. Statistical User and Error Mod-
elling for Spoken Dialogue Systems. Ph.D. thesis, Uni-
versity of Cambridge.
B Thomson, J Schatzmann, K Weilhammer, H Ye, and
SJ Young. 2007. Training a real-world POMDP-based
Dialog System. In HLT/NAACL Workshop ?Bridging
the Gap: Academic and Industrial Research in Dialog
Technologies?, Rochester.
B Thomson, J Schatzmann, and SJ Young. 2008.
Bayesian Update of Dialogue State for Robust Dia-
logue Systems. In Int Conf Acoustics Speech and Sig-
nal Processing ICASSP, Las Vegas.
JD Williams and SJ Young. 2007a. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language, 21(2):393?
422.
JD Williams and SJ Young. 2007b. Scaling POMDPs
for Spoken Dialog Management. IEEE Audio, Speech
and Language Processing, 15(7):2116?2129.
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to Dia-
log Management. In ICASSP 2007, Honolulu, Hawaii.
SJ Young. 2002. Talking to Machines (Statistically
Speaking). In Int Conf Spoken Language Processing,
Denver, Colorado.
B Zhang, Q Cai, J Mao, E Chang, and B Guo. 2001.
Spoken Dialogue Management as Planning and Acting
under Uncertainty. In Eurospeech, Aalborg, Denmark.
119
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 272?275,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
k-Nearest Neighbor Monte-Carlo Control Algorithm
for POMDP-based Dialogue Systems
F. Lefe`vre?, M. Gas?ic?, F. Jurc???c?ek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. Young
Spoken Dialogue Systems Group
Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK
{frfl2, mg436, fj228, sk561, farm2, brmt2, ky219, sjy}@eng.cam.ac.uk
Abstract
In real-world applications, modelling di-
alogue as a POMDP requires the use of
a summary space for the dialogue state
representation to ensure tractability. Sub-
optimal estimation of the value func-
tion governing the selection of system re-
sponses can then be obtained using a grid-
based approach on the belief space. In
this work, the Monte-Carlo control tech-
nique is extended so as to reduce training
over-fitting and to improve robustness to
semantic noise in the user input. This tech-
nique uses a database of belief vector pro-
totypes to choose the optimal system ac-
tion. A locally weighted k-nearest neigh-
bor scheme is introduced to smooth the de-
cision process by interpolating the value
function, resulting in higher user simula-
tion performance.
1 Introduction
In the last decade dialogue modelling as a Partially
Observable Markov Decision Process (POMDP)
has been proposed as a convenient way to improve
spoken dialogue systems (SDS) trainability, nat-
uralness and robustness to input errors (Young et
al., 2009). The POMDP framework models dia-
logue flow as a sequence of unobserved dialogue
states following stochastic moves, and provides a
principled way to model uncertainty.
However, to deal with uncertainty, POMDPs
maintain distributions over all possible states. But
then training an optimal policy is an NP hard
problem and thus not tractable for any non-trivial
application. In recent works this issue is ad-
dressed by mapping the dialog state representation
?Fabrice Lefe`vre is currently on leave from the Univer-
sity of Avignon, France.
space (the master space) into a smaller summary
space (Williams and Young, 2007). Even though
optimal policies remain out of reach, sub-optimal
solutions can be found by means of grid-based al-
gorithms.
Within the Hidden Information State (HIS)
framework (Young et al, 2009), policies are rep-
resented by a set of grid points in the summary be-
lief space. Beliefs in master space are first mapped
into summary space and then mapped into a sum-
mary action via the dialogue policy. The resulting
summary action is then mapped back into master
space and output to the user.
Methods which support interpolation between
points are generally required to scale well to large
state spaces (Pineau et al, 2003). In the current
version of the HIS framework, the policy chooses
the system action by associating each new belief
point with the single, closest, grid point. In the
present work, a k-nearest neighbour extension is
evaluated in which the policy decision is based on
a locally weighted regression over a subset of rep-
resentative grid points. This method thus lies be-
tween a strictly grid-based and a point-based value
iteration approach as it interpolates the value func-
tion around the queried belief point. It thus re-
duces the policy?s dependency on the belief grid
point selection and increases robustness to input
noise.
The next section gives an overview of the
CUED HIS POMDP dialogue system which we
extended for our experiments. In Section 3, the
grid-based approach to policy optimisation is in-
troduced followed by a presentation of the k-
nn Monte-Carlo policy optimization in Section 4,
along with an evaluation on a simulated user.
272
2 The CUED Spoken Dialogue System
2.1 System Architecture
The CUED HIS-based dialogue system pipelines
five modules: the ATK speech recogniser, an
SVM-based semantic tuple classifier, a POMDP
dialogue manager, a natural language generator,
and an HMM-based speech synthesiser. During
an interaction with the system, the user?s speech
is first decoded by the recogniser and an N-best
list of hypotheses is sent to the semantic classifier.
In turn the semantic classifier outputs an N-best
list of user dialogue acts. A dialogue act is a se-
mantic representation of the user action headed by
the user intention (such as inform, request,
etc) followed by a list of items (slot-value pairs
such as type=hotel, area=east etc). The
N-best list of dialogue acts is used by the dialogue
manager to update the dialogue state. Based on
the state hypotheses and the policy, a machine ac-
tion is determined, again in the form of a dialogue
act. The natural language generator translates the
machine action into a sentence, finally converted
into speech by the HMM synthesiser. The dia-
logue system is currently developed for a tourist
information domain (Towninfo). It is worth not-
ing that the dialogue manager does not contain any
domain-specific knowledge.
2.2 HIS Dialogue Manager
The unobserved dialogue state of the HIS dialogue
manager consists of the user goal, the dialogue his-
tory and the user action. The user goal is repre-
sented by a partition which is a tree structure built
according to the domain ontology. The nodes in
the partition consist mainly of slots and values.
When querying the venue database using the par-
tition, a set of matching entities can be produced.
The dialogue history consists of the grounding
states of the nodes in the partition, generated us-
ing a finite state automaton and the previous user
and system action. A hypothesis in the HIS ap-
proach is then a triple combining a partition, a user
action and the respective set of grounding states.
The distribution over all hypotheses is maintained
throughout the dialogue (belief state monitoring).
Considering the ontology size for any real-world
problem, the so-defined state space is too large for
any POMDP learning algorithm. Hence to obtain a
tractable policy, the state/action space needs to be
reduced to a smaller scale summary space. The set
of possible machine dialogue acts is also reduced
in summary space. This is mainly achieved by re-
Master Space
Masters Sppp c  uamyppp
eus Sers Sppp us SamypppMMM
Mast er S pr
cr S pr uSr pSm Mayt
Summary Space
Master Spcu rmymty
c
um
a
ycr
y rcsProceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1552?1561,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Phrase-based Statistical Language Generation using
Graphical Models and Active Learning
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek,
Simon Keizer, Blaise Thomson, Kai Yu and Steve Young?
Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK
{f.mairesse, mg436, fj228, sk561, brmt2, ky219, sjy}@eng.cam.ac.uk
Abstract
Most previous work on trainable language
generation has focused on two paradigms:
(a) using a statistical model to rank a
set of generated utterances, or (b) using
statistics to inform the generation deci-
sion process. Both approaches rely on
the existence of a handcrafted generator,
which limits their scalability to new do-
mains. This paper presents BAGEL, a sta-
tistical language generator which uses dy-
namic Bayesian networks to learn from
semantically-aligned data produced by 42
untrained annotators. A human evalua-
tion shows that BAGEL can generate nat-
ural and informative utterances from un-
seen inputs in the information presentation
domain. Additionally, generation perfor-
mance on sparse datasets is improved sig-
nificantly by using certainty-based active
learning, yielding ratings close to the hu-
man gold standard with a fraction of the
data.
1 Introduction
The field of natural language generation (NLG) is
one of the last areas of computational linguistics to
embrace statistical methods. Over the past decade,
statistical NLG has followed two lines of research.
The first one, pioneered by Langkilde and Knight
(1998), introduces statistics in the generation pro-
cess by training a model which reranks candi-
date outputs of a handcrafted generator. While
their HALOGEN system uses an n-gram language
model trained on news articles, other systems have
used hierarchical syntactic models (Bangalore and
Rambow, 2000), models trained on user ratings of
?This research was partly funded by the UK EPSRC un-
der grant agreement EP/F013930/1 and funded by the EU
FP7 Programme under grant agreement 216594 (CLASSiC
project: www.classic-project.org).
utterance quality (Walker et al, 2002), or align-
ment models trained on speaker-specific corpora
(Isard et al, 2006).
A second line of research has focused on intro-
ducing statistics at the generation decision level,
by training models that find the set of genera-
tion parameters maximising an objective function,
e.g. producing a target linguistic style (Paiva and
Evans, 2005; Mairesse and Walker, 2008), gener-
ating the most likely context-free derivations given
a corpus (Belz, 2008), or maximising the expected
reward using reinforcement learning (Rieser and
Lemon, 2009). While such methods do not suffer
from the computational cost of an overgeneration
phase, they still require a handcrafted generator to
define the generation decision space within which
statistics can be used to find an optimal solution.
This paper presents BAGEL (Bayesian networks
for generation using active learning), an NLG sys-
tem that can be fully trained from aligned data.
While the main requirement of the generator is to
produce natural utterances within a dialogue sys-
tem domain, a second objective is to minimise the
overall development effort. In this regard, a major
advantage of data-driven methods is the shift of
the effort from model design and implementation
to data annotation. In the case of NLG systems,
learning to produce paraphrases can be facilitated
by collecting data from a large sample of annota-
tors. Our meaning representation should therefore
(a) be intuitive enough to be understood by un-
trained annotators, and (b) provide useful gener-
alisation properties for generating unseen inputs.
Section 2 describes BAGEL?s meaning represen-
tation, which satisfies both requirements. Sec-
tion 3 then details how our meaning representation
is mapped to a phrase sequence, using a dynamic
Bayesian network with backoff smoothing.
Within a given domain, the same semantic
concept can occur in different utterances. Sec-
tion 4 details how BAGEL exploits this redundancy
1552
to improve generation performance on sparse
datasets, by guiding the data collection process
using certainty-based active learning (Lewis and
Catlett, 1994). We train BAGEL in the informa-
tion presentation domain, from a corpus of utter-
ances produced by 42 untrained annotators (see
Section 5.1). An automated evaluation metric is
used to compare preliminary model and training
configurations in Section 5.2, while Section 5.3
shows that the resulting system produces natural
and informative utterances, according to 18 hu-
man judges. Finally, our human evaluation shows
that training using active learning significantly im-
proves generation performance on sparse datasets,
yielding results close to the human gold standard
using a fraction of the data.
2 Phrase-based generation from
semantic stacks
BAGEL uses a stack-based semantic representa-
tion to constrain the sequence of semantic con-
cepts to be searched. This representation can be
seen as a linearised semantic tree similar to the
one previously used for natural language under-
standing in the Hidden Vector State model (He
and Young, 2005). A stack representation provides
useful generalisation properties (see Section 3.1),
while the resulting stack sequences are relatively
easy to align (see Section 5.1). In the context of
dialogue systems, Table 1 illustrates how the input
dialogue act is first mapped to a set of stacks of
semantic concepts, and then aligned with a word
sequence. The bottom concept in the stack will
typically be a dialogue act type, e.g. an utterance
providing information about the object under dis-
cussion (inform) or specifying that the request
of the user cannot be met (reject). Other con-
cepts include attributes of that object (e.g., food,
area), values for those attributes (e.g., Chinese,
riverside), as well as special symbols for negat-
ing underlying concepts (e.g., not) or specifying
that they are irrelevant (e.g., dontcare).
The generator?s goal is thus finding the
most likely realisation given an unordered
set of mandatory semantic stacks Sm derived
from the input dialogue act. For example,
s =inform(area(centre)) is a mandatory stack
associated with the dialogue act in Table 1 (frame
8). While mandatory stacks must all be conveyed
in the output realisation, Sm does not contain the
optional intermediary stacks Si that can refer to
(a) general attributes of the object under discus-
sion (e.g., inform(area) in Table 1), or (b) to
concepts that are not in the input at all, which are
associated with the singleton stack inform (e.g.,
phrases expressing the dialogue act type, or clause
aggregation operations). For example, the stack
sequence in Table 1 contains 3 intermediary stacks
for t = 2, 5 and 7.
BAGEL?s granularity is defined by the semantic
annotation in the training data, rather than external
linguistic knowledge about what constitutes a unit
of meaning, i.e. contiguous words belonging to
the same semantic stack are modelled as an atomic
observation unit or phrase.1 In contrast with word-
level models, a major advantage of phrase-based
generation models is that they can model long-
range dependencies and domain-specific idiomatic
phrases with fewer parameters.
3 Dynamic Bayesian networks for NLG
Dynamic Bayesian networks have been used suc-
cessfully for speech recognition, natural language
understanding, dialogue management and text-to-
speech synthesis (Rabiner, 1989; He and Young,
2005; Lefe`vre, 2006; Thomson and Young, 2010;
Tokuda et al, 2000). Such models provide a
principled framework for predicting elements in a
large structured space, such as required for non-
trivial NLG tasks. Additionally, their probabilistic
nature makes them suitable for modelling linguis-
tic variation, i.e. there can be multiple valid para-
phrases for a given input.
BAGEL models the generation task as finding
the most likely sequence of realisation phrases
R? = (r1...rL) given an unordered set of manda-
tory semantic stacks Sm, with |Sm| ? L. BAGEL
must thus derive the optimal sequence of semantic
stacks S? that will appear in the utterance given
Sm, i.e. by inserting intermediary stacks if needed
and by performing content ordering. Any num-
ber of intermediary stacks can be inserted between
two consecutive mandatory stacks, as long as all
their concepts are included in either the previous
or following mandatory stack, and as long as each
stack transition leads to a different stack (see ex-
ample in Table 1). Let us define the set of possi-
ble stack sequences matching these constraints as
Seq(Sm) ? {S = (s1...sL) s.t. st ? Sm ? Si}.
We propose a model which estimates the dis-
1The term phrase is thus defined here as any sequence of
one or more words.
1553
Charlie Chan is a Chinese restaurant near Cineworld in the centre of town
Charlie Chan Chinese restaurant Cineworld centre
name food type near near area area
inform inform inform inform inform inform inform inform
t = 1 t = 2 t = 3 t = 4 t = 5 t = 6 t = 7 t = 8
Table 1: Example semantic stacks aligned with an utterance for the dialogue act
inform(name(Charlie Chan) type(restaurant) area(centre) food(Chinese) near(Cineworld)). Mandatory
stacks are in bold.
tribution P (R|Sm) from a training set of real-
isation phrases aligned with semantic stack se-
quences, by marginalising over all stack sequences
in Seq(Sm):
P (R|Sm) =
?
S?Seq(Sm)
P (R,S|Sm)
=
?
S?Seq(Sm)
P (R|S,Sm)P (S|Sm)
=
?
S?Seq(Sm)
P (R|S)P (S|Sm) (1)
Inference over the model defined in (1) requires
the decoding algorithm to consider all possible or-
derings over Seq(Sm) together with all possible
realisations, which is intractable for non-trivial do-
mains. We thus make the additional assumption
that the most likely sequence of semantic stacks
S? given Sm is the one yielding the optimal reali-
sation phrase sequence:
P (R|Sm) ? P (R|S
?)P (S?|Sm) (2)
with S? = argmax
S?Seq(Sm)
P (S|Sm) (3)
The semantic stacks are therefore decoded first
using the model in Fig. 1 to solve the argmax
in (3). The decoded stack sequence S? is then
treated as observed in the realisation phase, in
which the model in Fig. 2 is used to find the real-
isation phrase sequence R? maximising P (R|S?)
over all phrase sequences of length L = |S?| in
our vocabulary:
R? = argmax
R=(r1...rL)
P (R|S?)P (S?|Sm) (4)
= argmax
R=(r1...rL)
P (R|S?) (5)
In order to reduce model complexity, we fac-
torise our model by conditioning the realisation
phrase at time t on the previous phrase rt?1,
and the previous, current, and following semantic
stacks. The semantic stack st at time t is assumed
last mandatory 
stack
stack set 
validator
first frame
semantic 
stack s
stack set tracker
repeated frame final frame
Figure 1: Graphical model for the semantic decod-
ing phase. Plain arrows indicate smoothed proba-
bility distributions, dashed arrows indicate deter-
ministic relations, and shaded nodes are observed.
The generation of the end semantic stack symbol
deterministically triggers the final frame.
to depend only on the previous two stacks and the
last mandatory stack su ? Sm with 1 ? u < t:
P (S|Sm) =
?
?
?
?T
t=1 P (st|st?1, st?2, su)
if S ? Seq(Sm)
0 otherwise
(6)
P (R|S?) =
T?
t=1
P (rt|rt?1, s
?
t?1, s
?
t , s
?
t+1) (7)
While dynamic Bayesian networks typically
take sequential inputs, mapping a set of seman-
tic stacks to a sequence of phrases is achieved
by keeping track of the mandatory stacks that
were visited in the current sequence (see stack set
tracker variable in Fig. 1), and pruning any se-
quence that has not included all mandatory input
stacks on reaching the final frame (see observed
stack set validator variable in Fig. 1). Since the
number of intermediary stacks is not known at de-
coding time, the network is unrolled for a fixed
number of frames T defining the maximum num-
ber of phrases that can be generated (e.g., T =
50). The end of the stack sequence is then deter-
mined by a special end symbol, which can only
be emitted within the T frames once all mandatory
stacks have been visited. The probability of the re-
sulting utterance is thus computed over all frames
up to the end symbol, which determines the length
1554
L of S? and R?. While the decoding constraints
enforce that L > |Sm|, the search for S? requires
comparing sequences of different lengths. A con-
sequence is that shorter sequences containing only
mandatory stacks are likely to be favoured. While
future work should investigate length normalisa-
tion strategies, we find that the learned transition
probabilities are skewed enough to favour stack
sequences including intermediary stacks.
Once the topology and the decoding constraints
of the network have been defined, any inference al-
gorithm can be used to search for S? and R?. We
use the junction tree algorithm implemented in the
Graphical Model ToolKit (GMTK) for our exper-
iments (Bilmes and Zweig, 2002), however both
problems can be solved using a standard Viterbi
search given the appropriate state representation.
In terms of computational complexity, it is impor-
tant to note that the number of stack sequences
Seq(Sm) to search over increases exponentially
with the number of input mandatory stacks. Nev-
ertheless, we find that real-time performance can
be achieved by pruning low probability sequences,
without affecting the quality of the solution.
3.1 Generalisation to unseen semantic stacks
In order to generalise to semantic stacks which
have not been observed during training, the re-
alisation phrase r is made dependent on under-
specified stack configurations, i.e. the tail l
and the head h of the stack. For example, the
last stack in Table 1 is associated with the head
centre and the tail inform(area). As a re-
sult, BAGEL assigns non-zero probabilities to re-
alisation phrases in unseen semantic contexts, by
backing off to the head and the tail of the stack.
A consequence is that BAGEL?s lexical realisa-
tion can generalise across contexts. For exam-
ple, if reject(area(centre)) was never ob-
served at training time, P (r = centre of town|s =
reject(area(centre))) will be estimated by
backing off to P (r = centre of town|h =
centre). BAGEL can thus generate ?there are
no venues in the centre of town? if the phrase
?centre of town? was associated with the con-
cept centre in a different context, such as
inform(area(centre)). The final realisation
model is illustrated in Fig. 2:
realisation phrase r
repeated frame final framefirst frame
stack head h
semantic 
stack s
stack tail l
Figure 2: Graphical model for the realisation
phase. Dashed arrows indicate deterministic re-
lations, and shaded node are observed.
!"#$%&& '(")*+
11111 ,,,,,,,| +?+?? ttttttttt sssllrlhr
ttttttt sllrlhr ,,,,,| 111 +??
111 ,,,,| +?? tttttt llrlhr
ttt lhr ,|21,| ?? ttt sss
uttt ssss ,,| 21 ??
tt hr |1| ?tt ss
trts
Figure 3: Backoff graphs for the semantic decod-
ing and realisation models.
P (R|S?) =
L?
t=1
P (rt|rt?1, ht, lt?1, lt, lt+1,
s?t?1, s
?
t , s
?
t+1) (8)
Conditional probability distributions are repre-
sented as factored language models smoothed us-
ing Witten-Bell interpolated backoff smoothing
(Bilmes and Kirchhoff, 2003), according to the
backoff graphs in Fig. 3. Variables which are the
furthest away in time are dropped first, and par-
tial stack variables are dropped last as they are ob-
served the most.
It is important to note that generating unseen se-
mantic stacks requires all possible mandatory se-
mantic stacks in the target domain to be prede-
fined, in order for all stack unigrams to be assigned
a smoothed non-zero probability.
3.2 High cardinality concept abstraction
While one should expect a trainable generator
to learn multiple lexical realisations for low-
cardinality semantic concepts, learning lexical
realisations for high-cardinality database entries
(e.g., proper names) would increase the number of
model parameters prohibitively. We thus divide
pre-terminal concepts in the semantic stacks into
two types: (a) enumerable attributes whose val-
ues are associated with distinct semantic stacks in
1555
our model (e.g., inform(pricerange(cheap))),
and (b) non-enumerable attributes whose values
are replaced by a generic symbol before train-
ing in both the utterance and the semantic stack
(e.g., inform(name(X)). These symbolic values
are then replaced in the surface realisation by the
corresponding value in the input specification. A
consequence is that our model can only learn syn-
onymous lexical realisations for enumerable at-
tributes.
4 Certainty-based active learning
A major issue with trainable NLG systems is the
lack of availability of domain-specific data. It is
therefore essential to produce NLG models that
minimise the data annotation cost.
BAGEL supports the optimisation of the data
collection process through active learning, in
which the next semantic input to annotate is de-
termined by the current model. The probabilis-
tic nature of BAGEL allows the use of certainty-
based active learning (Lewis and Catlett, 1994),
by querying the k semantic inputs for which the
model is the least certain about its output real-
isation. Given a finite semantic input space I
representing all possible dialogue acts in our do-
main (i.e., the set of all sets of mandatory seman-
tic stacks Sm), BAGEL?s active learning training
process iterates over the following steps:
1. Generate an utterance for each semantic input Sm ? I
using the current model.2
2. Annotate the k semantic inputs {S1m...S
k
m} yielding
the lowest realisation probability, i.e. for q ? (1..k)
Sqm = argmin
Sm?I\{S1m...S
q?1
m }
(max
R
P (R|Sm)) (9)
with P (R|Sm) defined in (2).
3. Retrain the model with the additional k data points.
The number of utterances to be queried k should
depend on the flexibility of the annotators and the
time required for generating all possible utterances
in the domain.
5 Experimental method
BAGEL?s factored language models are trained us-
ing the SRILM toolkit (Stolcke, 2002), and de-
coding is performed using GMTK?s junction tree
inference algorithm (Bilmes and Zweig, 2002).
2Sampling methods can be used if I is infinite or too
large.
Since each active learning iteration requires gen-
erating all training utterances in our domain, they
are generated using a larger clique pruning thresh-
old than the test utterances used for evaluation.
5.1 Corpus collection
We train BAGEL in the context of a dialogue
system providing information about restaurants
in Cambridge. The domain contains two dia-
logue act types: (a) inform: presenting infor-
mation about a restaurant (see Table 1), and (b)
reject: informing that the user?s constraints can-
not be met (e.g., ?There is no cheap restaurant
in the centre?). Our domain contains 8 restau-
rant attributes: name, food, near, pricerange,
postcode, phone, address, and area, out of
which food, pricerange, and area are treated
as enumerable.3 Our input semantic space is ap-
proximated by the set of information presentation
dialogue acts produced over 20,000 simulated di-
alogues between our statistical dialogue manager
(Young et al, 2010) and an agenda-based user
simulator (Schatzmann et al, 2007), which results
in 202 unique dialogue acts after replacing non-
enumerable values by a generic symbol. Each di-
alogue act contains an average of 4.48 mandatory
semantic stacks.
As one of our objectives is to test whether
BAGEL can learn from data provided by a large
sample of untrained annotators, we collected a
corpus of semantically-aligned utterances using
Amazon?s Mechanical Turk data collection ser-
vice. A crucial aspect of data collection for
NLG is to ensure that the annotators under-
stand the meaning of the semantics to be con-
veyed. Annotators were first asked to provide
an utterance matching an abstract description
of the dialogue act, regardless of the order in
which the constraints are presented (e.g., Offer
the venue Taj Mahal and provide the information
type(restaurant), area(riverside), food(Indian),
near(The Red Lion)). The order of the constraints
in the description was randomised to reduce the
effect of priming. The annotators were then asked
to align the attributes (e.g., Indicate the region of
the utterance related to the concept ?area?), and
the attribute values (e.g., Indicate only the words
related to the concept ?riverside?). Two para-
phrases were collected for each dialogue act in
our domain, resulting in a total of 404 aligned ut-
3With the exception of areas defined as proper nouns.
1556
rt st ht lt
<s> START START START
The Rice Boat inform(name(X)) X inform(name)
is a inform inform EMPTY
restaurant inform(type(restaurant)) restaurant inform(type)
in the inform(area) area inform
riverside inform(area(riverside)) riverside inform(area)
area inform(area) area inform
that inform inform EMPTY
serves inform(food) food inform
French inform(food(French)) French inform(food)
food inform(food) food inform
</s> END END END
Table 2: Example utterance annotation used to estimate the conditional probability distributions of the
models in Figs. 1 and 2 ( rt=realisation phrase, st=semantic stack, ht=stack head, lt=stack tail).
terances produced by 42 native speakers of En-
glish. After manually checking and normalising
the dataset,4 the layered annotations were auto-
matically mapped to phrase-level semantic stacks
by splitting the utterance into phrases at annotation
boundaries. Each annotated utterance is then con-
verted into a sequence of symbols such as in Ta-
ble 2, which are used to estimate the conditional
probability distributions defined in (6) and (8).
The resulting vocabulary consists of 52 distinct se-
mantic stacks and 109 distinct realisation phrases,
with an average of 8.35 phrases per utterance.
5.2 BLEU score evaluation
We first evaluate BAGEL using the BLEU auto-
mated metric (Papineni et al, 2002), which mea-
sures the word n-gram overlap between the gen-
erated utterances and the 2 reference paraphrases
over a test corpus (with n up to 4). While BLEU
suffers from known issues such as a bias towards
statistical NLG systems (Reiter and Belz, 2009), it
provides useful information when comparing sim-
ilar systems. We evaluate BAGEL for different
training set sizes, model dependencies, and active
learning parameters. Our results are averaged over
a 10-fold cross-validation over distinct dialogue
acts, i.e. dialogue acts used for testing are not seen
at training time,5 and all systems are tested on the
same folds. The training and test sets respectively
contain an average of 181 and 21 distinct dialogue
acts, and each dialogue act is associated with two
paraphrases, resulting in 362 training utterances.
4The normalisation process took around 4 person-hour for
404 utterances.
5We do not evaluate performance on dialogue acts used
for training, as the training examples can trivially be used as
generation templates.
!"#$
!"%
!"%$
!"#
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
!"$
!"$$
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
&'(()*+,-(
!".
!".$!"
#
$
%
&
'
(
)
%
*
+
,
-
"
/+)01234)5234+66/+)01234)5234+667)8+)6'1'9-)0-*281:30!";$ <! =! .! #! >! <!! <=! <$! =!! =$! ;!! ;#=
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
.-#/$/$0%*"1%*/2"
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
Figure 4: BLEU score averaged over a 10-fold
cross-validation for different training set sizes and
network topologies, using random sampling.
Results: Fig. 4 shows that adding a dependency
on the future semantic stack improves perfor-
mances for all training set sizes, despite the added
model complexity. Backing off to partial stacks
also improves performance, but only for sparse
training sets.
Fig. 5 compares the full model trained using
random sampling in Fig. 4 with the same model
trained using certainty-based active learning, for
different values of k. As our dataset only con-
tains two paraphrases per dialogue act, the same
dialogue act can only be queried twice during the
active learning procedure. A consequence is that
the training set used for active learning converges
towards the randomly sampled set as its size in-
creases. Results show that increasing the train-
ing set one utterance at a time using active learn-
ing (k = 1) significantly outperforms random
sampling when using 40, 80, and 100 utterances
(p < .05, two-tailed). Increasing the number of
utterances to be queried at each iteration to k = 10
results in a smaller performance increase. A possi-
1557
!"#
!"##
!"$
!"$#
!"%
!"%#
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
&'()*+,-'+./0(1
!"2#
!"3
!"3#
4! 5! 3! $! 6! 4!! 45! 4#! 5!! 5#! 2!! 2$5
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
.-#/$/$0%*"1%*/2"
7890:;,/;'<(0(1,=>47890:;,/;'<(0(1,=>4!
Figure 5: BLEU score averaged over a 10-fold
cross-validation for different numbers of queries
per iteration, using the full model with the query
selection criterion (9).
!"#
!"##
!"$
!"$#
!"%
!"%#
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
&'(()(*+,-*.
!"/#
!"0
!"0#
1! 2! 0! $! 3! 1!! 12! 1#! 2!! 2#! /!! /$2
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
.-#/$/$0%*"1%*/2"
4*+,-*.),5-)6-7854*9+5:;)<9,';)6<-:;
Figure 6: BLEU score averaged over a 10-fold
cross-validation for different query selection cri-
teria, using the full model with k = 1.
ble explanation is that the model is likely to assign
low probabilities to similar inputs, thus any value
above k = 1 might result in redundant queries
within an iteration.
As the length of the semantic stack sequence
is not known before decoding, the active learn-
ing selection criterion presented in (9) is biased
towards longer utterances, which tend to have a
lower probability. However, Fig. 6 shows that
normalising the log probability by the number of
semantic stacks does not improve overall learn-
ing performance. Although a possible explanation
is that longer inputs tend to contain more infor-
mation to learn from, Fig. 6 shows that a base-
line selecting the largest remaining semantic input
at each iteration performs worse than the active
learning scheme for training sets above 20 utter-
ances. The full log probability selection criterion
defined in (9) is therefore used throughout the rest
of the paper (with k = 1).
5.3 Human evaluation
While automated metrics provide useful informa-
tion for comparing different systems, human feed-
back is needed to assess (a) the quality of BAGEL?s
outputs, and (b) whether training models using ac-
tive learning has a significant impact on user per-
ceptions. We evaluate BAGEL through a large-
scale subjective rating experiment using Amazon?s
Mechanical Turk service.
For each dialogue act in our domain, partici-
pants are presented with a ?gold standard? human
utterance from our dataset, which they must com-
pare with utterances generated by models trained
with and without active learning on a set of 20, 40,
100, and 362 utterances (full training set), as well
as with the second human utterance in our dataset.
See example utterances in Table 3. The judges are
then asked to evaluate the informativeness and nat-
uralness of each of the 8 utterances on a 5 point
likert-scale. Naturalness is defined as whether the
utterance could have been produced by a human,
and informativeness is defined as whether it con-
tains all the information in the gold standard utter-
ance. Each utterance is taken from the test folds of
the cross-validation experiment presented in Sec-
tion 5.2, i.e. the models are trained on up to 90%
of the data and the training set does not contain the
dialogue act being tested.
Results: Figs. 7 and 8 compare the naturalness
and informativeness scores of each system aver-
aged over all 202 dialogue acts. A paired t-test
shows that models trained on 40 utterances or
less produce utterances that are rated significantly
lower than human utterances for both naturalness
and informativeness (p < .05, two-tailed). How-
ever, models trained on 100 utterances or more do
not perform significantly worse than human utter-
ances for both dimensions, with a mean difference
below .10 over 202 comparisons. Given the large
sample size, this result suggests that BAGEL can
successfully learn our domain using a fraction of
our initial dataset.
As far as the learning method is concerned, a
paired t-test shows that models trained on 20 and
40 utterances using active learning significantly
outperform models trained using random sam-
pling, for both dimensions (p < .05). The largest
increase is observed using 20 utterances, i.e. the
naturalness increases by .49 and the informative-
ness by .37. When training on 100 utterances, the
effect of active learning becomes insignificant. In-
1558
Input inform(name(the Fountain) near(the Arts Picture House) area(centre) pricerange(cheap))
Human There is an inexpensive restaurant called the Fountain in the centre of town near the Arts Picture House
Rand-20 The Fountain is a restaurant near the Arts Picture House located in the city centre cheap price range
Rand-40 The Fountain is a restaurant in the cheap city centre area near the Arts Picture House
AL-20 The Fountain is a restaurant near the Arts Picture House in the city centre cheap
AL-40 The Fountain is an affordable restaurant near the Arts Picture House in the city centre
Full set The Fountain is a cheap restaurant in the city centre near the Arts Picture House
Input reject(area(Barnwell) near(Saint Mary?s Church))
Human I am sorry but I know of no venues near Saint Mary?s Church in the Barnwell area
Full set I am sorry but there are no venues near Saint Mary?s Church in the Barnwell area
Input inform(name(the Swan)area(Castle Hill) pricerange(expensive))
Human The Swan is a restaurant in Castle Hill if you are seeking something expensive
Full set The Swan is an expensive restaurant in the Castle Hill area
Input inform(name(Browns) area(centre) near(the Crowne Plaza) near(El Shaddai) pricerange(cheap))
Human Browns is an affordable restaurant located near the Crowne Plaza and El Shaddai in the centre of the city
Full set Browns is a cheap restaurant in the city centre near the Crowne Plaza and El Shaddai
Table 3: Example utterances for different input dialogue acts and system configurations. AL-20 = active
learning with 20 utterances, Rand = random sampling.
!"## !"$%
!"&'!"(% !")* *"%% *"%#
*"%'
++"$
!!"$
**"$
$
!
"
#
$
%
$
#
&
'
(
#
)
$
"
*
*
%
*
+
,
(
"
,-./01
##"$ +% *% #%% !(+
!
"
#
$
%
$
#
&
'
(
#
)
$
"
*
*
%
*
+
,
(
"
-(#.$.$/%*"&%*.0"
234567897-:.5.;<=1-.8=447:-.378>8*"%'
Figure 7: Naturalness mean opinion scores for dif-
ferent training set sizes, using random sampling
and active learning. Differences for training set
sizes of 20 and 40 are all significant (p < .05).
terestingly, while models trained on 100 utterances
outperform models trained on 40 utterances using
random sampling (p < .05), they do not signifi-
cantly outperform models trained on 40 utterances
using active learning (p = .15 for naturalness and
p = .41 for informativeness). These results sug-
gest that certainty-based active learning is benefi-
cial for training a generator from a limited amount
of data given the domain size.
Looking back at the results presented in Sec-
tion 5.2, we find that the BLEU score correlates
with a Pearson correlation coefficient of .42 with
the mean naturalness score and .35 with the mean
informativeness score, over all folds of all systems
tested (n = 70, p < .01). This is lower than
previous correlations reported by Reiter and Belz
(2009) in the shipping forecast domain with non-
expert judges (r = .80), possibly because our do-
main is larger and more open to subjectivity.
!"## !"$$
#"%&!"'& !"()
#"%$ #"%#
#"&!
**"+
!!"+
##"+
+
!
"
#
$
%
&
$
'
(
)
*
#
+
&
,
"
$
"
-
-
%
-
.
(
)
"
,-./01
&&"+ *% #% &%% !)*
!
"
#
$
%
&
$
'
(
)
*
#
+
&
,
"
$
"
-
-
%
-
.
(
)
"
/)#&$&$0%-"+%-&1"
234567897-:.5.;<=1-.8=447:-.378>8#"&!
Figure 8: Informativeness mean opinion scores for
different training set sizes, using random sampling
and active learning. Differences for training set
sizes of 20 and 40 are all significant (p < .05).
6 Related work
While most previous work on trainable NLG re-
lies on a handcrafted component (see Section 1),
recent research has started exploring fully data-
driven NLG models.
Factored language models have recently been
used for surface realisation within the OpenCCG
framework (White et al, 2007; Espinosa et al,
2008). More generally, chart generators for
different grammatical formalisms have been
trained from syntactic treebanks (White et al,
2007; Nakanishi et al, 2005), as well as from
semantically-annotated treebanks (Varges and
Mellish, 2001). However, a major difference with
our approach is that BAGEL uses domain-specific
data to generate a surface form directly from se-
mantic concepts, without any syntactic annotation
(see Section 7 for further discussion).
1559
This work is strongly related to Wong and
Mooney?s WASP?1 generation system (2007),
which combines a language model with an in-
verted synchronous CFG parsing model, effec-
tively casting the generation task as a translation
problem from a meaning representation to natu-
ral language. WASP?1 relies on GIZA++ to align
utterances with derivations of the meaning repre-
sentation (Och and Ney, 2003). Although early
experiments showed that GIZA++ did not perform
well on our data?possibly because of the coarse
granularity of our semantic representation?future
work should evaluate the generalisation perfor-
mance of synchronous CFGs in a dialogue system
domain.
Although we do not know of any work on ac-
tive learning for NLG, previous work has used
active learning for semantic parsing and informa-
tion extraction (Thompson et al, 1999; Tang et al,
2002), spoken language understanding (Tur et al,
2003), speech recognition (Hakkani-Tu?r et al,
2002), word alignment (Sassano, 2002), and more
recently for statistical machine translation (Blood-
good and Callison-Burch, 2010). While certainty-
based methods have been widely used, future work
should investigate the performance of committee-
based active learning for NLG, in which examples
are selected based on the level of disagreement be-
tween models trained on subsets of the data (Fre-
und et al, 1997).
7 Discussion and conclusion
This paper presents and evaluates BAGEL, a sta-
tistical language generator that can be trained en-
tirely from data, with no handcrafting required be-
yond the semantic annotation. All the required
subtasks?i.e. content ordering, aggregation, lex-
ical selection and realisation?are learned from
data using a unified model. To train BAGEL in a di-
alogue system domain, we propose a stack-based
semantic representation at the phrase level, which
is expressive enough to generate natural utterances
from unseen inputs, yet simple enough for data to
be collected from 42 untrained annotators with a
minimal normalisation step. A human evaluation
over 202 dialogue acts does not show any differ-
ence in naturalness and informativeness between
BAGEL?s outputs and human utterances. Addition-
ally, we find that the data collection process can
be optimised using active learning, resulting in a
significant increase in performance when training
data is limited, according to ratings from 18 hu-
man judges.6 These results suggest that the pro-
posed framework can largely reduce the develop-
ment time of NLG systems.
While this paper only evaluates the most likely
realisation given a dialogue act, we believe that
BAGEL?s probabilistic nature and generalisation
capabilities are well suited to model the linguis-
tic variation resulting from the diversity of annota-
tors. Our first objective is thus to evaluate the qual-
ity of BAGEL?s n-best outputs, and test whether
sampling from the output distribution can improve
naturalness and user satisfaction within a dialogue.
Our results suggest that explicitly modelling
syntax is not necessary for our domain, possi-
bly because of the lack of syntactic complexity
compared with formal written language. Never-
theless, future work should investigate whether
syntactic information can improve performance in
more complex domains. For example, the reali-
sation phrase can easily be conditioned on syntac-
tic constructs governing that phrase, and the recur-
sive nature of syntax can be modelled by keeping
track of the depth of the current embedded clause.
While syntactic information can be included with
no human effort by using syntactic parsers, their
robustness to dialogue system utterances must first
be evaluated.
Finally, recent years have seen HMM-based
synthesis models become competitive with unit se-
lection methods (Tokuda et al, 2000). Our long
term objective is to take advantage of those ad-
vances to jointly optimise the language genera-
tion and the speech synthesis process, by combin-
ing both components into a unified probabilistic
concept-to-speech generation model.
References
S. Bangalore and O. Rambow. Exploiting a probabilistic hi-
erarchical model for generation. In Proceedings of the
18th International Conference on Computational Linguis-
tics (COLING), pages 42?48, 2000.
A. Belz. Automatic generation of weather forecast texts us-
ing comprehensive probabilistic generation-space models.
Natural Language Engineering, 14(4):431?455, 2008.
J. Bilmes and K. Kirchhoff. Factored language models and
generalized parallel backoff. In Proceedings of HLT-
NAACL, short papers, 2003.
J. Bilmes and G. Zweig. The Graphical Models ToolKit: An
open source software system for speech and time-series
processing. In Proceedings of ICASSP, 2002.
6The full training corpus and the generated
utterances used for evaluation are available at
http://mi.eng.cam.ac.uk/?farm2/bagel.
1560
M. Bloodgood and C. Callison-Burch. Bucking the trend:
Large-scale cost-focused active learning for statistical ma-
chine translation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics
(ACL), 2010.
D. Espinosa, M. White, and D. Mehay. Hypertagging: Su-
pertagging for surface realization with CCG. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), 2008.
Y. Freund, H. S. Seung, E.Shamir, and N. Tishby. Selective
sampling using the query by committee algorithm. Ma-
chine Learning, 28:133?168, 1997.
D. Hakkani-Tu?r, G. Riccardi, and A. Gorin. Active learn-
ing for automatic speech recognition. In Proceedings of
ICASSP, 2002.
Y. He and S. Young. Semantic processing using the Hidden
Vector State model. Computer Speech & Language, 19
(1):85?106, 2005.
A. Isard, C. Brockmann, and J. Oberlander. Individuality and
alignment in generated dialogues. In Proceedings of the
4th International Natural Language Generation Confer-
ence (INLG), pages 22?29, 2006.
I. Langkilde and K. Knight. Generation that exploits corpus-
based statistical knowledge. In Proceedings of the 36th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 704?710, 1998.
F. Lefe`vre. A DBN-based multi-level stochastic spoken lan-
guage understanding system. In Proceedings of the IEEE
Workshop on Spoken Language Technology (SLT), 2006.
D. D. Lewis and J. Catlett. Heterogeneous uncertainty am-
pling for supervised learning. In Proceedings of ICML,
1994.
F. Mairesse and M. A. Walker. Trainable generation of Big-
Five personality styles through data-driven parameter esti-
mation. In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL), 2008.
H. Nakanishi, Y. Miyao, , and J. Tsujii. Probabilistic methods
for disambiguation of an HPSG-based chart generator. In
Proceedings of the IWPT, 2005.
F. J. Och and H. Ney. A systematic comparison of various
statistical alignment models. Computational Linguistics,
29(1):19?51, 2003.
D. S. Paiva and R. Evans. Empirically-based control of nat-
ural language generation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational Lin-
guistics (ACL), pages 58?65, 2005.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a
method for automatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), 2002.
L. R. Rabiner. Tutorial on Hidden Markov Models and se-
lected applications in speech recognition. Proceedings of
the IEEE, 77(2):257?285, 1989.
E. Reiter and A. Belz. An investigation into the validity
of some metrics for automatically evaluating natural lan-
guage generation systems. Computational Linguistics, 25:
529?558, 2009.
V. Rieser and O. Lemon. Natural language generation as
planning under uncertainty for spoken dialogue systems.
In Proceedings of the Annual Meeting of the European
Chapter of the ACL (EACL), 2009.
M. Sassano. An empirical study of active learning with sup-
port vector machines for japanese word segmentation. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), 2002.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. Agenda-based user simulation for bootstrap-
ping a POMDP dialogue system. In Proceedings of HLT-
NAACL, short papers, pages 149?152, 2007.
A. Stolcke. SRILM ? an extensible language modeling
toolkit. In Proceedings of the International Conference
on Spoken Language Processing, 2002.
M. Tang, X. Luo, and S. Roukos. Active learning for statis-
tical natural language parsing. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), 2002.
C. Thompson, M. E. Califf, and R. J. Mooney. Active learn-
ing for natural language parsing and information extrac-
tion. In Proceedings of ICML, 1999.
B. Thomson and S. Young. Bayesian update of dialogue state:
A POMDP framework for spoken dialogue systems. Com-
puter Speech & Language, 24(4):562?588, 2010.
Y. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and
T. Kitamura. Speech parameter generation algorithms for
HMM-based speech synthesis. In Proceedings of ICASSP,
2000.
G. Tur, R. E. Schapire, and D. Hakkani-Tu?r. Active learn-
ing for spoken language understanding. In Proceedings of
ICASSP, 2003.
S. Varges and C. Mellish. Instance-based natural language
generation. In Proceedings of the Annual Meeting of the
North American Chapter of the ACL (NAACL), 2001.
M. A. Walker, O. Rambow, and M. Rogati. Training a sen-
tence planner for spoken dialogue using boosting. Com-
puter Speech and Language, 16(3-4), 2002.
M. White, R. Rajkumar, and S. Martin. Towards broad cov-
erage surface realization with CCG. In Proceedings of the
Workshop on Using Corpora for NLG: Language Genera-
tion and Machine Translation, 2007.
Y. W. Wong and R. Mooney. Generation by inverting a se-
mantic parser that uses statistical machine translation. In
Proceedings of HLT-NAACL, 2007.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, J. Schatzmann,
B. Thomson, and K. Yu. The Hidden Information State
model: a practical framework for POMDP-based spoken
dialogue management. Computer Speech and Language,
24(2):150?174, 2010.
1561
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 116?123,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Parameter estimation for agenda-based user simulation
Simon Keizer, Milica Gas?ic?, Filip Jurc???c?ek, Franc?ois Mairesse,
Blaise Thomson, Kai Yu, and Steve Young ?
University of Cambridge, Department of Engineering, Cambridge (UK)
{sk561,mg436,fj228,farm2,brmt2,ky219,sjy}@cam.ac.uk
Abstract
This paper presents an agenda-based user
simulator which has been extended to be
trainable on real data with the aim of more
closely modelling the complex rational be-
haviour exhibited by real users. The train-
able part is formed by a set of random de-
cision points that may be encountered dur-
ing the process of receiving a system act
and responding with a user act. A sample-
based method is presented for using real
user data to estimate the parameters that
control these decisions. Evaluation results
are given both in terms of statistics of gen-
erated user behaviour and the quality of
policies trained with different simulators.
Compared to a handcrafted simulator, the
trained system provides a much better fit
to corpus data and evaluations suggest that
this better fit should result in improved di-
alogue performance.
1 Introduction
In spoken dialogue systems research, modelling
dialogue as a (Partially Observable) Markov Deci-
sion Process ((PO)MDP) and using reinforcement
learning techniques for optimising dialogue poli-
cies has proven to be an effective method for de-
veloping robust systems (Singh et al, 2000; Levin
et al, 2000). However, since this kind of optimi-
sation requires a simulated user to generate a suffi-
ciently large number of interactions to learn from,
this effectiveness depends largely on the quality
of such a user simulator. An important require-
ment for a simulator is for it to be realistic, i.e., it
should generate behaviour that is similar to that of
real users. Trained policies are then more likely to
perform better on real users, and evaluation results
on simulated data are more likely to predict results
on real data more accurately.
?This research was partly funded by the UK EPSRC un-
der grant agreement EP/F013930/1 and by the EU FP7 Pro-
gramme under grant agreement 216594 (CLASSiC project:
www.classic-project.org).
This is one of the reasons why learning user
simulation models from data on real user be-
haviour has become an important direction of re-
search (Scheffler and Young, 2001; Cuaya?huitl et
al., 2005; Georgila et al, 2006). However, the data
driven user models developed so far lack the com-
plexity required for training high quality policies
in task domains where user behaviour is relatively
complex. Handcrafted models are still the most
effective in those cases.
This paper presents an agenda-based user simu-
lator which is handcrafted for a large part, but ad-
ditionally can be trained with data from real users
(Section 2). As a result, it generates behaviour that
better reflects the statistics of real user behaviour,
whilst preserving the complexity and rationality
required to effectively train dialogue management
policies. The trainable part is formed by a set of
random decision points, which, depending on the
context, may or may not be encountered during
the process of receiving a system act and decid-
ing on a response act. If such a point is encoun-
tered, the simulator makes a random decision be-
tween a number of options which may directly or
indirectly influence the resulting output. The op-
tions for each random decision point are reason-
able in the context in which it is encountered, but
a uniform distribution of outcomes might not re-
flect real user behaviour.
We will describe a sample-based method for es-
timating the parameters that define the probabili-
ties for each possible decision, using data on real
users from a corpus of human-machine dialogues
(Section 3). Evaluation results will be presented
both in terms of statistics on generated user be-
haviour and the quality of dialogue policies trained
with different user simulations (Section 4).
2 Agenda-based user simulation
In agenda-based user simulation, user acts are gen-
erated on the basis of a user goal and an agenda
(Schatzmann et al, 2007a). The simulator pre-
sented here is developed and used for a tourist in-
116
formation application, but is sufficiently generic to
accommodate slot-filling applications in any do-
main.1 The user goal consists of the type of venue,
for example hotel, bar or restaurant, a list
of constraints in the form of slot value pairs, such
as food=Italian or area=east, and a list
of slots the user wants to know the value of, such
as the address (addr), phone number (phone),
or price information (price) of the venue. The
user goals for the simulator are randomly gener-
ated from the domain ontology describing which
combinations of venue types and constraints are
allowed and what are the possible values for each
slot. The agenda is a stack-like structure contain-
ing planned user acts. When the simulator receives
a system act, the status of the user goal is updated
as well as the agenda, typically by pushing new
acts onto it. In a separate step, the response user
act is selected by popping one or more items off
the agenda.
Although the agenda-based user simulator in-
troduced by Schatzmann et al (2007a) was en-
tirely handcrafted, it was realistic enough to suc-
cessfully test a prototype POMDP dialogue man-
ager and train a dialogue policy that outperformed
a handcrafted baseline (Young et al, 2009). A
method to train an agenda-based user simula-
tor from data was proposed by Schatzmann et
al. (2007b). In this approach, operations on
the agenda are controlled by probabilities learned
from data using a variation of the EM algorithm.
However, this approach does not readily scale to
more complex interactions in which users can, for
example, change their goal midway through a dia-
logue.
2.1 Random decision parameters
Each time the user simulator receives a system act,
a complex, two-fold process takes place involving
several decisions, made on the basis of both the
nature of the incoming system act and the infor-
mation state of the user, i.e., the status of the user
goal and agenda. The first phase can be seen as
an information state update and involves actions
like filling requested slots or checking whether the
provided information is consistent with the user
goal constraints. In the second phase, the user de-
cides which response act to generate, based on the
updated agenda. Many of the decisions involved
are deterministic, allowing only one possible op-
tion given the context. Other decisions allow for
some degree of variation in the user behaviour and
are governed by probability distributions over the
1We have to date also implemented systems in appoint-
ment scheduling and bus timetable inquiries.
options allowed in that context. For example, if
the system has offered a venue that matches the
user?s goal, the user can randomly decide to either
change his goal or to accept the venue and ask for
additional information such as the phone number.
The non-deterministic part of the simulator is
formalised in terms of a set of random decision
points (RDPs) embedded in the decision process.
If an RDP is encountered (depending on the con-
text), a random choice between the options de-
fined for that point is made by sampling from a
probability distribution. Most of the RDPs are
controlled by a multinomial distribution, such as
deciding whether or not to change the goal after
a system offer. Some RDPs are controlled by a
geometric distribution, like in the case where the
user is planning to specify one of his constraints
(with an inform act popped from the agenda) and
then repeatedly adds an additional constraint to the
act (by combining it with an additional inform act
popped from the agenda) until it randomly decides
not to add any more constraints (or runs out of
constraints to specify). The parameter for this dis-
tribution thus controls how cautious the user is in
providing information to the system.
Hence, the user simulator can be viewed as
a ?decision network?, consisting of deterministic
and random decision points. This is illustrated in
Figure 1 for the simplified case of a network with
only four RDPs; the actual simulator has 23 RDPs,
with 27 associated parameters in total. Each time
the simulator receives a system act, it follows a
path through the network, which is partly deter-
mined by that system act and the user goal and
agenda, and partly by random decisions made ac-
cording to the probability distributions for each
random decision point i given by its parameters
?i.
3 Training the simulator from data
The parameterisation of the user simulator as de-
scribed in Section 2.1 forms the basis for a method
for training the simulator with real user data. The
parameters describing the probability distributions
for each RDP are estimated in order to generate
user behaviour that fits the user behaviour in the
corpus as closely as possible. In order to do so,
a sample based maximum likelihood approach is
taken, in which the simulator is run repeatedly
against the system acts in the corpus, and the ran-
dom decisions that lead to simulated acts matching
the true act in the corpus are recorded. The param-
eters are then estimated using the counts for each
of the random decision points.
117
incoming
system act
outgoing
user act
user goal + agenda
?2
?1
?3
?4
Figure 1: User simulator viewed as a ?decision network?: square nodes indicate deterministic decision
points; round nodes indicate random decision points, and have associated parameters ?i; the loop on one
of the nodes indicates it has a geometric distribution associated with it.
3.1 Parameter estimation
Before starting the process of matching simulated
acts with true acts and collecting counts for the
RDPs, the parameters are initialised to values cor-
responding to uniform distributions. Then, the
simulator is run against all dialogues in the cor-
pus in such a way that for each turn in a dialogue
(consisting of a system act and a user act), the user
simulator is provided with the system act and is
run repeatedly to generate several simulated user
response acts for that turn. For the first turn of a di-
alogue, the simulator is initialised with the correct
user state (see Section 3.2). For each response, the
simulator may make different random decisions,
generally leading to different user acts. The deci-
sions that lead to a simulated act that matches the
true act are recorded as successful. By generating
a sufficiently large number of simulated acts, all
possible combinations of decisions are explored to
find a matching act. Given the high complexity of
the simulator, this sampling approach is preferred
over directly enumerating all decision combina-
tions to identify the successful ones. If none of
the combinations are successful, then either a) the
processing of the dialogue is ended, or b) the cor-
rect context is set for the next turn and processing
is continued. Whereas the former approach aims at
matching sequences of turns, the latter only aims
at matching each user turn separately. In either
case, after all data is processed, the parameters are
estimated using the resulting counts of successful
decisions for each of the RDPs.
For each RDP i, let DPi represent the decision
taken, and dij the j?th possible decision. Then, for
each decision point i that is controlled by a multi-
nomial distribution, the corresponding parameter
estimates ?ij are obtained as follows from the de-
cision frequencies c(DPi = dij):
?ij =
c(DPi = dij)
?
j c(DPi = dij)
(1)
Random decision points that are controlled
by geometric distributions involve potentially
multiple random decisions between two options
(Bernoulli trials). The parameters for such RDPs
are estimated as follows:
?i =
(
1
n
n
?
k=1
bik
)?1
(2)
where bik is the number of Bernoulli trials re-
quired at the k?th time decision point i was en-
countered. In terms of the decision network, this
estimate is correlated with the average number of
times the loop of the node was taken.
3.2 User goal inference
In order to be able to set the correct user goal
state in any given turn, a set of update rules is
used to infer the user?s goals from a dialogue be-
forehand, on the basis of the entire sequence of
system acts and ?true? user acts (see Section 4.1)
in the corpus. These update rules are based on
the notion of dialogue act preconditions, which
specify conditions of the dialogue context that
must hold for a dialogue agent to perform that
act. For example, a precondition for the act
inform(area=central) is that the speaker
wants a venue in the centre. The user act model
118
of the HIS dialogue manager is designed accord-
ing to this same notion (Keizer et al, 2008). In this
model, the probability of a user act in a certain dia-
logue context (the last system act and a hypothesis
regarding the user goal) is determined by checking
the consistency of its preconditions with that con-
text. This contributes to updating the system?s be-
lief state on the basis of which it determines its re-
sponse action. For the user goal inference model,
the user act is given and therefore its precondi-
tions can be used to directly infer the user goal.
So, for example, in the case of observing the user
act inform(area=central), the constraint
(area=central) is added to the user goal.
In addition to using the inferred user goals, the
agenda is corrected in cases where there is a mis-
match between real and simulated user acts in the
previous turn.
In using this offline goal inference model, our
approach takes a position between (Schatzmann et
al., 2007b), in which the user?s goal is treated as
hidden, and (Georgila et al, 2006), in which the
user?s goal is obtained directly from the corpus an-
notation.
4 Evaluation
The parameter estimation technique for training
the user simulator was evaluated in two differ-
ent ways. The first evaluation involved compar-
ing the statistics of simulated and real user be-
haviour. The second evaluation involved compar-
ing dialogue manager policies trained with differ-
ent simulators.
4.1 Data
The task of the dialogue systems we are develop-
ing is to provide tourist information to users, in-
volving venues such as bars, restaurants and hotels
that the user can search for and ask about. These
venues are described in terms of features such as
price range, area, type of food, phone number,
address, and so on. The kind of dialogues with
these systems are commonly called slot-filling di-
alogues.
Within the range of slot-filling applications the
domain is relatively complex due to its hierarchi-
cal data structure and relatively large number of
slots and their possible values. Scalability is in-
deed one of the primary challenges to be addressed
in statistical approaches to dialogue system devel-
opment, including user simulation.
The dialogue corpus that was used for training
and evaluating the simulator was obtained from
the evaluation of a POMDP spoken dialogue sys-
tem with real users. All user utterances in the
resulting corpus were transcribed and semanti-
cally annotated in terms of dialogue acts. Dia-
logue acts consist of a series of semantic items,
including the type (describing the intention of
the speaker, e.g., inform or request) and a
list of slot value pairs (e.g., food=Chinese or
area=south). An extensive analysis of the an-
notations from three different people revealed a
high level of inter-annotator agreement (ranging
from 0.81 to 0.94, depending on which pair of an-
notations are compared), and a voting scheme for
selecting a single annotation for each turn ensured
the reliability of the ?true? user acts used for train-
ing the simulator.
4.2 Corpus statistics results
A first approach to evaluating user simulations is
to look at the statistics of the user behaviour that
is generated by a simulator and compare it with
that of real users as observed in a dialogue cor-
pus. Several metrics for such evaluations have
been considered in the literature, all of which have
both strong points and weaknesses. For the present
evaluation, a selection of metrics believed to give
a reasonable first indication of the quality of the
user simulations was considered2 .
4.2.1 Metrics
The first corpus-based evaluation metric is the Log
Likelihood (LL) of the data, given the user simu-
lation model. This is what is in fact maximised by
the parameter estimation algorithm. The log like-
lihood can be computed by summing the log prob-
abilities of each user turn du in the corpus data D:
ll(D|{?ij}, {?i}) =
?
u
log P (du|{?ij}, {?i})
(3)
The user turn probability is given by the prob-
ability of the decision paths (directed paths in the
decision network of maximal length, such as the
one indicated in Figure 1 in bold) leading to a sim-
ulated user act in that turn that matches the true
user act. The probability of a decision path is ob-
tained by multiplying the probabilities of the de-
cisions made at each decision point i that was en-
countered, which are given by the parameters ?ij
2Note that not all selected metrics are metrics in the strict
sense of the word; the term should therefore be interpreted as
a more general one.
119
and ?i:
logP (du|{?ij}, {?i}) =
?
i?Im(u)
log
(
?
j
?ij ? ?ij(u)
)
+ (4)
?
i?Ig(u)
log
(
?
k
(1 ? ?i)k?1 ? ?i ? ?ik(u)
)
where Im(u) = {i ? Im|?j ?ij(u) > 0} and
Ig(u) = {i ? Ig|
?
k ?ik(u) > 0} are the subsets
of the multinomial (Im) and geometric (Ig) de-
cision points respectively containing those points
that were encountered in any combination of deci-
sions resulting in the given user act:
?ij(u) =
?
?
?
?
?
1 if decision DPi = dij was
taken in any of the
matching combinations
0 otherwise
(5)
?ik(u) =
?
?
?
?
?
1 if any of the matching
combinations required
k > 0 trials
0 otherwise
(6)
It should be noted that the log likelihood only
represents those turns in the corpus for which the
simulated user can produce a matching simulated
act with some probability. Hence, it is impor-
tant to also take into account the corpus cover-
age when considering the log likelihood in cor-
pus based evaluation. Dividing by the number of
matched turns provides a useful normalisation in
this respect.
The expected Precision (PRE), Recall (RCL),
and F-Score (FS) are obtained by comparing the
simulated user acts with the true user acts in the
same context (Georgila et al, 2006). These scores
are obtained by pairwise comparison of the simu-
lated and true user act for each turn in the corpus
at the level of the semantic items:
PRE = #(matched items)#(items in simulated act) (7)
RCL = #(matched items)#(items in true act) (8)
FS = 2 ? PRE ? RCLPRE + RCL (9)
By sampling a sufficient number of simulated
acts for each turn in the corpus and comparing
them with the corresponding true acts, this results
in an accurate measure on average.
The problem with precision and recall is that
they are known to heavily penalise unseen data.
Any attempt to generalise and therefore increase
the variability of user behaviour results in lower
scores.
Another way of evaluating the user simulator
is to look at the global user act distributions it
generates and compare them to the distributions
found in the real user data. A common metric
for comparing such distributions is the Kullback-
Leibler (KL) distance. In (Cuaya?huitl et al,
2005) this metric was used to evaluate an HMM-
based user simulation approach. The KL dis-
tance is computed by taking the average of the
two KL divergences3 DKL(simulated||true) and
DKL(true||simulated), where:
DKL(p||q) =
?
i
pi ? log2(
pi
qi
) (10)
KL distances are computed for both full user act
distributions (taking into account both the dia-
logue act type and slot value pairs) and user act
type distributions (only regarding the dialogue act
type), denoted by KLF and KLT respectively.
4.2.2 Results
For the experiments, the corpus data was ran-
domly split into a training set, consisting of 4479
user turns in 541 dialogues, used for estimat-
ing the user simulator parameters, and a test set,
consisting of 1457 user turns in 175 dialogues,
used for evaluation only. In the evaluation, the
following parameter settings were compared: 1)
non-informative, uniform parameters (UNIF); 2)
handcrafted parameters (HDC); 3) parameters es-
timated from data (TRA); and 4) deterministic pa-
rameters (DET), in which for each RDP the prob-
ability of the most probable decision according to
the estimated parameters is set to 1, i.e., at all
times, the most likely decision according to the es-
timated parameters is chosen.
For both trained and deterministic parameters,
a distinction is made between the two approaches
to matching user acts during parameter estimation.
Recall that in the turn-based approach, in each
turn, the simulator is run with the corrected con-
text to find a matching simulated act, whereas in
the sequence-based approach, the matching pro-
cess for a dialogue is stopped in case a turn
is encountered which cannot be matched by the
simulator. This results in estimated parameters
TRA-T and deterministic parameters DET-T for
3Before computing the distances, add-one smoothing was
applied in order to avoid zero-probabilities.
120
PAR nLL-T nLL-S PRE RCL FS KLF KLT
UNIF ?3.78 ?3.37 16.95 (?0.75) 9.47 (?0.59) 12.15 3.057 2.318
HDC ?4.07 ?2.22 44.31 (?0.99) 34.74 (?0.95) 38.94 1.784 0.623
TRA-T ?2.97 - 37.60 (?0.97) 28.14 (?0.90) 32.19 1.362 0.336
DET-T ?? - 47.70 (?1.00) 40.90 (?0.98) 44.04 2.335 0.838
TRA-S - ?2.13 43.19 (?0.99) 35.68 (?0.96) 39.07 1.355 0.155
DET-S - ?? 49.39 (?1.00) 43.04 (?0.99) 46.00 2.310 0.825
Table 1: Results of the sample-based user simulator evaluation on the Mar?09 training
corpus (the corpus coverage was 59% for the turn-based and 33% for the sequence-based
matching approach).
PAR nLL-T nLL-S PRE RCL FS KLF KLT
UNIF ?3.61 ?3.28 16.59 (?1.29) 9.32 (?1.01) 11.93 2.951 2.180
HDC ?3.90 ?2.19 45.35 (?1.72) 36.04 (?1.66) 40.16 1.780 0.561
TRA-T ?2.84 - 38.22 (?1.68) 28.74 (?1.57) 32.81 1.405 0.310
DET-T ?? - 49.15 (?1.73) 42.17 (?1.71) 45.39 2.478 0.867
TRA-S - ?2.12 43.90 (?1.72) 36.52 (?1.67) 39.87 1.424 0.153
DET-S - ?? 50.73 (?1.73) 44.41 (?1.72) 47.36 2.407 0.841
Table 2: Results of the sample-based user simulator evaluation on the Mar?09 test corpus
(corpus coverage 59% for the turn-based, and 36% for sequence-based matching).
the turn-based approach and analogously TRA-S
and DET-S for the sequence-based approach. The
corresponding normalised (see Section 4.2.1) log-
likelihoods are indicated by nLL-T and nLL-S.
Tables 1 and 2 give the results on the training
and test data respectively. The results show that in
terms of log-likelihood and KL-distances, the es-
timated parameters outperform the other settings,
regardless of the matching method. In terms of
precision/recall (given in percentages with 95%
confidence intervals), the estimated parameters
are worse than the handcrafted parameters for
turn-based matching, but have similar scores for
sequence-based matching.
The results for the deterministic parameters il-
lustrate that much better precision/recall scores
can be obtained, but at the expense of variability as
well as the KL-distances. It will be easier to train
a dialogue policy on such a deterministic simula-
tor, but that policy is likely to perform significantly
worse on the more varied behaviour generated by
the trained simulator, as we will see in Section 4.3.
Out of the two matching approaches, the
sequence-based approach gives the best results:
TRA-S outperforms TRA-T on all scores, except
for the coverage which is much lower for the
sequence-based approach (33% vs. 59%).
4.3 Policy evaluation results
Although the corpus-based evaluation results give
a useful indication of how realistic the behaviour
generated by a simulator is, what really should be
evaluated is the dialogue management policy that
is trained using that simulator. Therefore, differ-
ent parameter sets for the simulator were used to
train and evaluate different policies for the Hidden
Information State (HIS) dialogue manager (Young
et al, 2009). Four different policies were trained:
one policy using handcrafted simulation param-
eters (POL-HDC); two policies using simulation
parameters estimated (using the sequence-based
matching approach) from two data sets that were
obtained by randomly splitting the data into two
parts of 358 dialogues each (POL-TRA1 and POL-
TRA2); and finally, a policy using a determin-
istic simulator (POL-DET) constructed from the
trained parameters as discussed in Section 4.2.2.
The policies were then each evaluated on the sim-
ulator using the four parameter settings at different
semantic error rates.
The performance of a policy is measured in
terms of a reward that is given for each dialogue,
i.e. a reward of 20 for a successful dialogue, mi-
nus the number of turns. A dialogue is consid-
ered successful if the system has offered a venue
matching the predefined user goal constraints and
has given the correct values of all requested slots
for this venue. During the policy optimisation, in
which a reinforcement learning algorithm tries to
optimise the expected long term reward, this dia-
logue scoring regime was also used.
In Figures 2, 3, and 4, evaluation results are
given resulting from running 3000 dialogues at
each of 11 different semantic error rates. The
curves show average rewards with 95% confidence
intervals. The error rate is controlled by a hand-
121
-2
 0
 2
 4
 6
 8
 10
 12
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
Error rate
POL-HDC
POL-TRA1
POL-TRA2
POL-DET
Figure 2: Average rewards for each policy when
evaluated on UM-HDC.
-4
-2
 0
 2
 4
 6
 8
 10
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
Error rate
POL-HDC
POL-TRA1
POL-TRA2
POL-DET
Figure 3: Average rewards for each policy when
evaluated on UM-TRA1.
 2
 4
 6
 8
 10
 12
 14
 16
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
Error rate
POL-HDC
POL-TRA1
POL-TRA2
POL-DET
Figure 4: Average rewards for each policy when
evaluated on UM-DET.
 0
 1
 2
 3
 4
 5
 6
 7
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
 lo
ss
Error rate
POL-HDC
POL-TRA2
POL-DET
Figure 5: Average loss in reward for each policy,
across three different simulators.
crafted error model that converts the user act gen-
erated by the simulator into an n-best list of dia-
logue act hypotheses.
The policy that was trained using the hand-
crafted simulator (POL-HDC) outperforms the
other policies when evaluated on that same sim-
ulator (see Figure 2), and both policies trained us-
ing the trained simulators (POL-TRA1 and POL-
TRA2) outperform the other policies when evalu-
ated on either trained simulator (see Figure 3 for
the evaluation on UM-TRA1; the evaluation on
UM-TRA2 is very similar and therefore omitted).
There is little difference in performance between
policies POL-TRA1 and POL-TRA2, which can
be explained by the fact that the two trained
parameter settings are quite similar, in contrast
to the handcrafted parameters. The policy that
was trained on the deterministic parameters (POL-
DET) is competitive with the other policies when
evaluated on UM-DET (see Figure 4), but per-
forms significantly worse on the other parameter
settings which generate the variation in behaviour
that the dialogue manager did not encounter dur-
ing training of POL-DET.
In addition to comparing the policies when eval-
uated on each simulator separately, another com-
parison was made in terms of the average perfor-
mance across all simulators. For each policy and
each simulator, we first computed the difference
between the policy?s performance and the ?maxi-
mum? performance on that simulator as achieved
by the policy that was also trained on that simu-
lator, and then averaged over all simulators. To
avoid biased results, only one of the trained simu-
lators was included. The results in Figure 5 show
that the POL-TRA2 policy is more robust than
POL-DET, and has similar robustness as POL-
HDC. Similar results are obtained when including
UM-TRA1 only.
Given that the results of Section 4.2 show that
the dialogues generated by the trained simulator
more closely match real corpus data, and given
that the above simulation results show that the
POL-TRA policies are at least as robust as the
122
other policies, it seems likely that policies trained
using the trained user simulator will show im-
proved performance when evaluated on real users.
However, this claim can only be properly
demonstrated in a real user evaluation of the di-
alogue system containing different dialogue man-
agement policies. Such a user trial would also be
able to confirm whether the results from evalua-
tions on the trained simulator can more accurately
predict the actual performance expected with real
users.
5 Conclusion
In this paper, we presented an agenda-based user
simulator extended to be trainable on real user
data whilst preserving the necessary rationality
and complexity for effective training and evalu-
ation of dialogue manager policies. The exten-
sion involved the incorporation of random deci-
sion points in the process of receiving and re-
sponding to a system act in each turn. The deci-
sions made at these points are controlled by prob-
ability distributions defined by a set of parameters.
A sample-based maximum likelihood approach
to estimating these parameters from real user data
in a corpus of human-machine dialogues was dis-
cussed, and two kinds of evaluations were pre-
sented. When comparing the statistics of real ver-
sus simulated user behaviour in terms of a selec-
tion of different metrics, overall, the estimated pa-
rameters were shown to give better results than
the handcrafted baselines. When evaluating dia-
logue management policies trained on the simula-
tor with different parameter settings, it was shown
that: 1) policies trained on a particular parame-
ter setting outperform other policies when evalu-
ated on the same parameters, and in particular, 2)
a policy trained on the trained simulator outper-
forms other policies on a trained simulator. With
the general goal of obtaining a dialogue manager
that performs better in practice, these results are
encouraging, but need to be confirmed by an eval-
uation of the policies on real users.
Additionally, there is still room for improving
the quality of the simulator itself. For example,
the variation in user behaviour can be improved by
adding more random decision points, in order to
achieve better corpus coverage. In addition, since
there is no clear consensus on what is the best met-
ric for evaluating user simulations, additional met-
rics will be explored in order to get a more bal-
anced indication of the quality of the user simu-
lator and how the various metrics are affected by
modifications to the simulator. Perplexity (related
to the log likelihood, see (Georgila et al, 2005)),
accuracy (related to precision/recall, see (Zuker-
man and Albrecht, 2001; Georgila et al, 2006)),
and Crame?r-von Mises divergence (comparing di-
alogue score distributions, see (Williams, 2008))
are some of the metrics worth considering.
References
H. Cuaya?huitl, S. Renals, O. Lemon, and H. Shi-
modaira. 2005. Human-computer dialogue sim-
ulation using hidden markov models. In Proc.
ASRU?05, pages 290?295.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning user simulations for information state up-
date dialogue systems. In Proc. Interspeech ?05.
K. Georgila, J. Henderson, and O. Lemon. 2006. User
simulation for spoken dialogue systems: Learning
and evaluation. In Proc. Interspeech/ICSLP.
S. Keizer, M. Gas?ic?, F. Mairesse, B. Thomson, K. Yu,
and S. Young. 2008. Modelling user behaviour in
the HIS-POMDP dialogue manager. In Proc. SLT,
Goa, India.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialogue strategies. IEEE Transactions on
Speech and Audio Processing, 8(1).
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. Young. 2007a. Agenda-based user simula-
tion for bootstrapping a POMDP dialogue system.
In Proceedings HLT/NAACL, Rochester, NY.
J. Schatzmann, B. Thomson, and S. Young. 2007b.
Statistical user simulation with a hidden agenda. In
Proc. SIGDIAL?07, pages 273?282, Antwerp, Bel-
gium.
K. Scheffler and S. Young. 2001. Corpus-based dia-
logue simulation for automatic strategy learning and
evaluation. In Proceedings NAACL Workshop on
Adaptation in Dialogue.
S. Singh, M. Kearns, D. Litman, and M. Walker. 2000.
Reinforcement learning for spoken dialogue sys-
tems. In S. Solla, T. Leen, and K. Mu?ller, editors,
Advances in Neural Information Processing Systems
(NIPS). MIT Press.
J. Williams. 2008. Evaluating user simulations with
the Crame?r-von Mises divergence. Speech Commu-
nication, 50:829?846.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, B. Thom-
son, and K. Yu. 2009. The Hidden Information
State model: a practical framework for POMDP
based spoken dialogue management. Computer
Speech and Language, 24(2):150?174.
I. Zukerman and D. Albrecht. 2001. Predictive statis-
tical models for user modeling. User Modeling and
User-Adapted Interaction, 11:5?18.
123
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 201?204,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Gaussian Processes for Fast Policy Optimisation of POMDP-based
Dialogue Managers
M. Gas?ic?, F. Jurc???c?ek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. Young
Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK
{mg436, fj228, sk561, farm2, brmt2, ky219, sjy}@eng.cam.ac.uk
Abstract
Modelling dialogue as a Partially Observ-
able Markov Decision Process (POMDP)
enables a dialogue policy robust to speech
understanding errors to be learnt. How-
ever, a major challenge in POMDP pol-
icy learning is to maintain tractability, so
the use of approximation is inevitable.
We propose applying Gaussian Processes
in Reinforcement learning of optimal
POMDP dialogue policies, in order (1) to
make the learning process faster and (2) to
obtain an estimate of the uncertainty of the
approximation. We first demonstrate the
idea on a simple voice mail dialogue task
and then apply this method to a real-world
tourist information dialogue task.
1 Introduction
One of the main challenges in dialogue manage-
ment is effective handling of speech understand-
ing errors. Instead of hand-crafting the error han-
dler for each dialogue step, statistical approaches
allow the optimal dialogue manager behaviour
to be learnt automatically. Reinforcement learn-
ing (RL), in particular, enables the notion of plan-
ning to be embedded in the dialogue management
criteria. The objective of the dialogue manager is
for each dialogue state to choose such an action
that leads to the highest expected long-term re-
ward, which is defined in this framework by the Q-
function. This is in contrast to Supervised learn-
ing, which estimates a dialogue strategy in such a
way as to make it resemble the behaviour from a
given corpus, but without directly optimising over-
all dialogue success.
Modelling dialogue as a Partially Observable
Markov Decision Process (POMDP) allows action
selection to be based on the differing levels of un-
certainty in each dialogue state as well as the over-
all reward. This approach requires that a distribu-
tion of states (belief state) is maintained at each
turn. This explicit representation of uncertainty in
the POMDP gives it the potential to produce more
robust dialogue policies (Young et al, 2010).
The main challenge in the POMDP approach is
the tractability of the learning process. A dis-
crete state space POMDP can be perceived as a
continuous space MDP where the state space con-
sists of the belief states of the original POMDP.
A grid-based approach to policy optimisation as-
sumes discretisation of this space, allowing for
discrete space MDP algorithms to be used for
learning (Brafman, 1997) and thus approximating
the optimal Q-function. Such an approach takes
the order of 100, 000 dialogues to train a real-
world dialogue manager. Therefore, the training
normally takes place in interaction with a simu-
lated user, rather than real users. This raises ques-
tions regarding the quality of the approximation
as well as the potential discrepancy between sim-
ulated and real user behaviour.
Gaussian Processes have been successfully used
in Reinforcement learning for continuous space
MDPs, for both model-free approaches (Engel et
al., 2005) and model-based approaches (Deisen-
roth et al, 2009). We propose using GP Rein-
forcement learning in a POMDP dialogue man-
ager to, firstly, speed up the learning process and,
secondly, obtain the uncertainty of the approxima-
tion. We opt for the model-free approach since it
has the potential to allow the policy obtained in
interaction with the simulated user to be further
refined in interaction with real users.
In the next section, the core idea of the method is
explained on a toy dialogue problem where differ-
ent aspects of GP learning are examined. Follow-
ing that, in Section 3, it is demonstrated how this
methodology can be effectively applied to a real
world dialogue. We conclude with Section 4.
2 Gaussian Process RL on a Toy Problem
2.1 Gaussian Process RL
A Gaussian Process is a generative model of
Bayesian inference that can be used for function
regression (Rasmussen and Williams, 2005). A
Gaussian Process is fully defined by a mean and a
kernel function. The kernel function defines prior
function correlations, which is crucial for obtain-
ing good posterior estimates with just a few ob-
servations. GP-Sarsa is an on-line reinforcement
learning algorithm for both continuous and dis-
crete MDPs that incorporates GP regression (En-
201
gel et al, 2005). Given the observation of rewards,
it estimates the Q-function utilising its correlations
in different parts of the state and the action space
defined by the kernel function. It also gives a vari-
ance of the estimate, thus modelling the uncer-
tainty of the approximation.
2.2 Voice Mail Dialogue Task
In order to demonstrate how this methodology
can be applied to a dialogue system, we first ex-
plain the idea on the voice mail dialogue prob-
lem (Williams, 2006).
The state space of this task consists of three states:
the user asked for the message either to be saved
or deleted, or the dialogue ended. The system
can take three actions: ask the user what to do,
save or delete the message. The observation of
what the user wants is corrupted with noise, there-
fore we model this as a three-state POMDP. This
POMDP can be viewed as a continuous MDP,
where the MDP state is the POMDP belief state,
a 3-dimensional vector of probabilities. For both
learning and evaluation, a simulated user is used
which makes an error with probability 0.3 and ter-
minates the dialogue after at most 10 turns. In the
final state, it gives a positive reward of 10 or a
penalty of ?100 depending on whether the system
performed a correct action or not. Each interme-
diate state receives the penalty of ?1. In order to
keep the problem simple, a model defining tran-
sition and observation probabilities is assumed so
that the belief can be easily updated, but the policy
optimisation is performed in an on-line fashion.
2.3 Kernel Choice for GP-Sarsa
The choice of kernel function is very important
since it defines the prior knowledge about the Q-
function correlations. They have to be defined on
both states and actions. In the voice mail dialogue
problem the action space is discrete, so we opt for
a simple ? kernel over actions:
k(a, a?) = 1 ? ?a(a?), (1)
where ?a is the Kronecker delta function. The
state space is a 3-dimensional continuous space
and the kernel functions over the state space that
we explore are given in Table 1. Each kernel func-
kernel function expression
polynomial k(x,x?) = ?x,x??
parametrised poly. k(x,x?) =
PD
i=1
xix
?
i
r2i
Gaussian k(x,x?) = p2 exp ? ?x ? x
??2
2?2k
scaled norm k(x,x?) = 1 ? ?x ? x
??2
?x?2?x??2
Table 1: Kernel functions
tion defines a different correlation. The polyno-
mial kernel views elements of the state vector as
features, the dot-product of which defines the cor-
relation. They can be given different relevance ri
in the parametrised version. The Gaussian ker-
nel accounts for smoothness, i.e., if two states are
close to each other the Q-function in these states
is correlated. The scaled norm kernel defines posi-
tive correlations in the points that are close to each
other and a negative correlation otherwise. This
is particularly useful for the voice mail problem,
where, if two belief states are very different, tak-
ing the same action in these states generates a neg-
atively correlated reward.
2.4 Optimisation of Kernel Parameters
Some kernel functions are in a parametrised
form, such as Gaussian or parametrised polyno-
mial kernel. These parameters, also called the
hyper-parameters, are estimated by maximising
the marginal likelihood1 on a given corpus (Ras-
mussen and Williams, 2005). We adapted the
available code (Rasmussen and Williams, 2005)
for the Reinforcement learning framework to ob-
tain the optimal hyper-parameters using a dialogue
corpus labelled with states, actions and rewards.
2.5 Grid-based RL Algorithms
To assess the performance of GP-Sarsa, it was
compared with a standard grid-based algorithm
used in (Young et al, 2010). The grid-based ap-
proach discretises the continuous space into re-
gions with their representative points. This then
allows discrete MDP algorithms to be used for pol-
icy optimisation, in this case the Monte Carlo Con-
trol (MCC) algorithm (Sutton and Barto, 1998).
2.6 Optimal POMDP Policy
The optimal POMDP policy was obtained us-
ing the POMDP solver toolkit (Cassandra, 2005),
which implements the Point Based Value Itera-
tion algorithm to solve the POMDP off-line using
the underlying transition and observation proba-
bilities. We used 300 sample dialogues between
the dialogue manager governed by this policy and
the simulated user as data for optimisation of the
kernel hyper-parameters (see Section 2.4).
2.7 Training set-up and Evaluation
The dialogue manager was trained in interaction
with the simulated user and the performance was
compared between the grid-based MCC algorithm
and GP-Sarsa across different kernel functions
from Table 1.
The intention was, not only to test which algo-
rithm yields the best policy performance, but also
to examine the speed of convergence to the opti-
mal policy. All the algorithms use an ?-greedy
approach where the exploration rate ? was fixed
at 0.1. The learning process greatly depends on
1Also called evidence maximisation in the literature.
202
the actions that are taken during exploration. If
early on during the training, the systems discovers
a path that generates high rewards due to a lucky
choice of actions, then the convergence is faster.
To alleviate this, we adopted the following proce-
dure. For every training set-up, exactly the same
training iterations were performed using 1000 dif-
ferent random generator seedings. After every 20
dialogues the resulting 1000 partially optimised
policies were evaluated. Each of them was tested
on 1000 dialogues. The average reward of these
1000 dialogues provides just one point in Fig. 1.
20 60 100 140 180 220 260 300 340 380 420 460 500 540 580 620
?50
?45
?40
?35
?30
?25
?20
?15
?10
?5
0
Training dialogues
Av
er
ag
e 
re
wa
rd
polynomial kernel ? 
? Gaussian kernel with learned hyper?parameters
? scaled norm kernel
polynomial kernel with learned hyper?parameters
?
 
 
Optimal POMDP Policy
GP?Sarsa
Grid?based Monte Carlo Control
Figure 1: Evaluation results on Voice Mail task
The grid-based MCC algorithm used a Euclid-
ian distance to generate the grid by adding every
point that was further than 0.01 from other points
as a representative of a new region. As can be
seen from Fig 1, the grid-Based MCC algorithm
has a relatively slow convergence rate. GP-Sarsa
with the polynomial kernel exhibited a learning
rate similar to MCC in the first 300 training di-
alogues, continuing with a more upward learning
trend. The parametrised polynomial kernel per-
forms slightly better. The Gaussian kernel, how-
ever, achieves a much faster learning rate. The
scaled norm kernel achieved close to optimal per-
formance in 400 dialogues, with a much higher
convergence rate then the other methods.
3 Gaussian Process RL on a Real-world
Task
3.1 HIS Dialogue Manager on CamInfo
Domain
We investigate the use of GP-Sarsa in a real-
world task by extending the Hidden Information
State (HIS) dialogue manager (Young et al, 2010).
The application domain is tourist information for
Cambridge, whereby the user can ask for informa-
tion about a restaurant, hotel, museum or another
tourist attraction in the local area. The database
consists of more than 400 entities each of which
has up to 10 attributes that the user can query.
The HIS dialogue manager is a POMDP-based di-
alogue manager that can tractably maintain belief
states for large domains. The key feature of this
approach is the grouping of possible user goals
into partitions, using relationships between differ-
ent attributes from possible user goals. Partitions
are combined with possible user dialogue actions
from the N-best user input as well as with the di-
alogue history. This combination forms the state
space ? the set of hypotheses, the probability dis-
tribution over which is maintained during the di-
alogue. Since the number of states for any real-
world problem is too large, for tractable policy
learning, both the state and the action space are
mapped into smaller scale summary spaces. Once
an adequate summary action is found in the sum-
mary space, it is mapped back to form an action in
the original master space.
3.2 Kernel Choice for GP-Sarsa
The summary state in the HIS system is a four-
dimensional space consisting of two elements that
are continuous (the probability of the top two hy-
potheses) and two discrete elements (one relating
the portion of the database entries that matches the
top partition and the other relating to the last user
action type). The summary action space is discrete
and consists of eleven elements.
In order to apply the GP-Sarsa algorithm, a kernel
function needs to be specified for both the sum-
mary state space and the summary action space.
The nature of this space is quite different from the
one described in the toy problem. Therefore, ap-
plying a kernel that has negative correlations, such
as the scaled norm kernel (Table 1) might give un-
expected results. More specifically, for a given
summary action, the mapping procedure finds the
most appropriate action to perform if such an ac-
tion exists. This can lead to a lower reward if
the summary action is not adequate but would
rarely lead to negatively correlated rewards. Also,
parametrised kernels could not be used for this
task, since there was no corpus available for hyper-
parameter optimisation. The polynomial kernel
(Table 1) assumes that the elements of the space
are features. Due to the way the probability is
maintained over this very large state space, the
continuous variables potentially encode more in-
formation than in the simple toy problem. There-
fore, we used the polynomial kernel for the con-
tinuous elements. For discrete elements, we utilise
the ?-kernel (Eq. 2.3).
3.3 Active Learning GP-Sarsa
The GP RL framework enables modelling the un-
certainty of the approximation. The uncertainty
estimate can be used to decide which actions
to take during the exploration (Deisenroth et al,
203
2009). In detail, instead of a random action, the
action in which the Q-function for the current state
has the highest variance is taken.
3.4 Training Set-up and Evaluation
Policy optimisation is performed by interacting
with a simulated user on the dialogue act level.
The simulated user gives a reward at the final state
of the dialogue, and that is 20 if the dialogue was
successful, 0 otherwise, less the number of turns
taken to fulfil the user goal. The simulated user
takes a maximum of 100 turns in each dialogue,
terminating it when all the necessary information
has been obtained or if it looses patience.
A grid-based MCC algorithm provides the base-
line method. The distance metric used ensures
that the number of regions in the grid is small
enough for the learning to be tractable (Young et
al., 2010).
In order to measure how fast each algorithm
learns, a similar training set-up to the one pre-
sented in Section 2.7 was adopted and the aver-
aged results are plotted on the graph, Fig. 2.
200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000
2
3
4
5
6
7
8
9
Training dialogues
Av
er
ag
e 
re
wa
rd
?  Grid?based Monte Carlo Control
?  GP?Sarsa with polynomial kernel
?  Active learning GP?Sarsa with polynomial kernel
Figure 2: Evaluation results on CamInfo task
The results show that in the very early stage of
learning, i.e., during the first 400 dialogues, the
GP-based method learns faster. Also, the learning
process can be accelerated by adopting the active
learning framework where the actions are selected
based on the estimated uncertainty.
After performing many iterations in an incremen-
tal noise learning set-up (Young et al, 2010) both
the GP-Sarsa and the grid-based MCC algorithms
converge to the same performance.
4 Conclusions
This paper has described how Gaussian Processes
in Reinforcement learning can be successfully ap-
plied to dialogue management. We implemented
a GP-Sarsa algorithm on a toy dialogue prob-
lem, showing that with an appropriate kernel func-
tion faster convergence can be achieved. We also
demonstrated how kernel parameters can be learnt
from a dialogue corpus, thus creating a bridge
between Supervised and Reinforcement learning
methods in dialogue management. We applied
GP-Sarsa to a real-world dialogue task showing
that, on average, this method can learn faster than
a grid-based algorithm. We also showed that the
variance that GP is estimating can be used in an
Active learning setting to further accelerate policy
optimisation.
Further research is needed in the area of kernel
function selection. The results here suggest that
the GP framework can facilitate faster learning,
which potentially allows the use of larger sum-
mary spaces. In addition, being able to learn ef-
ficiently from a small number of dialogues offers
the potential for learning from direct interaction
with real users.
Acknowledgements
The authors would like to thank Carl Rasmussen
for valuable discussions. This research was partly
funded by the UK EPSRC under grant agreement
EP/F013930/1 and by the EU FP7 Programme un-
der grant agreement 216594 (CLASSiC project).
References
RI Brafman. 1997. A Heuristic Variable Grid Solution
Method for POMDPs. In AAAI, Cambridge, MA.
AR Cassandra. 2005. POMDP solver.
http://www.cassandra.org/pomdp/
code/index.shtml.
MP Deisenroth, CE Rasmussen, and J Peters. 2009.
Gaussian Process Dynamic Programming. Neuro-
comput., 72(7-9):1508?1524.
Y Engel, S Mannor, and R Meir. 2005. Reinforcement
learning with Gaussian processes. In ICML ?05:
Proceedings of the 22nd international conference on
Machine learning, pages 201?208, New York, NY.
CE Rasmussen and CKI Williams. 2005. Gaussian
Processes for Machine Learning. MIT Press, Cam-
bridge, MA.
RS Sutton and AG Barto. 1998. Reinforcement Learn-
ing: An Introduction. Adaptive Computation and
Machine Learning. MIT Press, Cambridge, MA.
JD Williams. 2006. Partially Observable Markov De-
cision Processes for Spoken Dialogue Management.
Ph.D. thesis, University of Cambridge.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, and K Yu. 2010. The Hid-
den Information State Model: a practical frame-
work for POMDP-based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150?
174.
204
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 2?7,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Spoken Dialog Challenge 2010: 
 Comparison of Live and Control Test Results 
Alan W Black1, Susanne Burger1, Alistair Conkie4, Helen Hastie2, Simon Keizer3,  Oliver 
Lemon2, Nicolas Merigaud2, Gabriel Parent1, Gabriel Schubiner1, Blaise Thomson3, Jason 
D. Williams4, Kai Yu3, Steve Young3 and Maxine Eskenazi1 
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA 
2Dept of Mathematical and Computer Science, Heriot-Watt University, Edinburgh, UK 
3Engineering Department, Cambridge University, Cambridge, UK 
4AT&T Labs ? Research, Florham Park, NJ, USA 
awb@cs.cmu.edu 
 
 
Abstract 
The Spoken Dialog Challenge 2010 was an 
exercise to investigate how different spo-
ken dialog systems perform on the same 
task.  The existing Let?s Go Pittsburgh Bus 
Information System was used as a task and 
four teams provided systems that were first 
tested in controlled conditions with speech 
researchers as users. The three most stable 
systems were then deployed to real callers.  
This paper presents the results of the live 
tests, and compares them with the control 
test results. Results show considerable 
variation both between systems and be-
tween the control and live tests.  Interest-
ingly, relatively high task completion for 
controlled tests did not always predict 
relatively high task completion for live 
tests.  Moreover, even though the systems 
were quite different in their designs, we 
saw very similar correlations between word 
error rate and task completion for all the 
systems.  The dialog data collected is 
available to the research community. 
1 Background 
The goal of the Spoken Dialog Challenge (SDC) is 
to investigate how different dialog systems per-
form on a similar task.  It is designed as a regularly 
recurring challenge. The first one took place in 
2010. SDC participants were to provide one or 
more of three things: a system; a simulated user, 
and/or an evaluation metric.   The task chosen for 
the first SDC was one that already had a large 
number of real callers. This had several advan-
tages. First, there was a system that had been used 
by many callers. Second, there was a substantial 
dataset that participants could use to train their sys-
tems.  Finally, there were real callers, rather than 
only lab testers.  Past work has found systems 
which appear to perform well in lab tests do not 
always perform well when deployed to real callers, 
in part because real callers behave differently than 
lab testers, and usage conditions can be considera-
bly different [Raux et al2005, Ai et al2008].  De-
ploying systems to real users is an important trait 
of the Spoken Dialog Challenge. 
The CMU Let?s Go Bus Information system 
[Raux et al2006] provides bus schedule informa-
tion for the general population of Pittsburgh.  It is 
directly connected to the local Port Authority, 
whose evening calls for bus information are redi-
rected to the automated system.  The system has 
been running since March 2005 and has served 
over 130K calls. 
The software and the previous years of dialog 
data were released to participants of the challenge 
to allow them to construct their own systems.  A 
number of sites started the challenge, and four sites 
successfully built systems, including the original 
CMU system. 
An important aspect of the challenge is that 
the quality of service to the end users (people in 
Pittsburgh) had to be maintained and thus an initial 
robustness and quality test was carried out on con-
tributed systems.  This control test provided sce-
narios over a web interface and required 
researchers from the participating sites to call each 
of the systems.  The results of this control test were 
published in [Black et al 2010] and by the individ-
ual participants [Williams et al 2010, Thomson et 
al. 2010, Hastie et al 2010] and they are repro-
2
duced below to give the reader a comparison with 
the later live tests. 
Important distinctions between the control 
test callers and the live test callers were that the 
control test callers were primarily spoken dialog 
researchers from around the world.  Although they 
were usually calling from more controlled acoustic 
conditions, most were not knowledgeable about 
Pittsburgh geography.     
As mentioned above, four systems took part 
in the SDC.  Following the practice of other chal-
lenges, we will not explicitly identify the sites 
where these systems were developed. We simply 
refer to them as SYS1-4 in the results.  We will, 
however, state that one of the systems is the system 
that has been running for this task for several 
years. The architectures of the systems cover a 
number of different techniques for building spoken 
dialog systems, including agenda based systems, 
VoiceXML and statistical techniques. 
2 Conditions of Control and Live tests 
For this task, the caller needs to provide the depar-
ture stop, the arrival stop and the time of departure 
or arrival in order for the system to be able to per-
form a lookup in the schedule database. The route 
number can also be provided and used in the 
lookup, but it is not necessary. The present live 
system covers the East End of Pittsburgh.  Al-
though the Port Authority message states that other 
areas are not covered, callers may still ask for 
routes that are not in the East End; in this case, the 
live system must say it doesn?t have information 
available.  Some events that affect the length of the 
dialog include whether the system uses implicit or 
explicit confirmation or some combination of both, 
whether the system has an open-ended first turn or 
a directed one, and whether it deals with requests 
for the previous and/or following bus (this latter 
should have been present in all of the systems). 
Just before the SDC started, the Port Author-
ity had removed some of its bus routes. The sys-
tems were required to be capable of informing the 
caller that the route had been canceled, and then 
giving them a suitable alternative. 
SDC systems answer live calls when the Port 
Authority call center is closed in the evening and 
early morning.  There are quite different types and 
volumes of calls over the different days of the 
week.  Weekend days typically have more calls, in 
part because the call center is open fewer hours on 
weekends.  Figure 1 shows a histogram of average 
calls per hour for the evening and the early morn-
ing of each day of the week. 
 
calls per weekday / ave per hour
0
1
2
3
4
5
6
7
8
9
10
Fr-
19-
0
Sa-
0-8
Sa-
16
-
0
Su-
0-8
Su-
16
-
0
Mo
-
0-7
Mo
-
19
-
0
Tu
-
0-7
Tu
-
19-
0
We
-
0-7
We
-
19
-
0
Th
-
0-7
Th
-
19-
0
Fr-
0-7
 
Figure 1: average number of calls per hour on weekends 
(dark bars) and weekdays. Listed are names of days and 
times before and after midnight when callers called the 
system. 
 
The control tests were set up through a simple 
web interface that presented 8 different scenarios 
to callers. Callers were given a phone number to 
call; each caller spoke to each of the 4 different 
systems twice.  A typical scenario was presented 
with few words, mainly relying on graphics in or-
der to avoid influencing the caller?s choice of vo-
cabulary.  An example is shown in Figure 2. 
 
 
 
Figure 2: Typical scenario for the control tests.  This 
example requests that the user find a bus from the cor-
ner of Forbes and Morewood (near CMU) to the airport, 
using bus route 28X, arriving by 10:45 AM. 
 
3
3 Control Test Results 
The logs from the four systems were labeled for 
task success by hand.  A call is successful if any of 
the following outputs are correctly issued: 
 
? Bus schedule for the requested departure and 
arrival stops for the stated bus number (if giv-
en). 
? A statement that there is no bus available for 
that route. 
? A statement that there is no scheduled bus at 
that time. 
 
We additionally allowed the following boundary 
cases: 
 
? A departure/arrival stop within 15 minutes 
walk. 
? Departure/arrival times within one hour of re-
quested time. 
? An alternate bus number that serves the re-
quested route. 
 
In the control tests, SYS2 had system connection 
issues that caused a number of calls to fail to con-
nect, as well as a poorer task completion.  It was 
not included in the live tests.  It should be pointed 
out that SYS2 was developed by a single graduate 
student as a class project while the other systems 
were developed by teams of researchers.  The re-
sults of the Control Tests are shown in Table 1 and 
are discussed further below. 
 
Table 1. Results of hand analysis of the four systems in 
the control test 
 The three major classes of system response 
are as follows.  no_info: this occurs when the sys-
tem gives neither a specific time nor a valid excuse 
(bus not covered, or none at that time).  no_info 
calls can be treated as errors (even though there 
maybe be valid reasons such as the caller hangs up 
because the bus they are waiting for arrives).  
donthave: identifies calls that state the requested 
bus is not covered by the system or that there is no 
bus at the requested time. pos_out: identifies calls 
where a specific time schedule is given.  Both 
donthave and pos_out calls may be correct or er-
roneous (e.g the given information is not for the 
requested bus,  the departure stop is wrong, etc). 
4 Live Tests Results 
In the live tests the actual Pittsburgh callers had 
access to three systems: SYS1, SYS3, and SYS4.  
Although engineering issues may not always be 
seen to be as relevant as scientific results, it is im-
portant to acknowledge several issues that had to 
be overcome in order to run the live tests. 
Since the Pittsburgh Bus Information System 
is a real system, it is regularly updated with new 
schedules from the Port Authority. This happens 
about every three months and sometimes includes 
changes in bus routes as well as times and stops. 
The SDC participants were given these updates 
and were allowed the time to make the changes to 
their systems. Making things more difficult is the 
fact that the Port Authority often only releases the 
schedules a few days ahead of the change. Another 
concern was that the live tests be run within one 
schedule period so that the change in schedule 
would not affect the results.   
The second engineering issue concerned 
telephony connectivity. There had to be a way to 
transfer calls from the Port Authority to the par-
ticipating systems (that were run at the participat-
ing sites, not at CMU) without slowing down or 
perturbing service to the callers.  This was 
achieved by an elaborate set of call-forwarding 
mechanisms that performed very reliably.  How-
ever, since one system was in Europe, connections 
to it were sometimes not as reliable as to the US-
based systems.  
 
 SYS1 SYS3 SYS4 
Total Calls 678 451 742 
Non-empty calls 633 430 670 
no_ info 18.5% 14.0% 11.0% 
donthave 26.4% 30.0% 17.6% 
donthave_corr 47.3% 40.3% 37.3% 
donthave_incorr 52.7% 59.7% 62.7% 
pos_out 55.1% 56.0% 71.3% 
pos_out_corr 86.8% 93.8% 91.6% 
pos_out_incorr 13.2% 6.2% 8.4% 
 
Table 2. Results of hand analysis of the three systems in 
the live tests.  Row labels are the same as in Table 1. 
 SYS1 SYS2 SYS3 SYS4 
Total Calls 91 61 75 83 
no_ info 3.3% 37.7% 1.3% 9.6% 
donthave 17.6% 24.6% 14.7% 9.6% 
donthave_corr 68.8% 33.3% 100.0% 100.0% 
donthave_incorr 31.3% 66.7% 0.0% 0.0% 
pos_out 79.1% 37.7% 84.0% 80.7% 
pos_out_corr 66.7% 78.3% 88.9% 80.6% 
pos_out_incorr 33.3% 21.7% 11.1% 19.4% 
4
We ran each of the three systems for multiple two 
day periods over July and August 2010.  This de-
sign gave each system an equal distribution of 
weekdays and weekends, and also ensured that 
repeat-callers within the same day experienced the 
same system. 
One of the participating systems (SYS4) 
could support simultaneous calls, but the other two 
could not and the caller would receive a busy sig-
nal if the system was already in use.  This, how-
ever, did not happen very often. 
Results of hand analysis of real calls are 
shown in Table 4 alongside the results for the Con-
trol Test for easy comparison.  In the live tests we 
had an additional category of call types ? empty 
calls (0-turn calls) ? which are calls where there 
are no user turns, for example because the caller 
hung up or was disconnected before saying any-
thing.  Each system had 14 days of calls and exter-
nal daily factors may change the number of calls. 
We do suspect that telephony issues may have pre-
vented some calls from getting through to SYS3 on 
some occasions.   
Table 3 provides call duration information for 
each of the systems in both the control and live 
tests. 
 
 
 Length (s) Turns/call Words/turn 
SYS1 control 155 18.29 2.87 (2.84) 
SYS1 live 111 16.24 2.15 (1.03) 
SYS2 control 147 17.57 1.63 (1.62) 
SYS3 control 96 10.28 2.73 (1.94) 
SYS3 live 80 9.56 2.22 (1.14) 
SYS4 control 154 14.70 2.25 (1.78) 
SYS4 live 126 11.00 1.63 (0.77) 
 
Table 3: For live tests, average length of each call, aver-
age number of turns per call, and average number of 
words per turn (numbers in brackets are standard devia-
tions). 
 
Each of the systems used a different speech 
recognizer.  In order to understand the impact of 
word error rate on the results, all the data were 
hand transcribed to provide orthographic transcrip-
tions of each user turn.   Summary word error sta-
tistics are shown in Table 4.   However, summary 
statistics do not show the correlation between word 
error rate and dialogue success.  To achieve this, 
following Thomson et al(2010), we computed a 
logistic regression of success against word error 
rate (WER) for each of the systems. Figure 3 
shows the regressions for the Control Tests and 
Figure 4 for the Live Tests.  
 
 SYS1 SYS3 SYS4 
Control 38.4 27.9 27.5 
Live 43.8 42.5 35.7 
 
Table 4: Average dialogue word error rate (WER). 
 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
es
s 
Ra
te
Sys4
Sys3
Sys1
 
Figure 3: Logistic regression of control test success vs 
WER for the three fully tested systems 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
e
ss
Sys1
Sys3
Sys4
 
Figure 4: Logistic regression of live success vs WER for 
the three fully tested systems 
 
5
In order to compare the control and live tests, 
we can calculate task completion as the percentage 
of calls that gave a correct result.  We include only 
non-empty calls (excluding 0-turn calls), and treat 
all no_info calls as being incorrect, even though 
some may be due to extraneous reasons such as the 
bus turning up (Table 5). 
 
 SYS1 SYS3 SYS4 
Control 64.9% (5.0%) 89.4% (3.6%) 74.6% (4.8%) 
Live 60.3% (1.9%) 64.6% (2.3%) 71.9% (1.7%) 
 
Table 5: Live and control test task completion (std. err).  
 
5 Discussion 
All systems had lower WER and higher task com-
pletion in the controlled test vs. the live test.  This 
agrees with past work [Raux et al2005, Ai et al
2008], and underscores the challenges of deploying 
real-world systems. 
For all systems, dialogs with controlled sub-
jects were longer than with live callers ? both in 
terms of length and number of turns.  In addition, 
for all systems, live callers used shorter utterances 
than controlled subjects.  Controlled subjects may 
be more patient than live callers, or perhaps live 
callers were more likely to abandon calls in the 
face of higher recognition error rates.   
Some interesting differences between the sys-
tems are evident in the live tests.  Looking at dia-
log durations, SYS3 used confirmations least often, 
and yielded the fastest dialogs (80s/call).  SYS1 
made extensive use of confirmations, yielding the 
most turns of any system and slightly longer dia-
logs (111s/call).  SYS4 was the most system-
directed, always collecting information one ele-
ment at a time.  As a result it was the slowest of the 
systems (126s/call), but because it often used im-
plicit confirmation instead of explicit confirmation, 
it had fewer turns/call than SYS1.   
For task completion, SYS3 performed best in 
the controlled trials, with SYS1 worst and SYS4 in 
between.  However in the live test, SYS4 per-
formed best, with SYS3 and SYS1 similar and 
worse.  It was surprising that task completion for 
SYS3 was the highest for the controlled tests yet 
among the lowest for the live tests.  Investigating 
this, we found that much of the variability in task 
completion for the live tests appears to be due to 
WER.  In the control tests SYS3 and SYS4 had 
similar error rates but the success rate of SYS3 was 
higher.  The regression in Figure 3 shows this 
clearly.   In the live tests SYS3 had a significantly 
higher word error rate and average success rate 
was much lower than in SYS4.   
It is interesting to speculate on why the rec-
ognition rates for SYS3 and SYS4 were different 
in the live tests, but were comparable in the control 
tests.  In a spoken dialogue system the architecture 
has a considerable impact on the measured word 
error rate.  Not only will the language model and 
use of dialogue context be different, but the dia-
logue design and form of system prompts will in-
fluence the form and content of user inputs.   Thus, 
word error rates do not just depend on the quality 
of the acoustic models ? they depend on the whole 
system design.  As noted above, SYS4 was more 
system-directed than SYS3 and this probably con-
tributed to the comparatively better ASR perform-
ance with live users.   In the control tests, the 
behavior of users (research lab workers) may have 
been less dependent on the manner in which users 
were prompted for information by the system.  
Overall, of course, it is user satisfaction and task 
success which matter. 
6 Corpus Availability and Evaluation 
The SDC2010 database of all logs from all systems 
including audio plus hand transcribed utterances, 
and hand defined success values is released 
through CMU?s Dialog Research Center 
(http://dialrc.org). 
One of the core goals of the Spoken Dialog 
Challenge is to not only create an opportunity for 
researchers to test their systems on a common plat-
form with real users, but also create common data 
sets for testing evaluation metrics.  Although some 
work has been done on this for the control test data 
(e.g. [Zhu et al2010]), we expect further evalua-
tion techniques will be applied to these data. 
One particular issue which arose during this 
evaluation concerned the difficulty of defining pre-
cisely what constitutes task success.  A precise de-
finition is important to developers, especially if 
reinforcement style learning is being used to opti-
mize the success.  In an information seeking task 
of the type described here, task success is straight-
forward when the user?s requirements can be satis-
fied but more difficult if some form of constraint 
relaxation is required.   For example, if the user 
6
asks if there is a bus from the current location to 
the airport ? the answer ?No.? may be strictly cor-
rect but not necessarily helpful.  Should this dia-
logue be scored as successful or not?  The answer 
?No, but there is a stop two blocks away where 
you can take the number 28X bus direct to the air-
port.? is clearly more useful to the user.  Should 
success therefore be a numeric measure rather than 
a binary decision?  And if a measure, how can it be 
precisely defined?  A second and related issue is 
the need for evaluation algorithms which deter-
mine task success automatically.   Without these, 
system optimization will remain an art rather than 
a science. 
7 Conclusions 
This paper has described the first attempt at an ex-
ercise to investigate how different spoken dialog 
systems perform on the same task.  The existing 
Let?s Go Pittsburgh Bus Information System was 
used as a task and four teams provided systems 
that were first tested in controlled conditions with 
speech researchers as users. The three most stable 
systems were then deployed ?live? with real call-
ers. Results show considerable variation both be-
tween systems and between the control and live 
tests.  Interestingly, relatively high task completion 
for controlled tests did not always predict rela-
tively high task completion for live tests.  This 
confirms the importance of testing on live callers, 
not just usability subjects. 
 The general organization and framework 
of the evaluation worked well.  The ability to route 
audio telephone calls to anywhere in the world us-
ing voice over IP protocols was critical to the suc-
cess of the challenge since it provides a way for 
individual research labs to test their in-house sys-
tems without the need to port them to a central co-
ordinating site. 
 Finally, the critical role of precise evalua-
tion metrics was noted and the need for automatic 
tools to compute them.  Developers need these at 
an early stage in the cycle to ensure that when sys-
tems are subsequently evaluated, the results and 
system behaviors can be properly compared.  
Acknowledgments 
Thanks to AT&T Research for providing telephony 
support for transporting telephone calls during the 
live tests.  This work was in part supported by the 
US National Science foundation under the project 
?Dialogue Research Center?.   
References  
Ai, H., Raux, A., Bohus, D., Eskenzai, M., and Litman, 
D.  (2008)  ?Comparing spoken dialog corpora col-
lected with recruited subjects versus real users?, Proc 
SIGDial, Columbus, Ohio, USA.  
Black, A., Burger, S., Langner, B., Parent, G., and Es-
kenazi, M. (2010) ?Spoken Dialog Challenge 2010?, 
SLT 2010, Berkeley, CA.  
Hastie, H., Merigaud, N., Liu, X and Oliver Lemon. 
(2010) ? ?Let?s Go Dude?, Using The Spoken Dia-
logue Challenge to Teach Spoken Dialogue Devel-
opment?, SLT 2010, Berkeley, CA. 
Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi, 
M.  (2005)  ?Let?s go public! Taking a spoken dialog 
system to the real world?, Interspeech 2005, Lisbon, 
Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. (2006) ?Doing Research on a Deployed 
Spoken Dialogue System: One Year of Let's Go! Ex-
perience?, Interspeech 2006 - ICSLP, Pittsburgh, PA.  
Thomson B., Yu, K. Keizer, S., Gasic, M., Jurcicek, F.,  
Mairesse, F. and Young, S. ?Bayesian Dialogue Sys-
tem for the Let?s Go Spoken Dialogue Challenge?, 
SLT 2010, Berkeley, CA. 
Williams, J., Arizmendi, I., and Conkie, A. ?Demonstra-
tion of AT&T ?Let?s Go?: A Production-Grade Statis-
tical Spoken Dialog System.? SLT 2010, Berkeley, 
CA. 
Zhu, Y., Yang, Z., Meng, H., Li, B., Levow, G., and 
King, I. (2010) ?Using Finite State Machines for 
Evaluating Spoken Dialog Systems?, SLT 2010, 
Berkeley, CA. 
7
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 74?78,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
The Effect of Cognitive Load on a Statistical Dialogue System
M. Gas?ic??, P. Tsiakoulis?, M. Henderson?, B. Thomson?, K. Yu?, E. Tzirkel?? and S. Young?
?Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK
{mg436, pt344, mh521, brmt2, ky219, sjy}@eng.cam.ac.uk
??General Motors Advanced Technical Centre, Israel
eli.tzirkel@gm.com
Abstract
In recent years statistical dialogue systems
have gained significant attention due to their
potential to be more robust to speech recogni-
tion errors. However, these systems must also
be robust to changes in user behaviour caused
by cognitive loading. In this paper, a statistical
dialogue system providing restaurant informa-
tion is evaluated in a set-up where the sub-
jects used a driving simulator whilst talking to
the system. The influences of cognitive load-
ing were investigated and some clear differ-
ences in behaviour were discovered. In partic-
ular, it was found that users chose to respond
to different system questions and use different
speaking styles, which indicate the need for an
incremental dialogue approach.
1 Introduction
A spoken dialogue system enables a user to obtain
information while using their hands to perform some
other task, which in many cases is the user?s primary
task. A typical example is an in-car spoken dialogue
system where the spoken interaction is secondary to
the main task of driving the car (Weng et al, 2004).
This domain is particularly challenging since it in-
volves dealing with the errors caused by the varying
noise levels and changes in user behaviour caused
by the cognitive load.
A statistical approach to dialogue modelling has
been proposed as a means of automatically optimis-
ing dialogue policies. In particular, the partially ob-
servable Markov decision process (POMDP) model
for dialogue provides a representation of varying
levels of uncertainty of the user input, yielding more
robust dialogue policies (Williams and Young, 2007;
Thomson and Young, 2010; Young et al, 2010).
Another thread of research deals with speech
interfaces for in-car applications, see (Baron and
Green, 2006) for a review. Past research has inves-
tigated the extent to which speaking is cognitively
less demanding than typing (Gartner et al, 2001;
Tsimhoni et al, 2004; Kun et al, 2007). In addi-
tion, considerable research has examined how driv-
ing safety is influenced by a dialogue system (Lai
et al, 2001; Lee et al, 2001; Nielsen et al, 2008).
However, to the best of our knowledge, little work
has been done to investigate the effect of the cog-
nitive load when interacting with a real conversa-
tional spoken dialogue system. The work presented
in (Mishra et al, 2004) suggests that the user speech
is more disfluent when the user is performing an-
other task. However, this work is based on a Wiz-
ard of Oz framework, where a human provides the
system?s responses. Also, a push-to-talk button was
used for every utterance which will have affected the
natural flow of the dialogue. It is important to know
if the change of cognitive load has an effect on the
speaking style and whether the system can alter its
behaviour to accommodate for this.
In this paper we try to answer these questions by
examining dialogues where users drove a car simu-
lator and talked to an open-microphone fully auto-
mated spoken dialogue system at the same time.
The rest of the paper is organised as follows. Sec-
tion 2 provides an overview of the dialogue system
used and section 3 describes the evaluation set-up.
The analysis of the results is given in Section 4. Sec-
tion 5 concludes the paper.
74
Table 1: Example dialogue task
You are looking for a cheap restaurant and it
should be in the east part of town. Make sure you
get the address of the venue.
2 System overview
The user speaks to the system, and the acoustic sig-
nal is converted by the speech recogniser into a set
of sentence hypotheses, which represents a proba-
bility distribution over all possible things that the
user might have said. The sentence hypotheses are
converted into an N-best list of dialogue acts by a
semantic decoder. Since the dialogue state cannot
be directly observed it maintains a probability dis-
tribution over all states, which is called the belief
state. The belief state is updated at every user turn
using Bayesian inference treating the input dialogue
acts as evidence. Based on belief state, the optimal
system act is selected using a policy and which is
trained automatically using reinforcement learning.
The abstract system dialogue act is converted to an
appropriate utterance by a natural language genera-
tor and then converted to speech by an HMM-based
speech synthesiser. To enable in-car speech inter-
action via mobile phone, a VoIP interface is imple-
mented. The domain is Cambridge restaurant infor-
mation with a database of about 150 venues and 7
slots that users can query.
3 Evaluation set-up
Our goal is to understand system performance
when driving. However, due to the safety restric-
tions, performance was tested using a driving simu-
lator. The following sections explain the set-up.
3.1 Car simulator
The car simulator used in the evaluation was the
same as in (Davies and Robinson, 2011). It con-
sists of a seat, a steering wheel and pedals, which
give a realistic cab-like environment for the par-
ticipants. There is also a projection screen which
largely fills the visual field of the driver. The sim-
ulation software is a modified version of Rockstar
Games? Grand Theft Auto: San Andreas, with over
500 km of roads. For the purpose of the evaluation,
the subjects were asked to drive on the main motor-
way, to keep the lane and not to drive over 70mph.
3.2 Subjects
For the study 28 subjects were recruited, 22 where
native speakers. Each subject had to complete three
scenarios: (1) to drive the car simulator for 10 min-
utes, (2) to talk to the system for 7 dialogues and (3)
to talk to the system for 7 dialogues while driving.
The scenarios were in counter-balanced order.
While they were driving, the speed and the road
position were recorded. If the scenario involved
talking to the system, the instructor read out the di-
alogue task (see an example in Table 1) and dialled
the phone number. In addition, the subject had the
dialogue task displayed on a small screen next to the
driving wheel. The subject talked to the system us-
ing loud speaker mode on the mobile phone.
4 Results
To examine the influence of cognitive load, the
following examinations were performed. First, we
investigate if the subjects felt any change in the cog-
nitive load (Section 4.1). Then, in Section 4.2, we
examine how the driving was influenced by the sub-
jects talking to the system. Finally, we investigate
how successfully the subjects were able to complete
the dialogue tasks while driving (Section 4.3). This
is followed with an examination of the conversa-
tional patterns that occurred when the subjects were
driving whilst talking to the system (Section 4.4).
4.1 Cognitive load
After each scenario the subjects were asked to an-
swer five questions based on the NASA-TLX self-
reporting scheme for workload measurement. They
answered by providing a rating from 1 (very easy)
to 5 (very hard). The averaged results are given
in Table 2. We performed a Kruskal test, followed
by pairwise comparisons for every scenario for each
answer and all differences are statistically signifi-
cant (p < 0.03) apart from the differences in the
frustration, the stress and the pace between talking
and talking and driving. This means that they were
clearly able to feel the change in cognitive load.
75
Table 2: Subjective evaluation of the cognitive load
Driving Talking Talking&Driving
How mentally demanding was the scenario?
1.61 2.21 2.89
How hurried was the pace of the scenario?
1.21 1.71 1.89
How hard did you have to work?
1.5 2.32 2.96
How frustrated did you feel during the task?
1.29 2.61 2.61
How stressed did you feel during the task?
1.29 2.0 2.32
Table 3: Analysis of driving speed to determine which
measures are larger for Talking&Driving than Driving
Measure Percentage of
users
Confidence in-
terval
Higher speed 8% [1%, 25%]
Larger std.dev 77% [56%, 91%]
Larger entropy 85% [65%, 95%]
4.2 Driving performance
For 26 subjects we recorded position on the road
and the speed. Since these measurements vary sig-
nificantly across the subjects, for each subject we
calculated the average speed, the standard deviation
and the entropy and similarly for the average posi-
tion in the lane. For the speed, we computed how
many subjects had a higher average speed when they
were talking and driving versus when they were just
talking and similarly for the standard deviation and
the entropy. The results are given in Table 3. It
can be seen that the user?s speed is lower when they
are driving and talking, however, the increase in the
standard deviation and the entropy suggest that their
driving is more erratic. No significant differences
were observed for the road position.
4.3 Dialogue task completion
Each participant performed 14 dialogues, 7 for each
scenario. In total, there were 196 dialogues per sce-
nario. After each dialogue they told the instruc-
tor if they thought the dialogue was successful, and
this information was used to compute the subjective
Table 4: Subjective and Objective Task completion (196
Dialogues per scenario)
Talking Talking&Driving
Subjective 78.6% 74.0%
Objective 68.4% 64.8%
Table 5: Percentage of turns that are in line with the pre-
defined task
Talking Talking&Driving
Percentage of turns
that follow the task
98.3% 96.79%
Number of turns 1354 1388
completion rate. In addition, all dialogues were tran-
scribed and analysed to see if the system provided
information the user asked for and hence calculate
an objective completion rate. The results are given
in Table 4. These differences are not statistically sig-
nificant due to the small sample size. However, it
can be seen that the trend is that the dialogues where
the subject was not performing another task at the
same time were more successful. Also, it is inter-
esting that the subjective scores are higher than the
objective ones. This can be explained by the fact that
the dialogue tasks were predefined and the subjects
do not always pay sufficient attention to their task
descriptions.
4.4 Conversational patterns
Given that the subjects felt the change of cognitive
load when they were talking to the system and op-
erating the car simulator at the same time, we were
interested to see if there are any changes in the dia-
logues which might suggest this.
First, we examine how well they follow the given
task on a turn-to-turn basis. For example, if the task
is to find a cheap restaurant and if at some point
in the dialogue the user says I?d like an expensive
restaurant that turn is not consistent with the task.
The results are given in Table 5 and they are statisti-
cally significant (p < 0.01).
We then examine the number of contradictions on
a turn-to-turn basis. For example, if the user says I?d
like a cheap restaurant and later on they say I?d like
76
Table 6: User obedience to system questions
1. system requests or confirms and requests
Samples Obedience
Talking 392 67.6%
Talking&Driving 390 63.9%
2. system confirms
Samples Obedience
Talking 91 73.6%
Talking&Driving 92 81.5%
an expensive restaurant the latter turn is clearly a
contradiction. The percentage of contradicting turns
is less than 1% and the difference between the sce-
narios is not statistically significant. This suggests
that while users tend to forget the task they are given
when they are driving, they still act rationally despite
the increase in the cognitive load.
The next analysis concerns the user obedience,
i.e. the extent to which subjects answer the sys-
tem questions. We grouped the system questions in
two classes. The first class represents the questions
where the system requests a value for a particular
slot, for instance What part of town are you looking
for? and the questions where the system confirms
and requests at the same time, for instance You are
looking for a cheap restaurant. What part of town
are you looking for? The second class correspond to
system confirmations, for example Did you say you
are looking for a cheap restaurant? The percent-
age of the obedient user turns per class is given in
Table 6. Due to the small sample size these results
are not statistically significant. Still, it is interest-
ing to see that when driving the subjects appear to
be more obedient to the system confirmations than
when they are just talking. When the system makes
a confirmation, the user can answer with simple yes
or no, whereas when the system requests the value
of a particular slot, the user needs to think more to
provide an answer.
The number of barge-ins, the number of filler
words and the average speech intensity vary con-
siderably among the subjects. Therefore, we aver-
age these statistics per user and examine the number
of users for which the particular measure is greater
for the scenario where they talked to the system and
drove the simulator at the same time. The results
Table 7: Analysis of measures related to the speaking
style which values are larger for Talking&Driving than
Talking
Measure % of users Conf. interval
More barge-ins 87% [69%, 96%]
More fillers 73% [54%, 88%]
Higher intensity 67% [47%, 83%]
(Table 7) show that the number of barge-ins and the
number of fillers is significantly greater for the sce-
nario when they are talking and driving and the in-
tensity on average tend to be greater.
5 Conclusion and Future work
There are several important observations arising
from this study. Firstly, dialogues with cognitively
loaded users tend to be less successful. This sug-
gests that the system should alter its behaviour to
match user behaviour and alleviate the cognitive
load in order to maintain the level of performance.
This necessitates rapid on-line adaptation of dia-
logue policies.
The second observation is that cognitively loaded
users tend to respond to some types of system ques-
tions more than others. This indicates that the user
model within a POMDP dialogue system should be
conditioned on a measure of cognitive load.
Finally, this study has found that users barge-in
and use filler words significantly more often when
they are cognitively loaded. This suggests the need
for a much richer turn-taking model which allows
the system to use back-channels and barge-in when
the user hesitates. An obvious candidate is the in-
cremental approach (Schlangen and Skantze, 2009;
DeVault et al, 2009) which allows the system to pro-
cess partial user inputs, back-channels, predict short
term user input and interrupt the user during hesita-
tions. While incremental dialogue is a growing area
of study, it has not so far been examined in the con-
text of dialogue for secondary tasks. We signpost
this as an important area for future work.
Acknowledgments
We would like to thank to Peter Robinson and Ian
Davies for their help with the experiments.
77
References
A Baron and P Green. 2006. Safety and Usability of
Speech Interfaces for In-Vehicle Tasks while Driving:
A Brief Literature Review. Technical Report UMTRI-
2006-5.
I Davies and P Robinson. 2011. Emotional investment
in naturalistic data collection. In International Con-
ference on Affective Computing and Intelligent Inter-
action.
D DeVault, K Sagae, and DR Traum. 2009. Can I fin-
ish? Learning when to respond to incremental inter-
pretation results in interactive dialogue. In 10th An-
nual SIGDIAL meeting on Discourse and Dialogue.
U Gartner, W Konig, and T Wittig. 2001. Evaluation of
Manual vs. Speech Input When Using a Driver Infor-
mation System in Real Traffic. In International Driv-
ing Symposium on Human Factors in Driving Assess-
ment, Training and Vehicle Design.
A Kun, T Paek, and Z? Medenica. 2007. The effect of
speech interface accuracy on driving performance. In
Interspeech.
J Lai, K Cheng, P Green, and O Tsimhoni. 2001. On the
Road and on the Web? Comprehension of synthetic
and human speech while driving. In SIGCHI.
JD Lee, B Caven, S Haake, and TL Brown. 2001.
Speech-based Interaction with In-vehicle Computers:
The Effect of Speech-based E-mail on Drivers? Atten-
tion to the Roadway. Human Factors, 43:631?640.
R Mishra, E Shriberg, S Upson, J Chen, F Weng, S Pe-
ters, L Cavedon, J Niekrasz, H Cheng, and H Bratt.
2004. A wizard of Oz framework for collecting spo-
ken human-computer dialogs. In Interspeech.
BS Nielsen, B Harsham, B Raj, and C Forlines. 2008.
Speech-Based UI Design for the Automobile. Hand-
book of Research on User Interface Design and Eval-
uation for Mobile Technology, pages 237?252.
David Schlangen and Gabriel Skantze. 2009. A general,
abstract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL ?09, pages 710?718.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken di-
alogue systems. Computer Speech and Language,
24(4):562?588.
O Tsimhoni, D Smith, and P Green. 2004. Address Entry
While Driving: Speech Recognition Versus a Touch-
Screen Keyboard. Human Factors, 46:600?610.
F Weng, L Cavedon, B Raghunathan, D Mirkovic,
H Cheng, H Schmidt, H Bratt, R Mishra, S Peters,
L Zhao, S Upson, E Shriberg, and C Bergmann. 2004.
Developing a conversational dialogue system for cog-
nitively overloaded users. In Proceedings of the Inter-
national Congress on Intelligent Transportation Sys-
tems.
JD Williams and SJ Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language, 21(2):393?
422.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatzmann,
B Thomson, and K Yu. 2010. The Hidden Information
State Model: a practical framework for POMDP-based
spoken dialogue management. Computer Speech and
Language, 24(2):150?174.
78
Proceedings of the SIGDIAL 2013 Conference, pages 214?222,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
POMDP-based dialogue manager adaptation to extended domains
M. Gas?ic?, C. Breslin, M. Henderson, D. Kim, M. Szummer, B. Thomson, P. Tsiakoulis and S. Young
Cambridge University Engineering Department
{mg436,cb404,mh521,dk449,mos25,brmt2,pt344,sjy}@eng.cam.ac.uk
Abstract
Existing spoken dialogue systems are typ-
ically designed to operate in a static and
well-defined domain, and are not well
suited to tasks in which the concepts and
values change dynamically. To handle dy-
namically changing domains, techniques
will be needed to transfer and reuse ex-
isting dialogue policies and rapidly adapt
them using a small number of dialogues in
the new domain. As a first step in this di-
rection, this paper addresses the problem
of automatically extending a dialogue sys-
tem to include a new previously unseen
concept (or slot) which can be then used
as a search constraint in an information
query. The paper shows that in the con-
text of Gaussian process POMDP optimi-
sation, a domain can be extended through
a simple expansion of the kernel and then
rapidly adapted. As well as being much
quicker, adaptation rather than retraining
from scratch is shown to avoid subjecting
users to unacceptably poor performance
during the learning stage.
1 Introduction
Existing spoken dialogue systems are typically de-
signed to operate in a static and well-defined do-
main, and are not well suited to tasks in which
the concepts and values change dynamically. For
example, consider a spoken dialogue system in-
stalled in a car, which is designed to provide in-
formation about nearby hotels and restaurants. In
this case, not only will the data change as the
car moves around, but the concepts (or slots) that
a user might wish to use to frame a query will
also change. For example, a restaurant system de-
signed to be used within cities might not have the
concept of ?al fresco? dining and could not there-
fore handle a query such as ?Find me a French
restaurant where I can eat outside?. In order to
make this possible, techniques will be needed to
extend and adapt existing dialogue policies.
Adaptation can be viewed as a process of im-
proving action selection in a different condition to
the one in which the policy was originally trained.
While adaptation has been extensively studied in
speech recognition (see an overview in (Gales and
Young, 2007)), in spoken dialogue systems it is
still relatively novel and covers a wide range of
possible research topics (Litman and Pan, 1999;
Litman and Pan, 2002; Georgila and Lemon, 2004;
Janarthanam and Lemon, 2010).
A recent trend in statistical dialogue modelling
has been to model dialogue as a partially ob-
servable Markov decision process (POMDP). This
provides increased robustness to errors in speech
understanding and automatic dialogue policy op-
timisation via reinforcement learning (Roy et al,
2000; Zhang et al, 2001; Williams and Young,
2007; Young et al, 2010; Thomson and Young,
2010). A POMDP-based dialogue manager main-
tains a distribution over every possible dialogue
state at every dialogue turn. This is called the
belief state. Based on that distribution the sys-
tem chooses the action that gives the highest ex-
pected reward, measured by the Q-function. The
Q-function for a belief state and an action is the
expected cumulative reward that can be obtained
if that action is taken in that belief state. The opti-
misation typically requires O(105) to O(106) di-
alogues, so is normally done in interaction with a
simulated user (Jurc???c?ek et al, 2011b).
In reinforcement learning, policy adaptation has
been addressed in the context of transfer learn-
ing (Taylor and Stone, 2009). The core idea is to
exploit expertise gained in one domain (source do-
main) to improve learning in another domain (tar-
get domain). A number of techniques have been
developed but they have not been previously ap-
plied to dialogue management.
214
Gaussian process (GP) based reinforcement
learning (Engel, 2005) has been recently applied
to POMDP dialogue policy optimisation in or-
der to exploit the correlations between different
belief states and thus reduce the number of dia-
logues needed for the learning process (Gas?ic? et
al., 2010).
An important feature of a Gaussian process is
that it can incorporate a prior mean and variance
for the function it estimates, in this case the Q-
function. Setting these appropriately can signif-
icantly speed up the process of learning. If the
mean or the variance are estimated in one envi-
ronment, for example a particular user type or a
particular domain, they can be used as a prior for
adaptation in a different environment, i.e. another
user type or another domain. A Gaussian process
does not depend on the belief state but on the cor-
relation between two belief states encoded by the
kernel function. Therefore, if one defines a kernel
function for two belief states in one domain, the
policy can be used in a different domain, provided
that the correlations between belief states follow a
similar pattern.
This paper explores the problem of extending an
existing domain by introducing a previously un-
seen slot. Specifically, a simple restaurant system
is considered which allows a user to search for
restaurants based on food-type and area. This do-
main is then extended by introducing an additional
price-range slot. The policy is trained for the basic
two-slot domain and then reused in the extended
domain by defining a modified kernel function and
using adaptation. This strategy not only allows for
the knowledge of a previously trained policy to be
reused but it also guards against poor performance
in the early stages of learning. This is particularly
useful in a real-world situation where the adapta-
tion is performed in direct interaction with users.
In addition, a potential application of this tech-
nique to reduce the number of training dialogues
is examined. The domain is decomposed into a
series of simple domains and the policy is grad-
ually adapted to the final domain with a smaller
number of dialogues than are normally needed for
training.
The rest of the paper is organised as follows. In
Section 2 the background on Gaussian processes
in POMDP optimisation is given. Then Section 3
gives a description of the Bayesian Update of Di-
alogue State dialogue manager, which is used as
a test-bed for the experiments. In Section 4, a
simple method of kernel modification is described
which allows a policy trained in the basic domain
to be used in an extended domain. Methods of
fast adaptation are investigated in Section 5 and
this adaptation strategy is then tested via interac-
tion with humans using the Amazon Mechanical
Turk service in Section 6. Finally, the use of re-
peated adaptation to speed up the process of policy
optimisation by learning gradually from simple to
more complex domains is explored in Section 7,
before presenting conclusions in Section 8.
2 Gaussian processes in POMDPs
The role of a dialogue policy pi is to map each be-
lief state b ? B into an action a ? A so as to
maximise the expected cumulative reward, a mea-
sure of how good the dialogue is.
The expected cumulative reward is defined by
the Q-function as:
Q(b, a) = Epi
( T?
?=t+1
???t?1r? |bt = b, at = a
)
,
(1)
where r? is the reward obtained at time ? , T is
the dialogue length and ? is the discount factor,
0 < ? ? 1. Optimising the Q-function is then
equivalent to optimising the policy pi.
A Gaussian process (GP) is a non-parametric
Bayesian probabilistic model that can be used
for function regression (Rasmussen and Williams,
2005). It is fully defined by a mean and a kernel
function which defines prior function correlations.
GP-Sarsa is an on-line RL algorithm that mod-
els the Q-function as a Gaussian process (Engel
et al, 2005), Q(b, a) ? GP (0, k((b, a), (b, a)))
where the kernel k(?, ?) is factored into separate
kernels over the belief state and action spaces
kC(b,b?)kA(a, a?). For a sequence of belief state-
action pairs Bt = [(b0, a0), . . . , (bt, at)]T visited
in a dialogue and the corresponding observed im-
mediate rewards rt = [r1, . . . , rt]T, the posterior
of the Q-function for any belief state-action pair
(b, a) is defined by the following:
215
Q(b, a)|rt,Bt ? N (Q(b, a), cov((b, a), (b, a))),
Q(b, a) = kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1rt,
cov((b, a), (b, a)) = k((b, a), (b, a))? kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1Htkt(b, a)
Ht =
?
????
1 ?? ? ? ? 0 0
0 1 ? ? ? 0 0
... . . . . . . ... ...
0 ? ? ? 0 1 ??
?
???? ,
kt(b, a) = [k((b0, a0), (b, a)), . . . , k((bt, at), (b, a))]T,
Kt = [kt((b0, a0)), . . . ,kt((bt, at))]
(2)
where Kt is the Gram matrix ? the matrix of the
kernel function values for visited points Bt, Ht is
a linear operator that captures the reward looka-
head from the Q-function (see Eq. 1) and ?2 is
an additive noise parameter which controls how
much variability in theQ-function estimate we ex-
pect during the process of learning.
If we assume that the Gaussian process
places a prior mean on the Q-function,
Q(b, a) ? GP (m(b, a), k((b, a), (b, a)))
then the posterior mean Q(b, a) is given by (Ras-
mussen and Williams, 2005):
Q(b, a) = m(b, a) + kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1(rt ?mt), (3)
where mt = [m(b0, a0), . . . ,m(bt, at)]T. The
estimate of the variance is same as in Eq. 2.
The Q-function posterior in Eqs. 2 and 3
defines a Gaussian distribution for every be-
lief state-action pair. Thus, when a new be-
lief state b is encountered, for each action a ?
A, there is a Gaussian distribution Q(b, a) ?
N (Q(b, a), cov((b, a), (b, a)))). Sampling from
these Gaussian distributions gives a set of Q-
values for each action {Q(b, a) : a ? A} from
which the action with the highest sampledQ-value
can be selected:
pi(b) = argmax
a
{Q(b, a) : a ? A} . (4)
In this way, the stochastic model of theQ-function
is effectively transformed into a stochastic policy
model, which can be optimised to maximise the re-
ward (Geist and Pietquin, 2011; Gas?ic? et al, 2011;
Gas?ic? et al, 2012).
Due to the matrix inversion in Eq. 2, the compu-
tational complexity of calculating the Q-function
posterior is O(t3), where t is the number of data
points in Bt, and this poses a serious computa-
tional problem. The algorithm used here to ap-
proximate the Gaussian process is the kernel span
sparsification method described in (Engel, 2005).
In this case, only a set of representative data points
is retained ? called the dictionary of visited points.
3 BUDS dialogue manager
The Bayesian Update of Dialogue State (BUDS)
dialogue manager is a POMDP-based dialogue
manager (Thomson and Young, 2010) which fac-
torises the dialogue state into conditionally de-
pendent elements. These elements are arranged
into a dynamic Bayesian network, which allows
for their marginal probability distributions to be
updated during the dialogue. Thus, the belief
state of the BUDS dialogue manager consists of
the marginal posterior probability distribution over
hidden nodes in the Bayesian network. The hidden
nodes in the BUDS system consist of the history
nodes and the goal nodes for each concept in the
dialogue. For instance in a restaurant information
domain these include area, food-type, address.
The history nodes define possible dialogue histo-
ries for a particular concept, eg. system-informed,
user-requested. The goal nodes define possible
values for a particular concept, eg. Chinese, In-
dian. The role of the policy pi is then to map each
216
belief state into a summary action a from the sum-
mary action space A. Once a summary action is
found it is heuristically mapped into the master
action that the system finally takes (Gas?ic? et al,
2012). The master actions are composed of dia-
logue act type and list of slot value pairs. There are
15 dialogue act types in the BUDS system that fa-
cilitate not only simple information providing sce-
narios but also more complex dialogues where the
user can change their mind and ask for alterna-
tives.
To apply GP policy optimisation, a kernel func-
tion must be defined on both the belief state space
B and the action space A. The kernel function
over the belief state b is constructed from the sum
of individual kernels over the hidden node distri-
butions, such that the kernel function of two cor-
responding nodes is based on the expected likeli-
hood kernel (Jebara et al, 2004), which is also a
simple linear inner product:
kB(b,b?) =
?
h
?bh,b?h?, (5)
where bh is the probability distribution encoded
in the hth hidden node. This kernel gives the ex-
pectation of one belief state distribution under the
other.
For history nodes, the kernel is a simple inner
product between the corresponding node distribu-
tions. While it is possible to calculate the kernel
function for the goal nodes in the same way as for
the history nodes, in this case, the choice of sys-
tem action, such as confirm or inform, does not
depend on the actual values. It rather depends on
the shape of the distribution and, in particular, it
depends on the probability of the most likely value
compared to the rest. Therefore, to exploit the cor-
relations further, the kernel over two goal nodes
is calculated as the dot product of vectors, where
each vector represents the corresponding distribu-
tion sorted into order of probability. The only ex-
ceptions are the goal for the method node and the
discourse act node. The former defines whether
the user is searching for a venue by name or by
constraints and the latter defines which discourse
act the user used, eg. acknowledgement, thank you.
Their kernels are calculated in the same way as for
the history nodes.
For the action space kernel, the ?-kernel is used
defined by:
kA(a, a?) = ?a(a?). (6)
where ?a(a?) = 1 iff a = a?.
3.1 TopTable domain
The TopTable domain consists of restaurants in
Cambridge, UK automatically extracted from the
TopTable web service (TopTable, 2012). There are
about 150 restaurants and each restaurant has 7 at-
tributes ? slots. This results in a belief space that
consists of 25 concepts where each concept takes
from 3 to 150 values and each value has a proba-
bility in [0, 1]. The summary action space consists
of 16 summary actions.
3.2 The agenda-based simulated user
In training and testing a simulated user was used.
The agenda-based user simulator (Schatzmann,
2008; Keizer et al, 2010) factorises the user state
into an agenda and a goal. The goal ensures
that the user simulator exhibits consistent, goal-
directed behaviour. The role of the agenda is to
elicit the dialogue acts that are needed for the user
simulator to fulfil the goal. In addition, an er-
ror model adds confusions to the simulated user
input such that it resembles those found in real
data (Thomson et al, 2012). The length of the N-
best list was set to 10 and the confusion rate was
set to 15% during training and testing.1 This error
rate means that 15% of time the true hypothesis is
not in the N-best list. Intermediate experimenta-
tion showed that these confusion rates are typical
of real data.
The reward function was set to give a reward
of 20 for successful dialogues, zero otherwise. In
addition, 1 is deducted for each dialogue turn to
encourage shorter dialogues. The discount factor
? is set to 1 and the dialogue length is limited to
30 turns.
4 Extended domains
Transfer learning is a reinforcement learning tech-
nique which address three problems:
? given a target domain, how to select the
most appropriate source domain from a set of
source domains,
? given a target and a source domain how to
find the relationship between them, and
? given a target and a source domain and the
relationship between them, how to effectively
transfer knowledge between them.
1Except of course where the system is explicitly tested on
varying noise levels.
217
Here we assume that we are given a source and
a target domain and that the relationship between
them is defined by mapping the kernel function.
Knowledge transfer is then effected by adapting
the source domain policy for use in the target do-
main. For the latter, two forms of adaptation are
investigated: one simply continues to update the
set of source data dictionary points with new dic-
tionary points, the second uses the source domain
posterior as a prior for the new target domain.
In this case, the source is a basic restaurant do-
main with slots name, area, food-type, phone, ad-
dress, and postcode. The extended target domain
has an additional price-range slot. We are inter-
ested primarily in training the policy on the ba-
sic domain and testing it on the extended domain.
However, since real applications may also require
a slot to be forgotten, we also investigate the re-
verse where the policy is trained in the extended
domain and tested on the basic domain.
In order to enable the required cross domain
portability, a kernel function defining the correla-
tion between belief states from differing domains
is needed. Since the extended domain has an ex-
tra slot and thus extra hidden nodes, we need to
define the correlations between the extra hidden
nodes and the hidden nodes in the belief state of
the basic domain. This can be performed in vari-
ous ways, but the simplest approach is to specify
which slot from the basic domain is most similar
to the new slot in the extended domain and then
match their corresponding hidden nodes. In that
way the belief state kernel function between two
belief states bB, bE for the basic B and the ex-
tended E domain becomes:
kB(bB,bE) =
?
h?B
?bBh ,bEh?+
?
e/?B
?bBl(e),bEe ?, (7)
where h are the hidden nodes in the basic domain,
e are the hidden nodes in the extended domain and
function l : E? B for each hidden node that does
not exist in the basic domain finds its appropriate
replacement. In the particular case studied here,
the slot area is most similar to the new price-range
slot since they both have a relatively small number
of values, about 5. Hence, l(price-range)? area.
If the cardinality of the mapped slots differ, the
shorter is padded with zeros though other forms of
normalisation are clearly possible.
The (summary) action space for the extended
domain has more actions than the basic domain.
For example, one action that exists in the extended
domain and does not exist in the basic domain is
request(price-range). To define the kernel func-
tion between these sets of actions, one can specify
for each extra action in the extended domain its
most similar action in the basic domain:
kA(aB, aE) =
{
?aB(aE) aE ? AB,
?aB(L(aE)) aE /? AB,
(8)
where function L : AE ? AB for each action
that does not exist in the basic domain finds its
replacement action.
Functions L and l are here defined manually.
However, a simple but effective heuristic would be
to find for each new slot in the extended domain, a
slot in the basic domain with similar cardinality.
Porting in the reverse direction from the ex-
tended to the basic domain is easier since one can
simply disregard the extra hidden nodes and ac-
tions in the kernel calculation.
To experimentally examine the extent to which
this method supports cross domain portability, we
trained policies for both domains until conver-
gence, using 105 dialogues on the simulated user.
We then cross tested them on the mismatching do-
mains at varying user input error rates. The results
are given in Fig. 1.
0 10 20 30 40 50ErrorRate2
0
2
4
6
8
10
12
Rewa
rd
bsc-trn&tstextd-trn&tstextd-trn&bsc-tstbsc-trn&extd-tst
Figure 1: Cross testing policies trained on differ-
ent domains. bsc refers to the basic domain, extd is
the extended domain, trn is training and tst is test-
ing.
From the results it can be seen that the policy
trained for the basic domain has a better perfor-
mance than the policy trained on the extended do-
main, when tested on the matching domain (com-
218
pare bsc-trn&tst with extd-trn&tst). The extended do-
main has more slots so it is more difficult for the
system to fulfil the user request, especially in noisy
conditions. Secondly, the performance of the pol-
icy trained on the extended domain and tested on
the basic domain is close to optimal (compare bsc-
trn&tst with extd-trn&bsc-tst). However, the pol-
icy trained on the basic domain and tested on the
extended domain has much worse performance
(compare bsc-trn&extd-tst with extd-trn&tst). It is
hard for the policy to adequately extrapolate from
the basic to the extended domain. This difference
in performance, however, motivates the need for
adaptation and this is investigated in the next sec-
tion.
5 Adaptation
Adaptation of a policy trained on one domain to
another can be performed in several ways. Here
we examine two adaptation strategies similar to
the method described in (Taylor et al, 2007),
where every action-value for each state in the tar-
get domain is initialised with learned source do-
main values.
The first strategy is to take the policy trained in
the source domain and simply continue training it
in the target domain until convergence. In Gaus-
sian process reinforcement learning, this means
that we assume a zero-mean prior on the Gaussian
process for theQ-function and let the dictionary of
visited points Bt from Eq. 2 consist of both points
visited in the source domain and the extended tar-
get domain, making sure that the Gram matrix
Kt uses extended domain kernel function where
necessary. However, the estimate of the variance
decreases with the number of visited points (see
Eq. 2). The danger therefore when performing
adaptation in this way is that the estimate of vari-
ances obtained in the source domain will be very
small since the policy has already been trained un-
til convergence with a large number of dialogues.
As a consequence, the rate of exploration defined
by sampling in Eq. 4 will be reduced and thus lead
to the subsequent optimisation in the new target
domain falling prematurely into a local optimum.
As an alternative, we propose another adapta-
tion strategy. The estimate of the posterior of the
mean for the Q-function, Q in Eq. 2, from the pol-
icy trained on the basic domain can be taken to be
the prior of the mean when the policy is trained on
the extended domain as in Eq. 3. More precisely, if
Qbsc is the posterior mean of the policy trained on
the basic domain then mextd = Qbsc. In this case
it is also important to make sure that the kernel
function used to calculateQbsc is redefined for the
extended domain where necessary. The prior on
the variance is the original kernel function renor-
malised:
k((b, a), (b?, a?))? k((b,a),(b?,a?))?
k((b,a),(b,a))k((b?,a?),(b?,a?))
.
(9)
Given that the estimate of the mean provides rea-
sonable performance, it is not necessary to place
a flat prior on the variance of the Q-function and
therefore the kernel is normalised as in Eq. 9.
When comparing adaptation strategies, we are
interested in two aspects of performance. The first
is the performance of the policy during training.
The second is how quickly the policy reaches the
optimal performance. For that reason we adopt
the following evaluation scheme. After every 100
adaptation dialogues we test the partially opti-
mised policy with 1000 simulated dialogues, dif-
ferent to the ones used in adaptation. These 1000
dialogues are the same for every test point on the
graph. The results are given in Fig. 2.
0 200 400 600 800 1000 1200 1400 1600Training dialogues20
15
10
5
0
5
10
Rewa
rd
PRIORADAPTTRAINbsc-trn&extd-tstextd-trn&tst
Figure 2: Different adaptation strategies
The lower horizontal line represents the perfor-
mance of the policy trained on the basic source
domain and tested on the extended target domain.
This is the baseline. The upper horizontal line
represents the policy trained until convergence on
the extended domain and also tested on the ex-
tended domain. This provides the gold standard.
The adaptation strategy that takes both the mean
and variance of the policy trained on the basic do-
main and retrains the policy on the extended do-
219
main is denoted as ADAPT in Fig. 2. The adap-
tation strategy that uses the posterior mean of the
policy trained on the source domain as the prior
mean for adaptation is denoted as PRIOR in Fig. 2.
Finally, for comparison purposes we show the per-
formance of the policy that is trained from scratch
on the extended domain. This is denoted as TRAIN
on the graph. It can be seen that both adapta-
tion strategies significantly reduce the number of
training dialogues and, more importantly, main-
tain the level of performance during adaptation.
The adaptation strategy that places the prior on the
mean has slightly worse performance in the begin-
ning but provides the best performance after 1500
dialogues. As already noted, this could be due
to overly confident variances in the ADAPT case
leading to a local optimum.
6 Human experiments
In order to adapt and evaluate policies with hu-
mans, we used crowd-sourcing via the Ama-
zon Mechanical Turk service in a set-up similar
to (Jurc???c?ek et al, 2011a; Gas?ic? et al, 2013).
The BUDS dialogue manager was incorporated
in a live telephone-based spoken dialogue system.
The Mechanical Turk users were assigned spe-
cific tasks in the extended TopTable domain. They
were asked to find restaurants that have particu-
lar features as defined by the given task. To elicit
more complex dialogues, the users were some-
times asked to find more than one restaurant, and
in cases where such a restaurant did not exist they
were required to seek an alternative, for example
find a Chinese restaurant instead of a Vietnamese
one. After each dialogue the users filled in a feed-
back form indicating whether they judged the di-
alogue to be successful or not. Based on that bi-
nary rating, the subjective success was calculated
as well as the average reward. An objective rat-
ing can also be obtained by comparing the system
outputs with the predefined task.
During policy adaptation, at the end of each
call, users were asked to press 1 if they were satis-
fied (i.e. believed that they had been successful in
fulfilling the assigned task) and 0 otherwise. The
objective success was also calculated. The dia-
logue was then only used for adaptation if the user
rating agreed with the objective measure of suc-
cess as in (Gas?ic? et al, 2013). The performance
based on user ratings during adaptation for both
adaptation strategies is given in Table 1.
Table 1: Policy performance during adaptation
#Diags Reward Success (%)
ADAPT 251 11.7? 0.5 92.0? 1.7
PRIOR 329 12.1? 0.4 96.7? 1.0
We then evaluated four policies with real users:
the policy trained on the basic domain, the pol-
icy trained on the extended domain and the pol-
icy adapted to the extended domain using the prior
and the policy adapted to the extended domain via
interaction with real users using retraining. The
results are given in Table 2.
Table 2: Human evaluation of four systems in the
extended domain: trained in the basic domain,
trained in the extended domain, trained in the ba-
sic and adapted in the extended domain using both
ADAPT and PRIOR methods.
Training #Diags Reward Success(%)
Basic 246 11.0? 0.5 91.9? 1.7
Extended 250 12.1? 0.4 94.4? 1.5
ADAPT 268 12.6? 0.4 94.4? 1.4
PRIOR 252 12.4? 0.4 95.6? 1.3
The results show two important features of
these adaptation strategies. The first is that it is
possible to adapt the policy from one domain to
another with a small number of dialogues. Both
adaptation techniques achieve results statistically
indistinguishable from the matched case where the
policy was trained directly in the extended do-
main. The second important feature is that both
adaptation strategies guarantee a minimum level
of performance during training, which is better
than the performance of the basic policy tested on
the extended domain. This is particularly impor-
tant when training with real users so that they are
not exposed to poor performance at any time dur-
ing training.
7 Application to fast learning
The above results show that transfer learning
through policy adaptation can be relatively fast.
Since complex domains can be decomposed into a
series of domains with gradually increasing com-
plexity, an alternative to training a system to con-
vergence starting from an uninformative prior is
220
to train a system in stages iteratively adapting to
successively more complex domains (Taylor and
Stone, 2009).
We explored this idea by training the extended
system in three stages. The first has only one slot
that the user can specify: food-type and additional
slots phone, address and postcode that can be re-
quested (initial in Fig. 3). The second has an ad-
ditional area slot (intermediate in Fig. 3) and the
final domain has a the price-range slot added (final
on the graph).
A policy for each of these domains was trained
until convergence and the average rewards of these
policies are the horizontal lines on Fig. 3. In addi-
tion, the following adaptation schedule was imple-
mented. An initial policy was trained from scratch
for the one-slot initial system using only 1500 dia-
logues. The resulting policy was then retrained for
the intermediate two-slot system using again just
1500 dialogues. Finally, the required three-slot
system was trained using 1500 dialogues. At each
stage the policy was tested every 100 training dia-
logues, and the resulting performances are shown
by the three graphs initial-train, intermediate-adapt
and final-adapt in Fig. 3. The policies were tested
on the domains they are trained on or adapted to.
It can be seen that after just 500 dialogues of
the third stage (i.e. after just 3500 dialogues in to-
tal) the policy reaches optimal performance. It has
been shown previously that Gaussian process re-
inforcement learning for this task normally takes
104 dialogues (Gas?ic? et al, 2012) so this schedule
halves the number of dialogues needed for train-
ing. Also it is important to note that when training
from scratch the average reward is less than 5 for
300 dialogues (see TRAIN in Fig. 2), in this case
that only happens for about 100 dialogues (see
initial-train in Fig. 3).
8 Conclusions
This paper has investigated the problem of ex-
tending a dialogue system to handle new previ-
ously unseen concepts (i.e. slots) using adapta-
tion based transfer learning. It has been shown that
a GP kernel can be mapped to establish a relation-
ship between a basic and an extended domain and
that GP-based adaptation can restore a system to
optimal performance within 200 to 300 adaptation
dialogues. A major advantage of this technique is
that it allows a minimum level of performance to
be guaranteed and hence guards against subject-
0 200 400 600 800 1000 1200 1400 1600Training dialogues15
10
5
0
5
10
15
Rewa
rd
initial-trainintermediate-adaptfinal-adaptintermediateinitialfinal
Figure 3: Application of transfer learning to fast
training. The target is to achieve the performance
of the fully trained 3 slot system as shown by the
lower horizontal line final. This is achieved in three
stages, with the target being achieved part way
through the 3rd stage using just 3500 dialogues in
total.
ing the user to poor performance during the early
stages of adaptation.
Two methods of adaptation have been studied ?
one based on augmenting the training points from
the source domain with new points from the tar-
get domain, and a second which treats the source
policy as a prior for the target policy. Results us-
ing the prior method were consistently better. In a
further experiment, it was also shown that starting
with a simple system and successively extending
and adapting it slot by slot, can achieve optimal
performance faster than one trained directly from
scratch.
These results suggest that it should be feasi-
ble to construct dialogue systems which can dy-
namically update and extend their domains of dis-
course automatically during direct conversations
with users. However, further investigation of
methods for learning the relationship between the
new and the old domains is needed. Also, the
scalability of these results to large-scale domain
expansion remains a topic for future work.
Acknowledgments
This work was partly supported by PAR-
LANCE (www.parlance-project.eu), an EU Sev-
enth Framework Programme project (grant num-
ber 287615).
221
References
Y Engel, S Mannor, and R Meir. 2005. Reinforcement
learning with Gaussian processes. In Proceedings of
ICML.
Y Engel. 2005. Algorithms and Representations for
Reinforcement Learning. PhD thesis, Hebrew Uni-
versity.
M Gales and S Young. 2007. The application of hid-
den Markov models in speech recognition. Found.
Trends Signal Process., 1:195?304.
M Gas?ic?, F Jurc???c?ek, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, K Yu, and S Young. 2010.
Gaussian Processes for Fast Policy Optimisation of
POMDP-based Dialogue Managers. In Proceedings
of SIGDIAL.
M Gas?ic?, F Jurc???c?ek, B Thomson, K Yu, and S Young.
2011. On-line policy optimisation of spoken dia-
logue systems via live interaction with human sub-
jects. In Proceedings of ASRU.
M Gas?ic?, M Henderson, B Thomson, P Tsiakoulis, and
S Young. 2012. Policy optimisation of POMDP-
based dialogue systems without state space com-
pression. In Proceedings of SLT.
M Gas?ic?, C. Breslin, M. Henderson, Szummer M.,
B Thomson, P. Tsiakoulis, and S Young. 2013.
On-line policy optimisation of Bayesian Dialogue
Systems by human interaction. In Proceedings of
ICASSP.
M Geist and O Pietquin. 2011. Managing Uncertainty
within the KTD Framework. In Proceedings of the
Workshop on Active Learning and Experimental De-
sign, Sardinia (Italy).
K Georgila and O Lemon. 2004. Adaptive multimodal
dialogue management based on the information state
update approach. In W3C Workshop on Multimodal
Interaction.
S Janarthanam and O Lemon. 2010. Adaptive Re-
ferring Expression Generation in Spoken Dialogue
Systems: Evaluation with Real Users. In Proceed-
ings of SIGDIAL.
T Jebara, R Kondor, and A Howard. 2004. Probability
product kernels. J. Mach. Learn. Res., 5:819?844,
December.
F Jurc???c?ek, S Keizer, M Gas?ic?, F Mairesse, B Thomson,
K Yu, and S Young. 2011a. Real user evaluation of
spoken dialogue systems using Amazon Mechanical
Turk. In Proceedings of Interspeech.
F Jurc???c?ek, B Thomson, and S Young. 2011b. Natural
actor and belief critic: Reinforcement algorithm for
learning parameters of dialogue systems modelled as
POMDPs. ACM Transactions on Speech and Lan-
guage Processing.
S Keizer, M Gas?ic?, F Jurc???c?ek, F Mairesse, B Thomson,
K Yu, and S Young. 2010. Parameter estimation
for agenda-based user simulation. In Proceedings of
SIGDIAL.
DJ Litman and S Pan. 1999. Empirically evaluating
an adaptable spoken dialogue system. In Proceed-
ings of the seventh international conference on User
modelling.
DJ Litman and S Pan. 2002. Designing and evaluat-
ing an adaptive spoken dialogue system. User Mod-
elling and User-Adapted Interaction, 12:111?137.
CE Rasmussen and CKI Williams. 2005. Gaussian
Processes for Machine Learning. MIT Press, Cam-
bridge, Massachusetts.
N Roy, J Pineau, and S Thrun. 2000. Spoken dialogue
management using probabilistic reasoning. In Pro-
ceedings of ACL.
J Schatzmann. 2008. Statistical User and Error Mod-
elling for Spoken Dialogue Systems. Ph.D. thesis,
University of Cambridge.
ME Taylor and P Stone. 2009. Transfer learning for
reinforcement learning domains: A survey. J. Mach.
Learn. Res., 10:1633?1685, December.
ME Taylor, P Stone, and Y Liu. 2007. Transfer learn-
ing via inter-task mappings for temporal difference
learning. J. Mach. Learn. Res., 8:2125?2167, De-
cember.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken di-
alogue systems. Computer Speech and Language,
24(4):562?588.
B Thomson, M Gas?ic?, M Henderson, P Tsiakoulis, and
S Young. 2012. N-Best error simulation for training
spoken dialogue systems. In Proceedings of SLT.
TopTable. 2012. TopTable. https://www.
toptable.com.
JD Williams and SJ Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language, 21(2):393?
422.
S Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, and K Yu. 2010. The Hid-
den Information State model: A practical frame-
work for POMDP-based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150?
174.
B Zhang, Q Cai, J Mao, E Chang, and B Guo.
2001. Spoken Dialogue Management as Planning
and Acting under Uncertainty. In Proceedings of
Eurospeech.
222
Proceedings of the SIGDIAL 2013 Conference, pages 467?471,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Deep Neural Network Approach for the
Dialog State Tracking Challenge
Matthew Henderson, Blaise Thomson and Steve Young
Department of Engineering,
University of Cambridge, U.K.
{mh521, brmt2, sjy}@eng.cam.ac.uk
Abstract
While belief tracking is known to be im-
portant in allowing statistical dialog sys-
tems to manage dialogs in a highly robust
manner, until recently little attention has
been given to analysing the behaviour of
belief tracking techniques. The Dialogue
State Tracking Challenge has allowed for
such an analysis, comparing multiple be-
lief tracking approaches on a shared task.
Recent success in using deep learning for
speech research motivates the Deep Neu-
ral Network approach presented here. The
model parameters can be learnt by directly
maximising the likelihood of the training
data. The paper explores some aspects of
the training, and the resulting tracker is
found to perform competitively, particu-
larly on a corpus of dialogs from a system
not found in the training.
1 Introduction
Statistical dialog systems, in maintaining a distri-
bution over multiple hypotheses of the true dialog
state, are able to behave in a robust manner when
faced with noisy conditions and ambiguity. Such
systems rely on probabilistic tracking of dialog
state, with improvements in the tracking quality
being important in the system-wide performance
in a dialog system (see e.g. Young et al (2009)).
This paper presents a Deep Neural Network
(DNN) approach for dialog state tracking which
has been evaluated in the context of the Dia-
log State Tracking Challenge (DSTC) (Williams,
2012a; Williams et al, 2013)1.
Using Deep Neural Networks allows for the
modelling of complex interactions between arbi-
trary features of the dialog. This paper shows im-
provements in using deep networks over networks
1More information on the DSTC is available at
http://research.microsoft.com/en-us/events/dstc/
with fewer hidden layers. Recent developments in
speech research have shown promising results us-
ing deep learning, motivating its use in the context
of dialog (Hinton et al, 2012; Li et al, 2013).
This paper presents a technique which solves
the task of outputting a sequence of probability
distributions over an arbitrary number of possible
values using a single neural network, by learning
tied weights and using a form of sliding window.
As the classification task is not split into multiple
sub-tasks for a given slot, the log-likelihood of the
tracker on training data can be directly maximised
using gradient ascent techniques.
The domain of the DSTC is bus route informa-
tion in the city of Pittsburgh, but the presented
technique is easily transferable to new domains,
with the learned models in fact being domain in-
dependent. No domain specific knowledge is used,
and the classifier learned does not require knowl-
edge of the set of possible values. The tracker per-
formed highly competitively in the ?test4? dataset,
which consists of data from a dialog system not
seen in training. This suggests the model is ca-
pable of capturing the important aspects of dia-
log in a robust manner without overtuning to the
specifics of a particular system.
Most attention in the dialog state belief tracking
literature has been given to generative Bayesian
network models (Paek and Horvitz, 2000; Thom-
son and Young, 2010). Few trackers have been
published using discriminative classifiers, a no-
table exception being Bohus and Rudnicky (2006).
An analysis by Williams (2012b) demonstrates
how such generative models can in fact degrade
belief tracking performance relative to a simple
baseline. The successful use of discriminative
models for belief tracking has recently been al-
luded to by Williams (2012a) and Li et al (2013),
and was a prominent theme in the results of the
DSTC.
467
2 The Dialog State Tracking Challenge
This section describes the domain and method-
ology of the Dialog State Tracking Challenge
(DSTC). The Challenge uses data collected during
the course of the Spoken Dialog Challenge (Black
et al, 2011), in which participants implemented
dialog systems to provide bus route information in
the city of Pittsburgh. This provides a large cor-
pus of real phonecalls from members of the public
with real information needs.
Set Number of calls Notes
train1a 1013 Labelled training data
train1b&c 10619 Same dialog system astrain1a, but unlabelled
train2 678 Similar to train1*
train3 779 Different participant toother train sets
test1 765 Very similar to train1*and train2
test2 983 Somewhat similar totrain1* and train2
test3 1037 Very similar to train3
test4 451 System not found inany training set
Table 1: Summary of datasets in the DSTC
Table 1 summarises the data provided in the
challenge. Labelled training sets provide labels
for the caller?s true goal in each dialog for 5 slots;
route, from, to, date and time.
Participants in the DSTC were asked to report
the results of their tracker on the four test sets in
the form of a probability distribution over each
slot for each turn. Performance was determined
using a basket of metrics designed to capture dif-
ferent aspects of tracker behaviour Williams et al
(2013). These are discussed further in Section 4.
The DNN approach described here is referred to
in the results of the DSTC as ?team1/entry1?.
3 Model
For a given slot s at turn t in a dialog, let St, s de-
note the set of possible values for s which have oc-
curred as hypotheses in the SLU for turns ? t. A
tracker must report a probability distribution over
St, s ? {other} representing its belief of the user?s
true goal for the slot s. The probability of ?other?
represents the probability that the user?s true goal
is yet to appear as an SLU hypothesis.
A neural network structure is defined which
gives a discrete distribution over the |St, s|+1 val-
ues, taking the turns ? t as input.
Figure 1 illustrates the structure used in this ap-
proach. Feature functions fi (t, v) for i = 1 . . .M
f1 (t, v) f1 (t? T + 1, v) ?t?Tt?=0 f1 (t?, v)
f2 (t, v) f2 (t? T + 1, v) ?t?Tt?=0 f2 (t?, v)
fM (t, v) fM (t? T + 1, v) ?t?Tt?=0 fM (t?, v)
t t? T + 1 (0 . . . t? T )
f1
f2
fM
. . .
...
h1 [= tanh(W0fT + b0)]
h2 [= tanh(W1hT1 + b1)]
h3 [= tanh(W2hT2 + b2)]
E(t, v) [= W3hT3 ]
Figure 1: The Neural Network structure for computing
E (t, v) ? R for each possible value v in the set St, s. The
vector f is a concatenation of all the input nodes.
are defined which extract information about the
value v from the SLU hypotheses and machine
actions at turn t. A simple example would be
fSLU (t, v), the SLU score that s=v was informed
at turn t. A list of the feature functions actually
used in the trial is given in Section 3.1. For nota-
tional convenience, feature functions at negative t
are defined to be zero:
?i ?v, t? < 0? fi (t?, v) = 0.
The input layer for a given value v is fixed in
size by choosing a window size T , such that the
feature functions are summed for turns ? t ? T .
The input layer therefore consists of (T ?M) in-
put nodes set to fi (t?, v) for t? = t?T+1 . . . t and
i = 1 . . .M , and M nodes set to ?t?Tt?=0 fi (t?, v)
for i = 1 . . .M .
A feed-forward structure of hidden layers is
chosen, which reduces to a single node denoted
E (t, v). Each hidden layer introduces a weight
matrix Wi and a bias vector bi as parameters,
which are independent of v but possibly trained
separately for each s. The equations for each layer
in the network are given in Figure 1.
The final distribution from the tracker is:
P (s = v) = eE(t, v)/Z
P (s /? St, s) = eB/Z
Z = eB +
?
v??St, s
eE(t, v
?)
where B is a new parameter of the network, in-
dependent of v and possibly trained separately for
each slot s.
468
3.1 Feature Functions
As explained above, a feature function is a func-
tion f (t, v) which (for a given dialog) returns a
real number representing some aspect of the turn
t with respect to a possible value v. A turn con-
sists of a machine action and the subsequent Spo-
ken Language Understanding (SLU) results. The
functions explored in this paper are listed below:
1. SLU score; the score assigned by the SLU to
the user asserting s=v.
2. Rank score; 1/r where r is the rank of s=v in
the SLU n-best list, or 0 if it is not on the list.
3. Affirm score; SLU score for an affirm action
if the system just confirmed s=v.
4. Negate score; as previous but with negate.
5. Go back score; the score assigned by the SLU
to a goback action matching s=v.
6. Implicit score; 1? the score given in the SLU
to a contradictory action if the system just im-
plicitly confirmed s=v, otherwise 0.
7. User act type; a feature function for each pos-
sible user act type, giving the total score of the
user act type in the SLU. Independent of s & v.
8. Machine act type; a feature function for each
possible machine act type, giving the total num-
ber of machine acts with the type in the turn.
Independent of s & v.
9. Cant help; 1 if the system just said that it can-
not provide information on s=v, otherwise 0.
10. Slot confirmed; 1 if s=v? was just confirmed
by the system for some v?, otherwise 0.
11. Slot requested; 1 if the value of s was just re-
quested by the system, otherwise 0.
12. Slot informed; 1 if the system just gave infor-
mation on a set of bus routes which included a
specific value of s, otherwise 0.
4 Training
The derivatives of the training data likelihood with
respect to all the parameters of the model can
be computed using back propagation, i.e. the
chain rule. Stochastic Gradient Descent with mini-
batches is used to optimise the parameters by de-
scending the negative log-likelihood in the direc-
tion of the derivatives (Bottou, 1991). Termina-
tion is triggered when performance on a held-out
development set stops improving.
Each turn t and slot s in a dialog for which
|St, s| > 0 provides a non-zero summand to the
total log-likelihood of the training data. These in-
stances may be split up by slot to train a separate
network for each slot. Alternatively the data can
be combined to learn a slot independent model.
The best approach found was to train a slot inde-
pendent model for a few epochs, and then switch
to training one model per slot (see Section 4.4).
This section presents experiments varying the
training of the model. In each case the parameters
are trained using all of the labelled training sets.
The results are reported for test4 since this system
is not found in the training data. They are therefore
unbiassed and avoid overtuning problems.
The ROC curves, accuracy, Mean Reciprocal
Rank (MRR) and l2 norm of the tracker across all
slots are reported here. (A full definition of the
metrics is found in Williams et al (2013).) These
are computed throughout using statistics at every
turn t where |St, s| > 0 (referred to as ?schedule
2? in the terminology of the challenge.) Table 2
and Figure 3 in Appendix A show these metrics.
The ?Baseline? system (?team0/entry1? in the chal-
lenge), considers only the top SLU hypothesis so
far, and assigns the SLU confidence score as the
tracker probability. It does not therefore incorpo-
rate any belief tracking.
4.1 Window Size
The window size, T , was varied from 2 to 20. T
must be selected so that it is large enough to cap-
ture enough of the sequence of the dialog, whilst
ensuring sufficient data to train the weights con-
necting the inputs from the earlier turns. The re-
sults suggest that T = 10 is a good compromise.
4.2 Feature Set
The features enumerated in Section 3.1 were split
into 4 sets. F1 = {1} includes only the SLU
scores; F2 = {1, ..., 6} includes feature func-
tions which depend on the user act and the value;
F3 = {1, ..., 8} also includes the user act and ma-
chine act types; and finally F4 = {1, ..., 12} in-
cludes functions which depend on the system act
and the value. The results clearly show that adding
more and more features in this manner monotoni-
cally increases the performance of the tracker.
4.3 Structure
Some candidate structures of the hidden layers
(h1, h2, ...) were evaluated, including having no
hidden layers at all, which gives a logistic regres-
sion model. In Table 2 the structure is represented
as a list giving the size of each hidden layer in turn.
Three layers in a funnelling [20, 10, 2] configu-
ration is found to outperform the other structures.
The l2 norm is highly affected by the use of deeper
network structure, suggesting it is most useful in
tweaking the calibration of the confidence scores.
469
ROC Acc. MRR l2
Baseline
0.5841 0.7574 0.5728
Window Size
T =2 0.6679 0.8044 0.5405
5 0.6875 0.8191 0.5164
10 0.6922 0.8207 0.5331
15 0.6718 0.8107 0.5352
20 0.6817 0.8190 0.5174
Feature Set
F1 0.5495 0.7364 0.6838
F2 0.6585 0.7954 0.6631
F3 0.6823 0.8134 0.5525
F4 0.6922 0.8207 0.5331
Structure
[] 0.6751 0.8074 0.5658
[50] 0.6679 0.8046 0.5450
[20] 0.6656 0.8060 0.5394
[50, 10] 0.6645 0.8045 0.5404
[20, 2] 0.6543 0.7952 0.5514
[20, 10, 2] 0.6922 0.8207 0.5331
Initialisation
Separate 0.6907 0.8206 0.5472
Single Model 0.6779 0.8111 0.5570
Shared Init. 0.6922 0.8207 0.5331
Table 2: Results for variant trackers described in Section
4. By default, we train using the shared initialisation training
method with T = 10, all the features enumerated in Section
3.1, and 3 hidden layers of size 20, 10 and 2.
4.4 Initialisation
The three methods of training alluded to in Sec-
tion 4 were evaluated; training a model for each
slot without sharing data between slots (Separate);
training a single slot independent model (Single
Model); and training for a few epochs a slot in-
dependent model, then using this to initialise the
training of separate models (Shared Initialisation).
The method of shared initialisation appears to
be the most effective, scoring the best on accu-
racy, MRR and l2. Training in this manner is par-
ticularly beneficial for slots which are under rep-
resented in the training data, as it initiates the pa-
rameters to sensible values before going on to spe-
cialise to that particular slot.
5 Performance in the DSTC
A DNN tracker was trained for entry in the
DSTC. Training used T=10, the full feature set,
a [20, 10, 2] hidden structure and the shared ini-
tialisation training method. Other parameters such
as the learning rate and regularisation coefficient
were tweaked by analysing performance on a held
out subset of the training data. All the labelled
test1
Acc.MRR
test2
Acc.MRR
test3
Acc.MRR
test4
Acc.MRR
all
Acc.MRR
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 2: Accuracy and MRR of the 28 entries in the DSTC
for all slots. Boxplots show minimum, maximum, quartiles
and the median. Dark dot is location of the entry presented in
this paper (DNN system).
training data available was used. The tracker is
labelled as ?team1/entry1? in the DSTC.
The DNN approach performed competitively in
the challenge. Figure 2 summarises the perfor-
mance of the approach relative to all 28 entries in
the DSTC. The results are less competitive in test2
and test3 but very strong in test1 and test4.
The performance in test4, dialogs with an un-
seen system, was probably the best because the
chosen feature functions forced the learning of a
general model which was not able to exploit the
specifics of particular ASR+SLU configurations.
Features which depend on the identity of the slot-
values would have allowed better performance in
test2 and test3, allowing the model to learn dif-
ferent behaviours for each value and learn typical
confusions. It would also have been possible to ex-
ploit the system-specific data available in the chal-
lenge, such as more detailed confidence metrics
from the ASR.
For a full comparison across the entries in the
DSTC, see Williams et al (2013). In making com-
parisons it should be noted that this team did not
alter the training for different test sets, and submit-
ted only one entry.
6 Conclusion
This paper has presented a discriminative ap-
proach for tracking the state of a dialog which
takes advantage of deep learning. While sim-
ple Gradient Ascent training was tweaked in this
paper using the ?Shared Initialisation? scheme, a
possible promising future direction would be to
further experiment with more recent methods for
training deep structures e.g. initialising the net-
works layer by layer (Hinton et al, 2006).
Richer feature representations of the dialog con-
tribute strongly to the performance of the model.
The feature set presented is applicable across a
broad range of slot-filling dialog domains, sug-
gesting the possibility of using the models across
domains without domain-specific training data.
470
A ROC Curves
Window Size
0.1 0.2 0.3 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Feature Set
0.1 0.2 0.3 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Structure
0.1 0.2 0.3 0.4
0.1
0.2
0.3
0.4
0.5
0.6
Initialisation
0.1 0.2 0.3 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Figure 3: ROC (Receiver Operating Characteristic) Curves
x-axis and y-axis are false acceptance and true acceptance
respectively. Lines are annotated as per Table 2.
Acknowledgments
The authors would like to thank the organisers of
the DSTC. The principal author was funded by a
studentship from the EPSRC.
References
Alan W. Black, Susanne Burger, Alistair Conkie, He-
len Wright Hastie, Simon Keizer, Oliver Lemon,
Nicolas Merigaud, Gabriel Parent, Gabriel Schu-
biner, Blaise Thomson, Jason D. Williams, Kai Yu,
Steve Young, and Maxine Eskenazi. 2011. Spoken
dialog challenge 2010: Comparison of live and con-
trol test results. In SigDIAL.
Dan Bohus and Alex Rudnicky. 2006. A K-
hypotheses+ Other Belief Updating Model. Proc.
of the AAAI Workshop on Statistical and Empirical
Methods in Spoken Dialogue Systems.
Le?on Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of Neuro-N??mes
91, N??mes, France. EC2.
Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh.
2006. A Fast Learning Algorithm for Deep Belief
Nets. Neural computation.
Geoffrey Hinton, Li Deng, Dong Yu, George Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara
Sainath, and Brian Kingsbury. 2012. Deep neural
networks for acoustic modeling in speech recogni-
tion. Signal Processing Magazine.
Deng Li, Jinyu Li, Jui-Ting Huang, Kaisheng Yao,
Dong Yu, Frank Seide, Michael L Seltzer, Geoff
Zweig, Xiaodong He, Jason D. Williams, Yifan
Gong, and Alex Acero. 2013. Recent Advances in
Deep Learning for Speech Research at Microsoft. In
ICASSP.
Tim Paek and Eric Horvitz. 2000. Conversation as
action under uncertainty. In The Sixteenth Confer-
ence on Uncertainty in Artificial Intelligence. Mor-
gan Kaufmann.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech & Lan-
guage.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan W. Black. 2013. The Dialogue State
Tracking Challenge. In SigDIAL.
Jason D. Williams. 2012a. A belief tracking chal-
lenge task for spoken dialog systems. In NAACL
HLT 2012 Workshop on Future directions and needs
in the Spoken Dialog Community: Tools and Data.
Association for Computational Linguistics.
Jason D. Williams. 2012b. Challenges and opportu-
nities for state tracking in statistical spoken dialog
systems: Results from two public deployments. J.
Sel. Topics Signal Processing, 6(8):959?970.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2009. The Hidden Information State model:
A practical framework for POMDP-based spoken
dialogue management. Computer Speech & Lan-
guage.
471
Proceedings of the SIGDIAL 2014 Conference, page 1,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
Keynote: Statistical Approaches to Open-domain
Spoken Dialogue Systems
Steve Young
Cambridge University, UK
sjy@eng.cam.ac.uk
In contrast to traditional rule-based approaches to building spoken
dialogue systems, recent research has shown that it is possible to imple-
ment all of the required functionality using statistical models trained
using a combination of supervised learning and reinforcement learning.
This approach to spoken dialogue is based on the mathematics of par-
tially observable Markov decision processes (POMDPs) in which user
inputs are treated as observations of some underlying belief state, and
system responses are determined by a policy which maps belief states
into actions.
Virtually all current spoken dialogue systems are designed to op-
erate in either a specific carefully defined domain such as restaurant
information and appointment booking, or they have very limited con-
versational ability such as in Siri and Google Now. However, if voice is
to become a significant input modality for accessing web-based infor-
mation and services, then techniques will be needed to enable conver-
sational spoken dialogue systems to operate within open domains.
This talk will discuss methods by which current statistical approaches
to spoken dialogue can be extended to cover much wider domains. It
will be argued that unlike many other areas of machine learning, spoken
dialogue systems always have a user on-hand to provide supervision.
Hence spoken dialogue systems provide a unique opportunity to auto-
matically adapt on large quantities of speech data without the need for
costly annotation.
1
Proceedings of the SIGDIAL 2014 Conference, pages 260?262,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
The Parlance Mobile Application for Interactive Search in
English and Mandarin
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos,
Hugues Bouchard, Catherine Breslin, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Ga?i?, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Tim Potter, Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay,
Boris Villazon-Terrazas, Majid Yazdani, Steve Young and Yanchao Yu
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
We demonstrate a mobile application in
English and Mandarin to test and eval-
uate components of the Parlance di-
alogue system for interactive search un-
der real-world conditions.
1 Introduction
With the advent of evaluations ?in the wild?,
emphasis is being put on converting re-
search prototypes into mobile applications that
can be used for evaluation and data col-
lection by real users downloading the ap-
plication from the market place. This is
the motivation behind the work demonstrated
here where we present a modular framework
whereby research components from the Par-
lance project (Hastie et al., 2013) can be
plugged in, tested and evaluated in a mobile
environment.
The goal of Parlance is to perform inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
Cambridge, UK for Mandarin and San Fran-
cisco, USA for English. The scenario is that
Mandarin speaking tourists would be able to
download the application and use it to learn
about restaurants in English speaking towns
and cities.
2 System Architecture
Here, we adopt a client-server approach as il-
lustrated in Figure 1 for Mandarin and Figure
2 for English. The front end of the demon-
stration system is an Android application that
calls the Google Automatic Speech Recogni-
tion (ASR) API and sends the recognized user
utterance to a server running the Interaction
?Authors are in alphabetical order
Manager (IM), Spoken Language Understand-
ing (SLU) and Natural Language Generation
(NLG) components.
Figure 1: Overview of the Parlance Man-
darin mobile application system architecture
Figure 2: Overview of the Parlance En-
glish mobile application system architecture
extended to use the Yahoo API to populate
the application with additional restaurant in-
formation
When the user clicks the Start button, a di-
alogue session starts. The phone application
first connects to the Parlance server (via
the Java Socket Server) to get the initial sys-
tem greeting which it speaks via the Google
260
Text-To-Speech (TTS) API. After the system
utterance finishes the recognizer starts to lis-
ten for user input to send to the SLU compo-
nent. The SLU converts text into a semantic
interpretation consisting of a set of triples of
communicative function, attribute, and (op-
tionally) value1. Probabilities can be associ-
ated with candidate interpretations to reflect
uncertainty in either the ASR or SLU. The
SLU then passes the semantic interpretation
to the IM within the same server.
Chinese sentences are composed of strings of
characters without any space to mark words as
other languages do, for example:
In order to correctly parse and understand
Chinese sentences, Chinese word segmenta-
tions must be performed. To do this segmen-
tation, we use the Stanford Chinese word seg-
mentor2, which relies on a linear-chain condi-
tional random field (CRF) model and treats
word segmentation as a binary decision task.
The Java Socket Server then sends the seg-
mented Chinese sentence to the SLU on the
server.
The IM then selects a dialogue act, accesses
the database and in the case of English passes
back the list of restaurant identification num-
bers (ids) associated with the relevant restau-
rants. For the English demonstration system,
these restaurants are displayed on the smart
phone as seen in Figures 4 and 5. Finally,
the NLG component decides how best to re-
alise the restaurant descriptions and sends the
string back to the phone application for the
TTS to realise. The example output is illus-
trated in Figure 3 for Mandarin and Figure 4
for English.
As discussed above, the Parlance mobile
application can be used as a test-bed for com-
paring alternative techniques for various com-
ponents. Here we discuss two such compo-
nents: IM and NLG.
1This has been implemented for English; Mandarin
uses the rule-based Phoenix parser.
2http://nlp.stanford.edu/projects/chinese-
nlp.shtml
Figure 3: Screenshot and translation of the
Mandarin system
Figure 4: Screenshot of dialogue and the list
of recommended restaurants shown on a map
and in a list for English
2.1 Interaction Management
The Parlance Interaction Manager is based
on the partially observable Markov decision
process (POMDP) framework, where the sys-
tem?s decisions can be optimised via reinforce-
ment learning. The model adopted for Par-
lance is the Bayesian Update of Dialogue
State (BUDS) manager (Thomson and Young,
2010). This POMDP-based IM factors the di-
alogue state into conditionally dependent ele-
ments. Dependencies between these elements
can be derived directly from the dialogue on-
tology. These elements are arranged into a dy-
namic Bayesian network which allows for their
marginal probabilities to be updated during
the dialogue, comprising the belief state. The
belief state is then mapped into a smaller-scale
summary space and the decisions are optimised
using the natural actor critic algorithm. In the
Parlance application, hand-crafted policies
261
Figure 5: Screenshot of the recommended
restaurant for the English application
can be compared to learned ones.
2.2 Natural Language Generation
As mentioned above, the server returns the
string to be synthesised by the Google TTS
API. This mobile framework allows for testing
of alternative approaches to NLG. In particu-
lar, we are interested in comparing a surface re-
aliser that uses CRFs against a template-based
baseline. The CRFs take semantically anno-
tated phrase structure trees as input, which it
uses to keep track of rich linguistic contexts.
Our approach has been compared with a num-
ber of competitive state-of-the art surface real-
izers (Dethlefs et al., 2013), and can be trained
from example sentences with annotations of se-
mantic slots.
2.3 Local Search and Knowledge Base
For the English system, the domain database is
populated by the search Yahoo API (Bouchard
and Mika, 2013) with restaurants in San Fran-
sisco. These restaurant search results are
returned based on their longitude and lati-
tude within San Francisco for 5 main areas, 3
price categories and 52 cuisine types contain-
ing around 1,600 individual restaurants.
The Chinese database has been partially
translated from an English database for restau-
rants in Cambridge, UK and search is based
on 3 price categories, 5 areas and 35 cuisine
types having a total of 157 restaurants. Due
to the language-agnostic nature of the Par-
lance system, only the name and address
fields needed to be translated.
3 Future Work
Investigating application side audio compres-
sion and audio streaming over a mobile in-
ternet connection would enable further assess-
ment of the ASR and TTS components used
in the original Parlance system (Hastie et
al., 2013). This would allow for entire research
systems to be plugged directly into the mobile
interface without the use of third party ASR
and TTS.
Future work also involves developing a feed-
back mechanism for evaluation purposes that
does not put undue effort on the user and put
them off using the application. In addition,
this framework can be extended to leverage
hyperlocal and social information of the user
when displaying items of interest.
Acknowledgements
The research leading to this work was funded
by the EC FP7 programme FP7/2011-14
under grant agreement no. 287615 (PAR-
LANCE).
References
H. Bouchard and P. Mika. 2013. Interactive hy-
perlocal search API. Technical report, Yahoo
Iberia, August.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and
O. Lemon. 2013. Conditional Random Fields
for Responsive Surface Realisation Using Global
Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (ACL), Sofia, Bulgaria.
H. Hastie, M.A. Aufaure, P. Alexopoulos,
H. Cuay?huitl, N. Dethlefs, M. Gasic,
J. Henderson, O. Lemon, X. Liu, P. Mika,
N. Ben Mustapha, V. Rieser, B. Thomson,
P. Tsiakoulis, Y. Vanrompay, B. Villazon-
Terrazas, and S. Young. 2013. Demonstration
of the PARLANCE system: a data-driven
incremental, spoken dialogue system for in-
teractive search. In Proceedings of the 14th
Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL), Metz,
France, August.
B. Thomson and S. Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework
for spoken dialogue systems. Computer Speech
and Language, 24(4):562?588.
262
Proceedings of the SIGDIAL 2014 Conference, pages 292?299,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Word-Based Dialog State Tracking
with Recurrent Neural Networks
Matthew Henderson, Blaise Thomson and Steve Young
Department of Engineering,
University of Cambridge, U.K.
{mh521, brmt2, sjy}@eng.cam.ac.uk
Abstract
Recently discriminative methods for track-
ing the state of a spoken dialog have been
shown to outperform traditional generative
models. This paper presents a new word-
based tracking method which maps di-
rectly from the speech recognition results
to the dialog state without using an explicit
semantic decoder. The method is based on
a recurrent neural network structure which
is capable of generalising to unseen dialog
state hypotheses, and which requires very
little feature engineering. The method
is evaluated on the second Dialog State
Tracking Challenge (DSTC2) corpus and
the results demonstrate consistently high
performance across all of the metrics.
1 Introduction
While communicating with a user, statistical spo-
ken dialog systems must maintain a distribution
over possible dialog states in a process called di-
alog state tracking. This distribution, also called
the belief state, directly determines the system?s
decisions. In MDP-based systems, only the most
likely dialog state is considered and in this case
the primary metric is dialog state accuracy (Bo-
hus and Rudnicky, 2006). In POMDP-based sys-
tems, the full distribution is considered and then
the shape of the distribution as measured by an L2
norm is equally important (Young et al., 2009). In
both cases, good quality state tracking is essential
to maintaining good overall system performance.
Typically, state tracking has assumed the output
of a Spoken Language Understanding (SLU) com-
ponent in the form of a semantic decoder, which
maps the hypotheses from Automatic Speech
Recognition (ASR) to a list of semantic hypothe-
ses. This paper considers mapping directly from
ASR hypotheses to an updated belief state at each
turn in the dialog, omitting the intermediate SLU
processing step. This word-based state tracking
avoids the need for an explicit semantic represen-
tation and also avoids the possibility of informa-
tion loss at the SLU stage.
Recurrent neural networks (RNNs) provide a
natural model for state tracking in dialog, as
they are able to model and classify dynamic se-
quences with complex behaviours from step to
step. Whereas, most previous approaches to dis-
criminative state tracking have adapted station-
ary classifiers to the temporal process of dialog
(Bohus and Rudnicky, 2006; Lee and Eskenazi,
2013; Lee, 2013; Williams, 2013; Henderson et
al., 2013b). One notable exception is Ren et al.
(2013), which used conditional random fields to
model the sequence temporally.
Currently proposed methods of discriminative
state tracking require engineering of feature func-
tions to represent the turn in the dialog (Ren et
al., 2013; Lee and Eskenazi, 2013; Lee, 2013;
Williams, 2013; Henderson et al., 2013b). It is un-
clear whether differences in performance are due
to feature engineering or the underlying models.
This paper proposes a method of using simple n-
gram type features which avoid the need for fea-
ture engineering. Instead of using inputs with a se-
lect few very informative features, the approach is
to use high-dimensional inputs with all the infor-
mation to potentially reconstruct any such hand-
crafted feature. The impact of significantly in-
creasing the dimensionality of the inputs is man-
aged by careful initialisation of model parameters.
Accuracy on unseen or infrequent slot values
is an important concern, particularly for discrim-
inative classifiers which are prone to overfitting
training data. This is addressed by structuring
the recurrent neural network to include a compo-
nent which is independent of the actual slot value
in question. It thus learns general behaviours for
specifying slots enabling it to successfully decode
292
ASR output which includes previously unseen slot
values.
In summary, this paper presents a word-based
approach to dialog state tracking using recurrent
neural networks. The model is capable of gen-
eralising to unseen dialog state hypotheses, and
requires very little feature engineering. The ap-
proach is evaluated in the second Dialog State
Tracking Challenge (DSTC2) (Henderson et al.,
2014) where it is shown to be extremely competi-
tive, particularly in terms of the quality of its con-
fidence scores.
Following a brief outline of DSTC2 in section
2, the definition of the model is given in section
3. Section 4 then gives details on the initialisation
methods used for training. Finally results on the
DSTC2 evaluation are given in 5.
2 The Second Dialog State Tracking
Challenge
This section describes the domain and method-
ology of the second Dialog State Tracking Chal-
lenge (DSTC2). The challenge is based on a
large corpus collected using a variety of telephone-
based dialog systems in the domain of finding a
restaurant in Cambridge. In all cases, the subjects
were recruited using Amazon Mechanical Turk.
The data is split into a train, dev and test set.
The train and dev sets were supplied with labels,
and the test set was released unlabelled for a one
week period. At the end of the week, all partici-
pants were required to submit their trackers? out-
put on the test set, and the labels were revealed. A
mis-match was ensured between training and test-
ing conditions by choosing dialogs for the eval-
uation collected using a separate dialog manager.
This emulates the mis-match a new tracker would
encounter if it were actually deployed in an end-
to-end system.
In summary, the datasets used are:
? dstc2 train - Labelled training consisting of
1612 dialogs with two dialog managers and
two acoustic conditions.
? dstc2 dev - Labelled dataset consisting
of 506 calls in the same conditions as
dstc2 train, but with no caller in common.
? dstc2 test - Evaluation dataset consisting of
1117 dialogs collected using a dialog man-
ager not seen in the labelled data.
In contrast with DSTC1, DSTC2 introduces dy-
namic user goals, tracking of requested slots and
tracking the restaurant search method. A DSTC2
tracker must therefore report:
? Goals: A distribution over the user?s goal for
each slot. This is a distribution over the possi-
ble values for that slot, plus the special value
None, which means no valid value has been
mentioned yet.
? Requested slots: A reported probability for
each requestable slot that has been requested
by the user, and should be informed by the
system.
? Method: A distribution over methods, which
encodes how the user is trying to use the di-
alog system. E.g. ?by constraints?, when the
user is trying to constrain the search, and ?fin-
ished?, when the user wants to end the dialog.
A tracker may report the goals as a joint over
all slots, but in this paper the joint is reported as a
product of the marginal distributions per slot.
Full details of the challenge are given in Hen-
derson et al. (2013a), Henderson et al. (2014). The
trackers presented in this paper are identified un-
der ?team4? in the reported results.
3 Recurrent Neural Network Model
This section defines the RNN structure used for
dialog state tracking. One such RNN is used per
slot, taking the most recent dialog turn (user input
plus last machine dialog act) as input, updating its
internal memory and calculating an updated belief
over the values for the slot. In what follows, the
notation a?b is used to denote the concatenation
of two vectors, a and b. The i
th
component of the
vector a is written a|
i
.
3.1 Feature Representation
Extracting n-grams from utterances and dialog
acts provides the feature representations needed
for input into the RNN. This process is very sim-
ilar to the feature extraction described in Hender-
son et al. (2012), and is outlined in figure 1.
For n-gram features extracted from the ASR
N -best list, unigram, bigram and trigram features
are calculated for each hypothesis. These are
then weighted by the N -best list probabilities and
summed to give a single vector.
Dialog acts in this domain consist of
a list of component acts of the form
acttype(slot=value) where the slot=value
pair is optional. The n-gram type features
293
extracted from each such component act are
?acttype?, ?slot?, ?value?, ?acttype
slot?, ?slot value? and ?acttype slot
value?, or just ?acttype? for the act acttype().
Each feature is given weight 1, and the features
from individual component acts are summed.
To provide a contrast, trackers have also been
implemented using the user dialog acts output by
an SLU rather than directly from the ASR output.
In this case, the SLU N -best dialog act list is en-
coded in the same way except that the n-grams
from each hypothesis are weighted by the corre-
sponding probabilities, and summed to give a sin-
gle feature vector.
Consider a word-based tracker which takes an
ASR N -best list and the last machine act as input
for each turn, as shown in figure 1. A combined
feature representation of both the ASR N -best list
and the last machine act is obtained by concate-
nating the vectors. This means that in figure 1 the
food feature from the ASR and the food feature
from the machine act contribute to separate com-
ponents of the final vector f .
fv
ASR
foodjamaican
indian food
1.00.9
0.1
<value> food<value> 0.90.9
Machine Act
confirm food
confirm food jamaican
food jamaican
1.0
1.0
1.0
e.g. v = jamaican
confirm food <value>
food <value>
1.0
1.0
for each value, v
jamaican food 0.9
<slot>
<value> 
1.01.0<value> <slot> 1.0
confirm 1.0
<value> 1.0
confirm <slot> <value>
<slot> <value>
1.0
1.0
<value> 1.0
indian 0.1
<value> food 1.0jamaican <slot> 0.9indian <slot> 0.1
confirm food <value> 1.0
food <value> 1.0
confirm <slot> jamaican 1.0
<slot> jamaican 1.0
0.9jamaican food 0.1indian food confirm(food=jamaican)
food 1.0
<slot> 1.0
fs
f
5 non-zero elements
6 non-zero elements
2 non-zero elements
6 non-zero elements
8 non-zero elements
3 non-zero elements
11 non-zero elements
14 non-zero elements
5 non-zero elements
jamaican 1.0
Figure 1: Example of feature extraction for one
turn, giving f , f
s
and f
v
. Here s = food. For all
v /?{indian, jamaican}, f
v
= 0.
Note that all the methods for tracking reported
in DSTC1 required designing feature functions.
For example, suggested feature functions included
the SLU score in the current turn, the probabil-
ity of an ?affirm? act when the value has been
confirmed by the system, the output from base-
line trackers etc. (e.g. Lee and Eskenazi (2013),
Williams (2013), Henderson et al. (2013b)). In
contrast, the approach described here is to present
the model with all the information it would need
to reconstruct any feature function that might be
useful.
3.2 Generalisation to Unseen States
One key issue in applying machine learning to the
task of dialog state tracking is being able to deal
with states which have not been seen in training.
For example, the system should be able to recog-
nise any obscure food type which appears in the
set of possible food types. A na??ve neural net-
work structure mapping n-gram features to an up-
dated distribution for the food slot, with no tying
of weights, would require separate examples of
each of the food types to learn what n-grams are
associated with each. In reality however n-grams
like ?<value> food? and ?serving <value>? are likely
to correspond to the hypothesis food=?<value>? for
any food-type replacing ?<value>?.
The approach taken here is to embed a network
which learns a generic model of the updated belief
of a slot-value assignment as a function of ?tagged?
features, i.e. features which ignore the specific
identity of a value. This can be considered as re-
placing all occurrences of a particular value with
a tag like ?<value>?. Figure 1 shows the process of
creating the tagged feature vectors, f
s
and f
v
from
the untagged vector f .
3.3 Model Definition
In this section an RNN is described for tracking
the goal for a given slot, s, throughout the se-
quence of a dialog. The RNN holds an internal
memory, m ? R
N
mem
which is updated at each
step. If there are N possible values for slot s, then
the probability distribution output p is in R
N+1
,
with the last component p|
N
giving the probabil-
ity of the None hypothesis. Figure 2 provides an
overview of how p and m are updated in one turn
to give the new belief and memory, p
?
and m
?
.
One part of the neural network is used to learn
a mapping from the untagged inputs, full memory
and previous beliefs to a vector h ? R
N
which
goes directly into the calculation of p
?
:
h = NNet (f ? p?m) ? R
N
294
p m
hN. Net.
g v
p v
N. Net.for each value, v
h+g
p?softmax m?logisticfor each slot,  s
f
fs
fv
pN
Figure 2: Calculation of p
?
and m
?
for one turn
where NNet(?) denotes a neural network function
of the input. In this paper all such networks have
one hidden layer with a sigmoidal activation func-
tion.
The sub-network for h requires examples of ev-
ery value in training, and is prone to poor general-
isation as explained in section 3.2. By including a
second sub-network for g which takes tagged fea-
tures as input, it is possible to exploit the obser-
vation that the string corresponding to a value in
various contexts is likely to be good evidence for
or against that value. For each value v, a compo-
nent of g is calculated using the neural network:
g|
v
= NNet
(
f? f
s
? f
v
?
{p|
v
, p|
N
} ?m
)
? R
By using regularisation, the learning will pre-
fer where possible to use the sub-network for g
rather than learning the individual weights for
each value required in the sub-network for h. This
sub-network is able to deal with unseen or infre-
quently seen dialog states, so long as the state can
be tagged in the feature extraction. This model can
also be shared across slots since f
s
is included as
an input, see section 4.2.
The sub-networks applied to tagged and un-
tagged inputs are combined to give the new belief:
p
?
= softmax ([h + g]? {B}) ? R
N+1
where B is a parameter of the RNN, contributing
to the None hypothesis. The contribution from g
may be seen as accounting for general behaviour
of tagged hypotheses, while h makes corrections
due to correlations with untagged features and
value specific behaviour e.g. special ways of ex-
pressing specific goals and fitting to specific ASR
confusions.
Finally, the memory is updated according to the
logistic regression:
m
?
= ? (W
m
0
f +W
m
1
m) ? R
N
mem
where the W
m
i
are parameters of the RNN.
3.4 Requested Slots and Method
A similar RNN is used to track the requested slots.
Here the v runs over all the requestable slots, and
requestable slot names are tagged in the feature
vectors f
v
. This allows the neural network calcu-
lating g to learn general patterns across slots just
as in the case of goals. The equation for p
?
is
changed to:
p
?
= ? (h + g)
so each component of p
?
represents the probability
(between 0 and 1) of a slot being requested.
For method classification, the same RNN struc-
ture as for a goal is used. No tagging of the feature
vectors is used in the case of methods.
4 Training
The RNNs are trained using Stochastic Gradient
Descent (SGD), maximizing the log probability of
the sequences of observed beliefs in the training
data (Bottou, 1991). Gradient clipping is used to
avoid the problem of exploding gradients (Pascanu
et al., 2012). A regularisation term is included,
which penalises the l2 norm of all the parameters.
It is found empirically to be beneficial to give more
weight in the regularisation to the parameters used
in the network calculating h.
When using the ASR N -best list, f is typi-
cally of dimensionality around 3500. With so
many weights to learn, it is important to initialise
the parameters well before starting the SGD algo-
rithm. Two initialisation techniques have been in-
vestigated, the denoising autoencoder and shared
initialisation. These were evaluated by training
trackers on the dstc2 train set, and evaluating on
dstc2 dev (see table 1).
4.1 Denoising Autoencoder
The denoising autoencoder (dA), which provides
an unsupervised method for learning meaningful
295
Joint Goals Method Requested
Shared
init.
dA
init.
Acc L2 Acc L2 Acc L2
0.686 0.477 0.913 0.147 0.963 0.059
X 0.688 0.466 0.915 0.144 0.962 0.059
X 0.680 0.479 0.910 0.152 0.962 0.059
X X 0.696 0.463 0.915 0.144 0.965 0.057
Baseline: 0.612 0.632 0.830 0.266 0.894 0.174
Table 1: Performance on the dev set when varying initialisation techniques for word-based tracking. Acc
denotes the accuracy of the most likely belief at each turn, and L2 denotes the squared l2 norm between
the estimated belief distribution and correct (delta) distribution. For each row, 5 trackers are trained
and then combined using score averaging. The final row shows the results for the focus-based baseline
tracker (Henderson et al., 2014).
underlying representations of the input, has been
found effective as an initialisation technique in
deep learning (Vincent et al., 2008).
A dA is used to initialise the parameters of the
RNN which multiply the high-dimensional input
vector f . The dA learns a matrix W
dA
which re-
duces f to a lower dimensional vector such that
the original vector may be recovered with minimal
loss in the presence of noise.
For learning the dA, f is first mapped such that
feature values lie between 0 and 1. The dA takes as
input f
noisy
, a noisy copy of f where each compo-
nent is set to 0 with probability p. This is mapped
to a lower dimensional hidden representation h:
h = ? (W
dA
f
noisy
+ b
0
)
A reconstructed vector, f
rec
, is then calculated
as:
f
rec
= ?
(
W
T
dA
h+ b
1
)
The cross-entropy between f and f
rec
is used as
the objective function in gradient descent, with an
added l1 regularisation term to ensure the learning
of sparse weights. As the ASR features are likely
to be very noisy, dense weights would be prone to
overfitting the examples.
1
When using W
dA
to initialise weights in the
RNN, training is observed to converge faster. Ta-
ble 1 shows that dA initialisation leads to better
solutions, particularly for tracking the goals.
4.2 Shared Initialisation
It is possible to train a slot-independent RNN, us-
ing training data from all slots, by not including h
in the model (the dimensionality of h is dependent
1
The state-of-the-art in dialog act classification with very
similar data also uses sparse weights Chen et al. (2013).
on the slot). In shared initialisation, such an RNN
is trained for a few epochs, then the learnt param-
eters are used to initialise slot-dependent RNNs
for each slot. This follows the shared initialisation
procedure presented in Henderson et al. (2013b).
Table 1 suggests that shared initialisation when
combined with dA initialisation gives the best per-
formance.
4.3 Model Combination
In DSTC1, the most competitive results were
achieved with model combination whereby the
output of multiple trackers were combined to give
more accurate classifications (Lee and Eskenazi,
2013). The technique for model combination used
here is score averaging, where the final score for
each component of the dialog state is computed as
the mean of the scores output by all the trackers
being combined. This is one of the simplest meth-
ods for model combination, and requires no extra
training data. It is guaranteed to improve the accu-
racy if the outputs from the individual trackers are
not correlated, and the individual trackers operate
at an accuracy > 0.5.
Multiple runs of training the RNNs were found
to give results with high variability and model
combination provides a method to exploit this
variability. In order to demonstrate the effect,
10 trackers with varying regularisation parame-
ters were trained on dstc2 train and used to track
dstc2 dev. Figure 3 shows the effects of combin-
ing these trackers in larger groups. The mean ac-
curacy in the joint goals from combining m track-
ers is found to increase with m. The single output
from combining all 10 trackers outperforms any
single tracker in the group.
The approach taken for the DSTC2 challenge
was therefore to train multiple trackers with vary-
296
Accuracy
# trackers combined, m
1 2 3 4 5 6 7 8 9 10
0.64
0.65
0.66
0.67
0.68
0.69
0.70
0.71
0.72
Figure 3: Joint goal accuracy on dstc2 dev from system
combination. Ten total trackers are trained with varying reg-
ularisation parameters. For each m = 1 . . . 10, all subsets
of size m of the 10 trackers are used to generate
10
C
m
com-
bined results, which are plotted as a boxplot. Boxplots show
minimum, maximum, the interquartile range and the median.
The mean values are plotted as connected points.
ing model hyper-parameters (e.g. regularisation
parameters, memory size) and combine their out-
put using score averaging. Note that maintaining
around 10 RNNs for each dialog state components
is entirely feasible for a realtime system, as the
RNN operations are quick to compute. An un-
optimised Python implementation of the tracker
including an RNN for each dialog state compo-
nent is able to do state tracking at a rate of around
50 turns per second on an Intel? Core? i7-970
3.2GHz processor.
5 Results
The strict blind evaluation procedure defined for
the DSTC2 challenge was used to investigate the
effect on performance of two contrasts. The first
contrast compares word-based tracking and con-
ventional tracking based on SLU output. The sec-
ond contrast investigates the effect of including
and omitting the sub-network for h in the RNN.
Recall h is the part of the model that allows learn-
ing special behaviours for particular dialog state
hypotheses, and correlations with untagged fea-
tures. These two binary contrasts resulted in a to-
tal of 4 system variants being entered in the chal-
lenge.
Each system is the score-averaged combined
output of 12 trackers trained with varying hyper-
parameters (see section 4.3). The performance of
the 4 entries on the featured metrics of the chal-
lenge are shown in table 2.
It should be noted that the live SLU used the
word confusion network, not made available in the
challenge. The word confusion network is known
to provide stronger features than theN -best list for
language understanding (Henderson et al., 2012;
T?ur et al., 2013), so the word-based trackers us-
ing N -best ASR features were at a disadvantage
in that regard. Nevertheless, despite this hand-
icap, the best results were obtained from word-
based tracking directly on the ASR output, rather
than using the confusion network generated SLU
output. Including h always helps, though this is
far more pronounced for the word-based track-
ers. Note that trackers which do not include h are
value-independent and so are capable of handling
new values at runtime.
The RNN trackers performed very competi-
tively in the context of the challenge. Figure 4 vi-
sualises the performance of the four trackers rela-
tive to all the entries submitted to the challenge for
the featured metrics. For full details of the evalua-
tion metrics see Henderson et al. (2014). The box
in this figure gives the entry IDs under which the
results are reported in the DSTC (under the team
ID ?team4?). The word-based tracker including
h (h-ASR), was top for joint goals L2 as well as
requested slots accuracy and L2. It was close to
the top for the other featured metrics, following
closely entries from team 2. The RNN trackers
performed particularly well on measures assessing
the quality of the scores such as L2.
There are hundreds of numbers reported in the
DSTC2 evaluation, and it was found that the h-
ASR tracker ranked top on many of them. Consid-
ering L2, accuracy, average probability, equal er-
ror rate, log probability and mean reciprocal rank
across all components of the the dialog state, these
give a total of 318 metrics. The h-ASR tracker
ranked top of all trackers in the challenge in 89 of
these metrics, more than any other tracker. The
ASR tracker omitting h came second, ranking top
in 33 of these metrics.
The trackers using SLU features ranked top
in all of the featured metrics among the trackers
which used only the SLU output.
6 Conclusions
The RNN framework presented in this paper pro-
vides very good performance in terms of both ac-
curacy and the quality of reported probability dis-
tributions. Word-based tracking is shown to be one
of the most competitive approaches submitted to
DSTC2. By mapping straight from the ASR out-
put to a belief update, it avoids any information
297
Tracker
Inputs
Joint Goals Method Requested
entry
Include
h
Live
ASR
Live
SLU
Acc L2 ROC Acc L2 ROC Acc L2 ROC
0 X X 0.768 0.346 0.365 0.940 0.095 0.452 0.978 0.035 0.525
1 X 0.746 0.381 0.383 0.939 0.097 0.423 0.977 0.038 0.490
2 X X 0.742 0.387 0.345 0.922 0.124 0.447 0.957 0.069 0.340
3 X 0.737 0.406 0.321 0.922 0.125 0.406 0.957 0.073 0.385
Table 2: Featured metrics on the test set for the 4 RNN trackers entered to the challenge.
0.4
1.0
0.6
0.8
0.0
0.8
Accuracy
Joint Goals Method Requested All
0.2
0.4
0.6
L2
entry0
entry2
entry1
entry3
word-based
SLU input
full model no h
baseline
Figure 4: Relative performance of RNN trackers for fea-
tured metrics in DSTC2. Each dash is one of the 34 trackers
evaluated in the challenge. Note a lower L2 is better. ROC
metric is only comparable for systems of similar accuracies,
so is not plotted. The focus baseline system is shown as a
circle.
lost in the omitted SLU step.
In general, the RNN appears to be a promising
model, which deals naturally with sequential input
and outputs. High dimensional inputs are handled
well, with little feature engineering, particularly
when carefully initialised (e.g. as here using de-
noising autoencoders and shared initialisation).
Future work should include making joint pre-
dictions on components of the dialog state. In this
paper each component was tracked using its own
RNN. Though not presented in this paper, no im-
provement could be found by joining the RNNs.
However, this may not be the case for other do-
mains in which slot values are more highly cor-
related. The concept of tagging the feature func-
tions allows for generalisation to unseen values
and slots. This generalisation will be explored in
future work, particularly for dialogs in more open-
domains.
Acknowledgements
Matthew Henderson is a Google Doctoral Fellow.
References
Dan Bohus and Alex Rudnicky. 2006. A K-
hypotheses+ Other Belief Updating Model. Proc.
of the AAAI Workshop on Statistical and Empirical
Methods in Spoken Dialogue Systems.
L?eon Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of Neuro-N??mes
91, N??mes, France. EC2.
Yun-Nung Chen, William Yang Wang, and Alexan-
der I Rudnicky. 2013. An empirical investigation of
sparse log-linear models for improved dialogue act
classification. In Acoustics, Speech and Signal Pro-
cessing (ICASSP), 2013 IEEE International Confer-
ence on.
Matthew Henderson, Milica Ga?si?c, Blaise Thom-
son, Pirros Tsiakoulis, Kai Yu, and Steve Young.
2012. Discriminative Spoken Language Under-
standing Using Word Confusion Networks. In Spo-
ken Language Technology Workshop, 2012. IEEE.
298
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2013a. Dialog State Tracking Challenge
2 & 3 Handbook. camdial.org/
?
mh521/dstc/.
Matthew Henderson, Blaise Thomson, and Steve
Young. 2013b. Deep Neural Network Approach for
the Dialog State Tracking Challenge. In Proceed-
ings of SIGdial, Metz, France, August.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2014. The second dialog state tracking
challenge. In Proceedings of the SIGdial 2014 Con-
ference, Baltimore, U.S.A., June.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference, Metz,
France, August.
Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In Proceedings of the SIG-
DIAL 2013 Conference, Metz, France, August.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2012. Understanding the exploding gradient prob-
lem. CoRR.
Hang Ren, Weiqun Xu, Yan Zhang, and Yonghong Yan.
2013. Dialog state tracking using conditional ran-
dom fields. In Proceedings of the SIGDIAL 2013
Conference, Metz, France, August.
G?okhan T?ur, Anoop Deoras, and Dilek Hakkani-T?ur.
2013. Semantic parsing using word confusion net-
works with conditional random fields. In INTER-
SPEECH.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th International
Conference on Machine Learning, Helsinki, Fin-
land.
Jason Williams. 2013. Multi-domain learning and gen-
eralization in dialog state tracking. In Proceedings
of the SIGDIAL 2013 Conference, Metz, France, Au-
gust.
Steve Young, Milica Ga?si?c, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2009. The Hidden Information State model:
A practical framework for POMDP-based spoken
dialogue management. Computer Speech & Lan-
guage.
299
