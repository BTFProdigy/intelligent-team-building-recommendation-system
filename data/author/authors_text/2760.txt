Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 141?150, Prague, June 2007. c?2007 Association for Computational Linguistics
Structured Prediction Models via the Matrix-Tree Theorem
Terry Koo, Amir Globerson, Xavier Carreras and Michael Collins
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,gamir,carreras,mcollins}@csail.mit.edu
Abstract
This paper provides an algorithmic frame-
work for learning statistical models involv-
ing directed spanning trees, or equivalently
non-projective dependency structures. We
show how partition functions and marginals
for directed spanning trees can be computed
by an adaptation of Kirchhoff?s Matrix-Tree
Theorem. To demonstrate an application of
the method, we perform experiments which
use the algorithm in training both log-linear
and max-margin dependency parsers. The
new training methods give improvements in
accuracy over perceptron-trained models.
1 Introduction
Learning with structured data typically involves
searching or summing over a set with an exponen-
tial number of structured elements, for example the
set of all parse trees for a given sentence. Meth-
ods for summing over such structures include the
inside-outside algorithm for probabilistic context-
free grammars (Baker, 1979), the forward-backward
algorithm for hidden Markov models (Baum et
al., 1970), and the belief-propagation algorithm for
graphical models (Pearl, 1988). These algorithms
compute marginal probabilities and partition func-
tions, quantities which are central to many meth-
ods for the statistical modeling of complex struc-
tures (e.g., the EM algorithm (Baker, 1979; Baum
et al, 1970), contrastive estimation (Smith and Eis-
ner, 2005), training algorithms for CRFs (Lafferty et
al., 2001), and training algorithms for max-margin
models (Bartlett et al, 2004; Taskar et al, 2004a)).
This paper describes inside-outside-style algo-
rithms for the case of directed spanning trees. These
structures are equivalent to non-projective depen-
dency parses (McDonald et al, 2005b), and more
generally could be relevant to any task that involves
learning a mapping from a graph to an underlying
spanning tree. Unlike the case for projective depen-
dency structures, partition functions and marginals
for non-projective trees cannot be computed using
dynamic-programming methods such as the inside-
outside algorithm. In this paper we describe how
these quantities can be computed by adapting a well-
known result in graph theory: Kirchhoff?s Matrix-
Tree Theorem (Tutte, 1984). A na??ve application of
the theorem yields O(n4) and O(n6) algorithms for
computation of the partition function and marginals,
respectively. However, our adaptation finds the par-
tition function and marginals in O(n3) time using
simple matrix determinant and inversion operations.
We demonstrate an application of the new infer-
ence algorithm to non-projective dependency pars-
ing. Specifically, we show how to implement
two popular supervised learning approaches for this
task: globally-normalized log-linear models and
max-margin models. Log-linear estimation criti-
cally depends on the calculation of partition func-
tions and marginals, which can be computed by
our algorithms. For max-margin models, Bartlett
et al (2004) have provided a simple training al-
gorithm, based on exponentiated-gradient (EG) up-
dates, that requires computation of marginals and
can thus be implemented within our framework.
Both of these methods explicitly minimize the loss
incurred when parsing the entire training set. This
contrasts with the online learning algorithms used in
previous work with spanning-tree models (McDon-
ald et al, 2005b).
We applied the above two marginal-based train-
ing algorithms to six languages with varying de-
grees of non-projectivity, using datasets obtained
from the CoNLL-X shared task (Buchholz and
Marsi, 2006). Our experimental framework com-
pared three training approaches: log-linear models,
max-margin models, and the averaged perceptron.
Each of these was applied to both projective and
non-projective parsing. Our results demonstrate that
marginal-based training yields models which out-
141
perform those trained using the averaged perceptron.
In summary, the contributions of this paper are:
1. We introduce algorithms for inside-outside-
style calculations for directed spanning trees, or
equivalently non-projective dependency struc-
tures. These algorithms should have wide
applicability in learning problems involving
spanning-tree structures.
2. We illustrate the utility of these algorithms in
log-linear training of dependency parsing mod-
els, and show improvements in accuracy when
compared to averaged-perceptron training.
3. We also train max-margin models for depen-
dency parsing via an EG algorithm (Bartlett
et al, 2004). The experiments presented here
constitute the first application of this algorithm
to a large-scale problem. We again show im-
proved performance over the perceptron.
The goal of our experiments is to give a rigorous
comparative study of the marginal-based training al-
gorithms and a highly-competitive baseline, the av-
eraged perceptron, using the same feature sets for
all approaches. We stress, however, that the purpose
of this work is not to give competitive performance
on the CoNLL data sets; this would require further
engineering of the approach.
Similar adaptations of the Matrix-Tree Theorem
have been developed independently and simultane-
ously by Smith and Smith (2007) andMcDonald and
Satta (2007); see Section 5 for more discussion.
2 Background
2.1 Discriminative Dependency Parsing
Dependency parsing is the task of mapping a sen-
tence x to a dependency structure y. Given a sen-
tence x with n words, a dependency for that sen-
tence is a tuple (h,m) where h ? [0 . . . n] is the
index of the head word in the sentence, and m ?
[1 . . . n] is the index of a modifier word. The value
h = 0 is a special root-symbol that may only ap-
pear as the head of a dependency. We use D(x) to
refer to all possible dependencies for a sentence x:
D(x) = {(h,m) : h ? [0 . . . n],m ? [1 . . . n]}.
A dependency parse is a set of dependencies
that forms a directed tree, with the sentence?s root-
symbol as its root. We will consider both projective
Projective Non-projective
Single
Root 1 30 2Heroot saw her 1 30 2Heroot saw her
Multi
Root 1 30 2Heroot saw her 1 30 2Heroot saw her
Figure 1: Examples of the four types of dependency struc-
tures. We draw dependency arcs from head to modifier.
trees, where dependencies are not allowed to cross,
and non-projective trees, where crossing dependen-
cies are allowed. Dependency annotations for some
languages, for example Czech, can exhibit a signifi-
cant number of crossing dependencies. In addition,
we consider both single-root and multi-root trees. In
a single-root tree y, the root-symbol has exactly one
child, while in a multi-root tree, the root-symbol has
one or more children. This distinction is relevant
as our training sets include both single-root corpora
(in which all trees are single-root structures) and
multi-root corpora (in which some trees are multi-
root structures).
The two distinctions described above are orthog-
onal, yielding four classes of dependency structures;
see Figure 1 for examples of each kind of structure.
We use T sp (x) to denote the set of all possible pro-
jective single-root dependency structures for a sen-
tence x, and T snp(x) to denote the set of single-root
non-projective structures for x. The sets T mp (x) and
T mnp (x) are defined analogously for multi-root struc-
tures. In contexts where any class of dependency
structures may be used, we use the notation T (x) as
a placeholder that may be defined as T sp (x), T
s
np(x),
T mp (x) or T
m
np (x).
Following McDonald et al (2005a), we use a dis-
criminative model for dependency parsing. Fea-
tures in the model are defined through a function
f(x, h,m) which maps a sentence x together with
a dependency (h,m) to a feature vector in Rd. A
feature vector can be sensitive to any properties of
the triple (x, h,m). Given a parameter vector w,
the optimal dependency structure for a sentence x is
y?(x;w) = argmax
y?T (x)
?
(h,m)?y
w ? f(x, h,m) (1)
where the set T (x) can be defined as T sp (x), T
s
np(x),
T mp (x) or T
m
np (x), depending on the type of parsing.
142
The parameters w will be learned from a train-
ing set {(xi, yi)}Ni=1 where each xi is a sentence and
each yi is a dependency structure. Much of the pre-
vious work on learningw has focused on training lo-
cal models (see Section 5). McDonald et al (2005a;
2005b) trained global models using online algo-
rithms such as the perceptron algorithm or MIRA.
In this paper we consider training algorithms based
on work in conditional random fields (CRFs) (Laf-
ferty et al, 2001) and max-margin methods (Taskar
et al, 2004a).
2.2 Three Inference Problems
This section highlights three inference problems
which arise in training and decoding discriminative
dependency parsers, and which are central to the ap-
proaches described in this paper.
Assume that we have a vector ? with values
?h,m ? R for all (h,m) ? D(x); these values cor-
respond to weights on the different dependencies in
D(x). Define a conditional distribution over all de-
pendency structures y ? T (x) as follows:
P (y |x;?) =
exp
{?
(h,m)?y ?h,m
}
Z(x;?)
(2)
Z(x;?) =
?
y?T (x)
exp
?
?
?
?
(h,m)?y
?h,m
?
?
?
(3)
The function Z(x;?) is commonly referred to as the
partition function.
Given the distribution P (y |x;?), we can define
the marginal probability of a dependency (h,m) as
?h,m(x;?) =
?
y?T (x) : (h,m)?y
P (y |x;?)
The inference problems are then as follows:
Problem 1: Decoding:
Find argmaxy?T (x)
?
(h,m)?y ?h,m
Problem 2: Computation of the Partition Func-
tion: Calculate Z(x;?).
Problem 3: Computation of the Marginals:
For all (h,m) ? D(x), calculate ?h,m(x;?).
Note that all three problems require a maximiza-
tion or summation over the set T (x), which is ex-
ponential in size. There is a clear motivation for
being able to solve Problem 1: by setting ?h,m =
w ? f(x, h,m), the optimal dependency structure
y?(x;w) (see Eq. 1) can be computed. In this paper
the motivation for solving Problems 2 and 3 arises
from training algorithms for discriminative models.
As we will describe in Section 4, both log-linear and
max-margin models can be trained via methods that
make direct use of algorithms for Problems 2 and 3.
In the case of projective dependency structures
(i.e., T (x) defined as T sp (x) or T
m
p (x)), there are
well-known algorithms for all three inference prob-
lems. Decoding can be carried out using Viterbi-
style dynamic-programming algorithms, for exam-
ple the O(n3) algorithm of Eisner (1996). Com-
putation of the marginals and partition function can
also be achieved in O(n3) time, using a variant of
the inside-outside algorithm (Baker, 1979) applied
to the Eisner (1996) data structures (Paskin, 2001).
In the non-projective case (i.e., T (x) defined as
T snp(x) or T
m
np (x)), McDonald et al (2005b) de-
scribe how the CLE algorithm (Chu and Liu, 1965;
Edmonds, 1967) can be used for decoding. How-
ever, it is not possible to compute the marginals
and partition function using the inside-outside algo-
rithm. We next describe a method for computing
these quantities in O(n3) time using matrix inverse
and determinant operations.
3 Spanning-tree inference using the
Matrix-Tree Theorem
In this section we present algorithms for computing
the partition function and marginals, as defined in
Section 2.2, for non-projective parsing. We first re-
iterate the observation of McDonald et al (2005a)
that non-projective parses correspond to directed
spanning trees on a complete directed graph of n
nodes, where n is the length of the sentence. The
above inference problems thus involve summation
over the set of all directed spanning trees. Note that
this set is exponentially large, and there is no obvi-
ous method for decomposing the sum into dynamic-
programming-like subproblems. This section de-
scribes how a variant of Kirchhoff?s Matrix-Tree
Theorem (Tutte, 1984) can be used to evaluate the
partition function and marginals efficiently.
In what follows, we consider the single-root set-
ting (i.e., T (x) = T snp(x)), leaving the multi-root
143
case (i.e., T (x) = T mnp (x)) to Section 3.3. For a
sentence x with n words, define a complete directed
graph G on n nodes, where each node corresponds
to a word in x, and each edge corresponds to a de-
pendency between two words in x. Note thatG does
not include the root-symbol h = 0, nor does it ac-
count for any dependencies (0,m) headed by the
root-symbol. We assign non-negative weights to the
edges of this graph, yielding the following weighted
adjacency matrix A(?) ? Rn?n, for h,m = 1 . . . n:
Ah,m(?) =
{
0, if h = m
exp {?h,m} , otherwise
To account for the dependencies (0,m) headed by
the root-symbol, we define a vector of root-selection
scores r(?) ? Rn, form = 1 . . . n:
rm(?) = exp {?0,m}
Let the weight of a dependency structure y ? T snp(x)
be defined as:
?(y;?) = rroot(y)(?)
?
(h,m)?y : h 6=0
Ah,m(?)
Here, root(y) = m : (0,m) ? y is the child of the
root-symbol; there is exactly one such child, since
y ? T snp(x). Eq. 2 and 3 can be rephrased as:
P (y |x;?) =
?(y;?)
Z(x;?)
(4)
Z(x;?) =
?
y?T snp(x)
?(y;?) (5)
In the remainder of this section, we drop the nota-
tional dependence on x for brevity.
The original Matrix-Tree Theorem addressed the
problem of counting the number of undirected span-
ning trees in an undirected graph. For the models
we study here, we require a sum of weighted and
directed spanning trees. Tutte (1984) extended the
Matrix-Tree Theorem to this case. We briefly sum-
marize his method below.
First, define the Laplacian matrix L(?) ? Rn?n
of G, for h,m = 1 . . . n:
Lh,m(?) =
{ ?n
h?=1Ah?,m(?) if h = m
?Ah,m(?) otherwise
Second, for a matrix X , let X(h,m) be the minor of
X with respect to row h and column m; i.e., the
determinant of the matrix formed by deleting row h
and columnm fromX . Finally, define the weight of
any directed spanning tree of G to be the product of
the weights Ah,m(?) for the edges in that tree.
Theorem 1 (Tutte, 1984, p. 140). Let L(?) be the
Laplacian matrix of G. Then L(m,m)(?) is equal to
the sum of the weights of all directed spanning trees
of G which are rooted at m. Furthermore, the mi-
nors vary only in sign when traversing the columns
of the Laplacian (Tutte, 1984, p. 150):
?h,m : (?1)h+mL(h,m)(?) = L(m,m)(?) (6)
3.1 Partition functions via matrix determinants
From Theorem 1, it directly follows that
L(m,m)(?) =
?
y?U(m)
?
(h,m)?y : h 6=0
Ah,m(?)
where U(m) = {y ? T snp : root(y) = m}. A
na??ve method for computing the partition function is
therefore to evaluate
Z(?) =
n?
m=1
rm(?)L(m,m)(?)
The above would require calculating n determinants,
resulting in O(n4) complexity. However, as we
show below Z(?) may be obtained in O(n3) time
using a single determinant evaluation.
Define a newmatrix L?(?) to beL(?) with the first
row replaced by the root-selection scores:
L?h,m(?) =
{
rm(?) h = 1
Lh,m(?) h > 1
This matrix allows direct computation of the parti-
tion function, as the following proposition shows.
Proposition 1 The partition function in Eq. 5 is
given by Z(?) = |L?(?)|.
Proof: Consider the row expansion of |L?(?)| with
respect to row 1:
|L?(?)| =
n?
m=1
(?1)1+mL?1,m(?)L?(1,m)(?)
=
n?
m=1
(?1)1+mrm(?)L(1,m)(?)
=
n?
m=1
rm(?)L(m,m)(?) = Z(?)
The second line follows from the construction of
L?(?), and the third line follows from Eq. 6.
144
3.2 Marginals via matrix inversion
The marginals we require are given by
?h,m(?) =
1
Z(?)
?
y?T snp : (h,m)?y
?(y;?)
To calculate these marginals efficiently for all values
of (h,m) we use a well-known identity relating the
log partition-function to marginals
?h,m(?) =
? logZ(?)
??h,m
Since the partition function in this case has a closed-
form expression (i.e., the determinant of a matrix
constructed from ?), the marginals can also obtained
in closed form. Using the chain rule, the derivative
of the log partition-function in Proposition 1 is
?h,m(?) =
? log |L?(?)|
??h,m
=
n?
h?=1
n?
m?=1
? log |L?(?)|
?L?h?,m?(?)
?L?h?,m?(?)
??h,m
To perform the derivative, we use the identity
? log |X|
?X
=
(
X?1
)T
and the fact that ?L?h?,m?(?)/??h,m is nonzero for
only a few h?,m?. Specifically, when h = 0, the
marginals are given by
?0,m(?) = rm(?)
[
L??1(?)
]
m,1
and for h > 0, the marginals are given by
?h,m(?) = (1 ? ?1,m)Ah,m(?)
[
L??1(?)
]
m,m
?
(1 ? ?h,1)Ah,m(?)
[
L??1(?)
]
m,h
where ?h,m is the Kronecker delta. Thus, the com-
plexity of evaluating all the relevant marginals is
dominated by the matrix inversion, and the total
complexity is therefore O(n3).
3.3 Multiple Roots
In the case of multiple roots, we can still compute
the partition function and marginals efficiently. In
fact, the derivation of this case is simpler than for
single-root structures. Create an extended graph G?
which augmentsG with a dummy root node that has
edges pointing to all of the existing nodes, weighted
by the appropriate root-selection scores. Note that
there is a bijection between directed spanning trees
ofG? rooted at the dummy root and multi-root struc-
tures y ? T mnp (x). Thus, Theorem 1 can be used to
compute the partition function directly: construct a
Laplacian matrix L(?) for G? and compute the mi-
nor L(0,0)(?). Since this minor is also a determi-
nant, the marginals can be obtained analogously to
the single-root case. More concretely, this technique
corresponds to defining the matrix L?(?) as
L?(?) = L(?) + diag(r(?))
where diag(v) is the diagonal matrix with the vector
v on its diagonal.
3.4 Labeled Trees
The techniques above extend easily to the case
where dependencies are labeled. For a model with
L different labels, it suffices to define the edge
and root scores as Ah,m(?) =
?L
`=1 exp {?h,m,`}
and rm(?) =
?L
`=1 exp {?0,m,`}. The partition
function over labeled trees is obtained by operat-
ing on these values as described previously, and
the marginals are given by an application of the
chain rule. Both inference problems are solvable in
O(n3 + Ln2) time.
4 Training Algorithms
This section describes two methods for parameter
estimation that rely explicitly on the computation of
the partition function and marginals.
4.1 Log-Linear Estimation
In conditional log-linear models (Johnson et al,
1999; Lafferty et al, 2001), a distribution over parse
trees for a sentence x is defined as follows:
P (y |x;w) =
exp
{?
(h,m)?y w ? f(x, h,m)
}
Z(x;w)
(7)
where Z(x;w) is the partition function, a sum over
T sp (x), T
s
np(x), T
m
p (x) or T
m
np (x).
We train the model using the approach described
by Sha and Pereira (2003). Assume that we have a
training set {(xi, yi)}Ni=1. The optimal parameters
145
are taken to be w? = argminw L(w) where
L(w) = ?C
N?
i=1
logP (yi |xi;w) +
1
2
||w||2
The parameterC > 0 is a constant dictating the level
of regularization in the model.
Since L(w) is a convex function, gradient de-
scent methods can be used to search for the global
minimum. Such methods typically involve repeated
computation of the loss L(w) and gradient ?L(w)?w ,
requiring efficient implementations of both func-
tions. Note that the log-probability of a parse is
logP (y |x;w) =
?
(h,m)?y
w ? f(x, h,m)? logZ(x;w)
so that the main issue in calculating the loss func-
tion L(w) is the evaluation of the partition functions
Z(xi;w). The gradient of the loss is given by
?L(w)
?w
= w ? C
N?
i=1
?
(h,m)?yi
f(xi, h,m)
+ C
N?
i=1
?
(h,m)?D(xi)
?h,m(xi;w)f(xi, h,m)
where
?h,m(x;w) =
?
y?T (x) : (h,m)?y
P (y |x;w)
is the marginal probability of a dependency (h,m).
Thus, the main issue in the evaluation of the gradient
is the computation of the marginals ?h,m(xi;w).
Note that Eq. 7 forms a special case of the log-
linear distribution defined in Eq. 2 in Section 2.2.
If we set ?h,m = w ? f(x, h,m) then we have
P (y |x;w) = P (y |x;?), Z(x;w) = Z(x;?), and
?h,m(x;w) = ?h,m(x;?). Thus in the projective
case the inside-outside algorithm can be used to cal-
culate the partition function and marginals, thereby
enabling training of a log-linear model; in the non-
projective case the algorithms in Section 3 can be
used for this purpose.
4.2 Max-Margin Estimation
The second learning algorithm we consider is
the large-margin approach for structured prediction
(Taskar et al, 2004a; Taskar et al, 2004b). Learning
in this framework again involves minimization of a
convex function L(w). Let the margin for parse tree
y on the i?th training example be defined as
mi,y(w) =
?
(h,m)?yi
w?f(xi, h,m) ?
?
(h,m)?y
w?f(xi, h,m)
The loss function is then defined as
L(w) = C
N?
i=1
max
y?T (xi)
(Ei,y ?mi,y(w)) +
1
2
||w||2
where Ei,y is a measure of the loss?or number of
errors?for parse y on the i?th training sentence. In
this paper we take Ei,y to be the number of incorrect
dependencies in the parse tree y when compared to
the gold-standard parse tree yi.
The definition of L(w) makes use of the expres-
sion maxy?T (xi) (Ei,y ?mi,y(w)) for the i?th train-
ing example, which is commonly referred to as the
hinge loss. Note that Ei,yi = 0, and also that
mi,yi(w) = 0, so that the hinge loss is always non-
negative. In addition, the hinge loss is 0 if and only
ifmi,y(w) ? Ei,y for all y ? T (xi). Thus the hinge
loss directly penalizes margins mi,y(w) which are
less than their corresponding losses Ei,y.
Figure 2 shows an algorithm for minimizing
L(w) that is based on the exponentiated-gradient al-
gorithm for large-margin optimization described by
Bartlett et al (2004). The algorithm maintains a set
of weights ?i,h,m for i = 1 . . . N, (h,m) ? D(xi),
which are updated example-by-example. The algo-
rithm relies on the repeated computation of marginal
values ?i,h,m, which are defined as follows:1
?i,h,m =
?
y?T (xi) : (h,m)?y
P (y |xi) (8)
P (y |xi) =
exp
{?
(h,m)?y ?i,h,m
}
?
y??T (xi) exp
{?
(h,m)?y? ?i,h,m
}
A similar definition is used to derive marginal val-
ues ??i,h,m from the values ?
?
i,h,m. Computation of
the ? and ?? values is again inference of the form
described in Problem 3 in Section 2.2, and can be
1Bartlett et al (2004) write P (y |xi) as ?i,y . The ?i,y vari-
ables are dual variables that appear in the dual objective func-
tion, i.e., the convex dual of L(w). Analysis of the algorithm
shows that as the ?i,h,m variables are updated, the dual vari-
ables converge to the optimal point of the dual objective, and
the parameters w converge to the minimum of L(w).
146
Inputs: Training examples {(xi, yi)}Ni=1.
Parameters: Regularization constant C, starting point ?,
number of passes over training set T .
Data Structures: Real values ?i,h,m and li,h,m for i =
1 . . . N, (h,m) ? D(xi). Learning rate ?.
Initialization: Set learning rate ? = 1C . Set ?i,h,m = ? for
(h,m) ? yi, and ?i,h,m = 0 for (h,m) /? yi. Set li,h,m = 0
for (h,m) ? yi, and li,h,m = 1 for (h,m) /? yi. Calculate
initial parameters as
w = C
?
i
?
(h,m)?D(xi)
?i,h,mf(xi, h,m)
where ?i,h,m = (1? li,h,m ??i,h,m) and the ?i,h,m values
are calculated from the ?i,h,m values as described in Eq. 8.
Algorithm: Repeat T passes over the training set, where
each pass is as follows:
Set obj = 0
For i = 1 . . . N
? For all (h,m) ? D(xi):
??i,h,m = ?i,h,m + ?C (li,h,m +w ? f(xi, h,m))
? For example i, calculate marginals ?i,h,m
from ?i,h,m values, and marginals ??i,h,m
from ??i,h,m values (see Eq. 8).
? Update the parameters:
w = w + C
?
(h,m)?D(xi)
?i,h,mf(xi, h,m)
where ?i,h,m = ?i,h,m ? ??i,h,m,
? For all (h,m) ? D(xi), set ?i,h,m = ??i,h,m
? Set obj = obj + C
?
(h,m)?D(xi)
li,h,m??i,h,m
Set obj = obj ? ||w||
2
2 . If obj has decreased
compared to last iteration, set ? = ?2 .
Output: Parameter values w.
Figure 2: The EG Algorithm for Max-Margin Estimation.
The learning rate ? is halved each time the dual objective func-
tion (see (Bartlett et al, 2004)) fails to increase. In our experi-
ments we chose ? = 9, which was found to work well during
development of the algorithm.
achieved using the inside-outside algorithm for pro-
jective structures, and the algorithms described in
Section 3 for non-projective structures.
5 Related Work
Global log-linear training has been used in the con-
text of PCFG parsing (Johnson, 2001). Riezler et al
(2004) explore a similar application of log-linear
models to LFG parsing. Max-margin learning
has been applied to PCFG parsing by Taskar et al
(2004b). They show that this problem has a QP
dual of polynomial size, where the dual variables
correspond to marginal probabilities of CFG rules.
A similar QP dual may be obtained for max-margin
projective dependency parsing. However, for non-
projective parsing, the dual QP would require an ex-
ponential number of constraints on the dependency
marginals (Chopra, 1989). Nevertheless, alternative
optimization methods like that of Tsochantaridis et
al. (2004), or the EGmethod presented here, can still
be applied.
The majority of previous work on dependency
parsing has focused on local (i.e., classification of
individual edges) discriminative training methods
(Yamada and Matsumoto, 2003; Nivre et al, 2004;
Y. Cheng, 2005). Non-local (i.e., classification of
entire trees) training methods were used by McDon-
ald et al (2005a), who employed online learning.
Dependency parsing accuracy can be improved
by allowing second-order features, which consider
more than one dependency simultaneously. McDon-
ald and Pereira (2006) define a second-order depen-
dency parsing model in which interactions between
adjacent siblings are allowed, and Carreras (2007)
defines a second-order model that allows grandpar-
ent and sibling interactions. Both authors give poly-
time algorithms for exact projective parsing. By
adapting the inside-outside algorithm to these mod-
els, partition functions and marginals can be com-
puted for second-order projective structures, allow-
ing log-linear and max-margin training to be ap-
plied via the framework developed in this paper.
For higher-order non-projective parsing, however,
computational complexity results (McDonald and
Pereira, 2006; McDonald and Satta, 2007) indicate
that exact solutions to the three inference problems
of Section 2.2 will be intractable. Exploration of ap-
proximate second-order non-projective inference is
a natural avenue for future research.
Two other groups of authors have independently
and simultaneously proposed adaptations of the
Matrix-Tree Theorem for structured inference on di-
rected spanning trees (McDonald and Satta, 2007;
Smith and Smith, 2007). There are some algorithmic
differences between these papers and ours. First, we
define both multi-root and single-root algorithms,
whereas the other papers only consider multi-root
147
parsing. This distinction can be important as one
often expects a dependency structure to have ex-
actly one child attached to the root-symbol, as is the
case in a single-root structure. Second, McDonald
and Satta (2007) propose an O(n5) algorithm for
computing the marginals, as opposed to the O(n3)
matrix-inversion approach used by Smith and Smith
(2007) and ourselves.
In addition to the algorithmic differences, both
groups of authors consider applications of the
Matrix-Tree Theorem which we have not discussed.
For example, both papers propose minimum-risk
decoding, and McDonald and Satta (2007) dis-
cuss unsupervised learning and language model-
ing, while Smith and Smith (2007) define hidden-
variable models based on spanning trees.
In this paper we used EG training methods only
for max-margin models (Bartlett et al, 2004). How-
ever, Globerson et al (2007) have recently shown
how EG updates can be applied to efficient training
of log-linear models.
6 Experiments on Dependency Parsing
In this section, we present experimental results
applying our inference algorithms for dependency
parsing models. Our primary purpose is to estab-
lish comparisons along two relevant dimensions:
projective training vs. non-projective training, and
marginal-based training algorithms vs. the averaged
perceptron. The feature representation and other rel-
evant dimensions are kept fixed in the experiments.
6.1 Data Sets and Features
We used data from the CoNLL-X shared task
on multilingual dependency parsing (Buchholz and
Marsi, 2006). In our experiments, we used a subset
consisting of six languages; Table 1 gives details of
the data sets used.2 For each language we created
a validation set that was a subset of the CoNLL-X
2Our subset includes the two languages with the lowest ac-
curacy in the CoNLL-X evaluations (Turkish and Arabic), the
language with the highest accuracy (Japanese), the most non-
projective language (Dutch), a moderately non-projective lan-
guage (Slovene), and a highly projective language (Spanish).
All languages but Spanish have multi-root parses in their data.
We are grateful to the providers of the treebanks that constituted
the data of our experiments (Hajic? et al, 2004; van der Beek et
al., 2002; Kawata and Bartels, 2000; Dz?eroski et al, 2006; Civit
and Mart??, 2002; Oflazer et al, 2003).
language %cd train val. test
Arabic 0.34 49,064 5,315 5,373
Dutch 4.93 178,861 16,208 5,585
Japanese 0.70 141,966 9,495 5,711
Slovene 1.59 22,949 5,801 6,390
Spanish 0.06 78,310 11,024 5,694
Turkish 1.26 51,827 5,683 7,547
Table 1: Information for the languages in our experiments.
The 2nd column (%cd) is the percentage of crossing dependen-
cies in the training and validation sets. The last three columns
report the size in tokens of the training, validation and test sets.
training set for that language. The remainder of each
training set was used to train the models for the dif-
ferent languages. The validation sets were used to
tune the meta-parameters (e.g., the value of the reg-
ularization constantC) of the different training algo-
rithms. We used the official test sets and evaluation
script from the CoNLL-X task. All of the results that
we report are for unlabeled dependency parsing.3
The non-projective models were trained on the
CoNLL-X data in its original form. Since the pro-
jective models assume that the dependencies in the
data are non-crossing, we created a second train-
ing set for each language where non-projective de-
pendency structures were automatically transformed
into projective structures. All projective models
were trained on these new training sets.4 Our feature
space is based on that of McDonald et al (2005a).5
6.2 Results
We performed experiments using three training al-
gorithms: the averaged perceptron (Collins, 2002),
log-linear training (via conjugate gradient descent),
and max-margin training (via the EG algorithm).
Each of these algorithms was trained using pro-
jective and non-projective methods, yielding six
training settings per language. The different
training algorithms have various meta-parameters,
which we optimized on the validation set for
each language/training-setting combination. The
3Our algorithms also support labeled parsing (see Section
3.4). Initial experiments with labeled models showed the same
trend that we report here for unlabeled parsing, so for simplicity
we conducted extensive experiments only for unlabeled parsing.
4The transformations were performed by running the pro-
jective parser with score +1 on correct dependencies and -1 oth-
erwise: the resulting trees are guaranteed to be projective and to
have a minimum loss with respect to the correct tree. Note that
only the training sets were transformed.
5It should be noted that McDonald et al (2006) use a richer
feature set that is incomparable to our features.
148
Perceptron Max-Margin Log-Linear
p np p np p np
Ara 71.74 71.84 71.74 72.99 73.11 73.67
Dut 77.17 78.83 76.53 79.69 76.23 79.55
Jap 91.90 91.78 92.10 92.18 91.68 91.49
Slo 78.02 78.66 79.78 80.10 78.24 79.66
Spa 81.19 80.02 81.71 81.93 81.75 81.57
Tur 71.22 71.70 72.83 72.02 72.26 72.62
Table 2: Test data results. The p and np columns show results
with projective and non-projective training respectively.
Ara Dut Jap Slo Spa Tur AV
P 71.74 78.83 91.78 78.66 81.19 71.70 79.05
E 72.99 79.69 92.18 80.10 81.93 72.02 79.82
L 73.67 79.55 91.49 79.66 81.57 72.26 79.71
Table 3: Results for the three training algorithms on the differ-
ent languages (P = perceptron, E = EG, L = log-linear models).
AV is an average across the results for the different languages.
averaged perceptron has a single meta-parameter,
namely the number of iterations over the training set.
The log-linear models have two meta-parameters:
the regularization constant C and the number of
gradient steps T taken by the conjugate-gradient
optimizer. The EG approach also has two meta-
parameters: the regularization constant C and the
number of iterations, T .6 For models trained using
non-projective algorithms, both projective and non-
projective parsing was tested on the validation set,
and the highest scoring of these two approaches was
then used to decode test data sentences.
Table 2 reports test results for the six training sce-
narios. These results show that for Dutch, which is
the language in our data that has the highest num-
ber of crossing dependencies, non-projective train-
ing gives significant gains over projective training
for all three training methods. For the other lan-
guages, non-projective training gives similar or even
improved performance over projective training.
Table 3 gives an additional set of results, which
were calculated as follows. For each of the three
training methods, we used the validation set results
to choose between projective and non-projective
training. This allows us to make a direct com-
parison of the three training algorithms. Table 3
6We trained the perceptron for 100 iterations, and chose the
iteration which led to the best score on the validation set. Note
that in all of our experiments, the best perceptron results were
actually obtained with 30 or fewer iterations. For the log-linear
and EG algorithms we tested a number of values for C, and for
each value of C ran 100 gradient steps or EG iterations, finally
choosing the best combination of C and T found in validation.
shows the results of this comparison.7 The results
show that log-linear and max-margin models both
give a higher average accuracy than the perceptron.
For some languages (e.g., Japanese), the differences
from the perceptron are small; however for other
languages (e.g., Arabic, Dutch or Slovene) the im-
provements seen are quite substantial.
7 Conclusions
This paper describes inference algorithms for
spanning-tree distributions, focusing on the funda-
mental problems of computing partition functions
and marginals. Although we concentrate on log-
linear and max-margin estimation, the inference al-
gorithms we present can serve as black-boxes in
many other statistical modeling techniques.
Our experiments suggest that marginal-based
training produces more accurate models than per-
ceptron learning. Notably, this is the first large-scale
application of the EG algorithm, and shows that it is
a promising approach for structured learning.
In line with McDonald et al (2005b), we confirm
that spanning-tree models are well-suited to depen-
dency parsing, especially for highly non-projective
languages such as Dutch. Moreover, spanning-tree
models should be useful for a variety of other prob-
lems involving structured data.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their constructive comments. In addi-
tion, the authors gratefully acknowledge the follow-
ing sources of support. Terry Koo was funded by
a grant from the NSF (DMS-0434222) and a grant
from NTT, Agmt. Dtd. 6/21/1998. Amir Glober-
son was supported by a fellowship from the Roth-
schild Foundation - Yad Hanadiv. Xavier Carreras
was supported by the Catalan Ministry of Innova-
tion, Universities and Enterprise, and a grant from
NTT, Agmt. Dtd. 6/21/1998. Michael Collins was
funded by NSF grants 0347631 and DMS-0434222.
7We ran the sign test at the sentence level to measure the
statistical significance of the results aggregated across the six
languages. Out of 2,472 sentences total, log-linear models gave
improved parses over the perceptron on 448 sentences, and
worse parses on 343 sentences. The max-margin method gave
improved/worse parses for 500/383 sentences. Both results are
significant with p ? 0.001.
149
References
J. Baker. 1979. Trainable grammars for speech recognition. In
97th meeting of the Acoustical Society of America.
P. Bartlett, M. Collins, B. Taskar, and D. McAllester. 2004. Ex-
ponentiated gradient algorithms for large?margin structured
classification. In NIPS.
L.E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970. A max-
imization technique occurring in the statistical analysis of
probabilistic functions of markov chains. Annals of Mathe-
matical Statistics, 41:164?171.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In Proc. CoNLL-X.
X. Carreras. 2007. Experiments with a higher-order projective
dependency parser. In Proc. EMNLP-CoNLL.
S. Chopra. 1989. On the spanning tree polyhedron. Oper. Res.
Lett., pages 25?29.
Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a
directed graph. Science Sinica, 14:1396?1400.
M. Civit and Ma A. Mart??. 2002. Design principles for a Span-
ish treebank. In Proc. of the First Workshop on Treebanks
and Linguistic Theories (TLT).
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In Proc. EMNLP.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Z?abokrtsky, and
A. Z?ele. 2006. Towards a Slovene dependency treebank. In
Proc. of the Fifth Intern. Conf. on Language Resources and
Evaluation (LREC).
J. Edmonds. 1967. Optimum branchings. Journal of Research
of the National Bureau of Standards, 71B:233?240.
J. Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proc. COLING.
A. Globerson, T. Koo, X. Carreras, and M. Collins. 2007. Ex-
ponentiated gradient algorithms for log-linear structured pre-
diction. In Proc. ICML.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka. 2004.
Prague Arabic dependency treebank: Development in data
and tools. In Proc. of the NEMLAR Intern. Conf. on Arabic
Language Resources and Tools, pages 110?117.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999.
Estimators for stochastic unification-based grammars. In
Proc. ACL.
M. Johnson. 2001. Joint and conditional estimation of tagging
and parsing models. In Proc. ACL.
Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese
treebank in VERBMOBIL. Verbmobil-Report 240, Seminar
fu?r Sprachwissenschaft, Universita?t Tu?bingen.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditonal ran-
dom fields: Probabilistic models for segmenting and labeling
sequence data. In Proc. ICML.
R. McDonald and F. Pereira. 2006. Online learning of approx-
imate dependency parsing algorithms. In Proc. EACL.
R. McDonald and G. Satta. 2007. On the complexity of non-
projective data-driven dependency parsing. In Proc. IWPT.
R. McDonald, K. Crammer, and F. Pereira. 2005a. Online
large-margin training of dependency parsers. In Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proc. HLT-EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual
dependency parsing with a two-stage discriminative parser.
In Proc. CoNLL-X.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proc. CoNLL.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In A. Abeille?, editor, Tree-
banks: Building and Using Parsed Corpora, chapter 15.
Kluwer Academic Publishers.
M.A. Paskin. 2001. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical Report
UCB/CSD-01-1148, University of California, Berkeley.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent Sys-
tems: Networks of Plausible Inference (2nd edition). Mor-
gan Kaufmann Publishers.
S. Riezler, R. Kaplan, T. King, J. Maxwell, A. Vasserman, and
R. Crouch. 2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proc. HLT-NAACL.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL.
N.A. Smith and J. Eisner. 2005. Contrastive estimation: Train-
ing log-linear models on unlabeled data. In Proc. ACL.
D.A. Smith and N.A. Smith. 2007. Probabilistic models of
nonprojective dependency trees. In Proc. EMNLP-CoNLL.
B. Taskar, C. Guestrin, and D. Koller. 2004a. Max-margin
markov networks. In NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004b. Max-margin parsing. In Proc. EMNLP.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdependent
and structured output spaces. In Proc. ICML.
W. Tutte. 1984. Graph Theory. Addison-Wesley.
L. van der Beek, G. Bouma, R. Malouf, and G. van Noord.
2002. The Alpino dependency treebank. In Computational
Linguistics in the Netherlands (CLIN).
Y. Matsumoto Y. Cheng, M. Asahara. 2005. Machine learning-
based dependency analyzer for chinese. In Proc. ICCC.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. IWPT.
150
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1368?1378, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning to Map into a Universal POS Tagset
Yuan Zhang, Roi Reichart, Regina Barzilay
Massachusetts Institute of Technology
{yuanzh, roiri, regina}@csail.mit.edu
Amir Globerson
The Hebrew University
gamir@cs.huji.ac.il
Abstract
We present an automatic method for mapping
language-specific part-of-speech tags to a set
of universal tags. This unified representation
plays a crucial role in cross-lingual syntactic
transfer of multilingual dependency parsers.
Until now, however, such conversion schemes
have been created manually. Our central hy-
pothesis is that a valid mapping yields POS
annotations with coherent linguistic proper-
ties which are consistent across source and
target languages. We encode this intuition
in an objective function that captures a range
of distributional and typological characteris-
tics of the derived mapping. Given the ex-
ponential size of the mapping space, we pro-
pose a novel method for optimizing over soft
mappings, and use entropy regularization to
drive those towards hard mappings. Our re-
sults demonstrate that automatically induced
mappings rival the quality of their manually
designed counterparts when evaluated in the
context of multilingual parsing.1
1 Introduction
In this paper, we explore an automatic method for
mapping language-specific part-of-speech tags to a
universal tagset. In multilingual parsing, this uni-
fied input representation is required for cross-lingual
syntactic transfer. Specifically, the universal tagset
annotations enable an unlexicalized parser to capi-
talize on annotations from one language when learn-
ing a model for another.
1The source code and data for the work presented in this
paper is available at http://groups.csail.mit.edu/
rbg/code/unitag/emnlp2012
While the notion of a universal POS tagset is
widely accepted, in practice it is hardly ever used
for annotation of monolingual resources. In fact,
available POS annotations are designed to capture
language-specific idiosyncrasies and therefore are
substantially more detailed than a coarse universal
tagset. To reconcile these cross-lingual annotation
differences, a number of mapping schemes have
been proposed in the parsing community (Zeman
and Resnik, 2008; Petrov et al 2011; Naseem et
al., 2010). In all of these cases, the conversion is
performed manually and has to be repeated for each
language and annotation scheme anew.
Despite the apparent simplicity, deriving a map-
ping is by no means easy, even for humans. In fact,
the universal tagsets manually induced by Petrov
et al(2011) and by Naseem et al(2010) disagree
on 10% of the tags. An example of such discrep-
ancy is the mapping of the Japanese tag ?PVfin? to
the universal tag ?particle? according to one scheme,
and to ?verb? according to another. Moreover, the
quality of this conversion has a direct implication on
the parsing performance. In the Japanese example
above, this difference in mapping yields a 6.7% dif-
ference in parsing accuracy.
The goal of our work is to induce the mapping
for a new language, utilizing existing manually-
constructed mappings as training data. The exist-
ing mappings developed in the parsing community
rely on gold POS tags for the target language. A
more realistic scenario is to employ the mapping
technique to resource-poor languages where gold
POS annotations are lacking. In such cases, a map-
ping algorithm has to operate over automatically in-
1368
duced clusters on the target language (e.g., using
the Brown algorithm) and convert them to universal
tags. We are interested in a mapping approach that
can effectively handle both gold tags and induced
clusters.
Our central hypothesis is that a valid mapping
yields POS annotations with coherent linguistic
properties which are consistent across languages.
Since universal tags play the same linguistic role
in source and target languages, we expect similar-
ity in their global distributional statistics. Figure 1a
shows statistics for two close languages, English and
German. We can see that their unigram frequencies
on the five most common tags are very close. Other
properties concern POS tag per sentence statistics ?
e.g., every sentence has to have at least one verb. Fi-
nally, the mappings can be further constrained by ty-
pological properties of the target language that spec-
ify likely tag sequences. This information is readily
available even for resource poor language (Haspel-
math et al 2005). For instance, since English and
German are prepositional languages, we expect to
observe adposition-noun sequences but not the re-
verse (see Figure 1b for sample sentences). We en-
code these heterogeneous properties into an objec-
tive function that guides the search for the optimal
mapping.
Having defined a quality measure for mappings,
our goal is to find the optimal mapping. However,
such partition optimization problems2 are NP hard
(Garey and Johnson, 1979). A naive approach to
the problem is to greedily improve the map, but it
turns out that this approach yields poor quality map-
pings. We therefore develop a method for optimiz-
ing over soft mappings, and use entropy regulariza-
tion to drive those towards hard mappings. We con-
struct the objective in a way that facilitates simple
monotonically improving updates corresponding to
solving convex optimization problems.
We evaluate our mapping approach on 19
languages that include representatives of Indo-
European, Semitic, Basque, Japonic and Turkic fam-
ilies. We measure mapping quality based on the
target language parsing accuracy. In addition to
considering gold POS tags for the target language,
2Instances of related hard problems are 3-partition and
subset-sum.
we also evaluate the mapping algorithm on auto-
matically induced POS tags. In all evaluation sce-
narios, our model consistently rivals the quality
of manually induced mappings. We also demon-
strate that the proposed inference procedure outper-
forms greedy methods by a large margin, highlight-
ing the importance of good optimization techniques.
We further show that while all characteristics of
the mapping contribute to the objective, our largest
gain comes from distributional features that capture
global statistics. Finally, we establish that the map-
ping quality has a significant impact on the accuracy
of syntactic transfer, which motivates further study
of this topic.
2 Related Work
Multilingual Parsing Early approaches for multi-
lingual parsing used parallel data to bridge the gap
between languages when modeling syntactic trans-
fer. In this setup, finding the mapping between var-
ious POS annotation schemes was not essential; in-
stead, the transfer algorithm could induce it directly
from the parallel data (Hwa et al 2005; Xi and
Hwa, 2005; Burkett and Klein, 2008). However,
more recent transfer approaches relinquish this data
requirement, learning to transfer from non-parallel
data (Zeman and Resnik, 2008; McDonald et al
2011; Cohen et al 2011; Naseem et al 2010).
These approaches assume access to a common input
representation in the form of universal tags, which
enables the model to connect patterns observed in
the source language to their counterparts in the tar-
get language.
Despite ongoing efforts to standardize POS tags
across languages (e.g., EAGLES initiative (Eynde,
2004)), many corpora are still annotated with
language-specific tags. In previous work, their map-
ping to universal tags was performed manually. Yet,
even though some of these mappings have been de-
veloped for the same CoNLL dataset (Buchholz and
Marsi, 2006; Nivre et al 2007), they are not identi-
cal and yield different parsing performance (Zeman
and Resnik, 2008; Petrov et al 2011; Naseem et al
2010). The goal of our work is to automate this pro-
cess and construct mappings that are optimized for
performance on downstream tasks (here we focus on
parsing). As our results show, we achieve this goal
1369
Noun Verb Det. Prep. Adj.0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Unigr
amFr
equen
cy
EnglishGerman -Investors [are appealing] to the Securities and Exchange Commission not to [limit] their access to information [about stock purchases]and sales [by corporate insiders]
-Einer der sich [f?r den Milliard?r] [ausspricht] [ist] Steve Jobs dem Perot [f?r den aufbau]der Computerfirma Next 20 Millionen Dollar [bereitstellte]
(a) (b)
Figure 1: Illustration of similarities in POS tag statistics across languages. (a) The unigram frequency statistics on five
tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in
blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions.
on a broad range of languages and evaluation sce-
narios.
Syntactic Category Refinement Our work also
relates to work in syntactic category refinement in
which POS categories and parse tree non-terminals
are refined in order to improve parsing perfor-
mance (Finkel et al 2007; Klein and Manning,
2003; Matsuzaki et al 2005; Petrov et al 2006;
Petrov and Klein, 2007; Liang et al 2007). Our
work differs from these approaches in two ways.
First, these methods have been developed in the
monolingual setting, while our mapping algorithm is
designed for multilingual parsing. Second, these ap-
proaches are trained on the syntactic trees of the tar-
get language, which enables them to directly link the
quality of newly induced categories with the quality
of syntactic parsing. In contrast, we are not given
trees in the target language. Instead, our model is
informed by mappings derived for other languages.
3 Task Formulation
The input to our task consists of a target corpus writ-
ten in a language T , and a set of non-parallel source
corpora written in languages {S1, . . . , Sn}. In the
source corpora, each word is annotated with both
a language-specific POS tag and a universal POS
tag (Petrov et al 2011). In the target corpus each
word is annotated only with a language-specific POS
tag, either gold or automatically induced.
Our goal is to find a map from the set of LT target
language tags to the set of K universal tags. We as-
sume that each language-specific tag is only mapped
to one universal tag, which means we never split a
language-specific tag and LT ? K holds for every
language. We represent the map by a matrix A of
size K ? LT where A(c|f) = 1 if the target lan-
guage tag f is mapped to the universal tag c, and
A(c|f) = 0 otherwise.3 Note that each column of
A should contain a single value of 1. We will later
relax the requirement thatA(c|f) ? {0, 1}. A candi-
date mappingA can be applied to the target language
to produce sentences labeled with universal tags.
4 Model
In this section we describe an objective that reflects
the quality of an automatic mapping.
Our key insight is that for a good mapping, the
statistics over the universal tags should be similar for
source and target languages because these tags play
the same role cross-linguistically. For example, we
should expect the frequency of a particular universal
tag to be similar in the source and target languages.
One choice to make when constructing an objec-
tive is the source languages to which we want to be
similar. It is clear that choosing all languages is not a
good idea, since they are not all expected to have dis-
tributional properties similar to the target language.
There is strong evidence that projecting from sin-
gle languages can lead to good parsing performance
3We use c and f to reflect the fact that universal tags are
a coarse version (hence c) of the language specific fine tags
(hence f ).
1370
(McDonald et al 2011). Therefore, our strategy is
to choose a single source language for comparison.
The choice of the source language is based on sim-
ilarity between typological properties; we describe
this in detail in Section 5.
We must also determine which statistical proper-
ties we expect to be preserved across languages. Our
model utilizes three linguistic phenomena which are
consistent across languages: POS tag global distri-
butional statistics, POS tag per sentence statistics,
and typology-based ordering statistics. We define
each of these below.
4.1 Mapping Characterization
We focus on three categories of mapping properties.
For each of the relevant statistics we define a func-
tion Fi(A) that has low values if the source and tar-
get statistics are similar.
Global distributional statistics: The unigram and
bigram statistics of the universal tags are expected
to be similar across languages with close typological
profiles. We use pS(c1, c2) to denote the bigram dis-
tribution over universal tags in the source language,
and pT (f1, f2) to denote the bigram distribution over
language specific tags in the target language. The
bigram distribution over universal tags in the target
language depends on A and pT (f1, f2) and is given
by:
pT (c1, c2;A) =
?
f1,f2
A(c1|f1)A(c2|f2)pT (f1, f2)
(1)
To enforce similarity between source and target dis-
tributions, we wish to minimize the KL divergence
between the two: 4
Fbi(A) = DKL[pS(c1, c2)|pT (c1, c2;A)] (2)
We similarly define Funi(A) as the distance be-
tween unigram distributions.
Per sentence statistics: Another defining property
of POS tags is their average count per sentence.
Specifically, we focus on the verb count per sen-
tence, which we expect be similar across languages.
4We use the KL divergence because it assigns low weights
to infrequent universal tags. Furthermore, this choice results in
a simple, EM-like parameter estimation algorithm as discussed
in Section 5.
To express this constraint, we use nv(s,A) to
denote the number of verbs (i.e., the universal
tags corresponding to verbs according to A) in
sentence s. This is a linear function of A. We also
use E[nv(s,A)] to denote the average number of
verbs per sentence, and V [nv(s,A)] to denote the
variance. We estimate these two statistics from
the source language and denote them by ESv, VSv.
Good mappings are expected to follow these
patterns by having a variance upper bounded by
VSv and an average lower bounded by ESv.5 This
corresponds to minimizing the following objectives:
FEv(A) = max [0, ESv ? E[nv(s,A)]]
FV v(A) = max [0, V [nv(s,A)]? VSv]
Note that the above objectives are convex in A,
which will make optimization simpler. We refer to
the two terms jointly as Fverb(A).
Typology-based ordering statistics: Typolog-
ical features can be useful for determining the
relative order of different tags. If we know that
the target language has a particular typological
feature, we expect its universal tags to obey the
given relative ordering. Specifically, we expect it to
agree with ordering statistics for source languages
with a similar typology. We consider two such
features here. First, in pre-position languages the
preposition is followed by the noun phrase. Thus, if
T is such a language, we expect the probability of
a noun phrase following the adposition to be high,
i.e., cross some threshold. Formally, we define C1 =
{noun, adj, num, pron, det} and consider the set of
bigram distributions Spre that satisfy the following
constraint:
?
c?C1
pT (adp,c) ? apre (3)
where apre =
?
c?C1 pS(adp,c) is calculated from
the source language. This constraint set is non-
convex in A due to the bilinearity of the bi-
gram term. To simplify optimization6 we take an
5The rationale is that we want to put a lower bound on the
number of verbs per sentence, and induce it from the source
language. Furthermore, we expect the number of verbs to be
well concentrated, and we induce its maximal variance from
the source language.
6In Section 5 we shall see that this makes optimization eas-
ier.
1371
approach inspired by the posterior regularization
method (Ganchev et al 2010) and use the objective:
Fc(A) = min
r(c1,c2)?Spre
DKL[r(c1, c2)|pT (c1, c2;A)]
(4)
The above objective will attain lower values for A
such that pT (c1, c2;A) is close to the constraint set.
Specifically, it will have a value of zero when the
bigram distribution induced by A has the property
specified in Spre. We similarly define a set Spost
for post-positional languages.
As a second typological feature, we consider the
Demonstrative-Noun ordering. In DN languages we
want the probability of a determiner to come be-
fore C2 = {noun, adj, num}, (i.e., frequent universal
noun-phrase tags), to cross a threshold. This con-
straint translates to:
?
c?C2
pT (det, c) ? adet (5)
where adet =
?
c?C2 pS(det, c) is a threshold de-
termined from the source language. We denote the
set of distributions that have this property by SDN,
and add them to the constraint in (4). The overall
constraint set is denoted by S.
4.2 The Overall Objective
We have defined a set of functions Fi(A) that are
expected to have low values for good mappings. To
combine those, we use a weighted sum: F?(A) =?
i ?i ? Fi(A). (The weights in this equation are
learned; we discussed the procedure in Section 5)
Optimizing over the set of mappings is difficult
since each mapping is a discrete set whose size is
exponential size in LT . Technically, the difficulty
comes from the requirement that elements of A are
integral and its columns sum to one. To relax this
restriction, we will allow A(c|f) ? [0, 1] and en-
courage A to correspond to a mapping by adding an
entropy regularization term:
H[A] = ?
?
f
?
c
A(c|f) logA(c|f) (6)
This term receives its minimal value when the con-
ditional probability of the universal tags given a
language-specific tag is 1 for one universal tag and
zero for the others.
The overall objective is then: F (A) = F?(A) +
? ?H[A], where ? is the weight of the entropy term.7
The resulting optimization problem is:
min
A??
F (A) (7)
where ? is the set of non-negative matrices whose
columns sum to one:
? =
{
A :
A(c|f) ? 0 ?c, f
?K
c=1A(c|f) = 1 ?f
}
(8)
5 Parameter Estimation
In this section we describe the parameter estimation
process for our model. We start by describing how
to optimize A. Next, we discuss the weight selec-
tion algorithm, and finally the method for choosing
source languages.
5.1 Optimizing the Mapping A
Recall that our goal is to solve the optimization
problem in Eq. (7). This objective is non convex
since the function H[A] is concave, and the objec-
tive F (A) involves bilinear terms in A and loga-
rithms of their sums (see Equations (1) and (2)).
While we do not attempt to solve the problem
globally, we do have a simple update scheme that
monotonically decreases the objective. The update
can be derived in a similar manner to expectation
maximization (EM) (Neal and Hinton, 1999) and
convex concave procedures (Yuille and Rangarajan,
2003). Figure 2 describes our optimization algo-
rithm. The key ideas in deriving it are using pos-
terior distributions as in EM, and using a variational
formulation of entropy. The term Fc(A) is handled
in a similar way to the posterior regularization algo-
rithm derivation. A detailed derivation is provided
in the supplementary file. 8
The kth iteration of the algorithm involves several
steps:
? In step 1, we calculate the current esti-
mate of the bigram distribution over tags,
pT (c1, c2;Ak).
7Note that as ? ? ?, only valid maps will be selected by
the objective.
8The supplementary file is available at http://groups.
csail.mit.edu/rbg/code/unitag/emnlp2012.
1372
? In step 2, we find the bigram distribution in
the constraint set S that is closest in KL di-
vergence to pT (c1, c2;Ak), and denote it by
rk(c1, c2). This optimization problem is con-
vex in r(c1, c2).
? In step 3, we calculate the bigram posterior
over language specific tags given a pair of uni-
versal tags. This is analogous to the standard
E-step in EM.
? In step 4, we use the posterior in step 3 and the
bigram distributions pS(c1, c2) and rk(c1, c2)
to obtain joint counts over language specific
and universal bigrams.
? In step 5, we use the joint counts from step 4
to obtain counts over pairs of language specific
and universal tags.
? In step 6, analogous to the M-step in EM, we
optimize over the mapping matrix A. The ob-
jective is similar to the Q function in EM, and
also includes the Fverb(A) term, and a linear
upper bound on the entropy term. The objec-
tive can be seen to be convex in A.
As mentioned above, each of the optimization prob-
lems in steps 2 and 6 is convex, and can therefore be
solved using standard convex optimization solvers.
Here, we use the CVX package (Grant and Boyd,
2008; Grant and Boyd, 2011). It can be shown that
the algorithm improves F (A) at every iteration and
converges to a local optimum.
The above algorithm generates a mapping A that
may contain fractional entries. To turn it into a hard
mapping we round A by mapping each f to the c
that maximizes A(c|f) and then perform greedy im-
provement steps (one f at a time) to further improve
the objective. The regularization constant ? is tuned
to minimize the F?(A) value of the rounded A.
5.2 Learning the Objective Weights
Our F?(A) objective is a weighted sum of the in-
dividual Fi(A) functions. In the following, we de-
scribe how to learn the ?i weights for every target
language. We would like F?(A) to have low values
when A is a good map. Since our performance goal
is parsing accuracy, we consider a map to be good
Initialize A0.
Repeat
Step 1 (calculate current bigram estimate):
pT (c1, c2;A
k) =
?
f1,f2
Ak(c1|f1)Ak(c2|f2)pT (f1, f2)
Step 2 (incorporate constraints):
rk(c1, c2) = arg min
r?S
DKL[r(c1, c2)|pT (c1, c2;A
k)]
Step 3 (calculate model posterior):
p(f1, f2|c1, c2;Ak) ? Ak(c1|f1)Ak(c2|f2)pT (f1, f2)
Step 4: (complete joint counts):
Nk(c1, c2, f1, f2) = p(f1, f2|c1, c2;A
k)
(
rk(c1, c2) + pS(c1, c2)
)
Step 5 (obtain pairwise):
Mk(c, f) = Nk1 (c, f) +N
k
2 (c, f)
where Nk1 (c, f) =
?
c2,f2
Nk(c, c2, f, f2) and similarly for
Nk2 (c, f).
Step 6 (M step with entropy linearization): Set Ak+1 to be the
solution of
min
A??
?
?
c,f
[
Mk(c, f) logA(c|f) + A(c|f) logAk(c|f)
]
+ Fverb(A)
Until Convergence of Ak
Figure 2: An iterative algorithm for minimizing our ob-
jective in Eq. (7). For simplicity we assume that all the
weights ?i and ? are equal to one. It can be shown that
the objective monotonically decreases in every iteration.
if it results in high parsing accuracy, as measured
when projecting a parser from to S to T .
Since we do not have annotated parses in T , we
use the other source languages S = {S1, . . . , Sn}
to learn the weight. For each Si as the target, we
first train a parser for each language in S \ {Si} as
if it was the source, using the map of Petrov et al
(2011), and choose S?i ? S \ {Si} which gives the
highest parsing accuracy on Si. Next we generate
7000 candidate mappings for Si by randomly per-
turbing the map of (Petrov et al 2011). We evalu-
ate the quality of each candidate A by projecting the
parser of S?i to Si, and recording the parsing accu-
racy. Among all the candidates we choose the high-
est accuracy one and denote it by A?(Si). We now
want the score F (A?(Si)) to be lower than that of all
other candidates. To achieve this, we train a ranking
SVM whose inputs are pairs of mapsA?(Si) and an-
1373
other worse A(Si). These map pairs are taken from
many different traget languages, i.e. many different
Si. The features given to the SVM are the terms of
the score Fi(A). The goal of the SVM is to weight
these terms such that the better map A?(Si) has a
lower score. The weights assigned by the SVM are
taken as ?i.
5.3 Source Language Selection
As noted in Section 4 we construct F (A) by choos-
ing a single source language S. Here we describe the
method for choosing S. Our goal is to choose S that
is closest to T in terms of typology. Assume that
languages are described by binary typological vec-
tors vL. We would like to learn a diagonal matrix
D such that d(S, T ;D) = (vS ? vT )TD(vS ? vT )
reflects the similarity between the languages. In our
context, a good measure of similarity is the perfor-
mance of a parser trained on S and projected on T
(using the optimal map A). We thus seek a matrix
D such that d(S, T ;D) is ranked according to the
parsing accuracy. The matrix D is trained using an
SVM ranking algorithm that tries to follow the rank-
ing of parsing accuracy. Similar to the technique for
learning the objective weights, we train across many
pairs of source languages.9
The typological features we use are a subset
of the features described in ?The World Atlas of
Languages Structure? (WALS, (Haspelmath et al
2005)), and are shown in Table 1.
6 Evaluation Set-Up
Datasets We test our model on 19 languages: Ara-
bic, Basque, Bulgarian, Catalan, Chinese, Czech,
Danish, Dutch, English, German, Greek, Hungar-
ian, Italian, Japanese, Portuguese, Slovene, Span-
ish, Swedish, and Turkish. Our data is taken from
the CoNLL 2006 and 2007 shared tasks (Buch-
holz and Marsi, 2006; Nivre et al 2007). The
CoNLL datasets consist of manually created depen-
dency trees and language-specific POS tags. Fol-
lowing Petrov et al(2011), our model maps these
language-specific tags to a set of 12 universal tags:
noun, verb, adjective, adverb, pronoun, determiner,
adposition, numeral, conjunction, particle, punctua-
tion mark and X (a general tag).
9Ties are broken using the F (A) objective.
Evaluation Procedure We perform a separate ex-
periment for each of the 19 languages as the tar-
get and a source language chosen from the rest (us-
ing the method from Section 5.3). For the selected
source language, we assume access to the mapping
of Petrov et al(2011).
Evaluation Measures We evaluate the quality of
the derived mapping in the context of the target lan-
guage parsing accuracy. In both the training and
test data, the language-specific tags are replaced
with universal tags: Petrov?s tags for the source lan-
guages and learned tags for the target language. We
train two non-lexicalized parsers using source anno-
tations and apply them to the target language. The
first parser is a non-lexicalized version of the MST
parser (McDonald et al 2005) successfully used in
the multilingual context (McDonald et al 2011). In
the second parser, parameters of the target language
are estimated as a weighted mixture of parameters
learned from supervised source languages (Cohen et
al., 2011). For the parser of Cohen et al(2011), we
trained the model on the four languages used in the
original paper ? English, German, Czech and Ital-
ian. When measuring the performance on each of
these four languages, we selected another set of four
languages with a similar level of diversity.10
Following the standard evaluation practice in
parsing, we use directed dependency accuracy as our
measure of performance.
Baselines We compare mappings induced by our
model against three baselines: the manually con-
structed mapping of Petrov et al(2011), a randomly
constructed mapping and a greedy mapping. The
greedy mapping uses the same objective as our full
model, but optimizes it using a greedy method. In
each iteration, this method makes |LT | passes over
the language-specific tags, selecting a substitution
that contributes the most to the objective.
Initialization To reduce the dimension of our al-
gorithm?s search space and speed up our method, we
start by clustering the language-specific POS tags of
the target into |K| = 12 clusters using an unsuper-
10We also experimented with a version of the Cohen et al
(2011) model trained on all the source languages. This set-up
resulted in decreased performance. For this reason, we chose to
train the model on the four languages.
1374
ID Feature Description Values
81A Order of Subject, Object and Verb SVO, SOV, VSO, VOS, OVS, OSV
85A Order of Adposition and Noun Postpositions, Prepositions, Inpositions
86A Order of Genitive and Noun Genitive-Noun, Noun-Genitive
87A Order of Adjective and Noun Adjective-Noun, Noun-Adjective
88A Order of Demonstrative and Noun Demonstrative-Noun, Noun-Demonstrative, before and after
Table 1: The set of typological features that we use for source language selection. The first column gives the ID of
the feature as listed in WALS. The second column describes the feature and the last column enumerates the allowable
values for each feature; besides these values each feature can also have a value of ?No dominant order?.
vised POS induction algorithm (Lee et al 2010).11
Our mapping algorithm then learns the connection
between these clusters and universal tags.
For initialization, we perform multiple random
restarts and select the one with the lowest final ob-
jective score.
7 Results
We first present the results of our model using the
gold POS tags for the target language. Table 2 sum-
marizes the performance of our model and the base-
lines.
Comparison against Baselines On average, the
mapping produced by our model yields parsers with
higher accuracy than all of the baselines. These re-
sults are consistent for both parsers (McDonald et
al., 2011; Cohen et al 2011). As expected, random
mappings yield abysmal results ? 20.2% and 12.7%
for the two parsers. The low accuracy of parsers that
rely on the Greedy mapping ? 29.9% and 25.4% ?
show that a greedy approach is a poor strategy for
mapping optimization.
Surprisingly, our model slightly outperforms the
mapping of (Petrov et al 2011), yielding an aver-
age accuracy of 56.7% as compared to the 55.4%
achieved by its manually constructed counterpart for
the direct transfer method (McDonald et al 2011).
Similar results are observed for the mixture weights
parser (Cohen et al 2011). The main reason for
these differences comes from mistakes introduced in
the manual mapping. For example, in Czech tag ?R?
is labeled as ?pronoun?, while actually it should be
mapped to ?adposition?. By correcting this mistake,
we gain 5% in parsing accuracy for the direct trans-
fer parser.
11This pre-clustering results in about 3% improvement, pre-
sumably since it uses contextual information beyond what our
algorithm does.
Overall, the manually constructed mapping and
our model?s output disagree on 21% of the assign-
ments (measured on the token level). However,
the extent of disagreement is not necessarily predic-
tive of the difference in parsing performance. For
instance, the manual and automatic mappings for
Catalan disagree on 8% of the tags and their pars-
ing accuracy differs by 5%. For Greek on the other
hand, the disagreement between mappings is much
higher ? 17%, yet the parsing accuracy is very
close. This phenomenon shows that not all mistakes
have equal weight. For instance, a confusion be-
tween ?pronoun? and ?noun? is less severe in the
parsing context than a confusion between ?pronoun?
and ?adverb?.
Impact of Language Selection To assess the
quality of our language selection method, we com-
pare the model against an oracle that selects the best
source for a given target language. As Table 2 shows
our method is very close to the oracle performance,
with only 0.7% gap between the two. In fact, for
10 languages our method correctly predicts the best
pairing. This result is encouraging in other contexts
as well. Specifically, McDonald et al(2011) have
demonstrated that projecting from a single oracle-
chosen language can lead to good parsing perfor-
mance, and our technique may allow such projection
without an oracle.
Relations between Objective Values and Opti-
mization Performance The suboptimal perfor-
mance of the Greedy method shows that choosing
a good optimization strategy plays a critical role in
finding the desired mapping. A natural question to
ask is whether the objective value is predictive of the
end goal parsing performance. Figure 3 shows the
objective values for the mappings computed by our
method and the baselines for four languages. Over-
1375
Direct Transfer Parser (Accuracy) Mixture Weight Parser (Accuracy)
Tag Diff.
Random Greedy Petrov Model Best Pair Random Greedy Petrov Model.
Catalan 15.9 32.5 74.8 79.3 79.3 12.6 24.6 65.6 73.9 8.8
Italian 16.4 41.0 68.7 68.3 71.4 11.7 33.5 64.2 61.9 6.7
Portuguese 15.8 24.6 72.0 75.1 75.1 10.7 14.1 70.4 72.6 12.2
Spanish 11.5 27.4 72.1 68.9 68.9 6.4 26.5 58.8 62.8 7.5
Danish 35.5 23.7 46.6 46.5 49.2 4.2 23.7 51.4 51.7 5.0
Dutch 18.0 22.1 58.2 56.8 57.3 7.1 15.3 54.9 53.2 4.9
English 14.7 19.0 51.6 49.0 49.0 13.3 15.1 47.5 41.8 17.7
German 15.8 24.3 55.7 50.4 51.6 20.9 18.7 52.4 51.8 15.0
Swedish 15.1 26.3 63.1 63.1 63.1 9.1 36.5 55.7 55.9 8.2
Bulgarian 17.4 28.0 51.6 63.4 63.4 22.6 39.9 64.6 60.4 35.7
Czech 19.0 34.4 47.7 57.3 57.3 12.7 26.2 48.3 55.7 28.5
Slovene 15.6 21.8 43.5 51.4 52.8 11.3 20.7 42.2 53.0 38.8
Greek 17.3 19.5 62.3 59.7 59.8 22.0 15.2 56.2 57.0 17.0
Hungarian 28.4 44.1 53.8 52.3 52.3 4.0 43.8 46.4 51.7 18.1
Arabic 22.1 45.4 51.5 51.2 52.9 3.9 40.9 48.3 51.1 15.7
Basque 18.0 19.2 27.9 33.1 35.1 6.3 8.3 32.3 30.6 43.8
Chinese 22.4 34.1 46.0 47.6 49.5 17.7 34.9 44.0 40.4 38.1
Japanese 36.5 46.2 51.4 53.6 53.6 15.4 18.0 25.7 28.7 73.8
Turkish 28.8 34.9 53.2 49.8 49.8 19.7 20.3 27.7 27.5 9.9
Average 20.2 29.9 55.4 56.7 57.4 12.7 25.4 50.8 51.7 21.3
Table 2: Directed dependency accuracy of our model and the baselines using gold POS tags for the target language.
The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). The second section
is for the weighted mixture parsing model (Cohen et al 2011). The first two columns (Random and Greedy) of each
section present the parsing performance with a random or a greedy mapping. The third column (Petrov) shows the
results when the mapping of Petrov et al(2011) is used. The fourth column (Model) shows the results when our
mapping is used and the fifth column in the first section (Best Pair) shows the performance of our model when the best
source language is selected for every target language. The last column (Tag Diff.) presents the difference between our
mapping and the mapping of Petrov et al(2011) by showing the percentage of target language tokens for which the
two mappings select a different universal tag.
all, our method and the manual mappings reach sim-
ilar values, both considerably better than other base-
lines. While the parsing performance correlates with
the objective, the correlation is not perfect. For in-
stance, on Greek our mapping has a better objective
value, but lower parsing performance.
Ablation Analysis We next analyze the contribu-
tion of each component of our objective to the result-
ing performance.12 The strongest factor in our ob-
jective is the distributional features capturing global
statistics. Using these features alone achieves an
average accuracy of 51.1%, only 5.6% less than
the full model score. Adding just the verb-related
constraints to the distributional similarity objectives
improves the average model performance by 2.1%.
12The results are consistent for both parsers, here we report
the accuracy for the direct transfer method (McDonald et al
2011).
Adding just the typological constraints yields a very
modest performance gain of 0.5%. This is not sur-
prising ? the source language is selected to be typo-
logically similar to the target language, and thus its
distributional properties are consistent with typolog-
ical features. However, adding both the verb-related
constraints and the typological constraints results in
a synergistic performance gain of 5.6% over the dis-
tributional similarity objective, a gain which is much
better than the sum of the two individual gains.
Application to Automatically Induced POS Tags
A potential benefit of the proposed method is to re-
late automatically induced clusters in the target lan-
guage to universal tags. In our experiments, we in-
duce such clusters using Brown clustering,13 which
13In our experiments, we employ Liang?s implementation
http://cs.stanford.edu/?pliang/software/. The number of clus-
ters is set to 30.
1376
Catalan German Greek Arabic0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
 
 ModelPetrovGreedyRandom
Figure 3: Objective values for the different mappings
used in our experiments for four languages. Note that
the goal of the optimization procedure is to minimize the
objective value.
has been successfully used for similar purposes in
parsing research (Koo et al 2008). We then map
these clusters to the universal tags using our algo-
rithm.
The average parsing accuracy on the 19 languages
is 45.5%. Not surprisingly, automatically induced
tags negatively impact parsing performance, yield-
ing a decrease of 11% when compared to mappings
obtained using manual POS annotations (see Ta-
ble 2). To further investigate the impact of inaccu-
rate tags on the mapping performance, we compare
our model against the oracle mapping model that
maps each cluster to the most common universal tag
of its members. Parsing accuracy obtained using this
method is 45.1%, closely matching the performance
of our mapping algorithm.
An alternative approach to mapping words into
universal tags is to directly partition words into K
clusters (without passing through language specific
tags). In order for these clusters to be meaningful
as universal tags, we can provide several prototypes
for each cluster (e.g., ?walk? is a verb etc.). To test
this approach we used the prototype driven tagger of
Haghighi and Klein (2006) with 15 prototypes per
universal tag.14 The resulting universal tags yield
an average parsing accuracy of 40.5%. Our method
(using Brown clustering as above) outperforms this
14Oracle prototypes were obtained by taking the 15 most
frequent words for each universal tag. This yields almost the
same total number of prototypes as those in the experiment of
(Haghighi and Klein, 2006).
baseline by about 5%.
8 Conclusions
We present an automatic method for mapping
language-specific part-of-speech tags to a set of uni-
versal tags. Our work capitalizes on manually de-
signed conversion schemes to automatically create
mappings for new languages. Our experimental re-
sults demonstrate that automatically induced map-
pings rival the quality of their hand-crafted coun-
terparts. We also establish that the mapping quality
has a significant impact on the accuracy of syntactic
transfer, which motivates further study of this topic.
Finally, our experiments show that the choice of
mapping optimization scheme plays a crucial role in
the quality of the derived mapping, highlighting the
importance of optimization for the mapping task.
Acknowledgments
The authors acknowledge the support of the NSF
(IIS-0835445), the MURI program (W911NF-10-1-
0533) and the DARPA BOLT program. We thank
Tommi Jaakkola, the members of the MIT NLP
group and the ACL reviewers for their suggestions
and comments. Any opinions, findings, conclu-
sions, or recommendations expressed in this paper
are those of the authors, and do not necessarily re-
flect the views of the funding organizations.
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP, pages 877?886.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP,
pages 50?61.
Frank Van Eynde. 2004. Part of speech tagging en lem-
matisering van het corpus gesproken nederlands. In
Technical report.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
ACL, pages 272?279.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. JMLR, 11:2001?2049.
1377
Michael Garey and David S. Johnson. 1979. Comput-
ers and Intractability: A Guide to the Theory of NP-
Completeness. W. H. Freeman & Co.
Michael C. Grant and Stephen P. Boyd. 2008. Graph im-
plementations for nonsmooth convex programs. In Re-
cent Advances in Learning and Control, Lecture Notes
in Control and Information Sciences, pages 95?110.
Springer-Verlag Limited.
Michael C. Grant and Stephen P. Boyd. 2011. CVX:
Matlab software for disciplined convex programming,
version 1.21.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
NAACL, pages 320?327.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie, editors. 2005. The World Atlas of
Language Structures. Oxford University Press.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Journal of Natural Language Engineering, 11:311?
325.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL, pages 595?603.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging. In
Proceedings of EMNLP, pages 853?861.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite pcfg using hierarchi-
cal dirichlet processes. In Proceedings of EMNLP-
CoNLL, pages 688?697.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of ACL, pages 75?82.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of EMNLP, pages 523?530.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP, pages 62?72.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
EMNLP, pages 1234?1244.
Radford M. Neal and Geoffrey E. Hinton. 1999. A view
of the em algorithm that justifies incremental, sparse,
and other variants. In Michael I. Jordan, editor, Learn-
ing in Graphical Models, pages 355?368. MIT Press.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL-
COLING, pages 433?440.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. In ArXiv, April.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-english languages.
In Proceedings of EMNLP, pages 851?858.
Alan Yuille and Anand Rangarajan. 2003. The concave-
convex procedure (cccp). In Proceedings of Neural
Computation, volume 15, pages 915?936.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In Pro-
ceedings of IJCNLP-08 Workshop on NLP for Less
Privileged Languages, pages 35?42.
1378
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1434?1444, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Improved Parsing and POS Tagging Using Inter-Sentence
Consistency Constraints
Alexander M. Rush1? Roi Reichart1? Michael Collins2 Amir Globerson3
1MIT CSAIL, Cambridge, MA, 02139, USA
{srush|roiri}@csail.mit.edu
2Department of Computer Science, Columbia University, New-York, NY 10027, USA
mcollins@cs.columbia.edu
3School of Computer Science and Engineering, The Hebrew University, Jerusalem, 91904, Israel
gamir@cs.huji.ac.il
Abstract
State-of-the-art statistical parsers and POS
taggers perform very well when trained with
large amounts of in-domain data. When train-
ing data is out-of-domain or limited, accuracy
degrades. In this paper, we aim to compen-
sate for the lack of available training data by
exploiting similarities between test set sen-
tences. We show how to augment sentence-
level models for parsing and POS tagging with
inter-sentence consistency constraints. To deal
with the resulting global objective, we present
an efficient and exact dual decomposition de-
coding algorithm. In experiments, we add
consistency constraints to the MST parser
and the Stanford part-of-speech tagger and
demonstrate significant error reduction in the
domain adaptation and the lightly supervised
settings across five languages.
1 Introduction
State-of-the-art statistical parsers and POS taggers
perform very well when trained with large amounts
of data from their test domain. When training data is
out-of-domain or limited, the performance of the re-
sulting model often degrades. In this paper, we aim
to compensate for the lack of available training data
by exploiting similarities between test set sentences.
Most parsing and tagging models are defined at the
sentence-level, which makes such inter-sentence in-
formation sharing difficult. We show how to aug-
ment sentence-level models with inter-sentence con-
straints to encourage consistent descisions in similar
? Both authors contributed equally to this work.
contexts, and we give an efficient algorithm with for-
mal guarantees for decoding such models.
In POS tagging, most taggers perform very well
on word types that they have observed in training
data, but they perform poorly on unknown words.
With a global objective, we can include constraints
that encourage a consistent tag across all occur-
rences of an unknown word type to improve accu-
racy. In dependency parsing, the parser can benefit
from surface-level features of the sentence, but with
sparse or out-of-domain training data these features
are very noisy. Using a global objective, we can add
constraints that encourage similar surface-level con-
texts to exhibit similar syntactic behaviour.
The first contribution of this work is the use of
Markov random fields (MRFs) to model global con-
straints between sentences in dependency parsing
and POS tagging. We represent each word as a node,
the tagging or parse decision as its label, and add
constraints through edges. MRFs allow us to include
global constraints tailored to these problems, and to
reason about inference in the corresponding global
models.
The second contribution is an efficient dual de-
composition algorithm for decoding a global ob-
jective with inter-sentence constraints. These con-
straints generally make direct inference challenging
since they tie together the entire test corpus. To alle-
viate this issue, our algorithm splits the global infer-
ence problem into subproblems - decoding of indi-
vidual sentences, and decoding of the global MRF.
These subproblems can be solved efficiently through
known methods. We show empirically that by iter-
atively solving these subproblems, we can find the
1434
exact solution to the global model.
We experiment with domain adaptation and
lightly supervised training. We demonstrate that
global models with consistency constraints can im-
prove upon sentence-level models for dependency
parsing and part-of-speech tagging. For domain
adaptation, we show an error reduction of up to 7.7%
when adapting the second-order projective MST
parser (McDonald et al 2005) from newswire to
the QuestionBank domain. For lightly supervised
learning, we show an error reduction of up to 12.8%
over the same parser for five languages and an error
reduction of up to 10.3% over the Stanford trigram
tagger (Toutanova et al 2003) for English POS tag-
ging. The algorithm requires, on average, only 1.7
times the costs of sentence-level inference and finds
the exact solution on the vast majority of sentences.
2 Related Work
Methods that combine inter-sentence information
with sentence-level algorithms have been applied to
a number of NLP tasks. The most similar models to
our work are skip-chain CRFs (Sutton and Mccal-
lum, 2004), relational markov networks (Taskar et
al., 2002), and collective inference with symmetric
clique potentials (Gupta et al 2010). These mod-
els use a linear-chain CRF or MRF objective mod-
ified by potentials defined over pairs of nodes or
clique templates. The latter model makes use of La-
grangian relaxation. Skip-chain CRFs and collective
inference have been applied to problems in IE, and
RMNs to named entity recognition (NER) (Bunescu
and Mooney, 2004). Finkel et al(2005) also inte-
grated non-local information into entity annotation
algorithms using Gibbs sampling.
Our model can be applied to a variety of off-the-
shelf structured prediction models. In particular, we
focus on dependency parsing which is characterized
by a more complicated structure compared to the IE
tasks addressed by previous work.
Another line of work that integrates corpus-level
declarative information into sentence-level models
includes the posterior regularization (Ganchev et al
2010; Gillenwater et al 2010), generalized expec-
tation (Mann and McCallum, 2007; Mann and Mc-
Callum, ), and Bayesian measurements (Liang et al
2009) frameworks. The power of these methods has
been demonstrated for a variety of NLP tasks, such
as unsupervised and semi-supervised POS tagging
and parsing. The constraints used by these works
differ from ours in that they encourage the posterior
label distribution to have desired properties such as
sparsity (e.g. a given word can take a small number
of labels with a high probability). In addition, these
methods use global information during training as
opposed to our approach which applies test-time in-
ference global constraints.
The application of dual decomposition for infer-
ence in MRFs has been explored by Wainwright et
al. (2005), Komodakis et al(2007), and Globerson
and Jaakkola (2007). In NLP, Rush et al(2010)
and Koo et al(2010) applied dual decomposition to
enforce agreement between different sentence-level
algorithms for parsing and POS tagging. Work on
dual decomposition for NLP is related to the work
of Smith and Eisner (2008) who apply belief prop-
agation to inference in dependency parsing, and to
constrained conditional models (CCM) (Roth and
Yih, 2005) that impose inference-time constraints
through an ILP formulation.
Several works have addressed semi-supervised
learning for structured prediction, suggesting objec-
tives based on the max-margin principles (Altun and
Mcallester, 2005), manifold regularization (Belkin
et al 2005), a structured version of co-training
(Brefeld and Scheffer, 2006) and an entropy-based
regularizer for CRFs (Wang et al 2009). The com-
plete literature on domain adaptation is beyond the
scope of this paper, but we refer the reader to Blitzer
and Daume (2010) for a recent survey.
Specifically for parsing and POS tagging, self-
training (Reichart and Rappoport, 2007), co-training
(Steedman et al 2003) and active learning (Hwa,
2004) have been shown useful in the lightly su-
pervised setup. For parser adaptation, self-training
(McClosky et al 2006; McClosky and Charniak,
2008), using weakly annotated data from the tar-
get domain (Lease and Charniak, 2005; Rimell and
Clark, 2008), ensemble learning (McClosky et al
2010), hierarchical bayesian models (Finkel and
Manning, 2009) and co-training (Sagae and Tsujii,
2007) achieve substantial performance gains. For a
recent survey see Plank (2011). Constraints simi-
lar to those we use for POS tagging were used by
Subramanya et al(2010) for POS tagger adaptation.
1435
Their work, however, does not show how to decode
a global, corpus-level, objective that enforces these
constraints, which is a major contribution of this pa-
per.
Inter-sentence syntactic consistency has been ex-
plored in the psycholinguistics and NLP literature.
Phenomena such as parallelism and syntactic prim-
ing ? the tendency to repeat recently used syntactic
structures ? have been demonstrated in human lan-
guage corpora (e.g. WSJ and Brown) (Dubey et al
2009) and were shown useful in generative and dis-
criminative parsers (e.g. (Cheung and Penn, 2010)).
We complement these works, which focus on con-
sistency between consecutive sentences, and explore
corpus level consistency.
3 Structured Models
We begin by introducing notation for sentence-
level dependency parsing as a structured prediction
problem. The goal of dependency parsing is to
find the best parse y for a tagged sentence x =
(w1/t1, . . . , wn/tn) with words w and POS tags t.
Define the index set for dependency parsing as
I(x) = {(m,h) : m ? {1 . . . n},
h ? {0 . . . n},m 6= h}
where h = 0 represents the root word. A depen-
dency parse is a vector y = {y(m,h) : (m,h) ?
I(x)} where y(m,h) = 1 if m is a modifier of the
head word h. We define the set Y(x) ? {0, 1}|I(x)|
to be the set of all valid dependency parses for a sen-
tence x. In this work, we use projective dependency
parses, but the method also applies to the set of non-
projective parse trees.
Additionally, we have a scoring function f :
Y(x)? R. The optimal parse y? for a sentence x is
given by, y? = argmaxy?Y(x) f(y). This sentence-
level decoding problem can often be solved effi-
ciently. For example in commonly used projec-
tive dependency parsing models (McDonald et al
2005), we can compute y? efficiently using variants
of the Viterbi algorithm.
For this work, we make the assumption that we
have an efficient algorithm to find the argmax of
f(y) +
?
(m,h)?I(x)
u(m,h)y(m,h) = f(y) + u ? y
where u is a vector in R|I(x)|. In practice, u will be
a vector of Lagrange multipliers associated with the
dependencies of y in our dual decomposition algo-
rithm given in Section 6.
We can construct a very similar setting for POS
tagging where the goal is to find the best tagging
y for a sentence x = (w1, . . . , wn). We skip the
formal details here.
We next introduce notation for Markov random
fields (MRFs) (Koller and Friedman, 2009). An
MRF consists of an undirected graph G = (V,E),
a set of possible labels for each node Li for i ?
{1, . . . , |V |}, and a scoring function g. The index
set for MRFs is
IMRF = {(i, l) : i ? {1 . . . |V |}, l ? Li}
? {((i, j), li, lj) : (i, j) ? E, li ? Li, lj ? Lj}
A label assignment in the MRF is a binary vector
z with z(i, l) = 1 if the label l is selected at node i
and z((i, j), li, lj) = 1 if the labels li, lj are selected
for the nodes i, j.
In applications such as parsing and POS tagging,
some of the label assignments are not allowed. For
example, in dependency parsing the resulting struc-
ture must be a tree. Consequently, if every node
in the MRF corresponds to a word in a document
and its label corresponds to the index of its head
word, the resulting dependency structure for each
sentence must be acyclic. The set of all valid la-
bel assignments (one label per node) is given by
Z ? {0, 1}|IMRF|.
We score label assignments in the MRF with a
scoring function g : Z ? R. The best assignment
z? in an MRF is given by, z? = argmaxz?Z g(z).
We focus on pairwise MRFs where this function g is
a linear function of z whose parameters are denoted
by ?
g(z) = z ? ? =
?
(i,l)?IMRF
z(i, l)?(i, l) +
?
((i,j),li,lj)?IMRF
z((i, j), li, lj)?((i, j), li, lj)
As in parsing, we make the assumption that we
have an efficient algorithm to find the argmax of
g(z) +
?
(i,l)?IMRF(x)
u(i, l)z(i, l)
1436
He/PRP saw/VBD an/DT American/JJ man/NN
The/DT smart/JJ girls/NNS stood/VBD outside/RB
Danny/DT walks/VBZ a/DT long/JJ distance/NN
NN
Figure 1: An example constraint from dependency pars-
ing. The black nodes are modifiers observed in the train-
ing data. Each gray node corresponds to a possible mod-
ifier in the test corpus. The constraint applies to all mod-
ifiers in the context DT JJ. The white node corresponds
to the consensus POS tag of the head word of these mod-
ifiers.
4 A Parsing Example
In this section we give a detailed example of global
constraints for dependency parsing. The aim is to
construct a global objective that encourages similar
contexts across the corpus to exhibit similar syntac-
tic behaviour. We implement this objective using an
MRF with a node for each word in the test set. The
label of each node is the index of the word it mod-
ifies. We add edges to this MRF to reward consis-
tency among similar contexts. Furthermore, we add
nodes with a fixed label to incorporate contexts seen
in the training data.
Specifically, we say that the context of a word is
its POS tag and the POS tags of some set of the
words around it. We expand on this notion of con-
text in Section 8; for simplicity we assume here that
the context includes only the previous word?s POS
tag. Our constraints are designed to bias words in
the same context to modify words with similar POS
tags.
Figure 1 shows a global MRF over a small parsing
example with one training sentence and two test sen-
tences. The MRF contains a node associated with
each word instance, where the label of the node is
the index of the word it modifies. In this corpus, the
context DT JJ appears once in training and twice in
testing. We hope to choose head words with similar
POS tags for these two test contexts biased by the
observed training context.
More concretely, for each context c ?
{1, . . . , C}, we have a set Sc of associated
word indices (s,m) that appear in the context,
where s is a sentence index and m is a position
in that sentence. For instance, in our example
S1 = {(1, 2), (2, 4)} consists of all positions in
the test set where we see JJ preceded by DT.
Futhermore, we have a set Oc of indices (s,m,TR)
of observed instances of the context in the training
data where TR denotes a training index. In our
example O1 = {(1, 4,TR)} consists of the one
training instance. We associate each word instance
with a single context c.
We then define our MRF to include one consensus
node for each set Sc as well as a word node for each
instance in the set Sc ?Oc. Thus the set of variables
corresponds to V = {1, . . . , C} ? (
?C
c=1 Sc ? Oc).
Additionally, we include an edge from each node
i ? Sc?Oc to its consensus node c,E = {(i, c) : c ?
{1, . . . , C}, i ? Sc ?Oc}. The word nodes from Sc
have the label set of possible head indices L(s,m) =
{0, . . . , ns} where ns is the length of the sentence s.
The observed nodes from Oc have a singleton label
set L(s,m,TR) with the observed index. The consen-
sus nodes have the label set Lc = T ? {NULL}
where T is the set of POS tags and the NULL sym-
bol represents the constraint being turned off.
We can now define the scoring function g for this
MRF. The scoring function aims to reward consis-
tency among the head POS tag at each word and the
consensus node
?((i, c), li, lc) =
?
???
???
?1 if pos(li) = lc
?2 if pos(li) is close to lc
?3 lc = NULL
0 otherwise
where posmaps a word index to its POS tag. The pa-
rameters ?1 ? ?2 ? ?3 ? 0 determine the bonus for
identical POS tags, similar POS tags, and for turning
off the constraint .
We construct a similar model for POS tagging.
We choose sets Tc corresponding to the c?th un-
known word type in the corpus. The MRF graph
is identical to the parsing case with Tc replacing Sc
and we no longer have Oc. The label sets for the
word nodes are now L(s,m) = T where the label is
1437
the POS tag chosen at that word, and the label set for
the consensus node is Lc = T ? {NULL}. We use
the same scoring function as in parsing to enforce
consistency between word nodes and the consensus
node.
5 Global Objective
Recall the definition of sentence-level parsing,
where the optimal parse y? for a single sentence
x under a scoring function f is given by: y? =
argmaxy?Y(x) f(y). We apply this objective to
a set of sentences, specified by the tuple X =
(x1, ..., xr), and the product of possible parses
Y(X) = Y(x1) ? . . . ? Y(xr). The sentence-level
decoding problem is to find the optimal dependency
parses Y ? = (Y ?1 , ..., Y ?r ) ? Y(X) under a global
objective
Y ? = argmax
Y ?Y(X)
F (Y ) = argmax
Y ?Y(X)
r?
s=1
f(Ys)
where F : Y(X) ? R is the global scoring func-
tion.
We now consider scoring functions where the
global objective includes inter-sentence constraints.
Objectives of this form will not factor directly
into individual parsing problems; however, we can
choose to write them as the sum of two convenient
terms: (1) A simple sum of sentence-level objec-
tives; and (2) A global MRF that connects the local
structures.
For convenience, we define the following index
set.
J (X) = {(s,m, h) : s ? {1, . . . , r},
(m,h) ? I(xs)}
This set enumerates all possible dependencies at
each sentence in the corpus. We say the parses Ys
are consistent with a label assignment z if for all
(s,m, h) ? J (X) we have that z((s,m), h) =
Ys(m,h). In other words, the labels in z match the
head words chosen in parse Ys.
With this notation we can write the full global de-
coding objective as
(Y ?, z?) = argmax
Y ?Y(X), z?Z
F (Y ) + g(z) (1)
s.t. ?(s,m, h) ? J (X), z((s,m), h) = Ys(m,h)
Set u(1)(s,m, h)? 0 for all (s,m, h) ? J (X)
for k = 1 to K do
z(k) ? argmax
z?Z
(g(z) +
?
(s,m,h)?J (X)
u(k)(s,m, h)z((s,m), h))
Y (k) ? argmax
Y ?Y(X)
(F (Y ) ?
?
(s,m,h)?J (X)
u(k)(s,m, h)Ys(m,h))
if Y (k)s (m,h) = z(k)((s,m), h)
for all (s,m, h) ? J (X) then
return (Y (k), z(k))
for all (s,m, h) ? J (X),
u(k+1)(s,m, h)? u(k)(s,m, h) +
?k(z(k)((s,m), h)? Y (k)s (m,h))
return (Y (K), z(K))
Figure 2: The global decoding algorithm for dependency
parsing models.
The solution to this objective maximizes the local
models as well as the global MRF, while maintain-
ing consistency among the models. Specifically, the
MRF we use in the experiments has a simple naive
Bayes structure with the consensus node connected
to all relevant word nodes.
The global objective for POS tagging has a similar
form. As before we add a node to the MRF for each
word in the corpus. We use the POS tag set as our
labels for each of these nodes. The index set con-
tains an element for each possible tag at each word
instance in the corpus.
6 A Global Decoding Algorithm
We now consider the decoding question: how to
find the structure Y ? that maximizes the global ob-
jective. We aim for an efficient solution that makes
use of the individual solvers at the sentence-level.
For this work, we make the assumption that the
graph chosen for the MRF has small tree-width, e.g.
our naive Bayes constraints, and can be solved effi-
ciently using dynamic programming.
Before we describe our dual decomposition al-
gorithm, we consider the difficulty of solving the
global objective directly. We have an efficient dy-
namic programming algorithm for solving depen-
dency parsing at the sentence-level, and efficient al-
gorithms for solving the MRF. It follows that we
1438
could construct an intersected dynamic program-
ming algorithm that maintains the product of states
over both models. This algorithm is exact, but it
is very inefficient. Solving the intersected dynamic
program requires decoding simultaneously over the
entire corpus, with an additional multiplicative fac-
tor for solving the MRF. On top of this cost, we need
to alter the internal structure of the sentence-level
models.
In contrast, we can construct a dual decomposi-
tion algorithm which is efficient, produces a certifi-
cate when it finds an exact solution, and directly
uses the sentence-level parsing models. Considering
again the global objective of equation 1, we note that
the difficulty in decoding this objective comes en-
tirely from the constraints z((s,m), h) = Ys(m,h).
If these were not there, the problem would factor
into two parts, an optimization of F over the test
corpus Y(X) and an optimization of g over possible
MRF assignments Z . The first problem factors nat-
urally into sentence-level parsing problems and the
second can be solved efficiently given our assump-
tions on the MRF topology G.
Recent work has shown that a relaxation based
on dual decomposition often produces an exact so-
lution for such problems (Koo et al 2010). To
apply dual decomposition, we introduce Lagrange
multipliers u(s,m, h) for the agreement constraints
between the sentence-level models and the global
MRF. The Lagrangian dual is the function L(u) =
maxz g(z, u) + maxy F (y, u) where
g(z, u) = g(z) +
?
(s,m,h)?J (X)
u(s,m, h)z((s,m), h)),
F (y, u) = F (Y ) ?
?
(s,m,h)?J (X)
u(s,m, h)Ys(m,h)
In order to find minu L(u), we use subgradient de-
scent. This requires computing g(z, u) and F (y, u)
for fixed values of u, which by our assumptions from
Section 3 are efficient to calculate.
The full algorithm is given in Figure 2. We start
with the values of u initialized to 0. At each itera-
tion k, we find the best set of parses Y (k) over the
entire corpus and the best MRF assignment z(k). We
then update the value of u based on the difference
between Y (k) and z(k) and a rate parameter ?. On
the next iteration, we solve the same decoding prob-
? 0.7 ?0.8 ? 0.9 1.0
All Contexts 66.8 57.9 46.8 33.3
Head in Context 76.0 67.9 57.2 42.3
Table 1: Exploratory statistics for constraint selection.
The table shows the percentage of context types for which
the probability of the most frequent head tag is at least p.
Head in Context refers to the subset of contexts where the
most frequent head is within the context itself. Numbers
are based on Section 22 of the Wall Street Journal and are
given for contexts that appear at least 10 times.
lems modified by the new value of u. If at any point
the current solutions Y (k) and z(k) satisfy the con-
sistency constraint, we return their current values.
Otherwise, we stop at a max iteration K and return
the values from the last iteration.
We now give a theorem for the formal guarantees
of this algorithm.
Theorem 1 If for some k ? {1 . . .K} in the algo-
rithm in Figure 2, Y (k)s (m,h) = z(k)(s,m, h) for
all (s,m, h) ? J , then (Y (k), z(k)) is a solution to
the maximization problem in equation 1.
We omit the proof for brevity. It is a slight variation
of the proof given by Rush et al(2010).
7 Consistency Constraints
In this section we describe the consistency con-
straints used for the global models of parsing and
tagging.
Parsing Constraints. Recall from Section 4 that
we choose parsing constraints based on the word
context. We encourage words in similar contexts to
choose head words with similar POS tags.
We use a simple procedure to select which con-
straints to add. First define a context template to
be a set of offsets {r, . . . , s} with r ? 0 ? s that
specify the neighboring words to include in a con-
text. In the example of Figure 1, the context tem-
plate {?1, 0, 1, 2} applied to the word girls/NNS
would produce the context JJ NNS VBD RB. For
each word in the corpus, we consider all possible
templates with s? r < 4. We use only contexts that
predict the head POS of the context in the training
data with probability 1 and prefer long over short
contexts. Once we select the context of each word,
we add a consensus node for each context type in
1439
the corpus. We connect each word node to its corre-
sponding consensus node.
Local context does not fully determine the POS
tag of the head word, but for certain contexts it pro-
vides a strong signal. Table 1 shows context statis-
tics for English. For 46.8% of the contexts, the most
frequent head tag is chosen ? 90% of the time. The
pattern is even stronger for contexts where the most
frequent head tag is within the context itself. In
this case, for 57.2% of the contexts the most fre-
quent head tag is chosen ? 90% of the time. Con-
sequently, if more than one context can be selected
for a word, we favor the contexts where the most
frequent head POS is inside the context.
POS Tagging Constraints. For POS tagging, our
constraints focus on words not observed in the train-
ing data. It is well-known that each word type ap-
pears only with a small number of POS tags. In Sec-
tion 22 of the WSJ corpus, 96.35% of word types
appear with a single POS tag.
In most test sets we are unlikely to see an un-
known word more than once or twice. To fix this
sparsity issue, we import additional unannotated
sentences for each unknown word from the New
York Times Section of the NANC corpus (Graff,
1995). These sentences give additional information
for unknown word types.
Additionally, we note that morphologically re-
lated words often have similar POS tags. We can
exploit this relationship by connecting related word
types to the same consensus node. We experimented
with various morphological variants and found that
connecting a word type with the type generated by
appending the suffix ?s? was most beneficial. For
each unknown word type, we also import sentences
for its morphologically related words.
8 Experiments and Results
We experiment in two common scenarios where
parsing performance is reduced from the fully su-
pervised, in-domain case. In domain adaptation, we
train our model completely in one source domain
and test it on a different target domain. In lightly su-
pervised training, we simulate the case where only
a limited amount of annotated data is available for a
language.
Base ST Model ER
WSJ? QTB 89.63 89.99 90.43 7.7
QTB?WSJ 74.89 74.97 75.76 3.5
Table 2: Dependency parsing UAS for domain adapta-
tion. WSJ is the Penn TreeBank. QTB is the Question-
Bank. ER is error reduction. Results are significant using
the sign test with p ? 0.05.
Data for Domain Adaptation We perform do-
main adaptation experiments in English using the
WSJ PennTreebank (Marcus et al 1993) and the
QuestionBank (QTB) (Judge et al 2006). In the
WSJ ? QTB scenario, we train on sections 2-21
of the WSJ and test on the entire QTB (4000 ques-
tions). In the QTB?WSJ scenario, we train on the
entire QTB and test on section 23 of the WSJ.
Data for Lightly Supervised Training For all
English experiments, our data was taken from the
WSJ PennTreebank: training sentences from Sec-
tion 0, development sentences from Section 22, and
test sentences from Section 23. For experiments
in Bulgarian, German, Japanese, and Spanish, we
use the CONLL-X data set (Buchholz and Marsi,
2006) with training data taken from the official train-
ing files. We trained the sentence-level models with
50-500 sentences. To verify the robustness of our
results, our test sets consist of the official test sets
augmented with additional sentences from the offi-
cial training files such that each test file consists of
25,000 words. Our results on the official test sets are
very similar to the results we report and are omitted
for brevity.
Parameters The model parameters, ?1, ?2, and ?3
of the scoring function (Section 4) and ? of the
Lagrange multipliers update rule (Section 6), were
tuned on the English development data. In our dual
decomposition inference algorithm, we use K =
200 maximum iterations and tune the decay rate fol-
lowing the protocol described by Koo et al(2010).
Sentence-Level Models For dependency parsing
we utilize the second-order projective MST parser
(McDonald et al 2005)1 with the gold-standard
POS tags of the corpus. For POS tagging we use
the Stanford POS tagger (Toutanova et al 2003)2.
1http://sourceforge.net/projects/mstparser/
2http://nlp.stanford.edu/software/tagger.shtml
1440
50 100 200 500
Base ST Model (ER) Base ST Model (ER) Base ST Model (ER) Base ST Model (ER)
Jap 79.10 80.19 81.78 (12.82) 81.53 81.59 83.09 (8.45) 84.84 85.05 85.50 (4.35) 87.14 87.24 87.44 (2.33)
Eng 69.60 69.73 71.62 (6.64) 73.97 74.01 75.27 (4.99) 77.67 77.68 78.69 (4.57) 81.83 81.90 82.18 (1.93)
Spa 71.67 71.72 73.19 (5.37) 74.53 74.63 75.41 (3.46) 77.11 77.09 77.44 (1.44) 79.97 79.88 80.04 (0.35)
Bul 71.10 70.59 72.13 (3.56) 73.35 72.96 74.61 (4.73) 75.38 75.54 76.17 (3.21) 81.95 81.75 82.18 (1.27)
Ger 68.21 68.28 68.83 (1.95) 72.19 72.29 72.76 (2.05) 74.34 74.45 74.95 (2.4) 77.20 77.09 77.51 (1.4)
Table 3: Dependency parsing UAS by size of training set and language. English data is from the WSJ. Bulgarian,
German, Japanese, and Spanish data is from the CONLL-X data sets. Base is the second-order, projective dependency
parser of McDonald et al(2005). ST is a self-training model based on Reichart and Rappoport (2007). Model is the
same parser augmented with inter-sentence constraints. ER is error reduction. Using the sign test with p ? 0.05, all
50, 100, and 200 results are significant, as are Eng and Ger 500.
50 100 200 500
Base Model (ER) Base Model (ER) Base Model (ER) Base Model (ER)
Acc 79.67 81.77 (10.33) 85.42 86.37 (6.52) 88.63 89.37 (6.51) 91.59 91.98 (4.64)
Unk 62.88 67.16 (11.53) 71.10 73.32 (7.68) 75.82 78.07 (9.31) 80.67 82.28 (8.33)
Table 4: POS tagging accuracy. Stanford POS tagger refers to the maximum entropy trigram tagger of Toutanova et
al. (2003). Our inter-sentence POS tagger augments this baseline with global constraints. ER is error reduction. All
results are significant using the sign test with p ? 0.05.
Evaluation and Baselines To measure parsing
performance, we use unlabeled attachment score
(UAS) given by the CONLL-X dependency parsing
shared task evaluation script (Buchholz and Marsi,
2006). We compare the accuracy of dependency
parsing with global constraints to the sentence-level
dependency parser of McDonald et al(2005) and to
a self-training baseline (Steedman et al 2003; Re-
ichart and Rappoport, 2007). The parsing baseline is
equivalent to a single round of dual decomposition.
For the self-training baseline, we parse the test cor-
pus, append the labeled test sentences to the training
corpus, train a new parser, and then re-parse the test
set. We run this procedure for a single iteration.
For POS tagging we measure token level POS ac-
curacy for all the words in the corpus and also for
unknown words (words not observed in the train-
ing data). We compare the accuracy of POS tagging
with global constraints to the accuracy of the Stan-
ford POS tagger 3.
Domain Adaptation Accuracy Results are pre-
sented in Table 2. The constrained model reduces
the error of the baseline on both cases. Note that
when the base parser is trained on the WSJ corpus its
UAS performance on the QTB is 89.63%. Yet, the
constrained model is still able to reduce the baseline
error by 7.7%.
3We do not run self-training for POS tagging as it has been
shown unuseful for this application (Clark et al 2003).
Lightly Supervised Accuracy The parsing results
are given in Table 3. Our model improves over
the baseline parser and self-training across all lan-
guages and training set sizes. The best results are
for Japanese and English with error reductions of
2.33 ? 12.82% and 1.93 ? 6.64% respectively. The
self-training baseline achieves small gains on some
languages, but generally performs similarly to the
standard parser.
The POS tagging results are given in Table 4. Our
model improves over the baseline tagger for the en-
tire training size range. For 50 training sentences
we reduce 10.33% of the overall error, and 11.53%
of the error on unknown words. Although the tagger
performance substantially improves when the train-
ing set grows to 500 sentences, our model still pro-
vides an overall error reduction of 4.64% and of
8.33% for unknown words.
9 Discussion
Efficiency Since dual decomposition often re-
quires hundreds of iterations to converge, a naive im-
plementation would be orders of magnitude slower
than the underlying sentence-level model. We use
two techniques to speed-up the algorithm.
First, we follow Koo et al(2010) and use lazy
decoding as part of dual decomposition. At each it-
eration k, we cache the result of the MRF z(k) and
set of parse tree Y (k). In the next iteration, we only
1441
50 100 150 200iteration0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
percen
tage of
 parsin
g time
englishgermanjapanese
Figure 3: Efficiency of dependency parsing decoding for
three languages. The plot shows the speed of each iter-
ation of the subgradient algorithm relative to a round of
unconstrained parsing.
Most Effective Contexts
WSJ? QTB QTB?WSJ
WRB VBP VBD NN NN ,
DT JJS NN IN IN PRP VBZ
VBP PRP VB JJ JJ NN ,
DT NN NN VB IN JJ JJ NN
RBS JJ NN IN NN POS NN NN
Table 5: The five most effective constraint contexts from
the domain adaptation experiments. The bold POS tag
indicates the modifier word of the context.
Where/
WRB
VBN
are/
VBP
diamonds/
NNS
mined/
VBN
?
How/
WRB
VBP
do/
VBP
you/
PRP
measure/
VB
earthquakes/
NNS
?
Why/
WRB
VBP
do/
VBP
people/
NNS
get/
VB
calluses/
NNS
?
VBP
Figure 4: Subset of sentences with the context WRB VBP
from WSJ? QTB domain adaptation. In the first round,
the parser chooses VBN for the first sentence, which is in-
consistent with similar contexts. The constraints correct
this choice in later rounds.
recompute the solution Y ?s for a sentence s if the
weight u(s,m, h) for some m,h was updated. A
similar technique is applied to the MRF.
Second, during the first iteration of the algorithm
we apply max-marginal based pruning using the
threshold defined by Weiss and Taskar (2010). This
produces a pruned hypergraph for each sentence,
which allows us to avoid recomputing parse features
and to solve a simplified search problem.
To measure efficiency, we compare the time spent
in dual decomposition to the speed of unconstrained
inference. Across experiments, the mean dual de-
composition time is 1.71 times the cost of uncon-
strained inference. Figure 3 shows how this time is
spent after the first iteration. The early iterations are
around 1% of the total cost, and because of lazy de-
coding this quickly drops to almost nothing.
Exactness To measure exactness, we count the
number of sentences for which we should remove
the constraints in order for the model to reach con-
vergence. For dependency parsing, across languages
removing constraints on 0.6% of sentences yields
exact convergence. Removing these constraints has
very little effect on the final outcome of the model.
For POS tagging, the algorithm finds an exact so-
lution after removing constraints from 0.2% of the
sentences.
Constraint Analysis We can also look at the num-
ber, size, and outcome of the constraints chosen in
the experiments. In the lightly supervised experi-
ments, the average number of constraints is 3298 for
25000 tokens, where the median constraint connects
19 different tokens. Of these constraints around 70%
are active (non-NULL). The domain adaptation ex-
periments have a similar number of constraints with
around 75% of constraints active. In both experi-
ments many of the constraints are found to be con-
sistent after the first iteration, but as Figure 3 im-
plies, other constraints take multiple iterations to
converge.
Qualitative Analysis In order to understand why
these simple consistency constraints are effective,
we take a qualitative look at the the domain adap-
tation experiments on the QuestionBank. Table 5
ranks the five most effective contextual constraints
from both experiments. For the WSJ? QTB exper-
iment, the most effective constraint relates the inital
question word with an adjacent verb. Figure 4 shows
1442
sentences where this constraint applies in the Ques-
tionBank. For the QTB?WSJ experiment, the ef-
fective contexts are mostly long base noun phrases.
These occur often in the WSJ but are rare in the sim-
pler QuestionBank sentences.
10 Conclusion
In this work we experiment with inter-sentence
consistency constraints for dependency parsing and
POS tagging. We have proposed a corpus-level ob-
jective that augments sentence-level models with
such constraints and described an exact and effi-
cient dual decomposition algorithm for its decod-
ing. In future work, we intend to explore efficient
techniques for joint parameter learning for both the
global MRF and the local models.
Acknowledgments Columbia University gratefully
acknowledges the support of the Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not
necessarily reflect the view of DARPA, AFRL, or the
US government. Alexander Rush was supported by a
National Science Foundation Graduate Research Fellow-
ship.
References
Y. Altun and D. Mcallester. 2005. Maximum margin
semi-supervised learning for structured variables. In
NIPS.
M. Belkin, P. Niyogi, and V. Sindhwani. 2005. On man-
ifold regularization. In AISTATS.
John Blitzer and Hal Daume. 2010. Icml 2010 tutorial
on domain adaptation. In ICML.
U. Brefeld and T. Scheffer. 2006. Semi-supervised learn-
ing for structured output variables. In ICML.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
R.C. Bunescu and R.J. Mooney. 2004. Collective infor-
mation extraction with relational markov networks. In
ACL.
J.C.K Cheung and G. Penn. 2010. Utilizing extra-
sentential context for parsing. In EMNLP.
Stephen Clark, James Curran, and Miles Osborne. 2003.
Bootstrapping pos taggers using unlabelled data. In
CoNLL.
A. Dubey, F. Keller, and P. Sturt. 2009. A proba-
bilistic corpus-based model of parallelism. Cognition,
109(2):193?210.
Jenny Rose Finkel and Christopher Manning. 2009. Hi-
erarchical bayesian domain adaptation. In NAACL.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In ACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior Regularization for Structured Latent
Variable Models. Journal of Machine Learning Re-
search, 11:2001?2049.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Sparsity in dependency grammar in-
duction. In Proceedings of the ACL Conference Short
Papers.
A. Globerson and T. Jaakkola. 2007. Fixing max-
product: Convergent message passing algorithms for
map lp-relaxations. In NIPS.
D. Graff. 1995. North american news text corpus. Lin-
guistic Data Consortium, LDC95T21.
Rahul Gupta, Sunita Sarawagi, and Ajit A. Diwan. 2010.
Collective inference for extraction mrfs coupled with
symmetric clique potentials. JMLR.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3):253?276.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In ACL-COLING.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In ICCV.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In IJCNLP.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
from measurements in exponential families. In ICML.
G.S. Mann and A. McCallum. Generalized expectation
criteria for semi-supervised learning with weakly la-
beled data. Journal of Machine Learning Research,
11:955?984.
G.S. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In ICML.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313?330.
1443
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In ACL, sort papers.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adapatation for parsing. In
NAACL.
R.T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In HLT/EMNLP.
Barbara Plank. 2011. Domain Adaptation for Parsing.
Ph.d. thesis, University of Groningen.
R. Reichart and A. Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In ACL.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains. In
EMNLP.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In ICML.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
Kenji Sagae and Junichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with lr models and parser
ensembles. In EMNLP-CoNLL.
D.A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In EMNLP.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In EACL.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-supervised
learning of structured tagging models. In EMNLP.
C. Sutton and A. Mccallum. 2004. Collective segmen-
tation and labeling of distant entities in information
extraction. In In ICML Workshop on Statistical Re-
lational Learning and Its Connections.
B. Taskar, P. Abbeel, and d. Koller. 2002. Discriminative
probabilistic models for relational data. In UAI.
K. Toutanova, D. Klein, C.D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP
estimation via agreement on trees: message-passing
and linear programming. In IEEE Transactions on In-
formation Theory, volume 51, pages 3697?3717.
Y. Wang, G. Haffari, S. Wang, and G. Mori. 2009.
A rate distortion approach for semi-supervised condi-
tional random fields. In NIPS.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS, volume 1284.
1444
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 629?637,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Selective Sharing for Multilingual Dependency Parsing
Tahira Naseem
CSAIL, MIT
tahira@csail.mit.edu
Regina Barzilay
CSAIL, MIT
regina@csail.mit.edu
Amir Globerson
Hebrew University
gamir@cs.huji.ac.il
Abstract
We present a novel algorithm for multilin-
gual dependency parsing that uses annotations
from a diverse set of source languages to parse
a new unannotated language. Our motiva-
tion is to broaden the advantages of multilin-
gual learning to languages that exhibit signif-
icant differences from existing resource-rich
languages. The algorithm learns which as-
pects of the source languages are relevant for
the target language and ties model parame-
ters accordingly. The model factorizes the
process of generating a dependency tree into
two steps: selection of syntactic dependents
and their ordering. Being largely language-
universal, the selection component is learned
in a supervised fashion from all the training
languages. In contrast, the ordering decisions
are only influenced by languages with simi-
lar properties. We systematically model this
cross-lingual sharing using typological fea-
tures. In our experiments, the model con-
sistently outperforms a state-of-the-art multi-
lingual parser. The largest improvement is
achieved on the non Indo-European languages
yielding a gain of 14.4%.1
1 Introduction
Current top performing parsing algorithms rely on
the availability of annotated data for learning the
syntactic structure of a language. Standard ap-
proaches for extending these techniques to resource-
lean languages either use parallel corpora or rely on
1The source code for the work presented in this paper is
available at http://groups.csail.mit.edu/rbg/code/unidep/
annotated trees from other source languages. These
techniques have been shown to work well for lan-
guage families with many annotated resources (such
as Indo-European languages). Unfortunately, for
many languages there are no available parallel cor-
pora or annotated resources in related languages.
For such languages the only remaining option is to
resort to unsupervised approaches, which are known
to produce highly inaccurate results.
In this paper, we present a new multilingual al-
gorithm for dependency parsing. In contrast to pre-
vious approaches, this algorithm can learn depen-
dency structures using annotations from a diverse
set of source languages, even if this set is not re-
lated to the target language. In our selective shar-
ing approach, the algorithm learns which aspects of
the source languages are relevant for the target lan-
guage and ties model parameters accordingly. This
approach is rooted in linguistic theory that charac-
terizes the connection between languages at various
levels of sharing. Some syntactic properties are uni-
versal across languages. For instance, nouns take ad-
jectives and determiners as dependents, but not ad-
verbs. However, the order of these dependents with
respect to the parent is influenced by the typological
features of each language.
To implement this intuition, we factorize genera-
tion of a dependency tree into two processes: selec-
tion of syntactic dependents and their ordering. The
first component models the distribution of depen-
dents for each part-of-speech tag, abstracting over
their order. Being largely language-universal, this
distribution can be learned in a supervised fashion
from all the training languages. On the other hand,
629
ordering of dependents varies greatly across lan-
guages and therefore should only be influenced by
languages with similar properties. Furthermore, this
similarity has to be expressed at the level of depen-
dency types ? i.e., two languages may share noun-
adposition ordering, but differ in noun-determiner
ordering. To systematically model this cross-lingual
sharing, we rely on typological features that reflect
ordering preferences of a given language. In addi-
tion to the known typological features, our parsing
model embeds latent features that can capture cross-
lingual structural similarities.
While the approach described so far supports a
seamless transfer of shared information, it does not
account for syntactic properties of the target lan-
guage unseen in the training languages. For in-
stance, in the CoNLL data, Arabic is the only lan-
guage with the VSO ordering. To handle such cases,
our approach augments cross-lingual sharing with
unsupervised learning on the target languages.
We evaluated our selective sharing model on 17
languages from 10 language families. On this di-
verse set, our model consistently outperforms state-
of-the-art multilingual dependency parsers. Per-
formance gain, averaged over all the languages, is
5.9% when compared to the highest baseline. Our
model achieves the most significant gains on non-
Indo-European languages, where we see a 14.4%
improvement. We also demonstrate that in the ab-
sence of observed typological information, a set of
automatically induced latent features can effectively
work as a proxy for typology.
2 Related Work
Traditionally, parallel corpora have been a main-
stay of multilingual parsing (Wu, 1997; Kuhn, 2004;
Smith and Smith, 2004; Hwa et al, 2005; Xi and
Hwa, 2005; Burkett and Klein, 2008; Snyder et al,
2009). However, recent work in multilingual pars-
ing has demonstrated the feasibility of transfer in the
absence of parallel data. As a main source of guid-
ance, these methods rely on the commonalities in de-
pendency structure across languages. For instance,
Naseem et al (2010) explicitly encode these similar-
ities in the form of universal rules which guide gram-
mar induction in the target language. An alterna-
tive approach is to directly employ a non-lexicalized
parser trained on one language to process a target
language (Zeman and Resnik, 2008; McDonald et
al., 2011; S?gaard, 2011). Since many unlexicalized
dependencies are preserved across languages, these
approaches are shown to be effective for related
languages. For instance, when applied to the lan-
guage pairs within the Indo-European family, such
parsers outperform unsupervised monolingual tech-
niques by a significant margin.
The challenge, however, is to enable dependency
transfer for target languages that exhibit structural
differences from source languages. In such cases,
the extent of multilingual transfer is determined by
the relation between source and target languages.
Berg-Kirkpatrick and Klein (2010) define such a re-
lation in terms of phylogenetic trees, and use this
distance to selectively tie the parameters of mono-
lingual syntactic models. Cohen et al (2011) do not
use a predefined linguistic hierarchy of language re-
lations, but instead learn the contribution of source
languages to the training mixture based on the like-
lihood of the target language. S?gaard (2011)
proposes a different measure of language related-
ness based on perplexity between POS sequences
of source and target languages. Using this measure,
he selects a subset of training source sentences that
are closer to the target language. While all of the
above techniques demonstrate gains from modeling
language relatedness, they still underperform when
the source and target languages are unrelated.
Our model differs from the above approaches in
its emphasis on the selective information sharing
driven by language relatedness. This is further com-
bined with monolingual unsupervised learning. As
our evaluation demonstrates, this layered approach
broadens the advantages of multilingual learning to
languages that exhibit significant differences from
the languages in the training mix.
3 Linguistic Motivation
Language-Independent Dependency Properties
Despite significant syntactic differences, human lan-
guages exhibit striking similarity in dependency pat-
terns. For a given part-of-speech tag, the set of tags
that can occur as its dependents is largely consistent
across languages. For instance, adverbs and nouns
are likely to be dependents of verbs, while adjectives
630
are not. Thus, these patterns can be freely trans-
ferred across languages.
Shared Dependency Properties Unlike dependent
selection, the ordering of dependents in a sentence
differs greatly across languages. In fact, cross-
lingual syntactic variations are primarily expressed
in different ordering of dependents (Harris, 1968;
Greenberg, 1963). Fortunately, the dimensions of
these variations have been extensively studied in lin-
guistics and are documented in the form of typo-
logical features (Comrie, 1989; Haspelmath et al,
2005). For instance, most languages are either dom-
inantly prepositional like English or post-positional
like Urdu. Moreover, a language may be close to dif-
ferent languages for different dependency types. For
instance, Portuguese is a prepositional language like
English, but the order of its noun-adjective depen-
dency is different from English and matches that of
Arabic. Therefore, we seek a model that can express
parameter sharing at the level of dependency types
and can benefit from known language relations.
Language-specific Dependency Variations Not
every aspect of syntactic structure is shared across
languages. This is particularly true given a limited
number of supervised source languages; it is quite
likely that a target language will have previously un-
seen syntactic phenomena. In such a scenario, the
raw text in the target language might be the only
source of information about its unique aspects.
4 Model
We propose a probabilistic model for generating
dependency trees that facilitates parameter sharing
across languages. We assume a setup where de-
pendency tree annotations are available for a set of
source languages and we want to use these annota-
tions to infer a parser for a target language. Syn-
tactic trees for the target language are not available
during training. We also assume that both source
and target languages are annotated with a coarse
parts-of-speech tagset which is shared across lan-
guages. Such tagsets are commonly used in multilin-
gual parsing (Zeman and Resnik, 2008; McDonald
et al, 2011; S?gaard, 2011; Naseem et al, 2010).
The key feature of our model is a two-tier ap-
proach that separates the selection of dependents
from their ordering:
1. Selection Component: Determines the depen-
dent tags given the parent tag.
2. Ordering Component: Determines the position
of each dependent tag with respect to its parent
(right or left) and the order within the right and
left dependents.
This factorization constitutes a departure from
traditional parsing models where these decisions are
tightly coupled. By separating the two, the model
is able to support different degrees of cross-lingual
sharing on each level.
For the selection component, a reasonable ap-
proximation is to assume that it is the same for all
languages. This is the approach we take here.
As mentioned in Section 3, the ordering of depen-
dents is largely determined by the typological fea-
tures of the language. We assume that we have a
set of such features for every language l, and denote
this feature vector by vl. We also experiment with a
variant of our model where typological features are
not observed. Instead, the model captures structural
variations across languages by means of a small set
of binary latent features. The values of these fea-
tures are language dependent. We denote the set of
latent features for language l by bl.
Finally, based on the well known fact that long
distance dependencies are less likely (Eisner and
Smith, 2010), we bias our model towards short de-
pendencies. This is done by imposing a corpus-level
soft constraint on dependency lengths using the pos-
terior regularization framework (Grac?a et al, 2007).
4.1 Generative Process
Our model generates dependency trees one fragment
at a time. A fragment is defined as a subtree com-
prising the immediate dependents of any node in the
tree. The process recursively generates fragments
in a head outwards manner, where the distribution
over fragments depends on the head tag. If the gen-
erated fragment is not empty then the process con-
tinues for each child tag in the fragment, drawing
new fragments from the distribution associated with
the tag. The process stops when there are no more
non-empty fragments.
A fragment with head node h is generated in lan-
guage l via the following stages:
631
h{N,A,N, V,D}
h
{N,N, V } {A,D}
h
N NV D A
(a) (b) (c)
Figure 1: The steps of the generative process for a fragment with head h. In step (a), the unordered set of dependents
is chosen. In step (b) they are partitioned into left and right unordered sets. Finally, each set is ordered in step (c).
? Generate the set of dependents of h via a distri-
bution Psel(S|h). Here S is an unordered set of
POS tags. Note that this part is universal (i.e.,
it does not depend on the language l).
? For each element in S decide whether it should
go to the right or left of h as follows: for every
a ? S, draw its direction from the distribution
Pord(d|a, h, l), where d ? {R,L}. This results
in two unordered sets SR, SL, the right and left
dependents of h. This part does depend on the
language l, since the relative ordering of depen-
dents is not likely to be universal.
? Order the sets SR, SL. For simplicity, we as-
sume that the order is drawn uniformly from
all the possible unique permutations over SR
and SL. We denote the number of such unique
permutations of SR by n(SR).2 Thus the prob-
ability of each permutation of SR is 1n(SR)
3.
Figure 1 illustrates the generative process. The first
step constitutes the selection component and the last
two steps constitute the ordering component. Given
this generation scheme, the probability P (D) of
generating a given fragment D with head h will be:
Psel({D}|h)
?
a?D
Pord(dD(a)|a, h, l)
1
n(DR)n(DL)
(1)
Where we use the following notations:
? DR, DL denote the parts of the fragment that
are to the left and right of h.
2This number depends on the count of each distinct tag in
SR. For example if SR = {N,N,N} then n(SR) = 1. If
SR = {N,D, V } then n(SR) = 3!.
3We acknowledge that assuming a uniform distribution over
the permutations of the right and left dependents is linguistically
counterintuitive. However, it simplifies the model by greatly
reducing the number of parameters to learn.
? {D} is the unordered set of tags in D.
? dD(a) is the position (either R or L) of the de-
pendent a w.r.t. the head of D.
In what follows we discuss the parameterizations
of the different distributions.
4.1.1 Selection Component
The selection component draws an unordered set
of tags S given the head tag h. We assume that the
process is carried out in two steps. First the number
of dependents n is drawn from a distribution:
Psize(n|h) = ?size(n|h) (2)
where ?size(n|h) is a parameter for each value of
n and h. We restrict the maximum value of n to
four, since this is a reasonable bound on the total
number of dependents for a single parent node in
a tree. These parameters are non-negative and sat-
isfy
?
n ?size(n|h) = 1. In other words, the size
is drawn from a categorical distribution that is fully
parameterized.
Next, given the size n, a set S with |S| = n is
drawn according to the following log-linear model:
Pset(S|h, n) =
1
Zset(h, n)
e
?
Si?S
?sel(Si|h)
Zset(h, n) =
?
S:|S|=n
e
?
Si?S
?sel(Si|h)
In the above, Si is the ith POS tag in the unordered
set S, and ?sel(Si|h) are parameters. Thus, large val-
ues of ?sel(Si|h) indicate that POS Si is more likely
to appear in the subset with parent POS h.
Combining the above two steps we have the fol-
lowing distribution for selecting a set S of size n:
Psel(S|h) = Psize(n|h)Pset(S|h, n) . (3)
632
ID Feature Description Values
81A Order of Subject, Object and Verb SVO, SOV, VSO, VOS, OVS, OSV
85A Order of Adposition and Noun Postpositions, Prepositions, Inpositions
86A Order of Genitive and Noun Genitive-Noun, Noun-Genitive
87A Order of Adjective and Noun Adjective-Noun, Noun-Adjective
88A Order of Demonstrative and Noun Demonstrative-Noun, Noun-Demonstrative
89A Order of Numeral and Noun Numeral-Noun, Noun-Numeral
Table 1: The set of typological features that we use in our model. For each feature, the first column gives the ID of
the feature as used in WALS, the second column describes the feature and the last column enumerates the allowable
values for the feature. Besides these values, each feature can also have a value of ?No dominant order?.
4.1.2 Ordering Component
The ordering component consists of distributions
Pord(d|a, h, l) that determine whether tag a will be
mapped to the left or right of the head tag h. We
model it using the following log-linear model:
Pord(d|a, h, l) =
1
Zord(a, h, l)
eword?g(d,a,h,vl)
Zord(a, h, l) =
?
d?{R,L}
eword?g(d,a,h,vl)
Note that in the above equations the ordering
component depends on the known typological fea-
tures vl. In the setup when typological features are
not known, vl is replaced with the latent ordering
feature set bl.
The feature vector g contains indicator features
for combinations of a, h, d and individual features
vli (i.e., the ith typological features for language l).
4.2 Typological Features
The typological features we use are a subset of
order-related typological features from ?The World
Atlas of Language Structure? (Haspelmath et al,
2005). We include only those features whose val-
ues are available for all the languages in our dataset.
Table 1 summarizes the set of features that we use.
Note that we do not explicitly specify the correspon-
dence between these features and the model param-
eters. Instead, we leave it for the model to learn this
correspondence automatically.
4.3 Dependency Length Constraint
To incorporate the intuition that long distance de-
pendencies are less likely, we impose a posterior
constraint on dependency length. In particular, we
use the Posterior Regularization (PR) framework of
Grac?a et al (2007). The PR framework incorporates
constraints by adding a penalty term to the standard
likelihood objective. This term penalizes the dis-
tance of the model posterior from a set Q, where
Q contains all the posterior distributions that satisfy
the constraints. In our case the constraint is that the
expected dependency length is less than or equal to
a pre-specified threshold value b. If we denote the
latent dependency trees by z and the observed sen-
tences by x then
Q = {q(z|x) : Eq[f(x, z)] ? b} (4)
where f(x, z) computes the sum of the lengths of all
dependencies in z with respect to the linear order of
x. We measure the length of a dependency relation
by counting the number of tokens between the head
and its modifier. The PR objective penalizes the KL-
divergence of the model posterior from the set Q:
L?(x)?KL (Q ? p?(z|x))
where ? denotes the model parameters and the first
term is the log-likelihood of the data. This objective
can be optimized using a modified version of the EM
algorithm (Grac?a et al, 2007).
5 Parameter Learning
Our model is parameterized by the parameters ?sel,
?size and word. We learn these by maximizing the
likelihood of the training data. As is standard, we
add `2 regularization on the parameters and tune it
on source languages. The likelihood is marginalized
over all latent variables. These are:
? For sentences in the target language: all pos-
sible derivations that result in the observed
POS tag sequences. The derivations include
the choice of unordered sets size n, the un-
ordered sets themselves S, their left/right al-
633
locations and the orderings within the left and
right branches.
? For all languages: all possible values of the la-
tent features bl.4
Since we are learning with latent variables, we use
the EM algorithm to monotonically improve the
likelihood. At each E step, the posterior over latent
variables is calculated using the current model. At
the M step this posterior is used to maximize the
likelihood over the fully observed data. To com-
pensate for the differences in the amount of training
data, the counts from each language are normalized
before computing the likelihood.
The M step involves finding maximum likelihood
parameters for log-linear models in Equations 3 and
4. This is done via standard gradient based search;
in particular, we use the method of BFGS.
We now briefly discuss how to calculate the pos-
terior probabilities. For estimating the word param-
eters we require marginals of the type P (bli|Dl;wt)
where Dl are the sentences in language l, bli is the
ith latent feature for the language l and wt are the
parameter values at iteration t. Consider doing this
for a source language l. Since the parses are known,
we only need to marginalize over the other latent
features. This can be done in a straightforward man-
ner by using our probabilistic model. The complex-
ity is exponential in the number of latent features,
since we need to marginalize over all features other
than bli. This is feasible in our case, since we use a
relatively small number of such features.
When performing unsupervised learning for the
target language, we need to marginalize over possi-
ble derivations. Specifically, for the M step, we need
probabilities of the form P (a modifies h|Dl;wt).
These can be calculated using a variant of the inside
outside algorithm. The exact version of this algo-
rithm would be exponential in the number of depen-
dents due to the 1n(Sr) term in the permutation factor.
Although it is possible to run this exact algorithm in
our case, where the number of dependents is limited
to 4, we use an approximation that works well in
practice: instead of 1n(Sr) we use
1
|Sr|!
. In this case
the runtime is no longer exponential in the number
of children, so inference is much faster.
4This corresponds to the case when typological features are
not known.
Finally, given the trained parameters we generate
parses in the target language by calculating the max-
imum a posteriori derivation. This is done using a
variant of the CKY algorithm.
6 Experimental Setup
Datasets and Evaluation We test the effectiveness
of our approach on 17 languages: Arabic, Basque,
Bulgarian, Catalan, Chinese, Czech, Dutch, English,
German, Greek, Hungarian, Italian, Japanese, Por-
tuguese, Spanish, Swedish and Turkish. We used
datasets distributed for the 2006 and 2007 CoNLL
Shared Tasks (Buchholz and Marsi, 2006; Nivre
et al, 2007). Each dataset provides manually an-
notated dependency trees and POS tags. To en-
able crosslingual sharing, we map the gold part-
of-speech tags in each corpus to a common coarse
tagset (Zeman and Resnik, 2008; S?gaard, 2011;
McDonald et al, 2011; Naseem et al, 2010). The
coarse tagset consists of 11 tags: noun, verb, ad-
jective, adverb, pronoun, determiner, adposition, nu-
meral, conjunction, particle, punctuation mark, and
X (a catch-all tag). Among several available fine-
to-coarse mapping schemes, we employ the one of
Naseem et al (2010) that yields consistently better
performance for our method and the baselines than
the mapping proposed by Petrov et al (2011).
As the evaluation metric, we use directed depen-
dency accuracy. Following standard evaluation prac-
tices, we do not evaluate on punctuation. For both
the baselines and our model we evaluate on all sen-
tences of length 50 or less ignoring punctuation.
Training Regime Our model typically converges
quickly and does not require more than 50 iterations
of EM. When the model involves latent typological
variables, the initialization of these variables can im-
pact the final performance. As a selection criterion
for initialization, we consider the performance of the
final model averaged over the supervised source lan-
guages. We perform ten random restarts and select
the best according to this criterion. Likewise, the
threshold value b for the PR constraint on the depen-
dency length is tuned on the source languages, using
average test set accuracy as the selection criterion.
Baselines We compare against the state-of-the-art
multilingual dependency parsers that do not use par-
allel corpora for training. All the systems were eval-
634
uated using the same fine-to-coarse tagset mapping.
The first baseline, Transfer, uses direct transfer of a
discriminative parser trained on all the source lan-
guages (McDonald et al, 2011). This simple base-
line achieves surprisingly good results, within less
than 3% difference from a parser trained using par-
allel data. In the second baseline (Mixture), pa-
rameters of the target language are estimated as a
weighted mixture of the parameters learned from an-
notated source languages (Cohen et al, 2011). The
underlying parsing model is the dependency model
with valance (DMV) (Klein and Manning, 2004).
Originally, the baseline methods were evaluated on
different sets of languages using a different tag map-
ping. Therefore, we obtained new results for these
methods in our setup. For the Transfer baseline,
for each target language we trained the model on
all other languages in our dataset. For the Mixture
baseline, we trained the model on the same four lan-
guages used in the original paper ? English, Ger-
man, Czech and Italian. When measuring the per-
formance on these languages, we selected another
set of four languages with a similar level of diver-
sity.5
7 Results
Table 2 summarizes the performance for different
configurations of our model and the baselines.
Comparison against Baselines On average, the
selective sharing model outperforms both base-
lines, yielding 8.9% gain over the weighted mixture
model (Cohen et al, 2011) and 5.9% gain over the
direct transfer method (McDonald et al, 2011). Our
model outperforms the weighted mixture model on
15 of the 17 languages and the transfer method on
12 of the 17 languages. Most of the gains are ob-
tained on non-Indo-European languages, that have
little similarity with the source languages. For this
set, the average gain over the transfer baseline is
14.4%. With some languages, such as Japanese,
achieving gains of as much as 30%.
On Indo-European languages, the model perfor-
mance is almost equivalent to that of the best per-
forming baseline. To explain this result we con-
5We also experimented with a version of the Cohen et al
(2011) model trained on all the source languages. This setup
resulted in decreased performance. For this reason, we chose to
train the model on the four languages.
sider the performance of the supervised version of
our model which constitutes an upper bound on the
performance. The average accuracy of our super-
vised model on these languages is 66.8%, compared
to the 76.3% of the unlexicalized MST parser. Since
Indo-European languages are overrepresented in our
dataset, a target language from this family is likely
to exhibit more similarity to the training data. When
such similarity is substantial, the transfer baseline
will benefit from the power of a context-rich dis-
criminative parser.
A similar trait can be seen by comparing the per-
formance of our model to an oracle version of our
model which selects the optimal source language
for a given target language (column 7). Overall,
our method performs similarly to this oracle variant.
However, the gain for non Indo-European languages
is 1.9% vs -1.3% for Indo-European languages.
Analysis of Model Properties We first test our
hypothesis about the universal nature of the depen-
dent selection. We compare the performance of
our model (column 6) against a variant (column 8)
where this component is trained from annotations on
the target language. The performance of the two is
very close ? 1.8%, supporting the above hypothesis.
To assess the contribution of other layers of selec-
tive sharing, we first explore the role of typological
features in learning the ordering component. When
the model does not have access to observed typo-
logical features, and does not use latent ones (col-
umn 4), the accuracy drops by 2.6%6. For some
languages (e.g., Turkish) the decrease is very pro-
nounced. Latent typological features (column 5) do
not yield the same gain as observed ones, but they do
improve the performance of the typology-free model
by 1.4%.
Next, we show the importance of using raw tar-
get language data in training the model. When
the model has to make all the ordering decisions
based on meta-linguistic features without account
for unique properties of the target languages, the
performance decreases by 0.9% (see column 3).
To assess the relative difficulty of learning the
ordering and selection components, we consider
model variants where each of these components is
6In this setup, the ordering component is trained in an unsu-
pervised fashion on the target language.
635
Baselines Selective Sharing Model
Mixture Transfer (D-,To) (D+) (D+,Tl) (D+,To) Best Pair Sup. Sel. Sup. Ord. MLE
Catalan 64.9 69.5 71.9 66.1 66.7 71.8 74.8 70.2 73.2 72.1
Italian 61.9 68.3 68.0 65.5 64.2 65.6 68.3 65.1 70.7 72.3
Portuguese 72.9 75.8 76.2 72.3 76.0 73.5 76.4 77.4 77.6 79.6
Spanish 57.2 65.9 62.3 58.5 59.4 62.1 63.4 61.5 62.6 65.3
Dutch 50.1 53.9 56.2 56.1 55.8 55.9 57.8 56.3 58.6 58.0
English 45.9 47.0 47.6 48.5 48.1 48.6 44.4 46.3 60.0 62.7
German 54.5 56.4 54.0 53.5 54.3 53.7 54.8 52.4 56.2 58.0
Swedish 56.4 63.6 52.0 61.4 60.6 61.5 63.5 67.9 67.1 73.0
Bulgarian 67.7 64.0 67.6 63.5 63.9 66.8 66.1 66.2 69.5 71.0
Czech 39.6 40.3 43.9 44.7 45.4 44.6 47.5 53.2 51.2 58.9
Arabic 44.8 40.7 57.2 58.8 60.3 58.9 57.6 62.9 61.9 64.2
Basque 32.8 32.4 39.7 40.1 39.8 47.6 42.0 46.2 47.9 51.6
Chinese 46.7 49.3 59.9 52.2 52.0 51.2 65.4 62.3 65.5 73.5
Greek 56.8 60.4 61.9 67.5 67.3 67.4 60.6 67.2 69.0 70.5
Hungarian 46.8 54.3 56.9 58.4 58.8 58.5 57.0 57.4 62.0 61.6
Japanese 33.5 34.7 62.3 56.8 61.4 64.0 54.8 63.4 69.7 75.6
Turkish 28.3 34.3 59.1 43.6 57.8 59.2 56.9 66.6 59.5 67.6
Average 50.6 53.6 58.6 56.9 58.3 59.5 59.5 61.3 63.7 66.8
Table 2: Directed dependency accuracy of different variants of our selective sharing model and the baselines. The
first section of the table (column 1 and 2) shows the accuracy of the weighted mixture baseline (Cohen et al, 2011)
(Mixture) and the multi-source transfer baseline (McDonald et al, 2011) (Transfer). The middle section shows the
performance of our model in different settings. D? indicates the presence/absence of raw target language data during
training. To indicates the use of observed typological features for all languages and Tl indicates the use of latent
typological features for all languages. The last section shows results of our model with different levels of oracle
supervision: a. (Best Pair) Model parameters are borrowed from the best source language based on the accuracy on
the target language b. (Sup. Sel.) Selection component is trained using MLE estimates from target language c. (Sup.
Ord.) Ordering component is trained using MLE estimates from the target language d. (MLE) All model parameters
are trained on the target language in a supervised fashion. The horizontal partitions separate language families. The
first three families are sub-divisions of the Indo-European language family.
trained using annotations in the target language. As
shown in columns 8 and 9, these two variants out-
perform the original model, achieving 61.3% for su-
pervised selection and 63.7% for supervised order-
ing. Comparing these numbers to the accuracy of
the original model (column 6) demonstrates the dif-
ficulty inherent in learning the ordering information.
This finding is expected given that ordering involves
selective sharing from multiple languages.
Overall, the performance gap between the selec-
tive sharing model and its monolingual supervised
counterpart is 7.3%. In contrast, the unsupervised
monolingual variant of our model achieves a mea-
ger 26%.7 This demonstrates that our model can ef-
fectively learn relevant aspects of syntactic structure
from a diverse set of languages.
7This performance is comparable to other generative models
such as DMV (Klein and Manning, 2004).
8 Conclusions
We present a novel algorithm for multilingual de-
pendency parsing that uses annotations from a di-
verse set of source languages to parse a new unan-
notated language. Overall, our model consistently
outperforms the multi-source transfer based depen-
dency parser of McDonald et al (2011). Our ex-
periments demonstrate that the model is particularly
effective in processing languages that exhibit signif-
icant differences from the training languages.
Acknowledgments
The authors acknowledge the support of the NSF
(IIS-0835445), the MURI program (W911NF-10-
1-0533), the DARPA BOLT program, and the ISF
(1789/11). We thank Tommi Jaakkola, Ryan Mc-
Donald and the members of the MIT NLP group for
their comments.
636
References
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In ACL, pages 1288?1297.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP, pages 877?886.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In EMNLP, pages 50?61.
Bernard Comrie. 1989. Language Universals and Lin-
guistic Typology: Syntax and Morphology. Oxford:
Blackwell.
Jason Eisner and Noah A. Smith. 2010. Favor short de-
pendencies: Parsing with soft and hard constraints on
dependency length. In Trends in Parsing Technology:
Dependency Parsing, Domain Adaptation, and Deep
Parsing, pages 121?150.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
In Advances in NIPS, pages 569?576.
Joseph H Greenberg. 1963. Some universals of language
with special reference to the order of meaningful ele-
ments. In Joseph H Greenberg, editor, Universals of
Language, pages 73?113. MIT Press.
Z.S. Harris. 1968. Mathematical structures of language.
Wiley.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie, editors. 2005. The World Atlas of
Language Structures. Oxford University Press.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion across parallel texts. Journal of Natural Language
Engineering, 11(3):311?325.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL,
pages 478?485.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the ACL, pages
470?477.
Ryan T. McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In EMNLP, pages 62?72.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowledge
to guide grammar induction. In EMNLP, pages 1234?
1244.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. In ArXiv, April.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceeding of EMNLP, pages 49?
56.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proceedings of ACL/AFNLP, pages 73?81.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In ACL
(Short Papers), pages 682?686.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-english languages.
In Proceedings of EMNLP, pages 851 ? 858.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In Pro-
ceedings of the IJCNLP-08 Workshop on NLP for Less
Privileged Languages, pages 35?42, January.
637
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 291?301,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Transfer Learning for Constituency-Based Grammars
Yuan Zhang, Regina Barzilay
Massachusetts Institute of Technology
{yuanzh, regina}@csail.mit.edu
Amir Globerson
The Hebrew University
gamir@cs.huji.ac.il
Abstract
In this paper, we consider the problem
of cross-formalism transfer in parsing.
We are interested in parsing constituency-
based grammars such as HPSG and CCG
using a small amount of data specific for
the target formalism, and a large quan-
tity of coarse CFG annotations from the
Penn Treebank. While all of the target
formalisms share a similar basic syntactic
structure with Penn Treebank CFG, they
also encode additional constraints and se-
mantic features. To handle this appar-
ent discrepancy, we design a probabilistic
model that jointly generates CFG and tar-
get formalism parses. The model includes
features of both parses, allowing trans-
fer between the formalisms, while pre-
serving parsing efficiency. We evaluate
our approach on three constituency-based
grammars ? CCG, HPSG, and LFG, aug-
mented with the Penn Treebank-1. Our ex-
periments show that across all three for-
malisms, the target parsers significantly
benefit from the coarse annotations.1
1 Introduction
Over the last several decades, linguists have in-
troduced many different grammars for describing
the syntax of natural languages. Moreover, the
ongoing process of developing new formalisms is
intrinsic to linguistic research. However, before
these grammars can be used for statistical pars-
ing, they require annotated sentences for training.
The difficulty of obtaining such annotations is a
key limiting factor that inhibits the effective use of
these grammars.
1The source code for the work is available at
http://groups.csail.mit.edu/rbg/code/
grammar/acl2013.
The standard solution to this bottleneck has re-
lied on manually crafted transformation rules that
map readily available syntactic annotations (e.g,
the Penn Treebank) to the desired formalism. De-
signing these transformation rules is a major un-
dertaking which requires multiple correction cy-
cles and a deep understanding of the underlying
grammar formalisms. In addition, designing these
rules frequently requires external resources such
as Wordnet, and even involves correction of the
existing treebank. This effort has to be repeated
for each new grammar formalism, each new anno-
tation scheme and each new language.
In this paper, we propose an alternative ap-
proach for parsing constituency-based grammars.
Instead of using manually-crafted transformation
rules, this approach relies on a small amount of
annotations in the target formalism. Frequently,
such annotations are available in linguistic texts
that introduce the formalism. For instance, a
textbook on HPSG (Pollard and Sag, 1994) il-
lustrates grammatical constructions using about
600 examples. While these examples are infor-
mative, they are not sufficient for training. To
compensate for the annotation sparsity, our ap-
proach utilizes coarsely annotated data readily
available in large quantities. A natural candidate
for such coarse annotations is context-free gram-
mar (CFG) from the Penn Treebank, while the
target formalism can be any constituency-based
grammars, such as Combinatory Categorial Gram-
mar (CCG) (Steedman, 2001), Lexical Functional
Grammar (LFG) (Bresnan, 1982) or Head-Driven
Phrase Structure Grammar (HPSG) (Pollard and
Sag, 1994). All of these formalisms share a sim-
ilar basic syntactic structure with Penn Treebank
CFG. However, the target formalisms also encode
additional constraints and semantic features. For
instance, Penn Treebank annotations do not make
an explicit distinction between complement and
adjunct, while all the above grammars mark these
291
roles explicitly. Moreover, even the identical syn-
tactic information is encoded differently in these
formalisms. An example of this phenomenon is
the marking of subject. In LFG, this informa-
tion is captured in the mapping equation, namely
? SBJ =?, while Penn Treebank represents it as
a functional tag, such as NP-SBJ. Figure 1 shows
derivations in the three target formalisms we con-
sider, as well as a CFG derivation. We can see that
the derivations of these formalisms share the same
basic structure, while the formalism-specific infor-
mation is mainly encoded in the lexical entries and
node labels.
To enable effective transfer the model has to
identify shared structural components between
the formalisms despite the apparent differences.
Moreover, we do not assume parallel annotations.
To this end, our model jointly parses the two cor-
pora according to the corresponding annotations,
enabling transfer via parameter sharing. In partic-
ular, we augment each target tree node with hidden
variables that capture the connection to the coarse
annotations. Specifically, each node in the target
tree has two labels: an entry which is specific to
the target formalism, and a latent label containing
a value from the Penn Treebank tagset, such as NP
(see Figure 2). This design enables us to repre-
sent three types of features: the target formalism-
specific features, the coarse formalism features,
and features that connect the two. This model-
ing approach makes it possible to perform transfer
to a range of target formalisms, without manually
drafting formalism-specific rules.
We evaluate our approach on three
constituency-based grammars ? CCG, HPSG,
and LFG. As a source of coarse annotations,
we use the Penn Treebank.2 Our results clearly
demonstrate that for all three formalisms, pars-
ing accuracy can be improved by training with
additional coarse annotations. For instance, the
model trained on 500 HPSG sentences achieves
labeled dependency F-score of 72.3%. Adding
15,000 Penn Treebank sentences during training
leads to 78.5% labeled dependency F-score, an
absolute improvement of 6.2%. To achieve similar
performance in the absence of coarse annotations,
the parser has to be trained on about 1,500
sentences, namely three times what is needed
when using coarse annotations. Similar results are
2While the Penn Treebank-2 contains richer annotations,
we decided to use the Penn Treebank-1 to demonstrate the
feasibility of transfer from coarse annotations.
CFG CCG 
LFG 
I                eat               apples NP                VB                   NP 
VP 
S 
I                eat                apples NP        (S[dcl]\NP)/NP         NP 
S[dcl]\NP 
S[dcl] 
 I               eat                 apples [Pron.I]     [   SBJ,   OBJ]       [N.3pl] 
ROOT ?=?? ?=?SBJ!? =?OBJ!??=? HPSG   I              eat                 apples [N.no3sg]   [N<V.bse>N]        [N.3pl] head_comp subj_head 
Figure 1: Derivation trees for CFG as well as
CCG, HPSG and LFG formalisms.
also observed on CCG and LFG formalisms.
2 Related Work
Our work belongs to a broader class of research
on transfer learning in parsing. This area has gar-
nered significant attention due to the expense asso-
ciated with obtaining syntactic annotations. Trans-
fer learning in parsing has been applied in differ-
ent contexts, such as multilingual learning (Sny-
der et al, 2009; Hwa et al, 2005; McDonald et
al., 2006; McDonald et al, 2011; Jiang and Liu,
2009), domain adaptation (McClosky et al, 2010;
Dredze et al, 2007; Blitzer et al, 2006), and cross-
formalism transfer (Hockenmaier and Steedman,
2002; Miyao et al, 2005; Cahill et al, 2002; Rie-
zler et al, 2002; Chen and Shanker, 2005; Candito
et al, 2010).
There have been several attempts to map anno-
tations in coarse grammars like CFG to annota-
tions in richer grammar, like HPSG, LFG, or CCG.
Traditional approaches in this area typically rely
on manually specified rules that encode the rela-
tion between the two formalisms. For instance,
mappings may specify how to convert traces and
functional tags in Penn Treebank to the f-structure
in LFG (Cahill, 2004). These conversion rules
are typically utilized in two ways: (1) to create a
new treebank which is consequently used to train a
parser for the target formalism (Hockenmaier and
Steedman, 2002; Clark and Curran, 2003; Miyao
et al, 2005; Miyao and Tsujii, 2008), (2) to trans-
late the output of a CFG parser into the target for-
malism (Cahill et al, 2002).
The design of these rules is a major linguis-
tic and computational undertaking, which requires
multiple iterations over the data to increase cov-
erage (Miyao et al, 2005; Oepen et al, 2004).
By nature, the mapping rules are formalism spe-
292
cific and therefore not transferable. Moreover, fre-
quently designing such mappings involves modifi-
cation to the original annotations. For instance,
Hockenmaier and Steedman (2002) made thou-
sands of POS and constituent modifications to the
Penn Treebank to facilitate transfer to CCG. More
importantly, in some transfer scenarios, determin-
istic rules are not sufficient, due to the high am-
biguity inherent in the mapping. Therefore, our
work considers an alternative set-up for cross-
formalism transfer where a small amount of an-
notations in the target formalism is used as an al-
ternative to using deterministic rules.
The limitation of deterministic transfer rules
has been recognized in prior work (Riezler et al,
2002). Their method uses a hand-crafted LFG
parser to create a set of multiple parsing candi-
dates for a given sentence. Using the partial map-
ping from CFG to LFG as the guidance, the re-
sulting trees are ranked based on their consistency
with the labeled LFG bracketing imported from
CFG. In contrast to this method, we neither require
a parser for the target formalism, nor manual rules
for partial mapping. Consequently, our method
can be applied to many different target grammar
formalisms without significant engineering effort
for each one. The utility of coarse-grained tree-
banks is determined by the degree of structural
overlap with the target formalism.
3 The Learning Problem
Recall that our goal is to learn how to parse the tar-
get formalisms while using two annotated sources:
a small set of sentences annotated in the target for-
malism (e.g., CCG), and a large set of sentences
with coarse annotations. For the latter, we use the
CFG parses from the Penn Treebank. For sim-
plicity we focus on the CCG formalism in what
follows. We also generalize our model to other
formalisms, as explained in Section 5.4.
Our notations are as follows: an input sentence
is denoted by S. A CFG parse is denoted by yCFG
and a CCG parse is denoted by yCCG. Clearly the
set of possible values for yCFG and yCCG is deter-
mined by S and the grammar. The training set is a
set of N sentences S1, . . . , SN with CFG parses
y1CFG, . . . , yNCFG, and M sentences S?1, . . . , S?M
with CCG parses y1CCG, . . . , yMCCG. It is impor-
tant to note that we do not assume we have parallel
data for CCG and CFG.
Our goal is to use such a corpus for learning
eat apples 
coarse feature on yCFG VP VP,NP 
VP    (S[dcl]\NP)/NP 
VP    S[dcl]\NP 
NP    NP 
formalism feature on yCCG S[dcl]\NP (S[dcl]\NP)/NP,NP 
joint feature on yCFG, yCCG VP, S[dcl]\NP (VP, (S[dcl]\NP)/NP), (NP, NP) 
Figure 2: Illustration of the joint CCG-CFG representa-
tion. The shadowed labels correspond to the CFG deriva-
tion yCFG, whereas the other labels correspond to the CCG
derivation yCCG. Note that the two derivations share the
same (binarized) tree structure. Also shown are features that
are turned on for this joint derivation (see Section 6).
how to generate CCG parses to unseen sentences.
4 A Joint Model for Two Formalisms
The key idea behind our work is to learn a joint
distribution over CCG and CFG parses. Such a
distribution can be marginalized to obtain a distri-
bution over CCG or CFG and is thus appropriate
when the training data is not parallel, as it is in our
setting.
It is not immediately clear how to jointly model
the CCG and CFG parses, which are structurally
quite different. Furthermore, a joint distribution
over these will become difficult to handle com-
putationally if not constructed carefully. To ad-
dress this difficulty, we make several simplifying
assumptions. First, we assume that both parses are
given in normal form, i.e., they correspond to bi-
nary derivation trees. CCG parses are already pro-
vided in this form in CCGBank. CFG parses in the
Penn Treebank are not binary, and we therefore bi-
narize them, as explained in Section 5.3.
Second, we assume that any yCFG and yCCG
jointly generated must share the same derivation
tree structure. This makes sense. Since both for-
malisms are constituency-based, their trees are ex-
pected to describe the same constituents. We de-
note the set of valid CFG and CCG joint parses for
sentence S by Y(S).
The above two simplifying assumptions make
it easy to define joint features on the two parses,
as explained in Section 6. The representation and
features are illustrated in Figure 2.
We shall work within the discriminative frame-
work, where given a sentence we model a dis-
tribution over parses. As is standard in such
settings, the distribution will be log-linear in a
set of features of these parses. Denoting y =
(yCFG, yCCG), we seek to model the distribution
293
p(y|S) corresponding to the probability of gen-
erating a pair of parses (CFG and CCG) given a
sentence. The distribution thus has the following
form:
pjoint(y|S; ?) =
1
Z(S; ?)e
f(y,S)?? . (1)
where ? is a vector of parameters to be learned
from data, and f(y, S) is a feature vector. Z(S; ?)
is a normalization (partition) function normalized
over y ? Y(S) the set of valid joint parses.
The feature vector contains three types of fea-
tures: CFG specific, CCG specific and joint CFG-
CCG. We denote these by fCFG, fCCG, fjoint.
These depend on yCCG, yCFG and y respectively.
Accordingly, the parameter vector ? is a concate-
nation of ?CCG, ?CFG and ?joint.
As mentioned above, we can use Equation 1
to obtain distributions over yCCG and yCFG via
marginalization. For the distribution over yCCG
we do precisely this, namely use:
pCCG(yCCG|S; ?) =
?
yCFG
pjoint(y|S; ?) (2)
For the distribution over yCFG we could have
marginalized pjoint over yCCG. However, this
computation is costly for each sentence, and has
to be repeated for all the sentences. Instead, we
assume that the distribution over yCFG is a log-
linear model with parameters ?CFG (i.e., a sub-
vector of ?) , namely:
pCFG(yCFG|S; ?CFG) =
efCFG(yCFG,S)??CFG
Z(S; ?CFG)
.
(3)
Thus, we assume that both pjoint and pCFG have
the same dependence on the fCFG features.
The Likelihood Objective: Given the models
above, it is natural to use maximum likelihood to
find the optimal parameters. To do this, we define
the following regularized likelihood function:
L(?) =
N?
i=1
log
(
pCFG(yiCFG|Si, ?CFG)
)
+
M?
i=1
log
(
pCCG(yiCCG|S?i, ?)
)
? ?2 ???
2
2
where pCCG and pCFG are defined in Equations
2 and 3 respectively. The last term is the l2-norm
regularization. Our goal is then to find a ? that
maximizes L(?).
Training Algorithm: For maximizing L(?)
w.r.t. ? we use the limited-memory BFGS algo-
rithm (Nocedal and Wright, 1999). Calculating
the gradient of L(?) requires evaluating the ex-
pected values of f(y, S) and fCFG under the dis-
tributions pjoint and pCFG respectively. This can
be done via the inside-outside algorithm.3
Parsing Using the Model: To parse a sentence
S, we calculate the maximum probability assign-
ment for pjoint(y|S; ?).4 The result is both a CFG
and a CCG parse. Here we will mostly be inter-
ested in the CCG parse. The joint parse with max-
imum probability is found using a standard CYK
chart parsing algorithm. The chart construction
will be explained in Section 5.
5 Implementation
This section introduces important implementa-
tion details, including supertagging, feature for-
est pruning and binarization methods. Finally,
we explain how to generalize our model to other
constituency-based formalisms.
5.1 Supertagging
When parsing a target formalism tree, one needs
to associate each word with a lexical entry. How-
ever, since the number of candidates is typically
more than one thousand, the size of the chart ex-
plodes. One effective way of reducing the number
of candidates is via supertagging (Clark and Cur-
ran, 2007). A supertagger is used for selecting a
small set of lexical entry candidates for each word
in the sentence. We use the tagger in (Clark and
Curran, 2007) as a general suppertagger for all the
grammars considered. The only difference is that
we use different lexical entries in different gram-
mars.
5.2 Feature Forest Pruning
In the BFGS algorithm (see Section 4), feature ex-
pectation is computed using the inside-outside al-
gorithm. To perform this dynamic programming
efficiently, we first need to build the packed chart,
namely the feature forest (Miyao, 2006) to rep-
resent the exponential number of all possible tree
3To speed up the implementation, gradient computation
is parallelized, using the Message Passing Interface pack-
age (Gropp et al, 1999).
4An alternative approach would be to marginalize over
yCFG and maximize over yCCG. However, this is a harder
computational problem.
294
structures. However, a common problem for lex-
icalized grammars is that the forest size is too
large. In CFG, the forest is pruned according to
the inside probability of a simple generative PCFG
model and a prior (Collins, 2003). The basic idea
is to prune the trees with lower probability. For the
target formalism, a common practice is to prune
the forest using the supertagger (Clark and Cur-
ran, 2007; Miyao, 2006). In our implementation,
we applied all pruning techniques, because the for-
est is a combination of CFG and target grammar
formalisms (e.g., CCG or HPSG).
5.3 Binarization
We assume that the derivation tree in the target for-
malism is in a normal form, which is indeed the
case for the treebanks we consider. As mentioned
in Section 4, we would also like to work with bi-
narized CFG derivations, such that all trees are in
normal form and it is easy to construct features
that link the two (see Section 6).
Since Penn Treebank trees are not binarized, we
construct a simple procedure for binarizing them.
The procedure is based on the available target for-
malism parses in the training corpus, which are bi-
narized. We illustrate it with an example. In what
follows, we describe derivations using the POS of
the head words of the corresponding node in the
tree. This makes it possible to transfer binariza-
tion rules between formalisms.
Suppose we want to learn the binarization rule
of the following derivation in CFG:
NN? (DT JJ NN) (4)
We now look for binary derivations with these
POS in the target formalism corpus, and take the
most common binarization form. For example, we
may find that the most common binarization to bi-
narize the CFG derivation in Equation 4 is:
NN? (DT (JJ NN))
If no (DT JJ NN) structure is observed in the
CCG corpus, we first apply the binary branching
on the children to the left of the head, and then on
the children to the right of the head.
We also experiment with using fixed binariza-
tion rules such as left/right branching, instead of
learning them. This results in a drop on the depen-
dency F-score by about 5%.
5.4 Implementation in Other Formalisms
We introduce our model in the context of CCG,
but the model can easily be generalized to other
constituency-based grammars, such as HPSG and
LFG. In a derivation tree, the formalism-specific
information is mainly encoded in the lexical en-
tries and the applied grammar rules, rather than the
tree structures. Therefore we only need to change
the node labels and lexical entries to the language-
specific ones, while the framework of the model
remains the same.
6 Features
Feature functions in log-linear models are de-
signed to capture the characteristics of each
derivation in the tree. In our model, as mentioned
in Section 1, the features are also defined to en-
able information transfer between coarse and rich
formalisms. In this section, we first introduce how
different types of feature templates are designed,
and then show an example of how the features help
transfer the syntactic structure information. Note
that the same feature templates are used for all the
target grammar formalisms.
Recall that our y contains both the CFG and
CCG parses, and that these use the same derivation
tree structure. Each feature will consider either the
CFG derivation, the CCG derivation or these two
derivations jointly.
The feature construction is similar to construc-
tions used in previous work (Miyao, 2006). The
features are based on the atomic features listed in
Table 1. These will be used to construct f(y, S) as
explained next.
hl lexical entries/CCG categories of the head word
r grammar rules, i.e. HPSG schema, resulting CCG
categories, LFG mapping equations
sy CFG syntactic label of the node (e.g. NP, VP)
d distance between the head words of the children
c whether a comma exists between the head words
of the children
sp the span of the subtree rooted at the node
hw surface form of the head word of the node
hp part-of-speech of the head word
pi part-of-speech of the i-th word in the sentence
Table 1: Templates of atomic features.
We define the following feature templates:
fbinary for binary derivations, funary for unary
derivations, and froot for the root nodes. These
use the atomic features in Table 1, resulting in the
295
following templates:
fbinary =
? r, syp, d, c
syl, spl, hwl, hpl, hll,
syr, spr, hwr, hpr, hlr,
pst?1, pst?2, pen+1, pen+2
?
funary = ?r, syp, hw, hp, hl?
froot = ?sy, hw, hp, hl?
In the above we used the following notation: p, l, r
denote the parent node and left/right child node,
and st, en denote the starting and ending index of
the constituent.
We also consider templates with subsets of the
above features. The final list of binary feature tem-
plates is shown in Table 2. It can be seen that some
features depend only on the CFG derivations (i.e.,
those without r,hl), and are thus in fCFG(y, S).
Others depend only on CCG derivations (i.e.,
those without sy), and are in fCCG(y, S). The
rest depend on both CCG and CFG and are thus
in fjoint(y, S).
Note that after binarization, grandparent and
sibling information becomes very important in en-
coding the structure. However, we limit the fea-
tures to be designed locally in a derivation in order
to run inside-outside efficiently. Therefore we use
the preceding and succeeding POS tag information
to approximate the grandparent and sibling infor-
mation. Empirically, these features yield a signifi-
cant improvement on the constituent accuracy.
fCFG
?d,wl,r, hpl,r, syp,l,r?, ?d,wl,r, syp,l,r?,
?c, wl,r, hpl,r, syp,l,r?, ?c, wl,r, syp,l,r?,
?d, c, hpl,r, syp,l,r?, ?d, c, syp,l,r?,
?c, spl,r, hpl,r, syp,l,r?, ?c, spl,r, syp,l,r?,
?pst?1, syp,l,r?, ?pen+1, syp,l,r?,
?pst?1, pen+1, syp,l,r?,
?pst?1, pst?2, syp,l,r?, ?pen+1, pen+2, syp,l,r?,
?pst?1, pst?2, pen+1, pen+2, syp,l,r?,
fCCG
?r, d, c, hwl,r, hpl,r, hll,r?, ?r, d, c, hwl,r, hpl,r?
?r, d, c, hwl,r, hll,r?,
?r, c, spl,r, hwl,r, hpl,r, hll,r?
?r, c, spl,r, hwl,r, hpl,r, ?, ?r, c, spl,r, hwl,r, hll,r?
?r, d, c, hpl,r, hll,r?, ?r, d, c, hpl,r?, ?r, d, c, hll,r?
?r, c, hpl,r, hll,r?, ?r, c, hpl,r?, ?r, c, hll,r?
fjoint ?r, d, c, syl,r, hll,r?, ?r, d, c, syl,r??r, c, spl,r, syl,r, hll,r?, ?r, c, spl,r, syl,r?
Table 2: Binary feature templates used in f(y, S).
Unary and root features follow a similar pattern.
In order to apply the same feature templates to
other target formalisms, we only need to assign
the atomic features r and hl with the formalism-
specific values. We do not need extra engineering
work on redesigning the feature templates.
eat apples VP    (S[dcl]\NP)/NP 
VP    S[dcl]\NP 
NP    NP 
VP VP,NP 
S[dcl]\NP (S[dcl]\NP)/NP,NP 
VP, S[dcl]\NP (VP, (S[dcl]\NP)/NP), (NP, NP) 
CCGbank 
VP 
Penn Treebank 
VP NP write letters 
VP VP,NP fCFG (y,S) : fCFG (y,S) :fCCG (y,S) :f joint (y, S) :
Figure 3: Example of transfer between CFG and
CCG formalisms.
Figure 3 gives an example in CCG of how
features help transfer the syntactic information
from Penn Treebank and learn the correspondence
to the formalism-specific information. From the
Penn Treebank CFG annotations, we can learn
that the derivation VP?(VP, NP) is common, as
shown on the left of Figure 3. In a CCG tree, this
tendency will encourage the yCFG (latent) vari-
ables to take this CFG parse. Then weights on the
fjoint features will be learned to model the con-
nection between the CFG and CCG labels. More-
over, the formalism-specific features fCCG can
also encode the formalism-specific syntactic and
semantic information. These three types of fea-
tures work together to generate a tree skeleton and
fill in the CFG and CCG labels.
7 Evaluation Setup
Grammar Train Dev. Test
CCG
Sec. 02-21
Sec. 00 Sec. 23HPSG
LFG 140 sents. in 560 sents. inPARC700 PARC700
Table 3: Training/Dev./Test split on WSJ sections
and PARC700 for different grammar formalisms.
Datasets: As a source of coarse annotations, we
use the Penn Treebank-1 (Marcus et al, 1993). In
addition, for CCG, HPSG and LFG, we rely on
formalism-specific corpora developed in prior re-
search (Hockenmaier and Steedman, 2002; Miyao
et al, 2005; Cahill et al, 2002; King et al, 2003).
All of these corpora were derived via conversion
of Penn Treebank to the target formalisms. In par-
ticular, our CCG and HPSG datasets were con-
verted from the Penn Treebank based on hand-
296
0 1000 3000 7000 11000 1500074
7678
8082
8486
88
 
 
Labeled DepUnlabeled DepUnlabeled Parseval
(a) CCG
0 1000 3000 7000 11000 1500072
7476
7880
8284
86
 
 
Labeled DepUnlabeled DepUnlabeled Parseval
(b) HPSG
0 1000 3000 7000 11000 1500065
70
75
80
 
 
Labeled DepUnlabeled DepUnlabeled Parseval
(c) LFG
Figure 4: Model performance with 500 target formalism trees and different numbers of CFG trees,
evaluated using labeled/unlabeled dependency F-score and unlabeled PARSEVAL.
crafted rules (Hockenmaier and Steedman, 2002;
Miyao et al, 2005). Table 3 shows which sec-
tions of the treebanks were used in training, test-
ing and development for both formalisms. Our
LFG training dataset was constructed in a sim-
ilar fashion (Cahill et al, 2002). However, we
choose to use PARC700 as our LFG tesing and de-
velopment datasets, following the previous work
by (Kaplan et al, 2004). It contains 700 man-
ually annotated sentences that are randomly se-
lected from Penn Treebank Section 23. The split
of PARC700 follows the setting in (Kaplan et al,
2004). Since our model does not assume parallel
data, we use distinct sentences in the source and
target treebanks. Following previous work (Hock-
enmaier, 2003; Miyao and Tsujii, 2008), we only
consider sentences not exceeding 40 words, except
on PARC700 where all sentences are used.
Evaluation Metrics: We use two evaluation
metrics. First, following previous work, we eval-
uate our method using the labeled and unlabeled
predicate-argument dependency F-score. This
metric is commonly used to measure parsing qual-
ity for the formalisms considered in this paper.
The detailed definition of this measure as applied
for each formalism is provided in (Clark and Cur-
ran, 2003; Miyao and Tsujii, 2008; Cahill et al,
2004). For CCG, we use the evaluation script
from the C&C tools.5 For HPSG, we evaluate
all types of dependencies, including punctuations.
For LFG, we consider the preds-only dependen-
cies, which are the dependencies between pairs
of words. Secondly, we also evaluate using unla-
beled PARSEVAL, a standard measure for PCFG
parsing (Petrov and Klein, 2007; Charniak and
Johnson, 2005; Charniak, 2000; Collins, 1997).
The dependency F-score captures both the target-
5Available at http://svn.ask.it.usyd.edu.au/trac/candc/wiki
grammar labels and tree-structural relations. The
unlabeled PARSEVAL is used as an auxiliary mea-
sure that enables us to separate these two aspects
by focusing on the structural relations exclusively.
Training without CFG Data: To assess the
impact of coarse data in the experiments be-
low, we also consider the model trained only on
formalism-specific annotations. When no CFG
sentences are available, we assign all the CFG la-
bels to a special value shared by all the nodes. In
this set-up, the model reduces to a normal log-
linear model for the target formalism.
Parameter Settings: During training, all the
feature parameters ? are initialized to zero. The
hyperparameters used in the model are tuned on
the development sets. We noticed, however, that
the resulting values are consistent across differ-
ent formalisms. In particular, we set the l2-norm
weight to ? = 1.0, the supertagger threshold to
? = 0.01, and the PCFG pruning threshold to
? = 0.002.
8 Experiment and Analysis
Impact of Coarse Annotations on Target For-
malism: To analyze the effectiveness of annota-
tion transfer, we fix the number of annotated trees
in the target formalism and vary the amount of
coarse annotations available to the algorithm dur-
ing training. In particular, we use 500 sentences
with formalism-specific annotations, and vary the
number of CFG trees from zero to 15,000.
As Figure 4 shows, CFG data boosts parsing ac-
curacy for all the target formalisms. For instance,
there is a gain of 6.2% in labeled dependency
F-score for HPSG formalism when 15,000 CFG
trees are used. Moreover, increasing the number
of coarse annotations used in training leads to fur-
ther improvement on different evaluation metrics.
297
0 1000 2000 3000 4000 5000 60007475
7677
7879
8081
8283
84
 
 
w/o CFG15000 CFG
(a) CCG
0 1000 2000 3000 4000 5000 600072
7476
7880
8284
 
 
w/o CFG15000 CFG
(b) HPSG
0 1000 2000 3000 4000 5000 600066
6870
7274
7678
 
 
w/o CFG15000 CFG
(c) LFG
0 1000 2000 3000 4000 5000 60007778
7980
8182
8384
8586
8788
 
 
w/o CFG15000 CFG
(d) CCG
0 1000 2000 3000 4000 5000 60007072
7476
7880
8284
86
 
 
w/o CFG15000 CFG
(e) HPSG
0 1000 2000 3000 4000 5000 600068
7072
7476
7880
8284
 
 
w/o CFG15000 CFG
(f) LFG
Figure 5: Model performance with different target formalism trees and zero or 15,000 CFG trees. The
first row shows the results of labeled dependency F-score and the second row shows the results of unla-
beled PARSEVAL.
Tradeoff between Target and Coarse Annota-
tions: We also assess the relative contribution
of coarse annotations when the size of annotated
training corpus in the target formalism varies. In
this set of experiments, we fix the number of CFG
trees to 15,000 and vary the number of target an-
notations from 500 to 4,000. Figure 5 shows
the relative contribution of formalism-specific an-
notations compared to that of the coarse annota-
tions. For instance, Figure 5a shows that the pars-
ing performance achieved using 2,000 CCG sen-
tences can be achieved using approximately 500
CCG sentences when coarse annotations are avail-
able for training. More generally, the result con-
vincingly demonstrates that coarse annotations are
helpful for all the sizes of formalism-specific train-
ing data. As expected, the improvement margin
decreases when more formalism-specific data is
used.
Figure 5 also illustrates a slightly different char-
acteristics of transfer performance between two
evaluation metrics. Across all three grammars,
we can observe that adding CFG data has a
more pronounced effect on the PARSEVAL mea-
sure than the dependency F-score. This phe-
nomenon can be explained as follows. The un-
labeled PARSEVAL score (Figure 5d-f) mainly re-
lies on the coarse structural information. On
the other hand, predicate-argument dependency F-
score (Figure 5a-c) also relies on the target gram-
mar information. Because that our model only
transfers structural information from the source
treebank, the gains of PARSEVAL are expected to
be larger than that of dependency F-score.
Grammar Parser # Grammar trees1,000 15,000
CCG C&C 74.1 / 83.4 82.6 / 90.1Model 76.8 / 85.5 84.7 / 90.9
HPSG Enju 75.8 / 80.6 84.2 / 87.3Model 76.9 / 82.0 84.9 / 88.3
LFG
Pipeline
Annotator 68.5 / 74.0 82.6 / 85.9
Model 69.8 / 76.6 81.1 / 84.7
Table 4: The labeled/unlabeled dependency F-
score comparisons between our model and state-
of-the-art parsers.
Comparison to State-of-the-art Parsers: We
would also like to demonstrate that the above
gains of our transfer model are achieved using
an adequate formalism-specific parser. Since our
model can be trained exclusively on formalism-
specific data, we can compare it to state-of-the-
art formalism-specific parsers. For this experi-
ment, we choose the C&C parser (Clark and Cur-
ran, 2003) for CCG, Enju parser (Miyao and Tsu-
jii, 2008) for HPSG and pipeline automatic an-
notator (Cahill et al, 2004) with Charniak parser
for LFG. For all three parsers, we use the imple-
mentation provided by the authors with the default
parameter values. All the models are trained on
either 1,000 or 15,000 sentences annotated with
formalism-specific trees, thus evaluating their per-
formances on small scale or large scale of data.
As Table 4 shows, our model is competitive with
298
all the baselines described above. It?s not sur-
prising that Cahill?s model outperforms our log-
linear model because it relies heavily on hand-
crafted rules optimized for the dataset.
Correspondence between CFG and Target For-
malisms: Finally, we analyze highly weighted
features. Table 5 shows such features for HPSG;
similar patterns are also found for the other
grammar formalisms. The first two features are
formalism-specific ones, the first for HPSG and
the second for CFG. They show that we correctly
learn a frequent derivation in the target formalism
and CFG. The third one shows an example of a
connection between CFG and the target formal-
ism. Our model correctly learns that a syntactic
derivation with children VP and NP is very likely
to be mapped to the derivation (head comp)?
([N?V?N],[N.3sg]) in HPSG.
Feature type Features with high weight
Target
formalism
Template
(r) ? (hll, hpl)(hlr, pr)
Examples
(head comp)?
([N?V?N],VB)([N.3sg],NN)
Coarse
formalism
Template
(syp) ? (syl, hpl)(syr, hpr)
Examples
(VP)?(VP,VB)(NP,NN)
Joint
features
Template
(r) ? (hll, syl)(ler, syr)
Examples
(head comp)?
([N?V?N],VP)([N.3sg],NP)
Table 5: Example features with high weight.
9 Conclusions
We present a method for cross-formalism trans-
fer in parsing. Our model utilizes coarse syn-
tactic annotations to supplement a small num-
ber of formalism-specific trees for training on
constituency-based grammars. Our experimen-
tal results show that across a range of such for-
malisms, the model significantly benefits from the
coarse annotations.
Acknowledgments
The authors acknowledge the support of the Army
Research Office (grant 1130128-258552). We
thank Yusuke Miyao, Ozlem Cetinoglu, Stephen
Clark, Michael Auli and Yue Zhang for answering
questions and sharing the codes of their work. We
also thank the members of the MIT NLP group
and the ACL reviewers for their suggestions and
comments. Any opinions, findings, conclusions,
or recommendations expressed in this paper are
those of the authors, and do not necessarily reflect
the views of the funding organizations.
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120?128. Association for Com-
putational Linguistics.
Joan Bresnan. 1982. The mental representation of
grammatical relations, volume 1. The MIT Press.
Aoife Cahill, Mairad McCarthy, Josef van Genabith,
and Andy Way. 2002. Parsing with pcfgs and au-
tomatic f-structure annotation. In Proceedings of
the Seventh International Conference on LFG, pages
76?95. CSLI Publications.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
Van Genabith, and Andy Way. 2004. Long-distance
dependency resolution in automatically acquired
wide-coverage pcfg-based lfg approximations. In
Proceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics, page 319. As-
sociation for Computational Linguistics.
Aoife Cahill. 2004. Parsing with Automatically Ac-
quired, Wide-Coverage, Robust, Probabilistic LFG
Approximation. Ph.D. thesis.
Marie Candito, Beno??t Crabbe?, Pascal Denis, et al
2010. Statistical french dependency parsing: tree-
bank conversion and first results. In Proceed-
ings of the Seventh International Conference on
Language Resources and Evaluation (LREC 2010),
pages 1840?1847.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132?139.
John Chen and Vijay K Shanker. 2005. Automated
extraction of tags from the penn treebank. New de-
velopments in parsing technology, pages 73?89.
Stephen Clark and James R Curran. 2003. Log-linear
models for wide-coverage ccg parsing. In Proceed-
ings of the 2003 conference on Empirical methods
in natural language processing, pages 97?104. As-
sociation for Computational Linguistics.
299
Stephen Clark and James R Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and
log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins. 1997. Three generative, lexicalised
models for statistical pprsing. In Proceedings of the
eighth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 16?23.
Association for Computational Linguistics.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Mark Dredze, John Blitzer, Partha Pratim Talukdar,
Kuzman Ganchev, Joao V Grac?a, and Fernando
Pereira. 2007. Frustratingly hard domain adap-
tation for dependency parsing. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL,
volume 2007.
William Gropp, Ewing Lusk, and Anthony Skjellum.
1999. Using MPI: portable parallel programming
with the message passing interface, volume 1. MIT
press.
Julia Hockenmaier and Mark Steedman. 2002.
Acquiring compact lexicalized grammars from a
cleaner treebank. In Proceedings of the Third LREC
Conference, pages 1974?1981.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with combinatory categorial grammar.
Rebecca Hwa, Philip Resnik, and Amy Weinberg.
2005. Breaking the resource bottleneck for multi-
lingual parsing. Technical report, DTIC Document.
Wenbin Jiang and Qun Liu. 2009. Automatic adap-
tation of annotation standards for dependency pars-
ing: using projected treebank as source corpus. In
Proceedings of the 11th International Conference on
Parsing Technologies, pages 25?28. Association for
Computational Linguistics.
Ronald M. Kaplan, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alexander Vasserman, and Richard
Crouch. 2004. Speed and accuracy in shallow and
deep stochastic parsing. In Proceedings of NAACL.
Tracy Holloway King, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ronald M Kaplan. 2003. The
parc 700 dependency bank. In Proceedings of the
EACL03: 4th International Workshop on Linguisti-
cally Interpreted Corpora (LINC-03), pages 1?8.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 28?36. Association for Computational Lin-
guistics.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, pages 216?220. Association for
Computational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 62?72. Association for Computational Lin-
guistics.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic hpsg parsing. Computa-
tional Linguistics, 34(1):35?80.
Yusuke Miyao, Takashi Ninomiya, and Junichi Tsu-
jii. 2005. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. Natural Language
Processing?IJCNLP 2004, pages 684?693.
Yusuke Miyao. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. thesis.
Jorge Nocedal and Stephen J Wright. 1999. Numerical
optimization. Springer verlag.
Stephan Oepen, Dan Flickinger, and Francis Bond.
2004. Towards holistic grammar engineering
and testing?grafting treebank maintenance into the
grammar revision cycle. In Proceedings of the IJC-
NLP workshop beyond shallow analysis. Citeseer.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 404?411.
Carl Pollard and Ivan A Sag. 1994. Head-driven
phrase structure grammar. University of Chicago
Press.
Stefan Riezler, Tracy H King, Ronald M Kaplan,
Richard Crouch, John T Maxwell III, and Mark
Johnson. 2002. Parsing the wall street journal us-
ing a lexical-functional grammar and discriminative
estimation techniques. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 271?278. Association for Com-
putational Linguistics.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the
300
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume
1, pages 73?81. Association for Computational Lin-
guistics.
Mark Steedman. 2001. The syntactic process. MIT
press.
Yue Zhang, Stephen Clark, et al 2011. Shift-reduce
ccg parsing. In Proceedings of the 49th Meeting
of the Association for Computational Linguistics,
pages 683?692.
301
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 197?207,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Steps to Excellence: Simple Inference with Refined Scoring of
Dependency Trees
Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola
Massachusetts Institute of Technology
{yuanzh, taolei, regina, tommi}@csail.mit.edu
Amir Globerson
The Hebrew University
gamir@cs.huji.ac.il
Abstract
Much of the recent work on depen-
dency parsing has been focused on solv-
ing inherent combinatorial problems as-
sociated with rich scoring functions. In
contrast, we demonstrate that highly ex-
pressive scoring functions can be used
with substantially simpler inference pro-
cedures. Specifically, we introduce a
sampling-based parser that can easily han-
dle arbitrary global features. Inspired
by SampleRank, we learn to take guided
stochastic steps towards a high scoring
parse. We introduce two samplers for
traversing the space of trees, Gibbs and
Metropolis-Hastings with Random Walk.
The model outperforms state-of-the-art re-
sults when evaluated on 14 languages
of non-projective CoNLL datasets. Our
sampling-based approach naturally ex-
tends to joint prediction scenarios, such
as joint parsing and POS correction. The
resulting method outperforms the best re-
ported results on the CATiB dataset, ap-
proaching performance of parsing with
gold tags.
1
1 Introduction
Dependency parsing is commonly cast as a max-
imization problem over a parameterized scoring
function. In this view, the use of more expres-
sive scoring functions leads to more challenging
combinatorial problems of finding the maximiz-
ing parse. Much of the recent work on parsing has
been focused on improving methods for solving
the combinatorial maximization inference prob-
lems. Indeed, state-of-the-art results have been ob-
1
The source code for the work is available at
http://groups.csail.mit.edu/rbg/code/
global/acl2014.
tained by adapting powerful tools from optimiza-
tion (Martins et al, 2013; Martins et al, 2011;
Rush and Petrov, 2012). We depart from this view
and instead focus on using highly expressive scor-
ing functions with substantially simpler inference
procedures. The key ingredient in our approach is
how learning is coupled with inference. Our com-
bination outperforms the state-of-the-art parsers
and remains comparable even if we adopt their
scoring functions.
Rich scoring functions have been used for some
time. They first appeared in the context of rerank-
ing (Collins, 2000), where a simple parser is used
to generate a candidate list which is then reranked
according to the scoring function. Because the
number of alternatives is small, the scoring func-
tion could in principle involve arbitrary (global)
features of parse trees. The power of this method-
ology is nevertheless limited by the initial set of
alternatives from the simpler parser. Indeed, the
set may already omit the gold parse. We dispense
with the notion of a candidate set and seek to ex-
ploit the scoring function more directly.
In this paper, we introduce a sampling-based
parser that places few or no constraints on the
scoring function. Starting with an initial candi-
date tree, our inference procedure climbs the scor-
ing function in small (cheap) stochastic steps to-
wards a high scoring parse. The proposal distri-
bution over the moves is derived from the scoring
function itself. Because the steps are small, the
complexity of the scoring function has limited im-
pact on the computational cost of the procedure.
We explore two alternative proposal distributions.
Our first strategy is akin to Gibbs sampling and
samples a new head for each word in the sentence,
modifying one arc at a time. The second strat-
egy relies on a provably correct sampler for first-
order scores (Wilson, 1996), and uses it within a
Metropolis-Hastings algorithm for general scoring
functions. It turns out that the latter optimizes the
197
score more efficiently than the former.
Because the inference procedure is so simple,
it is important that the parameters of the scoring
function are chosen in a manner that facilitates
how we climb the scoring function in small steps.
One way to achieve this is to make sure that im-
provements in the scoring functions are correlated
with improvements in the quality of the parse.
This approach was suggested in the SampleRank
framework (Wick et al, 2011) for training struc-
tured prediction models. This method was origi-
nally developed for a sequence labeling task with
local features, and was shown to be more effec-
tive than state-of-the-art alternatives. Here we ap-
ply SampleRank to parsing, applying several mod-
ifications such as the proposal distributions men-
tioned earlier.
The benefits of sampling-based learning go be-
yond stand-alone parsing. For instance, we can
use the framework to correct preprocessing mis-
takes in features such as part-of-speech (POS)
tags. In this case, we combine the scoring func-
tion for trees with a stand-alone tagging model.
When proposing a small move, i.e., sampling a
head of the word, we can also jointly sample its
POS tag from a set of alternatives provided by
the tagger. As a result, the selected tag is influ-
enced by a broad syntactic context above and be-
yond the initial tagging model and is directly opti-
mized to improve parsing performance. Our joint
parsing-tagging model provides an alternative to
the widely-adopted pipeline setup.
We evaluate our method on benchmark multi-
lingual dependency corpora. Our method outper-
forms the Turbo parser across 14 languages on av-
erage by 0.5%. On four languages, we top the best
published results. Our method provides a more
effective mechanism for handling global features
than reranking, outperforming it by 1.3%. In terms
of joint parsing and tagging on the CATiB dataset,
we nearly bridge (88.38%) the gap between in-
dependently predicted (86.95%) and gold tags
(88.45%). This is better than the best published
results in the 2013 SPMRL shared task (Seddah et
al., 2013), including parser ensembles.
2 Related Work
Earlier works on dependency parsing focused on
inference with tractable scoring functions. For in-
stance, a scoring function that operates over each
single dependency can be optimized using the
maximum spanning tree algorithm (McDonald et
al., 2005). It was soon realized that using higher
order features could be beneficial, even at the cost
of using approximate inference and sacrificing op-
timality. The first successful approach in this arena
was reranking (Collins, 2000; Charniak and John-
son, 2005) on constituency parsing. Reranking
can be combined with an arbitrary scoring func-
tion, and thus can easily incorporate global fea-
tures over the entire parse tree. Its main disadvan-
tage is that the output parse can only be one of the
few parses passed to the reranker.
Recent work has focused on more powerful in-
ference mechanisms that consider the full search
space (Zhang and McDonald, 2012; Rush and
Petrov, 2012; Koo et al, 2010; Huang, 2008). For
instance, Nakagawa (2007) deals with tractabil-
ity issues by using sampling to approximate
marginals. Another example is the dual decompo-
sition (DD) framework (Koo et al, 2010; Martins
et al, 2011). The idea in DD is to decompose the
hard maximization problem into smaller parts that
can be efficiently maximized and enforce agree-
ment among these via Lagrange multipliers. The
method is essentially equivalent to linear program-
ming relaxation approaches (Martins et al, 2009;
Sontag et al, 2011), and also similar in spirit to
ILP approaches (Punyakanok et al, 2004).
A natural approach to approximate global in-
ference is via search. For instance, a transition-
based parsing system (Zhang and Nivre, 2011)
incrementally constructs a parsing structure us-
ing greedy beam-search. Other approaches op-
erate over full trees and generate a sequence
of candidates that successively increase the
score (Daum?e III et al, 2009; Li et al, 2013;
Wick et al, 2011). Our work builds on one such
approach ? SampleRank (Wick et al, 2011), a
sampling-based learning algorithm. In SampleR-
ank, the parameters are adjusted so as to guide the
sequence of candidates closer to the target struc-
ture along the search path. The method has been
successfully used in sequence labeling and ma-
chine translation (Haddow et al, 2011). In this
paper, we demonstrate how to adapt the method
for parsing with rich scoring functions.
3 Sampling-Based Dependency Parsing
with Global Features
In this section, we introduce our novel sampling-
based dependency parser which can incorporate
198
arbitrary global features. We begin with the no-
tation before addressing the decoding and learning
algorithms. Finally, we extend our model to a joint
parsing and POS correction task.
3.1 Notations
We denote sentences by x and the corresponding
dependency trees by y ? Y(x). Here Y(x) is the
set of valid (projective or non-projective) depen-
dency trees for sentence x. We use x
j
to refer
to the jth word of sentence x, and h
j
to the head
word of x
j
. A training set of size N is given as a
set of pairs D = {(x
(i)
, y
(i)
)}
N
i=1
where y
(i)
is the
ground truth parse for sentence x
(i)
.
We parameterize the scoring function s(x, y) as
s(x, y) = ? ? f(x, y) (1)
where f(x, y) is the feature vector associated with
tree y for sentence x. We do not make any assump-
tions about how the feature function decomposes.
In contrast, most state-of-the-art parsers operate
under the assumption that the feature function de-
composes into a sum of simpler terms. For exam-
ple, in the second-order MST parser (McDonald
and Pereira, 2006), all the feature terms involve
arcs or consecutive siblings. Similarly, parsers
based on dual decomposition (Martins et al, 2011;
Koo et al, 2010) assume that s(x, y) decomposes
into a sum of terms where each term can be maxi-
mized over y efficiently.
3.2 Decoding
The decoding problem consists of finding a valid
dependency tree y ? Y(x) that maximizes the
score s(x, y) = ? ? f(x, y) with parameters ?.
For scoring functions that extend beyond first-
order arc preferences, finding the maximizing non-
projective tree is known to be NP-hard (McDonald
and Pereira, 2006). We find a high scoring tree
through sampling, and (later) learn the parameters
? so as to further guide this process.
Our sampler generates a sequence of depen-
dency structures so as to approximate independent
samples from
p(y|x, T, ?) ? exp (s(x, y)/T ) (2)
The temperature parameter T controls how con-
centrated the samples are around the maximum
of s(x, y) (e.g., see Geman and Geman (1984)).
Sampling from target distribution p is typically as
hard as (or harder than) that maximizing s(x, y).
Inputs: ?, x, T
0
(initial temperature), c (temperature
update rate), proposal distribution q.
Outputs: y
?
T ? T
0
Set y
0
to some random tree
y
?
? y
0
repeat
y
?
? q(?|x, y
t
, T, ?)
if s(x, y
?
) > s(x, y
?
) then
y
?
? y
?
? = min
[
1,
p(y
?
)q(y
t
|y
?
)
p(y
t
)q(y
?
|y
t
)
]
Sample Bernouli variable Z with P [Z = 1] = ?.
if Z = 0 then
y
t+1
? y
t
else
y
t+1
? y
?
t? t+ 1
T ? c ? T
until convergence
return y
?
Figure 1: Sampling-based algorithm for decoding
(i.e., approximately maximizing s(x, y)).
We follow here a Metropolis-Hastings sampling
algorithm (e.g., see Andrieu et al (2003)) and
explore different alternative proposal distributions
q(y
?
|x, y, ?, T ). The distribution q governs the
small steps that are taken in generating a sequence
of structures. The target distribution p folds into
the procedure by defining the probability that we
will accept the proposed move. The general struc-
ture of our sampling algorithm is given in Figure 1.
3.2.1 Gibbs Sampling
Perhaps the most natural choice of the proposal
distribution q is a conditional distribution from p.
This is feasible if we restrict the proposed moves
to only small changes in the current tree. In our
case, we choose a word j randomly, and then sam-
ple its head h
j
according to p with the constraint
that we obtain a valid tree (when projective trees
are sought, this constraint is also incorporated).
For this choice of q, the probability of accepting
the new tree (? in Figure 1) is identically one.
Thus new moves are always accepted.
3.2.2 Exact First-Order Sampling
One shortcoming of the Gibbs sampler is that it
only changes one variable (arc) at a time. This
usually leads to slow mixing, requiring more sam-
ples to get close to the parse with maximum
score. Ideally, we would change multiple heads
in the parse tree simultaneously, and sample those
choices from the corresponding conditional distri-
bution of p. While in general this is increasingly
difficult with more heads, it is indeed tractable if
199
Inputs: x, y
t
, ?, K (number of heads to change).
Outputs: y
?
for i = 1 to |x| do
inTree[i]? false
ChangeNode[i]? false
Set ChangeNode to true for K random nodes.
head[0]? ?1
for i = 1 to |x| do
u? i
while not inTree[u] do
if ChangeNode[u] then
head[u]? randomHead(u, ?)
else
head[u]? y
t
(u)
u? head[u]
if LoopExist(head) then
EraseLoop(head)
u? i
while not inTree[u] do
inTree[u]? true
u? head[u]
return Construct tree y
?
from the head array.
Figure 2: A proposal distribution q(y
?
|y
t
) based
on the random walk sampler of Wilson (1996).
The function randomHead samples a new head for
node u according to the first-order weights given
by ?.
the model corresponds to a first-order parser. One
such sampling algorithm is the random walk sam-
pler of Wilson (1996). It can be used to obtain
i.i.d. samples from distributions of the form:
p(y) ?
?
i?j?y
w
ij
, (3)
where y corresponds to a tree with a spcified root
and w
ij
is the exponential of the first-order score.
y is always a valid parse tree if we allow multiple
children of the root and do not impose projective
constraint. The algorithm in Wilson (1996) iter-
ates over all the nodes, and for each node performs
a random walk according to the weights w
ij
until
the walk creates a loop or hits a tree. In the first
case the algorithm erases the loop and continues
the walk. If the walk hits the current tree, the walk
path is added to form a new tree with more nodes.
This is repeated until all the nodes are included in
the tree. It can be shown that this procedure gen-
erates i.i.d. trees from p(y).
Since our features do not by design correspond
to a first-order parser, we cannot use the Wilson
algorithm as it is. Instead we use it as the proposal
function and sample a subset of the dependen-
cies from the first-order distribution of our model,
while fixing the others. In each step we uniformly
sample K nodes to update and sample their new
1!
2!
not?Monday? not ssssssssssss" ?""" wasloop erased!Black?Monday?was
ROOT! It! was! not! Black! Monday!
2!
1!
3!
ROOT! It! was! not! Black! Monday!
(b) walk path:!
(c) walk path:!
(a) original tree!
ROOT! It! was! not! Black! Monday!
Figure 3: An illustration of random walk sam-
pler. The index on each edge indicates its order on
each walk path. The heads of the red words are
sampled while others are fixed. The blue edges
represent the current walk path and the black ones
are already in the tree. Note that the walk direc-
tion is opposite to the dependency direction. (a)
shows the original tree before sampling; (b) and
(c) show the walk path and how the tree is gener-
ated in two steps. The loop not? Monday? not
in (b) is erased.
heads using the Wilson algorithm (in the experi-
ments we use K = 4). Note that blocked Gibbs
sampling would be exponential in K, and is thus
very slow already at K = 4. The procedure is de-
scribed in Figure 2 with a graphic illustration in
Figure 3.
3.3 Training
In this section, we describe how to learn the
adjustable parameters ? in the scoring function.
The parameters are learned in an on-line fash-
ion by successively imposing soft constraints be-
tween pairs of dependency structures. We intro-
duce both margin constraints and constraints per-
taining to successive samples generated along the
search path. We demonstrate later that both types
of constraints are essential.
We begin with the standard margin constraints.
An ideal scoring function would always rank the
gold parse higher than any alternative. Moreover,
alternatives that are far from the gold parse should
score even lower. As a result, we require that
s(x
(i)
, y
(i)
)? s(x
(i)
, y) ? ?(y
(i)
, y) ?y (4)
where ?(y
(i)
, y) is the number of head mistakes
in y relative to the gold parse y
(i)
. We adopt here
a shorthand Err(y) = ?(y
(i)
, y), where the de-
200
pendence on y
(i)
is implied from context. Note
that Equation 4 contains exponentially many con-
straints and cannot be enforced jointly for general
scoring functions. However, our sampling proce-
dure generates a small number of structures along
the search path. We enforce only constraints cor-
responding to those samples.
The second type of constraints are enforced be-
tween successive samples along the search path.
To illustrate the idea, consider a parse y that dif-
fers from y
(i)
in only one arc, and a parse y
?
that
differs from y
(i)
in ten arcs. We cannot necessarily
assume that s(x, y) is greater than s(x, y
?
) without
additional encouragement. Thus, we can comple-
ment the constraints in Equation 4 with additional
pairwise constraints (Wick et al, 2011):
s(x
(i)
, y)? s(x
(i)
, y
?
) ? Err(y
?
)? Err(y) (5)
where similarly to Equation 4, the difference in
scores scales with the differences in errors with re-
spect to the target y
(i)
. We only enforce the above
constraints for y, y
?
that are consecutive samples
in the course of the sampling process. These con-
straints serve to guide the sampling process de-
rived from the scoring function towards the gold
parse.
We learn the parameters ? in an on-line fashion
to satisfy the above constraints. This is done via
the MIRA algorithm (Crammer and Singer, 2003).
Specifically, if the current parameters are ?
t
, and
we enforce constraint Equation 5 for a particular
pair y, y
?
, then we will find ?
t+1
that minimizes
min ||? ? ?
t
||
2
+ C?
s.t. ? ? (f(x, y)? f(x, y
?
)) ? Err(y
?
)? Err(y)? ?
(6)
The updates can be calculated in closed form. Fig-
ure 4 summarizes the learning algorithm. We re-
peatedly generate parses based on the current pa-
rameters ?
t
for each sentence x
(i)
, and use succes-
sive samples to enforce constraints in Equation 4
and Equation 5 one at a time.
3.4 Joint Parsing and POS Correction
It is easy to extend our sampling-based parsing
framework to joint prediction of parsing and other
labels. Specifically, when sampling the new heads,
we can also sample the values of other variables at
the same time. For instance, we can sample the
POS tag, the dependency relation or morphology
information. In this work, we investigate a joint
Inputs: D = {(x
(i)
, y
(i)
)}
N
i=1
.
Outputs: Learned parameters ?.
?
0
? 0
for e = 1 to #epochs do
for i = 1 toN do
y
?
? q(?|x
(i)
, y
t
i
i
, ?
t
)
y
+
= arg min
y?
{
y
t
i
i
,y
?
}
Err(y)
y
?
= arg max
y?
{
y
t
i
i
,y
?
}
Err(y)
y
t
i
+1
i
? acceptOrReject(y
?
, y
t
i
i
, ?
t
)
t
i
? t
i
+ 1
?f = f(x
(i)
, y
+
)? f(x
(i)
, y
?
)
?Err = Err(y
+
)? Err(y
?
)
if ?Err 6= 0 and ?
t
? ?f < ?Err then
?
t+1
? updateMIRA(?f,?Err, ?
t
)
t? t+ 1
?f
g
= f(x
(i)
, y
(i)
)? f(x
(i)
, y
t
i
i
)
if ?
t
? ?f
g
< Err(y
t
i
i
) then
?
t+1
? updateMIRA(?f
g
, Err(y
t
i
i
), ?
t
)
t? t+ 1
return Average of ?
0
, . . . , ?
t
parameters.
Figure 4: SampleRank algorithm for learning. The
rejection strategy is as in Figure 1. y
t
i
i
is the t
i
th
tree sample of x
(i)
. The first MIRA update (see
Equation 6) enforces a ranking constraint between
two sampled parses. The second MIRA update en-
forces constraints between a sampled parse and the
gold parse. In practice several samples are drawn
for each sentence in each epoch.
POS correction scenario in which only the pre-
dicted POS tags are provided in the testing phase,
while both gold and predicted tags are available
for the training set.
We extend our model such that it jointly learns
how to predict a parse tree and also correct the pre-
dicted POS tags for a better parsing performance.
We generate the POS candidate list for each word
based on the confusion matrix on the training set.
Let c(t
g
, t
p
) be the count when the gold tag is t
g
and the predicted one is t
p
. For each word w, we
first prune out its POS candidates by using the vo-
cabulary from the training set. We don?t prune
anything if w is unseen. Assuming that the pre-
dicted tag forw is t
p
, we further remove those tags
t if their counts are smaller than some threshold
c(t, t
p
) < ? ? c(t
p
, t
p
)
2
.
After generating the candidate lists for each
word, the rest of the extension is rather straight-
forward. For each sampling, let H be the set of
candidate heads and T be the set of candidate POS
tags. The Gibbs sampler will generate a new sam-
ple from the space H ? T . The other parts of the
algorithm remain the same.
2
In our work we choose ? = 0.003, which gives a 98.9%
oracle POS tagging accuracy on the CATiB development set.
201
arc!
head bigram!!h h m m+1arbitrary sibling!?!h m sh m consecutive sibling!h m s grandparent!g h m
grand-sibling!g h m s tri-siblings!h m s t grand-grandparent!g h mgg
outer-sibling-grandchild!h m sgc h s gcminner-sibling-grandchild!
Figure 5: First- to third-order features.
4 Features
First- to Third-Order Features The feature
templates of first- to third-order features are
mainly drawn from previous work on graph-
based parsing (McDonald and Pereira, 2006),
transition-based parsing (Nivre et al, 2006) and
dual decomposition-based parsing (Martins et al,
2011). As shown in Figure 5, the arc is the basic
structure for first-order features. We also define
features based on consecutive sibling, grandpar-
ent, arbitrary sibling, head bigram, grand-sibling
and tri-siblings, which are also used in the Turbo
parser (Martins et al, 2013). In addition to these
first- to third-order structures, we also consider
grand-grandparent and sibling-grandchild struc-
tures. There are two types of sibling-grandchild
structures: (1) inner-sibling when the sibling is
between the head and the modifier and (2) outer-
sibling for the other cases.
Global Features We used feature shown promis-
ing in prior reranking work Charniak and Johnson
(2005), Collins (2000) and Huang (2008).
? Right Branch This feature enables the model
to prefer right or left-branching trees. It counts
the number of words on the path from the root
node to the right-most non-punctuation word,
normalized by the length of the sentence.
? Coordination In a coordinate structure, the two
adjacent conjuncts usually agree with each other
on POS tags and their span lengths. For in-
stance, in cats and dogs, the conjuncts are both
short noun phrases. Therefore, we add differ-
ent features to capture POS tag and span length
consistency in a coordinate structure.
? PP Attachment We add features of lexical tu-
eat! with! knife! and! fork!
Figure 6: An example of PP attachment with coor-
dination. The arguments should be knife and fork,
not and.
ples involving the head, the argument and the
preposition of prepositional phrases. Generally,
this feature can be defined based on an instance
of grandparent structure. However, we also han-
dle the case of coordination. In this case, the ar-
guments should be the conjuncts rather than the
coordinator. Figure 6 shows an example.
? Span Length This feature captures the distribu-
tion of the binned span length of each POS tag.
It also includes flags of whether the span reaches
the end of the sentence and whether the span is
followed by the punctuation.
? Neighbors The POS tags of the neighboring
words to the left and right of each span, together
with the binned span length and the POS tag at
the span root.
? Valency We consider valency features for each
POS tag. Specifically, we add two types of va-
lency information: (1) the binned number of
non-punctuation modifiers and (2) the concate-
nated POS string of all those modifiers.
? Non-projective Arcs A flag indicating if a de-
pendency is projective or not (i.e. if it spans a
word that does not descend from its head) (Mar-
tins et al, 2011). This flag is also combined with
the POS tags or the lexical words of the head and
the modifier.
POS Tag Features In the joint POS correction
scenario, we also add additional features specifi-
cally for POS prediction. The feature templates
are inspired by previous feature-rich POS tagging
work (Toutanova et al, 2003). However, we are
free to add higher order features because we do
not rely on dynamic programming decoding. In
our work we use feature templates up to 5-gram.
Table 1 summarizes all POS tag feature templates.
5 Experimental Setup
Datasets We evaluate our model on standard
benchmark corpora ? CoNLL 2006 and CoNLL
2008 (Buchholz and Marsi, 2006; Surdeanu et al,
2008) ? which include dependency treebanks for
14 different languages. Most of these data sets
202
1-gram
?t
i
?, ?t
i
, w
i?2
?, ?t
i
, w
i?1
?, ?t
i
, w
i
?, ?t
i
, w
i+1
?,
?t
i
, w
i+2
?
2-gram
?t
i?1
, t
i
?, ?t
i?2
, t
i
?, ?t
i?1
, t
i
, w
i?1
?,
?t
i?1
, t
i
, w
i
?
3-gram
?t
i?1
, t
i
, t
i+1
?, ?t
i?2
, t
i
, t
i+1
, ?, ?t
i?1
, t
i
, t
i+2
?,
?t
i?2
, t
i
, t
i+2
?
4-gram
?t
i?2
, t
i?1
, t
i
, t
i+1
?, ?t
i?2
, t
i?1
, t
i
, t
i+2
?,
?t
i?2
, t
i
, t
i+1
, t
i+2
?
5-gram ?t
i?2
, t
i?1
, t
i
, t
i+1
, t
i+2
?
Table 1: POS tag feature templates. t
i
and w
i
de-
notes the POS tag and the word at the current posi-
tion. t
i?x
and t
i+x
denote the left and right context
tags, and similarly for words.
contain non-projective dependency trees. We use
all sentences in CoNLL datasets during training
and testing. We also use the Columbia Arabic
Treebank (CATiB) (Marton et al, 2013). CATiB
mostly includes projective trees. The trees are an-
notated with both gold and predicted versions of
POS tags and morphology information. Follow-
ing Marton et al (2013), for this dataset we use
12 core POS tags, word lemmas, determiner fea-
tures, rationality features and functional genders
and numbers.
Some CATiB sentences exceed 200 tokens. For
efficiency, we limit the sentence length to 70 to-
kens in training and development sets. However,
we do not impose this constraint during testing.
We handle long sentences during testing by apply-
ing a simple split-merge strategy. We split the sen-
tence based on the ending punctuation, predict the
parse tree for each segment and group the roots of
resulting trees into a single node.
Evaluation Measures Following standard prac-
tice, we use Unlabeled Attachment Score (UAS)
as the evaluation metric in all our experiments.
We report UAS excluding punctuation on CoNLL
datasets, following Martins et al (2013). For the
CATiB dataset, we report UAS including punctu-
ation in order to be consistent with the published
results in the 2013 SPMRL shared task (Seddah et
al., 2013).
Baselines We compare our model with the Turbo
parser and the MST parser. For the Turbo parser,
we directly compare with the recent published re-
sults in (Martins et al, 2013). For the MST parser,
we train a second-order non-projective model us-
ing the most recent version of the code
3
.
We also compare our model against a discrim-
inative reranker. The reranker operates over the
3
http://sourceforge.net/projects/mstparser/
top-50 list obtained from the MST parser
4
. We
use a 10-fold cross-validation to generate candi-
date lists for training. We then train the reranker
by running 10 epochs of cost-augmented MIRA.
The reranker uses the same features as our model,
along with the tree scores obtained from the MST
parser (which is a standard practice in reranking).
Experimental Details Following Koo and Collins
(2010), we always first train a first-order pruner.
For each word x
i
, we prune away the incoming
dependencies ?h
i
, x
i
? with probability less than
0.005 times the probability of the most likely head,
and limit the number of candidate heads up to 30.
This gives a 99% pruning recall on the CATiB
development set. The first-order model is also
trained using the algorithm in Figure 4. Af-
ter pruning, we tune the regularization parameter
C = {0.1, 0.01, 0.001} on development sets for
different languages. Because the CoNLL datasets
do not have a standard development set, we ran-
domly select a held out of 200 sentences from the
training set. We also pick the training epochs from
{50, 100, 150} which gives the best performance
on the development set for each language. After
tuning, the model is trained on the full training set
with the selected parameters.
We apply the Random Walk-based sampling
method (see Section 3.2.2) for the standard de-
pendency parsing task. However, for the joint
parsing and POS correction on the CATiB dataset
we do not use the Random Walk method because
the first-order features in normal parsing are no
longer first-order when POS tags are also vari-
ables. Therefore, the first-order distribution is not
well-defined and we only employ Gibbs sampling
for simplicity. On the CATiB dataset, we restrict
the sample trees to always be projective as de-
scribed in Section 3.2.1. However, we do not im-
pose this constraint for the CoNLL datasets.
6 Results
Comparison with State-of-the-art Parsers Ta-
ble 2 summarizes the performance of our model
and of the baselines. We first compare our model
to the Turbo parser using the Turbo parser fea-
ture set. This is meant to test how our learning
and inference methods compare to a dual decom-
position approach. The first column in Table 2
4
The MST parser is trained in projective mode for rerank-
ing because generating top-k list from second-order non-
projective model is intractable.
203
Our Model (UAS)
Turbo (UAS)
MST 2nd-Ord.
(UAS)
Best Published UAS
Top-50
Reranker
Top-500
RerankerTurbo Feat. Full Feat.
Arabic 79.86 80.21 79.64 78.75 81.12 (Ma11) 79.03 78.91
Bulgarian 92.97 93.30 93.10 91.56 94.02 (Zh13) 92.81 -
Chinese 92.06 92.63 89.98 91.77 91.89 (Ma10) 92.25 -
Czech 90.62 91.04 90.32 87.30 90.32 (Ma13) 88.14 -
Danish 91.45 91.80 91.48 90.50 92.00 (Zh13) 90.88 90.91
Dutch 85.83 86.47 86.19 84.11 86.19 (Ma13) 81.01 -
English 92.79 92.94 93.22 91.54 93.22 (Ma13) 92.41 -
German 91.79 92.07 92.41 90.14 92.41 (Ma13) 91.19 -
Japanese 93.23 93.42 93.52 92.92 93.72 (Ma11) 93.40 -
Portuguese 91.82 92.41 92.69 91.08 93.03 (Ko10) 91.47 -
Slovene 86.19 86.82 86.01 83.25 86.95 (Ma11) 84.81 85.37
Spanish 88.24 88.21 85.59 84.33 87.96 (Zh13) 86.85 87.21
Swedish 90.48 90.71 91.14 89.05 91.62 (Zh13) 90.53 -
Turkish 76.82 77.21 76.90 74.39 77.55 (Ko10) 76.35 76.23
Average 88.87 89.23 88.72 86.86 89.33 87.92 -
Table 2: Results of our model, the Turbo parser, and the MST parser. ?Best Published UAS? includes the
most accurate parsers among Nivre et al (2006), McDonald et al (2006), Martins et al (2010), Martins
et al (2011), Martins et al (2013), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald
(2012) and Zhang et al (2013). Martins et al (2013) is the current Turbo parser. The last two columns
shows UAS of the discriminative reranker.
shows the result for our model with an average of
88.87%, and the third column shows the results
for the Turbo parser with an average of 88.72%.
This suggests that our learning and inference pro-
cedures are as effective as the dual decomposition
method in the Turbo parser.
Next, we add global features that are not used by
the Turbo parser. The performance of our model
is shown in the second column with an average of
89.23%. It outperforms the Turbo parser by 0.5%
and achieves the best reported performance on
four languages. Moreover, our model also outper-
forms the 88.80% average UAS reported in Mar-
tins et al (2011), which is the top performing sin-
gle parsing system (to the best of our knowledge).
Comparison with Reranking As column 6 of Ta-
ble 2 shows, our model outperforms the reranker
by 1.3%
5
. One possible explanation of this perfor-
mance gap between the reranker and our model is
the small number of candidates considered by the
reranker. To test this hypothesis, we performed
experiments with top-500 list for a subset of lan-
guages.
6
As column 7 shows, this increase in the
list size does not change the relative performance
of the reranker and our model.
Joint Parsing and POS Correction Table 3
shows the results of joint parsing and POS cor-
rection on the CATiB dataset, for our model and
5
Note that the comparison is conservative because we
can also add MST scores as features in our model as in
reranker. With these features our model achieves an average
UAS 89.28%.
6
We ran this experiment on 5 languages with small
datasets due to the scalability issues associated with rerank-
ing top-500 list.
state-of-the-art systems. As the upper part of the
table shows, the parser with corrected tags reaches
88.38% compared to the accuracy of 88.46% on
the gold tags. This is a substantial increase from
the parser that uses predicted tags (86.95%).
To put these numbers into perspective, the bot-
tom part of Table 3 shows the accuracy of the best
systems from the 2013 SPMRL shared task on
Arabic parsing using predicted information (Sed-
dah et al, 2013). Our system not only out-
performs the best single system (Bj?orkelund et
al., 2013) by 1.4%, but it also tops the ensem-
ble system that combines three powerful parsers:
the Mate parser (Bohnet, 2010), the Easy-First
parser (Goldberg and Elhadad, 2010) and the
Turbo parser (Martins et al, 2013)
Impact of Sampling Methods We compare two
sampling methods introduced in Section 3.2 with
respect to their decoding efficiency. Specifically,
we measure the score of the retrieved trees in test-
ing as a function of the decoding speed, measured
by the number of tokens per second. We change
the temperature update rate c in order to decode
with different speed. In Figure 7 we show the cor-
responding curves for two languages: Arabic and
Chinese. We select these two languages as they
correspond to two extremes in sentence length:
Arabic has the longest sentences on average, while
Chinese has the shortest ones. For both languages,
the tree score improves over time. Given sufficient
time, both sampling methods achieve the same
score. However, the Random Walk-based sam-
pler performs better when the quality is traded for
speed. This result is to be expected given that each
204
Dev. Set (? 70) Testing Set
POS Acc. UAS POS Acc. UAS
Gold - 90.27 - 88.46
Predicted 96.87 88.81 96.82 86.95
POS Correction 97.72 90.08 97.49 88.38
CADIM 96.87 87.4- 96.82 85.78
IMS-Single - - - 86.96
IMS-Ensemble - - - 88.32
Table 3: Results for parsing and corrective tagging
on the CATiB dataset. The upper part shows UAS
of our model with gold/predicted information or
POS correction. Bottom part shows UAS of the
best systems in the SPMRL shared task. IMS-
Single (Bj?orkelund et al, 2013) is the best single
parsing system, while IMS-Ensemble (Bj?orkelund
et al, 2013) is the best ensemble parsing system.
We also show results for CADIM (Marton et al,
2013), the second best system, because we use
their predicted features.
0 20 40 60 80 1002.648
2.65
2.652
2.654
2.656
2.658 x 104
Toks/sec
Score
 
 
GibbsRandom Walk
(a) Arabic
0 100 200 300 400 500 600 700 8001.897
1.898
1.899
1.9 x 10
4
Toks/sec
Score
 
 
GibbsRandom Walk
(b) Chinese
Figure 7: Total score of the predicted test trees as
a function of the decoding speed, measured in the
number of tokens per second.
iteration of this sampler makes multiple changes
to the tree, in contrast to a single-edge change of
Gibbs sampler.
The Effect of Constraints in Learning Our train-
ing method updates parameters to satisfy the pair-
wise constraints between (1) subsequent samples
on the sampling path and (2) selected samples and
the ground truth. Figure 8 shows that applying
both types of constraints is consistently better than
using either of them alone. Moreover, these re-
sults demonstrate that comparison between subse-
quent samples is more important than comparison
against the gold tree.
Decoding Speed Our sampling-based parser is an
Danish Japanese Portuguese Swedish89
90
91
92
93
94
UAS
(%)
 
 BothNeighborGold
Figure 8: UAS on four languages when train-
ing with different constraints. ?Neighbor? corre-
sponds to pairwise constraints between subsequent
samples, ?Gold? represents constraints between a
single sample and the ground truth, ?Both? means
applying both types of constraints.
anytime algorithm, and therefore its running time
can be traded for performance. Figure 7 illustrates
this trade-off. In the experiments reported above,
we chose a conservative cooling rate and contin-
ued to sample until the score no longer changed.
The parser still managed to process all the datasets
in a reasonable time. For example, the time that it
took to decode all the test sentences in Chinese and
Arabic were 3min and 15min, respectively. Our
current implementation is in Java and can be fur-
ther optimized for speed.
7 Conclusions
This paper demonstrates the power of combining a
simple inference procedure with a highly expres-
sive scoring function. Our model achieves the best
results on the standard dependency parsing bench-
mark, outperforming parsing methods with elabo-
rate inference procedures. In addition, this frame-
work provides simple and effective means for joint
parsing and corrective tagging.
Acknowledgments
This research is developed in collaboration with
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the IYAS project. The authors acknowledge
the support of the MURI program (W911NF-10-
1-0533, the DARPA BOLT program and the US-
Israel Binational Science Foundation (BSF, Grant
No 2012330). We thank the MIT NLP group and
the ACL reviewers for their comments.
205
References
Christophe Andrieu, Nando De Freitas, Arnaud
Doucet, and Michael I Jordan. 2003. An introduc-
tion to mcmc for machine learning. Machine learn-
ing, 50(1-2):5?43.
Anders Bj?orkelund, Ozlem Cetinoglu, Rich?ard Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(re)ranking meets morphosyntax: State-of-the-art
results from the SPMRL 2013 shared task. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 135?
145, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In COLING,
pages 89?97.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149?164.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning, ICML ?00, pages 175?182.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
The Journal of Machine Learning Research, 3:951?
991.
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
learning, 75(3):297?325.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, gibbs distributions, and the bayesian
restoration of images. Pattern Analysis andMachine
Intelligence, IEEE Transactions on, (6):721?741.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742?750. Association for Computa-
tional Linguistics.
Barry Haddow, Abhishek Arun, and Philipp Koehn.
2011. Samplerank training for phrase-based ma-
chine translation. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 261?
271. Association for Computational Linguistics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586?
594.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1?11. Association for
Computational Linguistics.
Terry Koo, Alexander M Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1288?1298. Association for Compu-
tational Linguistics.
Quannan Li, Jingdong Wang, Zhuowen Tu, and
David P Wipf. 2013. Fixed-point model for struc-
tured labeling. In Proceedings of the 30th Interna-
tional Conference on Machine Learning (ICML-13),
pages 214?221.
Andr?e FT Martins, Noah A Smith, and Eric P Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 342?350. Association for
Computational Linguistics.
Andr?e FT Martins, Noah A Smith, Eric P Xing, Pe-
dro MQ Aguiar, and M?ario AT Figueiredo. 2010.
Turbo parsers: Dependency parsing by approxi-
mate variational inference. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 34?44. Association for
Computational Linguistics.
Andr?e FT Martins, Noah A Smith, Pedro MQ Aguiar,
and M?ario AT Figueiredo. 2011. Dual decompo-
sition with many overlapping components. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 238?249. As-
sociation for Computational Linguistics.
Andr?e FT Martins, Miguel B Almeida, and Noah A
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proceedings of the
51th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Yuval Marton, Nizar Habash, Owen Rambow, and
Sarah Alkhulani. 2013. Spmrl13 shared task sys-
tem: The cadim arabic dependency parser. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 76?
80.
Ryan T McDonald and Fernando CN Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In EACL.
206
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 523?530.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, pages 216?220. Association for
Computational Linguistics.
Tetsuji Nakagawa. 2007. Multilingual dependency
parsing using global features. In EMNLP-CoNLL,
pages 952?956.
Joakim Nivre, Johan Hall, Jens Nilsson, G?uls?en Eryiit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector
machines. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 221?225. Association for Computational Lin-
guistics.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1346. Association for Computa-
tional Linguistics.
Alexander M Rush and Slav Petrov. 2012. Vine prun-
ing for efficient multi-pass dependency parsing. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 498?507. Association for Computational Lin-
guistics.
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie
Candito, Jinho D Choi, Rich?ard Farkas, Jennifer
Foster, Iakes Goenaga, Koldo Gojenola Gallete-
beitia, Yoav Goldberg, et al 2013. Overview of the
spmrl 2013 shared task: A cross-framework evalua-
tion of parsing morphologically rich languages. In
Proceedings of the Fourth Workshop on Statistical
Parsing of Morphologically-Rich Languages, pages
146?182.
D. Sontag, A. Globerson, and T. Jaakkola. 2011. In-
troduction to dual decomposition for inference. In
Optimization for Machine Learning, pages 219?254.
MIT Press.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
conll-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning, pages 159?177. Association for
Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Michael L. Wick, Khashayar Rohanimanesh, Kedar
Bellare, Aron Culotta, and Andrew McCallum.
2011. Samplerank: Training factor graphs with
atomic gradients. In Lise Getoor and Tobias Schef-
fer, editors, Proceedings of the 28th International
Conference on Machine Learning, ICML 2011,
pages 777?784.
David Bruce Wilson. 1996. Generating random span-
ning trees more quickly than the cover time. In
Proceedings of the twenty-eighth annual ACM sym-
posium on Theory of computing, pages 296?303.
ACM.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 320?331. Association for Computational Lin-
guistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
188?193. Association for Computational Linguis-
tics.
Hao Zhang, Liang Huang Kai Zhao, and Ryan McDon-
ald. 2013. Online learning for inexact hypergraph
search. In Proceedings of EMNLP.
207
