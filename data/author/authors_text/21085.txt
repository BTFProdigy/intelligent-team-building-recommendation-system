Learning to identify animate references
Constantin Ora?san
School of Humanities, Languages
and Social Sciences
University of Wolverhampton
C.Orasan@wlv.ac.uk
Richard Evans
School of Humanities, Languages
and Social Sciences
University of Wolverhampton
R.J.Evans@wlv.ac.uk
Abstract
Information about the animacy of
nouns is important for a wide range of
tasks in NLP. In this paper, we present
a method for determining the animacy
of English nouns using WordNet and
machine learning techniques. Our
method firstly categorises the senses
from WordNet using an annotated
corpus and then uses this information
in order to classify nouns for which
the sense is not known. Our evaluation
results show that the accuracy of the
classification of a noun is around 97%
and that animate entities are more
difficult to identify than inanimate
ones.
1 Introduction
Information on the gender of noun phrase (NP)
referents can be exploited in a range of NLP
tasks including anaphora resolution and the
applications that can benefit from it such as
coreference resolution, information retrieval,
information extraction, machine translation,
etc. The gender of NP referents is explicitly
realised morphologically in languages such as
Romanian, French, Russian, etc. in which the
head of the NP or the NP?s determiner undergoes
predictable morphological transformation or
affixation to reflect its referent?s gender. In the
English language, the gender of NPs? referents is
not predictable from the surface morphology.
Moreover, in (Evans and Ora?san, 2000) it was
argued that it is not always desirable to obtain
information concerning the specific gender of
a NP?s referent in English. Instead, it is more
effective to obtain the animacy of each NP. We
define animacy as the property of a NP whereby
its referent, in singular rather than plural number,
can be referred to using a pronoun in the set
fhe, him, his, himself, she, her, hers, herselfg.
During the course of this paper, we will discuss
animate and inanimate senses of nouns and verbs.
We use these expressions to denote the senses
of nouns that are the heads of NPs referring to
animate/inanimate entities and the senses of verbs
whose agents are typically animate/inanimate
entities.
In our previous work, we investigated the use
of WordNet in order to determine the animacy of
entities in discourse. There, we used the fact that
each noun and verb sense is derived from unique
classes called unique beginners. We classified
each unique beginner as being a hypernym of a
set of senses that were for the most part either
animate or inanimate (in the case of nouns) or
indicative of animacy/inanimacy in their subjects
(in the case of verbs). In classifying a noun, the
number of its senses that belong to an animate
class is compared with the number belonging
to an inanimate class, and this information is
used to make the final classification. In addition,
if the noun is the head of a subject, the same
information is computed for the verb. Our
assumption was that a noun with many animate
senses is likely to be used to refer to an animate
entity. For subjects, the information from the
main verb was used to take into consideration the
context of the sentence. That system, referred
to in this paper as the previous system also used
a proper name gazetteer and some simple rules
which mainly assisted in the classification of
named entities. For reasons explained in Section
4.2, these additions to the basic algorithm were
ignored in the comparative evaluation described
there.
Experiments with that algorithm showed it to
be useful. Applied to a system for automatic
pronominal anaphora resolution, it led to a
substantial improvement in the ratio of suitable
and unsuitable candidates in the sets considered
by the anaphora resolver (Evans and Ora?san,
2000).
However, the previous system has two main
weaknesses. The first one comes from the fact
that the classes used to determine the number
of animate/inanimate senses are too general, and
in most cases they do not reliably indicate the
animacy of each sense in the class. The second
weakness is due to the naive nature of the rules
that decide if a NP is animate or not. Their
application is simple and involves a comparison
of values obtained for a NP with threshold values
that were determined on the basis of a relatively
small number of experiments.
In this paper, we present a new method for
animacy identification which uses WordNet and
machine learning techniques. The remainder
of the paper is structured as follows. Section
2 briefly describes some concepts concerning
WordNet that are used in this paper. In Section 3,
our two step method is described. An evaluation
of the method and discussion of the results is
presented in Section 4. We end the paper by
reviewing previous related work and drawing
some conclusions.
2 Background information
As previously mentioned, in this research
WordNet (Fellbaum, 1998) is used to identify
the animacy of a noun. In this section several
important concepts from WordNet are explained.
WordNet is an electronic lexical resource
organized hierarchically by relations between
sets of synonyms or near-synonyms called
synsets. Each of the four primary classes of
content-words, nouns, verbs, adjectives and
adverbs are arranged under a small set of so-
called unique beginners. In the case of nouns
and verbs, which are the concern of the present
paper, the unique beginners are the most general
concepts under which the entire set of entries is
organized on the basis of hyponymy/hypernymy
relations. Hypernymy is the relation that holds
between such word senses as vehicle
1
-ship
1
or
human
1
-politician
1
, in which the first items
in the pairs are more general than the second.
Conversely, the second items are more specific
than the first, and are their hyponyms.
It is usual to regard hypernymy as a vertically
arranged relationship, with general senses
positioned higher than more specific ones in an
ontology. In WordNet, the top-most senses are
called unique beginners. Senses at the same
vertical level in the ontology are also clustered
horizontally through the synonymy relation in
synsets. In this paper, the term node is used
interchangeably with synset.
As explained in Section 3.1, our method
requires that the nodes in WordNet are classified
according to their animacy. Given the size of
WordNet, this task cannot be done manually and
a corpus where words are annotated with their
senses was necessary. A corpus that meets these
requirements is SEMCOR (Landes et al, 1998),
a subset of the Brown Corpus in which the nouns
and the verbs have been manually annotated with
their senses from WordNet.
3 The method
In this section a two step method used to classify
words according to their animacy is presented. In
Section 3.1, we present an automatic method for
determining the animacy of senses from WordNet
on the basis of an annotated corpus. Once the
senses from WordNet have been classified, a
classical machine learning technique uses this
information to determine the animacy of a noun
for which the sense is not known. This technique
is presented in Section 3.2.
3.1 The classification of the senses
As previously mentioned, the unique beginners
are too general to be satisfactorily classified as
animate or inanimate. However, this does not
26
6
4
HYPERNYM
ani
h
= ani
i
inani
h
= inani
i
3
7
7
5
2
6
6
4
Sense
1
ani
1
inani
1
3
7
7
5
2
6
6
4
Sense
2
ani
2
inani
2
3
7
7
5
2
6
6
4
Sense
3
ani
3
inani
3
3
7
7
5
  
2
6
6
4
Sense
n
ani
n
inani
n
3
7
7
5
Figure 1: Example of hypernymy relation between senses in WordNet
Sense
1
Sense
2
Sense
3
... Sense
n
Observed ani
1
ani
2
ani
3
... ani
n
Expected ani
1
+ inani
1
ani
2
+ inani
2
ani
3
+ inani
3
... ani
n
+ inani
n
Table 1: Contingency table for testing if a hypernym is animate
mean that it is not possible to uniquely classify
more specific senses as animate or inanimate. In
this section, we present a corpus-based method
which classifies the synsets from WordNet
according to their animacy.
The NPs in a 52 file subset of the SEMCOR
corpus were manually annotated with animacy
information and then used by an automatic system
to classify the nodes. These 52 files contain 2512
animate entities and 17514 inanimate entities.
The system attempts to classify the senses
from WordNet that explicitly appear in the
corpus directly, on the basis of their frequency.1
However, our goal is to design a procedure which
is also able to classify senses that are not found
in the corpus. To this end, we decided to use a
bottom up procedure which starts by classifying
the terminal nodes and then continues with more
general nodes. The terminal nodes are classified
using the information straight from the annotated
files. When classifying a more general node,
the following hypothesis is used: ?if all the
1Due to linguistic ambiguities and tagging errors, not all
the senses at this level can be classified adequately in this
way.
hyponyms of a sense are animate, then the sense
itself is animate?. However, this does not always
hold because of annotation errors or rare uses of
a sense and instead, a statistical measure must be
used to test the animacy of a more general node.
Several measures were considered and the most
appropriate one seemed to be chi-square.
Chi-square is a non-parametric test which can
be used for estimating whether or not there is
any difference between the frequencies of items
in frequency tables (Oakes, 1998). The formula
used to calculate chi-square is:

2
=
X
(O  E)
2
E
(1)
where O is the observed number of cases and E
the expected number of cases. If 2 is less than
or equal to a critical level, we may conclude that
the observed and expected values do not differ
significantly.
Each time that a more general node is to be
classified, its hyponyms are considered. If all the
hyponyms observed in the corpus2 are annotated
as either animate or inanimate (but not both), the
2Either directly or indirectly via the hyponymy relations.
Generalisation rejected.... for hypernym Def:(any living entity)
Ani 16 Inani 3 person (sense 1)
++++Def: (a human being; "there was too much for one person to do")
Ani 0 Inani 11 animal (sense 1)
++++Def: (a living organism characterized by voluntary movement)
Figure 2: Example of generalisation rejected
Generalisation accepted .... for hypernym Def:(the continuum of
experience in which events pass from the future through the
present to the past)
Ani 0 Inani 9 past (sense 1)
++++Def: (the time that has elapsed; "forget the past")
Ani 0 Inani 6 future (sense 1)
++++Def: (the time yet to come)
Figure 3: Example of generalisation accepted
more general node is classified as its hyponyms
are. However, for the aforementioned reasons,
this rule does not apply in all cases. In the
remaining cases the chi-square test is applied.
For each more general node which is about to
be classified, two hypotheses are tested: the first
one considers the node animate and the second
one inanimate. The system classifies the node
according to which test is passed. If neither are
passed, it means that the node is too general and
it and all its hypernyms can equally refer to both
animate and inanimate entities.
For example, a more general node can have
several hyponyms as shown in Figure 1. In that
case, the hypernym has n hyponyms. We consider
each sense to have two attributes: the number
of times it has been annotated as animate (ani
i
)
and the number of times it has been annotated
as inanimate (inani
i
). For more general nodes,
these attributes are the sum of the number of
animate/inanimate instances of its hyponyms.
When the node is tested to determine whether or
not it is animate, a contingency table like Table
1 is built. Given that we are testing to see if the
more general node is animate or not, for each of
its hyponyms, the total number of occurrences of
a sense in the annotated corpus is the expected
value (meaning that all the instances should be
animate) and the number of times the hyponym is
annotated as referring to an animate entity is the
observed value. Formula 1 is used to compute
chi-square, and the result is compared with the
critical level obtained for n-1 degrees of freedom
and a significance level of .05. If the test is
passed, the more general node is classified as
animate. In a similar way, more general nodes
are tested for inanimacy. Figures 2 and 3 show
two small examples in which the generalisation
is rejected and accepted, respectively.
In order to be a valid test of significance, chi-
square usually requires expected frequencies to be
5 or more. If the contingency table is larger than
two-by-two, some few exceptions are allowed as
long as no expected frequency is less than one and
no more than 20% of the expected frequencies are
less than 5 (Sirkin, 1995). In our case it is not
possible to have expected frequencies less than
one because this would entail no presence in the
corpus. If, when the test is applied, more than
20% of the senses have an expected frequency
less than 5, the two similar senses with the lowest
frequency are merged and the test is repeated.3 If
no senses can be merged and still more than 20%
of the expected frequencies are less than 5, the
test is rejected.
3.2 The classification of a word
The classification described in the previous
section is useful for determining the animacy of a
sense, even for those which were not previously
found in the annotated corpus, but which are
hyponyms of a node that has been classified.
However, nouns whose sense is unknown cannot
be classified directly and therefore an additional
level of processing is necessary. In this section,
we show how TiMBL (Daelemans et al, 2000)
3Two senses are considered similar if they both have the
same attribute equal to zero.
was used to determine the animacy of nouns.
TiMBL is a program which implements several
machine learning techniques. After trying the
algorithms available in TiMBL with different
configurations, the best results were obtained
using instance-based learning with gain ratio as
the weighting measure (Quinlan, 1993; Mitchell,
1997). In this type of learning, all the instances
are stored without trying to infer anything from
them. At the classification stage, the algorithm
compares a previously unseen instance with
all the data stored at the training stage. The
most frequent class in the k nearest neighbours
is assigned as the class to which that instance
belongs. After experimentation, it was noticed
that the best results were obtained when k=3.
In our case the instances used in training
and classification consist of the following
information:
 The lemma of the noun which is to be
classified.
 The number of animate and inanimate senses
of the word. As we mentioned before, in
the cases where the animacy of a sense is
not known, it is inferred from its hypernyms.
If this information cannot be found for any
of a word?s hypernyms, information on the
unique beginners for the word?s sense is
used, in a manner similar to that used in
(Evans and Ora?san, 2000).
 If the word is the head of a subject, the
number of animate/inanimate senses of
its verb. For those senses for which the
classification is not known, an algorithm
similar to the one described for nouns is
employed. These values are 0 for heads of
non-subjects.
 The ratio of the number of animate singular
pronouns (e.g he or she) to inanimate
singular pronouns (e.g. it) in the whole text.
The output of this stage is a list of nouns
classified according to their animacy.
4 Evaluation and discussion
In this section we examine the performance
of the system, particularly with respect to the
classification of nouns; investigate sources of
errors; and highlight directions for future research
and improvements to the system.
4.1 The performance of the system
The system was evaluated with respect to two
corpora. The first one consists of the files selected
from the SEMCOR corpus stripped of the sense
annotation. The second one is a selection of
texts from Amnesty International (AI) used in our
previous research. These texts have been selected
because they include a relatively large number of
references to animate entities. By including the
texts from the second corpus we could compare
the results of our previous system with those
obtained here. In addition, we can assess the
results of the algorithm on data which was not
used to determine the animacy of the senses. The
characteristics of the two corpora are presented in
Table 2.
In this research three measures were used
to assess the performance of the algorithm:
accuracy, precision and recall. The accuracy is
the ratio between the number of items correctly
classified and the total number of items to be
classified. This measure assesses the performance
of the classification algorithm, but can be slightly
misleading because of the greater number of
inanimate entities in texts. In order to alleviate
this problem, we computed the precision and
recall for each type of classification. The
precision with which the method classifies
animate entities is defined as the ratio between
the number of entities it correctly classifies
as animate and the total number of entities it
classifies as animate (including the ones wrongly
assigned to this class). The method?s recall
over this task is defined as the ratio between the
number of entities correctly classified as animate
by the method and the total number of animate
entities to be classified. The precision and recall
for inanimate entities is defined in a similar
manner.
We consider that by using recall and precision
for each type of entity we can better assess the
performance of the algorithms. This is mainly
because the large number of inanimate entities are
considered separately from the smaller number of
animate entities. In addition to this, by separating
Corpus No of words No. of animate entities No of inanimate entities
SEMCOR 104612 2512 17514
AI 15767 537 2585
Table 2: The characteristics of the two corpora used
Animacy Inanimacy
Experiment Accuracy Precision Recall Precision Recall
Baseline on SEMCOR 37.62% 8.40% 74.44% 88.41% 31.64%
Baseline on AI 31.01% 18.07% 76.48% 79.27% 20.60%
Previous system on AI 64.87% 93.88% 36.09% 81.00 % 99.14%
New System on SEMCOR 97.51% 88.93% 91.03% 98.74% 98.41%
New System on AI 97.69% 94.28% 92.17% 98.38% 98.83%
Table 3: The results of the evaluation
the evaluation of the classification of animate
entities from the one for inanimate entities we can
assess the difficulty of each classification.
Table 3 presents the results of the method on
the two data sets. For the experiment with the
SEMCOR corpus, we evaluated it using five-fold
cross-validation. We randomly split the whole
corpus into five disjoint parts, using four parts for
training and one for evaluation. We repeated the
training-evaluation cycle five times, making sure
that the whole corpus was used. Note that for
each iteration of the cross-validation, the learning
process begins from scratch. The results reported
were obtained by averaging the error rates from
each of the 5 runs. In the second experiment, all
52 files from the SEMCOR corpus were used for
training and the texts from Amnesty International
for testing.
In addition to the results of the method
presented in this paper, Table 3 presents the
results of a baseline method and of the method
previously proposed in (Evans and Ora?san, 2000).
In the baseline method, the probability that an
entity is classified as animate is proportional
to the number of animate third person singular
pronouns in the text.
As can be seen in Table 3 the accuracy of the
baseline is very low. The results of our previous
method are considerably higher, but still poor
in the case of animate entities with many of
these being classified as inanimate.4 This can
4Due to time constraints and the large amount of effort
be explained by the fact that most of the unique
beginners were classified as inanimate, and
therefore there is a tendency to classify entities
as inanimate. The best results were obtained
by the new method over both corpora, the main
improvement being noticed in the classification
of animate entities.
Throughout this section we referred to the
classification of ambiguous nouns without trying
to assess how successful the classification of the
synsets in WordNet was. Such an assessment
would be interesting, but would require manual
classification of the nodes in WordNet, and
therefore would be somewhat time consuming.
Even though this evaluation was not carried out,
the high accuracy of the system suggests that the
current classification is useful.
4.2 Comments and error analysis
During the training phase of TiMBL, the program
computes the importance of each feature for
the classification. The most important feature
according to the gain ratio is the number of
animate senses of a noun followed by the number
of inanimate senses of the noun. This was
expected given that our method is based on the
idea that in most of the cases the number of
animate and inanimate senses determines the
animacy of a noun. However, this would mean
that the same noun will be classified in the same
required to transform the input data into a format usable
by the previous method, it was not possible to assess its
performance with respect to the SEMCOR corpus.
way regardless of the text. Therefore, three text
dependent features were introduced. They are the
number of animate and inanimate senses of the
predicate of the sentence if the noun is a subject,
and the ratio between the number of animate
third-person singular pronouns and inanimate
third-person singular pronouns in the text. In
terms of importance, gain ratio ranks them fourth,
fifth and sixth, respectively, after the lemma of
the noun. The lemma of the noun was included
because it was noticed that this improves the
accuracy of the method.
During the early stages of the evaluation, the
classification of personal names proved to be a
constant source of errors. Further investigation
showed that the system performed poorly on all
types of named entities. For the named entities
referring to companies, products, etc. this can
be explained by the fact that in many cases they
are not found in WordNet. However, in most
cases the system correctly classified them as
inanimate, having learned that most unknown
words belong to this class. Entities denoted by
personal names were constantly misclassified
either because the names were not in WordNet or
else they appeared with a substantial number of
inanimate senses (e.g. the names Bob and Maria
do not have any senses in WordNet which could
relate them to animate entities). In light of these
errors we decided not to present our system with
named entities. With no access to more accurate
techniques, we considered non-sentence-initial
capitalised words as named entities and removed
them from the evaluation data. Even when this
crude filtering was applied, we still presented
a significant number of proper names to our
system. This partially explains its lower accuracy
with respect to the classification of animate
entities.
By attempting to filter proper names, we
could not compare the new system with the one
referred to as the extended algorithm in (Evans
and Ora?san, 2000). In future, we plan to address
the problem of named entities by using gazetteers
or, alternatively, developing more sophisticated
named entity recognition methods.
Another source of errors is the unusual usage
of senses. For example someone can refer to their
pet with he or she, and therefore according to
our definition they should be considered animate.
However, given the way the algorithm is designed
there is no way to take these special uses into
consideration.5
Another problem with the method is the fact
that all the senses have the same weight. This
means that a word like pupil, which has two
animate senses and one inanimate, is highly
unlikely to be classified as inanimate, even if
it used to refer to a specific part of the eye.6
The ideal solution to this problem would be to
disambiguate the words, but this would require an
accurate disambiguation method. An alternative
solution is to weight the senses with respect to
the text. In this way, if a sense is more likely to
be used in a text, its animacy/inanimacy will have
greater influence on the classification process. At
present, we are trying to integrate the word sense
disambiguation method proposed in (Resnik,
1995) into our system. We hope that this will
particularly improve the classification of animate
entities.
5 Related work
Most of the work on animacy/gender recognition
has been done in the field of anaphora resolution.
The automatic recognition of NP gender on
the basis of statistical information has been
attempted before (Hale and Charniak, 1998).
That method operates by counting the frequency
with which a NP is identified as the antecedent of
a gender-marked pronoun by a simplistic pronoun
resolution system. It is reported that by using
the syntactic Hobbs algorithm (Hobbs, 1976)
for pronoun resolution, the method was able to
assign the correct gender to proper nouns in a
text with 68.15% precision, though the method
was not evaluated with respect to the recognition
of gender in common NPs. The method has
two main drawbacks. Firstly, it is likely to be
ineffective over small texts. Secondly, it seems
5However, it is possible to reclassify the nodes from
WordNet using an annotated corpus where the pets are
animate, but this would make the system consider all the
animals which can be pets animate.
6Actually the only way this word would be classified as
inanimate is if it is in the subject position, and most of the
senses of its main verb are inanimate. This is explained by
the way the senses are weighted by the machine learning
algorithm.
that the approach makes the assumption that
anaphora resolution is already effective, even
though, in general, anaphora resolution systems
rely on gender filtering.
In (Denber, 1998), WordNet was used to
determine the animacy of nouns and associate
them with gender-marked pronouns. The details
presented are sparse and no evaluation is given.
Cardie and Wagstaff (1999) combined the use of
WordNet with proper name gazetteers in order
to obtain information on the compatibility of
coreferential NPs in their clustering algorithm.
Again, no evaluation was presented with respect
to the accuracy of this animacy classification
task.
6 Conclusions and future work
In this paper, a two step method for animacy
recognition was proposed. In the first step, it
tries to determine the animacy of senses from
WordNet on the basis of an annotated corpus. In
the second step, this information is used by an
instance based learning algorithm to determine
the animacy of a noun. This area has been
relatively neglected by researchers, therefore a
comparison with other methods is difficult to
make. The accuracy obtained is around 97%,
more than 30% higher than that obtained by our
previous system.
Investigation of the results showed that in
order to obtain accuracy close to 100%, several
resources have to be used. As we point out in
Section 4.2, a method which is able to weight
the senses of a noun according to the text,
and a named entity recogniser are necessary.
The requirement for such components helps to
emphasise the problematic nature of NP animacy
recognition. We believe that such an investment
should be made in order to go forward with this
useful enterprise.
References
Claire Cardie and Kiri Wagstaff. 1999. Noun
phrase coreference as clustering. In Proceedings of
the 1999 Joint SIGDAT conference on Emphirical
Methods in NLP and Very Large Corpora (ACL?99),
pages 82 ? 89, University of Maryland, USA.
Walter Daelemans, Jakub Zavarel, Ko van der Sloot,
and Antal van den Bosch. 2000. Timbl: Tilburg
memory based learner, version 3.0, reference guide,
ilk technical report 00-01. ILK 00-01, Tilburg
University.
Michael Denber. 1998. Automatic resolution of
anaphora in english. Technical report, Eastman
Kodak Co, Imaging Science Division.
Richard Evans and Constantin Ora?san. 2000.
Improving anaphora resolution by identifying
animate entities in texts. In Proceedings of
the Discourse Anaphora and Reference Resolution
Conference (DAARC2000), pages 154 ? 162,
Lancaster, UK, 16 ? 18 November.
Christiane Fellbaum, editor. 1998. WordNet: An
Eletronic Lexical Database. The MIT Press.
John Hale and Eugene Charniak. 1998. Getting useful
gender statistics from english textx. Technical
Report CS-98-06, Brown University.
Jerry Hobbs. 1976. Pronoun resolution. Research
report 76-1, City College, City University of New
York.
Shari Landes, Claudia Leacock, and Randee I. Tengi.
1998. Building semantic concordances. In
Fellbaum (Fellbaum, 1998), pages 199 ? 216.
Tom M. Mitchell. 1997. Machine learning. McGraw-
Hill.
Michael P. Oakes. 1998. Statistics for Corpus
Linguistics. Edinburgh Textbooks in Empirical
Linguistics. Edinburgh University Press.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Philip Resnik. 1995. Disambiguating noun groupings
with respect to Wordnet senses. In David Yarovsky
and Kenneth Church, editors, Proceedings of the
Third Workshop on Very Large Corpora, pages
54?68, Somerset, New Jersey. Association for
Computational Linguistics.
R. Mark Sirkin. 1995. Statistics for the social
sciences. SAGE Publications.
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 131?140,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
An evaluation of syntactic simplification rules for people with autism
Richard Evans, Constantin Or
?
asan and Iustin Dornescu
Research Institute in Information and Language Processing
University of Wolverhampton
United Kingdom
{R.J.Evans, C.Orasan, I.Dornescu2}@wlv.ac.uk
Abstract
Syntactically complex sentences consti-
tute an obstacle for some people with
Autistic Spectrum Disorders. This pa-
per evaluates a set of simplification rules
specifically designed for tackling complex
and compound sentences. In total, 127 dif-
ferent rules were developed for the rewrit-
ing of complex sentences and 56 for the
rewriting of compound sentences. The
evaluation assessed the accuracy of these
rules individually and revealed that fully
automatic conversion of these sentences
into a more accessible form is not very re-
liable.
1 Introduction
People with Autistic Spectrum Disorders (ASD)
show a diverse range of reading abilities: on
the one hand, 5%-10% of users have the capac-
ity to read words from an early age without the
need for formal learning (hyperlexia), on the other
hand many users demonstrate weak comprehen-
sion of what has been read (Volkmar and Wiesner,
2009). They may have difficulty inferring contex-
tual information or may have trouble understand-
ing mental verbs or emotional language, as well
as long sentences with complex syntactic structure
(Tager-Flusberg, 1981; Kover et al., 2012). To ad-
dress these difficulties, the FIRST project
1
is de-
veloping a tool which makes texts more accessible
for people with ASD. In order to get a better un-
derstanding of the needs of these readers, a thor-
ough analysis was carried out to derive a list of
high priority obstacles to reading comprehension.
Some of these obstacles are related to syntactic
complexity and constitute the focus of this paper.
Even though the research in the FIRST project fo-
cuses on people with ASD, many of the obstacles
1
http://first-asd.eu
identified in the project can pose difficulties for a
wide range of readers such as language learners
and people with other language disorders.
This paper presents and evaluates a set of rules
used for simplifying English complex and com-
pound sentences. These rules were developed as
part of a syntactic simplification system which was
initially developed for users with ASD, but which
can also be used for other tasks that require syn-
tactic simplification of sentences. In our research,
we consider that syntactic complexity is usually
indicated by the occurrence of certain markers or
signs of syntactic complexity, referred to hereafter
as signs, such as punctuation ([,] and [;]), con-
junctions ([and], [but], and [or]), complementis-
ers ([that]) or wh-words ([what], [when], [where],
[which], [while], [who]). These signs may have
a range of syntactic linking and bounding func-
tions which need to be automatically identified,
and which we analysed in more detail in (Evans
and Orasan, 2013).
Our syntactic simplification process operates in
two steps. In the first, signs of syntactic complex-
ity are automatically classified and in the second,
manually crafted rules are applied to simplify the
relevant sentences. Section 3 presents more details
about the method. Evaluation of automatic simpli-
fication is a difficult issue. Given that the purpose
of this paper is to gain a better understanding of
the performance of the rules used for simplifying
compound sentences and complex sentences, Sec-
tion 4 presents the methodology developed for this
evaluation and discusses the results obtained. The
paper finishes with conclusions.
2 Background information
Despite some findings to the contrary (Arya et al.,
2011), automatic syntactic simplification has been
motivated by numerous neurolinguistic and psy-
cholinguistic studies. Brain imaging studies indi-
cate that processing syntactically complex struc-
131
tures requires more neurological activity than pro-
cessing simple structures (Just et al., 1996). A
study undertaken by Levy et al. (2012) showed
that people with aphasia are better able to un-
derstand syntactically simple reversible sentences
than syntactically complex ones.
Further motivation is brought by research in
NLP, which demonstrates that performance levels
in information extraction (Agarwal and Boggess,
1992; Rindflesch et al., 2000; Evans, 2011),
syntactic parsing (Tomita, 1985; McDonald and
Nivre, 2011), and, to some extent, machine trans-
lation (Gerber and Hovy, 1998) are somewhat de-
termined by the length and syntactic complexity of
the sentences being processed.
Numerous rule-based methods for syntactic
simplification have been developed (Siddharthan,
2006) and used to facilitate NLP tasks such as
biomedical information extraction (Agarwal and
Boggess, 1992; Rindflesch et al., 2000; Evans,
2011). In these approaches, rules are triggered
by pattern-matching applied to the output of text
analysis tool such as partial parsers and POS tag-
gers. Chandrasekar and Srinivas (1997) presented
an automatic method to learn syntactic simplifi-
cation rules for use in such systems. Unfortu-
nately, that approach is only capable of learning
a restricted range of rules and requires access to
expensive annotated resources.
With regard to applications improving text ac-
cessibility for human readers, Max (2000) de-
scribed the use of syntactic simplification for
aphasic readers. In work on the PSET project,
Canning (2002) implemented a system which ex-
ploits a syntactic parser in order to rewrite com-
pound sentences as sequences of simple sentences
and to convert passive sentences into active ones
for readers with aphasia. The success of these sys-
tems is tied to the performance levels of the syn-
tactic parsers that they employ.
More recently, the availability of resources such
as Simple Wikipedia has enabled text simplifi-
cation to be included in the paradigm of statis-
tical machine translation (Yatskar et al., 2010;
Coster and Kauchak, 2011). In this context,
translation models are learned by aligning sen-
tences in Wikipedia with their corresponding ver-
sions in Simple Wikipedia. Manifesting Basic En-
glish (Ogden, 1932), the extent to which Simple
Wikipedia is accessible to people with autism has
not yet been fully assessed.
The field of text summarisation includes numer-
ous approaches that can be regarded as examples
of syntactic simplification. For example, Cohn and
Lapata (2009) present a tree-to-tree transduction
method that is used to filter non-essential infor-
mation from syntactically parsed sentences. This
compression process often reduces the syntactic
complexity of those sentences. An advantage of
this approach is that it can identify elements for
deletion even when such elements are not indi-
cated by explicit signs of syntactic complexity.
The difficulty is that they rely on high levels of ac-
curacy and granularity of automatic syntactic anal-
ysis. As noted earlier, it has been observed that the
accuracy of parsers is inversely proportional to the
length and complexity of the sentences being anal-
ysed (Tomita, 1985; McDonald and Nivre, 2011).
The approach to syntactic simplification de-
scribed in the current paper is a two step pro-
cess involving detection and tagging of the bound-
ing and linking functions of various signs of syn-
tactic complexity followed by a rule-based sen-
tence rewriting step. Relevant to the first step, Van
Delden and Gomez (2002) developed a machine
learning method to determine the syntactic roles
of commas. Meier et al. (2012) describe German
language resources in which the linking functions
of commas and semicolons are annotated. The an-
notated resources exploited by the machine learn-
ing method presented in Section 3.2.1 of the cur-
rent paper are presented in (Evans and Orasan,
2013). From a linguistic perspective, Nunberg et
al. (2002) provide a grammatical analysis of punc-
tuation in English.
The work described in this paper was under-
taken in a project aiming to improve the accessibil-
ity of text for people with autism. It was motivated
at least in part by the work of O?Connor and Klein
(2004), which describes strategies to facilitate the
reading comprehension of people with ASD.
The proposed method is intended to reduce
complexity caused by both complex and com-
pound sentences and differs from those described
earlier in this section. Sentence compression
methods are not suitable for the types of rewrit-
ing required in simplifying compound sentences.
Parsers are more likely to have lower accuracy
when processing these sentences, and therefore the
proposed method does not use information about
the syntactic structure of sentences in the process.
Our method is presented in the next section.
132
3 The syntactic simplifier
In our research, we regard coordination and sub-
ordination as key elements of syntactic complex-
ity. A thorough study of the potential obstacles to
the reading comprehension of people with autism
highlighted particular types of syntactic complex-
ity, many of which are linked to coordination
and subordination. Section 3.1 briefly presents
the main obstacles linked to syntactic complexity
identified by the study. It should be mentioned that
most of the obstacles are problematic not only for
autistic people and other types of reader can also
benefit from their removal. The obstacles identi-
fied constituted the basis for developing the sim-
plification approach briefly described in Section
3.2.
3.1 User requirements
Consultations with 94 subjects meeting the strict
DSM-IV criteria for ASD and with IQ > 70 led to
the derivation of user preferences and high priority
user requirements related to structural processing.
A comprehensive explanation of the findings can
be found in (Martos et al., 2013). This section dis-
cusses briefly the two types of information of rel-
evance to the processing of sentence complexity
obtained in our study.
First, in terms of the demand for access to texts
of particular genres/domains, it was found that
young people (aged 12-16) seek access to doc-
uments in informative (arts/leisure) domains and
they have less interest in periodicals and newspa-
pers or imaginative texts. Adults (aged 16+) seek
access to informative and scientific texts (includ-
ing newspapers), imaginative text, and the lan-
guage of social networking and communication.
In an attempt to accommodate the interests of both
young people and adults, we developed a cor-
pus which contains newspaper articles, texts about
health, and literary texts.
Second, the specific morpho-syntactic phenom-
ena that pose obstacles to reading comprehension
that are relevant to this paper are:
1. Compound sentences, which should be split
into sentences containing a single clause.
2. Complex sentences: in which relative clauses
should either be:
(a) converted into adjectival pre-modifiers
or
(b) deleted from complex sentences and
used to generate copular constructions
linking the NP in the matrix clause with
the predication of the relative clause
In addition, the analysis revealed other types
of obstacles such as explicative clauses, which
should be deleted, and uncommon conjunctions
(including conjuncts) which should be replaced
by more common ones. Conditional clauses that
follow the main clause and non-initial adverbial
clauses should be pre-posed, and passive sen-
tences should be converted in the active form. Var-
ious formatting issues such as page breaks that oc-
cur within paragraphs and end-of-line hyphenation
are also problematic and should be avoided.
Section 3.2 describes the method developed to
address the obstacles caused by compound and
complex sentences.
3.2 The approach
Processing of obstacles to reading comprehension
in this research has focused on detection and re-
duction of syntactic complexity caused by the oc-
currence in text of compound sentences (1) and
complex sentences (2).
(1) Elaine Trego never bonded with 16-month-old
Jacob [and] he was often seen with bruises, a
murder trial was told.
(2) The two other patients, who are far more
fragile than me, would have been killed by
the move.
In (1), the underlined phrases are the conjoins
of a coordinate constituent. In (2), the underlined
phrase is a subordinate constituent of the larger,
superordinate phrase the two other patients, who
are far more fragile than me.
The overall syntactic simplification pipeline
consists of the following steps:
Step 1. Tagging of signs of syntactic complexity
with information about their syntactic linking
or bounding functions
Step 2. The complexity of sentences tagged in
step 1 is assessed and used to trigger the ap-
plication of two iterative simplification pro-
cesses, which are applied exhaustively and
sequentially to each input sentence:
133
a. Decomposition of compound sentences
(the simplification function converts one
input string into two output strings)
b. Decomposition of complex sentences
(the simplification function converts one
input string into two output strings)
Step 3. Personalised transformation of sentences
according to user preference profiles which
list obstacles to be tackled and the threshold
complexity levels that specify whether sim-
plification is necessary.
Steps 1 and 2 are applied iteratively ensuring
that an input sentence can be exhaustively simpli-
fied by decomposition of the input string into pairs
of progressively simpler sentences. No further
simplification is applied to a sentence when the
system is unable to detect any signs of syntactic
complexity within it. This paper reports on steps 1
and 2. The personalisation step, which takes into
consideration the needs of individual users, is not
discussed.
3.2.1 Identification of signs of complexity
Signs of syntactic complexity typically indicate
constituent boundaries, e.g. punctuation marks,
conjunctions, and complementisers. To facilitate
information extraction, a rule-based approach to
simplify coordinated conjoins was proposed by
Evans (2011), which relies on classifying signs
based on their linking functions.
In more recent work, an extended annotation
scheme was proposed in (Evans and Orasan, 2013)
which enables the encoding of links and bound-
aries between a wider range of syntactic con-
stituents and covers more syntactic phenomena.
A corpus covering three text categories (news ar-
ticles, literature, and patient healthcare informa-
tion leaflets), was annotated using this extended
scheme.
2
Most sign labels contain three types of infor-
mation: boundary type, syntactic projection level,
and grammatical category of the constituent(s).
Some labels cover signs which bound interjec-
tions, tag questions, and reported speech and a
class denoting false signs of syntactic complex-
ity, such as use of the word that as a specifier or
anaphor. The class labels are a combination of the
following acronyms:
2
http://clg.wlv.ac.uk/resources/
SignsOfSyntacticComplexity/
1. {C|SS|ES}, the generic function as a coor-
dinator (C), the left boundary of a subordi-
nate constituent (SS), or the right boundary
of a subordinate constituent (ES).
2. {P |L|I|M |E}, the syntactic projection level
of the constituent(s): prefix (P), lexical (L),
intermediate (I), maximal (M), or extended/-
clausal (E).
3. {A|Adv|N |P |Q|V }, the grammatical cate-
gory of the constituent(s): adjectival (A), ad-
verbial (Adv), nominal (N), prepositional (P),
quantificational (Q), and verbal (V).
4. {1|2}, used to further differentiate sub-
classes on the basis of some other label-
specific criterion.
The scheme uses a total of 42 labels to distin-
guish between different syntactic functions of the
bounded constituents. Although signs are marked
by a small set of tokens (words and punctuation),
the high number of labels and their skewed dis-
tribution make signs highly ambiguous. In addi-
tion, each sign is only assigned exactly one label,
i.e. that of the dominant constituent in the case of
nesting, further increasing ambiguity. These char-
acteristics make automatic classification of signs
challenging.
The automatic classification of signs of syntac-
tic complexity is achieved using a machine learn-
ing approach described in more detail in Dornescu
et al. (2013). After experimenting with several
methods of representing the training data and with
several classifiers, the best results were obtained
by using the BIO model to train a CRF tagger. The
features used were the signs? surrounding con-
text (a window of 10 tokens and their POS tags)
together with information about the distance to
other signs signs in the same sentence and their
types. The method achieved an overall accuracy
of 82.50% (using 10 fold cross-validation) on the
manually annotated corpus.
3.2.2 Rule-based approach to simplification
of compound sentences and complex
sentences
The simplification method exploits two iterative
processes that are applied in sequence to input
text that has been tokenised with respect to sen-
tences, words, punctuation, and signs of syntac-
tic complexity. The word tokens in the input text
134
Rule ID CEV-12
Sentence type Compound (coordination)
Match pattern A that [B] sign
CEV
[C] .
Transform pattern A that [B]. A that [C].
Ex: input [Investigations showed]
A
that [the glass came from a car?s side window]
B
and
CEV
[thousands of batches had been tampered with on five separate weekends]
C
.
Ex: output [Investigations showed]
A
that [the glass came from a car?s side window]
B
.
[Investigations showed]
A
that [thousands of batches had been tampered with on five
separate weekends]
C
.
Rule ID CEV-26
Sentence type Compound (coordination)
Match pattern A v
CC
B: ?[C] sign
CEV
[D]?.
Transform pattern A v B: ?[C]?. A v B: ?[D]?.
Ex: input [He]
A
added[]
B
: ?[If I were with Devon and Cornwall police I?d be very interested in
the result of this case]
C
and
CEV
[I certainly expect them to renew their interest]
D
.?
Ex: output [He]
A
added[]
B
: ?[If I were with Devon and Cornwall police I?d be very interested in
the result of this case]
C
.?
[He]
A
added[]
B
: ?[I certainly expect them to renew their interest]
D
.?
Table 1: Patterns used to identify conjoined clauses.
have also been labelled with their parts of speech
and the signs have been labelled with their gram-
matical linking and bounding functions. The pat-
terns rely mainly on nine sign labels which delimit
clauses (*EV)
3
, noun phrases (*MN) and adjecti-
val phrases (*MA). These sign labels can signal
either coordinated conjoins (C*) or the start (SS*)
or end (ES*) of a constituent.
The first iterative process exploits patterns in-
tended to identify the conjoins of compound sen-
tences. The elements common to these patterns
are signs tagged as linking clauses in coordination
(label CEV). The second process exploits patterns
intended to identify relative clauses in complex
sentences. The elements common to these patterns
are signs tagged as being left boundaries of subor-
dinate clauses (label SSEV).
The identification of conjoint clauses depends
on accurate tagging of words with information
about their parts of speech and signs with informa-
tion about their general roles in indicating the left
or right boundaries of subordinate constituents.
The identification of subordinate clauses requires
more detailed information. In addition to the in-
formation required to identify clause conjoins, in-
formation about the specific functions of signs is
required. The simplification process is thus highly
dependent on the performance of the automatic
sign tagger.
Table 1 displays two patterns for identifying
conjoined clauses and Table 2 displays two pat-
terns for identifying subordinate clauses. In the
3
In these example the * character is used to indicate any
sequence of characters, representing the bounding or linking
function of the sign.
tables, upper case letters denote contiguous se-
quences of text,
4
the underbar denotes signs of
class CEV (in row Compound) and SSEV (in row
Complex). Verbs with clause complements are
denoted by v
CC
, while words of part of speech
X are denoted by w
X
. The symbol s is used
to denote additional signs of syntactic complex-
ity while v denotes words with verbal POS tags.
Words explicitly appearing in the input text are
italicised. Elements of the patterns representing
clause conjoins and subordinate clauses appear in
square brackets.
Each pattern is associated with a sentence
rewriting rule. A rule is applied on each itera-
tion of the algorithm. Sentences containing signs
which correspond to conjoint clauses are con-
verted into two strings which are identical to the
original save that, in one, the conjoint clause is
replaced by a single conjoin identified in the con-
joint while in the other, the identified conjoin is
omitted. Sentences containing signs which indi-
cate subordinate clauses are converted into two
new strings. One is identical to the original save
that the relative clause is deleted. The second is
automatically generated, and consists of the NP in
the matrix clause modified by the relative clause, a
conjugated copula, and the predication of the rela-
tive clause. Tables 1 and 2 give examples of trans-
formation rules for the given patterns. In total,
127 different rules were developed for the rewrit-
ing of complex sentences and 56 for the rewriting
of compound sentences.
4
Note that these sequences of text may contain additional
signs tagged CEV or SSEV.
135
Rule ID SSEV-61
Sentence type Complex (subordination)
Match pattern A s B [sign
SSEV
C v D].
Transform pattern A s B. That C v D.
Ex: input [During the two-week trial, the jury heard how Thomas became a frequent visitor to
Roberts?s shop in the summer of 1997]
A
, [after meeting him through a friend]
B
[who
[lived near the shop,]
C
[described as a ?child magnet? by one officer]
D
.
Ex: output [During the two-week trial, the jury heard how Thomas became a frequent visitor to
Roberts?s shop in the summer of 1997]
A
, [after meeting him through a friend]
B
.
That friend [lived near the shop,]
C
[described as a ?child magnet? by one officer]
D
.
Rule ID SSEV-72
Sentence type Complex (subordination)
Match pattern [A w
IN
w
DT
* n {n|of}* sign
SSEV
] w
V BD
B {.|?|!}
Transform pattern N/A
Pattern SSEV-72 is used to prevent rewriting of complex sentences when the subordinate
clause is the argument of a clause complement verb. The result of this rule is to strip the
tag from the triggering sign of syntactic complexity
Ex: input [Eamon Reidy, 32,]
A
fled [across fields in Windsor Great Park after the crash[, the court
heard.]
Table 2: Patterns used to identify subordinate clauses.
4 Evaluation
The detection and classification of signs of syntac-
tic complexity can be evaluated via standard meth-
ods in LT based on comparing classifications made
by the system with classifications made by linguis-
tic experts. This evaluation is reported in (Dor-
nescu et al., 2013). Unfortunately, the evaluation
of the actual simplification process is difficult, as
there are no well established methods for measur-
ing its accuracy. Potential methodologies for eval-
uation include comparison of system output with
human simplification of a given text, analysis of
the post-editing effort required to convert an au-
tomatically simplified text into a suitable form for
end users, comparisons using experimental meth-
ods such as eye tracking and extrinsic evaluation
via NLP applications such as information extrac-
tion, all of which have weaknesses in terms of ad-
equacy and expense.
Due to the challenges posed by these previously
established methods, we decided that before we
employ them and evaluate the output of the sys-
tem as a whole, we focus first on the evaluation
of the accuracy of the two rule sets employed by
the syntactic processor. The evaluation method is
based on comparing sets of simplified sentences
derived from an original sentence by linguistic ex-
perts with sets derived by the method described in
Section 3.
4.1 The gold standard
Two gold standards were developed to support
evaluation of the two rule sets. Texts from the gen-
res of health, literature, and news were processed
by different versions of the syntactic simplifier. In
one case, the only rules activated in the syntac-
tic simplifier were those concerned with rewriting
compound sentences. In the second case, the only
rules activated were those concerned with rewrit-
ing complex sentences. The output of the two ver-
sions was corrected by a linguistic expert to ensure
that each generated sentence was grammatically
well-formed and consistent in meaning with the
original sentence. Sentences for which even man-
ual rewriting led to the generation of grammati-
cally well-formed sentences that were not consis-
tent in meaning with the originals were removed
from the test data. After filtering, the test data
contained nearly 1,500 sentences for use in eval-
uating rules to simplify of compound sentences,
and nearly 1,100 sentences in the set used in eval-
uating rules to simplify complex sentences. The
break down per genre/domain is given in Tables
3a and 3b.
The subset of sentences included in the gold
standard contained manually annotated informa-
tion about the signs of syntactic complexity. This
was done to enable reporting of the evaluation re-
sults in two modes: one in which the system con-
sults an oracle for classification of signs of syntac-
tic complexity and one in which the system con-
sults the output of the automatic sign tagger.
4.2 Evaluation results
Evaluation results are reported in terms of accu-
racy of the simplification process and the change
in readability of the generated sentences. Com-
putation of accuracy is based on the mean Leven-
136
Text category
News Health Literature
#Compound sentences 698 325 418
Accuracy Oracle 0.758 0.612 0.246
Classifier 0.314 0.443 0.115
?Flesch Oracle 11.1 8.2 15.3
Classifier 9.9 10.2 13.6
?Avg. Oracle -12.58 -9.86 -16.69
Sent. Len. Classifier -13.08 -12.30 -16.79
(a) Evaluation of simplification of compound sentences
Text category
News Health Literature
#Complex sentences 369 335 379
Accuracy Oracle 0.452 0.292 0.475
Classifier 0.433 0.227 0.259
?Flesch Oracle 2.5 0.8 2.3
Classifier 2.3 0.9 2.3
?Avg. Oracle -2.96 -0.90 -2.80
Sent. Len. Classifier -2.80 -0.99 -2.11
(b) Evaluation of simplification of complex sentences
Table 3: Evaluation results for the two syntactic phenomena on three text genres
shtein similarity
5
between the sentences generated
by the system and the most similar simplified sen-
tences verified by the linguistic expert. Once the
most similar sentence in the key has been found,
that element is no longer considered for the rest of
the simplified sentences in the system?s response
to the original. In this evaluation, sentences are
considered to be converted correctly if their LS >
0.95. The reason for setting such a high threshold
for the Levenshtein ratio is because the evaluation
method should only reward system responses that
match the gold standard almost perfectly save for a
few characters which could be caused by typos or
variations in the use of punctuation and spaces. A
sentence is considered successfully simplified, and
implicitly all the rules used in the process are con-
sidered correctly applied, when all the sentences
produced by the system are converted correctly ac-
cording to the gold standard. This evaluation ap-
proach may be considered too inflexible as it does
not take into consideration the fact that a sentence
can be simplified in several ways. However, the
purpose here is to evaluate the way in which sen-
tences are simplified using specific rules.
In order to calculate the readability of the gen-
erated sentences we initially used the Flesch score
(Flesch, 1949). However, our system changes the
text only by rewriting sentences into sequences of
simpler sentences and does not make any changes
at the lexical level. For this reason, any changes
observed in the Flesch score are due to changes
in the average sentence length. Therefore, for our
experiments we report both ?Flesch score and
?average sentence length.
The evaluation results are reported separately
for the three domains. In addition, the results are
calculated when the classes of the signs are de-
5
Defined as 1 minus the ratio of Levenshtein distance be-
tween the two sentences to the length in characters of the
longest of the two sentences being compared.
rived from the manually annotated data (Oracle)
and from use of the automatic classifier (Classi-
fier).
Table 3a presents the accuracy of the rules im-
plemented to convert compound sentences into a
more accessible form. The row #Compound sen-
tences displays the number of sentences in the test
data that contain signs of conjoint clauses (signs
of class CEV). The results obtained are not unex-
pected. In all cases the accuracy of the simplifi-
cation rules is higher when the labels of signs are
assigned by the oracle. With the exception of the
health domain, the same pattern is observed when
?Flesch is considered. The highest accuracy is
obtained on the news texts, then the health do-
main, and finally the literature domain. However,
despite significantly lower accuracy on the litera-
ture domain, the readability of the sentences from
the literature domain benefits most from the auto-
matic simplification. This can be noticed both in
the improved Flesch scores and reduced sentence
length.
Table 3b presents the accuracy of the rules
which simplify complex sentences. In this table,
#Complex sentences denotes the number of sen-
tences in the test data that contain relative clauses.
The rest of the measures are calculated in the same
way as in Table 3a. Inspection of the table shows
that, for the news and health domains, the accu-
racy of these simplification rules is significantly
lower than the simplification rules used for com-
pound sentences. Surprisingly, the rules work bet-
ter for the literature domain than for the others.
The improvement in the readability of texts from
the health domain is negligible, which can be ex-
plained by the poor performance of the simplifica-
tion rules on this domain.
137
4.3 Error analysis
In order to have a better understanding of the per-
formance of the system, the performance of the
individual rules was also recorded. Tables 4 and 5
contain the most error prone trigger patterns for
conjoined and subordinate clauses respectively.
The statistics were derived from rules applied to
texts of all three categories of texts and the signs
of syntactic complexity were classified using an
oracle, in order to isolate the influence of the rules
in the system output. In this context, the accu-
racy with which the syntactic processor converts
sentences containing conjoint clauses into a more
accessible form is 0.577. The accuracy of this task
with regard to subordinate clauses is 0.411.
The most error-prone trigger patterns for con-
joined clauses are listed in Table 4, together with
information on the conjoin that they are intended
to detect (left or right), their error rate, and the
number of number of errors made. The same in-
formation is presented for the rules converting sen-
tences containing subordinate clauses in Table 5,
but in this case the patterns capture the subordina-
tion relations. In the patterns, words with partic-
ular parts of speech are denoted by the symbol w
with the relevant Penn Treebank tag appended as a
subscript. Verbs with clause complements are de-
noted v
CC
. Signs of syntactic complexity are de-
noted by the symbol s with the abbreviation of the
functional class appended as a subscript. Specific
words are printed in italics. In the patterns, the
clause coordinator is denoted ? ? and upper case
letters are used to denote stretches of contiguous
text.
Rules CEV-25a and SSEV-78a are applied when
the input sentence triggers none of the other imple-
mented patterns. Errors of this type quantify the
number of sentences containing conjoint or subor-
dinate clauses that cannot be converted into a more
accessible form by rules included in the structural
complexity processor. Both rules have quite high
error rates, but these errors can only be addressed
via the addition of new rules or the adjustment of
already implemented rules.
SSEV-36a is a pattern used to prevent process-
ing of sentences that contain verbs with clause
complements. This pattern was introduced be-
cause using the sentence rewriting algorithm pro-
posed here to process sentences containing these
subordinate clauses would generate ungrammati-
cal output.
Table 5 contains only 4 items because for the
rest of the patterns the number of errors was less
than 3. A large number of these rules had an error
rate of 1 which motivated their deactivation. Un-
fortunately this did not lead to improved accuracy
of the overall conversion process.
5 Conclusions and future work
Error analysis revealed that fully automatic con-
version compound and complex sentences into a
more accessible form is quite unreliable, partic-
ularly for texts of the literature category. It was
noted that conversion of complex sentences into a
more accessible form is more difficult than con-
version of compound sentences. However, sub-
ordinate clauses are significantly more prevalent
than conjoint clauses in the training and testing
data collected so far.
The evaluation of the rule sets used in the con-
version of compound and complex sentences into
a more accessible form motivates further specific
development of the rule sets. This process in-
cludes deletion of rules that do not meet particu-
lar thresholds for accuracy and the development of
new rules to address cases where input sentences
fail to trigger any conversion rules (signalled by
activation of redundant rules CEV-25a and SSEV-
78a).
The results are disappointing given that the
syntactic simplification module presented in this
paper is expected to be integrated in a system
that makes texts more accessible for people with
autism. However, this simplification module will
be included in a post-editing environment for peo-
ple with ASD. In this setting, it may still prove
useful, despite its low accuracy.
Acknowledgments
The research described in this paper was par-
tially funded by the European Commission un-
der the Seventh (FP7-2007-2013) Framework Pro-
gramme for Research and Technological Devel-
opment (FP7- ICT-2011.5.5 FIRST 287607). We
gratefully acknowledge the contributions of all the
members of the FIRST consortium for their feed-
back and comments during the development of the
methods, and to Laura Hasler for her help with the
evaluation.
138
ID Conjoin Trigger pattern Error rate #Errors
CEV-24b B A B 0.131 59
CEV-24a A A B 0.119 54
CEV-12b A that C A that B C 0.595 25
CEV-25a NA NA 0.956 22
CEV-26a A v
CCV
B : ?C? A v
CC
B : ?C D? 0.213 16
CEV-26b A v
CCV
B : ?D? A v
CC
B : ?C D? 0.203 14
Table 4: Error rates for rules converting sentences with conjoint clauses
ID Matrix clause / subordinate clause Trigger pattern Error rate #Errors
SSEV-78a NA NA 0.517 45
SSEV-72a A , C w
{verb}
D A s B C w
{verb}
D 0.333 4
SSEV-36a NA A told w
{noun|PRP|DT|IN}
* B 0.117 4
SSEV-13b w
VBN
w
IN
(w
{DT|PRP$|noun|CD}
|-|,)* w
{noun}
B
A w
VBN
w
IN
{w
{DT|PRP$|noun|CD}
|-|,}* w
{noun}
B
1 3
Table 5: Error rates for rules converting sentences with subordinate clauses
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but
useful approach to conjunct identification. In Pro-
ceedings of the 30th annual meeting for Computa-
tional Linguistics, pages 15?21, Newark, Delaware.
Association for Computational Linguistics.
D. J. Arya, Elfrieda H. Hiebert, and P. D. Pearson.
2011. The effects of syntactic and lexical com-
plexity on the comprehension of elementary science
texts. International Electronic Journal of Elemen-
tary Education, 4 (1):107?125.
Y. Canning. 2002. Syntactic Simplification of Text.
Ph.d. thesis, University of Sunderland.
R Chandrasekar and B Srinivas. 1997. Automatic in-
duction of rules for text simplification. Knowledge-
Based Systems, 10:183?190.
T. Cohn and M. Lapata. 2009. Sentence Compression
as Tree Transduction. Journal of Artificial Intelli-
gence Research, 20(34):637?74.
W. Coster and D. Kauchak. 2011. Simple english
wikipedia: A new text simplification task. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2011),
pages 665?669, Portland, Oregon, June. Association
of Computational Linguistics.
Iustin Dornescu, Richard Evans, and Constantin
Or?asan. 2013. A Tagging Approach to Identify
Complex Constituents for Text Simplification. In
Proceedings of Recent Advances in Natural Lan-
guage Processing, pages 221 ? 229, Hissar, Bul-
garia.
Richard Evans and Constantin Orasan. 2013. Annotat-
ing signs of syntactic complexity to support sentence
simplification. In I. Habernal and V. Matousek, edi-
tors, Text, Speech and Dialogue. Proceedings of the
16th International Conference TSD 2013, pages 92?
104. Springer, Plzen, Czech Republic.
R. Evans. 2011. Comparing methods for the syn-
tactic simplification of sentences in information ex-
traction. Literary and Linguistic Computing, 26
(4):371?388.
R. Flesch. 1949. The art of readable writing. Harper,
New York.
Laurie Gerber and Eduard H. Hovy. 1998. Improving
translation quality by manipulating sentence length.
In David Farwell, Laurie Gerber, and Eduard H.
Hovy, editors, AMTA, volume 1529 of Lecture Notes
in Computer Science, pages 448?460. Springer.
M. A. Just, P. A. Carpenter, and K. R. Thulborn. 1996.
Brain activation modulated by sentence comprehen-
sion. Science, 274:114?116.
S. T. Kover, E. Haebig, A. Oakes, A. McDuffie, R. J.
Hagerman, and L. Abbeduto. 2012. Syntactic com-
prehension in boys with autism spectrum disorders:
Evidence from specific constructions. In Proceed-
ings of the 2012 International Meeting for Autism
Research, Athens, Greece. International Society for
Autism Research.
J. Levy, E. Hoover, G. Waters, S. Kiran, D. Caplan,
A. Berardino, and C. Sandberg. 2012. Effects of
syntactic complexity, semantic reversibility, and ex-
plicitness on discourse comprehension in persons
with aphasia and in healthy controls. American
Journal of Speech?Language Pathology, 21(2):154
? 165.
Wolfgang Maier, Sandra K?ubler, Erhard Hinrichs, and
Julia Kriwanek. 2012. Annotating coordination in
the penn treebank. In Proceedings of the Sixth Lin-
guistic Annotation Workshop, pages 166?174, Jeju,
Republic of Korea, July. Association for Computa-
tional Linguistics.
Juan Martos, Sandra Freire, Ana Gonzlez, David Gil,
Richard Evans, Vesna Jordanova, Arlinda Cerga,
Antoneta Shishkova, and Constantin Orasan. 2013.
User preferences: Updated report. Technical report,
139
The FIRST Consortium, Available at http://first-
asd.eu/D2.2.
A. Max. 2000. Syntactic simplification - an applica-
tion to text for aphasic readers. Mphil in computer
speech and language processing, University of Cam-
bridge, Wolfson College.
Ryan T. McDonald and Joakim Nivre. 2011. Analyz-
ing and integrating dependency parsers. Computa-
tional Linguistics, 37(1):197?230.
Geoffrey Nunberg, Ted Briscoe, and Rodney Huddle-
ston. 2002. Punctuation. chapter 20 In Huddleston,
Rodney and Geoffrey K. Pullum (eds) The Cam-
bridge Grammar of the English Language, pages
1724?1764. Cambridge University Press.
I. M. O?Connor and P. D. Klein. 2004. Exploration
of strategies for facilitating the reading comprehen-
sion of high-functioning students with autism spec-
trum disorders. Journal of Autism and Developmen-
tal Disorders, 34:2:115?127.
C. K. Ogden. 1932. Basic English: a general intro-
duction with rules and grammar. K. Paul, Trench,
Trubner & Co., Ltd., London.
Thomas C. Rindflesch, Jayant V. Rajan, and Lawrence
Hunter. 2000. Extracting molecular binding rela-
tionships from biomedical text. In Proceedings of
the sixth conference on Applied natural language
processing, pages 188?195, Seattle, Washington.
Association of Computational Linguistics.
A. Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language and Compu-
tation, 4:1:77?109.
Helen Tager-Flusberg. 1981. Sentence comprehen-
sion in autistic children. Applied Psycholinguistics,
2:1:5?24.
Masaru Tomita. 1985. Efficient Parsing for Natural
Language: A Fast Algorithm for Practical Systems.
Kluwer Academic Publishers, Norwell, MA, USA.
Sebastian van Delden and Fernando Gomez. 2002.
Combining finite state automata and a greedy learn-
ing algorithm to determine the syntactic roles of
commas. In Proceedings of the 14th IEEE Inter-
national Conference on Tools with Artificial Intel-
ligence, ICTAI ?02, pages 293?, Washington, DC,
USA. IEEE Computer Society.
F.R. Volkmar and L. Wiesner. 2009. A Practical Guide
to Autism. Wiley, Hoboken, NJ.
M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, and
L. Lee. 2010. For the sake of simplicity: Unsu-
pervised extraction of lexical simplifications from
wikipedia. In Proceedings of Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the ACL, pages 365?
368, Los Angeles, California, June. Association of
Computational Linguistics.
140
Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual Society, pages 1?10,
Dublin, Ireland, August 24th 2014.
Relative clause extraction for syntactic simplification
Iustin Dornescu, Richard Evans, Constantin Or
?
asan
Research Group in Computational Linguistics
University of Wolverhampton
United Kingdom
{i.dornescu2,r.j.evans,c.orasan}@wlv.ac.uk
Abstract
This paper investigates non-destructive simplification, a type of syntactic text simplification
which focuses on extracting embedded clauses from structurally complex sentences and rephras-
ing them without affecting their original meaning. This process reduces the average sentence
length and complexity to make text simpler. Although relevant for human readers with low read-
ing skills or language disabilities, the process has direct applications in NLP. In this paper we
analyse the extraction of relative clauses through a tagging approach. A dataset covering three
genres was manually annotated and used to develop and compare several approaches for auto-
matically detecting appositions and non-restrictive relative clauses. The best results are obtained
by a ML model developed using crfsuite, followed by a rule based method.
1 Introduction
Text simplification (TS) is the process of reducing the complexity of a text while preserving its meaning
(Chandrasekar et al., 1996; Siddharthan, 2002a; Siddharthan, 2006). There are two main types of sim-
plification: syntactic and lexical. The focus of syntactic simplification is to take long and structurally
complicated sentences and rewrite them as sequences of sentences which are shorter and structurally
simpler. Lexical simplification focuses on replacing words which could make reading texts difficult with
more common terms and expressions. The focus of this paper is on syntactic simplification and more
specifically on how to identify noun post-modifying clauses from complex sentences.
The occurrence of embedded clauses due to subordination and coordination increases the structural
complexity of sentences, especially in long sentences where such phenomena are more prevalent. Sim-
ple sentences are usually much easier to understand by humans and can be more reliably processed by
Natural Language Processing (NLP) tools. Psycholinguistic and neurolinguistic imaging studies show
that syntactically complex sentences require more effort to process than syntactically simple ones (Just
et al., 1996; Levy et al., 2012). For this reason, complex sentences can cause problems to people with
language disabilities. At the same time, previous work indicates that syntactic simplification can im-
prove the reliability of NLP applications such as information extraction (Agarwal and Boggess, 1992;
Rindflesch et al., 2000; Evans, 2011), and machine translation (Gerber and Hovy, 1998). In the field
of syntactic parsing, studies show that parsing accuracy is lower for longer sentences (Tomita, 1985;
McDonald and Nivre, 2011). Therefore, the impact of this paper can be two-fold: on the one hand, it
can help increase the accuracy of automatic language processing, and on the other hand, it can be used
to make text more accessible to people with reading difficulties.
The research presented in this paper was carried out in the context of FIRST
1
, an EU funded project
which develops tools to make texts more accessible to people with Autism Spectrum Disorders (ASD). In
order to have a proper understanding of the obstacles which pose difficulties to people with ASD, a survey
of the literature on reading comprehension and questionnaires completed by people with ASD were
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://first-asd.eu
1
Figure 1: The online annotation using brat
conducted (Martos et al., 2013). The research confirmed that among other types of syntactic complexity,
subordinated clauses should be processed by a syntactic simplifier to make the text easier to read.
In this paper, we tackle non-destructive simplification, a form of syntactic simplification in which
a clause-based approach is employed to rephrase text in such a way that the meaning of the original
text is preserved as much as possible. This is specifically linked to certain types of syntactic structures
which can be extracted from the matrix clause without affecting meaning. subordinates, with These
types include appositions and non-restrictive relative clauses (Siddharthan, 2002b). This paper presents
a method specifically developed for identifying appositions and non-restrictive relative clauses which
can be removed from a text without losing essential information.
This paper is structured as follows: Section 2 presents the dataset used to carry out the experiments
presented in this paper, including the annotation guidelines and inter-annotator agreement. The machine
learning method developed to detect relative clauses is presented in Section 3 and the evaluation results
in Section 4. In Section 5, conclusions are drawn.
2 Dataset
To carry out the research presented in this paper, a corpus was annotated. This section presents the
annotation guidelines used in the process and discusses issues encountered during the annotation. The
annotation was performed using the BRAT
2
tool (Stenetorp et al., 2012). The guidelines were given
to the annotators and were explained in a group discussion were several examples were also analysed.
Subsequently annotators were given a small set of sentences to trial individually and their questions and
feedback led to a revised set of guidelines. Once this training phase was complete, the actual annotation
was carried out. The corpus was split randomly each part being annotated by at least two annotators.
2.1 Text genres
The corpus consists of sentences extracted from texts collected in the FIRST project and covers three
genres: newswire, healthcare and literature, with some additional sentences from the Penn Treebank
(Marcus et al., 1993). The corpus was developed to assist TS for people with ASD (Evans and Or?asan,
2013), following the notion that structurally complex constituents are explicitly indicated by signs of
complexity such as conjunctions, complementisers and punctuation marks. Evans and Or?asan (2013)
developed an annotation scheme and manually labeled these signs.
2
brat rapid annotation tool http://brat.nlplab.org/about.html
2
Figure 1 shows the interface of the annotation tool. For each annotated span, annotators were asked
to fill in three attributes: a) the type (relative, nominal, adjectival, verbal, prepositional), b) whether it is
restrictive (no, yes, unknown) and c) the annotator?s confidence (low, medium, high). The amount of data
in the corpus is listed for each genre in Table 1, i.e. number of sentences and tokens. On average, around
half of sentences contain an annotated span, but they occur more frequently in newswire and healthcare
than in literature.
Table 1: Corpora used and the total number of annotated spans
Genre (Corpus) Sentences Tokens Spans Span tokens Sent. len. Span len.
healthcare 1214 27379 958 6094 22.55 6.36
news (METER1) 1038 28367 732 5592 27.33 7.64
news (METER2) 1377 37515 1165 9203 27.24 7.90
literature 1946 48620 431 3834 24.98 8.90
News (Penn T.B.) 1733 39740 625 5652 22.93 9.04
Overall 7308 181621 3911 30375 24.85 7.77
2.2 Annotation guidelines
The annotation task involved tagging contiguous sequences of words that comprise post-modifiers of
nouns. These are syntactic constituents which follow the head noun in a complex noun phrase (NP), pro-
viding additional information about it. We are interested in those post-modifiers which provide additional
information but are not part of the parent clause and can be extracted from the sentence without chang-
ing its core meaning. These constituents can be either phrases or clauses and are typically bounded by
punctuation marks (such as commas, dashes, parentheses) or by functional words (prepositions, relative
pronouns, etc.).
The noun post-modifiers of interest are typically clauses or phrases rather than individual words, so
not every noun modifier should be marked. Typically they follow the noun phrase whose head they are
providing details about and cover several type of subordinated structures (appositions, relative clauses,
etc.) In the annotation, the most important aspect is to detect correctly the extent of the annotation (e.g.
include surrounding commas). The type is marked as an attribute and evaluated separately. Another
attribute indicates whether or not the modifier is a restrictive relative clause.
2.2.1 Restrictive and non-restrictive relative clauses
Restrictive modifiers serve to restrict or limit the set of possible referents of a phrase. In (1a), the subject
is restricted to one particular set of chips in a discourse model that may contain many different sets of
chips. In (1b), no such restriction is in effect. In this discourse model, all of the chips are made of gallium
arsenide and are fragile. Sentences containing restrictive noun post-modifiers require a different method
for conversion into a more accessible form than sentences containing non-restrictive noun post-modifiers.
(1) a. The chips made of gallium arsenide are very fragile (restrictive)
b. The chips, which are made of gallium arsenide, are very fragile (non-restrictive)
Deletion of the noun post-modifier from (1a) produces a sentence that is inconsistent in meaning with
the original. All chips, not just the set made of gallium arsenide, are then described as very fragile.
When converting sentences with restrictive post-modifiers, it is necessary to generate two sentences: one
to put the set of chips made of gallium arsenide into focus and to distinguish this set from the other sets
that exist in the discourse model and the other to assert the fact that this set of chips is very fragile. By
contrast, deletion of the post-modifier in (1b) produces a sentence that is still consistent in meaning with
the original.
In a particular context, it can be quite clear to understand if a specific entity is meant or whether a
restricted category of entities is referred to. Normally the sentence is read with two different intonations
3
to indicate the two different meanings (which is why commas usually mark non-restrictive clauses). The
presence, or absence of commas should be used to differentiate ambiguous cases in which not enough
context is available to decide which is the intended meaning.
(2) a. They visited two companies today: one in Manchester and one in Liverpool. The company
[which is located] in Manchester was remarkable. (restrictive)
b. They visited a company and a school. The company, [which is located] in Manchester, was
remarkable. (non-restrictive)
Restrictive relative clauses are also called integrated, defining or identifying relative clauses. Similarly,
non-restrictive relative clauses are called supplementary, appositive, non-defining or non-identifying rel-
ative clauses.
2.2.2 Types of noun post-modifier
Depending on their syntactic function, there are five types of noun post-modifier:
1. Relative clauses (usually marked by relative pronouns who(m), which, that). These finite clauses
are constituents of subclausal elements (noun phrases) within a superordinate clause. They differ from
other types of clause such as adverbial clauses, because they are not direct elements of the superordinate
clause.
3
They have only an indirect link to the main clause.
(3) A Bristol hospital that retained the hearts of 300 children who died in complex operations behaved
in a ?cavalier? fashion towards the parents, an inquiry was told yesterday.
(4) The florist [who was] sent the flowers was pleased.
2. Nominal-appositives (which are themselves NPs). Apposition is a relation holding between two
NPs (the appositives) in which one serves to define the other. The second NP commonly has a defining
role with regard to the first.
(5) Catherine Hawkins, regional general manager for the National Health Service in the South-west
until 1992, appeared before the inquiry yesterday without a solicitor - one should have been pro-
vided by the department.
(6) My wife, a nurse by training, has helped the accident victim.
(7) Goldwater, the junior senator from Arizona, received the Republican nomination in 1964.
3. Non-finite clauses (VP, typically start with an -ing participle or -ed participle verb). These clauses
have a non-finite verb and are non-restrictive.
4
These post-modifiers can be regarded as examples of
post-modification by a reduced relative clause. To illustrate, in example (8), the non-finite clause is a
reduction of the relative clause who was sitting across from the defendant.
(8) Assistant Chief Constable Robin Searle, sitting across from the defendant, said that the police had
suspected his involvement since 1997.
(9) Lord Melchett led a dawn raid on a farm in Norfolk, causing 17,400 of damage to a genetically
modified crop and disrupting a research programme, a court was told yesterday.
4. Prepositional phrase post-modification (PP, typically starting with a preposition). Similar to
non-finite clauses, post-modification by PPs, can be regarded as post-modification by ?reduced? relative
clauses. For example, the PP in example (10) can be considered a reduction of the relative clause who is
of Chelmsford, Essex.
3
In syntax, elements are the fundamental units of a clause: subject, verb, object, complement, or adverbial.
4
They occur in a different tone unit, typically bounded by commas, from the noun head that they modify. They do not
restrict or limit the set of possible referents of the complex noun phrase that they modify.
4
(10) Boe, of Chelmsford, Essex, admitted six fraud charges and asked for 35 similar offences to be taken
into consideration.
5. Adjectival post-modification (AP, including attributes such as height or age). Similar to non-
finite clauses, post-modification by adjectival phrases, can be regarded as post-modification by ?reduced?
relative clauses. For example, the adjectival phrase in example (11) below can be considered a reduction
of the relative clause who is 58 [years old].
(11) Stanley Cameron, 58 [years old], was convicted in August of 16 counts including vessel homicide
and driving under the influence in the November 1997 crash in Fort Lauderdale, Florida.
(12) Student Richard, 5ft 1Oins tall, has now left home.
2.3 Annotation insights
The corpus was split into chunks of roughly 100-150 sentences and each was annotated by 2 to 5 an-
notators. Sentences were randomly selected from the corpus based on length and the presence of signs
of complexity (conjunctions, commas, parentheses). Where annotated spans did not match, a reviewer
made a final adjudication.
The agreement on detecting the span of post-modifiers was relatively low, on average, the pairwise
F1 score was 54.90%. This is mainly because of the way annotators interpreted the instructions. For
example, some annotators systematically marked all parenthetical expressions whereas others never did
this. The most frequent error was omission of relevant post-modifiers; annotators typically reached
higher precision than recall. This suggests a systematic disagreement which affects files annotated by
few annotators. A way to address this problem is by aggregating all annotated spans for each document
and asking annotators to confirm which of them are indeed post-modifiers. This is being carried out in a
separate study, where a voting scheme is used to mitigate the recall problem.
Looking only at the cases where two annotators marked the same span as a post-modifier, we can
investigate the level of agreement reached on the individual attributes: type and restrictiveness. Anno-
tators reached high pairwise agreement (kappa=0.78) when marking the type of the post-modifier. This
suggests that the beginning of the post-modifiers has reliable markers which could be used to automat-
ically predict type. The pairwise agreement for restrictiveness is lower (kappa=0.51), but still good,
considering that the two values are not equally distributed (70% of post-modifiers are non restrictive).
Possible causes of this are: lack of context (sentences were extracted from their source documents), lack
of domain knowledge (where the post-modifers are not about entities, but specialised terminology, such
as symptoms, procedures, strategies).
Although agreement on the two attributes can be improved, the biggest challenge is to ensure that all
post-modifiers are annotated, i.e. to address situations when only one annotator marks a span. One com-
mon cause of disagreement concerns noun modifiers within the same NP, such as prepositional phrases.
For example:
(13) The $2.5 billion Byron 1 plant near Rockford was completed in 1985.
While this span modifies a noun, it is part of the NP itself, and it is arguably too short to be relevant
for rephrasing the sentence in a TS system; it is more likely a candidate for deletion as is the case with
sentence compression systems.
Another frequent issue concerns nested modifiers, where annotators usually marked only one of the
possible constituents. A related issue is how to deal with nested and overlapping spans, not only from
the point of view of the guidelines, but also in the way the annotation is used in practice.
(14) The new plant, located in Chinchon, about 60 miles from Seoul, will help meet increased demand.
An interesting debate concerns ambiguous constituents which can have several interpretations. In the
previous sentence, the second constituent about 60 miles from Seoul can be considered an apposition
modifying the proper noun Chinchon, or a prepositional phrase modifying the verb located; both entail a
5
similar meaning to a human reader. This example illustrates a situation which frequently occurs in natural
language text: for stylistic or editorial reasons writers omit words which are implied by the context. The
effect is that the syntactic structure becomes ambiguous, but the information communicated to the reader
is nevertheless unaffected. This issue also suggests that distinguishing the type of a post-modifier (i.e.
relative, nominal, adjectival, verbal, prepositional) only reflects its form and less so its role. For example,
it is easy to rephrase most post-modifiers as relative clauses, e.g.:
(15) a. Nominal-appositives: My wife, who is a nurse by training, has helped the accident victim.
b. Verbal-appositives: Lord Melchett led a dawn raid on a farm in Norfolk, which caused causing
17,400 of damage... , a court was told yesterday.
c. Prepositional-appositives: Boe, who lives in of Chelmsford, Essex, admitted six fraud charges
and asked for 35 similar offences to be taken into consideration.
d. Adjectival-appositives: Student Richard, who is 5ft 1Oins tall, has now left home.
During the annotation process there were several issues raised by the annotators, both seeking clarifi-
cations of the guidelines as well as identifying new situations in the corpus. A conclusion of the feedback
we gathered is that the most important decision is whether a post-modifier is restrictive or not, as this
will lead to different strategies for rephrasing the content in order to preserve the meaning as much as
possible. The type can be deduced based on the first tokens in the post-modifier.
3 Detection of relative clauses
In this paper we follow the sign complexity scheme introduced by Evans and Or?asan (2013), where
punctuation marks and functional words are considered explicit markers of coordinated and subordinated
constituents, the two syntactic mechanisms leading to structurally complex sentences.
The signs of syntactic complexity comprise conjunctions ([and], [but], [or]), a complementiser
([that]), wh-words ([what], [when], [where], [which], [while], [who]), punctuation marks ([,], [;],
[:]), and 30 compound signs consisting of one of these lexical items immediately preceded by a punctu-
ation mark (e.g. [, and]). These signs are automatically tagged with a label indicating type of constituent
they delimit, such as finite clauses (EV) or strict appositives (MN), and the position of the sign, such as
start/left boundary (SS*) or end/right boundary (ES*). For example, the label ESMA indicates end of an
adjectival phrase. An automatic tagger for signs of syntactic complexity was developed using a sequence
tagging approach (Dornescu et al., 2013) and is used in this work to select complex sentences from the
corpus and to provide linguistic information to the proposed approach.
The two baselines used are rule based systems for detecting post modifiers. System RC1 uses a set of
rules to detect appositives which are delimited by punctuation marks and do not contain any verbs. Such
expressions are typically nominal appositives or parenthetical expressions e.g.
(16) a. The chief financial officer, Gregory Barnum, announced the merger in an interview.
b. Oxygen can be given with a face mask or through little tubes (nasal cannulae or ?nasal specs?)
that sit just under your nostrils.
c. The business depends heavily on the creativity of its chief designer, Seymour Cray.
3.1 Rule-based approach
The second baseline used as a reference, DAPR (Detection of Adnominal Post-modifiers by Rules),
is a component of a text simplification system for people with autistic spectrum disorders (Evans et
al., 2014). Although the system can also rephrase complex sentences, in this paper we only used the
appositive constituents detected in a sentence by DAPR.
It employs several hand-crafted linguistic rules which detect the extent of appositions based on the
presence of signs of syntactic complexity, in this case punctuation marks, relative pronouns, etc. DAPR
6
exploits rules and patterns to convert sentences containing noun post-modifiers such as finite clauses
(EV), strict appositives (MN), adjective phrases (MA), prepositional phrases (MP), and non-restrictive
non-finite clauses (MV) into a more accessible form.
The conversion procedure is implemented as an iterative process. When a pattern matches the input
sentence, the detected post-modifier is deleted and the resulting sentence is then processed. The priority
of each pattern determines the order in which they are matched when processing sentences which contain
multiple left boundaries of relevant constituents (i.e. signs of complexity tagged with certain labels). The
patterns are implemented to match the first (leftmost) sign of syntactic complexity in the sentence.
The rules used to convert sentences containing noun post-modifiers exploit patterns to identify both
the noun post-modifier and the preceding part of the matrix NP, which can be used to re-phrase the post-
modifier as a coherent, stand-alone sentence. Table 3 provides examples of patterns and the strings that
they match for each class of signs serving as the left boundary of a noun post-modifier. The patterns are
expressed using terms described in Table 2.
Table 2: Terms used in the patterns
Element Description
w
v
Verbal words, including ?ed verbs tagged as adjectives
w
n
Nominal words
w
a
Adjectival words with POS tags JJ, JJS, and VBN
w
nmod
Nominal modifiers: adjectives, nouns
w
nspec
Nomimal specifiers: determiners, numbers, possessive pronouns
w
POS
Word with part-of-speech tag POS (from the Penn Treebank tagset (Santorini, 1990)
utilised by the part of speech tagger distributed with the LT TTT2 package)
CLASS Sign of syntactic complexity of functional class CLASS (Evans and Or?asan, 2013).
? Quotation marks
B-F Sequences of zero or more characters
Table 3: Rules used to used to detect noun post-modifiers
Type Rule Trigger pattern & Example
SSEV 61 w
IN
w
DT
* w
n
{wn|of}* SSEV w
VBD
C sb ?*
But he was chased for a mile-and-a-half by a passer-by who gave police a description of the Citroen
driver.
7 w
{n|DT}
* w
n
SSEV B ESCCV
Some staff at the factory, which employed 800 people, said they noticed cuts on his fingers.
SSMA 81 w
NNP
* w
NNP
SSMA w
{RB|CD}
* w
CD
ESMA w
VBD
Matthew?s pregnant mum Collette Jackson, 24, collapsed sobbing after the pair were sentenced.
83 w
NNP
* w
NNP
SSMA w
CD
ESMA
The court heard that Khattab, 25, a trainee pharmacist, confused double strength chloroform water
with concentrated chloroform.
SSMN 6 w
{NNP|NNPS}
* w
{n|a}
* w
n
SSMN B ESMN
Mr Justice Forbes told the pharmacists that both Mr Young and his girlfriend, Collette Jackson, 24, of
Runcorn, Cheshire, had been devastated by the premature loss of their son.
3 w
{DT|PRP$}
{w
{n|a}
|of}* w
n
SSMN B ESMN
Police became aware that a car, a VW Golf, was arriving in Nottingham from London.
SSMP 4 w
{NNP|NNPS}
* w
{NNP|NNPS}
SSMP ?* w
IN
B ESMP
Justin Rushbrooke, for the Times, said: ?We say libel it is, but it?s a very, very long way from being a
grave libel.
1 w
{NNP|NNPS}
* w
{NNP|NNPS}
{is|are|was|were} w
CD
SSMP w
IN
B ESMA
In the same case Stephen Warner, 33, of Nottingham, was jailed for five years for possession of heroin
with intent to supply.
SSMV 12 w
PRP
w
RB
* w
VBD
B SSMV w
RB
* w
VBG
C {sb|}
He attended anti-drugs meetings with Nottinghamshire police, sitting across from Assistant Chief
Constable Robin Searle.
2 w
{NNP|NNPS}
* w
{NNP|NNPS}
SSMV w
{VBG|VBN}
B ESMV
Andrew Easteal, prosecuting, said police had suspected Francis might be involved in drugs and had
begun to investigate him early last year.
The underlined examples in Table 3 mark only the noun post-modifier. The patterns also identify the
7
preceding part of the matrix NP (in square brackets in the example below). The rules include substitutions
of indefinite articles by demonstratives or definite articles. Following the method applied to sentences
containing noun post-modifiers, rule SSEV-63 would convert:
(17) But he was chased for a mile-and-a-half by a passer-by who gave police a description of the driver.
Into the more accessible sequence of sentences:
(18) a. But he was chased for a mile-and-a-half by [a passer-by].
b. [That passer-by] gave police a description of the driver.
3.2 ML-based approach
As many types of appositive modifiers are simple in structure, we also follow a tagging approach for
the task of detecting noun post-modifiers. We employ the common IOB2 format where the beginning of
each noun post-modifier is tagged as B-PM and tokens inside it are tagged as I-PM. All other tokens are
tagged as other: O. This is similar to a named entity recognition or to a chunking task where only one
type of entity/chunk is detected. For comparison we compare the performance of the approach with a
rule based method for detecting appositive post-modifiers.
The corpus was used to build two supervised tagging models based on Conditional Random Fields
(Lafferty et al., 2001): CRF++
5
and crfsuite
6
. Four feature sets were used. Model A contains standard
features used in chunking, such as word form, lemma and part of speech (POS) tag. Model B adds the
predictions of baseline system RC1 as an additional feature: using the IOB2 models, tokens have one
of three values: B-RC1, I-RC1 or O-RC1. Similarly, model C adds the predictions of baseline system
DAPR also using the IOB2 approach. This allows us to test whether the baseline systems are robust
enough to be employed as input to the sequence tagging models. Model D adds information about the
tokens of the sentence which are signs of syntactic complexity. These are produced automatically.
4 Results and analysis
Results reported by conlleval
7
, the standard tool for evaluating tagging, are presented in Table 4. Al-
though the two baselines, RC1 and DAPR, out-perform the CRF++ models, the best overall performance
is achieved by the crfsuite models.
The rules employed by the RC1 baseline can be misled by sentences containing enumerations, nu-
merical expressions and direct speech due to false positive matches. Although few and addressing the
simplest post-modifiers, the rules perform well.
The more complex baseline, DAPR, appears to be more conservative (it makes the fewest predictions
overall), which suggests it covers fewer types of appositions than covered by our dataset. Compared to
the previous baseline, DAPR detects more complex appositions and relative clauses with better precision,
but with reduced recall.
Although the CRF models also use as features the predictions made by the two baseline models, due
to the level of noise, the improvement is small, between 1 and 2 points. Adding information about the
tagged signs of syntactic complexity actually has a negative impact on both models, suggesting that the
signs are less relevant for this type of syntactic constituent. A large difference in performance is noted
between the two CRF tools: whereas CRF++ is outperformed by both baselines, crfsuite achieves much
better performance despite using the same input features.
To gain better insights into the performance of the best model, Table 5 presents label-wise results.
Given that the average length of a post-modifier is 7, inside tokens (I-PM) are 7 times more prevalent
than beginning tokens (B-PM). Despite this, the model achieves similar performance for both (F1 score
just below 0.60). The two tables also bring evidence suggesting that detecting the end token of a post-
modifier is challenging: although the start is correctly detected for 48.89% of appositives, only 39.94%
5
http://crfpp.googlecode.com/svn/trunk/doc/index.html
6
http://www.chokkan.org/software/crfsuite/
7
http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt
8
Table 4: Results reported by conlleval on the test set (90076 tokens, 2098 annotated post-modifiers)
#predicted #correct
phrases phrases accuracy precision recall F1
RC1 baseline 1287 371 81.01 28.83 17.68 21.92
DAPR baseline 535 163 81.25 30.47 7.77 12.38
CRF++ A:word & POS 3372 289 85.48 8.57 13.78 10.57
+B:RC1 predictions 3381 315 85.66 9.32 15.01 11.50
+C:DAPR predictions 3586 319 85.63 8.90 15.20 11.22
+D:tagged signs 3680 319 85.60 8.67 15.20 11.04
crfsuite A:word & POS 1391 790 87.54 56.79 37.65 45.29
+B:RC1 predictions 1437 825 87.55 57.41 39.32 46.68
+C:DAPR predictions 1470 838 87.56 57.01 39.94 46.97
+D:tagged signs 1481 838 87.56 56.58 39.94 46.83
are a perfect match. This suggests that more work is necessary to improve the ability to detect post-
modifiers but also to better determine their correct extent. The second part is critical to the perceived
performance of the TS system, as incorrect detection usually leads to incorrect text being generated for
users, whereas a loss in recall may be transparent.
Table 5: Label-wise performance for the best model (crfsuite C)
label #match #model #ref precision recall F1
O 70452 77884 73955 90.46 95.26 92.80
B-PM 1014 1469 2074 69.03 48.89 57.24
I-PM 7406 10723 14047 69.07 52.72 59.80
Macro-average 76.18 65.63 69.95
5 Conclusions
The paper presents a new resource for syntactic text simplification, a corpus annotated with relative
clauses and appositions which can be used to develop and evaluate non-destructive simplification sys-
tems. These systems extract certain types of syntactic constituents and embedded clauses and rephrase
them as stand-alone sentences to generate less structurally complex text while preserving the meaning
intact. A supervised tagging model for automatic detection of appositions was built using the corpus and
will be included in a text simplification system.
Acknowledgements
The research described in this paper was partially funded by the European Commission under the Sev-
enth (FP7-2007-2013) Framework Programme for Research and Technological Development (FP7-ICT-
2011.5.5 FIRST 287607). We gratefully acknowledge the contributions of all the members of the FIRST
consortium for their feedback and comments, and to the annotators for their useful insights.
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but useful approach to conjunct identification. In Proceedings
of the 30th Annual Meeting on Association for Computational Linguistics, ACL ?92, pages 15?21, Stroudsburg,
PA, USA. Association for Computational Linguistics.
R. Chandrasekar, Christine Doran, and B. Srinivas. 1996. Motivation and Methods for Text Simplification. In
Proceedings of the 16th conference on Computational linguistics - Volume 2, pages 1041?1044.
9
Iustin Dornescu, Richard Evans, and Constantin Or?asan. 2013. A Tagging Approach to Identify Complex Con-
stituents for Text Simplification. In Galia Angelova, Kalina Bontcheva, and RuslanMitkov, editors, Proceedings
of Recent Advances in Natural Language Processing, RANLP?13, pages 221 ? 229, Hissar, Bulgaria. RANLP
2011 Organising Committee / ACL.
Richard Evans and Constantin Or?asan. 2013. Annotating signs of syntactic complexity to support sentence sim-
plification. In Ivan Habernal and Vclav Matou?sek, editors, Text, Speech, and Dialogue, volume 8082 of Lecture
Notes in Computer Science, pages 92?104. Springer Berlin Heidelberg.
Richard Evans, Constantin Or?asan, and Iustin Dornescu. 2014. An evaluation of syntactic simplification rules
for people with autism. In Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for
Target Reader Populations (PITR), pages 131?140, Gothenburg, Sweden, April. Association for Computational
Linguistics.
Richard J. Evans. 2011. Comparing methods for the syntactic simplification of sentences in information extraction.
LLC, 26(4):371?388.
Laurie Gerber and Eduard H. Hovy. 1998. Improving translation quality by manipulating sentence length. In
David Farwell, Laurie Gerber, and Eduard H. Hovy, editors, AMTA, volume 1529 of Lecture Notes in Computer
Science, pages 448?460. Springer.
M. A. Just, P. A. Carpenter, and K. R. Thulborn. 1996. Brain activation modulated by sentence comprehension.
Science, 274:114?116.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine
Learning, pages 282?289.
J. Levy, E. Hoover, G. Waters, S. Kiran, D. Caplan, A. Berardino, and C. Sandberg. 2012. Effects of syntactic
complexity, semantic reversibility, and explicitness on discourse comprehension in persons with aphasia and in
healthy controls. American Journal of Speech?Language Pathology, 21(2):154 ? 165.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of
english: The penn treebank. Computational Linguistics, 19(2):313?330.
Juan Martos, Sandra Freire, Ana Gonzlez, David Gil, Richard Evans, Vesna Jordanova, Arlinda Cerga, Antoneta
Shishkova, and Constantin Orasan. 2013. User preferences: Updated report. Technical report, The FIRST
Consortium.
Ryan McDonald and Joakim Nivre. 2011. Analyzing and integrating dependency parsers. Comput. Linguist.,
37(1):197?230, March.
Thomas C. Rindflesch, Jayant V. Rajan, and Lawrence Hunter. 2000. Extracting molecular binding relationships
from biomedical text. In ANLP, pages 188?195.
Beatrice Santorini. 1990. Part-Of-Speech tagging guidelines for the Penn Treebank project (3rd revision, 2nd
printing). Technical report, Department of Linguistics, University of Pennsylvania, Philadelphia, PA, USA.
A. Siddharthan. 2002a. An architecture for a text simplification system. Language Engineering Conference, 2002.
Proceedings.
Advaith Siddharthan. 2002b. Resolving Attachment and Clause Boundary Ambiguities for Simplifying Relative
Clause Constructs. Association for Computational Linguistics Student Research Workshop, pages 60?65.
Advaith Siddharthan. 2006. Syntactic simplification and text cohesion. Research on Language and Computation,
4:77?109.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c, Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsujii. 2012.
Brat: A web-based tool for nlp-assisted text annotation. In Proceedings of the Demonstrations at the 13th
Conference of the European Chapter of the Association for Computational Linguistics, EACL ?12, pages 102?
107, Stroudsburg, PA, USA. Association for Computational Linguistics.
Masaru Tomita. 1985. Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems. Kluwer
Academic Publishers, Norwell, MA, USA.
10
Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual Society, pages 53?63,
Dublin, Ireland, August 24th 2014.
Assessing Conformance of Manually Simplified Corpora with
User Requirements: the Case of Autistic Readers
Sanja
?
Stajner and Richard Evans and Iustin Dornescu
Research Group in Computational Linguistics
Research Institute of Information and Language Processing
University of Wolverhampton, UK
{SanjaStajner, R.J.Evans, I.Dornescu2}@wlv.ac.uk
Abstract
In the state of the art, there are scarce resources available to support development and evaluation
of automatic text simplification (TS) systems for specific target populations. These comprise
parallel corpora consisting of texts in their original form and in a form that is more accessible
for different categories of target reader, including neurotypical second language learners and
young readers. In this paper, we investigate the potential to exploit resources developed for such
readers to support the development of a text simplification system for use by people with autistic
spectrum disorders (ASD). We analysed four corpora in terms of nineteen linguistic features
which pose obstacles to reading comprehension for people with ASD. The results indicate that the
Britannica TS parallel corpus (aimed at young readers) and the Weekly Reader TS parallel corpus
(aimed at second language learners) may be suitable for training a TS system to assist people
with ASD. Two sets of classification experiments intended to discriminate between original and
simplified texts according to the nineteen features lent further support for those findings.
1 Introduction
As a fundamental human right, people with reading and comprehension difficulties are entitled to access
written information (UN, 2006). This entitlement enables better inclusion into society. However, the
vast majority of texts that such people encounter in their everyday life ? especially newswire texts ? are
lexically and syntactically very complex. Since the late nineties, several initiatives have emerged which
propose guidelines for producing plain, easy-to-read and more accessible documents. These include
the ?Federal Plain Language Guidelines?
1
, ?Make it Simple, European Guidelines for the Production of
Easy-to-Read Information for people with Learning Disability? (Freyhoff et al., 1998), ?Am I making
myself clear? Mencap?s guidelines for accessible writing?
2
, and the W3C ? Web Accessibility Initiative
guidelines
3
. However, manual adaptation of texts cannot match the speed with which new texts are pub-
lished on the web in order to provide up to date information. The aim of Automatic Text Simplification
(ATS) is to automatically (or at least semi-automatically) convert complex sentences into a more accessi-
ble form while preserving their original meaning. In the last twenty years, many ATS systems have been
proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank,
2006; Saggion et al., 2011; Inui et al., 2003; Alu??sio et al., 2008). Due to the scarcity of parallel corpora
of original and manually simplified texts, most of these systems are rule-based.
The emergence of Simple English Wikipedia (SEW)
4
, together with the existing English Wikipedia
(EW)
5
provided a large amount of parallel TS training data, which motivated a shift in English TS from
rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata,
2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf
2
http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf
3
http://www.w3.org/WAI/
4
http://simple.wikipedia.org/wiki/Main Page
5
http://wikipedia.org/wiki/Main Page
53
ever been made of the quality of the simplifications made in SEW and the usefulness of the transfor-
mations learned from EW?SEW parallel corpora for any of the specified target populations. The only
instructions given to the authors of SEW are to use Basic English vocabulary and shorter sentences. The
main page states that SEW is for everyone, including children and adults who are learning English. All
previously mentioned studies conducted on that corpus evaluated the quality of the generated output in
terms of grammaticality, meaning preservation, and simplicity, but not usefulness. Also, there have been
no comparisons of the types of transformations present in EW?SEW with any of the other TS corpora
in English which were simplified with a specific target population in mind, e.g. Encyclopedia Britan-
nica and its manually simplified versions for children ? Britannica Elementary (Barzilay and Elhadad,
2003)
6
, Guardian Weekly and its manually simplified versions for language learners (Allen, 2009), and
the FIRST corpus of various texts simplified for people with autism spectrum disorder (ASD)
7
.
In this study, we compare the original and simplified texts of the four aforementioned TS corpora in
terms of nineteen features which measure the complexity of texts for people with ASD. Although these
features were derived from user requirements for people with ASD, many of them are known to present
reading obstacles for other target populations as well (e.g. children or language learners). Given the lack
of parallel TS corpora for people with ASD, our main goal is to investigate whether the EW?SEW or the
other two corpora aimed at children and language learners could be used as training material for a TS
system to assist people with ASD and thus enable data-driven approaches (instead of the currently used
rule-based ones). In order to further support the results of this analysis, we conduct several classification
experiments in which we try to distinguish between original and simplified texts in each of the four
corpora, using the nineteen features.
2 The FIRST Project and User Requirements
Autistic Spectrum Disorders (ASD) are neurodevelopmental disorders characterised by qualitative im-
pairment in communication and stereotyped repetitive behaviour. People with ASD show a diverse range
of reading abilities: 5-10% have the capacity to read words from an early age without the need for
formal learning (hyperlexia) but many demonstrate reduced comprehension of what has been read (Volk-
mar and Wiesner, 2009). They may have difficulty inferring contextual information or may have trouble
understanding mental verbs, emotional language, and long sentences with complex syntactic structure
(Tager-Flusberg, 1981; Kover et al., 2012). To address these difficulties, a tool is being developed in the
FIRST project
8
to assist in the process of making texts more accessible for people with ASD. To achieve
this, three modues are exploited:
1. Structural complexity processor, which detects syntactically complex sentences and generates
alternatives to such sentences in the form of sequences of shorter sentences (Evans et al., 2014;
Dornescu et al., 2013).
2. Meaning disambiguator, which resolves pronominal references, performs word sense disambigua-
tion, and detects lexicalised (conventional) metaphors (Barbu et al., 2013).
3. Personalised document generator, which aggregates the output of processors 1 and 2 and gener-
ates additional elements such as glossaries, illustrative images, and document summaries.
The system, named Open Book, is deployed as an editing tool for healthcare and educational service
providers. It functions semi-automatically, exploiting the three processors and requiring the user to
authorise the application of the conversion operations. The system is required to assess the readability
of texts, not only to decide which texts should be converted, but also to assess the readability of texts
that are undergoing conversion. It is expected that people working to improve the accessibiity of a
given text will benefit from relevant feedback concerning the effects of the changes being introduced.
Automatic assessment of readability is one method by which such feedback can be delivered. In the
6
http://www.cs.columbia.edu/ noemie/alignment/
7
Available at: http://www.first-asd.eu/?q=system/files/FIRST D7.2 20130228 annex.pdf
8
www.first-asd.eu
54
context of improving the accessibility of texts, relevant feedback should indicate the extent to which
different versions of a text meet the particular requirements of intended readers.
User requirements were obtained through consulatation of 94 subjects meeting the strict DSM-IV cri-
teria for ASD and with IQ > 70. 43 user requirements were derived and assigned a reference code. The
requirements link linguistic phenomena to editing operations, such as deletion, explanation, or trans-
formation, that will convert the text to a more accessible form. The linguistic phenomena of concern
include instances of syntactic complexity such as long sentences containing more than 15 words (possi-
bly containing multiple copulative coordinated clauses (UR301), subordinate adjective clauses (UR302),
explicative clauses (UR303), non-initial adverbial clauses (UR307)), sentences containing passive verbs
(UR313), rarely used conjunctions and antithetic conjuncts (UR304, UR305, UR306), uncommon syn-
onyms of polysemic words (UR401, UR425, UR504, UR505, UR511), rarely-used symbols and punc-
tuation marks (UR311), anaphors, words containing more than 7 characters, adjectives ending with -ly,
long numerical expressions (UR417), negation (UR314), words more than 7 characters long and adverbs
with suffix -ly (UR317-319), anaphors, including pronouns (UR418-420).
Additional linguistic phenomena such as phraseological units (UR402, UR410, UR425, UR507), and
non-lexicalised metaphors (UR422, UR508), were also found to pose obstacles to reading comprehension
for people with ASD. At present, there is a scarcity of resources enabling accurate detection of these
items. For this reason, changes in the prevalence of these items in original and converted versions of
texts are not captured in this study. The full set of user requirements is detailed in Martos et al. (2013).
More generally, it is infrequent linguistic phenomena that cause the greatest difficulty.
3 Related Work
There have been several studies analysing the existing TS corpora. However, their main focus was on
determining necessary transformations in TS: for children (Bautista et al., 2011); for people with intel-
lectual disability (Drndarevi?c and Saggion, 2012); for language learners (Petersen and Ostendorf, 2007);
and for people with low literacy (Gasperin et al., 2009). Unfortunately, those studies are not directly
comparable (neither among themselves nor with our study), either because they focus on different types
of transformations (the study of Bautista et al. (2011) focuses on general transformations while the other
three studies focus on sentence transformations), or because they treat different languages (Spanish,
English, and Brazilian Portuguese).
Two previous studies most relevant to ours are those by Napoles and Dredze (2010), and by
?
Stajner
et al. (2013). Napoles and Dredze (2010) built a statistical classification system that discriminates
simple English from ordinary English, based on EW?SEW corpus. They used four different groups of
features: lexical, part-of-speech, surface, and syntactic parse features. The accuracy of the best classifier
(SVM) on the document classification task when using all features was 99.90%, while the accuracy
of the best classifier (maximum entropy) on the sentence classification task when using all features
was 80.80%. However, this study only demonstrated that it is fairly easy to discriminate sentences and
documents of EW from those of SEW. It did not investigate whether the simple English used in SEW
complies with the user requirements of any specific population with reading difficulties.
?
Stajner et al.
(2013) analysed a corpus of 37 newswire texts in Spanish and their manual simplifications aimed at
people with Down?s syndrome, compiled in the Simplext project
9
. They built a classification system that
discriminates the original texts from those which are simple with an F-measure of 1.00 using the SVM,
and only seven features: average number of punctuation marks (not counting end of sentence markers),
numerical expressions, average word length in characters, the ratio of simple and complex sentences,
sentence complexity index, lexical density and lexical richness. They reported the average sentence
length as being the feature with the best discriminative power, leading to an F-measure of 0.99 when
used on its own.
In spite of the many linguistic phenomena that pose obstacles to reading comprehension for different
target populations, there have been almost no studies investigating whether a TS system built with a
specific target population in mind could be successfully applied ? or adapted ? to a different target
9
www.simplext.es
55
Corpus Aimed at Version Code Texts SentPerText WordsPerText
Weekly Reader Language learners
Original Learn.-O 100 39.41 ? 14.43 746.83 ? 174.25
Simple Learn.-S 100 38.40?12.59 621.11 ? 157.17
Enc. Britannica Children
Original Brit.-O 20 27.10 ? 8.91 628.30 ? 198.19
Simple Brit.-S 20 26.45 ? 9.35 382.35 ? 127.69
Wikipedia Various
Original Wiki-O 110 34.55 ? 1.87 716.57 ? 117.82
Simple Wiki-S 110 34.49 ? 1.82 675.07 ? 107.03
FIRST People with ASD
Original FIRST-O 25 13.64 ? 3.95 285.68 ? 34.46
Simple FIRST-S 25 22.92 ? 4.79 311.36 ? 76.82
Table 1: Corpora characteristics
population. The only exception to this is the study by
?
Stajner and Saggion (2013), which demonstrated
that two classifiers ? one which discriminates sentences which should be split from those which should
be left unsplit, and another which discriminates sentences which should be deleted from those which
should be preserved ? can successfully be trained on one type of corpora and applied to the other. Both
corpora consisted of texts in Spanish, one containing newswire texts manually simplified for people with
Down?s syndrome, and the other various text genres manually simplified for people with ASD.
Motivated by those previous studies and the lack of parallel corpora aimed specifically to people with
ASD, in this paper, we investigate whether some of already existing corpora for TS in English could
potentially be used for building a data-driven TS system for this target population.
4 Methodology
The corpora, features, and experimental settings used in this study are described in Sections 4.1?4.3.
4.1 Corpora
Four parallel corpora of original and manually simplified texts for different target populations were used
in this study (Table 1):
1. The corpus of 100 texts from Weekly Reader and their manual simplifications provided by Macmil-
lan English Campus and Onestopenglish
10
aimed at foreign language learners. The corpus is divided
into three sub-corpora ? advanced, intermediate and elementary ? each representing a different level
of simplification. Given that the other three corpora used in this study contain original texts and only
one level of simplification, we only used the texts from the advanced (henceforth original) and ele-
mentary (henceforth simplified) levels. A more detailed description of this corpus can be found in
(Allen, 2009).
2. The corpus of 20 texts from the Encyclopedia Britannica and their manually simplified versions
aimed at children ? Britannica Elementary (Barzilay and Elhadad, 2003)
11
.
3. The corpus of 110 randomly selected corresponding articles from EW and SEW. Here, it is impor-
tant to note that, in general, articles from SEW do not represent direct simplifications of the articles
from EW, they just have a matching topic. For this reason, we did not use complete EW and SEW
articles. We only used those sentences in original and simplified versions, which existed in the
sentence-aligned parallel corpora version 2.0
12
(Kauchak, 2013).
4. The corpus of 25 texts on various topics manually simplified for people with autism, compiled in
the FIRST project
13
, for the purpose of a piloting task
14
. The texts were simplified by carers of
people with ASD in accordance with specified guidelines.
10
http://www.onestopenglish.com/
11
http://www.cs.columbia.edu/ noemie/alignment/
12
http://www.cs.middlebury.edu/ dkauchak/simplification/
13
www.first-asd.eu
14
http://www.first-asd.eu/?q=system/files/FIRST D7.2 20130228 annex.pdf
56
4.2 Text Features Relevant to User Requirements
In this paper, a set of 15 text complexity measures and 4 formulae exploiting these measures was used
to estimate the accessibility of the texts. These features quantify the occurrence of linguistic phenomena
identified as potential obstacles to reading comprehension for people with ASD. The set of features is
presented in Table 2. The set of formulae is presented in Table 3. In every case, accessible texts are
expected to have smaller values of each metric.
# Code Linguistic feature Explanation/relevance
1 Illative Illative conjunctions Indicators of syntactic complexity, linking clauses.
2 CompConj Comparative conjunctions [UR304-306]
3 AdvConj Adversative conjunctions
4 LongSent Long sentences Motivated by the assumption that deriving the propositions in
5 Semicol Semicolons/suspension
points
complex sentences is more difficult than deriving connections be-
tween related propositions expressed in simple sentences
6 Passive Passive verbs (Arya et al., 2011). [UR309-310, UR313]
7 UnPunc Unusual punctuation Indicates syntactic complexity, ellipsis, alternatives, and mathe-
matical expressions [UR311]
8 Negations Negation The sum of adverbial and morphological negations (?Make it Sim-
ple? (Freyhoff et al., 1998), though contrary to the findings of Tat-
tamanti (2008)) [UR314]
9 Senses Possible senses The sum over all tokens in the text of the total number of possible
senses of each token. [UR401, UR425, UR504-505, UR511]
10 PolyW Polysemic words Words with two or more senses listed in WordNet. [UR401,
UR425, UR504, UR505, UR511]
11 Infreq Infrequent words Words that are not among the 5000 most frequent words in English
[UR304-306, UR401, UR425, UR504-505, UR511]
12 NumExp Numerical expressions Numbers written as sequences of words rather than digits [UR417]
13 Pron Pronouns Studies have shown that people with ASD can have
14 DefDescr Definite descriptions difficulty processing anaphora (Fine et al., 1994) [UR418-420]
15 SylLongW Long words Words with more than three syllables [UR317-319]
Table 2: Complexity measures (1 ? words such as therefore and hence; 2 ? words such as equally and
correspondingly; 3 ? words such as although and conversely; 4 ? sentences more than 15 words long; 8 ?
negative adverbials and negative prefixes such as un- and dis-; 11 ? derived from Wiktionary frequency
lists for English
16
)
# Code Metric Formula Relevance
16 PolyType Polysemic type ratio
ptyp
typ
Indicates the proportion of the text vocabulary that is
polysemous. [UR401, UR425, UR504-505, UR511]
17 CommaInd Comma index
10?c
w
Indicates the average syntactic complexity of the
sentences in the text [UR301-303, UR307]
18 WordsPerSent Words per sentence
w
s
Indicates the average length of the sentences in the text
[UR309]
19 TypeTokRat Type-token ratio
typ
tok
Indicate the range of vocabulary used in the text
[UR401, UR425, UR504, UR505, UR511]
Table 3: Text complexity formulae (w ? the number of words in the text; s ? the number of sentences in
the text; ptyp ? the number of polysemic word types in the text; c ? the number of commas in the text;
typ ? the number of word types in the text; tok ? the number of word tokens in the text)
Scores for these measures, and the text complexity formulae that exploit them where obtained auto-
matically by the tokeniser, part-of-speech tagger, and lemmatiser distributed with LT TTT2 (Grover et al.,
2000). Detection of the features used to derive complexity measures also involved the use of additional
resources such as WordNet, gazetteers of rare illative, comparative, and adversative conjunctions, nega-
tives (words and prefixes) and a set of lexico-syntactic patterns used to detect passive verbs (presented in
Figure 1).
57
am/are/is/was/were w
RB
* w
{V BN |V BD}
am/are/is/was/were w
RB
* being w
RB
* w
{V BN |V BD}
have/has/had w
RB
* been w
RB
* w
{V BN |V BD}
will w
RB
* be w
RB
* w
{V BN |V BD}
am/is/are w
RB
* going w
RB
* to w
RB
* be w
RB
* w
{V BN |V BD}
w
MD
w
RB
* be w
{V BN |V BD}
w
MD
w
RB
* have w
RB
* been w
RB
* w
{V BN |V BD}
Figure 1: Lexico-syntactic patterns used to detect passive verbs (?*? indicates zero or more repetitions of
the item it is attached to, while RB, V BN , V BD, and MD are Penn treebank tags returned by the LT
TTT PoS tagger: RB ? adverb; V BN ? past participle; V BD ? past tense; and MD ? modal verb)
4.3 Experiments
Two sets of experiments were performed in this study:
1. Analysis of differences between original and simplified texts in terms of nineteen selected features
(Section 4.2) across four corpora (Section 4.1). Statistical difference was measured using the t-
test for related samples in the cases where the features were normally distributed, and using the
related samples Wilcoxon signed rank test otherwise. Normality of the data was tested using the
Shapiro-Wilk test of normality, which is preferred over the Kolmogorov-Smirnov test when the
dataset contains less than 2,000 elements. All tests were performed in SPSS. Features 1?15 were
first normalised (as an average per sentence) in order to allow a fair comparison across the four TS
corpora (text length in words and sentences differed significantly across different corpora).
2. Classification experiments with the aim of discriminating original from simplified texts using the
nineteen selected features. All experiments were conducted using the Weka Experimenter (Witten
and Frank, 2005; Hall et al., 2009) in 10-fold cross-validation setup with 10 repetitions, using four
different classification algorithms: NB ? NaiveBayes (John and Langley, 1995), SMO ? Weka im-
plementation of Support Vector Machines (Keerthi et al., 2001) with normalisation, JRip ? a propo-
sitional rule learner (Cohen, 1995), and J48 ? Weka implementation of C4.5 (Quinlan, 1993). The
statistical significance of the observed differences in F-measures obtained by different algorithms
was calculated using the corrected paired t-test provided in the Weka Experimenter.
The TS system in FIRST is not only supposed to decide which texts should be converted, but also
to assess the readability of texts that are undergoing conversion. It is expected that people working to
improve the accessibility of a given text will benefit from relevant feedback concerning the effects of the
changes being introduced. Automatic assessment of readability is one method by which such feedback
can be delivered. Deriving a subset of features which, when trained with an appropriate classification
algorithm, can categorize a given text as either ?original? or ?simplified?, would facilitate automatic eval-
uation of TS systems. The resulting classifier would be suitable for assessing whether those systems
perform an appropriate level of simplification. This could serve as a rough estimation, an efficient first
step offering a quick evaluation prior to being tested with real users.
5 Results and Discussion
The results of the two sets of experiments are presented and discussed in the next two subsections.
5.1 Analysis of the Features across the Corpora
Mean values (with standard deviations) of each of the first eight features on each sub-corpus are dis-
played in Table 4. The number of unusual punctuation marks (UnPunc) is the only feature whose value
does not differ significantly between the original and simplified versions of the texts in any of the four
corpora. This feature was thus excluded from further classification experiments. The number of com-
parative conjunctions per sentence (CompConj) significantly decreases only when simplifying texts for
58
Corpus Illative CompConj AdvConj LongSent Semicol UnPunc Passive Negations
Lear.-O 0.24?0.12 0.04?0.13 0.21?0.08 0.62?0.15 0.03?0.05 0.00?0.01 0.21?0.10 0.33?0.15
Lear.-S 0.20?0.13 0.03?0.09 0.19?0.09 0.51?0.14 *0.03?0.05 0.00?0.01 0.09?0.09 0.26?0.14
Brit.-O 0.13?0.09 0.15?0.26 0.14?0.07 0.72?0.11 0.13?0.20 0?0 0.33?0.10 0.28?0.16
Brit.-S 0.08?0.05 *0.02?0.10 0.06?0.04 0.38?0.11 0.00?0.02 0?0 0.25?0.12 0.14?0.09
Wiki-O 0.20?0.11 0.11?0.19 0.16?0.10 0.65?0.12 0.04?0.04 0.04?0.10 0.34?0.15 0.32?0.23
Wiki-S 0.18?0.11 0.11?0.20 0.14?0.09 0.62?0.12 0.03?0.04 0.03?0.10 0.33?0.15 0.29?0.24
FIRST-O 0.18?0.14 0.06?0.19 0.18?0.15 0.68?0.15 0.03?0.10 0.01?0.02 0.27?0.23 0.42?0.28
FIRST-S 0.11?0.10 0.01?0.06 0.09?0.07 0.33?0.19 0.00?0.01 0?0 0.20?0.15 0.22?0.13
Table 4: Mean values (with standard deviation) of features 1?8 across the corpora (O ? the original texts
in the corpora; S ? the simplified texts in the corpora; bold ? significantly different from the value on the
original texts at a 0.01 level of significance; *bold ? significantly different from the value on the original
texts at a 0.05 level of significance (but not at 0.01); ?0.00? ? a value different from zero which rounded
at two decimals gives 0.00; ?0? ? a value equal to zero)
Corpus Senses PolyW Infreq NumExp Pron DefDescr SylLongW
Lear.-O 73.95?12.32 9.37?1.72 5.64?1.33 0.18?0.11 0.97?0.40 1.86?0.54 1.12?0.28
Lear.-S 64.21?11.16 7.85?1.45 4.14?1.01 0.16?0.10 0.90?0.37 1.62?0.45 0.92?0.27
Brit.-O 67.51? 8.83 9.87?1.15 9.37?1.10 0.18?0.12 0.40?0.18 2.86?0.44 1.45?0.20
Brit.-S 48.68? 4.17 6.48?0.57 5.39?0.58 0.09?0.06 0.28?0.13 1.86?0.20 1.17?0.19
Wiki-O 67.70?12.96 9.13?1.61 7.86?1.63 0.18?0.16 0.67?0.43 2.08?0.58 1.24?0.38
Wiki-S 68.20?13.56 8.71?1.56 7.16?1.51 *0.17?0.16 0.68?0.44 1.97?0.54 1.10?0.42
FIRST-O 82.28?24.20 10.16?2.65 7.11?2.72 0.19?0.19 1.05?0.73 2.12?0.92 1.17?0.58
FIRST-S 57.13?15.96 6.47?1.77 3.92?1.56 0.09?0.07 *0.82?0.44 1.62?0.54 *0.92?0.43
Table 5: Mean values (with standard deviation) of features 9?15 across the corpora (O ? the original texts
in the corpora; S ? the simplified texts in the corpora; bold ? significantly different from the value on the
original texts at a 0.01 level of significance; *bold ? significantly different from the value on the original
texts at a 0.05 level of significance (but not at 0.01))
children (Brit.-S), while the average number of passive constructions per sentence (Passive) decreases
when simplifying for both children (Brit.-S) and language learners (Lear.-S). It is interesting to note that
the average number of passive constructions per sentence (Passive) does not decrease in the EW?SEW
corpus and that its value on the simplified versions of Wikipedia articles (Wiki-S) is significantly higher
than on Brit.-S and Lear.-S, although SEW claims to provide articles simplified for both those target
populations. It can also be observed that the fact that all four corpora were reported to have significant
differences between original and simplified texts in terms of features Illative, AdvConj, LongSent, and
Negations does not necessarily mean that the average number of occurrences of those features is similar
in all four simplified corpora. The values of Illative, AdvConj, and LongSent in the simplified versions
of the texts in the FIRST corpus seem to correspond best to those in the simplified versions of the texts
in the Britannica corpus (Brit.-S). The value of Negations in FIRST-S, however, seems to correspond
best to that in Lear.-S. This suggests that if we wish to build a component of our TS system (to assist
people with ASD) which would remove negations (Negations), we should train it on the sentence pairs
from the corpora with simplifications aimed at second language learners. If we wish to build a com-
ponent which would remove illative conjunctions (Illative), adversative conjuctions (AdvConj), or long
sentences (LongSent), we should probably train it on the sentence pairs from the corpora with simplifi-
cations aimed at young readers.
The number of occurrences per sentence of features 9?15 in the original versions of the texts was sig-
nificantly higher than in the simplified versions of the texts in all four corpora, with only two exceptions
? features Senses and Pron in the EW?SEW corpus (Wiki-O and Wiki-S), as can be observed in Table
5. Again, the mean values of all features in the simplified versions of the texts in the FIRST corpora
FIRST-S, seems to correspond better to the simplified versions of Encyclopedia Britannica (Brit.-S) and
59
Corpus PolyType CommaInd WordsPerSent TypeTokRat
Lear.-O 0.76?0.04 0.56?0.12 19.91?3.46 0.51?0.04
Lear.-S 0.77?0.04 0.46?0.15 16.69?2.78 0.47?0.05
Brit.-O 0.69?0.03 0.78?0.15 23.46?2.78 0.51?0.04
Brit.-S *0.71?0.02 *0.67?0.14 14.61?1.21 0.55?0.04
Wiki-O 0.71?0.05 0.65?0.15 20.73?3.16 0.48?0.05
Wiki-S 0.71?0.05 0.60?0.16 19.57?2.90 *0.48?0.05
FIRST-O 0.73?0.04 0.51?0.18 22.20?5.43 0.59?0.05
FIRST-S 0.75?0.06 0.19?0.15 13.86?3.41 0.53?0.08
Table 6: Mean values (with standard deviation) of features 16?19 across the corpora (O ? the original
texts in the corpora; S ? the simplified texts in the corpora; bold and *bold ? used in the same way as in
the previous two tables)
Weekly Readers (Lear.-S) than to those in the simplified versions of the Wikipedia articles (Wiki-S). It is
also interesting to note that many of the features (LongSent, Negations, Senses, PolyW, Infreq, DefDesc)
seem to have a significantly higher number of occurrences per sentence in the simplified versions of
the Wikipedia articles (Wiki-S) than in the simplified versions of Encyclopedia Britannica (Brit.-S) and
Weekly Reader (Lear.-S).
The comma index (CommaInd), type-token ratio (TypeTokRat), and the average number of words per
sentence (WordsPerSent) were found to be significantly higher in original texts than in their simplified
versions in all four corpora (Table 6). However, the values of those three text complexity formulae were
not similar in the simplified texts across the four corpora. In terms of the average number of words
per sentence (WordsPerSent) and the type-token ratio (TypeTokRat), the simplified versions of the texts
in the FIRST corpora (FIRST-S) seem to correspond better to the texts simplified for young readers
(Brit.-S), than to those simplified for second language learners (Lear.-S) and those aimed at various
target populations (Brit.-S). The comma index (CommaInd) obtained for simplified texts in the FIRST
corpora was several times lower than that obtained for simplified texts in the three other corpora. The
polysemic type ratio (PolyType) was not significantly different in original and in simplified texts of the
FIRST corpora (Table 6). The higher polysemic type ratio (PolyType) for simplified rather than original
versions of the texts in the other three corpora was unexpected, as it is usually assumed that polysemous
words can pose an obstacle for various target populations. However, it is important to bear in mind that
polysemous words usually pose an obstacle when conveying one of their infrequently used meanings.
Findings in cognitive psychology indicate that the words with the highest number of possible meanings
are actually understood more quickly, due to their high frequency (Jastrzembski, 1981). A common
lexical simplification strategy is to replace infrequent words with their more frequent synonyms, and
long words with their shorter synonyms. This strategy leads to a higher polysemic type ratio (PolyType)
in simplified versions of the texts as the shorter words are usually more frequent (Balota et al., 2004),
and frequent words tend to be more polysemous than infrequent ones (Glanzer and Bowles, 1976).
5.2 Classification between Original and Simplified Texts
Classification experiments were conducted using two different sets of features on each of the corpora:
1. all ? all 18 features (UnPunc was excluded as it was not reported as significant for any of the
corpora)
2. best ? 11 features which were reported as significant for all four corpora (Illative, AdvConj,
LongSent, Negations, PolyW, NumExp, DefDescr, SylLongW, CommaInd, WordsPerSent, Type-
TokRat)
As can be observed from Table 7, use of the SMO-n classification algorithm using the subset of 11
best features achieves perfect 1.00 F-measure for discriminating original from simplified versions of the
Encyclopedia Britannica. The same classification algorithm performs less well on the FIRST and Weekly
Readers corpora (though still quite well), while it performs significantly worse on the Wikipedia corpus.
60
The baseline (which chooses majority class) would be 0.50 in all cases. These results indicate that the
Encyclopedia Britannica TS parallel corpus, and possibly the Weekly Readers TS parallel corpus, may
serve as suitable training material for building a TS system (or at least some of its components) aimed at
people with ASD.
Dataset SMO-n NB JRip J48
Brit-all 0.98?0.09 0.94?0.12 0.94?0.14 0.97?0.11
Brit-best 1.00?0.00 0.99?0.05 0.94?0.13 0.97?0.11
FIRST-all 0.88?0.15 0.86?0.19 0.79?0.23 0.75?0.25
FIRST-best 0.88?0.15 0.85?0.20 0.78?0.25 0.76?0.25
Lear-all 0.81?0.08 0.74?0.10* 0.75?0.07* 0.72?0.10*
Lear-best 0.77?0.08 0.74?0.11 0.70?0.10* 0.73?0.10
Wiki-all 0.54?0.12 0.50?0.12 0.51?0.14 0.35?0.20*
Wiki-best 0.55?0.13 0.55?0.12 0.51?0.12 0.33?0.20*
Table 7: F-measure with standard deviation in a 10-fold cross-validation setup with 10 repetitions for
four classification algorithms: SMO-n, NB, JRip, and J48 (* ? statistically significant degradation in
comparison with SMO-n)
6 Conclusions
Automatic Text Simplification (ATS) aims to convert complex texts into a simpler form, which is more
accessible to a wider audience. Due to the lack of parallel corpora for TS consisting of original and
manually simplified texts, most of the ATS systems for specific target populations are still rule-based.
Our main goal was to explore whether some of the existing TS parallel corpora in English, aimed at dif-
ferent audiences (children ? Encyclopedia Britannica, language learners ? Weekly Reader, and various ?
Wikipedia) could be used as training material to build a TS system aimed at people with ASD. We anal-
ysed the four corpora (FIRST, Britannica, Weekly Reader, and Wikipedia) in terms of nineteen linguistic
features which pose obstacles to reading comprehension for people with ASD. The preliminary results
indicate that the Britannica TS parallel corpus, and possibly the Weekly Reader TS parallel corpus, could
be used to train a TS system aimed at people with ASD. Two sets of classification experiments which
tried to discriminate original from simplified texts according to the nineteen features derived from user
requirements further supported those findings. The results of the classification experiments indicated
that the SVM classifier trained on the Britannica corpus might be suitable for discriminating original
from simplified texts for people with ASD, and thus might be used as the initial evaluation of the texts
simplified by the TS system developed in the FIRST project.
Acknowledgements
The research described in this paper was partially funded by the European Commission under the Sev-
enth (FP7-2007-2013) Framework Programme for Research and Technological Development (FP7-ICT-
2011.5.5 FIRST 287607).
References
D. Allen. 2009. A Corpus-Based Study of the Role of Relative Clauses in the Simplification of News Texts for
Learners of English. System, 37(4):585?599.
S. M. Alu??sio, L. Specia, T. A.S. Pardo, E. G. Maziero, and R. P.M. Fortes. 2008. Towards brazilian portuguese
automatic text simplification systems. In Proceedings of the eighth ACM symposium on Document engineering,
DocEng ?08, pages 240?248, New York, NY, USA. ACM.
D. J. Arya, Elfrieda H. Hiebert, and P. D. Pearson. 2011. The effects of syntactic and lexical complexity on
the comprehension of elementary science texts. International Electronic Journal of Elementary Education, 4
(1):107?125.
61
D. Balota, M. J. Cortese, S. D. Sergent-Marshall, D. H. Spieler, and M. J. Yap. 2004. Visual word recognition of
single-syllabe words. Journal of Experimental Psychology: General, 133:283?316.
E. Barbu, M. Mart??n-Valdivia, L. Alfonso, and U. Lopez. 2013. Open book: a tool for helping asd users? semantic
comprehension. In Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual
Accessibility (NLP4ITA), pages 11?19, Atlanta, US. Association for Computational Linguistics.
R. Barzilay and N. Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the
2003 conference on Empirical methods in natural language processing, EMNLP ?03, pages 25?32, Stroudsburg,
PA, USA. Association for Computational Linguistics.
S. Bautista, C. Le?on, R. Herv?as, and P. Gerv?as. 2011. Empirical identification of text simplification strategies for
reading-impaired people. In European Conference for the Advancement of Assistive Technology.
O. Biran, S. Brody, and N. Elhadad. 2011. Putting it Simply: a Context-Aware Approach to Lexical Simplification.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 496?501, Portland, Oregon, USA. Association for Computational Linguistics.
J. Carroll, G. Minnen, Y. Canning, S. Devlin, and J. Tait. 1998. Practical Simplification of English Newspaper
Text to Assist Aphasic Readers. In Proceedings of AAAI-98 Workshop on Integrating Artificial Intelligence and
Assistive Technology, pages 7?10.
W. Cohen. 1995. Fast Effective Rule Induction. In Proceedings of the Twelfth International Conference on
Machine Learning, pages 115?123.
W. Coster and D. Kauchak. 2011. Learning to Simplify Sentences Using Wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics, pages 1?9.
S. Devlin and G. Unthank. 2006. Helping aphasic people process online information. In Proceedings of the 8th
international ACM SIGACCESS conference on Computers and accessibility, Assets ?06, pages 225?226, New
York, NY, USA. ACM.
I. Dornescu, R. Evans, and C. Orasan. 2013. A Tagging Approach to Identify Complex Constituents for Text
Simplification. In Proceedings of Recent Advances in Natural Language Processing, pages 221 ? 229, Hissar,
Bulgaria.
B Drndarevi?c and H. Saggion. 2012. Reducing Text Complexity through Automatic Lexical Simplification: an
Empirical Study for Spanish. SEPLN Journal, 49.
R. Evans, C. Orasan, and I. Dornescu. 2014. An evaluation of syntactic simplification rules for people with
autism. In Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader
Populations (PITR), pages 131?140, Gothenburg, Sweden, April. Association for Computational Linguistics.
J. Fine, G. Bartolucci, P. Szatmari, and G. Ginsberg. 1994. Cohesive discourse in pervasive developmental
disorders. Journal of Autism and Developmental Disorders, 24:315?329.
G. Freyhoff, G. Hess, L. Kerr, B. Tronbacke, and K. Van Der Veken, 1998. Make it Simple, European Guide-
lines for the Production of Easy-toRead Information for People with Learning Disability. ILSMH European
Association, Brussels.
C. Gasperin, L. Specia, T. Pereira, and S.M. Alu??sio. 2009. Learning When to Simplify Sentences for Natural Text
Simplification. In Proceedings of the Encontro Nacional de Inteligncia Artificial (ENIA-2009), Bento Gonalves,
Brazil., pages 809?818.
M. Glanzer and N. Bowles. 1976. Analysis of the word frequency effect in recognition memory. Journal of
Experimental Psychology: Human Learning and Memory, 2:21?31.
C. Grover, C. Matheson, A. Mikheev, and M. Moens. 2000. Lt ttt - a flexible tokenisation tool. In In Proceedings
of Second International Conference on Language Resources and Evaluation, pages 1147?1154.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The weka data mining
software: an update. SIGKDD Explor. Newsl., 11:10?18, November.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura. 2003. Text simplification for reading assistance: a project
note. In Proceedings of the second international workshop on Paraphrasing - Volume 16, PARAPHRASE ?03,
pages 9?16, Stroudsburg, PA, USA. Association for Computational Linguistics.
62
J. Jastrzembski. 1981. Multiple meaning, number or related meanings, frequency of occurrence and the lexicon.
Cognitive Psychology, 13:278?305.
G. H. John and P. Langley. 1995. Estimating Continuous Distributions in Bayesian Classifiers. In Proceedings of
the Eleventh Conference on Uncertainty in Artificial Intelligence, pages 338?345.
D. Kauchak. 2013. Improving text simplification language modeling using unsimplified text data. In Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
1537?1546, Sofia, Bulgaria, August. Association for Computational Linguistics.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. 2001. Improvements to Platt?s SMO
Algorithm for SVM Classifier Design. Neural Computation, 13(3):637?649.
S. T. Kover, E. Haebig, A. Oakes, A. McDuffie, R. J. Hagerman, and L. Abbeduto. 2012. Syntactic comprehension
in boys with autism spectrum disorders: Evidence from specific constructions. In Proceedings of the 2012
International Meeting for Autism Research, Athens, Greece. International Society for Autism Research.
J. Martos, S. Freire, A. Gonz?alez, D. Gil, R. Evans, V. Jordanova, A. Cerga, A. Shishkova, and C. Orasan. 2013.
FIRST Deliverable - User preferences: Updated. Technical Report D2.2, Deletrea, Madrid, Spain.
C. Napoles and M. Dredze. 2010. Learning simple wikipedia: a cogitation in ascertaining abecedarian language.
In Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing: Writing Pro-
cesses and Authoring Aids, CL&W ?10, pages 42?50, Stroudsburg, PA, USA. Association for Computational
Linguistics.
S. E. Petersen and M. Ostendorf. 2007. Text Simplification for Language Learners: A Corpus Analysis. In
Proceedings of Workshop on Speech and Language Technology for Education.
R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.
H. Saggion, E. G?omez Mart??nez, E. Etayo, A. Anula, and L. Bourg. 2011. Text Simplification in Simplext:
Making Text More Accessible. Revista de la Sociedad Espa?nola para el Procesamiento del Lenguaje Natural,
47:341?342.
H. Tager-Flusberg. 1981. Sentence comprehension in autistic children. Applied Psycholinguistics, 2:1:5?24.
M. Tattamanti, R. Manenti, P. A. Della Rosa, A. Falini, D. Perani, S. Cappa, and A. Moro. 2008. Negation in the
brain: Modulating action representations. NeuroImage, 43 (2008):358?367.
UN. 2006. Convention on the rigths of persons with disabilities.
F. R. Volkmar and L. Wiesner. 2009. A Practical Guide to Autism. Wiley, Hoboken, NJ, 2nd edition.
S.
?
Stajner and H. Saggion. 2013. Adapting Text Simplification Decisions to Different Text Genres and Target
Users. Procesamiento del Lenguaje Natural, 51:135?142.
S.
?
Stajner, B. Drndarevi?c, and H. Saggion. 2013. Corpus-based Sentence Deletion and Split Decisions for Spanish
Text Simplification. Computaci?on y Systemas, 17(2):251?262.
I. H. Witten and E. Frank. 2005. Data mining: practical machine learning tools and techniques. Morgan Kauf-
mann Publishers.
K. Woodsend and M. Lapata. 2011. Learning to Simplify Sentences with Quasi-Synchronous Grammar and
Integer Programming. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language
Processing (EMNLP).
S. Wubben, A. van den Bosch, and E. Krahmer. 2012. Sentence simplification by monolingual machine transla-
tion. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 1015?1024, Stroudsburg, PA, USA. Association for Computational Linguistics.
M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, and L. Lee. 2010. For the sake of simplicity: unsupervised
extraction of lexical simplifications from wikipedia. In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, HLT ?10, pages
365?368, Stroudsburg, PA, USA. Association for Computational Linguistics.
Z. Zhu, D. Berndard, and I. Gurevych. 2010. A Monolingual Tree-based Translation Model for Sentence Sim-
plification. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),
pages 1353?1361.
63
