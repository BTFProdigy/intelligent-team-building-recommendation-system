PartslD: A Dialogue-Based System for Identifying Parts for Medical 
Systems 
Amit BAGGA, Tomek STRZALKOWSKI, and G. Bowden WISE 
Information Technology Laboratory 
GE Corporate Research and Development 
1 Research Circle 
Niskayuna, USA, NY 12309 
{ bagga, strzalkowski, wisegb } @crd.ge.com 
Abstract 
This paper describes a system that 
provides customer service by allowing 
users to retrieve identification umbers of 
parts for medical systems using spoken 
natural language dialogue. The paper also 
presents an evaluation of the system 
which shows that the system successfully 
retrieves the identification numbers of 
approximately 80% of the parts. 
Introduction 
Currently people deal with customer service 
centers either over the phone or on the world 
wide web on a regular basis. These service 
centers upport a wide variety of tasks including 
checking the balance of a bank or a credit card 
account, transferring money from one account o 
another, buying airline tickets, and filing one's 
income tax returns. Most of these customer 
service centers use interactive voice response 
(IVR) systems on the front-end for determining 
the user's need by providing a list of options that 
the user can choose from, and then routing the 
call appropriately. The IVRs also gather 
essential information like the user's bank 
account number, social security number, etc. 
For back-end support, the customer service 
centers use either specialized computer systems 
(example: a system that retrieves the account 
balance from a database), or, as in most cases, 
human operators. 
However, the IVR systems are unwieldy 
to use. Often a user's needs are not covered by 
the options provided by the system forcing the 
user to hit 0 to transfer to a human operator. In 
addition, frequent users often memorize the 
sequence of options that will get them the 
desired information. Therefore, any change in 
the options greatly inconveniences these users. 
Moreover, there are users that always hit 0 to 
speak to a live operator because they prefer to 
deal with a human instead of a machine. 
Finally, as customer service providers continue 
to rapidly add functionality to their IVR 
systems, the size and complexity of these 
systems continues to grow proportionally. In 
some popular systems like the IVR system that 
provides customer service for the Internal 
Revenue Service (IRS), the user is initially 
bombarded with 10 different options with each 
option leading to sub-menus offering a further 3- 
5 options, and so on. The total number of nodes 
in the tree corresponding to the IRS' IVR system 
is quite large (approximately 100) making it 
extremely complex to use. 
Some customer service providers have 
started to take advantage of the recent advances 
in speech recognition technology. Therefore, 
some of the IVR systems now allow users to say 
the option number (1, 2, 3 . . . . .  etc.) instead of 
pressing the corresponding button. In addition, 
some providers have taken this a step further by 
allowing users to say a keyword or a phrase 
from a list of keywords and/or phrases. For 
example, AT&T, the long distance company, 
provides their users the following options: 
"Please say information for information on 
placing a call, credit for requesting credit, or 
operator to speak to an operator." 
However, given the improved speech 
recognition technology, and the research done in 
natural anguage dialogue over the last decade, 
there exists tremendous potential in enhancing 
29 
these customer service centers by allowing users 
to conduct a more natural human-like dialogue 
with an automated system to provide a 
customer-friendly s stem. In this paper we 
describe a system that uses natural language 
dialogue to provide customer service for a 
medical domain. The system allows field 
engineers to call and obtain identification 
numbers of parts for medical systems using 
natural language dialogue. We first describe 
some work done previously in using natural 
language dialogue for customer service 
applications. Next, we present he architecture 
of our system along with a description of each of 
the key components. Finally, we conclude by 
providing results from an evaluation of the 
system. 
1. Previous Work 
As mentioned earlier, some customer service 
centers now allow users to say either the option 
number or a keyword from a list of 
options/descriptions. However, the only known 
work which automates part of a customer service 
center using natural language dialogue is the one 
by Chu-Carroll and Carpenter (1999). The 
system described here is used as the front-end of 
a bank's customer service center. It routes calls 
by extracting key phrases from a user utterance 
and then by statistically comparing these phrases 
to phrases extracted from utterances in a training 
corpus consisting of pre-recorded calls where 
the routing was done by a human. The call is 
routed to the destination of the utterance from 
the training corpus that is most "similar" to the 
current utterance. On occasion, the system will 
interact with the user to clarify the user's request 
by asking a question. For example, if the user 
wishes to reach the loan department, the system 
will ask if the loan is for an automobile, or a 
home. Other related work is (Georgila et al, 
1998). 
While we are aware of the work being 
done by speech recognition companies like 
Nuance (www.nuance.com) and Speechworks 
(www.speechworks.com) in the area of 
providing more natural anguage dialogue-based 
customer service, we are not aware of any 
conference or journal publications from them. 
Some magazine articles which mention their 
work are (Rosen 1999; Rossheim 1999; 
Greenemeier 1999 ; Meisel 1999). In addition, 
when we tried out a demo of Nuance's ystems, 
we found that their systems had a very IVRish 
feel to them. For example, if one wanted to 
transfer $50 from one account o another, the 
system would first ask the account that the 
money was coming from, then the account hat 
the money was going to, and finally, the amount 
to be transferred. Therefore, a user could not 
say "I want to transfer $50 from my savings 
account o my checking account" and have the 
system conduct that transaction. 
In addition to the works mentioned above, 
there have been several classic projects in the 
area of natural language dialogue like 
TRAINS/TRIPS project at Rochester (Allen et 
al., 1989, 1995, 1996), Duke's Circuit-Fixit- 
Shoppe and Pascal Tutoring System (Biermann 
et al, 1997; 1995), etc. While the Circuit-Fixit- 
Shoppe system helps users fix a circuit through a
dialogue with the system, the TRIPS and the 
TRAINS projects allow users to plan their 
itineraries through dialogue. Duke's Pascal 
tutoring system helps students in an introductory 
programming class debug their programs by 
allowing them to analyze their syntax errors, get 
additional information on the error, and learn the 
correct syntax. Although these systems have 
been quite successful, they use detailed models 
of the domain and therefore cannot be used for 
diverse applications uch as the ones required 
for customer service centers. Other related work 
on dialogue include (Carberry, 1990; Grosz and 
Sidner, 1986; Reichman, 1981). 
2. PartslD: A System for Identification 
of Parts for Medical Systems 
Initially, we were approached by the medical 
systems business of our company for help in 
reducing the number of calls handled by human 
operators at their call center. An analysis of the 
types of customer service provided by their call 
center showed that a large volume of calls 
handled by their operators were placed by field 
engineers requesting identification umbers of 
parts for various medical systems. The ID 
numbers were most often used for ordering the 
corresponding parts using an automated IVR 
system. Therefore, the system we have built 
30 
Figure 1. PartslD System Architecture 
W 
I Parser l 
~ User 
Dia logue  Manager  
F . , .  
pros entetion 
helps automate some percentage of these calls 
by allowing the engineer to describe a part using 
natural language. The rest of this section 
describes our system in detail. 
2.1 Data 
The database we used for our system was the 
same as the one used by the operators at the call 
center. This database consists of the most 
common parts and was built by the operators 
themselves. However, the data contained in the 
database is not clean and there are several types 
of errors including mis-spellings, use of non- 
standard abbreviations, use of several different 
abbreviations for the same word, etc. 
The database consists of approximately 
7000 different parts. For each part, the database 
contains its identification umber, a description, 
and the product (machine type) that it is used in. 
The descriptions consist of approximately 
60,000 unique words of which approximately 
3,000 are words which either are non-standard 
abbreviations or are unique to the medical 
domain (example: collimator). 
Due to the large size of the database, we 
did not attempt to clean the data. However, we 
did build several data structures based on the 
database which were used by the system. The 
primary data structures built were two inverted 
hash tables corresponding to the product, and the 
part description fields in the database. The 
inverted hash tables were built as follows: 
1) Each product and part description field 
was split into words. 
2) Stop-words (words containing no 
information like: a, the, an, etc.) were 
filtered. 
3) Each remaining word was inserted as the 
index of the appropriate hash table with 
the identification number of the part 
being the value corresponding to the 
index. 
Therefore, for each non-stop-word word used in 
describing a part, the hash table contains a list of 
all the parts whose descriptions contained that 
word. Similarly, the products hash table 
contains a list of all parts corresponding to each 
product word. 
2.2 System Architecture 
The architecture of the system is shown in 
Figure 1. The system was designed in a manner 
such that it could be easily ported from one 
application to another with minimal effort other 
than providing the domain-specific knowledge 
regarding the new application. Therefore, we 
decided to abstract away the domain-specific 
information into self-contained modules while 
keeping the other modules completely 
independent. The domain-specific modules are 
shown in the dark shaded boxes in Figure I. 
The remainder of this section discusses each of 
the modules hown in the system architecture. 
2.2.1 The Speech Recognition System (ASR) 
Since customer service centers are meant o be 
used by a variety of users, we needed a user- 
independent speech recognition system. In 
31 
addition, since the system could not restrict he 
manner in which a user asked for service, the 
speech recognition system could not be 
grammar-based. Therefore, we used a general 
purpose dictation engine for the system. The 
dictation system used was Lernout & Hauspie's 
VoiceXPress ystem (www.lhs.com). Although 
the system was general purpose, we did provide 
to it the set of keywords and phrases that are 
commonly used in the domain thereby enabling 
it to better recognize these domain-specific 
keywords and phrases. The keywords and 
phrases used were simply the list of descriptions 
and product names corresponding to each part in 
the database. It should be noted that the set of 
domain-specific keywords and phrases was 
provided to the speech recognition system as a 
text document. In other words, the training was 
not done by a human speaking the keywords and 
phrases into the speech recognition system. In 
addition, the speech recognition system is far 
from perfect. The recognition rates hover 
around 50%, and the system has additional 
difficulty in identifying product names which 
are most often words not found in a dictionary 
(examples: 3MlaserCam, 8000BUCKY, etc.). 
2.2.2 Parser and the Lexicon 
The parser is domain-driven i the sense that it 
uses domain-dependent information produced by 
the lexicon to look for information, in a user 
utterance, that is useful in the current domain. 
However, it does not attempt to understand fully 
each user utterance. It is robust enough to 
handle ungrammatical sentences, hort phrases, 
and sentences that contain mis-recognized text. 
The lexicon, in addition to providing 
domain-dependent keywords and phrases to the 
parser, also provides the semantic knowledge 
associated with each keyword and phrase. 
Therefore, for each content word in the inverted 
hash tables, the lexicon contains entries which 
help the system determine whether the word was 
used in a part description, or a product name. In 
addition, the lexicon also provides the semantic 
knowledge associated with the pre-specified 
actions which can be taken by the user like 
"operator" which allows the user to transfer to 
an operator, and "stop," or "quit" which allow 
the user to quit the system. Some sample ntries 
are: 
collimator => (description_word, collimator) 
camera => (product_word, camera) 
operator => (user action, operator) 
etc. 
The parser scans a user utterance and 
returns, as output, a list of semantic tuples 
associated with each keyword/phrase contained 
in the utterance. It is mainly interested in "key 
words" (words that are contained in product and 
part descriptions, user action words, etc.) and it 
ignores all the other words in the user utterance. 
The parser also returns a special tuple containing 
the entire input string which may be used later 
by the context-based parser for sub-string 
matching specially in cases when the DM has 
asked a specific question to the user and is 
expecting a particular kind of response. 
2.2.3 The Filler and Template Modules 
The filler takes as input the set of tuples 
generated by the parser and attempts to check 
off templates contained in the templates module 
using these tuples, The set of templates in the 
templates module contains most of remaining 
domain-specific knowledge required by the 
system. Each template is an internal 
representation of a part in the database. It 
contains for each part, its ID, its description, and 
the product which contains it. In addition, there 
are several additional templates corresponding to
pre-specified user actions like "operator," and 
"quit." A sample template follows: 
tl__I = ( 
'product' = > 'SFD', 
'product__ids' = > 2229005" 
'product_descriptions' => 'IR RECEIVER PC 
BOARD CI104 BISTABLE MEMORY') 
For each tuple input from the parser, the 
filler checks off the fields which correspond to 
the tuple. For example, if the filler gets as input 
(description_word, collimator), it checks off the 
description fields of those templates containing 
collimator as a word in the field. A template is 
checked off iff one or more of its fields is 
checked off. In addition, the filler also 
maintains a list of all description and product 
words passed through the tuples (i.e. these words 
32
have been uttered by the user). These two lists 
are subsequently passed to the dialogue 
manager. 
Although the filler does not appear to be 
very helpful for the current application domain, 
it is an important part of the architecture for 
other application domains. For example, the 
current PartslD system is a descendant from an 
earlier system which allowed users to process 
financial transactions where the filler was 
instrumental in helping the dialogue manager 
determine the type of transaction being carried 
out by the user (Bagga et al, 2000). 
2.2.4 The Dialogue Manager (DM) 
The DM receives as input from the filler the set 
of templates which are checked off. In addition, 
it also receives two lists containing the list of 
description words, and product word uttered by 
the user. The DM proceeds using the following 
algorithm: 
1) It first checks the set of checked off 
templates input from the filler. If there is 
exactly one template in this set, the DM asks 
the user to confirm the part that the template 
corresponds to. Upon receipt of the 
confirmation from the user, it returns the 
identification number of the part to the user. 
2) Otherwise, for each description word uttered 
by the user, the DM looks up the set of parts 
(or templates) containing the word from the 
descriptions inverted hash table. It then 
computes the intersection of these sets. If 
the intersection is empty, the DM computes 
the union of these sets and proceeds treating 
the union as the intersection. 
3) If the intersection obtained from (2) above 
contains exactly one template, the DM asks 
the user to confirm the part corresponding to
the template as in (1) above. 
4) Otherwise, the DM looks at the set of 
product words uttered by the user. If this set 
is empty, the DM queries the user for the 
product name. Since the DM is expecting a
product name here, the input provided by the 
user is handled by the context-based parser. 
Since most product names consist of non- 
standard words consisting of alpha-numeric 
characters (examples: AMX3, 
8000BUCKY, etc.), the recognition quality 
is quite poor. Therefore, the context-based 
parser anks the input received from the user 
using a sub-string matching algorithm that 
uses character-based unigram and bigram 
counts (details are provided in the next 
section). The sub-string matching algorithm 
greatly enhances the performance of the 
system (as shown in the sample dialogue 
below). 
5) If the set of product words is non-empty, or 
if the DM has successfully queried the user 
for a product name, it extracts the set of 
parts (templates) containing each product 
word from the product words inverted hash 
table. It then computes an intersection of 
these sets with the intersection set of 
description words obtained from (2) above. 
The resulting intersection is the joint product 
and description i tersection. 
6) If the joint intersection has exactly one 
template, the DM proceeds as in (1) above. 
Alternatively, if the number of templates in 
the joint intersection is less than 4, the DM 
lists the parts corresponding toeach of these 
and asks the user to confirm the correct one. 
7) If there are more than 4 templates in the 
joint intersection, the DM ranks the 
templates based upon word overlap with the 
description words uttered by the user. If the 
number of resulting top-ranked templates i
less than 4, the DM proceeds as in the 
second half of (6) above. 
8) If the joint intersection is empty, or in the 
highly unlikely case of there being more 
than 4 top-ranked templates in (7), the DM 
asks the user to enter additional 
disambiguating information. 
The goal of the DM is to hone in on the part 
(template) desired by the user, and it has to 
determine this from the set of templates input to 
it by the filler. It has to be robust enough to deal 
with poor recognition quality, inadequate 
information input by the user, and ambiguous 
data. Therefore, the DM is designed to handle 
these issues. For example, description words 
that are mis-recognized as other description 
words usually cause the intersection of the sets 
of parts corresponding to these words to be 
empty. The DM, in this case, takes a union of 
the sets of parts corresponding to the description 
333333
words thereby ensuring that the template 
corresponding tothe desired part is in the union. 
The DM navigates the space of possibilities 
by first analyzing the intersection of the sets of 
parts corresponding to the description words 
uttered by the user. If no unique part emerges, 
the DM then checks to see if the user has 
provided any information about the product hat 
the part is going to be used in. If no product was 
mentioned by the user, the DM queries the user 
for the product name. Once this is obtained, the 
DM then checks to see if a unique part 
corresponds to the product name and the part 
description provided by the user. If no unique 
part emerges, then the DM backs off and asks 
the user to re-enter the part description. 
Alternatively, if more than one part corresponds 
to the specified product and part description, 
then the DM ranks the parts based upon the 
number of words uttered by the user. 
Obviously, since the DM in this case uses a 
heuristic, it asks the user to confirm the part that 
ranks the highest. If more than one (although 
less than 4) parts have the same rank, then the 
DM explicitly lists these parts and asks the user 
to specify the desired part. It should be noted 
that the DM has to ensure that the information it
receives is actually what the user meant. This is 
especially true when the DM uses heuristics, and 
sub-string matches (as in the case of product 
names). Therefore, the DM occasionally asks 
the user to confirm input it has received. 
2.2.5 The Sub-String Matching Algorithm 
When the dialogue manager is expecting a 
certain type of input (examples : product names, 
yes/no responses) from the user, the user 
response is processed by the context-based 
parser. Since the type of input is known, the 
context-based parser uses a sub-string matching 
algorithm that uses character-based unigram and 
bigram counts to match the user input with the 
expectation of the dialogue manager. Therefore, 
the sub-string matching module takes as input a 
user utterance string along with a list of 
expected responses, and it ranks the list of 
expected responses based upon the user 
response. Listed below are the details of the 
algorithm : 
1) The algorithm first concatenates the words 
of the user utterance into one long string. 
This is needed because the speech 
recognition system often breaks up the 
utterance into words even though a single 
word is being said. For example, the 
product name AMXl l0  is often broken up 
into the string 'Amex 110'. 
2) Next, the algorithm goes through the string 
formed in (1) and compares this character by 
character with the list of expected responses. 
It assigns one point for every common 
character. Therefore, the expected response 
'AMX3' gets three points for the utterance 
'Amex110'. 
3) The algorithm then compares the user 
utterance with the list of expected responses 
using 2 characters (bigrams) at a time. It 
assigns 2 points for each bigram match. For 
the example shown in (2), there are two 
bigram matches: the first is that the 
utterance starts with an 'A' (the previous 
character is this case is the null character), 
and the second is the bigram 'AM'. 
4) The algorithm now compares the length of 
the user utterance string and the expected 
response. If the length of the two strings is 
the same, then it assigns 2 points to the 
expected response. 
5) Finally, the algorithm calculates the number 
of unique characters in the expected 
response, and the user utterance string. If 
these characters are the same, then it assigns 
4 points to the expected response. 
The expected response which has the highest 
number of points is the most likely one. If two 
or more expected responses have the same 
number of points, then the system asks the user 
to confh'm the correct one. 
While we have not evaluated this sub- 
string matching algorithm independently, a brief 
evaluation in the context of the system resulted 
in about 90% accuracy. 
2.2.6 The Presentation Module 
The presentation module works in one of two 
possible modes: over the phone, and over the 
web. This module takes as input a string 
generated by the question-generation module 
and presents this string to the user in the 
appropriate mode of communication. If the 
speech option for the system is turned on, the 
speech-based output is generated using Lernout 
"~ld. 34
and Hauspie's RealSpeak text-to-speech system. 
Although the system currently cannot use both 
modes of communication simultaneously, we 
plan to incorporate this feature sometime in the 
future. 
2.2. 7 Robustness of  the System 
As with any dialogue system, it is extremely 
important for the system to be robust. Our 
system has the following two features which 
make it extremely robust: 
1) The user can, at any given moment, 
say operator to transfer to an 
operator, quit~stop to exit, and 
back~restart to start afresh. 
2) When expecting a response from the 
user, if the system does not receive an 
expected input, it repeats the question 
at most twice before transferring 
control to an operator. 
3. Sample Dialogues 
This section gives examples of two sample 
dialogues that occur during the testing of the 
system. The system's response is indicated by 
"S>", the user's response by "U>", and the 
recognition of the user's response by "V>". 
S> Welcome to the PartslD Center. Please tell 
me the part that you want the ID for. 
U> I need the ID for  PCB Control. 
V> I need the I the for PCB home thrown. 
S> What product do you need this for? 
U> COMPAX 40E 
V> home fax for a O0 E 
S> Is COMPAX 40E the product name that you 
said? 
U> yes 
V> gas 
S> Do you want the ID for PCB Display SID or 
the PCB Control? 
U> The second one. 
V> The second home. 
S> Do you need the ID for PCB Control used in 
product COMPAX 40E? 
U> yes 
V> yes 
S> The ID for PCB Control used in product 
COMPAX 40E is 2168983. Do you wan to exit 
or f ind the ID for another part ? 
U> look up another part. 
S> Welcome to the PartslD Center. Please tell 
me the part that you want the ID for. 
U> I need cable data for cradle 2M long. 
V> I needed cable date them for cradle to be 
loan. 
S> Do you need this part for a ANGIXM2004, 
or a AFM ? 
U> AFM 
V> a fan 
S> Is AFM the product name that you said? 
U> yes 
V> yes 
S> Do you want the ID for Cable Data for 
Cradle 2M long ? 
U> yes 
V> yes 
S> The ID for  Cable Data for Cradle 2M long 
used in product AFM is 45438628. Do you wan 
to exit or f ind the ID for another part? 
U> look up another part. 
4. Evaluation and Results 
The goal of our evaluation was to ensure that the 
system helped a user successfully identify parts 
irrespective of the performance of the speech 
recognition engine for the user. In other words, 
we wanted to see if the system was robust 
enough to conduct transactions with a diverse 
mix of users. We tested the system with 4 
different users two of whom had foreign accents. 
For each user, we randomly selected 20 parts 
from the database. The results are summarized 
in Table 1. 
These results show that the system was 
quite successful in handling requests from users 
with a variety of accents achieving varying 
recognition rates. Out of the 80 parts tested, 
only twice did the user feel that he/she had to 
transfer to an operator. The system successfully 
retrieved the identification umbers of 79% of 
the parts while transferring 19% of the cases to a 
human operator because of extremely bad 
:$5 
User Parts 
successfully 
identified 
15 
Calls system 
transfers to 
operator 
3 
Calls user 
transfers to 
operator 
2 
System 
prompts per 
call 
3.7 
Relevant words 
recognized per 
part 
2.5 
18 2 0 3 2.35 
13 7 0 2.5 1.65 
17 3 0 2.9 2.7 
Table 1: Summary of Results 
recognition. We are planning on conducting a
more elaborate test which a larger set of users. 
Conclusions 
In this paper we have described a robust system 
that provides customer service for a medical 
parts application. The preliminary results are 
extremely encouraging with the system being 
able to successfully process approximately 80% 
of the requests from users with diverse accents. 
Acknowledgements 
We wish to thank the GE Medical Systems team 
of Todd Reinke, Jim Tierney, and Lisa 
Naughton for providing support and funding for 
this project. In addition, we also wish to thank 
Dong Hsu of Lernout and Hauspie for his help 
on the ASR and the text-to-speech systems. 
Finally, we wish to thank the Information 
Technology Laboratory of GE CRD for 
providing additional funding for this project. 
References 
Allen, J. F. et al (1995) The TRAINS Project: A 
case study in building a conversational p anning 
agent. Journal of Experimental nd Theoretical AI, 
(7) 7-48. 
Allen, J. F., Miller, B. W.; Ringer, E. K.; and 
Sikorski, T. (1996) A Robust System for Natural 
Spoken Dialogue. 34th Annual Meeting of the 
ACL, Santa Cruz, 62-70. 
Bagga, A., Stein G. C., and Strzalkowski, T. (2000) 
FidelityXPress: A Multi-Modal System for 
Financial Transactions. Proceedings of the 6 a~ 
Conference on Content-Based Multimedia 
Information Access (RIAO'00). 
Biermann, A.W.; Rodman, R.; Rubin, D.; and 
Heidlage, J.R. (1985) Natural language with 
discrete speech as a mode for human to machine 
communication. Communication of the ACM 
18(6): 628-636. 
Biermann, Alan W.; Guinn, Curry I.; Fulkerson, M.: 
Keim, G.A.; Liang, Z.; Melamed, D.M.; and 
Rajagopalan, K. (1997) Goal-orientedMultimedia 
Dialogue with Variable Initiative. Lecture Notes in 
Artificial Intelligence 1325; Springer-Verlag, New 
York; pp. 1-16. 
Carberry, S. (1990) Plan Recognition in Natural 
Language Dialogue. Cambridge, Mass.: The MIT 
Press. 
Chu-Carroll, J, and R. Carpenter. (1999) Vector- 
Based Natural Language Call Routing. Journal of 
Computational Linguistics, 25(30), pp. 361-388. 
Georgila, K., A.Tsopanoglou, N.Fakotakis and 
G.Kokkinakis. (1998) An Integrated Dialogue 
System for the Automation of Call Centre Services. 
ICLSP'98, 5th International Conference on Spoken 
Language Processing, Sydney, Australia. 
Grosz, B.J. and Sidner, C.L. (1986) Attentions, 
intentions, and the structure of discourse. 
Computational Linguistics 12(3): 175-204. 
Greenemeier, L. (1999) Voice-Recognition 
Technology Builds a Following. Information 
Week, December 13. 
Meisel, W. (1999) Can Speech Recognition Give 
Telephones a New Face? Business 
Communications Review, November 1. 
Reichman, R.. (1981) Plain-speaking: A theory and 
grammar of spontaneous discourse. PhD thesis, 
Department of Computer Science, Harvard 
University, Cambridge, Massachusetts. 
Rosen, C. (1999) Speech Has Industry Talking. 
Business Travel News, November. 
Rossheim, J. (1999) Giving Voice to Customer 
Service. Datamation, November 1. 
36 
Analyzing the Reading Comprehension Task 
Amit  Bagga  
GE Corporate Research and Development 
1 Research Circle 
Niskayuna, NY 12309 
bagga@crd.ge, corn 
Abst rac t  
In this paper we describe a method for analyzing 
the reading comprehension task. First, we describe 
a method of classifying facts (information) into cat- 
egories or levels; where each level signifies a different 
degree of difficulty of extracting a fact from a piece 
of text containing it. We then proceed to show how 
one can use this model the analyze the complexity 
of the reading comprehension task. Finally, we ana- 
lyze five different reading comprehension tasks and 
present results from this analysis. 
1 In t roduct ion  
Recently there has been a spate of activity for build- 
ing question-answering systems (QA systems) driven 
largely by the recently organized QA track at the 
Eighth Text Retrieval Conference (TREC-8) (Har- 
man, 1999). This increase in research activity has 
also fueled research in a related area: building Read- 
ing Comprehension systems (Hirschman and others, 
1999). But while a number of successful systems 
have been developed for each of these tasks, little, 
if any, work has been done on analyzing the com- 
plexities of the tasks themselves. In this paper we 
describe a method of classifying facts (information) 
into categories or levels; where each level signifies 
a different degree of difficulty of extracting a fact 
from a piece of text containing it. We then proceed 
to show how one can use this model the analyze the 
complexity of the reading comprehension task. Fi- 
nally, we analyze five different reading comprehen- 
sion tasks and present results from this analysis. 
2 The  Complex i ty  o f  Ext rac t ing  a 
Fact  F rom Text  
Any text document is a collection of facts (infor- 
mation). These facts may be explicitly or implicitly 
stated in the text. In addition, there are "easy" facts 
which may be found in a single sentence (example: 
the name of a city) as well as "difficult" facts which 
are spread across several sentences (example: the 
reason for a particular event). 
For a computer system to be able to process text 
documents in applications like information extrac- 
35 
tion (IE), question answering, and reading compre- 
hension, it has to have the ability to extract facts 
from text. Obviously, the performance of the system 
will depend upon the type of fact it has to extract: 
explicit or implicit, easy or difficult, etc. (by no 
means is this list complete). In addition, the perfor- 
mance of such systems varies greatly depending on 
various additional factors including known vocabu- 
lary, sentence l ngth, the amount of training, quality 
of parsing, etc. Despite the great variations in the 
performances of such systems, it has been hypothe- 
sized that there are facts that are simply harder to 
extract han others (Hirschman, 1992). 
In this section we describe a method for estimat- 
ing the complexity of extracting a fact from text. 
The proposed model was initially used to analyze the 
information extraction task (Bagga and Biermann, 
1997). In addition to verifying Hirschman's hypoth- 
esis, the model  also provided us with a framework 
for analyzing and understanding the performance of 
several IE systems (Bagga and Biermann, 1998). We 
have also proposed using this model to analyze the 
complexity of the QA task Which is related to both 
the IE, and the reading comprehension tasks (Bagga 
et al, 1999). The  remainder of this section describes 
the model in detail, and provides a sample applica- 
tion of the model  to an IE task. In the following 
section, we discuss how this model  can be used to 
analyze the reading comprehension task. 
2.1 Def in i t ions  
Network :  
A network consists of a collection of nodes intercon- 
nected by an accompanying set of arcs. Each node 
denotes an object and each arc represents a binary 
relation between the objects. (Hendrix, 1979) 
A Part ia l  Network :  
A partial network is a collection of nodes intercon- 
nected by an accompanying set of arcs where the 
collection of nodes is a subset of a collection of nodes 
forming a network, and the accompanying set of arcs 
is a subset of the se.t of arcs accompanying the set 
of nodes which form the network. 
of 
onsibilit~ 
/ of 
on O~ El Espectador ) 
Figure 1: A Sample Network 
Figure 1 shows a sample network for the following 
piece of text: 
"The Extraditables," orthe Armed Branch 
of the Medellin Cartel have claimed respon- 
sibility for the murder of two employees of 
Bogota's daily E1 Espectador on Nov 15. 
The murders took place in Medellin. 
2.2 The Level of A Fact 
The level of a fact, F, in a piece of text is defined 
by the following algorithm: 
1. Build a network, S, for the piece of text. 
2. Identify the nodes that are relevant to the fact, 
F. Suppose {xl,x~,... ,Xn} are the nodes rel- 
evant to F. Let s be the partial network con- 
sisting of the set of nodes {xl, x~,.. . ,  x~} inter- 
connected by the set of arcs {tl, t2, . . . ,  tk}. 
We define the level of the fact, F, with respect to 
the network, S to be equal to k, the number of 
arcs linking the nodes which comprise the fact 
F ins .  
2.2.1 Observations 
Given the definition of the level of a fact, the follow- 
ing observations can be made: 
? The level of a fact is related to the concept 
of "semantic vicinity" defined by Schubert et. 
al. (Schubert and others, 1979). The semantic 
vicinity of a node in a network consists of the 
nodes and the arcs reachable from that node by 
traversing a small number of arcs. The funda- 
mental assumption used here is that "the knowl- 
edge required to perform an intellectual task 
generally lies in the semantic vicinity of the con- 
cepts involved in the task" (Schubert and oth- 
ers, 1979). 
The level of a fact is equal to the number of 
arcs that one needs to traverse to reach all the 
concepts (nodes) which comprise the fact of in- 
terest. 
? A level-0 fact consists of a single node (i.e. no 
transitions) in a network. 
? A level-k fact is a union of k level-1 facts: 
? Conjunctions/disjunctions increase the level of 
a fact. 
? A higher level fact is likely to be harder to ex- 
tract than a lower level fact. 
? A fact appear ing at one level in a piece of text 
may appear  at some other level in the same 
piece of text. 
? The  level of a fact in a piece of text depends  
on the granularity of the network  constructed 
for that piece of text. Therefore, the level of a 
fact with respect to a network  built at the word  
level (i.e. words  represent objects and  the re- 
lationships between the objects) will be greater 
than the level of a fact with respect to a network  
built at the phrase level (i.e. noun  groups repre- 
sent objects while verb groups and  preposition 
groups represent the relationships between the 
objects). 
2.2.2 Examples 
Let S be the network shown in Figure 1. S has been 
built at the phrase level. 
? The city mentioned, in S, is an example of a 
level-0 fact because the "city" fact consists only 
of one node "Medellin." 
? The type of attack, in S, is an example of a 
level-1 fact. 
36 
We define the type o/attack in the network to be 
an attack designator such as "murder, .... bomb- 
ing," or "assassination" with one modifier giv- 
ing the victim, perpetrator, date, location, or 
other information. 
In this case the type of attack fact is composed 
of the "the murder" and the "two employees" 
nodes and their connector. This makes the type 
of attack a level-1 fact. 
The type of attack could appear as a level-0 fact 
as in "the Medellin bombing" (assuming that 
the network is built at the phrase level) because 
in this case both the attack designator (bomb- 
ing) and the modifier (Medellin) occur in the 
same node. The type of attack fact occurs as a 
level-2 fact in the following sentence (once again 
assuming that the network is built at the phrase 
level): "10 people were killed in the offensive 
which included several bombings." In this case 
there is no direct connector between the attack 
designator (several bombings) and its modifier 
(10 people). They are connected by the inter- 
mediatory "the offensive" node; thereby making 
the type of attack a level-2 fact. The type of at- 
tack can also appear at higher levels. 
? In S, the date of the murder of the two employ- 
ees is an example of a level-2 fact. 
This is because the attack designator (the tour- 
der) along with its modifier (two employees) ac- 
count for one level and the arc to "Nov 15" ac- 
counts for the second level. 
The date of the attack, in this case, is not a 
level-1 fact (because of the two nodes "the tour- 
der" and "Nov 15") because the phrase "the 
murder on Nov 15" does not tell one that an at- 
tack actually took place. The article could have 
been talking about a seminar on murders that 
took place on Nov 15 and not about the murder 
of two employees which took place then. 
? In S, the location of the murder of the two em- 
ployees is an example of a level-2 fact. 
The exact same argument as the date of the 
murder of' the two employees applies here. 
? The complete information, in S, about the vic- 
tiros is an example of a level-2 fact because to 
know that two employees of Bogota's Daily E1 
Espectador were victims, one has to know that 
they were murdered. The attack designator (the 
murder) with its modifier (two employees) ac- 
counts for one level, while the connector be- 
tween "two employees" and "Bogota's Daily E1 
Espectador" accounts for the other. 
2.3 Bu i ld ing  the Networks  
As mentioned earlier, the level of a fact for a piece 
of text depends on the network constructed for the 
text. Since there is no unique network corresponding 
to a piece of text, care has to be taken so that the 
networks are built consistently. 
We used the following algorithm to build the net- 
works: 
1. Every article was broken up into a non- 
overlapping sequence of noun groups (NGs), 
verb groups (VGs), and preposition groups 
(PGs). The rules employed to identify the NGs, 
VGs, and PGs were almost he same as the ones 
employed by SRI's FASTUS system 1. 
2. The nodes of the network consisted of the NGs 
while the transitions between the nodes con- 
sisted of the VGs and the PGs. 
3. Identification of coreferent nodes and preposi- 
tional phrase attachments were done manually. 
The networks are built based largely upon the syn- 
tactic structure of the text contained in the articles. 
However, there is some semantics encoded into the 
networks because identification of coreferent nodes 
and preposition phrase attachments are done manu- 
ally. 
Obviously, if one were to employ a different al- 
gorithm for building the networks, one would get 
different numbers for the level of a fact. But, if the 
algorithm were employed consistently across all the 
facts of interest and across all articles in a domain, 
the numbers on the level of a fact would be consis- 
tently different and one would still be able to analyze 
the relative complexity of extracting that fact from 
a piece of text in the domain. 
3 Example: Analyzing the 
Complex i ty  of  an In fo rmat ion  
Ext rac t ion  Task 
In order to validate our model of complexity we ap- 
plied it to the Information Extraction (IE) task, 
or the Message Understanding task (DAR, 1991), 
(DAR, 1992), (ARP, 1993), (DAR, 1995), (DAR, 
1998). The goal of an IE task is to extract pre- 
specified facts from text and fill in predefined tem- 
plates containing labeled slots. 
We analyzed the complexity of the task used 
for the Fourth Message Understanding Conference 
(MUC-4) (DAR, 1992). In this task, the partici- 
pangs were asked to extract he following facts from 
articles describing terrorist activities in Latin Amer- 
ica: 
? The type of attack. 
? The date of the attack. 
? The location of the attack. 
1We wish to thank Jerry Hobbs of SRI for providing us 
with the rules of their partial parser. 
37 
LL  
E= 
z 
110 
105 
100 
95 
9O 
85 
8O 
75 
7O 
65 
60 
55 
5O 
45 
40 
35 
3O 
25 
20 
15 
10 
5 
0 
i):':--.x ........ x 
0 1 2 3 4 5 6 7 B 9 10 
Levels 
I * I I 
Attack Fact 
Date Fact -~--. 
Location Fact -E3--' 
Victim Fact ..x ...... 
Perpetrator Fact --,~.-- 
11 12 13 14 15 
Figure 2: MUC-4: Level Distribution of Each of the Five Facts 
50 i 
45 
40 
35 
30 
20 
\ ]5  
\ ]0  
5 
0 
0 
f I I I I l I I i I I I I I 
MUC 4:5 Facts 
1 2 3 4 5 6 7 S 9 101\ ]  "12"131415 
Levels 
Figure 3: MUC-4: Level Distribution of the Five 
Facts Combined 
? The victim (including damage to property). 
? The perpetrator(s) (including suspects). 
We analyzed a set of 100 articles from the MUC-4 
domain each of which reported one or more terror- 
ist attacks. Figure 2 shows the level distribution for 
each of the five facts. A closer analysis of the figure 
shows that the "type of attack" fact is the easiest to 
extract while the "perpetrator" fact is the hardest 
(the curve peaks at level-2 for this fact). In addition, 
Figure 3 shows the level distribution of the five facts 
combined. This figure gives some indication of the 
complexity of the MUC-4 task because it shows that 
almost 50% of the MUC-4 facts occur at level-1. The 
expected level of the five facts in the MUC-4 domain 
was 1.74 (this is simply the weighted average of the 
level distributions of the facts). We define this num- 
ber to be the Task Complexity for the MUC-4 task. 
Therefore, the MUC-4 task can now be compared to, 
say, the MUC-5 task by comparing their Task Com- 
plexities. In fact, we computed the Task Complexity 
of the MUC-5 task and discovered that it was equal 
to 2.5. In comparison, an analysis, using more "su- 
perficial" features, done by Beth Sundheim, shows 
that the nature of the MUC-5 EJV task is approx- 
imately twice as hard as the nature of the MUC-4 
task (Sundheim, 1993). The features used in the 
study included vocabulary size, the average number 
of words per sentence, and the average number of 
sentences per article. More details about this anal- 
ysis can be found in (Bagga and Biermann, 1998). 
4 Analyzing the Reading 
Comprehension Task 
The reading comprehension task differs from the QA 
task in the following way: while the goal of the QA 
task is to find answers for a set of questions from a 
collection of documents, the goal of the reading com- 
prehension task is to find answers to a set of ques- 
tions from a single related document. Since the QA 
task involves extracting answers from a collection of 
documents, the complexity of this task depends on 
the expected level of occurrence of the answers of 
the questions. While it is theoretically possible to 
compute the average level of any fact in the entire 
38 
Test 
Basic 
Basic-Interm 
# of 
sentences 
13 
avg # of 
levels/sent 
4.11 
2.69 
avg # of 
corefs/sent 
2.33 
2.39 
# of 
questions 
6 
avg # of 
levels/answer 
3.75 
3.33 
avg # of 
corers/answer 
2.25 
2.50 
Intermediate 56 3.50 2.55 9 4.44 3.33 
Interm-Adv 17 6.47 1.00 6 7.83 1.33 
Advanced 27 6.93 2.08 10 8.20 2.90 
Figure 4: Summary of Results 
document collection, it is not humanly possible to 
analyze every document in such large collections to 
compute this. For example, the TREC collection 
used for the QA track is approximately 5GB. How- 
ever, since the reading comprehension task involves 
extracting the answers from a single document, it is 
possible to analyze the document itself in addition 
to computing the level of the occurrence of each an- 
swer. Therefore, the results presented in this paper 
will provide both these values. 
4.1 Ana lys is  and Resu l t s  
We analyzed a set of five reading comprehension 
tests offered by the English Language Center at 
the University of Victoria in Canada 2. These 
five tests are listed in increasing order of diffi- 
culty and are classified by the Center as: Ba- 
sic, Basic-Intermediate, Intermediate, Intermediate- 
Advanced, and Advanced. For each of these tests, we 
calculated the level number of each sentence in the 
text, and the level number of the sentences contain- 
ing the answers to each question for every test. In 
addition, we also calculated the number of corefer- 
ences present in each sentence in the texts, and the 
corresponding number in the sentences containing 
each answer. It should be noted that we were forced 
to calculate the level number of the sentences con- 
taining the answer as opposed to calculating the level 
number of the answer itself because several ques- 
tions had only true/false answers. Since there was 
no way to compute the level numbers of true/false 
answers, we decided to calculate the level numbers of 
the sentences containing the answers in order to be 
consistent. For true/false answers this implied an- 
alyzing all the sentences which help determine the 
truth value of the question. 
Figure 4 shows for each text, the number of sen- 
tences in the text, the average level number of a sen- 
tence, the average number of coreferences per sen- 
tence, the number of questions corresponding to the 
test, the average level number of each answer, and 
the average number of coreferences per answer. 
The results shown in Figure 4 are consistent with 
the model. The figure shows that as the difficulty 
level of the tests increase, so do the corresponding 
level numbers per sentence, and the answers. One 
2 http://web2.uvcs.uvic.ca/elc/studyzone/index.htm 
conclusion that we can draw from the numbers is 
that the Basic-Intermediate st, based upon the 
analysis, is slightly more easy than the Basic test. 
We will address this issue in the next section. 
The numbers of coreferences, urprisingly, do no 
increase with the difficulty of the tests. However, 
a closer look at the types of coreference shows that 
while most of the coreferences in the first two tests 
(Basic, and Basic-Intermediate) are simple pronom- 
inal coreferences (he, she, it, etc.), the coreferences 
used in the last two tests (Intermediate-Advanced, 
and Advanced) require more knowledge to process. 
Some examples include marijuana coreferent with 
the drug, hemp with the pant, etc. Not being able 
to capture the complexity of the coreferences is one, 
among several, shortcomings of this model. 
4.2 A Compar i son  w i th  Qanda 
MITRE 3 ran its Qanda reading comprehension sys- 
tem on the five tests analyzed in the previous sec- 
tion. However, instead of producing a single answer 
for each question, Qanda produces a list of answers 
listed in decreasing order of confidence. The rest of 
this section describes an evaluation of Qanda's per- 
formance on the five tests and a comparison with 
the analysis done in the previous ection. 
In order to evaluate Qanda's performance on the 
five tests we decided to use the Mean Reciprocal 
Answer Rank (MRAR) technique which was used 
for evaluating question-answering systems at TREC- 
8 (Singhal, 1999). For each answer, this techniques 
assigns a score between 0 and 1 depending on its 
rank in the list of answers output. The score for 
answer, i, is computed as: 
1 
Scorel = rank of answeri 
If no correct answer is found in the list, a score of 
0 is assigned. Therefore, MRAR for a reading com- 
prehension test is the sum of the scores for answers 
corresponding to each question for that test. 
Figure 5 summarizes Qanda's results for the five 
tests. The figure shows, for each test, the number of 
questions, the cumulative MRAR for all answers for 
the test, and the average MRAR per answer. 
3We would like to thank Marc Light and Eric Breck for 
their help with running Qanda on our data. 
39 
Test ! # of ' MRAR for avg MRAR 
questions all answers per answer 
Basic 8 2.933 0.367 
Basic-Interm 6 3.360 0.560 
Intermediate 9 2.029 0.226 
Interm- Adv 6 1.008 0.168 
10 Advanced 7.833 0.783 
Figure 5: Summary of Qanda's Results 
The results from Qanda are more or less consis- 
tent with the analysis done earlier. Except for the 
Advanced test, the average Mean Reciprocal Answer 
Rank is consistent with the average number of levels 
per sentence (from Figure 4). It should be pointed 
out that the system performed significantly better on 
the Basic-Intermediate Test compared to the Basic 
test consistent with the numbers in Figure 4. How- 
ever, contrary to expectation, Qanda performed ex- 
ceedingly well on the Advanced test answering 7 out 
of the 10 questions with answers whose rank is 1 (i.e. 
the first answer among the list of possible answers 
for each question is the correct one). We are cur- 
rently consulting the developers of the system for 
conducting an analysis of the performance on this 
test in more detail. 
5 Shortcomings 
This measure is just the beginning of a search for 
useful complexity measures. Although the measure 
is a big step up from the measures used earlier, it has 
a number of shortcomings. The main shortcoming is 
the ambiguity regarding the selection of nodes from 
the network regarding the fact of interest. Consider 
the following sentence: "This is a report from the 
Straits of Taiwan . . . . . . . . .  Yesterday, China test 
fired a missile." Suppose we are interested in the 
location of the launch of the missile. The ambiguity 
here arises from the fact that the article does not 
explicitly mention that the missile was launched in 
the Straits of Taiwan. The decision to infer that 
fact from the information present depends upon the 
person building the network. 
In addition, the measure does not account for the 
following factors (the list is not complete): 
coreference: If the extraction of a fact requires the 
resolution of several coreferences, it is clearly 
more difficult than an extraction which does 
not. In addition, the degree of difficulty of re- 
solving coreferences it elf varies from simple ex- 
act matches~ and pronominal coreferences, to
ones that require external world knowledge. 
frequency of answers: The frequency of occur- 
rence of facts in a collection of documents has 
an impact on the performance ofsystems. 
occurrence of multiple (s imi lar)  facts: 
Clearly, if several similar facts are present 
in the same article, the systems will find it 
harder to extract he correct fact. 
vocabulary size: Unknown words present some 
problems to systems making it harder for them 
to perform well. 
On the other hand, no measure can take into ac- 
count all possible features in natural language. Con- 
sider the following example. In an article, suppose 
one initially encounters a series of statements hat 
obliquely imply that the following statement is false. 
Then the statement is given: "Bill Clinton visited 
Taiwan last week." Processing such discourse re- 
quires an ability to perfectly understand the initial 
series of statements before the truth value of tlie last 
statement can be properly evaluated. Such complete 
understanding is beyond the state of the art and is 
likely to remain so for many years. 
Despite these shortcomings, the current measure 
does quantify complexity on one very important di- 
mension, namely the number of clauses (or phrases) 
required to specify a fact. For the short term it 
appears to be the best available vehicle for under- 
standing the complexity of extracting a fact. 
6 Conclusions 
In this paper we have described a model that can be 
used to analyze the complexity of a reading compre- 
hension task. The model has been used to analyze 
five different reading comprehension tests, and the 
paper presents the results from the analysis. 
References 
ARPA. 1993. Fifth Message Understanding Confer- 
ence (MUC-5); San Mateo, August. Morgan Kauf- 
mann Publishers, Inc. 
Amit Bagga and Alan W. Biermann. 1997. Ana- 
lyzing the Complexity of a Domain With Respect 
To An Information Extraction Task. In Tenth In- 
ternational Conference on Research on Computa- 
tional Linguistics (ROCLING X), pages 175-194, 
August. 
Amit Bagga and Alan W. Biermann. 1998. Ana- 
lyzing the Performance of Message Understand- 
ing Systems. Journal of Computational Linguis- 
40 
tics and Chinese Language Processing, 3(1):1-26, 
February. 
Amit Bagga, Wlodek Zadrozny, and James Puste- 
jovsky. 1999. Semantics and Complexity of Ques- 
tion Answering Systems: Towards a Moore's Law 
for Natural Language Engineering. In 1999 AAAI 
Fall Symposium Series on Question Answering 
Systems, pages 1-10, November. 
DARPA. 1991. Third Message Understanding Con- 
ference (MUC-3), San Mateo, May. Morgan Kauf- 
mann Publishers, Inc. 
DARPA. 1992. Fourth Message Understanding 
Conference (MUC-4), San Mateo, June. Morgan 
Kaufmann Publishers, Inc. 
DARPA: TIPSTER Text Program. 1995. Sixth 
Message Understanding Conference (MUC-6), 
San Mateo, November. Morgan Kaufmann Pub- 
lishers, Inc. 
DARPA: TIPSTER Text Program. 1998. Seventh 
Message Understanding Conference (MUC- 
7). http://www.muc.saic.com/proceedings/ 
muc_7_toc.html, April. 
D. K. Harman, editor. 1999. Eighth Text RE- 
trieval Conference (TREC-8). National Institute 
of Standards and Technology (NIST), U.S. De- 
partment of Commerce, National Technical Infor- 
mation Service, November. 
Gary G. Hendrix. 1979. Encoding Knowledge in 
Partitioned Networks. In Nicholas V. Findler, edi- 
tor, Associative Networks, pages 51-92. Academic 
Press, New York. 
Lynette Hirschman et al 1999. Deep Read: A Read- 
ing Comprehension System. In 37th Annual Meet- 
ing of the Association of Computational Linguis- 
tics, pages 325-332, June. 
Lynette Hirschman. 1992. An Adjunct Test for 
Discourse Processing in MUC-4. In Fourth Mes- 
sage Understanding Conference (MUC-4) (DAR, 
1992), pages 67-77. 
Lenhart K. Schubert et al 1979. The Structure and 
Organization of a Semantic Net for Comprehen- 
sion and Inference. In Nicholas V. Findler, editor, 
Associative Networks, pages 121-175. Academic 
Press, New York. 
Amit Singhal. 1999. Question Answering Track at 
TREC-8. http://www, research, art. com/~ singhal/ 
qa-track-spec.txt, November. 
Beth M. Sundheim. 1993. Tipster/MUC-5 Informa- 
tion Extraction System Evaluation. In Fifth Mes- 
sage Understanding Conference (MUC-5) (ARP, 
1993), pages 27-44. 
41 
