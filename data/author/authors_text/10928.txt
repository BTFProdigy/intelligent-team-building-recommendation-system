Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 785?792
Manchester, August 2008
Toward a Psycholinguistically-Motivated Model of Language Processing
William Schuler
Computer Science and Engineering
University of Minnesota
schuler@cs.umn.edu
Samir AbdelRahman
Department of Computer Science
Cairo University
s.abdelrahman@fci-cu.edu.eg
Tim Miller
Computer Science and Engineering
University of Minnesota
tmill@cs.umn.edu
Lane Schwartz
Computer Science and Engineering
University of Minnesota
lschwar@cs.umn.edu
Abstract
Psycholinguistic studies suggest a model
of human language processing that 1) per-
forms incremental interpretation of spo-
ken utterances or written text, 2) preserves
ambiguity by maintaining competing anal-
yses in parallel, and 3) operates within
a severely constrained short-term memory
store ? possibly constrained to as few
as four distinct elements. This paper de-
scribes a relatively simple model of lan-
guage as a factored statistical time-series
process that meets all three of the above
desiderata; and presents corpus evidence
that this model is sufficient to parse natu-
rally occurring sentences using human-like
bounds on memory.
1 Introduction
Psycholinguistic studies suggest a model of human
language processing with three important proper-
ties. First, eye-tracking studies (Tanenhaus et al,
1995; Brown-Schmidt et al, 2002) suggest that hu-
mans analyze sentences incrementally, assembling
and interpreting referential expressions even while
they are still being pronounced. Second, humans
appear to maintain competing analyses in paral-
lel, with eye gaze showing significant attention to
competitors (referents of words with similar pre-
fixes to the correct word), even relatively long af-
ter the end of the word has been encountered, when
attention to other distractor referents has fallen off
(Dahan and Gaskell, 2007). Preserving ambigu-
ity in a parallel, non-deterministic search like this
may account for human robustness to missing, un-
known, mispronounced, or misspelled words. Fi-
nally, studies of short-term memory capacity sug-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
gest human language processing operates within a
severely constrained short-term memory store ?
possibly restricted to as few as four distinct ele-
ments (Miller, 1956; Cowan, 2001).
The first two observations may be taken to
endorse existing probabilistic beam-search mod-
els which maintain multiple competing analyses,
pruned by contextual preferences and dead ends
(e.g. Roark, 2001). But the last observation on
memory bounds imposes a restriction that until
now has not been evaluated in a corpus study. Can
a simple, useful human-like processing model be
defined using these constraints? This paper de-
scribes a relatively simple model of language as a
factored statistical time-series process that meets
all three of the above desiderata; and presents
corpus evidence that this model is sufficient to
parse naturally occurring sentences using human-
like bounds on memory.
The remainder of this paper is organized as fol-
lows: Section 2 describes some current approaches
to incremental parsing; Section 3 describes a statis-
tical framework for parsing using a bounded stack
of explicit constituents; Section 4 describes an ex-
periment to estimate the level of coverage of the
Penn Treebank corpus that can be achieved with
various stack memory limits, using a set of re-
versible tree transforms, and gives accuracy results
of a bounded-memory model trained on this cor-
pus.
2 Background
Much work on cognitive modeling in psycholin-
guistics is centered on modeling the concepts to
which utterances refer. Coarsely, these concepts
may correspond to activation patterns among neu-
rons in specific regions of the brain. In some the-
ories, a short-term memory store of several unre-
lated concepts may be retained by organizing the
activation of these concepts into compatible pat-
terns, only a few of which can be reliably main-
785
tained (Smolensky and Legendre, 2006). Activa-
tion is then theorized to spread through and among
these groups of concepts in proportion to some
learned probability that the concepts will be rel-
evant (Anderson and Reder, 1999), with the most
active concepts corresponding to the most likely
linguistic analyses. Competition between rival ac-
tivated groups of concepts (corresponding to in-
complete linguistic analyses) has even been linked
to reading delays (Hale, 2003).
This competition among mutually-exclusive
variously-activated short term memory stores of
concepts, essentially a weighted disjunction over
conjunctions of concepts, can be modeled in lan-
guage understanding as simple Viterbi decoding of
a factored HMM-like time-series model (Schuler
et al, in press). In this model, concepts (corre-
sponding to vectors of individuals in a first-order
world model) are introduced and composed (via
set operations like intersection) in each hypothe-
sized short-term memory store, using the elements
of the memory store as a stack. These vectors of
individuals can be considered a special case of vec-
tors of concept elements proposed by Smolensky,
with set intersection a special case of tensor prod-
uct in the composition model. Referents in this
kind of incremental model can be constrained by
? but still distinguished from ? higher-level ref-
erents while they are still being recognized.
It is often assumed that this semantic con-
cept composition proceeds isomorphically with
the composition of syntactic constituents (Frege,
1892). This parallel semantic and syntactic com-
position is considered likely to be performed in
short-term memory because it has many of the
characteristics of short-term memory processes,
including nesting limits (Miller and Chomsky,
1963) and susceptibility to degradation due to in-
terruption. Ericsson and Kintch (1995) propose a
theory of long-term working memory that extends
short-term memory, but only for inter-sentential
references, which do seem to be retained across
interruptions in reading. But while the relation-
ship between competing probability distributions
in such a model and experimental reading times
has been evaluated (e.g. by Hale), the relationship
between the syntactic demands on a short-term
memory store and observations of human short-
term memory limits is still largely untested. Sev-
eral models have been proposed to perform syntac-
tic analysis using a bounded memory store.
For example, Marcus (1980) proposed a deter-
ministic parser with an explicit four-element work-
ing memory store in order to model human parsing
limitations. But this model only stores complete
constituents (whereas the model proposed in this
paper stores incompletely recognized constituents,
in keeping with the Tanenhaus et al findings). As
a result, the Marcus model relies on a suite of spe-
cialized memory operations to compose complete
constituents out of complete constituents, which
are not independently cognitively motivated.
Cascaded finite-state automata, as in FASTUS
(Hobbs et al, 1996), also make use of a bounded
stack, but stack levels in such systems are typically
dedicated to particular syntactic operations: e.g.
a word group level, a phrasal level, and a clausal
level. As a result, some amount of constituent
structure may overflow its dedicated level, and be
sacrificed (for example, prepositional phrase at-
tachment may be left underspecified).
Finite-state equivalent parsers (and thus,
bounded-stack parsers) have asymptotically linear
run time. Other parsers (Sagae and Lavie, 2005)
have achieved linear runtime complexity with
unbounded stacks in incremental parsing by
using a greedy strategy, pursuing locally most
probable shift or reduce operations, conditioned
on multiple surrounding words. But without an
explicit bounded stack it is difficult to connect
these models to concepts in a psycholinguistic
model.
Abney and Johnson (1991) explore left-corner
parsing as a memory model, but again only in
terms of (complete) syntactic constituents. The
approach explored here is similar, but the trans-
form is reversed to allow the recognizer to store
recognized structure rather than structures being
sought, and the transform is somewhat simpli-
fied to allow more structure to be introduced into
syntactic constituents, primarily motivated by a
need to keep track of disconnected semantic con-
cepts rather than syntactic categories. Without this
link to disconnected semantic concepts, the syntax
model would be susceptible to criticism that the
separate memory levels could be simply chunked
together through repeated use (Miller, 1956).
Roark?s (2001) top-down parser generates trees
incrementally in a transformed representation re-
lated to that used in this paper, but requires dis-
tributions to be maintained over entire trees rather
than stack configurations. This increases the beam
786
width necessary to avoid parse failure. Moreover,
although the system is conducting a beam search,
the objects in this beam are growing, so the recog-
nition complexity is not linear, and the connection
to a bounded short-term memory store of uncon-
nected concepts becomes somewhat complicated.
The model described in this paper is arguably
simpler than many of the models described above
in that it has no constituent-specific mechanisms,
yet it is able to recognize the rich syntactic struc-
tures found in the Penn Treebank, and is still
compatible with the psycholinguistic notion of a
bounded short-term memory store of conceptual
referents.
3 Bounded-Memory Parsing with a Time
Series Model
This section describes a basic statistical framework
? a factored time-series model ? for recogniz-
ing hierarchic structures using a bounded store of
memory elements, each with a finite number of
states, at each time step. Unlike simple FSA com-
pilation, this model maintains an explicit represen-
tation of active, incomplete phrase structure con-
stituents on a bounded stack, so it can be readily
extended with additional variables that depend on
syntax (e.g. to track hypothesized entities or rela-
tions). These incomplete constituents are related
to ordinary phrase structure annotations through a
series of bidirectional tree transforms. These trans-
forms:
1. binarize phrase structure trees into linguisti-
cally motivated head-modifier branches (de-
scribed in Section 3.1);
2. transform right-branching sequences to left-
branching sequences (described in Sec-
tion 3.2); and
3. align transformed trees to an array of random
variable values at each depth and time step of
a probabilistic time-series model (described
in Section 3.3).
Following these transforms, a model can be trained
from example trees, then run as a parser on unseen
sentences. The transforms can then be reversed to
evaluate the output of the parser. This representa-
tion will ultimately be used to evaluate the cover-
age of a bounded-memory model on a large corpus
of tree-annotated sentences, and to evaluate the ac-
curacy of a basic (unsmoothed, unlexicalized) im-
plementation of this model in Section 4.
It is important to note that these transformations
are not postulated to be part of the human recog-
nition process. In this model, sentences can be
recognized and interpreted entirely in right-corner
form. The transforms only serve to connect this
process to familiar representations of phrase struc-
ture.
3.1 Binary branching structure
This paper will attempt to draw conclusions about
the syntactic complexity of natural language, in
terms of stack memory requirements in incremen-
tal (left-to-right) recognition. These requirements
will be minimized by recognizing trees in a right-
corner form, which accounts partially recognized
phrases and clauses as incomplete constituents,
lacking one instance of another constituent yet to
come.
In particular, this study will use the trees in the
Penn Treebank Wall Street Journal (WSJ) corpus
(Marcus et al, 1994) as a data set. In order to
obtain a linguistically plausible right-corner trans-
form representation of incomplete constituents, the
corpus is subjected to another, pre-process trans-
form to introduce binary-branching nonterminal
projections, and fold empty categories into non-
terminal symbols in a manner similar to that pro-
posed by Johnson (1998b) and Klein and Manning
(2003). This binarization is done in such a way
as to preserve linguistic intuitions of head projec-
tion, so that the depth requirements of right-corner
transformed trees will be reasonable approxima-
tions to the working memory requirements of a hu-
man reader or listener.
3.2 Right-Corner Transform
Phrase structure trees are recognized in this frame-
work in a right-corner form that can be mapped to
and from ordinary phrase structure via reversible
transform rules, similar to those described by
Johnson (1998a). This transformed grammar con-
strains memory usage in left-to-right traversal to a
bound consistent with the psycholinguistic results
described above.
This right-corner transform is simply the left-
right dual of a left-corner transform (Johnson,
1998a). It transforms all right branching sequences
in a phrase structure tree into left branching se-
quences of symbols of the form A
1
/A
2
, denoting
an incomplete instance of category A
1
lacking an
instance of category A
2
to the right. These incom-
plete constituent categories have the same form
787
a) binarized phrase structure tree:
S
NP
NP
JJ
strong
NN
demand
PP
IN
for
NP
NPpos
NNP
NNP
new
NNP
NNP
york
NNP
city
POS
?s
NNS
JJ
general
NNS
NN
obligation
NNS
bonds
VP
VBN
VBN
propped
PRT
up
NP
DT
the
NN
JJ
municipal
NN
market
b) result of right-corner transform:
S
S/NN
S/NN
S/NP
S/VP
NP
NP/NNS
NP/NNS
NP/NNS
NP/NP
NP/PP
NP
NP/NN
JJ
strong
NN
demand
IN
for
NPpos
NPpos/POS
NNP
NNP/NNP
NNP/NNP
NNP
new
NNP
york
NNP
city
POS
?s
JJ
general
NN
obligation
NNS
bonds
VBN
VBN/PRT
VBN
propped
PRT
up
DT
the
JJ
municipal
NN
market
Figure 1: Trees resulting from a) a binarization of a sample phrase structure tree for the sentence Strong
demand for New York City?s general obligations bonds propped up the municipal market, and b) a right-
corner transform of this binarized tree.
and much of the same meaning as non-constituent
categories in a Combinatorial Categorial Grammar
(Steedman, 2000).
Rewrite rules for the right-corner transform are
shown below, first to flatten out right-branching
structure:1
1The tree transforms presented in this paper will be de-
fined in terms of destructive rewrite rules applied iteratively
to each constituent of a source tree, from leaves to root, and
from left to right among siblings, to derive a target tree. These
rewrites are ordered; when multiple rewrite rules apply to the
same constituent, the later rewrites are applied to the results
of the earlier ones. For example, the rewrite:
A
0
. . . A
1
?
2
?
3
. . .
?
A
0
. . . ?
2
?
3
. . .
could be used to iteratively eliminate all binary-branching
nonterminal nodes in a tree, except the root. In the notation
used in this paper, Roman uppercase letters (A
i
) are variables
matching constituent labels, Roman lowercase letters (a
i
) are
variables matching terminal symbols, Greek lowercase letters
A
1
?
1
A
2
?
2
A
3
a
3
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
A
3
a
3
A
1
?
1
A
2
A
2
/A
3
?
2
. . .
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
. . .
then to replace it with left-branching structure:
(?
i
) are variables matching entire subtree structure, Roman
letters followed by colons, followed by Greek letters (A
i
:?
i
)
are variables matching the label and structure, respectively, of
the same subtree, and ellipses (. . . ) are taken to match zero
or more subtree structures, preserving the order of ellipses in
cases where there are more than one (as in the rewrite shown
above).
788
A1
A
1
/A
2
:?
1
A
2
/A
3
?
2
?
3
. . .
?
A
1
A
1
/A
3
A
1
/A
2
:?
1
?
2
?
3
. . .
Here, the first two rewrite rules are applied iter-
atively (bottom-up on the tree) to flatten all right
branching structure, using incomplete constituents
to record the original nonterminal ordering. The
third rule is then applied to generate left-branching
structure, preserving this ordering. Note that the
last rewrite above leaves a unary branch at the left-
most child of each flattened node. This preserves
the nodes at which the original tree was not right-
branching, so the original tree can be reconstructed
when the right-corner transform concatenates mul-
tiple right-branching sequences into a single left-
branching sequence.
An example of a right-corner transformed tree
is shown in Figure 1(b). An important property of
this transform is that it is reversible. Rewrite rules
for reversing a right-corner transform are simply
the converse of those shown above. The correct-
ness of this can be demonstrated by dividing a
tree into maximal sequences of right branches (that
is, maximal sequences of adjacent right children).
The first two ?flattening? rewrites of the right-
corner transform, applied to any such sequence,
will replace the right-branching nonterminal nodes
with a flat sequence of nodes labeled with slash
categories, which preserves the order of the non-
terminal category symbols in the original nodes.
Reversing this rewrite will therefore generate the
original sequence of nonterminal nodes. The final
rewrite similarly preserves the order of these non-
terminal symbols while grouping them from the
left to the right, so reversing this rewrite will re-
produce the original version of the flattened tree.
3.3 Hierarchic Hidden Markov Models
Right-corner transformed phrase structure trees
can then be mapped to random variable positions
in a Hierarchic Hidden Markov Model (Murphy
and Paskin, 2001), essentially a Hidden Markov
Model (HMM) factored into some fixed number of
stack levels at each time step.
HMMs characterize speech or text as a sequence
of hidden states q
t
(in this case, stacked-up syn-
tactic categories) and observed states o
t
(in this
case, words) at corresponding time steps t. A
most likely sequence of hidden states q?
1..T
can
then be hypothesized given any sequence of ob-
served states o
1..T
, using Bayes? Law (Equation 2)
and Markov independence assumptions (Equa-
tion 3) to define a full P(q
1..T
| o
1..T
) probabil-
ity as the product of a Transition Model (?
A
)
prior probability P(q
1..T
)
def
=
?
t
P
?
A
(q
t
| q
t-1
) and
an Observation Model (?
B
) likelihood probability
P(o
1..T
| q
1..T
)
def
=
?
t
P
?
B
(o
t
| q
t
):
q?
1..T
= argmax
q
1..T
P(q
1..T
| o
1..T
) (1)
= argmax
q
1..T
P(q
1..T
)?P(o
1..T
| q
1..T
) (2)
def
= argmax
q
1..T
T
?
t=1
P
?
A
(q
t
| q
t-1
)?P
?
B
(o
t
| q
t
) (3)
Transition probabilities P
?
A
(q
t
| q
t-1
) over com-
plex hidden states q
t
can be modeled using syn-
chronized levels of stacked-up component HMMs
in a Hierarchic Hidden Markov Model (HHMM)
(Murphy and Paskin, 2001). HHMM transition
probabilities are calculated in two phases: a re-
duce phase (resulting in an intermediate, marginal-
ized state f
t
), in which component HMMs may ter-
minate; and a shift phase (resulting in a modeled
state q
t
), in which unterminated HMMs transition,
and terminated HMMs are re-initialized from their
parent HMMs. Variables over intermediate f
t
and
modeled q
t
states are factored into sequences of
depth-specific variables ? one for each of D levels
in the HMM hierarchy:
f
t
= ?f
1
t
. . . f
D
t
? (4)
q
t
= ?q
1
t
. . . q
D
t
? (5)
Transition probabilities are then calculated as a
product of transition probabilities at each level, us-
ing level-specific reduce ?R and shift ?S models:
P
?
A
(q
t
|q
t-1
) =
?
f
t
P(f
t
|q
t-1
)?P(q
t
|f
t
q
t-1
) (6)
def
=
?
f
1..D
t
D
?
d=1
P
?R(f
d
t
|f
d+1
t
q
d
t-1
q
d-1
t-1
)?
P
?S(q
d
t
|f
d+1
t
f
d
t
q
d
t-1
q
d-1
t
) (7)
with fD+1
t
and q0
t
defined as constants. In Viterbi
decoding, the sums are replaced with argmax oper-
ators. This decoding process preserves ambiguity
by maintaining competing analyses of the entire
memory store. A graphical representation of an
HHMM with three levels is shown in Figure 3.
Shift and reduce probabilities can then be de-
fined in terms of finitely recursive Finite State Au-
tomata (FSAs) with probability distributions over
789
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8 t=9 t=10 t=11 t=12 t=13 t=14 t=15
strong
dem
and
for
new
york
city ?s
general
obligations
bonds
propped
up the
m
unicipal
m
arket
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ?
NNP/NNP
NNP/NNP
NPpos/POS
? ? ? ?
VBN/PRT
? ? ?
?
NP/NN
NP/PP
NP/NP
NP/NP
NP/NP
NP/NP
NP/NNS
NP/NNS
NP/NNS
S/VP
S/VP
S/NP
S/NN
S/NN
Figure 2: Sample tree from Figure 1 mapped to qd
t
variable positions of an HHMM at each stack depth
d (vertical) and time step t (horizontal). This tree uses only two levels of stack memory. Values for
final-state variables fd
t
are not shown. Note that some nonterminal labels have been omitted; labels for
these nodes can be reconstructed from their children.
transition, recursive expansion, and final-state sta-
tus of states at each hierarchy level. In simple HH-
MMs, each intermediate variable is a boolean vari-
able over final-state status fd
t
? {0,1} and each
modeled state variable is a syntactic, lexical, or
phonetic state qd
t
. The intermediate variable fd
t
is
true or false (equal to 1 or 0 respectively) accord-
ing to ?F-Reduce if there is a transition at the level
immediately below d, and false (equal to 0) with
probability 1 otherwise:2
P
?R(f
d
t
| f
d+1
t
q
d
t-1
q
d-1
t-1
)
def
=
{
if fd+1
t
=0 : [f
d
t
=0]
if fd+1
t
=1 : P
?F-Reduce(f
d
t
| q
d
t-1
, q
d-1
t-1
)
(8)
where fD+1
t
= 1 and q0
t
= ROOT.
Shift probabilities over the modeled variable qd
t
at each level are defined using level-specific tran-
sition ?Q-Trans and expansion ?Q-Expand models:
P
?S(q
d
t
| f
d+1
t
f
d
t
q
d
t-1
q
d-1
t
)
def
=
?
?
?
if fd+1
t
=0, f
d
t
=0 : [q
d
t
= q
d
t-1
]
if fd+1
t
=1, f
d
t
=0 : P
?Q-Trans(q
d
t
| q
d
t-1
q
d-1
t
)
if fd+1
t
=1, f
d
t
=1 : P
?Q-Expand(q
d
t
| q
d-1
t
)
(9)
where fD+1
t
= 1 and q0
t
= ROOT. This model
is conditioned on final-state switching variables at
and immediately below the current FSA level. If
there is no final state immediately below the cur-
rent level (the first case above), it deterministically
copies the current FSA state forward to the next
time step. If there is a final state immediately be-
low the current level (the second case above), it
2Here [?] is an indicator function: [?] = 1 if ? is true, 0
otherwise.
. . .
. . .
. . .
. . .
f
3
t?1
f
2
t?1
f
1
t?1
q
1
t?1
q
2
t?1
q
3
t?1
o
t?1
f
3
t
f
2
t
f
1
t
q
1
t
q
2
t
q
3
t
o
t
Figure 3: Graphical representation of a Hierarchic
Hidden Markov Model. Circles denote random
variables, and edges denote conditional dependen-
cies. Shaded circles are observations.
transitions the FSA state at the current level, ac-
cording to the distribution ?Q-Trans. And if the state
at the current level is final (the third case above),
it re-initializes this state given the state at the level
above, according to the distribution ?Q-Expand. The
overall effect is that higher-level FSAs are allowed
to transition only when lower-level FSAs termi-
nate. An HHMM therefore behaves like a prob-
abilistic implementation of a pushdown automaton
(or shift?reduce parser) with a finite stack, where
the maximum stack depth is equal to the number
of levels in the HHMM hierarchy.
Figure 2 shows the transformed tree from Fig-
ure 1 aligned to HHMM depth levels and time
steps. Because it uses a bounded stack, recognition
in this model is asymptotically linear (Murphy and
Paskin, 2001).
This model recognizes right-corner transformed
trees constrained to a stack depth corresponding to
observed human short term memory limits. This
790
HHMM depth limit sentences coverage
no memory 127 0.32%
1 memory element 3,496 8.78%
2 memory elements 25,909 65.05%
3 memory elements 38,902 97.67%
4 memory elements 39,816 99.96%
5 memory elements 39,832 100.00%
TOTAL 39,832 100.00%
Table 1: Percent coverage of right-corner trans-
formed treebank sections 2?21 with punctuation
omitted, using HHMMs with depth limits D from
zero to five.
is an attractive model of human language process-
ing because the incomplete syntactic constituents
it stores at each stack depth can be directly associ-
ated with (incomplete) semantic referents, e.g. by
adding random variables over environment or dis-
course referents at each depth and time step. If
these referents are calculated incrementally, recog-
nition decisions can be informed by the values of
these variables in an interactive model of language,
following Tanenhaus et al (1995). The corpus re-
sults described in the next section suggest that a
large majority of naturally occurring sentences can
be recognized using only three or four stack mem-
ory elements via this transform.
4 Empirical Results
In order to evaluate the coverage of this bounded-
memory model, Sections 2?21 of the Penn Tree-
bank WSJ corpus were transformed and mapped
to HHMM variables as described in Section 3.3. In
order to counter possible undesirable effects of an
arbitrary branching analysis of punctuation, punc-
tuation was removed. Coverage results on this cor-
pus are shown in Table 1.
Experiments training on transformed trees from
Sections 2?21 of the WSJ Treebank, evaluating
reversed-transformed output sequences from Sec-
tion 22 (development set) and Section 23 (test set),
show an accuracy (F score) of 82.1% and 80.1%
respectively.3 Although they are lower than those
for state-of-the-art parsers, these results suggest
that the bounded-memory parser described here is
doing a reasonably good job of modeling syntac-
tic dependencies, and therefore may have some
3Using unsmoothed relative frequency estimates from the
training set, a depth limit of D = 3, beam with of 2000, and
no lexicalization.
promise as a psycholinguistic model.
Although recognition in this system is linear, it
essentially works top-down, so it has larger run-
time constants than a bottom-up CKY-style parser.
The experimental system described above runs at
a rate of about 1 sentence per second on a 64-
bit 2.6GHz dual core desktop with a beam width
of 2000. In comparison, the Klein and Manning
(2003) CKY-style parser runs at about 5 sentences
per second on the same machine. On sentences
longer than 40 words, the HHMM and CKY-style
parsers are roughly equivalent, parsing at the rate
of .21 sentences per second, versus .24 for the
Klein and Manning CKY.
But since it is linear, the HHMM parser can be
directly integrated with end-of-sentence detection
(e.g. deciding whether ?.? is a sentence delimiter
based on whether the words preceding it can be
reduced as a sentence), or with n-gram language
models (if words are observations, this is simply
an autoregressive HMM topology). The use of
an explicit constituent structure in a time series
model also allows integration with models of dy-
namic phenomena such as semantics and corefer-
ence which may depend on constituency. Finally,
as a linear model, it can be directly applied to
speech recognition (essentially replacing the hid-
den layer of a conventional word-based HMM lan-
guage model).
5 Conclusion
This paper has described a basic incremental pars-
ing model that achieves worst-case linear time
complexity by enforcing fixed limits on a stack
of explicit (albeit incomplete) constituents. Ini-
tial results show a use of only three to four levels
of stack memory within this framework provides
nearly complete coverage of the large Penn Tree-
bank corpus.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their input. This research was
supported by National Science Foundation CA-
REER/PECASE award 0447685. The views ex-
pressed are not necessarily endorsed by the spon-
sors.
791
References
Abney, Steven P. and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
Anderson, J.R. and L.M. Reder. 1999. The fan effect:
New results and new theories. Journal of Experi-
mental Psychology: General, 128(2):186?197.
Brown-Schmidt, Sarah, Ellen Campana, and
Michael K. Tanenhaus. 2002. Reference res-
olution in the wild: Online circumscription of
referential domains in a natural interactive problem-
solving task. In Proceedings of the 24th Annual
Meeting of the Cognitive Science Society, pages
148?153, Fairfax, VA, August.
Cowan, Nelson. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Dahan, Delphine and M. Gareth Gaskell. 2007. The
temporal dynamics of ambiguity resolution: Evi-
dence from spoken-word recognition. Journal of
Memory and Language, 57(4):483?501.
Ericsson, K. Anders and Walter Kintsch. 1995.
Long-term working memory. Psychological Review,
102:211?245.
Frege, Gottlob. 1892. Uber sinn und bedeutung.
Zeitschrift fur Philosophie und Philosophischekritik,
100:25?50.
Hale, John. 2003. Grammar, Uncertainty and Sen-
tence Processing. Ph.D. thesis, Cognitive Science,
The Johns Hopkins University.
Hobbs, Jerry R., Douglas E. Appelt, John Bear,
David Israel, Megumi Kameyama, Mark Stickel, and
Mabry Tyson. 1996. Fastus: A cascaded finite-state
transducer for extracting information from natural-
language text. In Finite State Devices for Natural
Language Processing, pages 383?406. MIT Press,
Cambridge, MA.
Johnson, Mark. 1998a. Finite state approximation of
constraint-based grammars using left-corner gram-
mar transforms. In Proceedings of COLING/ACL,
pages 619?623.
Johnson, Mark. 1998b. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Marcus, Mitch. 1980. A theory of syntactic recognition
for natural language. MIT Press.
Miller, George and Noam Chomsky. 1963. Finitary
models of language users. In Luce, R., R. Bush, and
E. Galanter, editors, Handbook of Mathematical Psy-
chology, volume 2, pages 419?491. John Wiley.
Miller, George A. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. Psychological Review,
63:81?97.
Murphy, Kevin P. and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840.
Roark, Brian. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Sagae, Kenji and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies (IWPT?05).
Schuler, William, Stephen Wu, and Lane Schwartz. in
press. A framework for fast incremental interpre-
tation during speech decoding. Computational Lin-
guistics.
Smolensky, Paul and Ge?raldine Legendre. 2006.
The Harmonic Mind: From Neural Computation to
Optimality-Theoretic GrammarVolume I: Cognitive
Architecture. MIT Press.
Steedman, Mark. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Tanenhaus, Michael K., Michael J. Spivey-Knowlton,
Kathy M. Eberhard, and Julie E. Sedivy. 1995. Inte-
gration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
792
A Framework for Fast Incremental
Interpretation during Speech Decoding
William Schuler?
University of Minnesota
Stephen Wu?
University of Minnesota
Lane Schwartz?
University of Minnesota
This article describes a framework for incorporating referential semantic information from a
world model or ontology directly into a probabilistic language model of the sort commonly
used in speech recognition, where it can be probabilistically weighted together with phonological
and syntactic factors as an integral part of the decoding process. Introducing world model
referents into the decoding search greatly increases the search space, but by using a single
integrated phonological, syntactic, and referential semantic language model, the decoder is able to
incrementally prune this search based on probabilities associated with these combined contexts.
The result is a single unified referential semantic probability model which brings several kinds
of context to bear in speech decoding, and performs accurate recognition in real time on large
domains in the absence of example in-domain training sentences.
1. Introduction
The capacity to rapidly connect language to referential meaning is an essential aspect
of communication between humans. Eye-tracking studies show that humans listening
to spoken directives are able to actively attend to the entities that the words in these
directives might refer to, even while the words are still being pronounced (Tanenhaus
et al 1995; Brown-Schmidt, Campana, and Tanenhaus 2002). This timely access to
referential information about input utterances may allow listeners to adjust their pref-
erences among likely interpretations of noisy or ambiguous utterances to favor those
that make sense in the current environment or discourse context, before any lower-level
disambiguation decisions have been made. This same capability in a spoken language
interface system could allow reliable human?machine interaction in the idiosyncratic
language of day-to-day life, populated with proper names of co-workers, objects, and
events not found in broad training corpora. When domain-specific training corpora are
? Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455.
E-mail: schuler@cs.umn.edu; swu@cs.umn.edu; lane@cs.umn.edu.
Submission received: 25 April 2007; revised submission received: 4 March 2008; accepted for publication:
2 June 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 3
not available, a referential semantic interface could still exploit its model of the world:
the data to which it is an interface, and patterns characterizing these data.
This article describes a framework for incorporating referential semantic informa-
tion from a world model or ontology directly into a statistical language model of the
sort commonly used in speech recognition, where it can be probabilistically weighted
together with phonological and syntactic factors as an integral part of the decoding
process. Introducing world model referents into the decoding search greatly increases
the search space, but by using a single integrated phonological, syntactic, and referential
semantic language model, the decoder is able to incrementally prune this search based
on probabilities associated with these combined contexts.
Semantic interpretation is defined dynamically in this framework, in terms of transi-
tions over time from less constrained referents to more constrained referents. Because it
is defined dynamically, interpretation in this framework can incorporate dependencies
on referential context?for example, constraining interpretations to a presumed set of
entities, or a presumed setting?which may be fixed prior to recognition, or dynam-
ically hypothesized earlier in the recognition process. This contrasts with other recent
systemswhich interpret constituents only given fixed inter-utterance contexts or explicit
syntactic arguments (Schuler 2001; DeVault and Stone 2003; Gorniak and Roy 2004; Aist
et al 2007). Moreover, because it is defined dynamically, in terms of transitions, this
context-dependent interpretation framework can be directly integrated into a Viterbi
decoding search, like ordinary state transitions in a Hidden Markov Model. The result
is a single unified referential semantic probability model which brings several kinds
of referential semantic context to bear in speech decoding, and performs accurate
recognition in real time on large domains in the absence of example domain-specific
training sentences.
The remainder of this article is organized as follows: Section 2 will describe related
approaches to interleaving semantic interpretation with speech recognition. Section 3
will provide definitions for worldmodels used in semantic interpretation, and language
models used in speech decoding, which will form the basis of a referential semantic
language model, defined in Section 4. Then Section 5 will describe an evaluation of this
model in a sample spoken language interface application.
2. Related Work
Early approaches to incremental interpretation (Mellish 1985; Haddock 1989) apply
semantic constraints associated with each word in a sentence to progressively winnow
the set of individuals that could serve as referents in that sentence. These incrementally
constrained referents are then used to guide the syntactic analysis of the sentence, dis-
preferring analyses with empty interpretations in the current environment or discourse
context. Similar approaches were applied to broad-coverage text processing, querying a
large commonsense knowledge base as a world model (Martin and Riesbeck 1986). But
this winnowing is done deterministically, invoking default assumptions and potentially
exponential backtracking when default assumptions fail.
The idea of basing analysis decisions on constrained sets of referent individuals
was later extended to pursue multiple interpretations at once by exploiting polynomial
structure-sharing in a dynamic programming parser (Schuler 2001; DeVault and Stone
2003; Gorniak and Roy 2004; Aist et al 2007). The resulting shared interpretation is
similar to underspecified semantic representations (Bos 1996), except that the rep-
resentation mainly preserves syntactic ambiguity rather than semantic (e.g., quanti-
314
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
fier scoping) ambiguity, and the size complexity of the parser chart representation is
polynomially bounded. This approach was further extended to support hypothetical
referents (DeVault and Stone 2003), domains with continuous relations (Gorniak and
Roy 2004), and updates to the shared parser chart by components handling other levels
of linguistic analysis in parallel, during real-time recognition (Aist et al 2007).
The advantage of this use of the parser chart is that it allows a straightforward
mapping between syntax and semantics using familiar compositional semantic rep-
resentations. But the standard dynamic programming algorithm for parsing derives
its complexity bounds from the fact that each recognized constituent can be analyzed
independently of every other constituent. These independence assumptions must be
relaxed if dynamic context dependencies are to be applied across sibling constituents
(e.g., in the package data directory, open . . . , where the files to be opened should be
restricted to the contents of the package data directory). More importantly, from an
engineering perspective, the dynamic programming algorithm for parsing runs in cubic
time, not linear, which means this interpretation framework cannot be directly applied
to continuous audio streams. Interface systems therefore typically perform utterance
or sentence segmentation as a stand-alone pre-process, without integrating syntactic or
referential semantic dependencies into this decision.
Finally, some speech recognition systems employ inter-utterance context-dependent
language models that are pre-compiled into word n-grams for particular discourse or
environment states, and swapped out between utterances (Young et al 1989; Lemon
and Gruenstein 2004; Seneff et al 2004). But in some cases accurate interpretation will
require spoken language interfaces to exploit context continuously during utterance
recognition, not just between utterances. For example, the probability distribution over
the next word in the utterance go to the package data directory and get the . . . (or in the
package data directory get the . . . ) will depend crucially on the linguistic and environment
context leading up to this point: the meaning of package data directory in the first part of
this directive, as well as the objects that will be available once this part of the directive
has been carried out. Moreover, in rich environments pre-compilation to word n-grams
can be expensive, since all referents in the world model must be considered to build
accurate n-grams. This will not be practical if environments change frequently.
3. Background
In contrast to the approaches described in Section 2, this article proposes an incremental
interpretation framework which is entirely contained within a single-pass probabilistic
decoding search. Essentially, this approach directly integrates model theoretic seman-
tics, summarized in Section 3.1, with conventional probabilistic time-series models used
in speech recognition, summarized in Section 3.2.
3.1 Referential Semantics
Semantic interpretation requires a framework within which a speaker?s intended mean-
ings can be formalized. Sections 3.1.1 and 3.1.2 describe a model theoretic approach
to semantic interpretation that will later be extended in Section 4.1. The referential
states defined here will then be incorporated into a representation of nested syntactic
constituents in a hierarchic time-series model in Section 4.2. Some of the notation
introduced here is summarized later in Table 1 (Section 4).
315
Computational Linguistics Volume 35, Number 3
Figure 1
A subsumption lattice (laid on its side) over the power set of a domain containing three
individuals: ?1, ?2, and ?3. Subsumption relations are represented as gray arrows from supersets
(or super-concepts) to subsets (or sub-concepts).
3.1.1 Model Theory. The language model described in this article defines semantic ref-
erents in terms of a world model M. In model theory (Tarski 1933; Church 1940), a
world model is defined as a tuple M = ?E , ?? containing a domain of individuals E =
{?1, ?2, . . . } and an interpretation function ? to interpret expressions in terms of those
individuals. This interpretation function accepts expressions ? of various types: logical
statements, of simple type T (for example, the demo file is writable) which may be true
or false; references to individuals, of simple type E (for example, the demo file) which
may refer to any individual in the world model; or functors of complex type ??,??,
which take an argument of type ? and produce output of type ?. Functor expressions ?
of type ??,?? can be applied to other expressions ? of type ? as arguments to yield
expressions ?(?) of type ? (for example, writablemay take the demo file as an argument
and return true). By nesting functors, complex expressions can be defined, denoting
sets or properties of individuals: ?E, T? (for example, writable), relations over individual
pairs: ?E, ?E, T?? (for example, contains), or first-order functors over sets: ??E, T?, ?E, T??
(for example, a comparative adjective like larger).
3.1.2 Ontological Promiscuity. First-order or higher models (in which functors can take
sets as arguments) can be mapped to equivalent zero-order models (with functors
defined only on entities). This is generally motivated by a desire to allow sets of
individuals to be described in much the same way as individuals themselves (Hobbs
1985). Entities in a zero-order model M can be defined from individuals in a higher-
order model M? by mapping or reifying each set S = {?1, ?2, . . . } in P (EM? ) (or each
set of sets in P (P (EM? )), etc.) as an entity eS in a new domain EM.1 Relations l inter-
preted as zero-order functors inM can be defined directly from relations l? interpreted
as higher-order functors (over sets) in M? by mapping each instance of ?S1,S2? in
l?M? : P (EM? )?P (EM? ) to a corresponding instance of ?eS1 , eS2? in lM : EM?EM. Set
subsumption inM? can then be defined on entities made from reified sets inM, similar
to ?ISA? relations over concepts in knowledge representation systems (Brachman and
Schmolze 1985).
These subset or subsumption relations can be represented in a subsumption lattice,
as shown in Figure 1, with supersets to the left connecting to subsets to the right. This
representation will be used in Section 4 to define weighted transitions over first-order
referents in a statistical time-series model of interpretation.
1 Here, P (X) is the power set of X, containing the set of all subsets.
316
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
3.2 Language Modeling for Speech Recognition
The referential semantic language model described in this article is based on Hierar-
chic Hidden Markov Models (HHMMs), an existing extension of the standard Hidden
Markov Model (HMM) language modeling framework used in speech recognition,
which has been factored to represent hierarchic information about language structure
over time. This section will review HMMs (Section 3.2.1) and Hierarchic HMMs (Sec-
tions 3.2.2 and 3.2.3). This underlying framework will then be extended to include
random variables over semantic referents in Section 4.2.
3.2.1 HMMs and Language Models. The model described in this article is a specialization
of the HMM framework commonly used in speech recognition (Baker 1975; Jelinek,
Bahl, and Mercer 1975). HMMs characterize speech as a sequence of hidden states ht
(which may consist of speech sounds, words, or other hypothesized syntactic or se-
mantic information), and observed states ot (typically finite, overlapping frames of an
audio signal) at corresponding time steps t. A most-probable sequence of hidden states
h?1..T can then be hypothesized given any sequence of observed states o1..T, using Bayes?
Law (Equation 2) and Markov independence assumptions (Equation 3) to define the
full probability P(h1..T | o1..T ) as the product of a Language Model (LM) prior proba-
bility P(h1..T )
def
=
?
t P?LM (ht | ht?1) and an Acoustic Model (AM) likelihood probability
P(o1..T | h1..T )
def
=
?
t P?AM (ot | ht):
h?1..T = argmax
h1..T
P(h1..T | o1..T ) (1)
= argmax
h1..T
P(h1..T ) ? P(o1..T | h1..T ) (2)
def
= argmax
h1..T
T
?
t=1
P?LM (ht | ht?1) ? P?AM (ot | ht) (3)
The initial hidden state h0 may be defined as a constant.
2 HMM transitions can be
modeled using Weighted Finite State Automata (WFSAs), corresponding to regular
expressions. An HMM state ht may then be defined as a WFSA state, or a symbol
position in a corresponding regular expression.
3.2.2 Hierarchic HMMs. Language model transitions P?LM (?t |?t?1) over internally
structured hidden states ?t can be modeled using synchronized levels of stacked-
up component HMMs in an HHMM (Murphy and Paskin 2001), generalized here
as an abstract topology over unspecified random variables ? and ?. In this topol-
ogy, HHMM transition probabilities are calculated in two phases: a ?reduce? phase
(resulting in an intermediate, marginalized state ?t at time step t), in which compo-
nent HMMs may terminate; and a ?shift? phase (resulting in a modeled state ?t),
in which unterminated HMMs transition, and terminated HMMs are re-initialized
from their parent HMMs. Variables over intermediate and modeled states are factored
2 It is also common to define a prior distribution over initial states at h0, but this is not necessary here.
317
Computational Linguistics Volume 35, Number 3
into sequences of depth-specific variables?one for each of D levels in the HHMM
hierarchy:
?t = ??1t . . . ?
D
t ? (4)
?t = ??1t . . . ?
D
t ? (5)
Transition probabilities are then calculated as a product of transition probabilities at
each level, using level-specific ?reduce? ?? and ?shift? ?? models:
P?LM (?t |?t?1) =
?
?t
P(?t |?t?1) ? P(?t |?t ?t?1) (6)
def
=
?
?1t ...?
D
t
(
D
?
d=1
P?? (?
d
t |?
d+1
t ?
d
t?1?
d?1
t?1 )
)
?
(
D
?
d=1
P?? (?
d
t |?
d+1
t ?
d
t ?
d
t?1?
d?1
t )
)
(7)
with ?D+1t and ?
0
t defined as constants. In Viterbi (maximum likelihood) decoding, the
marginals (sums) in this equation may be approximated using an argmax operator. A
graphical representation of the dependencies in this model is shown in Figure 2.
3.2.3 Simple Hierarchic HMMs. The previous generalized definition can be considered a
template for factoring HMMs into synchronized levels, using ? and ? as parameters.
The specific Murphy?Paskin definition of HHMMs can then be considered a ?simple?
instantiation of this template using FSA states for ? and switching variables for ?. In
Section 4, this instantiation will be augmented (or further factored) to incorporate addi-
tional variables over semantic referents at each depth and time step, without changing
the overall topology of the model.
Figure 2
Graphical representation of a HHMMwith D = 3 hidden levels. Circles denote random
variables, and edges denote conditional dependencies. Shaded circles denote variables
with observed values.
318
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
In simple HHMMs, each intermediate state variable ?dt is a boolean switching vari-
able f d?,t ? {0, 1} and each modeled state variable ?
d
t is a syntactic, lexical, or phonetic
FSA state qd?,t:
?dt = f
d
?,t (8)
?dt = q
d
?,t (9)
Instantiating ?? as ?Simple-?, f
d is deterministic: true (equal to 1) with probability 1 if
there is a transition at the level immediately below d and the stack element qd?,t?1 is a
final state, and false (equal to 0) with probability 1 otherwise:3
P?Simple-? (?
d
t |?
d+1
t ?
d
t?1?
d?1
t?1 )
def
=
?
?
?
?
?
if f d+1?,t = 0 : [f
d
?,t= 0]
if f d+1?,t = 1, q
d
?,t?1 ?Final : [f
d
?,t= 0]
if f d+1?,t = 1, q
d
?,t?1?Final : [f
d
?,t= 1]
(10)
where f D+1?,t = 1 and q
0
?,t = ROOT.
Shift probabilities at each level (instantiating ?? as ?Simple-?) are defined using
level-specific transition ?Simple-Trans and expansion ?Simple-Init models:
P?Simple-? (?
d
t |?
d+1
t ?
d
t ?
d
t?1?
d?1
t )
def
=
?
?
?
?
?
if f d+1?,t = 0, f
d
?,t= 0 : [q
d
?,t= q
d
?,t?1]
if f d+1?,t = 1, f
d
?,t= 0 : P?Simple-Trans (q
d
?,t | q
d
?,t?1)
if f d+1?,t = 1, f
d
?,t= 1 : P?Simple-Init (q
d
?,t | q
d?1
?,t )
(11)
where f D+1?,t = 1 and q
0
?,t = ROOT. This model is conditioned on final-state switching
variables at and immediately below the current HHMM level: If there is no final state
immediately below the current level (the first case above), it deterministically copies the
current FSA state forward to the next time step; if there is a final state immediately below
the current level (the second case presented), it transitions the FSA state at the current
level, according to the distribution ?Simple-Trans; and if the state at the current level is
final (the third case presented), it re-initializes this state given the state at the level
above, according to the distribution ?Simple-Init. The overall effect is that higher-level
HMMs are allowed to transition only when lower-level HMMs terminate. An HHMM
therefore behaves like a probabilistic implementation of a pushdown automaton (or
?shift?reduce? parser) with a finite stack, where the maximum stack depth is equal to
the number of levels in the HHMM hierarchy.
Like HMM states, the states at each level in a simple HHMM also correspond to
weighted FSA (WFSA) states or symbol positions in regular expressions, except that
some states can be nonterminal states, which introduce corresponding sub-expressions
or sub-WFSAs governing state transitions at the level below. The process of expanding
each nonterminal state qd?1?,t to a sub-expression or WFSA (with start state q
d
?,t) is
modeled in ?Simple-Init. Transitions to adjacent (possibly final) states within each
expression or WFSA are modeled in ?Simple-Trans.
3 Here [?] is an indicator function: [?] = 1 if ? is true, 0 otherwise.
319
Computational Linguistics Volume 35, Number 3
For example, a simple HHMMmay factor a language model into word (q1?,t), phone
(q2?,t), and subphone (q
3
?,t) levels, where a word state may be a single word, a phone state
may be a position in a sequence of phones corresponding to a word, and a subphone
state may be a position in a sequence of subphone states (e.g., onset, middle, and end)
corresponding to a phone. In this case, ?Simple-Init would define a prior model over
words at level 1, a pronunciation model of phone sequences for each word at level 2,
and a state-sequence model of subphone states for each phone at level 3; and?Simple-Trans
would define a word bigram model at level 1, and would deterministically advance
along phone and subphone sequences at levels 2 and 3 (Bilmes and Bartels 2005).
This hierarchy of regular expressions may also be viewed as a probabilistic im-
plementation of a cascaded FSA, used for modeling syntax in information extraction
systems such as FASTUS (Hobbs et al 1996).
4. A Referential Semantic Language Model
A referential semantic language model can now be defined as an instantiation of an
HHMM (as described in Section 3.2), interpreting directives in a reified world model
(as described in Section 3.1). This interpretation framework is novel in that it is defined
dynamically in terms of transitions over referential states?evocations of entity referents
from a (e.g., first-order) world model?stacked up in a Hierarchic HMM. This allows
(1) a straightforward fast implementation of semantic interpretation (as transition) that
is compatible with conventional time-series models used in speech recognition; and (2)
a broader notion of semantic composition that exploits referential context in time order
(from previous constituents to later constituents) as well as bottom-up (from component
constituents to composed constituents).
First, Section 4.1 will describe a definition of semantic constraints as transitions in
a time-series model. Then Section 4.2 will apply these transitions to nested referents
in a Hierarchic HMM. Section 4.3 will introduce a state-based syntactic representa-
tion to link this semantic representation with recognized words. Finally, Section 4.4
will demonstrate the expressive power of this model on some common linguistic
constructions.
Because this section combines notation from different theoretical frameworks (in
particular, from formal semantics and statistical time-series modeling), a notation
summary is provided in Table 1.
4.1 Dynamic Relations
Semantic interpretationmay be easily integrated into a probabilistic time-series model if
it is formulated as a type of transition, from source to destination referents of equivalent
type at adjacent time steps. In other words, while relations in an ordinary Montagovian
interpretation framework (Montague 1973) may be functions from entity referents to
truth value referents, all relations in the world model defined here must be transition
functions from entity referents to entity referents.
One-place properties l may be modeled in this system by defining transitions from
preceding, unconstrained referents to referents constrained by l. The unconstrained
referents can be thought of as context arguments: For example, in the context of the
set of user-writable files, a property like EXECUTABLE evokes the subset of writable
executables. In the subsumption lattice shown in Figure 1, this will define a rightward
transition from each set referent to some subset referent, labeled with the traversed
relation (see Figure 4 in Section 4.4).
320
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Table 1
Summary of notation used in Section 4.
Model theory (see Section 3.1)
M : a world model
EM : the domain of individuals in world modelM
? : an individual
?M : an interpretation function from logical symbols (e.g., relation labels)
to logical functions over individuals, sets of individuals, etc.
variables with asterisks : refer to an initial world model prior to reification
Type theory (see Section 3.1.1)
E : the type of an individual
T : the type of a truth value
??,?? : the type of a function from type ? to type ? (variables over types)
Set theory (see Section 4.1)
S : a set of individuals
R : a relation over tuples of individuals
Random variables (see Sections 3.2 and 4.2)
h : a hidden variable in a time-series model
o : an observed variable in a time-series model,
(in this case, a frame of the acoustical signal)
? : a complex variable occurring in the reduce phase of processing;
for example, composed of ?e?, f??
? : a complex variable occurring in the shift phase of processing;
for example, composed of ?e?, q??
f : a random variable over final state status; for example, with value 1 or 0
q : a random variable over FSA (syntax) states,
in this case compiled from regular expressions; for example, with value q1 or q2
e : a random variable over referent entities; for example, with value e{?1?2?3}
l : a random variable over relation labels; for example, with value EXECUTABLE
(see Section 4.1)
t : a time step, from 1 to the end of the utterance T
d : a depth level, from 1 to the maximum depth level D
? : a probability model mapping variable values to probabilities
(real numbers form 0.0 to 1.0)
L : functions from FSA (syntax) states to relation labels
variables in boldface : instances or values of a random variable
non-bold variables with single subscripts : are specific to a time step; for example, ?t
non-bold variables with double subscripts : are specific to a reduce or shift phase within
a time step; for example, e?,t, q?,t
non-bold variables with superscripts : are specific to a depth level; for example, ?dt , e
d
?,t
General n-ary semantic relations l in this framework are therefore formulated as a
type of multi-source transition, distinguishing one argument of an original, ordinary
relation l? as an output (destination) and leaving the rest as input (source); then intro-
ducing a context referent as an additional input. Instead of defining simple transition
arcs on a subsumption lattice, n-ary relations more accurately define hyperarcs, with
multiple source referents: zero or more conventional arguments and one additional con-
text referent, leading to a destination referent intersectively constrained to this context.
321
Computational Linguistics Volume 35, Number 3
This model of interpretation as transition also allows referential semantic con-
straints to be applied that occur prior to hypothesized constituents, in addition to those
that occur as arguments. For example, in the sentence go to the package data directory
and hide the executable file, the phrase go to the package data directory provides a powerful
constraint on the referent of the executable file, although it does not occur as an argument
sub-constituent of this noun phrase. In this framework, the referent of the package data
directory (as a set of files) can be passed as a context argument to intersectively constrain
the interpretation of the executable file.
Recall the definition in Section 3.1.2 of a zero-order model M with refer-
ents e{?1,?2,...} reified from sets of individuals {?1, ?2, . . . } in some original first- or
higher-order modelM?. The referential semantic language model described in this arti-
cle interacts with this reified world modelM through queries of the form lM(eS1 , eS2 ),
where l is a relation, eS1 is an argument referent, and eS2 is a context referent (or eS1 is a
context referent if there is no argument). Each query returns a destination referent eS
such that S is a subset of the context set in the original world model M?. These
context-dependent relations l inM are then defined in terms of corresponding ordinary
relations l? of various types in the original world modelM? as follows:
lM(eS1 , eS2 ) = eS s.t.
?
?
?
if l?M? is type ?E, T? : S = S1 ? l?M?
if l?M? is type ?E, ?E, T?? : S = S2 ? (S1 ? l?M? )
if l?M? is type ??E, T?, ?E, T?? : S = S2 ? l?M? (S1)
(12)
where relation products are defined to resemble matrix products:
S ? R = {??? | ???S, ???, ?????R} (13)
For example, a property like EXECUTABLE would ordinarily bemodeled as a functor
of type ?E, T?: given an individual, it would return true if the individual can be executed.
The first case in Equation (12) casts this as a transition from an argument set S1 to
the set of individuals within S1 that are executable. On the other hand, a relation like
CONTAINS would ordinarily be modeled as ?E, ?E, T??: given an individual and then
another individual, it would return true if the relation holds over the pair. The second
case in Equation (12) casts this as a transition from a set of containers S1, given a context
set S2, to the subset of this context that are contained by an individual in S1. Finally, a
first-order functor like LARGEST would ordinarily be modeled as ??E, T?, ?E, T??: given
a set of individuals and then another individual, it would return true if the individual
belongs to the (singleton) set of things that are the largest in the argument set. The last
case in Equation (12) casts this as a transition from a set S1, given a context set S2, to
the (singleton) subset of this context that are members of S1 and are larger than all other
individuals in S1. More detailed examples of each relation type in Equation (12) are
provided in Section 4.4.
Relations in this world model have the character of being context-dependent in the
sense that relations like CAPTAIN that are traditionally one-place (denoting a set of enti-
ties with rank captain) are now two-place, dependent on an argument superconcept in
the subsumption lattice. Relations can therefore be given different meanings at different
places in the world model: in the context of a particular football team, CAPTAIN will
refer to a particular player; in the context of a different team, it will refer to someone
else. One-place relations can still be defined using a subsumption lattice root concept
322
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
?? as a context argument of course, but this will increase the perplexity (number of
choices) at the root concept, making recognition less reliable.
In this definition, referents e are similar to the information states in Dynamic Predi-
cate Logic (Groenendijk and Stokhof 1991), except that only limited working memory
for information states is assumed, containing only one referent (or variable binding in
DPL terms) per HHMM level.
4.2 Referential Semantic HHMM
Like the simple HHMM described in Section 3.2.3, the referential semantic language
model described in this article (henceforth RSLM), is defined by instantiating the gen-
eral HHMM ?template? defined in Section 3.2.2. This RSLM instantiation incorporates
both the switching variables f ?{0, 1} and FSA state variables q of the simple HHMM,
and adds variables over semantic referents e to the ?reduce? and ?shift? phases at each
level. Thus, the RSLM decomposes each HHMM reduce variable ?dt into a joint variable
subsuming an intermediate referent ed?,t and a final-state switching variable f
d
?,t; and
decomposes each HHMM shift variable ?dt into a joint variable subsuming a modeled
referent ed?,t and an ordinary FSA state q
d
?,t:
?dt = ?e
d
?,t, f
d
?,t? (14)
?dt = ?e
d
?,t, q
d
?,t? (15)
A graphical representation of this referential semantic language model is shown in
Figure 3.
The intermediate referents ed?,t in this framework correspond to the traditional notion
of compositional semantics (Frege 1892), in which meanings of composed constituents
(at higher levels in the HHMM hierarchy) are derived from meanings of component
constituents (at lower levels in the hierarchy). However, in addition to the referents
Figure 3
A graphical representation of the dependencies in the referential semantic language model
described in this article (compare with Figure 2). Again, circles denote random variables
and edges denote conditional dependencies. Shaded circles denote random variables with
observed values.
323
Computational Linguistics Volume 35, Number 3
of their component constituents, the intermediate referents in this framework are also
constrained by the referents at the same depth in the previous time step?the referen-
tial context described in Section 4.1. The modeled referents ed?,t in this framework then
correspond to a snapshot at each time step of the referential state of the recognizer,
after all completed constituents have been composed (or reduced), and after any new
constituents have been introduced (or shifted).
Both intermediate and modeled referents are constrained by labeled relations l
in ?M associated with ordinary FSA states. Thus, relation labels are defined for ?re-
duce? and ?shift? HHMM operations via label functions L? and L?, respectively, which
map FSA states q to relation labels l.
Entity referents ed? at each reduce phase of this HHMM are constrained by the pre-
vious FSA state qdt-1 using a reduce relation l
d
?,t = L?(q
d
?,t-1), such that e
d
? = l
d
?M(e
d+1
? , e
d
t-1).
Reduce probabilities at each level (instantiating ?? as ?RSLM-?) are therefore:
4
P?RSLM-? (?
d
t |?
d+1
t ?
d
t-1?
d-1
t-1 )
def
=
?
?
?
?
?
?
?
?
?
if f d+1?,t = 0 : [f
d
?,t= 0] ? [e
d
?,t= e
d
?,t]
if f d+1?,t = 1, q
d
?,t-1 ?Final : [f
d
?,t= 0] ? [e
d
?,t= e
d+1
?,t ]
if f d+1?,t = 1, q
d
?,t-1?Final : [f
d
?,t= 1] ?
[ed?,t= l
d
?,tM(e
d+1
?,t , e
d-1
?,t-1)]
(16)
where ?D+1?,t = ?e
D
?,t-1, 1? and ?
0
?,t = ?e,ROOT?. Here, it is assumed that L?(q
d
?,t-1) pro-
vides a non-trivial constraint only when qd?,t is a final state; otherwise it returns an
IDENTITY relation such that IDENTITYM(e, e
?) = e.
Entity referents ed?,t at each shift phase of this HHMM are constrained by the cur-
rent FSA state qd?,t using a shift relation l
d
?,t = L?(q
d
?,t), such that e
d
?,t = l
d
?,tM(e
d-1
?,t, e).
Shift probabilities at each level (instantiating ?? as ?RSLM-?) then generate relation
labels using a ?description? model ?Ref-Init, with referents e
d
?,t and state transitions q
d
?,t
conditioned on (or deterministically dependent on) these labels. The probability distri-
bution over modeled variables is therefore
P?RSLM-? (?
d
t |?
d+1
t ?
d
t ?
d
t-1?
d-1
t )
def
=
?
?
?
?
?
?
?
?
?
?
?
?
?
if f d+1?,t = 0, f
d
?,t= 0 : [e
d
?,t= e
d
?,t] ? [q
d
?,t= q
d
?,t-1]
if f d+1?,t = 1, f
d
?,t= 0 : [e
d
?,t= e
d
?,t] ? P?Syn-Trans (q
d
?,t | q
d
?,t-1)
if f d+1?,t = 1, f
d
?,t= 1 :
?
ld?,t
P?Ref-Init (l
d
?,t | e
d-1
?,t q
d-1
?,t)
?[ed?,t= l
d
?,tM(e
d-1
?,t, e)]
?P?Syn-Init (q
d
?,t | l
d
?,t q
d-1
?,t)
(17)
where ?D+1?,t = ?e
D
?,t-1, 1? and ?
0
?,t = ?e,ROOT?. Here, it is assumed that L?(q
d
?,t) pro-
vides a non-trivial constraint only when qd?,t is an initial state; otherwise it returns an
IDENTITY relation such that IDENTITYM(e, e
?) = e. The probability models?Ref-Init and
?Syn-Init are induced from corpus observations or defined by hand.
The cases in this equation, conditioned on final-state switching variables f d+1?,t
and f d?,t, correspond to those in Equation (11) in Section 3.2.3. In the first case, where
there is no final state immediately below the current level, referents and FSA states are
simply propagated forward. In the second case, where there is a final state immediately
below the current level, referents are propagated forward and the FSA state is advanced
4 Again, [?] is an indicator function: [?] = 1 if ? is true, 0 otherwise.
324
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
according to the distribution ?Syn-Trans. In the third case, where the current FSA state is
final and must be re-initialized, a new referent and FSA state are chosen by:
1. selecting, according to a ?description? model ?Ref-Init, a relation label l
d
?,t
with which to constrain the current referent,
2. deterministically generating a referent ed?,t given this label and the referent
at the level above, and
3. selecting, according to a ?lexicalization? model ?Syn-Init, an FSA state q
d
?,t
that is compatible with this label (i.e., has L?(q
d
?,t) = l
d
?,t).
4.3 Associating Semantic Relations with Syntactic Expressions
In this framework, semantic referents are constrained over time by instances of seman-
tic relations l? and l?. These relations are determined by instances of syntactic FSA
states q1, . . . ,qn, themselves expanded from higher-level FSA states q. These associa-
tions between syntactic and semantic random variable values can be represented in
expansion rules of the form
q  q1 . . . qn; with l? = L?(q1) and l? = L?(qn) (18)
where q1 . . . qn may be any regular expression initiating at state q1 and culminating at
(final) state qn. Note that regular expressions must therefore begin with shift relations
and end with reduce relations. This is in order to keep the syntactic and referential
semantic expansions synchronized.
These hierarchic regular expressions are defined to resemble expansion rules in
a context free grammar (CFG). However, unlike CFGs, HHMMs have memory limits
on nesting, in the form of a maximum depth D beyond which no expansion may take
place. As a result, the expressive power of an HHMM is restricted to the set of regular
languages, whereas CFGs may recognize the set of context-free languages; and HHMM
recognition is worst-case linear on the length of an utterance, whereas CFG recognition
is cubic.5 Similar limits have been proposed on syntax in natural languages, motivated
by limits on short term memory observed in humans (Miller and Chomsky 1963;
Pulman 1986). These have been applied to obtain memory-limited parsers (e.g., Marcus
1980), and depth-limited right-corner grammars that are equivalent to CFGs, except
that they restrict the number of internally recursive expansions allowed in recognition
(Schuler and Miller 2005).
4.4 Expressivity
The language model described herein defines referential semantics purely in terms of
HHMM shift and reduce operations over referent entities, made from reified sets of
individuals in some original world model. This section will show that this basic model
is sufficiently expressive to represent many commonly occurring linguistic phenomena,
5 When expressed as a function of the size of the grammar, HHMM recognition is asymptotically
exponential on D, whereas CFG recognition is cubic regardless of depth. In practice, however, exact
inference using either formalism is impractical, so approximate inference is used instead (e.g.,
maintaining a beam at each time step or at each constituent span in CFG parsing).
325
Computational Linguistics Volume 35, Number 3
Figure 4
A subsumption lattice (laid on its side, in gray) over the power set of a domain containing three
files: f1 (a writable executable), f2 (a read-only executable), and f3 (a read-only data file).
?Reference paths? made up of conjunctions of relations l (directed arcs, in black) traverse the
lattice from left to right toward the empty set, as referents (e{...}, corresponding to sets of files)
are incrementally constrained by intersection with each lM. (Some arcs are omitted for clarity.)
including intersective modifiers (e.g., adjectives like executable), multi-argument rela-
tions (e.g., prepositional phrases or relative clauses, involving trajector and landmark
referents), negation (as in the adverb not), and comparatives over continuous properties
(e.g., larger).
4.4.1 Properties. Properties (traditionally unary relations like EXECUTABLE or WRITABLE)
can be represented in theworldmodel as labeled edges lt from supersets et?1 to subsets et
defined by intersecting the set et?1 with the set ltM satisfying the property lt. Recall that
a reified world model can be cast as a subsumption lattice as described in Section 3.1.2.
The result of conjoining a property l with a context set e can therefore be found by
downward traversal of an edge in this lattice labeled l and departing from e.6
Thus, in Figure 4, the set of executables that are read-only would be reachable by
traversing a READ-ONLY relation from the set of executables, or by traversing an EX-
ECUTABLE relation from the set of read-only objects, or by a composed path READ-
ONLY?EXECUTABLE or EXECUTABLE?READ-ONLY from e. The resulting set may then
serve as context for subsequent traversals. Property relations may also result in self-
traversals (e.g., DATAFILE?READ-ONLY in Figure 4) or traversals to the empty set e?
(e.g., DATAFILE?WRITABLE). Property relations like EXECUTABLE can be defined using
the dynamic relations in the first case of Equation (12) in Section 4.1, which simply
ignore the non-context argument.
A general template for intersective nouns and modifiers can be expressed as a noun
phrase (NP) expansion using the following regular expression (where l? and l? indicate
relation labels constraining referents at the beginning and end of the NP):
NP ? Det
(
Adj
)?
Noun
(
PP |RC
)?
; with l? = IDENTITY and l? = IDENTITY (19)
6 Although properties (and later, n-ary relations) are defined in terms of an exponentially large
subsumption lattice, this lattice need not be an actual data structure. If the world model is queried from a
decoder trellis with a beam filter rather than from a complete search, only those lattice relations that are
phonologically, syntactically, and semantically most likely (in other words, those that are on this beam)
will be explored.
326
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
in which referents are successively constrained by the semantics of relations associated
with adjective and noun expansions:
Adj ? executable; with l? = EXECUTABLE and l? = IDENTITY (20)
Noun ? executable; with l? = EXECUTABLE and l? = IDENTITY (21)
(and are also constrained by the prepositional phrase (PP) and relative clause (RC)
modifiers, as described below). Here the relation EXECUTABLE traverses from refer-
ent e{f1f2f3} to referent e{f1f2}, a subset of e{f1f2f3} satisfying EXECUTABLE
?
M? .
4.4.2 n-ary Relations. Sequences of properties (traditionally unary relations) can be inter-
preted as simple nonbranching paths from referent to referent in a subsumption lattice,
but higher-arity relations define more complex paths that fork and rejoin. For example,
the referent of the directory containing the executable in Figure 5 would be reachable only
by:
1. storing the original set of directories e{d1d2d3} as a top-level referent in the
HHMM hierarchy, then
2. traversing a CONTAIN relation departing e{d1d2d3} to obtain the contents of
those directories e{f2f3}, then
3. traversing an EXECUTABLE relation departing e{f2f3} to constrain this set to
the set of contents that are also executable: e{f2}, then
4. traversing the inverse CONTAIN? of relation CONTAIN to obtain the
containers of these executables, then constraining the original set of
directories e{d1d2d3} by intersection with this resulting set to yield the
directories containing executables: e{d2}.
This ?forking? of referential semantic paths is handled via syntactic recursion: one path is
explored by the recognizer while the other waits on the HHMM hierarchy (essentially
Figure 5
Reference paths for a relation containing in the directory containing the executable file. A reference
path forks to specify referents using a two-place relation CONTAIN in a domain of directories
d1, d2, d3 and files f1, f2, f3. Here, d2 contains f2 and d3 contains f3, and f1 and f2 are executable. The
ellipsis in the referent set indicates the presence of additional individuals that are not directories.
Again, subsumption is represented in gray and relations are represented in black. (Portions of
the complete subsumption lattice and relation graph are omitted for clarity.)
327
Computational Linguistics Volume 35, Number 3
functioning as a stack). A sample template for branching reduced relative clauses (or
prepositional phrases) that exhibit this forking behavior can be expressed as below:
RC ? containing NP; with l? = CONTAIN and l? = CONTAIN? (22)
where the inverse relation CONTAIN? is applied when the NP expansion concludes or
reduces (when the forked paths are re-joined). Relations like CONTAIN are covered in
the second case of Equation (12) in Section 4.1, which define transitions from sets of
individuals associated with one argument of an original relation CONTAIN? to sets of
individuals associated with the other argument of this relation, in the presence of a
context set, which is a superset of the destination. The calculation of semantic tran-
sition probabilities for n-ary relations thus resembles that for properties, except that
the probability term associated with the relation l? and the inverse relation l? would
depend on both context and argument referents (to its left and below it, in the HHMM
hierarchy).
Note that there is ultimately a singleton referent {f2} of the executable file in Figure 5,
even though there are two executable files in the world model used in these examples.
This illustrates an important advantage of a dynamic context-dependent (three referent)
model of semantic composition over the strict compositional (two referent) model. In a
dynamic context model, the executable file is interpreted in the context of the files that are
contained in a directory. In a strict compositional model, the executable file is interpreted
only in the context of fixed constraints covering the entire utterance, and the constraints
related to the relation containing are applied only to the directories. This means that a
generative model based on strict composition will assign some probability to an infi-
nitely recursive description the directories containing executables contained by directories . . .
In generation systems, this problem has been addressed by adding machinery to keep
track of redundancy (Dale and Haddock 1991). But in this framework, a description
model (?Ref-Init) which is sensitive to the sizes of its source referent and destination ref-
erent at the end of each departing labeled transition will be able to disprefer referential
transitions that attempt to constrain already singleton referents, or that provide only
trivial or vacuous (redundant) constraints in general. This solution is therefore more in
line with graph-basedmodels of generation (Krahmer, van Erk, and Verleg 2003), except
that the graphs proposed here are over reified sets rather than individuals, and the goal
is a generative probability model of language rather than generation per se.
4.4.3 Negation. Negation can be modeled in this framework as a relation between sets.
Although it does not require any syntactic memory, negation does require referential
semantic memory, in that the complement of a specified set must be intersected with
some initial context set. Files that are not writable must still be files after all; only the
writable portion of this description should be negated.
A regular expression for negation of adjectives is
Adj ? not Adj; with l? = IDENTITY and l? = NOT (23)
and is applied to a world model in Figure 6. Relations like NOT are covered in the third
case of Equation (12) in Section 4.1, which define transitions between sets in an original
relation NOT?.
4.4.4 Comparatives, Superlatives, and Subsective Modifiers. Comparatives (e.g., larger),
superlatives (e.g., largest), and subsective modifiers (e.g., large, relative to some context
328
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Figure 6
Reference paths for negation in files that are not writable, using a world model with files f1, f2,
and f3 of which only f1 is writable. The recognizer first forks a copy of the set of files {f1, f2, f3}
using the relation IDENTITY, then applies the adjective relation WRITABLE to yield {f1}. The
complement of this set {f2, f3, . . . } is then intersected with the stored top-level referent
set {f1, f2, f3} to produce the set of files that are not writable: {f2, f3}. Ellipses in referent sets
indicate the presence of additional individuals that are not files.
set) define relations from sets to sets, or from sets to individuals (singleton sets). They
can be handled in much the same way as negation. Here the context is provided from
previous words and from sub-structure, in contrast to DeVault and Stone (2003), which
define the context of a comparative either from fixed inter-utterance constraints or as the
referent of the portion of the noun phrase dominated by the comparative (in addition
to inter-utterance constraints). One advantage of dynamic (time-order) constraints is
that implicit comparatives (in the Clark directory, select the file that is larger, with no
complement) can be modeled with no additional machinery. If substructure context is
not needed, then no additional HHMM storage is necessary.
A regular expression for superlative adjectives is
Noun ? largest Noun; with l? = IDENTITY and l? = LARGEST (24)
and is applied to a world model in Figure 7. Relations like LARGEST are also covered
in the third case of Equation (12), which defines transitions between sets in an original
relation LARGEST?.
5. Evaluation in a Spoken Language Interface
Much of the motivation for this approach has been to develop a human-like model of
language processing. But there are practical advantages to this approach as well. One
of the main practical advantages of the referential semantic language model described
Figure 7
Reference paths for a comparative in the largest executable; this forks a copy of the referent set
{f1, f2, f3} using the relation IDENTITY, applies EXECUTABLE to the forked set to obtain {f1, f2},
and returns the referent {f2}with the largest file size using LARGEST.
329
Computational Linguistics Volume 35, Number 3
in this article is that it may allow spoken language interfaces to be applied to content-
creation domains that are substantially developed by individual users themselves. Such
domains may include scheduling or reminder systems (organizing items containing
idiosyncratic person or event names, added by the user), shopping lists (containing
idiosyncratic brand names, added by the user), interactive design tools (containing new
objects designed and named by the user), or programming interfaces for home or small
business automation (containing new actions, defined by the user). Indeed, computers
are frequently used for content creation as well as content browsing; there is every
reason to expect that spoken language interfaces will be used this way as well.
But the critical problem of applying spoken language interfaces to these kinds of
content-creation domains is that the vocabulary of possible proper names that users
may add or invent is vast. Interface vocabularies in such domains must allow new
words to be created, and once they are created, these new words must be incorpo-
rated into the recognizer immediately, so that they can be used in the current context.
The standard tactic of training language models on example sentences prior to use
is not practical in such domains?except for relatively skeletal abstractions, example
sentences will often not be available. Even very large corpora gleaned from Internet
documents are unlikely to provide reliable statistics for users? made-up names with
contextually appropriate usage, as a referential semantic language model provides.
Content-creation applications such as this may have considerable practical value as
a means of improving accessibility to computers for disabled users. These domains also
provide an ideal proving ground for a referential semantic language model, because
directives in these domains mostly refer to a world model that is shared by the user
and the interfaced application, and because the idiosyncratic language used in such
domains makes it more resistant to domain-independent corpus training than other
domains. In contrast, domains such as database query (e.g., of airline reservations),
dictation, or information extraction are less likely to benefit from a referential semantic
language model, because the world model in such domains is not shared by either the
speaker (in database query) or by the interfaced application (in dictation or information
extraction),7 or because these domains are relatively fixed, so the expense ofmaintaining
linguistic training corpora in these domains can often be justified.
This section will describe an evaluation of an implementation of the referential
semantic language model as a spoken language interface in a very basic content-
creation domain: that of a file organizer, similar to a Unix shell.8 The performance of the
model on this domain will be evaluated in large environments containing thousands
of entities; more than will fit on the beam used in the Viterbi decoding search in this
implementation.
The experiments described in Sections 5.1 through 5.8 were conducted to investigate
the effect on recognition time and accuracy of using a referential semantic language
model to recognize common types of queries, generated by an experimenter and read by
several speakers. A thorough evaluation of the possible coverage of this kind of system
on spontaneous input (e.g., in usability experiments) would require a rich syntactic
representation and attention to disfluencies and speech repairs which are beyond the
scope of this article (see Section 6).
7 Techniques based on abductive reasoning may mitigate this problem of incomplete model sharing
(Hobbs et al 1993), but this would require considerable extensions to the proposed model, and is
beyond the scope of this article.
8 This is also similar to a spoken language version of Wilensky?s Unix consultant (Wilensky, Arens, and
Chin 1984).
330
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
5.1 Ontology Navigation Test Domain
To evaluate the contribution to recognition accuracy of referential semantics over that of
syntax and phonology alone, a baseline (syntax only) and test (baseline plus referential
semantics) recognizer were run on sample ontology manipulation directives in a ?stu-
dent activities? domain. This domain has the form of a simple tree-like taxonomy, with
some cross-listings (for example, studentsmay be listed in homerooms and in activities).
Taxonomic ontologies (e.g., for organizing biological classifications or computer file
directories) can be mapped to reified world models of the sort described in Section 3.1.2.
Concepts C in such an ontology define sets of individuals described by that concept:
{?|C(?)}. Subconcepts C? of a concept C then define subsets of individuals: {?|C?(?)} ?
{?|C(?)}. These sets and subsets can be reified as referent entities and arranged on
a subsumption lattice as described in Section 3.1.2. A sample taxonomic ontology is
shown in Figure 8a (tilted on its side tomatch the subsumption lattices shown elsewhere
in this article). Thus defined, such ontologies can be navigated using referent transitions
described in Section 4.1 by entering concept referents via ?downward? (rightward in the
figure) transitions, and leaving concept referents via ?upward? (leftward) transitions.
For example, this ontology can be manipulated using directives such as:
(1) set Crookston campus homeroom two Clark to sports football captain
which are incrementally interpreted by transitioning down the subsumption lattice (e.g.,
from sports to football to captain) or forking to another part of the lattice (e.g., from Clark
to sports).
As an ontology like this is navigated in spoken language, there is a sense in which
other referents e? at the same level of the ontology as the most recently described refer-
ent e, or at higher levels of the ontology than the most recently described entity, should
be semantically accessible without restating the ontological context (the path from the
root concept e) shared by e
? and e. Thus, in the context of having recently referred
to someone in Homeroom 2 at a particular campus in a school activities database,
other students in the same homeroom or other activities at the same campus should
be accessible without giving an explicit back up directive at each branch in the ontology.
To see the value of implicit upward transitions, compare Example (1) to a directive that
makes upward transitions explicit using the keyword back (similar to ?..? in the syntax of
Unix paths) to exit the homeroom two and Clark folders:
(2) set Crookston campus homeroom two Clark to back back sports football captain
or if starting from the Duluth campus sports football directory:
(3) set back back back Crookston campus homeroom two Clark to back back sports
football captain
Instead of requiring explicit back keywords, these upward transitions can be implic-
itly composed with downward transitions, resulting in transitions from source eS1 to
destination eS via some ancestor eS0 :
UP-lM(eS1 , eS2 ) = eS s.t. ?eS0 S0?S1, S0?S, lM(eS0 , e) = eS (25)
The composed transition function finds a referent eS0 which subsumes both eS1 and eS,
then finds an ordinary (downward) transition l connecting eS0 to eS. The result is a UP-l
transition to every immediate child of an ancestor a referent (or in genealogical terms,
331
Computational Linguistics Volume 35, Number 3
Figure 8
Upward and downward transitions in a sample student activities world model. Downward
transitions (a) define basic sub-type relations. Upward transitions (b) relate sibling, ancestor, and
(great-great-...-)aunt/uncle concepts. The entire model is reachable from any given referent via
these two kinds of transitions.
to every sibling, ancestor, and sibling of ancestor), making these contextually salient
concepts immediately accessible without explicit back-stepping (see Figure 8b).
Downward transitions are ordinary properties, as defined in the first case of
Equation (12) in Section 4.1.
5.2 Scaling to Richer Domains
Although navigation in this domain is constrained to tree-like graphs, this domain tests
all of the features of a referential semantic language model that would be required
332
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
in richer domains. As described in Section 4, rich domains (in particular, first-order
domains, in which users can describe sets of individuals as referents) are mapped to
transition edges on a simple graph, similar to the tree-like graphs used in this ontology.
In first-order domains, the size of this graph may be exponential on the number of
individuals in the world model. But once the number of referents exceeds the size of the
decoder beam, the time performance of the recognizer is constrained not by the number
of entities in the world model, but by the beam width and the number of outgoing
relations (labels) that can be traversed from each hypothesis. In a first-order system, just
as in the simple ontology navigation system evaluated here, this number of relations
is constrained to the set of words defined by the user up to that point. In both cases,
although the interface may be used to describe any one of an arbitrarily large set of
referents, the number of referents that can be evoked at the next time step is bounded by
a constant.
When this model is extended to first-order or continuous domains, the time re-
quired to calculate sets of individuals or hypothetical planner states that result from
a transition may be nontrivial, because it may not be possible in such domains to retain
the entire referent transition model in memory. In first-order domains, for example, this
may require evaluating certain binary relations over all pairs of individuals in the world
model, with time complexity proportional to the square of the size of the world model
domain. Fortunately the model described herein, like most generative languagemodels,
hypothesizes words before recognizing them. This means a recognizer based on this
model will be able to compute transitions thatmight follow a hypothesizedword during
the time that word is being recognized. If just the current set of possible transitions
is known (say, these have already been pre-fetched into a cache), the set of outgoing
transitions that will be required at some time following one of these current transitions
can be requested as soon as the beginning of this transition is hypothesized?as soon
as any word associated with this transition makes its way onto the decoder beam.
From this point, the recognizer will have the entire duration of the word to compute
(in parallel, in a separate thread, or on a separate server) the set of outgoing transitions
that may follow this word. In other words, the model described herein may be scaled to
richer domains because it is amenable to parallelization.
5.3 World Model
The student activities ontology used in this evaluation is a taxonomic world model
definedwith upward and downward transitions as described in Section 5.1. It organizes
extracurricular activities under subcategories (e.g., offense ? football ? sports), and
organizes students into homerooms, in which context they can be identified by a single
(first or last) name. Every student or activity is an entity e in the set of entities E , and
relations l are subcategory labels or student names.
5.3.1 World Model M240. In the original student activities world model M240, a total
of 240 entities were created in E : 158 concepts (groups or positions) and 82 instances
(students), each connected via a labeled arc from a parent concept.
Because a world model in this framework is a weighted set of labeled arcs, it is
possible to calculate a meaningful perplexity statistic for transitions in this model,
assuming all referents are equally likely to be a source. The perplexity of this world
model (the average number of departing arcs) is 16.79, after inserting ?UP? arcs as
described in Section 5.1.
333
Computational Linguistics Volume 35, Number 3
5.3.2 WorldModelM4175.An expanded version of the students ontology,M4175, includes
4,175 entities from 717 concepts and 3,458 instances. This model contains M240 as a
subgraph, so that the same directives may be used in either domain; but it expands
M240 from above, with additional campuses and schools, and below, with additional
students in each class. The perplexity of this world model was 37.77, after inserting
?UP? arcs as described in Section 5.1.
5.4 Test Corpus
A corpus of 144 test sentences (no training sentences) was collected from seven native
English speakers (5male, 2 female), whowere asked tomake specific edits to the student
activities ontology described previously. The subjects were all graduate students and
native speakers of English, from various parts of the United States. The edit directives
were recorded as isolated utterances, not as part of an interactive dialogue, and the
target concepts were identified by name in written prompts, so the corpus has much of
the character of read speech. The average sentence length in this collection is 7.17 words.
5.5 Acoustic Model
Baseline and test versions of this system were run using a Recurrent Neural Network
(RNN) acoustic model (Robinson 1994). This acoustic model performs competitively
with multi-state triphone models based on multivariate Gaussian mixtures, but has the
advantage of using only uniphones with single subphone states. As a result, less of the
HMM trellis beam is occupied with subphone variations, so that a larger number of
semantically distinct hypotheses may be considered at each frame.
Each model was evaluated using parameters trained from the TIMIT corpus of
read speech (Fisher et al 1987). This corpus yields several thousand examples for each
of the relatively small set of single-state uniphones used in the RNN model. Read
speech is also appropriate training data for this evaluation, because the test subjects
are constrained to perform fixed edit tasks given written prompts, and the number of
reasonable ways to perform these tasks is limited by the ontology, so hesitations and
disfluencies are relatively rare.
5.6 Phone and Subphone Models
The language model used in these experiments is decomposed into five hierarchic
levels, each with referent e and ordinary FSA state q components, as described in
Section 4.2. The top three levels of this model represent syntactic states as q (derived
from regular expressions defined in Section 4.3) and associated semantic referents as e.
The bottom two levels represent pronunciation and subphone states as q, and ignore e.
Transitions across pronunciation states are defined in terms of sequences of phones
associated with a word via a pronunciation model. The pronunciation model used in
these experiments is taken from the CMU ARPABET dictionary (Weide 1998). Transi-
tions across subphone states are defined in terms of sequences of subphones associated
with a phone. Because this evaluation used an acoustic model trained on the TIMIT
corpus (Fisher et al 1987), the TIMIT phone set was used as subphones. In most cases,
these subphones map directly to ARPABET phones, so each subphone HMM consists of
a single, final state; but in cases of plosive phones (B, D, G, K, P, and T), the subphone
HMM consists of a stop subphone (e.g., bcl) followed by a burst subphone (e.g., b).
334
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Referents are ignored in both the phone and subphone models, and therefore do not
need to be calculated.
State transitions within the phone level P?Pron-Trans (q
4
?,t | q
4
?,t?1) deterministically ad-
vance along a sequence of phones in a pronunciation; and initial phone sequences de-
pend on words in higher-level syntactic states q3?,t, via a pronunciation model ?Pron-Init:
P?RSLM-? (?
4
t |?
5
t ?
4
t ?
4
t?1 ?
3
t )
def
=
?
?
?
if f 5?,t= 0, f
4
?,t= 0 : [q
4
?,t= q
4
?,t?1]
if f 5?,t= 1, f
4
?,t= 0 : P?Pron-Trans (q
4
?,t | q
4
?,t?1)
if f 5?,t= 1, f
4
?,t= 1 : P?Pron-Init (q
4
?,t | q
3
?,t)
(26)
The student activities domain was developed with no synonymy?only one word de-
scribes each semantic relation. Alternate pronunciations are modeled using a uniform
distribution over all listed pronunciations.
Initialization and transition of subphone sequences depend on the phone at the
current time step and the subphone at the previous time step. This model was trained
directly using relative frequency estimation on the TIMIT corpus itself:
P?RSLM-? (?
5
t |?
6
t ?
5
t ?
5
t?1 ?
4
t )
def
= P?(q5?,t | q
4
?,t q
5
?,t?1) (27)
5.7 Syntax and Reference Models
The three upper levels of the HHMM comprise the syntactic and referential portion of
the language model. Concept error rate tests were performed on three baseline and test
versions of this portion of the language model, using the same acoustic, phone, and
subphone models, as described in Sections 5.5 and 5.6.
5.7.1 Language Model ?LM-Sem. First, the syntactic and referential portion of the language
model was implemented as described in Section 4.2. A subset of the regular expres-
sion grammar appears in Figure 9. Any nondeterminism resulting from disjunction or
Kleene-star repetition in the regular expressions was handled in?Syn-Trans using uniform
distributions over all available following states. Distributions over regular expression
expansions in ?Syn-Init were uniform over all available expansions. Distributions over
labels in?Ref-Init were also uniform over all labels departing the entity referent condition
that were compatible with the FSA state category generated by ?Syn-Init.
Figure 9
Sample grammar for student activities domain. Relations l?, l? = IDENTITY unless otherwise
specified.
335
Computational Linguistics Volume 35, Number 3
5.7.2 Language Model ?LM-NoSem. Second, in order to evaluate the contribution of refer-
ential semantics to recognition, a baseline version of the model was tested with all
relations defined to be equivalent to NIL, returning e at each depth and time step,
with all relation labels reachable in M from e. This has the effect of eliminating all
semantic constraints from the recognizer, while preserving the relation labels of the
original model as a resource from which to calculate concept error rate. The decoding
equations and grammar inModel?LM-NoSem are therefore the same as inModel?LM-Sem;
only the domain of possible referents is restricted.
Again, distributions over state transitions, expansions, and outgoing labels in
?Syn-Trans, ?Syn-Init, and ?Ref-Init are uniform over all available options.
5.7.3 Language Model ?LM-Trigram. Finally, the referential semantic language model (Lan-
guage Model ?LM-Sem) was compiled into a word trigram model, in order to test how
well the model would function as a pre-process to a conventional trigram-based speech
recognizer. This was done by iterating over all possible sequences of hidden state
transitions starting from every possible configuration of referents and FSA states on
a stack of depth D (where D = 3):
ht = ?wt?1,wt? (28)
P(ht | ht?1) = P(wt?1 wt |wt?2 wt?1) = P(wt |wt?2 wt?1) (29)
def
=
?
?t?2..t
?
wt?2,wt?1
P?Uniform (?t?2) ? [wt?2=W(q
1..D
?,t?2)]
?P?LM-Sem (?t?1 |?t?2) ? [wt?1=W(q
1..D
?,t?1)]
?P?LM-Sem (?t |?t?1) ? [wt=W(q
1..D
?,t )]
(30)
First, every valid combination of syntactic categories was calculated in a depth-
first search using ?LM-NoSem. Then every combination of three referents from M240 was
hypothesized as a possible referent configuration. A complete set of possible initial
values for ?t?2 was then filled with combinations from the set of syntactic category
configuration crossed with the set of referent configurations. From each possible ?t?2,
?LM-Sem was consulted to give a distribution over ?t?1 (assuming a word-level transition
occurs, with f 4?,t?1 = 1), and then again from each possible configuration of ?t?1 to give
a distribution over ?t (again assuming a word-level transition). The product of these
transition probabilities was then calculated and added to a trigram count, based on the
words wt?2, wt?1, and wt occurring in ?t?2, ?t?1, and ?t. These trigram counts were then
normalized over wt?2 and wt?1 to give P(wt |wt?2 wt?1).
5.8 Results
The following results report Concept Error Rate (CER), as the sum of the percentages of
insertions, deletions, and substitutions required to transform the most likely sequence
of relation labels hypothesized by the system into the hand-annotated transcript, ex-
pressed as a percentage of the total number of labels in the hand-annotated transcript.
Because there are few semantically unconstrained function words in this domain, this is
essentially word error rate, with a few multi-word labels (e.g., first chair, homeroom two)
concatenated together.
5.8.1 Language Model ?LM-Sem and World Model M240. Results using Language Model
?LM-Sem with the 240-entity world model (M240) show an overall 17.1% CER (Table 2).
336
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Table 2
Per-subject results for Language Model ?LM-Sem withM240.
subject % correct % substitute % delete % insert CER %
0 83.8 14.1 2.1 2.8 19.0
1 73.2 20.3 6.5 5.8 32.7
2 90.2 7.8 2.0 0.7 10.5
3 88.1 9.3 2.7 0.7 12.6
4 88.4 10.3 1.4 3.4 15.1
5 90.8 8.5 0.7 7.0 16.2
6 90.6 8.6 0.7 3.6 12.9
all 86.4 11.3 2.3 3.4 17.1
Table 3
Per-subject results for Language Model ?LM-Sem withM4175.
subject % correct % substitute % delete % insert CER %
0 85.2 14.1 0.7 2.1 16.9
1 70.6 25.5 3.9 7.2 36.6
2 86.9 9.2 3.9 3.9 17.0
3 86.8 11.3 2.0 2.0 15.2
4 83.6 14.4 2.1 6.9 23.3
5 89.4 9.9 0.7 3.5 14.1
6 89.9 9.4 0.7 5.0 15.1
all 84.5 13.5 2.1 4.4 19.9
Here the size of the vocabulary was roughly equal to the number of referents in the
world model. The sentence error rate for this experiment was 59.44%.
5.8.2 Language Model ?LM-Sem and World ModelM4175. With the number of entities (and
words) increased to 4,175 (M4175), the CER increases slightly to 19.9% (Table 3). Here
again, the size of the vocabulary was roughly equal to the number of referents in the
world model. The sentence error rate for this experiment was 62.24%. Here, the use of a
world model (Language Model ?LM-Sem) with no linguistic training data is comparable
to that reported for other large-vocabulary systems (Seneff et al 2004; Lemon and
Gruenstein 2004), which were trained on sample sentences.
5.8.3 LanguageModel?LM-NoSem with noWorldModel. In comparison, a baseline using only
the grammar and vocabulary from the students domainM240 without any world model
information and no linguistic training data (Language Model ?LM-NoSem) scores 43.5%
(Table 4).9 The sentence error rate for this experiment was 93.01%.
Ignoring the world model significantly raises error rates compared to Model
?LM-Sem (p < 0.01 using pairwise t-test against Language model ?LM-Sem with M240,
grouping scores by subject), suggesting that syntactic constraints are poor predictors of
9 Ordinarily a syntactic model would be interpolated with word n-gram probabilities derived from corpus
training, but in the absence of training sentences these statistics cannot be included.
337
Computational Linguistics Volume 35, Number 3
Table 4
Per-subject results for Language Model ?LM-NoSem.
subject % correct % substitute % delete % insert CER %
0 57.0 35.9 7.0 12.7 55.6
1 49.0 41.2 9.8 13.7 64.7
2 71.9 18.3 9.8 6.5 34.6
3 69.5 26.5 4.0 9.3 39.7
4 67.8 28.8 3.4 13.7 45.9
5 79.6 19.0 1.4 7.0 27.5
6 75.5 22.3 2.2 10.8 35.3
all 67.1 27.5 5.5 10.5 43.5
concepts without considering reference. But this is not surprising: because the grammar
by itself does not constrain the set of ontology labels that can be used to construct a
path, the perplexity of this model is 240 (reflecting a uniform distribution over nearly
the entire lexicon), whereas the perplexity ofM240 is only 16.79.
5.8.4 Language Model ?LM-Trigram and World Model M240. In order to test how well the
model would function as a pre-process to a conventional trigram-based speech recog-
nizer, the referential semantic languagemodel (LanguageModel?LM-Sem) was compiled
into a word trigram model. This word trigram language model (Language Model
?LM-Trigram), compiled from the referential semantic model (in the 240-entity domain),
shows a concept error rate of 26.6% on the students experiment (Table 5). The sentence
error rate for this experiment was 66.43%.
Using trigram context (Language Model ?LM-Trigram) similarly shows statistically
significant increases in error over Language Model ?LM-Sem with M240 (p = 0.01 using
pairwise t-test, grouping scores by subject), showing that referential context is also
more predictive than word n-grams derived from referential context. Moreover, the
compilation to trigrams required to build Language Model ?LM-Trigram is expensive
(requiring several hours of pre-processing) because it must consider all combinations
of entities in the world model. This would make the pre-compiled model impractical in
mutable domains.
Table 5
Per-subject results for Language Model ?LM-Trigram withM240.
subject % correct % substitute % delete % insert CER %
0 76.1 19.0 4.9 5.6 29.6
1 56.9 24.8 18.3 12.4 44.4
2 81.7 9.2 9.2 0.0 18.3
3 83.4 13.9 2.7 2.0 18.5
4 79.5 13.0 7.5 11.0 31.5
5 86.6 10.6 2.8 0.7 14.1
6 83.5 14.4 2.2 0.7 17.3
all 78.1 15.0 6.9 4.7 26.6
338
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
Table 6
Experimental results with four model configurations.
experiment correct substitute delete insert CER
?LM-Sem,M240 86.4 11.3 2.3 3.4 17.1
?LM-Sem,M4175 84.5 13.5 2.1 4.4 19.9
?LM-NoSem 67.1 27.5 5.5 10.5 43.5
?LM-Trigram,M240 78.1 15.0 6.9 4.7 26.6
5.8.5 Summary of Results. Results in Table 6 summarize the results of the four
experiments.
Some of the erroneously hypothesized directives in this domain described im-
plausible edits: for example, making one student a subset of another student. Domain
information or meta-data could eliminate some of these kinds of errors, but in content-
creation applications it is not always possible to provide this information in advance;
and given the subtle nature of the effect of this information on recognition, it is not clear
that users would want to manage it themselves, or allow it to be automatically induced
without supervision.10 In any case, the comparison described in this section to a non-
semantic model?LM-NoSem suggests that the worldmodel by itself is able to apply useful
constraints in the absence of domain knowledge. This suggests that, in an interpolated
approach, direct world model information may relieve some of the burden on authored
or induced domain knowledge to perform robustly, so that this domain knowledge may
be authored more sparsely or induced more conservatively than it otherwise might.
All evaluations ran in real time on a 4-processor dual-core 2.6GHz server, with a
beam width of 1,000 hypotheses per frame. Differences in runtime performance were
minimal, even between the simple trigrammodel and HHMM-based referential seman-
tic language models. This was due to two factors:
1. All recognizers were run with the same beam width. Although it might
be possible to narrow the beam width to produce faster than real-time
performance for some models, widening the beam beyond 1,000 did not
return significant reductions in CER in the experiments described herein.
2. The implementation of the Viterbi decoder used in these experiments was
optimized to skip combinations of joint variable values that would result
in zero probability transitions (which is a reasonable optimization for any
factored time-series model), significantly decreasing runtime for HHMM
recognition.
5.8.6 Statistical Significance vs. Magnitude of Gain. The experiments described in this
article show a statistically significant increase in accuracy due to the incorporation of
referential semantic information into speech decoding. But these results should not be
interpreted to demonstrate any particular magnitude of error reduction (as might be
claimed for the introduction of head words into parsing models, for example).
10 Ehlen et al (2008) provide an example of a user interface for managing imperfect automatically-induced
information about task assignments from meeting transcripts, which is much more concrete than the kind
of domain knowledge inference considered here.
339
Computational Linguistics Volume 35, Number 3
First, this is because the acoustic model used in these experiments was trained on
a relatively small corpus (6,000 utterances), which introduces the possibility that the
acoustic model was under-trained. As a result, the error rates for both baseline and
test systems may be greater here than if a larger training corpus had been used, so the
performance gain due to the introduction of referential semantics may be overstated.
Second, these experiments were designed with relatively strong referential con-
straints (a tree-like ontology, with a perplexity of about 17 for M240) and relatively
weak syntactic constraints (allowing virtually any sequence of relation labels, with a
much higher perplexity of about 240), in order to highlight differences due to referential
semantics. In general use, recognition accuracy gains due to the incorporation of ref-
erential semantic information will depend crucially on the relative perplexity of the
referential constraints combined with syntactic constraints, compared to that of syntac-
tic constraints alone. This paper has argued that in content-creation applications this
difference can be manipulated and exploited?in fact, by reorganizing folders into a
binary branching tree (with perplexity 2), a user could achieve nearly perfect speech
recognition?but in applications involving fixed ontologies and purely hypothetical
directives, as in database query applications, gains may be minimal or nonexistant.
6. Conclusion and Future Work
This article has described a referential semantic language model that achieves recogni-
tion accuracy favorably comparable to a pre-compiled trigram baseline in user-defined
domains with no available domain-specific training corpora, through the use of ex-
plicit hypothesized semantic referents. This architecture requires that the interfaced
application make available a queryable world model, but the combined phonological,
syntactic, and referential semantic decoding process ensures the world model is only
queriedwhen necessary, allowing accurate real time performance even in large domains
containing several thousand entities.
The framework described in this article is defined over first-order sets (of individu-
als), making transition functions over referents equivalent to expressions in first-order
logic. This framework can be extended to model other kinds of references (e.g., to time
intervals or events) by casting them as individuals (Hobbs 1985).
The system as defined herein also has some ability to recognize referents con-
strained by quantifiers: for example, the directory containing two files. Because its referents
are reified sets, the system can naturally model relations that are sensitive to cardinality
(self-transitioning if the set has N or greater individuals, transitioning to e? otherwise).
But a dynamic view of the referential semantics of nested quantifiers requires referents
to be indexed to particular iterations of quantifiers at higher levels of nesting in the
HHMM hierarchy (corresponding to higher-scoping quantifiers). Extending the system
to dynamically interpret nested quantifiers therefore requires that all semantic opera-
tions preserve an ?iteration context? of nested outer-quantified individuals for each
inner-quantified individual. This is left for future work.
Some analyses of phenomena like intensional or non-inherent adjectives?for ex-
ample, toy in toy guns, which are not actually guns; or old in old friends, who are
not necessarily elderly (Peters and Peters 2000)?involve referents corresponding to
second-order sets (this allows these adjectives to be composed before being applied to
a noun: old but casual friend). Unfortunately, extending the framework described in this
article to use a similarly explicit representation of second- or higher-order sets would
be impractical. Not only would the number of possible second- or higher-order sets
be exponentially larger than the number of possible first-order sets (which is already
340
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
exponential on the number of individuals), but the length of the description of each
referent itself would be exponential on the number of individuals (whereas the list of
individuals describing a first-order referent is merely linear).
The definition of semantic interpretation as a transition function does support
interesting extensions to hypothetical reasoning and planning beyond the standard
closed-world model-theoretic framework, however. Recall the sentence go to the package
data directory and hide the executable files, or equivalently, in the package data directory,
hide the executable files, exemplifying the continuous context-sensitivity of the referential
semantic language model. Here, the system focuses on the contents of this directory
because a sequence of transitions resulting from the combined phonological, syntactic,
and referential semantic context of the sentence led it to this state. One may characterize
the referential semantic transitions leading to this state as a hypothetical sequence of
change directory actions moving the active directory of the interface to this directory (for
the purpose of understanding the consequences of the first part of this directive). The
hypothesized context of this directory is then a world state or planning state resulting
from these actions. Thus characterized, the referential semantic decoder is performing
a kind of statistical plan recognition (Blaylock and Allen 2005). By viewing referents
as world states, or as having world-state components, it would then be possible to use
logical conclusions of other types of actions as implicit constraints?e.g., unpack the tar
file and hide the executable [which will result from this unpacking]?without adding extra
functionality to the recognizer implementation. Similarly, referents for hypothetical
objects like the noun phrase a tar file in the directive create a tar file, are not part of the
world model when the user describes them.
Recognizing references to these hypothetical states and objects requires a capacity
to dynamically generate referents not in the current world model. The domain of
referents in this extended system is therefore unbounded. Fortunately, as mentioned
in Section 5.2, the number of referents that can be generated at each time step is still
bounded by a constant, equal to the recognizer?s beam width multiplied by the num-
ber of traversable relation labels. This means that distributions over outgoing relation
labels are still well-defined for each referential state. The only difference is that, when
modeling hypothetical referents, these distributions must be calculated dynamically.
Finally, this article has primarily focused on connecting an explicit representation
of referential semantics to speech recognition decisions. Ordinarily this is thought of
as being mediated by syntax, which is covered in this article only through a rela-
tively simple framework of bounded recursive HHMM state transitions. However, the
bounded HHMM representation used in this paper has been applied (without seman-
tics) to rich syntactic parsing as well, using a transformed grammar to minimize stack
usage to cases of center-expansion (Schuler et al 2008). Coverage experiments with this
transformed grammar demonstrated that over 97% of the large syntactically annotated
Penn Treebank (Marcus, Santorini, andMarcinkiewicz 1994) could be parsed using only
three elements of stack memory, with four elements giving over 99% coverage. This
suggests that the relatively tight bounds on recursion described in this paper might be
expressively adequate if syntactic states are defined using this kind of transform.
This transform model (again, without semantics) was then further applied to pars-
ing speech repairs, in which speakers repeat or edit mistakes in their directives: for
example, select the red, uh, the blue folder (Miller and Schuler 2008). The resulting system
models incomplete disfluent constituents using transitions associated with ordinary
fluent speech until the repair point (the uh in the example), then processes the speech
repair using only a small number of learned repair reductions. Coverage results for
the same transform model on the Penn Treebank Switchboard Corpus of transcribed
341
Computational Linguistics Volume 35, Number 3
spontaneous speech showed a similar three- to four-element memory requirement. If
this HHMM speech repair model were combined with the HHMM model of referen-
tial semantics described in this article, referents associated with ultimately disfluent
constituents could similarly be recognized using referential transitions associated with
ordinary fluent speech until the repair point, then reduced using a repair rule that
discards the referent. These results suggest that an HHMM-based semantic framework
such as the one described in this article may be psycholinguistically plausible.
Acknowledgments
The authors would like to thank the
anonymous reviewers for their input. This
research was supported by National Science
Foundation CAREER/PECASE award
0447685. The views expressed are not
necessarily endorsed by the sponsors.
References
Aist, Gregory, James Allen, Ellen Campana,
Carlos Gallo, Scott Stoness, Mary Swift,
and Michael Tanenhaus. 2007. Incremental
understanding in human?computer
dialogue and experimental evidence
for advantages over nonincremental
methods. In Proceedings of DECALOG,
pages 149?154, Trento.
Baker, James. 1975. The Dragon system: an
overivew. IEEE Transactions on Acoustics,
Speech and Signal Processing, 23(1):24?29.
Bilmes, Jeff and Chris Bartels. 2005.
Graphical model architectures for speech
recognition. IEEE Signal Processing
Magazine, 22(5):89?100.
Blaylock, Nate and James Allen. 2005.
Recognizing instantiated goals using
statistical methods. In IJCAI Workshop
on Modeling Others from Observations
(MOO-2005), pages 79?86, Edinburgh.
Bos, Johan. 1996. Predicate logic unplugged.
In Proceedings of the 10th Amsterdam
Colloquium, pages 133?143, Amsterdam.
Brachman, Ronald J. and James G. Schmolze.
1985. An overview of the kl-one
knowledge representation system.
Cognitive Science, 9(2):171?216.
Brown-Schmidt, Sarah, Ellen Campana, and
Michael K. Tanenhaus. 2002. Reference
resolution in the wild: Online
circumscription of referential domains in a
natural interactive problem-solving task.
In Proceedings of the 24th Annual Meeting of
the Cognitive Science Society, pages 148?153,
Fairfax, VA.
Church, Alonzo. 1940. A formulation of the
simple theory of types. Journal of Symbolic
Logic, 5(2):56?68.
Dale, Robert and Nicholas Haddock. 1991.
Content determination in the generation of
referring expressions. Computational
Intelligence, 7(4):252?265.
DeVault, David and Matthew Stone. 2003.
Domain inference in incremental
interpretation. In Proceedings of ICoS,
pages 73?87, Nancy.
Ehlen, Patrick, Matthew Purver, John
Niekrasz, Stanley Peters, and Kari Lee.
2008. Meeting adjourned: Off-line
learning interfaces for automatic meeting
understanding. In Proceedings of the
International Conference on Intelligent User
Interfaces, pages 276?284, Canary Islands.
Fisher, William M., Victor Zue, Jared
Bernstein, and David S. Pallet. 1987. An
acoustic?phonetic data base. Journal of the
Acoustical Society of America, 81:S92?S93.
Frege, Gottlob. 1892. Uber sinn und
bedeutung. Zeitschrift fur Philosophie und
Philosophischekritik, 100:25?50.
Gorniak, Peter and Deb Roy. 2004. Grounded
semantic composition for visual scenes.
Journal of Artificial Intelligence Research,
21:429?470.
Groenendijk, Jeroen and Martin Stokhof.
1991. Dynamic predicate logic. Linguistics
and Philosophy, 14:39?100.
Haddock, Nicholas. 1989. Computational
models of incremental semantic
interpretation. Language and Cognitive
Processes, 4:337?368.
Hobbs, Jerry R. 1985. Ontological
promiscuity. In Proceedings of ACL,
pages 61?69, Chicago, IL.
Hobbs, Jerry R., Douglas E. Appelt,
John Bear, David Israel, Megumi
Kameyama, Mark Stickel, and
Mabry Tyson. 1996. Fastus: A cascaded
finite-state transducer for extracting
information from natural-language
text. In Yves Schabes, editor, Finite
State Devices for Natural Language
Processing. MIT Press, Cambridge, MA,
pages 383?406.
Hobbs, Jerry R., Mark Stickel, Douglas E.
Appelt, and Paul Martin. 1993.
Interpretation as abduction. Artificial
Intelligence, 63:69?142.
Jelinek, Frederick, Lalit R. Bahl, and Robert L.
Mercer. 1975. Design of a linguistic
342
Schuler, Wu, and Schwartz A Framework for Fast Incremental Interpretation
statistical decoder for the recognition of
continuous speech. IEEE Transactions on
Information Theory, 21:250?256.
Krahmer, Emiel, Sebastiaan van Erk, and
Andre Verleg. 2003. Graph-based
generation of referring expressions.
Computational Linguistics, 29(1):53?72.
Lemon, Oliver and Alexander Gruenstein.
2004. Multithreaded context for
robust conversational interfaces:
Context-sensitive speech recognition and
interpretation of corrective fragments.
ACM Transactions on Computer-Human
Interaction, 11(3):241?267.
Marcus, Mitch. 1980. A Theory of Syntactic
Recognition for Natural Language. MIT
Press, Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1994. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Martin, Charles and Christopher Riesbeck.
1986. Uniform parsing and inferencing
for learning. In Proceedings of AAAI,
pages 257?261, Philadelphia, PA.
Mellish, Chris. 1985. Computer Interpretation
of Natural Language Descriptions. Wiley,
New York.
Miller, George and Noam Chomsky. 1963.
Finitary models of language users. In
R. Luce, R. Bush, and E. Galanter, editors,
Handbook of Mathematical Psychology,
volume 2. John Wiley, New York,
pages 419?491.
Miller, Tim and William Schuler. 2008.
A unified syntactic model for parsing
fluent and disfluent speech. In
Proceedings of the 46th Annual Meeting
of the Association for Computational
Linguistics (ACL ?08) pages 105?108,
Columbus, OH.
Montague, Richard. 1973. The proper
treatment of quantification in ordinary
English. In J. Hintikka, J. M. E. Moravcsik,
and P. Suppes, editors, Approaches
to Natural Language. D. Riedel,
Dordrecht, pages 221?242. Reprinted in
R. H. Thomason ed., Formal Philosophy,
Yale University Press, New Haven,
CT, 1994.
Murphy, Kevin P. and Mark A. Paskin. 2001.
Linear time inference in hierarchical
HMMs. In Proceedings of NIPS,
pages 833?840, Vancouver.
Peters, Ivonne and Wim Peters. 2000.
The treatment of adjectives in simple:
Theoretical observations. In Proceedings
of LREC, paper # 366, Athens.
Pulman, Steve. 1986. Grammars, parsers
and memory limitations. Language and
Cognitive Processes, 1(3):197?225.
Robinson, Tony. 1994. An application of
recurrent nets to phone probability
estimation. In IEEE Transactions on
Neural Networks, 5:298?305.
Seneff, Stephanie, Chao Wang, Lee
Hetherington, and Grace Chung. 2004.
A dynamic vocabulary spoken dialogue
interface. In Proceedings of ICSLP,
pages 1457?1460, Jeju Island.
Schuler, William. 2001. Computational
properties of environment-based
disambiguation. In Proceedings of ACL,
pages 466?473, Toulouse.
Schuler, William, Samir AbdelRahman,
Tim Miller, and Lane Schwartz. 2008.
Toward a psycholinguistically-motivated
model of language. In Proceedings of
COLING, pages 785?792, Manchester, UK.
Schuler, William and Tim Miller. 2005.
Integrating denotational meaning into a
DBN language model. In Proceedings
of the 9th European Conference on Speech
Communication and Technology /
6th Interspeech Event (Eurospeech/
Interspeech?05), pages 901?904, Lisbon.
Tanenhaus, Michael K., Michael J.
Spivey-Knowlton, Kathy M. Eberhard,
and Julie E. Sedivy. 1995. Integration of
visual and linguistic information in
spoken language comprehension.
Science, 268:1632?1634.
Tarski, Alfred. 1933. Prace Towarzystwa
Naukowego Warszawskiego, Wydzial III Nauk
Matematyczno-Fizycznych, 34. Translated as
?The concept of truth in formalized
languages?, in J. Corcoran, editor, Logic,
Semantics, Metamathematics: Papers from
1923 to 1938. Hackett Publishing Company,
Indianapolis, IN, 1983, pages 152?278.
Weide, R. L. 1998. Carnegie Mellon
University Pronouncing Dictionary v0.6d.
Available at www.speech.cs.cmu.edu/
cgi-bin/cmudict.
Wilensky, Robert, Yigal Arens, and David
Chin. 1984. Talking to UNIX: An overview
of UC. Communications of the ACM,
27(6):574?593.
Young, S. L., A. G. Hauptmann, W. H. Ward,
E. T. Smith, and P. Werner. 1989. High
level knowledge sources in usable speech
recognition systems. Communications
of the ACM, 32(2):183?194.
343

Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25?28,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
Demonstration of Joshua: An Open Source Toolkit
for Parsing-based Machine Translation
?
Zhifei Li, Chris Callison-Burch, Chris Dyer
?
, Juri Ganitkevitch
+
, Sanjeev Khudanpur,
Lane Schwartz
?
, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University
? Computational Linguistics and Information Processing Lab, University of Maryland
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University
? Natural Language Processing Lab, University of Minnesota
Abstract
We describe Joshua (Li et al, 2009a)
1
,
an open source toolkit for statistical ma-
chine translation. Joshua implements all
of the algorithms required for transla-
tion via synchronous context free gram-
mars (SCFGs): chart-parsing, n-gram lan-
guage model integration, beam- and cube-
pruning, and k-best extraction. The toolkit
also implements suffix-array grammar ex-
traction and minimum error rate training.
It uses parallel and distributed computing
techniques for scalability. We also pro-
vide a demonstration outline for illustrat-
ing the toolkit?s features to potential users,
whether they be newcomers to the field
or power users interested in extending the
toolkit.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high barrier
to entry for other researchers, and makes experi-
ments difficult to duplicate and compare. In this
paper, we describe Joshua, a Java-based general-
purpose open source toolkit for parsing-based ma-
chine translation, serving the same role as Moses
(Koehn et al, 2007) does for regular phrase-based
machine translation.
?
This research was supported in part by the Defense Ad-
vanced Research Projects Agency?s GALE program under
Contract No. HR0011-06-2-0001 and the National Science
Foundation under grants No. 0713448 and 0840112. The
views and findings are the authors? alone.
1
Please cite Li et al (2009a) if you use Joshua in your
research, and not this demonstration description paper.
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: Joshua?s codebase consists of
a separate Java package for each major aspect
of functionality. This way, researchers can focus
on a single package of their choosing. Fuur-
thermore, extensible components are defined by
Java interfaces to minimize unintended inter-
actions and unseen dependencies, a common hin-
drance to extensibility in large projects. Where
there is a clear point of departure for research,
a basic implementation of each interface is
provided as an abstract class to minimize
work necessary for extensions.
End-to-end Cohesion: An MT pipeline con-
sists of many diverse components, often designed
by separate groups that have different file formats
and interaction requirements. This leads to a large
number of scripts for format conversion and to
facilitate interaction between the components, re-
sulting in untenable and non-portable projects, and
hindering repeatability of experiments. Joshua, on
the other hand, integrates the critical components
of an MT pipeline seamlessly. Still, each compo-
nent can be used as a stand-alone tool that does not
rely on the rest of the toolkit.
Scalability: Joshua, especially the decoder, is
scalable to large models and data sets. For ex-
ample, the parsing and pruning algorithms are im-
plemented with dynamic programming strategies
and efficient data structures. We also utilize suffix-
array grammar extraction, parallel/distributed de-
coding, and bloom filter language models.
Joshua offers state-of-the-art quality, having
been ranked 4th out of 16 systems in the French-
English task of the 2009 WMT evaluation, both in
automatic (Table 1) and human evaluation.
25
System BLEU-4
google 31.14
lium 26.89
dcu 26.86
joshua 26.52
uka 25.96
limsi 25.51
uedin 25.44
rwth 24.89
cmu-statxfer 23.65
Table 1: BLEU scores for top primary systems on
the WMT-09 French-English Task from Callison-
Burch et al (2009), who also provide human eval-
uation results.
2.1 Joshua Toolkit Features
Here is a short description of Joshua?s main fea-
tures, described in more detail in Li et al (2009a):
? Training Corpus Sub-sampling: We sup-
port inducing a grammar from a subset
of the training data, that consists of sen-
tences needed to translate a particular test
set. To accomplish this, we make use of the
method proposed by Kishore Papineni (per-
sonal communication), outlined in further de-
tail in (Li et al, 2009a). The method achieves
a 90% reduction in training corpus size while
maintaining state-of-the-art performance.
? Suffix-array Grammar Extraction: Gram-
mars extracted from large training corpora
are often far too large to fit into available
memory. Instead, we follow Callison-Burch
et al (2005) and Lopez (2007), and use a
source language suffix array to extract only
rules that will actually be used in translating
a particular test set. Direct access to the suffix
array is incorporated into the decoder, allow-
ing rule extraction to be performed for each
input sentence individually, but it can also be
executed as a standalone pre-processing step.
? Grammar formalism: Our decoder as-
sumes a probabilistic synchronous context-
free grammar (SCFG). It handles SCFGs
of the kind extracted by Hiero (Chiang,
2007), but is easily extensible to more gen-
eral SCFGs (as in Galley et al (2006)) and
closely related formalisms like synchronous
tree substitution grammars (Eisner, 2003).
? Pruning: We incorporate beam- and cube-
pruning (Chiang, 2007) to make decoding
feasible for large SCFGs.
? k-best extraction: Given a source sentence,
the chart-parsing algorithm produces a hy-
pergraph representing an exponential num-
ber of derivation hypotheses. We implement
the extraction algorithm of Huang and Chi-
ang (2005) to extract the k most likely deriva-
tions from the hypergraph.
? Oracle Extraction: Even within the large
set of translations represented by a hyper-
graph, some desired translations (e.g. the ref-
erences) may not be contained due to pruning
or inherent modeling deficiency. We imple-
ment an efficient dynamic programming al-
gorithm (Li and Khudanpur, 2009) for find-
ing the oracle translations, which are most
similar to the desired translations, as mea-
sured by a metric such as BLEU.
? Parallel and distributed decoding: We
support parallel decoding and a distributed
language model that exploit multi-core and
multi-processor architectures and distributed
computing (Li and Khudanpur, 2008).
? Language Models: We implement three lo-
cal n-gram language models: a straightfor-
ward implementation of the n-gram scoring
function in Java, capable of reading stan-
dard ARPA backoff n-gram models; a na-
tive code bridge that allows the decoder to
use the SRILM toolkit to read and score n-
grams
2
; and finally a Bloom Filter implemen-
tation following Talbot and Osborne (2007).
? Minimum Error Rate Training: Joshua?s
MERT module optimizes parameter weights
so as to maximize performance on a develop-
ment set as measured by an automatic evalu-
ation metric, such as BLEU. The optimization
consists of a series of line-optimizations us-
ing the efficient method of Och (2003). More
details on the MERT method and the imple-
mentation can be found in Zaidan (2009).
3
2
The first implementation allows users to easily try the
Joshua toolkit without installing SRILM. However, users
should note that the basic Java LM implementation is not as
scalable as the SRILM native bridge code.
3
The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
26
? Variational Decoding: spurious ambiguity
causes the probability of an output string
among to be split among many derivations.
The goodness of a string is measured by
the total probability of its derivations, which
means that finding the best output string is
computationally intractable. The standard
Viterbi approximation is based on the most
probable derivation, but we also implement
a variational approximation, which considers
all the derivations but still allows tractable
decoding (Li et al, 2009b).
3 Demonstration Outline
The purpose of the demonstration is 4-fold: 1) to
give newcomers to the field of statistical machine
translation an idea of the state-of-the-art; 2) to
show actual, live, end-to-end operation of the sys-
tem, highlighting its main components, targeting
potential users; 3) to illustrate, through visual aids,
the underlying algorithms, for those interested in
the technical details; and 4) to explain how those
components can be extended, for potential power
users who want to be familiar with the code itself.
The first component of the demonstration will
be an interactive user interface, where arbitrary
user input in a source language is entered into a
web form and then translated into a target lan-
guage by the system. This component specifically
targets newcomers to SMT, and demonstrates the
current state of the art in the field. We will have
trained multiple systems (for multiple language
pairs), hosted on a remote server, which will be
queried with the sample source sentences.
Potential users of the system would be inter-
ested in seeing an actual operation of the system,
in a similar fashion to what they would observe
on their own machines when using the toolkit. For
this purpose, we will demonstrate three main mod-
ules of the toolkit: the rule extraction module, the
MERT module, and the decoding module. Each
module will have a separate terminal window ex-
ecuting it, hence demonstrating both the module?s
expected output as well as its speed of operation.
In addition to demonstrating the functionality
of each module, we will also provide accompa-
nying visual aids that illustrate the underlying al-
gorithms and the technical operational details. We
will provide visualization of the search graph and
(Software and documentation at: http://cs.jhu.edu/
?
ozaidan/zmert.)
the 1-best derivation, which would illustrate the
functionality of the decoder, as well as alterna-
tive translations for phrases of the source sentence,
and where they were learned in the parallel cor-
pus, illustrating the functionality of the grammar
rule extraction. For the MERT module, we will
provide figures that illustrate Och?s efficient line
search method.
4 Demonstration Requirements
The different components of the demonstration
will be spread across at most 3 machines (Fig-
ure 1): one for the live ?instant translation? user
interface, one for demonstrating the different com-
ponents of the system and algorithmic visualiza-
tions, and one designated for technical discussion
of the code. We will provide the machines our-
selves and ensure the proper software is installed
and configured. However, we are requesting that
large LCD monitors be made available, if possi-
ble, since that would allow more space to demon-
strate the different components with clarity than
our laptop displays would provide. We will also
require Internet connectivity for the live demon-
stration, in order to gain access to remote servers
where trained models will be hosted.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
27
We will rely on 3 workstations: 
one for the instant translation 
demo, where arbitrary input is 
translated from/to a language pair 
of choice (top); one for runtime 
demonstration of the system, with 
a terminal window for each of the 
three main components of the 
systems, as well as visual aids, 
such as derivation trees (left); and 
one (not shown) designated for 
technical discussion of the code.
Remote server 
hosting trained 
translation models
JHU
Grammar extraction
Decoder
M
E
R
T
Figure 1: Proposed setup of our demonstration. When this paper is viewed as a PDF, the reader may
zoom in further to see more details.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March. As-
sociation for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In Proceedings of ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
28
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135?139,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Joshua: An Open Source Toolkit for Parsing-based Machine Translation
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,+ Sanjeev Khudanpur,
Lane Schwartz,? Wren N. G. Thornton, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe Joshua, an open source
toolkit for statistical machine transla-
tion. Joshua implements all of the algo-
rithms required for synchronous context
free grammars (SCFGs): chart-parsing, n-
gram language model integration, beam-
and cube-pruning, and k-best extraction.
The toolkit also implements suffix-array
grammar extraction and minimum error
rate training. It uses parallel and dis-
tributed computing techniques for scala-
bility. We demonstrate that the toolkit
achieves state of the art translation per-
formance on the WMT09 French-English
translation task.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high bar-
rier to entry for other researchers, and makes ex-
periments difficult to duplicate and compare. In
this paper, we describe Joshua, a general-purpose
open source toolkit for parsing-based machine
translation, serving the same role as Moses (Koehn
et al, 2007) does for regular phrase-based ma-
chine translation.
Our toolkit is written in Java and implements
all the essential algorithms described in Chiang
(2007): chart-parsing, n-gram language model in-
tegration, beam- and cube-pruning, and k-best ex-
traction. The toolkit also implements suffix-array
grammar extraction (Lopez, 2007) and minimum
error rate training (Och, 2003). Additionally, par-
allel and distributed computing techniques are ex-
ploited to make it scalable (Li and Khudanpur,
2008b). We have also made great effort to ensure
that our toolkit is easy to use and to extend.
The toolkit has been used to translate roughly
a million sentences in a parallel corpus for large-
scale discriminative training experiments (Li and
Khudanpur, 2008a). We hope the release of the
toolkit will greatly contribute the progress of the
syntax-based machine translation research.1
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: The Joshua code is organized
into separate packages for each major aspect of
functionality. In this way it is clear which files
contribute to a given functionality and researchers
can focus on a single package without worrying
about the rest of the system. Moreover, to mini-
mize the problems of unintended interactions and
unseen dependencies, which is common hinder-
ance to extensibility in large projects, all exten-
sible components are defined by Java interfaces.
Where there is a clear point of departure for re-
search, a basic implementation of each interface is
provided as an abstract class to minimize the work
necessary for new extensions.
End-to-end Cohesion: There are many compo-
nents to a machine translation pipeline. One of the
great difficulties with current MT pipelines is that
these diverse components are often designed by
separate groups and have different file format and
interaction requirements. This leads to a large in-
vestment in scripts to convert formats and connect
the different components, and often leads to unten-
able and non-portable projects as well as hinder-
1The toolkit can be downloaded at http://www.
sourceforge.net/projects/joshua, and the in-
structions in using the toolkit are at http://cs.jhu.
edu/?ccb/joshua.
135
ing repeatability of experiments. To combat these
issues, the Joshua toolkit integrates most critical
components of the machine translation pipeline.
Moreover, each component can be treated as a
stand-alone tool and does not rely on the rest of
the toolkit we provide.
Scalability: Our third design goal was to en-
sure that the decoder is scalable to large models
and data sets. The parsing and pruning algorithms
are carefully implemented with dynamic program-
ming strategies, and efficient data structures are
used to minimize overhead. Other techniques con-
tributing to scalability includes suffix-array gram-
mar extraction, parallel and distributed decoding,
and bloom filter language models.
Below we give a short description about the
main functions implemented in our Joshua toolkit.
2.1 Training Corpus Sub-sampling
Rather than inducing a grammar from the full par-
allel training data, we made use of a method pro-
posed by Kishore Papineni (personal communica-
tion) to select the subset of the training data con-
sisting of sentences useful for inducing a gram-
mar to translate a particular test set. This method
works as follows: for the development and test
sets that will be translated, every n-gram (up to
length 10) is gathered into a map W and asso-
ciated with an initial count of zero. Proceeding
in order through the training data, for each sen-
tence pair whose source-to-target length ratio is
within one standard deviation of the average, if
any n-gram found in the source sentence is also
found in W with a count of less than k, the sen-
tence is selected. When a sentence is selected, the
count of every n-gram in W that is found in the
source sentence is incremented by the number of
its occurrences in the source sentence. For our
submission, we used k = 20, which resulted in
1.5 million (out of 23 million) sentence pairs be-
ing selected for use as training data. There were
30,037,600 English words and 30,083,927 French
words in the subsampled training corpus.
2.2 Suffix-array Grammar Extraction
Hierarchical phrase-based translation requires a
translation grammar extracted from a parallel cor-
pus, where grammar rules include associated fea-
ture values. In real translation tasks, the grammars
extracted from large training corpora are often far
too large to fit into available memory.
In such tasks, feature calculation is also very ex-
pensive in terms of time required; huge sets of
extracted rules must be sorted in two directions
for relative frequency calculation of such features
as the translation probability p(f |e) and reverse
translation probability p(e|f) (Koehn et al, 2003).
Since the extraction steps must be re-run if any
change is made to the input training data, the time
required can be a major hindrance to researchers,
especially those investigating the effects of tok-
enization or word segmentation.
To alleviate these issues, we extract only a sub-
set of all available rules. Specifically, we follow
Callison-Burch et al (2005; Lopez (2007) and use
a source language suffix array to extract only those
rules which will actually be used in translating a
particular set of test sentences. This results in a
vastly smaller rule set than techniques which ex-
tract all rules from the training set.
The current code requires suffix array rule ex-
traction to be run as a pre-processing step to ex-
tract the rules needed to translate a particular test
set. However, we are currently extending the de-
coder to directly access the suffix array. This will
allow the decoder at runtime to efficiently extract
exactly those rules needed to translate a particu-
lar sentence, without the need for a rule extraction
pre-processing step.
2.3 Decoding Algorithms2
Grammar formalism: Our decoder assumes a
probabilistic synchronous context-free grammar
(SCFG). Currently, it only handles SCFGs of the
kind extracted by Heiro (Chiang, 2007), but is eas-
ily extensible to more general SCFGs (e.g., (Gal-
ley et al, 2006)) and closely related formalisms
like synchronous tree substitution grammars (Eis-
ner, 2003).
Chart parsing: Given a source sentence to de-
code, the decoder generates a one-best or k-best
translations using a CKY algorithm. Specifically,
the decoding algorithm maintains a chart, which
contains an array of cells. Each cell in turn main-
tains a list of proven items. The parsing process
starts with the axioms, and proceeds by applying
the inference rules repeatedly to prove new items
until proving a goal item. Whenever the parser
proves a new item, it adds the item to the appro-
priate chart cell. The item also maintains back-
2More details on the decoding algorithms are provided in
(Li et al, 2009a).
136
pointers to antecedent items, which are used for
k-best extraction.
Pruning: Severe pruning is needed in order to
make the decoding computationally feasible for
SCFGs with large target-language vocabularies.
In our decoder, we incorporate two pruning tech-
niques: beam and cube pruning (Chiang, 2007).
Hypergraphs and k-best extraction: For each
source-language sentence, the chart-parsing algo-
rithm produces a hypergraph, which represents
an exponential set of likely derivation hypotheses.
Using the k-best extraction algorithm (Huang and
Chiang, 2005), we extract the k most likely deriva-
tions from the hypergraph.
Parallel and distributed decoding: We also
implement parallel decoding and a distributed
language model by exploiting multi-core and
multi-processor architectures and distributed com-
puting techniques. More details on these two fea-
tures are provided by Li and Khudanpur (2008b).
2.4 Language Models
In addition to the distributed LM mentioned
above, we implement three local n-gram language
models. Specifically, we first provide a straightfor-
ward implementation of the n-gram scoring func-
tion in Java. This Java implementation is able to
read the standard ARPA backoff n-gram models,
and thus the decoder can be used independently
from the SRILM toolkit.3 We also provide a na-
tive code bridge that allows the decoder to use the
SRILM toolkit to read and score n-grams. This
native implementation is more scalable than the
basic Java LM implementation. We have also im-
plemented a Bloom Filter LM in Joshua, following
Talbot and Osborne (2007).
2.5 Minimum Error Rate Training
Johsua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu. The optimization
consists of a series of line-optimizations along
the dimensions corresponding to the parameters.
The search across a dimension uses the efficient
method of Och (2003). Each iteration of our
MERT implementation consists of multiple weight
3This feature allows users to easily try the Joshua toolkit
without installing the SRILM toolkit and compiling the native
bridge code. However, users should note that the basic Java
LM implementation is not as scalable as the native bridge
code.
updates, each reflecting a greedy selection of the
dimension giving the most gain. Each iteration
also optimizes several random ?intermediate ini-
tial? points in addition to the one surviving from
the previous iteration, as an approximation to per-
forming multiple random restarts. More details on
the MERT method and the implementation can be
found in Zaidan (2009).4
3 WMT-09 Translation Task Results
3.1 Training and Development Data
We assembled a very large French-English train-
ing corpus (Callison-Burch, 2009) by conducting
a web crawl that targted bilingual web sites from
the Canadian government, the European Union,
and various international organizations like the
Amnesty International and the Olympic Commit-
tee. The crawl gathered approximately 40 million
files, consisting of over 1TB of data. We converted
pdf, doc, html, asp, php, etc. files into text, and
preserved the directory structure of the web crawl.
We wrote set of simple heuristics to transform
French URLs onto English URLs, and considered
matching documents to be translations of each
other. This yielded 2 million French documents
paired with their English equivalents. We split the
sentences and paragraphs in these documents, per-
formed sentence-aligned them using software that
IBM Model 1 probabilities into account (Moore,
2002). We filtered and de-duplcated the result-
ing parallel corpus. After discarding 630 thousand
sentence pairs which had more than 100 words,
our final corpus had 21.9 million sentence pairs
with 587,867,024 English words and 714,137,609
French words.
We distributed the corpus to the other WMT09
participants to use in addition to the Europarl
v4 French-English parallel corpus (Koehn, 2005),
which consists of approximately 1.4 million sen-
tence pairs with 39 million English words and 44
million French words. Our translation model was
trained on these corpora using the subsampling de-
scried in Section 2.1.
For language model training, we used the
monolingual news and blog data that was as-
sembled by the University of Edinburgh and dis-
tributed as part of WMT09. This data consisted
4The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
137
of 21.2 million English sentences with half a bil-
lion words. We used SRILM to train a 5-gram
language model using a vocabulary containing the
500,000 most frequent words in this corpus. Note
that we did not use the English side of the parallel
corpus as language model training data.
To tune the system parameters we used News
Test Set from WMT08 (Callison-Burch et al,
2008), which consists of 2,051 sentence pairs
with 43 thousand English words and 46 thou-
sand French words. This is in-domain data that
was gathered from the same news sources as the
WMT09 test set.
3.2 Translation Scores
The translation scores for four different systems
are reported in Table 1.5
Baseline: In this system, we use the GIZA++
toolkit (Och and Ney, 2003), a suffix-array archi-
tecture (Lopez, 2007), the SRILM toolkit (Stol-
cke, 2002), and minimum error rate training (Och,
2003) to obtain word-alignments, a translation
model, language models, and the optimal weights
for combining these models, respectively.
Minimum Bayes Risk Rescoring: In this sys-
tem, we re-ranked the n-best output of our base-
line system using Minimum Bayes Risk (Kumar
and Byrne, 2004). We re-score the top 300 trans-
lations to minimize expected loss under the Bleu
metric.
Deterministic Annealing: In this system, in-
stead of using the regular MERT (Och, 2003)
whose training objective is to minimize the one-
best error, we use the deterministic annealing
training procedure described in Smith and Eisner
(2006), whose objective is to minimize the ex-
pected error (together with the entropy regulariza-
tion technique).
Variational Decoding: Statistical models in
machine translation exhibit spurious ambiguity.
That is, the probability of an output string is split
among many distinct derivations (e.g., trees or
segmentations). In principle, the goodness of a
string is measured by the total probability of its
many derivations. However, finding the best string
(e.g., during decoding) is then computationally in-
tractable. Therefore, most systems use a simple
Viterbi approximation that measures the goodness
5Note that the implementation of the novel techniques
used to produce the non-baseline results is not part of the cur-
rent Joshua release, though we plan to incorporate it in the
next release.
System BLEU-4
Joshua Baseline 25.92
Minimum Bayes Risk Rescoring 26.16
Deterministic Annealing 25.98
Variational Decoding 26.52
Table 1: The uncased BLEU scores on WMT-09
French-English Task. The test set consists of 2525
segments, each with one reference translation.
of a string using only its most probable deriva-
tion. Instead, we develop a variational approxima-
tion, which considers all the derivations but still
allows tractable decoding. More details will be
provided in Li et al (2009b). In this system, we
have used both deterministic annealing (for train-
ing) and variational decoding (for decoding).
4 Conclusions
We have described a scalable toolkit for parsing-
based machine translation. It is written in Java
and implements all the essential algorithms de-
scribed in Chiang (2007) and Li and Khudanpur
(2008b): chart-parsing, n-gram language model
integration, beam- and cube-pruning, and k-best
extraction. The toolkit also implements suffix-
array grammar extraction (Callison-Burch et al,
2005; Lopez, 2007) and minimum error rate train-
ing (Och, 2003). Additionally, parallel and dis-
tributed computing techniques are exploited to
make it scalable. The decoder achieves state of
the art translation performance.
Acknowledgments
This research was supported in part by the Defense
Advanced Research Projects Agency?s GALE pro-
gram under Contract No. HR0011-06-2-0001 and
the National Science Foundation under grants
No. 0713448 and 0840112. The views and find-
ings are the authors? alone.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08).
138
Chris Callison-Burch. 2009. A 109 word parallel cor-
pus. In preparation.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, , and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit,
Phuket, Thailand.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT/NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008a. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In Proceedings of AMTA.
Zhifei Li and Sanjeev Khudanpur. 2008b. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,
and Wren Thornton. 2009a. Decoding in joshua:
Open source, parsing-based machine translation.
The Prague Bulletin of Mathematical Linguistics,
91:47?56.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In preparation.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
AMTA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the ACL/Coling.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, Denver, Colorado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
139
Broad-Coverage Parsing Using Human-Like
Memory Constraints
William Schuler?
University of Minnesota
Samir AbdelRahman??
Cairo University
Tim Miller?
University of Minnesota
Lane Schwartz?
University of Minnesota
Human syntactic processing shows many signs of taking place within a general-purpose
short-term memory. But this kind of memory is known to have a severely constrained storage
capacity ? possibly constrained to as few as three or four distinct elements. This article describes
a model of syntactic processing that operates successfully within these severe constraints, by
recognizing constituents in a right-corner transformed representation (a variant of left-corner
parsing) and mapping this representation to random variables in a Hierarchical Hidden Markov
Model, a factored time-series model which probabilistically models the contents of a bounded
memory store over time. Evaluations of the coverage of this model on a large syntactically
annotated corpus of English sentences, and the accuracy of a bounded-memory parsing strategy
based on this model, suggest this model may be cognitively plausible.
1. Introduction
It is an interesting possibility that human syntactic processingmay occur entirely within
a general-purpose short-term memory. Like other short-term memory processes, syn-
tactic processing is susceptible to degradation if short-term memory capacity is loaded,
for example, when readers are asked to retain lists of words while reading (Just
and Carpenter 1992); and memory of words and syntax degrades over time within
and across sentences (Sachs 1967; Jarvella 1971), unlike semantics and discourse
information about referents from other sentences (Ericsson and Kintsch 1995). But
short-term memory is known to have severe capacity limitations of perhaps no more
than three to four distinct elements (Miller 1956; Cowan 2001). These limits may seem
? Department of Computer Science and Engineering, 200 Union St. SE, Minneapolis, MN 55455.
E-mail: schuler@cs.umn.edu; tmill@cs.umn.edu; lane@cs.umn.edu.
?? Computer Science Department, Faculty of Computers and Information, 5 Dr. Ahmed Zewail Street,
Postal Code: 12613, Orman, Giza, Egypt. E-mail: s.abdelrahman@fci-cu.edu.eg.
Submission received: 14 February 2008; revised submission received: 31 October 2008; accepted for
publication: 27 May 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
too austere to process the rich tree-like phrase structure commonly invoked to explain
word-order regularities in natural language.
This article aims to show that they are not. The article describes a comprehension
model, based on a right-corner transform?a reversible tree transform related to the
left-corner transform of Johnson (1998a)?that associates familiar phrase structure trees
with the contents of a memory store of three to four partially completed constituents
over time. Coverage results on the large syntactically annotated Penn Treebank corpus
show a vast majority of naturally occurring sentences can be recognized using a mem-
ory store containing a maximum of only three incomplete constituents, and nearly all
sentences can be recognized using four, consistent with estimates of human short-term
memory capacity.
This transform reduces memory usage in incremental (left to right) processing
by transforming right-branching constituent structures into left-branching structures,
allowing child constituents to be composed with parent constituents before either have
been completely recognized. But because this composition identifies an incomplete
child as the awaited portion of an incomplete parent, it implicitly predicts that this
child constituent will be the rightmost (i.e., last) child of the parent, before this child
has been completely recognized. Parsing accuracy results on the Penn Treebank
using a Hierarchical Hidden Markov Model (Murphy and Paskin 2001)?essentially a
probabilistic pushdown automaton with a bounded pushdown store?show that this
prediction can be reliably learned from training data.
The remainder of this article is organized as follows: Section 2 describes some
related models of human syntactic processing using a bounded memory store; Section 3
describes a Hierarchical Hidden Markov Model (HHMM) framework for statistical
parsing using this bounded store of incomplete constituents; Section 4 describes the
right-corner transform and how it relates conventional phrase structure to incomplete
constituents in a bounded memory store; Section 5 describes an experiment to estimate
the level of coverage of the Penn Treebank corpus that can be achieved using this
transform with various memory limits, given a linguistically motivated binarization of
this corpus; and Section 6 gives accuracy results of this bounded-memorymodel trained
on this corpus, given that some amount of incremental prediction (as described earlier)
must be involved.
2. Bounded-Memory Parsing
One of the earliest bounded-memory parsing models is that of Marcus (1980). This
model maintains a bounded store of complete but unattached constituents as a buffer,
and operates on them using a variety of specialized memory manipulation operations,
deferring certain attachment decisions until the contents of this buffer indicate it is safe
to do so. (In contrast, the model described in this article maintains a store of incom-
plete constituents using ordinary stack-like push and pop operations, defined to allow
constituents to be composed before being completely recognized.) The Marcus parser
provides a bounded-memory explanation for human difficulties in processing garden
path sentences: for example, the horse raced past the barn fell, with intended interpretation
[NP the horse [RC (which was) raced past the barn]] fell (Bever 1970), in which raced seems like
the main verb of the sentence until the word fell is encountered. But this explanation due
to memory exhaustion is not compatible with observations of unproblematic parsing
of sentences such as these when contextual information is provided in advance: for
example, two men on horseback had a race; one went by the meadow, and the other went by the
barn (Crain and Steedman 1985).
2
Schuler et al Parsing Using Human-Like Memory Constraints
Ades and Steedman (1982) introduce the idea of composing incomplete constituents
to reduce storage demands in incremental processing using Combinatorial Catego-
rial Grammar (CCG), avoiding the need to maintain large buffers of complete but
unattached constituents. The right-corner transform described in this article composes
incomplete constituents in very much the same way, but CCG is essentially a compe-
tence model, in that it seeks to unify lexical category representations used in processing
with learned generalizations about argument structure, whereas the model described
herein is exclusively a performance model, allowing generalizations about lexical ar-
gument structures to be learned in some other representation, then combined with
probabilistic information about parsing strategies to yield a set of derived incomplete
constituents. As a result, the model described in this article has a freer hand to satisfy
strict working memory bounds, which may not permit some of the alternative compo-
sition operations proposed in the CCG account, thought to be associated with available
prosody and quantifier scope analyses.1
Johnson-Laird (1983) and Abney and Johnson (1991) propose a pure processing
account of memory capacity limits in parsing ordinary phrase structure trees. The
Johnson-Laird and Abney and Johnson models adopt a left-corner parsing strategy, of
which the right-corner transform introduced in this article is a variant, in order to bring
memory usage for most parsable sentences to within seven or so active or awaited
phrase structure constituents. This account may be used to explain human processing
difficulties in processing triply center-embedded sentences like the rat that the cat that the
dog chased killed ate the malt, with intended interpretation [NP the rat that [NP the cat that [NP
the dog] chased] killed] ate the malt (Chomsky and Miller 1963). But this explanation does
not account for examples of triply center-embedded sentences that typically do not
cause processing problems: [NP that [NP the food that [NP John] ordered] tasted good] pleased
him (Gibson 1991). Moreover, the apparent competition between comprehension of
center-embedded object relatives and retention of unrelated words in general-purpose
memory (Just and Carpenter 1992) suggests that general-purpose memory is (or at
least, can be) used to store incomplete constituents during comprehension. This would
predict three or four elements of reliable storage, rather than seven (Cowan 2001).
The transform-based model described in this article exploits a conception of chunking
(Miller 1956) to combine pairs of active and awaited constituents from the Abney
and Johnson analysis, connected by recognized structure, in order to operate within
estimates of human short-term memory bounds.
Because of these counterexamples to the memory-exhaustion explanation of garden
path and center-embedding difficulties, recent work has turned to explanations other
than memory exhaustion for these phenomena. Lewis and Vasishth (2005) attribute
processing errors to activation interference among stored constituents that have sim-
ilar syntactic and semantic roles. Hale?s surprisal (2001) and entropic model (2006)
link human processing difficulties to significant changes in the relative probability of
competing hypotheses in incremental parsing, such that if activation is taken to be a
mechanism for probability estimation, processing difficulties may be ascribed to the
relatively slow speed of activation change within the brain (or to collapsing activation
when probabilities grow too small, as in the case of garden path sentences). These
models explain many processing difficulties without invoking memory limits, and are
1 The lack of support for some of these available scope analyses may not necessarily be problematic for the
present model. The complexity of interpreting nested raised quantifiers may place them beyond the
capability of human interactive incremental interpretation, but not beyond the capability of post hoc
interpretation (understood after the listener has had time to think about it).
3
Computational Linguistics Volume 36, Number 1
compatible with brain imaging evidence of increased cortical activity and recruitment
of auxiliary brain areas during periods of increased uncertainty in sentence processing
(Just and Varma 2007). But if interference or changing activation is posited as the source
of processing difficulty, and delays are not linked to memory exhaustion per se, then
these theories do not explain how (or whether) syntactic processing operates within
general-purpose short-term memory.
Toward this end, this article will specifically evaluate the claim that syntactic
processing can be performed entirely within general-purpose short-term memory by
using this memory to store unassimilated incomplete syntactic constituents, derived
through a right-corner transform from basic properties of phrase structure trees. As
a probabilistic incremental parser, the model described in this article is compatible
with surprisal-based explanations of processing difficulties; it is, however, in some
sense orthogonal, because it models a different dimension of resource allocation. The
surprisal framework models allocation of processing resources (in this case, activation)
among disjunctions of competing hypotheses, which are maintained for some amount
of time in parallel, whereas the framework described here can be taken to model the
allocation of processing resources (in this case, memory elements) among conjunctions
of incompletely recognized constituents within each competing hypothesis.2 Thus, in
this view, there are twoways to simultaneously activatemultiple concepts: disjunctively
(sharing activation among competing hypotheses) and conjunctively (sharing activation
among unassimilated constituents within a hypothesis). But only the inner conjunctive
allocation corresponds to the familiar discretely bounded store of short-term memory
as described by Miller (1956); the outer disjunctive allocation treats activation as a
continuous resource in which like-valued pockets expand and contract as they are
reinforced or contradicted by incoming observations. Indeed, it would be surprising
if these two dimensions of resource allocation did not exist: the former, because it
would contradict years of observations about the behavior of short-term memory; and
the latter, because it would require neural activation spreading to be instantaneous
and uniform, contradicting most neuropsychological evidence. Levy (2008) compares
the allocation of activation in this kind of framework to the distributed allocation
of resources in a particle filter (Gordon, Salmond, and Smith 1993), an approximate
inference technique for probabilistic time-series models in which particles in a (typically
fixed) reservoir are assigned randomly sampled hypotheses from learned transition
probabilities, essentially functioning as units of activation. The model described in this
paper qualifies this analogy by positing that each individual particle in this reservoir
endorses a coherent hypothesis about the contents of a three- to four-element memory
store at any given time, rather than about an entire unbounded phrase structure tree.3
2 Probability distributions in entropy-based models like Hale?s are typically assumed to be defined over
sets of hypotheses pursued in parallel, but other interpretations (for example, lookahead-based
deterministic models) are possible. The model described in this article is also compatible with
deterministic parsing frameworks, in which case it models allocation of processing resources among
incompletely-recognized constituents within a single non-competing hypothesis.
3 Pure connectionist models of syntactic processing (Elman 1991; Berg 1992; Rohde 2002) attempt to unify
storage of constituent structure with that of ambiguous alternative analyses, but the memory demands of
systems based on this approach typically do not scale well to broad-coverage parsing. Recent results for
using self-organizing maps as a unified memory resource are encouraging (Mayberry and Miikkulainen
2003), but are still limited to parsing relatively short travel planning queries with limited syntactic
complexity. Hybrid systems that generate explicit alternative hypotheses with explicit stacked-up
constituents, and use connectionist models for probability estimation over these hypotheses (Henderson
2004) typically achieve better performance in practice.
4
Schuler et al Parsing Using Human-Like Memory Constraints
Previous memory-based explanations of problematic sentences (explaining garden
path effects as exceeding a bound of four complete but unattached constituents, or ex-
plaining center embedding difficulties as exceeding a bound of seven active or awaited
constituents) have been shown to underestimate human sentence processing capacity
when equally complex but unproblematic sentences were examined. The hypothesis
advanced in this article, that human sentence processing uses general-purpose short-
term memory to store incomplete constituents as defined by a right-corner transform,
leaves the explanation of several negative examples of unparsable garden path and cen-
ter embedding sentences to orthogonal models of surprisal or interference. But in order
to determine whether this right-corner memory hypothesis still underestimates human
sentence processing capacity, a corpus study was performed on two complementary
corpora of transcribed spontaneous speech and newspaper text, manually annotated
with phrase structure trees (Marcus, Santorini, and Marcinkiewicz 1993). These spon-
taneous speech and newspaper text corpora contain only attested positive examples
of parsable sentences, but they may be considered complementary for this purpose
because the complexity of spontaneous speech may somewhat understate human recog-
nition capacity (potentially limiting it to the cost of spontaneously generating sentences
in an unusual social context), and the complexity of newspaper text may somewhat
overstate human recognition capacity (though it is composed and edited to be readable,
it is still composed and edited off-line), so results from these corpora may be taken
together to suggest generous and conservative upper bounds on human processing
capacity.
3. Bounded-Memory Parsing with a Time Series Model
The framework adopted in this article is a factored HMM-like time series model, which
maintains a probability distribution over the contents of a bounded set of random
variables over time, corresponding to hypothesized stores of memory elements. The
random variables in this store may be understood as simultaneous activations in a cog-
nitive model (similar to the superimposed roles described by Smolensky and Legendre
[2006]), and the probability distribution over these stores may be thought of as compet-
ing pockets of activation, as described in the previous section. Some of these variables
persist as elements of the short-term memory store, and some are transient as results of
hypothesized compositions, which are estimated and immediately discarded or folded
into the persistent store according to the dependencies in the model. The variables
have values or contents (or fillers)?in this case incomplete constituent categories?that
change over time, and although these values may be uncertain, the set of hypothesized
contents of this memory store at any given point in time are collectively constrained to
form a coherent (but possibly incomplete) syntactic analysis of a sentence.
The particular model used here is an HHMM (Murphy and Paskin 2001), which
mimics a bounded-memory pushdown automaton (PDA), supporting simple push and
pop operations on a bounded stack-like memory store. A time-series model is used
here instead of an explicit stack machine, first because the probability model is well
defined on a bounded memory store, and second because the plasticity of the random
variables that mimic stack behavior in this model makes the model cross-linguistically
attractive. By evoking additional random variables and dependencies, the model can
be defined (or presumably, trained) to mimic other types of automata, such as extended
pushdown automata (EPDAs) recognizing tree-adjoining languages with crossed and
nested dependencies, as have been hypothesized for languages like Dutch (Shieber
5
Computational Linguistics Volume 36, Number 1
1985). However, the remainder of this article will only discuss random variables and
dependencies necessary to mimic a bounded stack pushdown automaton.
3.1 Hierarchical HMMs
Hierarchical Hidden Markov Models (Murphy and Paskin 2001) are essentially Hidden
Markov Models factored into some fixed number of stack-like elements at each time
step.
HMMs characterize speech or text as sequences of hidden states qt (which may
consist of phones, words, or other hypothesized syntactic or semantic information), and
observed states ot at corresponding time steps t (typically short, overlapping frames
of an audio signal, or words or characters in a text processing application). A most
likely sequence of hidden states q?1..T can then be hypothesized given any sequence of
observed states o1..T:
q?1..T = argmax
q1..T
P(q1..T | o1..T ) (1)
= argmax
q1..T
P(q1..T ) ? P(o1..T | q1..T ) (2)
def
= argmax
q1..T
T
?
t=1
P?A (qt | qt?1) ? P?B (ot | qt) (3)
using Bayes? Law (Equation 2) and Markov independence assumptions (Equation 3)
to define a full P(q1..T | o1..T ) probability as the product of a Language Model (?A)
prior probability P(q1..T )
def
=
?
t P?A (qt | qt?1) and anObservation Model (?B) likelihood
probability P(o1..T | q1..T )
def
=
?
t P?B (ot | qt) (Baker 1975; Jelinek, Bahl, and Mercer 1975).
Language model transitions P?A (qt | qt?1) over complex hidden states qt can be
modeled using synchronized levels of stacked-up component HMMs in an HHMM,
analogous to a shift-reduce parser or pushdown automaton with a bounded stack.
HHMM transition probabilities are calculated in two phases: a ?reduce? phase (result-
ing in an intermediate, transient final-state variable ft), modeling whether component
HMMs terminate; and a ?shift? phase (resulting in a persistent modeled state qt), in
which unterminated HMMs transition and terminated HMMs are re-initialized from
their parent HMMs. Variables over intermediate ft and modeled qt states are factored
into sequences of depth-specific variables?one for each of D levels in the HHMM
hierarchy:
ft = ? f 1t . . . f
D
t ? (4)
qt = ?q1t . . . q
D
t ? (5)
Transition probabilities are then calculated as a product of transition probabilities at
each level, using level-specific ?reduce? ?F,d and ?shift? ?Q,d models:
P?A (qt | qt?1) =
?
ft
P( ft | qt?1) ? P(qt | ft qt?1) (6)
6
Schuler et al Parsing Using Human-Like Memory Constraints
Figure 1
Graphical representation of a Hierarchical Hidden Markov Model. Circles denote random
variables, and edges denote conditional dependencies. Shaded circles denote variables with
observed values.
def
=
?
f 1t..f
D
t
D
?
d=1
P?F,d ( f
d
t | f
d+1
t q
d
t?1q
d?1
t?1 ) ?
D
?
d=1
P?Q,d (q
d
t | f
d+1
t f
d
t q
d
t?1q
d?1
t ) (7)
with f D+1t and q
0
t defined as constants. In these equations, probabilities are marginalized
or summed over all combinations of intermediate variables f 1t ... f
D
t , so only the memory
store contents q1t ...q
D
t persist across time steps.
4 A graphical representation of anHHMM
with three depth levels is shown in Figure 1.
The independence assumptions in this model can be psycholinguistically moti-
vated. Independence across time points t (Equation 3) arise naturally from causality:
Any change to a memory store configuration to generate a configuration at time step
t+ 1 should depend only on the current memory store configuration at time step t;
memory operations should not be able to peek backward or forward in time to consult
past or future memory stores. Independence across depth levels d (Equation 7) arise
naturally from uncertainty about the structure between incomplete constituent chunks
(this property of right-corner transform categories is elaborated in Section 4).5
Shift and reduce probabilities can now be defined in terms of finitely recursive
HMMs with probability distributions over recursive expansion, transition, and reduc-
tion of states at each depth level. In the version of HHMMs used in this paper, each
modeled variable is a syntactic state qdt ? G?G (describing an incomplete constituent
consisting of an active grammatical category from domain G and an awaited grammat-
ical category from domain G?for example, an incomplete constituent S/NP consisting
of an active sentence S awaiting a noun phrase constituent NP); and each intermediate
4 In Viterbi decoding, probabilities over intermediate variables may be maximized rather than
marginalized, but in any case the intermediate variables do not persist.
5 Also, the fact that this is a generative model, in which observations are conditioned on hypotheses, then
flipped using Bayes? Law (Equation 2)?as opposed to a discriminative or conditional model, in which
hypotheses are conditioned directly on observations?is also appealing as a human model, in that it
allows the same architecture to be used for both recognition and generation. This is a desirable property
for modeling split utterances, in which interlocutors complete one another?s sentences (Lerner 1991;
Helasvuo 2004).
7
Computational Linguistics Volume 36, Number 1
variable is a reduction or non-reduction state f dt ? G?{1, 0} (indicating, respectively, a
reduction of incomplete constituent qdt?1 to a complete right child constituent of some
grammatical category from domainG, or a non-reduction of qdt?1 as a unary or left child,
as defined in Section 4). An intermediate variable f dt at depth d may indicate reduction
or non-reduction according to ?F-Reduction,d if there is a reduction at the depth level
immediately below d, but must indicate non-reduction ( f dt = 0) with probability 1 if
there is no reduction below:6
P?F,d ( f
d
t | f
d+1
t q
d
t?1q
d?1
t?1 )
def
=
{
if f d+1t ?G : [ f
d
t = 0]
if f d+1t ?G : P?F-Reduction,d ( f
d
t | q
d
t?1, q
d?1
t?1 )
(8)
where f D+1t = 1 and q
0
t = ROOT.
Shift probabilities over the modeled variable qdt at each level are defined using level-
specific transition ?Q-Transition,d and expansion ?Q-Expansion,d models:
P?Q,d (q
d
t | f
d+1
t f
d
t q
d
t?1q
d?1
t )
def
=
?
?
?
?
?
if f d+1t ?G, f
d
t ?G : [q
d
t = q
d
t?1]
if f d+1t ?G, f
d
t ?G : P?Q-Transition,d (q
d
t | f
d+1
t f
d
t q
d
t?1q
d?1
t )
if f d+1t ?G, f
d
t ?G : P?Q-Expansion,d (q
d
t | q
d?1
t )
(9)
where f D+1t = 1 and q
0
t = ROOT. This model is conditioned on final-state intermediate
variables f dt and f
d+1
t at and immediately below each HHMM level. If there is no re-
duction immediately below a given level (the first case provided), it deterministically
copies the current HHMM state forward to the next time step. If there is a reduction
immediately below the current level but no reduction at the current level (the second
case provided), it transitions the HHMM state at the current level, according to the
distribution ?Q-Transition,d. And if there is a reduction at the current level (the third case
above), it re-initializes this state given the state at the level above, according to the
distribution ?Q-Expansion,d.
Models ?F-Reduction,d, ?Q-Transition,d, and ?Q-Expansion,d are defined directly from train-
ing examples, for example (in the experiments described in this article), using relative
frequency estimation. The overall effect is that higher-level HMMs are allowed to
transition only when lower-level HMMs terminate. An HHMM therefore behaves like
a probabilistic implementation of a shift?reduce parser or pushdown automaton with a
bounded stack, where the maximum stack depth is equal to the number of depth levels
in the HHMM hierarchy.
4. Right-Corner Transform and Incomplete Constituents
The model described in this article recognizes trees in a right-corner transformed
representation to minimize usage of a bounded short-term memory store. This right-
corner transform is a variant of the left-corner transform described by Johnson (1998a),
but whereas the left-corner transform changes left-branching structure into right-
branching structure, the right-corner transform changes right-branching structure into
6 Here [?] is an indicator function: [?] = 1 if ? is true, 0 otherwise.
8
Schuler et al Parsing Using Human-Like Memory Constraints
left-branching structure. Recognition using this transformed grammar, extracted from
a transformed corpus, is similar to recognition using a left-corner parsing strategy (Aho
and Ullman 1972). This kind of strategy was shown to reduce memory requirements
for parsing sentences with mainly left- or right-recursive phrase structure to fewer than
seven active or awaited constituent categories (Abney and Johnson 1991). This is within
Miller?s (1956) estimate of human short-term memory capacity (if memory elements
store individual categories), whereas parsing heavily center-embedded sentences
(known to be difficult for human readers) would require seven or more elements at the
frontier of this capacity.
But recent research suggests that human memory capacity may be limited to as few
as three or four distinct items (Cowan 2001), with longer estimates of seven or more
possibly due to the human capacity to chunk remembered items into associated groups
(Miller 1956). The right-corner strategy described in this paper therefore assumes
constituent categories can similarly be chunked into incomplete constituents A/B formed
by pairing an active category Awith an awaited category B somewhere along the active
category?s right progeny (so, for example, a transitive verb may become an incomplete
constituent VP/NP consisting of an active verb phrase lacking an awaited noun phrase
yet to come).7 These chunked incomplete constituent categories A and B are naturally
related through fixed contiguous phrase structure between them, established during
the course of parsing prior to the beginning of B, and these incomplete constituents can
be composed with other incomplete constituents B/C to form similarly related category
pairs A/C.
These chunks are not only contiguous sections of phrase structure trees, they also
have contiguous string yields, so they correspond to the familiar notion of text chunks
used in shallow parsing approaches (Hobbs et al 1996). For example, a hypothesized
memory store may contain incomplete constituents S/NP (a sentence without a noun
phrase), followed by NP/NN (a noun phrase lacking a common noun), with cor-
responding string yields demand for bonds propped up and the municipal, respectively,
forming a complete contiguous segmentation of a sentence at any point in processing.
Although these two chunks could be composed into an incomplete constituent S/NN,
doing so at this point would close off the possibility of introducing another constituent
between these two, containing the recognized noun phrase as a left child (e.g., demand
for bonds propped up [NP [NP the municipal bonds]?s prices]).
This conception of chunking applied to right-branching progeny in phrase structure
trees does not have the power to eliminate the bounds of a memory store, however. In a
larger cognitive model, syntactic processing is assumed to occur as part of an interactive
semantic interpretation process, in which referents of constituents are calculated as
these constituents are recognized, and are used to constrain subsequent processing
decisions (Tanenhaus et al 1995; Brown-Schmidt, Campana, and Tanenhaus 2002).8
The chunked category pairs A and B in these incomplete constituents A/B result from
successful compositions of other such constituents earlier in the recognition process,
which means that the relationship between the referents of A and B is known and fixed
7 Incomplete constituents may also be defined through a left-corner transform, but left-corner transformed
categories are incomplete in the other direction?a goal category yet to come lacking an
already-recognized constituent?so stored incomplete constituent categories resulting from a left-corner
transform would have the character of future goal events, rather than remembered past events. This is
discussed in greater detail in Section 4.4.
8 This can be implemented in a time-series model by factoring the model to include additional random
variables over referents, as described in Schuler, Wu, and Schwartz (2009).
9
Computational Linguistics Volume 36, Number 1
in any hypothesized incomplete constituent. But syntactic and semantic relationships
between chunks in a hypothesized memory store are unspecified. Chunking beyond
the level of incomplete constituents would therefore involve grouping referents whose
interrelations have not necessarily been established by the parser. Because the set
of referents is presumably much larger than the set of syntactic categories, one may
assume there are real barriers to reliably chunking them in the absence of these fixed
relationships.
There certainly may be cases where syntactically unconnected referents (belonging
to different incomplete constituents) could be grouped together as chunks. But for
simplicity, this article will assume a very strict condition that only a single incomplete
constituent can be stored in each short-term memory element. Experimental results
described in Section 5 suggest that a vast majority of English sentences can be recog-
nized within these human-like memory bounds, even with this strict condition on
chunking. If parsing can be performed in bounded memory under such strict condi-
tions, it can reasonably be assumed to operate at least as well under more permissive
circumstances, where some amount of syntactically-unrelated referential chunking is
allowed.
Several existing incremental systems are organized around a left-corner parsing
strategy (Roark 2001; Henderson 2004). But these systems generally keep large numbers
of constituents open for modifier attachment in each hypothesis. This allows modifiers
to be attached as right children of any such open constituent. But if any number of
open constituents are allowed, then either the assumption that stored elements have
fixed syntactic (and semantic) internal structure will be violated, or the assumption that
syntax operates within a boundedmemory store will be violated, both of which are psy-
cholinguistically attractive as simplifying assumptions. The HHMM model described
in this article upholds both the fixed-element and bounded-memory assumptions by
hypothesizing fixed reductions of right child constituents into incomplete parents in the
same memory element, to make room for new constituents that may be introduced at a
later time. These in-element reductions are defined naturally on phrase structure trees
as the result of aligning right-corner transformed constituent structures to sequences of
random variables in a factored time-series model. The success of this predictive strategy
in corpus-based coverage and accuracy results described in Sections 5 and 6 suggests
that it may be plausible as a cognitive model.
Other accounts may model reductions in bounded memory as occurring as soon
as possible, by maintaining the option of undoing them when necessary (Stevenson
1998). This seems unattractive in the context of an interactive semantic model, however,
where syntactic constituents and semantic referents are composed in tandem, because
potentially very rich referential constraints introduced by composing a child constituent
into a parent would have to be systematically undone. An interesting possibility might
be that the appearance of syntactic restructuring may arise from a collection of hypoth-
esized stores of syntactically fixed incomplete constituents, pursued in parallel. The
results presented in this article suggest that this mechanism is possible, but these two
possibilities might be very difficult to distinguish empirically.
There is also a tradition of defining incomplete constituents as head-driven?
introduced in parsing only at the point in incremental recognition at which they can
be associated with a head word (Gibson 1991: Pritcher 1991: Gorrell 1995). In typically
head-initial languages such as English, incomplete constituents derived from these
head-driven models resemble those derived from a right-corner transform. But head-
driven incomplete constituents do not appear to obey general-purpose memory bounds
in head-final languages such as Japanese, and do not appear to obey attachment prefer-
10
Schuler et al Parsing Using Human-Like Memory Constraints
ences predicted by a head-driven account (Kamide and Mitchell 1999), favoring a pre-
head attachment account, as a right-corner transform would predict.
4.1 Tree Transforms Using Rewrite Rules
The incomplete constituents used in the present model are defined in terms of tree
transforms, which consist of recursive operations that change tree structures into other
tree structures. These transforms are not cognitive processes?syntax in this model is
learned and used entirely as time-series probabilities over random variable values in
the memory store. The role of these transforms is as a means to associate sequences of
configurations of incomplete constituents in a memory store with linguistically familiar
phrase structure representations, such as those studied in competence models or found
in annotated corpora.
The transforms presented in this article will be defined in terms of destructive rewrite
rules applied iteratively to each constituent of a source tree, from leaves to root, and from
left to right among siblings, to derive a target tree. These rewrites are ordered; when
multiple rewrite rules apply to the same constituent, the later rewrites are applied to
the results of the earlier ones.9 For example, the rewrite
A0
. . . A1
?2 ?3
. . . ?
A0
. . . ?2 ?3 . . .
could be used to iteratively eliminate all binary-branching nonterminal nodes in a tree,
except the root.
In the notation used in this article,
 Roman uppercase letters (Ai) are variables matching constituent labels,
 Roman lowercase letters (ai) are variables matching terminal symbols,
 Greek lowercase letters (?i) are variables matching entire subtree structure,
 Roman letters followed by colons, followed by Greek letters (Ai:?i) are
variables matching the label and structure, respectively, of the same
subtree, and
 ellipses (. . . ) are taken to match zero or more subtree structures,
preserving the order of ellipses in cases where there are more than one (as
in the rewrite shown herein).
Many of the transforms used in this article are reversible, meaning that the result
of applying a transform to a tree, then applying the reverse of that transform to the
resulting tree, will be the original tree itself. In general, a transform can be reversed
if the direction of its rewrite rules is reversed, and if each constituent in a target tree
9 The appropriate analogy here is to a Unix sed script, made sensitive to the beginning and end brackets of
a constituent and those of its children.
11
Computational Linguistics Volume 36, Number 1
matches a unique rewrite rule in the reversed transform. The fact that not all rewrites
can be unambiguously matched to HHMM output means that parse accuracy must be
evaluated on partially-binarized gold-standard trees, in order to remove the effect of
this ambiguous matching from the evaluation. This will be discussed in greater detail
in Section 6.
4.2 Right-Corner Transform Using Rewrite Rules
Rewrite rules for the right-corner transform are shown here, first to flatten out right-
recursive structure,
A1
?1 A2
?2 A3
a3
?
A1
A1/A2
?1
A2/A3
?2
A3
a3
,
A1
?1 A2
A2/A3
?2
. . .
?
A1
A1/A2
?1
A2/A3
?2
. . .
then to replace it with left-recursive structure,
A1
A1/A2:?1 A2/A3
?2
?3 . . . ?
A1
A1/A3
A1/A2:?1 ?2
?3 . . .
Here, the first two rewrite rules are applied iteratively (bottom-up on the tree) to flatten
all right recursion, using incomplete constituents to record the original nonterminal
ordering. The second rule is then applied to generate left-recursive structure, preserving
this ordering. Note that the last rewrite leaves a unary branch at the leftmost child of
each flattened node. This preserves the simple category labels of nodes that correspond
directly to nodes in the original tree, so the original tree can be reconstructed when
the right-corner transform concatenates multiple right-recursive sequences into a single
left-recursive sequence.
An example of a right-corner transformed tree is shown in Figure 2(c). An important
property of this transform is that it is reversible. Rewrite rules for reversing a right-
corner transform are simply the converse of those shown here. The correctness of this
can be demonstrated by dividing a tree into maximal sequences of right-recursive
branches (that is, maximal sequences of adjacent right children). The first two ?flatten-
ing? rewrites of the right-corner transform, applied to any such sequence, will replace
the right-branching nonterminal nodes with a flat sequence of nodes labeled with
slash categories, which preserves the order of the nonterminal category symbols in the
original nodes. Reversing this rewrite will therefore generate the original sequence of
nonterminal nodes. The final rewrite similarly preserves the order of these nonterminal
symbols while grouping them from the left to the right, so reversing this rewrite will
reproduce the flattened tree.
12
Schuler et al Parsing Using Human-Like Memory Constraints
Figure 2
A sample phrase structure tree (a) as it appears in the Penn Treebank, (b) after it has been
binarized, and (c) after it has been right-corner transformed.
4.3 Mapping Trees to HHMMDerivations
Any tree can be mapped to an HHMM derivation by aligning the nonterminals with qdt
categories. First, it is necessary to define rightward depth d, right index position t, and
final (rightmost) child status f dt+1, for every nonterminal node A in a tree, where
 d is defined to be the number of right branches between node A and the
root,
13
Computational Linguistics Volume 36, Number 1
 t is defined to be the number of words beneath or to the left of node A, and
 f dt+1 is defined to be 0 if A is a left child, 1 if A is a unary child, and A if A is
right.
Any right-corner transformed tree can then be annotated with these values and rewrit-
ten to define labels and final-state values for every combination of d and t covered by
the tree. This is done using the rewrite rule
d, t,A0, 0
d, t,A1, 1
? d, t,A1, 1
to replace unary branches with f dt+1 flags, and
d, t,A0, f
d
t+1
d, t?,A1, f
d
t?+1 d+1, t,A2,A2
?
d, t,A0, f
d
t+1
d, t?1,A1, 0
d, t?+1,A1, 0
d, t?,A1, f
d
t?+1
d+1, t,A2,A2
to copy stacked-up left child constituents over multiple time steps, while lower-level
(right child) constituents are being recognized. The dashed line on the right side of the
rewrite rule represents the variable number of time steps for a stacked-up higher-level
constituent (as seen, for example, in time steps 4?7 at depth 1 in Figure 3). Coordinates
d, t ? D, and T that have f dt+1=1 are assigned label ???, and coordinates not covered by
the tree are assigned label ??? and f dt+1=1.
The resulting label and final-state values at each node now define a value of qdt
and f dt for each depth d and time step t of the HHMM (see Figure 3). Probabilities for
HHMMmodels ?Q-Expansion,d, ?Q-Transition,d, and ?F-Reduction,d can then be estimated from
these values directly. Like the right-corner transform, this mapping is reversible, so qdt
and f dt values can be taken from a hypothesized most likely sequence and mapped back
Figure 3
Sample tree from Figure 2 mapped to qdt variable positions of an HHMM at each stack depth d
(vertical) and time step t (horizontal). This tree uses only two levels of stack memory. Values for
final-state variables f dt are not shown. Note that the mapping transform omits some nonterminal
labels; labels for these nodes can be reconstructed from their children.
14
Schuler et al Parsing Using Human-Like Memory Constraints
to trees (which can then undergo the reverse of the right-corner transform to become
ordinary phrase structure trees). Inspection of this rewrite rule will reveal the reverse of
this transform simply involves deleting unary-branching sequences that differ only in
the value of t and restoring unary branches when f dt+1=1.
This alignment of right-corner transformed trees also has the interesting property
that the categories on the stack at any given time step represent a segmentation of the
input up to that time step. For example, in Figure 3 at t = 12 the stack contains a sentence
lacking a verb phrase: S/VP (strong demand for . . . bonds), followed by a verb projection
lacking a particle: VBN/PRT (propped).
4.4 Comparison with Left-Corner Transform
A right-corner transform is used in this study, rather than a left-corner transform,
mainly because the right-corner version coincides with an intuition about how incom-
plete constituents might be stored in human memory. Stacked-up constituents in the
right-corner form correspond to chunks of words that have been encountered, rather
than hypothesized goal constituents. Intuitively, in the right-corner view, after a sen-
tence has been recognized, the stack memory contains a complete sentential constituent
(and some associated referent). In the left corner view, on the other hand, the stackmem-
ory after a sentence has been recognized contains only the lower-rightmost constituent
in the corresponding phrase structure tree (see Figure 4). This is because a time-order
Figure 4
A left-corner transformed version of the tree (a) and memory store (b) from Figures 2 and 3.
15
Computational Linguistics Volume 36, Number 1
alignment of a left-corner tree to elements in a bounded memory store corresponds to
a top-down traversal of the tree, whereas a time-order alignment of a right-corner tree
to elements in a bounded memory store corresponds to a bottom-up traversal of the
tree. If referential semantics are assumed to be calculated in tandem (as suggested by
the Tanenhaus et al [1995] results), a top-down traversal through time requires some
effort to reconcile with the traditional compositional semantic notion that the meanings
of constituents are composed from the meanings of their parts (Frege 1892).
4.5 Comparison with CCG
The incomplete constituent categories generated in the right-corner transform have the
same form and much of the same meaning as non-constituent categories in a CCG
(Steedman 2000).10 Both CCG operations of forward function application:
A1  A1/A2 A2
and forward function composition:
A1/A3  A1/A2 A2/A3
appear in the branching structure of right-corner transformed trees. Nested operations
can also occur in CCG derivations:
A1/A2  A1/A2/A3 A3
as well as in right-corner transformed trees (using underscore delimiters to denote
sequences of constituent categories, described in Section 5.1):
A1/A2  A1/A3 A2 A3
There are also correlates of type-raising (unary branches introduced by the right-corner
transform operations described in Section 4):
A1/A2  A3
But, importantly, the right-corner transform generates no correlates to the CCG
operations of backward function application or composition:
A1  A2 A1\A2
A1\A3  A2\A3 A1\A2
This has two consequences. First, right-corner transform models do not introduce am-
biguity between type-raised forward and backward operations, as CCG derivations do.
Second, because leftward dependencies (as between a verb and its subject in English)
cannot be incorporated into lexical categories, right-corner transform models cannot be
taken to explicitly encode argument structure, as CCGs are. The right-corner transform
model described in this article is therefore perhaps better regarded as a performance
model of processing, given subcategorizations specified in some other grammar (such
as in this case the Treebank grammar), rather than a constraint on grammar itself.
10 In fact, one of the original motivations for CCG as a model of language was to minimize stack usage in
incremental processing (Ades and Steedman 1982).
16
Schuler et al Parsing Using Human-Like Memory Constraints
4.6 Comparison with Cascaded FSAs in Information Extraction
Hierarchies of weighted finite-state automata (FSA)?equivalent HMMs may also be
viewed as probabilistic implementations of cascaded FSAs, used for modeling syntax
in information extraction systems such as FASTUS (Hobbs et al 1996). Indeed, the left-
branching sequences of transformed constituents recognized by this model (as shown
in Figure 3) bear a strong resemblance to the flattened phrase structure representations
recognized by cascaded FSA systems, in that most phrases are consolidated to flat
sequences at one hierarchy level. This flat structure is desirable in cascaded FSA systems
because it allows information to be extracted from noun or verb phrases using straight-
forward pattern matching rules, implemented as FSA-equivalent regular expressions.
Like FASTUS, this system produces layers of flat phrases that can be searched
using regular expression pattern-matching rules. It also has a fixed number of levels
and linear-time recognition complexity. But unlike FASTUS, the model described here
can produce?and can be trained on?complete phrase structure trees (accessible by
reversing the transforms described previously).
5. Coverage
The coverage of this model was evaluated on the large Penn Treebank corpus of
syntactically annotated sentences from the Switchboard corpus of transcribed speech
(Godfrey, Holliman, and McDaniel 1992) and the Wall Street Journal (Marcus, Santorini,
and Marcinkiewicz 1993). These sentences were right-corner transformed and mapped
to a time-aligned bounded memory store as described in Section 4 to determine the
amount of memory each sentence would require.
5.1 Binary Branching Structure
In order to obtain a linguistically plausible right-corner transform representation of
incomplete constituents, the corpus is subjected to another pre-process transform to
introduce binary-branching nonterminal projections, and fold empty categories into
nonterminal symbols in amanner similar to that proposed by Johnson (1998b) and Klein
and Manning (2003). This binarization is done in such a way as to preserve linguistic
intuitions of head projection, so that the depth requirements of right-corner transformed
trees will be reasonable approximations to the working memory requirements of a
human reader or listener.
First, ordinary phrases and clauses are decomposed into head projections, each
consisting of one subordinate head projection and one argument or modifier, for
example:
A0
. . . VB:?1 NP:?2 . . .
?
A0
. . . VB
VB:?1 NP:?2
. . .
The selection of head constituents is done using rewrite rules similar to the Magerman-
Black head rules (Magerman 1995). Any new constituent created by this process is
17
Computational Linguistics Volume 36, Number 1
assigned the label of the subordinate head projection. The subordinate projection may
be the left or complete list of head-projection rewrite rules is provided in Appendix A.11
Conjunctions are decomposed into purely right-branching structures using non-
terminals appended with a ?-LIST? suffix:
A0
. . . A1:?1 CC A1:?2
?
A0
. . . A1-LIST
A1:?1 CC A1:?2
A0
. . . A1:?1 A1-LIST:?2
?
A0
. . . A1-LIST
A1:?1 A1-LIST:?2
This right-branching decomposition of conjoined lists is motivated by the general
preference in English toward right branching structure, and the distinction of right
children as ?-LIST? categories is motivated by the asymmetry of conjunctions such as
and and or generally occurring only between constituents at the end of a list, not at the
beginning. (Thus, in decomposing coffee, tea or milk, the words tea or milk form an NP-
LIST constituent, whereas the words coffee, tea do not.)
Empty constituents are removed outright, along with any unary projections that
may arise from this removal. In the case of empty constituents representing traces, the
extracted category label is annotated onto the lowest nonterminal dominating the trace
using the suffix ?-extrX,? where ?X? is the category of the extracted constituent. To
preserve grammaticality, this annotation is then passed up the tree and eliminated when
awh-, topicalized, or othermoved constituent is encountered, in amanner similar to that
used in Head-driven Phrase Structure Grammar (Pollard and Sag 1994), but this does
not affect branching structure.
Together these rewrites remove about 65% of super-binary branches from the un-
processed Treebank. All remaining super-binary branches are ?nominally? decomposed
into right-branching structures by introducing intermediate nodes, each with a label
concatenated from the labels of its children, delimited by underscores:
A0
. . . A1:?1 A2:?2
?
A0
. . . A1 A2
A1:?1 A2:?2
This decomposition is ?nominal? in that the concatenated labels leave the resulting bi-
nary branches just as complex as the original n-ary branches prior to this decomposition.
It is equivalent to leaving super-binary branches intact and using dot rules in parsing
11 Although it is possible that in some cases these rules may generate counterintuitive branching patterns,
inspection of transformed trees during this experiment showed no unusual branching structure, except in
the case of noun sequences in base noun phrases (e.g. [general obligation] bonds or general [obligation
bonds]), which were left flat in the Treebank. Correct binarization of these structures would require
extensive annotator effort. However, because base noun phrases are often very small, and seldom contain
any sub-structure, it seems safe to assume that structural errors in these base noun phrases would not
drastically alter coverage results reported in this section.
18
Schuler et al Parsing Using Human-Like Memory Constraints
(Earley 1970). This decomposition therefore does nothing to reduce sparse data effects
in statistical parsing.
5.2 Coverage Results
Sections 2 and 3 (the standard training set) of the Penn Treebank Switchboard corpus
were binarized as described in Section 5.1, then right-corner transformed and mapped
to elements in a boundedmemory store as described in Section 4. Punctuation added by
transcribers was removed. Coverage of this corpus, in sentences, for a recognizer using
right-corner transform chunking with one to five levels of stack memory, is shown in
Table 1. These results show that a simple syntax-based chunking into incomplete con-
stituents, using the right-corner transform defined in Section 4 of this article, allows a
vast majority of Switchboard sentences (over 99%) to be recognized using three or fewer
elements of memory, with no sentences requiring more than five elements, essentially
as predicted by studies of human short-term memory.
Although spontaneous speech is arguably more natural test data than prepared
speech or edited text, it is possible that coverage results on these data may under-
estimate processing requirements, due to the preponderance of very short sentences
and sentence fragments in spontaneous speech (for example, nearly 30% of sentences in
the Switchboard corpus are only one word long). It may also be argued that coverage
results on this corpus more accurately reflect the complexity of speech planning under
somewhat awkward social circumstances (being asked to start a conversation with
a stranger), which may be more cognitively demanding than recognition. For these
reasons, the right-corner transform chunking was also evaluated on Sections 2?21 (the
standard training set) of the Penn Treebank Wall Street Journal (WSJ) text corpus (see
Table 2, column 1).
The WSJ text corpus results appear to show substantially higher memory
requirements than Switchboard, with only 93% of sentences recognizable using three or
fewer memory elements. But much of this increase is due to arguably arbitrary treebank
conventions in annotating punctuation (for example, commas between phrases are
attached to the leftmost phrase: ([Pierre Vinken . . . [61 years old] ,] joined . . . ) which
can lead to psycholinguistically implausible analyses in which phrases (in this case
61 years old) are center-embedded by lone punctuation marks on one side or the other.
In general, branching structure for punctuation can be difficult to motivate on linguistic
grounds, because punctuation marks do not have lexical projections or argument
structure in most linguistic theories. In spoken language, punctuation corresponds to
Table 1
Percent coverage of right-corner transformed Switchboard Treebank Sections 2?3.
memory capacity (right-corner, no punct) sentences coverage
no stack memory 26,201 28.38%
1 stack element 53,740 58.21%
2 stack elements 85,068 92.14%
3 stack elements 91,890 99.53%
4 stack elements 92,315 99.99%
5 stack elements 92,328 100.00%
TOTAL 92,328 100.00%
19
Computational Linguistics Volume 36, Number 1
Table 2
Percent coverage of left- and right-corner transformed WSJ Treebank Sections 2?21.
memory capacity right-corner, with punct right-corner, no punct left-corner, no punct
sentences coverage sentences coverage sentences coverage
no stack elements 35 0.09% 127 0.32% 127 0.32%
1 stack elements 3,021 7.57% 3,550 8.90% 4,284 10.74%
2 stack elements 21,916 54.95% 25,948 65.06% 26,750 67.07%
3 stack elements 37,203 93.28% 38,948 97.66% 38,853 97.42%
4 stack elements 39,703 99.54% 39,866 99.96% 39,854 99.93%
5 stack elements 39,873 99.97% 39,883 100.00% 39,883 100.00%
6 stack elements 39,883 100.00% - - - -
TOTAL 39,883 100.00% 39,883 100.00% 39,883 100.00%
pauses or patterns of inflection, distributed throughout an utterance. It therefore seems
questionable to account for punctuation marks in a psycholinguistic model as explicit
composable concepts in a memory store. In order to counter possible undesirable
effects of an arbitrary branching analysis of punctuation, a second evaluation of the
model was performed on a version of the WSJ corpus with punctuation removed.
An analysis (Table 2, column 2) of the Penn Treebank WSJ corpus Sections 2?21
without punctuation, using the right-corner transformed trees just described, shows
that 97.66% of trees can be recognized using three hidden levels, and 99.96% can be
recognized using four, and again (similar to the Switchboard results), no sentences
require more than five remembered incomplete constituents. Table 2, column 3, shows
similar results for a left-corner transformed corpus, using left-right reflections of the
rewrite rules presented in Section 4.
Cowan (2001) documents empirically observed short-term memory limits of about
four elements across awide variety of tasks. It is therefore not surprising to find a similar
limit in the memory required to parse the Treebank, assuming elements corresponding
to right-corner-transformed incomplete constituents.
As the table shows, some quintuply center-embedded constituents were found in
both corpora, suggesting that a three- to four-element limit may be soft, and can be
relaxed for short durations. Indeed, all quintuply embedded constituents were only a
few words long. Interestingly, many of the most heavily embedded words seemed to
strongly co-occur, which may suggest that these words arise from fixed expressions and
are not compositional. For example, Figure 5 shows one of the 13 phrase structure trees
in the Switchboard corpus which require five stack elements in right-corner parsing.
The complete sentence is:
So if there?s no one else around and I have a chance to listen to something I?ll turn that on.
If the construction there ?s NP AP in this sentence is parsed non-compositionally as a
single expression (and thus is rendered left-branching by the right-corner transform as
defined in Section 4), the sentence could be parsed using only four memory elements.
Even constrained to only four center embeddings, the existence of such sentences
confounds explanations of the center-embedding difficulties as directly arising from
stack limits in a left-corner (or right-corner) parser (Abney and Johnson 1991). It is
also interesting to note that three of the incomplete constituents in this example are
recursively nested or self-embedded instances of sentential projections, essentially with
20
Schuler et al Parsing Using Human-Like Memory Constraints
Figure 5
A phrase structure tree requiring five stack elements. Categories in bold will be incomplete at a
point after recognizing so if there?s no . . .
the same category, similar to the center-embedded constructions which human readers
found difficult to process. This suggests that restrictions on self-embedding of identical
constituent categories would also fail to predict readability.
Instead, these data seem to argue in favor of an explanation due to probability:
Although the five-element sentences found in the Treebank use mostly common phrase
structure rules, problematic center-embedded sentences like the salmon the man the dog
chased smoked fell may cause difficulty simply because they are examples of an unusual
construction: a nested object relative clause. The fact that this is an unusual construction
may in turn be a result of the fact that speakers tend to avoid nesting object relative
clauses because they can lead to memory exhaustion, though such constructions may
become readable with practice.
6. In-Element Composition Ambiguity and Parsing Accuracy
The right-corner transform described in Section 4 saves memory because it transforms
any right-branching sequence with left-child subtrees into a left-branching sequence of
incomplete constituents, with the same sequence of subtrees as right children. The left-
branching sequences of siblings resulting from this transform can then be composed
bottom-up through time by replacing each left child category with the category of the
resulting parent, within the same memory element (or depth level). For example, in
Figure 6(a) a left-child category NP/NP at time t = 4 is composed with a noun new of
category NP/NNP (a noun phrase lacking a proper noun yet to come), resulting in a
new parent category NP/NNP at time t = 5 replacing the left child category NP/NP in
the topmost d = 1 memory element.
This in-element composition preserves elements of the bounded memory store for
use in processing descendants of this composed constituent, yielding the human-like
memory demands reported in Section 5. But whenever an in-element composition like
this is hypothesized, it isolates an intermediate constituent (in this example, the noun
phrase new york city) from subsequent composition. Allowing access to this intermediate
constituent?for example, to allow new york city to become a modifier of bonds, which
itself becomes an argument of for?requires an analysis in which the intermediate
21
Computational Linguistics Volume 36, Number 1
Figure 6
Alternative analyses of strong demand for new york city ...: (a) using in-element composition,
compatible with strong demand for new york city is ... (in which the demand is for the city); and (b)
using cross-element (or delayed) composition, compatible with either strong demand for new york
city is ... (in which the demand is for the city) or strong demand for new york city bonds is ... (in
which a forthcoming referent?in this case, bonds?is associated with the city, and is in
demand). In-element composition (a) saves memory but closes off access to the noun phrase
headed by city, and so is not incompatible with the ...bonds completion. Cross-element
composition (b) requires more memory, but allows access to the noun phrase headed by city, so
is compatible with either completion. This ambiguity is introduced at t = 4 and propagated until
at least t = 7. An ordinary, non-right-corner stack machine would exclusively use analysis (b),
avoiding ambiguity.
constituent is stored in a separate memory element, shown in Figure 6(b). This creates
a local ambiguity in the parser (in this case, from time step t = 4) that may have to be
propagated across several words before it can be resolved (in this case, at time step
t = 7). This is essentially an ambiguity between arc-eager (in-element) and arc-standard
(cross-element) composition strategies, as described by Abney and Johnson (1991). In
contrast, an ordinary (purely arc-standard) parser with an unbounded stack would only
hypothesize analysis (b), avoiding this ambiguity.12
The right-corner HHMM approach described in this article relies on a learned
statistical model to predict when in-element (arc-eager) compositions will occur, in
addition to hypothesizing parse trees. The model encodes a mixed strategy: with some
probability arc-eager or arc-standard for each possible expansion. Accuracy results on
a right-corner HHMM model trained on the Penn Wall Street Journal Treebank suggest
that this kind of optionally arc-eager strategy can be reliably statistically learned.
6.1 Evaluation
In order to determinewhether amemory-preserving parsing strategy, like the optionally
arc-eager strategy, can be reliably learned, a baseline Cocke-Kasami-Younger (CKY)
parser and bounded-memory right-corner HHMM parser were evaluated on the stan-
dard Penn Treebank WSJ Section 23 parsing task, using the binarized tree set described
in Section 5.2 (WSJ Sections 2?21) as training data. Training examples requiring more
12 It is important to note that neither the right-corner nor left-corner parsing strategy by itself creates this
ambiguity. The ambiguity arises from the decision to use this optionally arc-eager strategy to reduce
memory store allocation in a bounded memory parser. Implementations of left-corner parsers such as
that of Henderson (2004) adopt an arc-standard strategy, essentially always choosing analysis (b), and
thus do not introduce this kind of local ambiguity. But in adopting this strategy, such parsers must
maintain a stack memory of unbounded size, and thus are not attractive as models of human parsing in
short-term memory (Resnik 1992).
22
Schuler et al Parsing Using Human-Like Memory Constraints
than four stack elements were excluded from training, in order to avoid generating
inconsistent model probabilities (e.g., from expansions that could not be re-composed
within the bounded memory store).
Most likely sequences of HHMM stack configurations are evaluated by reversing
the binarization, right-corner, and time-series mapping transforms described in Sec-
tions 4 and 5. But some of the binarization rewrites cannot be completely reversed,
because they cannot be unambiguously matched to output trees. Automatically derived
lexical projections below the annotated phrase level (e.g., binarizations of base noun
phrases) can be completely reversed, because the derived categories are character-
istically labeled with terminal symbols. So, too, can the conjunction and ?nominal?
binarizations described in Section 5.1, because they can be identified by characteristic
?-LIST? and underscore delimiters. But automatically derived projections above the
annotated phrase level cannot be reliably identified in parser output (for example, an
intermediate projection ?S PP S?may or may not be annotated in the corpus). In order
to isolate the evaluation from the effects of these ambiguous matchings, the evaluation
was performed using trees in a partially binarized format, obtained by reversing only
those rewrites that result in unambiguous matches. Evaluating on this partially bina-
rized data does not seem to unfairly increase parsing performance compared to other
published results?quite the contrary: an evaluation using the state-of-the-art Charniak
(2000) parser scores about half a point worse on labeled F-score (89.3% vs. 89.9%) when
its hypotheses and gold standard trees are converted into this format.13
Both CKY baseline and HHMM test systems were run with a simple part of speech
(POS) model using relative frequency estimates from the training set, backed off to a
discriminative (decision tree) model conditioned on the last five letters of each word,
normalized over unigram POS probabilities. The CKY baseline andHHMMresults were
obtained by training and evaluating on binarized trees, which is a necessary condition
for the right-corner transform. The CKY baseline results appear to be better than those
for a baseline probabilistic context-free grammar (PCFG) system reported by Klein and
Manning (2003) using no modifications to the corpus, and no parent or sibling condi-
tioning (see Table 3, top) because the binarization process allows the parser to avoid
some sparse data effects due to large flat branching structures in the Treebank, resulting
in improved parsing accuracy. Klein and Manning note that applying linguistically
motivated binarization transforms can yield substantial improvements in accuracy?as
much as nine points, in their study (in comparison, binarization only seems to improve
accuracy by about seven points above an unmodified baseline in the present study). But
the Klein and Manning results for binarization are provided only for models already
augmented with Markov dependencies (that is, conditioning on parent and sibling
categories, analogous to HHMM dependencies), so it was not possible to compare to
a binarized and un-Markovized benchmark.
The results for HHMM parsing, training, and evaluating on these same binarized
trees (modulo right-corner and variable-mapping transforms) were substantially bet-
ter than binarized CKY, most likely due to the expanded HHMM dependencies on
previous (qdt?1) and parent (q
d?1
t ) variables at each q
d
t . For example, binarized PCFG
probabilities may be defined in terms of three category symbols A, B, and C: P(A 
B C |A); whereas some of the HHMM probabilities are defined in terms of five category
13 This is presumably because the probability that a human annotator will annotate phrase structure
brackets at a particular projection or not is something existing parsers learn and exploit to improve their
accuracy. But it is not clear that this distinction is linguistically motivated.
23
Computational Linguistics Volume 36, Number 1
Table 3
Labeled recall (LR), labeled precision (LP), weighted average (F-score), and parse failure
(% of sentences yielding no tree output) results for basic CKY parser and HHMM parser on
unmodified and binarized WSJ Sections 22 (sentences 1?393: ?devset?) and 23?24 (all sentences).
Results are shown with and without punctuation, compared to Klein and Manning 2003
(KM?03) using baseline and parent+sibling (par+sib) conditioning, and Roark 2001 (R?01) using
parent+sibling conditioning. Baseline CKY and test (parent+sibling) cases for the HHMM
system start out at a higher accuracy than for the Klein-Manning system because the HHMM
system requires binarization of trees, which removes some data sparsity in the raw Treebank
annotation, whereas the Klein-Manning results are computed prior to binarization. Because it is
incremental, the parser occasionally eliminates all continuable analyses from the beam, and
therefore fails to find a parse. HHMM parse failures are accounted as zeros in the recall statistics,
but are also listed separately, because in principle it might be possible to recover useful syntactic
structure from partial sequences.
with punctuation: (?40 wds) LR LP F-score sentence error
failure reduction
KM?03: unmodified, devset ? ? 72.6 0
KM?03: par+sib, devset ? ? 77.4 0 17.5%
CKY: binarized, devset 80.3 79.9 80.1 0.8
HHMM: par+sib, devset 84.1 83.5 83.8 0.5 18.6%
CKY: binarized, sect 23 78.8 79.4 79.1 0.1
HHMM: par+sib, sect 23 83.4 83.7 83.5 0.1 21.1%
no punctuation: (?120 wds) LR LP F fail
R?01: par+sib, sect 23?24 75.2 77.4 ? 0.1
HHMM: par+sib, sect 23?24 77.2 78.3 77.7 0.0
labels: P(A/B |C/D, E) (transitioning from incomplete constituent C/D to incomplete
constituent A/B in the context of an expanding category E). This increases the number
of free parameters (estimated conditional probabilities) in the model,14 but apparently
not to the point of sparsity; this is similar to the effect of horizontal Markovization (con-
ditioning on the sibling category immediately previous to an expanded category) and
vertical Markovization (conditioning on the parent of an expanded category) commonly
used in PCFG parsing models (Collins 1999).
The improvement due to HHMM parsing over the PCFG baseline (18.6% reduction
in error) is comparable to that reported by Klein and Manning for parent and sibling
dependencies (first-order vertical and horizontal Markovization) over a baseline PCFG
without binarization (17.5% reduction in error). However, because it is not possible
to run the HHMM parser without binarization, and because Klein and Manning do
not report results for binarization transforms in the absence of parent and sibling
Markovization, it is potentially misleading to compare the results directly. For example,
it is possible that the binarization transforms described here may have performance-
optimizing effects that are latent in the binarized PCFG, but are brought out in HHMM
parsing.
Results on Section 23 of this corpus show close to 84% recall and precision, compa-
rable to that reported for state-of-the-art cubic-time parsers (with no constant bounds
14 Without punctuation, the HHMMmodel has 50,429 free parameters (including both Q and F models),
whereas the binarized PCFG has 12,373.
24
Schuler et al Parsing Using Human-Like Memory Constraints
on processing storage) using similar configurations of conditioning information, that is,
without lexicalization or smoothing.
Roark (2001) describes a similar incremental parser based on left-corner trans-
formed grammars, and also reports results for parsing with and without parent and
sibling Markovization. Again the performance is comparable under similar conditions
(Table 3, bottom).
This system was run with a beam width of 2,000 hypotheses. This beam width
was selected in order to compare the performance of the bounded-memory model,
which predicts in-element or cross-element composition, with that of conventional
broad-coverage parsers, which also maintain large beams. With better modeling and
vastly more data from which to learn, it is possible that the human processor may
need to maintain far fewer alternative analyses, or perhaps only one, conditioned on
a lookahead window of observations (Henderson 2004).15
These experiments used a maximum stack depth of four, and conditioned expan-
sion and transition probabilities for each qdt on only the portion of the parent category
following the slash (that is, only A2 of A1/A2), in order to avoid sparse data effects.
Examples requiring more than four stack elements were excluded from training. This
is because in the basic relative frequency estimation used here, training examples are
depth-specific. Because the (unpunctuated) training set contains only about a dozen
sentences requiring more than four depth levels, each occupying that level for only a
few words, the data on which the fifth level of this model would be trained are very
sparse. Models at greater stack depths, and models depending on complete parent cate-
gories (or grandparent categories, etc., as in state-of-the-art parsers) could be developed
using smoothing and backoff techniques or feature-based log-linear models, but this is
left for later work (see Section 7).
7. Conclusion
This article has described a model of human syntactic processing that recognizes com-
plete phrase structure trees using only a small store of memory elements of limited
complexity. Sequences of hypothesized contents of this memory store can be mapped to
and from conventional phrase structure trees using a reversible right-corner transform.
If this syntactic processing model is combined with a bounded-memory interpreter
(Schuler, Wu, and Schwartz 2009), however, allowing the contents of this store to be
incrementally interpreted within the same bounded memory, it stands to reason that
complete, explicit phrase structure trees would not need to be constructed at any time
in processing, in keeping with experimental results showing similar lack of retention of
words and syntactic structure during human processing (Sachs 1967; Jarvella 1971).
Initial results show the use of a memory store consisting of only three to four mem-
ory elements within this framework provides nearly complete coverage of the Penn
Treebank Switchboard and WSJ corpora, consistent with recent estimates of general-
purpose short-term memory capacity. This suggests that, unlike some earlier mod-
els, the hypothesis that human sentence processing uses general-purpose short-term
15 Although, if most competing analyses are unconscious, they would be difficult to detect. Formally, the
competing pockets of activation hypothesized in a parallel-processing version of this model could be
arbitrarily small and numerous, but it seems unlikely that very small pockets of activation would persist
for very long (just as low probability analyses would be unlikely to remain on the HHMM beam). This
possibility is discussed in the particle filter account of Levy (2008).
25
Computational Linguistics Volume 36, Number 1
memory to store incomplete constituents, as defined by a right-corner transform, does
not seem to substantially underestimate human processing capacity. Moreover, despite
additional predictions that must take place within this model to manage parsing in such
close quarters, preliminary accuracy results for an unlexicalized, un-smoothed version
of this model, using only a four-element memory store, show close to 84% recall and
precision on the standard parsing evaluation. This result is comparable to that reported
for state-of-the-art cubic-time parsers (with no constant bounds on processing storage)
using similar configurations of conditioning information, namely, without lexicalization
or smoothing.
This model does not attempt to derive processing difficulties frommemory bounds,
following evidence that garden path and center-embedding processing difficulties are
caused by interference or local probability estimates rather than encounters with mem-
ory capacity limits. But this does not mean that memory store capacity and probabilistic
explanations of processing difficulty are completely independent. Probability estima-
tion seems likely to be dependent on structural information from the memory store (for
example, incomplete object relative clauses seem to be very improbable in the context
of other incomplete object relative clauses). As hypotheses use more elements in the
memory store, the distribution over these hypotheses will tend to become broader,
taxing the reservoir of activation capacity, and making it more likely for low proba-
bility hypotheses to disappear, increasing the incidence of garden path errors. Further
investigations into how the memory store elements are allocated in various syntactic
contexts may allow these apparently disparate dimensions of processing capacity to be
unified.
The model described here may be promising as an engineering tool as well. But
to achieve competitive performance with unconstrained state-of-the-art parsers will
require the development of additional approximation algorithms beyond the scope of
this article. This is because most modern parsers are lexicalized, incorporating head-
word dependencies into parsing decisions, and employing finely tuned smoothing and
backoff techniques to integrate these potentially sparse head-word dependencies with
denser unlexicalized models. The bounded-memory right-corner HHMM described
in this article can also be lexicalized in this way, but because head word dependencies
are most straightforwardly defined in terms of top-down PCFG-like dependency
structures, this lexicalization requires the introduction of additional formal machinery
to transform PCFG probabilities into right-corner form (Schuler 2009). In other
words, rather than transforming a training set of trees and mapping them to a time
series model, it is necessary to transform a consistent probabilistically weighted
grammar (in some sense, an infinite set of trees) into appropriately weighted and
consistent right-corner PCFG and HHMM models. This requires the introduction of
an approximate inference algorithm, similar to that used in value iteration (Bellman
1957), which estimates probabilities of infinite left-recursive or right-recursive chains
by exploiting the fact that increasingly longer chains of events contribute exponentially
decreasing probability mass. On top of this, preserving head-word dependencies in
incremental processing also requires the introduction of a framework for storing head
words of modifier constituents that precede the head word of a parent constituent;
including some mechanism to ensure that probability assignments are fairly distributed
among competing hypotheses (e.g., by marginalizing over possible head words) in
cases where the calculation of accurate dependency probabilities must be deferred
until the head word of the parent constituent is encountered. For these reasons, a
complete lexicalized model is considered beyond the scope of this article, and is left for
future work.
26
Schuler et al Parsing Using Human-Like Memory Constraints
Appendix A: Head Transform Rules
The experiments described in this article used a binarization process that included the
following rewrite rules, designed to binarize flat Treebank constituents into linguisti-
cally motivated head projections:
1. NP: right-binarize basal NPs as much as possible; then left-binarize NPs
after left context reduced to nil:
A0=NP|WHNP
. . . A1=[A-Z]*:?1 A2=NN[A-Z]*:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=NP
A1=NN[A-Z]*|NP:?1 A2=PP|S|VP|WHSBAR:?2 . . .
?
A0
A1
A1:?1 A2:?2
. . .
2. VP: left-binarize basal VPs as much as possible; then right-binarize VPs
after right context reduced to nil:
A0=VP|SQ
. . . A1=VB[A-Z]*|BES:?1 A2=[A-Z]*:?2 . . .
?
A0
. . . A1
A1:?1 A2:?2
. . .
A0=VP
. . . A1=ADVP|RB[A-Z]*|PP:?1 A2=VB[A-Z]*|VP:?2
?
A0
. . . A2
A1:?1 A2:?2
3. ADJP: right-binarize basal ADJPs as much as possible; then left-binarize
ADJPs after left context reduced to nil:
A0=ADJP[A-Z]*
. . . A1=RB[A-Z]*:?1 A2=JJ[A-Z]*:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=ADJP
A1=JJ[A-Z]*|ADJP:?1 A2=PP|S:?2 . . .
?
A0
A1
A1:?1 A2:?2
. . .
4. ADVP: right-binarize basal ADVPs as much as possible; then left-binarize
ADVPs after left context reduced to nil:
A0=ADVP
. . . A1=RB[A-Z]*:?1 A2=RB[A-Z]*:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=ADVP
A1=RB[A-Z]*|ADVP:?1 A2=PP|S:?2 . . .
?
A0
A1
A1:?1 A2:?2
. . .
27
Computational Linguistics Volume 36, Number 1
5. PP: left-binarize PPs as much as possible; then right-binarize PPs after
right context reduced to nil:
A0=PP|SBAR
. . . A1=IN|TO:?1 A2=[A-Z]*:?2 . . .
?
A0
. . . A1
A1:?1 A2:?2
. . .
A0=PP
. . . A1=ADVP|RB|PP:?1 A2=PP:?2
?
A0
. . . A2
A1:?1 A2:?2
6. S: group subject NP and predicate VP of a sentence; then group modifiers
to right and left:
A0=S[A-Z]*
. . . A1=NP:?1 A2=VP:?2 . . .
?
A0
. . . S
A1:?1 A2:?2
. . .
A0=S[A-Z]*
. . . A1=ADVP|RB[A-Z]*|PP:?1 A2=VB[A-Z]*|VP:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=S[A-Z]*
. . . A1=ADVP|RB[A-Z]*|PP:?1 A2=A0:?2 . . .
?
A0
. . . A2
A1:?1 A2:?2
. . .
A0=S[A-Z]*
. . . A1=A0:?1 A2=ADVP|RB[A-Z]*|PP:?2 . . .
?
A0
. . . A1
A1:?1 A2:?2
. . .
Acknowledgments
The authors would like to thank
the anonymous reviewers for their input.
This research was supported by National
Science Foundation CAREER/PECASE
award 0447685 and by NASA award
NNX08AC36A. The views expressed are not
necessarily endorsed by the sponsors.
References
Abney, Steven P. and Mark Johnson. 1991.
Memory requirements and local
ambiguities of parsing strategies.
J. Psycholinguistic Research, 20(3):233?250.
Ades, Anthony E. and Mark Steedman. 1982.
On the order of words. Linguistics and
Philosophy, 4:517?558.
Aho, Alfred V. and Jeffery D. Ullman. 1972.
The Theory of Parsing, Translation and
Compiling; Volume. I: Parsing. Prentice-Hall,
Englewood Cliffs, NJ.
Baker, James. 1975. The Dragon system: an
overview. IEEE Transactions on Acoustics,
Speech and Signal Processing, 23(1):24?29.
Bellman, Richard. 1957. Dynamic
Programming. Princeton University Press,
Princeton, NJ.
Berg, George. 1992. A connectionist parser
with recursive sentence structure and
lexical disambiguation. In Proceedings of the
Tenth National Conference on Artificial
Intelligence, pages 32?37, San Jose, CA.
Bever, Thomas G.?1970. The cognitive basis
for linguistic structure. In J. R?. Hayes,
editor, Cognition and the Development of
Language. Wiley, New York, pages 279?362.
Brown-Schmidt, Sarah, Ellen Campana, and
Michael K. Tanenhaus. 2002. Reference
resolution in the wild: Online
circumscription of referential domains in a
28
Schuler et al Parsing Using Human-Like Memory Constraints
natural interactive problem-solving task.
In Proceedings of the 24th Annual Meeting of
the Cognitive Science Society, pages 148?153,
Fairfax, VA.
Charniak, Eugene. 2000. A maximum-
entropy inspired parser. In Proceedings
of the First Meeting of the North American
Chapter of the Association for Computational
Linguistics (ANLP-NAACL?00),
pages 132?139, Seattle, WA.
Chomsky, Noam and George A. Miller.
1963. Introduction to the formal
analysis of natural languages.
In Handbook of Mathematical Psychology.
Wiley, New York, pages 269?321.
Collins, Michael. 1999. Head-driven
statistical models for natural language parsing.
Ph.D. thesis, University of Pennsylvania.
Cowan, Nelson. 2001. The magical
number 4 in short-term memory:
A reconsideration of mental storage
capacity. Behavioral and Brain Sciences,
24:87?185.
Crain, Stephen and Mark Steedman.
1985. On not being led up the garden path:
The use of context by the psychological
syntax processor. In D. R. Dowty,
L. Karttunen, and A. M. Zwicky, editors,
Natural Language Parsing: Psychological,
Computational, and Theoretical Perspectives,
number 1 in Studies in Natural Language
Processing. Cambridge University
Press, Cambridge, pages 320?358.
Earley, Jay. 1970. An efficient context-free
parsing algorithm. CACM, 13(2):94?102.
Elman, Jeffrey L. 1991. Distributed
representations, simple recurrent
networks, and grammatical structure.
Machine Learning, 7:195?225.
Ericsson, K. Anders and Walter Kintsch.
1995. Long-term working memory.
Psychological Review, 102:211?245.
Frege, Gottlob. 1892. Uber sinn
und bedeutung. Zeitschrift fur Philosophie
und Philosophischekritik, 100:25?50.
Gibson, Edward. 1991. A computational theory
of human linguistic processing: Memory
limitations and processing breakdown.
Ph.D. thesis, Carnegie Mellon University.
Godfrey, John J., Edward C. Holliman,
and Jane McDaniel. 1992. Switchboard:
Telephone speech corpus for research
and development. In Proceedings of
ICASSP, pages 517?520, San Francisco, CA.
Gordon, N. J., D. J. Salmond, and A. F. M.
Smith. 1993. Novel approach to nonlinear/
non-gaussian bayesian state estimation.
IEE Proceedings F (Radar and Signal
Processing), 140(2):107?113.
Gorrell, Paul. 1995. Syntax and Parsing.
Cambridge University Press, Cambridge.
Hale, John. 2001. A probabilistic earley parser
as a psycholinguistic model. In Proceedings
of the Second Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 159?166, Pittsburgh, PA.
Hale, John. 2006. Uncertainty about the
rest of the sentence. Cognitive Science,
30(4):609?642.
Helasvuo, Marja-Liisa. 2004. Shared
syntax: the grammar of co-constructions.
Journal of Pragmatics, 36:1315?1336.
Henderson, James. 2004. Lookahead
in deterministic left-corner parsing.
In Proceedings Workshop on Incremental
Parsing: Bringing Engineering and
Cognition Together, pages 26?33, Barcelona.
Hobbs, Jerry R., Douglas E. Appelt, John Bear,
David Israel, Megumi Kameyama, Mark
Stickel, and Mabry Tyson. 1996. Fastus:
A cascaded finite-state transducer for
extracting information from natural-
language text. In Yves Schabes, editor,
Finite State Devices for Natural Language
Processing. MIT Press, Cambridge, MA,
pages 383?406.
Jarvella, Robert J. 1971. Syntactic
processing of connected speech. Journal
of Verbal Learning and Verbal Behavior,
10:409?416.
Jelinek, Frederick, Lalit R. Bahl, and Robert L.
Mercer. 1975. Design of a linguistic
statistical decoder for the recognition
of continuous speech. IEEE Transactions
on Information Theory, 21:250?256.
Johnson, Mark. 1998a. Finite state
approximation of constraint-based
grammars using left-corner grammar
transforms. In Proceedings of COLING/ACL,
pages 619?623, Montreal.
Johnson, Mark. 1998b. PCFG models
of linguistic tree representation.
Computational Linguistics, 24:613?632.
Johnson-Laird, P. N. 1983. Mental Models:
Towards a Cognitive Science of Language,
Inference and Consciousness. Harvard
University Press, Cambridge, MA.
Just, Marcel Adam and Patricia A.
Carpenter. 1992. A capacity theory
of comprehension: Individual differences
in working memory. Psychological Review,
99:122?149.
Just, Marcel Adam and Sashank Varma.
2007. The organization of thinking:
What functional brain imaging
reveals about the neuroarchitecture of
complex cognition. Cognitive, Affective,
& Behavioral Neuroscience, 7:153?191.
29
Computational Linguistics Volume 36, Number 1
Kamide, Yuki and Don C. Mitchell. 1999.
Incremental pre-head attachment in
Japanese parsing. Language and
Cognitive Processes, 14:631?662.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo.
Lerner, Gene H. 1991. On the syntax of
sentences in progress. Language in
Society, 20:441?458.
Levy, Roger. 2008. Modeling the effects of
memory on human online sentence
processing with particle filters. In
Proceedings of NIPS, pages 937?944,
Vancouver.
Lewis, Richard L. and Shravan Vasishth.
2005. An activation-based model of
sentence processing as skilled
memory retrieval. Cognitive Science,
29(3):375?419.
Magerman, David. 1995. Statistical decision-
tree models for parsing. In Proceedings of the
33rd Annual Meeting of the Association
for Computational Linguistics (ACL?95),
pages 276?283, Cambridge, MA.
Marcus, Mitch. 1980. Theory of Syntactic
Recognition for Natural Language. MIT Press,
Cambridge, MA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: the
Penn Treebank. Computational Linguistics,
19(2):313?330.
Mayberry, III, Marshall R. and Risto
Miikkulainen. 2003. Incremental
nonmonotonic parsing through semantic
self-organization. In Proceedings of the
25th Annual Conference of the Cognitive
Science Society, pages 798?803,
Boston, MA.
Miller, George A. 1956. The magical number
seven, plus or minus two: Some limits
on our capacity for processing information.
Psychological Review, 63:81?97.
Murphy, Kevin P. and Mark A. Paskin. 2001.
Linear time inference in hierarchical
HMMs. In Proceedings of NIPS,
pages 833?840, Vancouver.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure
Grammar. Chicago: University of Chicago
Press and Stanford: CSLI Publications.
Pritchett, Bradley L. 1991. Head position
and parsing ambiguity. Journal of
Psycholinguistic Research, 20:251?270.
Resnik, Philip. 1992. Left-corner parsing and
psychological plausibility. In Proceedings
of COLING, pages 191?197, Nantes.
Roark, Brian. 2001. Probabilistic top-down
parsing and language modeling.
Computational Linguistics, 27(2):249?276.
Rohde, Douglas L. T. 2002. A connectionist
model of sentence comprehension and
production. Ph.D. thesis, Computer Science
Department, Carnegie Mellon University.
Sachs, Jacqueline. 1967. Recognition memory
for syntactic and semantic aspects of
connected discourse. Perception and
Psychophysics, 2:437?442.
Schuler, William. 2009. Parsing with a
bounded stack using a model-based
right-corner transform. In Proceedings
of the North American Association for
Computational Linguistics (NAACL ?09),
pages 344?352, Boulder, CO.
Schuler, William, Stephen Wu, and
Lane Schwartz. 2009. A framework for
fast incremental interpretation during
speech decoding. Computational
Linguistics, 35(3):313?343.
Shieber, Stuart. 1985. Evidence
against the context-freeness of natural
language. Linguistics and Philosophy,
8:333?343.
Smolensky, P. and G. Legendre. 2006. The
Harmonic Mind: From Neural Computation
to Optimality-Theoretic Grammar. MIT Press,
Cambridge, MA.
Steedman, Mark. 2000. The Syntactic
Process. MIT Press/Bradford Books,
Cambridge, MA.
Stevenson, Suzanne. 1998. Parsing as
incremental restructuring. In J. D. Fodor
and F. Ferreira, editors, Reanalysis in
Sentence Processing. Kluwer Academic,
Boston, MA, pages 327?363.
Tanenhaus, Michael K., Michael J.
Spivey-Knowlton, Kathy M. Eberhard,
and Julie E. Sedivy. 1995. Integration of
visual and linguistic information in
spoken language comprehension. Science,
268:1632?1634.
30
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620?631,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Incremental Syntactic Language Models for Phrase-based Translation
Lane Schwartz
Air Force Research Laboratory
Wright-Patterson AFB, OH USA
lane.schwartz@wpafb.af.mil
Chris Callison-Burch
Johns Hopkins University
Baltimore, MD USA
ccb@cs.jhu.edu
William Schuler
Ohio State University
Columbus, OH USA
schuler@ling.ohio-state.edu
Stephen Wu
Mayo Clinic
Rochester, MN USA
wu.stephen@mayo.edu
Abstract
This paper describes a novel technique for in-
corporating syntactic knowledge into phrase-
based machine translation through incremen-
tal syntactic parsing. Bottom-up and top-
down parsers typically require a completed
string as input. This requirement makes it dif-
ficult to incorporate them into phrase-based
translation, which generates partial hypothe-
sized translations from left-to-right. Incre-
mental syntactic language models score sen-
tences in a similar left-to-right fashion, and are
therefore a good mechanism for incorporat-
ing syntax into phrase-based translation. We
give a formal definition of one such linear-
time syntactic language model, detail its re-
lation to phrase-based decoding, and integrate
the model with the Moses phrase-based trans-
lation system. We present empirical results
on a constrained Urdu-English translation task
that demonstrate a significant BLEU score im-
provement and a large decrease in perplexity.
1 Introduction
Early work in statistical machine translation viewed
translation as a noisy channel process comprised of
a translation model, which functioned to posit ad-
equate translations of source language words, and
a target language model, which guided the fluency
of generated target language strings (Brown et al,
This research was supported by NSF CAREER/PECASE
award 0447685, NSF grant IIS-0713448, and the European
Commission through the EuroMatrixPlus project. Opinions, in-
terpretations, conclusions, and recommendations are those of
the authors and are not necessarily endorsed by the sponsors or
the United States Air Force. Cleared for public release (Case
Number 88ABW-2010-6489) on 10 Dec 2010.
1990). Drawing on earlier successes in speech
recognition, research in statistical machine trans-
lation has effectively used n-gram word sequence
models as language models.
Modern phrase-based translation using large scale
n-gram language models generally performs well
in terms of lexical choice, but still often produces
ungrammatical output. Syntactic parsing may help
produce more grammatical output by better model-
ing structural relationships and long-distance depen-
dencies. Bottom-up and top-down parsers typically
require a completed string as input; this requirement
makes it difficult to incorporate these parsers into
phrase-based translation, which generates hypothe-
sized translations incrementally, from left-to-right.1
As a workaround, parsers can rerank the translated
output of translation systems (Och et al, 2004).
On the other hand, incremental parsers (Roark,
2001; Henderson, 2004; Schuler et al, 2010; Huang
and Sagae, 2010) process input in a straightforward
left-to-right manner. We observe that incremental
parsers, used as structured language models, pro-
vide an appropriate algorithmic match to incremen-
tal phrase-based decoding. We directly integrate in-
cremental syntactic parsing into phrase-based trans-
lation. This approach re-exerts the role of the lan-
guage model as a mechanism for encouraging syn-
tactically fluent translations.
The contributions of this work are as follows:
? A novel method for integrating syntactic LMs
into phrase-based translation (?3)
? A formal definition of an incremental parser for
1While not all languages are written left-to-right, we will
refer to incremental processing which proceeds from the begin-
ning of a sentence as left-to-right.
620
statistical MT that can run in linear-time (?4)
? Integration with Moses (?5) along with empiri-
cal results for perplexity and significant transla-
tion score improvement on a constrained Urdu-
English task (?6)
2 Related Work
Neither phrase-based (Koehn et al, 2003) nor hierar-
chical phrase-based translation (Chiang, 2005) take
explicit advantage of the syntactic structure of either
source or target language. The translation models in
these techniques define phrases as contiguous word
sequences (with gaps allowed in the case of hierar-
chical phrases) which may or may not correspond
to any linguistic constituent. Early work in statisti-
cal phrase-based translation considered whether re-
stricting translation models to use only syntactically
well-formed constituents might improve translation
quality (Koehn et al, 2003) but found such restric-
tions failed to improve translation quality.
Significant research has examined the extent to
which syntax can be usefully incorporated into sta-
tistical tree-based translation models: string-to-tree
(Yamada and Knight, 2001; Gildea, 2003; Imamura
et al, 2004; Galley et al, 2004; Graehl and Knight,
2004; Melamed, 2004; Galley et al, 2006; Huang
et al, 2006; Shen et al, 2008), tree-to-string (Liu
et al, 2006; Liu et al, 2007; Mi et al, 2008; Mi
and Huang, 2008; Huang and Mi, 2010), tree-to-tree
(Abeille? et al, 1990; Shieber and Schabes, 1990;
Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan
et al, 2006; Nesson et al, 2006; Zhang et al, 2007;
DeNeefe et al, 2007; DeNeefe and Knight, 2009;
Liu et al, 2009; Chiang, 2010), and treelet (Ding
and Palmer, 2005; Quirk et al, 2005) techniques
use syntactic information to inform the translation
model. Recent work has shown that parsing-based
machine translation using syntax-augmented (Zoll-
mann and Venugopal, 2006) hierarchical translation
grammars with rich nonterminal sets can demon-
strate substantial gains over hierarchical grammars
for certain language pairs (Baker et al, 2009). In
contrast to the above tree-based translation models,
our approach maintains a standard (non-syntactic)
phrase-based translation model. Instead, we incor-
porate syntax into the language model.
Traditional approaches to language models in
speech recognition and statistical machine transla-
tion focus on the use of n-grams, which provide a
simple finite-state model approximation of the tar-
get language. Chelba and Jelinek (1998) proposed
that syntactic structure could be used as an alterna-
tive technique in language modeling. This insight
has been explored in the context of speech recogni-
tion (Chelba and Jelinek, 2000; Collins et al, 2005).
Hassan et al (2007) and Birch et al (2007) use
supertag n-gram LMs. Syntactic language models
have also been explored with tree-based translation
models. Charniak et al (2003) use syntactic lan-
guage models to rescore the output of a tree-based
translation system. Post and Gildea (2008) investi-
gate the integration of parsers as syntactic language
models during binary bracketing transduction trans-
lation (Wu, 1997); under these conditions, both syn-
tactic phrase-structure and dependency parsing lan-
guage models were found to improve oracle-best
translations, but did not improve actual translation
results. Post and Gildea (2009) use tree substitution
grammar parsing for language modeling, but do not
use this language model in a translation system. Our
work, in contrast to the above approaches, explores
the use of incremental syntactic language models in
conjunction with phrase-based translation models.
Our syntactic language model fits into the fam-
ily of linear-time dynamic programming parsers de-
scribed in (Huang and Sagae, 2010). Like (Galley
and Manning, 2009) our work implements an in-
cremental syntactic language model; our approach
differs by calculating syntactic LM scores over all
available phrase-structure parses at each hypothesis
instead of the 1-best dependency parse.
The syntax-driven reordering model of Ge (2010)
uses syntax-driven features to influence word order
within standard phrase-based translation. The syn-
tactic cohesion features of Cherry (2008) encour-
ages the use of syntactically well-formed translation
phrases. These approaches are fully orthogonal to
our proposed incremental syntactic language model,
and could be applied in concert with our work.
3 Parser as Syntactic Language Model in
Phrase-Based Translation
Parsing is the task of selecting the representation ??
(typically a tree) that best models the structure of
621
???????
?s?
??0
???????
?s? the
??11
???????
?s? that
??12
???????
?s? president
??13
. . .
???????
the president
??21
???????
that president
??22
???????
president Friday
??23
. . .
???????
president meets
??31
???????
Obama met
??32
. . .
Figure 1: Partial decoding lattice for standard phrase-based decoding stack algorithm translating the German
sentence Der Pra?sident trifft am Freitag den Vorstand. Each node h in decoding stack t represents the
application of a translation option, and includes the source sentence coverage vector, target language n-
gram state, and syntactic language model state ??th . Hypothesis combination is also shown, indicating
where lattice paths with identical n-gram histories converge. We use the English translation The president
meets the board on Friday as a running example throughout all Figures.
sentence e, out of all such possible representations
? . This set of representations may be all phrase
structure trees or all dependency trees allowed by
the parsing model. Typically, tree ?? is taken to be:
?? = argmax
?
P(? | e) (1)
We define a syntactic language model P(e) based
on the total probability mass over all possible trees
for string e. This is shown in Equation 2 and decom-
posed in Equation 3.
P(e) =
?
???
P(?, e) (2)
P(e) =
?
???
P(e | ?)P(?) (3)
3.1 Incremental syntactic language model
An incremental parser processes each token of in-
put sequentially from the beginning of a sentence to
the end, rather than processing input in a top-down
(Earley, 1968) or bottom-up (Cocke and Schwartz,
1970; Kasami, 1965; Younger, 1967) fashion. After
processing the tth token in string e, an incremen-
tal parser has some internal representation of possi-
ble hypothesized (incomplete) trees, ?t. The syntac-
tic language model probability of a partial sentence
e1...et is defined:
P(e1...et) =
?
???t
P(e1...et | ?)P(?) (4)
In practice, a parser may constrain the set of trees
under consideration to ??t, that subset of analyses or
partial analyses that remains after any pruning is per-
formed. An incremental syntactic language model
can then be defined by a probability mass function
(Equation 5) and a transition function ? (Equation
6). The role of ? is explained in ?3.3 below. Any
parser which implements these two functions can
serve as a syntactic language model.
P(e1...et) ? P(?? t) =
?
???? t
P(e1...et | ?)P(?) (5)
?(et, ?? t?1)? ?? t (6)
622
3.2 Decoding in phrase-based translation
Given a source language input sentence f , a trained
source-to-target translation model, and a target lan-
guage model, the task of translation is to find the
maximally probable translation e? using a linear
combination of j feature functions h weighted ac-
cording to tuned parameters ? (Och and Ney, 2002).
e? = argmax
e
exp(
?
j
?jhj(e,f)) (7)
Phrase-based translation constructs a set of trans-
lation options ? hypothesized translations for con-
tiguous portions of the source sentence ? from a
trained phrase table, then incrementally constructs a
lattice of partial target translations (Koehn, 2010).
To prune the search space, lattice nodes are orga-
nized into beam stacks (Jelinek, 1969) according to
the number of source words translated. An n-gram
language model history is also maintained at each
node in the translation lattice. The search space
is further trimmed with hypothesis recombination,
which collapses lattice nodes that share a common
coverage vector and n-gram state.
3.3 Incorporating a Syntactic Language Model
Phrase-based translation produces target language
words in an incremental left-to-right fashion, gen-
erating words at the beginning of a translation first
and words at the end of a translation last. Similarly,
incremental parsers process sentences in an incre-
mental fashion, analyzing words at the beginning of
a sentence first and words at the end of a sentence
last. As such, an incremental parser with transition
function ? can be incorporated into the phrase-based
decoding process in a straightforward manner. Each
node in the translation lattice is augmented with a
syntactic language model state ??t.
The hypothesis at the root of the translation lattice
is initialized with ?? 0, representing the internal state
of the incremental parser before any input words are
processed. The phrase-based translation decoding
process adds nodes to the lattice; each new node
contains one or more target language words. Each
node contains a backpointer to its parent node, in
which ?? t?1 is stored. Given a new target language
word et and ?? t?1, the incremental parser?s transi-
tion function ? calculates ?? t. Figure 1 illustrates
S
NP
DT
The
NN
president
VP
VP
VB
meets
NP
DT
the
NN
board
PP
IN
on
NP
Friday
Figure 2: Sample binarized phrase structure tree.
S
S/NP
S/PP
S/VP
NP
NP/NN
DT
The
NN
president
VP
VP/NN
VP/NP
VB
meets
DT
the
NN
board
IN
on
NP
Friday
Figure 3: Sample binarized phrase structure tree af-
ter application of right-corner transform.
a sample phrase-based decoding lattice where each
translation lattice node is augmented with syntactic
language model state ??t.
In phrase-based translation, many translation lat-
tice nodes represent multi-word target language
phrases. For such translation lattice nodes, ? will
be called once for each newly hypothesized target
language word in the node. Only the final syntac-
tic language model state in such sequences need be
stored in the translation lattice node.
4 Incremental Bounded-Memory Parsing
with a Time Series Model
Having defined the framework by which any in-
cremental parser may be incorporated into phrase-
based translation, we now formally define a specific
incremental parser for use in our experiments.
The parser must process target language words
incrementally as the phrase-based decoder adds hy-
potheses to the translation lattice. To facilitate this
incremental processing, ordinary phrase-structure
trees can be transformed into right-corner recur-
623
r1t?1
r2t?1
r3t?1
s1t?1
s2t?1
s3t?1
r1t
r2t
r3t
s1t
s2t
s3t
et?1 et
. . .
. . .
. . .
. . .
Figure 4: Graphical representation of the depen-
dency structure in a standard Hierarchic Hidden
Markov Model with D = 3 hidden levels that can
be used to parse syntax. Circles denote random vari-
ables, and edges denote conditional dependencies.
Shaded circles denote variables with observed val-
ues.
sive phrase structure trees using the tree transforms
in Schuler et al (2010). Constituent nontermi-
nals in right-corner transformed trees take the form
of incomplete constituents c?/c?? consisting of an
?active? constituent c? lacking an ?awaited? con-
stituent c?? yet to come, similar to non-constituent
categories in a Combinatory Categorial Grammar
(Ades and Steedman, 1982; Steedman, 2000). As
an example, the parser might consider VP/NN as a
possible category for input ?meets the?.
A sample phrase structure tree is shown before
and after the right-corner transform in Figures 2
and 3. Our parser operates over a right-corner trans-
formed probabilistic context-free grammar (PCFG).
Parsing runs in linear time on the length of the input.
This model of incremental parsing is implemented
as a Hierarchical Hidden Markov Model (HHMM)
(Murphy and Paskin, 2001), and is equivalent to a
probabilistic pushdown automaton with a bounded
pushdown store. The parser runs in O(n) time,
where n is the number of words in the input. This
model is shown graphically in Figure 4 and formally
defined in ?4.1 below.
The incremental parser assigns a probability
(Eq. 5) for a partial target language hypothesis, using
a bounded store of incomplete constituents c?/c??.
The phrase-based decoder uses this probability value
as the syntactic language model feature score.
4.1 Formal Parsing Model: Scoring Partial
Translation Hypotheses
This model is essentially an extension of an HHMM,
which obtains a most likely sequence of hidden store
states, s?1..D1..T , of some length T and some maxi-
mum depth D, given a sequence of observed tokens
(e.g. generated target language words), e1..T , using
HHMM state transition model ?A and observation
symbol model ?B (Rabiner, 1990):
s?1..D1..T
def
= argmax
s1..D1..T
T?
t=1
P?A(s
1..D
t | s
1..D
t?1 )?P?B(et | s
1..D
t )
(8)
The HHMM parser is equivalent to a probabilis-
tic pushdown automaton with a bounded push-
down store. The model generates each successive
store (using store model ?S) only after considering
whether each nested sequence of incomplete con-
stituents has completed and reduced (using reduc-
tion model ?R):
P?A(s
1..D
t | s
1..D
t?1 )
def
=
?
r1t ..r
D
t
D?
d=1
P?R(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
? P?S(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t ) (9)
Store elements are defined to contain only the
active (c?) and awaited (c??) constituent categories
necessary to compute an incomplete constituent
probability:
sdt
def
= ?c?, c??? (10)
Reduction states are defined to contain only the
complete constituent category crdt necessary to com-
pute an inside likelihood probability, as well as a
flag frdt indicating whether a reduction has taken
place (to end a sequence of incomplete constituents):
rdt
def
= ?crdt , frdt ? (11)
The model probabilities for these store elements
and reduction states can then be defined (from Mur-
phy and Paskin 2001) to expand a new incomplete
constituent after a reduction has taken place (frdt =
1; using depth-specific store state expansion model
?S-E,d), transition along a sequence of store elements
624
s11
s21
s31
e1
t=1
r12
r22
r32
s12
s22
s32
e2
t=2
r13
r23
r33
s13
s23
s33
e3
t=3
r14
r24
r34
s14
s24
s34
e4
t=4
r15
r25
r35
s15
s25
s35
e5
t=5
r16
r26
r36
s16
s26
s36
e6
t=6
r17
r27
r37
s17
s27
s37
e7
t=7
r18
r28
r38
=DT
=NP/NN
=NP
=NN
=S/VP
=VB
=S/VP
=VP/NP
=DT
=VP/NN
=S/VP
=NN
=VP
=S/PP
=IN
=S/NP
=S
=NP
=The =president =meets =the =board =on =Friday
Figure 5: Graphical representation of the Hierarchic Hidden Markov Model after parsing input sentence The
president meets the board on Friday. The shaded path through the parse lattice illustrates the recognized
right-corner tree structure of Figure 3.
if no reduction has taken place (frdt =0; using depth-
specific store state transition model ?S-T,d): 2
P?S(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
def
=
?
?
?
if frd+1t =1, frdt =1 : P?S-E,d(s
d
t | s
d?1
t )
if frd+1t =1, frdt =0 : P?S-T,d(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
if frd+1t =0, frdt =0 : Js
d
t = s
d
t?1K
(12)
and possibly reduce a store element (terminate
a sequence) if the store state below it has re-
duced (frd+1t = 1; using depth-specific reduction
model ?R,d):
P?R(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
def
=
{
if frd+1t =0 : Jr
d
t = r?K
if frd+1t =1 : P?R,d(r
d
t | r
d+1
t s
d
t?1 s
d?1
t?1 )
(13)
where r? is a null state resulting from the failure of
an incomplete constituent to complete, and constants
are defined for the edge conditions of s0t and r
D+1
t .
Figure 5 illustrates this model in action.
These pushdown automaton operations are then
refined for right-corner parsing (Schuler, 2009),
distinguishing active transitions (model ?S-T-A,d, in
which an incomplete constituent is completed, but
not reduced, and then immediately expanded to a
2An indicator function J?K is used to denote deterministic
probabilities: J?K = 1 if ? is true, 0 otherwise.
new incomplete constituent in the same store el-
ement) from awaited transitions (model ?S-T-W,d,
which involve no completion):
P?S-T,d(s
d
t | r
d+1
t r
d
t s
d
t?1s
d?1
t )
def
=
{
if rdt 6=r? : P?S-T-A,d(s
d
t | s
d?1
t r
d
t )
if rdt =r? : P?S-T-W,d(s
d
t | s
d
t?1r
d+1
t )
(14)
P?R,d(r
d
t | r
d+1
t s
d
t?1s
d?1
t?1 )
def
=
{
if crd+1t 6=xt : Jr
d
t = r?K
if crd+1t =xt : P?R-R,d(r
d
t | s
d
t?1s
d?1
t?1 )
(15)
These HHMM right-corner parsing operations are
then defined in terms of branch- and depth-specific
PCFG probabilities ?G-R,d and ?G-L,d: 3
3Model probabilities are also defined in terms of left-
progeny probability distribution E?G-RL?,d which is itself defined
in terms of PCFG probabilities:
E?G-RL?,d(c?
0
? c?0 ...)
def
=
?
c?1
P?G-R,d(c? ? c?0 c?1) (16)
E?G-RL?,d(c?
k
? c?0k0 ...)
def
=
?
c
?0k
E?G-RL?,d(c?
k?1
? c?0k ...)
?
?
c
?0k1
P?G-L,d(c?0k ? c?0k0 c?0k1) (17)
E?G-RL?,d(c?
?
? c?? ...)
def
=
??
k=0
E?G-RL?,d(c?
k
? c?? ...) (18)
E?G-RL?,d(c?
+
? c?? ...)
def
= E?G-RL?,d(c?
?
? c?? ...)
? E?G-RL?,d(c?
0
? c?? ...) (19)
625
???????
president meets
??31
. . .
???????
the board
??51
. . .
s13
s23
s33
e3
r14
r24
r34
s14
s24
s34
e4
r15
r25
r35
s15
s25
s35
e5=meets =the =board
Figure 6: A hypothesis in the phrase-based decoding lattice from Figure 1 is expanded using translation op-
tion the board of source phrase den Vorstand. Syntactic language model state ??31 contains random variables
s1..33 ; likewise ??51 contains s
1..3
5 . The intervening random variables r
1..3
4 , s
1..3
4 , and r
1..3
5 are calculated by
transition function ? (Eq. 6, as defined by ?4.1), but are not stored. Observed random variables (e3..e5) are
shown for clarity, but are not explicitly stored in any syntactic language model state.
? for expansions:
P?S-E,d(?c??, c
?
??? | ??, c??)
def
=
E?G-RL?,d(c?
?
? c?? ...) ? Jx?? = c??? = c??K (20)
? for awaited transitions:
P?S-T-W,d(?c?, c??1? | ?c
?
?, c??? c??0)
def
=
Jc? = c??K ?
P?G-R,d(c?? ? c??0 c??1)
E?G-RL?,d(c??
0
? c??0 ...)
(21)
? for active transitions:
P?S-T-A,d(?c??, c??1? | ??, c?? c??0)
def
=
E?G-RL?,d(c?
?
? c?? ...) ? P?G-L,d(c?? ? c??0 c??1)
E?G-RL?,d(c?
+
? c??0 ...)
(22)
? for cross-element reductions:
P?R-R,d(c??,1 | ??, c?? ?c
?
??,??)
def
=
Jc?? = c???K ?
E?G-RL?,d(c?
0
? c?? ...)
E?G-RL?,d(c?
?
? c?? ...)
(23)
? for in-element reductions:
P?R-R,d(c??,0 | ??, c?? ?c
?
??,??)
def
=
Jc?? = c???K ?
E?G-RL?,d(c?
+
? c?? ...)
E?G-RL?,d(c?
?
? c?? ...)
(24)
We use the parser implementation of (Schuler,
2009; Schuler et al, 2010).
5 Phrase Based Translation with an
Incremental Syntactic Language Model
The phrase-based decoder is augmented by adding
additional state data to each hypothesis in the de-
626
coder?s hypothesis stacks. Figure 1 illustrates an ex-
cerpt from a standard phrase-based translation lat-
tice. Within each decoder stack t, each hypothe-
sis h is augmented with a syntactic language model
state ??th . Each syntactic language model state is
a random variable store, containing a slice of ran-
dom variables from the HHMM. Specifically, ??th
contains those random variables s1..Dt that maintain
distributions over syntactic elements.
By maintaining these syntactic random variable
stores, each hypothesis has access to the current
language model probability for the partial transla-
tion ending at that hypothesis, as calculated by an
incremental syntactic language model defined by
the HHMM. Specifically, the random variable store
at hypothesis h provides P(??th) = P(e
h
1..t, s
1..D
1..t ),
where eh1..t is the sequence of words in a partial hy-
pothesis ending at h which contains t target words,
and where there are D syntactic random variables in
each random variable store (Eq. 5).
During stack decoding, the phrase-based decoder
progressively constructs new hypotheses by extend-
ing existing hypotheses. New hypotheses are placed
in appropriate hypothesis stacks. In the simplest
case, a new hypothesis extends an existing hypothe-
sis by exactly one target word. As the new hypothe-
sis is constructed by extending an existing stack ele-
ment, the store and reduction state random variables
are processed, along with the newly hypothesized
word. This results in a new store of syntactic ran-
dom variables (Eq. 6) that are associated with the
new stack element.
When a new hypothesis extends an existing hy-
pothesis by more than one word, this process is first
carried out for the first new word in the hypothe-
sis. It is then repeated for the remaining words in
the hypothesis extension. Once the final word in
the hypothesis has been processed, the resulting ran-
dom variable store is associated with that hypoth-
esis. The random variable stores created for the
non-final words in the extending hypothesis are dis-
carded, and need not be explicitly retained.
Figure 6 illustrates this process, showing how a
syntactic language model state ??51 in a phrase-based
decoding lattice is obtained from a previous syn-
tactic language model state ??31 (from Figure 1) by
parsing the target language words from a phrase-
based translation option.
In-domain Out-of-domain
LM WSJ 23 ppl ur-en dev ppl
WSJ 1-gram 1973.57 3581.72
WSJ 2-gram 349.18 1312.61
WSJ 3-gram 262.04 1264.47
WSJ 4-gram 244.12 1261.37
WSJ 5-gram 232.08 1261.90
WSJ HHMM 384.66 529.41
Interpolated WSJ
5-gram + HHMM 209.13 225.48
Giga 5-gram 258.35 312.28
Interp. Giga 5-gr
+ WSJ HHMM 222.39 123.10
Interp. Giga 5-gr
+ WSJ 5-gram 174.88 321.05
Figure 7: Average per-word perplexity values.
HHMM was run with beam size of 2000. Bold in-
dicates best single-model results for LMs trained on
WSJ sections 2-21. Best overall in italics.
Our syntactic language model is integrated into
the current version of Moses (Koehn et al, 2007).
6 Results
As an initial measure to compare language models,
average per-word perplexity, ppl, reports how sur-
prised a model is by test data. Equation 25 calculates
ppl using log base b for a test set of T tokens.
ppl = b
?logbP(e1...eT )
T (25)
We trained the syntactic language model from
?4 (HHMM) and an interpolated n-gram language
model with modified Kneser-Ney smoothing (Chen
and Goodman, 1998); models were trained on sec-
tions 2-21 of the Wall Street Journal (WSJ) tree-
bank (Marcus et al, 1993). The HHMM outper-
forms the n-gram model in terms of out-of-domain
test set perplexity when trained on the same WSJ
data; the best perplexity results for in-domain and
out-of-domain test sets4 are found by interpolating
4In-domain is WSJ Section 23. Out-of-domain are the En-
glish reference translations of the dev section , set aside in
(Baker et al, 2009) for parameter tuning, of the NIST Open
MT 2008 Urdu-English task.
627
Sentence Moses +HHMM +HHMM
length beam=50 beam=2000
10 0.21 533 1143
20 0.53 1193 2562
30 0.85 1746 3749
40 1.13 2095 4588
Figure 8: Mean per-sentence decoding time (in sec-
onds) for dev set using Moses with and without syn-
tactic language model. HHMM parser beam sizes
are indicated for the syntactic LM.
HHMM and n-gram LMs (Figure 7). To show the
effects of training an LM on more data, we also re-
port perplexity results on the 5-gram LM trained for
the GALE Arabic-English task using the English Gi-
gaword corpus. In all cases, including the HHMM
significantly reduces perplexity.
We trained a phrase-based translation model on
the full NIST Open MT08 Urdu-English translation
model using the full training data. We trained the
HHMM and n-gram LMs on the WSJ data in order
to make them as similar as possible. During tuning,
Moses was first configured to use just the n-gram
LM, then configured to use both the n-gram LM and
the syntactic HHMM LM. MERT consistently as-
signed positive weight to the syntactic LM feature,
typically slightly less than the n-gram LM weight.
In our integration with Moses, incorporating a
syntactic language model dramatically slows the de-
coding process. Figure 8 illustrates a slowdown
around three orders of magnitude. Although speed
remains roughly linear to the size of the source sen-
tence (ruling out exponential behavior), it is with an
extremely large constant time factor. Due to this
slowdown, we tuned the parameters using a con-
strained dev set (only sentences with 1-20 words),
and tested using a constrained devtest set (only sen-
tences with 1-20 words). Figure 9 shows a statis-
tically significant improvement to the BLEU score
when using the HHMM and the n-gram LMs to-
gether on this reduced test set.
7 Discussion
This paper argues that incremental syntactic lan-
guages models are a straightforward and appro-
Moses LM(s) BLEU
n-gram only 18.78
HHMM + n-gram 19.78
Figure 9: Results for Ur-En devtest (only sentences
with 1-20 words) with HHMM beam size of 2000
and Moses settings of distortion limit 10, stack size
200, and ttable limit 20.
priate algorithmic fit for incorporating syntax into
phrase-based statistical machine translation, since
both process sentences in an incremental left-to-
right fashion. This means incremental syntactic LM
scores can be calculated during the decoding pro-
cess, rather than waiting until a complete sentence is
posited, which is typically necessary in top-down or
bottom-up parsing.
We provided a rigorous formal definition of in-
cremental syntactic languages models, and detailed
what steps are necessary to incorporate such LMs
into phrase-based decoding. We integrated an incre-
mental syntactic language model into Moses. The
translation quality significantly improved on a con-
strained task, and the perplexity improvements sug-
gest that interpolating between n-gram and syntactic
LMs may hold promise on larger data sets.
The use of very large n-gram language models is
typically a key ingredient in the best-performing ma-
chine translation systems (Brants et al, 2007). Our
n-gram model trained only on WSJ is admittedly
small. Our future work seeks to incorporate large-
scale n-gram language models in conjunction with
incremental syntactic language models.
The added decoding time cost of our syntactic
language model is very high. By increasing the
beam size and distortion limit of the baseline sys-
tem, future work may examine whether a baseline
system with comparable runtimes can achieve com-
parable translation quality.
A more efficient implementation of the HHMM
parser would speed decoding and make more exten-
sive and conclusive translation experiments possi-
ble. Various additional improvements could include
caching the HHMM LM calculations, and exploiting
properties of the right-corner transform that limit the
number of decisions between successive time steps.
628
References
Anne Abeille?, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized tree adjoining grammars for
machine translation. In Proceedings of the 13th Inter-
national Conference on Computational Linguistics.
Anthony E. Ades and Mark Steedman. 1982. On the
order of words. Linguistics and Philosophy, 4:517?
558.
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation
(SIMT). SCALE summer workshop final report, Hu-
man Language Technology Center Of Excellence.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 9?16.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, John Lafferty,
Robert Mercer, and Paul Roossin. 1990. A statisti-
cal approach to machine translation. Computational
Linguistics, 16(2):79?85.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proceedings of the Ninth Ma-
chine Translation Summit of the International Associ-
ation for Machine Translation.
Ciprian Chelba and Frederick Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In Pro-
ceedings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics, pages 225?
231.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University.
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452.
John Cocke and Jacob Schwartz. 1970. Program-
ming languages and their compilers. Technical report,
Courant Institute of Mathematical Sciences, New York
University.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 507?514.
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 232?241.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 727?736.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755?763.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 541?548.
Jay Earley. 1968. An efficient context-free parsing algo-
rithm. Ph.D. thesis, Department of Computer Science,
Carnegie Mellon University.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
205?208.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 773?781.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
629
Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 273?
280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961?
968.
Niyu Ge. 2010. A direct syntax-driven reordering model
for phrase-based machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 849?857.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 80?87.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 105?112.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 288?295.
James Henderson. 2004. Lookahead in deterministic
left-corner parsing. In Proceedings of the Workshop
on Incremental Parsing: Bringing Engineering and
Cognition Together, pages 26?33.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 273?283.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077?1086.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
conference of the Association for Machine Translation
in the Americas.
Kenji Imamura, Hideo Okuma, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Example-based machine transla-
tion based on syntactic transfer with statistical models.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 99?105.
Frederick Jelinek. 1969. Fast sequential decoding al-
gorithm using a stack. IBM Journal of Research and
Development, pages 675?685.
T. Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Laboratory.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 177?180.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 609?616.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 704?711.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558?566.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics, pages
653?660.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 206?214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 192?199.
630
Kevin P. Murphy and Mark A. Paskin. 2001. Linear time
inference in hierarchical HMMs. In Proceedings of
Neural Information Processing Systems, pages 833?
840.
Rebecca Nesson, Stuart Shieber, and Alexander Rush.
2006. Induction of probabilistic synchronous tree-
insertion grammars for machine translation. In Pro-
ceedings of the 7th Biennial conference of the Associ-
ation for Machine Translation in the Americas, pages
128?137.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, pages
161?168.
Matt Post and Daniel Gildea. 2008. Parsers as language
models for statistical machine translation. In Proceed-
ings of the Eighth Conference of the Association for
Machine Translation in the Americas, pages 172?181.
Matt Post and Daniel Gildea. 2009. Language modeling
with tree substitution grammars. In NIPS workshop on
Grammar Induction, Representation of Language, and
Language Learning.
Arjen Poutsma. 1998. Data-oriented translation. In
Ninth Conference of Computational Linguistics in the
Netherlands.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 271?279.
Lawrence R. Rabiner. 1990. A tutorial on hid-
den Markov models and selected applications in
speech recognition. Readings in speech recognition,
53(3):267?296.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage incremental
parsing using human-like memory constraints. Com-
putational Linguistics, 36(1):1?30.
William Schuler. 2009. Positive results for parsing with a
bounded stack using a model-based right-corner trans-
form. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 344?352.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 577?585.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree adjoining grammars. In Proceedings of the 13th
International Conference on Computational Linguis-
tics.
Stuart M. Shieber. 2004. Synchronous grammars as tree
transducers. In Proceedings of the Seventh Interna-
tional Workshop on Tree Adjoining Grammar and Re-
lated Formalisms.
Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523?530.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n cubed. Information
and Control, 10(2):189?208.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Seng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of the 11th Machine Translation Summit
of the International Association for Machine Transla-
tion, pages 535?542.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141.
631
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133?137,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joshua 2.0: A Toolkit for Parsing-Based Machine Translation
with Syntax, Semirings, Discriminative Training and Other Goodies
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,
Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,? Wren N.G. Thornton,
Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe the progress we have made in
the past year on Joshua (Li et al, 2009a),
an open source toolkit for parsing based
machine translation. The new functional-
ity includes: support for translation gram-
mars with a rich set of syntactic nonter-
minals, the ability for external modules to
posit constraints on how spans in the in-
put sentence should be translated, lattice
parsing for dealing with input uncertainty,
a semiring framework that provides a uni-
fied way of doing various dynamic pro-
gramming calculations, variational decod-
ing for approximating the intractable MAP
decoding, hypergraph-based discrimina-
tive training for better feature engineering,
a parallelized MERT module, document-
level and tail-based MERT, visualization
of the derivation trees, and a cleaner
pipeline for MT experiments.
1 Introduction
Joshua is an open-source toolkit for parsing-based
machine translation that is written in Java. The
initial release of Joshua (Li et al, 2009a) was a
re-implementation of the Hiero system (Chiang,
2007) and all its associated algorithms, includ-
ing: chart parsing, n-gram language model inte-
gration, beam and cube pruning, and k-best ex-
traction. The Joshua 1.0 release also included
re-implementations of suffix array grammar ex-
traction (Lopez, 2007; Schwartz and Callison-
Burch, 2010) and minimum error rate training
(Och, 2003; Zaidan, 2009). Additionally, it in-
cluded parallel and distributed computing tech-
niques for scalability (Li and Khudanpur, 2008).
This paper describes the additions to the toolkit
over the past year, which together form the 2.0 re-
lease. The software has been heavily used by the
authors and several other groups in their daily re-
search, and has been substantially refined since the
first release. The most important new functions in
the toolkit are:
? Support for any style of synchronous context
free grammar (SCFG) including syntax aug-
ment machine translation (SAMT) grammars
(Zollmann and Venugopal, 2006)
? Support for external modules to posit transla-
tions for spans in the input sentence that con-
strain decoding (Irvine et al, 2010)
? Lattice parsing for dealing with input un-
certainty, including ambiguous output from
speech recognizers or Chinese word seg-
menters (Dyer et al, 2008)
? A semiring architecture over hypergraphs
that allows many inference operations to be
implemented easily and elegantly (Li and
Eisner, 2009)
? Improvements to decoding through varia-
tional decoding and other approximate meth-
ods that overcome intractable MAP decoding
(Li et al, 2009b)
? Hypergraph-based discriminative training for
better feature engineering (Li and Khudan-
pur, 2009b)
? A parallelization of MERT?s computations,
and supporting document-level and tail-based
optimization (Zaidan, 2010)
? Visualization of the derivation trees and hy-
pergraphs (Weese and Callison-Burch, 2010)
? A convenient framework for designing and
running reproducible machine translation ex-
periments (Schwartz, under review)
The sections below give short descriptions for
each of these new functions.
133
2 Support for Syntax-based Translation
The initial release of Joshua supported only
Hiero-style SCFGs, which use a single nontermi-
nal symbol X. This release includes support for ar-
bitrary SCFGs, including ones that use a rich set
of linguistic nonterminal symbols. In particular
we have added support for Zollmann and Venu-
gopal (2006)?s syntax-augmented machine trans-
lation. SAMT grammar extraction is identical to
Hiero grammar extraction, except that one side of
the parallel corpus is parsed, and syntactic labels
replace the X nonterminals in Hiero-style rules.
Instead of extracting this Hiero rule from the bi-
text
[X]? [X,1] sans [X,2] | [X,1] without [X,2]
the nonterminals can be labeled according to
which constituents cover the nonterminal span on
the parsed side of the bitext. This constrains what
types of phrases the decoder can use when produc-
ing a translation.
[VP]? [VBN] sans [NP] | [VBN] without [NP]
[NP]? [NP] sans [NP] | [NP] without [NP]
Unlike GHKM (Galley et al, 2004), SAMT has
the same coverage as Hiero, because it allows
non-constituent phrases to get syntactic labels us-
ing CCG-style slash notation. Experimentally, we
have found that the derivations created using syn-
tactically motivated grammars exhibit more coher-
ent syntactic structure than Hiero and typically re-
sult in better reordering, especially for languages
with word orders that diverge from English, like
Urdu (Baker et al, 2009).
3 Specifying Constraints on Translation
Integrating output from specialized modules
(like transliterators, morphological analyzers, and
modality translators) into the MT pipeline can
improve translation performance, particularly for
low-resource languages. We have implemented
an XML interface that allows external modules
to propose alternate translation rules (constraints)
for a particular word span to the decoder (Irvine
et al, 2010). Processing that is separate from
the MT engine can suggest translations for some
set of source side words and phrases. The XML
format allows for both hard constraints, which
must be used, and soft constraints, which compete
with standard extracted translation rules, as well
as specifying associated feature weights. In ad-
dition to specifying translations, the XML format
allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a par-
ticular span to be translated as an NP. We modi-
fied Joshua?s chart-based decoder to support these
constraints.
4 Semiring Parsing
In Joshua, we use a hypergraph (or packed forest)
to compactly represent the exponentially many
derivation trees generated by the decoder for an
input sentence. Given a hypergraph, we may per-
form many atomic inference operations, such as
finding one-best or k-best translations, or com-
puting expectations over the hypergraph. For
each such operation, we could implement a ded-
icated dynamic programming algorithm. How-
ever, a more general framework to specify these
algorithms is semiring-weighted parsing (Good-
man, 1999). We have implemented the in-
side algorithm, the outside algorithm, and the
inside-outside speedup described by Li and Eis-
ner (2009), plut the first-order expectation semir-
ing (Eisner, 2002) and its second-order version (Li
and Eisner, 2009). All of these use our newly im-
plemented semiring framework.
The first- and second-order expectation semi-
rings can also be used to compute many interesting
quantities over hypergraphs. These quantities in-
clude expected translation length, feature expec-
tation, entropy, cross-entropy, Kullback-Leibler
divergence, Bayes risk, variance of hypothesis
length, gradient of entropy and Bayes risk, covari-
ance and Hessian matrix, and so on.
5 Word Lattice Input
We generalized the bottom-up parsing algorithm
that generates the translation hypergraph so that
it supports translation of word lattices instead of
just sentences. Our implementation?s runtime and
memory overhead is proportional to the size of the
lattice, rather than the number of paths in the lat-
tice (Dyer et al, 2008). Accepting lattice-based
input allows the decoder to explore a distribution
over input sentences, allowing it to select the best
translation from among all of them. This is es-
pecially useful when Joshua is used to translate
the output of statistical preprocessing components,
such as speech recognizers or Chinese word seg-
menters, which can encode their alternative analy-
ses as confusion networks or lattices.
134
6 Variational Decoding
Statistical models in machine translation exhibit
spurious ambiguity. That is, the probability of an
output string is split among many distinct deriva-
tions (e.g., trees or segmentations) that have the
same yield. In principle, the goodness of a string
is measured by the total probability of its many
derivations. However, finding the best string dur-
ing decoding is then NP-hard. The first version of
Joshua implemented the Viterbi approximation,
which measures the goodness of a translation us-
ing only its most probable derivation.
The Viterbi approximation is efficient, but it ig-
nores most of the derivations in the hypergraph.
We implemented variational decoding (Li et al,
2009b), which works as follows. First, given a for-
eign string (or lattice), the MT system produces a
hypergraph, which encodes a probability distribu-
tion p over possible output strings and their deriva-
tions. Second, a distribution q is selected that ap-
proximates p as well as possible but comes from
a family of distributions Q in which inference is
tractable. Third, the best string according to q
(instead of p) is found. In our implementation,
the q distribution is parameterized by an n-gram
model, under which the second and third steps can
be performed efficiently and exactly via dynamic
programming. In this way, variational decoding
considers all derivations in the hypergraph but still
allows tractable decoding.
7 Hypergraph-based Discriminative
Training
Discriminative training with a large number of
features has potential to improve the MT perfor-
mance. We have implemented the hypergraph-
based minimum risk training (Li and Eisner,
2009), which minimizes the expected loss of the
reference translations. The minimum-risk objec-
tive can be optimized by a gradient-based method,
where the risk and its gradient can be computed
using a second-order expectation semiring. For
optimization, we use both L-BFGS (Liu et al,
1989) and Rprop (Riedmiller and Braun, 1993).
We have also implemented the average Percep-
tron algorithm and forest-reranking (Li and Khu-
danpur, 2009b). Since the reference translation
may not be in the hypergraph due to pruning or in-
herent defficiency of the translation grammar, we
need to use an oracle translation (i.e., the transla-
tion in the hypergraph that is most simmilar to the
reference translation) as a surrogate for training.
We implemented the oracle extraction algorithm
described by Li and Khudanpur (2009a) for this
purpose.
Given the current infrastructure, other training
methods (e.g., maximum conditional likelihood or
MIRA as used by Chiang et al (2009)) can also be
easily supported with minimum coding. We plan
to implement a large number of feature functions
in Joshua so that exhaustive feature engineering is
possible for MT.
8 Minimum Error Rate Training
Joshua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu (Och, 2003).
We have parallelized our MERT module in
two ways: parallelizing the computation of met-
ric scores, and parallelizing the search over pa-
rameters. The computation of metric scores is
a computational concern when tuning to a met-
ric that is slow to compute, such as translation
edit rate (Snover et al, 2006). Since scoring a
candidate is independent from scoring any other
candidate, we parallelize this computation using a
multi-threaded solution1. Similarly, we parallelize
the optimization of the intermediate initial weight
vectors, also using a multi-threaded solution.
Another feature is the module?s awareness of
document information, and the capability to per-
form optimization of document-based variants of
the automatic metric (Zaidan, 2010). For example,
in document-based Bleu, a Bleu score is calculated
for each document, and the tuned score is the aver-
age of those document scores. The MERT module
can furthermore be instructed to target a specific
subset of those documents, namely the tail subset,
where only the subset of documents with the low-
est document Bleu scores are considered.2
More details on the MERT method and the im-
plementation can be found in Zaidan (2009).3
1Based on sample code by Kenneth Heafield.
2This feature is of interest to GALE teams, for instance,
since GALE?s evaluation criteria place a lot of focus on trans-
lation quality of tail documents.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
135
9 Visualization
We created tools for visualizing two of the
main data structures used in Joshua (Weese and
Callison-Burch, 2010). The first visualizer dis-
plays hypergraphs. The user can choose from a
set of input sentences, then call the decoder to
build the hypergraph. The second visualizer dis-
plays derivation trees. Setting a flag in the con-
figuration file causes the decoder to output parse
trees instead of strings, where each nonterminal is
annotated with its source-side span. The visual-
izer can read in multiple n-best lists in this format,
then display the resulting derivation trees side-by-
side. We have found that visually inspecting these
derivation trees is useful for debugging grammars.
We would like to add visualization tools for
more parts of the pipeline. For example, a chart
visualizer would make it easier for researchers to
tell where search errors were happening during
decoding, and why. An alignment visualizer for
aligned parallel corpora might help to determine
how grammar extraction could be improved.
10 Pipeline for Running MT
Experiments
Reproducing other researchers? machine transla-
tion experiments is difficult because the pipeline is
too complex to fully detail in short conference pa-
pers. We have put together a workflow framework
for designing and running reproducible machine
translation experiments using Joshua (Schwartz,
under review). Each step in the machine transla-
tion workflow (data preprocessing, grammar train-
ing, MERT, decoding, etc) is modeled by a Make
script that defines how to run the tools used in that
step, and an auxiliary configuration file that de-
fines the exact parameters to be used in that step
for a particular experimental setup. Workflows
configured using this framework allow a complete
experiment to be run ? from downloading data and
software through scoring the final translated re-
sults ? by executing a single Makefile.
This framework encourages researchers to sup-
plement research publications with links to the
complete set of scripts and configurations that
were actually used to run the experiment. The
Johns Hopkins University submission for the
WMT10 shared translation task was implemented
in this framework, so it can be easily and exactly
reproduced.
Acknowledgements
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors? alone.
References
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,
and Chris Callison-Burch. 2010. Integrating out-
put from specialized modules in machine transla-
tion: Transliteration in joshua. The Prague Bulletin
of Mathematical Linguistics, 93:107?116.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP, Singapore.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
136
Zhifei Li and Sanjeev Khudanpur. 2009b. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge
Nocedal. 1989. On the limited memory bfgs
method for large scale optimization. Mathematical
Programming, 45:503?528.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTER-
NATIONAL CONFERENCE ON NEURAL NET-
WORKS, pages 586?591.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua.
The Prague Bulletin of Mathematical Linguistics,
93:157?166.
Lane Schwartz. under review. Reproducible results in
parsing-based machine translation: The JHU shared
task submission. In WMT10.
Matthew Snover, Bonnie J. Dorr, and Richard
Schwartz. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Jonathan Weese and Chris Callison-Burch. 2010. Vi-
sualizing data structures in parsing-based machine
translation. The Prague Bulletin of Mathematical
Linguistics, 93:127?136.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Omar F. Zaidan. 2010. Document- and tail-based min-
imum error rate training of machine translation sys-
tems. In preparation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
137
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 177?182,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Reproducible Results in Parsing-Based Machine Translation:
The JHU Shared Task Submission
Lane Schwartz ?
University of Minnesota
Minneapolis, MN
lane@cs.umn.edu
Abstract
We present the Johns Hopkins Univer-
sity submission to the 2010 WMT shared
translation task. We describe processing
steps using open data and open source
software used in our submission, and pro-
vide the scripts and configurations re-
quired to train, tune, and test our machine
translation system.
1 Introduction
Research investigating natural language process-
ing and computational linguistics can and should
have an extremely low barrier to entry. The data
with which we work is customarily available in
common electronic formats. The computational
techniques which we apply can typically be per-
formed on commodity computing resources which
are widely available. In short, there should be no
reason why small research groups and even lone
researchers should not be able to join and make
substantive contributions furthering our field. The
reality is less encouraging.
Many published articles describe novel tech-
niques and provide interesting results, yet fail to
describe technical details in sufficient detail to al-
low their results to be reproduced by other re-
searchers. While there are notable and laudable
exceptions, many publications fail to provide the
source code and scripts necessary to reproduce re-
sults. The use of restricted data, not freely avail-
able for download by any interested researcher
only compounds these problems. Pedersen (2008)
rightly argues that the implementation details so
often ignored in publications are in fact essential
for our research to be reproducible science.
Reproducibility in machine translation is made
more challenging by the complexity of experi-
mental workflows. Results in machine translation
?Research conducted as a visiting researcher at Johns
Hopkins University
tasks are dependent on a cascade of processing
steps and configurations. While interesting sub-
sets of these usually appear in experimental de-
scriptions, many steps (preprocessing techniques,
alignment parameters, translation rule extraction
parameters, language model parameters, list of
features used) are invariably omitted, even though
these configurations are often critical to reproduc-
ing results.
This paper describes the Johns Hopkins Univer-
sity submission to the 2010 Workshop on Statis-
tical Machine Translation shared translation task.
Links to the software, scripts, and configurations
used to run the experiments described herein are
provided. The remainder of this paper is struc-
tured as follows. Section 2 lists the major ex-
amples of publicly available open source machine
translation systems, parallel corpora, and machine
translation workflow management systems. Sec-
tion 3 describes the experimental workflow used
to run the shared task translations, with the corre-
sponding experimental design in section 4. Sec-
tion 5 presents the shared task results.
2 Related Work
The last four years have witnessed the implemen-
tation and release of numerous open source ma-
chine translation systems. The widely used Moses
system (Koehn et al, 2007) implements the stan-
dard phrase-based translation model. Parsing-
based translation models are implemented by
Joshua (Li et al, 2009), SAMT (Zollmann and
Venugopal, 2006), and cdec (Dyer et al, 2010).
Cunei (Phillips and Brown, 2009) implements
statistical example-based translation. Olteanu et
al. (2006) and Schwartz (2008) respectively pro-
vide additional open-source implementations of
phrase-based and hierarchical decoders.
The SRILM (Stolcke, 2002), IRSTLM (Fed-
erico et al, 2008), and RandLM (Talbot and Os-
borne, 2007) toolkits enable efficient training and
177
No
rm
ali
ze
Ru
nM
ER
T
Ru
nM
BR
on
Tr
ue
ca
se
dT
ran
sla
tio
ns
Un
zip
Da
ta
To
ke
niz
e
Hi
ero
Tr
an
sla
te
Su
bs
am
ple
fo
rT
ru
ec
as
ing
Tr
ain
LM
Ex
tra
ct
Tr
ue
ca
se
Gr
am
ma
r
Tr
ain
Tr
ue
ca
se
LM
W
M
T
Sc
rip
ts
SR
IL
M
Ru
nM
BR
on
Tr
an
sla
tio
ns
Ex
tra
ct
Gr
am
ma
r
Al
ign
Tr
ue
ca
se
dD
ata
Al
ign
Su
bs
am
ple
Re
sto
re
to
Tr
ue
ca
seJ
os
hu
a
Do
wn
loa
dD
ata
Re
mo
ve
XM
L
Be
rk
ele
yA
lig
ne
r
Figure 1: Machine translation workflow. Square nodes in grey indicate software and scripts.
The scripts and configuration files used to implement and run this workflow are available
for download at http://sourceforge.net/projects/joshua/files/joshua/1.3/
wmt2010-experiment.tgz/download
178
querying of n-gram language models.
Freely available parallel corpora for numer-
ous European languages have also been released
in recent years. These include the Europarl
(Koehn, 2005) and JRC-Acquis (Steinberger et al,
2006) legislative corpora, each of which includes
data for most EU language pairs. The smaller
News Commentary corpora (Callison-Burch et al,
2007; Callison-Burch et al, 2008) provide smaller
amounts of parallel data in the news genre. The re-
cent Fr-En 109 (Callison-Burch et al, 2009) cor-
pus aggregates huge numbers of parallel French-
English sentences from the web.
Open source systems to address the complex
workflows required to run non-trivial machine
translation experiments have also been developed.
These include experiment.perl (Koehn et
al., 2010), developed as a workflow management
system at the University of Edinburgh, and Loony-
Bin (Clark et al, 2010), a general hyperworkflow
management utility from Carnegie Melon Univer-
sity.
3 Managing Experiment Workflows
Running a statistical machine translation system to
achieve state-of-the-art performance involves the
configuration and execution of numerous interde-
pendent intermediate tools. To manage task de-
pendencies and tool configuration, our shared task
workflow consists of a set of dependency scripts
written for GNU Make (Stallman et al, 2006).
Figure 1 shows a graph depicting the steps in
our experimental workflow, and the dependencies
between steps. Each node in the graph represents
a step in the workflow; each step is implemented
as a Make script that defines how to run the tools
required in that step. In each experiment, an ad-
ditional configuration script is provided for each
experimental step, defining the parameters to be
used when running that step in the current experi-
ment. Optional front-end wrapper scripts can also
be provided, allowing for a complete experiment
to be run - from downloading data and software
through truecasing translated results - by execut-
ing a single make file.
This framework is also conducive to paralleliza-
tion. Many tasks, such as preprocessing numerous
training files, are not dependent on one another.
In such cases make can be configured to exe-
cute multiple processes simultaneously on a single
multi-processor machine. In cases where sched-
uled distributed computing environments such as
the Sun Grid Engine are configured, make files can
be processed by scheduler-aware make variants
(distmake, SGE qmake, Sun Studio dmake)
which distribute outstanding tasks to available dis-
tributed machines using the relevant distributed
scheduler.
4 Experimental Configuration
Experimental workflows were configured1 and run
for six language pairs in the translation shared
task: English-French, English-German, English-
Spanish, French-English, German-English, and
Spanish-English.
In all experiments, only data freely available
for download was used. No restricted data from
the LDC or other sources was used. Table 1 lists
the parallel corpora used in training the translation
model for each experiment. The monolingual cor-
pora used in training each target language model
are listed in table 2. In all experiments, news-
test2008 was used as a development tuning corpus
during minimum error rate training; newstest2009
was used as a development test set. The shared
task data set newstest2010 was used as a final blind
test set.
All data was automatically downloaded, un-
zipped, and preprocessed prior to use. Files pro-
vided in XML format were converted to plain text
by selecting lines with <seg> tags, then removing
the beginning and end tags for each segment; this
processing was applied using GNU grep and sed.
The tokenize.perl and lowercase.perl
scripts provided for the shared task2 were applied
to all data.
Interpolated n-gram language models for the
four target languages were built using the SRI
Language Model Toolkit3, with n-gram order set
to 5. The Chen and Goodman (1998) technique
for modified Kneser-Ney discounting (Kneser and
Ney, 1995) was applied during language model
training.
Following Li et al (2009), a subset of the avail-
able training sentences was selected via subsam-
1http://sourceforge.net/projects/joshua/files/joshua/1.3/wmt2010-
experiment.tgz/download
2http://www.statmt.org/wmt08/scripts.tgz with md5sum:
tokenize.perl 45cd1832827131013245eca76481441a
lowercase.perl a1958ab429b1e29d379063c3b9cd7062
3http://www-speech.sri.com/projects/srilm
SRILM version 1.5.7. Our experimental workflow requires
that SRILM be compiled separately, with the $SRILM envi-
ronment variable set to the install location.
179
Source Target Parallel Corpora
German English news-commentary10.de-en europarl-v5.de-en
English German news-commentary10.de-en europarl-v5.de-en
French English news-commentary10.fr-en europarl-v5.fr-en giga-fren.release2 undoc.2000.en-fr
English French news-commentary10.fr-en europarl-v5.fr-en giga-fren.release2 undoc.2000.en-fr
Spanish English news-commentary10.es-en europarl-v5.es-en undoc.2000.en-es
English Spanish news-commentary10.es-en europarl-v5.es-en undoc.2000.en-es
Table 1: Parallel training data used for training translation model, per language pair
Target Monolingual Corpora
English europarl-v5.en news-commentary10.en news.en.shuffled undoc.2000.en-fr.en giga-fren.release2.en
French europarl-v5.fr news-commentary10.fr news.fr.shuffled undoc.2000.en-fr.fr giga-fren.release2.fr
German europarl-v5.de news-commentary10.de news.de.shuffled
Spanish europarl-v5.es news-commentary10.es news.es.shuffled undoc.2000.en-es.es
Table 2: Monolingual training data used for training language model, per target language
pling; training sentences are selected based on the
estimated likelihood of each sentence being useful
later for translating a particular test corpus.
Given a subsampled parallel training corpus,
word alignment is performed using the Berkeley
aligner4 (Liang et al, 2006).
For each language pair, a synchronous context
free translation grammar is extracted for a particu-
lar test set, following the methods of Lopez (2008)
as implemented in (Schwartz and Callison-Burch,
2010). For the largest training sets (French-
English and English-French) the original (Lopez,
2008) implementation included with Hiero was
used to save time during training5.
Because of the use of subsampling, the ex-
tracted translation grammars are targeted for use
with a specific test set. Our experiments were be-
gun prior to the release of the blind newstest2010
shared task test set. Subsampling was performed
for the development tuning set, news-test2008,
and the development test set, newstest2009. Once
the newstest2010 test set was released, the process
of subsampling, alignment, and grammar extrac-
tion was repeated to obtain translation grammars
targeted for use with the shared task test set.
Our experiments used hierarchical phrase-based
grammars containing exactly two nonterminals -
the wildcard nonterminal X, and S, used to glue
4http://berkeleyaligner.googlecode.com/files/berkeleyaligner
unsupervised-2.1.tar.gz ? Berkeley aligner version 2.1
5It is expected that using the Joshua implementation
should result in nearly identical results, albeit with somewhat
more time required to extract the grammar.
together neighboring constituents. Recent work
has shown that parsing-based machine translation
using SAMT (Zollmann and Venugopal, 2006)
grammars with rich nonterminal sets can demon-
strate substantial gains over hierarchical grammars
for certain language pairs (Baker et al, 2009).
Joshua supports such grammars; the experimental
workflow presented here could easily be extended
in future research to incorporate the use of SAMT
grammars with additional language pairs.
The Z-MERT implementation (Zaidan, 2009) of
minimum error rate training (Och, 2003) was used
for parameter tuning. Tuned grammars were used
by Joshua to translate all test sets. The Joshua de-
coder produces n-best lists of translations.
Rather than simply selecting the top candidate
from each list, we take the preferred candidate af-
ter perform minimum Bayes risk rescoring (Ku-
mar and Byrne, 2004).
Once a single translation has been extracted
for each sentence in the test set, we repeat the
procedures described above to train language and
translation models for use in translating lower-
cased results into a more human-readable true-
cased form. A truecase language model is
trained as above, but on the tokenized (but not
normalized) monolingual target language corpus.
Monotone word alignments are deterministically
created, mapping normalized lowercase training
text to the original truecase text. As in bilin-
gual translation, subsampling is performed for
the training set, and a translation grammar for
lowercase-to-truecase is extracted. No tuning is
180
performed. The Joshua decoder is used to trans-
late the lowercased target language test results into
truecase format. The detokenize.perl and
wrap-xml.perl scripts provided for the shared
task were manually applied to truecased transla-
tion results prior to final submission of results.
The code used for subsampling, grammar ex-
traction, decoding, minimum error rate training,
and minimum Bayes risk rescoring is provided
with Joshua6, with the exception of the original
(Lopez, 2008) grammar extraction implementa-
tion.
5 Experimental Results
The experiments described in sections 3 and
4 above provided truecased translations for
six language pairs in the translation shared
task: English-French, English-German, English-
Spanish, French-English, German-English, and
Spanish-English. Table 3 lists the automatic met-
ric scores for the newstest2010 test set, accord-
ing to the BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006) metrics.
Source Target BLEU BLEU- TER
cased
German English 21.3 19.5 0.660
English German 15.2 14.6 0.738
French English 27.7 26.4 0.614
English French 23.8 22.8 0.681
Spanish English 29.0 27.6 0.595
English Spanish 28.1 26.5 0.596
Table 3: Automatic metric scores for the test set
newstest2010
The submitted system ranked highest among
shared task participants for the German-English
task, according to TER.
In order to provide points of comparison with
the 2009 Workshop on Statistical Machine Trans-
lation shared translation task participants, table
4 lists automatic metric scores for our systems?
translations of the newstest2009 test set, which we
used as a development test set.
6 Steps to Reproduce
The experiments in this paper can be reproduced
by running the make scripts provided in the
6http://sourceforge.net/projects/joshua/files/joshua/1.3/joshua-
1.3.tgz/download ? Joshua version 1.3
Source Target BLEU
German English 18.19
English German 13.57
French English 26.41
English French 25.28
Spanish English 25.28
English Spanish 24.02
Table 4: Automatic metric scores for the develop-
ment test set newstest2009
following file: http://sourceforge.net/
projects/joshua/files/joshua/1.3/
wmt2010-experiment.tgz/download.
The README file details how to configure the
workflow for your environment. Note that SRILM
must be downloaded and compiled separately
before running the experimental steps.
Acknowledgements
This work was supported by the DARPA GALE
program (Contract No HR0011-06-2-0001).
References
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation (WMT07).
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation (WMT09), March.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Harvard
University, Cambridge, MA, USA, August.
181
Jonathan Clark, Jonathan Weese, Byung Gyu Ahn,
Andreas Zollman, Qin Gao, Kenneth Heafield, and
Alon Lavie. 2010. The machine translation tool-
pack for LoonyBin: Automated management of
experimental machine translation hyperworkflows.
The Prague Bulletin of Mathematical Linguistics,
93:117?126, January.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese,
F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and
P. Resnik. 2010. cdec: A decoder, alignment, and
learning framework for finite-state and context-free
translation models. In Proc. ACL (Demonstration
Track), Uppsala, Sweden.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: An open source toolkit for
handling large scale language models. In Proc. In-
terspeech, Brisbane, Australia.
Reinhard Kneser and Hermann Ney. 1995. Improved
smoothing for mgram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech and Signal Processing.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL-2007 Demo and Poster Sessions.
Philipp Koehn, Anthony Rousseau, Ben Gottesmann,
Aurora Marsye, Fre?de?ric Blain, and Eun-Jin Park,
2010. An Experiment Management System. Fourth
Machine Translation Marathon, Dublin, Ireland,
January.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit,
Phuket, Thailand.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT/NAACL.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
135?139, Athens, Greece, March. Association for
Computational Linguistics.
P. Liang, B. Taskar, and D. Klein. 2006. Align-
ment by agreement. In North American Association
for Computational Linguistics (NAACL), pages 104?
111.
Adam Lopez. 2008. Machine Translation by Pattern
Matching. Ph.D. thesis, University of Maryland.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Marian Olteanu, Chris Davis, Ionut Volosen, and Dan
Moldovan. 2006. Phramer an open source statis-
tical pharse-based translator. In HLT-NAACL 2006:
Proceedings of the Workshop on Statistical Machine
Translation, New York, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Ted Pedersen. 2008. Empiricism is not a matter of
faith. Computational Linguistics, 34(3):465?470.
Aaron B. Phillips and Ralf D. Brown. 2009. Cunei ma-
chine translation platform: System description. In
3rd Workshop on Example-Based Machine Transla-
tion, Dublin, Ireland.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua
suix arrays and prex trees. The Prague Bulletin of
Mathematical Linguistics, 93:157?166.
Lane Schwartz. 2008. An open-source hierarchical
phrase-based translation system. In Proceedings of
the 5th Midwest Computational Linguistics Collo-
quium (MCLC?08), May.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA.
Richard M. Stallman, Roland McGrath, and Paul D.
Smith, 2006. GNU Make. Free Software Founda-
tion, Boston, MA, 0.70 edition, April.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The JRC-acquis: A multi-
lingual aligned parallel corpus with 20+ languages.
In Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC),
Genoa, Italy.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, Denver, Colorado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
182
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 186?194,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Machine Translation and Monolingual Postediting:
The AFRL WMT-14 System
Lane O.B. Schwartz
Air Force Research Laboratory
lane.schwartz@us.af.mil
Timothy Anderson
Air Force Research Laboratory
timothy.anderson.20@us.af.mil
Jeremy Gwinnup
SRA International?
jeremy.gwinnup.ctr@us.af.mil
Katherine M. Young
N-Space Analysis LLC?
katherine.young.1.ctr@us.af.mil
Abstract
This paper describes the AFRL sta-
tistical MT system and the improve-
ments that were developed during the
WMT14 evaluation campaign. As part
of these efforts we experimented with
a number of extensions to the stan-
dard phrase-based model that improve
performance on Russian to English
and Hindi to English translation tasks.
In addition, we describe our efforts
to make use of monolingual English
speakers to correct the output of ma-
chine translation, and present the re-
sults of monolingual postediting of the
entire 3003 sentences of the WMT14
Russian-English test set.
1 Introduction
As part of the 2014 Workshop on Machine
Translation (WMT14) shared translation task,
the human language technology team at the
Air Force Research Laboratory participated
in two language pairs: Russian-English and
Hindi-English. Our machine translation sys-
tem represents enhancements to our system
from IWSLT 2013 (Kazi et al., 2013). In this
paper, we focus on enhancements to our pro-
cedures with regard to data processing and the
handling of unknown words.
In addition, we describe our efforts to make
use of monolingual English speakers to correct
the output of machine translation, and present
the results of monolingual postediting of the
entire 3003 sentences of the WMT14 Russian-
English test set. Using a binary adequacy clas-
sification, we evaluate the entire postedited
?This work is sponsored by the Air Force Research
Laboratory under Air Force contract FA-8650-09-D-
6939-029.
test set for correctness against the reference
translations. Using bilingual judges, we fur-
ther evaluate a substantial subset of the post-
edited test set using a more fine-grained ade-
quacy metric; using this metric, we show that
monolingual posteditors can successfully pro-
duce postedited translations that convey all or
most of the meaning of the original source sen-
tence in up to 87.8% of sentences.
2 System Description
We submitted systems for the Russian-to-
English and Hindi-to-English MT shared
tasks. In all submitted systems, we use the
phrase-based moses decoder (Koehn et al.,
2007). We used only the constrained data sup-
plied by the evaluation for each language pair
for training our systems.
2.1 Data Preparation
Before training our systems, a cleaning pass
was performed on all data. Unicode charac-
ters in the unallocated and private use ranges
were all removed, along with C0 and C1 con-
trol characters, zero-width and non-breaking
spaces and joiners, directionality and para-
graph markers.
2.1.1 Hindi Processing
The HindEnCorp corpus (Bojar et al., 2014)
is distributed in tokenized form; in order to
ensure a uniform tokenization standard across
all of our data, we began by detokenized this
data using the Moses detokenization scripts.
In addition to normalizing various extended
Latin punctuation marks to their Basic Latin
equivalents, following Bojar et al. (2010) we
normalized Devanagari Danda (U+0964),
Double Danda (U+0965), and Abbrevia-
tion Sign (U+0970) punctuation marks to
Latin Full Stop (U+002E), any Devana-
186
gari Digit to the equivalent ASCII Digit,
and decomposed all Hindi data into Unicode
Normalization Form D (Davis and Whistler,
2013) using charlint.1 In addition, we per-
formed Hindi diacritic and vowel normaliza-
tion, following Larkey et al. (2003).
Since no Hindi-English development test
set was provided in WMT14, we randomly
sampled 1500 sentence pairs from the Hindi-
English parallel training data to serve this pur-
pose. Upon discovering duplicate sentences in
the corpus, 552 sentences that overlapped with
the training portion were removed from the
sample, leaving a development test set of 948
sentences.
2.1.2 Russian Processing
The Russian sentences contained many exam-
ples of mixed-character spelling, in which both
Latin and Cyrillic characters are used in a sin-
gle word, relying on the visual similarity of the
characters. For example, although the first
letter and last letter in the word c????? ap-
pear visually indistinguishable, we find that
the former is U+0063 Latin Small Letter
C and the latter is U+0441 Cyrillic Small
Letter Es. We created a spelling normal-
ization program to convert these words to all
Cyrillic or all Latin characters, with a pref-
erence for all-Cyrillic conversion if possible.
Normalization also removes U+0301 Combin-
ing Acute Accent ( ??) and converts U+00F2
Latin Small Letter O with Grave (?)
and U+00F3 Latin Small Letter O with
Acute (?) to the unaccented U+043E Cyril-
lic Small Letter O (?).
The Russian-English Common Crawl par-
allel corpus (Smith et al., 2013) is relatively
noisy. A number of Russian source sentences
are incorrectly encoded using characters in the
Latin-1 supplement block; we correct these
sentences by shifting these characters ahead
by 350hex code points into the correct Cyrillic
character range.2
We examine the Common Crawl parallel
sentences and mark for removal any non-
Russian source sentences and non-English tar-
get sentences. Target sentences were marked
as non-English if more than half of the charac-
1http://www.w3.org/International/charlint
2For example: ???????? ?? ??????? ?????? ? ????.?
becomes ???????? ?? ??????? ?????? ? ????.?
ters in the sentence were non-Latin, or if more
than half of the words were unknown to the
aspell English spelling correction program,
not counting short words, which frequently
occur as (possibly false) cognates across lan-
guages (English die vs. German die, English
on vs. French on, for example). Because
aspell does not recognize some proper names,
brand names, and borrowed words as known
English words, this method incorrectly flags
for removal some English sentences which have
a high proportion of these types of words.
Source sentences were marked as non-
Russian if less than one-third of the charac-
ters were within the Russian Cyrillic range, or
if non-Russian characters equal or outnumber
Russian characters and the sentence contains
no contiguous sequence of at least three Rus-
sian characters. Some portions of the Cyrillic
character set are not used in typical Russian
text; source sentences were therefore marked
for removal if they contained Cyrillic exten-
sion characters Ukrainian I (? ?), Yi(? ?),
Ghe With Upturn (? ?) or Ie (? ?) in ei-
ther upper- or lowercase, with exceptions for
U+0406 Ukrainian I (?) in Roman numerals
and for U+0491 Ghe With Upturn (?) when
it occurred as an encoding error artifact.3
Sentence pairs where the source was identi-
fied as non-Russian or the target was identified
as non-English were removed from the parallel
corpus. Overall, 12% of the parallel sentences
were excluded based on a non-Russian source
sentence (94k instances) or a non-English tar-
get sentence (11.8k instances).
Our Russian-English parallel training data
includes a parallel corpus extracted from
Wikipedia headlines (Ammar et al., 2013),
provided as part of the WMT14 shared trans-
lation task. Two files in this parallel cor-
pus (wiki.ru-en and guessed-names.ru-en)
contained some overlapping data. We re-
moved 6415 duplicate lines within wiki.ru-en
(about 1.4%), and removed 94 lines of
guessed-names.ru-en that were already
present in wiki.ru-en (about 0.17%).
3Specifically, we allowed lines containing ? where it
appears as an encoding error in place of an apostro-
phe within English words. For example: ?????? The
Kelly Family I?m So Happy ???????????? ??? Lyrics-
Keeper.?
187
2.2 Machine Translation
Our baseline system is a variant of the MIT-
LL/AFRL IWSLT 2013 system (Kazi et al.,
2013) with some modifications to the training
and decoding processes.
2.2.1 Phrase Table Training
For our Russian-English system, we trained
a phrase table using the Moses Experiment
Management System (Koehn, 2010b), with
mgiza (Gao and Vogel, 2008) as the word
aligner; this phrase table was trained using the
Russian-English Common Crawl, News Com-
mentary, Yandex (Bojar et al., 2013), and
Wikipedia headlines parallel corpora.
The phrase table for our Hindi-English sys-
tem was trained using a similar in-house train-
ing pipeline, making use of the HindEnCorp
and Wikipedia headlines parallel corpora.
2.2.2 Language Model Training
During the training process we built n-gram
language models (LMs) for use in decoding
and rescoring using the KenLM language mod-
elling toolkit (Heafield et al., 2013). Class-
based language models (Brown et al., 1992)
were also trained, for later use in n-best list
rescoring, using the SRILM language mod-
elling toolkit (Stolcke, 2002).We trained a 6-
gram language model from the LDC English
Gigaword Fifth Edition, for use in both the
Hindi-English and Russian-English systems.
All language models were binarized in order
to reduce model disk usage and loading time.
For the Russian-to-English task, we concate-
nated the English portion of the parallel train-
ing data for the WMT 2014 shared transla-
tion task (Common Crawl, News Commen-
tary, Wiki Headlines and Yandex corpora) in
addition to the shared task English monolin-
gual training data (Europarl, News Commen-
tary and News Crawl corpora) into a training
set for a large 6-gram language model using
KenLM. We denote this model as ?BigLM?. In-
dividual 6-gram models were also constructed
from each respective corpus.
For the Hindi-to-English task, individual 6-
gram models were constructed from the re-
spective English portions of the HindEnCorp
and Wikipedia headlines parallel corpora, and
from the monolingual English sections of the
Europarl and News Crawl corpora.
Decoding Features
P(f | e)
P(e | f)
P
w
(f | e)
P
w
(e | f)
Phrase Penalty
Lexical Backoff
Word Penalty
Distortion Model
Unknown Word Penalty
Lexicalized Reordering Model
Operation Sequence Model
Rescoring Features
P
class
(E) ? 7-gram class-based LM
P
lex
(F | E) ? sentence-level averaged
lexical translation score
Table 1: Models used in log-linear combina-
tion
2.2.3 Decoding, n-best List Rescoring,
and Optimization
We decode using the phrase-based moses de-
coder (Koehn et al., 2007), choosing the best
translation for each source sentence according
to a linear combination of decoding features:
?E = arg max
E
?
?r
?
r
h
r
(E,F) (1)
We make use of a standard set of decoding
features, listed in Table 1. In contrast to our
IWSLT 2013 system, all experiments submit-
ted to this year?s WMT evaluation made use
of version 2.1 of moses, and incorporated ad-
ditional decoding features, namely the Oper-
ation Sequence Model (Durrani et al., 2011)
and Lexicalized Reordering Model (Tillman,
2004; Galley and Manning, 2008).
Following Shen et al. (2006), we use
the word-level lexical translation probabili-
ties P
w
(f
j
| e
i
) to obtain a sentence-level aver-
aged lexical translation score (Eq. 2), which is
added as an additional feature to each n-best
list entry.
P
lex
(F | E) =
?
j?1...J
1
I + 1
?
i?1...I
P
w
(f
j
| e
i
)
(2)
Shen et al. (2006) use the term ?IBM model 1
score? to describe the value calculated in Eq.
2. While the lexical probability distribution
188
from IBM Model 1 (Brown et al., 1993) could
in fact be used as the P
w
(f
j
| e
i
) in Eq. 2, in
practice we use a variant of P
w
(f
j
| e
i
) defined
by Koehn et al. (2003).
We also add a 7-gram class language model
score P
class
(E) (Brown et al., 1992) as an ad-
ditional feature of each n-best list entry. After
adding these features to each translation in an
n-best list, Eq. 1 is applied, rescoring the en-
tries to extract new 1-best translations.
To optimize system performance we train
scaling factors, ?
r
, for both decoding and
rescoring features so as to minimize an ob-
jective error criterion. In our systems we use
DREM (Kazi et al., 2013) or PRO (Hopkins
and May, 2011) to perform this optimization.
For development data during optimization,
we used newstest2013 for the Russian-to-
English task and newsdev2014 for the Hindi-
to-English task supplied by WMT14.
2.2.4 Unknown Words
For the Hindi-to-English task, unknown words
were marked during the decoding process and
were transliterated by the icu4j Devanagari-
to-Latin transliterator.4
For the Russian-to-English task, we selec-
tively stemmed and inflected input words not
found in the phrase table. Each input sentence
was examined to identify any source words
which did not occur as a phrase of length 1
in the phrase table. For each such unknown
word, we used treetagger (Schmid, 1994;
Schmid, 1995) to identify the part of speech,
and then we removed inflectional endings to
derive a stem. We applied all possible Rus-
sian inflectional endings for the given part of
speech; if an inflected form of the unknown
word could be found as a stand-alone phrase
in the phrase table, that form was used to re-
place the unknown word in the original Rus-
sian file. If multiple candidates were found,
we used the one with the highest frequency of
occurrence in the training data. This process
replaces words that we know we cannot trans-
late with semantically similar words that we
can translate, replacing unknown words like
??????? ?photon? (instrumental case) with
a known morphological variant ????? ?pho-
ton? (nominative case) that is found in the
4http://site.icu-project.org
BLEU BLEU-cased
Sy
ste
m
1 hi-en 13.1 12.1
2 ru-en 32.0 30.8
3 ru-en 32.2 31.0
4 ru-en 31.5 30.3
5 ru-en 33.0 31.1
Table 2: Translation results, as measured by
BLEU (Papineni et al., 2002).
phrase table. Selective stemming of just the
unknown words allows us to retain informa-
tion that would be lost if we applied stemming
to all the data.
Any remaining unknown words were
transliterated as a post-process, using a
simple letter-mapping from Cyrillic characters
to Latin characters representing their typical
sounds.
2.3 MT Results
Our best Hindi-English system for
newstest2014 is listed in Table 2 as System
1. This system uses a combination of 6-gram
language models built from HindEnCorp,
News Commentary, Europarl, and News
Crawl corpora. Transliteration of unknown
words was performed after decoding but
before n-best list rescoring.
System 2 is Russian-English, and handles
unknown words following ?2.2.4. We used as
independent decoder features separate 6-gram
LMs trained respectively on Common Crawl,
Europarl, News Crawl, Wiki headlines and
Yandex corpora. This system was optimized
with DREM. No rescoring was performed. We
also tested a variant of System 2 which did
perform rescoring. That variant (not listed in
Table 2) performed worse than System 2, with
scores of 31.2 BLEU and 30.1 BLEU-cased.
System 3, our best Russian-English system
for newstest2014, used the BigLM and Giga-
word language models (see ?2.2.2) as indepen-
dent decoder features and was optimized with
DREM. Rescoring was performed after de-
coding. Instead of following ?2.2.4, unknown
words were dropped to maximize BLEU score.
We note that the optimizer assigned weights of
0.314 and 0.003 to the BigLM and Gigaword
models, respectively, suggesting that the opti-
mizer found the BigLM to be much more use-
189
Figure 1: Posteditor user interface
Documents Sentences Words
Po
ste
di
to
r
1 44 950 20086
2 21 280 6031
3 25 476 10194
4 25 298 6164
5 20 301 5809
6 15 210 4433
7 10 140 2650
8 15 348 6743
All 175 3003 62110
Table 3: Number of documents within the
Russian-English test set processed by each
monolingual human posteditor. Number of
machine translated sentences processed by
each posteditor is also listed, along with the
total number of words in the corresponding
Russian source sentences.
# ? # ? % ?
Po
ste
di
to
r
1 684 266 72.0%
2 190 90 67.9%
3 308 168 64.7%
4 162 136 54.4%
5 194 107 64.5%
6 94 116 44.8%
7 88 52 62.9%
8 196 152 56.3%
All 1916 1087 63.8%
Table 4: For each monolingual posteditor, the
number and percentage of sentences judged to
be correct (?) versus incorrect (?) according
to a monolingual human judge.6
12 The postedited translation is superior
to the reference translation
10 The meaning of the Russian source
sentence is fully conveyed in the post-
edited translation
8 Most of the meaning is conveyed
6 Misunderstands the sentence in a ma-
jor way; or has many small mistakes
4 Very little meaning is conveyed
2 The translation makes no sense at all
Table 5: Evaluation guidelines for bilingual
human judges, adapted from Albrecht et al.
(2009).
Evaluation Category
2 4 6 8 10 12
0.2% 2.2% 9.8% 24.7% 60.2% 2.8%
Table 6: Percentage of evaluated sentences
judged to be in each category by a bilingual
judge. Category labels are defined in Table 5.
Evaluation Category
2 4 6 8 10 12
# ? 2 20 72 89 79 4
# ? 0 1 21 146 493 23
% ? 0% 5% 23% 62% 86% 85%
Table 7: Number of sentences in each evalu-
ation category (see Table 5) that were judged
as correct (?) or incorrect (?) according to a
monolingual human judge.
190
ful than the Gigaword LM. This intuition was
confirmed by an experimental variation of Sys-
tem 3 (not listed in Table 2) where we omitted
the BigLM; that variant performed substan-
tially worse, with scores of 25.3 BLEU and
24.2 BLEU-cased. We also tested a variant
of System 3 which did not perform rescoring;
that variant (also not listed in Table 2) per-
formed worse, with scores of 31.7 BLEU and
30.6 BLEU-cased.
The results of monolingual postediting (see
?3) of System 4 (a variant of System 2 tuned
using PRO) uncased output is System 5. Due
to time constraints, the monolingual post-
editing experiments in ?3 were conducted (us-
ing the machine translation results from Sys-
tem 4) before the results of Systems 2 and 3
were available. The Moses recaser was applied
in all experiments except for System 5.
3 Monolingual Postediting
Postediting is the process whereby a human
user corrects the output of a machine trans-
lation system. The use of basic postediting
tools by bilingual human translators has been
shown to yield substantial increases in terms
of productivity (Plitt and Masselot, 2010) as
well as improvements in translation quality
(Green et al., 2013) when compared to bilin-
gual human translators working without as-
sistance from machine translation and post-
editing tools. More sophisticated interactive
interfaces (Langlais et al., 2000; Barrachina
et al., 2009; Koehn, 2009b; Denkowski and
Lavie, 2012) may also provide benefit (Koehn,
2009a).
We hypothesize that for at least some lan-
guage pairs, monolingual posteditors with no
knowledge of the source language can success-
fully translate a substantial fraction of test
sentences. We expect this to be the case espe-
cially when the monolingual humans are do-
main experts with regard to the documents to
be translated. If this hypothesis is confirmed,
this could allow for multi-stage translation
workflows, where less highly skilled monolin-
gual posteditors triage the translation pro-
cess, postediting many of the sentences, while
forwarding on the most difficult sentences to
more highly skilled bilingual translators.
Small-scale studies have suggested that
monolingual human posteditors, working
without knowledge of the source language, can
also improve the quality of machine trans-
lation output (Callison-Burch, 2005; Koehn,
2010a; Mitchell et al., 2013), especially if well-
designed tools provide automated linguistic
analysis of source sentences (Albrecht et al.,
2009).
In this study, we designed a simple user in-
terface for postediting that presents the user
with the source sentence, machine transla-
tion, and word alignments for each sentence
in a test document (Figure 1). While it may
seem counter-intuitive to present monolingual
posteditors with the source sentence, we found
that the presence of alignment links between
source words and target words can in fact aid
a monolingual posteditor, especially with re-
gard to correcting word order. For example, in
our experiments posteditors encountered some
sentences where a word or phrase was enclosed
within bracketing punctuation marks (such as
quotation marks, commas, or parentheses) in
the source sentence, and the machine transla-
tion system incorrectly reordered the word or
phrase outside the enclosing punctuation; by
examining the alignment links the posteditors
were able to correct such reordering mistakes.
The Russian-English test set comprises 175
documents in the news domain, totaling 3003
sentences. We assigned each test document
to one of 8 monolingual5 posteditors (Table
3). The postediting tool did not record tim-
ing information. However, several posteditors
informally reported that they were able to pro-
cess on average approximately four documents
per hour; if accurate, this would indicate a
processing speed of around one sentence per
minute.
Following Koehn (2010a), we evaluated
postedited translation quality according to
a binary adequacy metric, as judged by a
monolingual English speaker6 against the En-
5All posteditors are native English speakers. Poste-
ditors 2 and 3 know Chinese and Arabic, respectively,
but not Russian. Posteditor 8 understands the Cyrillic
character set and has a minimal Russian vocabulary
from two undergraduate semesters of Russian taken
several years ago.
6All monolingual adequacy judgements were per-
formed by Posteditor 1. Additional analysis of Post-
editor 1?s 950 postedited translations were indepen-
dently judged by bilingual judges against the reference
and the source sentence (Table 7).
191
glish references. In this metric, incorrect
spellings of transliterated proper names were
not grounds to judge as incorrect an otherwise
adequate postedited translation. Binary ade-
quacy results are shown in Table 4; we observe
that correctness varied widely between poste-
ditors (44.8?72.0%), and between documents.
Interestingly, several posteditors self-
reported that they could tell which documents
were originally written in English and were
subsequently translated into Russian, and
which were originally written in Russian,
based on observations that sentences from
the latter were substantially more difficult to
postedit. Once per-document source language
data is released by WMT14 organizers, we
intend to examine translation quality on a
per-document basis and test whether postedi-
tors did indeed perform worse on documents
which originated in Russian.
Using bilingual judges, we further evaluate a
substantial subset of the postedited test set us-
ing a more fine-grained adequacy metric (Ta-
ble 5). Because of time constraints, only the
first 950 postedited sentences of the test set6
were evaluated in this manner. Each sentence
was evaluated by one of two bilingual human
judges. In addition to the 2-10 point scale of
Albrecht et al. (2009), judges were instructed
to indicate (with a score of 12) any sentences
where the postedited machine translation was
superior to the reference translation. Using
this metric, we show in Table 6 that monolin-
gual posteditors can successfully produce post-
edited translations that convey all or most of
the meaning of the original source sentence in
up to 87.8% of sentences; this includes 2.8%
which were superior to the reference.
Finally, as part of WMT14, the results of
our Systems 1 (hi-en), 3 (ru-en), and 5 (post-
edited ru-en) were ranked by monolingual hu-
man judges against the machine translation
output of other WMT14 participants. These
judgements are reported in WMT (2014).
Due to time constraints, the machine trans-
lations (from System 4) presented to postedi-
tors were not evaluated by human judges, nei-
ther using our 12-point evaluation scale nor
as part of the WMT human evaluation rank-
ings. However, to enable such evaluation by
future researchers, and to enable replication of
our experimental evaluation, the System 4 ma-
chine translations, the postedited translations,
and the monolingual and bilingual evaluation
results are released as supplementary data to
accompany this paper.
4 Conclusion
In this paper, we present data preparation and
language-specific processing techniques for our
Hindi-English and Russian-English submis-
sions to the 2014 Workshop on Machine Trans-
lation (WMT14) shared translation task. Our
submissions examine the effectiveness of han-
dling various monolingual target language cor-
pora as individual component language mod-
els (System 2) or alternatively, concatenated
together into a single big language model (Sys-
tem 3). We also examine the utility of n-
best list rescoring using class language model
and lexicalized translation model rescoring
features.
In addition, we present the results of mono-
lingual postediting of the entire 3003 sentences
of the WMT14 Russian-English test set. Post-
editing was performed by monolingual English
speakers, who corrected the output of ma-
chine translation without access to external
resources, such as bilingual dictionaries or on-
line search engines. This system scored high-
est according to BLEU of all Russian-English
submissions to WMT14.
Using a binary adequacy classification, we
evaluate the entire postedited test set for cor-
rectness against the reference translations. Us-
ing bilingual judges, we further evaluate a sub-
stantial subset of the postedited test set us-
ing a more fine-grained adequacy metric; using
this metric, we show that monolingual postedi-
tors can successfully produce postedited trans-
lations that convey all or most of the meaning
of the original source sentence in up to 87.8%
of sentences.
Acknowledgements
We would like to thank the members of the
SCREAM group at Wright-Patterson AFB.
Opinions, interpretations, conclusions and recom-
mendations are those of the authors and are not nec-
essarily endorsed by the United States Government.
Cleared for public release on 1 Apr 2014. Origina-
tor reference number RH-14-112150. Case number
88ABW-2014-1328.
192
References
Joshua S. Albrecht, Rebecca Hwa, and G. Elisa-
beta Marai. 2009. Correcting automatic trans-
lations through collaborations between MT and
monolingual target-language users. In Proceed-
ings of the 12th Conference of the European
Chapter of the Association for Computational
Linguistics (EACL ?12), pages 60?68, Athens,
Greece, March?April.
Waleed Ammar, Victor Chahuneau, Michael
Denkowski, Greg Hanneman, Wang Ling,
Austin Matthews, Kenton Murray, Nicola
Segall, Yulia Tsvetkov, Alon Lavie, and Chris
Dyer. 2013. The CMU machine translation sys-
tems at WMT 2013: Syntax, synthetic trans-
lation options, and pseudo-references. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation (WMT ?13), pages 70?77,
Sofia, Bulgaria, August.
Sergio Barrachina, Oliver Bender, Francisco
Casacuberta, Jorge Civera, Elsa Cubel,
Shahram Khadivi, Antonio Lagarda, Hermann
Ney, Jesu?s Toma?s, Enrique Vidal, and Juan-
Miguel Vilar. 2009. Statistical approaches to
computer-assisted translation. Computational
Linguistics, 35(1):3?28, March.
Ondr?ej Bojar, Pavel Stran?a?k, and Daniel Zeman.
2010. Data issues in English-to-Hindi machine
translation. In Proceedings of the Seventh In-
ternational Conference on Language Resources
and Evaluation (LREC ?10), pages 1771?1777,
Valletta, Malta, May.
Ond?rej Bojar, Christian Buck, Chris Callison-
Burch, Christian Federmann, Barry Haddow,
Philipp Koehn, Christof Monz, Matt Post, Radu
Soricut, and Lucia Specia. 2013. Findings of the
2013 Workshop on Statistical Machine Trans-
lation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation (WMT ?13),
pages 1?44, Sofia, Bulgaria, August.
Ond?rej Bojar, Vojt?ech Diatka, Pavel Rychl?y, Pavel
Stra?n?ak, Ale?s Tamchyna, and Dan Zeman.
2014. Hindi-English and Hindi-only corpus for
machine translation. In Proceedings of the Ninth
International Language Resources and Evalua-
tion Conference (LREC ?14), Reykjavik, Ice-
land, May. ELRA, European Language Re-
sources Association.
Peter Brown, Vincent Della Pietra, Peter deSouza,
Jenifer Lai, and Robert Mercer. 1992. Class-
based n-gram models of natural language. Com-
putational Linguistics, 18(4):467?479, Decem-
ber.
Peter Brown, Vincent Della Pietra, Stephen Della
Pietra, and Robert Mercer. 1993. The math-
ematics of statistical machine translation: pa-
rameter estimation. Computational Linguistics,
19(2):263?311, June.
Chris Callison-Burch. 2005. Linear B system de-
scription for the 2005 NIST MT evaluation exer-
cise. In Proceedings of the NIST 2005 Machine
Translation Evaluation Workshop.
Mark Davis and Ken Whistler. 2013. Unicode nor-
malization forms. Technical Report UAX #15,
The Unicode Consortium, September. Rev. 39.
Michael Denkowski and Alon Lavie. 2012. Trans-
Center: Web-based translation research suite.
In Proceedings of the AMTA 2012 Workshop
on Post-Editing Technology and Practice Demo
Session, November.
Nadir Durrani, Helmut Schmid, and Alexander
Fraser. 2011. A joint sequence translation
model with integrated reordering. In Proceed-
ings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?11),
pages 1045?1054, Portland, Oregon, June.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase re-
ordering model. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?08), pages 848?856,
Honolulu, Hawai?i, October.
Qin Gao and Stephan Vogel. 2008. Parallel im-
plementations of word alignment tool. In Soft-
ware Engineering, Testing and Quality Assur-
ance for Natural Language Processing, pages
49?57, Columbus, Ohio, June.
Spence Green, Jeffrey Heer, and Christopher D.
Manning. 2013. The efficacy of human post-
editing for language translation. In Proceedings
of the ACM SIGCHI Conference on Human Fac-
tors in Computing Systems (CHI ?13), pages
439?448, Paris, France, April?May.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable mod-
ified Kneser-Ney language model estimation. In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (ACL
?13), pages 690?696, Sofia, Bulgaria, August.
Mark Hopkins and Jonathan May. 2011. Tuning
as ranking. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP ?11), pages 1352?1362, Ed-
inburgh, Scotland, U.K.
Michaeel Kazi, Michael Coury, Elizabeth Salesky,
Jessica Ray, Wade Shen, Terry Gleason, Tim
Anderson, Grant Erdmann, Lane Schwartz,
Brian Ore, Raymond Slyh, Jeremy Gwinnup,
Katherine Young, and Michael Hutt. 2013.
The MIT-LL/AFRL IWSLT-2013 MT system.
In The 10th International Workshop on Spo-
ken Language Translation (IWSLT ?13), pages
136?143, Heidelberg, Germany, December.
193
Philipp Koehn, Franz Joseph Och, and Daniel
Marcu. 2003. Statistical phrase-based trans-
lation. In Proceedings of the 2003 Human
Language Technology Conference of the North
American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL ?13), pages
48?54, Edmonton, Canada, May?June.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL ?07) Demo and Poster
Sessions, pages 177?180, Prague, Czech Repub-
lic, June.
Philipp Koehn. 2009a. A process study of com-
puter aided translation. Machine Translation,
23(4):241?263, November.
Philipp Koehn. 2009b. A web-based interactive
computer aided translation tool. In Proceedings
of the ACL-IJCNLP 2009 Software Demonstra-
tions, pages 17?20, Suntec, Singapore, August.
Philipp Koehn. 2010a. Enabling monolingual
translators: Post-editing vs. options. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL ?10), pages 537?545, Los Ange-
les, California, June.
Philipp Koehn. 2010b. An experimental manage-
ment system. The Prague Bulletin of Mathemat-
ical Linguistics, 94:87?96, December.
Philippe Langlais, George Foster, and Guy La-
palme. 2000. TransType: A computer-aided
translation typing system. In Proceedings of
the ANLP/NAACL 2000 Workshop on Embed-
ded Machine Translation Systems, pages 46?51,
Seattle, Washington, May.
Leah S. Larkey, Margaret E. Connell, and Nasreen
Abduljaleel. 2003. Hindi CLIR in thirty days.
ACM Transactions on Asian Language Informa-
tion Processing (TALIP), 2(2):130?142, June.
Linda Mitchell, Johann Roturier, and Sharon
O?Brien. 2013. Community-based post-editing
of machine translation content: monolingual vs.
bilingual. In Proceedings of the 2nd Work-
shop on Post-editing Technology and Practice
(WPTP-2), pages 35?43, Nice, France, Septem-
ber. EAMT.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A method for au-
tomatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL
?02), pages 311?318, Philadelphia, Pennsylva-
nia, July.
Mirko Plitt and Franc?ois Masselot. 2010. A pro-
ductivity test of statistical machine translation
post-editing in a typical localisation context.
The Prague Bulletin of Mathematical Linguis-
tics, 93:7?16, January.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods
in Language Processing, Manchester, England,
September.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Proceedings of the EACL SIGDAT Workshop,
Dublin, Ireland, March.
Wade Shen, Brian Delaney, and Tim Anderson.
2006. The MIT-LL/AFRL IWSLT-2006 MT
system. In The 3rd International Workshop on
Spoken Language Translation (IWSLT ?06), Ky-
oto, Japan.
Jason R. Smith, Herve Saint-Amand, Magdalena
Plamada, Philipp Koehn, Chris Callison-Burch,
and Adam Lopez. 2013. Dirt cheap web-scale
parallel text from the common crawl. In Pro-
ceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (ACL
?13), pages 1374?1383, Sofia, Bulgaria, August.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
7th International Conference on Spoken Lan-
guage Processing (ICSLP ?02), pages 901?904,
Denver, Colorado, September.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL ?04), Companion Volume, pages
101?104, Boston, Massachusetts, May.
WMT. 2014. Findings of the 2014 Workshop on
Statistical Machine Translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation (WMT ?14), Baltimore, Maryland,
June.
194
