Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 897?904
Manchester, August 2008
Training Conditional Random Fields Using Incomplete Annotations
Yuta Tsuboi, Hisashi Kashima
Tokyo Research Laboratory,
IBM Research, IBM Japan, Ltd
Yamato, Kanagawa 242-8502, Japan
{yutat,hkashima}@jp.ibm.com
Shinsuke Mori
Academic Center for Computing and
Media Studies, Kyoto University
Sakyo-ku, Kyoto 606-8501, Japan
forest@i.kyoto-u.ac.jp
Hiroki Oda
Shinagawa, Tokyo, Japan
oda@fw.ipsj.or.jp
Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology
Takayama, Ikoma, Nara 630-0101, Japan
matsu@is.naist.jp
Abstract
We address corpus building situations,
where complete annotations to the whole
corpus is time consuming and unrealistic.
Thus, annotation is done only on crucial
part of sentences, or contains unresolved
label ambiguities. We propose a parame-
ter estimation method for Conditional Ran-
dom Fields (CRFs), which enables us to
use such incomplete annotations. We show
promising results of our method as applied
to two types of NLP tasks: a domain adap-
tation task of a Japanese word segmenta-
tion using partial annotations, and a part-
of-speech tagging task using ambiguous
tags in the Penn treebank corpus.
1 Introduction
Annotated linguistic corpora are essential for
building statistical NLP systems. Most of the
corpora that are well-known in NLP communi-
ties are completely-annotated in general. However
it is quite common that the available annotations
are partial or ambiguous in practical applications.
For example, in domain adaptation situations, it is
time-consuming to annotate all of the elements in a
sentence. Rather, it is efficient to annotate certain
parts of sentences which include domain-specific
expressions. In Section 2.1, as an example of such
efficient annotation, we will describe the effective-
ness of partial annotations in the domain adapta-
tion task for Japanese word segmentation (JWS).
In addition, if the annotators are domain experts
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
rather than linguists, they are unlikely to be confi-
dent about the annotation policies and may prefer
to be allowed to defer some linguistically complex
decisions. For many NLP tasks, it is sometimes
difficult to decide which label is appropriate in a
particular context. In Section 2.2, we show that
such ambiguous annotations exist even in a widely
used corpus, the Penn treebank (PTB) corpus.
This motivated us to seek to incorporate such
incomplete annotations into a state of the art ma-
chine learning technique. One of the recent ad-
vances in statistical NLP is Conditional Random
Fields (CRFs) (Lafferty et al, 2001) that evaluate
the global consistency of the complete structures
for both parameter estimation and structure infer-
ence, instead of optimizing the local configurations
independently. This feature is suited to many NLP
tasks that include correlations between elements
in the output structure, such as the interrelation of
part-of-speech (POS) tags in a sentence. However,
conventional CRF algorithms require fully anno-
tated sentences. To incorporate incomplete anno-
tations into CRFs, we extend the structured out-
put problem in Section 3. We focus on partial an-
notations or ambiguous annotations in this paper.
We also propose a parameter estimation method
for CRFs using incompletely annotated corpora in
Section 4. The proposed method marginalizes out
the unknown labels so as to optimize the likelihood
of a set of possible label structures which are con-
sistent with given incomplete annotations.
We conducted two types of experiments and ob-
served promising results in both of them. One was
a domain adaptation task for JWS to assess the
proposed method for partially annotated data. The
other was a POS tagging task using ambiguous an-
notations that are contained in the PTB corpus. We
summarize related work in Section 6, and conclude
897
      
cut
incised wound 
cut injury
abrasionor
file (or rasp)
infl. injuryinfl.infl.
pickpocket
Figure 1: An example of word boundary ambigui-
ties: infl. stands for an inflectional suffix of a verb.
in Section 7.
2 Incomplete Annotations
2.1 Partial Annotations
In this section, we describe an example of an effi-
cient annotation which assigns partial word bound-
aries for the JWS task.
It is not trivial to detect word boundaries for
non-segmented languages such as Japanese or Chi-
nese. For example, the correct segmentation of
the Japanese phrase ????????? (incised
wound or abrasion) is shown by the lowest boxes
segmented by the solid lines in Figure 1. How-
ever, there are several overlapping segmentation
candidates, which are shown by the other boxes,
and possible segmentation by the dashed lines.
Thus, the decisions on the word segmentation re-
quire considering the context, so simple dictionary
lookup approach is not appropriate. Therefore sta-
tistical methods have been successfully used for
JWS tasks. Previous work (Kudo et al, 2004)
showed CRFs outperform generative Markov mod-
els and discriminative history-based methods in
JWS. In practice, a statistical word segment an-
alyzer tends to perform worse for text from dif-
ferent domains, so that additional annotations for
each target domain are required. A major cause of
errors is the occurrence of unknown words. For ex-
ample, if ????? (abrasion) is an unknown word,
the system may accept the word sequence of ???
?????? as ????? (incised wound), ???
?? (file), and ??? (injury) by mistake.
On one hand, lists of new terms in the target
domain are often available in the forms of techni-
cal term dictionaries, product name lists, or other
sources. To utilize those domain word lists, Mori
(2006) proposed a KWIC (KeyWord In Context)
style annotation user interface (UI) with which a
user can delimit a word in a context with a single
user action. In Figure 2, an annotator marks the oc-
currences of ?????, a word in the domain word
??????? ??? ??????
? ??????? ??? ??????
? ??????? ??? ??????
Figure 2: An example of KWIC style annotation:
marked lines are identified as a correct segmenta-
tion.
list, if they are used as a real word in their con-
text. The ????? in the first row is a part of an-
other word ?????? (scratch), and the annotator
marks the last two rows as correctly segmented ex-
amples. This UI simplifies annotation operations
for segmentation to yes/no decisions, and this sim-
plification can also be effective for the reduction
of the annotation effort for other NLP tasks. For
example, the annotation operations for unlabeled
dependency parsing can be simplified into a series
of yes/no decisions as to whether or not given two
words have syntactic dependency. Compared with
sentence-wise annotation, the partial annotation is
not only effective in terms of control operations,
but also reduces annotation errors because it does
not require annotating the word boundaries that an
annotator is unsure of. This feature is crucial for
annotations made by domain experts who are not
linguists.
1
We believe partial annotation is effec-
tive in creating corpora for many other structured
annotations in the context of the domain adapta-
tions.
2.2 Ambiguous Annotations
Ambiguous annotations in this paper refer to a set
of candidate labels annotated for a part of a struc-
tured instance. For example, the following sen-
tence from the PTB corpus includes an ambiguous
annotation for the POS tag of ?pending?:
That/DT suit/NN is/VBZ pending/VBG|JJ ./. ,
where words are paired with their part-of-speech
tag by a forward slash (?/?).
2
Uncertainty concern-
ing the proper POS tag of ?pending? is represented
by the disjunctive POS tag (?VBG and JJ?) as in-
dicated by a vertical bar.
The existence of the ambiguous annotations is
due to the task definition itself, the procedure man-
1
The boundary policies of some words are different even
among linguists. In addition, the boundary agreement is even
lower in Chinese (Luo, 2003).
2
These POS tags used here are DT:determiner,
NN:common noun, VBZ:present tense 3rd person singular
verb, VBG:gerund or present participle verb, JJ:adjective,
NNS:plural noun, RBR:comparative adverb, IN:preposition
or subordinating conjunction, and RB:adverb.
898
frequency word POS tags
15 data NN|NNS
10 more JJR|RBR
7 pending JJ|VBG
4 than IN|RB
Table 1: Words in the PTB with ambiguous POSs.
ual for the annotators, or the inadequate knowl-
edge of the annotators. Ideally, the annotations
should be disambiguated by a skilled annotator for
the training data. However, even the PTB cor-
pus, whose annotation procedure is relatively well-
defined, includes more than 100 sentences contain-
ing POS ambiguities such as those listed in Ta-
ble 1. Although the number of ambiguous an-
notations is not considerably large in PTB cor-
pus, corpora could include more ambiguous anno-
tations when we try to build wider coverage cor-
pora. Also, ambiguous annotations are more com-
mon in the tasks that deal with semantics, such as
information extraction tasks so that learning algo-
rithms must deal with ambiguous annotations.
3 Problem Definition
In this section, we give a formal definition of the
supervised structured output problem that uses par-
tial annotations or ambiguous annotations in the
training phase. Note that we assume the input and
output structures are sequences for the purpose of
explanation, though the following discussion is ap-
plicable to other structures, such as trees.
Let x=(x
1
, x
2
, ? ? ? , x
T
) be a sequence of ob-
served variables x
t
? X and y=(y
1
, y
2
, ? ? ? , y
T
)
be a sequence of label variables y
t
? Y . Then the
supervised structured output problem can be de-
fined as learning a map X ? Y . In the Japanese
word segmentation task, x can represent a given
sequence of character boundaries and y is a se-
quence of the corresponding labels, which spec-
ify whether the current position is a word bound-
ary.
3
In the POS tagging task, x represents a word
sequence and y is a corresponding POS tag se-
quence. An incomplete annotation, then, is defined
as a sequence of subset of the label set instead of a
sequence of labels. Let L=(L
1
, L
2
, ? ? ? , L
T
) be a
sequence of label subsets for an observed sequence
3
Peng et al (2004) defined the word segmentation prob-
lem as labeling each character as whether or not the previous
character boundary of the current character is a word bound-
ary. However, we employ our problem formulation since it
is redundant to assign the first character of a sentence as the
word boundary in their formulation.
x, where L
t
? 2
Y
? {?}. The partial annotation
at position s is where L
s
is a singleton and the rest
L
t6=s
is Y . For example, if a sentence with 6 char-
acter boundaries (7 characters) is partially anno-
tated using the KWIC UI described in Section 2.1,
a word annotation where its boundary begins with
t = 2 and ends with t = 5 will be represented as:
L = ({?,?}, {?}, {?}, {?}, {?}
? ?? ?
partial annotation
, {?,?}),
where ? and ? denote the word boundary la-
bel and the non-word boundary label, respectively.
The ambiguous annotation is represented as a set
which contains candidate labels. The example sen-
tence including the ambiguous POS tag in Sec-
tion 2.2 can be represented as:
L = ({DT}, {NN}, {VBZ}, {VBG, JJ}
? ?? ?
ambiguous annotation
, {.}).
Note that, if all the elements of a given sequence
are annotated, it is the special case such that the
size of all elements is one, i.e. |L
t
| = 1 for all
t = 1, ? ? ? , T . The goal in this paper is training
a statistical model from partially or ambiguously
annotated data, D = {(x
(n)
, L
(n)
)}
N
n=1
.
4 Marginalized Likelihood for CRFs
In this section, we propose a parameter estimation
procedure for the CRFs (Lafferty et al, 2001) in-
corporating partial or ambiguous annotations. Let
?(x, y) : X ?Y ? <
d
denote a map from a pair
of an observed sequence x and a label sequence y
to an arbitrary feature vector of d dimensions, and
? ? <
d
denotes the vector of the model parame-
ters. CRFs model the conditional probability of a
label sequence y given an observed sequence x as:
P?(y|x) =
e
???(x,y)
Z?,x,Y
? (1)
where ? denotes the inner product of the vectors,
and the denominator is the normalization term that
guarantees the model to be a probability:
Z?,x,S =
?
y?S
e
???(x,y)
.
Then once ? has been estimated, the la-
bel sequence can be predicted by
?
y =
argmaxy?Y P?(y|x). Since the original CRF
learning algorithm requires a completely labeled
sequence y, the incompletely annotated data
(x, L) is not directly applicable to it.
899
Let YL denote all of the possible label sequence
consistent with L. We propose to use the condi-
tional probability of the subset YL given x:
P?(YL|x) =
?
y?Y
L
P?(y|x), (2)
which marginalizes the unknown ys out. Then
the maximum likelihood estimator for this model
can be obtained by maximizing the log likelihood
function:
LL(?) =
N
?
n=1
lnP?(YL(n) |x
(n)
) (3)
=
N
?
n=1
(
lnZ?,x(n),Y
L
(n)
? lnZ?,x(n),Y
)
.
This modeling naturally embraces label ambigui-
ties in the incomplete annotation.
4
Unfortunately, equation (3) is not a concave
function
5
so that there are local maxima in the
objective function. Although this non-concavity
prevents efficient global maximization of equation
(3), it still allows us to incorporate incomplete an-
notations using gradient ascent iterations (Sha and
Pereira, 2003). Gradient ascent methods require
the partial derivative of equation (3):
? LL(?)
??
=
N
?
n=1
?
?
?
y?Y
L
(n)
P?(y|YL(n) , x
(n)
)?(x
(n)
, y)
?
?
y?Y
P?(y|x
(n)
)?(x
(n)
, y)
?
?
, (4)
where
P?(y|YL, x) =
e
???(x,y)
Z?,x,Y
L
(5)
is a conditional probability that is normalized over
YL.
Equations (3) and (4) include the summations
of all of the label sequences in Y or YL. It is not
practical to enumerate and evaluate all of the label
configurations explicitly, since the number of all of
the possible label sequences is exponential on the
number of positions t with |L
t
| > 1. However, un-
der the Markov assumption, a modification of the
4
It is common to introduce a prior distribution over the pa-
rameters to avoid over-fitting in CRF learning. In the experi-
ments in Section 5, we used a Gaussian prior with the mean 0
and the variance ?
2
so that ?
||?||
2
2?
2
is added to equation (3).
5
Since its second order derivative can be positive.
domain #sentences #words
(A) conversation 11,700 145,925
(B) conversation 1,300 16,348
(C) medical manual 1,000 29,216
Table 2: Data statistics.
Types Template
Characters c
?1
, c
+1
,
Character types c
?2
c
?1
, c
?1
c
+1
, c
+1
c
+2
,
Term in dic. c
?2
c
?1
c
+1
, c
?1
c
+1
c
+2
Term in dic. starts at
c
?1
, c
+1
Term in dic. ends at
Table 3: Feature templates: Each subscript stands
for the relative distance from a character boundary.
Forward-Backward algorithm guarantees polyno-
mial time computation for the equations (3) and
(4). We explain this algorithm in Appendix A.
5 Experiments
We conducted two types of experiments, assessing
the proposed method in 1) a Japanese word seg-
mentation task using partial annotations and 2) a
POS tagging task using ambiguous annotations.
5.1 Japanese Word Segmentation Task
In this section, we show the results of domain
adaptation experiments for the JWS task to assess
the proposed method. We assume that only par-
tial annotations are available for the target domain.
In this experiment, the corpus for the source do-
main is composed of example sentences in a dic-
tionary of daily conversation (Keene et al, 1992).
The text data for the target domain is composed
of sentences in a medical reference manual (Beers,
2004) . The sentences of all of the source domain
corpora (A), (B) and a part of the target domain
text (C) were manually segmented into words (see
Table 2).
The performance measure in the experiments is
the standard F measure score, F = 2RP/(R + P )
where
R =
# of correct words
# of words in test data
? 100
P =
# of correct words
# of words in system output
? 100.
In this experiment, the performance was evaluated
using 2-fold cross-validation that averages the re-
sults over two partitions of the data (C) into the
900
91
91.5
92
92.5
93
93.5
94
94.5
95
0 100 200 300 400 500 600 700 800 900 1000
Number of word annotations
F
Proposed method
Argmax as training data
Point-wise classifier
Figure 3: Average performances varying the num-
ber of word annotations over 2 trials.
data for annotation and training (C1) versus the
data for testing (C2).
We implemented first order Markov CRFs. As
the features for the observed variables, we use the
characters and character type n-gram (n=1, 2, 3)
around the current character boundary. The
character types are categorized into Hiragana,
Katakana, Kanji, English alphabet, Arabic numer-
als, and symbols. We also used lexical features
consulting a dictionary: one is to check if any
of the above defined character n-grams appear in
a dictionary (Peng et al, 2004), and the other is
to check if there are any words in the dictionary
that start or end at the current character boundary.
We used the unidic
6
(281K distinct words) as the
general purpose dictionary, and the Japanese Stan-
dard Disease Code Master (JSDCM)
7
(23K dis-
tinct words) as the medical domain dictionary. The
templates for the features we used are summarized
in Table 3. To reduce the number of parameters,
we selected only frequent features in the source do-
main data (A) or in about 50K of the unsegmented
sentences of the target domain.
8
The total number
of distinct features was about 300K.
A CRF that was trained using only the source
domain corpus (A), CRF
S
, achieved F=96.84 in
the source domain validation data (B). However,
it showed the need for the domain adaptation that
this CRF
S
suffered severe performance degrada-
tion (F=92.3) on the target domain data. This
experiment was designed for the case in which a
user selects the occurrences of words in the word
list using the KWIC interface described in Sec-
tion 2.1. We employed JSDCM as a word list
in which 224 distinct terms appeared on average
over 2 test sets (C1). The number of word an-
6
Ver. 1.3.5; http://www.tokuteicorpus.jp/dist/
7
Ver. 2.63; http://www2.medis.or.jp/stdcd/byomei/
8
The data (B) and (C), which were used for validation and
test, were excluded from this feature selection process.
notations varied from 100 to 1000 in this exper-
iment. We prioritized the occurrences of each
word in the list using a selective sampling tech-
nique. We used label entropy (Anderson et al,
2006), H(y
s
t
) =
?
ys
t
?Y s
t
P
??
(y
s
t
|x) lnP
??
(y
s
t
|x)
, as importance metric of each word occurrence,
where
?
? is the model parameter of CRF
S
, and y
s
t
=
(y
t
, y
t+1
, ? ? ? , y
s
) ? Y
s
t
is a subsequence starting
at t and ending at s in y. Intuitively, this metric
represents the prediction confidence of CRF
S
.
9
As
training data, we mixed the complete annotations
(A) and these partial annotations on data (C1) be-
cause that performance was better than using only
the partial annotations.
We used conjugate gradient method to find the
local maximum value with the initial value being
set to be the parameter vector of CRF
S
. Since the
amount of annotated data for the target domain was
limited, the hyper-parameter ? was selected using
the corpus (B).
For the comparison with the proposed method,
the CRFs were trained using the most probable
label sequences consistent with L (denoted as
argmax). The most probable label sequences were
predicted by the CRF
S
. Also, we used a point-wise
classifier, which independently learns/classifies
each character boundary and just ignores the unan-
notated positions in the learning phase. As the
point-wise classifier, we implemented a maximum
entropy classifier which uses the same features and
optimizer as CRFs.
Figure 3 shows the performance comparisons
varying the number of word annotations. The
combination of both the proposed method and the
selective sampling method showed that a small
number of word annotations effectively improved
the word segmentation performance. In addi-
tion, the proposed method significantly outper-
formed argmax and point-wise classifier based on
the Wilcoxon signed rank test at the significance
level of 5%. This result suggests that the pro-
posed method maintains CRFs? advantage over the
point-wise classifier and properly incorporates par-
tial annotations.
5.2 Part-of-speech Tagging Task
In this section, we show the results of the POS tag-
ging experiments to assess the proposed method
using ambiguous annotations.
9
We selected word occurrences in a batch mode since each
training of the CRFs takes too much time for interactive use.
901
Ex.1 Ex.2
ambiguous sentences (training) 118
unique sentences (training) 1,480 2,960
unique sentences (test) 11,840
Table 4: Training and test data for POS tagging.
As mentioned in Section 2.2, there are words
which have two or more candidate POS tags in the
PTB corpus (Marcus et al, 1993). In this experi-
ment, we used 118 sentences in which some words
(82 distinct words) are annotated with ambiguous
POS tags, and these sentences are called the POS
ambiguous sentences. On the other hand, we call
sentences in which the POS tags of these terms are
uniquely annotated as the POS unique sentences.
The goal of this experiment is to effectively im-
prove the tagging performance using both these
POS ambiguous sentences and the POS unique
sentences as the training data. We assume that the
amount of training data is not sufficient to ignore
the POS ambiguous sentences, or that the POS am-
biguous sentences make up a substantial portion of
the total training data. Therefore we used a small
part (1/10 or 1/5) of the POS unique sentences for
training the CRFs and evaluated their performance
using other (4/5) POS unique sentences. We con-
ducted two experiments in which different num-
bers of unique sentences were used in the training
phases, and these settings are summarized in Ta-
ble 4.
The feature sets for each word are the case-
insensitive spelling, the orthographic features of
the current word, and the sentence?s last word. The
orthographic features are whether a spelling begins
with a number or an upper case letter; whether
it begins with an upper case letter and contains a
period (?.?); whether it is all upper case letters or
all lower case letters; whether it contains a punc-
tuation mark or a hyphen; and the last one, two,
and three letters of the word. Also, the sentence?s
last word corresponds to a punctuation mark (e.g.
?.?, ???, ?!?). We employed only features that ap-
peared more than once. The total number of re-
sulting distinct features was about 14K. Although
some symbols are treated as distinct tags in the
PTB tag definitions, we aggregated these symbols
into a symbol tag (SYM) since it is easy to restore
original symbol tags from the SYM tag. Then, the
number of the resulting tags was 36.
For the comparison with the proposed method
(mrg), we used three heuristic rules that disam-
biguated the annotated candidate POS tags in the
POS ambiguous sentences. These rules selected a
POS tag 1) at random, 2) as the first one in the
description order
10
, 3) as the most frequent tag
in the corpus. In addition, we evaluated the case
when the POS ambiguous sentences are 4) dis-
carded from the training data.
For evaluation, we employed the Precision
(P) and Average Precision for Ambiguous words
(APA):
P=
# of correctly tagged word
# of all word occurrences
?100?
APA=
1
|A|
?
w?A
# of the correctly tagged w
# of all occurrences of w
?100?
where A is a word set and is composed of the word
for which at least one of its occurrences is ambigu-
ously annotated. Here, we employed APA to eval-
uate each ambiguous words equally, and |A| was
82 in this experiment. Again, we used the conju-
gate gradient method to find the local maximum
value with the initial value being set to be the pa-
rameters obtained in the CRF learning for the dis-
carded setting.
Table 5 shows the average performance of POS
tagging over 5 different POS unique data. Since
the POS ambiguous sentences are only a fraction
of all of the training data, the overall performance
(P) was slightly improved by the proposed method.
However, according to the performance for am-
biguously annotated words (APA), the proposed
method outperformed other heuristics for POS dis-
ambiguation. The P and APA scores between
the proposed method and the comparable methods
are significantly different based on the Wilcoxon
signed rank test at the 5% significance level. Al-
though the performance improvement in this POS
tagging task was moderate, we believe the pro-
posed method will be more effective to the NLP
tasks whose corpus has a considerable number of
ambiguous annotations.
6 Related Work
Pereira and Schabes (1992) proposed a grammar
acquisition method for partially bracketed corpus.
Their work can be considered a generative model
for the tree structure output problem using partial
annotations. Our discriminative model can be ex-
tended to such parsing tasks.
10
Although the order in which the candidate tags appear
has not been standardized in the PTB corpus, we assume that
annotators might order the candidate tags with their confi-
dence.
902
mrg random first frequent discarded
Ex.1
P 94.39 94.27 94.26 94.27 94.19
APA 73.10 71.58 72.65 71.68 71.91
Ex.2
P 95.08 94.98 94.97 94.97 94.98
APA 76.70 74.27 75.28 74.32 75.16
Table 5: The average POS tagging performance over 5 trials.
Our model is interpreted as one of the CRFs
with hidden variables (Quattoni et al, 2004).
There are previous work which handles hidden
variables in discriminative parsers (Clark and Cur-
ran, 2006; Petrov and Klein, 2008). In their meth-
ods, the objective functions are also formulated as
same as equation (3).
For interactive annotation, Culotta et al (2006)
proposed corrective feedback that effectively re-
duces user operations utilizing partial annotations.
Although they assume that the users correct en-
tire label structures so that the CRFs are trained as
usual, our proposed method extends their system
when the users cannot annotate all of the labels in
a sentence.
7 Conclusions and Future Work
We are proposing a parameter estimation method
for CRFs incorporating partial or ambiguous an-
notations of structured data. The empirical results
suggest that the proposed method reduces the do-
main adaptation costs, and improves the prediction
performance for the linguistic phenomena that are
sometimes difficult for people to label.
The proposed method is applicable to other
structured output tasks in NLP, such as syntactic
parsing, information extraction, and so on. How-
ever, there are some NLP tasks, such as the word
alignment task (Taskar et al, 2005), in which it is
not possible to efficiently calculate the sum score
of all of the possible label configurations. Re-
cently, Verbeek and Triggs (2008) independently
proposed a parameter estimation method for CRFs
using partially labeled images. Although the ob-
jective function in their formulation is equivalent
to equation (3), they used Loopy Belief Propaga-
tion to approximate the sum score for their ap-
plication (scene segmentation). Their results im-
ply these approximation methods can be used for
such applications that cannot use dynamic pro-
gramming techniques.
Acknowledgments
We would like to thank the anonymous reviewers
for their comments. We also thank Noah Smith,
Ryu Iida, Masayuki Asahara, and the members
of the T-PRIMAL group for many helpful discus-
sions.
References
Anderson, Brigham, Sajid Siddiqi, and Andrew Moore.
2006. Sequence selection for active learning. Tech-
nical Report CMU-IR-TR-06-16, Carnegie Mellon
University.
Beers, Mark H. 2004. The Merck Manual of Medical
Information (in Japanese). Nikkei Business Publi-
cations, Inc, Home edition.
Clark, Stephen and James R. Curran. 2006. Par-
tial training for a lexicalized-grammar parser. In
Proceedings of the Annual Meeting of the North
American Association for Computational Linguis-
tics, pages 144?151.
Culotta, Aron, Trausti Kristjansson, Andrew McCal-
lum, and Paul Viola. 2006. Corrective feedback and
persistent learning for information extraction. Artifi-
cial Intelligence Journal, 170:1101?1122.
Keene, Donald, Hiroyoshi Hatori, Haruko Yamada, and
Shouko Irabu, editors. 1992. Japanese-English Sen-
tence Equivalents (in Japanese). Asahi Press, Elec-
tronic book edition.
Kudo, Taku, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings of
Empirical Methods in Natural Language Processing.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the 18th International Con-
ference on Machine Learning.
Luo, Xiaoquan. 2003. A maximum entropy chinese
character-based parser. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 192?199.
Marcus, Mitchell P., Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2).
Mori, Shinsuke. 2006. Language model adaptation
with a word list and a raw corpus. In Proceedings
of the 9th International Conference on Spoken Lan-
guage Processing.
903
Peng, Fuchun, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of
the International Conference on Computational Lin-
guistics.
Pereira, Fernando C. N. and Yves Schabes. 1992.
Inside-outside reestimation from partially bracketed
corpora. In Proceedings of Annual Meeting Associ-
ation of Computational Linguistics, pages 128?135.
Petrov, Slav and Dan Klein. 2008. Discriminative
log-linear grammars with latent variables. In Ad-
vances in Neural Information Processing Systems,
pages 1153?1160, Cambridge, MA. MIT Press.
Quattoni, Ariadna, Michael Collins, and Trevor Darrell.
2004. Conditional random fields for object recogni-
tion. In Advances in Neural Information Processing
Systems.
Sha, Fei and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of Human Language Technology-NAACL, Edmon-
ton, Canada.
Taskar, Ben, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Verbeek, Jakob and Bill Triggs. 2008. Scene segmen-
tation with CRFs learned from partially labeled im-
ages. In Advances in Neural Information Processing
Systems, pages 1553?1560, Cambridge, MA. MIT
Press.
Appendix A Computation of Objective
and Derivative functions
Here we explain the effective computation proce-
dure for equation (3) and (4) using dynamic pro-
gramming techniques.
Under the first-order Markov assumption
11
, two
types of features are usually used: one is pairs of
an observed variable and a label variable (denoted
as f(x
t
, y
t
) : X ? Y ), the other is pairs of two
label variables (denoted as g(y
t?1
, y
t
) : Y ? Y )
at time t. Then the feature vector can be de-
composed as ?(x, y) =
?
T+1
t=1
?(x
t
, y
t?1
, y
t
)
where ?(x
t
, y
t?1
, y
t
) = f(x
t
, y
t
) + g(y
t?1
, y
t
).
In addition, let S and E be special label vari-
ables to encode the beginning and ending of a se-
quence, respectively. We define ?(x
t
, y
t?1
, y
t
) to
be ?(x
t
, S, y
t
) at the head t = 1 and g(y
t?1
, E) at
the tail where t = T + 1. The technique of the ef-
fective calculation of the normalization value is the
11
Note that, although the rest of the explanation based on
the first-order Markov models for purposes of illustration, the
following arguments are easily extended to the higher order
Markov CRFs and semi-Markov CRFs.
precomputation of the ??,x,L[t, j], and??,x,L[t, j]
matrices with given ?,x, and L. The matrices ?
and ? are defined as follows, and should be cal-
culated in the order of t = 1, ? ? ? , T , and t =
T + 1, ? ? ? , 1, respectively
??,x,L[t, j]
=
?
?
?
?
?
?
?
?
?
0 if j /? L
t
? ? ?(x
t
, S, j) else if t = 1
ln
?
i?L
t?1
e
?[t?1,i]+??ffi(x
t
,i,j)
else
??,x,L[t, j]
=
?
?
?
?
?
?
?
?
?
0 if j /? L
t
? ? g(j, E) else if t = T + 1
ln
?
k?L
t+1
e
??ffi(x
t
,j,k)+?[t+1,k]
else
Note that L = (Y, ? ? ? , Y ) is used to calculate all
the entries in Y . In the rest of this section, we omit
the subscripts ?, x, and L of ?, ?, Z unless mis-
understandings could occur. The time complexity
of the ?[t, j] or ?[t, j] computation is O(T |Y |
2
).
Finally, equations (3) and (4) are efficiently cal-
culated using ?, ?. The logarithm of Z in equation
(3) is calculated as:
lnZ?,Y
L
= ln
?
j?L
T
e
?
?,L
[T,j]+??g(j,E)
.
Similarly, the first and second terms of equation
(4) can be computed as:
?
y?Y
L
P?,L(y|x)?(x, y) =
?
i?L
T
?L(T, i, E)g(i, E)
+
T
?
t=1
?
j?L
t
?
?
?L(t, j)f(xt, j) +
?
i?L
t?1
?L(t, i, j)g(i, j)
?
?
where ?, x are omitted in this equation, and ??,x,L
and ??,x,L are the marginal probabilities:
??,x,L(t, j) = P?,L(yt = j|x)
= e
?[t,j]+?[t,j]?ln Z
Y
L
, and
??,x,L(t, i, j) = P?,L(yt?1 = i, yt = j|x)
= e
?[t?1,i]+??ffi(x
t
,i,j)+?[t,j]?ln Z
Y
L
.
Note that YL is replaced with Y and L =
(Y, ? ? ? , Y ) to compute the second term.
904
A Stochast i c  Parser  Based  on a S t ructura l  Word  Pred ic t ion  Mode l  
Shinsuke MORI, Masafumi NISHIMURA, Nobuyasu ITOH, 
Shiho OGINO, Hideo WATANABE 
IBM Research ,  Tokyo  Resem:ch Laboratory ,  IBM Japan ,  Ltd.  
1623-14 Sh imotsurumz~ Ym~atosh i ,  24:2-8502, Japan.  
mor i~t r l . ibm.co . jp  
Abstract 
\]in this paper, we present a stochastic language 
model using dependency. This model considers a 
sentence as a word sequence and predicts each word 
from left to right. The history at each step of pre- 
diction is a sequence of partial parse krees covering 
the preceding words. First ore: model predicts the 
partial parse trees which have a dependency relation 
with the next word among them and then predicts 
the next word fi'om only the trees which have a de- 
pendency relation with the next word. Our: model is 
a generative stochastic model, thus this can be used 
not only as a parser but also as ~ language model 
of a speech recognizer. In our experiment, we pre- 
pared about 1,000 syntactically annotated Japanese 
sentences xtracted fl'om a financial newspaper and 
estimated the parameters of our model. We built a 
parser based on our: model and tested it on approx- 
imately 10O sentences of the same newspaper. The 
accuracy of the dependency relation was 89.9%, the 
highest, accuracy level obtained by Japanese stochas- 
tic parsers. 
1 In t roduct ion  
The stochastic language modeling, imported fl:om 
the speech recognition area, is one of the snccessflfl 
methodologies of natural language processing. In 
fact, all language models for speech recognition are, 
as far" a.s we know, based on an n-gram model and 
most practical part-of-speech (POS) taggers are also 
based on a word or POS n-gram model or its exten- 
sion (Church, 1.988; Cutting et el., 1992; Merialdo, 
1994; l)ennatas and Kokkinakis, 1.995). POS tag- 
ging is the first step of natural language process- 
ing, and stochastic taggers have solved this problem 
with satisfying accuracy for many applications. The 
next step is parsing, or that is to say discovering 
the structure of a given sentence. Recently, many 
parsers based on the stochastic approach ave been 
proposed. Although their reported accuracies are 
high, they are not accurate nough for many appli- 
cations at this stage, and more attempts have to be 
made to improve them fm:ther. 
One of the major applications of a parser is to 
parse the spoken text recognized by a speech rec- 
ognizer. This attempt is clearly aiming at spoken 
language understanding. If we consider how to con> 
bine a parser and a speech recognizer~ it is better if 
the parser is based on a generative stochastic model, 
as required for the language model of a speech rec- 
ognizer. Here, "generative" means that the sum of 
probabilities over all possible sentences is equal to 
or less than 1. If the language model is generative, 
it allows a seamless combination of the parser and 
the speech recognizer. This means that the speech 
recognizer has the stochastic parser as its language 
model and benefits richer information than a nor- 
mal n-gram model. Even though such a Colnbiim- 
tion is not possible in practices , the recognizer out- 
puts N-best sentences with their probabilities, and 
the parser, taking them as input, parses all of them 
and outputs the sentence with its parse tree that 
has the highest probability of all possible combina- 
tions. As a resnlt, a parser based on a generative 
stochastic language model may hell) a speech rec- 
ognizer to select the most syntactically reasonable 
sentence among candidates. Therefore, it is better 
if the language model of a parser is generative. 
In this paper, taking Japanese as the object lan- 
guage, we propose a generative stochastic language 
model and a parser based on it. This model treats a 
sentence as a word sequence and predicts each word 
from left to right. The history at each step of predic- 
tion is a sequence of partial parse trees covering the 
preceding words. To predict a word, our model first 
predicts which of the partial parse trees at this stage 
have dependency relation with the word, and then 
predicts the word fi'om the selected partial parse 
trees. In Japanese each word depends on a subse- 
quent word, that is to say, each dependency relation 
is left to right, it is not necessary to predict the di- 
rection of each dependency relation. So in order to 
extend our model to other languages, the model may 
have to predict the direction of each dependency. We 
built a parser based on this model, whose parame- 
ters are estimated fl:om 1,072 sentences in a finan- 
cial newspaper, and tested it on 1.19 sentences fl:om 
the same newspaper:. The accuracy of the depen- 
558 
dency relation was 89.9%, the highest obt.ained by 
any aapa.nese stochastic parsers. 
2 Stochast i c  Language Mode l  based  
on Dependency  
In this section, we propose a stochastic /angua.ge 
model based on dependency. Unlike most stochas- 
tic language models %r a. parser, our model is the- 
oreticMly based on a hidden Markov model. In our 
model a. sentence is predicted word by word fi'om left 
to right and the state at ea.ch step of prediction is 
basieMly a. sequence of words whose modifiea.nd has 
not appeared yet. According to a psyeholinguistic 
report on la.nguage structure (Yngve, 1960), there is 
an upper limit on the number of the words whose 
inodificaJ~ds ha.ve not appeared yet. This limit is de- 
termined by tim mmfloer of slots in sl~ort-term em- 
ory, 7 :k 2 (Miller, 1956). With this limitation, we 
Call design a pa.rser based on a linite state model. 
2.1 Sentence  Mode l  
'\]'he I)asic idett of our model is that each word would 
be better predicted from the words that have a. de- 
pendency rela.tion with the. word to be predicted 
than from the preceding two words (l.ri-gram model). 
Let us consider the complete structur('~ of the sen- 
tence in /"igure I and a \]tyl)otheti(:al struetm:e after 
the 1)rediction of tile lifth word at the top of Fig- 
ure 2. In this hypothetica.l st ; ructure ,  there are three 
trees: one root-only tree (/q, eomposc'd of wa) a.nd 
two two-node trees (l. conta.ining 'wz and 'w2, and l(, 
containing w4 an(1 'w5). If the last two trees (& and 
le) de4)end on the word we, this word may better 
be predicted from thes(~ two trees. I"rom this I)oint 
of view, our model Ill-st: predicts the t rees  del)cnding 
on the next word and then l)redicts the next word 
from thes(" trees. 
Now, let us make the tbllowing definitions in order 
to explain our model formally. 
? 11~ ~- t tq lv2 . . . ' t t )~  : a, seq l l cnce  o f  words .  \ ] \ ]ere a. 
word is define(l as a, pair consisting of a string of 
alplmbetic hara.cters and a, pa.rt of speech (e.g. 
the/DT).  
? ti = l i l 2 " " lk ,  : a, sequence of parrtiM parse 
trees covering the i-pretix words ('w~ w~. . .  wi). 
? t + trod t~- : subsequences of ti ha.ving a.nd 
not having a. dependeney relation with the next 
word respectively. In .h~p~mese, like many other 
langua.ges, no two dependency relations cross 
each other; thus tl = t~ t +, 
? (t w) : a tree with 'w as its root a.nd t as the 
sequence of all subtrees connected to the root. 
After wi+l has been predicted from the trees 
depending on it (t+), there a.re trees renmin- 
i,,~ (iT) a.d a. ,,ewly prod.eed t,.,'~e ((t?w~+,)); 
th,,s t~+, = t~ . (~,+,,,,~+,). 
1'(t~) 
--.~5 ................................................................................ , ( 
i ~ - -~ I i - -~1  - '~  " 
Wl W 2 W 3 W 4 W 5 W 6 
I }  ........................ + 
~hei  isubj\]il lending}!\[  : j ...................... ~la, ~  ~t,,el _ _ , _ _  
1 z ii - -3  w4 W5 i 1?6 
?;'(~,,01q) 
.... t.} .............................. <1 ~ w,~ > .................................................... 
w, w, }{ w:, w~ w.~ I% { 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  * ' .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  2 
/)(t~;), where t ,  t~ 4-. = = .(tat~;) 
Figure 2: Word prediction from a par t ia l  parse 
? Jhna:r : upper l imit on the munber number of 
words whose moditicands have not appeared 
yet. 
Under these definitions, our stochastic language 
model is defined as follows: 
~'("") = H t'(,,,I,,,,, w~...,,,~_~) 
i=1 
~ H l '(w, l t L , ) I ' ( t L , I t ; -~)  (:l) 
t,,, ET',~ i=1 
where 7;, is all possible bhm.ry trees with n nodes. 
lie,','., the first fi~.ctor, (P(wi l t+ 1)), is ca.lled the word 
prediction model and the second, (P (~'~1 } ti-1 ))' the 
state prediction model. Let us consider Figure 2 
aga.in. At. the top is the state just a.fter the predic- 
tion of the tilth word. The state prediction model 
then predicts the pa.rtial purse trees depending on 
the next word a.mong all partial parse trees, as shown 
in the second figure. Finally, the word prediction 
model predicts the next word Dora the partial parse 
trees depending on it. 
As described above, there may be an upper limit 
on the number of words whose modificands ha.ve not 
yet appeared. To put it in a.nother way, the length 
of the sequence of l)artial parse trees (ti) is limited. 
559 
W I W 2 W 3 W 4 W 5 W 6 W 7 W 8 W 9 Wlo 
Figure 1: Dependency struetm:e of a sentence. 
There%re, if the depth of the partial parse tree is also 
limited, the number of possible states is limited. Un- 
der this constraint, our model can be considered as 
a hidden Markov model. In a hidden Marker model, 
the first factor is called the output probability and 
the second, the transition probability. 
Since we assmne that no two dependency relations 
cross each other, the state prediction model only 
has to predict the mmaber of the trees depending 
on the ,text word. Tln,  s S'(t+_,lt, ._,) = ~':'(ylt~_~) 
where y is the number of trees in the sequence t?_ 1, 
According to the above assumption, the last y par- 
tial parse trees depend on the i-th word. Since the 
nmnber of possible parse trees for a word sequence 
grows exponentially with the number of the words, 
the space of the sequence of partial parse trees is 
huge even if the length of the sequence is limited. 
'?his inevitably causes a data-sparseness problem. 
To avoid this problern, we limited the number of 
levels of nodes used to distinguish trees. In our ex- 
periment, only the root and its children are checked 
to identify a partial parse tree. Hereafter, we repre- 
sent \]JLL to denote this model, in which the lexicon 
of the first level and thai; of the second level are con- 
sidered. Thus, in our experiment each word and the 
number of partial parse trees depending on it are 
predicted by a sequence of partial parse trees that 
take account of the nodes whose depth is two or less. 
It is worth noting that if the dependency structure 
of a sentence is linear - -  that is to say, if each word 
depends on the next word, - -  then our model will 
be equivalent to a word tri-gram model. 
We introduce an interpolation technique (Jelinek 
et al, 1991) into our model ike those used in n-gram 
models. By loosening tree identification regulations, 
we obtain a more general model. For example, if 
we check only the POS of the root and the I?OS 
of its children, we will obtain a model similar to a 
POS tri-gram model (denoted PPs' hereafter). If 
we check the lexicon of the root, but not that of its 
children, the model will be like a word bi-gram model 
(denoted PNL hereafter). As a smoothing method, 
we can interpolate the model PLL, similar to a word 
tri-gram model, with a more general model, PPP or 
PNL. In our experiment, as the following formula 
indicates, we interpolated seven models of different 
generalization levels: 
P(wiltt_l) ~--- ~6PLL(WiI',?_I) q- ,~5\]3pL(Wiit+i_l) 
q-,~4\]3pp (Wtit?_1) @ )t3PNL(W i\[tF_\] ) 
+ s'N,, It +, ) + x, PNN I*?-,) 
+,X0 (2) 
where X in PYx is the check level of the first level 
of the tree (N: none, P: POS, L: lexicon) and Y is 
that of the second level, and lG,c-gr<~m is the uniform 
distribution over the vocabulary W (-\])~U,O--gI'D,I~\](*/)) =
l / IWl). 
The state predictio,, model also in- 
terpola.ted in the salne way. in this case, the possi- 
ble events are y = 1 ,2 , . . . ,  Ym(~x, thus; /~a,0-gr<~m = 
l / y,,,ax .
2.2 Parmneter  Es t imat ion  
Since our model is a hidden Markov model, the pa- 
rameters of a model can l)e estimated from at. row 
corpus by EM algorithm (13amn, 1972). With this 
algorithm, the probability of the row corpus is ex- 
pected to be maxinfized regardless of the structure of 
ea.ch sentence. So the obtained model is not always 
appropriate for a. parser. 
In order to develop a model appropriate for a 
parser, it is better that the parameters are estimated 
from a syntactically annotated corlms by a maxi- 
mmn likelihood estimation (MI,E) (Meriaklo, 1994:) 
as follows: 
1,(wit+) MZ,,  j'(<t,+ 
-- f(< t+ w,:>) 
P(t+lt) M&E f(t+,t) 
f(t) 
where f (x)  represents the frequency of an event x in 
tile training corpus. 
The interpolation coeificients in the formula (2) 
are estimated by the deleted interpolation method 
(aelinek et al, 1991). 
2.3 Se lec t ing  Words  to  be Lex iea l i zed  
Generally speaking, a word-based n-gram model is 
better than a l>OS-based 'n-gram model in terms of 
560 
predictive power; however lexica.lization of some in- 
frequent words may be ha.rmfu\] beta.use it may c;mse 
a. data-sparseness problem. In a. practiea.1 tagger 
(I(upiec, \] 989), only the nlost, frequent \] 00 words a.re 
lexicalized. Also, in a, sta.te-ofthe-a.rt English pa.rser 
(Collins, 1997) only the words tha, t occur more tha,n 
d times in training data. are lexicalized. 
For this reason, our pa.rser selectn the words to be 
lexicalized at the time of lea.rning. In the lexical-  
ized models described above (P/A;, I},L and f~VL), 
only the selected words a.re \]exica.lized. The selec- 
tion criterion is parsing a.ccuracy (see section 4) of 
a. hekl-out corpus, a small part of the learning co l  
pus excluded from l)a, ramcter cstima.tion. Thus only 
the words tliat a.re 1)redicte(1 to improve the parsing 
a.Ccllra.oy of  the test corpilS> or i l l lkl loWll illpll{,> i/3"e 
lexicalized. The algorithm is as follows (see l,'igurc 
a): 
\]. In the initial sta.te a.ll words are in the class of 
their I)OS. 
2. All words are sorted ill descending order of their 
frequency, a.nd the following 1)rocens is executed 
for each word in this order: 
(a.) The word is lexicalizcd provisionally and 
the accura.cy el tile held-oul, corpus is (:;/l- 
cilia.ted. 
(b) Ir a.n illiproven\]ont in observed, the word is 
10xica.lized definitively. 
Tile result of this \]exica.liza.tion algoril.lun is used to 
identil~y a. \])a.rtia.l l)arse tree. That is to say, ()ill 3, Icx- 
iealized wordn are distinguished in lexicalized mod- 
els. It" IlO wordn Were nelcctxxl I:o be lexica/ized, {;hell 
/ 'NL  = \])N1 ' a,ii(I \ ])LL - -  \[)t"L = 191'1'. I t  is wort \ ] l  
nol ; ing that  i f  we t ry  to .ioi,, a word  w i th  al lo/,/ ler 
wet( l ,  then this  a,lgOlJithnl w i l l  be a, l lo r lna\ ]  top-down 
c, lustcring a.igorithnl. 
2.4 Unknown Word  Mode l  
To enable olir stocllastic la.nguage lnodel to handle 
unknowil words> we added a.li ui/knowii word model 
based Oil a cha.ra,cter \])i-giPa,nl nio(M, lr the next  
word is not in the vocabula.ry, the n\]o(lel predicts 
its POS a.nd the llllklloWll word model predicts the 
string of the word as folkm, s: 
re.q- \] 
s'(,wlS'OS) = 1-\[ ) 
i=1 
where 'w = x tx2 . . .xm,  xcl == aSm+l = \]~'\]'. 
1}'1", a special character corresponding to a word 
l)oundary, in introduced so tha.t the ntlilt of the l)rob- 
ability over all sl, r i l lgs is eqlla,\] to 71. 
In the l)ara.lneter cstima.tion described a.1)ove, a. 
learning corpus is divided into k parts. In our ex- 
i)erirnent, the vocabulary is composed of the wordn 
: - (~  ~ ,- '" l  .............................. -10& ....................................... 10S2 L_I @ 
. . .................................. . ,,,,,. 
i 
* / /  ..................... I 'OS ~ ........... ~ ............... I 'OS 2 
: - ~  ............... I 'OS I ...................... I 'OS 2 
lexicalize 
yes 
I10 
yes 
yes 
Figure 3: A conceptual figure of the le, xicaliza.tion 
algorit.hm. 
occurring in more than one l)artia\] corlmn and the 
other words are used for 1)arameter entima, tion of 
the unknown word model. The l)i-gram l)robal)ility 
of the unknown word model of a. I)OS in estimated 
\['ronl the words among them and belonging to the 
POS as follows: 
1 !l, o s (.~, i la: i - J ) M)~V .fP o s (*  i, * i -  ~ ) 
fPos (,,;~-,) 
The character I)i%ram model is also interl)ola.ted 
Wi/ , \ ] I  a l l i l i-~r~iill i l lode\]  and  a Zel'O-~l'aill l l lodc\] .  
The interl)olation coellicients are estinmi.d by the 
deleted interpolation method (,lelinek el. al., 1991). 
3 Syntact i c  Analysis 
(,el cJ dl.y, a. l)a.rscr may I)c considered an a module 
that recdvcs a. sequence of words annotated with a, 
I'()S and oul.putn its structm'e. Our parner, which 
includes a stochastic mflmown word model, however, 
is a.I)le to a.cc.el)t a cha.ra.ctc'r sequence as an input and 
execute segmenta.tion, POS tagging, and syntactic 
analysis nimultaneously I . In this section, wc exphfin 
our pa.rser, which is based on the language modal 
described in the preceding section. 
3.1 S to ( 'has t ie  Syntac | , i c  Ana lyzer  
A syntactic analyzer, bancd on a. stochastic language 
model, ca.lculatc's the pa.rse tree (see Figure 1) with 
the highest probabil ity for a given scquencc of char- 
acters x according to the following tbrmula.: 
:/' = , , , 'g i i i , ixP(Tl . ,  0 
"H~ (fl') :=;/~ 
1 There is no space \])etweell words ill ,Japo.llese 
561 
= argmax P(TIx)P(~ ) 
w(:c)=m 
= argn,~,xP(mlT)I~(r) ('." 13ayes' forn~ula) 
w(T)=.'e 
= argnmxP(T)  ('." P(mlT)= 1), 
W(T)=m 
where w(T) represents the concatenation of the 
word string in the syntactic trek T. P(T) in the last 
line is a stochastic language model, in our parser, 
it is the probability of a parse tree T defined by the 
stochastic dependency model including the unknown 
word model described in section 2. 
p(T) = I I  Its_,), (a) 
i=1 
where wlw2". "wn = w(T). 
3.2 Solut ion Search Algor i thm 
As shown in formula (3), our parser is based on a hid- 
den Markov model. It follows that Viterbi algorithm 
is applicable to search the best solution. Viterbi al- 
gorithm is capable of calculating the best solution in 
O(n) time, where n is the number of input charac- 
ters. 
The parser repeats a state tra.nsition, reading 
characters of the input sentence from left to right. In 
order that the structure of the input sentence may be 
a tree, the number of trees of the final state tn must 
be 1 and no more. Among the states that satisfy this 
condition, the parser selects the state with the high- 
est probability. Since our language model uses only 
the root and its children of a partial parse tree to dis- 
tinguish states, the last state does not have enough 
information to construct he parse tree. The parser 
can, however, calculate the parse tree fi'om the se- 
quence of states, or both the word sequence and the 
sequence of y, the number of trees that depend on 
the next word. Thus it memorizes these values at 
each step of prediction. After the most probable 
last state has been selected, the parser constructs 
the parse tree by reading these sequences fi:om top 
to bottom. 
4 Eva luat ion  
We developed a POS-based model and its lexical- 
ized version explained in section 2 to evaluate their 
predictive power, and implemented parsers based on 
them that calculate the most probable dependency 
tree fi'om a given character sequence, using the so- 
lution search algorithm explained in section 3 to ob- 
serve their accuracy. In this section, we present and 
discuss the experimental results. 
4.1 Condit ions on the Exper iments 
The corpus used in our experiments consists of ar- 
ticles extracted from a financial newspaper (Nihon 
Table 1: Corpus. 
learning 
test 
#sentences ~words 
1,072 30,292 
119 3,268 
#chars 
46,212 
4,909 
Keizai ,%inbun). Each sentence in tile articles is 
segmented into words and its dependency structure 
is annotated by linguists using an editor specially 
designed for this task at our site. The corpus was 
divided into ten parts; the parameters of the model 
were estimated fi:om nine of them and the model 
was tested on the rest (see Table 1). A small part 
of each leaning corpus is withheld from parameter 
estimation and used to select the words to be lex- 
icalized. After checking the learning corpus, the 
maximum number of partial parse trees is set to 10 
To evaluate the predictive power of our model, we 
calculated their cross entropy on the test corpns. In 
this process, the annotated tree in the test corpus is 
used as the structure of the sentences. Therefore the 
probability of each sentence in the test corpus is not 
the summation over all its possible derivations. To 
compare the POS-based model and the \]exicalized 
model, we constructed these models using the same 
learning corpus and calcnlated their cross entropy 
on the same test corpus. The POS-based model and 
the }exicalized model have the same mfl~nown word 
model, thus its contribution to the cross entropy is 
constant. 
We implemented a parser based on the depen- 
dency models. Since our models, inchsding a 
character-l)ased unknown word model, can return 
the best parse tree with its probability for any in- 
put, we can build a parser that receives a character 
sequence as input. It is not easy to evaluate, how- 
ever, because errors may occur in segmentation of
the sequence into words and in estimation of their 
POSs. For this reason, in the tbllowing description, 
we assume a word sequence as the input. 
The criterion for a parser is the accuracy of its out- 
put dependency relations. This criterion is widely 
used to evahmte Japanese dependency parsers. The 
accuracy is the ratio of the nnmber of the words a.n- 
notated with the same dependency to the numl)er of 
the words as in the corpus: 
accuracy 
=#=words ependiug on tilt correct word 
~words 
Tile last word and the second-to-last word of" a sen- 
tence are excluded, because there is no ambiguity. 
The last word has no word to depend on and the 
second-todast word depends always on the last word. 
562 
Table 2: Czoss entorpy alld acellraey of each model. 
language model cross enl\[,lTopy a, CCUFaicy 
selectively lexicalized 6.927 - ~ -  
completely lexicalized 6.651 87.1% 
POS-based 7.000 87.5% 
linear structure* -- 78.7% 
* F, adl word del)ends on l;he next word. 
4.2 Ewduat ion  
Table 2 shows the cross entropy and parsing accu- 
racy Of the baseline, where all words depend on the 
next word, the POS-based dependency model and 
two lexicalized dependency models. In the selec- 
tively lexicalized model, words to be lexicalized are 
selected by the aJgo,:ithm described in section 2. In 
the completely lexicalized model, all words arc lcxi- 
calized. This result attests experimentally that the 
pa.rser based on the selectively lexicalized model is 
the best parser. As for predictive power, however, 
the completely lexica.lized model has the lowest cross 
e~/tropy. Thus this model is estimated to be the best 
language model for speech recognition. Although 
there is no direct relation between cross entropy of 
l;he language model and error ra.te of a speech rec- 
ognizer, if we consider a spoken la.nguage parser, it 
ma.y be better to select the words to be lexicalized 
using other criterion. 
We calculated the cross entropy and the parsing 
accuracy (if' the model whose parameters arc esti- 
mated fi'om ;I/d, 1/16, and 1/64 of the learning 
corpus. The relation between the learning corpus 
size and the cross entrol)y or the l)arsing a.ccm:acy is 
shown in Figure d. The cross ent ropy  has a stronger 
tendency to decrease as the corpus size increases. 
As for accuracy, there is aJso a tendency for parsers 
to become more accurate as the size of the learning 
increases. The size of the cor/)us we h~we all this 
stage is not at all large, ltowever, its accuracy is at 
the top level of Japanese parsers, which nse 50,000- 
1.90,000 sentences. Therefore, we conclude that our 
approach is quite promising. 
5 Re la ted  Works  
lIistorica.lly, structures of natural languages have 
been described by a context-free grammar a.nd all\]- 
biguities have been resolved by parsers based on a 
context-free grammar (Fujisaki et al, 1989). In re- 
eenl, years, some attempts have been made in the 
area of parsing by a tinite state model (Otlazer, 1999) 
etc. Our parser is also based on a finite state model. 
Unlike these models, we focused on reports on a 
limit on language structure caused by the capacity 
our memory (Yngve, 1960) (Miller, 19561. Thus our 
20- -  
16 
q 
12 
0 ,-~ 8 
accuracy - -4  - _--4 
84.0% 86.2% 88.1% 89.9% 
% 
t'ross Clltropy \+\  "\  
I00 % 
8O 
60 N' 
40 
20 
0100 101 102 103 104 105 0 
#characters in learning corpus 
I,'igure d: Relation between cross entropy a.nd pars- 
i~lg accuracy .  
model is psycholinguistically more al)propriate. 
Recently, in the area of parsers based oll a. stochas- 
tic context-fi:ee grammar (SCFG), some researchers 
have pointed out the importance of t.he lexicon 
and proposed lexiealized models (Charniak, 1997; 
Collins, 1997). 111 these papers, they reported sig- 
nificant improvement of parsing accuracy. Taking 
these reports into account, we introduced a method 
of pa.rlJal lexicalization and reported significant im-- 
provement of parsing accuracy. Our lexicalization 
method is also a.pplicable to a. SCFG-based parser 
and improves its parsing accuracy. 
The model we present in this pal)er is a genera- 
tire stochastic language model. Chelba and aelinek 
119981 presented a similar model. In their model, 
each word is predicted t?om two right-most head 
words regardless of dependency rela.tion between 
these head words and the word. Eisner (\[996) also 
presented a. st;ochastie structura.1 language model, in 
which ea.ch word is predicted t?om its head word 
and the nearest one. This model is very similar to 
the parser presented by Collins 11.9961. The great- 
est difference between our model and these models 
is in that our model predicts the next word from the 
head words, or partial parse trees, depending on it. 
Clearly, it is not always two right-most head words 
that have dependency relation with the next word. 
It. follows that our model is linguistically more ap- 
propirate. 
There have been some attempts at stochastic 
Japal, ese parser (llaruno et al, 1998) (l"ujio and 
Matsmnoto, 19981 (Mori and Naga.o, 1.998). These 
Japanese parsers are based on a unit called bunsetsu, 
a sequence of one or more content words followed by 
zero or more traction words. The parsers take a 
sequence of units and outputs dependency relations 
between them. Unlike these parsers, our model de- 
563 
scribes dependencies between words; thus our model 
can easily be extended to other languages. As tbr 
the accuracy, although a direct comparison is not 
easy between our parser (89.9%; 1.,072 sentences) 
and these parsers (82% - 85%; 50,000 - 190,000 sen- 
tenees) because of the difference of the units and 
the corpus, our parser is one of the state-of-the-art 
parsers \[br Japanese language. It should be noted 
that ore: model describes relations among three or 
more units (case frame, consecutive dependency re- 
lations, etc.); thus our model benefits a greater deal 
from increase ot.' corpus size. 
6 Conc lus ion  
In this paper we have presented a stochastic lan- 
guage model based on dependency structure. This 
model treats a sentence as a word sequence and pre- 
dicts each word from left to right. "The history at 
each step of prediction is a sequence of partial parse 
trees covering the preceding words. To predict a 
word, ore: model first selects the partial parse trees 
that have a dependency relation with the word, and 
then predicts the next word from the selected partial 
parse trees. We also presented an algorithm %r lexi- 
calization. We lmilt parsers based on the POS-based 
model and its lexicalized version, whose parameters 
are estimated from 1,072 sentences of a financial 
newspaper. We tested the parsers on 119 sentences 
Dom the same newspaper, which we.re excluded fl:om 
the learning. The accuracy of the dependency rela- 
tion of the lexicalized parser was 89.9%, the highest 
obtained by any Japanese stochastic parser. 
References  
L. E. Baum. 1.972. An inequality and associated 
maximization technique in statistical estimation 
for probabilistie functions of Ma.rkov process. In- 
equalities, 3:1-8. 
Eugene Charniak. 1997. Statistical parsing with a 
context-fl:ee grammar and word statistics. In Pro- 
ceedings of the l/ith National ConfeTvnce on Arti- 
ficial Intclligence, pages 598-603. 
Ciprian Chelba and Frederic .l elinek. 1998. F, xploit- 
ing syntactic structure for language modeling. In 
Proceedings of the I Tth hdernational Conference 
on Computational Linguistics, pages 225-231. 
Kenneth Ward Church. 1988. A stochastic pa.rts 
program and noun phrase parser for unrestricted 
text. In Proceedings of the 3eeond Conference on 
Applied Natural Language Processing, pages 136- 
143. 
Michael John Collins, 1996. A new statistical parser 
based on bigram lexical dependencies. In Proceed- 
ings of the 34th Annual Meeting of the Association 
for Computational Linguistics, pages 184-191. 
Michael Collins. 1.997. Three genera.tire, lexiea.lised 
models for statistical parsing. In Proceedings of 
th(~ 35th Annual Meeting of the Association for 
Computational Linguistics, pages 16 -23. 
l)oug Cutting, Julian Kupiec, .Jan 1)edersen, and 
Penelope Sibun. 1992. A practical part-of-speech 
tagger. In Proceedings of the "lldrd Conference on 
Applied Natural Language Processing, pages 133 
l.d0. 
Evangelos Dermatas and George Kokkinakis. 1995. 
Automatic stochastic tagging of ha.tin:el language 
texts. Computational Linguistics, 21 (2):137-103. 
Jason M. Eisner. 1.996. Three new probabilistic 
models for dependency parsing: An exploration. 
In Proceedings of the 16th lnlernational Co~@r- 
ence on Computational Linguistics , pages 340 
345. 
Masakazu Fujio and Yuji Matsumoto. 1998. 
Japanese dependency structure analysis based on 
lexicalized statistics. In Proceedings (of the Third 
Conference on Empirical Methods in Natural Lan- 
guage Processing, pages 87-96. 
T. Fujisaki, F. ,\]elinek, .1. Cocke, E. Black, and 
T. Nishino. 1.989. A probabilistic parsing method 
for sentence disambiguation. In Proceedings of the 
International .Parsing Workshop. 
Masahiko Itarmm, Satoshi Shirai, and Yoshifnmi 
Ooyama. 1998, Using decision trees to construct a
practical parser. In Proceedings of the ITlh Inter- 
national Confer(race on Compul, alior~al Linguis- 
tics~ pages 505-511. 
Fredelick .\]elinek, 11,obert L. Mercer, and Salinr 
\[{oukos. 1991. Principles of lexica.1 language 
modeling for speech recognition. In Advances in 
,5'peeeh ,5'ignal Processing, chapter 21, pages 651- 
699. l)ekker. 
Julian Knpic'c, 1989. Augmenting a hidden Markov 
model 'or phrase-dependent word t.agging. In .Pro- 
ceedings of the DAI{I)A ,5'peeeh and Natural Lan- 
guage Workshop, pages 92- 08. 
Bernard Merialdo. 1994. '15~.gging English text with 
a probabilistie model. Computational Linguislics, 
~0(~):155 -171. 
George A. Miller. 1956. The magical number seven, 
plus or minus two: Some limits on our capacity 
for processing information. The Psychological l~e- 
view, 63:81-97. 
Shinsuke Mori and Makoto Nagao. 1998. A stochas- 
tic language model using dependency and its im- 
provement by word clustering. In Proceedings of 
the ITth International Co@'renee on Computa- 
tional Linguistics, pages 898-90~t. 
Kernel Ollazer. 1.999. l)ependency parsing with an 
extended finite state approach. In Proceedings of 
the 37t, h Annual Mecti~,g of the Association for 
Computational Linguistics, pages 254-260. 
Victor H. Yngve. 11960. A model and a hypothesis 
for language structure. The American Philosoph- 
ical Society, 104(5):444-466. 
564 
A Stochastic Parser
Based on an SLM with Arboreal Context Trees
Shinsuke MORI
IBM Research, Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimotsuruma Yamato-shi, 242-8502, Japan
mori@trl.ibm.co.jp
Abstract
In this paper, we present a parser based on a stochas-
tic structured language model (SLM) with a exible
history reference mechanism. An SLM is an alterna-
tive to an n-gram model as a language model for a
speech recognizer. The advantage of an SLM against
an n-gram model is the ability to return the struc-
ture of a given sentence. Thus SLMs are expected
to play an important part in spoken language under-
standing systems. The current SLMs refer to a xed
part of the history for prediction just like an n-gram
model. We introduce a exible history reference
mechanism called an ACT (arboreal context tree;
an extension of the context tree to tree-shaped his-
tories) and describe a parser based on an SLM with
ACTs. In the experiment, we built an SLM-based
parser with a xed history and one with ACTs, and
compared their parsing accuracies. The accuracy of
our parser was 92.8%, which was higher than that
for the parser with the xed history (89.8%). This
result shows that the exible history reference mech-
anism improves the parsing ability of an SLM, which
has great importance for language understanding.
1 Introduction
Currently, the state-of-the-art speech recognizers
can take dictation with satisfactory accuracy. Al-
though continuing attempts for improvements in
predictive power are needed in the language mod-
eling area for speech recognizers, another research
topic, understanding of the dictation results, is com-
ing into focus. Structured language models (SLMs)
(Chelba and Jelinek, 1998; Charniak, 2001; Mori et
al., 2001) were proposed for these purposes. Their
predictive powers are reported to be slightly higher
than an orthodox word tri-gram model if the SLMs
are interpolated with a word tri-gram model. In
contrast with word n-gram models, SLMs use the
syntactic structure (a partial parse tree) covering
the preceding words at each step of word predic-
tion. The syntactic structure also grows in parallel
with the word prediction. Thus after the predic-
tion of the last word of a sentence, SLMs are able to
give syntactic structures covering all the words of an
input sentence (parse trees) with associated proba-
bilities. Though the impact on the predictive power
is not major, this ability, which is indispensable to
spoken language understanding, is a clear advantage
of SLMs over word n-gram models. With an SLM
as a language model, a speech recognizer is able to
directly output a recognition result with its syntac-
tic structure after being given a sequence of acoustic
signals.
The early SLMs refer to only a limited and
xed part of the histories for each step of word
and structure prediction in order to avoid a data-
sparseness problem. For example, in an English
model (Chelba and Jelinek, 2000) the next word is
predicted from the two right-most exposed heads.
Also in a Japanese model (Mori et al, 2000) the
next word is predicted from 1) all exposed heads
depending on the next word and 2) the words de-
pending on those exposed heads. One of the natural
improvements in predictive power for an SLM can
be achieved by adding some exibility to the history
reference mechanism. For a linear history, which is
referred to by using word n-gram models, we can
use a context tree (Ron et al, 1996) as a exible
history reference mechanism. In an n-gram model
with a context tree, the length of each n-gram is in-
creased selectively according to an estimate of the
resulting improvement in predictive quality. Thus,
in general, an n-gram model with a context tree has
more predictive power in a smaller model.
In SLMs, the history is not a simple word se-
quence but a sequence of partial parse trees. For
a tree-shaped context, there is also a exible history
reference mechanism called an arboreal context tree
(ACT) (Mori et al, 2001).
1
Similar to a context
tree, an SLM with ACTs selects, depending on the
context, the region of the tree-shaped history to be
referred to for the next word prediction and the next
structure prediction. Mori et al (2001) report that
an SLM with ACTs has more predictive power than
an SLM with a xed history reference mechanism.
Therefore, if a parser based on an SLM with ACTs
outperforms an SLM without ACTs, an SLM with
1
In the original paper, it was called an arbori-context tree.
ACTs is a promising language model as the next re-
search milestone for spoken language understanding
systems.
In this paper, rst we describe an SLM with ACTs
for a Japanese dependency grammar. Next, we
present our stochastic parser based on the SLM. Fi-
nally, we report two experimental results: a compar-
ison with an SLM without ACTs and another com-
parison with a state-of-the-art Japanese dependency
parser. The parameters of our parser were estimated
from 9,108 syntactically annotated sentences from a
nancial newspaper. We then tested the parser on
1,011 sentences from the same newspaper. The ac-
curacy of the dependency relationships reported by
our parser was 92.8%, higher than the accuracy of
the parser based on an SLM without ACTs (89.8%).
This proved experimentally that an ACT improves
a parser based on an SLM.
2 Structured Language Model based
on Dependency
The most popular language model for a speech rec-
ognizer is a word n-gram model, in which each word
is predicted from the last (n 1) words. This model
works so well that the current recognizer can take
dictation with an almost satisfactory accuracy. Now
the research focus in the language model area is un-
derstanding the dictation results. In this situation, a
structured language model (SLM) was proposed by
Chelba and Jelinek (1998). In this section, we de-
scribe the dependency grammar version of an SLM.
2.1 Structured Language Model
The basic idea of an SLM is that each word would
be better predicted from the words that may have
a dependency relationship with the word to be pre-
dicted than from the proceeding (n 1) words. Thus
the probability P of a sentence w = w
1
w
2
  w
n
and
its parse tree T is given as follows:
P (T ) =
n
Y
i=1
P (w
i
jt
i 1
)P (t
i
jw
i
; t
i 1
); (1)
where t
i
is the i-th partial parse tree sequence. The
partial parse tree depicted at the top of Figure 1
shows the status before the 9th word is predicted.
From this status, for example, rst the 9th word w
9
is predicted from the 8th partial parse tree sequence
t
8
= t
8;3
t
8;2
t
8;1
, and then the 9th partial parse tree
sequence t
9
is predicted from the 9th word w
9
and
the 8th partial parse tree sequence t
8
to get ready
for the 10th word prediction. The problem here
is how to classify the conditional parts of the two
conditional probabilities in Equation (1) in order to
predict the next word and the next structure with-
out encountering a data-sparseness problem. In an
English model (Chelba and Jelinek, 2000) the next
P (t
8
)
w2 w4 w6w1 w3 w7w5 w8
t8,3 t8,1t8,2
?
P (w
9
jt
8
)
w2 w4 w6w1 w3 w7w5 w8 w9
t8,3 t8,1t8,2
?
P (t
9
jw
9
; t
8
)
w2 w4 w6w1 w3 w7w5 w8 w9
t9,2 t9,1
= P (t
9
); where t
9
= t
8;3
 ht
8;2
t
8;1
iw
9
Figure 1: Word prediction from a partial parse
word is predicted from the two right-most exposed
heads (for example w
6
and w
8
in Figure 1) as follows:
P (w
i
jt
i 1
)  P (w
i
jroot(t
i 1;2
); root(t
i 1;1
));
where root(t) is a function returning the root la-
bel word of the tree t. A similar approximation is
adapted to the probability function for structure pre-
diction. In a Japanese model (Mori et al, 2000) the
next word is predicted from 1) all exposed heads
depending on the next word and 2) the words de-
pending on those exposed heads.
It is clear, however, that in some cases some child
nodes of the tree t
i 1;2
or t
i 1;1
are useful for the
next word prediction and in other cases even the
consideration of an exposed head (root of the tree
t
i 1;1
or t
i 1;2
) suers from a data-sparseness prob-
lem because of the limitation of the learning corpus
size. Therefore a more exible mechanism for history
classication should improve the predictive power of
the SLM.
2.2 SLM for Dependency Grammar
Since in a dependency grammar of Japanese, every
dependency relationship is in a unique direction as
shown in Figure 1 and since no two dependency re-
lationships cross each other, the structure prediction
model only has to predict the number of trees. Thus,
the second conditional probability in the right hand
side of Equation (1) is rewritten as P (l
i
jw
i
; t
i 1
),
where l
i
is the length (number of elements) of the
tree sequence t
i
. Our SLM for the Japanese depen-
w2
w4
w6
w1
w3
w7w5
w8
t8,3
h
-2h-3 h-1
t8,2
t8,1
vr
virtual
root
Figure 2: A history tree.
dency grammar is dened as follows:
P (T ) =
n
Y
i=1
P (w
i
jt
i 1
)P (l
i
jw
i
; t
i 1
): (2)
According to a psycholinguistic report on lan-
guage structure (Yngve, 1960), there is an upper
limit on l
i
, the number of words whose modicands
have not appeared yet. We set the upper limit to 9,
the maximum number of slots in human short-term
memory (Miller, 1956). With this limitation, our
SLM becomes a hidden Markov model.
3 Arboreal Context Tree
A variable memory length Markov model (Ron et
al., 1996), a natural extension of the n-gram model,
is a exible mechanism for a linear context (word
sequence) which selects, depending on the context,
the length of the history to be referred to for the
next word prediction. This model is represented by
a sux tree, called a context tree, whose nodes are
labeled with a sux of the context. In this model,
the length of each n-gram is increased selectively ac-
cording to an estimate of the resulting improvement
in predictive quality.
In SLMs, the history is not a simple word se-
quence but a sequence of partial parse trees. For
a tree-shaped context, there is also a exible history
reference mechanism called an arboreal context tree
(ACT) (Mori et al, 2001) which selects, depending
on the context, the region of the tree-shaped history
to be referred to for the next word prediction and
for the next structure prediction. In this section, we
explain ACTs and their application to SLMs.
3.1 Data Structure
As we mentioned above, in SLMs the history is a
sequence of partial parse trees. This can be regarded
as a single tree, called a history tree, by adding a
virtual root node having these partial trees under it.
For example, Figure 2 shows the history tree for the
a
b
zb
b b
b
b
b
vr
vr vr vr
vr vr vr
vr vr vr
a
a a
b z
z
a
a
b
p(x | )bz ? a
Figure 3: An arboreal context tree (ACT).
9th word prediction based on the status depicted at
the top of Figure 1. An arboreal context tree is a
data structure for exible history tree classication.
Each node of an ACT is labeled with a subtree of
the history tree. The label of the root is a null tree
and if a node has child nodes, their labels are the
series of trees made by expanding a leaf of the tree
labeling the parent node. For example, each child
node of the root in Figure 3 is labeled with a tree
produced by adding the right-most child to the label
of the root. Each node of an ACT has a probability
distribution P (xjt), where x is an symbol and t is the
label of the node. For example, let ha
k
   a
2
a
1
ia
0
represent a tree consisting of the root labeled with
a
0
and k child nodes labeled with a
k
;    ; a
2
, and a
1
,
so the right-most node at the bottom of the ACT in
Figure 3 has a probability distribution of the symbol
x under the condition that the history matches the
partial parse trees hhz?iaihbi, where \?" matches
with an arbitrary symbol. Putting it in another way,
the next word is predicted from the history having b
as the head of the right-most partial parse tree, a as
the head of the second right-most partial parse tree,
and z as the second right-most child of the second
right-most partial parse tree. For example, in Figure
2 the subtree consisting of w
4
, w
6
, and w
8
is referred
to for the prediction of the 9th word w
9
in Figure
1 under the following set of conditions: a = w
6
,
b = w
8
, and z = w
4
.
3.2 An SLM with ACTs
An ACT is applied to a classication of the condition
parts of both of the two conditional probabilities in
Equation (2). Thus, an SLM with ACTs is:
P (T ) =
n
Y
i=1
P (w
i
jACT
w
(ht
i 1
i))
P (l
i
jACT
s
(ht
i 1
w
i
i)); (3)
where ACT
w
is an ACT for word prediction and
ACT
s
is an ACT for structure prediction. Note that
this is a generalization of the prediction from the two
right-most exposed heads (w
6
and w
8
) in the English
model (Chelba and Jelinek, 2000). In general, SLMs
with ACTs includes SLMs with xed history refer-
ence mechanisms as special cases.
4 Parser
In this section, we explain our parser based on the
SLM with ACTs we described in Sections 2 and 3.
4.1 Stochastic Parser Based on an SLM
A syntactic analyzer, based on a stochastic language
model, calculates the parse tree with the highest
probability
^
T for a given sequence of words w ac-
cording to
^
T = argmax
T
P (T jw)
= argmax
T
P (T jw)P (w)
= argmax
T
P (wjT )P (T ) (



Bayes' formula)
= argmax
T
P (T ) (



P (wjT ) = 1);
where the concatenation of the words in the syntac-
tic tree T is equal to w. P (T ) is an SLM. In our
parser, P (T ) is the probability of a parse tree T de-
ned by the SLM based on the dependency with the
ACTs (see Equation (3)).
4.2 Solution Search Algorithm
As shown in Equation (3), our parser is based on a
hidden Markov model. It follows that the Viterbi
algorithm is applicable to search for the best solu-
tion. The Viterbi algorithm is capable of nding the
best solution in O(n) time, where n is the number
of input words.
The parser repeats state transitions, reading
words of the input sentence from beginning to end.
So that the structure of the input sentence will be
a single parse tree, the number of trees in the nal
state t
n
must be 1 (l
n
= 1). Among the nal pos-
sible states that satisfy this constraint, the parser
selects the state with the highest probability. Since
our language model uses only a limited part of a
partial parse tree to distinguish among states, the
nal state does not contain enough information to
construct the parse tree. The parser can, however,
calculate the parse tree from the sequence of states,
Table 1: Corpus.
#sentences #words #chars
learning 9,108 260,054 400,318
test 1,011 28,825 44,667
Table 2: Word-based parsing accuracy.
language model parsing accuracy
SLM with ACTs 92.8% (24,867/26,803)
SLM with xed history 89.8% (24,060/26,803)
baseline

79.4% (21,278/26,803)
* Each word depends on the next one.
or from the combination of the word sequence and
the sequence of l
i
, the number of words whose modi-
cands have not appeared yet. Therefore our parser
records these values at each prediction step. After
the most probable last state has been selected, the
parser constructs the parse tree by reading these se-
quences from beginning to end.
5 Evaluation
We developed an SLM with a constant history ref-
erence (Mori et al, 2000) and one with ACTs as
explained in Section 3, and then implemented SLM-
based parsers using the solution search algorithm
presented in Section 4. In this section, we report
the results of the parsing experiments and discuss
them.
5.1 Conditions on the Experiments
The corpus used in our experiments consisted of ar-
ticles extracted from a nancial newspaper (Nihon
Keizai Shinbun). Each sentence in the articles is seg-
mented into words and each word is annotated with
a part-of-speech (POS) and the word it depends on.
There are 16 basic POSs in this corpus. Table 1
shows the corpus size. The corpus was divided into
ten parts, and the parameters of the model were es-
timated from nine of them (learning) and the model
was tested on the remaining one (test).
In parameter estimation and parsing, the SLM
with ACTs distinguishes lexicons of function words
(4 POSs) and ignores lexicons of content words (12
POSs) in order to avoid the data-sparseness prob-
lem. As a result, the alphabet of the SLM with
ACTs consists of 192 function words, 4 symbols for
unknown function words, and 12 symbols for content
words. The SLM of the constant history reference
selects words to be lexicalized referring to the ac-
curacy of a withheld corpus (a small portion of the
learning corpus).
85%
90%
95%
80%
100%
0 1 2 3 4 5 6 701 010101 0101 0101
#characters in learning corpus
a
ccu
ra
cy
83.15%
86.98%
89.17%
90.97%
92.78%
Figure 4: Relation between corpus size and parsing
accuracy.
5.2 Evaluation
One of the major criteria for a dependency parser is
the accuracy of its output dependency relationships.
For this criterion, the input of a parser is a sequence
of words, each annotated with a POS. The accuracy
is the ratio of correct dependencies (matches in the
corpus) to the number of the words in the input:
accuracy
=
#words depending on the correct word
#words
:
The last word and the second-to-last word of a sen-
tence are excluded, because there is no ambiguity.
The last word has no word to depend on and the
second-to-last word always depends on the last word.
Table 2 shows the accuracies of the SLM with
ACTs, the SLM of the constant history reference,
and a baseline in which each word depends on the
next one. This result shows that the variable his-
tory reference mechanism based on ACTs reduces
30% of the errors of the SLM of a constant history
reference. This proves experimentally that ACTs
improve an SLM for use as a spoken language un-
derstanding engine.
We calculated the parsing accuracy of the models
whose parameters were estimated from 1/4, 1/16,
and 1/64 of the learning corpus and plotted them in
Figure 4. The gradient of the accuracy curve at the
point of the maximum learning corpus size is still im-
portant. It suggests that an accuracy of 95% should
be achieved by annotating about 30,000 sentences.
Similar to most of the parsers for many languages,
our parser is based on words. However, most other
parsers for Japanese are based on a unique phrasal
unit called a bunsetsu, a concatenation of one or
more content words followed by some grammatical
w4 w6w1 w3 w7w5 w9w8
1b 3b2b b4
w2
Figure 5: Conversion from word dependencies to
bunsetsu dependencies.
Table 3: Bunsetsu-based parsing accuracy.
language model parsing accuracy
SLM with ACTs 87.8% (674/768)
JUMAN+KNP 85.3% (655/768)
baseline

62.4% (479/768)
* Each bunsetsu depends on the next one.
function words. In order to compare our parser with
one of the state-of-the-art parsers, we calculated the
bunsetsu-based accuracies of our model and KNP
(Kurohashi and Nagao, 1994) on the rst 100 sen-
tences of the test corpus. First the sentences were
segmented into words by JUMAN (Kurohashi et al,
1994) and the output word sequences are parsed by
KNP. Next, the word-based dependencies output by
our parser were changed into bunsetsu as used by
KNP, where the bunsetsu which is depended upon
by a bunsetsu is dened as the bunsetsu containing
the word depended upon by the last word of the
source bunsetsu (see Figure 5). Table 3 shows the
bunsetsu-based accuracies of our model and KNP. In
accuracy, our parser outperformed KNP, but the dif-
ference was not statistically signicant. In addition,
there were dierences in the experimental environ-
ment:
 The test corpus size was limited.
 The POS system for the KNP input is detailed,
so it has much more information than our SLM-
based parser.
 KNP in this experiment was not equipped with
commercial dictionaries.
As we mentioned above, our current model does
not attempt to use lexical information about con-
tent words because of the data-sparseness problem.
If we select the content words to be lexicalized by
referring to the accuracy of the withheld corpus, the
accuracy increases slightly to 92.9%. This means,
however, our method is not able to eciently use
lexical information about the content words at this
stage. Some model renement should be explored
for further improvements.
6 Related Work
Historically, the structures of natural languages have
been described by a CFG and most parsers (Fujisaki
et al, 1989; Pereira and Schabes, 1992; Charniak,
1997) are based on it. An SLM for English (Chelba
and Jelinek, 2000), proposed as a language model for
speech recognition, is also based on a CFG. On the
other hand, an SLM for Japanese (Mori et al, 2000)
is based on a Markov model by introducing a limit on
language structures caused by our human memory
limitations (Yngve, 1960; Miller, 1956). We intro-
duced the same limitation into our language model
and our parser is also based on a Markov model.
In the last decade, the importance of the lexi-
con has come into focus in the area of stochastic
parsers. Nowadays, many state-of-the-art parsers
are based on lexicalized models (Charniak, 1997;
Collins, 1997). In these papers, they reported sig-
nicant improvement in parsing accuracy by lexi-
calization. Our model is also lexicalized, the lexi-
calization is limited to grammatical function words
because of the sparseness of data at the step of next
word prediction. The greatest dierence between
our parser and many state-of-the-art parsers is that
our parser is based on a generative language model,
which works as a language model of a speech recog-
nizer. Therefore, a speech recognizer equipped with
our parser as its language model should be useful
for a spoken language understanding system. The
greatest advantage of our model over other struc-
tured language models is the ablity to refer to a vari-
able part of the structured history by using ACTs.
There have been several attempts at Japanese
parsers (Kurohashi and Nagao, 1994; Haruno et al,
1998; Fujio and Matsumoto, 1998; Kudo and Mat-
sumoto, 2000). These Japanese parsers have all been
based on a unique phrasal unit called a bunsetsu, a
concatenation of one or more content words followed
by some grammatical function words. Unlike these
parsers, our model describes dependencies between
words. Thus our parser can more easily be extended
to other languages. In addition, since almost all
pasers in other languages than Japanese output re-
lationships between words, the output of our parser
can be used by post-parser language processing sys-
tems proposed for many other languages (such as a
word-level structural alignment of sentences in dif-
ferent languages).
7 Conclusion
In this paper we have described a structured lan-
guage model (SLM) based on a dependency gram-
mar. An SLM treats a sentence as a word sequence
and predicts each word from beginning to end. The
history at each step of prediction is a sequence of
partial parse trees covering the preceding words.
The problem is how to classify the tree-shaped histo-
ries to predict each word and structure while avoid-
ing data-sparseness problems. As an answer, we pro-
pose to apply arboreal context trees (ACTs) to an
SLM. An ACT is an extension of a context tree to
a tree-shaped history. We built a parser based on
an SLM with ACTs, whose parameters were esti-
mated from 9,108 syntactically annotated sentences
from a nancial newspaper. We then tested the
parser on 1,011 sentences from the same newspa-
per. The accuracy of the dependency relationships
of the parser was 92.8%, higher than the accuracy of
a parser based on an SLM without ACTs (89.8%).
This proved experimentally that ACTs improve a
parser based on an SLM.
References
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the 14th National Conference on Arti-
cial Intelligence, pages 598{603.
Eugene Charniak. 2001. Immediate-head parsing
for language models. In Proceedings of the 39th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 124{131.
Ciprian Chelba and Frederic Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In
Proceedings of the 17th International Conference
on Computational Linguistics, pages 225{231.
Ciprian Chelba and Frederic Jelinek. 2000. Struc-
tured language modeling. Computer Speech and
Language, 14:283{332.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, pages 16{23.
Masakazu Fujio and Yuji Matsumoto. 1998.
Japanese dependency structure analysis based on
lexicalized statistics. In Proceedings of the Third
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 87{96.
T. Fujisaki, F. Jelinek, J. Cocke, E. Black, and
T. Nishino. 1989. A probabilistic parsing method
for sentence disambiguation. In Proceedings of the
International Parsing Workshop.
Masahiko Haruno, Satoshi Shirai, and Yoshifumi
Ooyama. 1998. Using decision trees to construct a
practical parser. In Proceedings of the 17th Inter-
national Conference on Computational Linguis-
tics, pages 505{511.
Taku Kudo and Yuji Matsumoto. 2000. Japanese
dependency structure analysis based on support
vector machines. In Proceedings of the 2000 Joint
SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora.
Sadao Kurohashi and Makoto Nagao. 1994. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistics, 20(4):507{534.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improvements
of Japanese morphological analyzer JUMAN. In
Proceedings of the International Workshop on
Sharable Natural Language Resources, pages 22{
28.
George A. Miller. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. The Psychological Re-
view, 63:81{97.
Shinsuke Mori, Masafumi Nishimura, Nobuyasu
Itoh, Shiho Ogino, and Hideo Watanabe. 2000. A
stochastic parser based on a structural word pre-
diction model. In Proceedings of the 18th Interna-
tional Conference on Computational Linguistics,
pages 558{564.
Shinsuke Mori, Masafumi Nishimura, and Nobuyasu
Itoh. 2001. Improvement of a structured language
model: Arbori-context tree. In Proceedings of the
Seventh European Conference on Speech Commu-
nication and Technology.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Annual Meeting
of the Association for Computational Linguistics,
pages 128{135.
Dana Ron, Yoram Singer, and Naftali Tishby. 1996.
The power of amnesia: Learning probabilistic au-
tomata with variable memory length. Machine
Learning, 25:117{149.
Victor H. Yngve. 1960. A model and a hypothesis
for language structure. The American Philosoph-
ical Society, 104(5):444{466.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 729?736,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Phoneme-to-Text Transcription System with an Infinite Vocabulary
Shinsuke Mori Daisuke Takuma Gakuto Kurata
IBM Research, Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimotsuruma Yamato-shi, 242-8502, Japan
mori@fw.ipsj.or.jp
Abstract
The noisy channel model approach is suc-
cessfully applied to various natural lan-
guage processing tasks. Currently the
main research focus of this approach is
adaptation methods, how to capture char-
acteristics of words and expressions in a
target domain given example sentences in
that domain. As a solution we describe a
method enlarging the vocabulary of a lan-
guage model to an almost infinite size and
capturing their context information. Espe-
cially the new method is suitable for lan-
guages in which words are not delimited
by whitespace. We applied our method
to a phoneme-to-text transcription task in
Japanese and reduced about 10% of the er-
rors in the results of an existing method.
1 Introduction
The noisy channel model approach is being suc-
cessfully applied to various natural language pro-
cessing (NLP) tasks, such as speech recognition
(Jelinek, 1985), spelling correction (Kernighan
et al, 1990), machine translation (Brown et al,
1990), etc. In this approach an NLP system
is composed of two modules: one is a task-
dependent part (an acoustic model for speech
recognition) which describes a relationship be-
tween an input signal sequence and a word, the
other is a language model (LM) which measures
the likelihood of a sequence of words as a sen-
tence in the language. Since the LM is a common
part, its improvement augments the accuracies of
all NLP systems based on a noisy channel model.
Recently the main research focus of LM is shift-
ing to the adaptation method, how to capture the
characteristics of words and expressions in a tar-
get domain. The standard adaptation method is to
prepare a corpus in the application domain, count
the frequencies of words and word sequences, and
manually annotate new words with their input sig-
nal sequences to be added to the vocabulary. It is
now easy to gather machine-readable sentences in
various domains because of the ease of publication
and access via the Web (Kilgarriff and Grefen-
stette, 2003). In addition, traditional machine-
readable forms of medical reports or business re-
ports are also available. When we need to develop
an NLP system in various domains, there is a huge
but unannotated corpus.
For languages, such as Japanese and Chinese, in
which the words are not delimited by whitespace,
one encounters a word identification problem be-
fore counting the frequencies of words and word
sequences. To solve this problem one must have a
good word segmenter in the domain of the corpus.
The only robust and reliable word segmenter in the
domain is, however, a word segmenter based on
the statistics of the lexicons in the domain! Thus
we are obliged to pay a high cost for the manual
annotation of a corpus for each new subject do-
main.
In this paper, we propose a novel framework for
building an NLP system based on a noisy chan-
nel model with an almost infinite vocabulary. In
our method, first we estimate the probability of a
word boundary existing between two characters at
each point of a raw corpus in the target domain.
Using these probabilities we regard the corpus as
a stochastically segmented corpus (SSC). We then
estimate word  -gram probabilities from the SSC.
Then we build an NLP system, the phoneme-to-
text transcription system in this paper. To de-
scribe the stochastic relationship between a char-
acter sequence and its phoneme sequence, we also
propose a character-based unknown word model.
With this unknown word model and a word  -
gram model estimated from the SSC, the vocab-
ulary of our LM, a set of known words with their
context information, is expanded from words in a
729
small annotated corpus to an almost infinite size,
including all substrings appearing in the large cor-
pus in the target domain. In experiments, we esti-
mated LMs from a relatively small annotated cor-
pus in the general domain and a large raw corpus
in the target domain. A phoneme-to-text transcrip-
tion system based on our LM and unknown word
model eliminated about 10% of the errors in the
results of an existing method.
2 Task Complexity
In this section we explain the phoneme-to-text
transcription task which our new framework is ap-
plied to.
2.1 Phoneme-to-text Transcription
To input a sentence in a language using a device
with fewer keys than the alphabet we need some
kind of transcription system. In French stenotypy,
for example, a special keyboard with 21 keys is
used to input French letters with accents (Der-
ouault and Merialdo, 1986). A similar problem
arises when we write an e-mail in any language
with a mobile phone or a PDA. For languages
with a much larger character set, such as Chi-
nese, Japanese, and Korean, a transcription system
called an input method is indispensable for writing
on a computer (Lunde, 1998).
The task we chose for the evaluation of
our method is phoneme-to-text transcription in
Japanese, which can also be regarded as a pseudo-
speech recognition in which the acoustic model
is perfect. In order to input Japanese to a com-
puter, the user types phoneme sequences and the
computer offers possible transcription candidates
in the descending order of their estimated simi-
larities to the characters the user wants to input. 
Then the user chooses the proper one.
2.2 Ambiguities
A phoneme sequence in Japanese (written in sans-
serif font in this paper) is highly ambiguous for
a computer. There are many possible word se-
quences with similar pronunciations. These am-
biguities are mainly due to three factors:
  Homonyms: There are many words sharing the
same phoneme sequences. In the spoken lan-
guage, they are less ambiguous since they are
  Generally one of Japanese phonogram sets is used as
phoneme. A phonogram is input by a combination of un-
ambiguous ASCII characters.
pronounced with different intonations. Intona-
tional signals are, however, omitted in the input
of phoneme-to-text transcription.
  Lack of word boundaries: A word of a long
sequence of phonemes can be split into sev-
eral shorter words, such as frequent content
words, particles, etc. (ex.  -- --/thanks
vs.  -/ant  /is -/ten).
  Variations in writing: Some words have more
than one acceptable spellings. For example,?
???/--	-
/bank-transfer is often writ-
ten as??/--	-
 omitting two verbal end-
ings, especially in business writing.
Most of these ambiguities are not difficult to re-
solve for a native speaker who is familiar with the
domain. So the transcription system should offer
the candidate word sequences for each context and
domain.
2.3 Available Resources
Generally speaking, three resources are available
for a phoneme-to-text transcription based on the
noisy channel model:
  annotated corpus:
a small corpus in the general domain annotated
with word boundary information and phoneme
sequences for each word
  single character dictionary:
a dictionary containing all possible phoneme se-
quences for each single character
  raw corpus in the target domain:
a collection of text samples in the target do-
main extracted from the Web or documents in
machine-readable form
3 Language Model and its Application
A stochastic LM  is a function from a sequence
of characters      to the probability. The sum-
mation over all possible sequences of characters
must be equal to or less than 1. This probability is
used as the likelihood in the NLP system.
3.1 Word  -gram Model
The most famous LM is an  -gram model based
on words. In this model, a sentence is regarded as
a word sequence  
 
(  
 


  
 
) and words
are predicted from beginning to end:


  
  
 
 


 
 

730
where 

   and 
  
is a special symbol
called a   (boundary token). Since it is impossi-
ble to define the complete vocabulary, we prepare
a special token  for unknown words and an un-
known word spelling    
 
is predicted by the fol-
lowing character-based  -gram model after  is
predicted by 

:


 
 
 
 
  
 
 
 
 
 

 
 
 
 (1)
where 

   and 
 
 
 
is a special symbol  .
Thus, when 

is outside of the vocabulary  ,
 


 
 
   



 
 
 
	
3.2 Automatic Word Segmentation
Nagata (1994) proposed a stochastic word seg-
menter based on a word  -gram model to solve
the word segmentation problem. According to this
method, the word segmenter divides a sentence  
into a word sequence with the highest probability

   argmax
 


	
Nagata (1994) reported an accuracy of about 97%
on a test corpus in the same domain using a learn-
ing corpus of 10,945 sentences in Japanese.
3.3 Phoneme-to-text Transcription
A phoneme-to-text transcription system based on
an LM 
 (Mori et al, 1999) receives a phoneme
sequence  and returns a list of candidate sen-
tences  
 
  

    in descending order of the
probability   :

     
 
  

   
where       

 	   

	
Similar to speech recognition, the probability is
decomposed into two independent parts: a pronun-
ciation model (PM) and an LM.
  

 	   



  

  


 
	
  

  


 
   

  

 	   

  

 (2)




  is independent of  

and  

	
In this formula    is an LM representing the
likelihood of a sentence  . For the LM, we can
use a word  -gram model we explained above.
The other part in the above formula    is a
PM representing the probability that a given sen-
tence   is pronounced as . Since it is impossible
to collect the phoneme sequences  for all pos-
sible sentences  , the model is decomposed into
a word-based model 

in which the words are
pronounced independently


  
 
 
 



 (3)
where 

is a phoneme sequence corresponding to
the word 

and the condition     
 
is met.
The probabilities  



 are estimated from
a corpus in which each word is annotated with a
phoneme sequence as follows:
 



  


 





 (4)
where  stands for the frequency of an event 
in the corpus. For unknown words no transcription
model has been proposed and the phoneme-to-text
transcription system (Mori et al, 1999) simply re-
turns the phoneme sequence itself. This is done
by replacing the unknown word model based on
the Japanese character set 

  by a model
based on the phonemic alphabet 

.
Thus the candidate evaluation metric of a
phoneme-to-text transcription (Mori et al, 1999)
composed of the word  -gram model and the
word-based pronunciation model is as follows:
      
 
 
 



 


 



 

 (5)
 

 


 
 
 



 if 

 
 
 
 




 if 


 	
4 LM Estimation from a Stochastically
Segmented Corpus (SSC)
To cope with segmentation errors, the concept
of stochastic segmentation is proposed (Mori and
Takuma, 2004). In this section, we briefly explain
a method of calculating word  -gram probabilities
on a stochastically segmented corpus in the target
domain. For a detailed explanation and proofs of
the mathematical soundness, please refer to the pa-
per (Mori and Takuma, 2004).
 One of the Japanese syllabaries Katakana is used to spell
out imported words by imitating their Japanese-constrained
pronunciation and the phoneme sequence itself is the correct
transcription result for them. Mori et. al. (1999) reported that
approximately 33.0% of the unknown words in a test corpus
were imported words.
731
xk+1xbn nexbn+1x
wn
x i xb1 xe1 xb2 e2x
1w w2
1-Pbn( ) 1-Pbn+1( ) P neP Pi e1 Pe2b21-P( )1-Pb1( )r 1nf (w ) =
Figure 1: Word  -gram frequency in a stochastically segmented corpus (SSC).
4.1 Stochastically Segmented Corpus (SSC)
A stochastically segmented corpus (SSC) is de-
fined as a combination of a raw corpus 
	
(here-
after referred to as the character sequence   
 
)
and word boundary probabilities 

that a word
boundary exists between two characters 

and

 
. Since there are word boundaries before the
first character and after the last character of the
corpus, 

  

 
  .
In (Mori and Takuma, 2004), the word bound-
ary probabilities are defined as follows. First the
word boundary estimation accuracy  of an auto-
matic word segmenter is calculated on a test cor-
pus with word boundary information. Then the
raw corpus is segmented by the word segmenter.
Finally 

is set to be  for each  where the word
segmenter put a word boundary and 

is set to
be    for each  where it did not put a word
boundary. We adopted the same method in the ex-
periments.
4.2 Word  -gram Frequency
Word  -gram frequencies on an SSC is calculated
as follows:
Word 0-gram frequency: This is defined as an
expected number of words in the SSC:
    

 
 

 


	
Word  -gram frequency (    ): Let us think
of a situation (see Figure 1) in which a word se-
quence 
 
occurs in the SSC as a subsequence
beginning at the   -th character and end-
ing at the -th character and each word 


in the word sequence is equal to the character
sequence beginning at the 


-th character and
ending at the 


-th character ( 


  


  
   ; 


    

 
        ;

 
    ; 

  ). The word  -gram fre-
quency of a word sequence 
	


 
 in the SSC is
defined by the summation of the stochastic fre-
quency at each occurrence of the character se-
quence of the word sequence 
 
over all of the
occurrences in the SSC:

	


 
  



 







 

 





 
 


 


	









where 
 
  
 
 

     

 and 

 
 

 
 




  


      .
4.3 Word  -gram probability
Similar to the word  -gram probability estimation
from a decisively segmented corpus, word  -gram
probabilities in an SSC are estimated by the maxi-
mum likelihood estimation method as relative val-
ues of word  -gram frequencies:

	
  

	


	



	



 
 
  

	


 


	

 
 

  	 	
5 Phoneme-to-Text Transcription with
an Infinite Vocabulary
The vocabulary of an LM estimated from an
SSC consists of all subsequences occurring in it.
Adding a module describing a stochastic relation-
ship between these subsequences and input signal
sequences, we can build a phoneme-to-text tran-
scription system equipped with an almost infinite
vocabulary.
5.1 Word Candidate Enumeration
Given a phoneme sequence as an input, the dic-
tionary of a phoneme-to-text transcription system
described in Subsection 3.3 returns pairs of a word
and a probability per Equation (4). Similarly, the
dictionary of a phoneme-to-text system with an in-
finite vocabulary must be able to take a phoneme
sequence  and return all possible pairs of a char-
acter sequence  and the probability   as
word candidates. This is done as follows:
1. First we prepare a single character dictionary
containing all characters  in the language an-
notated with their all possible phoneme se-
quences 

  
 
 

 	 	 	  

. For
732
example, the Japanese single character dictio-
nary contains a character    ??? annotated
with its all possible phoneme sequences ?  
   	    .
2. Then we build a phoneme-to-text transcrip-
tion system for single characters equipped with
the vocabulary consisting of the union set of
phoneme sequences for all characters. Given
a phoneme sequence , this module returns all
possible character sequences  with its gener-
ation probability  . For example, given
a subsequence of the input phoneme sequence
   , this module returns    ??
??????????? ???? ???
?    as a word candidate set alng with their
generation probabilities.
3. There are various methods to calculate the
probability  . The only condition is that
given    
 


   


,   must be a
stochastic language model (cf. Section 3) on the
alphabet  . In the experiments, we assumed the
uniform distribution of phoneme sequences for
each character as follows:
     
 


   


  


 
 





	 (6)
The module we described above receives a
phoneme sequence and enumerates its decomposi-
tions to subsequences contained in the single char-
acter dictionary. This module is implemented us-
ing a dynamic programming method. In the ex-
periments we limited the maximum length of the
input to 16 phonemes.
5.2 Modeling Contexts of Word Candidates
Word  -gram probability estimated from an SSC
may not be as accurate as an LM estimated from a
corpus segmented appropriately by hand. Thus we
use the following interpolation technique:
 



   







  
	

	





where 

is history before 

, 

is the probabil-
ity estimated from a segmented corpus 

, and 
	
is the probability estimated by our method from a
raw corpus 
	
. The 

and 
	
are interpolation
coefficients which are estimated by the deleted in-
terpolation method (Jelinek et al, 1991).
 More precisely, it may happen that the same phoneme
sequence is generated from a character sequence in multiple
ways. In this case the generation probability is calculated as
the summation over all possible generations.
In the experiments, the word bi-gram model in
our phoneme-to-text transcription system is com-
bined with word bi-gram probabilities estimated
from an SSC. Thus the phoneme-to-text transcrip-
tion system of our new framework refers to the
following LM to measure the likelihood of word
sequences:
 

 (7)
 
























 
  
	

	



 

if 

 





 





	

	



 

if 


   

 
	






 









	


   
if 


   


 
	

where 
	
is the set of all subsequences appearing
in the SSC.
Our LM based on Equation (7) and an existing
LM (cf. Equation (5)) behave differently when
they predict an out-of-vocabulary word appearing
in the SSC, that is 


   

 
	
. In
this case our LM has reliable context informa-
tion on the OOV word to help the system choose
the proper word. Our system also clearly func-
tions better than the LM interpolated with a word
 -gram model estimated from the automatic seg-
mentation result of the corpus when the result is a
wrong segmentation. For example, when the au-
tomatic segmentation result of the sequence ??
??? (the abbreviation of Japan TV broadcasting
corporation) has a word boundary between ???
and ??,? the uni-gram probability  ??? is
equal to 0 and an OOV word ????? is never
enumerated as a candidate.	 To the contrary, us-
ing our method  ???   when the sequence
????? appears in the SSC at least once. Thus
the sequence is enumerated as a candidate word.
In addition, when the sequence appears frequently
in the SSC,  ???   and the word may ap-
pear at a high position in the candidate list even if
the automatic segmenter always wrongly segments
the sequence into ??? and ??? .?
5.3 Default Character for Phoneme
In very rare cases, it happens that the input
phoneme sequence cannot be decomposed into
phoneme sequences in the vocabulary and those
 Two word fragments ??? and ???? may be enumer-
ated as word candidates. The notion of word may be neces-
sary for the user?s facility. However, we do not discuss the
necessity of the notion of word in the phoneme-to-text tran-
scription system.
733
corresponding to subsequences of the SSC and,
as a result, the transcription system does not out-
put any candidate sentence. To avoid this sit-
uation, we prepare a default character for every
phoneme and the transcription system also enu-
merates the default character for each phoneme. In
Japanese from the viewpoint of transcription ac-
curacy, it is better to set the default characters to
katakana, which are used mainly for translitera-
tion of imported words. Since a katakana is pro-
nunced uniquely (


   ),
     
 


   


   	 (8)
From Equations (4), (6), and (8), the PM of our
transcription system is as follows:
 



 (9)
 
















 





 if 

 


 
 





 if 


  

 
	

 if 


  


 
	

where 

  
 


   


.
5.4 Phoneme-to-Text Transcription with an
Infinite Vocabulary
Finally, the transcription system with an infinite
vocabulary enumerates candidate sentence    

 


  
 
in the descending order of the follow-
ing evaluation function value composed of an LM
 

 defined by Equation (7) and a PM  




defined by Equation (9):
      
 
 
 



 


Note that there are only three cases since the case
decompositions in Equation (7) and Equation (9)
are identical.
6 Evaluation
As an evaluation of our phoneme-to-text transcrip-
tion system, we measured transcription accuracies
of several systems on test corpora in two domains:
one is a general domain in which we have a small
annotated corpus with word boundary information
and phoneme sequence for each word, and the
other is a target domain in which only a large raw
corpus is available. As the transcription result, we
took the word sequence of the highest probability.
In this section we show the results and evaluate
our new framework.
Table 1: Annotated corpus in general domain
#sentences #words #chars
learning 20,808 406,021 598,264
test 2,311 45,180 66,874
Table 2: Raw corpus in the target domain
#sentences #words #chars
learning 797,345 ? 17,645,920
test 1,000 ? 20,935
6.1 Conditions on the Experiments
The segmented corpus used in our experiments is
composed of articles extracted from newspapers
and example sentences in a dictionary of daily
conversation. Each sentence in the corpus is seg-
mented into words and each word is annotated
with a phoneme sequence. The corpus was di-
vided into ten parts. The parameters of the model
were estimated from nine of them (learning) and
the model was tested on the remaining one (test).
Table 1 shows the corpus size. Another corpus
we used in the experiments is composed of daily
business reports. This corpus is not annotated
with word boundary information nor phoneme se-
quence for each word. For evaluation, we se-
lected 1,000 sentences randomly and annotated
them with the phoneme sequences to be used as
a test set. The rest was used for LM estimation
(see Table 2).
6.2 Evaluation Criterion
The criterion we used for transcription systems is
precision and recall based on the number of char-
acters in the longest common subsequence (LCS)
(Aho, 1990). Let 

be the number of char-
acters in the correct sentence, 
 
be that in the
output of a system, and 

be that of the LCS
of the correct sentence and the output of the sys-
tem, so the recall is defined as 



and
the precision as 


 
.
6.3 Models for Comparison
In order to clarify the difference in the usages of
the target domain corpus, we built four transcrip-
tion systems and compared their accuracies. Be-
low we explain the models in detail.
Model : Baseline
A word bi-gram model built from the segmented
general domain corpus.
734
Table 3: Phoneme-to-text transcription accuracy.
word bi-gram from raw corpus unknown General domain Target domain
the annotated corpus usage word model Precision Recall Precision Recall
 Yes No No 89.80% 92.30% 68.62% 78.40%
 Yes Auto. Seg. No 92.67% 93.42% 80.59% 86.19%

 Yes Auto. Seg. Yes 92.52% 93.17% 90.35% 93.48%
 Yes Stoch. Seg. Yes 92.78% 93.40% 91.10% 94.09%
The vocabulary contains 10,728 words appearing
in more than one corpora of the nine learning cor-
pora. The automatic word segmenter used to build
the other three models is based on the method ex-
plained in Section 3 with this LM.
Model : Decisive segmentation
A word bi-gram model estimated from the au-
tomatic segmentation result of the target corpus
interpolated with model .
Model : Decisive segmentation
Model  extended with our PM for unknown
words
Model : Stochastic segmentation
A word bi-gram model estimated from the SSC
in the target domain interpolated with model 
and equipped with our PM for unknown words
6.4 Evaluation
Table 3 shows the transcription accuracy of the
models. A comparison of the accuracies in the
target domain of the Model  and Model  con-
firms the well known fact that even an automatic
segmentation result containing errors helps an LM
improve its performance. The accuracy of Model
 in the general domain is also higher than that of
Model . From this result we can say that over-
adaptation has not occurred.
Model , equipped with our PM for unknown
words, is a natural extension of Model , a model
based on an existing method. The accuracy of
Model  is higher than that of Model  in the tar-
get domain, but worse in the general domain. This
is because the vocabulary of Model  is enlarged
with the words and the word fragments contained
in the automatic segmentation result. Though no
study has been reported on the method of Model


, below we take Model  as an existing method
for a more severe evaluation.
Comparing the accuracies of Model  and
Model  in both domain, it can be said that using
our method we can build a more accurate model
than the existing methods. The main reason is that
Table 4: Relationship between the raw corpus size
and the accuracies.
Raw corpus size Precision Recall
		
  

 chars (1/100) 89.18% 92.32%
		
  
 chars (1/10) 90.33% 93.40%
		
  
 chars (1/1) 91.10% 94.09%
our phoneme model PM is able to enumerate tran-
scription candidates for out-of-vocabulary words
and word  -gram probabilities estimated from the
SSC helps the model choose the appropriate ones.
A detailed study of Table 3 tells us that the re-
duction rate of character error rate ( recall)
of Model  in the target domain (9.36%) is much
larger than that in the general domain (3.37%).
The reason for this is that the automatic word seg-
menter tends to make mistakes around character-
istic words and expressions in the target domain
and our method is much less influenced by those
segmentation errors than the existing method is.
In order to clarify the relationship between the
size of the SSC and the transcription accuracy, we
calculated the accuracies while changing the size
of the SSC (1/1, 1/10, 1/100). The result, shown
in Table 4, shows that we can still achieve a fur-
ther improvement just by gathering more example
sentences in the target domain.
The main difference between the models is the
LM part. Thus the accuracy increase is yielded by
the LM improvements. This fact indicates that we
can expect a similar improvement in other gener-
ative NLP systems using the noisy channel model
by expanding the LM vocabulary with context in-
formation to an infinite size.
7 Related Work
The well-known methods for the unknown word
problem are classified into two groups: one is to
use an unknown word model and the other is to
extract word candidates from a corpus before the
application. Below we describe the relationship
735
between these methods and the proposed method.
In the method using an unknown word model,
first the generation probability of an unknown
word is modeled by a character  -gram, and then
an NLP system, such as a morphological analyzer,
searches for the best solution considering the pos-
sibility that all subsequences might be unknown
words (Nagata, 1994; Bazzi and Glass, 2000).
In the same way, we can build a phoneme-to-
text transcription system which can enumerate un-
known word candidates, but the LM is not able to
refer to lexical context information to choose the
appropriate word, since the unknown words are
modeled to be generated from a single state. We
solved this problem by allowing the LM to refer to
information from an SSC.
When a machine-readable corpus in the target
domain is available, we can extract word candi-
dates from the corpus with a certain criterion and
use them in application. An advantage of this
method is that all of the occurrences of each can-
didate in the corpus are considered. Nagata (1996)
proposed a method calculating word candidates
with their uni-gram frequencies using a forward-
backward algorithm. and reported that the accu-
racy of a morphological analyzer can be improved
by adding the extracted words to its vocabulary.
Comparing our method with this research, it can
be said that our method executes the word can-
didate enumeration and their context calculation
dynamically at the time of the solution search for
an NLP task, phoneme-to-text transcription here.
One of the advantages of our framework is that
the system considers all substrings in the corpus
as word candidates (that is the recall of the word
extraction is 100%) and a higher accuracy is ex-
pected using a consistent criterion, namely the
generation probability, for the word candidate enu-
meration process and solution search process.
The framework we propose in this paper, en-
larging the vocabulary to an almost infinite size,
is general and applicable to many other NLP sys-
tems based on the noisy channel model, such as
speech recognition, statistical machine translation,
etc. Our framework is potentially capable of im-
proving the accuracies in these tasks as well.
8 Conclusion
In this paper we proposed a generative NLP sys-
tem with an almost infinite vocabulary for lan-
guages without obvious word boundary informa-
tion in written texts. In the experiments we com-
pared four phoneme-to-text transcription systems
in Japanese. The transcription system equipped
with an infinite vocabulary showed a higher accu-
racy than the baseline model and the model based
on the existing method. These results show the
efficacy of our method and tell us that our ap-
proach is promising for the phoneme-to-text tran-
scription task or other NLP systems based on the
noisy channel model.
References
Alfred V. Aho. 1990. Algorithms for finding pat-
terns in strings. In Handbook of Theoretical Com-
puter Science, volume A: Algorithms and Complex-
ity, pages 273?278. Elseveir Science Publishers.
Issam Bazzi and James R. Glass. 2000. Modeling out-
of-vocabulary words for robust speech recognition.
In Proc. of the ICSLP2000.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Frederick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79?85.
Anne-Marie Derouault and Bernard Merialdo. 1986.
Natural language modeling for phoneme-to-text
transcription. IEEE PAMI, 8(6):742?749.
Frederick Jelinek, Robert L. Mercer, and Salim
Roukos. 1991. Principles of lexical language
modeling for speech recognition. In Advances in
Speech Signal Processing, chapter 21, pages 651?
699. Dekker.
Frederick Jelinek. 1985. Self-organized language
modeling for speech recognition. Technical report,
IBM T. J. Watson Research Center.
Mark D. Kernighan, Kenneth W. Church, and
William A. Gale. 1990. A spelling correction pro-
gram based on a noisy channel model. In Proc. of
the COLING90, pages 205?210.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistics, 29(3):333?347.
Ken Lunde. 1998. CJKV Information Processing.
O?Reilly & Associates.
Shinsuke Mori and Daisuke Takuma. 2004. Word
n-gram probability estimation from a Japanese raw
corpus. In Proc. of the ICSLP2004.
Shinsuke Mori, Tsuchiya Masatoshi, Osamu Yamaji,
and Makoto Nagao. 1999. Kana-kanji conver-
sion by a stochastic model. Transactions of IPSJ,
40(7):2946?2953. (in Japanese).
Masaaki Nagata. 1994. A stochastic Japanese morpho-
logical analyzer using a forward-DP backward-A 
n-best search algorithm. In Proc. of the COLING94,
pages 201?207.
Masaaki Nagata. 1996. Automatic extraction of
new words from Japanese texts using generalized
forward-backward search. In EMNLP.
736
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 843?853, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Inducing a Discriminative Parser to Optimize Machine
Translation Reordering
Graham Neubig1,2, Taro Watanabe2, Shinsuke Mori1
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
This paper proposes a method for learning
a discriminative parser for machine trans-
lation reordering using only aligned par-
allel text. This is done by treating the
parser?s derivation tree as a latent variable
in a model that is trained to maximize re-
ordering accuracy. We demonstrate that
efficient large-margin training is possible
by showing that two measures of reorder-
ing accuracy can be factored over the parse
tree. Using this model in the pre-ordering
framework results in significant gains in
translation accuracy over standard phrase-
based SMT and previously proposed unsu-
pervised syntax induction methods.
1 Introduction
Finding the appropriate word ordering in the
target language is one of the most difficult prob-
lems for statistical machine translation (SMT),
particularly for language pairs with widely di-
vergent syntax. As a result, there is a large
amount of previous research that handles the
problem of reordering through the use of im-
proved reordering models for phrase-based SMT
(Koehn et al2005), hierarchical phrase-based
translation (Chiang, 2007), syntax-based trans-
lation (Yamada and Knight, 2001), or pre-
ordering (Xia and McCord, 2004).
In particular, systems that use source-
language syntax allow for the handling of long-
distance reordering without large increases in
The first author is now affiliated with the Nara Institute
of Science and Technology.
decoding time. However, these require a good
syntactic parser, which is not available for many
languages. In recent work, DeNero and Uszko-
reit (2011) suggest that unsupervised grammar
induction can be used to create source-sentence
parse structure for use in translation as a part
of a pre-ordering based translation system.
In this work, we present a method for inducing
a parser for SMT by training a discriminative
model to maximize reordering accuracy while
treating the parse tree as a latent variable. As a
learning framework, we use online large-margin
methods to train the model to directly minimize
two measures of reordering accuracy. We pro-
pose a variety of features, and demonstrate that
learning can succeed when no linguistic informa-
tion (POS tags or parse structure) is available in
the source language, but also show that this lin-
guistic information can be simply incorporated
when it is available. Experiments find that the
proposed model improves both reordering and
translation accuracy, leading to average gains
of 1.2 BLEU points on English-Japanese and
Japanese-English translation without linguistic
analysis tools, or up to 1.5 BLEU points when
these tools are incorporated. In addition, we
show that our model is able to effectively max-
imize various measures of reordering accuracy,
and that the reordering measure that we choose
has a direct effect on translation results.
2 Preordering for SMT
Machine translation is defined as transforma-
tion of source sentence F = f1 . . . fJ to target
sentence E = e1 . . . eI . In this paper, we take
843
Figure 1: An example with a source sentence F re-
ordered into target order F ?, and its corresponding
target sentence E. D is one of the BTG derivations
that can produce this ordering.
the pre-ordering approach to machine transla-
tion (Xia and McCord, 2004), which performs
translation as a two step process of reordering
and translation (Figure 1). Reordering first de-
terministically transforms F into F ?, which con-
tains the same words as F but is in the order of
E. Translation then transforms F ? into E using
a method such as phrase-based SMT (Koehn et
al., 2003), which can produce accurate transla-
tions when only local reordering is required.
This general framework has been widely stud-
ied, with the majority of works relying on a
syntactic parser being available in the source
language. Reordering rules are defined over
this parse either through machine learning tech-
niques (Xia and McCord, 2004; Zhang et al
2007; Li et al2007; Genzel, 2010; Dyer and
Resnik, 2010; Khalilov and Sima?an, 2011) or
linguistically motivated manual rules (Collins et
al., 2005; Xu et al2009; Carpuat et al2010;
Isozaki et al2010b). However, as building a
parser for each source language is a resource-
intensive undertaking, there has also been some
interest in developing reordering rules without
the use of a parser (Rottmann and Vogel, 2007;
Tromble and Eisner, 2009; DeNero and Uszko-
reit, 2011; Visweswariah et al2011), and we
will follow this thread of research in this paper.
In particular, two methods deserve mention
for being similar to our approach. First, DeNero
and Uszkoreit (2011) learn a reordering model
through a three-step process of bilingual gram-
mar induction, training a monolingual parser
to reproduce the induced trees, and training
a reordering model that selects a reordering
based on this parse structure. In contrast, our
method trains the model in a single step, treat-
ing the parse structure as a latent variable in
a discriminative reordering model. In addition
Tromble and Eisner (2009) and Visweswariah et
al. (2011) present models that use binary clas-
sification to decide whether each pair of words
should be placed in forward or reverse order. In
contrast, our method uses traditional context-
free-grammar models, which allows for simple
parsing and flexible parameterization, including
features such as those that utilize the existence
of a span in the phrase table. Our work is also
unique in that we show that it is possible to di-
rectly optimize several measures of reordering
accuracy, which proves important for achieving
good translations.1
3 Training a Reordering Model with
Latent Derivations
In this section, we provide a basic overview of
the proposed method for learning a reordering
model with latent derivations using online dis-
criminative learning.
3.1 Space of Reorderings
The model we present here is based on the
bracketing transduction grammar (BTG, Wu
(1997)) framework. BTGs represent a binary
tree derivation D over the source sentence F
as shown in Figure 1. Each non-terminal node
can either be a straight (str) or inverted (inv)
production, and terminals (term) span a non-
empty substring f .2
The ordering of the sentence is determined by
the tree structure and the non-terminal labels
str and inv, and can be built bottom-up. Each
subtree represents a source substring f and its
reordered counterpart f ?. For each terminal
node, no reordering occurs and f is equal to f ?.
1The semi-supervised method of Katz-Brown et al
(2011) also optimizes reordering accuracy, but requires
manually annotated parses as seed data.
2In the original BTG framework used in translation,
terminals produce a bilingual substring pair f/e, but as
we are only interested in reordering the source F , we
simplify the model by removing the target substring e.
844
For each non-terminal node spanning f with its
left child spanning f1 and its right child span-
ning f2, if the non-terminal symbol is str, the
reordered strings will be concatenated in order
as f ? = f ?1f ?2, and if the non-terminal symbol is
inv, the reordered strings will be concatenated
in inverted order as f ? = f ?2f ?1.
We define the space of all reorderings that can
be produced by the BTG as F ?, and attempt to
find the best reordering F? ? within this space.3
3.2 Reorderings with Latent
Derivations
In order to find the best reordering F? ? given only
the information in the source side sentence F , we
define a scoring function S(F ?|F ), and choose
the ordering of maximal score:
F? ? = arg max
F ?
S(F ?|F ).
As our model is based on reorderings licensed
by BTG derivations, we also assume that there
is an underlying derivation D that produced F ?.
As we can uniquely determine F ? given F and
D, we can define a scoring function S(D|F ) over
derivations, find the derivation of maximal score
D? = arg max
D
S(D|F )
and use D? to transform F into F ?.
Furthermore, we assume that the score
S(D|F ) is the weighted sum of a number of fea-
ture functions defined over D and F
S(D|F,w) =
?
i
wi?i(D,F )
where ?i is the ith feature function, and wi is
its corresponding weight in weight vector w.
Given this model, we must next consider how
to learn the weights w. As the final goal of our
model is to produce good reorderings F ?, it is
natural to attempt to learn weights that will al-
low us to produce these high-quality reorderings.
3BTGs cannot reproduce all possible reorderings, but
can handle most reorderings occurring in natural trans-
lated text (Haghighi et al2009).
Figure 2: An example of (a) the ranking function
r(fj), (b) loss according to Kendall?s ? , (c) loss ac-
cording to chunk fragmentation.
4 Evaluating Reorderings
Before we explain the learning algorithm, we
must know how to distinguish whether the F ?
produced by the model is good or bad. This
section explains how to calculate oracle reorder-
ings, and assign each F ? a loss and an accuracy
according to how well it reproduces the oracle.
4.1 Calculating Oracle Orderings
In order to calculate reordering quality, we first
define a ranking function r(fj |F,A), which indi-
cates the relative position of source word fj in
the proper target order (Figure 2 (a)). In or-
der to calculate this ranking function, we define
A = a1, . . . ,aJ , where each aj is a set of the in-
dices of the words in E to which fj is aligned.4
Given these alignments, we define an ordering
function aj1 < aj2 that indicates that the in-
dices in aj1 come before the indices in aj2 . For-
mally, we define this function as ?the first index
in aj1 is at most the first index in aj2 , similarly
for the last index, and either the first or last
index in aj1 is less than that of aj2 .?
Given this ordering, we can sort every align-
ment aj , and use its relative position in the sen-
tence to assign a rank to its word r(fj). In
4Null alignments require special treatment. To do so,
we can place unaligned brackets and quotes directly be-
fore and after the spans they surround, and attach all
other unaligned words to the word directly to the right
for head-initial languages (e.g. English), or left for head-
final languages (e.g. Japanese).
845
the case of ties, where neither aj1 < aj2 nor
aj2 < aj1 , both fj1 and fj2 are assigned the
same rank. We can now define measures of re-
ordering accuracy for F ? by how well it arranges
the words in order of ascending rank. It should
be noted that as we allow ties in rank, there
are multiple possible F ? where all words are in
strictly ascending order, which we will call ora-
cle orderings.
4.2 Kendall?s ?
The first measure of reordering accuracy that
we will consider is Kendall?s ? (Kendall, 1938),
a measure of pairwise rank correlation which
has been proposed for evaluating translation re-
ordering accuracy (Isozaki et al2010a; Birch
et al2010) and pre-ordering accuracy (Talbot
et al2011). The fundamental idea behind the
measure lies in comparisons between each pair of
elements f ?j1 and f ?j2 of the reordered sentence,
where j1 < j2. Because j1 < j2, f ?j1 comes before
f ?j2 in the reordered sentence, the ranks should
be r(f ?j1) ? r(f ?j2) in order to produce the cor-
rect ordering.
Based on this criterion, we first define a loss
Lt(F ?) that will be higher for orderings that are
further from the oracle. Specifically, we take the
sum of all pairwise orderings that do not follow
the expected order
Lt(F ?) =
J?1
?
j1=1
J
?
j2=j1+1
?(r(f ?j1) > r(f
?
j2))
where ?(?) is an indicator function that is 1 when
its condition is true, and 0 otherwise. An exam-
ple of this is given in Figure 2 (b).
To calculate an accuracy measure for ordering
F ?, we first calculate the maximum loss for the
sentence, which is equal to the total number of
non-equal rank comparisons in the sentence5
max
F ?
Lt(F ?) =
J?1
?
j1=1
J
?
j2=j1+1
?(r(f ?j1) 6= r(f
?
j2)).
(1)
5The traditional formulation of Kendall?s ? assumes
no ties in rank, and thus the maximum loss can be cal-
culated as J(J ? 1)/2.
Finally, we use this maximum loss to normalize
the actual loss to get an accuracy
At(F ?) = 1?
Lt(F ?)
max
F? ?
Lt(F? ?)
,
which will take a value between 0 (when F ? has
maximal loss), and 1 (when F ? matches one of
the oracle orderings). In Figure 2 (b), Lt(F ?) =
2 and max
F? ?
Lt(F? ?) = 8, so At(F ?) = 0.75.
4.3 Chunk Fragmentation
Another measure that has been used in eval-
uation of translation accuracy (Banerjee and
Lavie, 2005) and pre-ordering accuracy (Talbot
et al2011) is chunk fragmentation. This mea-
sure is based on the number of chunks that the
sentence needs to be broken into to reproduce
the correct ordering, with a motivation that the
number of continuous chunks is equal to the
number of times the reader will have to jump to
a different position in the reordered sentence to
read it in the target order. One way to measure
the number of continuous chunks is considering
whether each word pair f ?j and f ?j+1 is discon-
tinuous (the rank of f ?j+1 is not equal to or one
greater than f ?j)
discont(f ?j , f ?j+1) =
?(r(f ?j) 6= r(f ?j+1) ? r(f ?j) + 1 6= r(f ?j+1))
and sum over all word pairs in the sentence to
create a sentence-based loss
Lc(F ?) =
J?1
?
j=1
discont(f ?j , f ?j+1) (2)
While this is the formulation taken by previ-
ous work, we found that this under-penalizes
bad reorderings of the first and last words of
the sentence, which can contribute to the loss
only once, as opposed to other words which can
contribute to the loss twice. To account for
this, when calculating the chunk fragmentation
score, we additionally add two sentence bound-
ary words f0 and fJ+1 with ranks r(f0) = 0 and
r(fJ+1) = 1 + max
f ?j?F ?
r(f ?j) and redefine the sum-
mation in Equation (2) to consider these words
(e.g. Figure 2 (c)).
846
procedure WeightUpdate(F , A, w)
D ? parse(F,w) . Create parse forest
D? ? argmax
D?D
S(D|F,w) + L(D|F,A)
. Find the model parse
D? ? argmin
D?D
L(D|F,A)? ?S(D|F,w)
. Find the oracle parse
if L(D?|F,A) 6= L(D?|F,A) then
w ? ?(w + ?(?(D?, F )? ?(D?, F )))
. Perform weight update
end if
end procedure
Figure 3: An online update for sentence F , alignment
A, and weight vector w. ? is a very small constant,
and ? and ? are defined by the update strategy.
Similarly to Kendall?s ? , we can also define
an accuracy measure between 0 and 1 using the
maximum loss, which will be at most J + 1,
which corresponds to the total number of com-
parisons made in calculating the loss6
Ac(F ?) = 1?
Lc(F ?)
J + 1
.
In Figure 2 (c), Lc(F ?) = 3 and J + 1 = 6, so
Ac(F ?) = 0.5.
5 Learning a BTG Parser for
Reordering
Now that we have a definition of loss over re-
orderings produced by the model, we have a
clear learning objective: we would like to find
reorderings F ? with low loss. The learning algo-
rithm we use to achieve this goal is motivated
by discriminative training for machine transla-
tion systems (Liang et al2006), and extended
to use large-margin training in an online frame-
work (Watanabe et al2007).
5.1 Learning Algorithm
Learning uses the general framework of large-
margin online structured prediction (Crammer
et al2006), which makes several passes through
the data, finding a derivation with high model
score (the model parse) and a derivation with
6It should be noted that for sentences of length one or
sentences with tied ranks, the maximum loss may be less
than J +1, but for simplicity we use this approximation.
minimal loss (the oracle parse), and updating w
if these two parses diverge (Figure 3).
In order to create both of these parses effi-
ciently, we first create a parse forest encoding a
large number of derivations Di according to the
model scores. Next, we find the model parse D?i,
which is the parse in the forest Di that maxi-
mizes the sum of the model score and the loss
S(Dk|Fk,w)+L(Dk|Fk, Ak). It should be noted
that here we are considering not only the model
score, but also the derivation?s loss. This is
necessary for loss-driven large-margin training
(Crammer et al2006), and follows the basic
intuition that during training, we would like to
make it easier to select negative examples with
large loss, causing these examples to be penal-
ized more often and more heavily.
We also find an oracle parse D?i, which is se-
lected solely to minimize the loss L(Dk|Fk, Ak).
One important difference between the model we
describe here and traditional parsing models is
that the target derivation D?k is a latent variable.
Because many Dk achieve a particular reorder-
ing F ?, many reorderings F ? are able to mini-
mize the loss L(F ?k|Fk, Ak). Thus it is necessary
to choose a single oracle derivation to treat as
the target out of many equally good reorderings.
DeNero and Uszkoreit (2011) resolve this ambi-
guity with four features with empirically tuned
scores before training a monolingual parser and
reordering model. In contrast, we follow previ-
ous work on discriminative learning with latent
variables (Yu and Joachims, 2009), and break
ties within the pool of oracle derivations by se-
lecting the derivation with the largest model
score. From an implementation point of view,
this can be done by finding the derivation that
minimizes L(Dk|Fk, Ak)??S(Dk|Fk,w), where
? is a constant small enough to ensure that the
effect of the loss will always be greater than the
effect of the score.
Finally, if the model parse D?k has a loss that
is greater than that of the oracle parse D?k, we
update the weights to increase the score of the
oracle parse and decrease the score of the model
parse. Any criterion for weight updates may be
used, such as the averaged perceptron (Collins,
2002) and MIRA (Crammer et al2006), but
847
we opted to use Pegasos (Shalev-Shwartz et al
2007) as it allows for the introduction of regu-
larization and relatively stable learning.
To perform this full process, given a source
sentence Fk, alignment Ak, and model weights
w we need to be able to efficiently calculate
scores, calculate losses, and create parse forests
for derivations Dk, the details of which will be
explained in the following sections.
5.2 Scoring Derivation Trees
First, we must consider how to efficiently assign
scores S(D|F,w) to a derivation or forest during
parsing. The most standard and efficient way to
do so is to create local features that can be cal-
culated based only on the information included
in a single node d in the derivation tree. The
score of the whole tree can then be expressed as
the sum of the scores from each node:
S(D|F,w) =
?
d?D
S(d|F,w)
=
?
d?D
?
i
wi?i(d, F ).
Based on this restriction, we define a number of
features that can be used to score the parse tree.
To ease explanation, we represent each node in
the derivation as d = ?s, l, c, c + 1, r?, where s
is the node?s symbol (str, inv, or term), while
l and r are the leftmost and rightmost indices
of the span that d covers. c and c + 1 are the
rightmost index of the left child and leftmost
index of the right child for non-terminal nodes.
All features are intersected with the node la-
bel s, so each feature described below corre-
sponds to three different features (or two for
features applicable to only non-terminal nodes).
? ?lex: Identities of words in positions fl, fr,
fc, fc+1, fl?1, fr+1, flfr, and fcfc+1.
? ?class: Same as ?lex, but with words ab-
stracted to classes. We use the 50 classes
automatically generated by Och (1999)?s
method that are calculated during align-
ment in standard SMT systems.
? ?balance: For non-terminals, features indi-
cating whether the length of the left span
(c? l+1) is lesser than, equal to, or greater
than the length of the right span (r ? c).
? ?table: Features, bucketed by length, that
indicate whether ?fl . . . fr? appears as a
contiguous phrase in the SMT training
data, as well as the log frequency of the
number of times the phrase appears total
and the number of times it appears as a
contiguous phrase (DeNero and Uszkoreit,
2011). Phrase length is limited to 8, and
phrases of frequency one are removed.
? ?pos: Same as ?lex, but with words ab-
stracted to language-dependent POS tags.
? ?cfg: Features indicating the label of the
spans fl . . . fr, fl . . . fc, and fc+1 . . . fr in a
supervised parse tree, and the intersection
of the three labels. When spans do not cor-
respond to a span in the supervised parse
tree, we indicate ?no span? with the label
?X? (Zollmann and Venugopal, 2006).
Most of these features can be calculated from
only a parallel corpus, but ?pos requires a POS
tagger and ?cfg requires a full syntactic parser
in the source language. As it is preferable to
have a method that is applicable in languages
where these tools are not available, we perform
experiments both with and without the features
that require linguistic analysis tools.
5.3 Finding Losses for Derivation Trees
The above features ? and their corresponding
weights w are all that are needed to calculate
scores of derivation trees at test time. However,
during training, it is also necessary to find model
parses according to the loss-augmented scoring
function S(D|F,w)+L(D|F,A) or oracle parses
according to the loss L(D|F,A). As noted by
Taskar et al2003), this is possible if our losses
can be factored in the same way as the feature
space. In this section, we demonstrate that the
loss L(d|F,A) for the evaluation measures we
defined in Section 4 can (mostly) be factored
over nodes in a fashion similar to features.
848
5.3.1 Factoring Kendall?s ?
For Kendall?s ? , in the case of terminal nodes,
Lt(d = ?term, l, r?|F,A) can be calculated by
performing the summation in Equation (1). We
can further define this sum recursively and use
memoization for improved efficiency
Lt(d|F,A) =Lt(?term, l, r ? 1?|F,A)
+
r?1
?
j=l
?(r(fj) > r(fr)). (3)
For non-terminal nodes, we first focus on
straight non-terminals with parent node d =
?str, l, c, c+1, r?, and left and right child nodes
dl = ?sl, l, lc, lc+1, c? and dr = ?sr, c+1, rc, rc+
1, r?. First, we note that the loss for the subtree
rooted at d can be expressed as
Lt(d|F,A) =Lt(dl|F,A) + Lt(dr|F,A)
+
c
?
j1=l
r
?
j2=c+1
?(r(fj1) > r(fj2)).
In other words, the subtree?s total loss can be
factored into the loss of its left subtree, the
loss of its right subtree, and the additional loss
contributed by comparisons between the words
spanning both subtrees. In the case of inverted
terminals, we must simply reverse the compari-
son in the final sum to be ?(r(fj1) < r(fj2)).
5.3.2 Factoring Chunk Fragmentation
Chunk fragmentation loss can be factored in a
similar fashion. First, it is clear that the loss for
the terminal nodes can be calculated efficiently
in a fashion similar to Equation (3). In order to
calculate the loss for non-terminals d, we note
that the summation in Equation (2) can be di-
vided into the sum over the internal bi-grams
in the left and right subtrees, and the bi-gram
spanning the reordered trees
Lc(d|F,A) =Lc(dl|F,A) + Lc(dr|F,A)
+ discont(f ?c, f ?c+1).
However, unlike Kendall?s ? , this equation re-
lies not on the ranks of fc and fc+1 in the origi-
nal sentence, but on the ranks of f ?c and f ?c+1 in
the reordered sentence. In order to keep track
of these values, it is necessary to augment each
node in the tree to be d = ?s, l, c, c + 1, r, tl, tr?
with two additional values tl and tr that indi-
cate the position of the leftmost and rightmost
words after reordering. Thus, a straight non-
terminal parent d with children dl = ?sl, l, lc, lc+
1, c, tl, tlr? and dr = ?sr, c+1, rc, rc+1, r, trl, tr?
will have loss as follows
Lc(d|F,A) =Lc(dl|F,A) + Lc(dr|F,A)
+ discont(ftlr, ftrl)
with a similar calculation being possible for in-
verted non-terminals.
5.4 Parsing Derivation Trees
Finally, we must be able to create a parse forest
from which we select model and oracle parses.
As all feature functions factor over single nodes,
it is possible to find the parse tree with the high-
est score in O(J3) time using the CKY algo-
rithm. However, when keeping track of target
positions for calculation of chunk fragmentation
loss, there are a total of O(J5) nodes, an unrea-
sonable burden in terms of time and memory.
To overcome this problem, we note that this set-
ting is nearly identical to translation using syn-
chronous CFGs with an integrated bigram LM,
and thus we can employ cube-pruning to reduce
our search space (Chiang, 2007).
6 Experiments
Our experiments test the reordering and trans-
lation accuracy of translation systems using the
proposed method. As reordering metrics, we use
Kendall?s ? and chunk fragmentation (Talbot et
al., 2011) comparing the system F ? and oracle
F ? calculated with manually created alignments.
As translation metrics, we use BLEU (Papineni
et al2002), as well as RIBES (Isozaki et al
2010a), which is similar to Kendall?s ? , but eval-
uated on the target sentence E instead of the re-
ordered sentence F ?. All scores are the average
of three training runs to control for randomness
in training (Clark et al2011).
For translation, we use Moses (Koehn et al
2007) with lexicalized reordering (Koehn et al
2005) in all experiments. We test three types
849
en-ja ja-en
Chunk ? BLEU RIBES Chunk ? BLEU RIBES
orig 61.22 73.46 21.87 68.25 66.42 72.99 18.34 65.36
3-step 63.51 72.55 21.45 67.66 67.17 73.01 17.78 64.42
3-step+?pos 64.28 72.11 21.45 67.44 67.56 74.21 18.18 64.65
3-step+?cfg 65.76 75.32 21.67 68.47 67.23 74.06 18.18 64.93
lader 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93
lader+?pos 73.97 79.24 23.32 69.78 75.49 78.79 19.89 67.24
lader+?cfg 75.06 80.53 23.36 70.89 75.14 77.80 19.35 66.12
Table 2: Reordering (chunk, ?) and translation (BLEU, RIBES) results for each system. Bold numbers
indicate no significant difference from the best system (bootstrap resampling with p > 0.05) (Koehn, 2004).
sent. word (ja) word (en)
RM-train 602 14.5k 14.3k
RM-test 555 11.2k 10.4k
TM/LM 329k 6.08M 5.91M
Tune 1166 26.8k 24.3k
Test 1160 28.5k 26.7k
Table 1: The number of sentences and words for
training and testing the reordering model (RM),
translation model (TM), and language model (LM).
of pre-ordering: original order with F ? ? F
(orig), pre-orderings learned using the 3-step
process of DeNero and Uszkoreit (2011) (3-
step), and the proposed model with latent
derivations (lader).7 Except when stated oth-
erwise, lader was trained to minimize chunk
fragmentation loss with a cube pruning stack
pop limit of 50, and the regularization constant
of 10?3 (chosen through cross-validation).
We test our systems on Japanese-English and
English-Japanese translation using data from
the Kyoto Free Translation Task (Neubig, 2011).
We use the training set for training translation
and language models, the development set for
weight tuning, and the test set for testing (Table
1). We use the designated development and test
sets of manually created alignments as training
data for the reordering models, removing sen-
tences of more than 60 words.
As default features for lader and the mono-
lingual parsing and reordering models in 3-step,
we use all the features described in Section 5.2
7Available open-source: http://phontron.com/lader
except ?pos and ?cfg. In addition, we test sys-
tems with ?pos and ?cfg added. For English,
we use the Stanford parser (Klein and Manning,
2003) for both POS tagging and CFG parsing.
For Japanese, we use the KyTea tagger (Neu-
big et al2011) for POS tagging,8 and the EDA
word-based dependency parser (Flannery et al
2011) with simple manual head-rules to convert
a dependency parse to a CFG parse.
6.1 Effect of Pre-ordering
Table 2 shows reordering and translation results
for orig, 3-step, and lader. It can be seen
that the proposed lader outperforms the base-
lines in both reordering and translation.9 There
are a number of reasons why lader outper-
forms 3-step. First, the pipeline of 3-step
suffers from error propogation, with errors in
monolingual parsing and reordering resulting
in low overall accuracy.10 Second, as Section
5.1 describes, lader breaks ties between ora-
cle parses based on model score, allowing easy-
to-reproduce model parses to be chosen dur-
ing training. In fact, lader generally found
trees that followed from syntactic constituency,
while 3-step more often used terminal nodes
8In addition, following the example of Sudoh et al
(2011a)?s reordering rules, we lexicalize all particles.
9It should be noted that our results for 3-step are
significantly worse than those of DeNero and Uszkoreit
(2011). Likely reasons include a 20x difference in training
data size, the fact that we are using naturally translated
text as opposed to text translated specifically to create
word alignments, or differences in implementation.
10When using oracle parses, chunk accuracy was up to
81%, showing that parsing errors are highly detrimental.
850
en-ja ja-en
Chunk ? BLEU RIBES Chunk ? BLEU RIBES
Lc 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93
Lt 70.37 79.57 22.57 69.47 72.51 78.93 18.52 66.26
Lc + Lt 72.55 80.58 22.89 70.34 74.44 79.82 19.21 66.48
Table 3: Results for systems trained to optimize chunk fragmentation (Lc) or Kendall?s ? (Lt).
that spanned constituent boundaries (as long as
the phrase frequency was high). Finally, as Sec-
tion 6.2 shows in detail, the ability of lader to
maximize reordering accuracy directly allows for
improved reordering and translation results.
It can also be seen that incorporating POS
tags or parse trees improves accuracy of both
lader and 3-step, particularly for English-
Japanese, where syntax has proven useful for
pre-ordering, and less so for Japanese-English,
where syntactic pre-ordering has been less suc-
cessful (Sudoh et al2011b).
We also tested Moses?s implementation of hi-
erarchical phrase-based SMT (Chiang, 2007),
which achieved BLEU scores of 23.21 and 19.30
for English-Japanese and Japanese-English re-
spectively, approximately matching lader in
accuracy, but with a significant decrease in de-
coding speed. Further, when pre-ordering with
lader and hierarchical phrase-based SMT were
combined, BLEU scores rose to 23.29 and 19.69,
indicating that the two techniques can be com-
bined for further accuracy improvements.
6.2 Effect of Training Loss
Table 3 shows results when one of three losses is
optimized during training: chunk fragmentation
(Lc), Kendall?s ? (Lt), or the linear interpola-
tion of the two with weights chosen so that both
losses contribute equally (Lt + Lc). In general,
training successfully maximizes the criterion it is
trained on, and Lt +Lc achieves good results on
both measures. We also find that Lc and Lc+Lt
achieve the best translation results, which is
in concert with Talbot et al2011), who find
chunk fragmentation is better correlated with
translation accuracy than Kendall?s ? . This is
an important result, as methods such as that
of Tromble and Eisner (2009) optimize pairwise
en-ja ja-en
BLEU/RIBES BLEU/RIBES
orig 21.87 68.25 18.34 65.36
man-602 23.11 69.86 19.54 66.93
auto-602 22.39 69.19 18.58 66.07
auto-10k 22.53 69.68 18.79 66.89
Table 4: Results based on data size, and whether
manual or automatic alignments are used in training.
word comparisons equivalent to Lt, which may
not be optimal for translation.
6.3 Effect of Automatic Alignments
Table 4 shows the difference between using man-
ual and automatic alignments in the training of
lader. lader is able to improve over the orig
baseline in all cases, but when equal numbers
of manual and automatic alignments are used,
the reorderer trained on manual alignments is
significantly better. However, as the number of
automatic alignments is increased, accuracy im-
proves, approaching that of the system trained
on a smaller number of manual alignments.
7 Conclusion
We presented a method for learning a discrim-
inative parser to maximize reordering accuracy
for machine translation. Future work includes
application to other language pairs, develop-
ment of more sophisticated features, investiga-
tion of probabilistic approaches to inference, and
incorporation of the learned trees directly in
tree-to-string translation.
Acknowledgments
We thank Isao Goto, Tetsuo Kiso, and anony-
mous reviewers for their helpful comments, and
Daniel Flannery for helping to run his parser.
851
References
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation
with improved correlation with human judgments.
In Proc. ACL Workshop.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, 24(1):15?26.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving arabic-to-english statistical ma-
chine translation by reordering post-verbal sub-
jects for alignment. In Proc. ACL.
David Chiang. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2).
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Control-
ling for optimizer instability. In Proc. ACL, pages
176?181.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In Proc.
EMNLP, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
John DeNero and Jakob Uszkoreit. 2011. Induc-
ing sentence structure from parallel corpora for
reordering. In Proc. EMNLP.
Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Proc. HLT-
NAACL.
Daniel Flannery, Yusuke Miyao, Graham Neubig,
and Shinsuke Mori. 2011. Training dependency
parsers from partially annotated corpora. In Proc.
IJCNLP, pages 776?784, Chiang Mai, Thailand,
November.
Dmitriy Genzel. 2010. Automatically learning
source-side reordering rules for large scale machine
translation. In Proc. COLING.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proc. ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Kat-
suhito Sudoh, and Hajime Tsukada. 2010a. Auto-
matic evaluation of translation quality for distant
language pairs. In Proc. EMNLP, pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada,
and Kevin Duh. 2010b. Head finalization: A
simple reordering rule for sov languages. In Proc.
WMT and MetricsMATR.
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proc. EMNLP, pages 183?192.
Maurice G. Kendall. 1938. A new measure of rank
correlation. Biometrika, 30(1/2):81?93.
Maxim Khalilov and Khalil Sima?an. 2011. Context-
sensitive syntactic source-reordering by statistical
transduction. In Proc. IJCNLP.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. ACL, pages
423?430.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proc. HLT, pages 48?54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation eval-
uation. In Proc. IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. ACL, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proc.
EMNLP.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proc. ACL.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimi-
native approach to machine translation. In Proc.
ACL, pages 761?768.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proc. ACL,
pages 529?533, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proc. EACL.
852
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
COLING, pages 311?318.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
pos-based distortion model. In Proc. of TMI-2007.
Shai Shalev-Shwartz, Yoram Singer, and Nathan
Srebro. 2007. Pegasos: Primal estimated sub-
gradient solver for SVM. In Proc. ICML, pages
807?814.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada,
Masaaki Nagata, Xianchao Wu, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2011a. NTT-
UT statistical machine translation in NTCIR-9
PatentMT. In Proc. NTCIR.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Ha-
jime Tsukada, and Masaaki Nagata. 2011b. Post-
ordering in statistical machine translation. In
Proc. MT Summit.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-
son Katz-Brown, Masakazu Seno, and Franz Och.
2011. A lightweight evaluation framework for ma-
chine translation reordering. In Proc. WMT.
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin Markov networks. Proc. NIPS,
16.
Roy Tromble and Jason Eisner. 2009. Learning lin-
ear ordering problems for better translation. In
Proc. EMNLP.
Karthik Visweswariah, Rajakrishnan Rajkumar,
Ankur Gandhe, Ananthakrishnan Ramanathan,
and Jiri Navratil. 2011. A word reordering
model for improved machine translation. In Proc.
EMNLP.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proc.
EMNLP, pages 764?773.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3).
Fei Xia and Michael McCord. 2004. Improving a
statistical MT system with automatically learned
rewrite patterns. In Proc. COLING.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proc.
NAACL.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables.
In Proc. ICML, pages 1169?1176.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statis-
tical machine translation. In Proc. SSST.
Andreas Zollmann and Ashish Venugopal. 2006.
Syntax augmented machine translation via chart
parsing. In Proc. WMT, pages 138?141.
853
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 204?209,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Noise-aware Character Alignment for Bootstrapping Statistical Machine
Transliteration from Bilingual Corpora
Katsuhito Sudoh?? Shinsuke Mori? Masaaki Nagata?
?NTT Communication Science Laboratories
?Graduate School of Informatics, Kyoto University
?Academic Center for Computing and Media Studies, Kyoto University
sudoh.katsuhito@lab.ntt.co.jp
Abstract
This paper proposes a novel noise-aware char-
acter alignment method for bootstrapping sta-
tistical machine transliteration from automat-
ically extracted phrase pairs. The model is
an extension of a Bayesian many-to-many
alignment method for distinguishing non-
transliteration (noise) parts in phrase pairs. It
worked effectively in the experiments of boot-
strapping Japanese-to-English statistical ma-
chine transliteration in patent domain using
patent bilingual corpora.
1 Introduction
Transliteration is used for providing translations for
source language words that have no appropriate
counterparts in target language, such as some tech-
nical terms and named entities. Statistical machine
transliteration (Knight and Graehl, 1998) is a tech-
nology to solve it in a statistical manner. Bilin-
gual dictionaries can be used to train its model, but
many of their entries are actually translation but not
transliteration. Such non-transliteration pairs hurt
the transliteration model and should be eliminated
beforehand.
Sajjad et al (2012) proposed a method to iden-
tify such non-transliteration pairs, and applied it
successfully to noisy word pairs obtained from au-
tomatic word alignment on bilingual corpora. It
enables the statistical machine transliteration to be
bootstrapped from bilingual corpora. This approach
is beneficial because it does not require carefully-
developed bilingual transliteration dictionaries and
it can learn domain-specific transliteration patterns
from bilingual corpora in the target domain. How-
ever, their transliteration mining approach is sample-
wise; that is, it makes a decision whether a bilingual
phrase pair is transliteration or not. Suppose that
a compound word in a language A is transliterated
into two words in another language B. Their corre-
spondence may not be fully identified by automatic
word alignment and a wrong alignment between the
compound word in A and only one component word
in B is found. The sample-wise mining cannot make
a correct decision of partial transliteration on the
aligned candidate, and may introduces noise to the
statistical transliteration model.
This paper proposes a novel transliteration mining
method for such partial transliterations. The method
uses a noise-aware character alignment model that
distinguish non-transliteration (noise) parts from
transliteration (signal) parts. The model is an ex-
tension of a Bayesian alignment model (Finch and
Sumita, 2010) and can be trained by a sampling al-
gorithm extended for a constraint on noise. Our
experiments of Japanese-to-English transliteration
achieved 16% relative error reduction in transliter-
ation accuracy from the sample-wise method. The
main contribution of this paper is two-fold:
? we formulate alignment over string pairs with
partial noise and present a solution with a
noise-aware alignment model;
? we proved its effectiveness by experiments
with frequent unknown words in actual
Japanese-to-English patent translation data.
204
2 Bayesian many-to-many alignment
We briefly review a Bayesian many-to-many charac-
ter alignment proposed by Finch and Sumita (2010)
on which our model is based. The model is based
on a generative process of bilingual substring pairs
?s?, t?? by the following Dirichlet process (DP):
G|?,G0 ? DP(?,G0)
?s?, t??|G ? G,
where G is a probability distribution over substring
pairs according to a DP prior with base measure G0
and hyperparameter ?. G0 is modeled as a joint
spelling model as follows:
G0 (?s?, t??) =
?|s?|s
|s?|! e
??sv?|s?|s ?
?|t?|t
|?t|! e
??tv?|t?|t . (1)
This is a simple joint probability of the spelling
models, in which each alphabet appears based on
a uniform distribution over the vocabulary (of size
vs and vt) and each string length follows a Poisson
distribution (with the average length ?s and ?t).
The model handles infinite number of substring
pairs according to the Chinese Restaurant Process
(CRP). The probability of a substring pair ?s?k, t?k?
is based on the counts of all other substring pairs as
follows:
p
(
?s?k, t?k?| {?s?, t??}?k
)
= N (?s?k, t?k?) + ?G0 (?s?k, t?k?)?
i N (?s?i, t?i?) + ?
. (2)
Here {?s?, t??}?k means a set of substring pairs ex-
cluding ?s?k, t?k?, and N (?s?k, t?k?) is the number of
?s?k, t?k? in the current sample space. This align-
ment model is suitable for representing very sparse
distribution over arbitrary substring pairs, thanks to
reasonable CRP-based smoothing for unseen pairs
based on the spelling model.
3 Proposed method
We propose an extended many-to-many alignment
model that can handle partial noise. We extend the
model in the previous section by introducing a noise
symbol and state-based probability calculation.
? ?
k e y 
(a) no noise
? ?
f l y 
noise
noise
(b) noise
? ?
g i v 
? ? ?
noisee 
(c) partial noise: English
side should be ?give up?
? ?
k e y 
? ?
g i v 
? ? ?
noisee 
r e c 
? ? ?noise
o v e r 
(d) partial noise: Japanese side
should be ??????
Figure 1: Three types of noise in transliteration data.
Solid lines are correct many-to-many alignment links.
3.1 Partial noise in transliteration data
Figure 1 shows transliteration examples with ?no
noise,? ?noise,? and ?partial noise.? Solid lines in the
figure show correct many-to-many alignment links.
The examples (a) and (b) can be distinguished ef-
fectively by Sajjad et al (2012). We aim to do align-
ment as in the examples (c) and (d) by distinguishing
its non-transliteration (noise) part, which cannot be
handled by the existing methods.
3.2 Noise-aware alignment model
We introduce a noise symbol to handle partial noise
in the many-to-many alignment model. Htun et al
(2012) extended the many-to-many alignment for
the sample-wise transliteration mining, but its noise
model only handles the sample-wise noise and can-
not distinguish partial noise. We model partial noise
in the CRP-based joint substring model.
Partial noise in transliteration data typically ap-
pears in compound words as mentioned earlier, be-
cause their counterparts consisting of two or more
words may not be fully covered in automatically ex-
tracted words and phrases as shown in Figure 1(c).
Another type of partial noise is derived from mor-
phological differences due to inflection, which usu-
ally appear in the sub-word level as prefixes and suf-
fixes as shown in Figure 1(d). According to this
intuition, we assume that partial noise appears in
the beginning and/or end of transliteration data (in
case of sample-wise noise, we assume the noise is in
the beginning). This assumption derives a constraint
between signal and noise parts that helps to avoid
a welter of transliteration and non-transliteration
parts. It also has a shortcoming that it is generally
205
? ?
t h e 
? ? ?
sp 
? ? ?sp 
e t c h i n g sp m a s k s 
noise noise noise
Figure 2: Example of many-to-many alignment with par-
tial noise in the beginning and end. ?noise? stands for the
noise symbol and ?sp? stands for a white space.
not appropriate for noise in the middle, but handling
arbitrary number of noise parts increases computa-
tional complexity and sparseness. We rely on this
simple assumption in this paper and consider a more
complex mid-noise problem as future work.
Figure 2 shows a partial noise example in both
the beginning and end. This example is actually
correct translation but includes noise in a sense of
transliteration; an article ?the? is wrongly included
in the phrase pair (no articles are used in Japanese)
and a plural noun ?masks? is transliterated into
?????(mask). These non-transliteration parts are
aligned to noise symbols in the proposed model. The
noise symbols are treated as zero-length substrings
in the model, same as other substrings.
3.3 Constrained Gibbs sampling
Finch and Sumita (2010) used a blocked Gibbs sam-
pling algorithm with forward-filtering backward-
sampling (FFBS) (Mochihashi et al, 2009). We ex-
tend their algorithm for our noise-aware model us-
ing a state-based calculation over the three states:
non-transliteration part in the beginning (noiseB),
transliteration part (signal), non-transliteration part
in the end (noiseE).
Figure 3 illustrates our FFBS steps. At first in
the forward filtering, we begin with transition to
noiseB and signal. The calculation of forward
probabilities itself is almost the same as Finch and
Sumita (2010) except for state transition constraints:
from noiseB to signal, from signal to noiseE. The
backward-sampling traverses a path by probability-
based sampling with true posteriors, starting from
the choice of the ending state among noiseB (means
full noise), signal, and noiseE. This algorithm in-
creases the computational cost by three times to con-
sider three different states, compared to that of Finch
and Sumita (2010).
noiseB
signal
noiseE
noiseB
signal
noiseE
s
s
s
s
s
s
t
t
t
t
t
t
(a) Forward filtering
noiseB
signal
noiseE
noiseB
signal
noiseE
s
s
s
s
s
s
t
t
t
t
t
t
(b) Backward sampling
Figure 3: State-based FFBS for the proposed model.
4 Experiments
We conducted experiments comparing the pro-
posed method with the conventional sample-wise
method for the use in bootstrapping statistical
machine transliteration using Japanese-to-English
patent translation dataset (Goto et al, 2013).
4.1 Training data setup
First, we trained a phrase table on the 3.2M paral-
lel sentences by a standard training procedure using
Moses, with Japanese tokenization using MeCab1.
We obtained 591,840 phrase table entries whose
Japanese side was written in katakana (Japanese
phonogram) only2. Then, we iteratively ran the
method of Sajjad et al (2012) on these entries and
eliminate non-transliteration pairs, until the num-
ber of pairs converged. Finally we obtain 104,563
katakana-English pairs after 10 iterations; they were
our baseline training set mined by sample-wise
method. We used Sajjad et al?s method as pre-
processing for filtering sample-wise noise while the
proposed method could also do that, because the
proposed method took much more training time for
all phrase table entries.
4.2 Transliteration experiments
The transliteration experiment used a translation-
based implementation with Moses, using a
1http://code.google.com/p/mecab/
2This katakana-based filtering is a language dependent
heuristic for choosing potential transliteration candidate, be-
cause transliterations in Japanese are usually written in
katakana.
206
character-based 7-gram language model trained on
300M English patent sentences. We compared three
transliteration models below.
The test set was top-1000 unknown (in the
Japanese-to-English translation model) katakana
words appeared in 400M Japanese patent sentences.
They covered 15.5% of all unknown katakanawords
and 8.8% of all unknown words (excluding num-
bers); that is, more than a half of unknown words
were katakana words.
4.2.1 Sample-wise method (BASELINE)
We used the baseline training set to train sta-
tistical machine transliteration model for our base-
line. The training procedure was based on Moses:
MGIZA++ word alignment, grow-diag-final-and
alignment symmetrization and phrase extraction
with the maximum phrase length of 7.
4.2.2 Proposed method (PROPOSED)
We applied the proposed method to the baseline
training set with 30 sampling iterations and elimi-
nated partial noise. The transliteration model was
trained in the same manner as BASELINE after elim-
inating noise.
The hyperparameters, ?, ?s, and ?t, were op-
timized using a held-out set of 2,000 katakana-
English pairs that were randomly chosen from a
general-domain bilingual dictionary. The hyperpa-
rameter optimization was based on F-score values
on the held-out set with varying ? among 0.01, 0.02,
0.05, 0.1, 1.0, and ?s among 1, 2, 3, 5.
Table 1 compares the statistics on the training sets
of BASELINE and PROPOSED. Note that we ap-
plied the proposed method to BASELINE data (the
sample-wise method was already applied until con-
vergence). The proposed method eliminated only
two transliteration candidates in sample-wise but
also eliminated 5,714 (0.64%) katakana and 55,737
(4.1%) English characters3.
4.2.3 Proposed method using aligned joint
substrings as phrases (PROPOSED-JOINT)
The many-to-many character alignment actually
induces substring pairs, which can be used as
3The reason of larger number of partial noise in English side
would be a syntactic difference as shown in Figure 2 and the
katakana-based filtering heuristics.
Table 1: Statistics of the training sets.
Method #pairs #Ja chars. #En chars.
BASELINE 104,563 899,080 1,372,993
PROPOSED 104,561 893,366 1,317,256
phrases in statistical machine transliteration and
improved transliteration performance (Finch and
Sumita, 2010). We extracted them by: 1) generate
many-to-many word alignment, in which all possi-
ble word alignment links in many-to-many corre-
spondences (e.g., 0-0 0-1 0-2 1-0 1-1 1-2 for ?? ?,
c o m?), 2) run phrase extraction and scoring same as
a standard Moses training. This procedure extracts
longer phrases satisfying the many-to-many align-
ment constraints than the simple use of extracted
joint substring pairs as phrases.
4.3 Results
Table 2 shows the results. We used three evalua-
tion metrics: ACC, F-score, and BLEUc. ACC is
a sample-wise accuracy and F-score is a character-
wise F-measure-like score (Li et al, 2010). BLEUc
is BLEU (Papineni et al, 2002) in the character level
with n=4.
PROPOSED achieved 63% in ACC (16% rela-
tive error reduction from BASELINE), and 94.6% in
F-score (25% relative error reduction from BASE-
LINE). These improvements clearly showed an ad-
vantage of the proposed method over the sample-
wise mining. BLEUc showed a similar improve-
ments. Recall that BASELINE and PROPOSED had
a small difference in their training data, actually
0.64% (katakana) and 4.1% (English) in the num-
ber of characters. The results suggest that the partial
noise can hurt transliteration models.
PROPOSED-JOINT showed similar performance
as PROPOSED with a slight drop in BLEUc, al-
though many-to-many substring alignment was ex-
pected to improve transliteration as reported by
Finch and Sumita (2010). The difference may be
due to the difference in coverage of the phrase
tables; PROPOSED-JOINT retained relatively long
substrings by the many-to-many alignment con-
straints in contrast to the less-constrained grow-
diag-final-and alignments in PROPOSED. Since the
training data in our bootstrapping experiments con-
207
Table 2: Japanese-to-English transliteration results for
top-1000 unknown katakana words. ACC and F-score
stand for the ones used in NEWS workshop, BLEUc is
character-wise BLEU.
Method ACC F-score BLEUc
BASELINE 0.56 0.929 0.864
PROPOSED 0.63 0.946 0.897
PROPOSED-JOINT 0.63 0.943 0.888
tained many similar phrases unlike dictionary-based
data in Finch and Sumita (2010), the phrase table of
PROPOSED-JOINT may have a small coverage due
to long and sparse substring pairs with large prob-
abilities even if the many-to-many alignment was
good. This sparseness problem is beyond the scope
of this paper and worth further study.
4.4 Alignment Examples
Figure 4 shows examples of the alignment results in
the training data. As expected, partial noise both in
Japanese and English was identified correctly in (a),
(b), and (c). There were some alignment errors in the
signal part in (b), in which characters in boundary
positions were aligned incorrectly to adjacent sub-
strings. These alignment errors did not directly de-
grade the partial noise identification but may cause
a negative effect on overall alignment performance
in the sampling-based optimization. (d) is a nega-
tive example in which partial noise was incorrectly
aligned. (c) and (d) have similar partial noise in their
English word endings, but it could not be identified
in (d). One possible reason for that is the sparse-
ness problem mentioned above, as shown in erro-
neous long character alignments in (d).
5 Conclusion
This paper proposed a noise-aware many-to-many
alignment model that can distinguish partial noise in
transliteration pairs for bootstrapping statistical ma-
chine transliteration model from automatically ex-
tracted phrase pairs. The model and training al-
gorithm are straightforward extension of those by
Finch and Sumita (2010). The proposed method
was proved to be effective in Japanese-to-English
transliteration experiments in patent domain.
Future work will investigate the proposed method
? ?
a n sp 
? sp ?
a r c sp t a n g e n t 
? ? ? ? ?noise noise
(a) Correctly aligned
? ? 
d o p 
? ? ? 
i n g sp e n e r g y 
? ? ? ? ? ? ? ? ? 
noise noise
(b) Some alignment errors in transliteration part
? ? 
f o r 
? ? 
m e d 
noise
(c) Correctly aligned
? ? 
c u s 
? ? ? 
t o m i z e d 
? noise
(d) Errors in partial noise
Figure 4: Examples of noise-aware many-to-many align-
ment in the training data. ? stands for a zero-length sub-
string. Dashed lines show incorrect alignments, and bold
grey lines mean their corrections.
in other domains and language pairs. The partial
noise would appear in other language pairs, typ-
ically between agglutinative and non-agglutinative
languages. It is also worth extending the approach
into word alignment in statistical machine transla-
tion.
Acknowledgments
We would like to thank anonymous reviewers for
their valuable comments and suggestions.
References
Andrew Finch and Eiichiro Sumita. 2010. A Bayesian
Model of Bilingual Segmentation for Transliteration.
In Proceedings of the seventh International Workshop
on Spoken Language Translation (IWSLT), pages 259?
266.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K. Tsou. 2013. Overview of the Patent Ma-
chine Translation Task at the NTCIR-10 Workshop. In
The 10th NTCIR Conference, June.
Ohnmar Htun, Andrew Finch, Eiichiro Sumita, and
Yoshiki Mikami. 2012. Improving Transliteration
Mining by Integrating Expert Knowledge with Statis-
tical Approaches. International Journal of Computer
Applications, 58(17):12?22, November.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
208
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-
vouchine. 2010. Whitepaper of NEWS 2010 Shared
Task on Transliteration Generation. In Proceedings
of the 2010 Named Entities Workshop, pages 12?20,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian Unsupervised Word Segmentation
with Nested Pitman-Yor Language Modeling. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 100?108, Suntec, Singapore, August.
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and semi-
supervised transliteration mining. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
469?477, Jeju Island, Korea, July. Association for
Computational Linguistics.
209
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 632?641,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Unsupervised Model for Joint Phrase Alignment and Extraction
Graham Neubig1,2 Taro Watanabe2, Eiichiro Sumita2, Shinsuke Mori1, Tatsuya Kawahara1
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
We present an unsupervised model for joint
phrase alignment and extraction using non-
parametric Bayesian methods and inversion
transduction grammars (ITGs). The key con-
tribution is that phrases of many granulari-
ties are included directly in the model through
the use of a novel formulation that memorizes
phrases generated not only by terminal, but
also non-terminal symbols. This allows for
a completely probabilistic model that is able
to create a phrase table that achieves com-
petitive accuracy on phrase-based machine
translation tasks directly from unaligned sen-
tence pairs. Experiments on several language
pairs demonstrate that the proposed model
matches the accuracy of traditional two-step
word alignment/phrase extraction approach
while reducing the phrase table to a fraction
of the original size.
1 Introduction
The training of translation models for phrase-
based statistical machine translation (SMT) systems
(Koehn et al, 2003) takes unaligned bilingual train-
ing data as input, and outputs a scored table of
phrase pairs. This phrase table is traditionally gen-
erated by going through a pipeline of two steps, first
generating word (or minimal phrase) alignments,
then extracting a phrase table that is consistent with
these alignments.
However, as DeNero and Klein (2010) note, this
two step approach results in word alignments that
are not optimal for the final task of generating
phrase tables that are used in translation. As a so-
lution to this, they proposed a supervised discrimi-
native model that performs joint word alignment and
phrase extraction, and found that joint estimation of
word alignments and extraction sets improves both
word alignment accuracy and translation results.
In this paper, we propose the first unsuper-
vised approach to joint alignment and extraction of
phrases at multiple granularities. This is achieved
by constructing a generative model that includes
phrases at many levels of granularity, from minimal
phrases all the way up to full sentences. The model
is similar to previously proposed phrase alignment
models based on inversion transduction grammars
(ITGs) (Cherry and Lin, 2007; Zhang et al, 2008;
Blunsom et al, 2009), with one important change:
ITG symbols and phrase pairs are generated in
the opposite order. In traditional ITG models, the
branches of a biparse tree are generated from a non-
terminal distribution, and each leaf is generated by
a word or phrase pair distribution. As a result, only
minimal phrases are directly included in the model,
while larger phrases must be generated by heuris-
tic extraction methods. In the proposed model, at
each branch in the tree, we first attempt to gener-
ate a phrase pair from the phrase pair distribution,
falling back to ITG-based divide and conquer strat-
egy to generate phrase pairs that do not exist (or are
given low probability) in the phrase distribution.
We combine this model with the Bayesian non-
parametric Pitman-Yor process (Pitman and Yor,
1997; Teh, 2006), realizing ITG-based divide and
conquer through a novel formulation where the
Pitman-Yor process uses two copies of itself as a
632
base measure. As a result of this modeling strategy,
phrases of multiple granularities are generated, and
thus memorized, by the Pitman-Yor process. This
makes it possible to directly use probabilities of the
phrase model as a replacement for the phrase table
generated by heuristic extraction techniques.
Using this model, we perform machine transla-
tion experiments over four language pairs. We ob-
serve that the proposed joint phrase alignment and
extraction approach is able to meet or exceed results
attained by a combination of GIZA++ and heuristic
phrase extraction with significantly smaller phrase
table size. We also find that it achieves superior
BLEU scores over previously proposed ITG-based
phrase alignment approaches.
2 A Probabilistic Model for Phrase Table
Extraction
The problem of SMT can be defined as finding the
most probable target sentence e for the source sen-
tence f given a parallel training corpus ?E ,F?
e? = argmax
e
P (e|f , ?E ,F?).
We assume that there is a hidden set of parameters
? learned from the training data, and that e is condi-
tionally independent from the training corpus given
?. We take a Bayesian approach, integrating over all
possible values of the hidden parameters:
P (e|f , ?E ,F?) =
?
?
P (e|f , ?)P (?|?E ,F?). (1)
If ? takes the form of a scored phrase table, we
can use traditional methods for phrase-based SMT to
find P (e|f , ?) and concentrate on creating a model
for P (?|?E ,F?). We decompose this posterior prob-
ability using Bayes law into the corpus likelihood
and parameter prior probabilities
P (?|?E ,F?) ? P (?E ,F?|?)P (?).
In Section 3 we describe an existing method, and
in Section 4 we describe our proposed method for
modeling these two probabilities.
3 Flat ITG Model
There has been a significant amount of work in
many-to-many alignment techniques (Marcu and
Wong (2002), DeNero et al (2008), inter alia), and
in particular a number of recent works (Cherry and
Lin, 2007; Zhang et al, 2008; Blunsom et al, 2009)
have used the formalism of inversion transduction
grammars (ITGs) (Wu, 1997) to learn phrase align-
ments. By slightly limit reordering of words, ITGs
make it possible to exactly calculate probabilities
of phrasal alignments in polynomial time, which is
a computationally hard problem when arbitrary re-
ordering is allowed (DeNero and Klein, 2008).
The traditional flat ITG generative probabil-
ity for a particular phrase (or sentence) pair
Pflat(?e, f?; ?x, ?t) is parameterized by a phrase ta-
ble ?t and a symbol distribution ?x. We use the fol-
lowing generative story as a representative of the flat
ITG model.
1. Generate symbol x from the multinomial distri-
bution Px(x; ?x). x can take the values TERM,
REG, or INV.
2. According to the x take the following actions.
(a) If x = TERM, generate a phrase pair from
the phrase table Pt(?e, f?; ?t).
(b) If x = REG, a regular ITG rule, gener-
ate phrase pairs ?e1, f1? and ?e2, f2? from
Pflat, and concatenate them into a single
phrase pair ?e1e2, f1f2?.
(c) If x = INV, an inverted ITG rule, follows
the same process as (b), but concatenate
f1 and f2 in reverse order ?e1e2, f2f1?.
By taking the product of Pflat over every sentence
in the corpus, we are able to calculate the likelihood
P (?E ,F?|?) =
?
?e,f???E,F?
Pflat(?e, f?; ?).
We will refer to this model as FLAT.
3.1 Bayesian Modeling
While the previous formulation can be used as-is in
maximum likelihood training, this leads to a degen-
erate solution where every sentence is memorized as
a single phrase pair. Zhang et al (2008) and others
propose dealing with this problem by putting a prior
probability P (?x, ?t) on the parameters.
633
We assign ?x a Dirichlet prior1, and assign the
phrase table parameters ?t a prior using the Pitman-
Yor process (Pitman and Yor, 1997; Teh, 2006),
which is a generalization of the Dirichlet process
prior used in previous research. It is expressed as
?t ?PY (d, s, Pbase) (2)
where d is the discount parameter, s is the strength
parameter, and Pbase is the base measure. The dis-
count d is subtracted from observed counts, and
when it is given a large value (close to one), less
frequent phrase pairs will be given lower relative
probability than more common phrase pairs. The
strength s controls the overall sparseness of the dis-
tribution, and when it is given a small value the dis-
tribution will be sparse. Pbase is the prior probability
of generating a particular phrase pair, which we de-
scribe in more detail in the following section.
Non-parametric priors are well suited for mod-
eling the phrase distribution because every time a
phrase is generated by the model, it is ?memorized?
and given higher probability. Because of this, com-
mon phrase pairs are more likely to be re-used (the
rich-get-richer effect), which results in the induc-
tion of phrase tables with fewer, but more helpful
phrases. It is important to note that only phrases
generated by Pt are actually memorized and given
higher probability by the model. In FLAT, only min-
imal phrases generated after Px outputs the terminal
symbol TERM are generated from Pt, and thus only
minimal phrases are memorized by the model.
While the Dirichlet process is simply the Pitman-
Yor process with d = 0, it has been shown that the
discount parameter allows for more effective mod-
eling of the long-tailed distributions that are often
found in natural language (Teh, 2006). We con-
firmed in preliminary experiments (using the data
described in Section 7) that the Pitman-Yor process
with automatically adjusted parameters results in su-
perior alignment results, outperforming the sparse
Dirichlet process priors used in previous research2.
The average gain across all data sets was approxi-
mately 0.8 BLEU points.
1The value of ? had little effect on the results, so we arbi-
trarily set ? = 1.
2We put weak priors on s (Gamma(? = 2, ? = 1)) and
d (Beta(? = 2, ? = 2)) for the Pitman-Yor process, and set
? = 1?10 for the Dirichlet process.
3.2 Base Measure
Pbase in Equation (2) indicates the prior probability
of phrase pairs according to the model. By choosing
this probability appropriately, we can incorporate
prior knowledge of what phrases tend to be aligned
to each other. We calculate Pbase by first choosing
whether to generate an unaligned phrase pair (where
|e| = 0 or |f | = 0) according to a fixed probabil-
ity pu3, then generating from Pba for aligned phrase
pairs, or Pbu for unaligned phrase pairs.
For Pba, we adopt a base measure similar to that
used by DeNero et al (2008):
Pba(?e, f?) =M0(?e, f?)Ppois(|e|;?)Ppois(|f |;?)
M0(?e, f?) =(Pm1(f |e)Puni(e)Pm1(e|f)Puni(f))
1
2 .
Ppois is the Poisson distribution with the average
length parameter ?. As long phrases lead to spar-
sity, we set ? to a relatively small value to allow
us to bias against overly long phrases4. Pm1 is the
word-based Model 1 (Brown et al, 1993) probabil-
ity of one phrase given the other, which incorporates
word-based alignment information as prior knowl-
edge in the phrase translation probability. We take
the geometric mean5of the Model 1 probabilities in
both directions to encourage alignments that are sup-
ported by both models (Liang et al, 2006). It should
be noted that while Model 1 probabilities are used,
they are only soft constraints, compared with the
hard constraint of choosing a single word alignment
used in most previous phrase extraction approaches.
For Pbu, if g is the non-null phrase in e and f , we
calculate the probability as follows:
Pbu(?e, f?) = Puni(g)Ppois(|g|;?)/2.
Note that Pbu is divided by 2 as the probability is
considering null alignments in both directions.
4 Hierarchical ITG Model
While in FLAT only minimal phrases were memo-
rized by the model, as DeNero et al (2008) note
3We choose 10?2, 10?3, or 10?10 based on which value
gave the best accuracy on the development set.
4We tune ? to 1, 0.1, or 0.01 based on which value gives the
best performance on the development set.
5The probabilities of the geometric mean do not add to one,
but we found empirically that even when left unnormalized, this
provided much better results than the using the arithmetic mean,
which is more theoretically correct.
634
and we confirm in the experiments in Section 7, us-
ing only minimal phrases leads to inferior transla-
tion results for phrase-based SMT. Because of this,
previous research has combined FLAT with heuris-
tic phrase extraction, which exhaustively combines
all adjacent phrases permitted by the word align-
ments (Och et al, 1999). We propose an alterna-
tive, fully statistical approach that directly models
phrases at multiple granularities, which we will refer
to as HIER. By doing so, we are able to do away with
heuristic phrase extraction, creating a fully proba-
bilistic model for phrase probabilities that still yields
competitive results.
Similarly to FLAT, HIER assigns a probability
Phier(?e, f?; ?x, ?t) to phrase pairs, and is parame-
terized by a phrase table ?t and a symbol distribu-
tion ?x. The main difference from the generative
story of the traditional ITG model is that symbols
and phrase pairs are generated in the opposite order.
While FLAT first generates branches of the derivation
tree using Px, then generates leaves using the phrase
distribution Pt, HIER first attempts to generate the
full sentence as a single phrase from Pt, then falls
back to ITG-style derivations to cope with sparsity.
We allow for this within the Bayesian ITG context
by defining a new base measure Pdac (?divide-and-
conquer?) to replace Pbase in Equation (2), resulting
in the following distribution for ?t.
?t ? PY (d, s, Pdac) (3)
Pdac essentially breaks the generation of a sin-
gle longer phrase into two generations of shorter
phrases, allowing even phrase pairs for which
c(?e, f?) = 0 to be given some probability. The
generative process of Pdac, similar to that of Pflat
from the previous section, is as follows:
1. Generate symbol x from Px(x; ?x). x can take
the values BASE, REG, or INV.
2. According to x take the following actions.
(a) If x = BASE, generate a new phrase pair
directly from Pbase of Section 3.2.
(b) If x = REG, generate ?e1, f1? and ?e2, f2?
from Phier, and concatenate them into a
single phrase pair ?e1e2, f1f2?.
Figure 1: A word alignment (a), and its derivations ac-
cording to FLAT (b), and HIER (c). Solid and dotted lines
indicate minimal and non-minimal pairs respectively, and
phrases are written under their corresponding instance of
Pt. The pair hate/cou?te is generated from Pbase.
(c) If x = INV, follow the same process as
(b), but concatenate f1 and f2 in reverse
order ?e1e2, f2f1?.
A comparison of derivation trees for FLAT and
HIER is shown in Figure 1. As previously de-
scribed, FLAT first generates from the symbol dis-
tribution Px, then from the phrase distribution Pt,
while HIER generates directly from Pt, which falls
back to divide-and-conquer based on Px when nec-
essary. It can be seen that while Pt in FLAT only gen-
erates minimal phrases, Pt in HIER generates (and
thus memorizes) phrases at all levels of granularity.
4.1 Length-based Parameter Tuning
There are still two problems with HIER, one theo-
retical, and one practical. Theoretically, HIER con-
tains itself as its base measure, and stochastic pro-
cess models that include themselves as base mea-
sures are deficient, as noted in Cohen et al (2010).
Practically, while the Pitman-Yor process in HIER
shares the parameters s and d over all phrase pairs in
the model, long phrase pairs are much more sparse
635
Figure 2: Learned discount values by phrase pair length.
than short phrase pairs, and thus it is desirable to
appropriately adjust the parameters of Equation (2)
according to phrase pair length.
In order to solve these problems, we reformulate
the model so that each phrase length l = |f |+|e| has
its own phrase parameters ?t,l and symbol parame-
ters ?x,l, which are given separate priors:
?t,l ? PY (s, d, Pdac,l)
?x,l ? Dirichlet(?)
We will call this model HLEN.
The generative story is largely similar to HIER
with a few minor changes. When we generate a sen-
tence, we first choose its length l according to a uni-
form distribution over all possible sentence lengths
l ? Uniform(1, L),
where L is the size |e| + |f | of the longest sentence
in the corpus. We then generate a phrase pair from
the probability Pt,l(?e, f?) for length l. The base
measure for HLEN is identical to that of HIER, with
one minor change: when we fall back to two shorter
phrases, we choose the length of the left phrase from
ll ? Uniform(1, l ? 1), set the length of the right
phrase to lr = l?ll, and generate the smaller phrases
from Pt,ll and Pt,lr respectively.
It can be seen that phrases at each length are gen-
erated from different distributions, and thus the pa-
rameters for the Pitman-Yor process will be differ-
ent for each distribution. Further, as ll and lr must
be smaller than l, Pt,l no longer contains itself as a
base measure, and is thus not deficient.
An example of the actual discount values learned
in one of the experiments described in Section 7
is shown in Figure 2. It can be seen that, as ex-
pected, the discounts for short phrases are lower than
those of long phrases. In particular, phrase pairs of
length up to six (for example, |e| = 3, |f | = 3) are
given discounts of nearly zero while larger phrases
are more heavily discounted. We conjecture that this
is related to the observation by Koehn et al (2003)
that using phrases where max(|e|, |f |) ? 3 cause
significant improvements in BLEU score, while us-
ing larger phrases results in diminishing returns.
4.2 Implementation
Previous research has used a variety of sampling
methods to learn Bayesian phrase based alignment
models (DeNero et al, 2008; Blunsom et al, 2009;
Blunsom and Cohn, 2010). All of these techniques
are applicable to the proposed model, but we choose
to apply the sentence-based blocked sampling of
Blunsom and Cohn (2010), which has desirable con-
vergence properties compared to sampling single
alignments. As exhaustive sampling is too slow for
practical purpose, we adopt the beam search algo-
rithm of Saers et al (2009), and use a probability
beam, trimming spans where the probability is at
least 1010 times smaller than that of the best hypoth-
esis in the bucket.
One important implementation detail that is dif-
ferent from previous models is the management of
phrase counts. As a phrase pair ta may have been
generated from two smaller component phrases tb
and tc, when a sample containing ta is removed from
the distribution, it may also be necessary to decre-
ment the counts of tb and tc as well. The Chinese
Restaurant Process representation of Pt (Teh, 2006)
lends itself to a natural and easily implementable so-
lution to this problem. For each table representing a
phrase pair ta, we maintain not only the number of
customers sitting at the table, but also the identities
of phrases tb and tc that were originally used when
generating the table. When the count of the table
ta is reduced to zero and the table is removed, the
counts of tb and tc are also decremented.
5 Phrase Extraction
In this section, we describe both traditional heuris-
tic phrase extraction, and the proposed model-based
extraction method.
636
Figure 3: The phrase, block, and word alignments used
in heuristic phrase extraction.
5.1 Heuristic Phrase Extraction
The traditional method for heuristic phrase extrac-
tion from word alignments exhaustively enumerates
all phrases up to a certain length consistent with the
alignment (Och et al, 1999). Five features are used
in the phrase table: the conditional phrase proba-
bilities in both directions estimated using maximum
likelihood Pml(f |e) and Pml(e|f), lexical weight-
ing probabilities (Koehn et al, 2003), and a fixed
penalty for each phrase. We will call this heuristic
extraction from word alignments HEUR-W. These
word alignments can be acquired through the stan-
dard GIZA++ training regimen.
We use the combination of our ITG-based align-
ment with traditional heuristic phrase extraction as
a second baseline. An example of these alignments
is shown in Figure 3. In model HEUR-P, minimal
phrases generated from Pt are treated as aligned, and
we perform phrase extraction on these alignments.
However, as the proposed models tend to align rel-
atively large phrases, we also use two other tech-
niques to create smaller alignment chunks that pre-
vent sparsity. We perform regular sampling of the
trees, but if we reach a minimal phrase generated
from Pt, we continue traveling down the tree un-
til we reach either a one-to-many alignment, which
we will call HEUR-B as it creates alignments simi-
lar to the block ITG, or an at-most-one alignment,
which we will call HEUR-W as it generates word
alignments. It should be noted that forcing align-
ments smaller than the model suggests is only used
for generating alignments for use in heuristic extrac-
tion, and does not affect the training process.
5.2 Model-Based Phrase Extraction
We also propose a method for phrase table ex-
traction that directly utilizes the phrase probabil-
ities Pt(?e, f?). Similarly to the heuristic phrase
tables, we use conditional probabilities Pt(f |e)
and Pt(e|f), lexical weighting probabilities, and a
phrase penalty. Here, instead of using maximum
likelihood, we calculate conditional probabilities di-
rectly from Pt probabilities:
Pt(f |e) = Pt(?e, f?)/
?
{f? :c(?e,f??)?1}
Pt(?e, f??)
Pt(e|f) = Pt(?e, f?)/
?
{e?:c(?e?,f?)?1}
Pt(?e?, f?).
To limit phrase table size, we include only phrase
pairs that are aligned at least once in the sample.
We also include two more features: the phrase
pair joint probability Pt(?e, f?), and the average
posterior probability of each span that generated
?e, f? as computed by the inside-outside algorithm
during training. We use the span probability as it
gives a hint about the reliability of the phrase pair. It
will be high for common phrase pairs that are gen-
erated directly from the model, and also for phrases
that, while not directly included in the model, are
composed of two high probability child phrases.
It should be noted that while for FLAT and HIER Pt
can be used directly, as HLEN learns separate models
for each length, we must combine these probabilities
into a single value. We do this by setting
Pt(?e, f?) = Pt,l(?e, f?)c(l)/
L
?
l?=1
c(l?)
for every phrase pair, where l = |e|+ |f | and c(l) is
the number of phrases of length l in the sample.
We call this model-based extraction method MOD.
5.3 Sample Combination
As has been noted in previous works, (Koehn et al,
2003; DeNero et al, 2006) exhaustive phrase extrac-
tion tends to out-perform approaches that use syn-
tax or generative models to limit phrase boundaries.
DeNero et al (2006) state that this is because gen-
erative models choose only a single phrase segmen-
tation, and thus throw away many good phrase pairs
that are in conflict with this segmentation.
Luckily, in the Bayesian framework it is simple to
overcome this problem by combining phrase tables
637
from multiple samples. This is equivalent to approx-
imating the integral over various parameter configu-
rations in Equation (1). In MOD, we do this by taking
the average of the joint probability and span prob-
ability features, and re-calculating the conditional
probabilities from the averaged joint probabilities.
6 Related Work
In addition to the previously mentioned phrase
alignment techniques, there has also been a signif-
icant body of work on phrase extraction (Moore and
Quirk (2007), Johnson et al (2007a), inter alia).
DeNero and Klein (2010) presented the first work
on joint phrase alignment and extraction at multiple
levels. While they take a supervised approach based
on discriminative methods, we present a fully unsu-
pervised generative model.
A generative probabilistic model where longer
units are built through the binary combination of
shorter units was proposed by deMarcken (1996) for
monolingual word segmentation using the minimum
description length (MDL) framework. Our work dif-
fers in that it uses Bayesian techniques instead of
MDL, and works on two languages, not one.
Adaptor grammars, models in which non-
terminals memorize subtrees that lie below them,
have been used for word segmentation or other
monolingual tasks (Johnson et al, 2007b). The pro-
posed method could be thought of as synchronous
adaptor grammars over two languages. However,
adaptor grammars have generally been used to spec-
ify only two or a few levels as in the FLAT model in
this paper, as opposed to recursive models such as
HIER or many-leveled models such as HLEN. One
exception is the variational inference method for
adaptor grammars presented by Cohen et al (2010)
that is applicable to recursive grammars such as
HIER. We plan to examine variational inference for
the proposed models in future work.
7 Experimental Evaluation
We evaluate the proposed method on translation
tasks from four languages, French, German, Span-
ish, and Japanese, into English.
de-en es-en fr-en ja-en
TM (en) 1.80M 1.62M 1.35M 2.38M
TM (other) 1.85M 1.82M 1.56M 2.78M
LM (en) 52.7M 52.7M 52.7M 44.7M
Tune (en ) 49.8k 49.8k 49.8k 68.9k
Tune (other) 47.2k 52.6k 55.4k 80.4k
Test (en) 65.6k 65.6k 65.6k 40.4k
Test (other) 62.7k 68.1k 72.6k 48.7k
Table 1: The number of words in each corpus for TM and
LM training, tuning, and testing.
7.1 Experimental Setup
The data for French, German, and Spanish are from
the 2010 Workshop on Statistical Machine Transla-
tion (Callison-Burch et al, 2010). We use the news
commentary corpus for training the TM, and the
news commentary and Europarl corpora for training
the LM. For Japanese, we use data from the NTCIR
patent translation task (Fujii et al, 2008). We use
the first 100k sentences of the parallel corpus for the
TM, and the whole parallel corpus for the LM. De-
tails of both corpora can be found in Table 1. Cor-
pora are tokenized, lower-cased, and sentences of
over 40 words on either side are removed for TM
training. For both tasks, we perform weight tuning
and testing on specified development and test sets.
We compare the accuracy of our proposed method
of joint phrase alignment and extraction using the
FLAT, HIER and HLEN models, with a baseline of
using word alignments from GIZA++ and heuris-
tic phrase extraction. Decoding is performed using
Moses (Koehn and others, 2007) using the phrase
tables learned by each method under consideration,
as well as standard bidirectional lexical reordering
probabilities (Koehn et al, 2005). Maximum phrase
length is limited to 7 in all models, and for the LM
we use an interpolated Kneser-Ney 5-gram model.
For GIZA++, we use the standard training reg-
imen up to Model 4, and combine alignments
with grow-diag-final-and. For the proposed
models, we train for 100 iterations, and use the final
sample acquired at the end of the training process for
our experiments using a single sample6. In addition,
6For most models, while likelihood continued to increase
gradually for all 100 iterations, BLEU score gains plateaued af-
ter 5-10 iterations, likely due to the strong prior information
638
de-en es-en fr-en ja-en
Align Extract # Samp. BLEU Size BLEU Size BLEU Size BLEU Size
GIZA++ HEUR-W 1 16.62 4.91M 22.00 4.30M 21.35 4.01M 23.20 4.22M
FLAT MOD 1 13.48 136k 19.15 125k 17.97 117k 16.10 89.7k
HIER MOD 1 16.58 1.02M 21.79 859k 21.50 751k 23.23 723k
HLEN MOD 1 16.49 1.17M 21.57 930k 21.31 860k 23.19 820k
HIER MOD 10 16.53 3.44M 21.84 2.56M 21.57 2.63M 23.12 2.21M
HLEN MOD 10 16.51 3.74M 21.69 3.00M 21.53 3.09M 23.20 2.70M
Table 2: BLEU score and phrase table size by alignment method, extraction method, and samples combined. Bold
numbers are not significantly different from the best result according to the sign test (p < 0.05) (Collins et al, 2005).
we also try averaging the phrase tables from the last
ten samples as described in Section 5.3.
7.2 Experimental Results
The results for these experiments can be found in Ta-
ble 2. From these results we can see that when using
a single sample, the combination of using HIER and
model probabilities achieves results approximately
equal to GIZA++ and heuristic phrase extraction.
This is the first reported result in which an unsu-
pervised phrase alignment model has built a phrase
table directly from model probabilities and achieved
results that compare to heuristic phrase extraction. It
can also be seen that the phrase table created by the
proposed method is approximately 5 times smaller
than that obtained by the traditional pipeline.
In addition, HIER significantly outperforms FLAT
when using the model probabilities. This confirms
that phrase tables containing only minimal phrases
are not able to achieve results that compete with
phrase tables that use multiple granularities.
Somewhat surprisingly, HLEN consistently
slightly underperforms HIER. This indicates
potential gains to be provided by length-based
parameter tuning were outweighed by losses due
to the increased complexity of the model. In
particular, we believe the necessity to combine
probabilities from multiple Pt,l models into a single
phrase table may have resulted in a distortion of the
phrase probabilities. In addition, the assumption
that phrase lengths are generated from a uniform
distribution is likely too strong, and further gains
provided by Pbase. As iterations took 1.3 hours on a single
processor, good translation results can be achieved in approxi-
mately 13 hours, which could further reduced using distributed
sampling (Newman et al, 2009; Blunsom et al, 2009).
FLAT HIER
MOD 17.97 117k 21.50 751k
HEUR-W 21.52 5.65M 21.68 5.39M
HEUR-B 21.45 4.93M 21.41 2.61M
HEUR-P 21.56 4.88M 21.47 1.62M
Table 3: Translation results and phrase table size for var-
ious phrase extraction techniques (French-English).
could likely be achieved by more accurate modeling
of phrase lengths. We leave further adjustments to
the HLEN model to future work.
It can also be seen that combining phrase tables
from multiple samples improved the BLEU score
for HLEN, but not for HIER. This suggests that for
HIER, most of the useful phrase pairs discovered by
the model are included in every iteration, and the in-
creased recall obtained by combining multiple sam-
ples does not consistently outweigh the increased
confusion caused by the larger phrase table.
We also evaluated the effectiveness of model-
based phrase extraction compared to heuristic phrase
extraction. Using the alignments from HIER, we cre-
ated phrase tables using model probabilities (MOD),
and heuristic extraction on words (HEUR-W), blocks
(HEUR-B), and minimal phrases (HEUR-P) as de-
scribed in Section 5. The results of these ex-
periments are shown in Table 3. It can be seen
that model-based phrase extraction using HIER out-
performs or insignificantly underperforms heuris-
tic phrase extraction over all experimental settings,
while keeping the phrase table to a fraction of the
size of most heuristic extraction methods.
Finally, we varied the size of the parallel corpus
for the Japanese-English task from 50k to 400k sen-
639
Figure 4: The effect of corpus size on the accuracy (a) and
phrase table size (b) for each method (Japanese-English).
tences and measured the effect of corpus size on
translation accuracy. From the results in Figure 4
(a), it can be seen that at all corpus sizes, the re-
sults from all three methods are comparable, with
insignificant differences between GIZA++ and HIER
at all levels, and HLEN lagging slightly behind HIER.
Figure 4 (b) shows the size of the phrase table in-
duced by each method over the various corpus sizes.
It can be seen that the tables created by GIZA++ are
significantly larger at all corpus sizes, with the dif-
ference being particularly pronounced at larger cor-
pus sizes.
8 Conclusion
In this paper, we presented a novel approach to joint
phrase alignment and extraction through a hierar-
chical model using non-parametric Bayesian meth-
ods and inversion transduction grammars. Machine
translation systems using phrase tables learned di-
rectly by the proposed model were able to achieve
accuracy competitive with the traditional pipeline of
word alignment and heuristic phrase extraction, the
first such result for an unsupervised model.
For future work, we plan to refine HLEN to use
a more appropriate model of phrase length than
the uniform distribution, particularly by attempting
to bias against phrase pairs where one of the two
phrases is much longer than the other. In addition,
we will test probabilities learned using the proposed
model with an ITG-based decoder. We will also ex-
amine the applicability of the proposed model in the
context of hierarchical phrases (Chiang, 2007), or
in alignment using syntactic structure (Galley et al,
2006). It is also worth examining the plausibility
of variational inference as proposed by Cohen et al
(2010) in the alignment context.
Acknowledgments
This work was performed while the first author
was supported by the JSPS Research Fellowship for
Young Scientists.
References
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Proceed-
ings of the Human Language Technology: The 11th
Annual Conference of the North American Chapter of
the Association for Computational Linguistics.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
47th Annual Meeting of the Association for Computa-
tional Linguistics, pages 782?790.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint 5th Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of the NAACL Workshop on Syntax and
Structure in Machine Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars. In
Proceedings of the Human Language Technology: The
640
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 564?572.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 531?540.
Carl de Marcken. 1996. Unsupervised Language Acqui-
sition. Ph.D. thesis, Massachusetts Institute of Tech-
nology.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, pages 25?28.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1453?1463.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proceedings of the 1st Workshop
on Statistical Machine Translation, pages 31?38.
John DeNero, Alex Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 314?323.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent trans-
lation task at the NTCIR-7 workshop. In Proceedings
of the 7th NTCIR Workshop Meeting on Evaluation of
Information Access Technologies, pages 389?400.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 44th Annual Meeting of the Association for
Computational Linguistics, pages 961?968.
J. Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007a. Improving translation quality
by discarding most of the phrasetable. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007b. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
Advances in Neural Information Processing Systems,
19:641.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics.
Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology Conference (HLT-
NAACL), pages 48?54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of the International Workshop on Spoken
Language Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference - North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL), pages 104?111.
Daniel Marcu andWilliamWong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. pages 133?139.
Robert C. Moore and Chris Quirk. 2007. An iteratively-
trained segmentation-free phrase translation model for
statistical machine translation. In Proceedings of
the 2nd Workshop on Statistical Machine Translation,
pages 112?119.
David Newman, Arthur Asuncion, Padhraic Smyth, and
Max Welling. 2009. Distributed algorithms for
topic models. Journal of Machine Learning Research,
10:1801?1828.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 4th Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 20?28.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855?
900.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proceedings of the The 11th International Workshop
on Parsing Technologies.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 44th Annual Meeting of the Association for
Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics, pages 97?105.
641
Pointwise Prediction for Robust, Adaptable
Japanese Morphological Analysis
Graham Neubig, Yosuke Nakata, Shinsuke Mori
Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
Abstract
We present a pointwise approach to Japanese
morphological analysis (MA) that ignores
structure information during learning and tag-
ging. Despite the lack of structure, it is able to
outperform the current state-of-the-art struc-
tured approach for Japanese MA, and achieves
accuracy similar to that of structured predic-
tors using the same feature set. We also
find that the method is both robust to out-
of-domain data, and can be easily adapted
through the use of a combination of partial an-
notation and active learning.
1 Introduction
Japanese morphological analysis (MA) takes an un-
segmented string of Japanese text as input, and out-
puts a string of morphemes annotated with parts of
speech (POSs). As MA is the first step in Japanese
NLP, its accuracy directly affects the accuracy of
NLP systems as a whole. In addition, with the prolif-
eration of text in various domains, there is increasing
need for methods that are both robust and adaptable
to out-of-domain data (Escudero et al, 2000).
Previous approaches have used structured predic-
tors such as hidden Markov models (HMMs) or con-
ditional random fields (CRFs), which consider the
interactions between neighboring words and parts
of speech (Nagata, 1994; Asahara and Matsumoto,
2000; Kudo et al, 2004). However, while struc-
ture does provide valuable information, Liang et al
(2008) have shown that gains provided by struc-
tured prediction can be largely recovered by using a
richer feature set. This approach has also been called
?pointwise? prediction, as it makes a single indepen-
dent decision at each point (Neubig andMori, 2010).
While Liang et al (2008) focus on the speed ben-
efits of pointwise prediction, we demonstrate that it
also allows for more robust and adaptable MA. We
find experimental evidence that pointwise MA can
exceed the accuracy of a state-of-the-art structured
approach (Kudo et al, 2004) on in-domain data, and
is significantly more robust to out-of-domain data.
We also show that pointwise MA can be adapted
to new domains with minimal effort through the
combination of active learning and partial annota-
tion (Tsuboi et al, 2008), where only informative
parts of a particular sentence are annotated. In a
realistic domain adaptation scenario, we find that a
combination of pointwise prediction, partial annota-
tion, and active learning allows for easy adaptation.
2 Japanese Morphological Analysis
Japanese MA takes an unsegmented string of char-
acters xI1 as input, segments it into morphemes wJ1 ,
and annotates each morpheme with a part of speech
tJ1 . This can be formulated as a two-step process of
first segmenting words, then estimating POSs (Ng
and Low, 2004), or as a single joint process of find-
ing a morpheme/POS string from unsegmented text
(Kudo et al, 2004; Nakagawa, 2004; Kruengkrai et
al., 2009). In this section we describe an existing
joint sequence-based method for Japanese MA, as
well as our proposed two-step pointwise method.
2.1 Joint Sequence-Based MA
Japanese MA has traditionally used sequence based
models, finding a maximal POS sequence for en-
Figure 1: Joint MA (a) performs maximization over the
entire sequence, while two-step MA (b) maximizes the 4
boundary and 4 POS tags independently.
Type Feature Strings
Unigram tj , tjwj , c(wj), tjc(wj)
Bigram tj?1tj , tj?1tjwj?1,
tj?1tjwj , tj?1tjwj?1wj
Table 1: Features for the joint model using tags t and
words w. c(?) is a mapping function onto character types
(kanji, katakana, etc.).
tire sentences as in Figure 1 (a). The CRF-based
method presented by Kudo et al (2004) is gener-
ally accepted as the state-of-the-art in this paradigm.
CRFs are trained over segmentation lattices, which
allows for the handling of variable length sequences
that occur due to multiple segmentations. The model
is able to take into account arbitrary features, as well
as the context between neighboring tags.
We follow Kudo et al (2004) in defining our fea-
ture set, as summarized in Table 11. Lexical features
were trained for the top 5000 most frequent words in
the corpus. It should be noted that these are word-
based features, and information about transitions be-
tween POS tags is included. When creating training
data, the use of word-based features indicates that
word boundaries must be annotated, while the use
of POS transition information further indicates that
all of these words must be annotated with POSs.
1More fine-grained POS tags have provided small boosts in
accuracy in previous research (Kudo et al, 2004), but these in-
crease the annotation burden, which is contrary to our goal.
Type Feature Strings
Character xl, xr, xl?1xl, xlxr,
n-gram xrxr+1, xl?1xlxr, xlxrxr+1
Char. Type c(xl), c(xr)
n-gram c(xl?1xl), c(xlxr), c(xrxr+1)
c(xl?2xl?1xl), c(xl?1xlxr)
c(xlxrxr+1), c(xrxr+1xr+2)
WS Only ls, rs, is
POS Only wj , c(wj), djk
Table 2: Features for the two-step model. xl and xr indi-
cate the characters to the left and right of the word bound-
ary or word wj in question. ls, rs, and is represent the
left, right, and inside dictionary features, while djk indi-
cates that tag k exists in the dictionary for word j.
2.2 2-Step Pointwise MA
In our research, we take a two-step approach, first
segmenting character sequence xI1 into the word se-
quencewJ1 with the highest probability, then tagging
each word with parts of speech tJ1 . This approach is
shown in Figure 1 (b).
We follow Sassano (2002) in formulating word
segmentation as a binary classification problem, es-
timating boundary tags bI?11 . Tag bi = 1 indi-
cates that a word boundary exists between charac-
ters xi and xi+1, while bi = 0 indicates that a word
boundary does not exist. POS estimation can also
be formulated as a multi-class classification prob-
lem, where we choose one tag tj for each word wj .
These two classification problems can be solved by
tools in the standard machine learning toolbox such
as logistic regression (LR), support vector machines
(SVMs), or conditional random fields (CRFs).
We use information about the surrounding charac-
ters (character and character-type n-grams), as well
as the presence or absence of words in the dictio-
nary as features (Table 2). Specifically dictionary
features for word segmentation ls and rs are active
if a string of length s included in the dictionary is
present directly to the left or right of the present
word boundary, and is is active if the present word
boundary is included in a dictionary word of length
s. Dictionary feature djk for POS estimation indi-
cates whether the current word wj occurs as a dic-
tionary entry with tag tk.
Previous work using this two-stage approach has
used sequence-based prediction methods, such as
maximum entropy Markov models (MEMMs) or
CRFs (Ng and Low, 2004; Peng et al, 2004). How-
ever, as Liang et al (2008) note, and we confirm,
sequence-based predictors are often not necessary
when an appropriately rich feature set is used. One
important difference between our formulation and
that of Liang et al (2008) and all other previous
methods is that we rely only on features that are di-
rectly calculable from the surface string, without us-
ing estimated information such as word boundaries
or neighboring POS tags2. This allows for training
from sentences that are partially annotated as de-
scribed in the following section.
3 Domain Adaptation for Morphological
Analysis
NLP is now being used in domains such as medi-
cal text and legal documents, and it is necessary that
MA be easily adaptable to these areas. In a domain
adaptation situation, we have at our disposal both
annotated general domain data, and unannotated tar-
get domain data. We would like to annotate the
target domain data efficiently to achieve a maximal
gain in accuracy for a minimal amount of work.
Active learning has been used as a way to pick
data that is useful to annotate in this scenario for
several applications (Chan and Ng, 2007; Rai et
al., 2010) so we adopt an active-learning-based ap-
proach here. When adapting sequence-based predic-
tion methods, most active learning approaches have
focused on picking full sentences that are valuable to
annotate (Ringger et al, 2007; Settles and Craven,
2008). However, even within sentences, there are
generally a few points of interest surrounded by
large segments that are well covered by already an-
notated data.
Partial annotation provides a solution to this prob-
lem (Tsuboi et al, 2008; Sassano and Kurohashi,
2010). In partial annotation, data that will not con-
tribute to the improvement of the classifier is left
untagged. For example, if there is a single difficult
word in a long sentence, only the word boundaries
and POS of the difficult word will be tagged. ?Dif-
2Dictionary features are active if the string exists, regardless
of whether it is treated as a single word in wJ1 , and thus can be
calculated without the word segmentation result.
Type Train Test
General 782k 87.5k
Target 153k 17.3k
Table 3: General and target domain corpus sizes in words.
ficult? words can be selected using active learning
approaches, choosing words with the lowest classi-
fier accuracy to annotate. In addition, corpora that
are tagged with word boundaries but not POS tags
are often available; this is another type of partial an-
notation.
When using sequence-based prediction, learning
on partially annotated data is not straightforward,
as the data that must be used to train context-based
transition probabilities may be left unannotated. In
contrast, in the pointwise prediction framework,
training using this data is both simple and efficient;
unannotated points are simply ignored. A method
for learning CRFs from partially annotated data has
been presented by Tsuboi et al (2008). However,
when using partial annotation, CRFs? already slow
training time becomes slower still, as they must be
trained over every sequence that has at least one an-
notated point. Training time is important in an active
learning situation, as an annotator must wait while
the model is being re-trained.
4 Experiments
In order to test the effectiveness of pointwise MA,
we did an experiment measuring accuracy both on
in-domain data, and in a domain-adaptation situa-
tion. We used the Balanced Corpus of Contempo-
rary Written Japanese (BCCWJ) (Maekawa, 2008),
specifying the whitepaper, news, and books sections
as our general domain corpus, and the web text sec-
tion as our target domain corpus (Table 3).
As a representative of joint sequence-based MA
described in 2.1, we used MeCab (Kudo, 2006), an
open source implementation of Kudo et al (2004)?s
CRF-based method (we will call this JOINT). For the
pointwise two-step method, we trained logistic re-
gression models with the LIBLINEAR toolkit (Fan
et al, 2008) using the features described in Section
2.2 (2-LR). In addition, we trained a CRF-based
model with the CRFSuite toolkit (Okazaki, 2007)
using the same features and set-up (for both word
Train Test JOINT 2-CRF 2-LR
GEN GEN 97.31% 98.08% 98.03%
GEN TAR 94.57% 95.39% 95.13%
GEN+TAR TAR 96.45% 96.91% 96.82%
Table 4: Word/POS F-measure for each method when
trained and tested on general (GEN) or target (TAR) do-
main corpora.
segmentation and POS tagging) to examine the con-
tribution of context information (2-CRF).
To create the dictionary, we added all of the words
in the corpus, but left out a small portion of single-
tons to prevent overfitting on the training data3. As
an evaluation measure, we follow Nagata (1994) and
Kudo et al (2004) and use Word/POS tag pair F-
measure, so that both word boundaries and POS tags
must be correct for a word to be considered correct.
4.1 Analysis Results
In our first experiment we compared the accuracy of
the three methods on both the in-domain and out-
of-domain test sets (Table 4). It can be seen that
2-LR outperforms JOINT, and achieves similar but
slightly inferior results to 2-CRF. The reason for
accuracy gains over JOINT lies largely in the fact
that while JOINT is more reliant on the dictionary,
and thus tends to mis-segment unknown words, the
two-step methods are significantly more robust. The
small difference between 2-LR and 2-CRF indicates
that given a significantly rich feature set, context-
based features provide little advantage, although the
advantage is larger on out-of-domain data. In addi-
tion, training of 2-LR is significantly faster than 2-
CRF. 2-LR took 16m44s to train, while 2-CRF took
51m19s to train on a 3.33GHz Intel Xeon CPU.
4.2 Domain Adaptation
Our second experiment focused on the domain
adaptability of each method. Using the target do-
main training corpus as a pool of unannotated data,
we performed active learning-based domain adapta-
tion using two techniques.
? Sentence-based annotation (SENT), where sen-
tences with the lowest total POS and word
3For JOINT we removed singletons randomly until coverage
was 99.99%, and for 2-LR and 2-CRF coverage was set to 99%,
which gave the best results on held-out data.
Figure 2: Domain adaptation results for three approaches
and two annotation methods.
boundary probabilities were annotated first.
? Word-based partial annotation (PART), where
the word or word boundary with the smallest
probability margin between the first and second
candidates was chosen. This can only be used
with the pointwise 2-LR approach4 .
For both methods, 100 words (or for SENT until
the end of the sentence in which the 100th word
is reached) are annotated, then the classifier is re-
trained and new probability scores are generated.
Each set of 100 words is a single iteration, and 100
iterations were performed for each method.
From the results in Figure 2, it can be seen that
the combination of PART and 2-LR allows for sig-
nificantly faster adaptation than other approaches,
achieving accuracy gains in 15 iterations that are
achieved in 100 iterations with SENT, and surpassing
2-CRF after 15 iterations. Finally, it can be seen that
JOINT improves at a pace similar to PART, likely due
to the fact that its pre-adaptation accuracy is lower
than the other methods. It can be seen from Table 4
that even after adaptation with the full corpus, it will
still lag behind the two-step methods.
5 Conclusion
This paper proposed a pointwise approach to
Japanese morphological analysis. It showed that de-
spite the lack of structure, it was able to achieve re-
4In order to prevent wasteful annotation, each unique word
was only annotated once per iteration.
sults that meet or exceed structured prediction meth-
ods. We also demonstrated that it is both robust and
adaptable to out-of-domain text through the use of
partial annotation and active learning. Future work
in this area will include examination of performance
on other tasks and languages.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
tagger. In Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 21?27.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.
Gerard Escudero, Llu??s Ma`rquez, and German Rigau.
2000. An empirical study of the domain dependence
of supervised word sense disambiguation systems. In
Proceedings of the 2000 Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of the 47th Annual Meeting of
the Association for Computational Linguistics.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 230?237.
Taku Kudo. 2006. MeCab: yet another
part-of-speech and morphological analyzer.
http://mecab.sourceforge.net.
Percy Liang, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th International Conference
on Machine Learning, pages 592?599.
Kikuo Maekawa. 2008. Balanced corpus of contempo-
rary written Japanese. In Proceedings of the 6th Work-
shop on Asian Language Resources, pages 101?102.
Masaaki Nagata. 1994. A stochastic Japanese morpho-
logical analyzer using a forward-DP backward-A? N-
best search algorithm. In Proceedings of the 15th In-
ternational Conference on Computational Linguistics,
pages 201?207.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level in-
formation. In Proceedings of the 20th International
Conference on Computational Linguistics.
Graham Neubig and Shinsuke Mori. 2010. Word-based
partial annotation for efficient corpus construction. In
Proceedings of the 7th International Conference on
Language Resources and Evaluation.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: one-at-a-time or all-at-once? word-
based or character-based. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of the
20th International Conference on Computational Lin-
guistics.
Piyush Rai, Avishek Saha, Hal Daume? III, and Suresh
Venkatasubramanian. 2010. Domain Adaptation
meets Active Learning. In Workshop on Active Learn-
ing for Natural Language Processing (ALNLP-10).
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop,
pages 101?108.
Manabu. Sassano and Sadao Kurohashi. 2010. Us-
ing smaller constituents rather than sentences in ac-
tive learning for Japanese dependency parsing. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 356?365.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for Japanese
word segmentation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 505?512.
Burr Settles and Mark Craven. 2008. An analysis of
active learning strategies for sequence labeling tasks.
In Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1070?1079.
Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, Shinsuke
Mori, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations. In
Proceedings of the 22th International Conference on
Computational Linguistics, pages 897?904.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 165?174,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Machine Translation without Words through Substring Alignment
Graham Neubig1,2, Taro Watanabe2, Shinsuke Mori1, Tatsuya Kawahara1
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
In this paper, we demonstrate that accu-
rate machine translation is possible without
the concept of ?words,? treating MT as a
problem of transformation between character
strings. We achieve this result by applying
phrasal inversion transduction grammar align-
ment techniques to character strings to train
a character-based translation model, and us-
ing this in the phrase-based MT framework.
We also propose a look-ahead parsing algo-
rithm and substring-informed prior probabil-
ities to achieve more effective and efficient
alignment. In an evaluation, we demonstrate
that character-based translation can achieve
results that compare to word-based systems
while effectively translating unknown and un-
common words over several language pairs.
1 Introduction
Traditionally, the task of statistical machine trans-
lation (SMT) is defined as translating a source sen-
tence fJ1 = {f1, . . . , fJ} to a target sentence eI1 =
{e1, . . ., eI}, where each element of fJ1 and eI1 is
assumed to be a word in the source and target lan-
guages. However, the definition of a ?word? is of-
ten problematic. The most obvious example of this
lies in languages that do not separate words with
white space such as Chinese, Japanese, or Thai, in
which the choice of a segmentation standard has
a large effect on translation accuracy (Chang et
al., 2008). Even for languages with explicit word
The first author is now affiliated with the Nara Institute of Sci-
ence and Technology.
boundaries, all machine translation systems perform
at least some precursory form of tokenization, split-
ting punctuation and words to prevent the sparsity
that would occur if punctuated and non-punctuated
words were treated as different entities. Sparsity
also manifests itself in other forms, including the
large vocabularies produced by morphological pro-
ductivity, word compounding, numbers, and proper
names. A myriad of methods have been proposed
to handle each of these phenomena individually,
including morphological analysis, stemming, com-
pound breaking, number regularization, optimizing
word segmentation, and transliteration, which we
outline in more detail in Section 2.
These difficulties occur because we are translat-
ing sequences of words as our basic unit. On the
other hand, Vilar et al (2007) examine the possibil-
ity of instead treating each sentence as sequences of
characters to be translated. This method is attrac-
tive, as it is theoretically able to handle all sparsity
phenomena in a single unified framework, but has
only been shown feasible between similar language
pairs such as Spanish-Catalan (Vilar et al, 2007),
Swedish-Norwegian (Tiedemann, 2009), and Thai-
Lao (Sornlertlamvanich et al, 2008), which have
a strong co-occurrence between single characters.
As Vilar et al (2007) state and we confirm, accu-
rate translations cannot be achieved when applying
traditional translation techniques to character-based
translation for less similar language pairs.
In this paper, we propose improvements to the
alignment process tailored to character-based ma-
chine translation, and demonstrate that it is, in fact,
possible to achieve translation accuracies that ap-
165
proach those of traditional word-based systems us-
ing only character strings. We draw upon recent
advances in many-to-many alignment, which allows
for the automatic choice of the length of units to
be aligned. As these units may be at the charac-
ter, subword, word, or multi-word phrase level, we
conjecture that this will allow for better character
alignments than one-to-many alignment techniques,
and will allow for better translation of uncommon
words than traditional word-based models by break-
ing down words into their component parts.
We also propose two improvements to the many-
to-many alignment method of Neubig et al (2011).
One barrier to applying many-to-many alignment
models to character strings is training cost. In the
inversion transduction grammar (ITG) framework
(Wu, 1997), which is widely used in many-to-many
alignment, search is cumbersome for longer sen-
tences, a problem that is further exacerbated when
using characters instead of words as the basic unit.
As a step towards overcoming this difficulty, we in-
crease the efficiency of the beam-search technique of
Saers et al (2009) by augmenting it with look-ahead
probabilities in the spirit of A* search. Secondly,
we describe a method to seed the search process us-
ing counts of all substring pairs in the corpus to bias
the phrase alignment model. We do this by defining
prior probabilities based on these substring counts
within the Bayesian phrasal ITG framework.
An evaluation on four language pairs with differ-
ing morphological properties shows that for distant
language pairs, character-based SMT can achieve
translation accuracy comparable to word-based sys-
tems. In addition, we perform ablation studies,
showing that these results were not possible with-
out the proposed enhancements to the model. Fi-
nally, we perform a qualitative analysis, which finds
that character-based translation can handle unseg-
mented text, conjugation, and proper names in a uni-
fied framework with no additional processing.
2 Related Work on Data Sparsity in SMT
As traditional SMT systems treat all words as single
tokens without considering their internal structure,
major problems of data sparsity occur for less fre-
quent tokens. In fact, it has been shown that there
is a direct negative correlation between vocabulary
size (and thus sparsity) of a language and transla-
tion accuracy (Koehn, 2005). Sparsity causes trou-
ble for alignment models, both in the form of incor-
rectly aligned uncommon words, and in the form of
garbage collection, where uncommon words in one
language are incorrectly aligned to large segments
of the sentence in the other language (Och and Ney,
2003). Unknown words are also a problem during
the translation process, and the default approach is
to map them as-is into the target sentence.
This is a major problem in agglutinative lan-
guages such as Finnish or compounding languages
such as German. Previous works have attempted to
handle morphology, decompounding and regulariza-
tion through lemmatization, morphological analysis,
or unsupervised techniques (Nie?en and Ney, 2000;
Brown, 2002; Lee, 2004; Goldwater and McClosky,
2005; Talbot and Osborne, 2006; Mermer and Ak?n,
2010; Macherey et al, 2011). It has also been noted
that it is more difficult to translate into morpho-
logically rich languages, and methods for modeling
target-side morphology have attracted interest in re-
cent years (Bojar, 2007; Subotin, 2011).
Another source of data sparsity that occurs in all
languages is proper names, which have been handled
by using cognates or transliteration to improve trans-
lation (Knight and Graehl, 1998; Kondrak et al,
2003; Finch and Sumita, 2007), and more sophisti-
cated methods for named entity translation that com-
bine translation and transliteration have also been
proposed (Al-Onaizan and Knight, 2002).
Choosing word units is also essential for creat-
ing good translation results for languages that do
not explicitly mark word boundaries, such as Chi-
nese, Japanese, and Thai. A number of works have
dealt with this word segmentation problem in trans-
lation, mainly focusing on Chinese-to-English trans-
lation (Bai et al, 2008; Chang et al, 2008; Zhang et
al., 2008b; Chung and Gildea, 2009; Nguyen et al,
2010), although these works generally assume that a
word segmentation exists in one language (English)
and attempt to optimize the word segmentation in
the other language (Chinese).
We have enumerated these related works to
demonstrate the myriad of data sparsity problems
and proposed solutions. Character-based transla-
tion has the potential to handle all of the phenom-
ena in the previously mentioned research in a single
166
unified framework, requiring no language specific
tools such as morphological analyzers or word seg-
menters. However, while the approach is attractive
conceptually, previous research has only been shown
effective for closely related language pairs (Vilar et
al., 2007; Tiedemann, 2009; Sornlertlamvanich et
al., 2008). In this work, we propose effective align-
ment techniques that allow character-based transla-
tion to achieve accurate translation results for both
close and distant language pairs.
3 Alignment Methods
SMT systems are generally constructed from a par-
allel corpus consisting of target language sentences
E and source language sentences F . The first step
of training is to find alignments A for the words in
each sentence pair.
We represent our target and source sentences as
eI1 and fJ1 . ei and fj represent single elements of
the target and source sentences respectively. These
may be words in word-based alignment models or
single characters in character-based alignment mod-
els.1 We define our alignment as aK1 , where each
element is a span ak = ?s, t, u, v? indicating that the
target string es, . . . , et and source string fu, . . . , fv
are aligned to each-other.
3.1 One-to-Many Alignment
The most well-known and widely-used models for
bitext alignment are for one-to-many alignment, in-
cluding the IBM models (Brown et al, 1993) and
HMM alignment model (Vogel et al, 1996). These
models are by nature directional, attempting to find
the alignments that maximize the conditional prob-
ability of the target sentence P (eI1|fJ1 ,aK1 ). For
computational reasons, the IBM models are re-
stricted to aligning each word on the target side to
a single word on the source side. In the formal-
ism presented above, this means that each ei must
be included in at most one span, and for each span
u = v. Traditionally, these models are run in both
directions and combined using heuristics to create
many-to-many alignments (Koehn et al, 2003).
However, in order for one-to-many alignment
methods to be effective, each fj must contain
1Some previous work has also performed alignment using
morphological analyzers to normalize or split the sentence into
morpheme streams (Corston-Oliver and Gamon, 2004).
enough information to allow for effective alignment
with its corresponding elements in eI1. While this is
often the case in word-based models, for character-
based models this assumption breaks down, as there
is often no clear correspondence between characters.
3.2 Many-to-Many Alignment
On the other hand, in recent years, there have been
advances in many-to-many alignment techniques
that are able to align multi-element chunks on both
sides of the translation (Marcu and Wong, 2002;
DeNero et al, 2008; Blunsom et al, 2009; Neu-
big et al, 2011). Many-to-many methods can be ex-
pected to achieve superior results on character-based
alignment, as the aligner can use information about
substrings, which may correspond to letters, mor-
phemes, words, or short phrases.
Here, we focus on the model presented by Neu-
big et al (2011), which uses Bayesian inference in
the phrasal inversion transduction grammar (ITG,
Wu (1997)) framework. ITGs are a variety of syn-
chronous context free grammar (SCFG) that allows
for many-to-many alignment to be achieved in poly-
nomial time through the process of biparsing, which
we explain more in the following section. Phrasal
ITGs are ITGs that allow for non-terminals that can
emit phrase pairs with multiple elements on both
the source and target sides. It should be noted
that there are other many-to-many alignment meth-
ods that have been used for simultaneously discov-
ering morphological boundaries over multiple lan-
guages (Snyder and Barzilay, 2008; Naradowsky
and Toutanova, 2011), but these have generally been
applied to single words or short phrases, and it is not
immediately clear that they will scale to aligning full
sentences.
4 Look-Ahead Biparsing
In this work, we experiment with the alignment
method of Neubig et al (2011), which can achieve
competitive accuracy with a much smaller phrase ta-
ble than traditional methods. This is important in
the character-based translation context, as we would
like to use phrases that contain large numbers of
characters without creating a phrase table so large
that it cannot be used in actual decoding. In this
framework, training is performed using sentence-
167
Figure 1: (a) A chart with inside probabilities in boxes
and forward/backward probabilities marking the sur-
rounding arrows. (b) Spans with corresponding look-
aheads added, and the minimum probability underlined.
Lightly and darkly shaded spans will be trimmed when
the beam is log(P ) ? ?3 and log(P ) ? ?6 respectively.
wise block sampling, acquiring a sample for each
sentence by first performing bottom-up biparsing to
create a chart of probabilities, then performing top-
down sampling of a new tree based on the probabil-
ities in this chart.
An example of a chart used in this parsing can
be found in Figure 1 (a). Within each cell of the
chart spanning ets and fvu is an ?inside? probabil-
ity I(as,t,u,v). This probability is the combination
of the generative probability of each phrase pair
Pt(ets,fvu) as well as the sum the probabilities over
all shorter spans in straight and inverted order2
I(as,t,u,v) = Pt(ets, fvu)
+
?
s?S?t
?
u?U?v
Px(str)I(as,S,u,U )I(aS,t,U,v)
+
?
s?S?t
?
u?U?v
Px(inv)I(as,S,U,v)I(aS,t,u,U )
where Px(str) and Px(inv) are the probability of
straight and inverted ITG productions.
While the exact calculation of these probabilities
can be performed in O(n6) time, where n is the
2Pt can be specified according to Bayesian statistics as de-
scribed by Neubig et al (2011).
length of the sentence, this is impractical for all but
the shortest sentences. Thus it is necessary to use
methods to reduce the search space such as beam-
search based chart parsing (Saers et al, 2009) or
slice sampling (Blunsom and Cohn, 2010).3
In this section we propose the use of a look-ahead
probability to increase the efficiency of this chart
parsing. Taking the example of Saers et al (2009),
spans are pushed onto a different queue based on
their size, and queues are processed in ascending or-
der of size. Agendas can further be trimmed based
on a histogram beam (Saers et al, 2009) or probabil-
ity beam (Neubig et al, 2011) compared to the best
hypothesis a?. In other words, we have a queue dis-
cipline based on the inside probability, and all spans
ak where I(ak) < cI(a?) are pruned. c is a constant
describing the width of the beam, and a smaller con-
stant probability will indicate a wider beam.
This method is insensitive to the existence of
competing hypotheses when performing pruning.
Figure 1 (a) provides an example of why it is unwise
to ignore competing hypotheses during beam prun-
ing. Particularly, the alignment ?les/1960s? com-
petes with the high-probability alignment ?les/the,?
so intuitively should be a good candidate for prun-
ing. However its probability is only slightly higher
than ?anne?es/1960s,? which has no competing hy-
potheses and thus should not be trimmed.
In order to take into account competing hypothe-
ses, we can use for our queue discipline not only the
inside probability I(ak), but also the outside proba-
bility O(ak), the probability of generating all spans
other than ak, as in A* search for CFGs (Klein and
Manning, 2003), and tic-tac-toe pruning for word-
based ITGs (Zhang and Gildea, 2005). As the cal-
culation of the actual outside probability O(ak) is
just as expensive as parsing itself, it is necessary to
approximate this with heuristic function O? that can
be calculated efficiently.
Here we propose a heuristic function that is de-
signed specifically for phrasal ITGs and is com-
putable with worst-case complexity of n2, compared
with the n3 amortized time of the tic-tac-toe pruning
3Applying beam-search before sampling will sample from
an improper distribution, although Metropolis-in-Gibbs sam-
pling (Johnson et al, 2007) can be used to compensate. How-
ever, we found that this had no significant effect on results, so
we omit the Metropolis-in-Gibbs step for experiments.
168
algorithm described by (Zhang et al, 2008a). Dur-
ing the calculation of the phrase generation proba-
bilities Pt, we save the best inside probability I? for
each monolingual span.
I?e (s, t) = max
{a?=?s?,t?,u?,v??;s?=s,t?=t}
Pt(a?)
I?f (u, v) = max
{a?=?s?,t?,u?,v??;u?=u,v?=v}
Pt(a?)
For each language independently, we calculate for-
ward probabilities ? and backward probabilities ?.
For example, ?e(s) is the maximum probability of
the span (0, s) of e that can be created by concate-
nating together consecutive values of I?e :
?e(s) = max
{S1,...,Sx}
I?e (0, S1)I?e (S1, S2) . . . I?e (Sx, s).
Backwards probabilities and probabilities over f can
be defined similarly. These probabilities are calcu-
lated for e and f independently, and can be calcu-
lated in n2 time by processing each ? in ascending
order, and each ? in descending order in a fashion
similar to that of the forward-backward algorithm.
Finally, for any span, we define the outside heuristic
as the minimum of the two independent look-ahead
probabilities over each language
O?(as,t,u,v) = min(?e(s) ? ?e(t), ?f (u) ? ?f (v)).
Looking again at Figure 1 (b), it can be seen
that the relative probability difference between the
highest probability span ?les/the? and the spans
?anne?es/1960s? and ?60/1960s? decreases, allowing
for tighter beam pruning without losing these good
hypotheses. In contrast, the relative probability of
?les/1960s? remains low as it is in conflict with a
high-probability alignment, allowing it to be dis-
carded.
5 Substring Prior Probabilities
While the Bayesian phrasal ITG framework uses
the previously mentioned phrase distribution Pt dur-
ing search, it also allows for definition of a phrase
pair prior probability Pprior(ets,fvu), which can ef-
ficiently seed the search process with a bias towards
phrase pairs that satisfy certain properties. In this
section, we overview an existing method used to cal-
culate these prior probabilities, and also propose a
new way to calculate priors based on substring co-
occurrence statistics.
5.1 Word-based Priors
Previous research on many-to-many translation has
used IBM model 1 probabilities to bias phrasal
alignments so that phrases whose member words are
good translations are also aligned. As a representa-
tive of this existing method, we adopt a base mea-
sure similar to that used by DeNero et al (2008):
Pm1(e,f) =M0(e,f)Ppois(|e|;?)Ppois(|f |;?)
M0(e,f) =(Pm1(f |e)Puni(e)Pm1(e|f)Puni(f))
1
2 .
Ppois is the Poisson distribution with the average
length parameter ?, which we set to 0.01. Pm1 is the
word-based (or character-based) Model 1 probabil-
ity, which can be efficiently calculated using the dy-
namic programming algorithm described by Brown
et al (1993). However, for reasons previously stated
in Section 3, these methods are less satisfactory
when performing character-based alignment, as the
amount of information contained in a character does
not allow for proper alignment.
5.2 Substring Co-occurrence Priors
Instead, we propose a method for using raw sub-
string co-occurrence statistics to bias alignments to-
wards substrings that often co-occur in the entire
training corpus. This is similar to the method of
Cromieres (2006), but instead of using these co-
occurrence statistics as a heuristic alignment crite-
rion, we incorporate them as a prior probability in
a statistical model that can take into account mutual
exclusivity of overlapping substrings in a sentence.
We define this prior probability using three counts
over substrings c(e), c(f), and c(e,f). c(e) and
c(f) count the total number of sentences in which
the substrings e and f occur respectively. c(e,f) is
a count of the total number of sentences in which the
substring e occurs on the target side, and f occurs
on the source side. We perform the calculation of
these statistics using enhanced suffix arrays, a data
structure that can efficiently calculate all substrings
in a corpus (Abouelhoda et al, 2004).4
While suffix arrays allow for efficient calculation
of these statistics, storing all co-occurrence counts
c(e,f) is an unrealistic memory burden for larger
4Using the open-source implementation esaxx http://
code.google.com/p/esaxx/
169
corpora. In order to reduce the amount of mem-
ory used, we discount every count by a constant d,
which we set to 5. This has a dual effect of reducing
the amount of memory needed to hold co-occurrence
counts by removing values for which c(e,f) < d, as
well as preventing over-fitting of the training data. In
addition, we heuristically prune values for which the
conditional probabilities P (e|f) or P (f |e) are less
than some fixed value, which we set to 0.1 for the
reported experiments.
To determine how to combine c(e), c(f), and
c(e,f) into prior probabilities, we performed pre-
liminary experiments testing methods proposed by
previous research including plain co-occurrence
counts, the Dice coefficient, and ?-squared statistics
(Cromieres, 2006), as well as a newmethod of defin-
ing substring pair probabilities to be proportional to
bidirectional conditional probabilities
Pcooc(e,f) = Pcooc(e|f)Pcooc(f |e)/Z
=
(
c(e,f) ? d
c(f) ? d
)(
c(e,f) ? d
c(e) ? d
)
/Z
for all substring pairs where c(e,f) > d and where
Z is a normalization term equal to
Z =
?
{e,f ;c(e,f)>d}
Pcooc(e|f)Pcooc(f |e).
The experiments showed that the bidirectional con-
ditional probability method gave significantly better
results than all other methods, so we adopt this for
the remainder of our experiments.
It should be noted that as we are using discount-
ing, many substring pairs will be given zero proba-
bility according to Pcooc. As the prior is only sup-
posed to bias the model towards good solutions and
not explicitly rule out any possibilities, we linearly
interpolate the co-occurrence probability with the
one-to-many Model 1 probability, which will give
at least some probability mass to all substring pairs
Pprior(e,f) = ?Pcooc(e,f) + (1 ? ?)Pm1(e,f).
We put a Dirichlet prior (? = 1) on the interpolation
coefficient ? and learn it during training.
6 Experiments
In order to test the effectiveness of character-based
translation, we performed experiments over a variety
of language pairs and experimental settings.
de-en fi-en fr-en ja-en
TM (en) 2.80M 3.10M 2.77M 2.13M
TM (other) 2.56M 2.23M 3.05M 2.34M
LM (en) 16.0M 15.5M 13.8M 11.5M
LM (other) 15.3M 11.3M 15.6M 11.9M
Tune (en) 58.7k 58.7k 58.7k 30.8k
Tune (other) 55.1k 42.0k 67.3k 34.4k
Test (en) 58.0k 58.0k 58.0k 26.6k
Test (other) 54.3k 41.4k 66.2k 28.5k
Table 1: The number of words in each corpus for TM and
LM training, tuning, and testing.
6.1 Experimental Setup
We use a combination of four languages with En-
glish, using freely available data. We selected
French-English, German-English, Finnish-English
data from EuroParl (Koehn, 2005), with develop-
ment and test sets designated for the 2005 ACL
shared task on machine translation.5 We also did
experiments with Japanese-English Wikipedia arti-
cles from the Kyoto Free Translation Task (Neu-
big, 2011) using the designated training and tuning
sets, and reporting results on the test set. These lan-
guages were chosen as they have a variety of inter-
esting characteristics. French has some inflection,
but among the test languages has the strongest one-
to-one correspondence with English, and is gener-
ally considered easy to translate. German has many
compound words, which must be broken apart to
translate properly into English. Finnish is an ag-
glutinative language with extremely rich morphol-
ogy, resulting in long words and the largest vocab-
ulary of the languages in EuroParl. Japanese does
not have any clear word boundaries, and uses logo-
graphic characters, which contain more information
than phonetic characters.
With regards to data preparation, the EuroParl
data was pre-tokenized, so we simply used the to-
kenized data as-is for the training and evaluation of
all models. For word-based translation in the Kyoto
task, training was performed using the provided tok-
enization scripts. For character-based translation, no
tokenization was performed, using the original text
for both training and decoding. For both tasks, we
selected as training data all sentences for which both
5http://statmt.org/wpt05/mt-shared-task
170
de-en fi-en fr-en ja-en
GIZA-word 24.58 / 64.28 / 30.43 20.41 / 60.01 / 27.89 30.23 / 68.79 / 34.20 17.95 / 56.47 / 24.70
ITG-word 23.87 / 64.89 / 30.71 20.83 / 61.04 / 28.46 29.92 / 68.64 / 34.29 17.14 / 56.60 / 24.89
GIZA-char 08.05 / 45.01 / 15.35 06.91 / 41.62 / 14.39 11.05 / 48.23 / 17.80 09.46 / 49.02 / 18.34
ITG-char 21.79 / 64.47 / 30.12 18.38 / 62.44 / 28.94 26.70 / 66.76 / 32.47 15.84 / 58.41 / 24.58
en-de en-fi en-fr en-ja
GIZA-word 17.94 / 62.71 / 37.88 13.22 / 58.50 / 27.03 32.19 / 69.20 / 52.39 20.79 / 27.01 / 38.41
ITG-word 17.47 / 63.18 / 37.79 13.12 / 59.27 / 27.09 31.66 / 69.61 / 51.98 20.26 / 28.34 / 38.34
GIZA-char 06.17 / 41.04 / 19.90 04.58 / 35.09 / 11.76 10.31 / 42.84 / 25.06 01.48 / 00.72 / 06.67
ITG-char 15.35 / 61.95 / 35.45 12.14 / 59.02 / 25.31 27.74 / 67.44 / 48.56 17.90 / 28.46 / 35.71
Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal
ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant differ-
ence from the best system according to the bootstrap resampling method at p = 0.05 (Koehn, 2004).
source and target were 100 characters or less,6 the
total size of which is shown in Table 1. In character-
based translation, white spaces between words were
treated as any other character and not given any spe-
cial treatment. Evaluation was performed on tok-
enized and lower-cased data.
For alignment, we use the GIZA++ implementa-
tion of one-to-many alignment7 and the pialign im-
plementation of the phrasal ITG models8 modified
with the proposed improvements. For GIZA++, we
used the default settings for word-based alignment,
but used the HMM model for character-based align-
ment to allow for alignment of longer sentences.
For pialign, default settings were used except for
character-based ITG alignment, which used a prob-
ability beam of 10?4 instead 10?10.9 For decoding,
we use the Moses decoder,10 using the default set-
tings except for the stack size, which we set to 1000
instead of 200. Minimum error rate training was per-
formed to maximize word-based BLEU score for all
systems.11 For language models, word-based trans-
lation uses a word 5-gram model, and character-
based translation uses a character 12-gram model,
both smoothed using interpolated Kneser-Ney.
6100 characters is an average of 18.8 English words
7http://code.google.com/p/giza-pp/
8http://phontron.com/pialign/
9Improvement by using a beam larger than 10?4 was
marginal, especially with co-occurrence prior probabilities.
10http://statmt.org/moses/
11We chose this set-up to minimize the effect of tuning crite-
rion on our experiments, although it does indicate that we must
have access to tokenized data for the development set.
6.2 Quantitative Evaluation
Table 2 presents a quantitative analysis of the trans-
lation results for each of the proposed methods. As
previous research has shown that it is more diffi-
cult to translate into morphologically rich languages
than into English (Koehn, 2005), we perform exper-
iments translating in both directions for all language
pairs. We evaluate translation quality using BLEU
score (Papineni et al, 2002), both on the word and
character level (with n = 4), as well as METEOR
(Denkowski and Lavie, 2011) on the word level.
It can be seen that character-based translation
with all of the proposed alignment improvements
greatly exceeds character-based translation using
one-to-many alignment, confirming that substring-
based information is necessary for accurate align-
ments. When compared with word-based trans-
lation, character-based translation achieves better,
comparable, or inferior results on character-based
BLEU, comparable or inferior results on METEOR,
and inferior results on word-based BLEU. The dif-
ferences between the evaluation metrics are due to
the fact that character-based translation often gets
words mostly correct other than one or two letters.
These are given partial credit by character-based
BLEU (and to a lesser extent METEOR), but marked
entirely wrong by word-based BLEU.
Interestingly, for translation into English,
character-based translation achieves higher ac-
curacy compared to word-based translation on
Japanese and Finnish input, followed by German,
171
fi-en ja-en
ITG-word 2.851 2.085
ITG-char 2.826 2.154
Table 3: Human evaluation scores (0-5 scale).
Ref: directive on equality
Source Unk. Word: tasa-arvodirektiivi
(13/26) Char: equality directive
Ref: yoshiwara-juku station
Target Unk. Word: yoshiwara no eki
(5/26) Char: yoshiwara-juku station
Ref: world health organisation
Uncommon Word: world health
(5/26) Char: world health organisation
Table 4: The major gains of character-based translation,
unknown, hyphenated, and uncommon words.
and finally French. This confirms that character-
based translation is performing well on languages
that have long words or ambiguous boundaries, and
less well on language pairs with relatively strong
one-to-one correspondence between words.
6.3 Qualitative Evaluation
In addition, we performed a subjective evaluation of
Japanese-English and Finnish-English translations.
Two raters evaluated 100 sentences each, assigning
a score of 0-5 based on how well the translation con-
veys the information contained in the reference. We
focus on shorter sentences of 8-16 English words to
ease rating and interpretation. Table 3 shows that
the results are comparable, with no significant dif-
ference in average scores for either language pair.
Table 4 shows a breakdown of the sentences for
which character-based translation received a score
of at 2+ points more than word-based. It can be seen
that character-based translation is properly handling
sparsity phenomena. On the other hand, word-based
translation was generally stronger with reordering
and lexical choice of more common words.
6.4 Effect of Alignment Method
In this section, we compare the translation accura-
cies for character-based translation using the phrasal
ITG model with and without the proposed improve-
ments of substring co-occurrence priors and look-
ahead parsing as described in Sections 4 and 5.2.
fi-en en-fi ja-en en-ja
ITG +cooc +look 28.94 25.31 24.58 35.71
ITG +cooc -look 28.51 24.24 24.32 35.74
ITG -cooc +look 28.65 24.49 24.36 35.05
ITG -cooc -look 27.45 23.30 23.57 34.50
Table 5: METEOR scores for alignment with and without
look-ahead and co-occurrence priors.
Figure 5 shows METEOR scores12 for experi-
ments translating Japanese and Finnish. It can be
seen that the co-occurrence prior gives gains in all
cases, indicating that substring statistics are effec-
tively seeding the ITG aligner. The introduced look-
ahead probabilities improve accuracy significantly
when substring co-occurrence counts are not used,
and slightly when co-occurrence counts are used.
More importantly, they allow for more aggressive
beam pruning, increasing sampling speed from 1.3
sent/s to 2.5 sent/s for Finnish, and 6.8 sent/s to 11.6
sent/s for Japanese.
7 Conclusion and Future Directions
This paper demonstrated that character-based trans-
lation can act as a unified framework for handling
difficult problems in translation: morphology, com-
pound words, transliteration, and segmentation.
One future challenge includes scaling training up
to longer sentences, which can likely be achieved
through methods such as the heuristic span prun-
ing of Haghighi et al (2009) or sentence splitting
of Vilar et al (2007). Monolingual data could also
be used to improve estimates of our substring-based
prior. In addition, error analysis showed that word-
based translation performed better than character-
based translation on reordering and lexical choice,
indicating that improved decoding (or pre-ordering)
and language modeling tailored to character-based
translation will likely greatly improve accuracy. Fi-
nally, we plan to explore the middle ground between
word-based and character based translation, allow-
ing for the flexibility of character-based translation,
while using word boundary information to increase
efficiency and accuracy.
12Similar results were found for character and word-based
BLEU, but are omitted for lack of space.
172
References
Mohamed I. Abouelhoda, Stefan Kurtz, and Enno Ohle-
busch. 2004. Replacing suffix trees with enhanced
suffix arrays. Journal of Discrete Algorithms, 2(1).
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In Proc. ACL.
Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.
2008. Improving word alignment by adjusting Chi-
nese word segmentation. In Proc. IJCNLP.
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Proc.
HLT-NAACL, pages 238?241.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proc. ACL.
Ondr?ej Bojar. 2007. English-to-Czech factored machine
translation. In Proc. WMT.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19.
Ralf D. Brown. 2002. Corpus-driven splitting of com-
pound words. In Proc. TMI.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Proc.
WMT.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Proc.
EMNLP.
Simon Corston-Oliver and Michael Gamon. 2004. Nor-
malizing German and English inflectional morphology
to improve statistical word alignment. Machine Trans-
lation: From Real Users to Research.
Fabien Cromieres. 2006. Sub-sentential alignment us-
ing substring co-occurrence counts. In Proc. COL-
ING/ACL 2006 Student Research Workshop.
John DeNero, Alex Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proc. EMNLP.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization and
Evaluation of Machine Translation Systems. In Proc.
WMT.
Andrew Finch and Eiichiro Sumita. 2007. Phrase-based
machine transliteration. In Proc. TCAST.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Proc. EMNLP.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proc. ACL.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proc. NAACL.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: fast exact Viterbi parse selection. In Proc. HLT.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. HLT,
pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proc. HLT.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proc. HLT.
Klaus Macherey, Andrew Dai, David Talbot, Ashok
Popat, and Franz Och. 2011. Language-independent
compound splitting with morphological operations. In
Proc. ACL.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. EMNLP.
Cos?kun Mermer and Ahmet Afs??n Ak?n. 2010. Unsu-
pervised search for the optimal segmentation for sta-
tistical machine translation. In Proc. ACL Student Re-
search Workshop.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich hidden semi-Markov models.
In Proc. ACL.
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shin-
suke Mori, and Tatsuya Kawahara. 2011. An unsuper-
vised model for joint phrase alignment and extraction.
In Proc. ACL, pages 632?641, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation task.
http://www.phontron.com/kftt.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for machine
translation. In Proc. COLING.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In Proc. COL-
ING.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. COLING.
173
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proc. IWPT, pages 29?32.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. Proc. ACL.
Virach Sornlertlamvanich, Chumpol Mokarat, and Hi-
toshi Isahara. 2008. Thai-lao machine translation
based on phoneme transfer. In Proc. 14th Annual
Meeting of the Association for Natural Language Pro-
cessing.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. ACL.
David Talbot and Miles Osborne. 2006. Modelling lexi-
cal redundancy for machine translation. In Proc. ACL.
Jo?rg Tiedemann. 2009. Character-based PSMT for
closely related languages. In Proc. 13th Annual
Conference of the European Association for Machine
Translation.
David Vilar, Jan-T. Peter, and Hermann Ney. 2007. Can
we translate letters. In Proc. WMT.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. COLING.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
Proc. ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008a. Bayesian learning of
non-compositional phrases with synchronous parsing.
Proc. ACL.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008b. Improved statistical machine translation by
multiple Chinese word segmentation. In Proc. WMT.
174
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59?66,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Spoken Dialogue System based on Information Extraction
using Similarity of Predicate Argument Structures
Koichiro Yoshino, Shinsuke Mori and Tatsuya Kawahara
School of Informatics, Kyoto University
Sakyo-ku, Kyoto, 606-8501, Japan
Abstract
We present a novel scheme of spoken dialogue
systems which uses the up-to-date informa-
tion on the web. The scheme is based on in-
formation extraction which is defined by the
predicate-argument (P-A) structure and real-
ized by semantic parsing. Based on the in-
formation structure, the dialogue system can
perform question answering and also proac-
tive information presentation. Feasibility of
this scheme is demonstrated with experiments
using a domain of baseball news. In order to
automatically select useful domain-dependent
P-A templates, statistical measures are intro-
duced, resulting to a completely unsupervised
learning of the information structure given a
corpus. Similarity measures of P-A structures
are also introduced to select relevant infor-
mation. An experimental evaluation shows
that the proposed system can make more rel-
evant responses compared with the conven-
tional ?bag-of-words? scheme.
1 Introduction
Recently, a huge amount of information is accumu-
lated and distributed on the web day by day. As a
result, many people get information via web rather
than the conventional mass media. On the other
hand, the amount of information on the web is so
huge that we often encounter the difficulty in finding
information we want. Keyword search is the most
widely-used means for the web information access.
However, this style is not necessarily the best for
information demands of all users who do not have
definite goals or just want to know what would be
interesting. To cope with user?s vague information
demands is an important mission for interactive spo-
ken dialogue systems. Moreover, supporting user?s
information collection in a small-talk style is one of
the new directions of spoken dialogue systems.
Existing spoken dialogue systems can be clas-
sified into two types (T.Kawahara, 2009): those
using relational databases (RDB) such as the Air-
line Travel Information System (ATIS) (D.A.Dahl,
1994), and those using information retrieval tech-
niques based on statistical document matching
(T.Misu and T.Kawahara, 2010). The first scheme
can achieve a well-defined task by using a struc-
tural database, but this scheme cannot be applied to
the web information in which the structure and task
are not well defined. The second scheme has been
studied to handle large-scale texts such as web, but
most of the conventional systems adopt a ?bag-of-
words? model, and naive statistical matching often
generates irrelevant responses which have nothing
to do with the user?s requests. Our proposed scheme
solves this problem by using information extraction
based on semantic parsing from web texts, with-
out constructing an RDB. We adopt the predicate-
argument (P-A) structure generated by a parser as
a baseline, but every P-A structure is not useful for
information extraction and retrieval(Y.Kiyota et al,
2002; M.O.Dzikovska et al, 2003; S.Harabagiu et
al., 2005). In fact, the useful information structure
is dependent on domains. Conventionally, the tem-
plates for information extraction were hand-crafted
(R.Grishman, 2003), but this heuristic process is so
costly that it cannot be applied to a variety of do-
mains on the web. In this paper, therefore, we pro-
59
	

 	 
Proceedings of the 8th International Natural Language Generation Conference, pages 118?122,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
FlowGraph2Text: Automatic Sentence Skeleton Compilation
for Procedural Text Generation
?1Shinsuke Mori ?2Hirokuni Maeta 1Tetsuro Sasada 2Koichiro Yoshino
3Atsushi Hashimoto 1Takuya Funatomi 2Yoko Yamakata
1Academic Center for Computing and Media Studies, Kyoto University
2Graduate School of Informatics, Kyoto University
3Graduate School of Law, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
?forest@i.kyoto-u.ac.jp
Abstract
In this paper we describe a method for
generating a procedural text given its
flow graph representation. Our main
idea is to automatically collect sen-
tence skeletons from real texts by re-
placing the important word sequences
with their type labels to form a skeleton
pool. The experimental results showed
that our method is feasible and has a
potential to generate natural sentences.
1 Introduction
Along with computers penetrating in our daily
life, the needs for the natural language gener-
ation (NLG) technology are increasing more
and more. If computers understand both the
meaning of a procedural text and the progres-
sion status, they can suggest us what to do
next. In such situation they can show sen-
tences describing the next instruction on a dis-
play or speak it.
On this background we propose a method
for generating instruction texts from a flow
graph representation for a series of procedures.
Among various genres of procedural texts, we
choose cooking recipes, because they are one of
the most familiar procedural texts for the pub-
lic. In addition, a computerized help system
proposed by Hashimoto et al. (2008) called
smart kitchen is becoming more and more re-
alistic. Thus we try to generate cooking pro-
cedural texts from a formal representation for
a series of preparation instructions of a dish.
As the formal representation, we adopt the
flow graph representation (Hamada et al.,
2000; Mori et al., 2014), in which the vertices
and the arcs correspond to important objects
?His current affiliation is Cybozu Inc., Koraku 1-4-
14, Bunkyo, Tokyo, Japan.
or actions in cooking and relationships among
them, respectively. We use the flow graphs as
the input and the text parts as the references
for evaluation.
Our generation method first automatically
compiles a set of templates, which we call the
skeleton pool, from a huge number of real pro-
cedural sentences. Then it decomposes the in-
put flow graph into a sequence of subtrees that
are suitable for a sentence. Finally it converts
subtrees into natural language sentences.
2 Recipe Flow Graph Corpus
The input of our LNG system is the mean-
ing representation (Mori et al., 2014) for cook-
ing instructions in a recipe. A recipe con-
sists of three parts: a title, an ingredient list,
and sentences describing cooking instructions
(see Figure 1). The meaning of the instruc-
tion sentences is represented by a directed
acyclic graph (DAG) with a root (the final
dish) as shown in Figure 2. Its vertices have
a pair of an important word sequence in the
recipe and its type called a recipe named en-
tity (NE)1. And its arcs denote relationships
between them. The arcs are also classified into
some types. In this paper, however, we do
not use arc types for text generation, because
we want our system to be capable of generat-
ing sentences from flow graphs output by an
automatic video recognition system2 or those
drawn by internet users.
Each vertex of a flow graph has an NE com-
posed of a word sequence in the text and its
type such as food, tool, action, etc. Table 3
1Although the label set contains verb phrases, they
are called named entities.
2By computer vision techniques such as (Regneri et
al., 2013) we may be able to figure out what action
a person takes on what objects. But it is difficult to
distinguish the direct object and the indirect object,
for example.
118
1. ??????????
(In a Dutch oven, heat oil.)
?????????????????
(Add celery, green onions, and garlic.)
????????
(Cook for about 1 minute.)
2. ?????????????????
???????????????
(Add broth, water, macaroni, and pepper,
and simmer until the pasta is tender.)
3. ???????????
(Sprinkle the snipped sage.)
Figure 1: A recipe example. The sentences are
one of the ideal outputs of our problem. They
are also used as the reference in evaluation.
lists all of the type labels along with the aver-
age numbers of occurrences in a recipe text
and examples. The word sequences of ver-
bal NEs do not include their inflectional end-
ings. From the definition we can say that the
content words are included in the flow graph
representation. Thus an NLG system has to
decide their order and generate the function
words (including inflectional endings for verbs)
to connect them to form a sentence.
3 Recipe Text Generation
The problem in this paper is generating a pro-
cedural text for cooking (ex. Figure 1) from a
recipe flow graph (ex. Figure 2).
Our method is decomposed into two mod-
ules. In this section, we explain them in detail.
3.1 Skeleton Pool Compilation
Before the run time, we first prepare a skele-
ton pool. A skeleton pool is a collection of
skeleton sentences, or skeletons for short, and
a skeleton is a sentence in which NEs have
been replaced with NE tags. The skeletons
are similar to the so-called templates and the
main difference is that the skeletons are auto-
matically converted from real sentences. The
following is the process to prepare a skeleton
pool.
1. Crawl cooking procedural sentences from
recipe sites.
2. Segment sentences into words by a word
segmenter KyTea (Neubig et al., 2011).
Then recognize recipe NEs by an NE rec-
ognizer PWNER (Mori et al., 2012).
3. Replace the NE instances in the sentences
with NE tags.
Figure 2: The flow graph of the example
recipe.
Table 3: Named entity tags with average fre-
quence per recipe.
NE tag Meaning Freq.
F Food 11.87
T Tool 3.83
D Duration 0.67
Q Quantity 0.79
Ac Action by the chef 13.83
Af Action by foods 2.04
Sf State of foods 3.02
St State of tools 0.30
We store skeletons with a key which is the se-
quence of the NE tags in the order of their
occurrence.
3.2 Sentence Planning
Our sentence planner produces a sequence of
subtrees each of which corresponds to a sen-
tence. There are two conditions.
Cond. 1 Each subtree has an Ac as its root.
Cond. 2 Every vertex is included in at least
one subtree.
As a strategy for enumerating subtrees given a
flow graph, we choose the following algorithm.
1. search for an Ac vertex by the depth first
search (DFS),
2. each time it finds an Ac, return the largest
subtree which has an Ac as its root and
contains only unvisited vertices.
3. set the visited-mark to the vertices con-
tained in the returned subtree,
4. go back to 1 unless all the vertices are
marked as visited.
In DFS, we choose a child vertex randomly
because a recipe flow graph is unordered.
119
Table 1: Corpus specifications.
Usage #Recipes #Sent. #NEs #Words #Char.
Test 40 245 1,352 4,005 7,509
NER training 360 2,813 12,101 51,847 97,911
Skeleton pool 100,000 713,524 ?3,919,964 ?11,988,344 22,826,496
The numbers with asterisc are estimated values on the NLP result.
Table 2: Statistical results of various skeleton pool sizes.
No. of sentences used for 2,769 11,077 44,308 177,235 708,940
skeleton pool compilation (1/256) (1/64) (1/16) (1/4) (1/1)
No. of uncovered subtrees 52 27 17 9 4
Average no. of skeletons 37.4 124.3 450.2 1598.1 5483.3
BLEU 11.19 11.25 12.86 13.12 13.76
3.3 Sentence Generation
Given a subtree sequence, our text realizer
generates a sentence by the following steps.
1. Collect skeletons from the pool whose NE
key matches the NE tag sequence speci-
fied by the subtree3.
2. Select the skeleton that maximize a scor-
ing function among collected ones. As the
first trial we use the frequency of skeletons
in the pool as the scoring function.
3. Replace each NE in the skeleton with the
word sequence of the corresponding NE in
the subtree.
4 Evaluation
We conducted experiments generating texts
from flow graphs. In this section, we report
the coverage and the sentence quality.
4.1 Experimental Settings
The recipe flow graph corpus (Mori et al.,
2014) contains 200 recipes. We randomly se-
lected 40 flow graphs as the test data from
which we generate texts. The other 160 recipes
were used to train the NE recognizer PWNER
(Mori et al., 2012) with 200 more recipes that
we annotated with NE tags. To compile the
skeleton pool we crawled 100,000 recipes con-
taining 713,524 sentences (see Table 1).
4.2 Skeleton Pool Coverage
First we counted the numbers of the skeletons
that matches with a subtree (Step 1 in Subsec-
tion 3.3) for all the subtrees in the test set by
3This part is language dependent. Since Japanese is
SOV language, the instance of Ac is placed at the last
of the sentence to be generated. Languages of other
types like English may need some rules to change the
NE tag order specified by the subtree into the proper
sentence element order.
changing the number of the recipe sentences
used for the skeleton pool compilation.
Table 2 shows the numbers of subtrees that
do not have any matching skeleton in the pool
(uncovered subtrees) and the average number
of skeletons in the pool for a subtree. From
the results shown in the table we can say that
when we use 100,000 recipes for the skeleton
compilation, our method can generate a sen-
tence for 98.4% subtrees. And the table says
that we can halve the number of uncovered
subtrees by using about four times more sen-
tences. The average number of the skeletons
says that we have enough skeletons in average
to try more sophisticated scoring functions.
4.3 Text Quality
To measure the quality of generated texts, we
first calculated the BLEU (N = 4) (Papineni
et al., 2002) with taking the original recipe
texts as the references. The unit in our case
is a sequence of sentences for a dish. Table 2
shows the average BLEU for all the test set.
The result says that the more sentences we use
for the skeleton pool compilation, the better
the generated sentences become.
The absolute BLEU score, however, does
not tell much about the quality of generated
texts. As it is well known, we can sometimes
change the instruction order in dish prepa-
ration. Therefore we conducted a subjective
evaluation in addition. We asked four evalu-
ators to read 10 texts generated from 10 flow
graphs and answer the following questions.
Q1. How many ungrammatical two-word se-
quences does the text contain?
Q2. How many ambiguous wordings do you
find in the text?
Then we show the evaluators the original
recipe text and asked the following question.
120
Table 4: Result of text quality survey on 10 recipe texts.
Evaluator 1 Evaluator 2 Evaluator 3 Evaluator 4
BLEU Q1 Q2 Q3 Q1 Q2 Q3 Q1 Q2 Q3 Q1 Q2 Q3
6.50 13 2 4 11 0 3 12 0 2 7 1 2
7.99 7 2 2 5 2 2 7 1 1 4 2 2
10.09 18 2 4 15 2 1 17 4 1 11 4 2
11.60 24 1 4 13 2 4 18 2 4 13 1 2
13.35 6 1 4 6 0 4 7 1 5 4 1 2
14.70 16 1 4 12 2 4 12 0 3 6 2 2
16.76 9 2 3 6 1 3 7 1 3 5 3 2
19.65 8 2 5 6 1 1 4 1 4 4 2 4
22.85 18 1 4 15 2 5 12 2 2 7 3 2
31.35 5 1 5 5 0 4 5 1 3 5 1 4
Ave. 12.4 1.5 3.9 9.4 1.2 3.1 10.1 1.3 2.8 6.6 2.0 2.4
PCC ?0.30 ?0.46 +0.57 ?0.24 ?0.24 +0.36 ?0.46 ?0.04 +0.26 ?0.29 ?0.04 +0.70
PPC stands for Pearson correlation coefficient.
Q3. Will the dish be the same as the origi-
nal recipe when you cook according to the
generated text? Choose the one among 5:
completely, 4: almost, 3: partly, 2: differ-
ent, or 1: unexecutable.
Table 4 shows the result. The generated texts
contain 14.5 sentences in average. The an-
swers to Q1 tell that there are many grammat-
ical errors. We need some mechanism that se-
lects more appropriate skeletons. The number
of ambiguous wordings, however, is very low.
The reason is that the important words are
given along with the subtrees. The average of
the answer to Q3 is 3.05. This result says that
the dish will be partly the same as the original
recipe. There is a room for improvement.
Finally, let us take a look at the correlation
of the result of three Qs with BLEU. The num-
bers of grammatical errors, i.e. the answers
to Q1, has a stronger correlation with BLEU
than those of Q2 asking the semantic quality.
These are consistent with the intuition. The
answer to Q3, asking overall text quality, has
the strongest correlation with BLEU on aver-
age among all the questions. Therefore we can
say that for the time being the objective eval-
uation by BLEU is sufficient to measure the
performance of various improvements.
5 Related Work
Our method can be seen a member of
template-based text generation systems (Re-
iter, 1995). Contrary to the ordinary
template-based approach, our method first au-
tomatically compiles a set of templates, which
we call skeleton pool, by running an NE tagger
on the real texts. This allows us to cope with
the coverage problem with keeping the advan-
tage of the template-based approach, ability
to prevent from generating incomprehensible
sentence structures. The main contribution of
this paper is to use an accurate NE tagger to
convert sentences into skeletons, to show the
coverages of the skeleton pool, and to evaluate
the method in a realistic situation.
Among many applications of our method, a
concrete one is the smart kitchen (Hashimoto
et al., 2008), a computerized cooking help sys-
tem which watches over the chef by the com-
puter vision (CV) technologies etc. and sug-
gests the chef the next action to be taken or
a good way of doing it in a casual manner. In
this application, the text generation module
make a sentence from a subtree specified by
the process supervision module.
There are some other interesting applica-
tions: a help system for internet users to write
good sentences, machine translation of a recipe
in a different language represented as a flow
graph, or automatic recipe generation from
a cooking video based on CV and NLP re-
searches such as (Regneri et al., 2013; Ya-
makata et al., 2013; Yu and Siskind, 2013).
6 Conclusion
In this paper, we explained and evaluated our
method for generating a procedural text from
a flow graph representation. The experimental
results showed that our method is feasible es-
pecially when we have huge number of real sen-
tences and that some more sophistications are
possible to generate more natural sentences.
121
Acknowledgments
This work was supported by JSPS Grants-
in-Aid for Scientific Research Grant Numbers
26280084, 24240030, and 26280039.
References
Reiko Hamada, Ichiro Ide, Shuichi Sakai, and Hide-
hiko Tanaka. 2000. Structural analysis of cook-
ing preparation steps in japanese. In Proceedings
of the fifth international workshop on Informa-
tion retrieval with Asian languages, number 8 in
IRAL ?00, pages 157?164.
Atsushi Hashimoto, Naoyuki Mori, Takuya Fu-
natomi, Yoko Yamakata, Koh Kakusho, and
Michihiko Minoh. 2008. Smart kitchen: A user
centric cooking support system. In Proceedings
of the 12th Information Processing and Manage-
ment of Uncertainty in Knowledge-Based Sys-
tems, pages 848?854.
Shinsuke Mori, Tetsuro Sasada, Yoko Yamakata,
and Koichiro Yoshino. 2012. A machine learn-
ing approach to recipe text processing. In Pro-
ceedings of Cooking with Computer workshop.
Shinsuke Mori, Hirokuni Maeta, Yoko Yamakata,
and Tetsuro Sasada. 2014. Flow graph cor-
pus from recipe texts. In Proceedings of the
Nineth International Conference on Language
Resources and Evaluation.
Graham Neubig, Yosuke Nakata, and Shinsuke
Mori. 2011. Pointwise prediction for robust,
adaptable japanese morphological analysis. In
Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages
311?318.
Michaela Regneri, Marcus Rohrbach, Dominikus
Wetzel, Stefan Thater, Bernt Schiele, and Man-
fred Pinkal. 2013. Grounding action descrip-
tions in videos. Transactions of the Association
for Computational Linguistics, 1(Mar):25?36.
Ehud Reiter. 1995. Nlg vs. templates. In Pro-
ceedings of the the Fifth European Workshop on
Natural Language Generation, pages 147?151.
Yoko Yamakata, Shinji Imahori, Yuichi Sugiyama,
Shinsuke Mori, and Katsumi Tanaka. 2013.
Feature extraction and summarization of recipes
using flow graph. In Proceedings of the 5th In-
ternational Conference on Social Informatics,
LNCS 8238, pages 241?254.
Haonan Yu and Jeffrey Mark Siskind. 2013.
Grounded language learning from video de-
scribed with sentences. In Proceedings of the
51st Annual Meeting of the Association for
Computational Linguistics.
122
