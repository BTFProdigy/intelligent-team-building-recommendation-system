Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 63?68,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PARMA: A Predicate Argument Aligner
Travis Wolfe, Benjamin Van Durme, Mark Dredze, Nicholas Andrews,
Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder,
Jonathan Weese, Tan Xu?, and Xuchen Yao
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, Maryland USA
?University of Maryland, College Park, Maryland USA
Abstract
We introduce PARMA, a system for cross-
document, semantic predicate and argu-
ment alignment. Our system combines a
number of linguistic resources familiar to
researchers in areas such as recognizing
textual entailment and question answering,
integrating them into a simple discrimina-
tive model. PARMA achieves state of the
art results on an existing and a new dataset.
We suggest that previous efforts have fo-
cussed on data that is biased and too easy,
and we provide a more difficult dataset
based on translation data with a low base-
line which we beat by 17% F1.
1 Introduction
A key step of the information extraction pipeline
is entity disambiguation, in which discovered en-
tities across many sentences and documents must
be organized to represent real world entities. The
NLP community has a long history of entity dis-
ambiguation both within and across documents.
While most information extraction work focuses
on entities and noun phrases, there have been a
few attempts at predicate, or event, disambigua-
tion. Commonly a situational predicate is taken to
correspond to either an event or a state, lexically
realized in verbs such as ?elect? or nominaliza-
tions such as ?election?. Similar to entity coref-
erence resolution, almost all of this work assumes
unanchored mentions: predicate argument tuples
are grouped together based on coreferent events.
The first work on event coreference dates back to
Bagga and Baldwin (1999). More recently, this
task has been considered by Bejan and Harabagiu
(2010) and Lee et al (2012). As with unanchored
entity disambiguation, these methods rely on clus-
tering methods and evaluation metrics.
Another view of predicate disambiguation seeks
to link or align predicate argument tuples to an ex-
isting anchored resource containing references to
events or actions, similar to anchored entity dis-
ambiguation (entity linking) (Dredze et al, 2010;
Han and Sun, 2011). The most relevant, and per-
haps only, work in this area is that of Roth and
Frank (2012) who linked predicates across docu-
ment pairs, measuring the F1 of aligned pairs.
Here we present PARMA, a new system for pred-
icate argument alignment. As opposed to Roth and
Frank, PARMA is designed as a a trainable plat-
form for the incorporation of the sort of lexical se-
mantic resources used in the related areas of Rec-
ognizing Textual Entailment (RTE) and Question
Answering (QA). We demonstrate the effective-
ness of this approach by achieving state of the art
performance on the data of Roth and Frank despite
having little relevant training data. We then show
that while the ?lemma match? heuristic provides a
strong baseline on this data, this appears to be an
artifact of their data creation process (which was
heavily reliant on word overlap). In response, we
evaluate on a new and more challenging dataset for
predicate argument alignment derived from multi-
ple translation data. We release PARMA as a new
framework for the incorporation and evaluation of
new resources for predicate argument alignment.1
2 PARMA
PARMA (Predicate ARguMent Aligner) is a
pipelined system with a wide variety of features
used to align predicates and arguments in two doc-
uments. Predicates are represented as mention
spans and arguments are represented as corefer-
ence chains (sets of mention spans) provided by
in-document coreference resolution systems such
as included in the Stanford NLP toolkit. Results
indicated that the chains are of sufficient quality
so as not to limit performance, though future work
1https://github.com/hltcoe/parma
63
RF
? Australian [police]1 have [arrested]2 a man in the western city of Perth over an alleged [plot]3 to [bomb]4 Israeli diplomatic
[buildings]5 in the country , police and the suspect s [lawyer]6 [said]7
? Federal [police]1 have [arrested]2 a man over an [alleged]5 [plan]3 to [bomb]4 Israeli diplomatic [posts]8 in Australia , the
suspect s [attorney]6 [said]7 Tuesday
LDC MTC
? As I [walked]1 to the [veranda]2 side , I [saw]2 that a [tent]3 is being decorated for [Mahfil-e-Naat]4 -LRB- A [get-together]5
in which the poetic lines in praise of Prophet Mohammad are recited -RRB-
? I [came]1 towards the [balcony]2 , and while walking over there I [saw]2 that a [camp]3 was set up outside for the [Naatia]4
[meeting]5 .
Figure 1: Example of gold-standard alignment pairs from Roth and Frank?s data set and our data set
created from the LDC?s Multiple Translation Corpora. The RF data set exhibits high lexical overlap,
where most of the alignments are between identical words like police-police and said-said. The LDC
MTC was constructed to increase lexical diversity, leading to more challenging alignments like veranda-
balcony and tent-camp
may relax this assumption.
We refer to a predicate or an argument as an
?item? with type predicate or argument. An align-
ment between two documents is a subset of all
pairs of items in either documents with the same
type.2 We call the two documents being aligned
the source document S and the target document
T . Items are referred to by their index, and ai,j is a
binary variable representing an alignment between
item i in S and item j in T . A full alignment is an
assignment ~a = {aij : i ? NS , j ? NT }, where
NS and NT are the set of item indices for S and T
respectively.
We train a logistic regression model on exam-
ple alignmentsand maximize the likelihood of a
document alignment under the assumption that the
item alignments are independent. Our objective
is to maximize the log-likelihood of all p(S, T )
with an L1 regularizer (with parameter ?). After
learning model parameters w by regularized max-
imum likelihood on training data, we introducing
a threshold ? on alignment probabilities to get a
classifier. We perform line search on ? and choose
the value that maximizes F1 on dev data. Train-
ing was done using the Mallet toolkit (McCallum,
2002).
2.1 Features
The focus of PARMA is the integration of a diverse
range of features based on existing lexical seman-
tic resources. We built PARMA on a supervised
framework to take advantage of this wide variety
of features since they can describe many different
correlated aspects of generation. The following
features cover the spectrum from high-precision
2Note that type is not the same thing as part of speech: we
allow nominal predicates like ?death?.
to high-recall. Each feature has access to the pro-
posed argument or predicate spans to be linked and
the containing sentences as context. While we use
supervised learning, some of the existing datasets
for this task are very small. For extra training data,
we pool material from different datasets and use
the multi-domain split feature space approach to
learn dataset specific behaviors (Daume?, 2007).
Features in general are defined over mention
spans or head tokens, but we split these features
to create separate feature-spaces for predicates and
arguments.3
For argument coref chains we heuristically
choose a canonical mention to represent each
chain, and some features only look at this canon-
ical mention. The canonical mention is cho-
sen based on length,4 information about the head
word,5 and position in the document.6 In most
cases, coref chains that are longer than one are
proper nouns and the canonical mention is the first
and longest mention (outranking pronominal ref-
erences and other name shortenings).
PPDB We use lexical features from the Para-
phrase Database (PPDB) (Ganitkevitch et al,
2013). PPDB is a large set of paraphrases ex-
tracted from bilingual corpora using pivoting tech-
niques. We make use of the English lexical portion
which contains over 7 million rules for rewriting
terms like ?planet? and ?earth?. PPDB offers a
variety of conditional probabilities for each (syn-
chronous context free grammar) rule, which we
3While conceptually cleaner, In practice we found this
splitting to have no impact on performance.
4in tokens, not counting some words like determiners and
auxiliary verbs
5like its part of speech tag and whether the it was tagged
as a named entity
6mentions that appear earlier in the document and earlier
in a given sentence are given preference
64
treat as independent experts. For each of these rule
probabilities (experts), we find all rules that match
the head tokens of a given alignment and have a
feature for the max and harmonic mean of the log
probabilities of the resulting rule set.
FrameNet FrameNet is a lexical database based
on Charles Fillmore?s Frame Semantics (Fill-
more, 1976; Baker et al, 1998). The database
(and the theory) is organized around seman-
tic frames that can be thought of as descrip-
tions of events. Frames crucially include spec-
ification of the participants, or Frame Elements,
in the event. The Destroying frame, for in-
stance, includes frame elements Destroyer or
Cause Undergoer. Frames are related to other
frames through inheritance and perspectivization.
For instance the frames Commerce buy and
Commerce sell (with respective lexical real-
izations ?buy? and ?sell?) are both perspectives of
Commerce goods-transfer (no lexical re-
alizations) which inherits from Transfer (with
lexical realization ?transfer?).
We compute a shortest path between headwords
given edges (hypernym, hyponym, perspectivized
parent and child) in FrameNet and bucket by dis-
tance to get features. We also have a binary feature
for whether two tokens evoke the same frame.
TED Alignments Given two predicates or argu-
ments in two sentences, we attempt to align the
two sentences they appear in using a Tree Edit
Distance (TED) model that aligns two dependency
trees, based on the work described by (Yao et al,
2013). We represent a node in a dependency tree
with three fields: lemma, POS tag and the type
of dependency relation to the node?s parent. The
TED model aligns one tree with the other using
the dynamic programming algorithm of Zhang and
Shasha (1989) with three predefined edits: dele-
tion, insertion and substitution, seeking a solution
yielding the minimum edit cost. Once we have
built a tree alignment, we extract features for 1)
whether the heads of the two phrases are aligned
and 2) the count of how many tokens are aligned
in both trees.
WordNet WordNet (Miller, 1995) is a database
of information (synonyms, hypernyms, etc.) per-
taining to words and short phrases. For each entry,
WordNet provides a set of synonyms, hypernyms,
etc. Given two spans, we use WordNet to deter-
mine semantic similarity by measuring how many
synonym (or other) edges are needed to link two
terms. Similar words will have a short distance.
For features, we find the shortest path linking the
head words of two mentions using synonym, hy-
pernym, hyponym, meronym, and holonym edges
and bucket the length.
String Transducer To represent similarity be-
tween arguments that are names, we use a stochas-
tic edit distance model. This stochastic string-to-
string transducer has latent ?edit? and ?no edit?
regions where the latent regions allow the model
to assign high probability to contiguous regions of
edits (or no edits), which are typical between vari-
ations of person names. In an edit region, param-
eters govern the relative probability of insertion,
deletion, substitution, and copy operations. We
use the transducer model of Andrews et al (2012).
Since in-domain name pairs were not available, we
picked 10,000 entities at random from Wikipedia
to estimate the transducer parameters. The entity
labels were used as weak supervision during EM,
as in Andrews et al (2012).
For a pair of mention spans, we compute the
conditional log-likelihood of the two mentions go-
ing both ways, take the max, and then bucket to get
binary features. We duplicate these features with
copies that only fire if both mentions are tagged as
PER, ORG or LOC.
3 Evaluation
We consider three datasets for evaluating PARMA.
For richer annotations that include lemmatiza-
tions, part of speech, NER, and in-doc corefer-
ence, we pre-processed each of the datasets using
tools7 similar to those used to create the Annotated
Gigaword corpus (Napoles et al, 2012).
Extended Event Coreference Bank Based on
the dataset of Bejan and Harabagiu (2010), Lee et
al. (2012) introduced the Extended Event Coref-
erence Bank (EECB) to evaluate cross-document
event coreference. EECB provides document clus-
ters, within which entities and events may corefer.
Our task is different from Lee et al but we can
modify the corpus setup to support our task. To
produce source and target document pairs, we se-
lect the first document within every cluster as the
source and each of the remaining documents as
target documents (i.e. N ? 1 pairs for a cluster
of size N ). This yielded 437 document pairs.
Roth and Frank The only existing dataset for
our task is from Roth and Frank (2012) (RF), who
7https://github.com/cnap/anno-pipeline
65
annotated documents from the English Gigaword
Fifth Edition corpus (Parker et al, 2011). The data
was generated by clustering similar news stories
from Gigaword using TF-IDF cosine similarity of
their headlines. This corpus is small, containing
only 10 document pairs in the development set and
60 in the test set. To increase the training size,
we train PARMA with 150 randomly selected doc-
ument pairs from both EECB and MTC, and the
entire dev set from Roth and Frank using multi-
domain feature splitting. We tuned the threshold
? on the Roth and Frank dev set, but choose the
regularizer ? based on a grid search on a 5-fold
version of the EECB dataset.
Multiple Translation Corpora We constructed
a new predicate argument alignment dataset
based on the LDC Multiple Translation Corpora
(MTC),8 which consist of multiple English trans-
lations for foreign news articles. Since these mul-
tiple translations are semantically equivalent, they
provide a good resource for aligned predicate ar-
gument pairs. However, finding good pairs is a
challenge: we want pairs with significant overlap
so that they have predicates and arguments that
align, but not documents that are trivial rewrites
of each other. Roth and Frank selected document
pairs based on clustering, meaning that the pairs
had high lexical overlap, often resulting in mini-
mal rewrites of each other. As a result, despite ig-
noring all context, their baseline method (lemma-
alignment) worked quite well.
To create a more challenging dataset, we se-
lected document pairs from the multiple transla-
tions that minimize the lexical overlap (in En-
glish). Because these are translations, we know
that there are equivalent predicates and arguments
in each pair, and that any lexical variation pre-
serves meaning. Therefore, we can select pairs
with minimal lexical overlap in order to create
a system that truly stresses lexically-based align-
ment systems.
Each document pair has a correspondence be-
tween sentences, and we run GIZA++ on these
sentences to produce token-level alignments. We
take all aligned nouns as arguments and all aligned
verbs (excluding be-verbs, light verbs, and report-
ing verbs) as predicates. We then add negative ex-
amples by randomly substituting half of the sen-
tences in one document with sentences from an-
8LDC2010T10, LDC2010T11, LDC2010T12,
LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01,
LDC2003T18, and LDC2005T05
0.3 0.4 0.5 0.6 0.7 0.8
0.0
0.2
0.4
0.6
0.8
1.0
Performance vs Lexical Overlap
Doc-pair Cosine Similarity
F1
Figure 2: We plotted the PARMA?s performance on
each of the document pairs. Red squares show the
F1 for individual document pairs drawn from Roth
and Frank?s data set, and black circles show F1 for
our Multiple Translation Corpora test set. The x-
axis represents the cosine similarity between the
document pairs. On the RF data set, performance
is correlated with lexical similarity. On our more
lexically diverse set, this is not the case. This
could be due to the fact that some of the docu-
ments in the RF sets are minor re-writes of the
same newswire story, making them easy to align.
other corpus, guaranteed to be unrelated. The
amount of substitutions we perform can vary the
?relatedness? of the two documents in terms of
the predicates and arguments that they talk about.
This reflects our expectation of real world data,
where we do not expect perfect overlap in predi-
cates and arguments between a source and target
document, as you would in translation data.
Lastly, we prune any document pairs that have
more than 80 predicates or arguments or have a
Jaccard index on bags of lemmas greater than 0.5,
to give us a dataset of 328 document pairs.
Metric We use precision, recall, and F1. For the
RF dataset, we follow Roth and Frank (2012) and
Cohn et al (2008) and evaluate on a version of F1
that considers SURE and POSSIBLE links, which
are available in the RF data. Given an alignment
to be scored A and a reference alignment B which
contains SURE and POSSIBLE links, Bs andBp re-
spectively, precision and recall are:
P = |A ?Bp||A| R =
|A ?Bs|
|Bs|
(1)
66
F1 P R
EECB lemma 63.5 84.8 50.8
PARMA 74.3 80.5 69.0
RF lemma 48.3 40.3 60.3
Roth and Frank 54.8 59.7 50.7
PARMA 57.6 52.4 64.0
MTC lemma 42.1 51.3 35.7
PARMA 59.2 73.4 49.6
Table 1: PARMA outperforms the baseline lemma
matching system on the three test sets, drawn from
the Extended Event Coreference Bank, Roth and
Frank?s data, and our set created from the Multiple
Translation Corpora. PARMA achieves a higher F1
and recall score than Roth and Frank?s reported
result.
and F1 as the harmonic mean of the two. Results
for EECB and MTC reflect 5-fold cross validation,
and RF uses the given dev/test split.
Lemma baseline Following Roth and Frank we
include a lemma baseline, in which two predicates
or arguments align if they have the same lemma.9
4 Results
On every dataset PARMA significantly improves
over the lemma baselines (Table 1). On RF,
compared to Roth and Frank, the best published
method for this task, we also improve, making
PARMA the state of the art system for this task.
Furthermore, we expect that the smallest improve-
ments over Roth and Frank would be on RF, since
there is little training data. We also note that com-
pared to Roth and Frank we obtain much higher
recall but lower precision.
We also observe that MTC was more challeng-
ing than the other datasets, with a lower lemma
baseline. Figure 2 shows the correlation between
document similarity and document F1 score for
RF and MTC. While for RF these two measures
are correlated, they are uncorrelated for MTC. Ad-
ditionally, there is more data in the MTC dataset
which has low cosine similarity than in RF.
5 Conclusion
PARMA achieves state of the art performance on
three datasets for predicate argument alignment.
It builds on the development of lexical semantic
resources and provides a platform for learning to
utilize these resources. Additionally, we show that
9We could not reproduce lemma from Roth and Frank
(shown in Table 1) due to a difference in lemmatizers. We ob-
tained 55.4; better than their system but worse than PARMA.
task difficulty can be strongly tied to lexical simi-
larity if the evaluation dataset is not chosen care-
fully, and this provides an artificially high baseline
in previous work. PARMA is robust to drops in lex-
ical similarity and shows large improvements in
those cases. PARMA will serve as a useful bench-
mark in determining the value of more sophis-
ticated models of predicate-argument alignment,
which we aim to address in future work.
While our system is fully supervised, and thus
dependent on manually annotated examples, we
observed here that this requirement may be rela-
tively modest, especially for in-domain data.
Acknowledgements
We thank JHU HLTCOE for hosting the winter
MiniSCALE workshop that led to this collabora-
tive work. This material is based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program). The U.S. Government
is authorized to reproduce and distribute reprints
for Governmental purposes. The views and con-
clusions contained in this publication are those of
the authors and should not be interpreted as repre-
senting official policies or endorsements of NSF,
DARPA, or the U.S. Government.
References
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Empirical Methods in Natural
Language Processing (EMNLP).
Amit Bagga and Breck Baldwin. 1999. Cross-
document event coreference: Annotations, exper-
iments, and observations. In Proceedings of the
Workshop on Coreference and its Applications,
pages 1?8. Association for Computational Linguis-
tics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
67
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
put. Linguist., 34(4):597?614, December.
Hal Daume?. 2007. Frustratingly easy domain adap-
tation. In Annual meeting-association for computa-
tional linguistics, volume 45, page 256.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Conference on
Computational Linguistics (Coling).
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, 280(1):20?
32.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 945?
954. Association for Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012, June.
Robert Parker, David Graff, Jumbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword fifth
edition.
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 218?
227, Montre?al, Canada, 7-8 June. Association for
Computational Linguistics.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer extraction as
sequence tagging with tree edit distance. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262, De-
cember.
68
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 181?186,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
I?m a Belieber:
Social Roles via Self-identification and Conceptual Attributes
Charley Beller, Rebecca Knowles, Craig Harman
Shane Bergsma
?
, Margaret Mitchell
?
, Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
?
University of Saskatchewan, Saskatoon, Saskatchewan Canada
?
Microsoft Research, Redmond, Washington USA
charleybeller@jhu.edu, rknowles@jhu.edu, craig@craigharman.net,
shane.a.bergsma@gmail.com, memitc@microsoft.com, vandurme@cs.jhu.edu
Abstract
Motivated by work predicting coarse-
grained author categories in social me-
dia, such as gender or political preference,
we explore whether Twitter contains infor-
mation to support the prediction of fine-
grained categories, or social roles. We
find that the simple self-identification pat-
tern ?I am a ? supports significantly
richer classification than previously ex-
plored, successfully retrieving a variety of
fine-grained roles. For a given role (e.g.,
writer), we can further identify character-
istic attributes using a simple possessive
construction (e.g., writer?s ). Tweets
that incorporate the attribute terms in first
person possessives (my ) are confirmed
to be an indicator that the author holds the
associated social role.
1 Introduction
With the rise of social media, researchers have
sought to induce models for predicting latent au-
thor attributes such as gender, age, and politi-
cal preferences (Garera and Yarowsky, 2009; Rao
et al, 2010; Burger et al, 2011; Van Durme,
2012b; Zamal et al, 2012). Such models are
clearly in line with the goals of both computa-
tional advertising (Wortman, 2008) and the grow-
ing area of computational social science (Conover
et al, 2011; Nguyen et al, 2011; Paul and Dredze,
2011; Pennacchiotti and Popescu, 2011; Moham-
mad et al, 2013) where big data and computa-
tion supplement methods based on, e.g., direct hu-
man surveys. For example, Eisenstein et al (2010)
demonstrated a model that predicted where an au-
thor was located in order to analyze regional dis-
tinctions in communication. While some users ex-
plicitly share their GPS coordinates through their
Twitter clients, having a larger collection of au-
tomatically identified users within a region was
preferable even though the predictions for any
given user were uncertain.
We show that media such as Twitter can sup-
port classification that is more fine-grained than
gender or general location. Predicting social roles
such as doctor, teacher, vegetarian, christian,
may open the door to large-scale passive surveys
of public discourse that dwarf what has been pre-
viously available to social scientists. For exam-
ple, work on tracking the spread of flu infections
across Twitter (Lamb et al, 2013) might be en-
hanced with a factor based on aggregate predic-
tions of author occupation.
We present two studies showing that first-
person social content (tweets) contains intuitive
signals for such fine-grained roles. We argue that
non-trivial classifiers may be constructed based
purely on leveraging simple linguistic patterns.
These baselines suggest a wide range of author
categories to be explored further in future work.
Study 1 In the first study, we seek to determine
whether such a signal exists in self-identification:
we rely on variants of a single pattern, ?I am a ?,
to bootstrap data for training balanced-class binary
classifiers using unigrams observed in tweet con-
tent. As compared to prior research that required
actively polling users for ground truth in order to
construct predictive models for demographic in-
formation (Kosinski et al, 2013), we demonstrate
that some users specify such properties publicly
through direct natural language.
Many of the resultant models show intuitive
strongly-weighted features, such as a writer be-
ing likely to tweet about a story, or an ath-
lete discussing a game. This demonstrates self-
identification as a viable signal in building predic-
tive models of social roles.
181
Role Tweet
artist I?m an Artist..... the last of a dying breed
belieber @justinbieber I will support you in ev-
erything you do because I am a belieber
please follow me I love you 30
vegetarian So glad I?m a vegetarian.
Table 1: Examples of self-identifying tweets.
# Role # Role # Role
29,924 little 5,694 man 564 champion
21,822 big ... ... 559 teacher
18,957 good 4,007 belieber 556 writer
13,069 huge 3,997 celebrity 556 awful
13,020 bit 3,737 virgin ... ...
12,816 fan 3,682 pretty 100 cashier
10,832 bad ... ... 100 bro
10,604 girl 2,915 woman ... ...
9,981 very 2,851 beast 10 linguist
... ... ... ... ... ...
Table 2: Number of self-identifying users per ?role?. While
rich in interesting labels, cases such as very highlight the pur-
poseful simplicity of the current approach.
Study 2 In the second study we exploit a com-
plementary signal based on characteristic con-
ceptual attributes of a social role, or concept
class (Schubert, 2002; Almuhareb and Poesio,
2004; Pas?ca and Van Durme, 2008). We identify
typical attributes of a given social role by collect-
ing terms in the Google n-gram corpus that occur
frequently in a possessive construction with that
role. For example, with the role doctor we extract
terms matching the simple pattern ?doctor?s ?.
2 Self-identification
All role-representative users were drawn from
the free public 1% sample of the Twitter Fire-
hose, over the period 2011-2013, from the sub-
set that selected English as their native language
(85,387,204 unique users). To identify users of
a particular role, we performed a case-agnostic
search of variants of a single pattern: I am a(n)
, and I?m a(n) , where all single tokens filling
the slot were taken as evidence of the author self-
reporting for the given ?role?. Example tweets can
be seen in Table 1, examples of frequency per role
in Table 2. This resulted in 63,858 unique roles
identified, of which 44,260 appeared only once.
1
We manually selected a set of roles for fur-
ther exploration, aiming for a diverse sample
across: occupation (e.g., doctor, teacher), family
(mother), disposition (pessimist), religion (chris-
1
Future work should consider identifying multi-word role
labels (e.g., Doctor Who fan, or dog walker).
0.60
0.65
0.70
0.75
0.80
ll
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
l
l
direc
tione
r
belie
ber
optim
ist
sold
ier
soph
omo
re
pess
imis
t
ran
dom
.0
danc
er
hips
ter
ran
dom
.2
sing
er
fresh
man
mo
ther
ran
dom
.1
chee
rlead
er
rapp
er
chris
tian
artis
t
sm
oker acto
r
vege
taria
n
wo
ma
n
athle
te
geek engi
neer
wa
itres
s
nur
se
ma
n
stud
ent doct
or poet writ
er
athe
ist
gran
dmalawy
er
teac
her
Role
Cha
nce 
of S
ucce
ss
Figure 1: Success rate for querying a user. Random.0,1,2
are background draws from the population, with the mean of
those three samples drawn horizontally. Tails capture 95%
confidence intervals.
tian), and ?followers? (belieber, directioner).
2
We filtered users via language ID (Bergsma et al,
2012) to better ensure English content.
3
For each selected role, we randomly sampled up
to 500 unique self-reporting users and then queried
Twitter for up to 200 of their recent publicly
posted tweets.
4
These tweets served as represen-
tative content for that role, with any tweet match-
ing the self-reporting patterns filtered. Three sets
of background populations were extracted based
on randomly sampling users that self-reported En-
glish (post-filtered via LID).
Twitter users are empowered to at any time
delete, rename or make private their accounts.
Any given user taken to be representative based on
a previously posted tweet may no longer be avail-
able to query on. As a hint of the sort of user stud-
ies one might explore given access to social role
prediction, we see in Figure 1 a correlation be-
tween self-reported role and the chance of an ac-
count still being publicly visible, with roles such
as belieber and directioner on the one hand, and
doctor and teacher on the other.
The authors examined the self-identifying tweet
of 20 random users per role. The accuracy of the
self-identification pattern varied across roles and
is attributable to various factors including quotes,
e.g. @StarTrek Jim, I?m a DOCTOR not a down-
load!. While these samples are small (and thus
estimates of quality come with wide variance), it
2
Those that follow the music/life of the singer Justin
Bieber and the band One Direction, respectively.
3
This removes users that selected English as their primary
language, used a self-identification phrase, e.g. I am a be-
lieber, but otherwise tended to communicate in non-English.
4
Roughly half of the classes had less than 500 self-
reporting users in total, in those cases we used all matches.
182
actorartistatheist
athletebeliebercheerleader
christiandancerdirectioner
doctorengineerfreshman
geekgrandmahipster
lawyermanmother
nurseoptimistpessimist
poetrappersinger
smokersoldiersophomore
studentteachervegetarian
waitresswomanwriter
0 5 10 15
Figure 2: Valid self-identifying tweets from sample of 20.
is noteworthy that a non-trivial number for each
were judged as actually self-identifying.
Indicative Language Most work in user clas-
sification relies on featurizing language use,
most simply through binary indicators recording
whether a user did or did not use a particular word
in a history of n tweets. To explore whether lan-
guage provides signal for future work in fine-grain
social role prediction, we constructed a set of ex-
periments, one per role, where training and test
sets were balanced between users from a random
background sample and self-reported users. Base-
line accuracy in these experiments was thus 50%.
Each training set had a target of 600 users (300
background, 300 self-identified); for those roles
with less than 300 users self-identifying, all users
were used, with an equal number background. We
used the Jerboa (Van Durme, 2012a) platform
to convert data to binary feature vectors over a un-
igram vocabulary filtered such that the minimum
frequency was 5 (across unique users). Training
and testing was done with a log-linear model via
LibLinear (Fan et al, 2008). We used the pos-
itively annotated data to form test sets, balanced
with data from the background set. Each test set
had a theoretical maximum size of 40, but for sev-
eral classes it was in the single digits (see Fig-
ure 2). Despite the varied noisiness of our simple
pattern-bootstrapped training data, and the small
size of our annotated test set, we see in Figure 3
that we are able to successfully achieve statisti-
cally significant predictions of social role for the
majority of our selected examples.
Table 3 highlights examples of language indica-
tive of role, as determined by the most positively
weighted unigrams in the classification experi-
0.2
0.4
0.6
0.8
1.0
l
l l l
l
ll
l ll
l
l l
l
ll
l ll
l
l
ll l
l
l
l
l l l
l
l
l
sold
ier
wo
ma
n
pess
imis
t
chris
tian
gran
dma
nur
se
rapp
er
ma
n poet
chee
rlead
er
stud
ent
engi
neer acto
r
teac
her
vege
taria
n
mo
ther sing
er
lawy
er
optim
ist
wa
itres
s
sm
oker hips
ter doct
or
danc
er
artis
t
fresh
man
direc
tione
r
geek
soph
omo
re
athe
ist
athle
te
writ
er
belie
ber
Role
Acc
urac
y
Figure 3: Accuracy in classifying social roles.
Role :: Feature ( Rank)
artist morning, summer, life, most, amp, studio
atheist fuck, fucking, shit, makes, dead, ..., religion
19
athlete lol, game, probably, life, into, ..., team
9
belieber justin, justinbeiber, believe, beliebers, bieber
cheerleader cheer, best, excited, hate, mom, ..., prom
16
christian lol, ..., god
12
, pray
13
, ..., bless
17
, ..., jesus
20
dancer dance, since, hey, never, been
directioner harry, d, follow, direction, never, liam, niall
doctor sweet, oh, or, life, nothing
engineer (, then, since, may, ), test
9
, -
17
, =
18
freshman summer, homework, na, ..., party
19
, school
20
geek trying, oh, different, dead, been
grandma morning, baby, around, night, excited
hipster fucking, actually, thing, fuck, song
lawyer did, never, his, may, pretty, law, even, office
man man, away, ai, young, since
mother morning, take, fuck, fucking, trying
nurse lol, been, morning, ..., night
10
, nursing
11
, shift
13
optimist morning, enough, those, everything, never
poet feel, song, even, say, yo
rapper fuck, morning, lol, ..., mixtape
8
, songs
15
singer sing, song, music, lol, never
smoker fuck, shit, fucking, since, ass, smoke, weed
20
solider ai, beautiful, lol, wan, trying
sophmore summer, >, ..., school
11
, homework
12
student anything, summer, morning, since, actually
teacher teacher, morning, teach, ..., students
7
, ..., school
20
vegetarian actually, dead, summer, oh, morning
waitress man, try, goes, hate, fat
woman lol, into, woman, morning, never
writer write, story, sweet, very, working
Table 3: Most-positively weighted features per role, along
with select features within the top 20. Surprising mother
features come from ambigious self-identification, as seen in
tweets such as: I?m a mother f!cking starrrrr.
ment. These results qualitatively suggest many
roles under consideration may be teased out from a
background population by focussing on language
that follows expected use patterns. For example
the use of the term game by athletes, studio by
artists, mixtape by rappers, or jesus by Christians.
3 Characteristic Attributes
Bergsma and Van Durme (2013) showed that the
183
task of mining attributes for conceptual classes can
relate straightforwardly to author attribute predic-
tion. If one views a role, in their case gender, as
two conceptual classes, male and female, then ex-
isting attribute extraction methods for third-person
content (e.g., news articles) can be cheaply used to
create a set of bootstrapping features for building
classifiers over first-person content (e.g., tweets).
For example, if we learn from news corpora that:
a man may have a wife, then a tweet saying: ...my
wife... can be taken as potential evidence of mem-
bership in the male conceptual class.
In our second study, we test whether this idea
extends to our wider set of fine-grained roles. For
example, we aimed to discover that a doctor may
have a patient, while a hairdresser may have a
salon; these properties can be expressed in first-
person content as possessives like my patient or my
salon. We approached this task by selecting target
roles from the first experiment and ranking charac-
teristic attributes for each using pointwise mutual
information (PMI) (Church and Hanks, 1990).
First, we counted all terms matching a target
social role?s possessive pattern (e.g., doctor?s )
in the web-scale n-gram corpus Google V2 (Lin
et al, 2010)
5
. We ranked the collected terms
by computing PMI between classes and attribute
terms. Probabilities were estimated from counts of
the class-attribute pairs along with counts match-
ing the generic possessive patterns his and
her which serve as general background cate-
gories. Following suggestions by Bergsma and
Van Durme, we manually filtered the ranked list.
6
We removed attributes that were either (a) not
nominal, or (b) not indicative of the social role.
This left fewer than 30 attribute terms per role,
with many roles having fewer than 10.
We next performed a precision test to identify
potentially useful attributes in these lists. We ex-
amined tweets with a first person possessive pat-
tern for each attribute term from a small corpus
of tweets collected over a single month in 2013,
discarding those attribute terms with no positive
matches. This precision test is useful regardless
of how attribute lists are generated. The attribute
5
In this corpus, follower-type roles like belieber and di-
rectioner are not at all prevalent. We therefore focused on
occupational and habitual roles (e.g., doctor, smoker).
6
Evidence from cognitive work on memory-dependent
tasks suggests that such relevance based filtering (recogni-
tion) involves less cognitive effort than generating relevant
attributes (recall) see (Jacoby et al, 1979). Indeed, this filter-
ing step generally took less than a minute per class.
term chart, for example, had high PMI with doc-
tor; but a precision test on the phrase my chart
yielded a single tweet which referred not to a med-
ical chart but to a top ten list (prompting removal
of this attribute). Using this smaller high-precision
set of attribute terms, we collected tweets from the
Twitter Firehose over the period 2011-2013.
4 Attribute-based Classification
Attribute terms are less indicative overall than
self-ID, e.g., the phrase I?m a barber is a clearer
signal than my scissors. We therefore include a
role verification step in curating a collection of
positively identified users. We use the crowd-
sourcing platform Mechanical Turk
7
to judge
whether the person tweeting held a given role
Tweets were judged 5-way redundantly. Me-
chanical Turk judges (?Turkers?) were presented
with a tweet and the prompt: Based on this
tweet, would you think this person is a BAR-
BER/HAIRDRESSER? along with four response
options: Yes, Maybe, Hard to tell, and No.
We piloted this labeling task on 10 tweets per
attribute term over a variety of classes. Each an-
swer was associated with a score (Yes = 1, Maybe
= .5, Hard to tell = No = 0) and aggregated across
the five judges. We found in development that an
aggregate score of 4.0 (out of 5.0) led to an ac-
ceptable agreement rate between the Turkers and
the experimenters, when the tweets were randomly
sampled and judged internally. We found that
making conceptual class assignments based on a
single tweet was often a subtle task. The results of
this labeling study are shown in Figure 4, which
gives the percent of tweets per attribute that were
4.0 or above. Attribute terms shown in red were
manually discarded as being inaccurate (low on
the y-axis) or non-prevalent (small shape).
From the remaining attribute terms, we identi-
fied users with tweets scoring 4.0 or better as posi-
tive examples of the associated roles. Tweets from
those users were scraped via the Twitter API to
construct corpora for each role. These were split
intro train and test, balanced with data from the
same background set used in the self-ID study.
Test sets were usually of size 40 (20 positive, 20
background), with a few classes being sparse (the
smallest had only 16 instances). Results are shown
in Figure 5. Several classes in this balanced setup
can be predicted with accuracies in the 70-90%
7
https://www.mturk.com/mturk/
184
l l l l ll lll l l l
l l l l l l
l
l l ll
l
l l l l l
l
l l l
Actor/Actress Athlete Barber/Hairdresser Bartender Blogger Cheerleader
Christian College Student Dancer Doctor/Nurse Drummer Hunter
Jew Mom Musician Photographer Professor Rapper/Songwriter
Reporter Sailor Skier Smoker Soldier Student
Swimmer Tattoo Artist Waiter/Waitress Writer
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
rehear
sal theater directo
r lines
conc
ussionplayin
gprotein sport squadcondit
ioningjerseypositioncoach calves clien
t
scissor
s
shears salon bar blog bloggi
ng pom
hope testimo
ny
church bible schola
rship
syllabu
s
adviso
r
tuition campu
s
univer
sity
college tu
tu
scrub patient stethos
cope drum stand
shul angel deliver
y kid parent
ing set alum guitar piano shoot shutter lecture faculty studen
t lyrics
cove
rage editor article shi
p
goggle
s pipe smokin
g
tobacc
o
smoke cigaret
te
billet comba
t duffel orders bunk deploy
mentbarrac
ks stats cap lab philoso
phy
pool ink station tip apron script memo
ir poemKeyword
Above
 Thres
hold
log10(Count)
l l l l1 2 3 4Keep
l FALSE TRUE
Figure 4: Turker judged quality of attributes selected as
candidate features for bootstrapping positive instances of the
given social role.
0.5
0.6
0.7
0.8
acto
r
athle
te
barbe
r
blogg
er
chee
rlead
er
chris
tian docto
r
drum
mer
mo
m
mu
sicia
n
photo
graph
er
profe
ssor
repor
ter
smo
ker
soldi
er
stude
nt
waite
r
write
r
Accu
racy
Figure 5: Classifier accuracy on balanced set contrasting
agreed upon Twitter users of a given role against users pulled
at random from the 1% stream.
range, supporting our claim that there is discrimi-
nating content for a variety of these social roles.
Conditional Classification How accurately we
can predict membership in a given class when a
Twitter user sends a tweet matching one of the tar-
geted attributes? For example, if one sends a tweet
saying my coach, then how likely is it that author
Figure 6: Results of positive vs negative by attribute term.
Given that a user tweets . . . my lines . . . we are nearly 80%
accurate in identifying whether or not the user is an actor.
is an athlete?
Using the same collection as the previous ex-
periment, we trained classifiers conditioned on a
given attribute term. Positive instances were taken
to be those with a score of 4.0 or higher, with neg-
ative instances taken to be those with scores of 1.0
or lower (strong agreement by judges that the orig-
inal tweet did not provide evidence of the given
role). Classification results are shown in Figure 6.
5 Conclusion
We have shown that Twitter contains sufficiently
robust signal to support more fine-grained au-
thor attribute prediction tasks than have previously
been attempted. Our results are based on simple,
intuitive search patterns with minimal additional
filtering: this establishes the feasibility of the task,
but leaves wide room for future work, both in the
sophistication in methodology as well as the diver-
sity of roles to be targeted. We exploited two com-
plementary types of indicators: self-identification
and self-possession of conceptual class (role) at-
tributes. Those interested in identifying latent de-
mographics can extend and improve these indica-
tors in developing ways to identify groups of inter-
est within the general population of Twitter users.
Acknowledgements This material is partially
based on research sponsored by the NSF un-
der grants DGE-123285 and IIS-1249516 and by
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
185
References
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: an eval-
uation. In Proceedings of EMNLP.
Shane Bergsma and Benjamin Van Durme. 2013. Us-
ing Conceptual Class Attributes to Characterize So-
cial Media Users. In Proceedings of ACL.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clay Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the NAACL Workshop
on Language and Social Media.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
twitter. In Proceedings of EMNLP.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Jacob Eisenstein, Brendan O?Connor, Noah Smith, and
Eric P. Xing. 2010. A latent variable model of
geographical lexical variation. In Proceedings of
EMNLP.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of ACL.
Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979.
Effects of decision difficulty on recognition and re-
call. Journal of Verbal Learning and Verbal Behav-
ior, 18(5):585?600.
Michal Kosinski, David Stillwell, and Thore Graepel.
2013. Private traits and attributes are predictable
from digital records of human behavior. Proceed-
ings of the National Academy of Sciences.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
twitter. In Proceedings of NAACL.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale n-grams. In Proc. LREC, pages 2221?
2227.
Saif M. Mohammad, Svetlana Kiritchenko, and Joel
Martin. 2013. Identifying purpose behind elec-
toral tweets. In Proceedings of the Second Interna-
tional Workshop on Issues of Sentiment Discovery
and Opinion Mining, WISDOM ?13, pages 1?9.
Dong Nguyen, Noah A Smith, and Carolyn P Ros?e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 115?123. Association for Computational Lin-
guistics.
Marius Pas?ca and Benjamin Van Durme. 2008.
Weakly-Supervised Acquisition of Open-Domain
Classes and Class Attributes from Web Documents
and Query Logs. In Proceedings of ACL.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
User classification in Twitter. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data mining, pages 430?
438. ACM.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Con-
tents (SMUC).
Lenhart K. Schubert. 2002. Can we derive general
world knowledge from texts? In Proceedings of
HLT.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical
Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of EMNLP.
Jennifer Wortman. 2008. Viral marketing and the
diffusion of trends on social networks. Technical
Report MS-CIS-08-19, University of Pennsylvania,
May.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of ICWSM.
186
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 50?55,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Predicting Fine-grained Social Roles with Selectional Preferences
Charley Beller Craig Harman Benjamin Van Durme
charleybeller@jhu.edu craig@craigharman.net vandurme@cs.jhu.edu
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
Abstract
Selectional preferences, the tendencies of
predicates to select for certain semantic
classes of arguments, have been success-
fully applied to a number of tasks in
computational linguistics including word
sense disambiguation, semantic role label-
ing, relation extraction, and textual infer-
ence. Here we leverage the information
encoded in selectional preferences to the
task of predicting fine-grained categories
of authors on the social media platform
Twitter. First person uses of verbs that se-
lect for a given social role as subject (e.g.
I teach ... for teacher) are used to quickly
build up binary classifiers for that role.
1 Introduction
It has long been recognized that linguistic pred-
icates preferentially select arguments that meet
certain semantic criteria (Katz and Fodor, 1963;
Chomsky, 1965). The verb eat for example se-
lects for an animate subject and a comestible ob-
ject. While the information encoded by selectional
preferences can and has been used to support nat-
ural language processing tasks such as word sense
disambiguation (Resnik, 1997), syntactic disam-
biguation (Li and Abe, 1998) and semantic role
labeling (Gildea and Jurafsky, 2002), much of
the work on the topic revolves around developing
methods to induce selectional preferences from
data. In this setting, end-tasks can be used for
evaluation of the resulting collection. Ritter et al.
(2010) gave a recent overview of this work, break-
ing it down into class-based approaches (Resnik,
1996; Li and Abe, 1998; Clark and Weir, 2002;
Pantel et al., 2007), similarity-based approaches
(Dagan et al., 1999; Erk, 2007), and approaches
using discriminative (Bergsma et al., 2008) or gen-
erative probabilistic models (Rooth et al., 1999)
like their own.
One of our contributions here is to show that
the literature on selectional preferences relates to
the analysis of the first person content transmitted
through social media. We make use of a ?quick
and dirty? method for inducing selectional pref-
erences and apply the resulting collections to the
task of predicting fine-grained latent author at-
tributes on Twitter. Our method for inducing se-
lectional preferences is most similar to class-based
approaches, though unlike approaches such as by
Resnik (1996) we do not require a WordNet-like
ontology.
The vast quantity of informal, first-person text
data made available by the rise of social me-
dia platforms has encouraged researchers to de-
velop models that predict broad user categories
like age, gender, and political preference (Garera
and Yarowsky, 2009; Rao et al., 2010; Burger et
al., 2011; Van Durme, 2012b; Zamal et al., 2012).
Such information is useful for large scale demo-
graphic research that can fuel computational social
science advertising.
Similarly to Beller et al. (2014), we are inter-
ested in classification that is finer-grained than
gender or political affiliation, seeking instead to
predict social roles like smoker, student, and
artist. We make use of a light-weight, unsuper-
vised method to identify selectional preferences
and use the resulting information to rapidly boot-
strap classification models.
2 Inducing Selectional Preferences
Consider the task of predicting social roles in more
detail: For a given role, e.g. artist, we want a way
to distinguish role-bearing from non-role-bearing
users. We can view each social role as being a
fine-grained version of a semantic class of the sort
required by class-based approaches to selectional
preferences (e.g. the work by Resnik (1996) and
those reviewed by Light and Greiff (2002)). The
goal then is to identify a set of verbs that preferen-
50
tially select that particular class as argument. Once
we have a set of verbs for a given role, simple pat-
tern matches against first person subject templates
like I can be used to identify authors that bear
that social role.
In order to identify verbs that select for a given
role r as subject we use an unsupervised method
inspired by Bergsma and Van Durme (2013) that
extracts features from third-person content (i.e.
newswire) to build classifiers on first-person con-
tent (i.e. tweets). For example, if we read in a
news article that an artist drew ..., we can take a
tweet saying I drew ... as potential evidence that
the author bears the artist social role.
We first count all verbs v that appear with role r
as subject in the web-scale, part-of-speech tagged
n-gram corpus, Google V2 (Lin et al., 2010).
The resulting collection of verbs is then ranked
by computing their pointwise mutual information
(Church and Hanks, 1990) with the subject role r.
The PMI of a given role r and a verb v that takes
r as subject is given as:
PMI(r, v) = log
P (r, v)
P (r)P (v)
Probabilities are estimated from counts of the
role-verb pairs along with counts matching the
generic subject patterns he and she which
serve as general background cases. This gives us a
set of verbs that preferentially select for the subset
of persons filling the given role.
The output of the PMI ranking is a high-recall
list of verbs that preferentially select the given so-
cial role as subject over a background population.
Each such list then underwent a manual filtering
step to rapidly remove non-discriminative verbs
and corpus artifacts. One such artifact from our
corpus was the term wannabe which was spuri-
ously elevated in the PMI ranking based on the
relative frequency of the bigram artist wannabe as
compared to she wannabe. Note that in the first
case wannabe is best analyzed as a noun, while in
the second case a verbal analysis is more plausi-
ble. The filtering was performed by one of the au-
thors and generally took less than two minutes per
list. The rapidity of the filtering step is in line with
findings such as by Jacoby et al. (1979) that rele-
vance based filtering involves less cognitive effort
than generation. After filtering the lists contained
fewer than 40 verbs selecting each social role.
In part because of the pivot from third- to first-
person text we performed a precision test on the
remaining verbs to identify which of them are
likely to be useful in classifying twitter users. For
each remaining verb we extracted all tweets that
contained the first person subject pattern I from
a small corpus of tweets drawn from the free pub-
lic 1% sample of the Twitter Firehose over a single
month in 2013. Verbs that had no matches which
appeared to be composed by a member of the as-
sociated social role were discarded. Using this
smaller high-precision set of verbs, we collected
tweets from a much larger corpus drawn from 1%
sample over the period 2011-2013.
One notable feature of the written English in
social media is that sentence subjects can be op-
tionally omitted. Subject-drop is a recognized fea-
ture of other informal spoken and written registers
of English, particularly ?diary dialects? (Thrasher,
1977; Napoli, 1982; Haegeman and Ihsane, 2001;
Weir, 2012; Haegeman, 2013; Scott, 2013). Be-
cause of the prevalence of subjectless cases we
collected two sets of tweets: those matching the
first person subject pattern I and those where
the verb was tweet initial. Example tweets for each
of our social roles can be seen in Table 2.
3 Classification via selectional
preferences
We conducted a set of experiments to gauge the
strength of the selectional preference indicators
for each social role. For each experiment we used
balanced datasets for training and testing with half
of the users taken from a random background sam-
ple and half from a collection of users identified
as belonging to the social role. Base accuracy was
thus 50%.
To curate the collections of positively identified
users we crowdsourced a manual verification pro-
cedure. We use the popular crowdsourcing plat-
form Mechanical Turk
1
to judge whether, for a
tweet containing a given verb, the author held the
role that verb prefers as subject. Each tweet was
judged using 5-way redundancy.
Mechanical Turk judges (?Turkers?) were pre-
sented with a tweet and the prompt: Based on this
tweet, would you think this person is a ARTIST?
along with four response options: Yes, Maybe,
Hard to tell, and No. An example is shown in Fig-
ure 1.
We piloted this labeling task with a goal of
20 tweets per verb over a variety of social roles.
1
https://www.mturk.com/mturk/
51
Artist
draw Yeaa this a be the first time I draw my
shit onn
Athlete
play @[user] @[user] i have got the night off
tonight because I played last night and I
am going out for dinner so won?t be able
to come?
Blogger
blogged @[user] I decided not to renew. I
blogged about it on the fan club. a bit
shocked no neg comments back to me
Cheerleader
cheer I really dont wanna cheer for this game
I have soo much to do
Christian
thank Had my bday yesterday 3011 nd had a
good night with my friends. I thank God
4 His blessings in my life nd praise Him
4 adding another year.
DJ
spin Quick cut session before I spin tonight
Filmmaker
film @[user] apparently there was no au-
dio on the volleyball game I filmed
so...there will be no ?NAT sound? cause
I have no audio at all
Media Host
interview Oh. I interviewed her on the @[user] .
You should listen to the interview. Its
awesome! @[user] @[user] @[user]
Performer
perform I perform the flute... kareem shocked...
Producer
produce RT @[user]: Wow 2 films in Urban-
world this year-1 I produced ... [URL]
Smoker
smoke I smoke , i drank .. was my shit bra !
Stoner
puff I?m a cigarello fiend smokin weed like
its oxygen Puff pass, nigga I puff grass
till I pass out
Student
finish I finish school in March and my friend
birthday in March ...
Teacher
teach @[user] home schooled I really wanna
find out wat it?s like n making new
friends but home schooling is cool I
teach myself mums ill
Table 1: Example verbs and sample tweets collected using
them in the first person subject pattern (I ).
Each answer was associated with a score (Yes = 1,
Maybe = .5, Hard to tell = No = 0) and aggregated
across the five judges, leading to a range of pos-
sible scores from 0.0 to 5.0 per tweet. We found
in development that an aggregate score of 4.0 led
to an acceptable agreement rate between the Turk-
ers and the experimenters, when the tweets were
randomly sampled and judged internally.
Verbs were discarded for being either insuffi-
ciently accurate or insufficiently prevalent in the
corpus. From the remaining verbs, we identified
users with tweets scoring 4.0 or better as the posi-
tive examples of the associated social roles. These
positively identified user?s tweets were scraped us-
ing the Twitter API in order to construct user-
specific corpora of positive examples for each role.
Figure 1: Mechanical Turk presentation
0.5
0.6
0.7
0.8
Art
ist
Ath
lete
Blo
gge
r
Che
erle
ade
r
Chr
istia
n
DJ
Film
mak
er
Ho
st
Per
form
er
Pro
duc
er
Sm
oke
r
Sto
ner
Stu
den
t
Tea
che
r
Acc
ura
cy
Figure 2: Accuracy of classifier trained and tested on bal-
anced set contrasting agreed upon Twitter users of a given
role, against users pulled at random from the 1% stream.
3.1 General Classification
The positively annotated examples were balanced
with data from a background set of Twitter users
to produce training and test sets. These test sets
were usually of size 40 (20 positive, 20 back-
ground), with a few classes being sparser (the
smallest test set had only 28 instances). We used
the Jerboa (Van Durme, 2012a) platform to con-
vert our data to binary feature vectors over a uni-
gram vocabulary filtered such that the minimum
frequency was 5 (across unique users). Training
and testing was done with a log-linear model via
LibLinear (Fan et al., 2008). Results are shown
in Figure 2. As can be seen, a variety of classes in
this balanced setup can be predicted with accura-
cies in the range of 80%. This shows that the in-
formation encoded in selectional preferences con-
tains discriminating signal for a variety of these
social roles.
3.2 Conditional Classification
How accurately can we predict membership in a
given class when a Twitter user sends a tweet
matching one of the collected verbs? For exam-
ple, if one sends a tweet saying I race ..., then how
likely is it that the author is an athlete?
52
0.5
0.6
0.7
0.8
Art
ist :
 dra
w
Ath
lete
 : ra
ce
Ath
lete
 : ru
n
Blo
gge
r : b
log
ged
Che
erle
ade
r : c
hee
r
Chr
istia
n : 
pra
y
Chr
istia
n : 
serv
e
Chr
istia
n : 
than
k
DJ 
: sp
in
Film
mak
er 
: fil
m
Ho
st : 
inte
rvie
w
Per
form
er :
 per
form
Pro
duc
er :
 pro
duc
e
Sm
oke
r : 
sm
oke
Sto
ner
 : p
uff
Sto
ner
 : sp
ark
Stu
den
t : e
nro
ll
Stu
den
t : f
inis
h
Tea
che
r : t
eac
h
Acc
ura
cy
Figure 3: Results of positive vs negative by verb. Given
that a user writes a tweet containing I interview . . . or Inter-
viewing . . . we are about 75% accurate in identifying whether
or not the user is a Radio/Podcast Host.
# Users # labeled # Pos # Neg Attribute
199022 516 63 238 Artist-draw
45162 566 40 284 Athlete-race
1074289 1000 54 731 Athlete-run
9960 15 14 0 Blogger-blog
2204 140 57 18 College Student-enroll
247231 1000 85 564 College Student-finish
60486 845 61 524 Cheerleader-cheer
448738 561 133 95 Christian-pray
92223 286 59 180 Christian-serve
428337 307 78 135 Christian-thank
17408 246 17 151 DJ-spin
153793 621 53 332 Filmmaker-film
36991 554 42 223 Radio Host-interview
43997 297 81 97 Performer-perform
69463 315 71 100 Producer-produce
513096 144 74 8 Smoker-smoke
5542 124 49 15 Stoner-puff
5526 229 59 51 Stoner-spark
149244 495 133 208 Teacher-teach
Table 2: Numbers of positively and negatively identified
users by indicative verb.
Using the same collection as the previous ex-
periment, we trained classifiers conditioned on a
given verb term. Positive instances were taken to
be those with a score of 4.0 or higher, with nega-
tive instances taken to be those with scores of 1.0
or lower (strong agreement by judges that the orig-
inal tweet did not provide evidence of the given
role). Classification results are shown in figure 3.
Note that for a number of verb terms these thresh-
olds left very sparse collections of users. There
were only 8 users, for example, that tweeted the
phrase I smoke ... but were labeled as negative in-
stances of Smokers. Counts are given in Table 2.
Despite the sparsity of some of these classes,
many of the features learned by our classifiers
make intuitive sense. Highlights of the most
highly weighted unigrams from the classification
Verb Feature ( Rank)
draw drawing, art, book
4
, sketch
14
, paper
19
race race, hard, winter, won
11
, training
16
, run
17
run awesome, nike
6
, fast
9
, marathon
20
blog notes, boom, hacked
4
, perspective
9
cheer cheer, pictures, omg, text, literally
pray through, jesus
3
, prayers
7
, lord
14
, thank
17
serve lord, jesus, church, blessed, pray, grace
thank [ ], blessed, lord, trust
11
, pray
12
enroll fall, fat, carry, job, spend, fail
15
finish hey, wrong, may
8
, move
9
, officially
14
spin show, dj, music, dude, ladies, posted, listen
film please, wow, youtube, send, music
8
perform [ ], stuck, act, song, tickets
7
, support
16
produce follow, video
8
, listen
10
, single
11
, studio
13
,
interview fan, latest, awesome, seems
smoke weakness, runs, ti, simply
puff bout, $
7
, smh
9
, weed
10
spark dont, fat
5
, blunt
6
, smoke
11
teach forward, amazing, students, great, teacher
7
Table 3: Most-highly indicative features that a user holds
the associated role given that they used the phrase I VERB
along with select features within the top 20.
experiments are shown in Table 3. Taken together
these features suggest that several of our roles can
be distinguished from the background population
by focussing on typical language use. The use of
terms like, e.g., sketch by artists, training by ath-
letes, jesus by Chrisitians, and students by teach-
ers conforms to expected pattern of language use.
4 Conclusion
We have shown that verb-argument selectional
preferences relates to the content-based classifica-
tion strategy for latent author attributes. In particu-
lar, we have presented initial studies showing that
mining selectional preferences from third-person
content, such as newswire, can be used to inform
latent author attribute prediction based on first-
person content, such as that appearing in social
media services like Twitter.
Future work should consider the question of
priors. Our study here relied on balanced class
experiments, but the more fine-grained the social
role, the smaller the subset of the population we
might expect will possess that role. Estimating
these priors is thus an important point for future
work, especially if we wish to couple such demo-
graphic predictions within a larger automatic sys-
tem, such as the aggregate prediction of targeted
sentiment (Jiang et al., 2011).
Acknowledgements This material is partially based
on research sponsored by the NSF under grant IIS-1249516
and by DARPA under agreement number FA8750-13-2-0017
(DEFT).
53
References
Charley Beller, Rebecca Knowles, Craig Harman,
Shane Bergsma, Margaret Mitchell, and Benjamin
Van Durme. 2014. I?m a belieber: Social roles via
self-identification and conceptual attributes. In Pro-
ceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics.
Shane Bergsma and Benjamin Van Durme. 2013. Us-
ing Conceptual Class Attributes to Characterize So-
cial Media Users. In Proceedings of ACL.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional pref-
erence from unlabeled text. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 59?68. Association for
Computational Linguistics.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proceedings of EMNLP.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. Number 11. MIT press.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2).
Ido Dagan, Lillian Lee, and Fernando CN Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Machine Learning, 34(1-3):43?
69.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceeding of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, volume 45, page 216.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of ACL.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Liliane Haegeman and Tabea Ihsane. 2001. Adult null
subjects in the non-pro-drop languages: Two diary
dialects. Language acquisition, 9(4):329?346.
Liliane Haegeman. 2013. The syntax of registers: Di-
ary subject omission and the privilege of the root.
Lingua, 130:88?110.
Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979.
Effects of decision difficulty on recognition and re-
call. Journal of Verbal Learning and Verbal Behav-
ior, 18(5):585?600.
Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao.
2011. Target-dependent twitter sentiment classifi-
cation. In Proceedings of ACL.
Jerrold J Katz and Jerry A Fodor. 1963. The structure
of a semantic theory. language, pages 170?210.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational linguistics, 24(2):217?244.
Marc Light and Warren Greiff. 2002. Statistical mod-
els for the induction and use of selectional prefer-
ences. Cognitive Science, 26(3):269?281.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale n-grams. In Proc. LREC, pages 2221?
2227.
Donna Jo Napoli. 1982. Initial material deletion in
English. Glossa, 16(1):5?111.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard H Hovy. 2007.
ISP: Learning inferential selectional preferences. In
HLT-NAACL, pages 564?571.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Con-
tents (SMUC).
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61(1):127?159.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How, pages 52?57. Washington,
DC.
Alan Ritter, Masaum, and Oren Etzioni. 2010. A la-
tent dirichlet allocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 424?434. Association for Computational
Linguistics.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics, pages 104?
111. Association for Computational Linguistics.
54
Kate Scott. 2013. Pragmatically motivated null sub-
jects in English: A relevance theory perspective.
Journal of Pragmatics, 53:68?83.
Randolph Thrasher. 1977. One way to say more by
saying less: A study of so-called subjectless sen-
tences. Kwansei Gakuin University Monograph Se-
ries, 11.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical
Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of EMNLP.
Andrew Weir. 2012. Left-edge deletion in English and
subject omission in diaries. English Language and
Linguistics, 16(01):105?129.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of ICWSM.
55
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 45?53,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
A Comparison of the Events and Relations Across ACE, ERE, TAC-KBP,
and FrameNet Annotation Standards
Jacqueline Aguilar and Charley Beller and Paul McNamee and Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD, USA
Stephanie Strassel and Zhiyi Song and Joe Ellis
University of Pennsylvania
Linguistic Data Consortium (LDC)
Philadelphia, PA, USA
Abstract
The resurgence of effort within computa-
tional semantics has led to increased in-
terest in various types of relation extrac-
tion and semantic parsing. While var-
ious manually annotated resources exist
for enabling this work, these materials
have been developed with different stan-
dards and goals in mind. In an effort
to develop better general understanding
across these resources, we provide a sum-
mary overview of the standards underly-
ing ACE, ERE, TAC-KBP Slot-filling, and
FrameNet.
1 Overview
ACE and ERE are comprehensive annotation stan-
dards that aim to consistently annotate Entities,
Events, and Relations within a variety of doc-
uments. The ACE (Automatic Content Extrac-
tion) standard was developed by NIST in 1999 and
has evolved over time to support different evalua-
tion cycles, the last evaluation having occurred in
2008. The ERE (Entities, Relations, Events) stan-
dard was created under the DARPA DEFT pro-
gram as a lighter-weight version of ACE with the
goal of making annotation easier, and more con-
sistent across annotators. ERE attempts to achieve
this goal by consolidating some of the annotation
type distinctions that were found to be the most
problematic in ACE, as well as removing some
more complex annotation features.
This paper provides an overview of the relation-
ship between these two standards and compares
them to the more restricted standard of the TAC-
KBP slot-filling task and the more expansive stan-
dard of FrameNet. Sections 3 and 4 examine Rela-
tions and Events in the ACE/ERE standards, sec-
tion 5 looks at TAC-KBP slot-filling, and section
6 compares FrameNet to the other standards.
2 ACE and ERE Entity Tagging
Many of the differences in Relations and Events
annotation across the ACE and ERE standards
stem from differences in entity mention tagging.
This is simply because Relation and Event tagging
relies on the distinctions established in the entity
tagging portion of the annotation process. For ex-
ample, since ERE collapses the ACE Facility and
Location Types, any ACE Relation or Event that
relied on that distinction is revised in ERE. These
top-level differences are worth keeping in mind
when considering how Events and Relations tag-
ging is approached in ACE and ERE:
? Type Inventory: ACE and ERE share the Per-
son, Organization, Geo-Political Entity, and
Location Types. ACE has two additional
Types: Vehicle and Weapon. ERE does not
account for these Types and collapses the Fa-
cility and Location Types into Location. ERE
also includes a Title Type to address titles,
honorifics, roles, and professions (Linguis-
tic Data Consortium, 2006; Linguistic Data
Consortium, 2013a).
? Subtype Annotation: ACE further classifies
entity mentions by including Subtypes for
each determined Type; if the entity does not
fit into any Subtype, it is not annotated. ERE
annotation does not include any Subtypes.
? Entity Classes: In addition to Subtype, ACE
also classifies each entity mention according
45
1996 1998 2000 2002 2004 2006 2008 2010 2012
F
r
a
m
e
N
e
t
p
r
o
j
e
c
t
c
r
e
a
t
e
d
A
C
E
d
e
v
e
l
o
p
e
d
m
o
s
t
c
o
m
p
r
e
h
e
n
s
i
v
e
A
C
E
c
o
r
p
u
s
l
a
s
t
A
C
E
e
v
a
l
fi
r
s
t
T
A
C
-
K
B
P
E
R
E
c
r
e
a
t
e
d
Figure 1: Important Dates for the ACE, ERE, TAC-KBP, and FrameNet Standards
to entity class (Specific, Generic, Attributive,
and Underspecified).
? Taggability: ACE tags Attributive, Generic,
Specific, and Underspecified entity mentions.
ERE only tags Specific entity mentions.
? Extents and Heads: ACE marks the full noun
phrase of an entity mention and tags a head
word. ERE handles tagging based on the
mention level of an entity; in Name mentions
(NAM) the name is the extent, in Nominal
mentions (NOM) the full noun phrase is the
extent, in Pronoun mentions (PRO) the pro-
noun is the extent.
? Tags: ERE only specifies Type and Men-
tion level (NAM, NOM, PRO). ACE speci-
fies Type, Subtype, Entity Class (Attributive,
Generic, Specific, Underspecified), and Men-
tion Level (NAM, NOM, PRO, Headless).
3 Relations in ACE and ERE
In the ACE and ERE annotation models, the goal
of the Relations task is to detect and character-
ize relations of the targeted types between enti-
ties (Linguistic Data Consortium, 2008; Linguistic
Data Consortium, 2013c). The purpose of this task
is to extract a representation of the meaning of the
text, not necessarily tied to underlying syntactic
or lexical semantic representations. Both models
share similar overarching guidelines for determin-
ing what is taggable. For relations the differences
lie in the absence or presence of additional fea-
tures, syntactic classes, as well as differences in
assertion, trigger words, and minor subtype varia-
tions.
3.1 Similarities in Relations Annotation
In addition to comprising similar Types (both
models include Physical and Part.Whole Types as
well as slightly different Types to address Affilia-
tion and Social relations) used to characterize each
relation, ACE and ERE share important similar-
ities concerning their relation-tagging guidelines.
These include:
? Limiting relations to only those expressed in
a single sentence
? Tagging only for explicit mention
? No ?promoting? or ?nesting? of taggable en-
tities. In the sentence, Smith went to a hotel
in Brazil, (Smith, hotel) is a taggable Phys-
ical.Located relation, but (Smith, Brazil) is
not. This is because in order to tag this as
such, one would have to promote ?Brazil?.
? Tagging for past and former relations
? Two different Argument slots (Arg1 and
Arg2) are provided for each relation to cap-
ture the importance of Argument ordering.
? Arguments can be more than one token (al-
though ACE marks the head as well)
? Using ?templates? for each relation
Type/Subtype (e.g., in a Physical.Located
relation, the Person that is located some-
where will always be assigned to Arg1 and
the place in which the person is located will
always be assigned to Arg2).
? Neither model tags for negative relations
? Both methods contain argument span bound-
aries. That is, the relations should only in-
clude tagged entities within the extent of a
sentence.
3.2 Differences in Assertion, Modality, and
Tense
A primary difference between these two annota-
tion models is a result of ERE only annotating as-
serted events while ACE also includes hypothet-
icals. ACE accounts for these cases by including
two Modality attributes: ASSERTED and OTHER
46
(Linguistic Data Consortium, 2008). For exam-
ple, in the sentence, We are afraid that Al-Qaeda
terrorists will be in Baghdad, ACE would tag this
as an OTHER attribute, where OTHER pertains to
situations in ?some other world defined by coun-
terfactual constraints elsewhere in the context?,
whereas ERE would simply not tag a relation in
this sentence. Additionally, while both ACE and
ERE tag past and former relations, ACE goes fur-
ther to mark the Tense of each relation by means
of four attributes: Past, Future, Present and Un-
specified.
3.3 Syntactic Classes
ACE further justifies the tagging of each Relation
through Syntactic Classes. The primary function
of these classes is to serve as a sanity check on
taggability and as an additional constraint for tag-
ging. These classes include: Possessive, Prepo-
sition, PreMod, Coordination, Formulaic, Partic-
ipal, Verbal, Relations Expressed by Verbs, and
Other. Syntactic classes are not present in ERE
relations annotation.
3.4 Triggers
Explicit trigger words do not exist in ACE relation
annotation; instead, the model annotates the full
syntactic clause that serves as the ?trigger? for the
relation. ERE attempts to minimize the annotated
span by allowing for the tagging of an optional
trigger word, defined as ?the smallest extent of text
that indicates a relation Type and Subtype? (Lin-
guistic Data Consortium, 2013c). These triggers
are not limited to a single word, but can also be
composed of a phrase or any extent of the text that
indicates a Type/Subtype relation, left to the dis-
cretion of the annotator. It is common for preposi-
tions to be triggers, as in John is in Chicago. How-
ever, sometimes no trigger is needed because the
syntax of the sentence is such that it indicates a
particular relation Type/Subtype without a word to
explicitly signal the relation.
3.5 Types and Subtypes of Relations
There are three types of relations that contain var-
ied Subtypes between ERE and ACE. These are
the Physical, Part-Whole, Social and Affiliation
Types. The differences are a result of ERE collaps-
ing ACE Types and Subtypes into more concise, if
less specific, Type groups.
Physical Relation Type Differences The main
differences in the handling of the physical rela-
tions between ACE and ERE are shown in Table
1. ACE only marks Location for PERSON enti-
ties (for Arg1). ERE uses Location for PERSON
entities being located somewhere as well as for
a geographical location being part of another ge-
ographical location. Additionally, ACE includes
?Near? as a Subtype. This is used for when an en-
tity is explicitly near another entity, but neither en-
tity is a part of the other or located in/at the other.
ERE does not have an equivalent Subtype to ac-
count for this physical relation. Instead, ERE in-
cludes ?Origin? as a Subtype. This is used to de-
scribe the relation between a PER and an ORG.
ACE does not have a Physical Type equivalent,
but it does account for this type of relation within
a separate General Affiliation Type and ?Citizen-
Resident-Religion-Ethnicity? Subtype.
Part-Whole Relation Differences In Table 2,
note that ACE has a ?Geographical? Subtype
which captures the location of a FAC, LOC, or
GPE in or at, or as part of another FAC, LOC,
or GPE. Examples of this would be India con-
trolled the region or a phrase such as the Atlanta
area. ERE does not include this type of annota-
tion option. Instead, ERE tags these regional re-
lations as Physical.Located. ACE and ERE do
share a ?Subsidiary? Subtype which is defined in
both models as a ?category to capture the own-
ership, administrative and other hierarchical rela-
tionships between ORGs and/or GPEs? (Linguis-
tic Data Consortium, 2008; Linguistic Data Con-
sortium, 2013c).
Social and Affiliation Relation Differences
The most evident discrepancy in relation anno-
tation between the two models lies in the So-
cial and Affiliation Relation Types and Subtypes.
For social relations, ACE and ERE have three
Subtypes with similar goals (Business, Family,
Unspecified/Lasting-Personal) but ERE has an ad-
ditional ?Membership? Subtype, as shown in Ta-
ble 3. ACE addresses all ?Membership? relations
in its Affiliation Type. ERE also includes the ?So-
cial.Role? Subtype in order to address the TITLE
entity type, which only applies to ERE. How-
ever, both models agree that the arguments for
each relation must be PERSON entities and that
they should not include relationships implied from
interaction between two entities (e.g., President
47
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Physical Located PER, GPE, LOC GPE, LOC
Physical Origin PER, ORG GPE, LOC
ACE
Physical Located PER FAC, LOC, GPE
Physical Near PER, FAC, GPE, LOC FAC, GPE, LOC
Table 1: Comparison of Permitted Relation Arguments for the Physical Type Distinction in the ERE and
ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Part-Whole Subsidiary ORG ORG, GPE
ACE
Part-Whole Geographical FAC, LOC, GPE FAC, LOC, GPE
Part-Whole Subsidiary ORG ORG, GPE
Table 2: Comparison of Permitted Relation Arguments for the Part-Whole Type and Subtype Distinctions
in the ERE and ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Social Business PER PER
Social Family PER PER
Social Membership PER PER
Social Role TTL PER
Social Unspecified PER PER
ACE
Personal-Social Business PER PER
Personal-Social Family PER PER
Personal-Social Lasting-Personal PER PER
Table 3: Comparison of Permitted Relation Arguments for the Social Type and Subtype Distinctions in
the ERE and ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Affiliation Employment/Membership PER, ORG,
GPE
ORG, GPE
Affiliation Leadership PER ORG, GPE
ACE
ORG-Affiliation Employment PER ORG, GPE
ORG-Affiliation Ownership PER ORG
ORG-Affiliation Founder PER, ORG ORG, GPE
ORG-Affiliation Student-Alum PER ORG.Educational
ORG-Affiliation Sports-Affiliation PER ORG
ORG-Affiliation Investor-Shareholder PER, ORG,
GPE
ORG, GPE
ORG-Affiliation Membership PER, ORG,
GPE
ORG
Agent-Artifact User-Owner-Inventor-
Manufacturer
PER, ORG,
GPE
FAC
Gen-Affiliation Citizen-Resident-Religion-
Ethnicity
PER PER.Group,
LOC, GPE,
ORG
Gen-Affiliation Org-Location-Origin ORG LOC, GPE
Table 4: Comparison of Permitted Relation Arguments for the Affiliation Type and Subtype Distinctions
in the ERE and ACE Guidelines
48
Clinton met with Yasser Arafat last week would
not be considered a social relation).
As for the differences in affiliation relations,
ACE includes many Subtype possibilities which
can more accurately represent affiliation, whereas
ERE only observes two Affiliation Subtype op-
tions (Table 4).
4 Events in ACE and ERE
Events in both annotation methods are defined as
?specific occurrences?, involving ?specific partic-
ipants? (Linguistic Data Consortium, 2005; Lin-
guistic Data Consortium, 2013b). The primary
goal of Event tagging is to detect and character-
ize events that include tagged entities. The central
Event tagging difference between ACE and ERE
is the level of specificity present in ACE, whereas
ERE tends to collapse tags for a more simplified
approach.
4.1 Event Tagging Similarities
Both annotation schemas annotate the same ex-
act Event Types: LIFE, MOVEMENT, TRANS-
ACTION, BUSINESS, CONFLICT, CONTACT,
PERSONNEL, and JUSTICE events. Both anno-
tation ontologies also include 33 Subtypes for each
Type. Furthermore, both rely on the expression
of an occurrence through the use of a ?Trigger?.
ACE, however, restricts the trigger to be a single
word that most clearly expresses the event occur-
rence (usually a main verb), while ERE allows for
the trigger to be a word or a phrase that instanti-
ates the event (Linguistic Data Consortium, 2005;
Linguistic Data Consortium, 2013b). Both meth-
ods annotate modifiers when they trigger events
as well as anaphors, when they refer to previously
mentioned events. Furthermore, when there is
any ambiguity about which trigger to select, both
methods have similar rules established, such as
the Stand-Alone Noun Rule (In cases where more
than one trigger is possible, the noun that can be
used by itself to refer to the event will be selected)
and the Stand-Alone Adjective Rule (Whenever a
verb and an adjective are used together to express
the occurrence of an Event, the adjective will be
chosen as the trigger whenever it can stand-alone
to express the resulting state brought about by the
Event). Additionally, both annotation guidelines
agree on the following:
? Tagging of Resultative Events (states that re-
sult from taggable Events)
? Nominalized Events are tagged as regular
events
? Reported Events are not tagged
? Implicit events are not tagged
? Light verbs are not tagged
? Coreferential Events are tagged
? Tagging of multi-part triggers (both parts are
tagged only if they are contiguous)
4.2 Event Tagging Differences
One of the more general differences between ERE
and ACE Event tagging is the way in which each
model addresses Event Extent. ACE defines the
extent as always being the ?entire sentence within
which the Event is described? (Linguistic Data
Consortium, 2005). In ERE, the extent is the
entire document unless an event is coreferenced
(in which case, the extent is defined as the ?span
of a document from the first trigger for a par-
ticular event to the next trigger for a particular
event.? This signifies that the span can cross
sentence boundaries). Unlike ACE, ERE does
not delve into indicating Polarity, Tense, Gener-
icity, and Modality. ERE simplifies any anno-
tator confusion engendered by these features by
simply not tagging negative, future, hypotheti-
cal, conditional, uncertain or generic events (al-
though it does tag for past events). While ERE
only tags attested Events, ACE allows for irrealis
events, and includes attributes for marking them
as such: Believed Events; Hypothetical Events;
Commanded and Requested Events; Threatened,
Proposed and Discussed Events; Desired Events;
Promised Events; and Otherwise Unclear Con-
structions. Additionally both ERE and ACE tag
Event arguments as long as the arguments occur
within the event mention extent (another way of
saying that a taggable Event argument will occur
in the same sentence as the trigger word for its
Event). However, ERE and ACE have a diverging
approach to argument tagging:
? ERE is limited to pre-specified arguments for
each event and relation subtype. The pos-
sible arguments for ACE are: Event partici-
pants (limited to pre-specified roles for each
event type); Event-specific attributes that are
associated with a particular event type (e.g.,
the victim of an attack); and General event
attributes that can apply to most or all event
types (e.g., time, place).
49
? ACE tags arguments regardless of modal cer-
tainty of their involvement in the event. ERE
only tags asserted participants in the event.
? The full noun phrase is marked in both ERE
and ACE arguments, but the head is only
specified in ACE. This is because ACE han-
dles entity annotation slightly differently than
ERE does; ACE marks the full noun phrase
with a head word for entity mention, and ERE
treats mentions differently based on their syn-
tactic features (for named or pronominal en-
tity mentions the name or pronominal itself
is marked, whereas for nominal mentions the
full noun phrase is marked).
Event Type and Subtype Differences Both an-
notation methods have almost identical Event
Type and Subtype categories. The only differences
between both are present in the Contact and Move-
ment Event Types.
A minor distinction in Subtype exists as a re-
sult of the types of entities that can be trans-
ported within the Movement Type category. In
ACE, ARTIFACT entities (WEAPON or VEHI-
CLE) as well as PERSON entities can be trans-
ported, whereas in ERE, only PERSON entities
can be transported. The difference between the
Phone-Write and Communicate Subtypes merely
lies in the definition. Both Subtypes are the de-
fault Subtype to cover all Contact events where
a ?face-to-face? meeting between sender and re-
ceiver is not explicitly stated. In ACE, this contact
is limited to written or telephone communication
where at least two parties are specified to make
this event subtype less open-ended. In ERE, this
requirement is simply widened to comprise elec-
tronic communication as well, explicitly including
those via internet channels (e.g., Skype).
5 TAC-KBP
After the final ACE evaluation in 2008 there was
interest in the community to form an evaluation
explicitly focused on knowledge bases (KBs) cre-
ated from the output of extraction systems. NIST
had recently started the Text Analysis Conference
series for related NLP tasks such as Recognizing
Textual Entailment, Summarization, and Question
Answering. In 2009 the first Knowledge Base
Population track (TAC-KBP) was held featuring
two initial tasks: (a) Entity Linking ? linking en-
tities to KB entities, and (b) Slot Filling ? adding
information to entity profiles that is missing from
the KB (McNamee et al., 2010). Due to its gener-
ous license and large scale, a snapshot of English
Wikipedia from late 2008 has been used as the ref-
erence KB in the TAC-KBP evaluations.
5.1 Slot Filling Overview
Unlike ACE and ERE, Slot Filling does not have
as its primary goal the annotation of text. Rather,
the aim is to identify knowledge nuggets about a
focal named entity using a fixed inventory of re-
lations and attributes. For example, given a fo-
cal entity such as former Ukrainian prime minister
Yulia Tymoshenko, the task is to identify attributes
such as schools she attended, occupations, and im-
mediate family members. This is the same sort
of information commonly listed about prominent
people in Wikipedia Infoboxes and in derivative
databases such as FreeBase and DBpedia.
Consequently, Slot Filling is somewhat of a hy-
brid between relation extraction and question an-
swering ? slot fills can be considered as the cor-
rect responses to a fixed set of questions. The rela-
tions and attributes used in the 2013 task are pre-
sented in Table 5.
5.2 Differences with ACE-style relation
extraction
Slot Filling in TAC-KBP differs from extraction in
ACE and ERE in several significant ways:
? information is sought for named entities,
chiefly PERs and ORGs;
? the focus is on values not mentions;
? assessment is more like QA; and,
? events are handled as uncorrelated slots
In traditional IE evaluation, there was an
implicit skew towards highly attested in-
formation such as leader(Bush, US), or
capital(Paris, France). In contrast, TAC-KBP
gives full credit for finding a single instance of a
correct fill instead of every attestation of that fact.
Slot Filling assessment is somewhat simpler
than IE annotation. The assessor must decide
if provenance text is supportive of a posited fact
about the focal entity instead of annotating a doc-
ument with all evidenced relations and events for
any entity. For clarity and to increase assessor
agreement, guidelines have been developed to jus-
tify when a posited relation is deemed adequately
supported from text. Additionally, the problem of
50
Relations Attributes
per:children org:shareholders per:alternate names org:alternate names
per:other family org:founded by per:date of birth org:political religious affiliation
per:parents org:top members employees per:age org:number of employees members
per:siblings org:member of per:origin org:date founded
per:spouse org:members per:date of death org:date dissolved
per:employee or member of org:parents per:cause of death org:website
per:schools attended org:subsidiaries per:title
per:city of birth org:city of headquarters per:religion
per:stateorprovince of birth org:stateorprovince of headquarters per:charges
per:country of birth org:country of headquarters
per:cities of residence
per:statesorprovinces of residence
per:countries of residence
per:city of death
per:stateorprovince of death
per:country of death
Table 5: Relation and attributes for PERs and ORGs.
slot value equivalence becomes an issue - a sys-
tem should be penalized for redundantly asserting
that a person has four children named Tim, Beth,
Timothy, and Elizabeth, or that a person is both a
cardiologist and a doctor.
Rather than explicitly modeling events, TAC-
KBP created relations that capture events, more
in line with the notion of Infobox filling or ques-
tion answering (McNamee et al., 2010). For exam-
ple, instead of a criminal event, there is a slot fill
for charges brought against an entity. Instead of a
founding event, there are slots like org:founded by
(who) and org:date founded (when). Thus a state-
ment that ?Jobs is the founder and CEO of Apple?
is every bit as useful for the org:founded by rela-
tion as ?Jobs founded Apple in 1976.? even though
the date is not included in the former sentence.
5.3 Additional tasks
Starting in 2012 TAC-KBP introduced the ?Cold
Start? task, which is to literally produce a KB
based on the Slot Filling schema. To date, Cold
Start KBs have been built from collections of
O(50,000) documents, and due to their large size,
they are assessed by sampling. There is also
an event argument detection evaluation in KBP
planned for 2014.
Other TAC-KBP tasks have been introduced in-
cluding determining the timeline when dynamic
slot fills are valid (e.g., CEO of Microsoft), and
targeted sentiment.
6 FrameNet
The FrameNet project has rather different moti-
vations than either ACE/ERE or TAC-KBP, but
shares with them a goal of capturing informa-
tion about events and relations in text. FrameNet
stems from Charles Fillmore?s linguistic and lex-
icographic theory of Frame Semantics (Fillmore,
1976; Fillmore, 1982). Frames are descriptions
of event (or state) types and contain information
about event participants (frame elements), infor-
mation as to how event types relate to each other
(frame relations), and information about which
words or multi-word expressions can trigger a
given frame (lexical units).
FrameNet is designed with text annotation in
mind, but unlike ACE/ERE it prioritizes lexico-
graphic and linguistic completeness over ease of
annotation. As a result Frames tend to be much
finer grained than ACE/ERE events, and are more
numerous by an order of magnitude. The Berkeley
FrameNet Project (Baker et al., 1998) was devel-
oped as a machine readable database of distinct
frames and lexical units (words and multi-word
constructions) that were known to trigger specific
frames.
1
FrameNet 1.5 includes 1020 identified
frames and 11830 lexical units.
One of the most widespread uses of FrameNet
has been as a resource for Semantic Role Label-
ing (SRL) (Gildea and Jurafsky, 2002). FrameNet
related SRL was promoted as a task by the
SENSEVAL-3 workshop (Litkowski, 2004), and
the SemEval-2007 workshop (Baker et al., 2007).
(Das et al., 2010) is a current system for automatic
FrameNet annotation.
The relation and attribute types of TAC-KBP
and the relation and event types in the ACE/ERE
standards can be mapped to FrameNet frames.
The mapping is complicated by two factors.
The first is that FrameNet frames are gener-
ally more fine-grained than the ACE/ERE cate-
gories. As a result the mapping is sometimes
one-to-many. For example, the ERE relation Af-
1
This database is accessible via webpage (https:
//framenet.icsi.berkeley.edu/fndrupal/)
and as a collection of XML files by request.
51
Relations
FrameNet ACE ERE TAC-KBP
Kinship Personal-Social.Family Social.Family per:children
per:other family
per:parents
per:siblings
per:spouse
Being Employed ORG-Affiliation.Employment Affiliation.Employment/Membership per:employee or member of
Membership org:member of
Being Located Physical.Located Physical.Located org:city of headquarters
org:stateorprovince of headquarters
org:country of headquarters
Events
FrameNet ACE ERE
Contacting Phone-Write Communicate
Extradition Justice-Extradition Justice-Extradition
Attack Conflict-Attack Conflict-Attack
Being Born Life-Be Born Life-Be Born
Attributes
FrameNet TAC-KBP
Being Named per:alternate names
Age per:age
Table 6: Rough mappings between subsets of FrameNet, ACE, ERE, and TAC-KBP
filiation.Employment/Membership covers both
the Being Employed frame and the Member-
ship frame. At the same time, while TAC-
KBP has only a handful of relations relative to
FrameNet, some of these relations are more fine-
grained than the analogous frames or ACE/ERE
relations. For example, the frame Kinship, which
maps to the single ERE relation Social.Family,
maps to five TAC-KBP relations, and the Be-
ing Located, which maps to the ACE/ERE rela-
tion Being.Located, maps to three TAC-KBP re-
lations. Rough mappings from a selection of rela-
tions, events, and attributes are given in Table 6.
The second complication arises from the fact
that FrameNet frames are more complex objects
than ERE/ACE events, and considerably more
complex than TAC-KBP relations. Rather than the
two entities related via a TAC-KBP or ACE/ERE
relation, some frames have upwards of 20 frame
elements. Table 7 shows in detail the mapping be-
tween frame elements in the Extradition frame and
ACE?s and ERE?s Justice-Extradition events. The
?core? frame elements map exactly to the ERE
event, the remaining two arguments in the ACE
event map to two non-core frame elements, and
the frame includes several more non-core elements
with no analogue in either ACE or ERE standards.
7 Conclusion
The ACE and ERE annotation schemas have
closely related goals of identifying similar in-
formation across various possible types of docu-
ments, though their approaches differ due to sepa-
rate goals regarding scope and replicability. ERE
differs from ACE in collapsing different Type dis-
tinctions and in removing annotation features in
order to eliminate annotator confusion and to im-
FrameNet ACE ERE
Authorities Agent-Arg Agent-Arg
Crime jursidiction Destination-Arg Destination-Arg
Current jursidiction Origin-Arg Origin-Arg
Suspect Person-Arg Person-Arg
Reason Crime-Arg
Time Time-Arg
Legal Basis
Manner
Means
Place
Purpose
Depictive
Table 7: Mapping between frame elements of Ex-
tradition (FrameNet), and arguments of Justice-
Extradition (ACE/ERE): A line divides core frame
elements (above) from non-core (below).
prove consistency, efficiency, and higher inter-
annotator agreement. TAC-KPB slot-filling shares
some goals with ACE/ERE, but is wholly fo-
cused on a set collection of questions (slots to
be filled) concerning entities to the extent that
there is no explicit modeling of events. At the
other extreme, FrameNet seeks to capture the
full range of linguistic and lexicographic varia-
tion in event representations in text. In general, all
events, relations, and attributes that can be repre-
sented by ACE/ERE and TAC-KBP standards can
be mapped to FrameNet representations, though
adjustments need to be made for granularity of
event/relation types and granularity of arguments.
Acknowledgements
This material is partially based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
52
References
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics-Volume 1, pages 86?90. Associ-
ation for Computational Linguistics.
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic
structure extraction. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 99?104, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
parsing. In Proceedings of NAACL-HLT, pages 948?
956. Association for Computational Linguistics.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lancec Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ace) program- tasks, data, and evaluation. In
Proceedings of LREC 2004: Fourth International
Conference on Language Resources and Evaluation,
Lisbon, May 24-30.
Charles J Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy
of Sciences, 280(1):20?32.
Charles Fillmore. 1982. Frame semantics. In Linguis-
tics in the morning calm, pages 111?137. Hanshin
Publishing Co.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Linguistic Data Consortium. 2005. ACE (automatic
content extraction) English annotation guidelines
for events. https://www.ldc.upenn.edu/
collaborations/past-projects/ace.
Version 5.4.3 2005.07.01.
Linguistic Data Consortium. 2006. ACE (automatic
content extraction) English annotation guidelines
for entities. https://www.ldc.upenn.edu/
collaborations/past-projects/ace,
Version 5.6.6 2006.08.01.
Linguistic Data Consortium. 2008. ACE (automatic
content extraction) English annotation guidelines for
relations. https://www.ldc.upenn.edu/
collaborations/past-projects/ace.
Version 6.0 2008.01.07.
Linguistic Data Consortium. 2013a. DEFT ERE anno-
tation guidelines: Entities v1.1, 05.17.2013.
Linguistic Data Consortium. 2013b. DEFT ERE anno-
tation guidelines: Events v1.1. 05.17.2013.
Linguistic Data Consortium. 2013c. DEFT ERE anno-
tation guidelines: Relations v1.1. 05.17.2013.
Ken Litkowski. 2004. Senseval-3 task: Automatic
labeling of semantic roles. In Rada Mihalcea and
Phil Edmonds, editors, Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 9?12, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Paul McNamee, Hoa Trang Dang, Heather Simpson,
Patrick Schone, and Stephanie Strassel. 2010. An
evaluation of technologies for knowledge base pop-
ulation. In Proceedings of LREC.
53
