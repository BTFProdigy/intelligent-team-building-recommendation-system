Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 25?36, Dublin, Ireland, August 23-29 2014.
A Context-Aware NLP Approach For Noteworthiness Detection in
Cellphone Conversations
Francesca Bonin
?
Trinity College Dublin
Dublin, Ireland
boninf@tcd.ie
Jose San Pedro
Telefonica Research
Barcelona, Spain
jspw@tid.es
Nuria Oliver
Telefonica Research
Barcelona, Spain
nuriao@tid.es
Abstract
This papers presents a context-aware NLP approach to automatically detect noteworthy infor-
mation in spontaneous mobile phone conversations. The proposed method uses a supervised
modeling strategy which considers both features from the content of the conversation as well
as contextual information from the call. We empirically analyze the predictive performance of
features of different nature on a corpus of mobile phone conversations. The results of this study
reveal that the context of the conversation plays a crucial role on boosting the predictive perfor-
mance of the model.
1 Introduction
More than 6 billion people worldwide use their cellphones daily for a variety of purposes: contacting
colleagues, relatives or friends, doing business, getting help in emergency situations, etc. Previous work
(Carrascal et al., 2012) has shown that almost 40% of users frequently feel the need to recall bits of
information from their phone conversations and that 27% of the users consider the recall task to be
difficult, mainly because taking notes during a mobile phone call is not always possible (e.g. hands not
free, lack of time or devices for note-taking). In a related user study, Cycyl et al. reveal that users are
often engaged in concurrent tasks during mobile phone conversations (e.g. walking, jogging, driving,
cooking, etc), which makes taking notes an unfeasible task (Cycil et al., 2013).
In this setting, information extraction techniques could be applied to automatically detect noteworthy
information from mobile phone conversations. Related studies have focused on detecting noteworthiness
from meeting transcripts (Banerjee and Rudnicky, 2009). However, very little work has been done to
date to identify this kind of information in other types of human communication, such as spontaneous
phone conversations.
In this paper, we present a data-driven information extraction approach aimed at automatically detect-
ing fragments of phone conversations worth annotating for future recall, i.e. noteworthy. These call notes
could then be presented to the users to enable fast browsing of their conversation history, and leveraged
to design efficient information interaction techniques for supporting smart user interfaces.
Given the particular characteristics of mobile phone calls, detecting noteworthiness in them is chal-
lenging at many levels. First, the audio is captured in a natural environment rather than in controlled
settings, which results in noisy signals, and consequently in noisy transcriptions. Second, the conversa-
tions are highly fragmented due to their spontaneous nature. Finally, at a conceptual level, judging which
pieces of information are noteworthy is a very subjective task, as emerged in (Banerjee and Rudnicky,
2009), who investigated the feasibility of the task by conducting a Wizard of Oz-based user study.
Our noteworthiness modeling approach considers a supervised learning paradigm which takes into
account two types of information: (1) Contextual information both from the call (where, when, to whom,
. . . ) and the users (gender, age, . . . ); and (2) Content information of the conversation. The combination
?
* The work was conducted while the author was intern at Telefonica Research, Barcelona, Spain.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
25
of both sources of information enhances the flexibility of the model to accurately predict noteworthiness
in different use scenarios.
The main contributions of this paper are:
i) We propose and evaluate a supervised machine learning model to automatically detect notewor-
thy segments of phone conversations. Our approach adopts a hybrid strategy to model conversations
exploiting both content and context-related information.
ii) We propose a new set of content and context-based features specifically designed to detect note-
worthy information in our corpus of real-world cellphone conversations, and compare their effectiveness
iii) We provide a discussion of the results, derived from our quantitative and qualitative analyses.
The paper is structured as follows. Relevant previous work is presented in Section 2. Section 3
describes the corpus of phone conversations and the annotations provided by the participants. In Section 4
we describe in-depth the extracted features. Our experimental validation and results are presented in
Section 5. Finally, Section 6 summarizes our findings and highlights some lines of future research.
2 Related work
Noteworthiness detection in conversations can be considered to be a particular form of summarization:
the aim is to summarize the conversation by keeping only the relevant pieces of information that the
user would like to refer to at a later time. Although related, the main distinction between automatic
summarization and detection of noteworthy information lays in the notion of relevance. The former aims
at generating a comprehensive record of the conversation, while the latter considers only fragments worth
registering for future recall.
Considerable research activity has recently been devoted to automatic text and speech summarization
(Maskey and Hirschberg, 2003). Many approaches have been proposed in the literature, including cluster
(Zhang et al., 2005) and graph-based methods (Garg et al., 2009; Wang and Liu, 2011) and machine
learning techniques (Jian Zhang et al., 2007; Maskey and Hirschberg, 2006; Galley, 2006), where the
task is tackled as a binary classification problem considering whether the sentence is a good candidate
for a summary or not. In addition, different types of features have been used, including lexical, acoustic
and structural characteristics (Xie et al., 2008; Maskey and Hirschberg, 2005). Recent works have been
focused on adapting summarization to the social context, exploiting user generated contents associated
with the documents (Yang et al., 2011; Hu et al., 2012). Implicit and explicit community feedback in
online collaborative websites have also been leveraged to detect highlights of media assets (San Pedro et
al., 2009).
However, few studies have focused on noteworthiness detection. Banerjee et al. investigate the fea-
sibility of discovering noteworthy pieces of information in meetings by means of a Wizard of Oz-based
user study where a human suggested notes to meeting participants during the meeting. The authors found
that the human annotator obtained a precision of 35% and a recall of 41.5%. In the same work, Baner-
jee et al. reports a low inter annotator agreement (IAA) in noteworthiness discovery. In a related work
?probably the most relevant prior-art to our work, the authors apply extractive meeting summarization
techniques to automatically detect noteworthy utterances in meetings (Banerjee and Rudnicky, 2008).
They train a Decision Tree classifier over a collection of 5 meetings, obtaining an F-score of 0.14. This
result highlights the difficulty of the task at hand and motivates to explore alternative approaches.
To overcome the difficulties posed by this task we propose two main contributions: 1) the use of
novel features engineered ad-hoc for this task, and 2) the use of contextual information. While the
former adapts the document representation to the specific problem setting, the latter allows to enhance the
representation with orthogonal information which many times provides a higher discriminative power.
This approach has been used successfully in related fields; for instance, in information retrieval tasks
rich multimodal queries have been shown to effectively boost the retrieval performance compared to
pure textual queries (Yeh et al., 2011).
26
3 Corpus Collection
We used a corpus of cellphone conversations collected in a previous study (Carrascal et al., 2012). In
this study, a large sample of mobile phone conversations was recorded, semi-automatically transcribed
1
and manually annotated for relevance by their participants. Over 64 days, 796 mobile phone conversa-
tions from 62 volunteering subjects (20 female) were recorded. All the participants were Spanish native
speakers, and the conversations were recorded and transcribed in Spanish. Metadata about the call (e.g.
duration, date, time) was also stored along with the actual conversation and its transcript. More details
about the corpus collection process can be found in (Carrascal et al., 2012).
All the participants were first asked to fill out a pre-study questionnaire where they provided some
personal information, including gender, marital status, education and income. Then they were asked to
annotate what parts of their calls that they would like to take a note of: i.e. noteworthy fragments of
conversations. To this end, participants used a Web-based interface that gave them access to their calls
and allowed them to highlight with the mouse the parts of the transcript that they considered to be worth
keeping for future reference.
We used these annotations as the ground truth for the studies presented in this paper, considering them
as the ideal noteworthy parts of the calls. For privacy reasons, due to the sensible nature of the data
(i.e. private phone conversations) we could not consider alternative ground truth generation schemes, for
instance collecting annotations from users other than the callers themselves.
2
Finally, the participants were asked to fill out a questionnaire after annotating each call, which was
used to collect contextual information, including: location of the call (i.e at work, at home, while com-
muting, while doing shopping, while exercising), and category of the call (i.e. discuss a topic, taking an
appointment, give/receive information, asking a favor, social).
3.1 Characteristics of the Corpus
The original conversation collection consists of a total of 796 conversations, of an average length of 178
seconds (s = 384 sec.). We pre-filtered this original set to exclude calls with problems in the transcript
(e.g. empty transcript, only one speaker audible, etc). Out of the entire corpus we finally selected 659
conversations. We denote this subset of the corpus as the G dataset. The G dataset comprises 22, 474
turns, with an average of 34.10 (s = 45) turns per conversation. From these, only 671 are annotated as
being noteworthy (2.98%), which represent an average of 1.02 turns (s = 1.803) per call. Given that
the vast majority of turns (97.2%) are not annotated, this can be considered a highly unbalanced dataset,
which makes the automatic modeling problem more challenging.
Hence, we considered a second dataset which included only the 295 calls from the G dataset containing
at least one annotation. This second subset, denoted as A amounts for approximately 45% of the G
dataset. The A dataset features 10, 642 turns, with an average of 36.07 (s = 33) turns per conversation.
From these, again 671 (6.3%) are annotated, which represent an average of 2.275 (s = 2.09) per call.
The A dataset is still highly unbalanced but significantly less than the G dataset. Table 1 summarizes the
high level characteristics of each dataset.
Turns Annotated Turns
# Calls Total avg. per call Total Fraction
G 659 22, 474 34.1 (s = 45) 671 2.9%
A 295 10, 642 36 (s = 33) 671 6.3%
Table 1: General statistics on G andA datasets.
Class Annotations
I We are in front of the fruit shop
RoA Tomorrow we go to look for the swimsuit
RI Are you coming to eat? At what time
O Sure, it?s normal
Table 2: Examples of annotations.
Given the complexity of the modeling problem, we studied the note taking behavior of participants
to identify relevant patterns that would simplify the problem. To this end, we conducted a quantitative
analysis of the note taking behavior of participants. We found that users tend to highlight complete
1
Participants were given the opportunity to revise transcriptions during the annotation phase.
2
Receivers of the calls were aware of the study and were given the possibility to not participate in the call, but were not
directed involved in the study.
27
turns as relevant, instead of parts of the turns. On average, 66.57% (s = 35.87) of the words within an
annotated turn are highlighted, with a median value of 80%. Hence, we decided to use turns ?rather than
individual words? as the unit to be automatically detected as noteworthy. Using this approach, a turn is
considered to be noteworthy if it contains at least one annotated word.
3.2 Qualitative Analysis of the Corpus
Since our aim is to detect the noteworthy turns within a call, we conducted a preliminary qualitative
analysis to understand the nature of the annotations entered by the participants in the study. We distin-
guished 4 types of annotations: Giving Information (I), Requesting Information (RI), Reporting on an
Action (RoA) and Other (O). Examples of these 4 types of annotations are presented in Table 2. We
collected annotations from three collaborators of our lab for a total of 54 randomly selected turns from
the A dataset (IAA, Fleiss Kappa = 0.54 (Fleiss, 1971)).
We found that 47% of the turns were classified as belonging to the Giving Information category, 22%
of the turns to the Request Information category, 26% to the Other category, and only 3% were classified
as Report on an Action. Intuitively, we had expected the Giving Information category to be the most
common in the annotated turns. However, the results obtained show that the other types of annotations
are also well represented in the data.
Two main interesting aspects emerge. First, while the vast majority of annotations correspond to turns
where a piece of information is given (e.g. We meet at 3pm), turns where information is requested are
also well represented in the sample. There are plausible explanations for this behavior, such as users
trying to include more context in the annotations. Second, more than 25% of this manually annotated
dataset was marked under the Other category, which includes turns with very diverse functionalities
(e.g. greetings, statements of agreement). This reveals that participants tend to annotate turns with very
diverse functional aspects, which poses a challenge to be added to the unbalanced nature of the dataset.
4 Feature Extraction
We follow a supervised machine learning approach to automatically detect noteworthy turns in conver-
sations. In this section we describe the features that we compute to represent conversations and which
have been engineered to capture information relevant to the problem at hand. We have divided the set
of features into two categories: Content features, that we denote with the letter C, and conteXt features,
that we denote with the letter X.
4.1 Content Features
Content features are computed by analyzing the content of the conversations. We use as input the textual
information resulting from the semi-automatic transcription of the calls. Note that we do not make use of
any conversational acoustic information. While the analysis of the acoustic signal may reveal additional
cues useful for noteworthiness detection, it lies out of the scope of this work.
In order to extract features from the transcript, we first pre-proces the datasets (split in turns, lemma-
tized, PoS tagged). Also, we extract and classify Named Entities (NEs).
3
We extract 42 content-based
features which include both variations of features previously used in the meeting summarization litera-
ture and novel features particularly adapted to our task. However, in contrast to related work on meeting
summarization, we do not extract content features based on lexical similarity to the entire call or to
the main topic of the call, under the intuition that the notion of noteworthiness depends on the user?s
needs rather than on the main topic of the conversation. In addition and for robusteness purposes, we
decided not to rely on long distance dependency information (e.g. argument predicate relations) or deep
syntactical parsing, which are sensitive to the quality of the transcription.
The resulting features are grouped into three main classes: Turn-Based (C-T), Dynamic (C-D), and
Conversational (C-C). We compare them with a pure bag-of-words (BoW) representation. Table 3a
provides a summary of all the content-based features used in our system. Where applicable, we experi-
3
All pre-processing was performed using the Freeling Language Processing tools (Padro et al., 2010).
28
ment with two vector representations: binary and frequency-based. We will refer to these two different
encoding schemes as Bin for the binary case, and Freq for the frequency case.
CONTENT FEATURES
C-BoW (Bag of Words)
BoW
BoW for all words (except hapax)
C-T (Turn-based)
NE
Presence (or frequency) of NEs (Person, Location, Organization,
Numbers, Dates, Misc.)
TLN
Turn length in # words normalized
PoS
PoS distribution
TF
Max and Mean term frequency
IDF
Max and Mean inverse document frequency
C-D (Dynamic)
Rep
Repetition between t and t-1,t+1,t-2,t+2
Int
Presence (or total amount) of Int. pro./adj. in t-1
Q
Presence (or total amount) of question in t-1
C-C (Conversational)
Dur
Duration of the call (# turns and # words)
Cent
Conversation centrality
Spk
Speaker
Dom
Speaker dominance
(a) Content Feature
CONTEXT FEATURES
X-C (Call-based)
X-C-T
Time of the call
X-C-Loc
Location of the call
X-C-Day
Day of the call
X-C-Obj
Objective of the call
X-U (User-based)
X-U-G
Gender
X-U-A
Age
X-U -I
Income
X-U-E
Education
X-U-Ms
Marital Status
(b) Context Feature
Table 3: Content (a) and Context (b) based features.
4.1.1 Turn-Based Content features (C-T)
Turn-based content features take into account information related to individual turns. We distinguish
lexical and non-lexical C-T.
Lexical content features: Lexical C-T features capture the lexical properties of a turn. We include
NEs, such as Locations, Organizations, Persons, Miscs and Numbers, Dates, and temporal expressions.
For each turn t, we detect the presence of any NE as well as the presence of individual classes of NEs.
For each of these class of entities, we extract both a binary and a frequency feature vector. In the
text summarization literature, the appearance of particular lexical phrases (e.g. to summarize) has been
exploited to predict relevant sentences (Gupta and Lehal, 2010). In our study, attention has been given
to the presence of temporal expressions under the intuition that temporal cues are good indicators of
upcoming pieces of information (e.g. The meeting is tomorrow). We exploit temporal expressions, such
as today, tomorrow, etc.
4
Non-lexical content features: capture characteristics of the turn which do not involve lexical infor-
mation, namely: turn length, Part-of-Speech (PoS) distributions and Tf-Idf descriptive statistics at the
turn level.
In meeting summarization, the average length of a turn has been found to be a good feature to automat-
ically create a summary of a meeting (Xie et al., 2008). In our dataset, preliminary analyses revealed that
annotated turns tend to be longer in average. Hence, we include the turn length in the non-lexical content
feature set. The turn length is given by the number of tokens per turn normalized over the average turn
4
Note that, here and in the remainder of the paper, we report the English translations of the Spanish originals.
29
length (punctuation excluded). To further gauge discourse characteristics, we detect the distribution of
PoS at the turn level: i.e. for each turn, the frequency of nouns, pronouns, adjectives, adverbs, interjec-
tions, verbs, prepositions and conjunctions is calculated. Finally, we compute the term frequency (Tf)
and inverse document frequency (Idf) measures. In (Xie et al., 2008), authors report that Idf is among
the most discriminative features in sentence selection for text summarization. We compute maximum
and mean Tf and Idf values for each turn.
4.1.2 Dynamic content features (C-D)
Dynamic content features are designed to capture the semantic relationships between each turn and its
precedent and subsequent turns. In particular we refer to relations such as lexical and topical cohesion,
question-answer relationship, and the appearance of general cues that may anticipate relevant bits of
information in the subsequent turn. We consider: 1) the lexical and topical cohesion among consecutive
turns (Repetitions); 2) the appearance of general cues that may anticipate relevant bits of information
in the subsequent turn (Interrogative Pronouns); 3) the question-answer relationship among consecutive
turns (Question).
Repetitions: words repeated by different speakers in consecutive turns. Participants of a conversation
tend to align at several linguistic and paralinguistic levels in order to ease communication and increase
mutual understanding (Pickering and Ferreira, 2008). This phenomenon has been investigated in terms
of prosody, lexicon and syntax (Levitan and Hirschberg, 2011; Brennan, 1996; Bonin et al., 2013; Brani-
gan et al., 2010). From a lexical point of view, the alignment mechanism, often referred to as priming, is
realized by means of word repetitions among speakers. Many studies have investigated this phenomenon
assessing correlation between priming and mutual understanding or dialogue success (Vogel, 2013; Re-
itter and Moore, 2007).
We exploit the priming phenomenon to detect concepts in the conversation that are considered impor-
tant by both participants, relying on the fact that repeated words convey concepts that participants want
to make sure they have been successfully communicated to their interlocutor. Given a dataset D, a turn
in D, t ? D, and t ? i and t + i turns in the context of t, we calculate the amount of repeated lemmas
between t and t?i, and t and t+i for 1? i ? 2. In order to consider semantically meaningful repetitions,
we take into account only content words (nouns, adjectives, adverbs, verbs) when they activate one of the
C-T features described above. Being A the set of annotated turns, we noticed a significant difference in
the amount of repeated lemmas between t,t? i for t ? A rather than for t /? A. Find below an example
of consecutive turns with repetitions:
Turn Utterance
t-1: Starting at half past four.
t: Starting at half past four, yes.
Interrogative pronouns and questions: We also exploit indicators of an upcoming giving infor-
mation act. As shown in Sec 3.2, 47% of the annotations were marked as giving information, which
may have been triggered by a request of information in the precedent turn. Hence, in order to capture
these cases, we identify linguistic elements that indicate a request of information in t? 1 (questions and
interrogative pronouns/adjectives).
4.1.3 Conversational flow features (C-C)
They are designed to model information about the conversation?s flow and speakers? interaction.
Centrality of the turn: Distance of a turn from the center of the conversation. This feature is inspired
by the sentence location features used in text summarization (Chen et al., 2002). Chen et al. assign
different weights to sentences in the first, middle and final part of a paragraph, in order to favor sentences
that are in the central part of the paragraph as they are considered to be more informative for a summary.
In our corpus, we noticed the tendency of users to annotate turns that are in the central portion of the
conversation. Typically the first and the last quarters of the phone conversations are dedicated to social
talk. Hence, we introduce a temporal feature, referred to as conversation centrality, that captures the
distance of a turn from the center of the conversation. This distance is measured in terms of number of
words, excluding punctuation.
30
Speaker: Who is uttering the turn (caller vs callee).
Conversation duration: Length of the conversation in number of turns and in number of words. The
number of turns captures the dynamics of a dialogue (few longer turn vs a more dynamic exchange),
while the number of words captures the overall duration.
Speaker dominance: We consider whether the speaker is the dominant speaker of the conversation,
defining dominance in terms of amount of productions during the call. This is calculated by comparing
the number of turns of speaker a vs speaker b, normalized over the total amount of turns per call.
4.1.4 Bag-of-Words (BoW)
Finally, we explore the performance of a naive bag-of-words scheme to represent the content at the turn
level. Given the large vocabulary size of our corpus (10, 144 tokens) and the sparsity organic to bag-of-
word representations, we decided to use a trivial dimensionality reduction strategy filtering out the terms
that appear only once in the corpus. We decided not to apply a stop-list of functional words for further
reducing the feature space. This decision was based on the higher discrimintative power we observed
when comparing classification accuracy with and without them. We discarded the use of more aggressive
feature selection approaches (e.g. mutual information) to allow for a fair comparison of accuracy with
the rest of feature representations described in the paper. In total, our BoW representation had 5, 048
dimensions in the G dataset, and 3, 219 dimensions in the A dataset.
4.2 Context Features
Context features are introduced under the assumption that noteworthy information may depend on the
characteristics of the user and on the situation in which the call takes place. For example, people may
not need to annotate pieces of information that are part of their daily lives. Whereas while taking an
appointment, it is plausible the need to annotate the name of the doctor, in a social call with a friend, the
name of the friend is part of the background knowledge of the user. Therefore, while from a content (and
an NLP) point of view both names are Person NEs and carry the same amount of information, from the
point of view of the user they might have different weight (no need of taking note vs need of taking note).
Also, the current situation or location of the user may influence the necessity of taking notes: a user in a
supermarket will not need to annotate to buy milk, (s)he will rather take it directly from the shelf. A user
driving to the supermarket will need to keep in his/her mind the need to buy milk for later recall.
In line with this, we noted in Section 2 that pure NLP approaches applied to automatically detecting
noteworthy information in meetings are able to achieve an F-score of only 0.14. This low F-score under-
lines the complexity of the task and the limitations of a pure content-based approach. Contextual cues
may be used to increase the discriminative power of the classification model.
Since we consider the specific scenario of cellphone conversations, we can exploit contextual informa-
tion derived from the use of the mobile network, such as geo-location, and temporal information. Other
contextual features that we use, gathered during the pre-study questionaire, are organically much more
challenging to infer. We still decided to consider these as a way to assess the potential of several types of
contextual information with respect to the discriminative power of the classifier. We distinguish among
Call-based (X-C) and User-based (X-U) contextual features. A schematic overview of these features is
given in Table 3b.
4.2.1 Call-Based Features (X-C)
Call-based features are meant to capture contextual information at the call level. In particular, X-C
features include information about where, when and for what reason a call is made, under the intuition
that calls made, for example, during working hours may have different noteworthy information than calls
made in the weekend. We distinguish six location categories: home, work place, while commuting,
while exercising, while shopping, other. The location of the calls was provided by participants through
the post-call questionnaire. However location information is typically available from the mobile network.
In terms of temporal features, we consider the actual time of the call (over 24 hours). In addition, we
classify the time in two classes: working vs non working hours, and the day in also two classes: weekday
vs weekend. Finally, we also consider the objective of the call as described in Section 3. Note that,
31
although this information is not directly accessible from mobile data collected during the call, previous
literature on conversation classification supports the feasibility of inferring this information from the
content of the conversation (Koc?o et al., 2012).
4.2.2 User-Based Features (X-U)
Finally, we introduce a set of features that feed the model with information about the user. We exploit
information that could be provided by users upon registration to such a note-taking service. We capture
age, gender, educational level, income and marital status. Gender is represented as a binary feature, while
age is categorized in 5 groups: below 20 years old, between 20 and 30, between 30 and 40, between 40
and 50 and above 50. The education status is represented by the following categories: Primary education,
Secondary education, Bachelor degree or a Postgraduate education (Master or PhD). Yearly income is
categorized by: up to 10k, 20k, 30k, 40k and more than 40k. Finally, marital status is categorized as:
single, in a couple (married, with a stable partner), other.
5 Experiments
The goal of our system is to automatically identify information annotated by users in terms of its potential
need for future recall. We frame this problem as a binary classification task (noteworthy or not) at the
turn level. This task presents two main challenges. First, our dataset is extremely unbalanced, with less
than 3% of the corpus labeled as relevant by the participants. Second, the subjectivity of the task leads
to high variability of annotation behaviours, (see Sec. 3.2). In this section we describe the experimental
setting that we used to empirically evaluate the performance of different features sets and present the
results obtained using the ground truth data collected (Section 3) to provide classification performance
scores. In order to fully investigate the predictive performance of the different feature sets, we conducted
our experiments using both the entire corpus G, which includes all the selected conversations, and its
subset A, which considers only the calls with at least one annotation. Both sets are described in Sec. 3.
We experimented using both encoding schemes described in Section 4: binary based (Bin) and frequency
based (Freq).
We used Support Vector Machines (SVMs) with RBF kernel, as this classification approach yielded
the most consistent results throughout all the evaluated configurations. We used the same random split of
training and test sets for all the experiments, accounting for 70% and 30% of the dataset respectively. We
tune the hyperparameter C of the SVM model using a 3-fold cross-validation approach on the training
data only, where we chose F-score as the quality metric to optimize. Given the nature of our task, recall
is preferred to precision from a user-centric perspective: it is preferable to avoid missing any relevant
information than to include some non-relevant fragments. For this reason, we also report precision and
recall values.
5.1 Classification Results
This section presents the results obtained in our binary classification task (turns being noteworthy or not).
We study the performance of different combinations of features and present the results obtained using
only content information (C), and the combination of content and context information (CX).
5.1.1 Content features
We present a comparison of the different content feature sets using the naming scheme of Sec.4. We
considered four classification scenarios: C-T only, C-D only, the combination of C-T and C-D (C-TD),
and the combination of C-T, C-D and C-C (C-TDC). The results of these feature sets are shown in
Tables 4a and 4b for the G and A collections, respectively.
As shown in Table 4a, the maximum F-score for the G dataset is achieved for the combination of
all content features included the BoW. The low score (F = 0.18) is a direct consequence of the low
precision obtained (P = 0.11). For the A dataset (Table 4b) we observe a better F-score (F = 0.296),
still obtained by the combination of all content features, with a much higher precision (P = 0.18) due to
the significant amount of noise removed by considering only annotated calls. Note that in both the G and
the A datasets the C-TDC feature set outperforms the pure BoW approach (F = 0.158 vs. F = 0.14
32
Features Precision Recall F-score
Encoding Bin Freq Bin Freq Bin Freq
BoW 0.081 0.083 0.730 0.720 0.150 0.150
C-T 0.087 0.088 0.53 0.32 0.15 0.139
C-D 0.03 0.26 0.26 0.12 0.15 0.05
C-TD 0.087 0.09 0.754 0.33 0.1505 0.1419
C-TDC 0.09 0.093 0.58 0.37 0.158 0.149
C-TDC+BoW 0.11 0.11 0.52 0.51 0.18 0.18
(a) Results for the G dataset.
Features Precision Recall F-score
Encoding Bin Freq Bin Freq Bin Freq
BoW 0.14 0.14 0.54 0.54 0.22 0.22
C-T 0.135 0.147 0.56 0.555 0.218 0.23
C-D 0.149 0.11 0.24 0.15 0.18 0.12
C-TD 0.143 0.143 0.506 0.546 0.223 0.227
C-TDC 0.165 0.159 0.626 0.693 0.254 0.267
C-TDC+BoW 0.188 0.1659 0.57 0.568 0.283 0.296
(b) Results for the A dataset.
Table 4: Classification performance of Content features, BoW and their combination.
for G and F = 0.267 vs. F = 0.22 for A), using a fraction (about 1%) of the number of BoW features,
which leads to a considerably simpler model. On the other hand, the combination of C-TDC and BoW
features improves the results up to F = 0.18 for G (P = 0.11, R = 0.52) and F = 0.296 (P = 0.20,
R = 0.57) for the A subset. This result highlights how the lexical representation comprised by the BoW
provides the model with orthogonal information to the one provided by the C-TDC features set.
To the best of our knowledge no previous work has been done in noteworthy detection from telephone
conversations. For this reason, we report as a reference the results of the more similar prior art to our
work, (Banerjee and Rudnicky, 2008), where the authors implement an SVM classifier for the detection
of noteworthy information in meetings.
5
Although aware of the different nature of the dataset, these
results are reported to get a sense of the potentiality of the system. The best performance of our model
on the A dataset improves in 15% the F-score of F = 0.14 reported in (Banerjee and Rudnicky, 2008).
5.1.2 Combining Content and Context Features
In this section we report the performance of the model trained using both content and context features.
For simplicity, in the remainder of this section we refer to the entire set of content features, (C-TDC)
as C, to the entire set of context features as X, and to their combination as CX. When we test adding
BoW features the +BoW naming is used. The results are shown in Table 5 and Figure 1. We observe
that the fusion of content and context features (CX and CX+BoW) provides a noticeable overall increase
in the F-score for both datasets. This increase is particularly high for the G dataset, where the F-score
gets increased by almost a factor of 2, from F = 0.18 to F = 0.28. On the A dataset, the combination
of content and context features improves the F-score from F = 0.29 to F = 0.32, given by a better
precision (P = 0.24 vs P = 0.18) with similar recall.
Features Precision Recall F-score
Rep. B F B F B F
C+BoW 0.11 0.11 0.52 0.51 0.18 0.18
X 0.068 0.068 0.665 0.665 0.124 0.124
CX 0.169 0.20 0.38 0.286 0.2354 0.2394
CX+BoW 0.189 0.1919 0.524 0.5022 0.288 0.277
(a) Results for the G dataset.
Features Precision Recall F-score
Rep. B F B F B F
C+BoW 0.188 0.1659 0.57 0.568 0.283 0.296
X 0.087 0.087 0.56 0.56 0.15 0.15
CX 0.2075 0.212 0.5866 0.595 0.3066 0.3130
CX+BoW 0.223 0.2455 0.573 0.426 0.3212 0.3116
(b) Results for the A dataset.
Table 5: Classification performance using the combination of context and context-based features
Bin Freq
0.0
0.1
0.2
0.3
X C+BoW CX CX+BoW X C+BoW CX CX+BoW
F?
sc
o
re
(a) Results for the G dataset.
Bin Freq
0.0
0.1
0.2
0.3
X C+BoW CX CX+BoW X C+BoW CX CX+BoW
F?
sc
o
re
(b) Results for the A dataset.
Figure 1: Classification performance using Content, Context features and their combination
This result gives empirical evidence that these two sets of features convey complementary information
5
In their experimental settings all the meetings have at least one annotation as in our A scenario.
33
that is relevant for the task at hand. That is, the same words can carry different relevance depending on
the contextual information of the conversation.
Note that the BoW features add discriminative information in the G scenario, but have a minimal
effect in the less noisy A scenario where the combination of content and context features, without BoW,
provides already an F-score of F = 0.31.
An interesting remark about this combined model is that the difference in performance between the
G and the A dataset is vastly reduced. While in the pure content model the difference in F-score value
between both datasets was 0.10, in the combined model this difference is just 0.03. This result shows
that the combination of content and context features boosts considerably the results in the more noisy
and realistic dataset G, while its effect is weaker in the cleaner dataset.
5.2 Qualitative Analysis
In order to better understand the failure cases in our system, we carried out a qualitative analysis of both
false positives, i.e. turns annotated by the system but not by the user, and false negatives, i.e. turns
annotated by the user but not by the system. Table 6 illustrates a few representative examples. Note how
the proposed system does not perform well when detecting a request for information as something worth
annotating (e.g. What are you doing?). We noticed that in these cases, the model tended to annotate turns
where the information was actually provided (e.g.That is the package has arrived). We can hypothesize
that users annotate the request for information to give context to the a-priori more relevant information,
i.e. the answer to the question. However, in some cases, participants did not annotate the answer as
relevant. This counter-intuitive observation reflects the subjectivity and variability of the task.
False Positive False Negative
I am leaving soon, I start at 3 o?clock or [..] How are you? Can you hear me?
Let?s see if we can tell him. What are you doing?
That is, the package has arrived. Did you buy beautiful things for me?
Table 6: Examples of false positive and false negative turns.
5.3 Comparative Analysis and Discussion
To the best of our knowledge there are no previous works of similar nature to the study presented in this
paper. Yet, it is important to give a sense of the merits and limitations of the proposed approach in the
context of the state-of-the-art. For this reason, we compare our results with (Banerjee and Rudnicky,
2009), which is the most similar prior art to our work. In (Banerjee and Rudnicky, 2009), Barnejee et
al. perform a Wizard-of-Oz experiment and report a performance of the human annotator of P = 0.35
precision, R = 0.42 recall, leading to an F-score of F = 0.38. This result highlights the difficulty of
the task, even for a human annotator. When comparing our proposed system with this Wizard-of-Oz
experiment, we obtain an F-score of F = 0.32 against the human annotator?s F-score of F = 0.38,
with a significantly higher recall (0.57 vs 0.42) yet lower precision (0.245 vs 0.35). Given this human-
based prediction performance, the proposed approach represents a good first step towards realizing an
intelligent annotation system for mobile phone conversations.
6 Conclusions and Future Work
In this paper we have proposed and empirically evaluated a machine learning-based approach to auto-
matically detect noteworthy information in spontaneous mobile phone conversations. The subjectivity of
this task leads to a challenging classification problem even for human assessors. Our approach adopts a
hybrid strategy that exploits the content and the context of the conversation. We have shown that infor-
mation about the context of the conversation improves the predictive performance of the system over a
pure content based approach.
In the future, we plan to extend the model by including acoustic features which could improve the
performance by adding orthogonal information to the current model. To tackle the subjectivity of the task
we also intend to investigate the performance of personalization techniques, creating individual models
per user. Finally, we plan to conduct a study to evaluate our system from a user-centric perspective.
34
Acknowledgments
This work was partially supported by the Innovation Bursary of Trinity College Dublin (project: ?Tech-
nology for Harmonising Interpersonal Communication?).
References
Satanjeev Banerjee and Alexander I. Rudnicky. 2008. An extractive-summarization baseline for the automatic
detection of noteworthy utterances in multi-party human-human dialog. In Proceeding of SLT, pages 177?180.
Satanjeev Banerjee and Alexander Rudnicky. 2009. Detecting the noteworthiness of utterances in human meet-
ings. In Proceedings of the SIGDIAL 2009 Conference, pages 71?78, London, UK, September. Association for
Computational Linguistics.
Francesca Bonin, Celine De Looze, Sucheta Ghosh, Emer Gilmartin, Carl Vogel, Anna Polychroniou, Hugues
Salamin, Alessandro Vinciarelli, and Nick Campbell. 2013. Investigating fine temporal dynamics of prosodic
and lexical accommodation. In Proceedings of Interspeech 2013, Lyon, France, August.
H.P. Branigan, M.J. Pickering, J. Pearson, and J.F. McLean. 2010. Linguistic alignment between people and
computers. Journal of Pragmatics, 42(9):2355?2368.
Susan E. Brennan. 1996. Lexical entrainment in spontaneous dialog. Proceedings of ISSD, pages 41?44.
Juan Pablo Carrascal, Rodrigo de Oliveira, and Mauro Cherubini. 2012. A note paper on note-taking: understand-
ing annotations of mobile phone calls. In Proceedings of the 14th international conference on Human-computer
interaction with mobile devices and services, MobileHCI ?12, pages 21?24, New York, NY, USA. ACM.
Fang Chen, Kesong Han, and Guilin Chen. 2002. An approach to sentence-selection-based text summarization.
In TENCON?02. Proceedings. 2002 IEEE Region 10 Conference on Computers, Communications, Control and
Power Engineering, volume 1, pages 489?493. IEEE.
Chandrika Cycil, Mark Perry, Eric Laurier, and Alex Taylor. 2013. ?eyes free? in-car assistance: parent and child
passenger collaboration during phone calls. In Proceedings of the 15th international conference on Human-
computer interaction with mobile devices and services, MobileHCI ?13, pages 332?341, New York, NY, USA.
ACM.
Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin,
76(5):378?382.
Michel Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ?06,
pages 364?372, Stroudsburg, PA, USA. Association for Computational Linguistics.
Nikhil Garg, Benoit Favre, and Dilek Hakkani-T?ur. 2009. Clusterrank: a graph based method for meeting sum-
marization. In Technical Report, IDIAP.
Vishal Gupta and Gurpreet Singh Lehal. 2010. A survey of text summarization extractive techniques. Journal of
Emerging Technologies in Web Intelligence, 2(3):258?268.
Po Hu, Dong-Hong Ji, Chong Teng, and Yujing Guo. 2012. Context-enhanced personalized social summarization.
In COLING, pages 1223?1238.
J Jian Zhang, Ho Yin Chan, and Pascale Fung. 2007. Improving lecture speech summarization using rhetorical
information. In Automatic Speech Recognition & Understanding, 2007. ASRU. IEEE Workshop on, pages 195?
200. IEEE.
Sokol Koc?o, C?ecile Capponi, and Fr?ed?eric B?echet. 2012. Applying multiview learning algorithms to human-
human conversation classification. In INTERSPEECH.
Rivka Levitan and Julia Hirschberg. 2011. Measuring acoustic-prosodic entrainment with respect to multiple
levels and dimensions. In Twelfth Annual Conference of the International Speech Communication Association.
Sameer Maskey and Julia Hirschberg. 2003. Automatic summarization of broadcast news using structural features.
In INTERSPEECH.
35
Sameer Maskey and Julia Hirschberg. 2005. Comparing lexical, acoustic/prosodic, structural and discourse fea-
tures for speech summarization. In INTERSPEECH, pages 621?624.
Sameer Maskey and Julia Hirschberg. 2006. Summarizing speech without text using hidden markov models. In
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,
pages 89?92. Association for Computational Linguistics.
Lluis Padro, Samuel Reese, Eneko Agirre, and Aitor Soroa. 2010. Semantic services in freeling 2.1: Wordnet and
ukb. In Pushpak Bhattacharyya, Christiane Fellbaum, and Piek Vossen, editors, Principles, Construction, and
Application of Multilingual Wordnets, pages 99?105, Mumbai, India, February. Global Wordnet Conference
2010, Narosa Publishing House.
Martin J. Pickering and Victor S. Ferreira. 2008. Structural priming: a critical review. Psychological bulletin,
134(3):427.
David Reitter and Johanna D Moore. 2007. Predicting success in dialogue. In Annual Meeting-Association for
Computational Linguistics, volume 45, page 808.
Jose San Pedro, Vaiva Kalnikaite, and Steve Whittaker. 2009. You can play that again: Exploring social redun-
dancy to derive highlight regions in videos. In Proceedings of the 14th International Conference on Intelligent
User Interfaces, IUI ?09, pages 469?474, New York, NY, USA. ACM.
Carl Vogel. 2013. Attribution of mutual understanding. Journal of Law & Policy, pages 101?145.
Dong Wang and Yang Liu. 2011. A pilot study of opinion summarization in conversations. In Annual Meeting-
Association for Computational Linguistics, ACL, pages 331?339.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating the effectiveness of features and sampling in extractive
meeting summarization. In Spoken Language Technology Workshop, 2008. SLT 2008. IEEE, pages 157?160.
IEEE.
Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhong Su, and Juanzi Li. 2011. Social context summarization. In
Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR ?11, pages 255?264, New York, NY, USA. ACM.
Tom Yeh, Brandyn White, Jose San Pedro, Boriz Katz, and Larry S. Davis. 2011. A case for query by image and
text content: Searching computer help using screenshots and keywords. In Proceedings of the 20th International
Conference on World Wide Web, WWW ?11, pages 775?784, New York, NY, USA. ACM.
Yongzheng Zhang, Nur Zincir-Heywood, and Evangelos Milios. 2005. Narrative text classification for automatic
key phrase extraction in web document corpora. In Proceedings of the 7th annual ACM international workshop
on Web information and data management, WIDM ?05, pages 51?58, New York, NY, USA. ACM.
36
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 77?80,
Beijing, August 2010
Contrastive Filtering of Domain-Specific Multi-Word Terms from
Different Types of Corpora
Francesca Bonin? ?, Felice Dell?Orletta?, Giulia Venturi? and Simonetta Montemagni?
? Istituto di Linguistica Computazionale ?Antonio Zampolli? (ILC-CNR)
?Dipartimento di Informatica, Universita` di Pisa,
?CLIC Language Interaction and Computation Lab
{francesca.bonin, felice.dellorletta,
giulia.venturi, simonetta.montemagni}@ilc.cnr.it
Abstract
In this paper we tackle the challenging
task of Multi-word term (MWT) extrac-
tion from different types of specialized
corpora. Contrastive filtering of previ-
ously extracted MWTs results in a con-
siderable increment of acquired domain-
specific terms.
1 Introduction
Multi-word term (MWT) extraction is a challeng-
ing and well-known automatic term recognition
(ATR) subtask, aimed at retrieving complex do-
main terminology from specialized corpora. Al-
though domain sublanguages are characterized by
specific vocabularies, a well-defined border be-
tween specific sublanguages (SLs) and general
language (GL) vocabularies is difficult to establish
since lexicon shifts in a continuum from a highly
specialized area to a transition area between GL
and SLs (Rondeau et al, 1984). Within this con-
tinuum, Cabre? (1999) identifies three types of lex-
ical items: a. GL lexical items; b. SL terms, c.
lexical items belonging to a borderline area be-
tween GL and SL. The proportion of these dif-
ferent types of lexical items varies depending on
the text type. To our knowledge, automatic term
recognition methods proposed so far in the litera-
ture focussed on highly specialized corpora (typ-
ically, technical and scientific literature), mainly
characterized by SL terminology. However, the
same ATR methods may not be equally effective
when dealing with corpora characterized by a dif-
ferent proportion of term types; e.g. from texts
such as Wikipedia articles, which are conceived
for a more extended audience, both SL terms and
common words are acquired as long as they show
a statistically significant distribution. In this pa-
per, we claim that the contrastive approach to
MWT extraction described in Bonin et al (2010)
can be effectively exploited to distinguish be-
tween common words and domain-specific termi-
nology in different types of corpora as well as to
identify terms belonging to different SLs when oc-
curring in the same text. The latter is the case of
legal texts, characterized by a mixture of differ-
ent SLs, the legal and the regulated-domain SLs
(Breuker et al, 2004). Effectiveness and flexibil-
ity of the proposed ATR approach has been tested
with different experiments aimed at the extrac-
tion of domain terminology from corpora charac-
terized by different degrees of difficulty as far as
ATR is concerned, namely (i) environmental sci-
entific literature, (ii) Wikipedia environmental ar-
ticles, and (iii) a corpus of legal texts on environ-
mental domain.
2 General Extraction Method
The MWT extraction methodology we follow is
organized in two steps, described in detail in
Bonin et al (2010). Firstly, a shortlist of well-
formed and relevant candidate MWTs is extracted
from a given target corpus and secondly a con-
trastive method is applied against the selected
MWTs only. In fact, in the first stage, candi-
date MWTs are searched for in an automatically
POS-tagged and lemmatized text and they are then
weighted with the C-NC Value method (Frantzi et
al., 1999). In the second stage, the list of MWTs
extracted is revised and re-ranked with a con-
trastive score, based on the distribution of terms
across corpora of different domains; in particu-
77
lar, the Contrastive Selection of multi-word terms
(CSmw) function, newly introduced in Bonin et
al. (2010), was used, which proved to be partic-
ularly suitable for handling variation in low fre-
quency events. The main benefit of such an ap-
proach consists in its modularity; by first selecting
valid MWTs which have significant distributional
tendencies, and then by assessing their domain-
relevance using a contrastive function, the MWT
sparsity problem is overcome or at lest signifi-
cantly reduced.
3 Experiments
The MWT extraction methodology described
above has been followed in order to acquire envi-
ronmental terminology from three different kinds
of domain corpora. The first experiment has been
carried out on a corpus of scientific articles con-
cerning climate change research of Italian Na-
tional Research Council (CNR), of 397,297 to-
kens, while the second experiment has been car-
ried out on a corpus of Wikipedia articles from
the Italian Portal ?Ecologia e Ambiente? (Ecol-
ogy and Environment) (174,391 tokens). As gen-
eral contrastive corpus, we used, in both cases,
the PAROLE Corpus (Marinelli et al, 2003)1, in
order to filter out GL lexical items. The third
and more challenging experiment has been car-
ried out on a collection of Italian European legal
texts concerning the environmental domain for a
total of 394,088 word tokens. In this case, as con-
trastive corpus we exploited a collection of Ital-
ian European legal texts regulating a domain other
than the environmental one2, in order to extract
MWTs belonging to the environmental domain,
but also to single out legal-domain terms, used in
legal texts. For each acquisition corpus we fol-
lowed the two-layered approach described above,
selecting, firstly, a top list of 2000 environmental
MWTs from the candidate term list ranked on the
C-NC Value score and, secondly, re-ranking this
2000-term list on the basis of the CSmw function;
then we extracted the final top list of 300 envi-
ronmental MWTs. In order to assess the effec-
1It is made up of about 3 million word tokens and it in-
cludes Italian texts of different types.
2A corpus of Italian European Directives on consumer
protection domain for a total of 74,210 word tokens.
tiveness of the approach against different types of
corpora, we analyzed the two 300-term top lists
of MWTs acquired respectively after the first and
the second extraction steps. In both cases, we
divided the 300-term top lists in 30-term groups
which show domain-specific terms? distribution,
so that they could be easily compared. The eval-
uation has been carried out by comparing the lists
of MWTs extracted against a gold standard re-
source, i.e. the thesaurus EARTh (Environmen-
tal Applications Reference Thesaurus).3. In ad-
dition, a second resource has been used in the
third experiment for evaluating legal terms: the
Dizionario giuridico (Edizioni Simone)4. Those
terms which could not find a positive matching
against the gold standard resources were manually
validated by domain experts.
Scient.Lit. Wikipedia
Group C-NC CSmw C-NC CSmw
0-30 22 27 27 29
30-60 28 25 28 26
60-90 24 30 25 25
90-120 19 28 23 27
120-150 25 29 23 24
Sub-TOT 118 139 126 131
150-180 25 25 22 20
180-210 23 27 20 30
210-240 24 29 23 26
240-270 23 25 24 24
270-300 21 19 15 25
TOT 234 264 230 256
Table 1: Environmental terms in the 300-term top
lists from scientific articles (columns 2 and 3) and
from Wikipedia (columns 4 and 5).
3.1 Discussion of Results
Achieved experimental results highlight two main
issues. Firstly, they show that the proposed con-
trastive approach to domain-specific MWTs ex-
traction has a general good performance. As Fig-
ures 1, 2 and 3 show, the amount of environ-
mental MWTs after the contrastive stage increases
with respect to the amount of MWTs acquired af-
ter the candidate MWT extraction stage carried
3http://uta.iia.cnr.it/earth.htm#EARTh%202002. Con-
taining 12,398 environmental terms.
4Available online: http://www.simone.it/newdiz and in-
cluding 1,800 terms.
78
C-NC Value CSmw
Group Env Leg Env Leg
0-30 12 12 21 4
30-60 10 8 16 4
60-90 11 10 20 3
90-120 22 1 19 3
120-150 10 13 13 6
Sub-TOT 65 44 89 20
150-180 9 13 14 6
180-210 13 10 17 6
210-240 16 5 11 9
240-270 11 9 16 9
270-300 12 8 9 13
TOT 126 90 156 63
Table 2: Env(ironmental) and Leg(al) MWTs in
the 300-term top list from the legal corpus.
Type of text % relative increment
Wikipedia 11.30%
Scientific articles 12.82%
Legal texts 23.81%
Table 3: Relative increment of environmental
MWTs in the contrastive re-ranking stage.
out with the C-NC Value method. Secondly, re-
ported results witness that such performances are
differently affected by the different types of in-
put corpora: as summarized in Table 3, the rela-
tive increment of environmental MWTs after the
contrastive filtering stage ranges from 11.3% to
23.81%. Interestingly, as shown in Table 1, the
results obtained in the first and second experi-
ments show similar trends. This is due to the over-
whelming occurrence in the two input corpora of
specialized terminology with respect to the GL
items. Differently from what could have been
Figure 1: Scientific articles. Comparative pro-
gressive trend of environmental extracted terms.
expected, Wikipedia texts contain highly special-
ized terminology. However, a qualititative evalu-
ation of MTWs extracted revealed that this latter
corpus includes terms which belong to that bor-
derline area between GL and SL (case c. in the
Cabre? (1999) classification). It follows that in
the Wikipedia case the contrastive stage filtered
out not only common words, such as milione di
dollari ?a million dollars?, but also terms such as
unita` immobiliare ?real estate? belonging to such
borderline area of terminology; their difficult clas-
sification slightly decreases the contrastive stage
performance.
In the third experiment, the total amount of
environmental MWTs percentually increased by
23.81% after the second stage of contrastive re-
ranking. Differently from the previous experi-
ments, in this case we faced the need for dis-
cerning terms belonging to the vocabulary of two
SLs, i.e. regulated domain (i.e. environmental)
terms and legal ones (e.g. norma nazionale, na-
tional rule): this emerges clearly from the results
reported in Table 2 where it is shown that the
same number of environmental and legal MWTs
(i.e. 12 terms) are extracted at the first stage in
the first 30-term group, and that the contrastive
re-ranking allows the emergence of 21 environ-
mental MWTs against 4 legal MWTs only. This
trend can be observed in Figure 4, where the di-
vergent lines show the different distributions of
environmental and legal terms: interestingly, lines
cross each other where legal terms outnumber en-
vironmental terms, i.e. in the last 30-term group.
Such a relative increment with respect to the C-
NC Value ranking can be easily explained in terms
of the main features of the two methods, where C-
NC Value method is overtly aimed at extracting
domain-specific terminology (both environmental
and legal terms), and the contrastive re-ranking
step is specifically aimed at distinguishing the rel-
evance of acquired MWTs with respect to the in-
volved domains.
4 Conclusion
In this paper we tackled the challenging task of
MWT extraction from different kinds of domain
79
Figure 2: Wikipedia articles. Comparative pro-
gressive trend of environmental extracted terms.
Figure 3: Legal texts. Comparative progressive
trend of environmental extracted terms.
corpora, characterized by different types of termi-
nologies. We demonstrated that the multi-layered
approach proposed in Bonin et al (2010) can be
successfully exploited in distinguishing between
GL and SL items and in assessing the domain-
relevance of extracted terms. The latter is the case
of type of multi-domain corpora, characterized by
the occurrence of terms belonging to different SLs
(e.g. legal texts). Moreover, the results obtained
from different text types proved that the perfor-
mance of the contrastive filtering stage is dramat-
ically influenced by the nature of the acquisition
corpus.
5 Acknowledgments
The research has been supported in part by a grant
from the Italian FIRB project RBNE07C4R9.
Thanks are also due to Angela D?Angelo (Scuola
Figure 4: Legal texts. Trend of contrastive func-
tion.
Superiore Sant?Anna, Pisa) and Paolo Plini (EKO-
Lab, CNR, Rome), who contributed as domain ex-
perts to the evaluation.
References
Bonin, Francesca, Felice Dell?Orletta, Giulia Venturi,
and Simonetta Montemagni, 2010. A Contrastive
Approach to Multi-word Term Extraction from Do-
main Corpora, in Proceedings of the ?7th Interna-
tional Conference on Language Resources and Eval-
uation?, Malta, 19-21 May, 3222-3229.
Breuker, Joost, and Rinke Hoekstra, 2004. Epistemol-
ogy and Ontology in Core Ontologies: FOLaw and
LRI-Core, two core ontologies for law, in Proceed-
ings of the ?Workshop on Core Ontologies in Ontol-
ogy Engineering?, UK, 15-27.
Cabre?, M.Teresa, 1999. The terminology. Theory,
methods and applications. John Benjamins Publish-
ing Company.
Frantzi, Katerina, and Sofia Ananiadou, 1999. The
C-value / NC Value domain independent method for
multi-word term extraction. In Journal of Natural
Language Processing, 6(3):145-179.
Marinelli, Rita, et al, 2003. The Italian PAROLE cor-
pus: an overview. In A. Zampolli et al (eds.), Com-
putational Linguistics in Pisa, XVI-XVII, IEPI., I,
401-421.
Rondeau, Guy, and Juan Sager, 1984. Introduction
a` la terminologie (2nd ed.). Chicoutimi, Gatan
Morin.
80
Proceedings of the SIGDIAL 2013 Conference, pages 304?308,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Laughter and Topic Transition in Multiparty Conversation
Emer Gilmartin, Francesca Bonin, Carl Vogel, Nick Campbell
Trinity College Dublin
{gilmare, boninf, vogel, nick}@tcd.ie
Abstract
This study explores laughter distribution
around topic changes in multiparty conver-
sations. The distribution of shared and solo
laughter around topic changes was examined
in corpora containing two types of spoken in-
teraction; meetings and informal conversation.
Shared laughter was significantly more fre-
quent in the 15 seconds leading up to topic
change in the informal conversations. A sam-
ple of informal conversations was then anal-
ysed by hand to gain further insight into links
between laughter and topic change.
1 Introduction
Human spoken interaction comprises a bundle of
signals and cues, together and separately providing
information relevant to the topic or task at hand, and
serving to build or maintain social bonds. Dialogue
is multifunctional, serving social as well as informa-
tion transfer goals. Laughter is predominantly social
rather than a solo activity, is universally present in
humans, part of the ?universal human vocabulary?,
innate, instinctual, and inherited from primate an-
cestors (Provine, 2004; Glenn, 2003). In conversa-
tion, it predominantly punctuates rather than inter-
rupts speech. Accounts of laughter?s role range from
response to humour to a social cohesion or bonding
mechanism used since our primate days. It has been
suggested that laughter is often a co-operative mech-
anism which can provide clues to dialogue structure
(Holt, 2011). Herein, we investigate the relevance of
laughter to topic change by analysing two corpora of
conversational speech in terms of temporal distribu-
tion of laughter, first through statistical analysis of
laughter and topic change distribution, then by man-
ual study of an hour of spontaneous conversation.
2 Laughter and Topic Change
Conversation analysis has highlighted connections
between laughter and topic change; many conver-
sations in the Holt corpus of mostly two person tele-
phone dialogues include laughter at topic closings
(Holt, 2010). Laughter has been linked to topic
closure in situations where one participant produces
jokes or laughs, thus inviting others to join in, with
this invitation open to refusal if interlocutors con-
tinue speaking on the topic at hand (Jefferson, 1979).
Holt (2010) suggests that laughter may arise at topic
changes because turns consisting only of laughter
are backwards looking, not adding to the last topic,
and thus constituting a signal that the current topic
has been exhausted and that the conversation is at
a topic change relevant point. We hypothesise that
these laughter turns form a ?buffer? allowing partic-
ipants a reassuring moment of social bonding. In
a meeting, there is a set agenda, a chairperson, and
protocols for moving from topic to topic. In social
dialogue, the goal is to pass time together, and top-
ics are not lined up ready for use. Aversion to poten-
tially embarrassing silence may be more pertinent in
informal conversation; thus laughter preceding topic
change may be more likely in informal dialogue.
Although there is much mention of laughter in
conversation analysis, it is difficult to find quanti-
tative data on its distribution in spoken interaction.
Previous work (Bonin et al, 2012b) established that
laughter, particularly shared laughter, is less likely
to occur in the first quarter of a topic than in the fi-
nal quarter, and that this distinction is greater in so-
304
cial conversation. In this work we test the hypothe-
sis that laughter should be frequently found before
rather than simply around topic changes. We ex-
amine the frequency of laughter within a range of
distances from either side of a topic change, to in-
vestigate if there is a period of higher laughter fre-
quency independent of topic length. We are also
interested in exploring whether the turns leading
to topic change follow the observations on topic
change sequences and laughter distribution in two
party conversations in the literature. If there are
identifiable sequences involving laughter leading to
topic change, knowledge of their architecture will
aid in creating algorithms for discourse recognition
and segmentation in multiparty conversation.
The notion of topic in discourse has been stud-
ied extensively but a concise definition is diffi-
cult to find. Topic has been described at sen-
tence level (Lambrecht, 1996), at discourse level
(Van Dijk, 1981); as a manifestation of speakers in-
tentions (Passonneau and Litman, 1997), and as co-
herent segments of discourse about the same thing
(Van Dijk, 1996). Here, we consider topic at dis-
course level as a chunk of coherent content.
3 Corpora
We analysed two datasets to cover free natural inter-
action and more structured meetings.
3.1 Topic annotation in TableTalk and AMI
Both TableTalk and AMI have topic annotations
freely available. TableTalk topics were annotated
manually by two labellers at a single level; AMI
annotations include top-level or core topics whose
content reflects the main meeting structure, and
subtopics for small digressions inside the core top-
ics. Here we use the core topic segmentation which
is more in line with the TableTalk annotation.
3.2 TableTalk
The TableTalk corpus contains multimodal record-
ings of free flowing natural conversations among
five participants, recorded at the Advanced Telecom-
munication Research Labs in Japan (Campbell,
2009). In order to collect as natural data as possi-
ble, neither topics of discussion nor activities were
restricted in advance. Three sessions were recorded
over three consecutive days in an informal setting
over coffee, by three female (Australian, Finnish,
and Japanese) and two male (Belgian and British)
participants (Jokinen, 2009). The conversations are
fully transcribed and segmented for topic, and also
annotated for affective state of participants and for
gesture and postural communicative functions us-
ing MUMIN (Allwood et al, 2007). Table-talk has
been analyzed in terms of engagement and laugh-
ter (Bonin et al, 2012a) and lexical accommodation
(Vogel and Behan, 2012). Our analyses used tran-
scripts of the entire corpus: about 3h 30, 31523 to-
kens and 5980 turns. Laughter was transcribed in
intervals on the speech transcription tier as @w, (un-
less inserted as part of a longer utterance). The total
number of laughs is 713. Shared laughter was auto-
matically annotated as described in ?4.
3.3 AMI
The AMI (Augmented Multi-party Interaction)
Meeting Corpus is a multimodal data set of 100
hours of meeting recordings (McCowan et al,
2005). The corpus contains real and scenario-driven
meetings. We base our analysis on the scenario
based meetings, with a total of 717,239 tokens. Each
meeting has four participants, and the same subjects
meet over four different sessions to discuss a design
project. The sessions correspond to four different
project steps (Project kick-off meeting, Functional
Design, Conceptual Design and Detailed Design).
Each participant is given a role to play (project
manager, marketing expert, industrial designer and
user interface designer) and keeps this role until the
end of the scenario. Conversations are all in En-
glish, with 91 native speakers and 96 non-native
speakers participating. There are 11,277 instances
of laughter, annotated in the transcripts as vocal-
sounds/laugh. About 25% of these laughs are anno-
tated with start time only.
4 Analytical methodologies
4.1 Automated and manual analyses
Both corpora were also analysed automatically, and
a one-hour sample of the TableTalk corpus was anal-
ysed on a case-by-case basis to investigate if laugh-
ter around topic change did indeed follow the pat-
terns proposed in the literature.
For the initial stages of ongoing manual analysis
305
to gain more insight into the mechanisms underly-
ing laughter and topic change, a one-hour stretch of
conversation from the second day of the TableTalk
was selected for study. The mechanism outlined
by Holt, based on Jefferson?s work on laughter and
Schegloff?s topic final sequences (Schegloff, 2007),
hinges on whether a laughter invitation is taken up
an interlocutor in two party dialogue. If it is, then
one or more laughter turns ensue and the likelihood
of topic change is high. The opposite occurs when
the interlocutor does not take up the invitation but
rather continues with further talk on the topic, avert-
ing topic change. We were interested in observing if
this phenomenon occurred in multiparty conversa-
tion, and if subsequent topic change was dependent
on how many of the group took up the invitation to
laugh. As analysis of the two corpora showed higher
likelihood of laughter before topic change in more
informal conversation, we chose to examine a sam-
ple of TableTalk for preliminary study.
This sample contained 1834 utterances, 36 T-
event or topic change instants, and 329 laughs
among the five participants, of which 76 were solo
while the remainder contributed to a total of 68
shared laugh events, all of which were manually an-
notated on separate laughter tiers. For each instance
of laughter, we also annotated the number of partic-
ipants who laughed and the distance from the laugh-
ter to the next topic commencement.
4.2 Temporal definitions and measurement
We use an algorithm resulting from earlier work to
annotate shared and solo laughter. The algorithm
was motivated by the observation that in both cor-
pora laughter was sometimes annotated with start
time only, and also that laughter in response to the
same stimulus should be considered shared laugh-
ter. These two factors taken together allow us to
recover shared laughter that may be missed if we
simply count overlapping laughs of distinct speak-
ers. The algorithm defines shared laughter as: (a)
overlapping laughs of distinct speakers; or (b) con-
secutive laughs of distinct speakers within distance
?. We calculate ? using the probability distribution
that successive laughs with observation of start time
only are part of a shared laugh event, trained on a
subset of overlapping laughs from the corpora.
Topic changes (T-events) are the annotated time
points where topic shifts in conversation. We
counted the frequency of laughter, shared laughter,
and solo laughter into 5-second bins at T-event mi-
nus multiples of 5 seconds (T-5, T-10, T-15, T-20) in
order to look at the laughter trend near topic termi-
nation. A meaningful threshold emerges (T-15 sec-
onds) where a change in the laughter trend is vis-
ible. Hence we counted the frequency of laughter
between T-15 and T, and T and T+15.
5 Results
5.1 Automated processing
We counted the frequency of laughter, shared laugh-
ter, and solo laughter in 5-second bins at T- event
time T minus multiples of 5 seconds (T-5, T-10,
T-15, T-20). Fig. 1 shows the mean frequency of
laughs per bin in TableTalk. While in AMI the distri-
bution over the bins does not show significant trends,
in TableTalk, we noticed a significant change at T-
15.1 Hence we take T-15 as a rational threshold
marking some change in the laughter distribution be-
fore a topic boundary in informal chat.
Then we analyzed the frequency of laughter be-
tween T-15 and T (we call this segment wt) and
T+15 (wb). As shown in Fig. 2, we notice a signifi-
cant difference in the amount of both shared and solo
laughter between topic terminations (wt) and topic
beginnings (wb). In particular topic terminations
show a higher frequency of laughter than topic be-
ginnings. The result holds in AMI and in TableTalk.
5.2 Manual processing
The first observation from the manual analysis is
that the shared/solo laugh ratio is heavily skewed to-
wards shared laughter (253 laughs were shared vs 79
solo). Laughs were combined into laugh events ac-
cording to the number of participants involved. The
length of laugh events was significantly shorter for
one-person laugh events than for shared laughter, see
Fig. 3. Distance to next topic change and number of
1The laughter counts in the bins for each of T-5, T-10 and T-
15 are significantly greater than random samples of 5 sec. con-
versation slices (Wilcox directed test, p < 0.002); the counts
for T-20 are not significantly greater than random slices. Fur-
ther, the counts for T-20 are significantly less than those in each
of T-15 (p < 0.02), T-10 (p < 0.02) and T-5 (p < 0.005), while
the pairwise differences among T-15, T-10 and T-5 are not sign-
ficant. We conclude that T-15 contains an inflection point.
306
T?20 T?15 T?10 T?5
Bin of 5 seconds from T?20 to T
Mea
n # o
f Lau
gh
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Figure 1: Frequency of laughter in TableTalk between T-
20 and T in 5-second bins. Bars represent the mean laugh
count per bin
SH in wb SH in wt SO in wb SO in wt
Mea
n # o
f Lau
gh
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Figure 2: Shared (sh) and Solo (so) laughs in topic
termination (wt) and topic beginning segments (wb)-
TableTalk
laughers in a laugh event, seen in Fig. 4, showed sig-
nificant negative correlation (p < 0.05).
6 Discussion and Conclusion
Our results indicate a likelihood of shared laugher
appearing in the final 15 seconds before a new topic
commences. This is in line with the literature which
reports laughter at topic transition relevant places,
and thus before a topic change. We have also seen
that the number of people sharing laughter is re-
lated to reducing distance from the laughter to the
next topic change, and that laugh events are longer
1 2 3 4 5
Mea
n len
gth o
f lau
gh e
vent
 - se
c
0
2
4
6
8
Figure 3: Laughter event length by number of laughers.
1 2 3 4 5
Mea
n dis
tanc
e to 
next
 topi
c - s
ec
0
2
4
6
8
Figure 4: Distance to next topic by number of laughers.
as more participants join in. Models of a complex-
ity adequate to predict human behaviour require ex-
haustively detailed analysis of stretches of conver-
sation in addition to broad statistical analysis. Our
combination of approaches has proven fruitful. Sev-
eral observations from the preliminary close exami-
nation of the TableTalk data provide fruit for further
research. Many of the short solo laughs may be seen
as responses to one?s own or another participant?s
content, while stronger solo laughs may tend to in-
vite longer and stronger laughter from others, lead-
ing to topic change possibilities. An acoustic anal-
ysis of the laughter will investigate this. We also
observed that shared laughter among several partic-
ipants which did not result in topic change were fre-
quently interpretable as attempts to draw an ongo-
ing topic to a close. This merits investigation to
see whether these laugh events can be considered
topic transition relevant places. Analysis of speaker
changes and turn retrieval in and around these laugh-
ter events is underway to model these events.
307
Acknowledgments
This work is supported by the Innovation Bursary
of Trinity College Dublin, the Speech Communica-
tion Lab at TCD, and by the SFI FastNet project
09/IN.1/1263. We are grateful to the anonymous re-
viewers for helpful feedback.
References
Jens Allwood, Loredana Cerrato, Kristiina Jokinen,
Costanza Navarretta, and Patrizia Paggio. 2007. The
mumin coding scheme for the annotation of feedback,
turn management and sequencing phenomena. Lan-
guage Resources and Evaluation, 41(3-4):273?287.
Francesca Bonin, Ronald Bo?ck, and Nick Campbell.
2012a. How do we react to context? annotation of
individual and group engagement in a video corpus.
In SocialCom/PASSAT, pages 899?903.
Francesca Bonin, Nick Campbell, and Carl Vogel. 2012b.
Laughter and topic changes: Temporal distribution and
information flow. In Cognitive Infocommunications
(CogInfoCom), 2012 IEEE 3rd International Confer-
ence on, pages 53?58.
Nick Campbell. 2009. An audio-visual approach to mea-
suring discourse synchrony in multimodal conversa-
tion data. In Proceedings of Interspeech 2009.
P. Glenn. 2003. Laughter in Interaction. Studies in Inter-
actional Sociolinguistics. Cambridge University Press.
Elizabeth Holt. 2010. The last laugh: Shared laugh-
ter and topic termination. Journal of Pragmatics,
42:1513?1525.
Elizabeth Holt. 2011. On the nature of ?laughables? :
laughter as a response to overdone figurative phrases.
Pragmatics, 21(3):393?410, September.
Gail Jefferson. 1979. A technique for inviting laugh-
ter and its subsequent acceptance/declination. In
G Psathas, editor, Everyday language: Studies in eth-
nomethodology., pages 79?96. Irvington Publishers:
New York,NY.
Kristiina Jokinen. 2009. Gaze and gesture activity
in communication. In Constantine Stephanidis, ed-
itor, Universal Access in Human-Computer Interac-
tion. Intelligent and Ubiquitous Interaction Environ-
ments, volume 5615 of Lecture Notes in Computer Sci-
ence, pages 537?546. Springer Berlin / Heidelberg.
K. Lambrecht. 1996. Information Structure and Sen-
tence Form: Topic, Focus, and the Mental Represen-
tations of Discourse Referents. Cambridge Studies in
Linguistics. Cambridge University Press.
I. McCowan, G. Lathoud, M. Lincoln, A. Lisowska,
W. Post, D. Reidsma, and P. Wellner. 2005. The ami
meeting corpus. In In: Proceedings Measuring Be-
havior 2005, 5th International Conference on Meth-
ods and Techniques in Behavioral Research. L.P.J.J.
Noldus, F. Grieco, L.W.S. Loijens and P.H. Zimmer-
man (Eds.), Wageningen: Noldus Information Tech-
nology.
Rebecca J. Passonneau and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Computational Linguistics, 23(1):103?139.
Robert R. Provine. 2004. Laughing, tickling, and the
evolution of speech and self. Current Directions in
Psychological Science, 13(6):215?218.
E.A. Schegloff. 2007. Sequence Organization in Inter-
action: Volume 1: A Primer in Conversation Analysis.
Cambridge University Press.
Teun A. Van Dijk, 1981. Sentence Topic versus Dis-
course Topic, pages 177?194. Mouton.
Teun A. Van Dijk. 1996. Discourse, power and ac-
cess. In Carmen Rosa Caldas-Coulthard and Malcolm
Coulthard, editors, Texts and Practices, Readings in
Critical Discourse Analysis, pages 84?104. Routledge.
Carl Vogel and Lydia Behan. 2012. Measuring
synchrony in dialog transcripts. In Anna Espos-
ito, Antonietta M. Esposito, Alessandro Vinciarelli,
Ru?diger Hoffmann, and Vincent C. Mu?ller, edi-
tors, Behavioural Cognitive Systems, pages 73?88.
Springer, LNCS 7403.
308
