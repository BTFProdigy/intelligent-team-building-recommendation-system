Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 224?232,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
A Systematic Analysis of Translation Model Search Spaces
Michael Auli, Adam Lopez, Hieu Hoang and Philipp Koehn
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB
United Kingdom
m.auli@sms.ed.ac.uk, alopez@inf.ed.ac.uk, h.hoang@sms.ed.ac.uk, pkoehn@inf.ed.ac.uk
Abstract
Translation systems are complex, and
most metrics do little to pinpoint causes of
error or isolate system differences. We use
a simple technique to discover induction
errors, which occur when good transla-
tions are absent from model search spaces.
Our results show that a common prun-
ing heuristic drastically increases induc-
tion error, and also strongly suggest that
the search spaces of phrase-based and hi-
erarchical phrase-based models are highly
overlapping despite the well known struc-
tural differences.
1 Introduction
Most empirical work in translation analyzes mod-
els and algorithms using BLEU (Papineni et al,
2002) and related metrics. Though such met-
rics are useful as sanity checks in iterative sys-
tem development, they are less useful as analyti-
cal tools. The performance of a translation system
depends on the complex interaction of several dif-
ferent components. Since metrics assess only out-
put, they fail to inform us about the consequences
of these interactions, and thus provide no insight
into the errors made by a system, or into the de-
sign tradeoffs of competing systems.
In this work, we show that it is possible to ob-
tain such insights by analyzing translation sys-
tem components in isolation. We focus on model
search spaces (?2), posing a very simple question:
Given a model and a sentence pair, does the search
space contain the sentence pair? Applying this
method to the analysis and comparison of French-
English translation using both phrase-based and
hierarchical phrase-based systems yields surpris-
ing results, which we analyze quantitatively and
qualitatively.
? First, we analyze the induction error of a
model, a measure on the completeness of the
search space. We find that low weight phrase
translations typically discarded by heuristic
pruning nearly triples the number of refer-
ence sentences that can be exactly recon-
structed by either model (?3).
? Second, we find that the high-probability re-
gions in the search spaces of phrase-based
and hierarchical systems are nearly identical
(?4). This means that reported differences be-
tween the models are due to their rankings of
competing hypotheses, rather than structural
differences of the derivations they produce.
2 Models, Search Spaces, and Errors
A translation model consists of two distinct ele-
ments: an unweighted ruleset, and a parameteri-
zation (Lopez, 2008a; 2009). A ruleset licenses
the steps by which a source string f1...fI may be
rewritten as a target string e1...eJ . A parameter-
ization defines a weight function over every se-
quence of rule applications.
In a phrase-based model, the ruleset is simply
the unweighted phrase table, where each phrase
pair fi...fi?/ej ...ej? states that phrase fi...fi? in
the source can be rewritten as ej ...ej? in the tar-
get. The model operates by iteratively apply-
ing rewrites to the source sentence until each
source word has been consumed by exactly one
rule. There are two additional heuristic rules:
The distortion limit dl constrains distances over
which phrases can be reordered, and the transla-
tion option limit tol constrains the number of tar-
get phrases that may be considered for any given
source phrase. Together, these rules completely
determine the finite set of all possible target sen-
tences for a given source sentence. We call this set
of target sentences the model search space.
The parameterization of the model includes all
information needed to score any particular se-
224
quence of rule applications. In our phrase-based
model, it typically includes phrase translation
probabilities, lexical translation probabilities, lan-
guage model probabilities, word counts, and co-
efficients on the linear combination of these. The
combination of large rulesets and complex param-
eterizations typically makes search intractable, re-
quiring the use of approximate search. It is im-
portant to note that, regardless of the parameteri-
zation or search used, the set of all possible output
sentences is still a function of only the ruleset.
Germann et al (2004) identify two types of
translation system error: model error and search
error.1 Model error occurs when the optimal
path through the search space leads to an incorrect
translation. Search error occurs when the approxi-
mate search technique causes the decoder to select
a translation other than the optimum.
Given the decomposition outlined above, it
seems clear that model error depends on param-
eterization, while search error depends on approx-
imate search. However, there is no error type that
clearly depends on the ruleset (Table 1). We there-
fore identify a new type of error on the ruleset: in-
duction error. Induction error occurs when the
search space does not contain the correct target
sentence at all, and is thus a more fundamental
defect than model error. This is difficult to mea-
sure, since there could be many correct transla-
tions and there is no way to see whether they are
all absent from the search space.2 However, if we
assume that a given reference sentence is ground
truth, then as a proxy we can simply ask whether
or not the model search space contains the refer-
ence. This assumption is of course too strong, but
over a sufficiently large test set, it should correlate
with metrics which depend on the reference, since
under most metrics, exactly reproducing the ref-
erence results in a perfect score. More loosely, it
should correlate with translation accuracy?even
if there are many good translations, a model which
is systematically unable to produce any reference
sentences from a sufficiently large test sample is
almost certainly deficient in some way.
3 Does Ruleset Pruning Matter?
The heuristic translation option limit tol controls
the number of translation rules considered per
1They also identify variants within these types.
2It can also be gamed by using a model that can generate
any English word from any French word. However, this is
not a problem for the real models we investigate here.
ruleset induction error
parameterization model error
search search error
Table 1: Translation system components and their
associated error types.
100 101 102 1030
0.2
0.4
0.6
0.8
Translation Options
Phra
se P
roba
bility
 p(e|f)
Figure 1: Distribution p(f |e) of the English trans-
lation options for the French word proble`me.
source span. It plays a major role in keeping the
search space manageable. Ignoring reordering, the
complexity of the search in a phrase-based model
is O(ntol), where n is the number of French spans.
Therefore tol has a major effect on efficiency.
Tight pruning with tol is often assumed without
question to be a worthwhile tradeoff. However,
we wish to examine this assumption more closely.
Consider the French word proble`me. It has 288
different translation options in the phrase table
of our French-English phrase-based system. The
phrase translation probability p(e|f) over these
options is a familiar Zipf distribution (Figure 1).
The most likely candidate translation for the word
is problem with a probability of 0.71, followed by
issue with a much smaller probability of 0.12. Fur-
ther down, we find challenge at rank 25, obsta-
cle at 44 and dilemma at rank 105. Depending on
the context, these might be perfectly good transla-
tions. However, with a typical tol of 20, most of
these options are not considered during decoding.
Table 2 shows that 93.8% of rules are available
during decoding with the standard tol setting and
only about 0.1% of French spans of the entire rule-
set have more than 20 translation options. It seems
as if already most of the information is available
when using the default limit. However, a tol of
20 can clearly exclude good translations as illus-
trated by our example. Therefore we hypothesize
the following: Increasing the translation option
limit gives the decoder a larger vocabulary which
in turn will decrease the induction error. We sup-
225
tol Ruleset Size French Spans
20 93.8 99.9
50 96.8 100.0
100 98.3 100.0
200 99.2 100.0
400 99.7 100.0
800 99.9 100.0
All 100.0 100.0
Table 2: Ruleset size expressed as percentage of
available rules when varying the limit of transla-
tion options tol per English span and percentage
of French spans with up to tol translations.
port this hypothesis experimentally in ?5.4.
4 How Similar are Model Search Spaces?
Most work on hierarchical phrase-based transla-
tion focuses quite intently on its structural differ-
ences from phrase-based translation.
? A hierarchical model can translate discon-
tiguous groups of words as a unit. A phrase-
based model cannot. Lopez (2008b) gives in-
direct experimental evidence that this differ-
ence affects performance.
? A standard phrase-based model can reorder
phrases arbitrarily within the distortion limit,
while the hierarchical model requires some
lexical evidence for movement, resorting to
monotone translation otherwise.
? While both models can indirectly model
word deletion in the context of phrases, the
hierarchical model can delete words using
non-local context due to its use of discontigu-
ous phrases.
The underlying assumption in most discussions
of these models is that these differences in their
generative stories are responsible for differences
in performance. We believe that this assumption
should be investigated empirically.
In an interesting analysis of phrase-based and
hierarchical translation, Zollmann et al (2008)
forced a phrase-based system to produce the trans-
lations generated by a hierarchical system. Unfor-
tunately, their analysis is incomplete; they do not
perform the analysis in both directions. In ?5.5 we
extend their work by requiring each system to gen-
erate the 1-best output of the other. This allows us
to see how their search spaces differ.
5 Experiments
We analyse rulesets in isolation, removing the in-
fluence of the parametrization and heuristics as
much as possible for each system as follows: First,
we disabled beam search to avoid pruning based
on parametrization weights. Second, we require
our decoders to generate the reference via disal-
lowing reference-incompatible hypothesis or chart
entries. This leaves only some search restrictions
such as the distortion limit for the phrase-based
system for which we controlled, or the maximum
number of source words involved in a rule appli-
cation for the hierarchical system.
5.1 Experimental Systems
Our phrase-based system is Moses (Koehn et al,
2007). We set its stack size to 105, disabled the
beam threshold, and varied the translation option
limit tol. Forced translation was implemented by
Schwartz (2008) who ensures that hypothesis are
a prefix of the reference to be generated.
Our hierarchical system is Hiero (Chiang,
2007), modified to construct rules from a small
sample of occurrences of each source phrase in
training as described by Lopez (2008b). The
search parameters restricting the number of rules
or chart entries as well as the minimum threshold
were set to very high values (1050) to prevent prun-
ing. Forced translation was implemented by dis-
carding rules and chart entries which do not match
the reference.
5.2 Experimental Data
We conducted experiments in French-English
translation, attempting to make the experimental
conditions for both systems as equal as possible.
Each system was trained on French-English Eu-
roparl (Koehn, 2005), version 3 (40M words). The
corpus was aligned with GIZA++ (Och and Ney,
2003) and symmetrized with the grow-diag-final-
and heuristic (Koehn et al, 2003). A trigram
language model with modified Kneser-Ney dis-
counting and interpolation was used as produced
by the SRILM toolkit (Stolcke, 2002). Systems
were optimized on the WMT08 French-English
development data (2000 sentences) using mini-
mum error rate training (Och, 2003) and tested
on the WMT08 test data (2000 sentences). Rules
based on unaligned words at the edges of foreign
and source spans were not allowed unless other-
wise stated, this is denoted as the tightness con-
226
20 50 100 200 400 800 All10
15
20
25
30
35
Translation Option Limit
Rea
chab
ility 
(%)
 
 dl=6dl=7dl=8dl=9dl=10dl=11dl=12dl=13dl=14dl=15dl=16
Figure 2: Coverage for phrase-based reference
aligned translation on test data when varying the
translation option and the distortion limits (dl).
straint. Ayan and Dorr (2006) showed that under
certain conditions, this constraint could have sig-
nificant impact on system performance. The max-
imum phrase lengths for both the hierarchical and
phrase-based system were set to 7. The distortion
limit (dl) for the phrase-based system was set to
6 unless otherwise mentioned. All other settings
were left at their default values as described by
Chiang (2007) and Koehn et al (2007).
5.3 Metric: Reference Reachability
We measure system performance in terms of ref-
erence reachability, which is the inverse of in-
duction error: A system is required to be able to
exactly reproduce the reference, otherwise we re-
gard the result as an error.
5.4 Analysis of Ruleset Pruning
In ?3 we outlined the hypothesis that increas-
ing the number of English translation options per
French span can increase performance. Here we
present results for both phrase-based and hierar-
chical systems to support this claim.
5.4.1 Quantitative Results
Figure 2 shows the experimental results when
forcing our phrase-based system to generate un-
seen test data. We observe more than 30% in-
crease in reachability from tol = 20 to tol = 50
for all dl ? 6 which supports our hypothesis that
increasing tol by a small multiple can have a sig-
nificant impact on performance. With no limit on
tol, reachability nearly triples.
French Spans Number of Translations
des 3006
les 2464
la 1582
de 1557
en 1428
de la 1332
fait 1308
une 1303
a` 1291
le 1273
d? 1271
faire 1263
l? 1111
c? est 1109
a` la 1053
, 1035
Table 3: French spans with more than 1000 trans-
lation options.
Notably, the increase stems from the small frac-
tion of French spans (0.1%) which have more than
20 translation options (Table 2). There are only
16 French spans (Table 3) which have more than
1000 translation options, however, utilising these
can still achieve an increase in reachability of up
to 5%. The list shown in Table 3 includes common
articles, interpuncutation, conjunctions, preposi-
tions but also verbs which have unreliable align-
ment points and therefore a very long tail of low
probability translation options. Yet, the largest in-
crease does not stem from using such unreliable
translation options, but rather when increasing tol
by a relatively small amount.
The increases we see in reachability are pro-
portional to the size of the ruleset: The high-
est increases in ruleset size can be seen between
tol = 20 and tol = 200 (Table 2), similarly, reach-
ability performance has then the largest increase.
For higher tol settings both the increases of ruleset
size and reachability are smaller.
Figure 3 plots the average number of words per
sentence for the reachable sentences. The average
sentence length increases by up to six words when
using all translation options. The black line repre-
sents the average number of words per sentence of
the reference set. This shows that longer and more
complex sentences can be generated when using
more translation options.
Similarly, for our hierarchical system (see Fig-
227
20 50 100 200 400 800 All14
16
18
20
22
24
26
28
30
32
Translation Option Limit
Ave
rage
 Num
ber o
f Wo
rds p
er S
ente
nce
 
 dl=6dl=7dl=8dl=9dl=10dl=11dl=12dl=13dl=14dl=15dl=16Reference
Figure 3: Average number of words per sen-
tence for the reachable test data translations of the
phrase-based system (as shown in Figure 2).
25 50 100 200 400 800 1600 3200 6400 12800 Inf5
10
15
20
25
30
35
40
Sample Limit (SL)
Rea
chab
ility 
(%)
 
 
Figure 4: Coverage for hierarchical reference
aligned translation on test data when varying the
number of matching French samples (sl) drawn
from the training data. The baseline setting is
sl = 300.
ure 4) we find that reachability can be more than
doubled when drawing a richer ruleset sample than
in the baseline setting. Those results are not di-
rectly comparable to the phrase-based system due
to the slightly different nature of the parameters
which were varied: In the phrase-based case we
have tol different English spans per French span.
In the hierarchical system it is very likely to have
duplicate French spans in the sample drawn from
training data. Yet, the trend is the same and thus
supports our claim.
5.4.2 Qualitative Results
We were interested how the performance increase
could be achieved and therefore looked into which
kind of translation options were involved when a
translation was generable with a higher tol setting.
One possibility is that the long tail of translation
options includes all kinds of English spans that
match some part of the reference but are simply
an artifact of unreliable alignment points.
We looked at the first twenty translations pro-
duced by our phrase-based system under dl = 10
which could not be generated with tol = 20 but
with tol = 50. The aim was to find out which
translation options made it possible to reach the
reference under tol = 50.
We found that nearly half (9) involved transla-
tion options which used a common or less com-
mon translation of the foreign span. The first four
translations in Table 4 are examples for that. When
allowing unaligned words at the rule edges it turns
out that even 13 out of 20 translations are based on
sound translation options.
The remaining sentences involved translation
options which were an artifact of unreliable align-
ment points. An example rule is la / their, which
erroneously translates a common determiner into
an equally common adjective. The last translation
in Figure 4 involves such a translation option.
This analysis demonstrates that the performance
increase between tol = 20 to tol = 50 is to a
considerable extent based on translation options
which are meaningful.
5.5 Analysis of Mutual Reachability
The aim of this analysis was to find out by how
much the high-probability search spaces of the
phrase-based and hierarchical models differ. The
necessary data was obtained via forcing each sys-
tem to produce the 1-best translation of the other
system denoted as the unconstrained translation.
This unconstrained translation used the standard
setting for the number of translation options.
We controlled for the way unaligned words
were handled during rule extraction: The phrase-
based system allowed unaligned words at the
edges of phrases while the hierarchical system did
not. We varied this condition for the phrase-based
system. The distortion limit of the phrase-based
system was set to 10. This is equal to the maxi-
mum span a rule can be applied within the hierar-
chical system.
We carried out the same experiment for
German-English and English-German translation
which serve as examples for translating into a mor-
228
S: je voterai en faveur du projet de re`glement .
R: i will vote to approve the draft regulation .
O: i shall be voting in favour of the draft regulation .
S: ... il npeut y avoir de de?lai transitoire en matie`re de respect des re`gles de?mocratiques .
R: ... there can be no transitional period for complying with democratic rules .
O: ... there can be no transitional period in the field of democratic rules .
S: je souhaite aux ne?gociateurs la poursuite du succe`s de leur travail dans ce domaine important .
R: i wish the negotiators continued success with their work in this important area .
O: i wish the negotiators the continuation of the success of their work on this important area .
S: mais commencons par les points positifs .
R: but let us begin with the good news .
O: but let us begin with the positive points .
S: ... partage la plupart des conclusions que tire le rapporteur .
R: ... share the majority of conclusions that he draws .
O: ... share most of the conclusions that is the rapporteur .
Table 4: Example translations which could be generated with tol = 50 but not with tol = 20. For each
translation the source (S), reference (R) and the unconstrained output (O) are shown. Bold phrases mark
translation options which were not available under tol = 20.
phologically simpler and more complex language
respectively. The test and training sets for these
languages are similarly sized and are from the
WMT08 shared task.
5.5.1 Quantitative Results
Table 5 shows the mutual reachability perfor-
mance for our phrase-based and hierarchical sys-
tem. The hierarchical system can generate almost
all of the 1-best phrase-based translations, partic-
ularly when unaligned words at rule edges are dis-
allowed which is the most equal condition we ex-
perimented with. The phrase-based reachability
for English-German using tight rulesets is remark-
ably low. We found that this is because the hi-
erarchical model allows unaligned words around
gaps under the tight constraint. This makes it very
hard for the phrase-based system to reach the hi-
erarchical translation. However, the phrase-based
system can overcome this problem when the tight-
ness constraint is loosened (last row in Table 5).
Table 6 shows the translation performance mea-
sured in BLEU for both systems for normal un-
constrained translation. It can be seen that the dif-
ference is rather marginal which is in line with our
reachability results.
We were interested why certain translations of
one system were not reachable by the other sys-
tem. The following two subsections describe
our analysis of these translations for the French-
English language pair.
Translation Direction fr-en de-en en-de
Ht ? Pt 99.40 97.65 98.50
Ht ? Pnt 95.95 93.95 94.30
Pt ? Ht 93.75 92.30 82.95
Pnt ? Ht 97.55 97.55 96.30
Table 5: Mutual reachability performance for
French-English (fr-en), German-English (de-en)
and Enlgish-German (en-de). P? H denotes how
many hierarchical (H) high scoring outputs can be
reached by the phrase-based (P) system. The sub-
scripts nt (non-tight) and t (tight) denote the use
of rules with unaligned words or not.
5.5.2 Qualitative Analysis of Unreachable
Hierarchical Translations
We analysed the first twenty translations within
the set of unreachable hierarchical translations
when disallowing unaligned words at rule edges to
find out why the phrase-based system fails to reach
them. Two aspects were considered in this anal-
ysis: First, the successful hierarchical derivation
and second, the relevant part of the phrase-based
ruleset which was involved in the failed forced
translation i.e. how much of the input and the ref-
erence could be covered by the raw phrase-pairs
available to the phrase-based system.
Within the examined subset, the majority of
sentences (14) involved hierarchical rules which
could not be replicated by the phrase-based sys-
229
System fr-en de-en en-de
Phrase-based 31.96 26.94 19.96
Hierarchical 31.62 27.18 20.20
Difference absolute 0.34 0.24 0.24
Difference (%) 1.06 0.90 1.20
Table 6: Performance for phrase-based and hier-
archical systems in BLEU for French-English (fr-
en), German-English (de-en) and English-German
(en-de).
tem. We described this as the first structural dif-
ference in ?4. Almost all of these translations
(12 out of 14) could not be generated because
of the third structural difference which involved
rule that omits the translation of a word within
the French span. An example is the rule X ?
estX 1 ordinaireX 2 /isX 1 X 2 which omits a trans-
lation for the French word ordinaire in the English
span. For this particular subset the capability of
the hierarchical system to capture long-distance
reorderings did not make the difference, but rather
the ability to drop words within a translation rule.
The phrase-based system cannot learn many
rules which omit the translation of words because
we disallowed unaligned words at phrase edges.
The hierarchical system has the same restriction,
but the constraint does not prohibit rules which
have unaligned words within the rule. This allows
the hierarchical system to learn rules such as the
one presented above. The phrase-based system
can learn similar knowledge, although less gen-
eral, if it is allowed to have unaligned words at
the phrase edges. In fact, without this constraint
13 out of the 20 analysed rules can be generated
by the phrase-based system.
Figure 5 shows a seemingly simple hierarchi-
cal translation which fails to be constructed by the
phrase-based system: The second rule application
involves both the reordering of the translation of
postaux and the omittance of a translation for con-
currence. This translation could be easily captured
by a phrase-pair, however, it requires that the train-
ing data contains exactly such an example which
was not the case. The closest rule the phrase-based
rulestore contains is des services postaux / postal
services which fails since it does not cover all of
the input. This is an example for when the gen-
eralisation of the hierarchical model is superior to
the phrase-based approach.
5.5.3 Qualitative Analysis of Unreachable
Phrase-based Translations
The size of the set of unreachable phrase-based
translations is only 0.6% or 12 sentences. This
means that almost all of the 1-best outputs of the
phrase-based translations can be reached by the hi-
erarchical system. Similarly to above, we analysed
which words of the input as well as which words
of the phrase-based translation can be covered by
the available hierarchical translation rules.
We found that all of the translations were not
generable because of the second structural differ-
ence we identified in ?4. The hierarchical rule-
set did not contain a rule with the necessary lex-
ical evidence to perform the same reordering as
the phrase-based model. Figure 6 shows a phrase-
based translation which could not be reached by
the hierarchical system because a rule of the form
X ? e?lectoralesX 1 /X 1 electoral would be re-
quired to move the translation of e?lectorales (elec-
toral) just before the translation of re?unions (meet-
ings). Inspection of the hierarchical ruleset reveals
that such a rule is not available and so the transla-
tion cannot be generated.
The small size of the set of unreachable phrase-
based translations shows that the lexically in-
formed reordering mechanism of the hierarchical
model is not a large obstacle in generating most of
the phrase-based outputs.
In summary, each system can reproduce nearly
all of the highest-scoring outputs of the other sys-
tem. This shows that the 1-best regions of both
systems are nearly identical despite the differ-
ences discussed in ?4. This means that differences
in observed system performance are probably at-
tributable to the degree of model error and search
error in each system.
6 Related Work and Open Questions
Zhang et al (2008) and Wellington et al (2006)
answer the question: what is the minimal gram-
mar that can be induced to completely describe a
training set? We look at the related question of
what a heuristically induced ruleset can translate
in an unseen test set, considering both phrase- and
grammar-based models. We also extend the work
of Zollmann et al (2008) on Chinese-English, per-
forming the analysis in both directions and provid-
ing a detailed qualitative explanation.
Our focus has been on the induction error of
models, a previously unstudied cause of transla-
230
Source: concurrence des services postaux
Reference: competition between postal services
Hierarchical: postal services
Deviation:
( [0-4: @S -> @X?1 | @X?1 ]
( [0-4: @X -> concurrence @X?1 postaux | postal @X?1 ] postal
( [1-3: @X -> des services | services ] services
)
)
)
Figure 5: Derivation of a hierarchical translation which cannot be generated by the phrase-based system,
in the format of Zollmann et al (2008). The parse tree contains the outputs (shaded) at its leaves in infix
order and each non-leaf node denotes a rule, in the form: [ Source-span: LHS?RHS ].
Source: ceux qui me disaient cela faisaient par exemple re`fe`rence a` certaines des
re?unions e?lectorales auxquelles ils avaient assiste? .
Phrase-based: those who said to me that were for example refer to some of which
they had been electoral meetings .
Reference: they referred to some of the election meetings , for example , that
they had gone to .
Figure 6: Phrase-based translation which cannot be reached by the hierarchical system because no rule to
perform the necessary reordering is available. Marked sections are source and reference spans involved
in the largest possible partial hierarchical derivation.
tion errors. Although the results described here
are striking, our exact match criterion for reach-
ability is surely too strict?for example, we re-
port an error if even a single comma is missing.
One solution is to use a more tolerant criterion
such as WER and measure the amount of devia-
tion from the reference. We could also maximize
BLEU with respect to the reference as in Dreyer et
al. (2007), but it is less interpretable.
7 Conclusion and Future Work
Sparse distributions are common in natural lan-
guage processing, and machine translation is no
exception. We showed that utilizing more of the
entire distribution can dramatically improve the
coverage of translation models, and possibly their
accuracy. Accounting for sparsity explicitly has
achieved significant improvements in other areas
such as in part of speech tagging (Goldwater and
Griffiths, 2007). Considering the entire tail is chal-
lenging, since the search space grows exponen-
tially with the number of translation options. A
first step might be to use features that facilitate
more variety in the top 20 translation options. A
more elaborate aim is to look into alternatives to
maximum likelihood hood estimation such as in
Blunsom and Osborne (2008).
Additionally, our expressiveness analysis shows
clearly that the 1-best region of hierarchical and
phrase-based models is nearly identical. Dis-
counting cases in which systems handle unaligned
words differently, we observe an overlap of be-
tween 96% and 99% across three language pairs.
This implies that the main difference between the
models is in their parameterization, rather than in
the structural differences in the types of transla-
tions they can produce. Our results also suggest
that the search spaces of both models are highly
overlapping: The results for the 1-best region al-
low the conjecture that also other parts of the
search space are behaving similarly since it ap-
pears rather unlikely that spaces are nearly disjoint
with only the 1-best region being nearly identical.
In future work we aim to use n-best lists or lattices
to more precisely measure search space overlap.
We also aim to analyse the effects of the model
and search errors for these systems.
Acknowledgements
This research was supported by the Euromatrix
Project funded by the European Commission (6th
Framework Programme). The experiments were
conducted using the resources provided by the
Edinburgh Compute and Data Facility (ECDF).
Many thanks to the three anonymous reviewers for
very helpful comments on earlier drafts.
231
References
N. F. Ayan and B. Dorr. 2006. Going beyond AER:
An extensive analysis of word alignments and their
impact on MT. In Proc. of ACL-COLING, pages 9?
16, Jul.
P. Blunsom and M. Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
M. Dreyer, K. B. Hall, and S. P. Khudanpur. 2007.
Comparing reordering constraints for SMT using ef-
ficient BLEU oracle computation. In Proc. of Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 103?110, Apr.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2004. Fast and optimal decoding for machine
translation. Artificial Intelligence, 154(1?2):127?
143, Apr.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL, pages 744?751, Prague, Czech Re-
public, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL,
pages 48?54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
Demonstration Session, pages 177?180, Jun.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit.
A. Lopez. 2008a. Statistical machine translation.
ACM Computing Surveys, 40(3).
A. Lopez. 2008b. Tera-scale translation models via
pattern matching. In Proc. of COLING, pages 505?
512, Aug.
A. Lopez. 2009. Translation as weighted deduction.
In Proc. of EACL.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167, Morristown, NJ, USA.
K. Papineni, S. Roukos, T. Ward, and W. jing Zhu.
2002. BLEU: A method for automatic evaluation
of machine translation. In Proc. of ACL, pages 311?
318.
L. Schwartz. 2008. Multi-source translation methods.
In Proc. of AMTA, October.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. Int. Conf. Spoken Lan-
guage Processing (ICSLP 2002).
B. Wellington, S. Waxmonsky, and I. D. Melamed.
2006. Empirical lower bounds on the complexity
of translational equivalence. In Proc. of ACL, pages
977?984, Morristown, NJ, USA.
H. Zhang, D. Gildea, and D. Chiang. 2008. Extracting
synchronous grammar rules from word-level align-
ments in linear time. In Proc. of COLING, pages
1081?1088, Manchester, UK.
A. Zollmann, A. Venugopal, F. Och, and J. Ponte.
2008. A systematic comparison of phrase-based, hi-
erarchical and syntax-augmented statistical MT. In
Proc. of COLING.
232
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 333?343,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training a Log-Linear Parser with Loss Functions via Softmax-Margin
Michael Auli
School of Informatics
University of Edinburgh
m.auli@sms.ed.ac.uk
Adam Lopez
HLTCOE
Johns Hopkins University
alopez@cs.jhu.edu
Abstract
Log-linear parsing models are often trained
by optimizing likelihood, but we would prefer
to optimise for a task-specific metric like F-
measure. Softmax-margin is a convex objec-
tive for such models that minimises a bound
on expected risk for a given loss function, but
its na??ve application requires the loss to de-
compose over the predicted structure, which
is not true of F-measure. We use softmax-
margin to optimise a log-linear CCG parser for
a variety of loss functions, and demonstrate
a novel dynamic programming algorithm that
enables us to use it with F-measure, lead-
ing to substantial gains in accuracy on CCG-
Bank. When we embed our loss-trained parser
into a larger model that includes supertagging
features incorporated via belief propagation,
we obtain further improvements and achieve
a labelled/unlabelled dependency F-measure
of 89.3%/94.0% on gold part-of-speech tags,
and 87.2%/92.8% on automatic part-of-speech
tags, the best reported results for this task.
1 Introduction
Parsing models based on Conditional Random
Fields (CRFs; Lafferty et al, 2001) have been very
successful (Clark and Curran, 2007; Finkel et al,
2008). In practice, they are usually trained by max-
imising the conditional log-likelihood (CLL) of the
training data. However, it is widely appreciated that
optimizing for task-specific metrics often leads to
better performance on those tasks (Goodman, 1996;
Och, 2003).
An especially attractive means of accomplishing
this for CRFs is the softmax-margin (SMM) ob-
jective (Sha and Saul, 2006; Povey and Woodland,
2008; Gimpel and Smith, 2010a) (?2). In addition to
retaining a probabilistic interpretation and optimiz-
ing towards a loss function, it is also convex, mak-
ing it straightforward to optimise. Gimpel and Smith
(2010a) show that it can be easily implemented with
a simple change to standard likelihood-based train-
ing, provided that the loss function decomposes over
the predicted structure.
Unfortunately, the widely-used F-measure met-
ric does not decompose over parses. To solve this,
we introduce a novel dynamic programming algo-
rithm that enables us to compute the exact quanti-
ties needed under the softmax-margin objective us-
ing F-measure as a loss (?3). We experiment with
this and several other metrics, including precision,
recall, and decomposable approximations thereof.
Our ability to optimise towards exact metrics en-
ables us to verify the effectiveness of more effi-
cient approximations. We test the training proce-
dures on the state-of-the-art Combinatory Categorial
Grammar (CCG; Steedman 2000) parser of Clark
and Curran (2007), obtaining substantial improve-
ments under a variety of conditions. We then embed
this model into a more accurate model that incor-
porates additional supertagging features via loopy
belief propagation. The improvements are additive,
obtaining the best reported results on this task (?4).
2 Softmax-Margin Training
The softmax-margin objective modifies the standard
likelihood objective for CRF training by reweighting
333
each possible outcome of a training input according
to its risk, which is simply the loss incurred on a par-
ticular example. This is done by incorporating the
loss function directly into the linear scoring function
of an individual example.
Formally, we are given m training pairs
(x(1), y(1))...(x(m), y(m)), where each x(i) ? X is
drawn from the set of possible inputs, and each
y(i) ? Y(x(i)) is drawn from a set of possible
instance-specific outputs. We want to learn the K
parameters ? of a log-linear model, where each ?k ?
? is the weight of an associated feature hk(x, y).
Function f(x, y) maps input/output pairs to the vec-
tor h1(x, y)...hK(x, y), and our log-linear model as-
signs probabilities in the usual way.
p(y|x) = exp{?
Tf(x, y)}?
y??Y(x) exp{?Tf(x, y?)}
(1)
The conditional log-likelihood objective function is
given by Eq. 2 (Figure 1). Now consider a function
`(y, y?) that returns the loss incurred by choosing to
output y? when the correct output is y. The softmax-
margin objective simply modifies the unnormalised,
unexponentiated score ?Tf(x, y?) by adding `(y, y?)
to it. This yields the objective function (Eq. 3) and
gradient computation (Eq. 4) shown in Figure 1.
This straightforward extension has several desir-
able properties. In addition to having a probabilis-
tic interpretation, it is related to maximum margin
and minimum-risk frameworks, it can be shown to
minimise a bound on expected risk, and it is convex
(Gimpel and Smith, 2010b).
We can also see from Eq. 4 that the only differ-
ence from standard CLL training is that we must
compute feature expectations with respect to the
cost-augmented scoring function. As Gimpel and
Smith (2010a) discuss, if the loss function decom-
poses over the predicted structure, we can treat its
decomposed elements as unweighted features that
fire on the corresponding structures, and compute
expectations in the normal way. In the case of
our parser, where we compute expectations using
the inside-outside algorithm, a loss function decom-
poses if it decomposes over spans or productions of
a CKY chart.
3 Loss Functions for Parsing
Ideally, we would like to optimise our parser towards
a task-based evaluation. Our CCG parser is evalu-
ated on labeled, directed dependency recovery us-
ing F-measure (Clark and Hockenmaier, 2002). Un-
der this evaluation we will represent output y? and
ground truth y as variable-sized sets of dependen-
cies. We can then compute precision P (y, y?), recall
R(y, y?), and F-measure F1(y, y?).
P (y, y?) = |y ? y
?|
|y?| (5)
R(y, y?) = |y ? y
?|
|y| (6)
F1(y, y?) =
2PR
P +R =
2|y ? y?|
|y|+ |y?| (7)
These metrics are positively correlated with perfor-
mance ? they are gain functions. To incorporate
them in the softmax-margin framework we reformu-
late them as loss functions by subtracting from one.
3.1 Computing F-Measure-Augmented
Expectations at the Sentence Level
Unfortunately, none of these metrics decompose
over parses. However, the individual statistics that
are used to compute them do decompose, a fact we
will exploit to devise an algorithm that computes the
necessary expectations. Note that since y is fixed,
F1 is a function of two integers: |y ? y?|, represent-
ing the number of correct dependencies in y?; and
|y?|, representing the total number of dependencies
in y?, which we will denote as n and d, respectively.1
Each pair ?n, d? leads to a different value of F1. Im-
portantly, both n and d decompose over parses.
The key idea will be to treat F1 as a non-local fea-
ture of the parse, dependent on values n and d.2 To
compute expectations we split each span in an oth-
erwise usual inside-outside computation by all pairs
?n, d? incident at that span.
Formally, our goal will be to compute expecta-
tions over the sentence a1...aL. In order to abstract
away from the particulars of CCG we present the al-
gorithm in relatively familiar terms as a variant of
1For numerator and denominator.
2This is essentially the same trick used in the oracle F-measure
algorithm of Huang (2008), and indeed our algorithm is a sum-
product variant of that max-product algorithm.
334
min
?
m?
i=1
?
???Tf(x(i), y(i)) + log
?
y?Y(x(i))
exp{?Tf(x(i), y)}
?
? (2)
min
?
m?
i=1
?
???Tf(x(i), y(i)) + log
?
y?Y(x(i))
exp{?Tf(x(i), y) + `(y(i), y)}
?
? (3)
?
??k
=
m?
i=1
?
??hk(x(i), y(i)) +
?
y?Y(x(i))
exp{?Tf(x(i), y) + `(y(i), y)}?
y??Y(x(i)) exp{?Tf(x(i), y?) + `(y(i), y?)}
hk(x(i), y)
?
? (4)
Figure 1: Conditional log-likelihood (Eq. 2), Softmax-margin objective (Eq. 3) and gradient (Eq. 4).
the classic inside-outside algorithm (Baker, 1979).
We use the notation a : A for lexical entries and
BC ? A to indicate that categories B and C com-
bine to form category A via forward or backward
composition or application.3 The weight of a rule
is denoted with w. The classic algorithm associates
inside score I(Ai,j) and outside score O(Ai,j) with
category A spanning sentence positions i through j,
computed via the following recursions.
I(Ai,i+1) =w(ai+1 : A)
I(Ai,j) =
?
k,B,C
I(Bi,k)I(Ck,j)w(BC ? A)
I(GOAL) =I(S0,L)
O(GOAL) =1
O(Ai,j) =
?
k,B,C
O(Ci,k)I(Bj,k)w(AB ? C)+
?
k,B,C
O(Ck,j)I(Bk,i)w(BA? C)
The expectation of A spanning positions i through j
is then I(Ai,j)O(Ai,j)/I(GOAL).
Our algorithm extends these computations to
state-split itemsAi,j,n,d.4 Using functions n+(?) and
d+(?) to respectively represent the number of cor-
rect and total dependencies introduced by a parsing
action, we present our algorithm in Fig. 3. The fi-
nal inside equation and initial outside equation in-
corporate the loss function for all derivations hav-
ing a particular F-score, enabling us to obtain the
3These correspond respectively to unary rules A ? a and bi-
nary rules A ? BC in a Chomsky normal form grammar.
4Here we use state-splitting to refer to splitting an item Ai,j into
many items Ai,j,n,d, one for each ?n, d? pair.
desired expectations. A simple modification of the
goal equations enables us to optimise precision, re-
call or a weighted F-measure.
To analyze the complexity of this algorithm, we
must ask: how many pairs ?n, d? can be incident at
each span? A CCG parser does not necessarily re-
turn one dependency per word (see Figure 2 for an
example), so d is not necessarily equal to the sen-
tence length L as it might be in many dependency
parsers, though it is still bounded by O(L). How-
ever, this behavior is sufficiently uncommon that we
expect all parses of a sentence, good or bad, to have
close to L dependencies, and hence we expect the
range of d to be constant on average. Furthermore,
n will be bounded from below by zero and from
above by min(|y|, |y?|). Hence the set of all possi-
ble F-measures for all possible parses is bounded by
O(L2), but on average it should be closer to O(L).
Following McAllester (1999), we can see from in-
spection of the free variables in Fig. 3 that the algo-
rithm requires worst-case O(L7) and average-case
O(L5) time complexity, and worse-case O(L4) and
average-case O(L3) space complexity.
Note finally that while this algorithm computes
exact sentence-level expectations, it is approximate
at the corpus level, since F-measure does not decom-
pose over sentences. We give the extension to exact
corpus-level expectations in Appendix A.
3.2 Approximate Loss Functions
We will also consider approximate but more effi-
cient alternatives to our exact algorithms. The idea
is to use cost functions which only utilise statistics
335
I(Ai,i+1,n,d) = w(ai+1 : A) iffn = n+(ai+1 : A), d = d+(ai+1 : A)
I(Ai,j,n,d) =
?
k,B,C
?
{n?,n??:n?+n??+n+(BC?A)=n},
{d?,d??:d?+d??+d+(BC?A)=d}
I(Bi,k,n?,d?)I(Ck,j,n??,d??)w(BC ? A)
I(GOAL) =
?
n,d
I(S0,L,n,d)
(
1? 2nd+ |y|
)
O(S0,N,n,d) =
(
1? 2nd+ |y|
)
O(Ai,j,n,d) =
?
k,B,C
?
{n?,n??:n??n???n+(AB?C)=n},
{d?,d??:d??d???d+(AB?C)=d}
O(Ci,k,n?,d?)I(Bj,k,n??,d??)w(AB ? C)+
?
k,B,C
?
{n?,n??:n??n???n+(BA?C)=n},
{d?,d??:d??d???d+(BA?C)=d}
O(Ck,j,n?,d?)I(Bk,i,n??,d??)w(BA? C)
Figure 3: State-split inside and outside recursions for computing softmax-margin with F-measure.
Figure 2: Example of flexible dependency realisation in
CCG: Our parser (Clark and Curran, 2007) creates de-
pendencies arising from coordination once all conjuncts
are found and treats ?and? as the syntactic head of coor-
dinations. The coordination rule (?) does not yet estab-
lish the dependency ?and - pears? (dotted line); it is the
backward application (<) in the larger span, ?apples and
pears?, that establishes it, together with ?and - pears?.
CCG also deals with unbounded dependencies which po-
tentially lead to more dependencies than words (Steed-
man, 2000); in this example a unification mechanism cre-
ates the dependencies ?likes - apples? and ?likes - pears?
in the forward application (>). For further examples and
a more detailed explanation of the mechanism as used in
the C&C parser refer to Clark et al (2002).
available within the current local structure, similar to
those used by Taskar et al (2004) for tracking con-
stituent errors in a context-free parser. We design
three simple losses to approximate precision, recall
and F-measure on CCG dependency structures.
Let T (y) be the set of parsing actions required
to build parse y. Our decomposable approximation
to precision simply counts the number of incorrect
dependencies using the local dependency counts,
n+(?) and d+(?).
DecP (y) =
?
t?T (y)
d+(t)? n+(t) (8)
To compute our approximation to recall we require
the number of gold dependencies, c+(?), which
should have been introduced by a particular parsing
action. A gold dependency is due to be recovered
by a parsing action if its head lies within one child
span and its dependent within the other. This yields a
decomposed approximation to recall that counts the
number of missed dependencies.
DecR(y) =
?
t?T (y)
c+(t)? n+(t) (9)
336
Unfortunately, the flexible handling of dependencies
in CCG complicates our formulation of c+, render-
ing it slightly more approximate. The unification
mechanism of CCG sometimes causes dependencies
to be realised later in the derivation, at a point when
both the head and the dependent are in the same
span, violating the assumption used to compute c+
(see again Figure 2). Exceptions like this can cause
mismatches between n+ and c+. We set c+ = n+
whenever c+ < n+ to account for these occasional
discrepancies.
Finally, we obtain a decomposable approximation
to F-measure.
DecF1(y) = DecP (y) +DecR(y) (10)
4 Experiments
Parsing Strategy. CCG parsers use a pipeline strat-
egy: we first multitag each word of the sentence with
a small subset of its possible lexical categories us-
ing a supertagger, a sequence model over these cat-
egories (Bangalore and Joshi, 1999; Clark, 2002).
Then we parse the sentence under the requirement
that the lexical categories are fixed to those preferred
by the supertagger. In our experiments we used two
variants on this strategy.
First is the adaptive supertagging (AST) approach
of Clark and Curran (2004). It is based on a step
function over supertagger beam widths, relaxing the
pruning threshold for lexical categories only if the
parser fails to find an analysis. The process either
succeeds and returns a parse after some iteration or
gives up after a predefined number of iterations. As
Clark and Curran (2004) show, most sentences can
be parsed with very tight beams.
Reverse adaptive supertagging is a much less ag-
gressive method that seeks only to make sentences
parsable when they otherwise would not be due to an
impractically large search space. Reverse AST starts
with a wide beam, narrowing it at each iteration only
if a maximum chart size is exceeded. Table 1 shows
beam settings for both strategies.
Adaptive supertagging aims for speed via pruning
while the reverse strategy aims for accuracy by ex-
posing the parser to a larger search space. Although
Clark and Curran (2007) found no actual improve-
ments from the latter strategy, we will show that
with our softmax-margin-trained models it can have
a substantial effect.
Parser. We use the C&C parser (Clark and Cur-
ran, 2007) and its supertagger (Clark, 2002). Our
baseline is the hybrid model of Clark and Curran
(2007), which contains features over both normal-
form derivations and CCG dependencies. The parser
relies solely on the supertagger for pruning, using
exact CKY for search over the pruned space. Train-
ing requires calculation of feature expectations over
packed charts of derivations. For training, we lim-
ited the number of items in this chart to 0.3 million,
and for testing, 1 million. We also used a more per-
missive training supertagger beam (Table 2) than in
previous work (Clark and Curran, 2007). Models
were trained with the parser?s L-BFGS trainer.
Evaluation. We evaluated on CCGbank (Hocken-
maier and Steedman, 2007), a right-most normal-
form CCG version of the Penn Treebank. We use
sections 02-21 (39603 sentences) for training, sec-
tion 00 (1913 sentences) for development and sec-
tion 23 (2407 sentences) for testing. We supply
gold-standard part-of-speech tags to the parsers. We
evaluate on labelled and unlabelled predicate argu-
ment structure recovery and supertag accuracy.
4.1 Training with Maximum F-measure Parses
So far we discussed how to optimise towards task-
specific metrics via changing the training objective.
In our first experiment we change the data on which
we optimise CLL. This is a kind of simple base-
line to our later experiments, attempting to achieve
the same effect by simpler means. Specifically, we
use the algorithm of Huang (2008) to generate or-
acle F-measure parses for each sentence. Updating
towards these oracle parses corrects the reachabil-
ity problem in standard CLL training. Since the su-
pertagger is used to prune the training forests, the
correct parse is sometimes pruned away ? reducing
data utilisation to 91%. Clark and Curran (2007)
correct for this by adding the gold tags to the parser
input. While this increases data utilisation, it bi-
ases the model by training in an idealised setting not
available at test time. Using oracle parses corrects
this bias while permitting 99% data utilisation. The
labelled F-score of the oracle parses lies at 98.1%.
Though we expected that this might result in some
improvement, results (Table 3) show that this has no
337
Condition Parameter Iteration 1 2 3 4 5
AST
? (beam width) 0.075 0.03 0.01 0.005 0.001
k (dictionary cutoff) 20 20 20 20 150
Reverse
? 0.001 0.005 0.01 0.03 0.075
k 150 20 20 20 20
Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.
Parameter ? is a beam threshold while k bounds the number of lexical categories considered for each word.
Condition Parameter Iteration 1 2 3 4 5 6 7
Training ? 0.001 0.001 0.0045 0.0055 0.01 0.05 0.1k 150 20 20 20 20 20 20
C&C ?07 ? 0.0045 0.0055 0.01 0.05 0.1k 20 20 20 20 20
Table 2: Beam step functions used for training: The first row shows the large scale settings used for most experiments
and the standard C&C settings. (cf. Table 1)
LF LP LR UF UP UR Data Util (%)
Baseline 87.40 87.85 86.95 93.11 93.59 92.63 91%
Max-F Parses 87.46 87.95 86.98 93.09 93.61 92.57 99%
CCGbank+Max-F 87.45 87.96 86.94 93.09 93.63 92.55 99%
Table 3: Performance on section 00 of CCGbank when comparing models trained with treebank-parses (Baseline)
and maximum F-score parses (Max-F) using adaptive supertagging as well as a combination of CCGbank and Max-F
parses. Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
effect. However, it does serve as a useful baseline.
4.2 Training with the Exact Algorithm
We first tested our assumptions about the feasibil-
ity of training with our exact algorithm by measur-
ing the amount of state-splitting. Figure 4 plots the
average number of splits per span against the rela-
tive span-frequency; this is based on a typical set of
training forests containing over 600 million states.
The number of splits increases exponentially with
span size but equally so decreases the number of
spans with many splits. Hence the small number of
states with a high number of splits is balanced by a
large number of spans with only a few splits: The
highest number of splits per span observed with our
settings was 4888 but we find that the average num-
ber of splits lies at 44. Encouragingly, this enables
experimentation in all but very large scale settings.
Figure 5 shows the distribution of n and d pairs
across all split-states in the training corpus; since
0%	 ?
2%	 ?
4%	 ?
6%	 ?
8%	 ?
10%	 ?
12%	 ?
1	 ?
10	 ?
100	 ?
1000	 ?
1	 ? 11	 ? 21	 ? 31	 ? 41	 ? 51	 ? 61	 ? 71	 ?
%	 ?o
f	 ?to
tal
	 ?sp
ans
	 ?
Av
era
ge	 ?
nu
mb
er	 ?
of	 ?
spl
its	 ?
span	 ?length	 ?
Average	 ?number	 ?of	 ?splits	 ?
Percentage	 ?of	 ?total	 ?spans	 ?
Figure 4: Average number of state-splits per span length
as introduced by a sentence-level F-measure loss func-
tion. The statistics are averaged over the training forests
generated using the settings described in ?4.
n, the number of correct dependencies, over d, the
number of all recovered dependencies, is precision,
the graph shows that only a minority of states have
either very high or very low precision. The range
of values suggests that the softmax-margin criterion
338
will have an opportunity to substantially modify the
expectations, hopefully to good effect.
!"
#!"
$!"
%!"
&!"
'!"
!" #!" $!" %!" &!" '!" (!" )!"
!"#
$%&
'()'
*(&
&%*+
',%-
%,%
!*.%
/'0!
1'
!"#$%&'()'233',%-%!,%!*.%/'0,1'
!*'!!!!!!" '!!!!!!*#!!!!!!!" #!!!!!!!*#'!!!!!!" #'!!!!!!*$!!!!!!!"
Figure 5: Distribution of states with d dependencies of
which n are correct in the training forests.
We next turn to the question of optimization with
these algorithms. Due to the significant computa-
tional requirements, we used the computationally
less intensive normal-form model of Clark and Cur-
ran (2007) as well as their more restrictive training
beam settings (Table 2). We train on all sentences of
the training set as above and test with AST.
In order to provide greater control over the influ-
ence of the loss function, we introduce a multiplier
? , which simply amends the second term of the ob-
jective function (3) to:
log
?
y?Y (xi)
exp{?T f(xi, y) + ? ? `(yi, y)}
Figure 6 plots performance of the exact loss func-
tions across different settings of ? on various evalu-
ation criteria, for models restricted to at most 3000
items per chart at training time to allow rapid ex-
perimentation with a wide parameter set. Even in
this constrained setting, it is encouraging to see that
each loss function performs best on the criteria it op-
timises. The precision-trained parser also does very
well on F-measure; this is because the parser has a
tendency to perform better in terms of precision than
recall.
4.3 Exact vs. Approximate Loss Functions
With these results in mind, we conducted a compar-
ison of parsers trained using our exact and approxi-
mate loss functions. Table 4 compares their perfor-
mance head to head when restricting training chart
sizes to 100,000 items per sentence, the largest set-
ting our computing resources allowed us to experi-
ment with. The results confirm that the loss-trained
models improve over a likelihood-trained baseline,
and furthermore that the exact loss functions seem
to have the best performance. However, the approx-
imations are extremely competitive with their exact
counterparts. Because they are also efficient, this
makes them attractive for larger-scale experiments.
Training time increases by an order of magnitude
with exact loss functions despite increased theoreti-
cal complexity (?3.1); there is no significant change
with approximate loss functions.
Table 5 shows performance of the approximate
losses with the large scale settings initially outlined
(?4). One striking result is that the softmax-margin
trained models coax more accurate parses from the
larger search space, in contrast to the likelihood-
trained models. Our best loss model improves the
labelled F-measure by over 0.8%.
4.4 Combination with Integrated Parsing and
Supertagging
As a final experiment, we embed our loss-trained
model into an integrated model that incorporates
Markov features over supertags into the parsing
model (Auli and Lopez, 2011). These features have
serious implications on search: even allowing for the
observation of Fowler and Penn (2010) that our CCG
is weakly context-free, the search problem is equiva-
lent to finding the optimal derivation in the weighted
intersection of a regular and context-free language
(Bar-Hillel et al, 1964), making search very expen-
sive. Therefore parsing with this model requires ap-
proximations.
To experiment with this combined model we use
loopy belief propagation (LBP; Pearl et al, 1985),
previously applied to dependency parsing by Smith
and Eisner (2008). A more detailed account of its
application to our combined model can be found in
(2011), but we sketch the idea here. We construct a
graphical model with two factors: one is a distribu-
339
!"# !$#
!%# !&#
'()
'(*+)
'(*,)
'(*-)
'(*.)
'(*/)
,) -) .) /) () +0)
!"#
$%%$
&'(
)*
$"+
,-$
'
.",'
1"234563) 7+)4822)
9:3%52586)4822) ;3%"44)4822)
'/*<)
'(*+)
'(*-)
'(*/)
'(*=)
'(*<)
'=*+)
,) -) .) /) () +0)
!"#
$%%$
&'/
-$0
1+12
3'
.",'
1"234563) 7+)4822)
9:3%52586)4822) ;3%"44)4822)
'/*/)
'/*=)
'/*<)
'(*+)
'(*-)
'(*/)
,) -) .) /) () +0)
!"#
$%%$
&'4
$0"
%%'
.",'
1"234563) 7+)4822)
9:3%52586)4822) ;3%"44)4822)
<-*=)
<-*=/)
<-*')
<-*'/)
<-*<)
<-*</)
,) -) .) /) () +0)
5,6
$-7
"88
138
'90
0,-
"0:
'
.",'
1"234563) 7+)4822)
9:3%52586)4822) ;3%"44)4822)
Figure 6: Performance of exact cost functions optimizing F-measure, precision and recall in terms of (a) labelled
F-measure, (b) precision, (c) recall and (d) supertag accuracy across various settings of ? on the development set.
section 00 (dev) section 23 (test)
LF LP LR UF UP UR LF LP LR UF UP UR
CLL 86.76 87.16 86.36 92.73 93.16 92.30 87.46 87.80 87.12 92.85 93.22 92.49
DecP 87.18 87.93 86.44 92.93 93.73 92.14 87.75 88.34 87.17 93.04 93.66 92.43
DecR 87.31 87.55 87.07 93.00 93.26 92.75 87.57 87.71 87.42 92.92 93.07 92.76
DecF1 87.27 87.78 86.77 93.04 93.58 92.50 87.69 88.10 87.28 93.04 93.48 92.61
P 87.25 87.85 86.66 92.99 93.63 92.36 87.76 88.23 87.30 93.06 93.55 92.57
R 87.34 87.51 87.16 92.98 93.17 92.80 87.57 87.62 87.51 92.92 92.98 92.86
F1 87.34 87.74 86.94 93.05 93.47 92.62 87.71 88.01 87.41 93.02 93.34 92.70
Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable
precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).
Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
340
section 00 (dev) section 23 (test)
AST Reverse AST Reverse
LF UF ST LF UF ST LF UF ST LF UF ST
CLL 87.38 93.08 94.21 87.36 93.13 93.99 87.73 93.09 94.33 87.65 93.06 94.01
DecP 87.35 92.99 94.25 87.75 93.25 94.22 88.10 93.26 94.51 88.51 93.50 94.39
DecR 87.48 93.00 94.34 87.70 93.16 94.30 87.66 92.83 94.38 87.77 92.91 94.22
DecF1 87.67 93.23 94.39 88.12 93.52 94.46 88.09 93.28 94.50 88.58 93.57 94.53
Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and
unlabelled F-measure (LF/UF) and supertag accuracy (ST).
tion over supertag variables defined by a supertag-
ging model, and the other is a distribution over these
variables and a set of span variables defined by our
parsing model.5 The factors communicate by pass-
ing messages across the shared supertag variables
that correspond to their marginal distributions over
those variables. Hence, to compute approximate ex-
pectations across the entire model, we run forward-
backward to obtain posterior supertag assignments.
These marginals are passed as inside values to the
inside-outside algorithm, which returns a new set
of posteriors. The new posteriors are incorporated
into a new iteration of forward-backward, and the
algorithm iterates until convergence, or until a fixed
number of iterations is reached ? we found that a
single iteration is sufficient, corresponding to a trun-
cated version of the algorithm in which posteriors
are simply passed from the supertagger to the parser.
To decode, we use the posteriors in a minimum-risk
parsing algorithm (Goodman, 1996).
Our baseline models are trained separately as be-
fore and combined at test time. For softmax-margin,
we combine a parsing model trained with F1 and
a supertagger trained with Hamming loss. Table 6
shows the results: we observe a gain of up to 1.5%
in labelled F1 and 0.9% in unlabelled F1 on the test
set. The loss functions prove their robustness by im-
proving the more accurate combined models up to
0.4% in labelled F1. Table 7 shows results with au-
tomatic part-of-speech tags and a direct comparison
with the Petrov parser trained on CCGbank (Fowler
and Penn, 2010) which we outpeform on all metrics.
5These complex factors resemble those of Smith and Eisner
(2008) and Dreyer and Eisner (2009); they can be thought of
as case-factor diagrams (McAllester et al, 2008)
5 Conclusion and Future Work
The softmax-margin criterion is a simple and effec-
tive approach to training log-linear parsers. We have
shown that it is possible to compute exact sentence-
level losses under standard parsing metrics, not only
approximations (Taskar et al, 2004). This enables
us to show the effectiveness of these approxima-
tions, and it turns out that they are excellent sub-
stitutes for exact loss functions. Indeed, the approxi-
mate losses are as easy to use as standard conditional
log-likelihood.
Empirically, softmax-margin training improves
parsing performance across the board, beating the
state-of-the-art CCG parsing model of Clark and
Curran (2007) by up to 0.8% labelled F-measure.
It also proves robust, improving a stronger base-
line based on a combined parsing and supertagging
model. Our final result of 89.3%/94.0% labelled
and unlabelled F-measure is the best result reported
for CCG parsing accuracy, beating the original C&C
baseline by up to 1.5%.
In future work we plan to scale our exact loss
functions to larger settings and to explore training
with loss functions within loopy belief propagation.
Although we have focused on CCG parsing in this
work, we expect our methods to be equally appli-
cable to parsing with other grammar formalisms in-
cluding context-free grammar or LTAG.
Acknowledgements
We would like to thank Stephen Clark, Chris-
tos Christodoulopoulos, Mark Granroth-Wilding,
Gholamreza Haffari, Alexandre Klementiev, Tom
Kwiatkowski, Kira Mourao, Matt Post, and Mark
Steedman for helpful discussion related to this
work and comments on previous drafts, and the
341
section 00 (dev) section 23 (test)
AST Reverse AST Reverse
LF UF ST LF UF ST LF UF ST LF UF ST
CLL 87.38 93.08 94.21 87.36 93.13 93.99 87.73 93.09 94.33 87.65 93.06 94.01
BP 87.67 93.26 94.43 88.35 93.72 94.73 88.25 93.33 94.60 88.86 93.75 94.84
+DecF1 87.90 93.40 94.52 88.58 93.88 94.79 88.32 93.32 94.66 89.15 93.89 94.98
+SA 87.73 93.28 94.49 88.40 93.71 94.75 88.47 93.48 94.71 89.25 93.98 95.01
Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as
parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
section 00 (dev) section 23 (test)
LF LP LR UF UP UR LF LP LR UF UP UR
CLL 85.53 85.73 85.33 91.99 92.20 91.77 85.74 85.90 85.58 91.92 92.09 91.75
Petrov I-5 85.79 86.09 85.50 92.44 92.76 92.13 86.01 86.29 85.73 92.34 92.64 92.04
BP 86.45 86.75 86.17 92.60 92.92 92.29 86.84 87.08 86.61 92.57 92.82 92.32
+DecF1 86.73 87.07 86.39 92.79 93.16 92.43 87.08 87.37 86.78 92.68 93.00 92.37
+SA 86.51 86.86 86.16 92.60 92.98 92.23 87.20 87.50 86.90 92.76 93.08 92.44
Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
(2010); evaluation is based on sentences for which all parsers returned an analysis.
anonymous reviewers for helpful comments. We
also acknowledge funding from EPSRC grant
EP/P504171/1 (Auli); and the resources provided by
the Edinburgh Compute and Data Facility.
A Computing F-Measure-Augmented
Expectations at the Corpus Level
To compute exact corpus-level expectations for softmax-
margin using F-measure, we add an additional transition
before reaching the GOAL item in our original program.
To reach it, we must parse every sentence in the corpus,
associating statistics of aggregate ?n, d? pairs for the en-
tire training set in intermediate symbols ?(1)...?(m) with
the following inside recursions.
I(?(1)n,d) = I(S
(1)
0,|x(1)|,n,d)
I(?(`)n,d) =
?
n?,n??:n?+n??=n
I(?(`?1)n?,d? )I(S
(`)
0,N,n??,d??)
I(GOAL) =
?
n,d
I(?(m)n,d )
(
1? 2nd+ |y|
)
Outside recursions follow straightforwardly. Implemen-
tation of this algorithm would require substantial dis-
tributed computation or external data structures, so we
did not attempt it.
References
M. Auli and A. Lopez. 2011. A Comparison of Loopy
Belief Propagation and Dual Decomposition for Inte-
grated CCG Supertagging and Parsing. In Proc. of
ACL, June.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. Journal of the Acoustical Society of America,
65.
S. Bangalore and A. K. Joshi. 1999. Supertagging: An
Approach to Almost Parsing. Computational Linguis-
tics, 25(2):238?265, June.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application, pages 116?150.
S. Clark and J. R. Curran. 2004. The importance of su-
pertagging for wide-coverage CCG parsing. In COL-
ING, Morristown, NJ, USA.
S. Clark and J. R. Curran. 2007. Wide-Coverage Ef-
ficient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4):493?552.
S. Clark and J. Hockenmaier. 2002. Evaluating a Wide-
Coverage CCG Parser. In Proceedings of the LREC
2002 Beyond Parseval Workshop, pages 60?66, Las
Palmas, Spain.
S. Clark, J. Hockenmaier, and M. Steedman. 2002.
Building deep dependency structures with a wide-
coverage CCG parser. In Proc. of ACL.
342
S. Clark. 2002. Supertagging for Combinatory Catego-
rial Grammar. In TAG+6.
M. Dreyer and J. Eisner. 2009. Graphical models over
multiple strings. In Proc. of EMNLP.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008.
Feature-based, conditional random field parsing. In
Proceedings of ACL-HLT.
T. A. D. Fowler and G. Penn. 2010. Accurate context-
free parsing with combinatory categorial grammar. In
Proc. of ACL.
K. Gimpel and N. A. Smith. 2010a. Softmax-margin
CRFs: training log-linear models with cost functions.
In HLT ?10: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
K. Gimpel and N. A. Smith. 2010b. Softmax-margin
training for structured log-linear models. Technical
Report CMU-LTI-10-008, Carnegie Mellon Univer-
sity.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL, pages 177?183, Jun.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
L. Huang. 2008. Forest Reranking: Discriminative pars-
ing with Non-Local Features. In Proceedings of ACL-
08: HLT.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML,
pages 282?289.
D. McAllester, M. Collins, and F. Pereira. 2008. Case-
factor diagrams for structured probabilistic modeling.
Journal of Computer and System Sciences, 74(1):84?
96.
D. McAllester. 1999. On the complexity analysis of
static analyses. In Proc. of Static Analysis Symposium,
volume 1694/1999 of LNCS. Springer Verlag.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, Jul.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
D. Povey and P. Woodland. 2008. Minimum phone er-
ror and I-smoothing for improved discrimative train-
ing. In Proc. of ICASSP.
F. Sha and L. K. Saul. 2006. Large margin hidden
Markov models for automatic speech recognition. In
Proc. of NIPS.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
M. Steedman. 2000. The syntactic process. MIT Press,
Cambridge, MA.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of EMNLP,
pages 1?8, Jul.
343
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044?1054,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Language and Translation Modeling with Recurrent Neural Networks
Michael Auli, Michel Galley, Chris Quirk, Geoffrey Zweig
Microsoft Research
Redmond, WA, USA
{michael.auli,mgalley,chrisq,gzweig}@microsoft.com
Abstract
We present a joint language and transla-
tion model based on a recurrent neural net-
work which predicts target words based on
an unbounded history of both source and tar-
get words. The weaker independence as-
sumptions of this model result in a vastly
larger search space compared to related feed-
forward-based language or translation models.
We tackle this issue with a new lattice rescor-
ing algorithm and demonstrate its effective-
ness empirically. Our joint model builds on a
well known recurrent neural network language
model (Mikolov, 2012) augmented by a layer
of additional inputs from the source language.
We show competitive accuracy compared to
the traditional channel model features. Our
best results improve the output of a system
trained on WMT 2012 French-English data by
up to 1.5 BLEU, and by 1.1 BLEU on average
across several test sets.
1 Introduction
Recently, several feed-forward neural network-
based language and translation models have
achieved impressive accuracy improvements on sta-
tistical machine translation tasks (Allauzen et al,
2011; Le et al, 2012b; Schwenk et al, 2012). In this
paper we focus on recurrent neural network archi-
tectures, which have recently advanced the state of
the art in language modeling (Mikolov et al, 2010;
Mikolov et al, 2011a; Mikolov, 2012), outperform-
ing multi-layer feed-forward based networks in both
perplexity and word error rate in speech recognition
(Arisoy et al, 2012; Sundermeyer et al, 2013). The
major attraction of recurrent architectures is their
potential to capture long-span dependencies since
predictions are based on an unbounded history of
previous words. This is in contrast to feed-forward
networks as well as conventional n-gram models,
both of which are limited to fixed-length contexts.
Building on the success of recurrent architectures,
we base our joint language and translation model
on an extension of the recurrent neural network lan-
guage model (Mikolov and Zweig, 2012) that intro-
duces a layer of additional inputs (?2).
Most previous work on neural networks for
speech recognition or machine translation used a
rescoring setup based on n-best lists (Arisoy et al,
2012; Mikolov, 2012) for evaluation, thereby side
stepping the algorithmic and engineering challenges
of direct decoder-integration.1 Instead, we exploit
lattices, which offer a much richer representation
of the decoder output, since they compactly encode
an exponential number of translation hypotheses in
polynomial space. In contrast, n-best lists are typi-
cally very redundant, representing only a few com-
binations of top scoring arcs in the lattice. A major
challenge in lattice rescoring with a recurrent neural
network model is the effect of the unbounded history
on search since the usual dynamic programming as-
sumptions which are exploited for efficiency do not
hold up anymore. We apply a novel algorithm to the
task of rescoring with an unbounded language model
and empirically demonstrate its effectiveness (?3).
The algorithm proves robust, leading to signif-
icant improvements with the recurrent neural net-
work language model over a competitive n-gram
baseline across several language pairs. We even ob-
serve consistent gains when pairing the model with a
large n-gram model trained on up to 575 times more
1One notable exception is Le et al (2012a) who rescore reorder-
ing lattices with a feed-forward network-based model.
1044
data, demonstrating that the model provides comple-
mentary information (?4).
Our joint modeling approach is based on adding a
continuous space representation of the foreign sen-
tence as an additional input to the recurrent neu-
ral network language model. With this extension,
the language model can measure the consistency
between the source and target words in a context-
sensitive way. The model effectively combines the
functionality of both the traditional channel and lan-
guage model features. We test the power of this
new model by using it as the only source of tradi-
tional channel information. Overall, we find that the
model achieves accuracy competitive with the older
channel model features and that it can improve over
the gains observed with the recurrent neural network
language model (?5).
2 Model Structure
We base our model on the recurrent neural network
language model of Mikolov et al (2010) which is
factored into an input layer, a hidden layer with re-
current connections, and an output layer (Figure 1).
The input layer encodes the target language word at
time t as a 1-of-N vector et, where |V | is the size
of the vocabulary, and the output layer yt represents
a probability distribution over target words; both of
size |V |. The hidden layer state ht encodes the his-
tory of all words observed in the sequence up to time
step t. This model is extended by an auxiliary input
layer ft which provides complementary information
to the input layer (Mikolov and Zweig, 2012). While
the auxiliary input layer can be used to feed in arbi-
trary additional information, we focus on encodings
of the foreign sentence (?5).
The state of the hidden layer is determined by the
input layer, the auxiliary input layer and the hidden
layer configuration of the previous time step ht?1.
The weights of the connections between the layers
are summarized in a number of matrices: U, F and
W, represent weights from the input layer to the hid-
den layer, from the auxiliary input layer to the hid-
den layer, and from the previous hidden layer to the
current hidden layer, respectively. Matrix V repre-
sents connections between the current hidden layer
and the output layer; G represents direct weights be-
tween the auxiliary input and output layers.
et
ht-1
ft
ht
yt
V
G
F
W
U
D
Figure 1: Structure of the recurrent neural network
model, including the auxiliary input layer ft.
The hidden and output layers are computed via a
series of matrix-vector products and non-linearities:
ht = s(Uet +Wht?1 + Ff t)
yt = g(Vht +Gf t)
where
s(z) =
1
1 + exp {?z}
, g(zm) =
exp {zm}
?
k exp {zk}
are sigmoid and softmax functions, respectively.
Additionally, the network is interpolated with a
maximum entropy model of sparse n-gram features
over input words (Mikolov et al, 2011a).2 The max-
imum entropy weights are added to the output acti-
vations before computing the softmax.
The model is optimized via a maximum likeli-
hood objective function using stochastic gradient
descent. Training is based on the back propaga-
tion through time algorithm, which unrolls the net-
work and then computes error gradients over mul-
tiple time steps (Rumelhart et al, 1986). Af-
ter training, the output layer represents posteriors
p(et+1|ett?n+1,ht, ft); the probabilities of words in
the output vocabulary given the n previous input
words ett?n+1, the hidden layer configuration ht as
well as the auxiliary input layer configuration ft.
2While these features depend on multiple input words, we de-
picted them for simplicity as a connection between the current
input word vector et and the output layer (D).
1045
Na??ve computation of the probability distribution
over the next word is very expensive for large vo-
cabularies. A well established efficiency trick uses
word-classing to create a more efficient two-step
process (Goodman, 2001; Emami and Jelinek, 2005;
Mikolov et al, 2011b) where each word is assigned
a unique class. To compute the probability of a
word, we first compute the probability of its class,
and then multiply it by the probability of the word
conditioned on the class:
p(et+1|e
t
t?n+1,ht, ft) =
p(ci|e
t
t?n+1,ht, ft)? p(et+1|ci, e
t
t?n+1,ht, ft)
This factorization reduces the complexity of com-
puting the output probabilities from O(|V |) to
O(|C| + maxi |ci|) where |C| is the number of
classes and |ci| is the number of words in class
ci. The best case complexity O(
?
|V |) requires the
number of classes and words to be evenly balanced,
i.e., each class contains exactly as many words as
there are classes.
3 Lattice Rescoring with an Unbounded
Language Model
We evaluate our joint language and translation
model in a lattice rescoring setup, allowing us to
search over a much larger space of translations than
would be possible with n-best lists. While very
space efficient, lattices also impose restrictions on
the context available to features, a particularly chal-
lenging setting for our model which depends on the
entire prefix of a translation. In the ensuing de-
scription we introduce a new algorithm to efficiently
tackle this issue.
Phrase-based decoders operate by maintaining a
set of states representing competing translations, ei-
ther partial or complete. Each state is scored by a
number of features including the n-gram language
model. The independence assumptions of the fea-
tures determine the amount of context each state
needs to maintain in order for it to be possible to
assign a score to it. For example, a trigram language
model is indifferent to any context other than the
two immediately preceding words. Assuming the
trigram model dominates the Markov assumptions
of all other features, which is typically the case, then
we have to maintain at least two words at each state,
also known as the n-gram context.
1: function RESCORELATTICE(k, V , E, s, T )
2: Q? TOPOLOGICALLY-SORT(V )
3: for all v in V do . Heaps of split-states
4: Hv ? MINHEAP()
5: end for
6: h0 ? ~0 . Initialize start-state
7: Hs.ADD(h0)
8: for all v in Q do . Examine outgoing arcs
9: for ?v, x? in E do
10: for h in Hv do . Extend LM states
11: h? ? SCORERNN(h, phrase(h))
12: parent(h?)? h . Backpointers
13: if Hx.size() ? k? . Beam width
14: Hx.MIN()<score(h?) then
15: Hx.REMOVEMIN()
16: if Hx.size()<k then
17: Hx.ADD(h?)
18: end for
19: end for
20: end for
21: I = MAXHEAP()
22: for all t in T do . Find best final split-state
23: I.MERGE(Ht)
24: end for
25: return I.MAX()
26: end function
Figure 2: Push-forward rescoring with a recurrent neu-
ral network language model given a beam-width for lan-
guage model split-states k, decoder states V , edges E, a
start state s and final states T .
However, a recurrent neural network language
model makes much weaker independence assump-
tions. In fact, the predictions of such a model depend
on all previous words in the sentence, which would
imply a potentially very large context. But storing
all words is an inefficient solution from a dynamic
programming point of view. Fortunately, we do not
need to maintain entire translations as context in the
states: the recurrent model compactly encodes the
entire history of previous words in the hidden layer
configuration hi. It is therefore sufficient to add hi
as context, instead of the entire translation. The lan-
guage model can then simply score any new words
1046
based on hi from the previous state when a new state
is created.
A much larger problem is that items, that were
previously equivalent from a dynamic programming
perspective, may now be different. Standard phrase-
based decoders (Koehn et al, 2007) recombine de-
coder states with the same context into a single
state because they are equivalent to the model fea-
tures; usually recombination retains only the high-
est scoring candidate.3 However, if the context is
large, then the amount of recombination will de-
crease significantly, leading to less variety in the de-
coder beam. This was confirmed in preliminary ex-
periments where we simulated context sizes of up to
100 words but found that accuracy dropped by be-
tween 0.5-1.0 BLEU.
Integrating a long-span language model na??vely
requires to keep context equivalent to the entire left
prefix of the translation, a setting which would per-
mit very little recombination. Instead of using ineffi-
cient long-span contexts, we propose to maintain the
usual n-gram context and to keep a fixed number of
hidden layer configurations k at each decoder state.
This leads to a new split-state dynamic program
which splits each decoder state into at most k new
items, each with a separate hidden layer configura-
tion representing an unbounded history (Figure 2).
This maintains diversity in the explored translation
hypothesis space and preserves high-scoring hidden
layer configurations.
What is the effect of this strategy? To answer
this question we measured translation accuracy for
various settings of k on our lattice rescoring setup
(see ?4 for details). In the same experiment, we
compare lattices to n-best lists in terms of accuracy,
model score and wall time impact.4 The results (Ta-
ble 1 and Figure 3) show that reranking accuracy on
lattices is not significantly better, however, rescor-
ing lattices with k = 1 is much faster than n-best
lists. Similar observations have been made in previ-
ous work on minimum error-rate training (Macherey
3Assuming a max-translation decision rule. In a minimum-risk
setting, we may assign the sum of the scores of all candidates
to the retained item.
4We measured running times on an HP z800 workstation
equipped with 24 GB main memory and two Xeon E5640
CPUs with four cores each, clocked at 2.66 GHz. All experi-
ments were run single-threaded.
BLEU oracle sec/sent
Baseline 28.25 - 0.173
100-best 28.90 37.22 0.470
1000-best 28.99 40.06 3.920
lattice (k = 1) 29.00 43.50 0.093
lattice (k = 10) 29.04 43.50 0.599
lattice (k = 100) 29.03 43.50 4.531
Table 1: Rescoring n-best lists and lattices with various
language model beam widths k. Accuracy is based on
the news2011 French-English task. Timing results are in
addition to the baseline.
Figure 3: BLEU vs. log probabilities of 1-best transla-
tions when rescoring n-best lists and lattices (cf. Table 1).
et al, 2008). The recurrent language model adds an
overhead of about 54% at k = 1 on top of the time
to produce the baseline 1-best output, a consider-
able but not necessarily prohibitive overhead. Larger
values of k return higher probability solutions, but
there is little impact on accuracy: the BLEU score
is nearly identical when retaining up to 100 histories
compared to keeping only the highest scoring.
While surprising at first, we believe that this ef-
fect is due to the high similarity of the translations
represented by the histories in the beam. Each his-
tory represents a different translation but all transla-
tion hypothesis share the same n-gram context, and,
more importantly, they are translations of the same
foreign words, since they have exactly the same cov-
erage vector. These commonalities are likely to re-
sult in similar recurrent histories, which in turn re-
duces the effect of aggressive pruning.
4 Language Model Experiments
Recurrent neural network language models have
previously only been used in n-best rescoring
1047
settings and on small-scale tasks with baseline
language models trained on only 17.5m words
(Mikolov, 2012). We extend this work by experi-
menting on lattices using strong baselines with n-
gram models trained on over one billion words and
by evaluating on a number of language pairs.
4.1 Experimental Setup
Baseline. We experiment with an in-house phrase-
based system similar to Moses (Koehn et al,
2003), scoring translations by a set of common fea-
tures including maximum likelihood estimates of
source given target mappings pMLE(e|f) and vice
versa pMLE(f |e), as well as lexical weighting es-
timates pLW (e|f) and pLW (f |e), word and phrase-
penalties, a linear distortion feature and a lexicalized
reordering feature. Log-linear weights are estimated
with minimum error rate training (Och, 2003).
Evaluation. We use training and test data
from the WMT 2012 campaign and report results
on French-English, German-English and English-
German. Translation models are estimated on 102m
words of parallel data for French-English, 91m
words for German-English and English-German; be-
tween 3.5-5m words are newswire, depending on the
language pair, and the remainder are parliamentary
proceedings. The baseline systems use two 5-gram
modified Kneser-Ney language models; the first is
estimated on the target-side of the parallel data,
while the second is based on a large newswire corpus
released as part of the WMT campaign. For French-
English and German-English we use a language
model based on 1.15bn words, and for English-
German we train a model on 327m words. We eval-
uate on the newswire test sets from 2010-2011 con-
taining between 2034-3003 sentences. Log-linear
weights are estimated on the 2009 data set compris-
ing 2525 sentences. We rescore the lattices produced
by the baseline systems with an aggressive but effec-
tive context beam of k = 1 that did not harm accu-
racy in preliminary experiments (?3).
Neural Network Language Model. The vocab-
ularies of the language models are comprised of
the words in the training set after removing single-
tons. We obtain word-classes using a version of
Brown-Clustering with an additional regularization
term to optimize the runtime of the language model
(Brown et al, 1992; Zweig and Makarychev, 2013).
Direct connections use maximum entropy features
over unigrams, bigrams and trigrams (Mikolov et al,
2011a). We use the standard settings for the model
with the default learning rate ? = 0.1 that decays
exponentially if the validation set entropy does not
increase after each epoch. Back propagation through
time computes error gradients over the past twenty
time steps. Training is stopped after 20 epochs or
when the validation entropy does not decrease over
two epochs. We experiment with varying training
data sizes and randomly draw the data from the same
corpora used for the baseline systems. Throughout,
we use a hidden layer size of 100 which provided a
good trade-off between time and accuracy in initial
experiments.
4.2 Results
Training times for neural networks can be a major
bottleneck. Recurrent architectures are particularly
hard to parallelize due to their inherent dependence
on the previous hidden layer configuration. One
straightforward way to influence training time is to
change the size of the training corpus.
Our results (Table 2, Table 3 and Table 4) show
that even small models trained on only two million
words significantly improve over the 1-best decoder
output (Baseline); this represents only 0.6 percent
of the data available to the n-gram model used by
the baseline. Models of this size can be trained in
only about 3.5 hours. A model trained on 50m words
took 63 hours to train. When paired with an n-gram
model trained on 25 times more data, accuracy im-
proved by up to 0.7 BLEU on French-English.
5 Joint Model Experiments
In the next set of experiments, we turn to the joint
language and translation model, an extension of the
recurrent neural network language model with ad-
ditional inputs for the foreign sentence. We first
introduce two continuous space representations of
the foreign sentence (?5.1). Using these represen-
tations we evaluate the accuracy of the joint model
in the lattice rescoring setup and compare against the
traditional translation channel model features (?5.2).
Next, we establish an upper bound on accuracy for
the joint model via an oracle experiment (?5.3). In-
spired by the results of the oracle experiment we
1048
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 26.6 27.6 28.3 27.5 27.8
+RNNLM (2m) 27.5 28.1 28.6 28.1 28.3
+RNNLM (50m) 27.7 28.2 29.0 28.1 28.5
Table 2: French-English results when rescoring with the recurrent neural network language model; the baseline relies
on an n-gram model trained on 1.15bn words.
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 21.2 20.7 19.2 20.6 20.0
+RNNLM (2m) 21.8 20.9 19.4 20.9 20.3
+RNNLM (50m) 22.1 21.1 19.7 21.0 20.5
Table 3: German-English results when rescoring with the recurrent neural network language model.
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 15.2 15.6 14.3 15.7 15.1
+RNNLM (2m) 15.7 15.9 14.6 16.0 15.4
+RNNLM (50m) 15.8 15.9 14.7 16.1 15.5
Table 4: English-German results when rescoring with the recurrent neural network language model; the baseline relies
on an n-gram model trained on 327m words.
train a transform between the source words and the
reference representations. This leads to the best re-
sults improving 1.5 BLEU over the 1-best decoder
output and adding 0.2 BLEU on average to the gains
achieved by the recurrent language model (?5.4).
Setup. Conventional language models can be
trained on monolingual or bilingual data; however,
the joint model can only be trained on the latter.
In order to control for data size effects, we restrict
training of all models, including the baseline n-gram
model, to the target side of the parallel corpus, about
102m words for French-English. Furthermore we
train recurrent models only on the newswire portion
(about 3.5m words for training and 250k words for
validation) since initial experiments showed compa-
rable results to using the full parallel corpus, avail-
able to the baseline. This is reasonable since the test
data is newswire. Also, it allows for more rapid ex-
perimentation.
5.1 Foreign Sentence Representations
We represent foreign sentences either by latent se-
mantic analysis (LSA; Deerwester et al 1990) or by
word encodings produced as a by-product of train-
ing the recurrent neural network language model on
the source words.
LSA is widely used for representing words and
documents in low-dimensional vector space. The
method applies reduced singular value decomposi-
tion (SVD) to a matrix M of word counts; in our
setting, rows represent sentences and columns rep-
resent foreign words. SVD reduces the number
of columns while preserving similarity among the
rows, effectively mapping from a high-dimensional
representation of a sentence, as a set of words, to a
low-dimensional set of concepts. The output of SVD
is an approximation of M by three matrices: T con-
tains single word representations, R represents full
sentences, and S is a diagonal scaling matrix:
M ? TSRT
Given vocabulary V and n sentences, we construct
M as a matrix of size |V ?n|. The ij-th entry is the
number of times word i occurs in sentence j, also
known as the term frequency value; the entry is also
weighted by the inverse document frequency, the rel-
ative importance of word i among all sentences, ex-
pressed as the negative logarithm of the fraction of
sentences in which word i occurs.
As a second representation we use single word
1049
embeddings implicitly learned by the input layer
weights U of the recurrent neural network language
model (?2), denoted as RNN. Each word is repre-
sented by a vector of size |hi|, the number of neu-
rons in the hidden layer; in our experiments, we
consider concatenations of individual word vectors
to represent foreign word contexts. These encodings
have previously been found to capture syntactic and
semantic regularities (Mikolov et al, 2013) and are
readily available in our experimental framework via
training a recurrent neural network language model
on the source-side of the parallel corpus.
5.2 Results
We first experiment with the two previously intro-
duced representations of the source-side sentence.
Table 5 shows the results compared to the 1-best de-
coder output and an RNN language model (target-
only). We first try LSA encodings of the entire
foreign sentence as 80 or 240 dimensional vectors
(sent-lsa-dim80, sent-lsa-dim240). Next, we experi-
ment with single-word RNN representations of slid-
ing word-windows in the hope of representing rel-
evant context more precisely. Word-windows are
constructed relative to the source words aligned to
the current target word, and individual word vec-
tors are concatenated into a single vector. We
first try contexts which do not include the aligned
source words, in the hope of capturing information
not already modeled by the channel models, start-
ing with the next five words (ww-rnn-dim50.n5),
the five previous and the next five words (ww-rnn-
dim50.p5n5) as well as the previous three words
(ww-rnn-dim50.p3). Next, we experiment with
word-windows of up to five aligned source words
(ww-rnn-dim50.c5). Finally, we try contexts based
on LSA word vectors (ww-lsa-dim50.n5, ww-lsa-
dim50.p3).5
While all models improve over the baseline, none
significantly outperforms the recurrent neural net-
work language model in terms of BLEU. However,
the perplexity results suggest that the models uti-
lize the foreign representations since all joint mod-
els improve vastly over the target-only language
5We ignore the coverage vector when determining word-
windows which risks including already translated words.
Building word-windows based on the coverage vector requires
additional state in a rescoring setting meant to be light-weight.
?p(f |e)
?p(e|f) ?p(e|f)
Baseline without CM 24.0 22.5
+ target-only 24.5 22.6
+ sent-lsa-dim240 24.9 23.3
+ ww-rnn-dim50.n5 24.9 24.0
+ ww-rnn-dim50.p5n5 24.6 23.7
+ ww-rnn-dim50.p3 24.6 22.3
+ ww-rnn-dim50.c5 24.9 24.0
+ ww-lsa-dim50.n5 24.8 23.9
+ ww-lsa-dim50.p3 23.8 23.2
Table 6: Comparison of the joint model and the chan-
nel model features (CM) by removing channel features
corresponding to ?p(e|f) from the lattices, or both di-
rections ?p(e|f),?p(f |e) and replacing them by vari-
ous joint models. We re-tuned the log-linear weights for
different feature-sets. Accuracy is based on the average
BLEU over news2010, newssyscomb2010, news2011.
model. The lowest perplexity is achieved by the
context covering the aligned source words (ww-rnn-
dim50.c5) since the source words are a better pre-
dictor of the target words than outside context.
The experiments so far measured if the joint
model can improve in addition to the four channel
model features used by the baseline, that is, the max-
imum likelihood and lexical translation features in
both translation directions. The joint model clearly
overlaps with these features, but how well does
the recurrent model perform compared against the
channel model features? To answer this question,
we removed channel model features corresponding
to the same translation direction as the joint model,
specifically pMLE(e|f) and pLW (e|f), from the lat-
tices and measured the effect of adding the joint
models.
The results (Table 6, column ?p(e|f)) clearly
show that our joint models are competitive with the
channel model features by outperforming the orig-
inal baseline with all channel model features (24.7
BLEU) by 0.2 BLEU (ww-rnn-dim50.n5, ww-rnn-
dim50.c5). As a second experiment, we removed all
channel model features (column ?p(e|f), p(f |e)),
diminishing baseline accuracy to 22.5 BLEU. In this
setting, the best joint model is able to make up 1.5
of the 2.2 BLEU lost due to removal of the channel
1050
dev news2010 news2011 newssyscomb2010 Avg(test) PPL
Baseline 24.3 24.4 25.1 24.3 24.7 341
target-only 25.1 25.1 26.4 25.0 25.6 218
sent-lsa-dim80 25.2 25.2 26.3 25.1 25.6 147
sent-lsa-dim240 25.1 25.0 26.2 24.9 25.4 126
ww-rnn-dim50.n5 24.9 25.0 26.3 24.8 25.4 61
ww-rnn-dim50.p5n5 25.0 24.8 26.2 24.7 25.3 59
ww-rnn-dim50.p3 25.1 25.1 26.5 24.9 25.6 143
ww-rnn-dim50.c5 24.8 24.9 26.0 24.8 25.3 16
ww-lsa-dim50.n5 25.0 25.0 26.2 24.8 25.4 76
ww-lsa-dim50.p3 25.1 25.1 26.5 24.9 25.6 151
Table 5: Translation accuracy of the joint model with various encodings of the foreign sentence measured on the
French-English task. Perplexity (PPL) is based on news2011.
model features, while modeling only a single trans-
lation direction. This setup also shows the negligible
effect of the target-only language model in the ab-
sence of translation scores, whereas the joint models
are much more effective since they do model transla-
tion. Overall, the best joint models prove very com-
petitive to the traditional channel features.
5.3 Oracle Experiment
The previous section examined the effect of a set
of basic foreign sentence representations. Although
we find some benefit from these representations, the
differences are not large. One might naturally ask
whether there is greater potential upside from this
channel model. Therefore we turn to measuring the
upper bound on accuracy for the joint approach as a
whole.
Specifically, we would like to find a bound on ac-
curacy given an ideal representation of the source
sentence. To answer this question, we conducted an
experiment where the joint model has access to an
LSA representation of the reference translation.
Table 7 shows that the joint approach has an ora-
cle accuracy of up to 4.3 BLEU above the baseline.
This clearly confirms that the joint approach can ex-
ploit the additional information to improve BLEU,
given a good enough representation of the foreign
sentence. In terms of perplexity, we see an improve-
ment of up to 65% over the target-only model. It
should be noted that since LSA representations are
computed on reference words, perplexity no longer
has its standard meaning.
BLEU PPL
Baseline 25.2 341
target-only 26.4 218
oracle (sent-lsa-dim40) 27.7 124
oracle (sent-lsa-dim80) 28.5 103
oracle (sent-lsa-dim160) 29.0 86
oracle (sent-lsa-dim240) 29.5 76
Table 7: Oracle accuracy of the joint model when us-
ing an LSA encoding of the references, measured on the
news2011 French-English task.
5.4 Target Language Projections
Our experiments so far showed that joint models
based on direct representations of the source words
are very competitive to the traditional channel mod-
els (?5.2). However, these experiments have not
shown any improvements over the normal recurrent
neural network language model. The previous sec-
tion demonstrated that good representations can lead
to substantial gains (?5.3). In order to bridge the gap,
we propose to learn a separate transform from the
foreign words to an encoding of the reference target
words, thus making the source-side representations
look more like the target-side encodings used in the
oracle experiment.
Specifically, we learn a linear transform
d? : x? r mapping directly from a vector en-
coding of the foreign sentence x to an l-dimensional
LSA representation r of the reference sentence. At
test and training time we apply d? to the foreign
words and use the transformation instead of a direct
1051
dev news2010 news2011 newssyscomb2010 Avg(test) PPL
Baseline 24.3 24.4 25.1 24.3 24.7 341
target-only 25.1 25.1 26.4 25.0 25.6 218
proj-lsa-dim40 25.1 25.3 26.5 25.2 25.8 145
proj-lsa-dim80 25.1 25.3 26.6 25.2 25.8 134
Table 8: Translation accuracy of the joint model with a source-target transform, measured on the French-English task.
Perplexity (PPL) is based on news2011; differences to target-only are significant at the p < 0.001 level.
source-side representation.
The transform models all foreign words in the par-
allel corpus except singletons, which are collapsed
into a unique class, similar to the recurrent neural
network language model. We train the transform to
minimize the squared error with respect to the ref-
erence LSA vector using an SGD online learner:
?? = argmin
?
n?
i=1
(
ri ? d?(xi)
)2
(1)
We found a simple constant learning rate, tuned
on the validation data, to be as effective as sched-
ules based on constant decay, or reducing the learn-
ing rate when the validation error increased. Our
feature-set includes unigram and bigram word fea-
tures. The value of unigram features is simply the
unigram count in that sentence; bigram features re-
ceive a weight of the bigram count divided by two
to help prevent overfitting. Then the vector for each
sentence was divided by its L2 norm. Both weight-
ing and normalization led to substantial improve-
ments in test set error. More complex features such
as skip-bigrams, trigrams and character n-grams did
not yield any significant improvements. Even this
representation of sentences is composed of a large
number of instances, and so we resorted to feature
hashing by computing feature ids as the least signif-
icant 20 bits of each feature name. Our best trans-
form achieved a cosine similarity of 0.816 on the
training data, 0.757 on the validation data, and 0.749
on news2011.
The results (Table 8) show that the transform im-
proves over the recurrent neural network language
model on all test sets and by 0.2 BLEU on average.
We verified significance over the target-only model
using paired bootstrap resampling (Koehn, 2004)
over all test sets (7526 sentences) at the p < 0.001
level. Overall, we improve accuracy by up to 1.5
BLEU and by 1.1 BLEU on average across all test
sets over the decoder 1-best with our joint language
and translation model.
6 Related Work
Our approach of combining language and translation
modeling is very much in line with recent work on
n-gram-based translation models (Crego and Yvon,
2010), and more recently continuous space-based
translation models (Le et al, 2012a; Gao et al,
2013). The joint model presented in this paper dif-
fers in a number of key aspects: we use a recur-
rent architecture representing an unbounded history
of both source and target words, rather than a feed-
forward style network. Feed-forward networks and
n-gram models have a finite history which makes
predictions independent of anything but a small his-
tory of words. Furthermore, we only model the
target-side which is different to previous work mod-
eling both sides.
We introduced a new algorithm to tackle lattice
rescoring with an unbounded model. The auto-
matic speech recognition community has previously
addressed this issue by either approximating long-
span language models via simpler but more tractable
models (Deoras et al, 2011b), or by identifying con-
fusable subsets of the lattice from which n-best lists
are constructed and rescored (Deoras et al, 2011a).
We extend their work by directly mapping a recur-
rent neural network model onto the structure of the
lattice, rescoring all states instead of focusing only
on subsets.
7 Conclusion and Future Work
Joint language and translation modeling with recur-
rent neural networks leads to substantial gains over
the 1-best decoder output, raising accuracy by up
to 1.5 BLEU and by 1.1 BLEU on average across
1052
several test sets. The joint approach also improves
over the gains of the recurrent neural network lan-
guage model, adding 0.2 BLEU on average across
several test sets. Our models are competitive to the
traditional channel models, outperforming them in a
head-to-head comparison.
Furthermore, we tackled the issue of lattice
rescoring with an unbounded recurrent model by
means of a novel algorithm that keeps a beam of re-
current histories. Finally, we have shown that the
recurrent neural network language model can sig-
nificantly improve over n-gram baselines across a
range of language-pairs, even when the baselines
were trained on 575 times more data.
In future work we plan to directly learn represen-
tations of the source-side during training of the joint
model. Thus, the model itself can decide which en-
coding is best for the task. We also plan to change
the cross entropy objective to a BLEU-inspired ob-
jective in a discriminative training regime, which we
hope to be more effective. We would also like to ap-
ply recent advances in tackling the vanishing gradi-
ent problem (Pascanu et al, 2013) using a regular-
ization term to maintain the magnitude of the gradi-
ents during back propagation through time. Finally,
we would like to integrate the recurrent model di-
rectly into first-pass decoding, a straightforward ex-
tension of lattice rescoring using the algorithm we
developed.
Acknowledgments
We would like to thank Anthony Aue, Hany Has-
san Awadalla, Jon Clark, Li Deng, Sauleh Eetemadi,
Jianfeng Gao, Qin Gao, Xiaodong He, Will Lewis,
Arul Menezes, and Kristina Toutanova for helpful
discussions related to this work as well as for com-
ments on previous drafts. We would also like to
thank the anonymous reviewers for their comments.
References
Alexandre Allauzen, He?le`ne Bonneau-Maynard, Hai-Son
Le, Aure?lien Max, Guillaume Wisniewski, Franc?ois
Yvon, Gilles Adda, Josep Maria Crego, Adrien
Lardilleux, Thomas Lavergne, and Artem Sokolov.
2011. LIMSI @ WMT11. In Proc. of WMT, pages
309?315, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhu-
vana Ramabhadran. 2012. Deep Neural Network
Language Models. In NAACL-HLT Workshop on the
Future of Language Modeling for HLT, pages 20?28,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479, Dec.
Josep Crego and Franois Yvon. 2010. Factored bilingual
n-gram language models for statistical machine trans-
lation. Machine Translation, 24(2):159?175.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Anoop Deoras, Toma?s? Mikolov, and Kenneth Church.
2011a. A Fast Re-scoring Strategy to Capture Long-
Distance Dependencies. In Proc. of EMNLP, pages
1116?1127, Stroudsburg, PA, USA, July. Association
for Computational Linguistics.
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink,
M. Karafiat, and Sanjeev Khudanpur. 2011b. Varia-
tional Approximation of Long-Span Language Models
for LVCSR. In Proc. of ICASSP, pages 5532?5535.
Ahmad Emami and Frederick Jelinek. 2005. A Neural
Syntactic Language Model. Machine Learning, 60(1-
3):195?227, September.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng.
2013. Learning Semantic Representations for the
Phrase Translation Model. Technical Report MSR-
TR-2013-88, Microsoft Research, September.
Joshua Goodman. 2001. Classes for Fast Maximum En-
tropy Training. In Proc. of ICASSP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of HLT-NAACL, pages 127?133, Edmonton, Canada,
May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
of ACL Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic, Jun.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP,
pages 388?395, Barcelona, Spain, Jul.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous Space Translation Models with
1053
Neural Networks. In Proc. of HLT-NAACL, pages 39?
48, Montre?al, Canada. Association for Computational
Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,
Marianna Apidianaki, Li Gong, Aure?lien Max, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois Yvon.
2012b. LIMSI @ WMT12. In Proc. of WMT, pages
330?337, Montre?al, Canada, June. Association for
Computational Linguistics.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based Minimum
Error Rate Training for Statistical Machine Transla-
tion. In Proc. of EMNLP, pages 725?734, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Toma?s? Mikolov and Geoffrey Zweig. 2012. Con-
text Dependent Recurrent Neural Network Language
Model. In Proc. of Spoken Language Technologies
(SLT), pages 234?239, Dec.
Toma?s? Mikolov, Karafia?t Martin, Luka?s? Burget, Jan Cer-
nocky?, and Sanjeev Khudanpur. 2010. Recurrent
Neural Network based Language Model. In Proc. of
INTERSPEECH, pages 1045?1048.
Toma?s? Mikolov, Anoop Deoras, Daniel Povey, Luka?s?
Burget, and Jan C?ernocky?. 2011a. Strategies for
Training Large Scale Neural Network Language Mod-
els. In Proc. of ASRU, pages 196?201.
Toma?s? Mikolov, Stefan Kombrink, Luka?s? Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011b. Exten-
sions of Recurrent Neural Network Language Model.
In Proc. of ICASSP, pages 5528?5531.
Toma?s? Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space-
Word Representations. In Proc. of NAACL, pages
746?751, Stroudsburg, PA, USA, June. Association
for Computational Linguistics.
Toma?s? Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training Recurrent Neural
Networks. Proc. of ICML, abs/1211.5063.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In Symposium on Parallel and
Distributed Processing.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space Lan-
guage Models on a GPU for Statistical Machine Trans-
lation. In NAACL-HLT Workshop on the Future of
Language Modeling for HLT, pages 11?19. Associa-
tion for Computational Linguistics.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schlu?ter, and Hermann Ney. 2013.
Comparison of Feedforward and Recurrent Neural
Network Language Models. In IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing, pages 8430?8434, Vancouver, Canada, May.
Geoff Zweig and Konstantin Makarychev. 2013. Speed
Regularization and Optimality in Word Classing. In
Proc. of ICASSP.
1054
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250?1260,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Large-scale Expected BLEU Training of Phrase-based Reordering Models
Michael Auli, Michel Galley, Jianfeng Gao
Microsoft Research
Redmond, WA, USA
{michael.auli,mgalley,jfgao}@microsoft.com
Abstract
Recent work by Cherry (2013) has shown
that directly optimizing phrase-based re-
ordering models towards BLEU can lead
to significant gains. Their approach is lim-
ited to small training sets of a few thou-
sand sentences and a similar number of
sparse features. We show how the ex-
pected BLEU objective allows us to train
a simple linear discriminative reordering
model with millions of sparse features on
hundreds of thousands of sentences re-
sulting in significant improvements. A
comparison to likelihood training demon-
strates that expected BLEU is vastly more
effective. Our best results improve a hi-
erarchical lexicalized reordering baseline
by up to 2.0 BLEU in a single-reference
setting on a French-English WMT 2012
setup.
1 Introduction
Modeling reordering for phrase-based machine
translation has been a long standing problem.
Contrary to synchronous context free grammar-
based translation models (Wu, 1997; Galley et al.,
2004; Galley et al., 2006; Chiang, 2007), phrase-
based models (Koehn et al., 2003; Och and Ney,
2004) have no in-built notion of reordering beyond
what is captured in a single phrase pair, and the
first phrase-based decoders simply scored inter-
phrase reorderings using a restricted linear dis-
tortion feature, which scores a phrase reordering
proportionally to the length of its displacement.
While phrase-based models allow in theory com-
pletely unrestricted reordering patterns, move-
ments are generally limited to a finite distance for
complexity reasons. To address this limitation,
extensive prior work focused on richer feature
sets, in particular on lexicalized reordering mod-
els trained with maximum likelihood-based ap-
proaches (Tillmann, 2003; Xiong et al., 2006; Gal-
ley and Manning, 2008; Nguyen et al.,2009;?2).
More recently, Cherry (2013) proposed a very
effective sparse ordering model relying on a set
of only a few thousand indicator features which
are trained towards a task-specific metric such as
BLEU (Papineni et al., 2002). These features
are simply added to the log-linear framework of
translation that is trained with the Margin Infused
Relaxed Algorithm (MIRA; Chiang et al., 2009)
on a small development set of a few thousand
sentences. While simple, the approach outper-
forms the state-of-the-art hierarchical reordering
model of Galley and Manning (2008), a maximum
likelihood-based model trained on millions of sen-
tences to fit millions of parameters.
Ideally, we would like to scale sparse reorder-
ing models to similar dimensions but recent at-
tempts to increase the amount of training data for
MIRA was met with little success (Eidelman et
al., 2013). In this paper we propose much larger
sparse ordering models that combine the scalabil-
ity of likelihood-based approaches with the higher
accuracy of maximum BLEU training (?3). We
train on the output of a hierarchical reordering
model-based system and scale to millions of fea-
tures learned on hundreds of thousands of sen-
tences (?4). Specifically, we use the expected
BLEU objective function (Rosti et al., 2010; Rosti
et al., 2011; He and Deng, 2012; Gao and He,
2013; Gao et al., 2014; Green et al., 2014) which
allows us to train models that use training data and
feature sets that are two to three orders of magni-
tudes larger than in previous work (?5).
Our models significantly outperform the
state-of-the-art hierarchical lexicalized reordering
model on two language pairs and we demonstrate
that richer feature sets result in significantly
higher accuracy than with a feature set similar
to Cherry (2013). We also demonstrate that our
1250
approach greatly benefits from more training
data than is typically used for maximum BLEU
training. Previous work concluded that sparse
reordering models perform better than maximum
entropy models, however, the two approaches
do not only differ in the objective function but
also the type of training data (Cherry, 2013). Our
analysis isolates the objective function and shows
that expected BLEU optimization is the most
important factor to train accurate ordering models.
Finally, we compare expected BLEU training to
pair-wise ranked optimization (PRO) on a feature
set similar to Cherry (2013; ?7).
2 Reordering Models
Reordering models for phrase-based translation
are typically part of the log-linear framework
which forms the basis of many statistical machine
translation systems (Och and Ney, 2004).
Formally, we are given K training pairs D =
(f
(1)
, e
(1)
)...(f
(K)
, e
(K)
), where each f
(i)
? F
is drawn from a set of possible foreign sentences,
and each English sentence e
(i)
? E(f
(i)
) is drawn
from a set of possible English translations of f
(i)
.
The log-linear model is parameterized by m pa-
rameters ? where each ?
k
? ? is the weight of
an associated feature h
k
(f, e) such as a language
model or a reordering model. Function h(f, e)
maps foreign and English sentences to the vector
h
1
(f, e)...h
m
(f, e), and we usually choose trans-
lations e? according to the following decision rule:
e? = arg max
e?E(f)
?
T
h(f, e) (1)
In practice, computing e? exactly is intractable and
we resort to an approximate but more efficient
beam search (Och and Ney, 2004).
Early phrase-based models simply relied on a
linear distortion feature, which measures the dis-
tance between the first word of the current source
phrase and the last word of the previous source
phrase (Koehn et al., 2003; Och and Ney, 2004).
Unfortunately, this approach is agnostic to the ac-
tual phrases being reordered, and does not take
into account that certain phrases are more likely
to be reordered than others. This shortcoming led
to a range of lexicalized reordering models that
capture exactly those preferences for individual
phrases (Tillmann, 2003; Koehn et al., 2007).
Reordering models generally assume a se-
quence of English phrases e = {e?
1
, . . . , e?
n
} cur-
rently hypothesized by the decoder, a phrase align-
ment a = {a
1
, . . . , a
n
} that defines a foreign
phrase
?
f
a
i
for each English phrase e?
i
, and an ori-
entation o
i
which describes how a phrase pair
should be reordered with respect to the previous
phrases. There are typically three orientation types
and the exact definition depends on the specific
models which we describe below. Orientations can
be determined during decoding and from word-
aligned training corpora. Most models estimate
a probability distribution p(o
i
|pp
i
, a
1
, . . . , a
i
) for
the i-th phrase pair pp
i
= ?e?
i
,
?
f
a
i
? and the align-
ments a
1
, . . . , a
i
of the previous target phrases.
Lexicalized Reordering. This model defines the
three orientation types based only on the posi-
tion of the current and previously translated source
phrase a
i
and a
i?1
, respectively (Tillmann, 2003;
Koehn et al., 2007). The orientation types gen-
erally are: monotone (M), indicating that a
i?1
is
directly followed by a
i
. swap (S) assumes that a
i
precedes a
i?1
, i.e., the two phrases swap places.
Finally, discontinuous (D) indicates that a
i
is not
adjacent to a
i?1
. The probability distribution over
these reordering events is based on a maximum
likelihood estimate:
p(o|pp, a
i?1
, a
i
) =
cnt(o, pp)
cnt(pp)
(2)
where o ? {M,S,D} and cnt returns smoothed
frequency counts over a word-aligned corpus.
Hierarchical Reordering. An extension of the
lexicalized reordering model better handles long-
distance reordering by conditioning the orientation
of the current phrase on a context larger than just
the previous phrase (Galley and Manning, 2008).
In particular, the hierarchical reordering model
does so by building a compact representations
of the preceding context using an efficient shift-
reduce parser. During translation new phrases get
moved on a stack and are then combined with any
previous phrase if they are adjacent. Figure 1
shows an illustrative example: when the decoder
shifts phrase pp
8
onto the stack, this phrase is then
merged with pp
7
(reduce operation), which then
can be merged with previous phrases to finally
form a hierarchical block h
1
. These merge opera-
tions stop once we reach a phrase (here, pp
3
) that
is not contiguous with the current block. Then, as
another phrase (pp
9
) is hypothesized, the decoder
uses the hierarchical block at the top of the stack
(h
1
) to determine the orientation of the current
1251
    	 
       therussiansidehopestoholdconsultationswithiranonthisissueinthenearfuture..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
h1
pp9
pp8
pp7
pp3
d
e
c
o
d
e
d
 
o
u
t
p
u
t
input sentence
Figure 1: The hierarchical reordering model
(HRM) analyzes a non-local context to determine
the orientation of the current phrase. For exam-
ple, the phrase pair pp
9
has a swap orientation
(o
9
= S) with respect to a hierarchical block (h
1
)
that comprises the five preceding phrase pairs.
phrase pp
9
, which in this case is a swap (S) orien-
tation.
1
The model has the advantage that the ori-
entations computed are more robust to derivational
ambiguity of the underlying translation model. A
given surface translation may be derived through
different phrases but the shift-reduce parser com-
bines them into a single representation which is
more consistent with the orientations observed in
the word-aligned training data.
Maximum Entropy-based models. The statis-
tics used to estimate the lexicalized and the hierar-
chical reordering models are based on very sparse
estimates, simply because certain phrases are not
very frequent. Maximum entropy models address
this problem by estimating Eq. 2 through sparse
indicator features over phrase pairs instead, but
prior work with such models still relies on word
aligned corpora for estimation (Xiong et al., 2006;
Nguyen et al., 2009). However, recent evalua-
tions of the approach show little gain over the sim-
pler frequency-based estimation method (Cherry,
2013).
Sparse Hierarchical Reordering model. All of
the models so far are trained to maximize the like-
lihood of reordering decisions observed in word
aligned corpora. Cherry (2013) argues that it
is probably too difficult to learn human reorder-
ing patterns through noisy word alignments that
1
Galley and Manning (2008) provide a more formal ex-
planation.
were generated by unsupervised methods. Instead,
he proposes to learn a discriminative reordering
model based on the outputs of the actual machine
translation system, adjusting the feature weights
to maximize a task-specific objective, which is
BLEU in their case. Their model is based on a
set of sparse features derived from the hierarchi-
cal reordering model which we scale to millions
of features (?6).
3 A Simple Linear Reordering Model
Our reordering model is defined as a simple linear
model over the basic orientation types, similar to
Cherry (2013). In particular, our model defines
score s
?
(o, e, f) over orientations o = {M,S,D},
and a sentence pair {e, f, a} with alignment a as a
linear combination of weighted indicator features:
s
?
(o, e, f, a) = ?
T
u(o, e, f, a)
=
I
?
i=1
?
T
u(o, pp
i
, c
i
)
=
I
?
i=1
s
?
(o, pp
i
, c
i
) (3)
where ? is a vector of weights, {pp
i
}
I
i=1
is a
set of phrases that decompose the sentence pair
{e, f, a}, and u(o, pp
i
, c
i
) is a function that maps
orientation o, phrase pair pp
i
and local context c
i
to a sparse vector of indicator features. The lo-
cal context c
i
represents information used by the
model that is in addition to the phrase pair. For
example, the features of Cherry (2013) condition
on the top-stack of the hierarchical shift reduce
parser, information that is non-local with respect
to the phrase pair. In our experiments, we use fea-
tures that go beyond the top-stack, in order to con-
dition on various parts of the source and target side
contexts (?7).
4 Model Training
Optimization of our model is based on standard
stochastic gradient descent (SGD; Bottou, 2004)
with an expected BLEU loss l(?) which we detail
next (?5). The update is:
?
t
= ?
t?1
? ?
?l(?
t?1
)
??
t?1
(4)
where ?
t
and ?
t?1
are model weights at time t and
t? 1 respectively, and ? is a learning rate.
We add the model as a small number of dense
features to the log-linear framework of translation
1252
(Eq. 1). Specifically, we extend the m baseline
features by a set of new features h
m+1
, . . . , h
m+j
,
where each represents a linear combination of
sparse indicator features corresponding to one of
the orientation types. Exposing each orientation
as a separate dense feature within the log-linear
model is common practice for lexicalized reorder-
ing models (Koehn et al., 2005):
h
m+j
= s
?
(o
j
, e, f, a)
where o
j
? {M,S,D}.
The translation model is then parameterized by
both ?, the log-linear weights of the baseline fea-
tures, as well as ?, the weights of the reordering
model. The reordering model is learned as follows
(Gao and He, 2013; Gao et al., 2014):
1. We first train a baseline translation system to
learn ?, without the discriminative reordering
model, i.e., we set ?
m+1
= 0, . . . , ?
m+j
= 0.
2. Using these weights, we generate n-best lists
for the foreign sentences in the training data
using the setup described in the experimental
section (?7). The n-best lists serve as an ap-
proximation to E(f), the set of possible trans-
lations of f , used in the next step for expected
BLEU training of the reordering model (?5).
3. Next, we fix ?, set ?
m+1
= 1, . . . ?
m+j
= 1
and optimize ? with respect to the loss func-
tion on the training data using stochastic gra-
dient descent.
2
4. Finally, we fix ? and re-optimize ? in the
presence of the discriminative reordering
model using Minimum Error Rate Training
(MERT; Och 2003; ?7).
We found that re-optimizing ? after a few iter-
ations of stochastic gradient descent in step 3 did
not improve accuracy.
5 Expected BLEU Objective Function
The expected BLEU objective (Gao and He, 2013;
Gao et al., 2014) allows us to efficiently optimize
a large scale discriminative reordering model to-
wards the desired task-specific metric, which in
our setting is BLEU.
2
We tuned ?
m+1
, . . . ?
m+j
on the development set but
found that setting them uniformly to one resulted in faster
training and equal accuracy.
Formally, we define our loss function l(?) as
the negative expected BLEU score, denoted as
xBLEU(?), for a given foreign sentence f and a
log-linear parameter set ?:
l(?) =? xBLEU(?)
=?
?
e?E(f)
p
?,?
(e|f) sBLEU(e, e
(i)
) (5)
where sBLEU(e, e
(i)
) is a smoothed sentence-
level BLEU score with respect to the reference
translation e
(i)
, and E(f) is the generation set ap-
proximated by an n-best list. In our experiments
we use n-best lists with unique entries and there-
fore our definitions do not take into account mul-
tiple derivations of the same translation. Specif-
ically, our n-best lists are generated by choosing
the highest scoring derivation e? amongst string
identical translations e for f . We use a sentence-
level BLEU approximation similar to Gao et al.
(2014).
3
Finally, p
?,?
(e|f) is the normalized prob-
ability of translation e given f , defined as:
p
?,?
(e|f) =
exp{??
T
h(f, e)}
?
e
?
?E(f)
exp{??
T
h(f, e
?
)}
(6)
where ?
T
h(f, e) includes the discriminative re-
ordering model h
m+1
(e, f), . . . , h
m+j
(e, f) pa-
rameterized by ?, and ? ? [0, inf) is a tuned scal-
ing factor that flattens the distribution for ? < 1
and sharpens it for ? > 1 (Tromble et al., 2008).
4
Next, we define the gradient of the expected
BLEU loss function l(?). To simplify our notation
we omit the local context c in s
?
(o, pp, c) (Eq. 3)
from now on and assume it to be part of pp. Us-
ing the observation that the loss does not explicitly
depend on ?, we get:
?l(?)
??
=
?
o,pp
?l(?)
?s
?
(o, pp)
?s
?
(o, pp)
??
=
?
o,pp
??
o,pp
u(o, pp)
where ?
o,pp
is the error term for orientation o of
phrase pair pp:
?
o,pp
= ?
?l(?)
?s
?
(o, pp)
3
We found in early experiments that the BLEU+1 approx-
imation used by Liang et al. (2006) and Nakov et. al (2012)
worked equally well in our setting.
4
? is only used during expected BLEU training.
1253
The error term indicates how the expected BLEU
loss changes with the reordering score which we
derive in the next section.
Finally, the gradient of the reordering score
s
?
(o, pp) with respect to ? is simply given by this:
?s
?
(o, pp)
??
=
??
T
u(o, pp)
??
= u(o, pp)
5.1 Derivation of the Error Term ?
o,pp
We rewrite the loss function (Eq. 5) using Eq. 6
and separate it into two terms G(?) and Z(?):
l(?) = ?xBLEU(?) = ?
G(?)
Z(?)
(7)
= ?
?
e?E(f)
exp{??
T
h(f, e)} sBLEU(e, e
(i)
)
?
e
?
?E(f)
exp{??
T
h(f, e
?
)}
Next, we apply the quotient rule of differentiation:
?
o,pp
=
?xBLEU(?)
?s
?
(o, pp)
=
?(G(?)/Z(?))
?s
?
(o, pp)
=
1
Z(?)
(
?G(?)
?s
?
(o, pp)
?
?Z(?)
?s
?
(o, pp)
xBLEU(?)
)
The gradients for G(?) and Z(?) with respect to
s
?
(o, pp) are:
?G(?)
?s
?
(o, pp)
=
?
e?E(f)
sBLEU(e, e
(i)
)
? exp{??
T
h(f, e)}
?s
?
(o, pp)
?Z(?)
?s
?
(o, pp)
=
?
e?E(f)
? exp{??
T
h(f, e)}
?s
?
(o, pp)
By using the following definition:
U(?, e) = sBLEU(e, e
(i)
)? xBLEU(?)
together with the chain rule, Eq. 6 and Eq. 7, we
can rewrite ?
o,pp
as follows:
?
o,pp
=
1
Z(?)
?
e?E(f)
(
? exp{??
T
h(f, e)}
?s
?
(o, pp)
U(?, e)
)
=
?
e?E(f)
(
p
?,?
(e|f)
???
T
h(f, e)
?s
?
(o, pp)
U(?, e)
)
Because ? is only relevant to the reordering
model, represented by h
m+1
, . . . , h
m+j
, we have:
???
T
h(f, e)
?s
?
(o, pp)
= ??
k
?h
k
(e, f)
?s
?
(o, pp)
= ??
k
N (o, pp, e, f)
1: function TRAINSGD(D, ?)
2: t? 0
3: for all (f
(i)
, e
(i)
) in D do
4: xBLEU = 0 . Compute xBLEU
5: for all e in E(f
(i)
) do
6: wBLEU? p
?,?
t
(e|f) sBLEU(e, e
(i)
)
7: xBLEU? xBLEU + wBLEU
8: end for
9: for all e in E(f
(i)
) do
10: D = sBLEU(e, e
(i)
)? xBLEU
11: for all o, pp in ?e, f
(i)
? do
12: N = N (o, pp, e, f)
13: ?
o,pp
= p
?,?
t
(e|f
(i)
)??
k
ND
14: ?
t+1
= ?
t
? ??
o,pp
u(o, pp))
15: end for
16: end for
17: t? t+ 1
18: end for
19: end function
Figure 2: Algorithm for computing the expected
BLEU loss with SGD updates (Eq. 4) based on
training data D and learning rate ?.
where m + 1 ? k ? m + j and N (o, pp, e, f) is
the number of times pp with orientation o occurs
in the current sentence pair.
This simplifies the error term to:
?
o,pp
=
?
e?E(f)
p
?,?
(e|f)??
k
N (o, pp, e, f)U(?, e)
(8)
where ?
k
is the weight of the dense feature sum-
marizing orientation o in the log-linear model. We
use Eq. 8 in a simple algorithm to train our model
(Figure 2). Our SGD trainer uses a mini-batch size
of a single sentence (?7) which entails all hypoth-
esis in the n-best list for this sentence and the pa-
rameters are updated after each mini-batch.
6 Feature Sets
Our features are inspired by Cherry (2013)
who bases his features on the local phrase-pair
pp = ?e?,
?
f? as well as the top stack of the shift re-
duce parser of the baseline hierarchical ordering
model. We experiment with these variants and ex-
tensions:
? SparseHRMLocal: This feature set is exclu-
sively based on the local phrase-pair and
1254
consists of features over the first and last
word of both the source and target phrase.
5
We use four different word representations:
The word identity itself, but only for the
80 most common source and target language
words. The three other word representations
are based on Brown clustering with either 20,
50 or 80 classes (Brown et al., 1992). There
is one feature for every orientation type.
? SparseHRM: The main feature set of Cherry
(2013). This is an extension of SparseHRM-
Local adding features based on the first and
last word of both the source and the target of
the hierarchical block at the top of the stack.
There are also features based on the source
words in-between the current phrase and the
hierarchical block at the top of the stack.
? SparseHRM+UncommonWords: This set is
identical to SparseHRM, except that word-
identity features are not restricted to the 80
most frequent words, but can be instantiated
for all words, regardless of frequency.
? SparseHRM+BiPhrases: This augments
SparseHRM by phrase-identity features re-
sulting in millions of instances compared to
only a few thousand for SparseHRM. We add
three features for each possible phrase pair:
the source phrase, the target phrase, and the
whole phrase pair.
The baseline hierarchical lexicalized reorder-
ing model is most similar to SparseHRM+BiPhrases
feature set since both have parameters for phrase,
orientation pairs.
6
The feature set closest to
Cherry (2013) is SparseHRM. However, while
Cherry had to severely restrict his features for
batch lattice MIRA-based training, our maximum
expected BLEU approach can handle millions of
features.
7 Experiments
Baseline. We experiment with a phrase-based
system similar to Moses (Koehn et al., 2007),
5
Phrase-local features allow pre-computation which re-
sults in significant speed-ups at run-time. Cherry (2013)
shows that local features are responsible for most of his gains.
6
Although, our model is likely to learn significantly fewer
parameters since many phrase, orientation pairs will only be
seen in the word-aligned data but not in actual machine trans-
lation output.
scoring translations by a set of common fea-
tures including maximum likelihood estimates
of source given target phrases p
MLE
(e|f) and
vice versa, p
MLE
(f |e), lexically weighted esti-
mates p
LW
(e|f) and p
LW
(f |e), word and phrase-
penalties, as well as a linear distortion feature.
The baseline uses a hierarchical reordering model
with five orientation types, including monotone
and swap, described in ?2, as well as two discon-
tinuous orientations, distinguishing if the previous
phrase is to the left or right of the current phrase.
Finally, monotone global indicates that all previ-
ous phrases can be combined into a single hier-
archical block. The baseline includes a modified
Kneser-Ney word-based language model trained
on the target-side of the parallel data, which is de-
scribed below. Log-linear weights are estimated
with MERT (Och, 2003). We regard the 1-best
output of the phrase-based decoder with the hierar-
chical reordering model as the baseline accuracy.
Evaluation. We use training and test data from
the WMT 2012 campaign and report results on
French-English and German-English translation
(Callison-Burch et al., 2012). Translation mod-
els are estimated on 102M words of parallel data
for French-English and 91M words for German-
English; between 7.5-8.2M words are newswire,
depending on the language pair, and the remainder
are parliamentary proceedings. All discrimina-
tive reordering models are trained on the newswire
subset since we found this portion of the data to be
most useful in initial experiments. We evaluate on
six newswire domain test sets from 2008, 2010 to
2013 as well as the 2010 system combination test
set containing between 2034 to 3003 sentences.
Log-linear weights are estimated on the 2009 data
set comprising 2525 sentences. We evaluate using
BLEU with a single reference.
Discriminative Reordering Model. We use 100-
best lists generated by the phrase-based decoder
to train the discriminative reordering model. The
n-best lists are generated by ten systems, each
trained on 90% of the available data in order to de-
code the remaining 10%. The purpose of this pro-
cedure is to avoid a bias introduced by generating
n-best lists for sentences on which the translation
model was previously trained.
7
Unless otherwise
7
Later, we found that the bias has only a negligible effect
on end-to-end accuracy since we obtained very similar results
when decoding with a system trained on all data. This setting
increased the training data BLEU score from 27.5 to 37.8. We
used a maximum source and target phrase length of 7 words.
1255
dev 2008 2010 sc2010 2011 2012 2013 AllTest FeatTypes
noRM 23.37 20.18 24.24 24.18 24.83 24.23 24.85 23.93 -
HRM (baseline) 24.11 20.85 24.92 24.83 25.68 25.11 25.76 24.72 -
SparseHRMLocal 25.24 21.26 25.99 25.93 26.98 26.34 26.77 25.77 4,407
SparseHRM 25.29 21.43 26.17 26.14 26.99 26.63 27.01 25.95 9,463
+UncommonWords 25.32 21.76 26.30 26.29 27.15 26.77 27.18 26.12 897,537
+BiPhrases 25.46 21.67 26.19 26.19 27.55 27.07 27.41 26.26 3,043,053
Table 1: French-English results of expected BLEU trained sparse reordering models compared to no
reordering model at all (noRM) and the likelihood trained baseline hierarchical reordering model (HRM)
on WMT test sets; sc2010 is the 2010 system combination test set. FeatTypes is the number of different
types and AllTest is the average BLEU score over all the test sets, weighted by corpus size. All results
for our sparse reordering models include a likelihood-trained hierarchical reordering model.
dev 2008 2010 sc2010 2011 2012 2013 AllTest FeatTypes
noRM 18.54 19.28 20.14 20.01 18.90 18.87 21.60 19.81 -
HRM (baseline) 19.35 19.96 20.87 20.66 19.60 19.80 22.48 20.58 -
SparseHRMLocal 19.89 19.86 21.11 20.84 20.04 20.21 22.93 20.88 4,410
SparseHRM 19.83 20.27 21.26 21.05 20.22 20.44 23.17 21.11 9,477
+UncommonWords 20.06 20.35 21.45 21.31 20.28 20.55 23.30 21.24 1,136,248
+BiPhrases 20.09 20.33 21.62 21.47 20.66 20.75 23.27 21.40 3,640,693
Table 2: German-English results of expected BLEU trained sparse reordering models (cf. Table 1).
mentioned, we train our reordering model on the
news portion of the parallel data, corresponding to
136K-150K sentences, depending on the language
pair. We tuned the various hyper-parameters on a
held-out set, including the learning rate, for which
we found a simple setting of 0.1 to be useful. To
prevent overfitting, we experimented with `
2
regu-
larization, but found that it did not improve test ac-
curacy. We also tuned the probability scaling pa-
rameter ? (Eq. 6) but found ? = 1 to be very good
among other settings. We evaluate the perfor-
mance on a held-out validation set during training
and stop whenever the objective changes less than
a factor of 0.0003. For our PRO experiments, we
tuned three hyper-parameters controlling `
2
reg-
ularization, sentence-level BLEU smoothing, and
length. The latter is important to eliminate PRO?s
tendency to produce too short translations (Nakov
et al., 2012).
7.1 Scaling the Feature Set
We first compare our baseline, a likelihood trained
hierarchical reordering model (HRM; Galley &
Manning, 2008), to various expected BLEU
trained models, starting with SparseHRMLocal,
inspired by Cherry (2013) and compare it to
SparseHRM+BiPhrases, a set that is three orders of
magnitudes larger.
Our results on French-English translation (Ta-
ble 1) and German-English translation (Table 2)
show that the expected BLEU trained models scale
to millions of features and that we outperform the
baseline by up to 2.0 BLEU on newstest2012 for
French-English and by up to 1.1 BLEU on new-
stest2011 for German-English.
8
Increasing the
size of the feature set improves accuracy across
the board: The average accuracy over all test sets
improves from 1.0 BLEU for the most basic fea-
ture set to 1.5 BLEU for the largest feature set
on French-English and from 0.3 BLEU to 0.8
BLEU on German-English.
9
The most compa-
rable setting to Cherry (2013) is the feature set
SparseHRM, which we outperform by up to 0.5
BLEU on French-English and by 0.3 BLEU on av-
erage on both language pairs, demonstrating the
benefit of being able to effectively train large fea-
ture sets. Furthermore, the increase in the num-
ber of features does not affect runtime, since most
8
Different to the setups of Galley & Manning (2008) and
Cherry (2013) our WMT evaluation framework uses only one
instead of four references, which makes our BLEU score im-
provements not directly comparable.
9
We attribute smaller improvements on German-English
to the low distortion limit of only six words of our system and
the more difficult reordering patterns when translating from
German which may require more elaborate features.
1256
features can be pre-computed and stored in the
phrase-table, only requiring a constant time table-
lookup, similar to traditional reordering models.
Another appeal of our approach is that train-
ing is very fast given a set of n-best lists for the
training data. The SparseHRM model with 4,407
features is trained in only 26 minutes, while the
SparseHRM+BiPhrases model with over three mil-
lion parameters can be trained in just over two
hours (136K sentences and 100 epochs in both
cases). We attribute this to the training regime
(?4), which does not iteratively re-decode the
training data for expected BLEU training.
10
7.2 Varying Training Set Size
Previous work on sparse reordering models was
restricted to small data sets (Cherry, 2013) due
to the limited ability of standard machine trans-
lation optimizers to handle more than a few thou-
sand sentences. In particular, recent attempts to
scale the margin-infused relaxation algorithm, a
variation which was also used by Cherry (2013),
to larger data sets showed that more data does not
necessarily help to improve test set accuracy for
large feature sets (Eidelman et al., 2013).
In the next set of experiments, we shed light on
the advantage of training discriminative reordering
models with expected BLEU on large training sets.
Specifically, we start off by estimating a reorder-
ing model on only 2,000 sentences, similar to the
size of the development set used by Cherry (2013),
and incrementally increase the amount of training
data to nearly three hundred thousand sentences.
To avoid overfitting to small data sets we experi-
ment with our most basic feature set SparseHRM-
Local, comprising of just over 4,400 types.
For this experiment only, we measure accuracy
in a re-ranking framework for faster experimen-
tation where we use the 100-best output of the
baseline system relying on a likelihood-based hi-
erarchical reordering model. We re-estimate the
log-linear weights by running a further iteration of
MERT on the n-best list of the development set
which is augmented by scores corresponding to
the discriminative reordering model. The weights
of those features are initially set to one and we
use 20 random restarts for MERT. At test time we
rescore the 100-best list of the test set using the
new set of log-linear weights learned previously.
10
We would expect better accuracy when iteratively decod-
ing the training data but did not do so in this study for effi-
ciency reasons.
24.4
24.6
24.8
25.0
25.2
2K 4K 8K 16K 32K 64K 136K 272K
BLE
U
Training set size
dev
26.6 Training set size
25.6
25.8
26.0
26.2
26.4
26.6
2K 4K 8K 16K 32K 64K 136K 272K
BLE
U
Training set size
news2011
Figure 3: Effect of increasing the training set size
from 2,000 to 272,000 sentences measured on the
dev set (top) and news2011 (bottom) in an n-best
list rescoring setting.
Figure 3 confirms that more training data in-
creases accuracy and that the best model requires
a substantially larger amount of training data than
what is typically used for maximum BLEU train-
ing. We expect an even steeper curve for larger
feature sets where more parameters need to be es-
timated and where the amount of training data is
likely to have an even larger effect.
7.3 Likelihood versus BLEU Optimization
Previous research has shown that directly training
a reordering model for BLEU can vastly outper-
form a likelihood trained maximum entropy re-
ordering model (Cherry, 2013). However, the two
approaches do not only differ in the objectives
used, but also in the type of training data. The
maximum entropy reordering model is trained on
a word-aligned corpus, trying to learn human re-
ordering patterns, whereas the sparse reordering
model is trained on machine translation output,
trying to learn from the mistakes made by the ac-
tual system. It is therefore not clear how much
either one contributes to good accuracy.
Our next experiment teases those two aspects
apart and clearly shows the effect of the objec-
tive function. Specifically, we compare the tra-
ditionally used conditional log-likelihood (CLL)
objective to expected BLEU on the French-
English translation task in a small feature con-
dition (SparseHRM) of about 9K features and
1257
dev 2008 2010 sc2010 2011 2012 2013 AllTest
noRM 23.37 20.18 24.24 24.18 24.83 24.23 24.85 23.93
HRM (baseline) 24.11 20.85 24.92 24.83 25.68 25.11 25.76 24.72
SparseHRM (CLL) 24.28 21.02 25.11 25.10 25.92 25.24 25.76 24.88
SparseHRM (xBLEU) 25.29 21.43 26.17 26.14 26.99 26.63 27.01 25.95
SparseHRM+BiPhrases (CLL) 24.42 21.17 25.12 25.00 25.86 25.36 26.18 24.98
SparseHRM+BiPhrases (xBLEU) 25.46 21.67 26.19 26.19 27.55 27.07 27.41 26.26
Table 3: French-English results comparing the baseline hierarchical reordering model (HRM) to sparse
reordering model trained towards conditional log-likelihood (CLL) and expected BLEU (xBLEU).
dev 2008 2010 sc2010 2011 2012 2013 AllTest
PRO 24.05 20.90 25.42 25.28 25.79 25.09 26.07 24.94
xBLEU 25.24 21.26 25.99 25.93 26.98 26.34 26.77 25.77
Table 4: French-English results on the SparseHRMLocal feature set when when trained with pair-wise
ranked optimization (PRO) and expected BLEU (xBLEU).
a large feature setting of over 3M features
(SparseHRM+BiPhrases). In the CLL setting, we
maximize the likelihood of the hypothesis with the
highest BLEU score in the n-best list of each train-
ing sentence.
Our results (Table 3) show that CLL training
achieves only a fraction of the gains yielded by
the expected BLEU objective. For SparseHRM,
CLL improves the baseline by less than 0.2 BLEU
on average across all test sets, whereas expected
BLEU achieves 1.2 BLEU. Increasing the number
of features to 3M (SparseHRM+BiPhrases) results
in a slightly better average gain of 0.3 BLEU for
CLL but but expected BLEU still achieves a much
higher improvement of 1.5 BLEU. Because our
gains with likelihood training are similar to what
Cherry (2013) reported for his maximum entropy
model, we conclude that the objective function is
the most important factor to achieving good accu-
racy.
7.4 Comparison to PRO
In our final experiment we compare expected
BLEU training to pair-wise ranked optimization
(PRO), a popular off the shelf trainer for ma-
chine translation models with large feature sets
(Hopkins and May, 2011).
11
Previous work has
shown that PRO does not scale to truly large fea-
ture sets with millions of types (Yu et al., 2013)
and we therefore restrict ourselves to our smallest
11
MIRA is another popular optimizer but as previously
mentioned, even the best publicly available implementation
does not scale to large training sets (Eidelman et al., 2013).
set (SparseHRMLocal) of just over 4.4K features.
We train PRO on the development set compris-
ing of 2,525 sentences, a setup that is commonly
used by standard machine translation optimizers.
In this setting, PRO directly learns weights for the
baseline features (?7) as well as the 4.4K indica-
tor features corresponding to the sparse reordering
model. For expected BLEU training we use the
full 136K sentences from the training data. The
results (Table 4) demonstrate that expected BLEU
outperforms a typical setup commonly used to
train large feature sets.
8 Conclusion and Future Work
The expected BLEU objective is a simple and ef-
fective approach to train large-scale discriminative
reordering models. We have demonstrated that
it scales to millions of features, which is orders
of magnitudes larger than other modern machine
translation optimizers can currently handle.
Empirically, our sparse reordering model im-
proves machine translation accuracy across the
board, outperforming a strong hierarchical lexi-
calized reordering model by up to 2.0 BLEU on
a French to English WMT2012 setup, where the
baseline was trained on over two million sentence
pairs. We have shown that scaling to large train-
ing sets is crucial to good performance and that
the best performance is reached when hundreds
of thousands of training sentences are used. Fur-
thermore, we demonstrate that task-specific train-
ing towards expected BLEU is much more effec-
tive than optimizing conditional log-likelihood as
1258
is usually done. We attribute this to the fact that
likelihood is a strict zero-one loss that does not as-
sign credit to partially correct solutions, whereas
expected BLEU does.
In future work we plan to extend expected
BLEU training to lattices and to evaluate the ef-
fect of estimating weights for the dense baseline
features as well. Our current training procedure
(Gao and He, 2013; Gao et al., 2014) decodes
the training data only once. In future work, we
would like to compare this to repeated decoding
as done by conventional optimization methods as
well as other large-scale discriminative training
approaches (Yu et al., 2013). We expect this to
yield additional accuracy gains.
Acknowledgements
We would like to thank Arul Menezes and Xi-
aodong He for helpful discussion related to this
work and the three anonymous reviewers for their
comments.
References
L?eon Bottou. 2004. Stochastic learning. In
Olivier Bousquet and Ulrike von Luxburg, edi-
tors, Advanced Lectures in Machine Learning, Lec-
ture Notes in Artificial Intelligence, pages 146?168.
Springer Verlag, Berlin.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479,
Dec.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proc. of WMT, pages 10?51.
Association for Computational Linguistics, June.
Colin Cherry. 2013. Improved Reordering for Phrase-
Based Translation using Sparse Features. In Proc. of
NAACL, pages 9?14. Association for Computational
Linguistics, June.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 New Features for Statistical Machine Trans-
lation. In Proc. of NAACL, pages 218?226. Associ-
ation for Computational Linguistics, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Vladimir Eidelman, Ke Wu, Ferhan Ture1, Philip
Resnik, and Jimmy Lin. 2013. Mr. MIRA:
Open-Source Large-Margin Structured Learning on
MapReduce. In Proc. of ACL, pages 199?204. As-
sociation for Computational Linguistics, August.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of EMNLP, pages 848?856.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of HLT-NAACL, pages 273?280, Boston,
MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of ACL, pages 961?968, Sydney, Australia,
June.
Jianfeng Gao and Xiaodong He. 2013. Training MRF-
Based Phrase Translation Models using Gradient
Ascent. In Proc. of NAACL-HLT, pages 450?459.
Association for Computational Linguistics, June.
Jianfeng Gao, Xiaodong He, Scott Wen tau Yih, and
Li Deng. 2014. Learning Continuous Phrase Rep-
resentations for Translation Modeling. In Proc.
of ACL. Association for Computational Linguistics,
June.
Spence Green, Daniel Cer, and Christopher Manning.
2014. An Empirical Comparison of Features and
Tuning for Phrase-based Machine Translation. In
Proc. of WMT. Association for Computational Lin-
guistics, June.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proc. of ACL, pages 8?14. Association
for Computational Linguistics, July.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proc. of EMNLP. Association for Com-
putational Linguistics, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proc. of HLT-NAACL, pages 127?133, Edmonton,
Canada, May.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proc. of IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, Jun.
Percy Liang, Alexandre Bouchard-C?ot?e, Ben Taskar,
and Dan Klein. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of ACL-
COLING, pages 761?768, Jul.
1259
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for Sentence-Level BLEU+1
Yields Short Translations. In Proc. of COLING. As-
sociation for Computational Linguistics.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,
and Thai Phuong Nguyen. 2009. Improving A Lex-
icalized Hierarchical Reordering Model Using Max-
imum Entropy. In MT Summit XII. Association for
Computational Linguistics, August.
Franz Josef Och and Hermann Ney. 2004. The
alignment template approach to machine translation.
Computational Linguistics, 30(4):417?449, June.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318, Philadelphia, PA, USA, Jul.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN System De-
scription for WMT10 System Combination Task.
In Proc. of WMT, pages 321?326. Association for
Computational Linguistics, July.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proc. of
WMT, pages 159?165. Association for Computa-
tional Linguistics, July.
Christoph Tillmann. 2003. A Unigram Orientation
Model for Statistical Machine Translation. In Proc.
of NAACL, pages 106?108. Association for Compu-
tational Linguistics, June.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk Decoding for Statistical Machine Trans-
lation. In Proc. of EMNLP, pages 620?629. Associ-
ation for Computational Linguistics, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proc. of ACL-
COLING, pages 521?528, Sydney, Jul.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-Violation Perceptron and Forced Decod-
ing for Scalable MT Training. In Proc. of EMNLP,
pages 1112?1123. Association for Computational
Linguistics, October.
1260
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20?29,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Minimum Translation Modeling with Recurrent Neural Networks
Yuening Hu
Department of Computer Science
University of Maryland, College Park
ynhu@cs.umd.edu
Michael Auli, Qin Gao, Jianfeng Gao
Microsoft Research
Redmond, WA, USA
{michael.auli,qigao,jfgao}@microsoft.com
Abstract
We introduce recurrent neural network-
based Minimum Translation Unit (MTU)
models which make predictions based on
an unbounded history of previous bilin-
gual contexts. Traditional back-off n-gram
models suffer under the sparse nature of
MTUs which makes estimation of high-
order sequence models challenging. We
tackle the sparsity problem by modeling
MTUs both as bags-of-words and as a
sequence of individual source and target
words. Our best results improve the out-
put of a phrase-based statistical machine
translation system trained on WMT 2012
French-English data by up to 1.5 BLEU,
and we outperform the traditional n-gram
based MTU approach by up to 0.8 BLEU.
1 Introduction
Classical phrase-based translation models rely
heavily on the language model and the re-
ordering model to capture dependencies between
phrases. Sequence models over Minimum Trans-
lation Units (MTUs) have been shown to com-
plement both syntax-based (Quirk and Menezes,
2006) as well as phrase-based (Zhang et al., 2013)
models by explicitly modeling relationships be-
tween phrases. MTU models have been tradi-
tionally estimated using standard back-off n-gram
techniques (Quirk and Menezes, 2006; Crego and
Yvon, 2010; Zhang et al., 2013), similar to word-
based language models (?2).
However, the estimation of higher-order n-gram
models becomes increasingly difficult due to data
sparsity issues associated with large n-grams, even
when training on over one hundred billion words
(Heafield et al., 2013); bilingual units are much
sparser than words and are therefore even harder
to estimate. Another drawback of n-gram mod-
els is that future predictions are based on a limited
amount of previous context that is often not suf-
ficient to capture important aspects of human lan-
guage (Rastrow et al., 2012).
Recently, several feed-forward neural network-
based models have achieved impressive improve-
ments over traditional back-off n-gram models in
language modeling (Bengio et al., 2003; Schwenk
et al., 2007; Schwenk et al., 2012; Vaswani et al.,
2013), as well as translation modeling (Allauzen et
al., 2011; Le et al., 2012; Gao et al., 2013). These
models tackle the data sparsity problem by rep-
resenting words in continuous space rather than
as discrete units. Similar words are grouped in
the same sub-space rather than being treated as
separate entities. Neural network models can be
seen as functions over continuous representations
exploiting the similarity between words, thereby
making the estimation of probabilities over higher-
order n-grams easier.
However, feed-forward networks do not directly
address the limited context issue either, since pre-
dictions are based on a fixed-size context, similar
to back-off n-gram models. We therefore focus
in this paper on recurrent neural network architec-
tures, which address the limited context issue by
basing predictions on an unbounded history of pre-
vious events which allows to capture long-span de-
pendencies. Recurrent architectures have recently
advanced the state of the art in language model-
ing (Mikolov et al., 2010; Mikolov et al., 2011a;
Mikolov, 2012) outperforming multi-layer feed-
forward based networks in perplexity and word er-
ror rate for speech recognition (Arisoy et al., 2012;
Sundermeyer et al., 2013). Recent work has also
shown successful applications to machine transla-
tion (Mikolov, 2012; Auli et al., 2013; Kalchbren-
ner and Blunsom, 2013). We extend this work by
modeling Minimum Translation Units with recur-
rent neural networks.
Specifically, we introduce two recurrent neu-
ral network-based MTU models to address the is-
20
M1 M2 M3 M4 M5
Yu        ZuoTian JuXing Le HuiTan
held
=> null
=> Yesterday
=> held
=> the
=> meeting
? ?? ?? ? ??
Yu
ZuoTian
JuXing_Le
null
HuiTan
null        
the meeting null yesterday
M1: 
M2: 
M3: 
M4: 
M5: 
Figure 1: Example Minimum Translation Unit
partitioning based on Zhang et al. (2013).
sues regarding data sparsity and limited context
sizes by leveraging continuous representations and
the unbounded history of the recurrent architec-
ture. Our first approach frames the problem as a
sequence modeling task over minimal units (?3).
The second model improves over the first by mod-
eling an MTU as a bag-of-words, thereby allow-
ing us to learn representations over sub-structures
of minimal units that are shared across MTUs
(?4). Our models significantly outperform the tra-
ditional back-off n-gram based approach and we
show that they act complementary to a very strong
recurrent neural network-based language model
based solely on target words (?5).
2 Minimum Translation Units
Banchs et al. (2005) introduced the idea of framing
translation as a sequence modeling problem where
a sentence pair is generated in left-to-right order as
a sequence of bilingual n-grams. Minimum Trans-
lation Units (Quirk and Menezes, 2006; Zhang
et al., 2013) are an extension which additionally
permit tuples with empty source or target sides,
thereby allowing insertion or deletion phrase pairs.
The two basic requirements for MTUs are that
there are no overlapping word alignment links be-
tween phrase pairs and it should not be possible to
extract smaller phrase pairs without violating the
word alignment constraints. Informally, we can
think of MTUs as small phrase pairs that cannot
be broken down any further without violating the
two requirements.
Minimum Translation Units partition a sentence
pair into a set of minimal bilingual units or tu-
Words MTUs
Tokens 34,769,416 14,853,062
Types 143,524 1,315,512
Singleton types 34.9% 80.1%
Table 1: Token and type counts for both source
and target words as well as MTUs based on the
WMT 2006 German to English data set (cf. ?5).
ples obtained by an algorithm similar to phrase-
extraction (Koehn et al., 2003). Figure 1 illus-
trates such a partitioning. Modeling minimal units
has two advantages over considering larger phrase
pairs that are effectively composed of MTUs:
First, minimal units result in a unique partition-
ing of a sentence pair. This has the advantage that
we avoid modeling spurious derivations, that is,
multiple derivations generating the same sentence
pair. Second, minimal units result in smaller mod-
els with a smoother distribution than models based
on composed units (Zhang et al., 2013).
Sentence pairs can be generated in multiple or-
ders, such as left-to-right or right-to-left, either in
source or target order. For example, the source
left-to-right order of the sentence pair in Figure 1
is simply M1, M2, M3, M4, M5, while the tar-
get left-to-right order is M3, M4, M5, M1, M2.
We deal with inserted or deleted words similar to
Zhang et al. (2013): The source side null token of
an inserted target phrase is placed next to the last
source word aligned to the closest preceding non-
null aligned target phrase; a similar rule is applied
to null tokens on the target side. For example, in
Figure 1 we place M4 straight after M3 because
?the?, the aligned target phrase, is after ?held?, the
previous non-null aligned target phrase.
We can straightforwardly estimate an n-gram
model over MTUs to estimate the probability
of a sentence pair using standard back-off tech-
niques commonly employed in language mod-
eling. For example, a trigram model in tar-
get left-to-right order factors the sentence pair in
Figure 1 as p(M3) p(M4|M3) p(M5|M3,M4)
p(M1|M4,M5)p(M2|M5,M1).
If we would like to model larger contexts, then
we quickly run into data sparsity issues. To illus-
trate this point, consider the parameter growth of
an n-gram model which is driven by the vocabu-
lary size |V | and the n-gram order n: O(|V |
n
).
Clearly, the exact estimation of higher-order n-
21
gram probabilities becomes more difficult with
large n, leading to the estimation of events with
increasingly sparse statistics, or having to rely
on statistics from lower-order events with back-
off models, which is less desirable. Even word-
based language models rarely ventured so far
much beyond 5-gram statistics as demonstrated
by Heafield et al. (2013) who trained a, by to-
day?s standards, very large 5-gram model on 130B
words. Data sparsity is therefore an even more sig-
nificant issue for MTU models relying on much
larger vocabularies. In our setting, the MTU vo-
cabulary is an order of magnitude larger than a
word vocabulary obtained from the same data (Ta-
ble 1). Furthermore, most MTUs are observed
only once making the reliable estimation of prob-
abilities very challenging.
Neural network-based sequence models tackle
the data sparsity problem by learning continuous
word representations, that group similar words to-
gether in continuous space. For example, the
distributional representations induced by recurrent
neural networks have been found to have interest-
ing syntactic and semantic regularities (Mikolov
et al., 2013). Furthermore, these representations
can be exploited to estimate more reliable statis-
tics over higher-order n-grams than with discrete
word units. Recurrent neural networks go beyond
fixed-size contexts and allow the model to keep
track of long-span dependencies that are important
for future predictions. In the next sections we will
present Minimum Translation Unit models based
on recurrent architectures.
3 Atomic MTU RNN Model
The first model we introduce is based on the recur-
rent neural network language model of Mikolov
et al. (2010). We frame the problem as a tradi-
tional sequence modeling task which treats MTUs
as atomic units, similar to the approach taken by
the traditional back-off n-gram models.
The model is factored into an input layer, a hid-
den layer with recurrent connections, and an out-
put layer (Figure 2). The input layer encodes the
MTU at time t as a 1-of-N vector m
t
with all val-
ues being zero except for the entry representing
the MTU. The output layer y
t
represents a proba-
bility distribution over possible next MTUs; both
the input and output layers are of size |V |, the size
of the MTU vocabulary. The hidden layer state h
t
encodes the history of all MTUs observed in the
mt
ht-1
ht
yt
V
W
U
0
0
1
0
0
0
0
0
Figure 2: Structure of the atomic recurrent neu-
ral network MTU model following the word-based
RNN model of Mikolov (2012).
sequence up to time step t.
The state of the hidden layer is determined by
the input layer and the hidden layer configuration
of the previous time step h
t?1
. The weights of the
connections between the layers are summarized in
a number of matrices: U represents weights from
the input layer to the hidden layer, and W repre-
sents connections from the previous hidden layer
to the current hidden layer. Matrix V contains
weights between the current hidden layer and the
output layer.
The hidden and output layers are computed
via a series of matrix-vector products and non-
linearities:
h
t
= s(Um
t
+Wh
t?1
)
y
t
= g(Vh
t
)
where
s(z) =
1
1 + exp {?z}
, g(z
m
) =
exp {z
m
}
?
k
exp {z
k
}
are sigmoid and softmax functions, respectively.
Additionally, the network is interpolated with a
maximum entropy model of sparse n-gram fea-
tures over input MTUs (Mikolov et al., 2011a).
The maximum entropy weights D are added to
the output activations before applying the softmax
function and are estimated jointly with all other
parameters (Figure 3).
1
1
While these features depend on multiple input MTUs, we
22
mt
ht-1
ht
yt
V
W
U
0
0
1
0
0
0
0
0
T
D
c t
Figure 3: Structure of atomic recurrent neural net-
work MTU model with classing layer c
t
and direct
connections D between the input and output lay-
ers (cf. Figure 2).
The model is optimized via a maximum likeli-
hood objective function using stochastic gradient
descent. Training is based on the truncated back
propagation through time algorithm, which unrolls
the network and then computes error gradients
over multiple time steps (Rumelhart et al., 1986);
we use a cross entropy criterion to obtain the error
vector with respect to the output activations and
the desired prediction. After training, the output
layer represents posteriors p(m
t+1
|m
t
t?n+1
,h
t
),
the probability of the next MTU given the previ-
ous n input MTUs m
t
t?n+1
= m
t
, . . . ,m
t?n+1
and the current hidden layer configuration h
t
.
Na??ve computation of the probability distribu-
tion over the next MTU is very expensive for large
vocabularies, such as commonly encountered for
MTU models (Table 1). A well established ef-
ficiency trick assigns each possible output to a
unique class and then uses a two-step process to
find the probability of an MTU, instead of comput-
ing the probability of all possible outputs (Good-
man, 2001; Emami and Jelinek, 2005; Mikolov et
al., 2011b). Under this scheme we compute the
probability of an MTU by multiplying the prob-
ability of its class c
i
t
with the probability of the
depicted them for simplicity as a connection between the
current input vectorm
t
and the output layer.
minimal unit conditioned on the class:
p(m
t+1
|m
t
t?n+1
,h
t
) =
p(c
i
t
|m
t
t?n+1
,h
t
) p(m
t+1
|c
i
t
,m
t
t?n+1
,h
t
)
This factorization reduces the complexity of com-
puting the output probabilities from O(|V |) to
O(|C| + max
i
|c
i
|) where |C| is the number of
classes and |c
i
| is the number of minimal units
in class c
i
. The best case complexity O(
?
|V |)
requires the number of classes and MTUs to be
evenly balanced, i.e., each class contains exactly
as many minimal units as there are classes.
Figure 3 illustrates how classing changes the
structure of the network by adding an additional
output layer for the class probabilities.
4 Bag-of-words MTU RNN Model
The previous model treats MTUs as atomic sym-
bols which leads to large vocabularies requir-
ing large parameter sets and expensive inference.
However, similar MTUs may share the same
words, or words which are related in continuous
space. The atomic MTU model does not exploit
this since it cannot access the internal structure of
a minimal unit.
The approach we pursue next is to break MTUs
into individual source and target words (Le et al.,
2012) in order to exploit structural similarities be-
tween infrequently observed minimal units. Sin-
gletons represent the vast majority of our MTU
vocabulary (Table 1). This resembles the word-
hashing trick of Huang et al. (2013) who repre-
sented individual words as a bag-of-character n-
grams to reduce the vocabulary size of a neural
network-based model in an information retrieval
setting.
2
We first describe a theoretically appealing but
computationally expensive model and then discuss
a more practical variation. The input layer of this
model accepts the current minimal unit as a K-of-
N vector representing K source and target words
as opposed to the 1-of-N encoding of entire MTUs
in the previous model (Figure 4). Larger MTUs
may contain the same word more than once and we
simply adjust their count to one.
3
Different to the
2
Applying the same technique would likely result in too many
collisions since we are dealing with multi-word units instead
of single words.
3
We found no effect on accuracy when using the unmodified
count in initial experiments.
23
x t
ht-1
ht
w t
V
W
U
1
0
1
1
0
1
0
0
D yt
C
. . .
. . .
. . .
sr c
tgt
MT U
Figure 4: Structure of MTU bag-of-words recur-
rent neural network model. The input layer rep-
resents a minimal unit as a bag-of-words and the
output layer y
t
is a probability distribution over
possible next MTUs depending on the activations
of the word layer w
t
representing source and tar-
get words of minimal units.
previous model, the input vector has now multiple
active entries whose signals are absorbed into the
new hidden layer configuration.
This bag-of-words encoding of minimal units
dramatically reduces the vocabulary size but it in-
evitably maps different MTUs to the same encod-
ing. On our data set, we observe less than 0.2% of
minimal units that are involved in collisions, a rate
that is similar to Huang et al. (2013). In practice
collisions are unlikely to affect accuracy in our set-
ting because MTUs that are mapped to the same
encoding usually do not differ much in semantic
meaning as illustrated by the following examples:
erfolg haben ? succeed collides with haben er-
folg? succeed, or damit ,? to and , damit? to;
in both examples either the auxiliary verb haben or
the comma changes position, neither of which sig-
nificantly changes the meaning for this particular
pair of MTUs.
The structure of the bag-of-words MTU RNN
models is shown in Figure 4. Similar to the atomic
MTU RNN model (?3), the hidden layer combines
the signal from the input layer and the previous
hidden layer configuration. The hidden layer acti-
vations feed into a word layer w
t
representing the
source and target words that part of all possible
MTUs; it is of the same size as the input layer. The
word layer is connected to a convolutional out-
put layer y
t
by weights summarized in the sparse
matrix C. The output layer represents all possi-
ble next minimal units, where each MTU entry is
only connected to neurons in the word layer repre-
senting its source and target words. The word and
MTU layers are then computed as follows:
w
t
= s(Vh
t
)
y
t
= g(Cw
t
)
However, there are a number of computational
issues with this model: First, we cannot efficiently
factor the word layer w
t
into classes such as for
the atomic MTU RNN model because we require
all its activations to compute the MTU output
layer y
t
. This reduces the best case complex-
ity of computing the word layer from O(
?
|V |)
back to linear in the number of source and tar-
get words |V |. In practice this results in between
200-1000 more activations that need to be com-
puted, depending on the word vocabulary size.
Second, turning the MTU output layer into a con-
volutional layer is not enough to sufficiently re-
duce the computational effort to compute the out-
put activations since the number of connections
between the word and MTU layers is very imbal-
anced. This is because frequent words, such as
function words, are part of many MTUs and there-
fore have a very high out-degree, e.g., the neuron
representing ?the? has over 82K outgoing edges.
On the other hand, infrequent words, have a very
low out-degree. This imbalance makes it hard
to efficiently compute activations and error gradi-
ents, even on a GPU, since some neurons require
substantially more work than others.
4
For these reasons we decided to design a sim-
pler, more tractable version of this model (Fig-
ure 5). The simplified model still represents an
input MTU as a bag-of-words but minimal units
are generated word-by-word, first emitting source
words and then target words. This is in contrast
to the original model which predicted an MTU as
a single unit. Decomposing the next MTU into
individual words dramatically reduces the size of
the output layer, thereby resulting in faster com-
putation of the outputs and making normalization
4
In initial experiments we found this model to be over twenty
times slower than the atomic MTU RNN model with esti-
mated training times of over 6 weeks. This was despite us-
ing a vastly smaller vocabulary and by computing the word
layer on a, by current standards, high-end GPU (NVIDIA
Tesla K20c) using sparse matrix optimizations (cuSPARSE)
for the convolutional layer.
24
mt
ht-1
ht
yt
V
W
U
1
0
1
1
0
1
0
0
T
D
c t
mt+ 1
sr c
tgt
MT U
Figure 5: Simplified MTU bag-of-words recurrent
neural network model (cf. Figure 4). An MTU is
input as bag-of-words and the next MTU is pre-
dicted as a sequence of both source and target
words.
into probabilities easier. Furthermore, the output
layer can be factorized into classes requiring only
a fraction of the neurons to be computed, a much
more efficient solution compared to the original
model which required calculation of the entire out-
put layer.
The simplified model computes the probability
of the next MTU m
t+1
as a product of individual
word probabilities:
p(m
t+1
|m
t
t?n+1
,h
t
) = (1)
?
a
1
,...,a
u
?m
t+1
p(c
k
|m
t
t?n+1
,h
t
)
p(a
k
|c
k
,m
t
t?n+1
,h
t
)
where we predict a sequence of source and target
words a
1
, . . . , a
u
? m
t+1
with a class-structured
output layer, similar to the atomic model (?3).
Training still uses a cross entropy criterion and
back propagation through time, however, error
vectors are computed on a per-word basis, instead
of a per-MTU basis. Direct connections between
the input and output layers are based on source and
target words which is less sparse than basing direct
features on entire MTUs such as for the original
bag-of-words model.
Overall, the simplified model retains the bag-of-
words input representation of the original model,
while permitting the efficient factorization of the
word-output layer into classes.
5 Experiments
We evaluate the effectiveness of both the atomic
MTU RNN model (?3) and the simplified bag-of-
words MTU RNN model (?4) in an n-best rescor-
ing setting, comparing against a trigram back-off
MTU model as well as the phrasal decoder 1-best
output which we denote as the baseline.
5.1 Experimental Setup
Baselines. We experiment with an in-house
phrase-based system similar to Moses (Koehn et
al., 2007), scoring translations by a set of common
features including maximum likelihood estimates
of source given target mappings p
MLE
(e|f) and
vice versa p
MLE
(f |e), as well as lexical weight-
ing estimates p
LW
(e|f) and p
LW
(f |e), word and
phrase-penalties, a linear distortion feature and
a lexicalized reordering feature. The baseline
includes a standard modified Kneser-Ney word-
based language model trained on the target-side of
the parallel corpora described below. Log-linear
weights are estimated with minimum error rate
training (MERT; Och, 2003).
The 1-best output by the phrase-based decoder
is the baseline accuracy. As a second baseline we
experiment with a trigram back-off MTU model
trained on all extracted MTUs, denoted as n-gram
MTU. The trigram MTU model is estimated with
the same modified Kneser-Ney framework as the
target side language model. All MTU models are
trained in target left-to-right MTU order which
performed well in initial experiments.
Evaluation. We test our approach on two differ-
ent data sets. First, we train a German to English
system based on the data of the WMT 2006 shared
task (Koehn and Monz, 2006). The parallel corpus
includes about 35M words of parliamentary pro-
ceedings for training, a development set and two
test sets with 2000 sentences each.
Second, we experiment with a French to En-
glish system based on 102M words of training data
from the WMT 2012 campaign. The majority of
the training data set is parliamentary proceedings
except for about 5m words which are newswire; all
MTU models are trained on the newswire subset
since we found similar accuracy to using all data in
initial experiments. We evaluate on four newswire
domain test sets from 2008, 2010 and 2011 as well
as the 2010 system combination test set contain-
ing between 2034 to 3003 sentences. Log-linear
weights are estimated on the 2009 data set com-
25
prising 2525 sentences. We evaluate all systems
in a single reference BLEU setting.
Rescoring Setup. We rescore the 1000-best out-
put of the baseline phrase-based decoder by ei-
ther the trigram back-off MTU model or the
RNN models. The baseline accuracy is obtained
by choosing the 1-best decoder output. We re-
estimate the log-linear weights for rescoring by
running a further iteration of MERT with the ad-
ditional feature values; we initialize the rescoring
feature weight to zero and try 20 random restarts.
At test time we use the new set of log-linear
weights to rescore the test set n-best list.
Neural Network Setup. We trained the recur-
rent neural network models on between 88% and
93% of each data set and used the remainder as
validation data. The vocabulary of the atomic
MTU RNN model is comprised of all MTU types
which were observed more than once in the train-
ing data.
5
Similarly, we modeled all non-singleton
words for the bag-of-words MTU RNN model.
We obtain classes for words or MTUs using a
version of Brown-Clustering with an additional
regularization term to optimize the runtime of
the language model (Brown et al., 1992; Zweig
and Makarychev, 2013). Direct connections use
features over unigrams, bigrams and trigrams of
words or MTUs, depending on the model. Fea-
tures are hashed to a table with at most 500 million
values following Mikolov et al. (2011a). We use
the standard settings for the model with the default
learning rate ? = 0.1 that decays exponentially if
the validation set entropy does not decrease. Back
propagation through time computes error gradi-
ents over the past twenty time steps. Training
is stopped after 20 epochs or when the valida-
tion entropy does not decrease over two epochs.
Throughout, we use a hidden layer size of 100
which provided a good trade-off between time and
accuracy in initial experiments.
5.2 Results
We first report the decoder 1-best output as the
first baseline and then rescore our two data sets
(Table 2 and Table 3) with the n-gram back-off
MTU model to establish a second baseline (n-
gram MTU). The n-gram model improves by 0.4
BLEU over the decoder 1-best on all test sets for
German to English. On French-English accuracy
5
We tried modeling all MTUs which did not contain a single-
ton word but observed no significant effect on accuracy.
dev test1 test2
Baseline 25.8 26.0 26.0
n-gram MTU 26.3 26.6 26.4
atomic MTU RNN 26.5 26.8 26.5
BoW MTU RNN 26.5 27.0 26.9
word RNNLM 26.5 27.1 26.8
Combined 26.8 27.3 27.1
Table 2: German to English BLEU results for
the decoder 1-best output (Baseline) compared to
rescoring with a target left-to-right trigram MTU
model (n-gram MTU), our two recurrent neural
network-based MTU models, a word-based RNN-
based language model (word RNNLM), as well
as a combination of the three RNN-based models
(Combined).
improves on three out of five sets by up to 0.7
BLEU.
Next, we evaluate the accuracy of the MTU
RNN models. The atomic MTU RNN model im-
proves over the n-gram MTU model on all test sets
for German to English, however, for French to En-
glish the back-off model performs better on two
out of four test sets.
The next question we answer is if breaking
MTUs into individual units to leverage similarities
in the internal structure can help accuracy. The re-
sults (Table 2 and Table 3) for the bag-of-words
model (BoW MTU RNN) clearly show that this is
the case for both language pairs. We significantly
improve over the n-gram MTU model as well as
the atomic RNN model on all test sets. We observe
gains of up to 0.5 BLEU over the n-gram MTU
model for German to English as well as French to
English; improvements over the decoder baseline
are up to 1.2 BLEU for French to English.
How do our models compare to other neural net-
work approaches that rely only on target side in-
formation? To answer this question we compare
to the strong language model of Mikolov (2012;
RNNLM) which has recently improved the state-
of-the-art in language modeling perplexity. The
results (Table 2 and Table 3) show that RNNLM
performs competitively. However, our approaches
model translation since we use both source and tar-
get information as opposed to scoring only the flu-
ency of the target side, such as done by RNNLM.
Can our models act complementary to a strong
RNN language model? Our final experiment com-
bines the atomic MTU RNN model, the BoW
26
dev news2008 news2010 news2011 newssyscomb2010
Baseline 24.3 20.5 24.4 25.1 24.3
n-gram MTU 24.6 20.8 24.4 25.8 24.3
atomic MTU RNN 24.6 20.7 24.4 25.5 24.3
BoW MTU RNN 25.2 21.2 24.8 26.3 24.6
word RNNLM 25.1 21.4 25.1 26.4 24.9
Combined 25.4 21.4 25.1 26.6 24.9
Table 3: French to English BLEU results for the decoder 1-best output (Baseline) compared to various
MTU models (cf. Table 2).
MTU RNN model, and the RNNLM (Combined).
The results (Table 2 and Table 3) confirm that this
is the case. For German to English translation
accuracy improves by 0.2 to 0.3 BLEU over the
RNNLM alone, with gains of up to 1.3 BLEU over
the baseline and up to 0.7 BLEU over the n-gram
MTU model. Improvements for French to English
are lower but we can see some gains on news2011
and on the dev set. Overall, we improve accuracy
on the French to English task by up to 1.5 BLEU
over the decoder 1-best, and by up to 0.8 BLEU
over the n-gram MTU model.
6 Related Work
Our approach of modeling Minimum Translation
Units is very much in line with recent work on n-
gram-based translation models (Crego and Yvon,
2010), and more recently, continuous space-based
translation models (Le et al., 2012). The mod-
els presented in this paper differ in a number of
key aspects: We use a recurrent architecture repre-
senting an unbounded history of MTUs rather than
a feed-forward style network. Feed-forward net-
works as well as back-off n-gram models rely on a
finite history which results in predictions indepen-
dent of anything but a short context of words. A
recent side-by-side comparison between recurrent
and feed-forward style neural networks (Sunder-
meyer et al., 2013) has shown that recurrent ar-
chitectures outperform feed-forward networks in
a language modeling task, a similar problem to
modeling sequences over Minimum Translation
Units.
Furthermore, the input of our best model is a
bag-of-words representation of an MTU, unlike
the ordered source and target word n-grams used
by Crego and Yvon (2010) as well as Le et al.
(2012). Finally, we model both source and target
words in a single recurrent neural network. The
approach of Le et al. (2012) factorizes the joint
probability over an MTU sequence in a way that
suggests the use of separate neural network mod-
els for the source and the target sides, where each
model generates words on the respective side only.
Other work on applying recurrent neural net-
works to machine translation (Mikolov, 2012; Auli
et al., 2013; Kalchbrenner and Blunsom, 2013)
concentrated on word-based language and transla-
tion models, whereas we model Minimum Trans-
lation Units.
7 Conclusion and Future Work
Minimum Translation Unit models based on recur-
rent neural networks lead to substantial gains over
their classical n-gram back-off models. We intro-
duced two models of which the best improves ac-
curacy by up to 1.5 BLEU over the 1-best decoder
output, and by 0.8 BLEU over a trigram MTU
model in an n-best rescoring setting.
Our experiments have shown that representing
MTUs as bags-of-words leads to better accuracy
since this exploits similarities in the internal struc-
ture of Minimum Translation Units, which is not
possible when modeling them as atomic symbols.
We have also shown that our models are comple-
mentary to a very strong RNN language model
(Mikolov, 2012).
In future work, we would like to make the initial
version of the bag-of-words model computation-
ally more tractable using a better GPU implemen-
tation. This model combines the efficient bag-of-
words input representation with the ability to pre-
dict MTUs as single units while explicitly model-
ing the constituent words in an intermediate layer.
8 Acknowledgements
We would like to thank Kristina Toutanova for
providing a dataset and for helpful discussions re-
lated to this work. We also thank the four anony-
mous reviewers for their comments.
27
References
Alexandre Allauzen, H?el`ene Bonneau-Maynard, Hai-
Son Le, Aur?elien Max, Guillaume Wisniewski,
Franc?ois Yvon, Gilles Adda, Josep Maria Crego,
Adrien Lardilleux, Thomas Lavergne, and Artem
Sokolov. 2011. LIMSI @ WMT11. In Proc. of
WMT, pages 309?315, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep Neural Net-
work Language Models. In NAACL-HLT Work-
shop on the Future of Language Modeling for HLT,
pages 20?28, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Proc.
of EMNLP, October.
Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert,
Patrik Lambert, and Jos?e B. Mari?no. 2005. Statis-
tical Machine Translation of Euparl Data by Using
bilingual n-grams. In Proc. of ACL Workshop on
Building and Using Parallel Texts, pages 133?136,
Jun.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479,
Dec.
Josep Crego and Franc?ois Yvon. 2010. Factored bilin-
gual n-gram language models for statistical machine
translation. Machine Translation, 24(2):159?175.
Ahmad Emami and Frederick Jelinek. 2005. A Neu-
ral Syntactic Language Model. Machine Learning,
60(1-3):195?227, September.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning Semantic Representations
for the Phrase Translation Model. Technical Report
MSR-TR-2013-88, Microsoft Research, September.
Joshua Goodman. 2001. Classes for Fast Maximum
Entropy Training. In Proc. of ICASSP.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Proc.
of ACL, August.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning Deep
Structured Semantic Models for Web Search using
Clickthrough Data. In Proc. of CIKM, October.
Nal Kalchbrenner and Phil Blunsom. 2013. Re-
current Continuous Translation Models. In Proc.
of EMNLP, pages 1700?1709, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In Proc. of NAACL Workshop
on Statistical Machine Translation, pages 102?121.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proc. of HLT-NAACL, pages 127?133, Edmonton,
Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, Jun.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Proc. of HLT-NAACL, pages
39?48, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Tom?a?s Mikolov, Karafi?at Martin, Luk?a?s Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proc. of INTERSPEECH, pages 1045?1048.
Tom?a?s Mikolov, Anoop Deoras, Daniel Povey, Luk?a?s
Burget, and Jan
?
Cernock?y. 2011a. Strategies
for Training Large Scale Neural Network Language
Models. In Proc. of ASRU, pages 196?201.
Tom?a?s Mikolov, Stefan Kombrink, Luk?a?s Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2011b. Ex-
tensions of Recurrent Neural Network Language
Model. In Proc. of ICASSP, pages 5528?5531.
Tom?a?s Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space-
Word Representations. In Proc. of NAACL, pages
746?751, Stroudsburg, PA, USA, June. Association
for Computational Linguistics.
Tom?a?s Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
Statistical Machine Translation. In Proc. of NAACL,
pages 8?16, New York, Jun.
28
Ariya Rastrow, Sanjeev Khudanpur, and Mark Dredze.
2012. Revisiting the Case for Explicit Syntactic
Information in Language Models. In NAACL-HLT
Workshop on the Future of Language Modeling for
HLT, pages 50?58. Association for Computational
Linguistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In Symposium on Parallel and
Distributed Processing.
Holger Schwenk, Marta R. Costa-juss`a, and Jos?e A. R.
Fonollosa. 2007. Smooth Bilingual N -Gram Trans-
lation. In Proc. of EMNLP, pages 430?438, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space
Language Models on a GPU for Statistical Machine
Translation. In NAACL-HLT Workshop on the Fu-
ture of Language Modeling for HLT, pages 11?19.
Association for Computational Linguistics.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl?uter, and Hermann Ney.
2013. Comparison of Feedforward and Recurrent
Neural Network Language Models. In IEEE In-
ternational Conference on Acoustics, Speech, and
Signal Processing, pages 8430?8434, Vancouver,
Canada, May.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-scale
Neural Language Models improves Translation. In
Proc. of EMNLP. Association for Computational
Linguistics, October.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond left-to-right: Multiple de-
composition structures for smt. In Proc. of NAACL,
pages 12?21, Atlanta, Georgia, June. Association
for Computational Linguistics.
Geoff Zweig and Konstantin Makarychev. 2013.
Speed Regularization and Optimality in Word Class-
ing. In Proc. of ICASSP.
29
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 470?480,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Comparison of Loopy Belief Propagation and Dual Decomposition for
Integrated CCG Supertagging and Parsing
Michael Auli
School of Informatics
University of Edinburgh
m.auli@sms.ed.ac.uk
Adam Lopez
HLTCOE
Johns Hopkins University
alopez@cs.jhu.edu
Abstract
Via an oracle experiment, we show that the
upper bound on accuracy of a CCG parser
is significantly lowered when its search space
is pruned using a supertagger, though the su-
pertagger also prunes many bad parses. In-
spired by this analysis, we design a single
model with both supertagging and parsing fea-
tures, rather than separating them into dis-
tinct models chained together in a pipeline.
To overcome the resulting increase in com-
plexity, we experiment with both belief prop-
agation and dual decomposition approaches to
inference, the first empirical comparison of
these algorithms that we are aware of on a
structured natural language processing prob-
lem. On CCGbank we achieve a labelled de-
pendency F-measure of 88.8% on gold POS
tags, and 86.7% on automatic part-of-speeoch
tags, the best reported results for this task.
1 Introduction
Accurate and efficient parsing of Combinatorial Cat-
egorial Grammar (CCG; Steedman, 2000) is a long-
standing problem in computational linguistics, due
to the complexities associated its mild context sen-
sitivity. Even for practical CCG that are strongly
context-free (Fowler and Penn, 2010), parsing is
much harder than with Penn Treebank-style context-
free grammars, with vast numbers of nonterminal
categories leading to increased grammar constants.
Where a typical Penn Treebank grammar may have
fewer than 100 nonterminals (Hockenmaier and
Steedman, 2002), we found that a CCG grammar
derived from CCGbank contained over 1500. The
same grammar assigns an average of 22 lexical cate-
gories per word (Clark and Curran, 2004a), resulting
in an enormous space of possible derivations.
The most successful approach to CCG parsing is
based on a pipeline strategy (?2). First, we tag (or
multitag) each word of the sentence with a lexical
category using a supertagger, a sequence model over
these categories (Bangalore and Joshi, 1999; Clark,
2002). Second, we parse the sentence under the
requirement that the lexical categories are fixed to
those preferred by the supertagger. Variations on
this approach drive the widely-used, broad coverage
C&C parser (Clark and Curran, 2004a; Clark and
Curran, 2007; Kummerfeld et al, 2010). However,
it fails when the supertagger makes errors. We show
experimentally that this pipeline significantly lowers
the upper bound on parsing accuracy (?3).
The same experiment shows that the supertag-
ger prunes many bad parses. So, while we want to
avoid the error propagation inherent to a pipeline,
ideally we still want to benefit from the key insight
of supertagging: that a sequence model over lexi-
cal categories can be quite accurate. Our solution
is to combine the features of both the supertagger
and the parser into a single, less aggressively pruned
model. The challenge with this model is its pro-
hibitive complexity, which we address with approx-
imate methods: dual decomposition and belief prop-
agation (?4). We present the first side-by-side com-
parison of these algorithms on an NLP task of this
complexity, measuring accuracy, convergence be-
havior, and runtime. In both cases our model signifi-
cantly outperforms the pipeline approach, leading to
the best published results in CCG parsing (?5).
470
2 CCG and Supertagging
CCG is a lexicalized grammar formalism encoding
for each word lexical categories that are either ba-
sic (eg. NN, JJ) or complex. Complex lexical cat-
egories specify the number and directionality of ar-
guments. For example, one lexical category for the
verb like is (S\NP )/NP , specifying the first argu-
ment as an NP to the right and the second as an NP
to the left; there are over 100 lexical categories for
like in our lexicon. In parsing, adjacent spans are
combined using a small number of binary combina-
tory rules like forward application or composition
(Steedman, 2000; Fowler and Penn, 2010). In the
first derivation below, (S\NP )/NP and NP com-
bine to form the spanning category S\NP , which
only requires an NP to its left to form a complete
sentence-spanning S. The second derivation uses
type-raising to change the category type of I.
I like tea
NP (S\NP)/NP NP
>
S\NP
<
S
I like tea
NP (S\NP)/NP NP
>T
S/(S\NP)
>B
S/NP
>
S
As can be inferred from even this small example,
a key difficulty in parsing CCG is that the number
of categories quickly becomes extremely large, and
there are typically many ways to analyze every span
of a sentence.
Supertagging (Bangalore and Joshi, 1999; Clark,
2002) treats the assignment of lexical categories (or
supertags) as a sequence tagging problem. Because
they do this with high accuracy, they are often ex-
ploited to prune the parser?s search space: the parser
only considers lexical categories with high posterior
probability (or other figure of merit) under the su-
pertagging model (Clark and Curran, 2004a). The
posterior probabilities are then discarded; it is the
extensive pruning of lexical categories that leads to
substantially faster parsing times.
Pruning the categories in advance this way has a
specific failure mode: sometimes it is not possible
to produce a sentence-spanning derivation from the
tag sequences preferred by the supertagger, since it
does not enforce grammaticality. A workaround for
this problem is the adaptive supertagging (AST) ap-
proach of Clark and Curran (2004a). It is based on
a step function over supertagger beam widths, re-
laxing the pruning threshold for lexical categories
only if the parser fails to find an analysis. The pro-
cess either succeeds and returns a parse after some
iteration or gives up after a predefined number of it-
erations. As Clark and Curran (2004a) show, most
sentences can be parsed with a very small number of
supertags per word. However, the technique is inher-
ently approximate: it will return a lower probability
parse under the parsing model if a higher probabil-
ity parse can only be constructed from a supertag
sequence returned by a subsequent iteration. In this
way it prioritizes speed over exactness, although the
tradeoff can be modified by adjusting the beam step
function. Regardless, the effect of the approxima-
tion is unbounded.
We will also explore reverse adaptive supertag-
ging, a much less aggressive pruning method that
seeks only to make sentences parseable when they
otherwise would not be due to an impractically large
search space. Reverse AST starts with a wide beam,
narrowing it at each iteration only if a maximum
chart size is exceeded. In this way it prioritizes ex-
actness over speed.
3 Oracle Parsing
What is the effect of these approximations? To
answer this question we computed oracle best and
worst values for labelled dependency F-score using
the algorithm of Huang (2008) on the hybrid model
of Clark and Curran (2007), the best model of their
C&C parser. We computed the oracle on our devel-
opment data, Section 00 of CCGbank (Hockenmaier
and Steedman, 2007), using both AST and Reverse
AST beams settings shown in Table 1.
The results (Table 2) show that the oracle best
accuracy for reverse AST is more than 3% higher
than the aggressive AST pruning.1 In fact, it is al-
most as high as the upper bound oracle accuracy of
97.73% obtained using perfect supertags?in other
words, the search space for reverse AST is theoreti-
cally near-optimal.2 We also observe that the oracle
1The numbers reported here and in later sections differ slightly
from those in a previously circulated draft of this paper, for
two reasons: we evaluate only on sentences for which a parse
was returned instead of all parses, to enable direct comparison
with Clark and Curran (2007); and we use their hybrid model
instead of their normal-form model, except where noted. De-
spite these changes our main findings remained unchanged.
2This idealized oracle reproduces a result from Clark and Cur-
471
Condition Parameter Iteration 1 2 3 4 5
AST
? (beam width) 0.075 0.03 0.01 0.005 0.001
k (dictionary cutoff) 20 20 20 20 150
Reverse
? 0.001 0.005 0.01 0.03 0.075
k 150 20 20 20 20
Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.
Parameter ? is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words
seen less than k times.
Viterbi F-score Oracle Max F-score Oracle Min F-score
LF LP LR LF LP LR LF LP LR cat/word
AST 87.38 87.83 86.93 94.35 95.24 93.49 54.31 54.81 53.83 1.3-3.6
Reverse 87.36 87.55 87.17 97.65 98.21 97.09 18.09 17.75 18.43 3.6-1.3
Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle
F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the
the number of lexical categories per word used (from first to last parsing attempt).
88.2	 ?
88.4	 ?
88.6	 ?
88.8	 ?
89.0	 ?
89.2	 ?
89.4	 ?
89.6	 ?
89.8	 ?
85600	 ?
85800	 ?
86000	 ?
86200	 ?
86400	 ?
86600	 ?
86800	 ?
87000	 ?
87200	 ?
87400	 ?
0.07
5	 ? 0.03
	 ?
0.01
	 ?
0.00
5	 ?
0.00
1	 ?
0.00
05	 ?
0.00
01	 ?
0.00
005	 ?
0.00
001	 ?
Lab
elle
ld	 ?F
-??sco
re	 ?
Mo
del
	 ?sco
re	 ?
Supertagger	 ?beam	 ?
Model	 ?score	 ? F-??measure	 ?
93.5	 ?
94.0	 ?
94.5	 ?
95.0	 ?
95.5	 ?
96.0	 ?
96.5	 ?
97.0	 ?
97.5	 ?
98.0	 ?
98.5	 ?
82500	 ?
83000	 ?
83500	 ?
84000	 ?
84500	 ?
85000	 ?
0.07
5	 ? 0.03
	 ?
0.01
	 ?
0.00
5	 ?
0.00
1	 ?
0.00
05	 ?
0.00
01	 ?
0.00
005	 ?
0.00
001	 ?
Lab
elle
ld	 ?F
-??sco
re	 ?
Mo
del
	 ?sco
re	 ?
Supertagger	 ?beam	 ?
Model	 ?score	 ? F-??measure	 ?
Figure 1: Comparison between model score and Viterbi F-score (left); and between model score and oracle F-score
(right) for different supertagger beams on a subset of CCGbank Section 00.
worst accuracy is much lower in the reverse setting.
It is clear that the supertagger pipeline has two ef-
fects: while it beneficially prunes many bad parses,
it harmfully prunes some very good parses. We can
also see from the scores of the Viterbi parses that
while the reverse condition has access to much better
parses, the model doesn?t actually find them. This
mirrors the result of Clark and Curran (2007) that
they use to justify AST.
Digging deeper, we compared parser model score
against Viterbi F-score and oracle F-score at a va-
ran (2004b). The reason that using the gold-standard supertags
doesn?t result in 100% oracle parsing accuracy is that some
of the development set parses cannot be constructed by the
learned grammar.
riety of fixed beam settings (Figure 1), considering
only the subset of our development set which could
be parsed with all beam settings. The inverse re-
lationship between model score and F-score shows
that the supertagger restricts the parser to mostly
good parses (under F-measure) that the model would
otherwise disprefer. Exactly this effect is exploited
in the pipeline model. However, when the supertag-
ger makes a mistake, the parser cannot recover.
4 Integrated Supertagging and Parsing
The supertagger obviously has good but not perfect
predictive features. An obvious way to exploit this
without being bound by its decisions is to incorpo-
rate these features directly into the parsing model.
472
In our case both the parser and the supertagger are
feature-based models, so from the perspective of a
single parse tree, the change is simple: the tree is
simply scored by the weights corresponding to all
of its active features. However, since the features of
the supertagger are all Markov features on adjacent
supertags, the change has serious implications for
search. If we think of the supertagger as defining a
weighted regular language consisting of all supertag
sequences, and the parser as defining a weighted
mildly context-sensitive language consisting of only
a subset of these sequences, then the search prob-
lem is equivalent to finding the optimal derivation
in the weighted intersection of a regular and mildly
context-sensitive language. Even allowing for the
observation of Fowler and Penn (2010) that our prac-
tical CCG is context-free, this problem still reduces
to the construction of Bar-Hillel et al (1964), mak-
ing search very expensive. Therefore we need ap-
proximations.
Fortunately, recent literature has introduced two
relevant approximations to the NLP community:
loopy belief propagation (Pearl, 1988), applied to
dependency parsing by Smith and Eisner (2008);
and dual decomposition (Dantzig and Wolfe, 1960;
Komodakis et al, 2007; Sontag et al, 2010, inter
alia), applied to dependency parsing by Koo et al
(2010) and lexicalized CFG parsing by Rush et al
(2010). We apply both techniques to our integrated
supertagging and parsing model.
4.1 Loopy Belief Propagation
Belief propagation (BP) is an algorithm for com-
puting marginals (i.e. expectations) on structured
models. These marginals can be used for decoding
(parsing) in a minimum-risk framework (Smith and
Eisner, 2008); or for training using a variety of al-
gorithms (Sutton and McCallum, 2010). We experi-
ment with both uses in ?5. Many researchers in NLP
are familiar with two special cases of belief prop-
agation: the forward-backward and inside-outside
algorithms, used for computing expectations in se-
quence models and context-free grammars, respec-
tively.3 Our use of belief propagation builds directly
on these two familiar algorithms.
3Forward-backward and inside-outside are formally shown to
be special cases of belief propagation by Smyth et al (1997)
and Sato (2007), respectively.
f(T
1
)
f(T
2
)
b(T
0
) b(T
1
)
t
1
T
0
T
1
t
2
T
2
e
0
e
1
e
2
Figure 2: Supertagging factor graph with messages. Cir-
cles are variables and filled squares are factors.
BP is usually understood as an algorithm on bi-
partite factor graphs, which structure a global func-
tion into local functions over subsets of variables
(Kschischang et al, 1998). Variables maintain a be-
lief (expectation) over a distribution of values and
BP passes messages about these beliefs between
variables and factors. The idea is to iteratively up-
date each variable?s beliefs based on the beliefs of
neighboring variables (through a shared factor), us-
ing the sum-product rule.
This results in the following equation for a mes-
sage mx?f (x) from a variable x to a factor f
mx?f (x) =
?
h?n(x)\f
mh?x(x) (1)
where n(x) is the set of all neighbours of x. The
message mf?x from a factor to a variable is
mf?x(x) =
?
?{x}
f(X)
?
y?n(f)\x
my?f (y) (2)
where ? {x} represents all variables other than x,
X = n(f) and f(X) is the set of arguments of the
factor function f .
Making this concrete, our supertagger defines a
distribution over tags T0...TI , based on emission
factors e0...eI and transition factors t1...tI (Fig-
ure 2). The message fi a variable Ti receives from its
neighbor to the left corresponds to the forward prob-
ability, while messages from the right correspond to
backward probability bi.
fi(Ti) =
?
Ti?1
fi?1(Ti?1)ei?1(Ti?1)ti(Ti?1, Ti) (3)
bi(Ti) =
?
Ti+1
bi+1(Ti+1)ei+1(Ti+1)ti+1(Ti, Ti+1) (4)
473
span
(0,2)
span
(1,3)
span
(0,3)
TREE
n(T
0
) o(T
2
)
o(T
0
)
n(T
2
)
T
0
e
0
e
1
e
2
T
1
T
2
f(T
1
)
f(T
2
)
b(T
0
)
b(T
1
)
t
1
t
2
Figure 3: Factor graph for the combined parsing and su-
pertagging model.
The current belief Bx(x) for variable x can be com-
puted by taking the normalized product of all its in-
coming messages.
Bx(x) =
1
Z
?
h?n(x)
mh?x(x) (5)
In the supertagger model, this is just:
p(Ti) =
1
Z
fi(Ti)bi(Ti)ei(Ti) (6)
Our parsing model is also a distribution over vari-
ables Ti, along with an additional quadratic number
of span(i, j) variables. Though difficult to represent
pictorially, a distribution over parses is captured by
an extension to graphical models called case-factor
diagrams (McAllester et al, 2008). We add this
complex distribution to our model as a single fac-
tor (Figure 3). This is a natural extension to the use
of complex factors described by Smith and Eisner
(2008) and Dreyer and Eisner (2009).
When a factor graph is a tree as in Figure 2, BP
converges in a single iteration to the exact marginals.
However, when the model contains cycles, as in Fig-
ure 3, we can iterate message passing. Under certain
assumptions this loopy BP it will converge to ap-
proximate marginals that are bounded under an in-
terpretation from statistical physics (Yedidia et al,
2001; Sutton and McCallum, 2010).
The TREE factor exchanges inside ni and outside
oi messages with the tag and span variables, tak-
ing into account beliefs from the sequence model.
We will omit the unchanged outside recursion for
brevity, but inside messages n(Ci,j) for category
Ci,j in span(i, j) are computed using rule probabil-
ities r as follows:
n(Ci,j) =
?
??
??
fi(Ci,j)bi(Ci,j)ei(Ci,j) if j=i+1
?
k,X,Y
n(Xi,k)n(Yk,j)r(Ci,j , Xi,k, Yk,j)
(7)
Note that the only difference from the classic in-
side algorithm is that the recursive base case of a cat-
egory spanning a single word has been replaced by
a message from the supertag that contains both for-
ward and backward factors, along with a unary emis-
sion factor, which doubles as a unary rule factor and
thus contains the only shared features of the original
models. This difference is also mirrored in the for-
ward and backward messages, which are identical to
Equations 3 and 4, except that they also incorporate
outside messages from the tree factor.
Once all forward-backward and inside-outside
probabilities have been calculated the belief of su-
pertag Ti can be computed as the product of all in-
coming messages. The only difference from Equa-
tion 6 is the addition of the outside message.
p(Ti) =
1
Z
fi(Ti)bi(Ti)ei(Ti)oi(Ti) (8)
The algorithm repeatedly runs forward-backward
and inside-outside, passing their messages back and
forth, until these quantities converge.
4.2 Dual Decomposition
Dual decomposition (Rush et al, 2010; Koo et al,
2010) is a decoding (i.e. search) algorithm for prob-
lems that can be decomposed into exactly solvable
subproblems: in our case, supertagging and parsing.
Formally, given Y as the set of valid parses, Z as the
set of valid supertag sequences, and T as the set of
supertags, we want to solve the following optimiza-
tion for parser f(y) and supertagger g(z).
argmax
y?Y,z?Z
f(y) + g(z) (9)
such that y(i, t) = z(i, t) for all (i, t) ? I (10)
Here y(i, t) is a binary function indicating whether
word i is assigned supertag t by the parser, for the
474
set I = {(i, t) : i ? 1 . . . n, t ? T} denoting
the set of permitted supertags for each word; sim-
ilarly z(i, t) for the supertagger. To enforce the con-
straint that the parser and supertagger agree on a
tag sequence we introduce Lagrangian multipliers
u = {u(i, t) : (i, t) ? I} and construct a dual ob-
jective over variables u(i, t).
L(u) = max
y?Y
(f(y)?
?
i,t
u(i, t)y(i, t)) (11)
+max
z?Z
(f(z) +
?
i,t
u(i, t)z(i, t))
This objective is an upper bound that we want to
make as tight as possible by solving for minu L(u).
We optimize the values of the u(i, t) variables using
the same algorithm as Rush et al (2010) for their
tagging and parsing problem (essentially a percep-
tron update).4 An advantages of DD is that, on con-
vergence, it recovers exact solutions to the combined
problem. However, if it does not converge or we stop
early, an approximation must be returned: following
Rush et al (2010) we used the highest scoring output
of the parsing submodel over all iterations.
5 Experiments
Parser. We use the C&C parser (Clark and Curran,
2007) and its supertagger (Clark, 2002). Our base-
line is the hybrid model of Clark and Curran (2007);
our integrated model simply adds the supertagger
features to this model. The parser relies solely on the
supertagger for pruning, using CKY for search over
the pruned space. Training requires repeated calcu-
lation of feature expectations over packed charts of
derivations. For training, we limited the number of
items in this chart to 0.3 million, and for testing, 1
million. We also used a more permissive training
supertagger beam (Table 3) than in previous work
(Clark and Curran, 2007). Models were trained with
the parser?s L-BFGS trainer.
Evaluation. We evaluated on CCGbank (Hocken-
maier and Steedman, 2007), a right-most normal-
form CCG version of the Penn Treebank. We
use sections 02-21 (39603 sentences) for training,
4The u terms can be interpreted as the messages from factors
to variables (Sontag et al, 2010) and the resulting message
passing algorithms are similar to the max-product algorithm, a
sister algorithm to BP.
section 00 (1913 sentences) for development and
section 23 (2407 sentences) for testing. We sup-
ply gold-standard part-of-speech tags to the parsers.
Evaluation is based on labelled and unlabelled pred-
icate argument structure recovery and supertag ac-
curacy. We only evaluate on sentences for which an
analysis was returned; the coverage for all parsers is
99.22% on section 00, and 99.63% on section 23.
Model combination. We combine the parser and
the supertagger over the search space defined by the
set of supertags within the supertagger beam (see Ta-
ble 1); this avoids having to perform inference over
the prohibitively large set of parses spanned by all
supertags. Hence at each beam setting, the model
operates over the same search space as the baseline;
the difference is that we search with our integrated
model.
5.1 Parsing Accuracy
We first experiment with the separately trained su-
pertagger and parser, which are then combined us-
ing belief propagation (BP) and dual decomposition
(DD). We run the algorithms for many iterations,
and irrespective of convergence, for BP we compute
the minimum risk parse from the current marginals,
and for DD we choose the highest-scoring parse
seen over all iterations. We measured the evolving
accuracy of the models on the development set (Fig-
ure 4). In line with our oracle experiment, these re-
sults demonstrate that we can coax more accurate
parses from the larger search space provided by the
reverse setting; the influence of the supertagger fea-
tures allow us to exploit this advantage.
One behavior we observe in the graph is that the
DD results tend to incrementally improve in accu-
racy while the BP results quickly stabilize, mirroring
the result of Smith and Eisner (2008). This occurs
because DD continues to find higher scoring parses
at each iteration, and hence the results change. How-
ever for BP, even if the marginals have not con-
verged, the minimum risk solution turns out to be
fairly stable across successive iterations.
We next compare the algorithms against the base-
line on our test set (Table 4). We find that the early
stability of BP?s performance generalises to the test
set as does DD?s improvement over several itera-
tions. More importantly, we find that the applying
475
Parameter Iteration 1 2 3 4 5 6 7
Training ? 0.001 0.001 0.0045 0.0055 0.01 0.05 0.1
k 150 20 20 20 20 20 20
Table 3: Beam step function used for training (cf. Table 1).
section 00 (dev) section 23 (test)
AST Reverse AST Reverse
LF UF ST LF UF ST LF UF ST LF UF ST
Baseline 87.38 93.08 94.21 87.36 93.13 93.99 87.73 93.09 94.33 87.65 93.06 94.01
C&C ?07 87.24 93.00 94.16 - - - 87.64 93.00 94.32 - - -
BPk=1 87.70 93.28 94.44 88.35 93.69 94.73 88.20 93.28 94.60 88.78 93.66 94.81
BPk=25 87.70 93.31 94.44 88.33 93.72 94.71 88.19 93.27 94.59 88.80 93.68 94.81
DDk=1 87.40 93.09 94.23 87.38 93.15 94.03 87.74 93.10 94.33 87.67 93.07 94.02
DDk=25 87.71 93.32 94.44 88.29 93.71 94.67 88.14 93.24 94.59 88.80 93.68 94.82
Table 4: Results for individually-trained submodels combined using dual decomposition (DD) or belief propagation
(BP) for k iterations, evaluated by labelled and unlabelled F-score (LF/UF) and supertag accuracy (ST). We compare
against the previous best result of Clark and Curran (2007); our baseline is their model with wider training beams (cf.
Table 3).
87.2	 ?
87.4	 ?
87.6	 ?
87.8	 ?
88.0	 ?
88.2	 ?
88.4	 ?
1	 ? 6	 ? 11	 ? 16	 ? 21	 ? 26	 ? 31	 ? 36	 ? 41	 ? 46	 ?
Lab
elle
d	 ?F
-??sc
ore
	 ?
Itera?ons	 ?
BL	 ?	 ?AST	 ? BL	 ?Rev	 ? BP	 ?AST	 ?
BP	 ?Rev	 ? DD	 ?AST	 ? DD	 ?Rev	 ?
Figure 4: Labelled F-score of baseline (BL), belief prop-
agation (BP), and dual decomposition (DD) on section
00.
our combined model using either algorithm consis-
tently outperforms the baseline after only a few iter-
ations. Overall, we improve the labelled F-measure
by almost 1.1% and unlabelled F-measure by 0.6%
over the baseline. To the best of our knowledge,
the results obtained with BP and DD are the best
reported results on this task using gold POS tags.
Next, we evaluate performance when using au-
tomatic part-of-speech tags as input to our parser
and supertagger (Table 5). This enables us to com-
pare against the results of Fowler and Penn (2010),
who trained the Petrov parser (Petrov et al, 2006)
on CCGbank. We outperform them on all criteria.
Hence our combined model represents the best CCG
parsing results under any setting.
Finally, we revisit the oracle experiment of ?3 us-
ing our combined models (Figure 5). Both show an
improved relationship between model score and F-
measure.
5.2 Algorithmic Convergence
Figure 4 shows that parse accuracy converges af-
ter a few iterations. Do the algorithms converge?
BP converges when the marginals do not change be-
tween iterations, and DD converges when both sub-
models agree on all supertags. We measured the
convergence of each algorithm under these criteria
over 1000 iterations (Figure 6). DD converges much
faster, while BP in the reverse condition converges
quite slowly. This is interesting when contrasted
with its behavior on parse accuracy?its rate of con-
vergence after one iteration is 1.5%, but its accu-
racy is already the highest at this point. Over the
entire 1000 iterations, most sentences converge: all
but 3 for BP (both in AST and reverse) and all but
476
section 00 (dev) section 23 (test)
LF LP LR UF UP UR LF LP LR UF UP UR
Baseline 85.53 85.73 85.33 91.99 92.20 91.77 85.74 85.90 85.58 91.92 92.09 91.75
Petrov I-5 85.79 86.09 85.50 92.44 92.76 92.13 86.01 86.29 85.74 92.34 92.64 92.04
BPk=1 86.44 86.74 86.14 92.54 92.86 92.23 86.73 86.95 86.50 92.45 92.69 92.21
DDk=25 86.35 86.65 86.05 92.52 92.85 92.20 86.68 86.90 86.46 92.44 92.67 92.21
Table 5: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
(2010); we evaluate on sentences for which all parsers returned an analysis (2323 sentences for section 23 and 1834
sentences for section 00).
89.4	 ?
89.5	 ?
89.6	 ?
89.7	 ?
89.8	 ?
89.9	 ?
90.0	 ?
60000	 ?
80000	 ?
100000	 ?
120000	 ?
140000	 ?
160000	 ?
180000	 ?
200000	 ?
0.07
5	 ? 0.03
	 ?
0.01
	 ?
0.00
5	 ?
0.00
1	 ?
0.00
05	 ?
0.00
01	 ?
0.00
005	 ?
0.00
001	 ?
Lab
elle
ld	 ?F
-??sco
re	 ?
Mo
del
	 ?sco
re	 ?
Supertagger	 ?beam	 ?
Model	 ?score	 ? F-??measure	 ?
89.2	 ?
89.3	 ?
89.4	 ?
89.5	 ?
89.6	 ?
89.7	 ?
89.8	 ?
89.9	 ?
85200	 ?
85400	 ?
85600	 ?
85800	 ?
86000	 ?
86200	 ?
86400	 ?
0.07
5	 ? 0.03
	 ?
0.01
	 ?
0.00
5	 ?
0.00
1	 ?
0.00
05	 ?
0.00
01	 ?
0.00
005	 ?
0.00
001	 ?
Lab
elle
ld	 ?F
-??sco
re	 ?
Mo
del
	 ?sco
re	 ?
Supertagger	 ?beam	 ?
Model	 ?score	 ? F-??measure	 ?
Figure 5: Comparison between model score and Viterbi F-score for the integrated model using belief propagation (left)
and dual decomposition (right); the results are based on the same data as Figure 1.
.
!"
#!"
$!"
%!"
&!"
'!"
(!"
)!"
*!"
+!"
#!!"
#" #!" #!!" #!!!"
!"#
$%&'
%#(
%)&*
+%),-
.)
/+%&*0"#1)
,-"./0"
,-"1232452"
66"./0"
66"1232452"
Figure 6: Rate of convergence for belief propagation (BP)
and dual decomposition (DD) with maximum k = 1000.
41 (2.6%) for DD in reverse (6 in AST).
5.3 Parsing Speed
Because the C&C parser with AST is very fast, we
wondered about the effect on speed for our model.
We measured the runtime of the algorithms under
the condition that we stopped at a particular iteration
(Table 6). Although our models improve substan-
tially over C&C, there is a significant cost in speed
for the best result.
5.4 Training the Integrated Model
In the experiments reported so far, the parsing and
supertagging models were trained separately, and
only combined at test time. Although the outcome
of these experiments was successful, we wondered
if we could obtain further improvements by training
the model parameters together.
Since the gradients produced by (loopy) BP
are approximate, for these experiments we used a
stochastic gradient descent (SGD) trainer (Bottou,
2003). We found that the SGD parameters described
by Finkel et al (2008) worked equally well for our
models, and, on the baseline, produced similar re-
sults to L-BFGS. Curiously, however, we found that
the combined model does not perform as well when
477
AST Reverse
sent/sec LF sent/sec LF
Baseline 65.8 87.38 5.9 87.36
BPk=1 60.8 87.70 5.8 88.35
BPk=5 46.7 87.70 4.7 88.34
BPk=25 35.3 87.70 3.5 88.33
DDk=1 64.6 87.40 5.9 87.38
DDk=5 41.9 87.65 3.1 88.09
DDk=25 32.5 87.71 1.9 88.29
Table 6: Parsing time in seconds per sentence (vs. F-
measure) on section 00.
AST Reverse
LF UF ST LF UF ST
Baseline 86.7 92.7 94.0 86.7 92.7 93.9
BP inf 86.8 92.8 94.1 87.2 93.1 94.2
BP train 86.3 92.5 93.8 85.6 92.1 93.2
Table 7: Results of training with SGD on approximate
gradients from LPB on section 00. We test LBP in both
inference and training (train) as well as in inference only
(inf); a maximum number of 10 iterations is used.
the parameters are trained together (Table 7). A pos-
sible reason for this is that we used a stricter su-
pertagger beam setting during training (Clark and
Curran, 2007) to make training on a single machine
practical. This leads to lower performance, particu-
larly in the Reverse condition. Training a model us-
ing DD would require a different optimization algo-
rithm based on Viterbi results (e.g. the perceptron)
which we will pursue in future work.
6 Conclusion and Future Work
Our approach of combining models to avoid the
pipeline problem (Felzenszwalb and McAllester,
2007) is very much in line with much recent work
in NLP. Such diverse topics as machine transla-
tion (Dyer et al, 2008; Dyer and Resnik, 2010;
Mi et al, 2008), part-of-speech tagging (Jiang et
al., 2008), named entity recognition (Finkel and
Manning, 2009) semantic role labelling (Sutton and
McCallum, 2005; Finkel et al, 2006), and oth-
ers have also been improved by combined models.
Our empirical comparison of BP and DD also com-
plements the theoretically-oriented comparison of
marginal- and margin-based variational approxima-
tions for parsing described by Martins et al (2010).
We have shown that the aggressive pruning used
in adaptive supertagging significantly harms the or-
acle performance of the parser, though it mostly
prunes bad parses. Based on these findings, we com-
bined parser and supertagger features into a single
model. Using belief propagation and dual decom-
position, we obtained more principled?and more
accurate?approximations than a pipeline. Mod-
els combined using belief propagation achieve very
good performance immediately, despite an initial
convergence rate just over 1%, while dual decompo-
sition produces comparable results after several iter-
ations, and algorithmically converges more quickly.
Our best result of 88.8% represents the state-of-the
art in CCG parsing accuracy.
In future work we plan to integrate the POS tag-
ger, which is crucial to parsing accuracy (Clark and
Curran, 2004b). We also plan to revisit the idea
of combined training. Though we have focused on
CCG in this work we expect these methods to be
equally useful for other linguistically motivated but
computationally complex formalisms such as lexi-
calized tree adjoining grammar.
Acknowledgements
We would like to thank Phil Blunsom, Prachya
Boonkwan, Christos Christodoulopoulos, Stephen
Clark, Michael Collins, Chris Dyer, Timothy
Fowler, Mark Granroth-Wilding, Philipp Koehn,
Terry Koo, Tom Kwiatkowski, Andre? Martins, Matt
Post, David Smith, David Sontag, Mark Steed-
man, and Charles Sutton for helpful discussion re-
lated to this work and comments on previous drafts,
and the anonymous reviewers for helpful comments.
We also acknowledge funding from EPSRC grant
EP/P504171/1 (Auli); the EuroMatrixPlus project
funded by the European Commission, 7th Frame-
work Programme (Lopez); and the resources pro-
vided by the Edinburgh Compute and Data Fa-
cility (http://www.ecdf.ed.ac.uk). The
ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk).
References
S. Bangalore and A. K. Joshi. 1999. Supertagging: An
Approach to Almost Parsing. Computational Linguis-
478
tics, 25(2):238?265, June.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application, pages 116?150.
L. Bottou. 2003. Stochastic learning. In Advanced Lec-
tures in Machine Learning, pages 146?168.
S. Clark and J. R. Curran. 2004a. The importance of su-
pertagging for wide-coverage CCG parsing. In COL-
ING, Morristown, NJ, USA.
S. Clark and J. R. Curran. 2004b. Parsing the WSJ using
CCG and log-linear models. In Proc. of ACL, pages
104?111, Barcelona, Spain.
S. Clark and J. R. Curran. 2007. Wide-Coverage Ef-
ficient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4):493?552.
S. Clark. 2002. Supertagging for Combinatory Catego-
rial Grammar. In TAG+6.
G. B. Dantzig and P. Wolfe. 1960. Decomposition
principle for linear programs. Operations Research,
8(1):101?111.
M. Dreyer and J. Eisner. 2009. Graphical models over
multiple strings. In Proc. of EMNLP.
C. Dyer and P. Resnik. 2010. Context-free reordering,
finite-state translation. In Proc. of HLT-NAACL.
C. J. Dyer, S. Muresan, and P. Resnik. 2008. Generaliz-
ing word lattice translation. In Proc. of ACL.
P. F. Felzenszwalb and D. McAllester. 2007. The Gener-
alized A* Architecture. In Journal of Artificial Intelli-
gence Research, volume 29, pages 153?190.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In Proc. of NAACL. Associ-
ation for Computational Linguistics.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
Bayesian inference for linguistic annotation pipelines.
In Proc. of EMNLP.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008.
Feature-based, conditional random field parsing. In
Proceedings of ACL-HLT.
T. A. D. Fowler and G. Penn. 2010. Accurate context-
free parsing with combinatory categorial grammar. In
Proc. of ACL.
J. Hockenmaier and M. Steedman. 2002. Generative
models for statistical parsing with Combinatory Cat-
egorial Grammar. In Proc. of ACL.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
L. Huang. 2008. Forest Reranking: Discriminative pars-
ing with Non-Local Features. In Proceedings of ACL-
08: HLT.
W. Jiang, L. Huang, Q. Liu, and Y. Lu?. 2008. A cas-
caded linear model for joint Chinese word segmen-
tation and part-of-speech tagging. In Proceedings of
ACL-08: HLT.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In Proc. of Int. Conf. on Computer
Vision (ICCV).
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual Decomposition for Parsing with Non-
Projective Head Automata. In In Proc. EMNLP.
F. R. Kschischang, B. J. Frey, and H.-A. Loeliger. 1998.
Factor graphs and the sum-product algorithm. IEEE
Transactions on Information Theory, 47:498?519.
J. K. Kummerfeld, J. Rosener, T. Dawborn, J. Haggerty,
J. R. Curran, and S. Clark. 2010. Faster parsing by
supertagger adaptation. In Proc. of ACL.
A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proc. of EMNLP.
D. McAllester, M. Collins, and F. Pereira. 2008. Case-
factor diagrams for structured probabilistic modeling.
Journal of Computer and System Sciences, 74(1):84?
96.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-based trans-
lation. In Proc. of ACL-HLT.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. of ACL.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
In Proc. EMNLP.
T. Sato. 2007. Inside-outside probability computation
for belief propagation. In Proc. of IJCAI.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
P. Smyth, D. Heckerman, and M. Jordan. 1997. Prob-
abilistic independence networks for hidden Markov
probability models. Neural computation, 9(2):227?
269.
D. Sontag, A. Globerson, and T. Jaakkola. 2010. Intro-
duction to dual decomposition. In S. Sra, S. Nowozin,
and S. J. Wright, editors, Optimization for Machine
Learning. MIT Press.
M. Steedman. 2000. The syntactic process. MIT Press,
Cambridge, MA.
C. Sutton and A. McCallum. 2005. Joint parsing and
semantic role labelling. In Proc. of CoNLL.
479
C. Sutton and A. McCallum. 2010. An introduction to
conditional random fields. arXiv:stat.ML/1011.4088.
J. Yedidia, W. Freeman, and Y. Weiss. 2001. Generalized
belief propagation. In Proc. of NIPS.
480
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577?1585,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Efficient CCG Parsing: A* versus Adaptive Supertagging
Michael Auli
School of Informatics
University of Edinburgh
m.auli@sms.ed.ac.uk
Adam Lopez
HLTCOE
Johns Hopkins University
alopez@cs.jhu.edu
Abstract
We present a systematic comparison and com-
bination of two orthogonal techniques for
efficient parsing of Combinatory Categorial
Grammar (CCG). First we consider adap-
tive supertagging, a widely used approximate
search technique that prunes most lexical cat-
egories from the parser?s search space using
a separate sequence model. Next we con-
sider several variants on A*, a classic exact
search technique which to our knowledge has
not been applied to more expressive grammar
formalisms like CCG. In addition to standard
hardware-independent measures of parser ef-
fort we also present what we believe is the first
evaluation of A* parsing on the more realistic
but more stringent metric of CPU time. By it-
self, A* substantially reduces parser effort as
measured by the number of edges considered
during parsing, but we show that for CCG this
does not always correspond to improvements
in CPU time over a CKY baseline. Combining
A* with adaptive supertagging decreases CPU
time by 15% for our best model.
1 Introduction
Efficient parsing of Combinatorial Categorial Gram-
mar (CCG; Steedman, 2000) is a longstanding prob-
lem in computational linguistics. Even with practi-
cal CCG that are strongly context-free (Fowler and
Penn, 2010), parsing can be much harder than with
Penn Treebank-style context-free grammars, since
the number of nonterminal categories is generally
much larger, leading to increased grammar con-
stants. Where a typical Penn Treebank grammar
may have fewer than 100 nonterminals (Hocken-
maier and Steedman, 2002), we found that a CCG
grammar derived from CCGbank contained nearly
1600. The same grammar assigns an average of 26
lexical categories per word, resulting in a very large
space of possible derivations.
The most successful strategy to date for efficient
parsing of CCG is to first prune the set of lexi-
cal categories considered for each word, using the
output of a supertagger, a sequence model over
these categories (Bangalore and Joshi, 1999; Clark,
2002). Variations on this approach drive the widely-
used, broad coverage C&C parser (Clark and Cur-
ran, 2004; Clark and Curran, 2007). However, prun-
ing means approximate search: if a lexical category
used by the highest probability derivation is pruned,
the parser will not find that derivation (?2). Since the
supertagger enforces no grammaticality constraints
it may even prefer a sequence of lexical categories
that cannot be combined into any derivation (Fig-
ure 1). Empirically, we show that supertagging im-
proves efficiency by an order of magnitude, but the
tradeoff is a significant loss in accuracy (?3).
Can we improve on this tradeoff? The line of in-
vestigation we pursue in this paper is to consider
more efficient exact algorithms. In particular, we
test different variants of the classical A* algorithm
(Hart et al, 1968), which has met with success in
Penn Treebank parsing with context-free grammars
(Klein and Manning, 2003; Pauls and Klein, 2009a;
Pauls and Klein, 2009b). We can substitute A* for
standard CKY on either the unpruned set of lexi-
cal categories, or the pruned set resulting from su-
1577
Valid supertag-sequences
Valid parses
High scoring 
supertags 
High scoring 
parses
Desirable parses
Attainable parses
Figure 1: The relationship between supertagger and
parser search spaces based on the intersection of their cor-
responding tag sequences.
pertagging. Our empirical results show that on the
unpruned set of lexical categories, heuristics em-
ployed for context-free grammars show substantial
speedups in hardware-independent metrics of parser
effort (?4). To understand how this compares to the
CKY baseline, we conduct a carefully controlled set
of timing experiments. Although our results show
that improvements on hardware-independent met-
rics do not always translate into improvements in
CPU time due to increased processing costs that are
hidden by these metrics, they also show that when
the lexical categories are pruned using the output of
a supertagger, then we can still improve efficiency
by 15% with A* techniques (?5).
2 CCG and Parsing Algorithms
CCG is a lexicalized grammar formalism encoding
for each word lexical categories which are either
basic (eg. NN, JJ) or complex. Complex lexical
categories specify the number and directionality of
arguments. For example, one lexical category (of
over 100 in our model) for the transitive verb like is
(S\NP2)/NP1, specifying the first argument as an
NP to the right and the second as an NP to the left. In
parsing, adjacent spans are combined using a small
number of binary combinatory rules like forward ap-
plication or composition on the spanning categories
(Steedman, 2000; Fowler and Penn, 2010). In the
first derivation below, (S\NP )/NP and NP com-
bine to form the spanning category S\NP , which
only requires an NP to its left to form a complete
sentence-spanning S. The second derivation uses
type-raising to change the category type of I.
I like tea
NP (S\NP)/NP NP
>
S\NP
<
S
I like tea
NP (S\NP)/NP NP
>T
S/(S\NP)
>B
S/NP
>
S
Because of the number of lexical categories and their
complexity, a key difficulty in parsing CCG is that
the number of analyses for each span of the sentence
quickly becomes extremely large, even with efficient
dynamic programming.
2.1 Adaptive Supertagging
Supertagging (Bangalore and Joshi, 1999) treats the
assignment of lexical categories (or supertags) as a
sequence tagging problem. Once the supertagger
has been run, lexical categories that apply to each
word in the input sentence are pruned to contain only
those with high posterior probability (or other figure
of merit) under the supertagging model (Clark and
Curran, 2004). The posterior probabilities are then
discarded; it is the extensive pruning of lexical cate-
gories that leads to substantially faster parsing times.
Pruning the categories in advance this way has a
specific failure mode: sometimes it is not possible
to produce a sentence-spanning derivation from the
tag sequences preferred by the supertagger, since it
does not enforce grammaticality. A workaround for
this problem is the adaptive supertagging (AST) ap-
proach of Clark and Curran (2004). It is based on a
step function over supertagger beam ratios, relaxing
the pruning threshold for lexical categories when-
ever the parser fails to find an analysis. The pro-
cess either succeeds and returns a parse after some
iteration or gives up after a predefined number of
iterations. As Clark and Curran (2004) show, most
sentences can be parsed with a very small number of
supertags per word. However, the technique is inher-
ently approximate: it will return a lower probability
parse under the parsing model if a higher probabil-
ity parse can only be constructed from a supertag
sequence returned by a subsequent iteration. In this
way it prioritizes speed over accuracy, although the
tradeoff can be modified by adjusting the beam step
function.
2.2 A* Parsing
Irrespective of whether lexical categories are pruned
in advance using the output of a supertagger, the
CCG parsers we are aware of all use some vari-
1578
ant of the CKY algorithm. Although CKY is easy
to implement, it is exhaustive: it explores all pos-
sible analyses of all possible spans, irrespective of
whether such analyses are likely to be part of the
highest probability derivation. Hence it seems nat-
ural to consider exact algorithms that are more effi-
cient than CKY.
A* search is an agenda-based best-first graph
search algorithm which finds the lowest cost parse
exactly without necessarily traversing the entire
search space (Klein and Manning, 2003). In contrast
to CKY, items are not processed in topological order
using a simple control loop. Instead, they are pro-
cessed from a priority queue, which orders them by
the product of their inside probability and a heuris-
tic estimate of their outside probability. Provided
that the heuristic never underestimates the true out-
side probability (i.e. it is admissible) the solution is
guaranteed to be exact. Heuristics are model specific
and we consider several variants in our experiments
based on the CFG heuristics developed by Klein and
Manning (2003) and Pauls and Klein (2009a).
3 Adaptive Supertagging Experiments
Parser. For our experiments we used the generative
CCG parser of Hockenmaier and Steedman (2002).
Generative parsers have the property that all edge
weights are non-negative, which is required for A*
techniques.1 Although not quite as accurate as the
discriminative parser of Clark and Curran (2007) in
our preliminary experiments, this parser is still quite
competitive. It is written in Java and implements
the CKY algorithm with a global pruning threshold
of 10?4 for the models we use. We focus on two
parsing models: PCFG, the baseline of Hockenmaier
and Steedman (2002) which treats the grammar as a
PCFG (Table 1); and HWDep, a headword depen-
dency model which is the best performing model of
the parser. The PCFG model simply generates a tree
top down and uses very simple structural probabili-
ties while the HWDep model conditions node expan-
sions on headwords and their lexical categories.
Supertagger. For supertagging we used Den-
nis Mehay?s implementation, which follows Clark
1Indeed, all of the past work on A* parsing that we are aware of
uses generative parsers (Pauls and Klein, 2009b, inter alia).
(2002).2 Due to differences in smoothing of the
supertagging and parsing models, we occasionally
drop supertags returned by the supertagger because
they do not appear in the parsing model 3.
Evaluation. All experiments were conducted on
CCGBank (Hockenmaier and Steedman, 2007), a
right-most normal-form CCG version of the Penn
Treebank. Models were trained on sections 2-21,
tuned on section 00, and tested on section 23. Pars-
ing accuracy is measured using labelled and unla-
belled predicate argument structure recovery (Clark
and Hockenmaier, 2002); we evaluate on all sen-
tences and thus penalise lower coverage. All tim-
ing experiments reported in the paper were run on a
2.5 GHz Xeon machine with 32 GB memory and are
averaged over ten runs4.
3.1 Results
Supertagging has been shown to improve the speed
of a generative parser, although little analysis has
been reported beyond the speedups (Clark, 2002)
We ran experiments to understand the time/accuracy
tradeoff of adaptive supertagging, and to serve as
baselines.
Adaptive supertagging is parametrized by a beam
size ? and a dictionary cutoff k that bounds the
number of lexical categories considered for each
word (Clark and Curran, 2007). Table 3 shows both
the standard beam levels (AST) used for the C&C
parser and looser beam levels: AST-covA, a sim-
ple extension of AST with increased coverage and
AST-covB, also increasing coverage but with bet-
ter performance for the HWDep model.
Parsing results for the AST settings (Tables 4
and 5) confirm that it improves speed by an order of
magnitude over a baseline parser without AST. Per-
haps surprisingly, the number of parse failures de-
creases with AST in some cases. This is because the
parser prunes more aggressively as the search space
increases.5
2http://code.google.com/p/statopenccg
3Less than 2% of supertags are affected by this.
4The timing results reported differ from an earlier draft since
we used a different machine
5Hockenmaier and Steedman (2002) saw a similar effect.
1579
Expansion probability p(exp|P ) exp ? {leaf, unary, left-head, right-head}
Head probability p(H|P, exp) H is the head daughter
Non-head probability p(S|P, exp,H) S is the non-head daughter
Lexical probability p(w|P )
Table 1: Factorisation of the PCFG model. H ,P , and S are categories, and w is a word.
Expansion probability p(exp|P, cP#wP ) exp ? {leaf, unary, left-head, right-head}
Head probability p(H|P, exp, cP#wP ) H is the head daughter
Non-head probability p(S|P, exp,H#cP#wP ) S is the non-head daughter
Lexcat probability p(cS |S#P,H, S) p(cTOP |P=TOP )
Headword probability p(wS |cS#P,H, S,wP ) p(wTOP |cTOP )
Table 2: Headword dependency model factorisation, backoff levels are denoted by ?#? between conditioning variables:
A # B # C indicates that P? (. . . |A,B,C) is interpolated with P? (. . . |A,B), which is an interpolation of P? . . . |A,B)
and P? (. . . |A). Variables cP and wP represent, respectively, the head lexical category and headword of category P .
Condition Parameter Iteration 1 2 3 4 5 6
AST
? (beam width) 0.075 0.03 0.01 0.005 0.001
k (dictionary cutoff) 20 20 20 20 150
AST-covA
? 0.075 0.03 0.01 0.005 0.001 0.0001
k 20 20 20 20 150 150
AST-covB
? 0.03 0.01 0.005 0.001 0.0001 0.0001
k 20 20 20 20 20 150
Table 3: Beam step function used for standard (AST) and high-coverage (AST-covA and AST-covB) supertagging.
Time(sec) Sent/sec Cats/word Fail UP UR UF LP LR LF
PCFG 290 6.6 26.2 5 86.4 86.5 86.5 77.2 77.3 77.3
PCFG (AST) 65 29.5 1.5 14 87.4 85.9 86.6 79.5 78.0 78.8
PCFG (AST-covA) 67 28.6 1.5 6 87.3 86.9 87.1 79.1 78.8 78.9
PCFG (AST-covB) 69 27.7 1.7 5 87.3 86.2 86.7 79.1 78.1 78.6
HWDep 1512 1.3 26.2 5 90.2 90.1 90.2 83.2 83.0 83.1
HWDep (AST) 133 14.4 1.5 16 89.8 88.0 88.9 82.6 80.9 81.8
HWDep (AST-covA) 139 13.7 1.5 9 89.8 88.3 89.0 82.6 81.1 81.9
HWDep (AST-covB) 155 12.3 1.7 7 90.1 88.7 89.4 83.0 81.8 82.4
Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative
CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall
(LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
3.2 Efficiency versus Accuracy
The most interesting result is the effect of the
speedup on accuracy. As shown in Table 6, the
vast majority of sentences are actually parsed with
a very tight supertagger beam, raising the question
of whether many higher-scoring parses are pruned.6
6Similar results are reported by Clark and Curran (2007).
Despite this, labeled F-score improves by up to 1.6
F-measure for the PCFG model, although it harms
accuracy for HWDep as expected.
In order to understand this effect, we filtered sec-
tion 00 to include only sentences of between 18
and 26 words (resulting in 610 sentences) for which
1580
Time(sec) Sent/sec Cats/word Fail UP UR UF LP LR LF
PCFG 326 7.4 25.7 29 85.9 85.4 85.7 76.6 76.2 76.4
PCFG (AST) 82 29.4 1.5 34 86.7 84.8 85.7 78.6 76.9 77.7
PCFG (AST-covA) 85 28.3 1.6 15 86.6 85.5 86.0 78.5 77.5 78.0
PCFG (AST-covB) 86 27.9 1.7 14 86.6 85.6 86.1 78.1 77.3 77.7
HWDep 1754 1.4 25.7 30 90.2 89.3 89.8 83.5 82.7 83.1
HWDep (AST) 167 14.4 1.5 27 89.5 87.6 88.5 82.3 80.6 81.5
HWDep (AST-covA) 177 13.6 1.6 14 89.4 88.1 88.8 82.2 81.1 81.7
HWDep (AST-covB) 188 12.8 1.7 14 89.7 88.5 89.1 82.5 81.4 82.0
Table 5: Results on CCGbank section 23 when applying adaptive supertagging (AST) to two models of a CCG parser.
? Cats/word Parses %
0.075 1.33 1676 87.6
0.03 1.56 114 6.0
0.01 1.97 60 3.1
0.005 2.36 15 0.8
0.001k=150 3.84 32 1.7
Fail 16 0.9
Table 6: Breakdown of the number of sentences parsed
for the HWDep (AST) model (see Table 4) at each of
the supertagger beam levels from the most to the least
restrictive setting.
we can perform exhaustive search without pruning7,
and for which we could parse without failure at all
of the tested beam settings. We then measured the
log probability of the highest probability parse found
under a variety of beam settings, relative to the log
probability of the unpruned exact parse, along with
the labeled F-Score of the Viterbi parse under these
settings (Figure 2). The results show that PCFG ac-
tually finds worse results as it considers more of the
search space. In other words, the supertagger can ac-
tually ?fix? a bad parsing model by restricting it to
a small portion of the search space. With the more
accurate HWDep model, this does not appear to be
a problem and there is a clear opportunity for im-
provement by considering the larger search space.
The next question is whether we can exploit this
larger search space without paying as high a cost in
efficiency.
7The fact that only a subset of short sentences could be exhaus-
tively parsed demonstrates the need for efficient search algo-
rithms.
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
87	 ?
88	 ?
95.0	 ?
95.5	 ?
96.0	 ?
96.5	 ?
97.0	 ?
97.5	 ?
98.0	 ?
98.5	 ?
99.0	 ?
99.5	 ?
100.0	 ?
0.07
5	 ?
0.03
0	 ?
0.01
0	 ?
0.00
5	 ?
0.00
1	 ?
0.00
1???	 ? 0.00
01	 ?
0.00
01??
?	 ?
exac
t	 ?
Lab
ele
d	 ?F
-??sc
ore
	 ?
%	 ?o
f	 ?o
p?
ma
l	 ?Lo
g	 ?P
rob
abi
lity
	 ?
Supertagger	 ?beam	 ?
PCFG	 ?Log	 ?Probability	 ?
HWDep	 ?Log	 ?Probability	 ?
PCFG	 ?F-??score	 ?
HWDep	 ?F-??score	 ?
Figure 2: Log-probability of parses relative to exact solu-
tion vs. labelled F-score at each supertagging beam-level.
4 A* Parsing Experiments
To compare approaches, we extended our baseline
parser to support A* search. Following (Klein and
Manning, 2003) we restrict our experiments to sen-
tences on which we can perform exact search via us-
ing the same subset of section 00 as in ?3.2. Before
considering CPU time, we first evaluate the amount
of work done by the parser using three hardware-
independent metrics. We measure the number of
edges pushed (Pauls and Klein, 2009a) and edges
popped, corresponding to the insert/decrease-key
operations and remove operation of the priority
queue, respectively. Finally, we measure the num-
ber of traversals, which counts the number of edge
weights computed, regardless of whether the weight
is discarded due to the prior existence of a better
weight. This latter metric seems to be the most ac-
curate account of the work done by the parser.
Due to differences in the PCFG and HWDep mod-
els, we considered different A* variants: for the
PCFG model we use a simple A* with a precom-
1581
puted heuristic, while for the the more complex
HWDep model, we used a hierarchical A* algo-
rithm (Pauls and Klein, 2009a; Felzenszwalb and
McAllester, 2007) based on a simple grammar pro-
jection that we designed.
4.1 Hardware-Independent Results: PCFG
For the PCFG model, we compared three agenda-
based parsers: EXH prioritizes edges by their span
length, thereby simulating the exhaustive CKY algo-
rithm; NULL prioritizes edges by their inside proba-
bility; and SX is an A* parser that prioritizes edges
by their inside probability times an admissible out-
side probability estimate.8 We use the SX estimate
devised by Klein and Manning (2003) for CFG pars-
ing, where they found it offered very good perfor-
mance for relatively little computation. It gives a
bound on the outside probability of a nonterminal P
with i words to the right and j words to the left, and
can be computed from a grammar using a simple dy-
namic program.
The parsers are tested with and without adap-
tive supertagging where the former can be seen as
performing exact search (via A*) over the pruned
search space created by AST.
Table 7 shows that A* with the SX heuristic de-
creases the number of edges pushed by up to 39%
on the unpruned search space. Although encourag-
ing, this is not as impressive as the 95% speedup
obtained by Klein and Manning (2003) with this
heuristic on their CFG. On the other hand, the NULL
heuristic works better for CCG than for CFG, with
speedups of 29% and 11%, respectively. These re-
sults carry over to the AST setting which shows that
A* can improve search even on the highly pruned
search graph. Note that A* only saves work in the
final iteration of AST, since for earlier iterations it
must process the entire agenda to determine that
there is no spanning analysis.
Since there are many more categories in the CCG
grammar we might have expected the SX heuristic to
work better than for a CFG. Why doesn?t it? We can
measure how well a heuristic bounds the true cost in
8The NULL parser is a special case of A*, also called uni-
form cost search, which in the case of parsing corresponds to
Knuth?s algorithm (Knuth, 1977; Klein and Manning, 2001),
the extension of Dijkstra?s algorithm to hypergraphs.
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
0.8	 ?
1	 ? 4	 ? 7	 ? 10	 ? 13	 ? 16	 ? 19	 ? 22	 ? 25	 ?
Av
era
ge
	 ?Sl
ac
k	 ?
Outside	 ?Span	 ?
Figure 3: Average slack of the SX heuristic. The figure
aggregates the ratio of the difference between the esti-
mated outside cost and true outside costs relative to the
true cost across the development set.
terms of slack: the difference between the true and
estimated outside cost. Lower slack means that the
heuristic bounds the true cost better and guides us to
the exact solution more quickly. Figure 3 plots the
average slack for the SX heuristic against the num-
ber of words in the outside context. Comparing this
with an analysis of the same heuristic when applied
to a CFG by Klein and Manning (2003), we find that
it is less effective in our setting9. There is a steep
increase in slack for outside contexts with size more
than one. The main reason for this is because a sin-
gle word in the outside context is in many cases the
full stop at the end of the sentence, which is very pre-
dictable. However for longer spans the flexibility of
CCG to analyze spans in many different ways means
that the outside estimate for a nonterminal can be
based on many high probability outside derivations
which do not bound the true probability very well.
4.2 Hardware-Independent Results: HWDep
Lexicalization in the HWDep model makes the pre-
computed SX estimate impractical, so for this model
we designed two hierarchical A* (HA*) variants
based on simple grammar projections of the model.
The basic idea of HA* is to compute Viterbi in-
side probabilities using the easier-to-parse projected
9Specifically, we refer to Figure 9 of their paper which uses a
slightly different representation of estimate sharpness
1582
Parser Edges pushed Edges popped Traversals
Std % AST % Std % AST % Std % AST %
EXH 34 100 6.3 100 15.7 100 4.2 100 133.4 100 13.3 100
NULL 24.3 71 4.9 78 13.5 86 3.5 83 113.8 85 11.1 83
SX 20.9 61 4.3 68 10.0 64 2.6 62 96.5 72 9.7 73
Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges
pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
grammar, use these to compute Viterbi outside prob-
abilities for the simple grammar, and then use these
as outside estimates for the true grammar; all com-
putations are prioritized in a single agenda follow-
ing the algorithm of Felzenszwalb and McAllester
(2007) and Pauls and Klein (2009a). We designed
two simple grammar projections, each simplifying
the HWDep model: PCFGProj completely re-
moves lexicalization and projects the grammar to
a PCFG, while as LexcatProj removes only the
headwords but retains the lexical categories.
Figure 4 compares exhaustive search, A* with no
heuristic (NULL), and HA*. For HA*, parsing ef-
fort is broken down into the different edge types
computed at each stage: We distinguish between the
work carried out to compute the inside and outside
edges of the projection, where the latter represent
the heuristic estimates, and finally, the work to com-
pute the edges of the target grammar. We find that
A* NULL saves about 44% of edges pushed which
makes it slightly more effective than for the PCFG
model. However, the effort to compute the gram-
mar projections outweighs their benefit. We suspect
that this is due to the large difference between the
target grammar and the projection: The PCFG pro-
jection is a simple grammar and so we improve the
probability of a traversal less often than in the target
grammar.
The Lexcat projection performs worst, for two
reasons. First, the projection requires about as much
work to compute as the target grammar without a
heuristic (NULL). Second, the projection itself does
not save a large amount of work as can be seen in
the statistics for the target grammar.
5 CPU Timing Experiments
Hardware-independent metrics are useful for under-
standing agenda-based algorithms, but what we ac-
tually care about is CPU time. We were not aware of
any past work that measures A* parsers in terms of
CPU time, but as this is the real objective we feel that
experiments of this type are important. This is espe-
cially true in real implementations because the sav-
ings in edges processed by an agenda parser come at
a cost: operations on the priority queue data struc-
ture can add significant runtime.
Timing experiments of this type are very
implementation-dependent, so we took care to im-
plement the algorithms as cleanly as possible and
to reuse as much of the existing parser code as we
could. An important implementation decision for
agenda-based algorithms is the data structure used
to implement the priority queue. Preliminary experi-
ments showed that a Fibonacci heap implementation
outperformed several alternatives: Brodal queues
(Brodal, 1996), binary heaps, binomial heaps, and
pairing heaps.10
We carried out timing experiments on the best A*
parsers for each model (SX and NULL for PCFG and
HWDep, respectively), comparing them with our
CKY implementation and the agenda-based CKY
simulation EXH; we used the same data as in ?3.2.
Table 8 presents the cumulative running times with
and without adaptive supertagging average over ten
runs, while Table 9 reports F-scores.
The results (Table 8) are striking. Although the
timing results of the agenda-based parsers track the
hardware-independent metrics, they start at a signif-
icant disadvantage to exhaustive CKY with a sim-
ple control loop. This is most evident when looking
at the timing results for EXH, which in the case of
the full PCFG model requires more than twice the
time than the CKY algorithm that it simulates. A*
10We used the Fibonacci heap implementation at
http://www.jgrapht.org
1583
Figure 4: Comparsion between a CKY simulation (EXH), A* with no heuristic (NULL), hierarchical A* (HA*) using
two grammar projections for standard search (left) and AST (right). The breakdown of the inside/outside edges for the
grammar projection as well as the target grammar shows that the projections, serving as the heuristic estimates for the
target grammar, are costly to compute.
Standard AST
PCFG HWDep PCFG HWDep
CKY 536 24489 34 143
EXH 1251 26889 41 155
A* NULL 1032 21830 36 121
A* SX 889 - 34 -
Table 8: Parsing time in seconds of CKY and agenda-
based parsers with and without adaptive supertagging.
Standard AST
PCFG HWDep PCFG HWDep
CKY 80.4 85.5 81.7 83.8
EXH 79.4 85.5 80.3 83.8
A* NULL 79.6 85.5 80.7 83.8
A* SX 79.4 - 80.4 -
Table 9: Labelled F-score of exact CKY and agenda-
based parsers with/without adaptive supertagging. All
parses have the same probabilities, thus variances are due
to implementation-dependent differences in tiebreaking.
makes modest CPU-time improvements in parsing
the full space of the HWDep model. Although this
decreases the time required to obtain the highest ac-
curacy, it is still a substantial tradeoff in speed com-
pared with AST.
On the other hand, the AST tradeoff improves sig-
nificantly: by combining AST with A* we observe
a decrease in running time of 15% for the A* NULL
parser of the HWDep model over CKY. As the CKY
baseline with AST is very strong, this result shows
that A* holds real promise for CCG parsing.
6 Conclusion and Future Work
Adaptive supertagging is a strong technique for ef-
ficient CCG parsing. Our analysis confirms tremen-
dous speedups, and shows that for weak models, it
can even result in improved accuracy. However, for
better models, the efficiency gains of adaptive su-
pertagging come at the cost of accuracy. One way to
look at this is that the supertagger has good precision
with respect to the parser?s search space, but low re-
call. For instance, we might combine both parsing
and supertagging models in a principled way to ex-
ploit these observations, eg. by making the supertag-
ger output a soft constraint on the parser rather than
a hard constraint. Principled, efficient search algo-
rithms will be crucial to such an approach.
To our knowledge, we are the first to measure
A* parsing speed both in terms of running time and
commonly used hardware-independent metrics. It
is clear from our results that the gains from A* do
not come as easily for CCG as for CFG, and that
agenda-based algorithms like A* must make very
large reductions in the number of edges processed
to result in realtime savings, due to the added ex-
pense of keeping a priority queue. However, we
1584
have shown that A* can yield real improvements
even over the highly optimized technique of adaptive
supertagging: in this pruned search space, a 44%
reduction in the number of edges pushed results in
a 15% speedup in CPU time. Furthermore, just as
A* can be combined with adaptive supertagging, it
should also combine easily with other search-space
pruning methods, such as those of Djordjevic et
al. (2007), Kummerfeld et al (2010), Zhang et al
(2010) and Roark and Hollingshead (2009). In fu-
ture work we plan to examine better A* heuristics
for CCG, and to look at principled approaches to
combine the strengths of A*, adaptive supertagging,
and other techniques to the best advantage.
Acknowledgements
We would like to thank Prachya Boonkwan, Juri
Ganitkevitch, Philipp Koehn, Tom Kwiatkowski,
Matt Post, Mark Steedman, Emily Thomforde, and
Luke Zettlemoyer for helpful discussion related to
this work and comments on previous drafts; Julia
Hockenmaier for furnishing us with her parser; and
the anonymous reviewers for helpful commentary.
We also acknowledge funding from EPSRC grant
EP/P504171/1 (Auli); the EuroMatrixPlus project
funded by the European Commission, 7th Frame-
work Programme (Lopez); and the resources pro-
vided by the Edinburgh Compute and Data Fa-
cility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
References
S. Bangalore and A. K. Joshi. 1999. Supertagging: An
Approach to Almost Parsing. Computational Linguis-
tics, 25(2):238?265, June.
G. S. Brodal. 1996. Worst-case efficient priority queues.
In Proc. of SODA, pages 52?58.
S. Clark and J. R. Curran. 2004. The importance of su-
pertagging for wide-coverage CCG parsing. In Proc.
of COLING.
S. Clark and J. R. Curran. 2007. Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4):493?552.
S. Clark and J. Hockenmaier. 2002. Evaluating a wide-
coverage CCG parser. In Proc. of LREC Beyond Par-
seval Workshop, pages 60?66.
S. Clark. 2002. Supertagging for Combinatory Catego-
rial Grammar. In Proc. of TAG+6, pages 19?24.
B. Djordjevic, J. R. Curran, and S. Clark. 2007. Improv-
ing the efficiency of a wide-coverage CCG parser. In
Proc. of IWPT.
P. F. Felzenszwalb and D. McAllester. 2007. The Gener-
alized A* Architecture. In Journal of Artificial Intelli-
gence Research, volume 29, pages 153?190.
T. A. D. Fowler and G. Penn. 2010. Accurate context-
free parsing with combinatory categorial grammar. In
Proc. of ACL.
P. Hart, N. Nilsson, and B. Raphael. 1968. A formal
basis for the heuristic determination of minimum cost
paths. Transactions on Systems Science and Cybernet-
ics, 4, Jul.
J. Hockenmaier and M. Steedman. 2002. Generative
models for statistical parsing with Combinatory Cat-
egorial Grammar. In Proc. of ACL, pages 335?342.
Association for Computational Linguistics.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
D. Klein and C. D. Manning. 2001. Parsing and hyper-
graphs. In Proc. of IWPT.
D. Klein and C. D. Manning. 2003. A* parsing: Fast
exact Viterbi parse selection. In Proc. of HLT-NAACL,
pages 119?126, May.
D. E. Knuth. 1977. A generalization of Dijkstra?s algo-
rithm. Information Processing Letters, 6:1?5.
J. K. Kummerfeld, J. Roesner, T. Dawborn, J. Haggerty,
J. R. Curran, and S. Clark. 2010. Faster parsing by
supertagger adaptation. In Proc. of ACL.
A. Pauls and D. Klein. 2009a. Hierarchical search for
parsing. In Proc. of HLT-NAACL, pages 557?565,
June.
A. Pauls and D. Klein. 2009b. k-best A* Parsing. In
Proc. of ACL-IJCNLP, ACL-IJCNLP ?09, pages 958?
966.
B. Roark and K. Hollingshead. 2009. Linear complexity
context-free parsing pipelines via chart constraints. In
Proc. of HLT-NAACL.
M. Steedman. 2000. The syntactic process. MIT Press,
Cambridge, MA.
Y. Zhang, B.-G. Ahn, S. Clark, C. V. Wyk, J. R. Cur-
ran, and L. Rimell. 2010. Chart pruning for fast
lexicalised-grammar parsing. In Proc. of COLING.
1585
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 136?142,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Decoder Integration and Expected BLEU Training
for Recurrent Neural Network Language Models
Michael Auli
Microsoft Research
Redmond, WA, USA
michael.auli@microsoft.com
Jianfeng Gao
Microsoft Research
Redmond, WA, USA
jfgao@microsoft.com
Abstract
Neural network language models are often
trained by optimizing likelihood, but we
would prefer to optimize for a task specific
metric, such as BLEU in machine trans-
lation. We show how a recurrent neural
network language model can be optimized
towards an expected BLEU loss instead
of the usual cross-entropy criterion. Fur-
thermore, we tackle the issue of directly
integrating a recurrent network into first-
pass decoding under an efficient approxi-
mation. Our best results improve a phrase-
based statistical machine translation sys-
tem trained on WMT 2012 French-English
data by up to 2.0 BLEU, and the expected
BLEU objective improves over a cross-
entropy trained model by up to 0.6 BLEU
in a single reference setup.
1 Introduction
Neural network-based language and translation
models have achieved impressive accuracy im-
provements on statistical machine translation tasks
(Allauzen et al, 2011; Le et al, 2012b; Schwenk
et al, 2012; Vaswani et al, 2013; Gao et al, 2014).
In this paper we focus on recurrent neural network
architectures which have recently advanced the
state of the art in language modeling (Mikolov et
al., 2010; Mikolov et al, 2011; Sundermeyer et al,
2013) with several subsequent applications in ma-
chine translation (Auli et al, 2013; Kalchbrenner
and Blunsom, 2013; Hu et al, 2014). Recurrent
models have the potential to capture long-span de-
pendencies since their predictions are based on an
unbounded history of previous words (?2).
In practice, neural network models for machine
translation are usually trained by maximizing the
likelihood of the training data, either via a cross-
entropy objective (Mikolov et al, 2010; Schwenk
et al, 2012) or more recently, noise-contrastive es-
timation (Vaswani et al, 2013). However, it is
widely appreciated that directly optimizing for a
task-specific metric often leads to better perfor-
mance (Goodman, 1996; Och, 2003; Auli and
Lopez, 2011). The expected BLEU objective pro-
vides an efficient way of achieving this for ma-
chine translation (Rosti et al, 2010; Rosti et al,
2011; He and Deng, 2012; Gao and He, 2013;
Gao et al, 2014) instead of solely relying on tra-
ditional optimizers such as Minimum Error Rate
Training (MERT) that only adjust the weighting
of entire component models within the log-linear
framework of machine translation (?3).
Most previous work on neural networks for ma-
chine translation is based on a rescoring setup
(Arisoy et al, 2012; Mikolov, 2012; Le et al,
2012a; Auli et al, 2013), thereby side stepping
the algorithmic and engineering challenges of di-
rect decoder-integration. One recent exception is
Vaswani et al (2013) who demonstrated that feed-
forward network-based language models are more
accurate in first-pass decoding than in rescoring.
Decoder integration has the advantage for the neu-
ral network to directly influence search, unlike
rescoring which is restricted to an n-best list or lat-
tice. Decoding with feed-forward architectures is
straightforward, since predictions are based on a
fixed size input, similar to n-gram language mod-
els. However, for recurrent networks we have to
deal with the unbounded history, which breaks the
usual dynamic programming assumptions for effi-
cient search. We show how a simple but effective
approximation can side step this issue and we em-
pirically demonstrate its effectiveness (?4).
We test the expected BLEU objective by train-
ing a recurrent neural network language model
and obtain substantial improvements. We also find
that our efficient approximation for decoder inte-
gration is very accurate, clearly outperforming a
rescoring setup (?5).
136
wt
ht-1
ht
yt
V
W
U
0
0
1
0
0
0
Figure 1: Structure of the recurrent neural network
language model.
2 Recurrent Neural Network LMs
Our model has a similar structure to the recurrent
neural network language model of Mikolov et al
(2010) which is factored into an input layer, a hid-
den layer with recurrent connections, and an out-
put layer (Figure 1). The input layer encodes the
word at position t as a 1-of-N vector w
t
. The out-
put layer y
t
represents scores over possible next
words; both the input and output layers are of size
|V |, the size of the vocabulary. The hidden layer
state h
t
encodes the history of all words observed
in the sequence up to time step t. The state of
the hidden layer is determined by the input layer
and the hidden layer configuration of the previous
time step h
t?1
. The weights of the connections
between the layers are summarized in a number
of matrices: U represents weights from the in-
put layer to the hidden layer, and W represents
connections from the previous hidden layer to the
current hidden layer. Matrix V contains weights
between the current hidden layer and the output
layer. The activations of the hidden and output
layers are computed by:
h
t
= tanh(Uw
t
+ Wh
t?1
)
y
t
= tanh(Vh
t
)
Different to previous work (Mikolov et al, 2010),
we do not use the softmax activation function to
output a probability over the next word, but in-
stead just compute a single unnormalized score.
This is computationally more efficient than sum-
ming over all possible outputs such as required
for the cross-entropy error function (Bengio et al,
2003; Mikolov et al, 2010; Schwenk et al, 2012).
Training is based on the back propagation through
time algorithm, which unrolls the network and
then computes error gradients over multiple time
steps (Rumelhart et al, 1986); we use the expected
BLEU loss (?3) to obtain the error with respect to
the output activations. After training, the output
layer represents scores s(w
t+1
|w
1
. . . w
t
,h
t
) for
the next word given the previous t input words and
the current hidden layer configuration h
t
.
3 Expected BLEU Training
We integrate the recurrent neural network lan-
guage model as an additional feature into the stan-
dard log-linear framework of translation (Och,
2003). Formally, our phrase-based model is pa-
rameterized by M parameters ? where each ?
m
?
?, m = 1 . . .M is the weight of an associated
feature h
m
(f, e). Function h(f, e) maps foreign
sentences f and English sentences e to the vector
h
1
(f, e) . . . (f, e), and the model chooses transla-
tions according to the following decision rule:
e? = arg max
e?E(f)
?
T
h(f, e)
We summarize the weights of the recurrent neural
network language model as ? = {U,W,V} and
add the model as an additional feature to the log-
linear translation model using the simplified nota-
tion s
?
(w
t
) = s(w
t
|w
1
. . . w
t?1
,h
t?1
):
h
M+1
(e) = s
?
(e) =
|e|
?
t=1
log s
?
(w
t
) (1)
which computes a sentence-level language model
score as the sum of individual word scores. The
translation model is parameterized by ? and ?
which are learned as follows (Gao et al, 2014):
1. We generate an n-best list for each foreign
sentence in the training data with the baseline
translation system given ? where ?
M+1
= 0
using the settings described in ?5. The n-best
lists serve as an approximation to E(f) used
in the next step for expected BLEU training
of the recurrent neural network model (?3.1).
2. Next, we fix ?, set ?
M+1
= 1 and opti-
mize ? with respect to the loss function on
the training data using stochastic gradient de-
scent (SGD).
1
1
We tuned ?
M+1
on the development set but found that
?
M+1
= 1 resulted in faster training and equal accuracy.
137
3. We fix ? and re-optimize ? in the presence
of the recurrent neural network model using
Minimum Error Rate Training (Och, 2003)
on the development set (?5).
3.1 Expected BLEU Objective
Formally, we define our loss function l(?) as
the negative expected BLEU score, denoted as
xBLEU(?) for a given foreign sentence f :
l(?) =? xBLEU(?)
=
?
e?E(f)
p
?,?
(e|f)sBLEU(e, e
(i)
) (2)
where sBLEU(e, e
(i)
) is a smoothed sentence-
level BLEU score with respect to the reference
translation e
(i)
, and E(f) is the generation set
given by an n-best list.
2
We use a sentence-level
BLEU approximation similar to He and Deng
(2012).
3
The normalized probability p
?,?
(e|f) of
a particular translation e given f is defined as:
p
?,?
(e|f) =
exp{??
T
h(f, e)}
?
e
?
?E(f)
exp{??
T
h(f, e
?
)}
(3)
where ?
T
h(f, e) includes the recurrent neural net-
work h
M+1
(e), and ? ? [0, inf) is a scaling factor
that flattens the distribution for ? < 1 and sharp-
ens it for ? > 1 (Tromble et al, 2008).
4
Next, we define the gradient of the expected
BLEU loss function l(?) using the observation that
the loss does not explicitly depend on ?:
?l(?)
??
=
?
e
|e|
?
t=1
?l(?)
?s
?
(w
t
)
?s
?
(w
t
)
??
=
?
e
|e|
?
t=1
??
w
t
?s
?
(w
t
)
??
where ?
w
t
is the error term for English word w
t
.
5
The error term indicates how the loss changes with
the translation probability which we derive next.
6
2
Our definitions do not take into account multiple derivations
for the same translation because our n-best lists contain only
unique entries which we obtain by choosing the highest scor-
ing translation among string identical candidates.
3
In early experiments we found that the BLEU+1 approxi-
mation used by Liang et al (2006) and Nakov et. al (2012)
worked equally well in our setting.
4
The ? parameter is only used during expected BLEU training
but not for subsequent MERT tuning.
5
A sentence may contain the same word multiple times and
we compute the error term for each occurrence separately
since the error depends on the individual history.
6
We omit the gradient of the recurrent neural network score
?s
?
(w
t
)
??
since it follows the standard form (Mikolov, 2012).
3.2 Derivation of the Error Term ?
w
t
We rewrite the loss function (2) using (3) and sep-
arate it into two terms G(?) and Z(?) as follows:
l(?) = ?xBLEU(?) = ?
G(?)
Z(?)
(4)
= ?
?
e?E(f)
exp{??
T
h(f, e)} sBLEU(e, e
(i)
)
?
e?E(f)
exp{??
T
h(f, e)}
Next, we apply the quotient rule of differentiation:
?
w
t
=
?xBLEU(?)
?s
?
(w
t
)
=
?(G(?)/Z(?))
?s
?
(w
t
)
=
1
Z(?)
(
?G(?)
?s
?
(w
t
)
?
?Z(?)
?s
?
(w
t
)
xBLEU(?)
)
Using the observation that ? is only relevant to the
recurrent neural network h
M+1
(e) (1) we have
???
T
h(f, e)
?s
?
(w
t
)
= ??
M+1
?h
M+1
(e)
?s
?
(w
t
)
=
??
M+1
s
?
(w
t
)
which together with the chain rule, (3) and (4) al-
lows us to rewrite ?
w
t
as follows:
?
w
t
=
1
Z(?)
?
e?E(f),
s.t.w
t
?e
(
? exp{??
T
h(f, e)}
?s
?
(w
t
)
U(?, e)
)
=
?
e?E(f),
s.t.w
t
?e
(
p
?,?
(e|f)U(?, e)?
M+1
?
s
?
(w
t
)
)
where U(?, e) = sBLEU(e, e
i
)? xBLEU(?).
4 Decoder Integration
Directly integrating our recurrent neural network
language model into first-pass decoding enables us
to search a much larger space than would be pos-
sible in rescoring.
Typically, phrase-based decoders maintain a set
of states representing partial and complete transla-
tion hypothesis that are scored by a set of features.
Most features are local, meaning that all required
information for them to assign a score is available
within the state. One exception is the n-gram lan-
guage model which requires the preceding n ? 1
words as well. In order to accommodate this fea-
ture, each state usually keeps these words as con-
text. Unfortunately, a recurrent neural network
makes even weaker independence assumptions so
138
that it depends on the entire left prefix of a sen-
tence. Furthermore, the weaker independence as-
sumptions also dramatically reduce the effective-
ness of dynamic programming by allowing much
fewer states to be recombined.
7
To solve this problem, we follow previous work
on lattice rescoring with recurrent networks that
maintained the usual n-gram context but kept a
beam of hidden layer configurations at each state
(Auli et al, 2013). In fact, to make decoding as
efficient as possible, we only keep the single best
scoring hidden layer configuration. This approx-
imation has been effective for lattice rescoring,
since the translations represented by each state are
in fact very similar: They share both the same
source words as well as the same n-gram context
which is likely to result in similar recurrent his-
tories that can be safely pruned. As future cost
estimate we score each phrase in isolation, reset-
ting the hidden layer at the beginning of a phrase.
While simple, we found our estimate to be more
accurate than no future cost at all.
5 Experiments
Baseline. We use a phrase-based system simi-
lar to Moses (Koehn et al, 2007) based on a set
of common features including maximum likeli-
hood estimates p
ML
(e|f) and p
ML
(f |e), lexically
weighted estimates p
LW
(e|f) and p
LW
(f |e),
word and phrase-penalties, a hierarchical reorder-
ing model (Galley and Manning, 2008), a linear
distortion feature, and a modified Kneser-Ney lan-
guage model trained on the target-side of the paral-
lel data. Log-linear weights are tuned with MERT.
Evaluation. We use training and test data from
the WMT 2012 campaign and report results on
French-English and German-English. Transla-
tion models are estimated on 102M words of par-
allel data for French-English, and 99M words
for German-English; about 6.5M words for each
language pair are newswire, the remainder are
parliamentary proceedings. We evaluate on six
newswire domain test sets from 2008 to 2013 con-
taining between 2034 to 3003 sentences. Log-
linear weights are estimated on the 2009 data set
comprising 2525 sentences. We evaluate accuracy
in terms of BLEU with a single reference.
Rescoring Setup. For rescoring we use ei-
7
Recombination only retains the highest scoring state if there
are multiple identical states, that is, they cover the same
source span, the same translation phrase and contexts.
ther lattices or the unique 100-best output of
the phrase-based decoder and re-estimate the log-
linear weights by running a further iteration of
MERT on the n-best list of the development set,
augmented by scores corresponding to the neural
network models. At test time we rescore n-best
lists with the new weights.
Neural Network Training. All neural network
models are trained on the news portion of the
parallel data, corresponding to 136K sentences,
which we found to be most useful in initial exper-
iments. As training data we use unique 100-best
lists generated by the baseline system. We use the
same data both for training the phrase-based sys-
tem as well as the language model but find that
the resulting bias did not hurt end-to-end accu-
racy (Yu et al, 2013). The vocabulary consists of
words that occur in at least two different sentences,
which is 31K words for both language pairs. We
tuned the learning rate ? of our mini-batch SGD
trainer as well as the probability scaling parameter
? (3) on a held-out set and found simple settings of
? = 0.1 and ? = 1 to be good choices. To prevent
over-fitting, we experimented with L2 regulariza-
tion, but found no accuracy improvements, prob-
ably because SGD regularizes enough. We evalu-
ate performance on a held-out set during training
and stop whenever the objective changes less than
0.0003. The hidden layer uses 100 neurons unless
otherwise stated.
5.1 Decoder Integration
We compare the effect of direct decoder integra-
tion to rescoring with both lattices and n-best lists
when the model is trained with a cross-entropy ob-
jective (Mikolov et al, 2010). The results (Ta-
ble 1 and Table 2) show that direct integration im-
proves accuracy across all six test sets on both lan-
guage pairs. For French-English we improve over
n-best rescoring by up to 1.1 BLEU and by up to
0.5 BLEU for German-English. We improve over
lattice rescoring by up to 0.4 BLEU on French-
English and by up to 0.3 BLEU on German-
English. Compared to the baseline, we achieve
improvements of up to 2.0 BLEU for French-
English and up to 1.3 BLEU for German-English.
The average improvement across all test sets is
1.5 BLEU for French-English and 1.0 BLEU for
German-English compared to the baseline.
139
dev 2008 2010 syscomb2010 2011 2012 2013 AllTest
Baseline 24.11 20.73 24.68 24.59 25.62 24.85 25.54 24.53
RNN n-best rescore 24.83 21.41 25.17 25.06 26.53 25.74 26.31 25.25
RNN lattice rescore 24.91 21.73 25.56 25.43 27.04 26.43 26.75 25.72
RNN decode 25.14 22.03 25.86 25.74 27.32 26.86 27.15 26.06
Table 1: French-English accuracy of decoder integration of a recurrent neural network language model
(RNN decode) compared to n-best and lattice rescoring as well as the output of a phrase-based system
using an n-gram model (Baseline); Alltest is the corpus-weighted average BLEU across all test sets.
dev 2008 2010 syscomb2010 2011 2012 2013 AllTest
Baseline 19.35 19.96 20.87 20.66 19.60 19.80 22.48 20.58
RNN n-best rescore 20.17 20.29 21.35 21.27 20.51 20.54 23.03 21.21
RNN lattice rescore 20.24 20.38 21.55 21.43 20.77 20.63 23.23 21.38
RNN decode 20.13 20.51 21.79 21.71 20.91 20.93 23.53 21.61
Table 2: German-English results of direct decoder integration (cf. Table 1).
dev 2008 2010 syscomb2010 2011 2012 2013 AllTest
Baseline 24.11 20.73 24.68 24.59 25.62 24.85 25.54 24.53
CE RNN 24.80 21.15 25.14 25.06 26.45 25.83 26.69 25.29
+ xBLEU RNN 25.11 21.74 25.52 25.42 27.06 26.42 26.72 25.71
Table 3: French-English accuracy of a decoder integrated cross-entropy recurrent neural network model
(CE RNN) and a combination with an expected BLEU trained model (xBLEU RNN). Results are not
comparable to Table 1 since a smaller hidden layer was used to keep training times manageable (?5.2).
5.2 Expected BLEU Training
Training with the expected BLEU loss is compu-
tationally more expensive than with cross-entropy
since each training example is an n-best list in-
stead of a single sentence. This increases the num-
ber of words to be processed from 3.5M to 340M.
To keep training times manageable, we reduce the
hidden layer size to 30 neurons, thereby greatly
increasing speed. Despite slower training, the ac-
tual scoring at test time of expected BLEU mod-
els is about 5 times faster than for cross-entropy
models since we do not need to normalize the out-
put layer anymore. The results (Table 3) show
improvements of up to 0.6 BLEU when combin-
ing a cross-entropy model with an expected BLEU
variant. Average gains across all test sets are 0.4
BLEU, demonstrating that the gains from the ex-
pected BLEU loss are additive.
6 Conclusion and Future Work
We introduce an empirically effective approxima-
tion to integrate a recurrent neural network model
into first pass decoding, thereby extending pre-
vious work on decoding with feed-forward neu-
ral networks (Vaswani et al, 2013). Our best re-
sult improves the output of a phrase-based decoder
by up to 2.0 BLEU on French-English translation,
outperforming n-best rescoring by up to 1.1 BLEU
and lattice rescoring by up to 0.4 BLEU. Directly
optimizing a recurrent neural network language
model towards an expected BLEU loss proves ef-
fective, improving a cross-entropy trained variant
by up 0.6 BLEU. Despite higher training complex-
ity, our expected BLEU trained model has five
times faster runtime than a cross-entropy model
since it does not require normalization.
In future work, we would like to scale up to
larger data sets and more complex models through
parallelization. We would also like to experiment
with more elaborate future cost estimates, such as
the average score assigned to all occurrences of a
phrase in a large corpus.
7 Acknowledgments
We thank Michel Galley, Arul Menezes, Chris
Quirk and Geoffrey Zweig for helpful discussions
related to this work as well as the four anonymous
reviewers for their comments.
140
References
Alexandre Allauzen, H?el`ene Bonneau-Maynard, Hai-
Son Le, Aur?elien Max, Guillaume Wisniewski,
Franc?ois Yvon, Gilles Adda, Josep Maria Crego,
Adrien Lardilleux, Thomas Lavergne, and Artem
Sokolov. 2011. LIMSI @ WMT11. In Proc. of
WMT, pages 309?315, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep Neural Net-
work Language Models. In NAACL-HLT Work-
shop on the Future of Language Modeling for HLT,
pages 20?28, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Auli and Adam Lopez. 2011. Training a
Log-Linear Parser with Loss Functions via Softmax-
Margin. In Proc. of EMNLP, pages 333?343. Asso-
ciation for Computational Linguistics, July.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Proc.
of EMNLP, October.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of EMNLP, pages 848?856.
Jianfeng Gao and Xiaodong He. 2013. Training MRF-
Based Phrase Translation Models using Gradient
Ascent. In Proc. of NAACL-HLT, pages 450?459.
Association for Computational Linguistics, June.
Jianfeng Gao, Xiaodong He, Scott Wen tau Yih, and
Li Deng. 2014. Learning Continuous Phrase Rep-
resentations for Translation Modeling. In Proc.
of ACL. Association for Computational Linguistics,
June.
Joshua Goodman. 1996. Parsing Algorithms and Met-
rics. In Proc. of ACL, pages 177?183, Santa Cruz,
CA, USA, June.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proc. of ACL, pages 8?14. Association
for Computational Linguistics, July.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum Translation Modeling with Recur-
rent Neural Networks. In Proc. of EACL. Associa-
tion for Computational Linguistics, April.
Nal Kalchbrenner and Phil Blunsom. 2013. Re-
current Continuous Translation Models. In Proc.
of EMNLP, pages 1700?1709, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, Jun.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous Space Translation Models with
Neural Networks. In Proc. of HLT-NAACL, pages
39?48, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur?elien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012b. LIMSI @ WMT12. In Proc.
of WMT, pages 330?337, Montr?eal, Canada, June.
Association for Computational Linguistics.
Percy Liang, Alexandre Bouchard-C?ot?e, Ben Taskar,
and Dan Klein. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of ACL-
COLING, pages 761?768, Jul.
Tom?a?s Mikolov, Karafi?at Martin, Luk?a?s Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proc. of INTERSPEECH, pages 1045?1048.
Tom?a?s Mikolov, Anoop Deoras, Daniel Povey, Luk?a?s
Burget, and Jan
?
Cernock?y. 2011. Strategies for
Training Large Scale Neural Network Language
Models. In Proc. of ASRU, pages 196?201.
Tom?a?s Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for Sentence-Level BLEU+1
Yields Short Translations. In Proc. of COLING. As-
sociation for Computational Linguistics.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN System De-
scription for WMT10 System Combination Task.
In Proc. of WMT, pages 321?326. Association for
Computational Linguistics, July.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proc. of
WMT, pages 159?165. Association for Computa-
tional Linguistics, July.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In Symposium on Parallel and
Distributed Processing.
141
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space
Language Models on a GPU for Statistical Machine
Translation. In NAACL-HLT Workshop on the Fu-
ture of Language Modeling for HLT, pages 11?19.
Association for Computational Linguistics.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl?uter, and Hermann Ney.
2013. Comparison of Feedforward and Recurrent
Neural Network Language Models. In IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing, pages 8430?8434, May.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk Decoding for Statistical Machine Trans-
lation. In Proc. of EMNLP, pages 620?629. Associ-
ation for Computational Linguistics, October.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-scale
Neural Language Models improves Translation. In
Proc. of EMNLP. Association for Computational
Linguistics, October.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-Violation Perceptron and Forced Decod-
ing for Scalable MT Training. In Proc. of EMNLP,
pages 1112?1123. Association for Computational
Linguistics, October.
142
