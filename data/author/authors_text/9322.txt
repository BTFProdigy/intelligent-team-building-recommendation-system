Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 162?165,
Sydney, July 2006. c?2006 Association for Computational Linguistics
162
163
164
165
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 30?39,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Semantic Dependency Parsing of NomBank and PropBank:
An Efficient Integrated Approach via a Large-scale Feature Selection
?
Hai Zhao(??)?, Wenliang Chen(???)?, Chunyu Kit?(???)
?
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Avenue, Kowloon, Hong Kong, China
?
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
haizhao@cityu.edu.hk, chenwl@nict.go.jp
Abstract
We present an integrated dependency-
based semantic role labeling system for
English from both NomBank and Prop-
Bank. By introducing assistant argument
labels and considering much more fea-
ture templates, two optimal feature tem-
plate sets are obtained through an effec-
tive feature selection procedure and help
construct a high performance single SRL
system. From the evaluations on the date
set of CoNLL-2008 shared task, the per-
formance of our system is quite close to
the state of the art. As to our knowl-
edge, this is the first integrated SRL sys-
tem that achieves a competitive perfor-
mance against previous pipeline systems.
1 Introduction
We investigate the possibility to construct an effec-
tive integrated system for dependency-based se-
mantic role labeling (SRL) task. This means in
this work that a single system handles all these
sub-tasks, predicate identification/disambiguation
and argument identification/classification, regard-
less of whether the predicate is verbal or nominal.
Traditionally, a SRL task, either dependency
or constituent based, is implemented as two sub-
tasks, namely, argument identification and clas-
sification. If the predicate is unknown, then a
predicate identification or disambiguation subtask
should be additionally considered. A pipeline
framework is usually adopted to handle all these
sub-tasks. The reason to divide the whole task
?
This study is partially supported by CERG grant
9040861 (CityU 1318/03H), CityU Strategic Research Grant
7002037.
into multiple stages is two-fold, one is each sub-
task asks for its favorable features, the other is
at the consideration of computational efficiency.
Generally speaking, a joint system is slower than
a pipeline system in training. (Xue and Palmer,
2004) fount out that different features suited for
different sub-tasks of SRL, i.e. argument identifi-
cation and classification. The results from CoNLL
shared tasks in 2005 and 2008 (Carreras and Mar-
quez, 2005; Koomen et al, 2005; Surdeanu et al,
2008; Johansson and Nugues, 2008), further show
that SRL pipeline may be one of the standard to
achieve a state-of-the-art performance in practice.
In the recent years, most works on SRL, includ-
ing two CoNLL shared task in 2004 and 2005,
focus on verbal predicates with the availability
of PropBank (Palmer et al, 2005). As a com-
plement to PropBank, NomBank (Meyers et al,
2004) annotates nominal predicates and their cor-
responding semantic roles using similar semantic
framework as PropBank. Though SRL for nomi-
nal predicates offers more challenge, it draws rel-
atively little attention (Jiang and Ng, 2006).
(Pustejovsky et al, 2005) discussed the issue of
merging various treebanks, including PropBank,
NomBank, and others. The idea of merging these
two different treebanks was implemented in the
CoNLL-2008 shared task (Surdeanu et al, 2008).
However, few empirical studies support the ne-
cessity of an integrated learning strategy from
NomBank and PropBank. Though aiming at Chi-
nese SRL, (Xue, 2006) reported that their exper-
iments show that simply adding the verb data to
the training set of NomBank and extracting the
same features from the verb and noun instances
will hurt the overall performance. From the re-
sults of CoNLL-2008 shared task, the top system
by (Johansson and Nugues, 2008) also used two
30
different subsystems to handle verbal and nominal
predicates, respectively.
Despite all the above facts, an integrated SRL
system still holds some sort of merits, being eas-
ier to implement, a single-stage feature selection
benefiting the whole system, an all-in-one model
outputting all required semantic role information
and so on.
The shared tasks at the CoNLL 2008 and 2009
are devoted to the joint learning of syntactic and
semantic dependencies, which show that SRL can
be well performed using only dependency syn-
tax input. Using data and evaluation settings
of the CoNLL-2008 shared task, this work will
only focus on semantic dependency parsing and
compares the best-performing SRL system in the
CoNLL-2009 shared Task (Zhao et al, 2009b)
with those in the CoNLL-2008 shared task (Sur-
deanu et al, 2008; Haji?c et al, 2009)
1
.
Aiming at main drawbacks of an integrated ap-
proach, two key techniques will be applied. 1)
Assistant argument labels are introduced for the
further improvement of argument pruning. This
helps the development of a fast and lightweight
SRL system. 2) Using a greedy feature selec-
tion algorithm, a large-scale feature engineering is
performed on a much larger feature template set
than that in previous work. This helps us find fea-
tures that may be of benefit to all SRL sub-tasks as
long as possible. As two optimal feature template
sets have been proven available, for the first time
we report that an integrated SRL system may pro-
vide a result close to the state-of-the-art achieved
by those SRL pipelines or individual systems for
some specific predicates.
2 Adaptive Argument Pruning
A word-pair classification is used to formulate se-
mantic dependency parsing as in (Zhao and Kit,
2008). As for predicate identification or disam-
biguation, the first word is set as a virtual root
(which is virtually set before the beginning of the
sentence.) and the second as a predicate candi-
date. As for argument identification/classification,
the first word in a word pair is specified as a predi-
1
CoNLL-2008 is an English-only task, while CoNLL-
2009 is a multilingual one. Though the English corpus in
CoNLL-2009 is almost identical to the corpus in the CoNLL-
2008 shared task evaluation, the latter holds more sophisti-
cated input structure as in (Surdeanu et al, 2008). The most
difference for these two tasks is that the identification of se-
mantic predicates is required in the task of CoNLL-2008 but
not in CoNLL-2009.
cate candidate and the second as an argument can-
didate. In either of case, the first word is called a
semantic head, and noted as p in our feature rep-
resentation, the second is called a semantic depen-
dent and noted as a.
Word pairs are collected for the classifier in
such order. The first word of the pair is set to the
virtual root at first, the second word is then spec-
ified as a predicate candidate. According to the
result that the predicate candidate is classified or
proven to be non-predicate, 1) the second word is
reset to next predicate candidate if the answer is
non-predicate, otherwise, 2) the first word of the
pair is reset to the predicate that is just determined,
and the second is set to every argument candidates
one by one. The classifier will scan the input sen-
tence from left to right to check if each word is a
true predicate.
Without any constraint, all word pairs in an in-
put sequence must be considered by the classifier,
leading to poor computational efficiency and un-
necessary performance loss. Thus, the training
sample for SRL task needs to be pruned properly.
We use a simple strategy to prune predicate can-
didates, namely, only verbs and nouns are chosen
in this case.
There are two paths to collect argument candi-
dates over the sequence. One is based on an input
syntactic dependency tree, the other is based on
a linear path of the sentence. As for the former
(hereafter it is referred to synPth), we continue to
use a dependency version of the pruning algorithm
of (Xue and Palmer, 2004). The pruning algorithm
is readdressed as the following.
Initialization: Set the given predicate as the
current node;
(1) The current node and all of its syntactic
children are selected as argument candidates
(children are traversed from left to right.).
(2) Reset the current node to its syntactic head
and repeat step (1) until the root is reached.
Note that this pruning algorithm is slightly dif-
ferent from that of (Xue and Palmer, 2004), the
predicate itself is also included in the argument
candidate list as the nominal predicate sometimes
takes itself as its argument.
The above pruning algorithm has been shown
effective. However, it is still inefficient for a SRL
31
system that needs to tackle argument identifica-
tion/classification in a single stage. Assuming that
arguments trend to surround their predicate, an as-
sistant argument label ? NoMoreArgument? is in-
troduced for further pruning. If an argument can-
didate in the above algorithm is assigned to such
a label, then the pruning algorithm will end im-
mediately. In training, this assistant label means
no more samples will be generated for the current
predicate, while in test, the decoder will not search
arguments any more. It will be seen that this adap-
tive technique more effectively prunes argument
candidates without missing more true arguments.
Along the linear path (hereafter referred to
linPth), the classifier will search all words before
and after the predicate. Similar to the pruning
algorithm for synPth, we also introduce two as-
sistant argument labels ? noLeft? and ? noRight?
to adaptively prune words too far away from the
predicate.
To show how assistant argument labels actually
work, we give an example for linP th. Suppose an
input sequence with argument labels for a predi-
cate is
a b c d e f g h .
A1 A0
Note that c and g are two boundary words as no
more arguments appear before or after them. After
two assistant argument labels are added, it will be
a b c d e f g h .
noLeft A1 A0 noRight
Training samples will generated from c to g ac-
cording to the above sequence.
We use a Maximum Entropy classifier with a
tunable Gaussian prior as usual. Our implemen-
tation of the model adopts L-BFGS algorithm for
parameter optimization.
3 Feature Templates
3.1 Elements for Feature Generation
Motivated by previous works, we carefully con-
sider those factors from a wide range of features
that can help semantic role labeling for both predi-
cate disambiguation, argument?s identification and
classification as the predicate is either verbal or
nominal. These works include (Gildea and Juraf-
sky, 2002; Carreras and Marquez, 2005; Koomen
et al, 2005; Marquez et al, 2005; Dang and
Palmer, 2005; Pradhan et al, 2005; Toutanova et
al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007;
Surdeanu et al, 2007; Johansson and Nugues,
2008; Che et al, 2008). Most feature templates
that we will adopt for this work will come from
various combinations or integrations of the follow-
ing basic elements.
Word Property. This type of elements include
word form (form and its split form, spForm)
2
,
lemma (lemma,spLemma), and part-of-speech tag
(pos, spPos), syntactic dependency label (dprel),
and semantic dependency label (semdprel)
3
.
Syntactic Connection. This includes syn-
tactic head (h), left(right) farthest(nearest) child
(lm, ln, rm, and rn), and high(low) support
verb or noun. We explain the last item, sup-
port verb(noun). From a given word to the
syntactic root along the syntactic tree, the first
verb/noun/preposition that is met is called as its
low support verb/noun/preposition, and the near-
est one to the root is called as its high support
verb/noun/preposition. The concept of support
verb was broadly used (Toutanova et al, 2005;
Xue, 2006; Jiang and Ng, 2006)
4
, we here extend
it to nouns and prepositions. In addition, we intro-
duce a slightly modified syntactic head, pphead,
it returns the left most sibling of a given word if
the word is headed by a preposition, otherwise it
returns the original head.
Path. There are two basic types of path between
the predicate and the argument candidates. One
is the linear path (linePath) in the sequence, the
other is the path in the syntactic parsing tree (dp-
Path). For the latter, we further divide it into four
sub-types with respect to the syntactic root, dp-
Path is the full path in the syntactic tree. Leading
two paths to the root from the predicate and the
argument, respectively, the common part of these
two paths will be dpPathShare. Assume that dp-
PathShare starts from a node r
?
, then dpPathPred
is from the predicate to r
?
, and dpPathArgu is from
the argument to r
?
.
Family. Two types of children sets for the pred-
icate or argument candidate are considered, the
2
In CoNLL-2008, Treebank tokens are split at the position
that a hyphen (-) or a forward slash (/) occurs. This leads to
two types of feature columns, non-split and split.
3
Lemma and pos for either training or test are from auto-
matically pre-analyzed columns in the input files.
4
Note that the meaning of support verb is slightly different
between (Toutanova et al, 2005) and (Xue, 2006; Jiang and
Ng, 2006)
32
first includes all syntactic children (children), the
second also includes all but excludes the left most
and the right most children (noFarChildren).
Concatenation of Elements. For all collected
elements according to linePath, children and so
on, we use three strategies to concatenate all those
strings to produce the feature value. The first is
seq, which concatenates all collected strings with-
out doing anything. The second is bag, which
removes all duplicated strings and sort the rest.
The third is noDup, which removes all duplicated
neighbored strings.
We address some other elements that are not in-
cluded by the above description as the following.
dpTreeRelation. It returns the relationship of a
and p in the input syntactic tree. The possible val-
ues for this feature include parent, sibling
etc.
isCurPred. It judges if a given word is the cur-
rent predicate. If the word is the predicate, then it
returns the predicate itself, otherwise it returns a
default value.
existCross. It judges if a forthcoming depen-
dency relation that is between a given word pair
may cause any cross with all existing dependency
relations.
distance. It counts the number of words along a
given path, either dpPath or linePath.
existSemdprel. It checks if the given argument
label for other predicates has been assigned to a
given word.
voice. This feature returns Active or Passive for
verbs, and a default value for nouns.
baseline. Two types of semantic role baseline
outputs are used for features from (Carreras and
Marquez, 2005)
5
. baseline Ax tags the head of
the first NP before the predicate as A0 and the
head of the first NP after the predicate as A1.
baseline Mod tags the dependant of the predicate
as AM-MOD as it is a modal verb.
We show some feature template examples de-
rived from the above mentioned items.
a.lm.lemma The lemma of the left most child of
the argument candidate.
p.h.dprel The dependant label of the syntactic
head of the predicate candidate.
p
?1
.pos+p.pos pos of the previous word of the
predicate and PoS of the predicate itself.
a:p|dpPath.lemma.bag Collect all lemmas
5
These baseline rules were developed by Erik Tjong Kim
Sang, from the University of Antwerp, Belgium.
along the syntactic tree path from the argument
to the predicate, then removed all duplicated
ones and sort the rest, finally concatenate all as a
feature string.
a:p.highSupportNoun|linePath.dprel.seq Col-
lect all dependant labels along with the line path
from the argument to the high support noun of the
predicate, then concatenate all as a feature string.
3.2 Feature Template Selection
Based on the above mentioned elements, 781 fea-
ture templates (hereafter the set of these templates
is referred to FT )
6
are initially considered. Fea-
ture templates in this initial set are constructed in
a generalized way. For example, if we find that
a feature template a.lm.lemma was once used in
some existing work, then such three templates,
a.rm.lemma, a.rn.lemma, a.ln.lemma will be also
added into the set.
As an optimal feature template subset cannot be
expected to be extracted from so large a set by
hand, a greedy feature selection similar to that in
(Jiang and Ng, 2006; Ding and Chang, 2008) is ap-
plied. The detailed algorithm is described in Algo-
rithm 1. Assuming that the number of feature tem-
plates in a given set is n, the algorithm of (Ding
and Chang, 2008) requires O(n
2
) times of train-
ing/test routines, it cannot handle a set that con-
sists of hundreds of templates. As the time com-
plexity of Algorithm 1 is only O(n), it permits a
large scale feature selection accomplished by pay-
ing a reasonable time cost. Though the time com-
plexity of the algorithm given by (Jiang and Ng,
2006) is also linear, it should assume all feature
templates in the initial selected set ?good? enough
and handles other feature template candidates in a
strict incremental way. However, these two con-
straints are not easily satisfied in our case, while
Algorithm 1 may release these two constraints.
Choosing the first 1/10 templates in FT as
the initial selected set S, the feature selection is
performed for two argument candidate traverse
schemes, synPth and linP th, respectively. 4686
machine learning routines run for the former,
while 6248 routines for the latter. Two feature
template sets, FT
syn
and FT
lin
, are obtained at
last. These two sets are given in Table 1-3. We see
that two sets share 30 identical feature templates
as in Table 1. FT
syn
holds 51 different templates
6
This set with detailed explanation will be available at our
website.
33
p.lm.dprel
p.rm.dprel
p.spForm
p
?1
.spLemma
p.spLemma
p
?1
.spLemma+p.spLemma
p.spLemma + p
1
.spLemma
p.spLemma + p.h.spForm
p.spLemma + p.currentSense
p.lemma
p.lemma + p
1
.lemma
p
?1
.pos+p.pos
a.isCurPred.lemma
a
?2
.isCurPred.lemma + a
?1
.isCurPred.lemma
a.isCurPred.spLemma
a
?1
.isCurPred.spLemma + a.isCurPred.spLemma
a.isCurPred.spLemma + a
1
.isCurPred.spLemma
a.children.dprel.bag
a
?1
.spLemma + a.spLemma
a
?1
.spLemma + a.dprel
a
?1
.spLemma + a.dprel + a.h.spLemma
a.lm
?1
.spLemma
a.rm
?1
.dprel + a.spPos
a
?1
.lemma + a.dprel + a.h.lemma
a.lemma + p.lemma
a.pos + p.pos
a.spLemma + p.spLemma
a:p|dpPath.dprel
a:p|dpPathArgu.dprel
a:p|dpPathPred.spPos
Table 1: Feature templates for both synPth and
linP th
as in Table 2 and FT
lin
holds 57 different tem-
plates as in Table 3. In these tables, the subscripts -
2(or -1) and 1(or 2) stand for the previous and next
words, respectively. For example, a.lm
?1
.lemma
returns the lemma of the previous word of the ar-
gument?s left most child.
4 Decoding
After the predicate sense is disambiguated, an op-
timal argument structure for each predicate is de-
termined by the following maximal probability.
S
p
= argmax
?
i
P (a
i
|a
i?1
, a
i?2
, ...), (1)
where S
p
is the argument structure, P (a
i
|a
i?1
...)
is the conditional probability to determine the la-
bel of the i-th argument candidate label. A beam
search algorithm is used to find the optimal argu-
ment structure.
5 Evaluation Results
Our evaluation is performed on the standard
training/development/test corpus of CoNLL-2008
shared task. The data is derived by merging a de-
pendency version of the Penn Treebank with Prop-
Bank and NomBank. More details on the data are
Algorithm 1 Greedy Feature Selection
Input:
The set of all feature templates: FT
The set of selected feature templates: S
0
Output:
The set of selected feature templates: S
Procedure:
Let the counter i = 1
Let S
i
= S
0
and C = FT ? S
i
while do
Train a model with features according to S
i
,
test on development set and the result is p
i
.
Let C
r
= null.
for each feature template f
j
in set S
i
do
Let S
?
= S
i
? f
j
.
Train a model with features according to
S
?
, test on development set and the result
is p
?
.
if p
?
> p
i
then
C
r
= C
r
+ f
j
.
end if
end for
C = C + C
r
S
i
= S
i
? C
r
Let S
?
i
= S
i
Train a model with features according to S
?
i
,
test on development set and the result is q
i
.
Let C
r
= null
for each feature template f
j
in set C do
Let C
?
= S
?
i
+ f
j
.
Train a model with features according to
C
?
, test on development set and the result
is p
?
.
if p
?
> q
i
then
C
r
= C
r
+ f
j
.
end if
end for
C = C ? C
r
S
?
i
= S
?
i
+ C
r
if S
i
= S
i?1
(No feature templates are added
or removed) or, neither p
i
nor q
i
is larger than
p
i?1
and q
i?1
then
Output S = argmax
p
i
,q
i
{S
i
, S
?
i
} and the
algorithm ends.
else
Let i = i+ 1, S
i
=S
i?1
and C = FT ? S
i
end if
end while
34
p?1
.lemma + p.lemma
p
?2
.pos
p.pos
p
?2
.spForm + p
?1
.spForm
p
1
.spForm
p.spForm + p.children.dprel.noDup
p.lm.spPos
p.spForm + p.lm.spPos
+ p.noFarChildren.spPos.bag + p.rm.spPos
p.dprel
p.children.dprel.bag
p.children.pos.seq
p.dprel = OBJ ?
a
a.dprel
a
?1
.lemma + a
1
.lemma
a
1
.lemma
a
?1
.pos
a
1
.spPos
a.h.lemma
a.h.spLemma
a.pphead.lemma
a.pphead.spLemma
a.lm.dprel + a.spPos
a.rm
?1
.pos
a.spLemma + a.h.spPos
a.existSemdprel A1
a.dprel = OBJ ?
a.form + a.children.pos.seq
a.children.adv.bag
b
a:p|linePath.distance
a:p|dpPath.distance
a:p|existCross
a:p|dpPath.dprel.bag
a:p|dpPathPred.dprel.bag
a:p|dpPath.spForm.seq
a:p|dpPathArgu.spForm.seq
a:p|dpPathPred.spForm.bag
a:p|dpPath.spLemma.seq
a:p|dpPathArgu.spLemma.seq
a:p|dpPathArgu.spLemma.bag
a:p|dpPathPred.spLemma.bag
a:p|dpPath.spPos.bag
a:p|dpPathPred.spPos.bag
(a:p|dpPath.dprel.seq) + p.spPos
(a:p|dpTreeRelation) + a.spPos
(a:p|dpTreeRelation) + p.spPos
(a.highSupportVerb:p|dpTreeRelation) + a.spPos
a.highSupportNoun:p|dpPath.dprel.seq
a.lowSupportVerb:p|dpPath.dprel.seq
a:p|linePath.spForm.bag
a:p|linePath.spLemma.bag
a:p|linePath.spLemma.seq
a
This feature checks if the dependant type is OBJ.
b
adv means all adverbs.
Table 2: Feature templates only for synPth
p.currentSense + a.spLemma
p.currentSense + a.spPos
p.voice + (a:p|direction)
p.rm.dprel
p.children.dprel.noDup
p.rm.form
p.lowSupportNoun.spForm
p.lowSupportProp:p|dpTreeRelation
p
?2
.form + p
?1
.form
p.voice
p.form + p.children.dprel.noDup
p.pos + p.dprel
p.spForm + p.children.dprel.bag
a.voice + (a:p|direction)
a
?1
.isCurPred.lemma
a
1
.isCurPred.lemma
a
?1
.isCurPred.lemma + a.isCurPred.lemma
a.isCurPred.lemma + a
1
.isCurPred.lemma
a
1
.isCurPred.spLemma
a
?2
.isCurPred.spLemma + a
?1
.isCurPred.spLemma
a.baseline Ax + a.voice + (a:p|direction)
a.baseline Mod
a.h.children.dprel.bag
a.lm.dprel + a.dprel
a.lm.dprel + a.pos
a.lm
?1
.lemma
a.lm.lemma
a.lm
1
.lemma
a.lm.pos + a.pos
a.lm.spForm
a.lm
?1
.spPos
a.lm.spPos
a.ln.dprel + a.pos
a.noFarChildren.spPos.bag + a.rm.spPos
a.children.spPos.seq + p.children.spPos.seq
a.rm.dprel + a.pos
a.rm
?1
.spPos
a.rm.spPos
a.rm
1
.spPos
a.rn.dprel + a.spPos
a.form
a.form + a
1
.form
a.form + a.pos
a
?1
.lemma
a
?1
.lemma + a.lemma
a
?2
.pos
a.spForm + a
1
.spForm
a.spForm + a.spPos
a.spLemma + a
1
.spLemma
a.spForm + a.children.spPos.seq
a.spForm + a.children.spPos.bag
a.spLemma + a.h.spForm
a.spLemma + a.pphead.spForm
a.existSemdprel A2
a:p|dpPathArgu.pos.seq
a:p|dpPathPred.dprel.seq
a:p|dpTreeRelation
Table 3: Feature templates only for linPth
35
in (Surdeanu et al, 2008). Note that CoNLL-2008
shared task is essentially a joint learning task for
both syntactic and semantic dependencies, how-
ever, we will focus on semantic part of this task.
The main semantic measure that we adopt is se-
mantic labeled F
1
score (Sem-F
1
). In addition, the
macro labeled F
1
scores (Macro-F
1
), which was
used for the ranking of the participating systems of
CoNLL-2008, the ratio between labeled F
1
score
for semantic dependencies and the LAS for syn-
tactic dependencies (Sem-F
1
/LAS), are also given
for reference.
5.1 Syntactic Dependency Parsers
We consider three types of syntactic information
to feed the SRL task. One is gold-standard syn-
tactic input, and other two are based on automati-
cally parsing results of two parsers, the state-of-
the-art syntactic parser described in (Johansson
and Nugues, 2008)
7
(it is referred to Johansson)
and an integrated parser described as the follow-
ing (referred to MST
ME
).
The parser is basically based on the MSTParser
8
using all the features presented by (McDonald et
al., 2006) with projective parsing. Moreover, we
exploit three types of additional features to im-
prove the parser. 1) Chen et al (2008) used fea-
tures derived from short dependency pairs based
on large-scale auto-parsed data to enhance depen-
dency parsing. Here, the same features are used,
though all dependency pairs rather than short de-
pendency pairs are extracted along with the de-
pendency direction from training data rather than
auto-parsed data. 2) Koo et al (2008) presented
new features based on word clusters obtained from
large-scale unlabeled data and achieved large im-
provement for English and Czech. Here, the same
features are also used as word clusters are gen-
erated only from the training data. 3) Nivre and
McDonald (2008) presented an integrating method
to provide additional information for graph-based
and transition-based parsers. Here, we represent
features based on dependency relations predicted
by transition-based parsers for the MSTParer. For
the sake of efficiency, we use a fast transition-
7
It is a 2-order maximum spanning tree parser with
pseudo-projective techniques. A syntactic-semantic rerank-
ing was performed to output the final results according to (Jo-
hansson and Nugues, 2008). However, only 1-best outputs of
the parser before reranking are used for our evaluation. Note
that the reranking may slightly improve the syntactic perfor-
mance according to (Johansson and Nugues, 2008).
8
It?s freely available at http://mstparser.sourceforge.net.
Parser Path Adaptive Pruning Coverage
/wo /w Rate
Gold synPth 2.13M 1.05M 98.4%
(49.30%)
linP th 5.29M 1.57M 100.0%
(29.68%)
Johansson synPth 2.15M 1.06M 95.4%
(49.30%)
linP th 5.28M 1.57M 100.0%
(29.73%)
MST
ME
synPth 2.15M 1.06M 95.0%
(49.30%)
linP th 5.29M 1.57M 100.0%
(29.68%)
Table 4: The number of training samples on argu-
ment candidates
synPth+FT
syn
linPth+FT
lin
Syn-Parser LAS Sem Sem-F
1
Sem Sem-F
1
F
1
/LAS F
1
/LAS
MST
ME
88.39 80.53 91.10 79.83 90.31
Johansson 89.28 80.94 90.66 79.84 89.43
Gold 100.00 84.57 84.57 83.34 83.34
Table 5: Semantic Labeled F
1
based parser based on maximum entropy as in
Zhao and Kit (2008). We still use the similar fea-
ture notations of that work.
5.2 The Results
At first, we report the effectiveness of the proposed
adaptive argument pruning. The numbers of argu-
ment candidates are in Table 4. The statistics is
conducted on three different syntactic inputs. The
coverage rate in the table means the ratio of how
many true arguments are covered by the selected
pruning scheme. Note that the adaptive pruning
of argument candidates using assistant labels does
not change this rate. This ratio only depends on
which path, either synPth or linP th, is chosen,
and how good the syntactic input is (if synPth
is the case). From the results, we see that more
than a half of argument candidates can be effec-
tively pruned for synPth and even 2/3 for linP th.
As mentioned by (Pradhan et al, 2004), argument
identification plays a bottleneck role in improving
the performance of a SRL system. The effective-
ness of the proposed additional pruning techniques
may be seen as a significant improvement over the
original algorithm of (Xue and Palmer, 2004). The
results also indicate that such an assumption holds
that arguments trend to close with their predicate,
at either type of distance, syntactic or linear.
Based on different syntactic inputs, we obtain
different results on semantic dependency parsing
36
as shown in Table 5. These results on differ-
ent syntactic inputs also give us a chance to ob-
serve how semantic performance varies according
to syntactic performance. The fact from the re-
sults is that the ratio Sem-F
1
/LAS becomes rela-
tively smaller as the syntactic input becomes bet-
ter. Though not so surprised, the results do show
that the argument traverse scheme synPth always
outperforms the other linP th. The result of this
comparison partially shows that an integrated se-
mantic role labeler is sensitive to the order of how
argument candidates are traversed to some extent.
The performance given by synPth is com-
pared to some other systems that participated in
the CoNLL-2008 shared task. They were cho-
sen among the 20 participating systems either be-
cause they held better results (the first four partic-
ipants) or because they used some joint learning
techniques (Henderson et al, 2008). The results of
(Titov et al, 2009) that use the similar joint learn-
ing technique as (Henderson et al, 2008) are also
included
9
. Results of these evaluations on the test
set are in Table 6. Top three systems of CoNLL-
2008, (Johansson and Nugues, 2008; Ciaramita et
al., 2008; Che et al, 2008), used SRL pipelines.
In this work, we partially use the similar
techniques (synPth) for our participation in the
shared tasks of CoNLL-2008 and 2009 (Zhao and
Kit, 2008; Zhao et al, 2009b; Zhao et al, 2009a).
Here we report that all SRL sub-tasks are tackled
in one integrated model, while the predicate dis-
ambiguation sub-task was performed individually
in both of our previous systems. Therefore, this is
our first attempt at a full integrated SRL system.
(Titov et al, 2009) reported the best result by
using joint learning technique up to now. The
comparison indicates that our integrated system
outputs a result quite close to the state-of-the-art
by the pipeline system of (Johansson and Nugues,
2008) as the same syntactic structure input is
adopted. It is worth noting that our system actu-
ally competes with two independent sub-systems
of (Johansson and Nugues, 2008), one for verbal
predicates, the other for nominal predicates. In ad-
dition, the results of our system is obtained with-
out using additional joint learning technique like
syntactic-semantic reranking. It indicates that our
system is expected to obtain some further perfor-
mance improvement by using such techniques.
9
In addition, the work of (Henderson et al, 2008) and
(Titov et al, 2009) jointly considered syntactic and semantic
dependencies, that is significantly different from the others.
6 Conclusion
We have described a dependency-based semantic
role labeling system for English from NomBank
and PropBank. From the evaluations, the result of
our system is quite close to the state of the art. As
to our knowledge, it is the first integrated SRL sys-
tem that achieves such a competitive performance
against previous pipeline systems.
According to the path that the word-pair classi-
fier traverses argument candidates, two integration
schemes are presented. Argument candidate prun-
ing and feature selection are performed on them,
respectively. These two schemes are more than
providing a trivial comparison. As assistant la-
beled are introduced to help further argument can-
didate pruning, and this techniques work well for
both schemes, it support the assumption that argu-
ments trend to surround their predicate. The pro-
posed feature selection procedure also work for
both schemes and output quite different two fea-
ture template sets, and either of the sets helps the
system obtain a competitive performance, this fact
suggests that the feature selection procedure is ro-
bust and effective, too.
Either of the presented integrated systems can
provide a competitive performance. This conclu-
sion about basic learning scheme for SRL is some
different from previous literatures. However, ac-
cording to our results, there does exist a ?harmony?
feature template set that is helpful to both predi-
cate and argument identification/classification, or
SRL for both verbal and nominal predicates. We
attribute this different conclusion to two main fac-
tors, 1) much more feature templates (for example,
ten times more than those used by Xue et al) than
previous that are considered for a successful fea-
ture engineering, 2) a maximum entropy classifier
makes it possible to accept so many various fea-
tures in one model. Note that maximum entropy is
not so sensitive to those (partially) overlapped fea-
tures, while SVM and other margin-based learners
are not so.
Acknowledgements
Our thanks give to Dr. Richard Johansson, who
kindly provided the syntactic output for his partic-
ipation in the CoNLL-2008 shared task.
37
Systems
a
LAS Sem-F
1
Macro Sem-F
1
pred-F
1
b
argu-F
1
c
Verb-F
1
d
Nomi-F
1
e
F
1
/LAS
Johansson:2008*
f
89.32 81.65 85.49 91.41 87.22 79.04 84.78 77.12
Ours:Johansson 89.28 80.94 85.12 90.66 86.57 78.30 83.66 76.93
Ours:MST
ME
88.39 80.53 84.93 91.10 86.80 77.60 82.77 77.23
Johansson:2008 89.32 80.37 84.86 89.98 85.40 78.02 84.45 74.32
Ciaramita:2008* 87.37 78.00 82.69 89.28 83.46 75.35 80.93 73.80
Che:2008 86.75 78.52 82.66 90.51 85.31 75.27 80.46 75.18
Zhao:2008* 87.68 76.75 82.24 87.53 78.52 75.93 78.81 73.59
Ciaramita:2008 86.60 77.50 82.06 89.49 83.46 74.56 80.15 73.17
Titov:2009 87.50 76.10 81.80 86.97 ? ? ? ?
Zhao:2008 86.66 76.16 81.44 87.88 78.26 75.18 77.67 73.28
Henderson:2008* 87.64 73.09 80.48 83.40 81.42 69.10 75.84 68.90
Henderson:2008 86.91 70.97 79.11 81.66 79.60 66.83 73.80 66.26
Ours:Gold 100.0 84.57 92.20 84.57 87.67 83.15 88.71 78.39
a
Ranking according to Sem-F
1
b
Labeled F
1
for predicate identification and classification
c
Labeled F
1
for argument identification and classification
d
Labeled F
1
for verbal predicates
e
Labeled F
1
for nominal predicates
f
* means post-evaluation results, which are available at the official website of CoNLL-2008 shared task,
http://www.yr-bcn.es/dokuwiki/doku.php?id=conll2008:start.
Table 6: Comparison of the best existing systems
References
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, pages 152?
164, Ann Arbor, Michigan, USA.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang
Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A
cascaded syntactic and semantic dependency pars-
ing system. In Proceedings of CoNLL-2008, pages
238?242, Manchester, England, August.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchi-
moto, Yujie Zhang, and Hitoshi Isahara. 2008. De-
pendency parsing with short dependency relations
in unlabeled data. In Proceedings of IJCNLP-2008,
Hyderabad, India, January 8-10.
Massimiliano Ciaramita, Giuseppe Attardi, Felice
Dell?Orletta, and Mihai Surdeanu. 2008. Desrl: A
linear-time semantic role labeling system. In Pro-
ceedings of CoNLL-2008, pages 258?262, Manch-
ester, England, August.
Hoa Trang Dang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings of ACL-2005, pages 42?49, Ann Arbor,
USA.
Weiwei Ding and Baobao Chang. 2008. Improving
chinese semantic role classification with hierarchi-
cal feature selection strategy. In Proceedings of
EMNLP-2008, pages 324?323, Honolulu, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, pages 1?
18, Boulder, Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of CoNLL-2008, pages
178?182, Manchester, England, August.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Seman-
tic role labeling of nombank: A maximum entropy
approach. In Proceedings of EMNLP-2006, pages
138?145, Sydney, Australia.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with propbank and nombank. In Proceedings of
CoNLL-2008, page 183?187, Manchester, UK.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, USA, June.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen
tau Yih. 2005. Generalized inference with multi-
ple semantic role labeling systems. In Proceedings
of CoNLL-2005, pages 181?184, Ann Arbor, Michi-
gan, USA.
Chang Liu and Hwee Tou Ng. 2007. Learning pre-
dictive structures for semantic role labeling of nom-
bank. In Proceedings of ACL-2007, pages 208?215,
Prague, Czech.
38
Lluis Marquez, Mihai Surdeanu, Pere Comas, and
Jordi Turmo. 2005. A robust combination strat-
egy for semantic role labeling. In Proceedings
of HLT/EMNLP-2005, page 644?651, Vancouver,
Canada.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
CoNLL-X, New York City, June.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In Proceedings of HLT/NAACL
Workshop on Frontiers in Corpus Annotation, pages
24?31, Boston, Massachusetts, USA, May 6.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. 2004. Shallow
semantic parsing using support vector machines. In
Proceedings of HLT/NAACL-2004, pages 233?240,
Boston, Massachusetts, USA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role labeling using different syntactic views.
In Proceedings of ACL-2005, pages 581?588, Ann
Arbor, USA.
James Pustejovsky, Adam Meyers, Martha Palmer, and
Massimo Poesio. 2005. Merging propbank, nom-
bank, timebank, penn discourse treebank and coref-
erence. In Proceedings of the Workshop on Frontiers
in Corpus Annotations II: Pie in the Sky, pages 5?12,
Ann Arbor, USA.
Mihai Surdeanu, Lluis Marquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for
semantic role labeling. Journal of Artificial Intelli-
gence Research, 29:105?151.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
CoNLL-2008, pages 159?177, Manchester, UK.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarisation
for synchronous parsing of semantic and syntactic
dependencies. In IJCAI-2009, Pasadena, California,
USA.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-2005, pages
589?596, Ann Arbor, USA.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP-2004, pages 88?94, Barcelona, Spain,
July 25-26.
Nianwen Xue. 2006. Semantic role labeling of nom-
inalized predicates in chinese. In Proceedings of
NAACL-2006, pages 431?438, New York City, USA,
June.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage max-
imum entropy models. In Proceeding of CoNLL-
2008, pages 203?207, Manchester, UK.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009a. Multilin-
gual dependency learning: Exploiting rich features
for tagging syntactic and semantic dependencies. In
Proceedings of the 13th Conference on Computa-
tional Natural Language Learning (CoNLL-2009),
June 4-5, pages 61?66, Boulder, Colorado, USA.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009b. Multilingual dependency learning:
A huge feature engineering method to semantic de-
pendency parsing. In Proceedings of CoNLL-2009,
pages 55?60, Boulder, Colorado, USA.
39
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 879?887,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Character-Level Dependencies in Chinese: Usefulness and Learning
Hai Zhao
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Avenue, Kowloon, Hong Kong, China
haizhao@cityu.edu.hk
Abstract
We investigate the possibility of exploit-
ing character-based dependency for Chi-
nese information processing. As Chinese
text is made up of character sequences
rather than word sequences, word in Chi-
nese is not so natural a concept as in En-
glish, nor is word easy to be defined with-
out argument for such a language. There-
fore we propose a character-level depen-
dency scheme to represent primary lin-
guistic relationships within a Chinese sen-
tence. The usefulness of character depen-
dencies are verified through two special-
ized dependency parsing tasks. The first
is to handle trivial character dependencies
that are equally transformed from tradi-
tional word boundaries. The second fur-
thermore considers the case that annotated
internal character dependencies inside a
word are involved. Both of these results
from character-level dependency parsing
are positive. This study provides an alter-
native way to formularize basic character-
and word-level representation for Chinese.
1 Introduction
In many human languages, word can be naturally
identified from writing. However, this is not the
case for Chinese, for Chinese is born to be written
in character1 sequence rather than word sequence,
namely, no natural separators such as blanks ex-
ist between words. As word does not appear in
a natural way as most European languages2 , it
1Character here stands for various tokens occurring in
a naturally written Chinese text, including Chinese charac-
ter(hanzi), punctuation, and foreign letters. However, Chi-
nese characters often cover the most part.
2Even in European languages, a naive but necessary
method to properly define word is to list them all by hand.
Thank the first anonymous reviewer who points this fact.
brings the argument about how to determine the
word-hood in Chinese. Linguists? views about
what is a Chinese word diverge so greatly that
multiple word segmentation standards have been
proposed for computational linguistics tasks since
the first Bakeoff (Bakeoff-1, or Bakeoff-2003)3
(Sproat and Emerson, 2003).
Up to Bakeoff-4, seven word segmentation stan-
dards have been proposed. However, this does not
effectively solve the open problem what a Chi-
nese word should exactly be but raises another is-
sue: what a segmentation standard should be se-
lected for the successive application. As word
often plays a basic role for the further language
processing, if it cannot be determined in a uni-
fied way, then all successive tasks will be affected
more or less.
Motivated by dependency representation for
syntactic parsing since (Collins, 1999) that has
been drawn more and more interests in recent
years, we suggest that character-level dependen-
cies can be adopted to alleviate this difficulty in
Chinese processing. If we regard traditional word
boundary as a linear representation for neighbored
characters, then character-level dependencies can
provide a way to represent non-linear relations be-
tween non-neighbored characters. To show that
character dependencies can be useful, we develop
a parsing scheme for the related learning task and
demonstrate its effectiveness.
The rest of the paper is organized as fol-
lows. The next section shows the drawbacks of
the current word boundary representation through
some language examples. Section 3 describes
a character-level dependency parsing scheme for
traditional word segmentation task and reports its
evaluation results. Section 4 verifies the useful-
ness of annotated character dependencies inside a
word. Section 5 looks into a few issues concern-
3First International Chinese Word Segmentation Bakeoff,
available at http://www.sighan.org/bakeoff2003.
879
ing the role of character dependencies. Section 6
concludes the paper.
2 To Segment or Not: That Is the
Question
Though most words can be unambiguously de-
fined in Chinese text, some word boundaries are
not so easily determined. We show such three ex-
amples as the following.
The first example is from the MSRA segmented
corpus of Bakeoff-2 (Bakeoff-2005) (Emerson,
2005):
? / ? / / / ????????? / ?
l /\| /y /0
a / piece of / ? / Beijing City Beijing Opera
OK Sodality / member / entrance / ticket / ?
As the guideline of MSRA standard requires any
organization?s full name as a word, many long
words in this form are frequently encountered.
Though this type of ?words? may be regarded as an
effective unit to some extent, some smaller mean-
ingful constituents can be still identified inside
them. Some researchers argue that these should
be seen as phrases rather than words. In fact, e.g.,
a machine translation system will have to segment
this type of words into some smaller units for a
proper translation.
The second example is from the PKU corpus of
Bakeoff-2,
?I / 7 / H? / ?,
China / in / South Africa / embassy
(the Chinese embassy in South Africa)
This example demonstrates how researchers can
also feel inconvenient if an organization name is
segmented into pieces. Though the word ??
,?(embassy) is right after ?H??(South Africa)
in the above phrase, the embassy does not belong
to South Africa but China, and it is only located in
South Africa.
The third example is an abbreviation that makes
use of the characteristics of Chinese characters.
(? / ? / n / ?
Week / one / three / five
(Monday, Wednesday and Friday)
This example shows that there will be in a
dilemma to perform segmentation over these char-
acters. If a segmentation position locates before
?n?(three) or ???(five), then this will make them
meaningless or losing its original meaning at least
because either of these two characters should log-
ically follow the substring ?(?? (week) to con-
struct the expected word ?(?n?(Wednesday) or
?(??? (Friday). Otherwise, to make all the
above five characters as a word will have to ig-
nore all these logical dependent relations among
these characters and segment it later for a proper
tackling as the above first example.
All these examples suggest that dependencies
exist between discontinuous characters, and word
boundary representation is insufficient to handle
these cases. This motivates us to introduce char-
acter dependencies.
3 Character-Level Dependency Parsing
Character dependency is proposed as an alterna-
tive to word boundary. The idea itself is extremely
simple, character dependencies inside sequence
are annotated or formally defined in the similar
way that syntactic dependencies over words are
usually annotated.
We will initially develop a character-level de-
pendency parsing scheme in this section. Es-
pecially, we show character dependencies, even
those trivial ones that are equally transformed
from pre-defined word boundaries, can be effec-
tively captured in a parsing way.
3.1 Formularization
Using a character-level dependency representa-
tion, we first show how a word segmentation task
can be transformed into a dependency parsing
problem. Since word segmentation is traditionally
formularized as an unlabeled character chunking
task since (Xue, 2003), only unlabeled dependen-
cies are concerned in the transformation. There are
many ways to transform chunks in a sequence into
dependency representation. However, for the sake
of simplicity, only well-formed and projective out-
put sequences are considered for our processing.
Borrowing the notation from (Nivre and Nils-
son, 2005), an unlabeled dependency graph is for-
mally defined as follows:
An unlabeled dependency graph for a string
of cliques (i.e., words and characters) W =
880
Figure 1: Two character dependency schemes
w1...wn is an unlabeled directed graph D =
(W,A), where
(a) W is the set of ordered nodes, i.e. clique
tokens in the input string, ordered by a
linear precedence relation <,
(b) A is a set of unlabeled arcs (wi, wj),
where wi, wj ? W ,
If (wi, wj) ? A, wi is called the head of wj
and wj a dependent of wi. Traditionally, the no-
tation wi ? wj means (wi, wj) ? A; wi ??
wj denotes the reflexive and transitive closure of
the (unlabeled) arc relation. We assume that the
designed dependency structure satisfies the fol-
lowing common constraints in existing literature
(Nivre, 2006).
(1) D is weakly connected, that is, the cor-
responding undirected graph is connected.
(CONNECTEDNESS)
(2) The graph D is acyclic, i.e., if wi ? wj then
not wj ?? wi. (ACYCLICITY)
(3) There is at most one arc (wi, wj) ? A, ?wj ?
W . (SINGLE-HEAD)
(4) An arc wi ? wk is projective iff, for every
word wj occurring between wi and wk in the
string (wi < wj < wk or wi > wj > wk),
wi ?? wj . (PROJECTIVITY)
We say that D is well-formed iff it is acyclic and
connected, and D is projective iff every arcs in A
are projective. Note that the above four conditions
entail that the graph D is a single-rooted tree. For
an arc wi ? wj , if wi < wj , then it is called right-
arc, otherwise left-arc.
Following the above four constraints and con-
sidering segmentation characteristics, we may
have two character dependency representation
schemes as shown in Figure 1 by using a series
of trivial dependencies inside or outside a word.
Note that we use arc direction to distinguish con-
nected and segmented relation among characters.
The scheme with the assistant root node before the
sequence in Figure 1 is called Scheme B, and the
other Scheme E.
3.2 Shift-reduce Parsing
According to (McDonald and Nivre, 2007), all
data-driven models for dependency parsing that
have been proposed in recent years can be de-
scribed as either graph-based or transition-based.
Since both dependency schemes that we construct
for parsing are well-formed and projective, the lat-
ter is chosen as the parsing framework for the sake
of efficiency. In detail, a shift-reduce method is
adopted as in (Nivre, 2003).
The method is step-wise and a classifier is used
to make a parsing decision step by step. In each
step, the classifier checks a clique pair 4, namely,
TOP, the top of a stack that consists of the pro-
cessed cliques, and, INPUT, the first clique in the
unprocessed sequence, to determine if a dependent
relation should be established between them. Be-
sides two arc-building actions, a shift action and a
reduce action are also defined, as follows,
Left-arc: Add an arc from INPUT to TOP and
pop the stack.
Right-arc: Add an arc from TOP to INPUT and
push INPUT onto the stack.
Reduce: Pop TOP from the stack.
Shift: Push INPUT onto the stack.
In this work, we adopt a left-to-right arc-eager
parsing model, that means that the parser scans the
input sequence from left to right and right depen-
dents are attached to their heads as soon as possi-
ble (Hall et al, 2007). In the implementation, as
for Scheme E, all four actions are required to pass
through an input sequence. However, only three
actions, i.e., reduce action will never be used, are
needed for Scheme B.
3.3 Learning Model and Features
While memory-based and margin-based learn-
ing approaches such as support vector machines
are popularly applied to shift-reduce parsing, we
apply maximum entropy model as the learning
model for efficient training and producing some
comparable results. Our implementation of max-
imum entropy adopts L-BFGS algorithm for pa-
rameter optimization as usual. No additional fea-
ture selection techniques are used.
With notations defined in Table 1, a feature set
as shown in Table 2 is adopted. Here, we explain
some terms in Tables 1 and 2.
4Here, clique means character or word in a sequence,
which depends on what constructs the sequence.
881
Table 1: Feature Notations
Notation Meaning
s The character in the top of stack
s?1,... The first character below the top of stack, etc.
i, i+1,... The first (second) character in the
unprocessed sequence, etc.
dprel Dependent label
h Head
lm Leftmost child
rm Rightmost child
rn Right nearest child
char Character form
. ?s, i.e., ?s.dprel? means dependent label
of character in the top of stack
+ Feature combination, i.e., ?s.char+i.char?
means both s.char and i.char work as a
feature function.
Since we only considered unlabeled depen-
dency parsing, dprel means the arc direction from
the head, either left or right. The feature cur-
root returns the root of a partial parsing tree that
includes a specified node. The feature cnseq re-
turns a substring started from a given character. It
checks the direction of the arc that passes the given
character and collects all characters with the same
arc direction to yield an output substring until the
arc direction is changed. Note that all combina-
tional features concerned with this one can be re-
garded as word-level features.
The feature av is derived from unsupervised
segmentation as in (Zhao and Kit, 2008a), and
the accessor variety (AV) (Feng et al, 2004) is
adopted as the unsupervised segmentation crite-
rion. The AV value of a substring s is defined as
AV (s) = min{Lav(s), Rav(s)},
where the left and right AV values Lav(s) and
Rav(s) are defined, respectively, as the numbers
of its distinct predecessor and successor charac-
ters. In this work, AV values for substrings are
derived from unlabeled training and test corpora
by substring counting. Multiple features are used
to represent substrings of various lengths identi-
fied by the AV criterion. Formally put, the feature
function for a n-character substring s with a score
AV (s) is defined as
avn = t, if 2t ? AV (s) < 2t+1, (1)
where t is an integer to logarithmize the score and
taken as the feature value. For an overlap character
of several substrings, we only choose the one with
Table 2: Features for Parsing
Basic Extension
x.char itself, its previous two and next two
characters, and all bigrams within the
five-character window. (x is s or i.)
s.h.char
s.dprel
s.rm.dprel
s?1.cnseq
s?1.cnseq+s.char
s?1.curroot.lm.cnseq
s?1.curroot.lm.cnseq+s.char
s?1.curroot.lm.cnseq+i.char
s?1.curroot.lm.cnseq+s?1.cnseq
s?1.curroot.lm.cnseq+s.char+s?1.cnseq
s?1.curroot.lm.cnseq+i.char+s?1.cnseq
s.avn+i.avn, n = 1, 2, 3, 4, 5
preact?1
preact?2
preact?2+preact?1
the greatest AV score to activate the above feature
function for that character.
The feature preactn returns the previous pars-
ing action type, and the subscript n stands for the
action order before the current action.
3.4 Decoding
Without Markovian feature like preact?1, a shift-
reduce parser can scan through an input sequence
in linear time. That is, the decoding of a parsing
method for word segmentation will be extremely
fast. The time complexity of decoding will be 2L
for Scheme E, and L for Scheme B, where L is
the length of the input sequence.
However, it is somewhat complicated as Marko-
vian features are involved. Following the work of
(Duan et al, 2007), the decoding in this case is to
search a parsing action sequence with the maximal
probability.
Sdi = argmax
?
i
p(di|di?1di?2...),
where Sdi is the object parsing action sequence,
p(di|di?1...) is the conditional probability, and di
is i-th parsing action. We use a beam search al-
gorithm as in (Ratnaparkhi, 1996) to find the ob-
ject parsing action sequence. The time complex-
ity of this beam search algorithm will be 4BL for
Scheme E and 3BL for Scheme B, where B is the
beam width.
3.5 Related Methods
Among character-based learning techniques for
word segmentation, we may identify two main
882
types, classification (GOH et al, 2004) and tag-
ging (Low et al, 2005). Both character classifi-
cation and tagging need to define the position of
character inside a word. Traditionally, the four
tags, b, m, e, and s stand, respectively, for the
beginning, midle, end of a word, and a single-
character as word since (Xue, 2003). The follow-
ing n-gram features from (Xue, 2003; Low et al,
2005) are used as basic features,
(a) Cn(n = ?2,?1, 0, 1, 2),
(b) CnCn+1(n = ?2,?1, 0, 1),
(c) C?1C1,
where C stands for a character and the subscripts
for the relative order to the current character C0. In
addition, the feature av that is defined in equation
(1) is also taken as an option. avn (n=1,...,5) is
applied as feature for the current character.
While word segmentation is conducted as a
classification task, each individual character will
be simply assigned a tag with the maximal prob-
ability given by the classifier. In this case, we re-
store word boundary only according to two tags
b and s. However, the output tag sequence given
by character classification may include illegal tag
transition (e.g., m is after e.). In (Low et al, 2005),
a dynamic programming algorithm is adopted to
find a tag sequence with the maximal joint prob-
ability from all legal tag sequences. If such a dy-
namic programming decoding is adopted, then this
method for word segmentation is regarded as char-
acter tagging 5.
The time complexity of character-based classifi-
cation method for decoding is L, which is the best
result in decoding velocity. As dynamic program-
ming is applied, the time complexity will be 16L
with four tags.
Recently, conditional random fields (CRFs) be-
comes popular for word segmentation since it pro-
vides slightly better performance than maximum
entropy method does (Peng et al, 2004). How-
ever, CRFs is a structural learning tool rather than
a simple classification framework. As shift-reduce
parsing is a typical step-wise method that checks
5Someone may argue that maximum entropy Markov
model (MEMM) is truly a tagging tool. Yes, this method was
initialized by (Xue, 2003). However, our empirical results
show that MEMM never outperforms maximum entropy plus
dynamic programming decoding as (Low et al, 2005) in Chi-
nese word segmentation. We also know that the latter reports
the best results in Bakeoff-2. This is why MEMM method is
excluded from our comparison.
each character one by one, it is reasonable to com-
pare it to a classification method over characters.
3.6 Evaluation Results
Table 3: Corpus size of Bakeoff-2 in number of
words
AS CityU MSRA PKU
Training(M) 5.45 1.46 2.37 1.1
Test(K) 122 41 107 104
The experiments in this section are performed
in all four corpora from Bakeoff-2. Corpus size
information is in Table 3.
Traditionally, word segmentation performance
is measured by F-score ( F = 2RP/(R + P ) ),
where the recall (R) and precision (P ) are the pro-
portions of the correctly segmented words to all
words in, respectively, the gold-standard segmen-
tation and a segmenter?s output. To compute the
word F-score, all parsing results will be restored
to word boundaries according to the direction of
output arcs.
Table 4: The results of parsing and classifica-
tion/tagging approaches using different feature
combinations
S.a Feature AS CityU MSRA PKU
Basicb .935 .922 .950 .917
B +AVc .941 .933 .956 .927
+Prevd .937 .923 .951 .918
+AV+Prev .942 .935 .958 .929
Basic .940 .932 .957 .926
E +AV .948 .947 .964 .942
+Prev .944 .940 .962 .931
+AV+Prev .949 .951 .967 .943
n-gram/ce .933 .923 .948 .923
Cf +AV/c .942 .936 .957 .933
n-gram/dg .945 .938 .956 .936
+AV/d .950 .949 .966 .945
aScheme
bFeatures in top two blocks of Table 2.
cFive av features are added on the above basic features.
dThree Markovian features in Table 2 are added on the above
basic features.
e/c: Classification
fCharacter classification or tagging using maximum entropy
g/d: Only search in legal tag sequences.
Our comparison with existing work will be con-
ducted in closed test of Bakeoff. The rule for the
closed test is that no additional information be-
yond training corpus is allowed, while open test
of Bakeoff is without such restrict.
883
The results with different dependency schemes
are in Table 4. As the feature preact is involved,
a beam search algorithm with width 5 is used to
decode, otherwise, a simple shift-reduce decod-
ing is used. We see that the performance given
by Scheme E is much better than that by Scheme
B. The results of character-based classification
and tagging methods are at the bottom of Table 46.
It is observed that the parsing method outperforms
classification and tagging method without Marko-
vian features or decoding throughout the whole se-
quence. As full features are used, the former and
the latter provide the similar performance.
Due to using a global model like CRFs, our pre-
vious work in (Zhao et al, 2006; Zhao and Kit,
2008c) reported the best results over the evaluated
corpora of Bakeoff-2 until now7. Though those
results are slightly better than the results here, we
still see that the results of character-level depen-
dency parsing approach (Scheme E) are compara-
ble to those state-of-the-art ones on each evaluated
corpus.
4 Character Dependencies inside a Word
We further consider exploiting annotated charac-
ter dependencies inside a word (internal depen-
dencies). A parsing task for these internal de-
pendencies incorporated with trivial external de-
pendencies 8 that are transformed from common
word boundaries are correspondingly proposed us-
ing the same parsing way as the previous section.
4.1 Annotation of Internal Dependencies
In Subsection 3.1, we assign trivial character de-
pendencies inside a word for the parsing task of
word segmentation, i.e., each character as the head
of its predecessor or successor. These trivial for-
mally defined dependencies may be against the
syntactic or semantic senses of those characters,
as we have discussed in Section 2. Now we will
consider human annotated character dependencies
inside a word.
As such an corpus with annotated inter-
nal dependencies has not been available until
6Only the results of open track are reported in (Low et
al., 2005), while we give a comparison following closed track
rules, so, our results here are not comparable to those of (Low
et al, 2005).
7As n-gram features are used, F-scores in (Zhao et al,
2006) are, AS:0.953, CityU:0.948, MSRA:0.974,PKU:0.952.
8We correspondingly call dependencies that mark word
boundary external dependencies that correspond to internal
dependencies.
now, we launched an annotation job based on
UPUC segmented corpus of Bakeoff-3(Bakeoff-
2006)(Levow, 2006). The training corpus is with
880K characters and test corpus 270K. However,
the essential of the annotation job is actually con-
ducted in a lexicon.
After a lexicon is extracted from CTB seg-
mented corpus, we use a top-down strategy to an-
notate internal dependencies inside these words
from the lexicon. A long word is first split
into some smaller constituents, and dependencies
among these constituents are determined, char-
acter dependencies inside each constituents are
then annotated. Some simple rules are adopted
to determine dependency relation, e.g., modifiers
are kept marking as dependants and the only
rest constituent will be marked as head at last.
Some words are hard to determine internal depen-
dency relation, such as foreign names, e.g., ??
:??(Portugal) and ??.?B?(Maradona), and
uninterrupted words (??), e.g., ????(ant)
and ?"h?(clover). In this case, we simply adopt
a series of linear dependencies with the last char-
acter as head to mark these words.
In the previous section, we have shown that
Scheme E is a better dependency representation
for encoding word boundaries. Thus annotated
internal dependencies are used to replace those
trivial internal dependencies in Scheme E to ob-
tain the corpus that we require. Note that now
we cannot distinguish internal and external de-
pendencies only according to the arc direction
any more, as both left- and right-arc can ap-
pear for internal character dependency represen-
tation. Thus two labeled left arcs, external and
internal, are used for the annotation disambigua-
tion. As internal dependencies are introduced,
we find that some words (about 10%) are con-
structed by two or more parallel constituent parts
according to our annotations, this not only lets
two labeled arcs insufficiently distinguish internal-
and external dependencies, but also makes pars-
ing extremely difficult, namely, a great amount
of non-projective dependencies will appear if we
directly introduce these internal dependencies.
Again, we adopt a series of linear dependencies
with the last character as head to represent in-
ternal dependencies for these words by ignor-
ing their parallel constituents. To handle the re-
mained non-projectivities, a strengthened pseudo-
projectivization technique as in (Zhao and Kit,
884
Figure 2: Annotated internal dependencies (Arc
label e notes trivial external dependencies.)
Table 5: Features for internal dependency parsing
Basic Extension
s.char itself, its next two characters, and all bigrams
within the three-character window.
i.char its previous one and next three characters, and
all bigrams within the four-character window.
s.char+i.char
s.h.char
s.rm.dprel
s.curtree
s.curtree+s.char
s?1.curtree+s.char
s.curroot.lm.curtree
s?1.curroot.lm.curtree
s.curroot.lm.curtree+s.char
s?1.curroot.lm.curtree+s.char
s.curtree+s.curroot.lm.curtree
s?1.curtree+s?1.curroot.lm.curtree
s.curtree+s.curroot.lm.curtree+s.char
s?1.curtree+s?1.curroot.lm.curtree+s.char
s?1.curtree+s?1.curroot.lm.curtree+i.char
x.avn, n = 1, ..., 5 (x is s or i.)
s.avn+i.avn, n = 1, ..., 5
preact?1
preact?2
preact?2+preact?1
2008b) is used during parsing. An annotated ex-
ample is illustrated in Figure 2.
4.2 Learning of Internal Dependencies
To demonstrate internal character dependencies
are helpful for further processing. A series of
similar word segmentation experiments as in Sub-
section 3.6 are performed. Note that this task is
slightly different from the previous one, as it is a
five-class parsing action classification task as left
arc has two labels to differ internal and external
dependencies. Thus a different feature set has to
be used. However, all input sequences are still pro-
jective.
Features listed in Table 5 are adopted for the
parsing task that annotated character dependencies
exist inside words. The feature curtree in Table
5 is similar to cnseq of Table 2. It first greedily
searches all connected character started from the
given one until an arc with external label is found
over some character. Then it collects all characters
that has been reached to yield an output substring
as feature value.
A comparison of classification/tagging and
parsing methods is given in Table 6. To evalu-
ate the results with word F-score, all external de-
pendencies in outputs are restored as word bound-
aries. There are three models are evaluated in Ta-
ble 6. It is shown that there is a significant perfor-
mance enhancement as annotated internal charac-
ter dependency is introduced. This positive result
shows that annotated internal character dependen-
cies are meaningful.
Table 6: Comparison of different methods
Approach a basic +AV +Prevb +AV+Prev
Class/Tagc .918 .935 .928 .941
Parsing/wod .921 .937 .924 .942
Parsing/w e .925 .940 .929 .945
aThe highest F-score in Bakeoff-3 is 0.933.
bAs for the tagging method, this means dynamic pro-
gramming decoding; As for the parsing method, this means
three Markovian features.
cCharacter-based classification or tagging method
dUsing trivial internal dependencies in Scheme E.
eUsing annotated internal character dependencies.
5 Is Word Still Necessary?
Note that this work is not about joint learning
of word boundaries and syntactic dependencies
such as (Luo, 2003), where a character-based tag-
ging method is used for syntactic constituent pars-
ing from unsegmented Chinese text. Instead, this
work is to explore an alternative way to repre-
sent ?word-hood? in Chinese, which is based on
character-level dependencies instead of traditional
word boundaries definition.
Though considering dependencies among
words is not novel (Gao and Suzuki, 2004),
we recognize that this study is the first work
concerned with character dependency. This
study originally intends to lead us to consider an
alternative way that can play the similar role as
word boundary annotations.
In Chinese, not word but character is the actual
minimal unit for either writing or speaking. Word-
hood has been carefully defined by many means,
and this effort results in multi-standard segmented
corpora provided by a series of Bakeoff evalu-
ations. However, from the view of linguistics,
Bakeoff does not solve the problem but technically
skirts round it. As one asks what a Chinese word
is, Bakeoff just answers that we have many def-
initions and each one is fine. Instead, motivated
from the results of the previous two sections, we
885
suggest that character dependency representation
could present a natural and unified way to allevi-
ate the drawbacks of word boundary representa-
tion that is only able to represent the relation of
neighbored characters.
Table 7: What we have done for character depen-
dency
Internal External Our work
trivial trivial Section 3
annotated trivial Section 4
annotated ?
If we regard that our current work is stepping
into more and more annotated character dependen-
cies as shown in Table 7, then it is natural to ex-
tend annotated internal character dependencies to
the whole sequence without those unnatural word
boundary constraints. In this sense, internal and
external character dependency will not need be
differed any more. A full character-level depen-
dency tree is illustrated as shown in Figure 3(a)9
With the help of such a tree, we may define word
or even phrase according to what part of subtree is
picked up. Word-hood, if we still need this con-
cept, can be freely determined later as further pro-
cessing purpose requires.
(a)
(b)
Figure 3: Extended character dependencies
Basically we only consider unlabeled depen-
dencies in this work, and dependant labels can be
emptied to do something else, e.g., Figure 3(b)
shows how to extend internal character dependen-
cies of Figure 2 to accommodate part-of-speech
tags. This extension can also be transplanted to a
full character dependency tree of Figure 3(a), then
this may leads to a character-based labeled syntac-
tic dependency tree. In brief, we see that charac-
9We may easily build such a corpus by embedding an-
notated internal dependencies into a word-level dependency
tree bank. As UPUC corpus of Bakeoff-3 just follows the
word segmentation convention of Chinese tree bank, we have
built such a full character-level dependency tree corpus.
ter dependencies provide a more general and nat-
ural way to reflect character relations within a se-
quence than word boundary annotations do.
6 Conclusion and Future Work
In this study, we initially investigate the possibil-
ity of exploiting character dependencies for Chi-
nese. To show that character-level dependency
can be a good alternative to word boundary rep-
resentation for Chinese, we carry out a series of
parsing experiments. The techniques are devel-
oped step by step. Firstly, we show that word seg-
mentation task can be effectively re-formularized
character-level dependency parsing. The results of
a character-level dependency parser can be com-
parable with traditional methods. Secondly, we
consider annotated character dependencies inside
a word. We show that a parser can still effectively
capture both these annotated internal character de-
pendencies and trivial external dependencies that
are transformed from word boundaries. The exper-
imental results show that annotated internal depen-
dencies even bring performance enhancement and
indirectly verify the usefulness of them. Finally,
we suggest that a full annotated character depen-
dency tree can be constructed over all possible
character pairs within a given sequence, though its
usefulness needs to be explored in the future.
Acknowledgements
This work is beneficial from many sources, in-
cluding three anonymous reviewers. Especially,
the authors are grateful to two colleagues, one re-
viewer from EMNLP-2008 who gave some very
insightful comments to help us extend this work,
and Mr. SONG Yan who annotated internal depen-
dencies of top frequent 22K words extracted from
UPUC segmentation corpus. Of course, it is the
duty of the first author if there still exists anything
wrong in this work.
References
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic parsing action models for multi-lingual de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
940?946, Prague, Czech, June 28-30.
886
Thomas Emerson. 2005. The second international
Chinese word segmentation bakeoff. In Proceed-
ings of the Fourth SIGHAN Workshop on Chinese
Language Processing, pages 123?133, Jeju Island,
Korea, October 14-15.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics,
30(1):75?93.
Jianfeng Gao and Hisami Suzuki. 2004. Capturing
long distance dependency in language modeling: An
empirical study. In K.-Y. Su, J. Tsujii, J. H. Lee, and
O. Y. Kwong, editors, Natural Language Processing
- IJCNLP 2004, volume 3248 of Lecture Notes in
Computer Science, pages 396?405, Sanya, Hainan
Island, China, March 22-24.
Chooi-Ling GOH, Masayuki Asahara, and Yuji Mat-
sumoto. 2004. Chinese word segmentation by clas-
sification of characters. In ACL SIGHAN Workshop
2004, pages 57?64, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
Johan Hall, Jens Nilsson, Joakim Nivre,
Gu?lsen Eryig?it, Bea?ta Megyesi, Mattias Nils-
son, and Markus Saers. 2007. Single malt or
blended? a study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 933?939,
Prague, Czech, June.
Gina-Anne Levow. 2006. The third international Chi-
nese language processing bakeoff: Word segmen-
tation and named entity recognition. In Proceed-
ings of the Fifth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 108?117, Sydney, Aus-
tralia, July 22-23.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to Chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161?164, Jeju Island, Korea, October 14-15.
Xiaoqiang Luo. 2003. A maximum entropy chinese
character-based parser. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2003), pages 192 ? 199,
Sapporo, Japan, July 11-12.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2007), pages 122?131,
Prague, Czech, June 28-30.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL-2005), pages 99?106, Ann
Arbor, Michigan, USA, June 25-30.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT 03), pages 149?160, Nancy, France, April
23-25.
Joakim Nivre. 2006. Constraints on non-projective de-
pendency parsing. In Proceedings of 11th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-2006), pages 73?
80, Trento, Italy, April 3-7.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In COLING
2004, pages 562?568, Geneva, Switzerland, August
23-27.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the Empiri-
cal Method in Natural Language Processing Confer-
ence, pages 133?142, University of Pennsylvania.
Richard Sproat and Thomas Emerson. 2003. The first
international Chinese word segmentation bakeoff.
In The Second SIGHAN Workshop on Chinese Lan-
guage Processing, pages 133?143, Sapporo, Japan.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29?48.
Hai Zhao and Chunyu Kit. 2008a. Exploiting unla-
beled text with different unsupervised segmentation
criteria for chinese word segmentation. In Research
in Computing Science, volume 33, pages 93?104.
Hai Zhao and Chunyu Kit. 2008b. Parsing syn-
tactic and semantic dependencies with two single-
stage maximum entropy models. In Twelfth Confer-
ence on Computational Natural Language Learning
(CoNLL-2008), pages 203?207, Manchester, UK,
August 16-17.
Hai Zhao and Chunyu Kit. 2008c. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named en-
tity recognition. In The Sixth SIGHAN Workshop
on Chinese Language Processing, pages 106?111,
Hyderabad, India, January 11-12.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of the 20th Asian Pacific
Conference on Language, Information and Compu-
tation, pages 87?94, Wuhan, China, November 1-3.
887
An Empirical Comparison of Goodness Measures for
Unsupervised Chinese Word Segmentation with a Unified Framework
Hai Zhao?and Chunyu Kit
Department of Chinese, Translation and Linguistics,
City University of Hong Kong,
83 Tat Chee Avenue, Kowloon, Hong Kong, China
Email: haizhao@cityu.edu.hk, ctckit@cityu.edu.hk
Abstract
This paper reports our empirical evaluation
and comparison of several popular good-
ness measures for unsupervised segmenta-
tion of Chinese texts using Bakeoff-3 data
sets with a unified framework. Assuming no
prior knowledge about Chinese, this frame-
work relies on a goodness measure to iden-
tify word candidates from unlabeled texts
and then applies a generalized decoding al-
gorithm to find the optimal segmentation
of a sentence into such candidates with the
greatest sum of goodness scores. Exper-
iments show that description length gain
outperforms other measures because of its
strength for identifying short words. Further
performance improvement is also reported,
achieved by proper candidate pruning and
by assemble segmentation to integrate the
strengths of individual measures.
1 Introduction
Unsupervised Chinese word segmentation was ex-
plored in a number of previous works for various
purposes and by various methods (Ge et al, 1999;
Fu and Wang, 1999; Peng and Schuurmans, 2001;
?The research described in this paper was supported by the
Research Grants Council of Hong Kong S.A.R., China, through
the CERG grant 9040861 (CityU 1318/03H) and by City Uni-
versity of Hong Kong through the Strategic Research Grant
7002037. Dr. Hai Zhao was supported by a postdoctoral Re-
search Fellowship in the Department of Chinese, Translation
and Linguistics, City University of Hong Kong. Thanks four
anonymous reviewers for their insightful comments!
SUN et al, 2004; Jin and Tanaka-Ishii, 2006). How-
ever, various heuristic rules are often involved in
most existing works, and there has not been a com-
prehensive comparison of their performance in a
unified way with available large-scale ?gold stan-
dard? data sets, especially, multi-standard ones since
Bakeoff-1 1.
In this paper we will propose a unified frame-
work for unsupervised segmentation of Chinese text.
Four existing approaches to unsupervised segmenta-
tions or word extraction are considered as its special
cases, each with its own goodness measurement to
quantify word likelihood. The output by each ap-
proach will be evaluated using benchmark data sets
of Bakeoff-32 (Levow, 2006). Note that unsuper-
vised segmentation is different from, if not more
complex than, word extraction, in that the former
must carry out the segmentation task for a text, for
which a segmentation (decoding) algorithm is indis-
pensable, whereas the latter only acquires a word
candidate list as output (Chang and Su, 1997; Zhang
et al, 2000).
2 Generalized Framework
We propose a generalized framework to unify the
existing methods for unsupervised segmentation, as-
suming the availability of a list of word candidates
each associated with a goodness for how likely it is
to be a true word. Let W = {{wi, g(wi)}i=1,...,n} be
such a list, where wi is a word candidate and g(wi)
1First International Chinese Word Segmentation Bakeoff, at
http://www.sighan.org/bakeoff2003
2The Third International Chinese Language Processing
Bakeoff, at http://www.sighan.org/bakeoff2006.
9
its goodness function.
Two generalized decoding algorithms, (1) and (2),
are formulated for optimal segmentation of a given
plain text. The first one, decoding algorithm (1), is a
Viterbi-style one to search for the best segmentation
S? for a text T , as follows,
S? = argmax
w1???wi???wn =T
n?
i=1
g(wi), (1)
with all {wi, g(wi)} ? W .
Another algorithm, decoding algorithm (2), is a
maximal-matching one with respect to a goodness
score. It works on T to output the best current word
w? repeatedly with T=t? for the next round as fol-
lows,
{w?, t?} = argmax
wt=T
g(w) (2)
with each {w, g(w)} ? W . This algorithm will back
off to forward maximal matching algorithm if the
goodness function is set to word length. Thus the
former may be regarded as a generalization of the
latter. Symmetrically, it has an inverse version that
works the other way around.
3 Goodness Measurement
An unsupervised segmentation strategy has to rest
on some predefined criterion, e.g., mutual informa-
tion (MI), in order to recognize a substring in the text
as a word. Sproat and Shih (1990) is an early inves-
tigation in this direction. In this study, we examine
four types of goodness measurement for a candidate
substring3. In principle, the higher goodness score
for a candidate, the more possible it is to be a true
word.
Frequency of Substring with Reduction A lin-
ear algorithm was proposed in (Lu? et al, 2004) to
produce a list of such reduced substrings for a given
corpus. The basic idea is that if two partially over-
lapped n-grams have the same frequency in the input
corpus, then the shorter one is discarded as a redun-
dant word candidate. We take the logarithm of FSR
3Although there have been many existing works in this di-
rection (Lua and Gan, 1994; Chien, 1997; Sun et al, 1998;
Zhang et al, 2000; SUN et al, 2004), we have to skip the de-
tails of comparing MI due to the length limitation of this paper.
However, our experiments with MI provide no evidence against
the conclusions in this paper.
as the goodness for a word candidate, i.e.,
gFSR(w) = log(p?(w)) (3)
where p?(w) is w?s frequency in the corpus. This
allows the arithmetic addition in (1). According to
Zipf?s Law (Zipf, 1949), it approximates the use of
the rank of w as its goodness, which would give it
some statistical significance. For the sake of effi-
ciency, only those substrings that occur more than
once are considered qualified word candidates.
Description Length Gain (DLG) The goodness
measure is proposed in (Kit and Wilks, 1999) for
compression-based unsupervised segmentation. The
DLG from extracting all occurrences of xixi+1...xj
(also denoted as xi..j) from a corpus X= x1x2...xn
as a word is defined as
DLG(xi..j) = L(X)? L(X[r ? xi..j ]? xi..j) (4)
where X[r ? xi..j ] represents the resultant corpus
from replacing all instances of xi..j with a new sym-
bol r throughout X and ? denotes the concatenation
of two substrings. L(?) is the empirical description
length of a corpus in bits that can be estimated by the
Shannon-Fano code or Huffman code as below, fol-
lowing classic information theory (Shannon, 1948).
L(X) .= ?|X|
?
x?V
p?(x)log2p?(x) (5)
where | ? | denotes string length, V is the character
vocabulary of X and p?(x) x?s frequency in X . For
a given word candidate w, we define gDLG(w) =
DLG(w). In principle, a substring with a negative
DLG do not bring any positive compression effect
by itself. Thus only substrings with a positive DLG
value are added into our word candidate list.
Accessor Variety (AV) Feng et al (2004) propose
AV as a statistical criterion to measure how likely a
substring is a word. It is reported to handle low-
frequent words particularly well. The AV of a sub-
string xi..j is defined as
AV (xi..j) = min{Lav(xi..j), Rav(xi..j)} (6)
where the left and right accessor variety Lav(xi..j)
and Rav(xi..j) are, respectively, the number of dis-
tinct predecessor and successor characters. For a
similar reason as to FSR, the logarithm of AV is used
10
as goodness measure, and only substrings with AV
> 1 are considered word candidates. That is, we
have gAV (w) = logAV (w) for a word candidate w.
Boundary Entropy (Branching Entropy, BE) It
is proposed as a criterion for unsupervised segmen-
tation in some existing works (Tung and Lee, 1994;
Chang and Su, 1997; Huang and Powers, 2003; Jin
and Tanaka-Ishii, 2006). The local entropy for a
given xi..j , defined as
h(xi..j) = ?
?
x?V
p(x|xi..j)log p(x|xi..j), (7)
indicates the average uncertainty after (or before)
xi..j in the text, where p(x|xi..j) is the co-occurrence
probability for x and xi..j . Two types of h(xi..j),
namely hL(xi..j) and hR(xi..j), can be defined for
the two directions to extend xi..j (Tung and Lee,
1994). Also, we can define hmin = min{hR, hL} in
a similar way as in (6). In this study, only substrings
with BE > 0 are considered word candidates. For a
candidate w, we have gBE (w) = hmin(w)4.
4 Evaluation
The evaluation is conducted with all four corpora
from Bakeoff-3 (Levow, 2006), as summarized in
Table 1 with corpus size in number of characters.
For unsupervised segmentation, the annotation in
the training corpora is not used. Instead, they
are used for our evaluation, for they are large and
thus provide more reliable statistics than small ones.
Segmentation performance is evaluated by word F-
measure F = 2RP/(R + P ). The recall R and
precision P are, respectively, the proportions of the
correctly segmented words to all words in the gold-
standard and a segmenter?s output5.
Note that a decoding algorithm always requires
the goodness score of a single-character candidate
4Both AV and BE share a similar idea from Harris (1970):
If the uncertainty of successive token increases, then it is likely
to be at a boundary. In this sense, one may consider them the
discrete and continuous formulation of the same idea.
5All evaluations will be represented in terms of word
F-measure if not otherwise specified. A standard scoring
tool with this metric can be found in SIGHAN website,
http://www.sighan.org/bakeoff2003/score. However, to com-
pare with related work, we will also adopt boundary F-measure
Fb = 2RbPb/(Rb + Pb), where the boundary recall Rb and
boundary precision Pb are, respectively, the proportions of the
correctly recognized boundaries to all boundaries in the gold-
standard and a segmenter?s output (Ando and Lee, 2000).
Table 1: Bakeoff-3 Corpora
Corpus AS CityU CTB MSRA
Training(M) 8.42 2.71 0.83 2.17
Test(K) 146 364 256 173
Table 2: Performance with decoding algorithm (1)
M. Good- Training corpus
L.a ness AS CityU CTB MSRA
FSR .400 .454 .462 .432
2 DLG/d .592 .610 .604 .603AV .568 .595 .596 .577
BE .559 .587 .592 .572
FSR .193 .251 .268 .235
7 DLG/d .331 .397 .409 .379AV .399 .423 .430 .407
BE .390 .419 .428 .403
aM.L.: Maximal length allowable for word candidates.
for computation. There are two ways to get this
score: (1) computed by the goodness measure,
which is applicable only if the measure allows; (2)
set to zero as default value, which is always appli-
cable even to single-character candidates not in the
word candidate list in use. For example, all single-
character candidates given up by DLG because of
their negative DLG scores will have a default value
during decoding. We will use a ?/d? to indicate ex-
periments using such a default value.
4.1 Comparison
We apply the decoding algorithm (1) to segment all
Bakeoff-3 corpora with the above goodness mea-
sures. Both word candidates and goodness values
are derived from the raw text of each training cor-
pus. The performance of these measures is presented
in Table 2. From the table we can see that DLG
and FSR have the strongest and the weakest perfor-
mance, respectively, whereas AV and BE are highly
comparable to each other.
Decoding algorithm (2) runs the forward and
backward segmentation with the respective AV
and BE criteria, i.e., LAV /hL for backward and
RAV /hR forward, and the output is the union of two
segmentations 6. A performance comparison of AV
and BE with both algorithms (1) and (2) is presented
in Table 3. We can see that the former has a rela-
6That is, all segmented points by either segmentation will be
accounted into the final segmentation.
11
Table 3: Performance comparison: AV vs. BE
M. Good- Training corpus
L. ness AS CityU CTB MSRA
AV(1) .568 .595 .596 .577
AV(2)/d .485 .489 .508 .471
AV(2) .445 .366 .367 .3872 BE(1) .559 .587 .592 .572
BE(2)/d .485 .489 .508 .471
BE(2) .504 .428 .446 .446
AV(1) .399 .423 .430 .407
AV(2)/d .570 .581 .588 .572
AV(2) .445 .366 .368 .3877 BE(1) .390 .419 .428 .403
BE(2)/d .597 .604 .605 .593
BE(2) .508 .431 .449 .446
2 3 4 5 6 70.35
0.4
0.45
0.5
0.55
0.6
 The Range of Word Length
 
F?meas
ure
BE/(2): ASBE/(2): CityUBE/(2): CTBBE/(2): MSRADLG/(1): ASDLG/(1): CityUDLG/(1): CTBDLG/(1): MSRA
Figure 1: Performance vs. word length
tively better performance on shorter words and the
latter outperforms on longer ones.
How segmentation performance varies along with
word length is exemplified with DLG and BE as ex-
amples in Figure 1, with (1) and (2) indicating a re-
spective decoding algorithm in use. It shows that
DLG outperforms on two-character words and BE
on longer ones.
4.2 Word Candidate Pruning
Up to now, word candidates are determined by the
default goodness threshold 0. The number of them
for each of the four goodness measures is presented
in Table 4. We can see that FSR generates the largest
set of word candidates and DLG the smallest. More
interestingly or even surprising, AV and BE generate
exactly the same candidate list for all corpora.
In addition to word length, another crucial factor
to affect segmentation performance is the quality of
the word candidates as a whole. Since each candi-
date is associated with a goodness score to indicate
how good it is, a straightforward way to ensure, and
further enhance, the overall quality of a candidate
set is to prune off those with low goodness scores.
Table 4: Word candidate number by threshold 0
Good- Training Corpus
ness AS CityU CTB MSRA
FSR 2,009K 832K 294K 661K
DLG 543K 265K 96K 232K
AV 1,153K 443K 160K 337K
BE 1,153K 443K 160K 337K
2 3 4 5 6 70.4
0.45
0.5
0.55
0.6
0.65
 The Range of Word Length
 
F?meas
ure
100% size89% size79% size74% size70% size65% size62% size48% size38% size
Figure 2: Performance by candidate pruning: DLG
To examine how segmentation performance changes
along with word candidate pruning and decide the
optimal pruning rate, we conduct a series of experi-
ments with each goodness measurements. Figures 2
and 3 present, as an illustration, the outcomes of two
series of our experiments with DLG by decoding al-
gorithm (1) and BE by decoding algorithm (1) and
(2) on CityU training corpus. We find that appro-
priate pruning does lead to significant performance
improvement and that both DLG and BE keep their
superior performance respectively on two-character
words and others. We also observe that each good-
ness measure has a stable and similar performance
in a range of pruning rates around the optimal one,
e.g., 79-62% around 70% in Figure 2.
The optimal pruning rates found through our ex-
periments for the four goodness measures are given
in Table 5, and their correspondent segmentation
performance in Table 6. These results show a re-
markable performance improvement beyond the de-
2 3 4 5 6 70.4
0.45
0.5
0.55
0.6
0.65
 The Range of Word Length
 
F?meas
ure 100% size/(1)38% size/(1)32% size/(1)19% size/(1)10% size/(1)100% size/(2)27% size/(2)19% size/(2)16% size/(2)13.5% size/(2)11% size/(2)4.5% size/(2)
Figure 3: Performance by candidate pruning: BE
12
Table 5: Optimal rates for candidate pruning (%)
Decoding Goodness measure
algorithm FSR DLG AV BE
(1) 1.8 70 12.5 20
(2) ? ? 8 12.5
Table 6: Performance via optimal candidate pruning
M. Good- Training corpus
L. ness AS CityU CTB MSRA
FSR(1) .501 .525 .513 .522
DLG(1)/d .710 .650 .664 .638
2 AV(1) .616 .625 .609 .618BE(1) .613 .614 .605 .611
AV(2)/d .585 .602 .589 .599
BE(2)/d .591 .599 .596 .593
FSR(1) .444 .491 .486 .486
DLG(1)/d .420 .447 .460 .423
7 AV(1) .517 .568 .549 .544BE(1) .501 .539 .510 .519
AV(2)/d .623 .624 .604 .615
BE(2)/d .630 .631 .620 .622
fault threshold setting. What remains unchanged is
the advantage of DLG for two-character words and
that of AV/BE for longer words. However, DLG
achieves the best overall performance among the
four, although it uses only single- and two-character
word candidates. The overwhelming number of two-
character words in Chinese allows it to triumph.
4.3 Ensemble Segmentation
Although proper pruning of word candidates brings
amazing performance improvement, it is unlikely
for one to determine an optimal pruning rate in prac-
tice for an unlabeled corpus. Here we put forth a
parameter-free method to tackle this problem with
the aids of all available goodness measures.
The first step of this method to do is to derive an
optimal set of word candidates from the input. We
have shown above that quality candidates play a crit-
ical role in achieving quality segmentation. Without
any better goodness criterion available, the best we
can opt for is the intersection of all word candidate
lists generated by available goodness measures with
the default threshold. A good reason for this is that
the agreement of them can give a more reliable de-
cision than any individual one of them. In fact, we
only need DLG and AV/BE to get this intersection,
because AV and BE give the same word candidates
Table 7: Performances of ensemble segmentation
M. Good- Training corpus
L. ness AS CityU CTB MSRA
FSR(1) .629 .635 .624 .623
2 DLG(1)/d .664 .653 .643 .650AV(1) .641 .644 .631 .634
BE(1) .640 .643 .632 .634
7 AV(2)/d .595 .637 .624 .610BE(2)/d .593 .635 .620 .609
DLG(1)/d+AV(2)/d .672 .684 .663 .665
DLG(1)/d+BE(2)/d .660 .681 .656 .653
and DLG generates only a subset of what FSR does.
The next step is to use this intersection set of
word candidates to perform optimal segmentation
with each goodness measures, to see if any fur-
ther improvement can be achieved. The best re-
sults are given in Table 7, showing that decoding al-
gorithm (1) achieves marvelous improvement using
short word candidates with all other goodness mea-
sures than DLG. Interestingly, DLG still remains at
the top by performance despite of some slip-back.
To explore further improvement, we also try
to combine the strengths of DLG and AV/BE re-
spectively for recognizing two- and multi-character
word. Our strategy to combine them together is to
enforce the multi-character words in AV/BE seg-
mentation upon the correspondent parts of DLG seg-
mentation. This ensemble method gives a better
overall performance than all others that we have
tried so far, as presented at the bottom of Table 7.
4.4 Yet Another Decoding Algorithm
Jin and Tanaka-Ishii (2006) give an unsupervised
segmentation criterion, henceforth referred to as de-
coding algorithm (3), to work with BE. It works as
follows: if g(xi..j+1) > g(xi..j) for any two over-
lapped substrings xi..j and xi..j+1, then a segment-
ing point should be located right after xi..j+1. This
algorithm has a forward and a backward version.
The union of the segmentation outputs by both ver-
sions is taken as the final output of the algorithm,
in exactly the same way as how decoding algorithm
(2) works7. This algorithm is evaluated in (Jin and
Tanaka-Ishii, 2006) using Peking University (PKU)
7Three segmentation criteria are given in (Jin and Tanaka-
Ishii, 2006), among which the entropy increase criterion,
namely, decoding algorithm (3), proves to be the best. Here we
would like to thank JIN Zhihui and Prof. Kumiko Tanaka-Ishii
for presenting the details of their algorithms.
13
Table 8: Performance comparison by word and
boundary F-measure on PKU corpus (M. L. = 6)
Good- Decoding algorithm
ness (1)/d (1) (2)/d (2) (3)/d (3)
AV .313 .325 .588 .373 .376 .453
F AV? .372 .372 .663 .663 .445 .445
BE .309 .319 .624 .501 .376 .624
BE? .370 .370 .676 .676 .447 .447
AV .695 .700 .830 .762 .762 .728
Fb AV? .728 .728 .865 .865 .783 .783
BE .696 .699 .849 .810 .762 .837a
BE? .728 .728 .872 .872 .784 .784
aWith the same hyperparameters, (Jin and Tanaka-Ishii, 2006)
report their best result of boundary precision 0.88 and boundary
recall 0.79, equal to boundary F-measure 0.833.
Corpus of 1.1M words8 as gold standard with a word
candidate list extracted from the 200M Contempo-
rary Chinese Corpus that mostly consists of several
years of Peoples? Daily9. Here, we carry out evalu-
ation with similar data: we extract word candidates
from the unlabeled texts of People?s Daily (1993 -
1997), of 213M and about 100M characters, in terms
of the AV and BE criteria, yielding a list of 4.42 mil-
lion candidates up to 6-character long10 for each cri-
terion. Then, the evaluation of the three decoding
algorithms is performed on PKU corpus.
The evaluation results with both word and bound-
ary F-measure are presented for the same segmenta-
tion outputs in Table 8, with ?*? to indicate candi-
date pruning by DLG > 0 as reported before. Note
that boundary F-measure gives much more higher
score than word F-measure for the same segmenta-
tion output. However, in either of metric, we can
find no evidence in favor of decoding algorithm (3).
Undesirably, this algorithm does not guarantee a sta-
ble performance improvement with the BE measure
through candidate pruning.
4.5 Comparison against Supervised
Segmentation
Huang and Zhao (2007) provide empirical evidence
to estimate the degree to which the four segmenta-
tion standards involved in the Bakeoff-3 differ from
each other. As quoted in Table 9, a consistency rate
8http://icl.pku.edu.cn/icl groups/corpus/dwldform1.asp
9http://ccl.pku.edu.cn:8080/ccl corpus/jsearch/index.jsp
10This is to keep consistence with (Jin and Tanaka-Ishii,
2006), where 6 is set as the maximum n-gram length.
Table 9: Consistency rate among Bakeoff-3 segmen-
tation standards (Huang and Zhao, 2007)
Test Training corpus
corpus AS CityU CTB MSRA
AS 1.000 0.926 0.959 0.858
CityU 0.932 1.000 0.935 0.849
CTB 0.942 0.910 1.000 0.877
MSRA 0.857 0.848 0.887 1.000
beyond 84.8% is found among the four standards.
If we do not over-expect unsupervised segmentation
to achieve beyond what these standards agree with
each other, it is reasonable to take this figure as the
topline for evaluation. On the other hand, Zhao et al
(2006) show that the words of 1 to 2 characters long
account for 95% of all words in Chinese texts, and
single-character words alone for about 50%. Thus,
we can take the result of the brute-force guess of ev-
ery single character as a word as a baseline.
To compare to supervised segmentation, which
usually involves training using an annotated train-
ing corpus and, then, evaluation using test corpus,
we carry out unsupervised segmentation in a com-
parable manner. For each data track, we first ex-
tract word candidates from both the training and test
corpora, all unannotated, and then evaluate the un-
supervised segmentation with reference to the gold-
standard segmentation of the test corpus. The re-
sults are presented in Table 10, together with best
and worst official results of the Bakeoff closed test.
This comparison shows that unsupervised segmen-
tation cannot compete against supervised segmenta-
tion in terms of performance. However, the experi-
ments generate positive results that the best combi-
nation of the four goodness measures can achieve an
F-measure in the range of 0.65-0.7 on all test corpora
in use without using any prior knowledge, but ex-
tracting word candidates from the unlabeled training
and test corpora in terms of their goodness scores.
5 Discussion: How Things Happen
Note that DLG criterion is to perform segmentation
with the intension to maximize the compression ef-
fect, which is a global effect through the text. Thus
it works well incorporated with a probability maxi-
mization framework, where high frequent but inde-
pendent substrings are effectively extracted and re-
14
Table 10: Comparison of performances against su-
pervised segmentation
Type Test corpusAS CityU CTB MSRA
Baseline .389 .345 .337 .353
DLG(1)/d .597 .616 .601 .602
DLG?(1)/d .655 .659 .632 .655
2 AV(1) .577 .603 .597 .583
AV?(1) .630 .650 .618 .638
BE(1) .570 .598 .594 .580
BE?(1) .629 .649 .618 .638
AV(2)/d .512 .551 .543 .526
AV?(2)/d .591 .644 .618 .604
7 BE(2)/d .518 .554 .546 .533
BE?(2)/d .587 .641 .614 .605
DLG?(1)/d +AV?(2)/d .663 .692 .658 .667
DLG?(1)/d +BE?(2)/d .650 .689 .650 .656
Worst closed .710 .589 0.818 .819
Best closed .958 .972 0.933 .963
combined. We know that most unsupervised seg-
mentation criteria will bring up long word bias prob-
lem, so does DLG measure. This explains why it
gives the worse results as long candidates are added.
As for AV and BE measures, both of them give the
metric of the uncertainty before or after the current
substring. This means that they are more concerned
with local uncertainty information near the current
substring, instead of global information among the
whole text as DLG. Thus local greedy search in
maximal matching style is more suitable for these
two measures than Viterbi search.
Our empirical results about word candidate list
with default threshold 0, where the same list is from
AV and BE, give another proof that both AV and BE
reflect the same uncertainty. The only difference is
behind the fact that the former and the latter is in the
discrete and continuous formulation, respectively.
6 Conclusion and Future Work
This paper reported our empirical comparison of a
number of goodness measures for unsupervised seg-
mentation of Chinese texts with the aid two gener-
alized decoding algorithms. We learn no previous
work by others for a similar attempt. The compari-
son is carried out with Bakeoff-3 data sets, showing
that all goodness measures exhibit their strengths for
recognizing words of different lengths and achieve a
performance far beyond the baseline. Among them,
DLG with decoding algorithm (1) can achieve the
best segmentation performance for single- and two-
character words identification and the best overall
performance as well. Our experiments also show
that the quality of word candidates plays a criti-
cal role in ensuring segmentation performance 11.
Proper pruning of candidates with low goodness
scores to enhance this quality enhances the seg-
mentation performance significantly. Also, the suc-
cess of unsupervised segmentation depends strongly
on an appropriate decoding algorithm. Generally,
Viterbi-style decoding produces better results than
best-first maximal-matching. But the latter is not shy
from exhibiting its particular strength for identifying
multi-character words.
Finally, the ensemble segmentation we put forth
to combine the strengths of different goodness mea-
sures proves to be a remarkable success. It achieves
an impressive performance improvement on top of
individual goodness measures.
As for future work, it would be natural for re-
searchers to enhance supervised learning for Chi-
nese word segmentation with goodness measures in-
troduced here. There does be two successful exam-
ples in our existing work (Zhao and Kit, 2007). This
is still an ongoing work.
References
Rie Kubota Ando and Lillian Lee. 2000. Mostly-
unsupervised statistical segmentation of Japanese: Ap-
plications to kanji. In Proceedings of the first Confer-
ence on North American Chapter of the Association
for Computational Linguistics and the 6th Conference
on Applied Natural Language Processing, pages 241?
248, Seattle, Washington, April 30.
Jing-Shin Chang and Keh-Yih Su. 1997. An unsuper-
vised iterative method for Chinese new lexicon ex-
traction. Computational Linguistics and Chinese Lan-
guage Processing, 2(2):97?148.
Lee-Feng Chien. 1997. PAT-tree-based keyword extrac-
tion for Chinese information retrieval. In Proceedings
of the 20th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 50?58, Philadelphia.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics,
30(1):75?93.
11This observation is shared by other researchers, e.g., (Peng
et al, 2002).
15
Guo-Hong Fu and Xiao-Long Wang. 1999. Unsu-
pervised Chinese word segmentation and unknown
word identification. In 5th Natural Language Process-
ing Pacific Rim Symposium 1999 (NLPRS?99), ?Clos-
ing the Millennium?, pages 32?37, Beijing, China,
November 5-7.
Xianping Ge, Wanda Pratt, and Padhraic Smyth. 1999.
Discovering Chinese words from unsegmented text. In
SIGIR ?99: Proceedings of the 22nd Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 271?272,
Berkeley, CA, USA, August 15-19. ACM.
Zellig Sabbetai Harris. 1970. Morpheme boundaries
within words. In Papers in Structural and Transfor-
mational Linguistics, page 68?77.
Jin Hu Huang and David Powers. 2003. Chinese
word segmentation based on contextual entropy. In
Dong Hong Ji and Kim-Ten Lua, editors, Proceedings
of the 17th Asian Pacific Conference on Language, In-
formation and Computation, pages 152?158, Sentosa,
Singapore, October, 1-3. COLIPS Publication.
Chang-Ning Huang and Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21(3):8?20.
Zhihui Jin and Kumiko Tanaka-Ishii. 2006. Unsuper-
vised segmentation of Chinese text by use of branch-
ing entropy. In COLING/ACL 2006, pages 428?435,
Sidney, Australia.
Chunyu Kit and Yorick Wilks. 1999. Unsupervised
learning of word boundary with description length
gain. In M. Osborne and E. T. K. Sang, editors,
CoNLL-99, pages 1?6, Bergen, Norway.
Gina-Anne Levow. 2006. The third international Chi-
nese language processing bakeoff: Word segmentation
and named entity recognition. In Proceedings of the
Fifth SIGHAN Workshop on Chinese Language Pro-
cessing, pages 108?117, Sydney, Australia, July.
Xueqiang Lu?, Le Zhang, and Junfeng Hu. 2004. Sta-
tistical substring reduction in linear time. In Keh-
Yih Su et al, editor, Proceeding of the 1st Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP-2004), volume 3248 of Lecture Notes
in Computer Science, pages 320?327, Sanya City,
Hainan Island, China, March 22-24. Springer.
Kim-Teng Lua and Kok-Wee Gan. 1994. An applica-
tion of information theory in Chinese word segmenta-
tion. Computer Processing of Chinese and Oriental
Languages, 8(1):115?123.
Fuchun Peng and Dale Schuurmans. 2001. Self-
supervised Chinese word segmentation. In The Fourth
International Symposium on Intelligent Data Analysis,
pages 238?247, Lisbon, Portugal, September, 13-15.
Fuchun Peng, Xiangji Huang, Dale Schuurmans, Nick
Cercone, and Stephen Robertson. 2002. Using self-
supervised word segmentation in Chinese information
retrieval. In Proceedings of the 25th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 349?350,
Tampere, Finland, August, 11-15.
Claude E. Shannon. 1948. A mathematical theory of
communication. The Bell System Technical Journal,
27:379?423, 623?656, July, October.
Richard Sproat and Chilin Shih. 1990. A statistical
method for finding word boundaries in Chinese text.
Computer Processing of Chinese and Oriental Lan-
guages, 4(4):336?351.
Maosong Sun, Dayang Shen, and Benjamin K. Tsou.
1998. Chinese word segmentation without using lexi-
con and hand-crafted training data. In COLING-ACL
?98, 36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Confer-
ence on Computational Linguistics, volume 2, pages
1265?1271, Montreal, Quebec, Canada.
Mao Song SUN, Ming XIAO, and Benjamin K. Tsou.
2004. Chinese word segmentation without using dic-
tionary based on unsupervised learning strategy (in
Chinese) (????????????????
???????). Chinese Journal of Computers,
27(6):736?742.
Cheng-Huang Tung and His-Jian Lee. 1994. Iden-
tification of unknown words from corpus. Compu-
tational Proceedings of Chinese and Oriental Lan-
guages, 8:131?145.
Jian Zhang, Jianfeng Gao, and Ming Zhou. 2000. Ex-
traction of Chinese compound words ? an experimen-
tal study on a very large corpus. In Proceedings of
the Second Chinese Language Processing Workshop,
pages 132?139, Hong Kong, China.
Hai Zhao and Chunyu Kit. 2007. Incorporating global
information into supervised learning for Chinese word
segmentation. In Proceedings of the 10th Conference
of the Pacific Association for Computational Linguis-
tics, pages 66?74, Melbourne, Australia, September
19-21.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese word
segmentation via conditional random field modeling.
In Proceedings of the 20th Asian Pacific Conference on
Language, Information and Computation, pages 87?
94, Wuhan, China, November 1-3.
George Kingsley Zipf. 1949. Human Behavior and
the Principle of Least Effort. Addison-Wesley, Cam-
bridge, MA.
16
Unsupervised Segmentation Helps Supervised Learning of
Character Tagging for Word Segmentation and Named Entity Recognition
Hai Zhao and Chunyu Kit
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Ave., Kowloon, Hong Kong
Email: {haizhao, ctckit}@cityu.edu.hk
Abstract
This paper describes a novel character tag-
ging approach to Chinese word segmenta-
tion and named entity recognition (NER) for
our participation in Bakeoff-4.1 It integrates
unsupervised segmentation and conditional
random fields (CRFs) learning successfully,
using similar character tags and feature tem-
plates for both word segmentation and NER.
It ranks at the top in all closed tests of word
segmentation and gives promising results for
all closed and open NER tasks in the Bake-
off. Tag set selection and unsupervised seg-
mentation play a critical role in this success.
1 Introduction
A number of recent studies show that character se-
quence labeling is a simple but effective formula-
tion of Chinese word segmentation and name en-
tity recognition for machine learning (Xue, 2003;
Low et al, 2005; Zhao et al, 2006a; Chen et al,
2006). Character tagging becomes a prevailing tech-
nique for this kind of labeling task for Chinese lan-
guage processing, following the current trend of ap-
plying machine learning as a core technology in the
field of natural language processing. In particular,
when a full-fledged general-purpose sequence learn-
ing model such as CRFs is involved, the only work
to do for a given application is to identify an ideal
set of features and hyperparameters for the purpose
1The Fourth International Chinese Language Processing
Bakeoff & the First CIPS Chinese Language Processing Evalu-
ation, at http://www.china-language.gov.cn/bakeoff08/bakeoff-
08 basic.html.
of achieving the best learning model that we can
with available training data. Our work in this aspect
provides a solid foundation for applying an unsuper-
vised segmentation criterion to enrich the supervised
CRFs learning for further performance enhancement
on both word segmentation and NER.
This paper is intended to present the research for
our participation in Bakeoff-4, with a highlight on
our strategy to select character tags and feature tem-
plates for CRFs learning. Particularly worth men-
tioning is the simplicity of our system in contrast to
its success. The rest of the paper is organized as fol-
lows. The next section presents the technical details
of the system and Section 3 its evaluation results.
Section 4 looks into a few issues concerning charac-
ter tag set, unsupervised segmentation, and available
name entities (NEs) as features for open NER test.
Section 5 concludes the paper.
2 System Description
Following our previous work (Zhao et al, 2006a;
Zhao et al, 2006b; Zhao and Kit, 2007), we con-
tinue to apply the order-1 linear chain CRFs (Laf-
ferty et al, 2001) as our learning model for Bakeoff-
4. Specifically, we use its implementation CRF++
by Taku Kudo2 freely available for research purpose.
We opt for a similar set of character tags and feature
templates for both word segmentation and NER.
In addition, two key techniques that we have ex-
plored in our previous work are applied. One is to
introduce more tags in the hope of utilizing more
precise contextual information to achieve more pre-
2http://crfpp.sourceforge.net/
106
Sixth SIGHAN Workshop on Chinese Language Processing
107
Sixth SIGHAN Workshop on Chinese Language Processing
Table 3: Training corpora for assistant learners
Track CityU NER MSRA NER
Ass. Seg. CityU (Bakeoff-1 to 4) MSRA (Bakeoff-2)
ANER-1 CityU(Bakeoff-3) CityU(Bakeoff-3)
ANER-2 MSRA(Bakeoff-3) CityU(Bakeoff-4)
Table 4: NE lists from Chinese Wikipedia
Category Number
Place name suffix 85
Chinese place name 6,367
Foreign place name 1,626
Chinese family name 573
Most common Chinese family name 109
Foreign name 2,591
Chinese university 515
didate s with a score AV (s) is defined as
fn(s) = t, if 2t ? AV (s) < 2t+1,
where t is an integer to logarithmize the score. This
is to alleviate the sparse data problem by narrowing
down the feature representation involved. Note that
t is used as a feature value rather than a parameter
for the CRFs training in our system. For an over-
lap character of several word candidates, we only
choose the one with the greatest AV score to activate
the above feature function for that character. It is
in this way that the unsupervised segmentation out-
comes are fit into the CRFs learning.
2.3 Features for Open NER
Three extra groups of feature template are used for
the open NER beyond those for the closed.
The first group includes three segmentation fea-
ture templates. One is character type feature tem-
plate T (C?1)T (C0)T (C1), where T (C) is the type
of character C. For this, five character types are de-
fined, namdely, number, foreign letter, punctuation,
date and time, and others. The other two are gener-
ated respectively by two assistant segmenters (Zhao
et al, 2006a), a maximal matching segmenter based
on a dictionary from Peking University3 and a CRFs
segmenter using the 6-tag set and the six n-gram fea-
ture templates for training.
3It consists of about 108K words of one to four character-
slong, available at http://ccl.pku.edu.cn/doubtfire/Course/Chi
nese%20Information%20Processing/Source Code/Chapter 8/
Lexicon full.zip.
Table 5: Segmentation results for previous Bakeoffs
Bakeoff-1 AS CityU CTB PKU
?AV
F .9727 .9473 .8720 .9558
ROOVa .7907 .7576 .7022 .7078
+AV
F .9725 .9554 .9023 .9612
ROOV .7597 .7616 .7502 .7208
Bakeoff-2 AS CityU MSRA PKU
?AV
F .9534 .9476 .9735 .9515
ROOV .6812 .6920 .7496 .6720
+AV
F .9570 .9610 .9758 .9540
ROOV .6993 .7540 .7446 .6765
Bakeoff-3 AS CityU CTB MSRA
?AV
F .9538 .9691 .9322 .9608
ROOV .6699 .7815 .7095 .6658
+AV
F .9586 .9747 .9431 .9660
ROOV .6935 .8005 .7608 .6620
aRecall of out-of-vocabulary (OOV) words.
The second group comes from the outputs of
two assistant NE recognizers (ANERs), both trained
with a corresponding 6-tag set and the same six n-
gram feature templates. They share a similar feature
representation as the assistant segmenter. Table 3
lists the training corpora for the assistant CRFs seg-
menter and the ANERs for various open NER tests.
The third group consists of feature templates gen-
erated from seven NE lists acquired from Chinese
Wikipedia.4 The categories and numbers of these
NE items are summarized in Table 4.
3 Evaluation Results
The performance of both word segmentation and
NER is measured in terms of the F-measure F =
2RP/(R + P ), where R and P are the recall and
precision of segmentation or NER.
We tested the techniques described above with
the previous Bakeoffs? data5 (Sproat and Emerson,
2003; Emerson, 2005; Levow, 2006). The evalua-
tion results for the closed tests of word segmentation
are reported in Table 5 and those for the NER on two
corpora of Bakeoff-3 are in the upper part of Table 7.
?+/?AV? indicates whether AV features are applied.
For Bakeoff-4, we participated in all five closed
tracks of word segmentation, namely, CityU, CKIP,
CTB, NCC, and SXU, and in all closed and open
NER tracks of CityU and MSRA.6 The evaluation
4http://zh.wikipedia.org/wiki/??
5http://www.sighan.org
6We declare that our team has never been exposed to the
108
Sixth SIGHAN Workshop on Chinese Language Processing
Table 6: Evaluation results of word segmentation on Bakeoff-4 data sets
Feature Data F P R FIVa PIV RIV FOOV POOV ROOV
CityU .9426 .9410 .9441 .9640 .9636 .9645 .7063 .6960 .7168
CKIP .9421 .9387 .9454 .9607 .9581 .9633 .7113 .7013 .7216
?AV CTB .9634 .9641 .9627 .9738 .9761 .9715 .7924 .7719 .8141
(n-gram) NCC .9333 .9356 .9311 .9536 .9612 .9461 .5678 .5182 .6280
SXU .9552 .9559 .9544 .9721 .9767 .9675 .6640 .6223 .7116
CityU .9510 .9493 .9526 .9667 .9626 .9708 .7698 .7912 .7495
CKIP .9470 .9440 .9501 .9623 .9577 .9669 .7524 .7649 .7404
+AV*b CTB .9589 .9596 .9583 .9697 .9704 .9691 .7745 .7761 .7730
NCC .9405 .9407 .9402 .9573 .9583 .9562 .6080 .5984 .6179
SXU .9623 .9625 .9622 .9752 .9764 .9740 .7292 .7159 .7429
aF-score for in-vocabulary (IV) words.
bHenceforth the official evaluation results in Bakeoff-4 are marked with ?*?.
Table 7: NER evaluation results
Track Setting FPER FLOC FORG FNE
Bakeoff-3
CityU ?AV .8849 .9219 .7905 .8807+AV .9063 .9281 .7981 .8918
MSRA ?AV .7851 .9072 .8242 .8525+AV .8171 .9139 .8164 .8630
Bakeoff-4
?AV .8222 .8682 .6801 .8092
CityU +AV* .8362 .8677 .6852 .8152Open1* .9125 .9216 .7862 .8869
Open2 .9137 .9214 .7853 .8870
?AV .9221 .9193 .8367 .8968
+AV* .9319 .9219 .8414 .9020
MSRA Open* 1.000 .9960 .9920 .9958
Open1a .9710 .9601 .9352 .9558
Open2b .9699 .9581 .9359 .9548
aFor our official submission to Bakeoff-4, we also used
an ANER trained on the MSRA NER training corpus of
Bakeoff-3. This makes our official evaluation results ex-
tremely high but trivial, for a part of this corpus is used as
the MSRA NER test corpus for Bakeoff-4. Presented here
are the results without using this ANER.
bOpen2 is the result of Open1 using no NE list feature.
results of word segmentation and NER for our sys-
tem are presented in Tables 6 and 7, respectively.
For the purpose of comparison, the word segmen-
tation performance of our system on Bakeoff-4 data
using the 2- and 4-tag sets and the best correspond-
ing n-gram feature templates as in (Tsai et al, 2006;
Low et al, 2005) are presented in Table 8.7 This
comparison reconfirms the conclusion in (Zhao et
CityU data sets in any other situation than the Bakeoff.
7The templates for the 2-tag set, adopted from (Tsai et al,
2006), include C?2, C?1, C0, C1, C?3C?1, C?2C0, C?2C?1,
C?1C0, C?1C1 and C0C1. Those for the 4-tag set, adopted
from (Xue, 2003) and (Low et al, 2005), include C?2, C?1,
C0, C1, C2, C?2C?1, C?1C0, C?1C1, C0C1and C1C2.
al., 2006b) about tag set selection for character tag-
ging for word segmentation that the 6-tag set is more
effective than others, each with its own best corre-
sponding feature template set.
Table 8: Segmentation F-scores by different tag sets
AV Tags CityU CKIP CTB NCC SXU
2 .9303 .9277 .9434 .9198 .9454
? 4 .9370 .9348 .9481 .9280 .9512
6 .9426 .9421 .9634 .9333 .9552
2 .9382 .9319 .9451 .9239 .9485
+ 4 .9482 .9423 .9527 .9356 .9593
6 .9510 .9470 .9589 .9405 .9623
4 Discussion
4.1 Tag Set and Computational Cost
Using more labels in CRFs learning is expected to
bring in performance enhancement. Inevitably, how-
ever, it also leads to a huge rise of computational
cost for model training. We conducted a series of ex-
periments to study the computational cost of CRFs
training with different tag sets using Bakeoff-3 data.
The experimental results are given in Table 9, show-
ing that the 6-tag set costs nearly twice as much time
as the 4-tag set and about three times as the 2-tag
set. Fortunately, its memory cost with the six n-gram
feature templates remains very close to that of the 2-
and 4-tag sets with the n-gram feature template sets
from (Tsai et al, 2006; Xue, 2003).
However, a 2-tag set is popular in use for word
segmentation and NER for the reason that CRFs
training is very computationally expensive and a
large tag set would make the situation worse. Cer-
109
Sixth SIGHAN Workshop on Chinese Language Processing
Table 9: Comparison of computational cost
Tags Templates AS CityU CTB MSRA
Training time (Minutes)
2 Tsai 112 52 16 35
4 Xue 206 79 28 73
6 Zhao 402 146 47 117
Feature numbers (?106)
2 Tsai 13.2 7.3 3.1 5.5
4 Xue 16.1 9.0 3.9 6.8
6 Zhao 15.6 8.8 3.8 6.6
Memory cost (Giga bytes)
2 Tsai 5.4 2.4 0.9 1.8
4 Xue 6.6 2.8 1.1 2.2
6 Zhao 6.4 2.7 1.0 2.1
tainly, a possible way out of this problem is the
computer hardware advancement, which is predicted
by Moore?s Law (Moore, 1965) to be improving at
an exponential rate in general, including processing
speed and memory capacity. Specifically, CPU can
be made twice faster every other year or even 18
months. It is predictable that computational cost will
not be a problem for CRFs training soon, and the ad-
vantages of using a larger tag set as in our approach
will be shared by more others.
4.2 Unsupervised Segmentation Features
Our evaluation results show that the unsupervised
segmentation features bring in performance im-
provement on both word segmentation and NER for
all tracks except CTB segmentation, as highlighted
in Table 6. We are unable explain this yet, and can
only attribute it to some unique text characteristics
of the CTB segmented corpus. An unsupervised seg-
mentation criterion provides a kind of global infor-
mation over the whole text of a corpus (Zhao and
Kit, 2007). Its effectiveness is certainly sensitive to
text characteristics.
Quite a number of other unsupervised segmen-
tation criteria are available for word discovery in
unlabeled texts, e.g., boundary entropy (Tung and
Lee, 1994; Chang and Su, 1997; Huang and Powers,
2003; Jin and Tanaka-Ishii, 2006) and description-
length-gain (DLG) (Kit and Wilks, 1999). We found
that among them AV could help the CRFs model to
achieve a better performance than others, although
the overall unsupervised segmentation by DLG was
slightly better than that by AV. Combining any two
of these criteria did not give any further performance
improvement. This is why we have opted for AV for
Bakeoff-4.
4.3 NE List Features for Open NER
We realize that the NE lists available to us are far
from sufficient for coping with all NEs in Bakeoff-
4. It is reasonable that using richer external NE
lists gives a better NER performance in many cases
(Zhang et al, 2006). Surprisingly, however, the NE
list features used in our NER do not lead to any sig-
nificant performance improvement, according to the
evaluation results in Table 7. This is certainly an-
other issue for our further inspection.
5 Conclusion
Without doubt our achievements in Bakeoff-4 owes
not only to the careful selection of character tag set
and feature templates for exerting the strength of
CRFs learning but also to the effectiveness of our un-
supervised segmentation approach. It is for the sake
of simplicity that similar sets of character tags and
feature templates are applied to two distinctive label-
ing tasks, word segmentation and NER. Relying on
little preprocessing and postprocessing, our system
simply follows the plain training and test routines
of machine learning practice with the CRFs model
and achieves the best or nearly the best results for all
tracks of Bakeoff-4 in which we participated. Sim-
ple is beautiful, as Albert Einstein said, ?Everything
should be made as simple as possible, but not one
bit simpler.? Our evaluation results also provide evi-
dence that simple can be powerful too.
Acknowledgements
The research described in this paper was sup-
ported by the Research Grants Council of Hong
Kong S.A.R., China, through the CERG grant
9040861 (CityU 1318/03H) and by City University
of Hong Kong through the Strategic Research Grant
7002037. Dr. Hai Zhao was supported by a Post-
doctoral Research Fellowship in the Department of
Chinese, Translation and Linguistics, City Univer-
sity of Hong Kong.
References
Jing-Shin Chang and Keh-Yih Su. 1997. An unsuper-
vised iterative method for Chinese new lexicon ex-
110
Sixth SIGHAN Workshop on Chinese Language Processing
traction. Computational Linguistics and Chinese Lan-
guage Processing, 2(2):97?148.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. 2006.
Chinese named entity recognition with conditional
random fields. In SIGHAN-5, pages 118?121, Sydney,
Australia, July 22-23.
Thomas Emerson. 2005. The second international Chi-
nese word segmentation bakeoff. In SIGHAN-4, pages
123?133, Jeju Island, Korea, October 14-15.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004a. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics,
30(1):75?93.
Haodi Feng, Kang Chen, Chunyu Kit, and Xiaotie Deng.
2004b. Unsupervised segmentation of Chinese cor-
pus using accessor variety. In First International
Joint Conference on Natural Language Processing
(IJCNLP-04), pages 255?261, Sanya, Hainan Island,
China, March 22-24. Also in K. Y. Su, J. Tsujii, J.
H. Lee & O. Y. Kwong (eds.), Natural Language Pro-
cessing - IJCNLP 2004, LNAI 3248, pages 694-703.
Springer.
Zellig Sabbetai Harris. 1955. From phoneme to mor-
pheme. Language, 31(2):190?222.
Zellig Sabbetai Harris. 1970. Morpheme boundaries
within words. In Papers in Structural and Transfor-
mational Linguistics, page 68?77.
Jin Hu Huang and David Powers. 2003. Chinese
word segmentation based on contextual entropy. In
Dong Hong Ji and Kim-Ten Lua, editors, PACLIC -
17, pages 152?158, Sentosa, Singapore, October, 1-3.
COLIPS Publication.
Zhihui Jin and Kumiko Tanaka-Ishii. 2006. Unsuper-
vised segmentation of Chinese text by use of branch-
ing entropy. In COLING/ACL?2006, pages 428?435,
Sidney, Australia, July 17-21.
Chunyu Kit and Yorick Wilks. 1998. The virtual corpus
approach to deriving n-gram statistics from large scale
corpora. In Changning Huang, editor, Proceedings of
1998 International Conference on Chinese Informa-
tion Processing Conference, pages 223?229, Beijing,
Nov. 18-20.
Chunyu Kit and Yorick Wilks. 1999. Unsupervised
learning of word boundary with description length
gain. In M. Osborne and E. T. K. Sang, editors,
CoNLL-99, pages 1?6, Bergen, Norway.
Chunyu Kit and Hai Zhao. 2007. Improving Chi-
nese word segmentation with description length gain.
In 2007 International Conference on Artificial Intelli-
gence (ICAI?07), Las Vegas, June 25-28.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML?2001, pages 282?289, San Francisco, CA.
Gina-Anne Levow. 2006. The third international Chi-
nese language processing bakeoff: Word segmentation
and named entity recognition. In SIGHAN-5, pages
108?117, Sydney, Australia, July 22-23.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to Chinese word seg-
mentation. In SIGHAN-4, pages 161?164, Jeju Island,
Korea, October 14-15.
Udi Manber and Gene Myers. 1993. Suffix arrays: A
new method for on-line string searches. SIAM Journal
on Computing, 22(5):935?948.
Gordon E. Moore. 1965. Cramming more components
onto integrated circuits. Electronics, 3(8), April 19.
Richard Sproat and Thomas Emerson. 2003. The first
international Chinese word segmentation bakeoff. In
SIGHAN-2, pages 133?143, Sapporo, Japan.
Richard Tzong-Han Tsai, Hsieh-Chuan Hung, Cheng-
Lung Sung, Hong-Jie Dai, and Wen-Lian Hsu. 2006.
On closed task of Chinese word segmentation: An im-
proved CRF model coupled with character clustering
and automatically generated template matching. In
SIGHAN-5, pages 108?117, Sydney, Australia, July
22-23.
Cheng-Huang Tung and His-Jian Lee. 1994. Iden-
tification of unknown words from corpus. Compu-
tational Proceedings of Chinese and Oriental Lan-
guages, 8:131?145.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29?48.
Suxiang Zhang, Ying Qin, Juan Wen, and Xiaojie Wang.
2006. Word segmentation and named entity recog-
nition for SIGHAN Bakeoff3. In SIGHAN-5, pages
158?161, Sydney, Australia, July 22-23.
Hai Zhao and Chunyu Kit. 2007. Incorporating global
information into supervised learning for Chinese word
segmentation. In PACLING-2007, pages 66?74, Mel-
bourne, Australia, September 19-21.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006a.
An improved Chinese word segmentation system with
conditional random field. In SIGHAN-5, pages 162?
165, Sydney, Australia, July 22-23.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006b. Effective tag set selection in Chinese word
segmentation via conditional random field modeling.
In PACLIC-20, pages 87?94, Wuhan, China, Novem-
ber 1-3.
111
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 61?66,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency Learning:
Exploiting Rich Features for Tagging Syntactic and Semantic Dependencies
Hai Zhao(??)?, Wenliang Chen(???)?,
Jun?ichi Kazama?, Kiyotaka Uchimoto?, and Kentaro Torisawa?
?Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
?Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
haizhao@cityu.edu.hk, chenwl@nict.go.jp
Abstract
This paper describes our system about mul-
tilingual syntactic and semantic dependency
parsing for our participation in the joint task
of CoNLL-2009 shared tasks. Our system
uses rich features and incorporates various in-
tegration technologies. The system is evalu-
ated on in-domain and out-of-domain evalu-
ation data of closed challenge of joint task.
For in-domain evaluation, our system ranks
the second for the average macro labeled F1 of
all seven languages, 82.52% (only about 0.1%
worse than the best system), and the first for
English with macro labeled F1 87.69%. And
for out-of-domain evaluation, our system also
achieves the second for average score of all
three languages.
1 Introduction
This paper describes the system of National In-
stitute of Information and Communications Tech-
nology (NICT) and City University of Hong Kong
(CityU) for the joint learning task of CoNLL-2009
shared task (Hajic? et al, 2009)1. The system is ba-
sically a pipeline of syntactic parser and semantic
parser. We use a syntactic parser that uses very rich
features and integrates graph- and transition-based
methods. As for the semantic parser, a group of well
selected feature templates are used with n-best syn-
tactic features.
1Our thanks give to the following corpus providers, (Taule?
et al, 2008; Palmer and Xue, 2009; Hajic? et al, 2006; Surdeanu
et al, 2008; Burchardt et al, 2006) and (Kawahara et al, 2002).
The rest of the paper is organized as follows. The
next section presents the technical details of our syn-
tactic dependency parsing. Section 3 describes the
details of the semantic dependency parsing. Section
4 shows the evaluation results. Section 5 looks into a
few issues concerning our forthcoming work for this
shared task, and Section 6 concludes the paper.
2 Syntactic Dependency Parsing
Basically, we build our syntactic dependency parsers
based on the MSTParser, a freely available imple-
mentation2, whose details are presented in the paper
of McDonald and Pereira (2006). Moreover, we ex-
ploit rich features for the parsers. We represent fea-
tures by following the work of Chen et al (2008) and
Koo et al (2008) and use features based on depen-
dency relations predicted by transition-based parsers
(Nivre and McDonald, 2008). Chen et al (2008) and
Koo et al (2008) proposed the methods to obtain
new features from large-scale unlabeled data. In our
system, we perform their methods on training data
because the closed challenge does not allow to use
unlabeled data. In this paper, we call these new ad-
ditional features rich features.
2.1 Basic Features
Firstly, we use all the features presented by McDon-
ald et al (2006), if they are available in data. Then
we add new features for the languages having FEAT
information (Hajic? et al, 2009). FEAT is a set of
morphological-features, e.g. more detailed part of
speech, number, gender, etc. We try to align differ-
ent types of morphological-features. For example,
2http://mstparser.sourceforge.net
61
we can obtain a sequence of gender tags of all words
from a head h to its dependent d. Then we represent
the features based on the obtained sequences.
Based on the results of development data, we per-
form non-projective parsing for Czech and German
and perform projective parsing for Catalan, Chinese,
English, Japanese, and Spanish.
2.2 Features Based on Dependency Pairs
I    see    a    beautiful    bird    .
Figure 1: Example dependency graph.
Chen et al (2008) presented a method of extract-
ing short dependency pairs from large-scale auto-
parsed data. Here, we extract all dependency pairs
rather than short dependency pairs from training
data because we believe that training data are reli-
able. In a parsed sentence, if two words have de-
pendency relation, we add this word pair into a list
named L and count its frequency. We consider the
direction. For example, in figure 1, a and bird have
dependency relation in the sentence ?I see a beauti-
ful bird.?. Then we add word pair ?a-bird:HEAD?3
into list L and accumulate its frequency.
We remove the pairs which occur only once in
training data. According to frequency, we then
group word pairs into different buckets, with bucket
LOW for frequencies 2-7, bucket MID for frequen-
cies 8-14, and bucket HIGH for frequencies 15+.
We set these threshold values by following the set-
ting of Chen et al (2008). For example, the fre-
quency of pair ?a-bird:HEAD? is 5. Then it is
grouped into bucket ?LOW?. We also add a vir-
tual bucket ?ZERO? to represent the pairs that are
not included in the list. So we have four buckets.
?ZERO?, ?LOW?, ?MID?, and ?HIGH? are used as
bucket IDs.
Based on the buckets, we represent new features
for a head h and its dependent d. We check word
pairs surrounding h and d. Table 1 shows the word
pairs, where h-word refers to the head word, d-word
refers to the dependent word, h-word-1 refers to
3HEAD means that bird is the head of the pair.
the word to the left of the head in the sentence, h-
word+1 refers to the word to the right of the head,
d-word-1 refers to the word to the left of the depen-
dent, and d-word+1 refers the word to the right of
the dependent. Then we obtain the bucket IDs of
these word pairs from L.
We generate new features consisting of indicator
functions for bucket IDs of word pairs. We call these
features word-pair-based features. We also generate
combined features involving bucket IDs and part-of-
speech tags of heads.
h-word, d-word
h-word-1, d-word
h-word+1, d-word
h-word, d-word-1
h-word, d-word+1
Table 1: Word pairs for feature representation
2.3 Features Based on Word Clusters
Koo et al (2008) presented new features based on
word clusters obtained from large-scale unlabeled
data and achieved large improvement for English
and Czech. Here, word clusters are generated only
from the training data for all the languages. We per-
form word clustering by using the clustering tool4,
which also was used by Koo et al (2008). The
cluster-based features are the same as the ones used
by Koo et al (2008).
2.4 Features Based on Predicted Relations
Nivre and McDonald (2008) presented an integrat-
ing method to provide additional information for
graph-based and transition-based parsers. Here, we
represent features based on dependency relations
predicted by transition-based parsers for graph-
based parser. Based on the results on development
data, we choose the MaltParser for Catalan, Czech,
German, and Spanish, and choose another MaxEnt-
based parser for Chinese, English, and Japanese.
2.4.1 A Transition-based Parser: MaltParser
For Catalan, Czech, German, and Spanish, we
use the MaltParser, a freely available implementa-
4http://www.cs.berkeley.edu/?pliang/software/brown-
cluster-1.2.zip
62
tion5, whose details are presented in the paper of
Nivre (2003). More information about the parser can
be available in the paper (Nivre, 2003).
Due to computational cost, we do not select new
feature templates for the MaltParser. Following the
features settings of Hall et al (2007), we use their
Czech feature file and Catalan feature file. To sim-
ply, we apply Czech feature file for German too, and
apply Catalan feature file for Spanish.
2.4.2 Another Transition-based Parser:
MaxEnt-based Parser
In three highly projective language, Chinese,
English and Japanese, we use the maximum en-
tropy syntactic dependency parser as in Zhao and
Kit (2008). We still use the similar feature notations
of that work. We use the same greedy feature selec-
tion of Zhao et al (2009) to determine an optimal
feature template set for each language. Full feature
sets for the three languages can be found at website,
http://bcmi.sjtu.edu.cn/?zhaohai.
2.4.3 Feature Representation
For training data, we use 2-way jackknifing to
generate predicted dependency parsing trees by two
transition-based parsers. Following the features of
Nivre and McDonald (2008), we define features for
a head h and its dependent d with label l as shown in
table 2, where GTran refers to dependency parsing
trees generated by the MaltParser or MaxEnt-base
Parser and ? refers to any label. All features are
conjoined with the part-of-speech tags of the words
involved in the dependency.
Is (h, d, ?) in GTran?
Is (h, d, l) in GTran?
Is (h, d, ?) not in GTran?
Is (h, d, l) not in GTran?
Table 2: Features set based on predicted labels
3 n-best Syntactic Features for Semantic
Dependency Parsing
Due to the limited computational resource that we
have, we used the the similar learning framework as
our participant in semantic-only task (Zhao et al,
5http://w3.msi.vxu.se/?nivre/research/MaltParser.html
Normal n-best Matched
Ca 53 54 50
Ch 75 65 55
En 73 70 63
Table 3: Feature template sets:n-best vs. non-n-best
2009). Namely, three languages, a single maximum
entropy model is used for all identification and clas-
sification tasks of predicate senses or argument la-
bels in four languages, Catalan, Czech, Japanese, or
Spanish. For the rest three languages, an individual
sense classifier still using maximum entropy is ad-
ditionally used to output the predicate sense previ-
ously. More details about argument candidate prun-
ing strategies and feature template set selection are
described in Zhao et al (2009).
The same feature template sets as the semantic-
only task are used for three languages, Czech, Ger-
man and Japanese. For the rest four languages, we
further use n-best syntactic features to strengthen
semantic dependency parsing upon those automati-
cally discovered feature template sets. However, we
cannot obtain an obvious performance improvement
in Spanish by using n-best syntactic features. There-
fore, only Catalan, Chinese and English semantic
parsing adopted these types of features at last.
Our work about n-best syntactic features still
starts from the feature template set that is originally
selected for the semantic-only task. The original fea-
ture template set is hereafter referred to ?the normal?
or ?non-n-best?. In practice, only 2nd-best syntactic
outputs are actually adopted by our system for the
joint task.
To generate helpful feature templates from the
2nd-best syntactic tree, we simply let al feature tem-
plates in the normal feature set that are based on
the 1st-best syntactic tree now turn to the 2nd-best
one. Using the same notations for feature template
representation as in Zhao et al (2009), we take an
example to show how the original n-best features
are produced. Assuming a.children.dprel.bag is
one of syntactic feature templates in the normal
set, this feature means that all syntactic children of
the argument candidate (a) are chosen, and their
dependant labels are collected, the duplicated la-
bels are removed and then sorted, finally all these
strings are concatenated as a feature. The cor-
63
Language Features
Catalan p:2.lm.dprel
a.lemma + a:2.h.form
a.lemma + a:2.pphead.form
(a:2:p:2|dpPath.dprel.seq) + p.FEAT1
Chinese a:2.h.pos
a:2.children.pos.seq + p:2.children.pos.seq
a:2:p:2|dpPath.dprel.bag
a:2:p:2|dpPathPred.form.seq
a:2:p:2|dpPath.pos.bag
(a:2:p:2|dpTreeRelation) + p.pos
(a:2:p:2|dpPath.dprel.seq) + a.pos
English a:2:p:2|dpPathPred.lemma.bag
a:2:p:2|dpPathPred.pos.bag
a:2:p:2|dpTreeRelation
a:2:p:2|dpPath.dprel.seq
a:2:p:2|dpPathPred.dprel.seq
a.lemma + a:2.dprel + a:2.h.lemma
(a:2:p:2|dpTreeRelation) + p.pos
Table 4: Features for n-best syntactic tree
responding 2nd-best syntactic feature will be a :
2.children.dprel.bag. As all operations to gener-
ate the feature for a.children.dprel.bag is within
the 1st-best syntactic tree, while those for a :
2.children.dprel.bag is within the 2nd-best one. As
all these 2nd-best syntactic features are generated,
we use the same greedy feature selection procedure
as in Zhao et al (2009) to determine the best fit fea-
ture template set according to the evaluation results
in the development set.
For Catalan, Chinese and English, three opti-
mal n-best feature sets are obtained, respectively.
Though dozens of n-best features are initially gen-
erated for selection, only few of them survive af-
ter the greedy selection. A feature number statis-
tics is in Table 3, and those additionally selected
n-best features for three languages are in Table
4. Full feature lists and their explanation for
all languages will be available at the website,
http://bcmi.sjtu.edu.cn/?zhaohai.
4 Evaluation Results
Two tracks (closed and open challenges) are pro-
vided for joint task of CoNLL2009 shared task.
We participated in the closed challenge and evalu-
ated our system on the in-domain and out-of-domain
evaluation data.
avg. Cz En Gr
Syntactic (LAS) 77.96 75.58 82.38 75.93
Semantic (Labeled F1) 75.01 82.66 74.58 67.78
Joint (Macro F1) 76.51 79.12 78.51 71.89
Table 6: The official results of our submission for out-of-
domain task(%)
Test Dev
Basic ALL Basic ALL
Catalan 82.91 85.88 83.15 85.98
Chinese 74.28 75.67 73.36 75.64
Czech 77.21 79.70 77.91 80.22
English 88.63 89.19 86.35 87.40
German 84.61 86.24 83.99 85.44
Japanese 92.31 92.32 92.01 92.85
Spanish 83.59 86.29 83.73 86.22
Average 83.32 85.04 82.92 84.82
(+1.72) (+1.90)
Table 7: The effect of rich features for syntactic depen-
dency parsing
4.1 Official Results
The official results for the joint task are in Table 5,
and the out-of-domain task in Table 6, where num-
bers in bold stand for the best performances for the
specific language. For out-of-domain (OOD) eval-
uation, we did not perform any domain adaptation.
For both in-domain and out-of-domain evaluation,
our system achieved the second best performance
for the average Macro F1 scores of all the languages.
And our system provided the first best performance
for the average Semantic Labeled F1 score and the
forth for the average Labeled Syntactic Accuracy
score for in-domain evaluation.
4.2 Further results
At first, we check the effect of rich features for syn-
tactic dependency parsing. Table 7 shows the com-
parative results of basic features and all features on
test and development data, where ?Basic? refers to
the system with basic features and ?ALL? refers to
the system with basic features plus rich features. We
found that the additional features provided improve-
ment of 1.72% for test data and 1.90% for develop-
ment data.
Then we investigate the effect of different train-
ing data size for semantic parsing. The learning
64
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 85.04 85.88 75.67 79.70 89.19 86.24 92.32 86.29
Semantic (Labeled F1) 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
Joint (Macro F1) 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
Table 5: The official results of our joint submission (%)
Data Czech Chinese English
normal n-best normal n-best
25% 80.71 75.12 75.24 82.02 82.06
50% 81.52 76.50 76.59 83.52 83.42
75% 81.90 76.92 77.01 84.21 84.30
100% 82.24 77.35 77.34 84.73 84.80
Table 8: The performance in development set (semantic
labeled F1) vs. training corpus size
curves are drawn for Czech, Chinese and English.
We use 25%, 50% and 75% training corpus, respec-
tively. The results in development sets are given in
Table 8. Note that in this table the differences be-
tween normal and n-best feature template sets are
also given for Chinese and English. The results
in the table show that n-best features help improve
Chinese semantic parsing as the training corpus is
smaller, while it works for English as the training
corpus is larger.
5 Discussion
This work shows our further endeavor in syntactic
and semantic dependency parsing, based on our pre-
vious work (Chen et al, 2008; Zhao and Kit, 2008).
Chen et al (Chen et al, 2008) and Koo et al (Koo
et al, 2008) used large-scale unlabeled data to im-
prove syntactic dependency parsing performance.
Here, we just performed their method on training
data. From the results, we found that the new fea-
tures provided better performance. In future work,
we can try these methods on large-scale unlabeled
data for other languages besides Chinese and En-
glish.
In Zhao and Kit (2008), we addressed that seman-
tic parsing should benefit from cross-validated train-
ing corpus and n-best syntactic output. These two
issues have been implemented during this shared
task. Though existing work show that re-ranking for
semantic-only or syntactic-semantic joint tasks may
bring higher performance, the limited computational
resources does not permit us to do this for multiple
languages.
To analyze the advantage and the weakness of our
system, the ranks for every languages of our sys-
tem?s outputs are given in Table 9, and the perfor-
mance differences between our system and the best
one in Table 106. The comparisons in these two ta-
bles indicate that our system is slightly weaker in the
syntactic parsing part, this may be due to the reason
that syntactic parsing in our system does not ben-
efit from semantic parsing as the other joint learn-
ing systems. However, considering that the seman-
tic parsing in our system simply follows the output
of the syntactic parsing and the semantic part of our
system still ranks the first for the average score, the
semantic part of our system does output robust and
stable results. It is worth noting that semantic la-
beled F1 in Czech given by our system is 4.47%
worse than the best one. This forby gap in this lan-
guage further indicates the advantage of our system
in the other six languages and some latent bugs or
learning framework misuse in Czech semantic pars-
ing.
6 Conclusion
We describe the system that uses rich features and
incorporates integrating technology for joint learn-
ing task of syntactic and semantic dependency pars-
ing in multiple languages. The evaluation results
show that our system is good at both syntactic and
semantic parsing, which suggests that a feature-
oriented method is effective in multiple language
processing.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
6The difference for Chinese in the latter table is actually
computed between ours and the second best system.
65
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 4 4 4 4 2 3 3 4
Semantic (Labeled F1) 1 1 3 4 1 2 2 1
Joint (Macro F1) 2 1 3 4 1 3 2 1
Table 9: Our system?s rank within the joint task according to three main measures
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 0.73 1.98 0.84 0.68 0.69 1.24 0.25 1.35
Semantic (Labeled F1) - - 0.38 4.47 - 2.42 0.09 -
Joint (Macro F1) 0.12 - 0.15 2.40 - 1.22 0.37 -
Table 10: The performance differences between our system and the best one within the joint task according to three
main measures
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC-2006,
Genoa, Italy.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto,
Yujie Zhang, and Hitoshi Isahara. 2008. Dependency
parsing with short dependency relations in unlabeled
data. In Proceedings of IJCNLP-2008, Hyderabad, In-
dia, January 8-10.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL-
2009, Boulder, Colorado, USA.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryig?it,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech, June.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of LREC-2002, pages 2008?
2013, Las Palmas, Canary Islands.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus,
Ohio, USA, June.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL-2006, pages 81?88,
Trento, Italy, April.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of CoNLL-
X, New York City, June.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160, Nancy, France, April 23-25.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the CoNLL-
2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the LREC-
2008, Marrakesh, Morroco.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage maxi-
mum entropy models. In Proceedings of CoNLL-2008,
pages 203?207, Manchester, UK, August 16-17.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In Proceedings of CoNLL-2009, Boul-
der, Colorado, USA.
66
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55?63,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Cross Language Dependency Parsing using a Bilingual Lexicon?
Hai Zhao(??)??, Yan Song(??)?, Chunyu Kit?, Guodong Zhou?
?Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
?School of Computer Science and Technology
Soochow University, Suzhou, China 215006
{haizhao,yansong,ctckit}@cityu.edu.hk, gdzhou@suda.edu.cn
Abstract
This paper proposes an approach to en-
hance dependency parsing in a language
by using a translated treebank from an-
other language. A simple statistical ma-
chine translation method, word-by-word
decoding, where not a parallel corpus but
a bilingual lexicon is necessary, is adopted
for the treebank translation. Using an en-
semble method, the key information ex-
tracted from word pairs with dependency
relations in the translated text is effectively
integrated into the parser for the target lan-
guage. The proposed method is evaluated
in English and Chinese treebanks. It is
shown that a translated English treebank
helps a Chinese parser obtain a state-of-
the-art result.
1 Introduction
Although supervised learning methods bring state-
of-the-art outcome for dependency parser infer-
ring (McDonald et al, 2005; Hall et al, 2007), a
large enough data set is often required for specific
parsing accuracy according to this type of meth-
ods. However, to annotate syntactic structure, ei-
ther phrase- or dependency-based, is a costly job.
Until now, the largest treebanks1 in various lan-
guages for syntax learning are with around one
million words (or some other similar units). Lim-
ited data stand in the way of further performance
enhancement. This is the case for each individual
language at least. But, this is not the case as we
observe all treebanks in different languages as a
whole. For example, of ten treebanks for CoNLL-
2007 shared task, none includes more than 500K
?The study is partially supported by City University of
Hong Kong through the Strategic Research Grant 7002037
and 7002388. The first author is sponsored by a research fel-
lowship from CTL, City University of Hong Kong.
1It is a tradition to call an annotated syntactic corpus as
treebank in parsing community.
tokens, while the sum of tokens from all treebanks
is about two million (Nivre et al, 2007).
As different human languages or treebanks
should share something common, this makes it
possible to let dependency parsing in multiple lan-
guages be beneficial with each other. In this pa-
per, we study how to improve dependency parsing
by using (automatically) translated texts attached
with transformed dependency information. As a
case study, we consider how to enhance a Chinese
dependency parser by using a translated English
treebank. What our method relies on is not the
close relation of the chosen language pair but the
similarity of two treebanks, this is the most differ-
ent from the previous work.
Two main obstacles are supposed to confront in
a cross-language dependency parsing task. The
first is the cost of translation. Machine translation
has been shown one of the most expensive lan-
guage processing tasks, as a great deal of time and
space is required to perform this task. In addition,
a standard statistical machine translation method
based on a parallel corpus will not work effec-
tively if it is not able to find a parallel corpus that
right covers source and target treebanks. How-
ever, dependency parsing focuses on the relations
of word pairs, this allows us to use a dictionary-
based translation without assuming a parallel cor-
pus available, and the training stage of translation
may be ignored and the decoding will be quite fast
in this case. The second difficulty is that the out-
puts of translation are hardly qualified for the pars-
ing purpose. The most challenge in this aspect is
morphological preprocessing. We regard that the
morphological issue should be handled aiming at
the specific language, our solution here is to use
character-level features for a target language like
Chinese.
The rest of the paper is organized as follows.
The next section presents some related existing
work. Section 3 describes the procedure on tree-
55
bank translation and dependency transformation.
Section 4 describes a dependency parser for Chi-
nese as a baseline. Section 5 describes how a
parser can be strengthened from the translated
treebank. The experimental results are reported in
Section 6. Section 7 looks into a few issues con-
cerning the conditions that the proposed approach
is suitable for. Section 8 concludes the paper.
2 The Related Work
As this work is about exploiting extra resources to
enhance an existing parser, it is related to domain
adaption for parsing that has been draw some in-
terests in recent years. Typical domain adaptation
tasks often assume annotated data in new domain
absent or insufficient and a large scale unlabeled
data available. As unlabeled data are concerned,
semi-supervised or unsupervised methods will be
naturally adopted. In previous works, two basic
types of methods can be identified to enhance an
existing parser from additional resources. The first
is usually focus on exploiting automatic generated
labeled data from the unlabeled data (Steedman
et al, 2003; McClosky et al, 2006; Reichart and
Rappoport, 2007; Sagae and Tsujii, 2007; Chen
et al, 2008), the second is on combining super-
vised and unsupervised methods, and only unla-
beled data are considered (Smith and Eisner, 2006;
Wang and Schuurmans, 2008; Koo et al, 2008).
Our purpose in this study is to obtain a further
performance enhancement by exploiting treebanks
in other languages. This is similar to the above
first type of methods, some assistant data should
be automatically generated for the subsequent pro-
cessing. The differences are what type of data are
concerned with and how they are produced. In our
method, a machine translation method is applied
to tackle golden-standard treebank, while all the
previous works focus on the unlabeled data.
Although cross-language technique has been
used in other natural language processing tasks,
it is basically new for syntactic parsing as few
works were concerned with this issue. The rea-
son is straightforward, syntactic structure is too
complicated to be properly translated and the cost
of translation cannot be afforded in many cases.
However, we empirically find this difficulty may
be dramatically alleviated as dependencies rather
than phrases are used for syntactic structure repre-
sentation. Even the translation outputs are not so
good as the expected, a dependency parser for the
target language can effectively make use of them
by only considering the most related information
extracted from the translated text.
The basic idea to support this work is to make
use of the semantic connection between different
languages. In this sense, it is related to the work of
(Merlo et al, 2002) and (Burkett and Klein, 2008).
The former showed that complementary informa-
tion about English verbs can be extracted from
their translations in a second language (Chinese)
and the use of multilingual features improves clas-
sification performance of the English verbs. The
latter iteratively trained a model to maximize the
marginal likelihood of tree pairs, with alignments
treated as latent variables, and then jointly parsing
bilingual sentences in a translation pair. The pro-
posed parser using features from monolingual and
mutual constraints helped its log-linear model to
achieve better performance for both monolingual
parsers and machine translation system. In this
work, cross-language features will be also adopted
as the latter work. However, although it is not es-
sentially different, we only focus on dependency
parsing itself, while the parsing scheme in (Bur-
kett and Klein, 2008) based on a constituent rep-
resentation.
Among of existing works that we are aware of,
we regard that the most similar one to ours is (Ze-
man and Resnik, 2008), who adapted a parser to a
new language that is much poorer in linguistic re-
sources than the source language. However, there
are two main differences between their work and
ours. The first is that they considered a pair of suf-
ficiently related languages, Danish and Swedish,
and made full use of the similar characteristics of
two languages. Here we consider two quite dif-
ferent languages, English and Chinese. As fewer
language properties are concerned, our approach
holds the more possibility to be extended to other
language pairs than theirs. The second is that a
parallel corpus is required for their work and a
strict statistical machine translation procedure was
performed, while our approach holds a merit of
simplicity as only a bilingual lexicon is required.
3 Treebank Translation and Dependency
Transformation
3.1 Data
As a case study, this work will be conducted be-
tween the source language, English, and the tar-
get language, Chinese, namely, we will investigate
56
how a translated English treebank enhances a Chi-
nese dependency parser.
For English data, the Penn Treebank (PTB) 3
is used. The constituency structures is converted
to dependency trees by using the same rules as
(Yamada and Matsumoto, 2003) and the standard
training/development/test split is used. However,
only training corpus (sections 2-21) is used for
this study. For Chinese data, the Chinese Treebank
(CTB) version 4.0 is used in our experiments. The
same rules for conversion and the same data split
is adopted as (Wang et al, 2007): files 1-270 and
400-931 as training, 271-300 as testing and files
301-325 as development. We use the gold stan-
dard segmentation and part-of-speech (POS) tags
in both treebanks.
As a bilingual lexicon is required for our task
and none of existing lexicons are suitable for trans-
lating PTB, two lexicons, LDC Chinese-English
Translation Lexicon Version 2.0 (LDC2002L27),
and an English to Chinese lexicon in StarDict2,
are conflated, with some necessary manual exten-
sions, to cover 99% words appearing in the PTB
(the most part of the untranslated words are named
entities.). This lexicon includes 123K entries.
3.2 Translation
A word-by-word statistical machine translation
strategy is adopted to translate words attached
with the respective dependency information from
the source language to the target one. In detail, a
word-based decoding is used, which adopts a log-
linear framework as in (Och and Ney, 2002) with
only two features, translation model and language
model,
P (c|e) = exp[
?2
i=1 ?ihi(c, e)]?
c exp[
?2
i=1 ?ihi(c, e)]
Where
h1(c, e) = log(p?(c|e))
is the translation model, which is converted from
the bilingual lexicon, and
h2(c, e) = log(p?(c))
is the language model, a word trigram model
trained from the CTB. In our experiment, we set
two weights ?1 = ?2 = 1.
2StarDict is an open source dictionary software, available
at http://stardict.sourceforge.net/.
The conversion process of the source treebank
is completed by three steps as the following:
1. Bind POS tag and dependency relation of a
word with itself;
2. Translate the PTB text into Chinese word by
word. Since we use a lexicon rather than a parallel
corpus to estimate the translation probabilities, we
simply assign uniform probabilities to all transla-
tion options. Thus the decoding process is actu-
ally only determined by the language model. Sim-
ilar to the ?bag translation? experiment in (Brown
et al, 1990), the candidate target sentences made
up by a sequence of the optional target words are
ranked by the trigram language model. The output
sentence will be generated only if it is with maxi-
mum probability as follows,
c = argmax{p?(c)p?(c|e)}
= argmax p?(c)
= argmax
?
p?(wc)
A beam search algorithm is used for this process
to find the best path from all the translation op-
tions; As the training stage, especially, the most
time-consuming alignment sub-stage, is skipped,
the translation only includes a decoding procedure
that takes about 4.5 hours for about one million
words of the PTB in a 2.8GHz PC.
3. After the target sentence is generated, the at-
tached POS tags and dependency information of
each English word will also be transferred to each
corresponding Chinese word. As word order is of-
ten changed after translation, the pointer of each
dependency relationship, represented by a serial
number, should be re-calculated.
Although we try to perform an exact word-by-
word translation, this aim cannot be fully reached
in fact, as the following case is frequently encoun-
tered, multiple English words have to be translated
into one Chinese word. To solve this problem,
we use a policy that lets the output Chinese word
only inherits the attached information of the high-
est syntactic head in the original multiple English
words.
4 Dependency Parsing: Baseline
4.1 Learning Model and Features
According to (McDonald and Nivre, 2007), all
data-driven models for dependency parsing that
have been proposed in recent years can be de-
scribed as either graph-based or transition-based.
57
Table 1: Feature Notations
Notation Meaning
s The word in the top of stack
s? The first word below the top of stack.
s?1,s1... The first word before(after) the word
in the top of stack.
i, i+1,... The first (second) word in the
unprocessed sequence, etc.
dir Dependent direction
h Head
lm Leftmost child
rm Rightmost child
rn Right nearest child
form word form
pos POS tag of word
cpos1 coarse POS: the first letter of POS tag of word
cpos2 coarse POS: the first two POS tags of word
lnverb the left nearest verb
char1 The first character of a word
char2 The first two characters of a word
char?1 The last character of a word
char?2 The last two characters of a word
. ?s, i.e., ?s.dprel? means dependent label
of character in the top of stack
+ Feature combination, i.e., ?s.char+i.char?
means both s.char and i.char work as a
feature function.
Although the former will be also used as compari-
son, the latter is chosen as the main parsing frame-
work by this study for the sake of efficiency. In de-
tail, a shift-reduce method is adopted as in (Nivre,
2003), where a classifier is used to make a parsing
decision step by step. In each step, the classifier
checks a word pair, namely, s, the top of a stack
that consists of the processed words, and, i, the
first word in the (input) unprocessed sequence, to
determine if a dependent relation should be estab-
lished between them. Besides two dependency arc
building actions, a shift action and a reduce ac-
tion are also defined to maintain the stack and the
unprocessed sequence. In this work, we adopt a
left-to-right arc-eager parsing model, that means
that the parser scans the input sequence from left
to right and right dependents are attached to their
heads as soon as possible (Hall et al, 2007).
While memory-based and margin-based learn-
ing approaches such as support vector machines
are popularly applied to shift-reduce parsing, we
apply maximum entropy model as the learning
model for efficient training and adopting over-
lapped features as our work in (Zhao and Kit,
2008), especially, those character-level ones for
Chinese parsing. Our implementation of maxi-
mum entropy adopts L-BFGS algorithm for pa-
rameter optimization as usual.
With notations defined in Table 1, a feature set
as shown in Table 2 is adopted. Here, we explain
some terms in Tables 1 and 2. We used a large
scale feature selection approach as in (Zhao et al,
2009) to obtain the feature set in Table 2. Some
feature notations in this paper are also borrowed
from that work.
The feature curroot returns the root of a par-
tial parsing tree that includes a specified node.
The feature charseq returns a character sequence
whose members are collected from all identified
children for a specified word.
In Table 2, as for concatenating multiple sub-
strings into a feature string, there are two ways,
seq and bag. The former is to concatenate all sub-
strings without do something special. The latter
will remove all duplicated substrings, sort the rest
and concatenate all at last.
Note that we systemically use a group of
character-level features. Surprisingly, as to our
best knowledge, this is the first report on using this
type of features in Chinese dependency parsing.
Although (McDonald et al, 2005) used the pre-
fix of each word form instead of word form itself
as features, character-level features here for Chi-
nese is essentially different from that. As Chinese
is basically a character-based written language.
Character plays an important role in many means,
most characters can be formed as single-character
words, and Chinese itself is character-order free
rather than word-order free to some extent. In ad-
dition, there is often a close connection between
the meaning of a Chinese word and its first or last
character.
4.2 Parsing using a Beam Search Algorithm
In Table 2, the feature preactn returns the previous
parsing action type, and the subscript n stands for
the action order before the current action. These
are a group of Markovian features. Without this
type of features, a shift-reduce parser may directly
scan through an input sequence in linear time.
Otherwise, following the work of (Duan et al,
2007) and (Zhao, 2009), the parsing algorithm is
to search a parsing action sequence with the max-
imal probability.
Sdi = argmax
?
i
p(di|di?1di?2...),
where Sdi is the object parsing action sequence,
p(di|di?1...) is the conditional probability, and di
58
Figure 1: A comparison before and after translation
Table 2: Features for Parsing
in.form, n = 0, 1
i.form + i1.form
in.char2 + in+1.char2, n = ?1, 0
i.char?1 + i1.char?1
in.char?2 n = 0, 3
i1.char?2 + i2.char?2 +i3.char?2
i.lnverb.char?2
i3.pos
in.pos + in+1.pos, n = 0, 1
i?2.cpos1 + i?1.cpos1
i1.cpos1 + i2.cpos1 + i3.cpos1
s?2.char1
s?.char?2 + s?1.char?2
s??2.cpos2
s??1.cpos2 + s?1.cpos2
s?.cpos2 + s?1.cpos2
s?.children.cpos2.seq
s?.children.dprel.seq
s?.subtree.depth
s?.h.form + s?.rm.cpos1
s?.lm.char2 + s?.char2
s.h.children.dprel.seq
s.lm.dprel
s.char?2 + i1.char?2
s.charn + i.charn, n = ?1, 1
s?1.pos + i1.pos
s.pos + in.pos, n = ?1, 0, 1
s : i|linePath.form.bag
s?.form + i.form
s?.char2 + in.char2, n = ?1, 0, 1
s.curroot.pos + i.pos
s.curroot.char2 + i.char2
s.children.cpos2.seq + i.children.cpos2.seq
s.children.cpos2.seq + i.children.cpos2.seq
+ s.cpos2 + i.cpos2
s?.children.dprel.seq + i.children.dprel.seq
preact?1
preact?2
preact?2+preact?1
is i-th parsing action. We use a beam search algo-
rithm to find the object parsing action sequence.
5 Exploiting the Translated Treebank
As we cannot expect too much for a word-by-word
translation, only word pairs with dependency rela-
tion in translated text are extracted as useful and
reliable information. Then some features based
on a query in these word pairs according to the
current parsing state (namely, words in the cur-
rent stack and input) will be derived to enhance
the Chinese parser.
A translation sample can be seen in Figure 1.
Although most words are satisfactorily translated,
to generate effective features, what we still have to
consider at first is the inconsistence between the
translated text and the target text.
In Chinese, word lemma is always its word form
itself, this is a convenient characteristic in com-
putational linguistics and makes lemma features
unnecessary for Chinese parsing at all. However,
Chinese has a special primary processing task, i.e.,
word segmentation. Unfortunately, word defini-
tions for Chinese are not consistent in various lin-
guistical views, for example, seven segmentation
conventions for computational purpose are for-
mally proposed since the first Bakeoff3.
Note that CTB or any other Chinese treebank
has its own word segmentation guideline. Chi-
nese word should be strictly segmented according
to the guideline before POS tags and dependency
relations are annotated. However, as we say the
3Bakeoff is a Chinese processing share task held by
SIGHAN.
59
English treebank is translated into Chinese word
by word, Chinese words in the translated text are
exactly some entries from the bilingual lexicon,
they are actually irregular phrases, short sentences
or something else rather than words that follows
any existing word segmentation convention. If the
bilingual lexicon is not carefully selected or re-
fined according to the treebank where the Chinese
parser is trained from, then there will be a serious
inconsistence on word segmentation conventions
between the translated and the target treebanks.
As all concerned feature values here are calcu-
lated from the searching result in the translated
word pair list according to the current parsing
state, and a complete and exact match cannot be
always expected, our solution to the above seg-
mentation issue is using a partial matching strat-
egy based on characters that the words include.
Above all, a translated word pair list, L, is ex-
tracted from the translated treebank. Each item in
the list consists of three elements, dependant word
(dp), head word (hd) and the frequency of this pair
in the translated treebank, f .
There are two basic strategies to organize the
features derived from the translated word pair list.
The first is to find the most matching word pair
in the list and extract some properties from it,
such as the matched length, part-of-speech tags
and so on, to generate features. Note that a
matching priority serial should be defined afore-
hand in this case. The second is to check every
matching models between the current parsing state
and the partially matched word pair. In an early
version of our approach, the former was imple-
mented. However, It is proven to be quite inef-
ficient in computation. Thus we adopt the sec-
ond strategy at last. Two matching model fea-
ture functions, ?(?) and ?(?), are correspondingly
defined as follows. The return value of ?(?) or
?(?) is the logarithmic frequency of the matched
item. There are four input parameters required
by the function ?(?). Two parameters of them
are about which part of the stack(input) words is
chosen, and other two are about which part of
each item in the translated word pair is chosen.
These parameters could be set to full or charn as
shown in Table 1, where n = ...,?2,?1, 1, 2, ....
For example, a possible feature could be
?(s.full, i.char1, dp.full, hd.char1), it tries to
find a match in L by comparing stack word and
dp word, and the first character of input word
Table 3: Features based on the translated treebank
?(i.char3, s?.full, dp.char3, hd.full)+i.char3
+s?.form
?(i.char3, s.char2, dp.char3, hd.char2)+s.char2
?(i.char3, s.full, dp.char3, hd.char2)+s.form
?(s?.char?2, hd.char?2, head)+i.pos+s?.pos
?(i.char3, s.full, dp.char3, hd.char2)+s.full
?(s?.full, i.char4, dp.full, hd.char4)+s?.pos+i.pos
?(i.full, hd.char2, root)+i.pos+s.pos
?(i.full, hd.char2, root)+i.pos+s?.pos
?(s.full, dp.full, dependant)+i.pos
pairscore(s?.pos, i.pos)+s?.form+i.form
rootscore(s?.pos)+s?.form+i.form
rootscore(s?.pos)+i.pos
and the first character of hd word. If such
a match item in L is found, then ?(?) returns
log(f). There are three input parameters required
by the function ?(?). One parameter is about
which part of the stack(input) words is chosen,
and the other is about which part of each item
in the translated word pair is chosen. The third
is about the matching type that may be set to
dependant, head, or root. For example, the
function ?(i.char1, hd.full, root) tries to find a
match in L by comparing the first character of in-
put word and the whole dp word. If such a match
item in L is found, then ?(?) returns log(f) as hd
occurs as ROOT f times.
As having observed that CTB and PTB share a
similar POS guideline. A POS pair list from PTB
is also extract. Two types of features, rootscore
and pairscore are used to make use of such infor-
mation. Both of them returns the logarithmic value
of the frequency for a given dependent event. The
difference is, rootscore counts for the given POS
tag occurring as ROOT, and pairscore counts for
two POS tag combination occurring for a depen-
dent relationship.
A full adapted feature list that is derived from
the translated word pairs is in Table 3.
6 Evaluation Results
The quality of the parser is measured by the pars-
ing accuracy or the unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
head. Two types of scores are reported for compar-
ison: ?UAS without p? is the UAS score without
all punctuation tokens and ?UAS with p? is the one
with all punctuation tokens.
The results with different feature sets are in Ta-
ble 4. As the features preactn are involved, a
60
beam search algorithm with width 5 is used for
parsing, otherwise, a simple shift-reduce decoding
is used. It is observed that the features derived
from the translated text bring a significant perfor-
mance improvement as high as 1.3%.
Table 4: The results with different feature sets
features with p without p
baseline -d 0.846 0.858
+da 0.848 0.860
+Tb -d 0.859 0.869
+d 0.861 0.870
a+d: using three Markovian features preact and
beam search decoding.
b+T: using features derived from the translated text
as in Table 3.
To compare our parser to the state-of-the-art
counterparts, we use the same testing data as
(Wang et al, 2005) did, selecting the sentences
length up to 40. Table 5 shows the results achieved
by other researchers and ours (UAS with p), which
indicates that our parser outperforms any other
ones 4. However, our results is only slightly better
than that of (Chen et al, 2008) as only sentences
whose lengths are less than 40 are considered. As
our full result is much better than the latter, this
comparison indicates that our approach improves
the performance for those longer sentences.
Table 5: Comparison against the state-of-the-art
full up to 40
(McDonald and Pereira, 2006)a - 0.825
(Wang et al, 2007) - 0.866
(Chen et al, 2008) 0.852 0.884
Ours 0.861 0.889
aThis results was reported in (Wang et al, 2007).
The experimental results in (McDonald and
Nivre, 2007) show a negative impact on the pars-
ing accuracy from too long dependency relation.
For the proposed method, the improvement rela-
tive to dependency length is shown in Figure 2.
From the figure, it is seen that our method gives
observable better performance when dependency
lengths are larger than 4. Although word order is
changed, the results here show that the useful in-
formation from the translated treebank still help
those long distance dependencies.
4There is a slight exception: using the same data splitting,
(Yu et al, 2008) reported UAS without p as 0.873 versus ours,
0.870.
1 4 7 10 13 16 19
0.4
0.5
0.6
0.7
0.8
0.9
1
 Dependency Length
 
F1
basline: +d
+T: +d
Figure 2: Performance vs. dependency length
7 Discussion
If a treebank in the source language can help im-
prove parsing in the target language, then there
must be something common between these two
languages, or more precisely, these two corre-
sponding treebanks. (Zeman and Resnik, 2008)
assumed that the morphology and syntax in the
language pair should be very similar, and that is
so for the language pair that they considered, Dan-
ish and Swedish, two very close north European
languages. Thus it is somewhat surprising that
we show a translated English treebank may help
Chinese parsing, as English and Chinese even be-
long to two different language systems. However,
it will not be so strange if we recognize that PTB
and CTB share very similar guidelines on POS and
syntactics annotation. Since it will be too abstract
in discussing the details of the annotation guide-
lines, we look into the similarities of two treebanks
from the matching degree of two word pair lists.
The reason is that the effectiveness of the proposed
method actually relies on how many word pairs at
every parsing states can find their full or partial
matched partners in the translated word pair list.
Table 6 shows such a statistics on the matching
degree distribution from all training samples for
Chinese parsing. The statistics in the table suggest
that most to-be-check word pairs during parsing
have a full or partial hitting in the translated word
pair list. The latter then obtains an opportunity to
provide a great deal of useful guideline informa-
tion to help determine how the former should be
tackled. Therefore we have cause for attributing
the effectiveness of the proposed method to the
similarity of these two treebanks. From Table 6,
61
we also find that the partial matching strategy de-
fined in Section 5 plays a very important role in
improving the whole matching degree. Note that
our approach is not too related to the characteris-
tics of two languages. Our discussion here brings
an interesting issue, which difference is more im-
portant in cross language processing, between two
languages themselves or the corresponding anno-
tated corpora? This may be extensively discussed
in the future work.
Table 6: Matching degree distribution
dependant-match head-match Percent (%)
None None 9.6
None Partial 16.2
None Full 9.9
Partial None 12.4
Partial Partial 42.6
Partial Full 7.3
Full None 3.7
Full Partial 7.0
Full Full 0.2
Note that only a bilingual lexicon is adopted in
our approach. We regard it one of the most mer-
its for our approach. A lexicon is much easier to
be obtained than an annotated corpus. One of the
remained question about this work is if the bilin-
gual lexicon should be very specific for this kind
of tasks. According to our experiences, actually, it
is not so sensitive to choose a highly refined lexi-
con or not. We once found many words, mostly
named entities, were outside the lexicon. Thus
we managed to collect a named entity translation
dictionary to enhance the original one. However,
this extra effort did not receive an observable per-
formance improvement in return. Finally we re-
alize that a lexicon that can guarantee two word
pair lists highly matched is sufficient for this work,
and this requirement may be conveniently satis-
fied only if the lexicon consists of adequate high-
frequent words from the source treebank.
8 Conclusion and Future Work
We propose a method to enhance dependency
parsing in one language by using a translated tree-
bank from another language. A simple statisti-
cal machine translation technique, word-by-word
decoding, where only a bilingual lexicon is nec-
essary, is used to translate the source treebank.
As dependency parsing is concerned with the re-
lations of word pairs, only those word pairs with
dependency relations in the translated treebank are
chosen to generate some additional features to en-
hance the parser for the target language. The ex-
perimental results in English and Chinese tree-
banks show the proposed method is effective and
helps the Chinese parser in this work achieve a
state-of-the-art result.
Note that our method is evaluated in two tree-
banks with a similar annotation style and it avoids
using too many linguistic properties. Thus the
method is in the hope of being used in other simi-
larly annotated treebanks 5. For an immediate ex-
ample, we may adopt a translated Chinese tree-
bank to improve English parsing. Although there
are still something to do, the remained key work
has been as simple as considering how to deter-
mine the matching strategy for searching the trans-
lated word pair list in English according to the
framework of our method. .
Acknowledgements
We?d like to give our thanks to three anonymous
reviewers for their insightful comments, Dr. Chen
Wenliang for for helpful discussions and Mr. Liu
Jun for helping us fix a bug in our scoring pro-
gram.
References
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79?85.
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP-2008, pages 877?886, Honolulu, Hawaii,
USA.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchi-
moto, Yujie Zhang, and Hitoshi Isahara. 2008. De-
pendency parsing with short dependency relations
in unlabeled data. In Proceedings of IJCNLP-2008,
Hyderabad, India, January 8-10.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic parsing action models for multi-lingual de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
940?946, Prague, Czech, June 28-30.
Johan Hall, Jens Nilsson, Joakim Nivre,
Gu?lsen Eryig?it, Bea?ta Megyesi, Mattias Nils-
son, and Markus Saers. 2007. Single malt or
5For example, Catalan and Spanish treebanks from the
AnCora(-Es/Ca) Multilevel Annotated Corpus that are an-
notated by the Universitat de Barcelona (CLiC-UB) and the
Universitat Polit?cnica de Catalunya (UPC).
62
blended? a study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 933?939,
Prague, Czech, June.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, USA, June.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of ACL-COLING 2006,
pages 337?344, Sydney, Australia, July.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2007), pages 122?131,
Prague, Czech, June 28-30.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL-2006, pages 81?88,
Trento, Italy, April.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL-2005,
pages 91?98, Ann Arbor, Michigan, USA, June 25-
30.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multilingual paradigm
for automatic verb classification. In ACL-2002,
pages 207?214, Philadelphia, Pennsylvania, USA.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, page
915?932, Prague, Czech, June.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of IWPT-
2003), pages 149?160, Nancy, France, April 23-25.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL-
2002, pages 295?302, Philadelphia, USA, July.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In Proceedings of
ACL-2007, pages 616?623, Prague, Czech Republic,
June.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, page
1044?1050, Prague, Czech, June 28-30.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proceedings of ACL-COLING 2006,
page 569?576, Sydney, Australia, July.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of EACL-2003, page
331?338, Budapest, Hungary, April.
Qin Iris Wang and Dale Schuurmans. 2008. Semi-
supervised convex training for dependency parsing.
In Proceedings of ACL-08: HLT, pages 532?540,
Columbus, Ohio, USA, June.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2005. Strictly lexical dependency parsing. In Pro-
ceedings of IWPT-2005, pages 152?159, Vancouver,
BC, Canada, October.
Qin Iris Wang, Dekang Lin, and Dale Schuurmans.
2007. Simple training of dependency parsers via
structured boosting. In Proceedings of IJCAI 2007,
pages 1756?1762, Hyderabad, India, January.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Sta-
tistical dependency analysis with support vector
machines. In Proceedings of IWPT-2003), page
195?206, Nancy, France, April.
Kun Yu, Daisuke Kawahara, and Sadao Kurohashi.
2008. Chinese dependency parsing with large
scale automatically constructed case structures. In
Proceedings of COLING-2008, pages 1049?1056,
Manchester, UK, August.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of IJCNLP 2008 Workshop
on NLP for Less Privileged Languages, pages 35?
42, Hyderabad, India, January.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage max-
imum entropy models. In Proceeding of CoNLL-
2008, pages 203?207, Manchester, UK.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning:
A huge feature engineering method to semantic de-
pendency parsing. In Proceedings of CoNLL-2009,
Boulder, Colorado, USA.
Hai Zhao. 2009. Character-level dependencies in
chinese: Usefulness and learning. In EACL-2009,
pages 879?887, Athens, Greece.
63
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 55?60,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency Learning:
A Huge Feature Engineering Method to Semantic Dependency Parsing ?
Hai Zhao(??)??, Wenliang Chen(???)?, Chunyu Kit?, Guodong Zhou?
?Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
?Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
?School of Computer Science and Technology
Soochow University, Suzhou, China 215006
haizhao@cityu.edu.hk, chenwl@nict.go.jp
Abstract
This paper describes our system about mul-
tilingual semantic dependency parsing (SR-
Lonly) for our participation in the shared task
of CoNLL-2009. We illustrate that semantic
dependency parsing can be transformed into
a word-pair classification problem and im-
plemented as a single-stage machine learning
system. For each input corpus, a large scale
feature engineering is conducted to select the
best fit feature template set incorporated with a
proper argument pruning strategy. The system
achieved the top average score in the closed
challenge: 80.47% semantic labeled F1 for the
average score.
1 Introduction
The syntactic and semantic dependency parsing in
multiple languages introduced by the shared task
of CoNLL-2009 is an extension of the CoNLL-
2008 shared task (Hajic? et al, 2009). Seven lan-
guages, English plus Catalan, Chinese, Czech, Ger-
man, Japanese and Spanish, are involved (Taule? et
al., 2008; Palmer and Xue, 2009; Hajic? et al, 2006;
Surdeanu et al, 2008; Burchardt et al, 2006; Kawa-
hara et al, 2002). This paper presents our research
for participation in the semantic-only (SRLonly)
challenge of the CoNLL-2009 shared task, with a
?This study is partially supported by CERG grant 9040861
(CityU 1318/03H), CityU Strategic Research Grant 7002037,
Projects 60673041 and 60873041 under the National Natural
Science Foundation of China and Project 2006AA01Z147 un-
der the ?863? National High-Tech Research and Development
of China.
highlight on our strategy to select features from a
large candidate set for maximum entropy learning.
2 System Survey
We opt for the maximum entropy model with Gaus-
sian prior as our learning model for all classification
subtasks in the shared task. Our implementation of
the model adopts L-BFGS algorithm for parameter
optimization as usual. No additional feature selec-
tion techniques are applied.
Our system is basically improved from its early
version for CoNLL-2008 (Zhao and Kit, 2008). By
introducing a virtual root for every predicates, The
job to determine both argument labels and predicate
senses is formulated as a word-pair classification
task in four languages, namely, Catalan, Spanish,
Czech and Japanese. In other three languages, Chi-
nese, English and German, a predicate sense clas-
sifier is individually trained before argument label
classification. Note that traditionally (or you may
say that most semantic parsing systems did so) ar-
gument identification and classification are handled
in a two-stage pipeline, while ours always tackles
them in one step, in addition, predicate sense classi-
fication are also included in this unique learning/test
step for four of all languages.
3 Pruning Argument Candidates
We keep using a word-pair classification procedure
to formulate semantic dependency parsing. Specif-
ically, we specify the first word in a word pair as a
predicate candidate (i.e., a semantic head, and noted
as p in our feature representation) and the next as an
argument candidate (i.e., a semantic dependent, and
55
noted as a). We do not differentiate between verbal
and non-verbal predicates and our system handles
them in the exactly same way.
When no constraint available, however, all word
pairs in the an input sequence must be considered,
leading to very poor efficiency in computation for
no gain in effectiveness. Thus, the training sample
needs to be pruned properly. As predicates overtly
known in the share task, we only consider how to
effectively prune argument candidates.
We adopt five types of argument pruning strate-
gies for seven languages. All of them assume that a
syntactic dependency parsing tree is available.
As for Chinese and English, we continue to use
a dependency version of the pruning algorithm of
(Xue and Palmer, 2004) as described in (Zhao and
Kit, 2008). The pruning algorithm is readdressed as
the following.
Initialization: Set the given predicate candidate
as the current node;
(1) The current node and all of its syntactic chil-
dren are selected as argument candidates.
(2) Reset the current node to its syntactic head and
repeat step (1) until the root is reached.
Note that the given predicate candidate itself is
excluded from the argument candidate list for Chi-
nese, that is slightly different from English.
The above pruning algorithm has been shown ef-
fective. However, it is still inefficient for a single-
stage argument identification/classification classifi-
cation task. Thus we introduce an assistant argument
label ? NoMoreArgument? to alleviate this difficulty.
If an argument candidate in the above algorithm is
labeled as such a label, then the pruning algorithm
will end immediately. In training, this assistant label
means no more samples will be generated for the
current predicate, while in test, the decoder will not
search more argument candidates any more. This
adaptive technique more effectively prunes the ar-
gument candidates. In fact, our experiments show
1/3 training memory and time may be saved from it.
As for Catalan and Spanish, only syntactic chil-
dren of the predicate are considered as the argument
candidates.
As for Czech, only syntactic children, grandchil-
dren, great-grandchildren, parent and siblings of the
predicate are taken as the argument candidates.
As for German, only syntactic children, grand-
children, parent, siblings, siblings of parent and sib-
lings of grandparent of the predicate are taken as the
argument candidates.
The case is somewhat sophisticated for Japanese.
As we cannot identify a group of simple predicate-
argument relations from the syntactic tree. Thus
we consider top frequent 28 syntactic relations be-
tween the predicate and the argument. The parser
will search all words before and after the predicate,
and only those words that hold one of the 28 syn-
tactic relations to the predicate are considered as
the argument candidate. Similar to the pruning al-
gorithm for Chinese/English/German, we also in-
troduce two assistant labels ? leftNoMoreArgument?
and ? rightNoMoreArgument? to adaptively prune
words too far away from the predicate.
4 Feature Templates
As we don?t think that we can benefit from know-
ing seven languages, an automatic feature template
selection is conducted for each language.
About 1000 feature templates (hereafter this tem-
plate set is referred to FT ) are initially considered.
These feature templates are from various combina-
tions or integrations of the following basic elements.
Word Property. This type of elements include
word form, lemma, part-of-speech tag (PoS), FEAT
(additional morphological features), syntactic de-
pendency label (dprel), semantic dependency label
(semdprel) and characters (char) in the word form
(only suitable for Chinese and Japanese)1.
Syntactic Connection. This includes syntactic
head (h), left(right) farthest(nearest) child (lm, ln,
rm, and rn), and high(low) support verb or noun.
We explain the last item, support verb(noun). From
the predicate or the argument to the syntactic root
along the syntactic tree, the first verb(noun) that is
met is called as the low support verb(noun), and the
nearest one to the root is called as the high support
verb(noun).
Semantic Connection. This includes semantic
1All lemmas, PoS, and FEAT for either training or test are
from automatically pre-analyzed columns of every input files.
56
FEATn 1 2 3 4 5 6 7 8 9 10 11
Catalan/Spanish postype gen num person mood tense punct
Czech SubPOS Gen Num Cas Neg Gra Voi Var Sem Per Ten
Table 1: Notations of FEATs
head (semhead), left(right) farthest(nearest) seman-
tic child (semlm, semln, semrm, semrn). We say
a predicate is its argument?s semantic head, and the
latter is the former?s child. Features related to this
type may track the current semantic parsing status.
Path. There are two basic types of path between
the predicate and the argument candidates. One is
the linear path (linePath) in the sequence, the other
is the path in the syntactic parsing tree (dpPath). For
the latter, we further divide it into four sub-types
by considering the syntactic root, dpPath is the full
path in the syntactic tree. Leading two paths to the
root from the predicate and the argument, respec-
tively, the common part of these two paths will be
dpPathShare. Assume that dpPathShare starts from
a node r?, then dpPathPred is from the predicate to
r?, and dpPathArgu is from the argument to r?.
Family. Two types of children sets for the predi-
cate or argument candidate are considered, the first
includes all syntactic children (children), the second
also includes all but excludes the left most and the
right most children (noFarChildren).
Concatenation of Elements. For all collected el-
ements according to linePath, children and so on, we
use three strategies to concatenate all those strings
to produce the feature value. The first is seq, which
concatenates all collected strings without doing any-
thing. The second is bag, which removes all dupli-
cated strings and sort the rest. The third is noDup,
which removes all duplicated neighbored strings.
In the following, we show some feature template
examples derived from the above mentioned items.
a.lm.lemma The lemma of the left most child of
the argument candidate.
p.h.dprel The dependant label of the syntactic
head of the predicate candidate.
a.pos+p.pos The concatenation of PoS of the ar-
gument and the predicate candidates.
p?1.pos+p.pos PoS of the previous word of the
predicate and PoS of the predicate itself.
a:p|dpPath.lemma.bag Collect all lemmas along
the syntactic tree path from the argument to the pred-
icate, then removed all duplicated ones and sort the
rest, finally concatenate all as a feature string.
a:p.highSupportNoun|linePath.dprel.seq Collect
all dependant labels along the line path from the ar-
gument to the high support noun of the predicate,
then concatenate all as a feature string.
(a:p|dpPath.dprel.seq)+p.FEAT1 Collect all de-
pendant labels along the line path from the argument
to the predicate and concatenate them plus the first
FEAT of the predicate.
An important feature for the task is dpTreeRela-
tion, which returns the relationship of a and p in a
syntactic parse tree and cannot be derived from com-
bining the above basic elements. The possible values
for this feature include parent, sibling etc.
5 Automatically Discovered Feature
Template Sets
For each language, starting from a basic feature tem-
plate set (a small subset of FT ) according to our
previous result in English dependency parsing, each
feature template outside the basic set is added and
each feature template inside the basic set is removed
one by one to check the effectiveness of each fea-
ture template following the performance change in
the development set. This procedure will be contin-
uously repeated until no feature template is added or
removed or the performance is not improved.
There are some obvious heuristic rules that help
us avoid trivial feature template checking, for ex-
ample, FEAT features are only suitable for Cata-
lan, Czech and Spanish. Though FEAT features are
also available for Japanese, we don?t adopt them for
this language due to the hight training cost. To sim-
plify feature representation, we use FEAT1, FEAT2,
and so on to represent different FEAT for every lan-
guages. A lookup list can be found in Table 1. Ac-
cording to the list, FEAT4 represents person for
Catalan or Spanish, but Cas for Czech.
As we don?t manually interfere the selection pro-
cedure for feature templates, ten quite different fea-
57
Ca Ch Cz En Gr Jp Sp
Ca 53
Ch 5 75
Cz 11 10 76
En 11 11 12 73
Gr 7 7 7 14 45
Jp 6 22 13 15 10 96
Sp 22 9 18 15 9 12 66
Table 2: Feature template set: argument classifier
Ch En Gr
Ch 46
En 5 9
Gr 17 2 40
Table 3: Feature template set: sense classifier
ture template sets are obtained at last. Statistical in-
formation of seven sets for argument classifiers is in
Table 2, and those for sense classifiers are in Table 3.
Numbers in the diagonals of these two tables mean
the numbers of feature templates, and others mean
how many feature templates are identical for every
language pairs. The most matched feature template
sets are for Catalan/Spanish and Chinese/Japanese.
As for the former, it is not so surprised because these
two corpora are from the same provider.
Besides the above statistics, these seven feature
template sets actually share little in common. For
example, the intersection set from six languages, as
Chinese is excluded, only includes one feature tem-
plate, p.lemma (the lemma of the predicate candi-
date). If all seven sets are involved, then such an in-
tersection set will be empty. Does this mean human
languages share little in semantic representation? :)
It is unlikely to completely demonstrate full fea-
ture template sets for all languages in this short re-
port, we thus only demonstrate two sets, one for En-
glish sense classification in Table 4 and the other for
Catalan argument classification in Table 52.
6 Word Sense Determination
The shared task of CoNLL-2009 still asks for the
predicate sense. In our work for CoNLL-2008 (Zhao
and Kit, 2008), this was done by searching for a right
2Full feature lists and their explanation for all languages will
be available at the website, http://bcmi.sjtu.edu.cn/?zhaohai.
p.lm.pos
p.rm.pos
p.lemma
p.lemma + p.lemma1
p.lemma + p.children.dprel.noDup
p.lemma + p.currentSense
p.form
p.form?1 + p.form
p.form + p.form1
Table 4: Feature set for English sense classification
example in the given dictionary. Unfortunately, we
late found this caused a poor performance in sense
determination. This time, an individual classifier is
used to determine the sense for Chinese, English or
German, and this is done by the argument classifier
by introducing a virtual root for every predicates for
the rest four languages3. Features used for sense
determination are also selected following the same
procedure in Section 5. The difference is only pred-
icate related features are used for selection.
7 Decoding
The decoding for four languages, Catalan, Czech,
Japanese and Spanish is trivial, each word pairs will
be checked one by one. The first word of the pair
is the virtual root or the predicate, the second is the
predicate or every argument candidates. Argument
candidates are checked in the order of different syn-
tactic relations to their predicate, which are enumer-
ated by the pruning algorithms in Section 3, or from
left to right for the same syntactic relation. After
the sense of the predicate is determined, the label of
each argument candidate will be directly classified,
or, it is proved non-argument.
As for the rest languages, Chinese, English or
German, after the sense classifier outputs its result,
an optimal argument structure for each predicate is
determined by the following maximal probability.
Sp = argmax
?
i
P (ai|ai?1, ai?2, ...), (1)
where Sp is the argument structure, P (ai|ai?1...)
is the conditional probability to determine the la-
bel of the i-th argument candidate label. Note that
3For Japanese, no senses for predicates are defined. Thus it
is actually a trivial classification task in this case.
58
p.currentSense + p.lemma
p.currentSense + p.pos
p.currentSense + a.pos
p?1.FEAT1
p.FEAT2
p1.FEAT3
p.semrm.semdprel
p.lm.dprel
p.form + p.children.dprel.bag
p.lemman (n = ?1, 0)
p.lemma + p.lemma1
p.pos?1 + p.pos
p.pos1
p.pos + p.children.dprel.bag
a.FEAT1 + a.FEAT3 + a.FEAT4
+ a.FEAT5 + a.FEAT6
a?1.FEAT2 + a.FEAT2
a.FEAT3 + a1.FEAT3
a.FEAT3 + a.h.FEAT3
a.children.FEAT1.noDup
a.children.FEAT3.bag
a.h.lemma
a.lm.dprel + a.form
a.lm.form
a.lm?1.lemma
a.lmn.pos (n=0,1)
a.noFarChildren.pos.bag + a.rm.form
a.pphead.lemma
a.rm.dprel + a.form
a.rm?1.form
a.rm.lemma
a.rn.dprel + a.form
a.lowSupportVerb.lemma
a?1.form
a.form + a1.form
a.form + a.children.pos
a.lemma + a.h.form
a.lemma + a.pphead.form
a1.lemma
a1.pos + a.pos.seq
a.pos + a.children.dprel.bag
a.lemma + p.lemma
(a:p|dpPath.dprel) + p.FEAT1
a:p|linePath.distance
a:p|linePath.FEAT1.bag
a:p|linePath.form.seq
a:p|linePath.lemma.seq
a:p|linePath.dprel.seq
a:p|dpPath.lemma.seq
a:p|dpPath.lemma.bag
a:p|dpPathArgu.lemma.seq
a:p|dpPathArgu.lemma.bag
Table 5: Feature set for Catalan argument classification
P (ai|ai?1, ...) in equation (1) may be simplified as
P (ai) if the input feature template set does not con-
cerned with the previous argument label output. A
beam search algorithm is used to find the parsing de-
cision sequence.
8 Evaluation Results
Our evaluation is carried out on two computational
servers, (1) LEGA, a 64-bit ubuntu Linux installed
server with double dual-core AMD Opteron proces-
sors of 2.8GHz and 24GB memory. This server was
also used for our previous participation in CoNLL-
2008 shared task. (2) MEGA, a 64-bit ubuntu Linux
installed server with six quad-core Intel Xeon pro-
cessors of 2.33GHz and 128GB memory.
Altogether nearly 60,000 machine learning rou-
tines were run to select the best fit feature template
sets for all seven languages within two months. Both
LEGA and MEGA were used for this task. How-
ever, training and test for the final submission of
Chinese, Czech and English run in MEGA, and the
rest in LEGA. As we used multiple thread training
and multiple routines run at the same time, the exact
time cost for either training or test is hard to esti-
mate. Here we just report the actual time and mem-
ory cost in Table 7 for reference.
The official evaluation results of our system are in
Table 6. Numbers in bold in the table stand for the
best performances for the specific languages. The
results in development sets are also given. The first
row of the table reports the results using golden in-
put features.
Two facts as the following suggest that our system
does output robust and stable results. The first is that
two results for development and test sets in the same
language are quite close. The second is about out-of-
domain (OOD) task. Though for each OOD task, we
just used the same model trained from the respective
language and did nothing to strengthen it, this does
not hinder our system to obtain top results in Czech
and English OOD tasks.
In addition, the feature template sets from auto-
matical selection procedure in this task were used
for the joint task of this shared task, and also output
top results according to the average score of seman-
tic labeled F1 (Zhao et al, 2009).
59
average Catalan Chinese Czech English German Japanese Spanish
Development with Gold 81.24 81.52 78.32 86.96 84.19 77.75 78.67 81.32
Development 80.46 80.66 77.90 85.35 84.01 76.55 78.41 80.39
Test (official scores) 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
Out-of-domain 74.34 85.44 73.31 64.26
Table 6: Semantic labeled F1
Catalan Chinese Czech English German Japanese Spanish
Sense Training memory (MB) 418.0 136.0 63.0
Training time (Min.) 11.0 2.5 1.7
Test time (Min.) 0.7 0.2 0.03
Argument Training memory (GB) 0.4 3.7 3.2 3.8 0.2 1.4 0.4
Training time (Hours) 3.0 13.8 24.9 12.4 0.2 6.1 4.4
Test time (Min.) 3.0 144.0 27.1 88.0 1.0 4.2 7.0
Table 7: Computational cost
9 Conclusion
As presented in the above sections, we have tackled
semantic parsing for the CoNLL-2009 shared task
as a word-pair classification problem. Incorporated
with a proper argument candidate pruning strategy
and a large scale feature engineering for each lan-
guage, our system produced top results.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Nianwen Xue and Martha Palmer. 2004. Calibrating fea-
tures for semantic role labeling. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2004), pages 88?94, Barcelona, Spain,
July 25-26.
Hai Zhao and Chunyu Kit. 2008. Parsing syntac-
tic and semantic dependencies with two single-stage
maximum entropy models. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 203?207, Manchester, UK, August 16-
17.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009. Multilin-
gual dependency learning: Exploiting rich features for
tagging syntactic and semantic dependencies. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), June 4-5,
Boulder, Colorado, USA.
60
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280?1288,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
Improving Nominal SRL in Chinese Language with Verbal SRL In-
formation and Automatic Predicate Recognition 
 
Junhui Li?   Guodong Zhou??   Hai Zhao??  Qiaoming Zhu?  Peide Qian? 
? Jiangsu Provincial Key Lab for Computer Information Processing Technologies
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
? Department of Chinese, Translation and Linguistics 
City University of HongKong, China 
Email: {lijunhui,gdzhou,hzhao,qmzhu,pdqian}@suda.edu.cn
 
                                                          
? Corresponding author 
Abstract 
This paper explores Chinese semantic role la-
beling (SRL) for nominal predicates. Besides 
those widely used features in verbal SRL, 
various nominal SRL-specific features are 
first included. Then, we improve the perform-
ance of nominal SRL by integrating useful 
features derived from a state-of-the-art verbal 
SRL system. Finally, we address the issue of 
automatic predicate recognition, which is es-
sential for a nominal SRL system. Evaluation 
on Chinese NomBank shows that our research 
in integrating various features derived from 
verbal SRL significantly improves the per-
formance. It also shows that our nominal SRL 
system much outperforms the state-of-the-art 
ones. 
1. Introduction 
Semantic parsing maps a natural language sen-
tence into a formal representation of its meaning. 
Due to the difficulty in deep semantic parsing, 
most of previous work focuses on shallow se-
mantic parsing, which assigns a simple structure 
(such as WHO did WHAT to WHOM, WHEN, 
WHERE, WHY, HOW) to each predicate in a 
sentence. In particular, the well-defined seman-
tic role labeling (SRL) task has been drawing 
more and more attention in recent years due to 
its importance in deep NLP applications, such as 
question answering (Narayanan and Harabagiu, 
2004), information extraction (Surdeanu et al, 
2003), and co-reference resolution (Ponzetto and 
Strube, 2006). Given a sentence and a predicate 
(either a verb or a noun) in it, SRL recognizes 
and maps all the constituents in the sentence into 
their corresponding semantic arguments (roles) 
of the predicate. According to the predicate 
types, SRL could be divided into SRL for verbal 
predicates (verbal SRL, in short) and SRL for 
nominal predicates (nominal SRL, in short). 
During the past few years, verbal SRL has 
dominated the research on SRL with the avail-
ability of FrameNet (Baker et al, 1998), Prop-
Bank (Palmer et al, 2005), and the consecutive 
CoNLL shared tasks (Carreras and M?rquez, 
2004 & 2005) in English language. As a com-
plement to PropBank on verbal predicates, 
NomBank (Meyers et al, 2004) annotates nomi-
nal predicates and their corresponding semantic 
roles using similar semantic framework as 
PropBank. As a representative, Jiang and Ng 
(2006) pioneered the exploration of various 
nominal SRL-specific features besides the tradi-
tional verbal SRL-related features on NomBank. 
They achieved the performance of 72.73 and 
69.14 in F1-measure on golden and automatic 
syntactic parse trees, respectively, given golden 
nominal predicates. 
For SRL in Chinese, Sun and Jurafsky (2004) 
and Pradhan et al (2004) pioneered the research 
on Chinese verbal and nominal SRLs, respec-
tively, on small private datasets. Taking the ad-
vantage of recent release of Chinese PropBank 
(Xue and Palmer, 2003) and Chinese NomBank 
(Xue, 2006a), Xue and his colleagues (Xue and 
Palmer 2005; Xue 2006b; Xue, 2008) pioneered 
the exploration of Chinese verbal and nominal 
SRLs, given golden predicates. Among them, 
Xue and Palmer (2005) studied Chinese verbal 
SRL using Chinese PropBank and achieved the 
performance of 91.3 and 61.3 in F1-measure on 
golden and automatic syntactic parse trees, re-
spectively. Xue (2006b) extended their study on 
Chinese nominal SRL and attempted to improve 
the performance of nominal SRL by simply in-
1280
 cluding the Chinese PropBank training instances 
into the training data for nominal SRL on Chi-
nese NomBank. However, such integration was 
empirically proven unsuccessful due to the dif-
ferent nature of certain features for verbal and 
nominal SRLs. Xue (2008) further improved the 
performance on both verbal and nominal SRLs 
with a better syntactic parser and new features. 
Ding and Chang (2008) focused on argument 
classification for Chinese verbal predicates with 
hierarchical feature selection strategy. They 
achieved the classification precision of 94.68% 
on golden parse trees on Chinese PropBank. 
This paper focuses on Chinese nominal SRL. 
This is done by adopting a traditional verbal 
SRL architecture to handle Chinese nominal 
predicates with additional nominal SRL-specific 
features. Moreover, we significantly enhance the 
performance of nominal SRL by properly inte-
grating various features derived from verbal 
SRL. Finally, this paper investigates the effect of 
automatic nominal predicate recognition on the 
performance of Chinese nominal SRL. Although 
previous research (e.g. CoNLL?2008) in English 
nominal SRL reveals the importance of auto-
matic predicate recognition, there has no re-
ported research on automatic predicate 
recognition in Chinese nominal SRL. 
The rest of this paper is organized as follows: 
Section 2 introduces Chinese NomBank while 
the baseline nominal SRL system is described in 
Section 3 with traditional and nominal SRL-
specific features. Then, the baseline nominal 
SRL system is improved by integrating useful 
features derived from verbal SRL (Section 4) 
and extended with automatic recognition of 
nominal predicates (Section 5). Section 6 gives 
experimental results and discussion. Finally, 
Section 7 concludes the paper.    
2. Chinese NomBank 
Chinese NomBank (Xue, 2006a) adopts similar 
semantic framework as NomBank, and focuses 
on Chinese nominal predicates with their argu-
ments in Chinese TreeBank. The semantic ar-
guments include:  
1) Core arguments: Arg0 to Arg5. Generally, 
Arg0 and Arg1 denotes the agent and the 
patient, respectively, while arguments from 
Arg2 to Arg5 are predicate-specific.  
2) Adjunct arguments, which are universal to 
all predicates, e.g. ArgM-LOC for locative, 
and ArgM-TMP for temporal. 
 
All the arguments are annotated on parse tree 
nodes with their boundaries aligning with the 
spans of tree nodes. Figure 1 gives an example 
with two nominal predicates and their respective 
arguments, while the nominal predicate ???
/investment? has two core arguments, ?NN(??
/foreign businessman)? as Arg0 and ?NN(??
/bank)? as Arg1, and the other nominal predicate 
??? /loan? also has two core arguments, 
?NP(???? /Bank of China)? as Arg1 and 
Figure 1: Two nominal predicates and their arguments in the style of NomBank. 
? 
?? ?? ??
??
???
P 
NN NN NN
VV
NN NN 
Arg0/Rel1 Rel1 Arg1/Rel1
NP 
PP 
Arg0/Rel2 
ArgM-MNR/Rel2 Rel2 
NP 
CD
QP
NP
VP
VP
??? ?? 
? 
NN NN 
PU 
NP 
Arg1/Rel2 
IP
?? ?? 
Sup/Rel2
Bank of China 
to 
Foreign  Investment  Bank 
provide
4 billion
RMB loan 
. 
Bank of China provides 4 billion RMB loan to Foreign Investment Bank. 
1281
 ?PP(??????? /to Foreign Investment 
Bank)? as Arg0,  and 1 adjunct argument, 
?NN(???/RMB)? as ArgM-MNR, denoting 
the manner of loan. It is worth noticing that 
there is a (Chinese) NomBank-specific label in 
Figure 1, Sup (support verb) (Xue, 2006a), in 
helping introduce the arguments, which occur 
outside the nominal predicate-headed noun 
phrase. This is illustrated by the nominal predi-
cate ???/loan?, whose Arg0 and Arg1 are both 
realized outside the nominal predicate-headed 
noun phrase, NP(????????/4 billion 
RMB loan). Normally, a verb is marked as a 
support verb only when it shares some argu-
ments with the nominal predicate. 
3. Baseline: Chinese Nominal SRL 
Popular SRL systems usually formulate SRL as 
a classification problem, which annotates each 
constituent in a parse tree with a semantic role 
label or with the non-argument label NULL. Be-
sides, we divide the system into three consecu-
tive phases so as to overcome the imbalance 
between the training instances of the NULL 
class and those of any other argument classes.  
Argument pruning. Here, several heuristic 
rules are adopted to filter out constituents, which 
are most likely non-arguments. According to the 
argument structures of nominal predicates, we 
categorize arguments into two types: arguments 
inside NP (called inside arguments) and argu-
ments introduced via a support verb (called out-
side arguments), and handle them separately. 
For the inside arguments, the following three 
heuristic rules are applied to find inside argu-
ment candidates: 
z All the sisters of the predicate are candi-
dates. 
z If a CP or DNP node is a candidate, its chil-
dren are candidates too. 
z For any node X, if its parent is an ancestral 
node of the predicate, and the internal 
nodes along the path between X and the 
predicate are all NPs, then X is a candidate. 
For outside arguments, we look for the sup-
port verb of the focus nominal predicate, and 
then adopt the rules as proposed in Xue and 
Palmer (2005) to find the candidates for the sup-
port verb, since outside argument candidates are 
introduced via this support verb. That to say, the 
argument candidates of the support verb are re-
garded as outside argument candidates of the 
nominal predicate. However, as support verbs 
are not annotated explicitly in the testing phase, 
we identify intervening verbs as alternatives to 
support verbs in both training and testing phases 
with the path between the nominal predicate and 
intervening verb in the form of 
?VV<VP>[NP>]+NN?, where ?[NP>]+? denotes 
one or more NPs.  Our statistics on Chinese 
NomBank shows that 51.96% of nominal predi-
cates have no intervening verb while 48.04% of 
nominal predicates have only one intervening 
verb. 
Taken the nominal predicate ???/loan? in 
Figure 1 as an example, NN(???/RMB) and 
QP(??? /4 billion) are identified as inside 
argument candidates, while PP(??????
?/to Foreign Investment Bank) and NP(???
?/Bank of China) are identified as outside ar-
gument candidates via the support verb VV(?
?/provide). 
Argument identification. A binary classifier 
is applied to determine the candidates as either 
valid arguments or non-arguments. It is worth 
pointing out that we only mark those candidates 
that are most likely to be NULL (with probabil-
ity > 0.90) as non-arguments. Our empirical 
study shows that this little trick much benefits 
nominal SRL, since argument identification for 
nominal predicates is much more difficult than 
that for verbal predicates and thus many argu-
ments would have been falsely marked as non-
arguments if the threshold is set as 0.5. 
Argument classification. A multi-class classi-
fier is employed to label identified arguments 
with specific argument labels (including the 
NULL class for non-argument). 
In the following, we first adapt some tradi-
tional features, which have been proven effec-
tive in verbal SRL, to nominal SRL, and then 
introduce several nominal SRL-specific features. 
3.1. Traditional Features 
Using the feature naming convention as adopted 
in Jiang and Ng (2006), Table 1 lists the tradi-
tional features, where ?I? and ?C? indicate the 
features for argument identification and classifi-
cation, respectively. Among them, the predicate 
class (b2) feature was first introduced in Xue 
and Palmer (2005) to overcome the imbalance of 
the predicate distribution in that some predicates 
can be only found in the training data while 
some predicates in the testing data are absent 
from the training data. In particular, the verb 
class is classified along three dimensions: the 
number of arguments, the number of framesets 
and selected syntactic alternations. For example, 
1282
 the verb class of ?C1C2a? means that it has two 
framesets, with the first frameset having one 
argument and the second having two arguments. 
The symbol ?a? in the second frameset repre-
sents a type of syntactic alternation. 
 
Feature Remarks: b1-b5(C, I), b6-b7(C) 
b1 Predicate: the nominal predicate itself. (??
/loan) 
b2 Predicate class: the verb class that the predi-
cate belongs to. (C4a) 
b3 Head word (b3H) and its POS (b3P).  (??
/bank, NN) 
b4 Phrase type: the syntactic category of the 
constituent. (NP) 
b5 Path: the path from the constituent to the 
nominal predicate. 
 (NP<IP>VP>VP>NP>NP>NN) 
b6 Position: the positional relationship of the 
constituent with the predicate. ?left? or 
?right?. (left) 
b7 First word (b7F) and last word (b7L) of the 
focus constituent. (??/China, ??/bank) 
Combined features: b11-b14(C, I), b15(C) 
b11: b1&b4;       b12: b1&b3H;       b13: b2&b4;  
b14: b2&b3H;    b15: b5&b6 
Table 1: Traditional features and their instantiations 
for argument identification and classification, with 
NP(????/Bank of China)  as the focus constitu-
ent and NN(??/loan) as the nominal predicate, re-
garding Figure 1. 
3.2. Nominal SRL-specific Features 
To capture more useful information in the predi-
cate-argument structure, we also study addi-
tional features which provide extra information. 
Statistics on Chinese NomBank show that about 
40% of pruned inside candidates are arguments. 
Since inside arguments usually locate near to the 
nominal predicate, its surroundings are expected 
to be helpful in SRL. Table 2 shows the features 
in better capturing the details between inside 
arguments and nominal predicates. Specially, 
features ai6 and ai7 are sister-related features, 
inspired by the features related with the 
neighboring arguments in Jiang and Ng (2006). 
Statistics on NomBank and Chinese Nom-
Bank show that about 20% and 22% of argu-
ments are introduced via a support verb, 
respectively. Since a support verb pivots outside 
arguments and the nominal predicate on its two 
sides, support verbs play an important role in 
labeling these arguments. Here, we also identify 
intervening verbs as alternatives to support verbs 
since support verbs are not explicitly in the test-
ing phase. Table 3 lists the intervening verb-
related features (ao1-ao4, ao11-ao14) employed 
in this paper. 
 
Feature Remarks 
ai1 Whether the focus constituent is adjacent to 
the predicate. Yes or No. (Yes) 
ai2 The headword (ai2H) and pos (ai2P) of the 
predicate?s nearest right sister. (??/bank, 
NN) 
ai3 Whether the predicate has right sisters. Yes 
or No. (Yes) 
ai4 Compressed path of b5: compressing se-
quences of identical labels into one. 
(NN<NP>NN) 
ai5 Whether the predicate has sisters. Yes or 
No. (Yes) 
ai6 For each sister of the focus constituent, 
combine b3H&b4&b5&b6. ( ? ?
/bank&NN & NN<NP>NN&right) 
ai7 Coarse version of ai6, b4&b6. (NN&right) 
Table 2: Additional features and their instantiations  
for inside argument candidates, with ?NN(??
/foreign businessman)? as the focus constituent and 
?NN(?? /investment)? as the nominal predicate, 
regarding Figure1. 
 
Feature Remarks 
ao1 Intervening verb itself. (??/provide) 
ao2 The verb class that the intervening verb 
belongs to. (C3b) 
ao3 The path from the focus constituent to the 
intervening verb. (NP<IP>VP>VP>VV) 
ao4 The compressed path of ao3: compressing 
sequences of identical labels into one. 
(NP<IP>VP>VV) 
Combined features: ao11-ao14 
ao11: ao1&ao3;      ao12: ao1&ao4;    
ao13: ao2&ao3;      ao14: ao2&ao4. 
Table 3: Additional features and their instantiations 
for outside argument candidates, with ?NP(????
/Bank of China)? as the focus constituent and ???
/loan? as the nominal predicate, regarding Figure1. 
Feature selection. Some Features proposed 
above may not be effective in tasks of identifica-
tion and classification. We adopt the greedy fea-
ture selection algorithm as described in Jiang 
and Ng (2006) to pick up positive features em-
pirically and incrementally according to their 
contributions on the development data. The al-
gorithm repeatedly selects one feature each time 
which contributes most, and stops when adding 
any of the remaining features fails to improve 
the performance. As far as the SRL task con-
cerned, the whole feature selection process could 
be done as follows: 1). Feature selection for ar-
gument identification: run the selection algo-
1283
 rithm with the basic set of features (b1-b5, b11-
b14) to pick up effective features from (ai1-ai7, 
ao1-ao4, ao11-ao14); 2). Feature selection for 
argument classification: fix the output returned 
in step1 as the feature set of argument identifica-
tion, and run the selection algorithm with the 
basic set of features (b1-b7, b11-b15) to select 
positive features from (ai1-ai7, ao1-ao4, ao11-
ao14) for argument classification. 
4. Integrating Features derived from 
Verbal SRL 
Since Chinese PropBank and NomBank are an-
notated on the same data set with the same lexi-
cal guidelines (e.g. frame files), it may be 
interesting to investigate the contribution of 
Chinese verbal SRL on the performance of Chi-
nese nominal SRL. In the frame files, argument 
labels are defined with regard to their semantic 
roles to the predicate, either a verbal or nominal 
predicate. For example, in the frame file of 
predicate ???/loan?, the borrower is always 
labeled with Arg0 and the lender labeled with 
Arg1. This can be demonstrated by the follow-
ing two sentences: ???/loan? is annotated as a 
nominal and a verbal predicate in S1 and S2, 
respectively. 
S1 [Arg1 ????/Bank of China] [Arg0 ???
????/to Foreign Investment Bank] ??
/provide [Rel??/loan] 
S2  [Arg0 ????/Bank of China] [Arg1 ???
????/from Foreign Investment Bank] [Rel 
??/loan] 
Therefore, it is straightforward to augment 
nominal training instances with verbal ones. 
However, Xue (2006b) found that simply adding 
the training instances for verbal SRL to the 
training data for nominal SRL and indiscrimi-
nately extracting the same features in both ver-
bal and nominal SRLs hurt the performance. 
This may be due to that certain features (e.g. the 
path feature) are much different for verbal and 
nominal SRLs. This can be illustrated in sen-
tences S1 and S2: the verbal instances in S2 are 
negative for semantic role labeling of the nomi-
nal predicate ???/loan? in S1, since ????
?/Bank of China? takes opposite roles in S1 
and S2. So does ????????/(from/to) 
Foreign Investment Bank?. 
Although several support verb-related features 
(ao1-ao4, ao11-ao14) have been proposed, one 
may still ask how large the role support verbs 
can play in nominal SRL. It is interesting to note 
that outside arguments and the highest NP 
phrase headed by the nominal predicate are also 
annotated as arguments of the support verb in 
Chinese PropBank. For example, Chinese Prop-
Bank marks ?????/Bank of China? as Arg0 
and ?????????/4 billion RMB loan? 
as Arg1 for verb ???/provide? in Figure1. Let 
OA be the outside argument, VV be the support 
verb, and NP be the highest NP phrase headed 
by the nominal predicate NN, then there exists a 
pattern ?OA VV NN? in the sentence, where the 
support verb VV plays a certain role in trans-
ferring roles between OA and NN. For example, 
if OA is the agent of VV, then OA is also the 
agent of phrase VP(VV NN). Like the example 
in Figure1, supposing a NP is the agent of sup-
port verb ???/provide? as well as VP phrase 
(??????????? /provide 4 billion 
RMB loan?), we can infer that the NP is the 
lender of the nominal predicate ???/loan? in-
dependently on any other information, such as 
the NP content and the path from the NP to the 
nominal predicate ???/loan?.  
Let C be the focus constituent, V be the inter-
vening verb, and NP be the highest NP headed 
by the nominal predicate. Table 4 shows the fea-
tures (ao5-ao8, p1-p7) derived from verbal SRL. 
In this paper, we develop a state-of-the-art Chi-
nese verbal SRL system, similar to the one as 
shown in Xue (2008), to achieve the goal. Based 
on golden parse trees on Chinese PropBank, our 
Chinese verbal SRL system achieves the per-
formance of 92.38 in F1-measure, comparable to 
Xue (2008) which achieved the performance of 
92.0 in F1-measure. 
 
Feature Remarks 
ao5 Whether C is an argument for V. Yes or No
ao6 The semantic role of C for V. 
ao7 Whether NP is an argument for V. Yes or No
ao8 The semantic role of NP for V. 
Combined features: p1-p7 
p1: ao1&ao5;         p2: ao1&ao6;    p3: ao1&ao5&b1; 
p4: ao1&ao6&b1;  p5: ao1&apo7;  p6: ao1&ao8;  
p7: ao5&ao7. 
Table 4: Features derived from verbal SRL. 
5. Automatic Predicate Recognition 
Unlike Chinese PropBank where almost all the 
verbs are annotated as predicates, Chinese Nom-
Bank only marks those nouns having arguments 
as predicates. Statistics on Chinese NomBank 
show that only 17.5% of nouns are marked as 
predicates. It is possible that a noun is a predi-
1284
 cate in some cases but not in others. Previous 
Chinese nominal SRL systems (Xue, 2006b; 
Xue, 2008) assume that nominal predicates have 
already been manually annotated and thus are 
available. To our best knowledge, there is no 
report on addressing automatic recognition of 
nominal predicates on Chinese nominal SRL. 
Automatic recognition of nominal predicates 
can be cast as a binary classification (e.g., Predi-
cate vs. Non-Predicate) problem. This paper 
employs the convolution tree kernel, as proposed 
in Collins and Duffy (2001), on automatic rec-
ognition of nominal predicates. 
Given the convolution tree kernel, the key 
problem is how to extract a parse tree structure 
from the parse tree for a nominal predicate can-
didate. In this paper, the parse tree structure is 
constructed as follows: 1) starting from the 
predicate candidate?s POS node, collect all of its 
sister nodes (with their headwords); 2). recur-
sively move one level up and collect all of its 
sister nodes (with their headwords) till reaching 
a non-NP node. Specially, in order to explicitly 
mark the positional relation between a node and 
the predicate candidate, all nodes on the left side 
of the candidate are augmented with tags 1 and 2 
for nodes on the right side. Figure 2 shows an 
example of the parse tree structure with regard 
to the predicate candidate ???/loan? as shown 
in Figure 1. 
In our extra experiments we found global sta-
tistic features (e.g. g1-g5) about the predicate 
candidate are helpful in a feature vector-based 
method for predicate recognition. Figure 2 
makes an attempt to utilize those features in ker-
nel-based method. We have explored other ways 
to include those global features. However, the 
way in Figure 2 works best.  
 
 
Let the predicate candidate be w0, and its left 
and right neighbor words be w-1 and w1, respec-
tively. The five global features are defined as 
follows. 
g1 Whether w0 is ever tagged as a verb in the 
training data? Yes or No. 
g2 Whether w0 is ever annotated as a nominal 
predicate in the training data? Yes or No. 
g3 The most likely label for w0 when it occurs 
together with w-1 and w1. 
g4 The most likely label for w0 when it occurs 
together with w-1. 
g5 The most likely label for w0 when it occurs 
together with w1. 
6. Experiment Results and Discussion 
We have evaluated our Chinese nominal SRL 
system on Chinese NomBank with Chinese 
PropBank 2.0 as its counterpart. 
6.1. Experimental Settings 
This version of Chinese NomBank consists of 
standoff annotations on the files (chtb_001 to 
1151.fid) of Chinese Penn TreeBank 5.1. Fol-
lowing the experimental setting in Xue (2008), 
648 files (chtb_081 to 899.fid) are selected as 
the training data, 72 files (chtb_001 to 040.fid 
and chtb_900 to 931.fid) are held out as the test 
data, and 40 files (chtb_041 to 080.fid) as the 
development data, with 8642, 1124, and 731 
propositions, respectively. 
As Chinese words are not naturally segmented 
in raw sentences, two Chinese automatic parsers 
are constructed: word-based parser (assuming 
golden word segmentation) and character-based 
parser (with automatic word segmentation). 
Here, Berkeley parser (Petrov and Klein, 2007)1 
is chosen as the Chinese automatic parser. With 
regard to character-based parsing, we employ a 
Chinese word segmenter, similar to Ng and Low 
(2004), to obtain the best automatic segmenta-
tion result for a given sentence, which is then 
fed into Berkeley parser for further syntactic 
parsing. Both the word segmenter and Berkeley 
parser are developed with the same training and 
development datasets as our SRL experiments. 
The word segmenter achieves the performance 
of 96.1 in F1-measure while the Berkeley parser 
gives a performance of 82.5 and 85.5 in F1-
measure on golden and automatic word segmen-
tation, respectively2.  
??? 1 In addition, SVMLight with the tree kernel 
function (Moschitti, 2004) 3  is selected as our 
classifier. In order to handle multi-classification 
                                                          
1 Berkeley Parser. http://code.google.com/p/berkeleyparser/ 
2 POSs are not counted in evaluating the performance of 
word-based syntactic parser, but they are counted in evalu-
ating the performance of character-based parser. Therefore 
the F1-measure for the later is higher than that for the for-
mer. 
3 SVM-LIGHT-TK. http://dit.unitn.it/~moschitt/ 
Figure 2: Semantic sub-tree for nominal predicate
RMB 
?? 
loan 
?? 1 
provide 
??? 1 
4 billion 
VV1 
NN1 NN 
NPQP1 
NP
VP 
g1 ?. g5
1285
 problem in argument classification, we apply the 
one vs. others strategy, which builds K classifi-
ers so as to separate one class from all others. 
For argument identification and classification, 
we adopt the linear kernel and the training pa-
rameter C is fine-tuned to 0.220. For automatic 
recognition of nominal predicates, the training 
parameter C and the decay factor ?  in the con-
volution tree kernel are fine-tuned to 2.0 and 0.2, 
respectively. 
6.2. Results with Golden Parse Trees and 
Golden Nominal Predicates 
Effect of nominal SRL-specific features 
 
 Rec.(%) Pre.(%) F1 
traditional features 62.83 73.58 67.78 
+nominal SRL-specific  
features 
69.90 75.11 72.55 
Table 5: The performance of nominal SRL on the 
development data with golden parse trees and golden 
nominal predicates 
After performing the greedy feature selection 
algorithm on the development data, features 
{ao1, ai6, ai2P, ai5, ao2, ao12, ao14}, as pro-
posed in Section 3.2, are selected consecutively 
for argument identification, while features {ai7, 
ao1, ai1, ao2, ai5, ao4} are selected for argument 
classification. Table 5 presents the SRL results 
on the development data. It shows that nominal 
SRL-specific features significantly improve the 
performance from 67.78 to 72.55 ( ) 
in F1-measure. 
05.0;2 <p?
Effect of features derived from verbal SRL 
 
Features Rec.(%) Pre.(%) F1 
baseline 67.86 73.63 70.63  
+ao5 68.15 73.60 70.77 (+0.14)
+ao6 67.66 72.80 70.14 (-0.49)
+ao7 68.20 75.41 71.62 (+0.99)
+ao8 68.30 75.39 71.67 (+1.04)
+p1 67.91 74.40 71.00 (+0.37)
+p2 67.76 74.20 70.83 (+0.20)
+p3 67.96 74.69 71.16 (+0.53)
+p4 68.01 74.18 70.96 (+0.33)
+p5 68.01 75.01 71.39 (+0.76)
+p6 68.20 75.12 71.49 (+0.86)
+p7 68.40 75.70 71.87 (+1.24)
Table 6: Effect of features derived from verbal SRL 
on the performance of nominal SRL on the test data 
with golden parse trees and golden nominal predi-
cates. The first row presents the performance using 
traditional and nominal SRL-specific features. 
 
 
 Rec.(%) Pre.(%) F1 
baseline  67.86 73.63 70.63 
+features derived 
from verbal SRL
68.40 77.51 72.67 
Xue (2008) 66.1 73.4 69.6 
Table 7: The performance of nominal SRL on the test 
data with golden parse trees and golden nominal 
predicates 
 
Table 6 shows the effect of features derived 
from verbal SRL in an incremental way. It 
shows that only the feature ao6 has negative ef-
fect due to its strong relevance with intervening 
verbs and thus not included thereafter. Table 7 
shows the performance on the test data with or 
without using the features derived from the ver-
bal SRL system. It shows these features signifi-
cantly improve the performance ( ) 
on nominal SRL. Table 7 also shows our system 
outperforms Xue (2008) by 3.1 in F1-measure. 
05.0;2 <p?
6.3. Results with Automatic Parse Trees 
and Golden Nominal Predicates 
In previous section we have assumed the avail-
ability of golden parse trees during the testing 
process. Here we conduct experiments on auto-
matic parse trees, using the Berkeley parser. 
Since arguments come from constituents in 
parse trees, those arguments, which do not align 
with any syntactic constituents, are simply dis-
carded. Moreover, for any nominal predicate 
segmented incorrectly by the word segmenter, 
all its arguments are unable to be labeled neither. 
Table 8 presents the SRL performance on the 
test data by using automatic parse trees. It shows 
that the performance drops from 72.67 to 60.87 
in F1-measure when replacing golden parse trees 
with word-based automatic ones, partly due to 
the absence of 6.9% arguments in automatic 
trees, and wrong POS tagging of nominal predi-
cates. Table 8 also compares our system with 
Xue (2008). It shows that our system also out-
performs Xue (2008) on Chinese NomBank. 
 Rec. (%) Pre. (%) F1 
This paper 56.95(53.55) 66.74(66.69) 60.87(59.40)
Xue (2008) 53.1 (52.9) 62.9 (62.3) 57.6 (57.3) 
Table 8: The performance of nominal SRL on the test 
data with automatic parse trees and golden predicates. 
Here, the numbers outside the parentheses indicate 
the performance using a word-based parser, while the 
numbers inside indicate the performance using a 
character-based parser4. 
                                                          
4 About 1.6% nominal predicates are mistakenly segmented 
by the character-based parser, thus their arguments are 
missed directly. 
1286
 6.4. Results with Automatic Nominal Predi-
cates 
So far nominal predicates are assumed to be 
manually annotated and available. Here we turn 
to a more realistic scenario in which both the 
parse tree and nominal predicates are automati-
cally obtained. In the following, we first report 
the results of automatic nominal predicate rec-
ognition and then the results of nominal SRL on 
automatic recognition of nominal predicates. 
Results of nominal predicate recognition 
Parses g1-g5 Rec.(%) Pre.(%) F1 
no 91.46 88.93 90.18 golden 
yes 92.62 89.36 90.96 
word-based yes 86.39 81.80 84.03 
character-based yes 84.79 81.94 83.34 
Table 9: The performance of automatic nominal 
predicate recognition on the test data 
 
Table 9 lists the predicate recognition results, 
using the parse tree structure, as shown in Sec-
tion 5, and the convolution tree kernel, as pro-
posed in Collins and Duffy (2001). The second 
column (g1-g5) indicates whether the global fea-
tures (g1-g5) are included in the parse tree struc-
ture. We have also defined a simple rule that 
treats a noun which is ever a verb or a nominal 
predicate in the training data as a nominal predi-
cate. Based on golden parse trees, the rule re-
ceives the performance of 81.40 in F1-measure. 
This suggests that our method significantly out-
performs the simple rule-based one. Table 9 also 
shows that: 
z As a complement to local structural informa-
tion, global features improve the performance 
of automatic nominal predicate recognition 
by 0.78 in F1-measure. 
z The word-based syntactic parser decreases 
the F1-measure from 90.96 to 84.03, mostly 
due to the POSTagging errors between NN 
and VV, while the character-based syntactic 
parser further drops the F1-measure by 0.69, 
due to automatic word segmentation. 
Results with automatic predicates 
 
Parses Predicates Rec.(%) Pre.(%) F1 
golden 68.40 77.51 72.67 golden 
automatic 65.07 74.65 69.53 
golden 55.95 66.74 60.87 word-
based automatic 52.67 59.56 55.90 
golden 53.55 66.69 59.40 character-
based automatic 50.66 59.60 54.77 
Table 10: The performance of nominal SRL on the 
test data with the choices of golden/automatic parse 
trees and golden/automatic predicates 
In order to have a clear performance comparison 
among nominal SRL on golden/automatic parse 
trees and golden/automatic predicates, Table 10 
lists all the results in those scenarios. 
6.5. Comparison 
Chinese nominal SRL vs. Chinese verbal SRL 
Comparison with Xue (2008) shows that the per-
formance of Chinese nominal SRL is about 20 
lower (e.g. 72.67 vs. 92.38 in F1-measure) than 
that of Chinese verbal SRL, partly due to the 
smaller amount of annotated data (about 1/5) in 
Chinese NomBank than that in Chinese Prop-
Bank. Moreover, according to Chinese Nom-
Bank annotation criteria (Xue 2006a), even 
when a noun is a true deverbal noun, not all of 
its modifiers are legitimate arguments or ad-
juncts of this predicate. Only arguments that can 
co-occur with both the nominal and verbal forms 
of the predicate are considered in the NomBank 
annotation. This means that the judgment of ar-
guments is semantic rather than syntactic. These 
facts may also partly explain the lower nominal 
SRL performance, especially the performance of 
argument identification. This can be illustrated 
by the statistics on the development data that 
96% (40%) of verbal (nominal) predicates? sis-
ters are annotated as arguments. Finally, the 
predicate-argument structure of nominal predi-
cates is more flexible and complicated than that 
of verbal predicates as illustrated in Xue (2006a). 
Chinese nominal SRL vs. English nominal 
SRL 
Liu and Ng (2007) reported the performance of 
77.04 and 72.83 in F1-measure on English Nom-
Bank when golden and automatic parse trees are 
used, respectively. Taking into account that Chi-
nese verbal SRL achieves comparable perform-
ance with English verbal SRL on golden parse 
trees, the performance gap between Chinese and 
English nominal SRL (e.g. 72.67 vs. 77.04 in 
F1-measure) presents great challenge for Chi-
nese nominal SRL. Moreover, while automatic 
parse trees only decrease the performance of 
English nominal SRL by about 4.2 in F1-
measure, automatic parse trees significantly de-
crease the performance of Chinese nominal SRL 
by more than 12 in F1-measure due to the much 
lower performance of Chinese syntactic parsing. 
7. Conclusion 
In this paper we investigate nominal SRL in 
Chinese language. In particular, some nominal 
SRL-specific features are included to improve 
1287
 the performance. Moreover, various features 
derived from verbal SRL are properly integrated 
into nominal SRL. Finally, a convolution tree 
kernel is adopted to address the issue of auto-
matic nominal predicates recognition, which is 
essential in a nominal SRL system.  
To our best knowledge, this is the first re-
search on 
1) Exploring Chinese nominal SRL on auto-
matic parse trees with automatic predicate 
recognition; 
2) Successfully integrating features derived 
from Chinese verbal SRL into Chinese nomi-
nal SRL with much performance improve-
ment. 
Acknowledgement  
This research was supported by Project 
60673041 and 60873150 under the National 
Natural Science Foundation of  China, Project 
2006AA01Z147 under the  ?863? National 
High-Tech Research and Development of China, 
and Project BK2008160 under the Natural Sci-
ence Foundation of the Jiangsu province of 
China. We also want to thank Dr. Nianwen Xue 
for share of the verb class file. We also want to 
thank the reviewers for insightful comments. 
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of COLING-ACL 1998. 
Xavier Carreras and Lluis M?rquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2004.  
Xavier Carreras and Lluis M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2005.  
Michael Collins and Nigel Duffy. 2001. Convolution 
Kernels for Natural Language. In Proceedings of 
NIPS 2001.  
Weiwei Ding and Baobao Chang. 2008. Improving 
Chinese Semantic Role Classification with Hierar-
chical Feature Selection Strategy. In Proceedings 
of EMNLP 2008. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic 
role Labeling of NomBank: a Maximum Entropy 
Approach. In Proceedings of EMNLP 2006.  
Chang Liu and Hwee Tou Ng. 2007. Learning Predic-
tive Structures for Semantic Role Labeling of 
NomBank. In Proceedings of ACL 2007. 
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Yong, and R. Grishman. 2004. Anno-
tating Noun Argument Structure for NomBank. In 
Proceedings of LREC 2004.  
Alessandro Moschitti. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. In Pro-
ceedings of ACL 2004. 
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion Answering based on Semantic Structures. In 
Proceedings of COLING 2004.  
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-speech Tagging: One-at-a-time or All-at-once? 
Word-based or Character-based? In Proceedings 
of EMNLP 2004. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics. 
Slav Petrov. and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceesings of 
NAACL 2007.  
Simone Paolo Ponzetto and Michael Strube. 2006. 
Semantic Role Labeling for Coreference Resolu-
tion. In Proceedings of EACL 2006. 
Sameer Pradhan, Honglin Sun, Wayne Ward, James 
H. Martin, and Dan Jurafsky. 2004. Parsing Ar-
guments of Nominalizations in English and Chi-
nese. In Proceedings of NAACL-HLT 2004.  
Honglin Sun and Daniel Jurafsky. 2004. Shallow 
Semantic Parsing of Chinese. In Proceedings of 
NAACL 2004.  
Mihai Surdeanu, Sanda Harabagiu, John Williams 
and Paul Aarseth. 2003. Using Predicate-argument 
Structures for Information Extraction. In Proceed-
ings of ACL 2003. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of 
Syntactic and Semantic Dependencies. In Pro-
ceedings of CoNLL 2008. 
Nianwen Xue and Martha Palmer. 2003. Annotating 
the Propositions in the Penn Chinese TreeBank. In 
Proceedings of 2nd SIGHAN Workshop on Chinese 
Language Processing.  
Nianwen Xue and Martha Palmer. 2005. Automatic 
Semantic Role Labeling for Chinese verbs. In 
Proceedings of IJCAI 2005.  
Nianwen Xue. 2006a. Annotating the Predicate-
Argument Structure of Chinese Nominalizations. 
In Proceedings of the LREC 2006. 
Nianwen Xue. 2006b. Semantic Role Labeling of 
Nominalized Predicates in Chinese. In Proceed-
ings of HLT-NAACL 2006. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
1288
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 845?850,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Converting Continuous-Space Language Models into
N-gram Language Models for Statistical Machine Translation
Rui Wang1,2,3, Masao Utiyama2, Isao Goto2, Eiichro Sumita2, Hai Zhao1,3 and Bao-Liang Lu1,3
1 Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering,
Shanghai Jiao Tong Unviersity, Shanghai, 200240, China
2 Multilingual Translation Laboratory, MASTAR Project,
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
3 MOE-Microsoft Key Lab. for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong Unviersity, Shanghai 200240 China
wangrui.nlp@gmail.com, mutiyama/igoto/eiichiro.sumita@nict.go.jp, zhaohai@cs.sjtu.edu.cn, bllu@sjtu.edu.cn
Abstract
Neural network language models, or
continuous-space language models (CSLMs),
have been shown to improve the performance
of statistical machine translation (SMT)
when they are used for reranking n-best
translations. However, CSLMs have not
been used in the first pass decoding of SMT,
because using CSLMs in decoding takes a lot
of time. In contrast, we propose a method
for converting CSLMs into back-off n-gram
language models (BNLMs) so that we can
use converted CSLMs in decoding. We show
that they outperform the original BNLMs and
are comparable with the traditional use of
CSLMs in reranking.
1 Introduction
Language models are important in natural language
processing tasks such as speech recognition and
statistical machine translation. Traditionally, back-
off n-gram language models (BNLMs) (Chen and
Goodman, 1996; Chen and Goodman, 1998;
Stolcke, 2002) are being widely used for these tasks.
Recently, neural network language models,
or continuous-space language models (CSLMs)
(Bengio et al, 2003; Schwenk, 2007; Le et al, 2011)
are being used in statistical machine translation
(SMT) (Schwenk et al, 2006; Son et al, 2010;
Schwenk et al, 2012; Son et al, 2012; Niehues
and Waibel, 2012). These works have shown that
CSLMs can improve the BLEU (Papineni et al,
2002) scores of SMT when compared with BNLMs,
on the condition that the training data for language
modeling are the same size. However, in practice,
CSLMs have not been widely used in SMT.
One reason is that the computational costs of
training and using CSLMs are very high. Various
methods have been proposed to tackle the training
cost issues (Son et al, 2010; Schwenk et al, 2012;
Mikolov et al, 2011). However, there has been little
work on reducing using costs. Since the using costs
of CSLMs are very high, it is difficult to use CSLMs
in decoding directly.
A common approach in SMT using CSLMs is
the two pass approach, or n-best reranking. In this
approach, the first pass uses a BNLM in decoding
to produce an n-best list. Then, a CSLM is used to
rerank those n-best translations in the second pass.
(Schwenk et al, 2006; Son et al, 2010; Schwenk et
al., 2012; Son et al, 2012)
Another approach is using restricted Boltzmann
machines (RBMs) (Niehues and Waibel, 2012)
instead of using multi-layer neural networks
(Bengio et al, 2003; Schwenk, 2007; Le et al,
2011). Since probability in a RBM can be calculated
very efficiently (Niehues and Waibel, 2012), they
can use the RBM language model in SMT decoding.
However, the RBM was just used in an adaptation of
SMT, not in a large SMT task, because the training
costs of RBMs are very high.
The last approach is using a BNLM to simulate
a CSLM (Deoras et al, 2011; Arsoy et al, 2013).
(Deoras et al, 2011) used a recurrent neural network
language model (RNNLM) to generate a large
amount of text, which was generated by sampling
words from the probability distributions calculated
by the RNNLM. Then, they trained the BNLM
845
from the text using the interpolated Kneser-Ney
smoothing method. (Arsoy et al, 2013) converted
neural network language models of increasing order
to pruned back-off language models, using lower-
order models to constrain the n-grams allowed in
higher-order models.
Both of these methods were used in decoding for
speech recognition. These methods were applied
to not-so-large scale experiments (55 million (M)
words for training their BNLMs) (Arsoy et al,
2013). In contrast, our method is applied to SMT
and can be used to improve a BNLM created from
746 M words by using a CSLM trained from 42 M
words.
Because BNLMs can be trained from much larger
corpora than those that can be used for training
CSLMs, improving a BNLM by using a CSLM
trained from a smaller corpus is very important.
Actually, a CSLM trained from a smaller corpus
can improve the BLEU scores of SMT if it is used
in the n-best reranking (Schwenk, 2010; Huang et
al., 2013). In contrast, we will demonstrate that a
BNLM simulating a CSLM can improve the BLEU
scores of SMT in the first pass decoding.
Our approach is as follows: (1) First, we train a
CSLM (Schwenk, 2007) from a corpus. (2) Second,
we also train a BNLM from the same corpus or
larger corpus. (3) Finally, we rewrite the probability
of each n-gram of the BNLM with that probability
calculated from the CSLM.We also re-normalize the
probabilities of the BNLM, then use the re-written
BNLM in SMT decoding.
In Section 2, we describe the BNLM and CSLM
(Schwenk, 2010) used for re-writing BNLMs. In
Section 3, we describe the method of converting
a CSLM into a BNLM. In Sections 4 and 5, we
evaluate our method and conclude.
2 Language Models
In this section, we will introduce the standard
BNLM and CSLM structure and probability
calculation.
2.1 Standard back-off ngram language model
A BNLM predicts the probability of a wordwi given
its preceding n ? 1 words hi = wi?1i?n+1. But
it will suffer from data sparseness if the context,
hi, does not appear in the training data. So an
estimation by ?backing-off? to models with smaller
histories is necessary. In the case of the modified
Kneser-Ney smoothing (Chen and Goodman, 1998),
the probability of wi given hi under a BNLM,
Pb(wi|hi), is:
Pb(wi|hi) = P?b(wi|hi) + ?(hi)Pb(wi|wi?1i?n+2) (1)
where P?b(wi|hi) is a discounted probability and
?(hi) is the back-off weight. A BNLM is used with
a CSLM as shown below.
2.2 CSLM structure and probability
calculation
The main structure of a CSLM using a multi-
layer neural network contains four layers: the input
layer projects all words in the context hi onto
the projection layer (the first hidden layer); the
second hidden layer and the output layer achieve the
non-liner probability estimation and calculate the
language model probability P (wi|hi) for the given
context. (Schwenk, 2007).
The CSLM calculates the probabilities of all
words in the vocabulary of the corpus given
the context at once. However, because the
computational complexity of calculating the
probabilities of all words is quite high, the CSLM is
only used to calculate the probabilities of a subset
of the whole vocabulary. This subset is called
a short-list, which consists of the most frequent
words in the vocabulary. The CSLM also calculates
the sum of the probabilities of all words not in the
short-list by assigning a neuron for that purpose.
The probabilities of other words not in the short-list
are obtained from a BNLM (Schwenk, 2007;
Schwenk, 2010).
Let wi, hi be the current word and history. The
CSLM with a BNLM calculates the probability of
wi given hi, P (wi|hi), as follows:
P (wi|hi) =
{
Pc(wi|hi)
1?Pc(o|hi)
Ps(hi) if wi ? short-list
Pb(wi|hi) otherwise
(2)
where Pc(?) is the probability calculated by the
CSLM, Pc(o|hi) is the probability of the neuron
for the words not in the short-list, Pb(?) is the
probability calculated by the BNLM as in Eq. 1,
and
Ps(hi) =
?
v?short-list
Pb(v|hi). (3)
846
It can be considered that the CSLM redistributes
the probability mass of all words in the short-list.
This probability mass is calculated by using the
BNLM.
3 Conversion of CSLM into BNLM
As described in the introduction, we first train a
CSLM from a corpus. We also train a BNLM from
the same corpus or a larger corpus. Then, we rewrite
the probability of each ngram in the BNLM with the
probability calculated from the CSLM.
First, we use the probabilities of 1-grams in
the BNLM as they are. Next, we rewrite the
probabilities of n-grams (n=2,3,4,5) in the BNLM
with the probabilities calculated by using the n-gram
CSLM, respectively. Note that the n-gram CSLM
means that the length of its history is n ? 1. Note
also that we only need to rewrite the probabilities
of n-grams ending with a word in the short-list.
Finally, we re-normalize the probabilities of the
BNLM using the SRILM?s ?-renorm? option.
When we rewrite a BNLM trained from a larger
corpus, the ngrams in the BNLM often contain
unknown words for the CSLM. In that case, we use
the probabilities in the BNLM as they are.
4 Experiments
4.1 Common settings
We used the patent data for the Chinese to English
patent translation subtask from the NTCIR-9 patent
translation task (Goto et al, 2011). The parallel
training, development, and test data consisted of 1
M, 2,000, and 2,000 sentences, respectively.
We followed the settings of the NTCIR-9 Chinese
to English translation baseline system (Goto et al,
2011) except that we used various language models
to compare them. We used the MOSES phrase-
based SMT system (Koehn et al, 2003), together
with Giza++ (Och and Ney, 2003) for alignment and
MERT (Och, 2003) for tuning on the development
data. The translation performance was measured by
the case-insensitive BLEU scores on the tokenized
test data. We used mteval-v13a.pl for
calculating BLEU scores.1
1It is available at http://www.itl.nist.gov/iad/
mig/tests/mt/2009/
We used the 14 standard SMT features: five
translation model scores, one word penalty score,
seven distortion scores and one language model
score. Each of the different language models was
used to calculate the language model score.
As the baseline BNLM, we trained a 5-gram
BNLM with modified Kneser-Ney smoothing using
the English side of the 1 M sentences training data,
which consisted of 42 M words. We did not discard
any n-grams in training this model. That is, we
did not use count cutoffs. We call this BNLM as
BNLM42.
A 5-gram CSLM was trained on the same
1 M training sentences using the CSLM toolkit
(Schwenk, 2010). The settings for the CSLM
were: projection layer of dimension 256 for each
word, hidden layer of dimension 384 and output
layer (short-list) of dimension 8192, which were
recommended in the CSLM toolkit. We call this
CSLM CSLM42. CSLM42 used BNLM42 as the
background BNLM.
We also trained a larger 5-gram BNLM with
modified Kneser-Ney smoothing by adding
sentences from the 2005 US patent data distributed
in the NTCIR-8 patent translation task (Fujii et al,
2010) to the 42 M words. The data consisted of
746 M words. We call this BNLM BNLM746. We
discarded 3,4,5-grams that occurred only once when
we created BNLM746.
Next, we re-wrote BNLM42 with CSLM42 by
using the method described in Section 3. This
re-written BNLM was interpolated with BNLM42.
The interpolation weight was determined by the grid
search. That is, we changed the interpolation weight
to 0.1, 0.3, 0.5, 0.7, 0.9 to create an interpolated
BNLM. Then we used that BNLM in the SMT
system to tune the weight parameters on the first
half of the development data. Next, we selected
the interpolation weight that obtained the highest
BLEU score on the second half of the development
data. After we selected the interpolation weight,
we applied MERT again to the 2,000 sentence
development data to tune the weight parameters.2
We call this BNLM CONV42. We also obtained
CONV746 by re-writing BNLM746 with CSLM42
2We aware that the interpolation weight might be
determined by minimizing the perplexity on the development
data. However, we opted to directly maximize the BLEU score.
847
in the same way.
The vocabulary of these language models was the
same, which was extracted from the 1 M training
sentences.
4.2 Experimental results
Table 1 shows the percent BLEU scores on the test
data. The figures in the ?1st pass? column show
the BLEU scores in the first pass decoding when
we changed the language model. The figures in the
?reranking? column show the BLEU scores when
we applied CSLM42 to rerank the 100-best lists for
the different language models. When we applied
CSLM42 for reranking, we added the CSLM42
score as the additional 15th feature. The weight
parameters were tuned by using Z-MERT (Zaidan,
2009).
LMs 1st pass rerank
BNLM42 31.60 32.44
CONV42 32.58 32.98
BNLM746 32.83 33.36
CONV746 33.22 33.54
Table 1: Comparison of BLEU scores
We also performed the paired bootstrap re-
sampling test (Koehn, 2004).3 We sampled 2000
samples for each significance test.
Table 2 shows the results of a statistical
significance test, in which the ?1st? is short for
the ?1st pass?. The marks indicate whether the
LM to the left of a mark is significantly better
than that above the mark at a certain level. (???:
significantly better at ? = 0.01, ?>?: ? = 0.05,
???: not significantly better at ? = 0.05)
First, as shown in the tables, the reranking
by applying CSLM42 increased the BLEU scores
for all language models. This observation is in
accordance with those of previous work (Schwenk,
2010; Huang et al, 2013).
Second, the reranking results of BNLM42 (32.44)
were not better than those of the first pass of
BNLM746 (32.83). This indicates that if the
underlying BNLM is made from a small corpus, the
reranking using CSLM can not compensate for it.
3We used the code available at http://www.ark.cs.
cmu.edu/MT/.
BN
LM
74
6
(re
ra
nk
)
CO
N
V
74
6
(1
st)
CO
N
V
42
(re
ra
nk
)
BN
LM
74
6
(1
st)
CO
N
V
42
(1
st)
BN
LM
42
(re
ra
nk
)
BN
LM
42
(1
st)
CONV746 (rerank) ? ? ? ? ? ? ?
BNLM746 (rerank) ? ? > ? ? ?
CONV746 (1st) ? ? ? ? ?
CONV42 (rerank) ? ? ? ?
BNLM746 (1st) ? ? ?
CONV42 (1st) ? ?
BNLM42 (rerank) ?
Table 2: Significance tests for systems with different LMs
Third, CONV42 was better than BNLM42 for
both first-pass and reranking. This also holds in the
case of CONV746 and BNLM746. This indicated
that our conversion method improved the BNLMs,
even if the underlying BNLMwas trained on a larger
corpus than that used for training the CSLM. As
described in the introduction, this is very important
because BNLMs can be trained from much larger
corpora than those that can be used for training
CSLMs. This observation has not been found in the
previous work.
In addition, the first-pass of CONV42 and
CONV746 (32.58 and 33.22) were comparable with
those of the reranking results of BNLM42 and
BNLM746 (32.44 and 33.36), respectively. That is,
there were no significant differences between these
results. This indicates that our conversion method
preserves the performance of the reranking using
CSLM.
5 Conclusion
We have proposed a method for converting CSLMs
into BNLMs. The method can be used to improve
a BNLM by using a CSLM trained from a smaller
corpus than that used for training the BNLM. We
have also shown that BNLMs created by our method
performs as good as the reranking using CSLMs.
Our future work is to compare our conversion
method with that of (Arsoy et al, 2013).4
4We aware that (Arsoy et al, 2013) compared their method
with the one that is identical with our method. However, the
experiments were conducted on a speech recognition task and
the scale of the experiment was not so large. Since we noticed
their work just before the submission of our paper, we did not
have time to compare their method with our method in SMT.
848
Acknowledgments
We appreciate the helpful discussion with Andrew
Finch and Paul Dixon, and three anonymous
reviewers for many invaluable comments and
suggestions to improve our paper. This work
is supported by the National Natural Science
Foundation of China (Grant No. 60903119, No.
61170114 and No. 61272248), the National
Basic Research Program of China (Grant No.
2013CB329401) and the Science and Technology
Commission of Shanghai Municipality (Grant No.
13511500200).
References
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2013. Converting neural network
language models into back-off language models for
efficient decoding in automatic speech recognition.
In Proc. of IEEE Int. Conf. on Acoustics, Speech
and Signal Processing (ICASSP 2013), Vancouver,
Canada, May. IEEE.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic
language model. Journal of Machine Learning
Research (JMLR), 3:1137?1155, March.
Stanley F. Chen and Joshua Goodman. 1996. An
empirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meeting
on Association for Computational Linguistics, ACL
?96, pages 310?318, Santa Cruz, California, June.
Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical report, Computer Science Group,
Harvard Univ.
A. Deoras, T. Mikolov, S. Kombrink, M. Karafiat,
and Sanjeev Khudanpur. 2011. Variational
approximation of long-span language models for lvcsr.
In Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages 5532?
5535, Prague, Czech Republic, May. IEEE.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2010. Overview of the patent
translation task at the ntcir-8 workshop. In In
Proceedings of the 8th NTCIR Workshop Meeting
on Evaluation of Information Access Technologies:
Information Retrieval, Question Answering and Cross-
lingual Information Access, pages 293?302, Tokyo,
Japan, June.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR-9 Workshop Meeting, pages
559?578, Tokyo, Japan, December.
Zhongqiang Huang, Jacob Devlin, and Spyros
Matsoukas. 2013. Bbn?s systems for the chinese-
english sub-task of the ntcir-10 patentmt evaluation.
In NTCIR-10, Tokyo, Japan, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the
North American Chapter of the Association for
Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54,
Edmonton, Canada. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and
F. Yvon. 2011. Structured output layer neural
network language model. In Acoustics, Speech and
Signal Processing (ICASSP), 2011 IEEE International
Conference on, pages 5524?5527, Prague, Czech
Republic, May. IEEE.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernock. 2011. Strategies for
training large scale neural network language models.
In Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages 196?
201, Prague, Czech Republic, May. IEEE.
Jan Niehues and Alex Waibel. 2012. Continuous space
language models using restricted boltzmann machines.
In Proceedings of the International Workshop for
Spoken Language Translation, IWSLT 2012, pages
311?318, Hong Kong.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for
Computational Linguistics, ACL ?02, pages 311?
849
318, Philadelphia, Pennsylvania, June. Association for
Computational Linguistics.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proceedings
of the COLING/ACL on Main conference poster
sessions, COLING-ACL ?06, pages 723?730, Sydney,
Australia, July. Association for Computational
Linguistics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of LanguageModeling for HLT,
WLM ?12, pages 11?19, Montreal, Canada, June.
Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492?
518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. The Prague
Bulletin of Mathematical Linguistics, pages 137?146.
Le Hai Son, Alexandre Allauzen, Guillaume Wisniewski,
and Franc?ois Yvon. 2010. Training continuous
space language models: some practical issues. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 778?788, Cambridge, Massachusetts,
October. Association for Computational Linguistics.
Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL HLT ?12, pages
39?48, Montreal, Canada, June. Association for
Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings International
Conference on Spoken Language Processing, pages
257?286, November.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
850
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183?188,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning Hierarchical Translation Spans
Jingyi Zhang
1,2
, Masao Utiyama
3
, Eiichro Sumita
3
, Hai Zhao
1,2
1
Center for Brain-Like Computing and Machine Intelligence, Department of Computer
Science and Engineering, Shanghai Jiao Tong Unviersity, Shanghai, 200240, China
2
Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong Unviersity, Shanghai, 200240, China
3
National Institute of Information and Communications Technology
3-5Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
zhangjingyizz@gmail.com, mutiyama/eiichiro.sumita@nict.go.jp,
zhaohai@cs.sjtu.edu.cn
Abstract
We propose a simple and effective ap-
proach to learn translation spans for
the hierarchical phrase-based translation
model. Our model evaluates if a source
span should be covered by translation
rules during decoding, which is integrated
into the translation system as soft con-
straints. Compared to syntactic con-
straints, our model is directly acquired
from an aligned parallel corpus and does
not require parsers. Rich source side
contextual features and advanced machine
learning methods were utilized for this
learning task. The proposed approach was
evaluated on NTCIR-9 Chinese-English
and Japanese-English translation tasks and
showed significant improvement over the
baseline system.
1 Introduction
The hierarchical phrase-based (HPB) translation
model (Chiang, 2005) has been widely adopted in
statistical machine translation (SMT) tasks. The
HPB translation rules based on the synchronous
context free grammar (SCFG) are simple and pow-
erful.
One drawback of the HPB model is the appli-
cations of translation rules to the input sentence
are highly ambiguous. For example, a rule whose
English side is ?X1 by X2? can be applied to any
word sequence that has ?by? in them. In Figure 1,
this rule can be applied to the whole sentence as
well as to ?experiment by tomorrow?.
In order to tackle rule application ambiguities,
a few previous works used syntax trees. Chi-
ang (2005) utilized a syntactic feature in the HPB
I  will  nish  this  experiment  by  tomorrow
?  ?  ?  ??  ??  ??  ??  ??
Figure 1: A translation example.
model, which represents if the source span cov-
ered by a translation rule is a syntactic constituent.
However, the experimental results showed this
feature gave no significant improvement. Instead
of using the undifferentiated constituency feature,
(Marton and Resnik, 2008) defined different soft
syntactic features for different constituent types
and obtained substantial performance improve-
ment. Later, (Mylonakis and Sima?an, 2011) in-
troduced joint probability synchronous grammars
to integrate flexible linguistic information. (Liu
et al., 2011) proposed the soft syntactic constraint
model based on discriminative classifiers for each
constituent type and integrated all of them into the
translation model. (Cui et al., 2010) focused on
hierarchical rule selection using many features in-
cluding syntax constituents.
These works have demonstrated the benefits of
using syntactic features in the HPB model. How-
ever, high quality syntax parsers are not always
easily obtained for many languages. Without this
problem, word alignment constraints can also be
used to guide the application of the rules.
Suppose that we want to translate the English
sentence into the Chinese sentence in Figure 1, a
translation rule can be applied to the source span
?finish this experiment by tomorrow?. Nonethe-
less, if a rule is applied to ?experiment by?, then
the Chinese translation can not be correctly ob-
tained, because the target span projected from ?ex-
183
periment by? contains words projected from the
source words outside ?experiment by?.
In general, a translation rule projects one con-
tinuous source word sequence (source span) into
one continuous target word sequence. Meanwhile,
the word alignment links between the source and
target sentence define the source spans where
translation rules are applicable. In this paper, we
call a source span that can be covered by a trans-
lation rule without violating word alignment links
a translation span.
Translation spans that have been correctly iden-
tified can guide translation rules to function prop-
erly, thus (Xiong et al., 2010) attempted to use
extra machine learning approaches to determine
boundaries of translation spans. They used two
separate classifiers to learn the beginning and end-
ing boundaries of translation spans, respectively.
A source word is marked as beginning (ending)
boundary if it is the first (last) word of a translation
span. However, a source span whose first and last
words are both boundaries is not always a transla-
tion span. In Figure 1, ?I? is a beginning boundary
since it is the first word of translation span ?I will?
and ?experiment? is an ending boundary since it is
the last word of translation span ?finish this exper-
iment? , but ?I will finish this experiment? is not a
translation span. This happens because the trans-
lation spans are nested or hierarchical. Note that
(He et al., 2010) also learned phrase boundaries to
constrain decoding, but their approach identified
boundaries only for monotone translation.
In this paper, taking fully into account that
translation spans being nested, we propose an
approach to learn hierarchical translation spans
directly from an aligned parallel corpus that
makes more accurate identification over transla-
tion spans.
The rest of the paper is structured as follows:
In Section 2, we briefly review the HPB transla-
tion model. Section 3 describes our approach. We
describe experiments in Section 4 and conclude in
Section 5.
2 Hierarchical Phrase-based Translation
Chiang?s HPB model is based on a weighted
SCFG. A translation rule is like: X ? ??, ?,??,
where X is a nonterminal, ? and ? are source and
target strings of terminals and nonterminals, and?
is a one-to-one correspondence between nontermi-
nals in ? and ?. The weight of each rule is:
w (X ? ??, ?,??) =
?
t
h
t
(X ? ??, ?,??)
?
t
(1)
where h
t
are the features defined on the rules.
Rewriting begins with a pair of linked start sym-
bols and ends when there is no nonterminal left.
Let D be a derivation of the grammar, f (D) and
e (D) be the source and target strings generated
by D. D consists of a set of triples ?r, i, j?, each
of which stands for applying a rule r on a span
f (D)
j
i
. The weight of D is calculated as:
w (D) =
?
?r,i,j??D
w (r)? P
lm
(e)
?
lm
? exp (??
wp
|e|)
(2)
where w (r) is the weight of rule r, the last two
terms represent the language model and word
penalty, respectively.
3 Learning Translation Spans
We will describe how to learn translation spans in
this section.
3.1 Our Model
We make a series of binary classifiers
{C
1
, C
2
, C
3
, ...} to learn if a source span
f (D)
j
i
should be covered by translation rules dur-
ing translation. C
k
is trained and tested on source
spans whose lengths are k, i.e., k = j ? i+ 1.
1
C
k
learns the probability
P
k
(v|f (D) , i, j) (3)
where v ? {0, 1}, v = 1 represents a rule is ap-
plied on f (D)
j
i
, otherwise v = 0.
Training instances for these classifiers are ex-
tracted from an aligned parallel corpus according
to Algorithm 1. For example, ?I will? and ?will
finish? are respectively extracted as positive and
negative instances in Figure 1.
Note that our model in Equation 3 only uses
the source sentence f (D) in the condition. This
means that the probabilities can be calculated be-
fore translation. Therefore, the predicted prob-
abilities can be integrated into the decoder con-
veniently as soft constraints and no extra time is
added during decoding. This enables us to use
rich source contextual features and various ma-
chine learning methods for this learning task.
1
We indeed can utilize just one classifier for all source
spans. However, it will be difficult to design features for such
a classifier unless only boundary word features are adopted.
On the contrary, we can fully take advantage of rich informa-
tion about inside words as we turn to the fixed span length
approach.
184
3.2 Integration into the decoder
It is straightforward to integrate our model into
Equation 2. It is extended as
w (D) =
?
?r,i,j??D
w (r)? P
lm
(e)
?
lm
? exp (??
wp
|e|)
? P
k
(v = 1|f (D) , i, j)
?
k
(4)
where ?
k
is the weight for C
k
.
During decoding, the decoder looks up the
probabilities P
k
calculated and stored before de-
coding.
Algorithm 1 Extract training instances.
Input: A pair of parallel sentence f
n
1
and e
m
1
with
word alignments A.
Output: Training examples for {C
1
, C
2
, C
3
, ...}.
1: for i = 1 to n do
2: for j = i to n do
3: if ?e
q
p
, 1 ? p ? q ? m
& ? (k, t) ? A, i ? k ? j, p ? t ? q
& ? (k, t) ? A, i ? k ? j ? p ? t ? q
then
4: f
j
i
is a positive instance for C
j?i+1
5: else
6: f
j
i
is a negative instance for C
j?i+1
7: end if
8: end for
9: end for
3.3 Classifiers
We compare two machine learning methods for
learning a series of binary classifiers.
For the first method, each C
k
is individually
learned using the maximum entropy (ME) ap-
proach (Berger et al., 1996):
P
k
(v|f (D) , i, j) =
exp
(?
t
?
t
h
t
(v, f (D) , i, j)
)
?
v
?
exp
(?
t
?
t
h
t
(v
?
, f (D) , i, j)
)
(5)
where h
t
is a feature function and ?
t
is weight
of h
t
. We use rich source contextual fea-
tures: unigram, bigram and trigram of the phrase
[f
i?3
, ..., f
j+3
].
As the second method, these classification tasks
are learned in the continuous space using feed-
forward neural networks (NNs). Each C
k
has
the similar structure with the NN language model
(Vaswani et al., 2013). The inputs to the NN are
indices of the words: [f
i?3
, ..., f
j+3
]. Each source
word is projected into an N dimensional vector.
The output layer has two output neurons, whose
values correspond to P
k
(v = 0|f (D) , i, j) and
P
k
(v = 1|f (D) , i, j).
For both ME and NN approaches, words that
occur only once or never occur in the training
corpus are treated as a special word ?UNK? (un-
known) during classifier training and predicting,
which can reduce training time and make the clas-
sifier training more smooth.
4 Experiment
We evaluated the effectiveness of the proposed ap-
proach for Chinese-to-English (CE) and Japanese-
to-English (JE) translation tasks. The datasets of-
ficially provided for the patent machine translation
task at NTCIR-9 (Goto et al., 2011) were used in
our experiments. The detailed training set statis-
tics are given in Table 1. The development and test
SOURCE TARGET
CE
#Sents 954k
#Words 37.2M 40.4M
#Vocab 288k 504k
JE
#Sents 3.14M
#Words 118M 104M
#Vocab 150k 273k
Table 1: Data sets.
sets were both provided for CE task while only the
test set was provided for JE task. Therefore, we
used the sentences from the NTCIR-8 JE test set
as the development set. Word segmentation was
done by BaseSeg (Zhao et al., 2006; Zhao and Kit,
2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao
et al., 2013) for Chinese and Mecab
2
for Japanese.
To learn the classifiers for each translation task,
the training set and development set were put to-
gether to obtain symmetric word alignment us-
ing GIZA++ (Och and Ney, 2003) and the grow-
diag-final-and heuristic (Koehn et al., 2003). The
source span instances extracted from the aligned
training and development sets were used as the
training and validation data for the classifiers.
The toolkit Wapiti (Lavergne et al., 2010) was
adopted to train ME classifiers using the classi-
cal quasi-newton optimization algorithm with lim-
ited memory. The NNs are trained by the toolkit
NPLM (Vaswani et al., 2013). We chose ?recti-
fier? as the activation function and the logarithmic
loss function for NNs. The number of epochs was
set to 20. Other parameters were set to default
2
http://sourceforge.net/projects/mecab/files/
185
Span
length
CE JE
Rate
ME NN
Rate
ME NN
P N P N P N P N
1 2.67 0.93 0.63 0.93 0.64 1.08 0.85 0.79 0.86 0.80
2 1.37 0.83 0.70 0.82 0.75 0.73 0.69 0.84 0.71 0.87
3 0.86 0.70 0.80 0.73 0.83 0.52 0.56 0.89 0.63 0.90
4 0.62 0.57 0.81 0.67 0.88 0.36 0.48 0.93 0.54 0.93
5 0.48 0.52 0.90 0.61 0.91 0.26 0.30 0.96 0.47 0.95
6 0.40 0.47 0.91 0.58 0.92 0.20 0.25 0.97 0.41 0.96
7 0.34 0.40 0.93 0.53 0.93 0.16 0.14 0.98 0.33 0.97
8 0.28 0.35 0.94 0.46 0.94 0.13 0 1 0.32 0.97
9 0.22 0.28 0.96 0.37 0.96 0.10 0 1 0.25 0.98
10 0.15 0.21 0.97 0.28 0.97 0.08 0 1 0.23 0.99
Table 2: Classification accuracies. The Rate column represents ratio of positive instances to negative
instances; the P and N columns give classification accuracies for positive and negative instances.
values. The training time of one classifier on a
12-core 3.47GHz Xeon X5690 machine was 0.5h
(2.5h) using ME (NN) approach for CE task; 1h
(4h) using ME (NN) approach for JE task .
The classification results are shown in Table 2.
Instead of the undifferentiated classification accu-
racy, we present separate classification accuracies
for positive and negative instances. The big differ-
ence between classification accuracies for positive
and negative instances was caused by the unbal-
anced rate of positive and negative instances in the
training corpus. For example, if there are more
positive training instances, then the classifier will
tend to classify new instances as positive and the
classification accuracy for positive instances will
be higher. In our classification tasks, there are less
positive instances for longer span lengths.
Since the word order difference of JE task is
much more significant than that of CE task, there
are more negative Japanese translation span in-
stances than Chinese. In JE tasks, the ME classi-
fiers C
8
, C
9
and C
10
predicted all new instances to
be negative due to the heavily unbalanced instance
distribution.
As shown in Table 2, NN outperformed ME ap-
proach for our classification tasks. As the span
length growing, the advantage of NN became
more significant. Since the classification accura-
cies deceased to be quite low for source spans with
more than 10 words, only {C
1
, ..., C
10
} were inte-
grated into the HPB translation system.
For each translation task, the recent version
of Moses HPB decoder (Koehn et al., 2007)
with the training scripts was used as the base-
line (Base). We used the default parameters for
Moses, and a 5-gram language model was trained
on the target side of the training corpus by IRST
LM Toolkit
3
with improved Kneser-Ney smooth-
ing. {C
1
, ..., C
10
} were integrated into the base-
line with different weights, which were tuned by
MERT (Och, 2003) together with other feature
weights (language model, word penalty,...) under
the log-linear framework (Och and Ney, 2002).
BLEU-n n-gram precisions
Method TER 4 1 2 3 4
CE
Base 49.39- - 33.07- - 69.9/40.7/25.8/16.9
BLM 48.60 33.93 70.0/41.4/26.6/17.6
ME 49.02- 33.63- 70.0/41.2/26.3/17.4
NN 48.09++ 34.35++ 70.1/41.9/27.0/18.0
JE
Base 57.39- - 30.13- - 67.1/38.3/23.0/14.0
BLM 56.79 30.81 67.7/38.9/23.6/14.5
ME 56.48 31.01 67.6/39.0/23.8/14.7
NN 55.96++ 31.77++ 67.8/39.7/24.6/15.4
Table 3: Translation results. The symbol ++ (- -)
represents a significant difference at the p < 0.01
level and - represents a significant difference at the
p < 0.05 level against the BLM.
We compare our method with the baseline and
the boundary learning method (BLM) (Xiong et
al., 2010) based on Maximum Entropy Markov
Models with Markov order 2. Table 3 reports
BLEU (Papineni et al., 2002) and TER (Snover
et al., 2006) scores. Significance tests are con-
ducted using bootstrap sampling (Koehn, 2004).
Our ME classifiers achieve comparable translation
improvement with the BLM and NN classifiers en-
hance translation system significantly compared to
others. Table 3 also shows that the relative gain
was higher for higher n-grams, which is reason-
able since the higher n-grams have higher ambi-
guities in the translation rule application.
It is true that because of multiple parallel sen-
tences, a source span can be applied with transla-
3
http://hlt.fbk.eu/en/irstlm
186
tion rules in one sentence pair but not in another
sentence pair. So we used the probability score
as a feature in the decoding. That is, we did not
use classification results directly but use the prob-
ability score for softly constraining the decoding
process.
5 Conclusion
We have proposed a simple and effective transla-
tion span learning model for HPB translation. Our
model is learned from aligned parallel corpora and
predicts translation spans for source sentence be-
fore translating, which is integrated into the trans-
lation system conveniently as soft constraints. We
compared ME and NN approaches for this learn-
ing task. The results showed that NN classifiers on
the continuous space model achieved both higher
classification accuracies and better translation per-
formance with acceptable training times.
Acknowledgments
Hai Zhao were partially supported by CSC fund
(201304490199), the National Natural Science
Foundation of China (Grant No.60903119, Grant
No.61170114, and Grant No.61272248), the Na-
tional Basic Research Program of China (Grant
No.2013CB329401), the Science and Technol-
ogy Commission of Shanghai Municipality (Grant
No.13511500200), the European Union Seventh
Framework Program (Grant No.247619), and the
art and science interdiscipline funds of Shang-
hai Jiao Tong University, a study on mobilization
mechanism and alerting threshold setting for on-
line community, and media image and psychology
evaluation: a computational intelligence approach.
References
Adam Berger, Vincent Della Pietra, and Stephen Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational linguis-
tics, 22(1):39?71.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270. As-
sociation for Computational Linguistics.
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and
Tiejun Zhao. 2010. A joint rule selection model
for hierarchical phrase-based translation. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 6?11. Association for Computational Linguis-
tics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the ntcir-9 workshop. In
Proceedings of NTCIR, volume 9, pages 559?578.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Learn-
ing phrase boundaries for hierarchical phrase-based
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 383?390. Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.
2010. Practical very large scale crfs. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 504?513, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Lemao Liu, Tiejun Zhao, Chao Wang, and Hailong
Cao. 2011. A unified and discriminative soft syn-
tactic constraint model for hierarchical phrase-based
translation. In the Thirteenth Machine Translation
Summit, pages 253?260. Asia-Pacific Association
for Machine Translation.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In ACL, pages 1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learn-
ing hierarchical translation structure with linguis-
tic annotations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 642?652. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295?302. Association for
Computational Linguistics.
187
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223?231.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387?1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010.
Learning translation boundaries for phrase-based
decoding. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 136?144. Association for Computa-
tional Linguistics.
Hai Zhao and Chunyu Kit. 2008. Exploiting unla-
beled text with different unsupervised segmentation
criteria for chinese word segmentation. Research in
Computing Science, 33:93?104.
Hai Zhao and Chunyu Kit. 2011. Integrating unsu-
pervised and supervised word segmentation: The
role of goodness measures. Information Sciences,
181(1):163?183.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, pages 162?165. Sydney: July.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 9(2):5.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word seg-
mentation for chinese machine translation. In Com-
putational Linguistics and Intelligent Text Process-
ing, pages 248?263. Springer.
188
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189?195,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Neural Network Based Bilingual Language Model Growing
for Statistical Machine Translation
Rui Wang
1,3,?
, Hai Zhao
1,3
, Bao-Liang Lu
1,3
, Masao Utiyama
2
and Eiichro Sumita
2
1
Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering,
Shanghai Jiao Tong University, Shanghai, 200240, China
2
Multilingual Translation Laboratory, MASTAR Project,
National Institute of Information and Communications Technology,
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
3
Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China
wangrui.nlp@gmail.com, {zhaohai, blu}@cs.sjtu.edu.cn,
{mutiyama, eiichiro.sumita}@nict.go.jp
Abstract
Since larger n-gram Language Model
(LM) usually performs better in Statistical
Machine Translation (SMT), how to con-
struct efficient large LM is an important
topic in SMT. However, most of the ex-
isting LM growing methods need an extra
monolingual corpus, where additional LM
adaption technology is necessary. In this
paper, we propose a novel neural network
based bilingual LM growing method, only
using the bilingual parallel corpus in SMT.
The results show that our method can im-
prove both the perplexity score for LM e-
valuation and BLEU score for SMT, and
significantly outperforms the existing LM
growing methods without extra corpus.
1 Introduction
?Language Model (LM) Growing? refers to adding
n-grams outside the corpus together with their
probabilities into the original LM. This operation
is useful as it can make LM perform better through
letting it become larger and larger, by only using a
small training corpus.
There are various methods for adding n-grams
selected by different criteria from a monolingual
corpus (Ristad and Thomas, 1995; Niesler and
Woodland, 1996; Siu and Ostendorf, 2000; Si-
ivola et al., 2007). However, all of these approach-
es need additional corpora. Meanwhile the extra
corpora from different domains will not result in
better LMs (Clarkson and Robinson, 1997; Iyer et
al., 1997; Bellegarda, 2004; Koehn and Schroeder,
?
Part of this work was done as Rui Wang visited in NICT.
2007). In addition, it is very difficult or even im-
possible to collect an extra large corpus for some
special domains such as the TED corpus (Cettolo
et al., 2012) or for some rare languages. There-
fore, to improve the performance of LMs, without
assistance of extra corpus, is one of important re-
search topics in SMT.
Recently, Continues Space Language Model
(CSLM), especially Neural Network based Lan-
guage Model (NNLM) (Bengio et al., 2003;
Schwenk, 2007; Mikolov et al., 2010; Le et al.,
2011), is being actively used in SMT (Schwenk
et al., 2006; Son et al., 2010; Schwenk, 2010;
Schwenk et al., 2012; Son et al., 2012; Niehues
and Waibel, 2012). One of the main advantages
of CSLM is that it can more accurately predic-
t the probabilities of the n-grams, which are not in
the training corpus. However, in practice, CSLM-
s have not been widely used in the current SMT
systems, due to their too high computational cost.
Vaswani and colleagues (2013) propose a
method for reducing the training cost of CSLM
and apply it to SMT decoder. However, they do
not show their improvement for decoding speed,
and their method is still slower than the n-gram
LM. There are several other methods for attempt-
ing to implement neural network based LM or
translation model for SMT (Devlin et al., 2014;
Liu et al., 2014; Auli et al., 2013). However, the
decoding speed using n-gram LM is still state-of-
the-art one. Some approaches calculate the prob-
abilities of the n-grams n-grams before decoding,
and store them in the n-gram format (Wang et al.,
2013a; Arsoy et al., 2013; Arsoy et al., 2014). The
?converted CSLM? can be directly used in SMT.
Though more n-grams which are not in the train-
189
ing corpus can be generated by using some of
these ?converting? methods, these methods only
consider the monolingual information, and do not
take the bilingual information into account.
We observe that the translation output of a
phrase-based SMT system is concatenation of
phrases from the phrase table, whose probabilities
can be calculated by CSLM. Based on this obser-
vation, a novel neural network based bilingual LM
growing method is proposed using the ?connecting
phrases?. The remainder of this paper is organized
as follows: In Section 2, we will review the exist-
ing CSLM converting methods. The new neural
network based bilingual LM growing method will
be proposed in Section 3. In Section 4, the exper-
iments will be conducted and the results will be
analyzed. We will conclude our work in Section
5.
2 Existing CSLM Converting Methods
Traditional Backoff N -gram LMs (BNLMs) have
been widely used in many NLP tasks (Zhang and
Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013;
Zhang et al., 2012; Xu and Zhao, 2012; Wang et
al., 2013b; Jia and Zhao, 2013; Wang et al., 2014).
Recently, CSLMs become popular because they
can obtain more accurate probability estimation.
2.1 Continues Space Language Model
A CSLM implemented in a multi-layer neural net-
work contains four layers: the input layer projects
(first layer) all words in the context h
i
onto the
projection layer (second layer); the hidden layer
(third layer) and the output layer (fourth layer)
achieve the non-liner probability estimation and
calculate the LM probability P (w
i
|h
i
) for the giv-
en context (Schwenk, 2007).
CSLM is able to calculate the probabilities of
all words in the vocabulary of the corpus given the
context. However, due to too high computational
complexity, CSLM is mainly used to calculate the
probabilities of a subset of the whole vocabulary
(Schwenk, 2007). This subset is called a short-
list, which consists of the most frequent words in
the vocabulary. CSLM also calculates the sum of
the probabilities of all words not included in the
short-list by assigning a neuron with the help of
BNLM. The probabilities of other words not in the
short-list are obtained from an BNLM (Schwenk,
2007; Schwenk, 2010; Wang et al., 2013a).
Let w
i
and h
i
be the current word and history,
respectively. CSLM with a BNLM calculates the
probability P (w
i
|h
i
) of w
i
given h
i
, as follows:
P (w
i
|h
i
) =
?
?
?
P
c
(w
i
|h
i
)
?
w?V
0
P
c
(w|h
i
)
P
s
(h
i
) if w
i
? V
0
P
b
(w
i
|h
i
) otherwise
(1)
where V
0
is the short-list, P
c
(?) is the probabil-
ity calculated by CSLM,
?
w?V
0
P
c
(w|h
i
) is the
summary of probabilities of the neuron for all the
words in the short-list, P
b
(?) is the probability cal-
culated by the BNLM, and
P
s
(h
i
) =
?
v?V
0
P
b
(v|h
i
). (2)
We may regard that CSLM redistributes the
probability mass of all words in the short-list,
which is calculated by using the n-gram LM.
2.2 Existing Converting Methods
As baseline systems, our approach proposed in
(Wang et al., 2013a) only re-writes the probabil-
ities from CSLM into the BNLM, so it can only
conduct a convert LM with the same size as the o-
riginal one. The main difference between our pro-
posed method in this paper and our previous ap-
proach is that n-grams outside the corpus are gen-
erated firstly and the probabilities using CSLM are
calculated by using the same method as our previ-
ous approach. That is, the proposed new method
is the same as our previous one when no grown
n-grams are generated.
The method developed by Arsoy and colleagues
(Arsoy et al., 2013; Arsoy et al., 2014) adds al-
l the words in the short-list after the tail word of
the i-grams to construct the (i+1)-grams. For ex-
ample, if the i-gram is ?I want?, then the (i+1)-
grams will be ?I want *?, where ?*? stands for any
word in the short list. Then the probabilities of
the (i+1)-grams are calculated using (i+1)-CSLM.
So a very large intermediate (i+1)-grams will have
to be grown
1
, and then be pruned into smaller
suitable size using an entropy-based LM pruning
method modified from (Stolcke, 1998). The (i+2)-
grams are grown using (i+1)-grams, recursively.
1
In practice, the probabilities of all the target/tail words
in the short list for the history i-grams can be calculated by
the neurons in the output layer at the same time, which will
save some time. According to our experiments, the time cost
for Arsoy?s growing method is around 4 times more than our
proposed method, if the LMs which are 10 times larger than
the original one are grown with other settings all the same.
190
3 Bilingual LM Growing
The translation output of a phrase-based SMT sys-
tem can be regarded as a concatenation of phrases
in the phrase table (except unknown words). This
leads to the following procedure:
Step 1. All the n-grams included in the phrase
table should be maintained at first.
Step 2. The connecting phrases are defined in
the following way.
The w
b
a
is a target language phrase starting from
the a-th word ending with the b-th word, and ?w
b
a
?
is a phrase includingw
b
a
as a part of it, where ? and
? represent any word sequence or none. An i-gram
phrase w
k
1
w
i
k+1
(1 ? k ? i ? 1) is a connecting
phrase
2
, if :
(1) w
k
1
is the right (rear) part of one phrase ?w
k
1
in the phrase table, or
(2) w
i
k+1
is the left (front) part of one phrase
w
i
k+1
? in the phrase table.
After the probabilities are calculated using C-
SLM (Eqs.1 and 2), we combine the n-grams in
the phrase table from Step 1 and the connecting
phrases from Step 2.
3.1 Ranking the Connecting Phrases
Since the size of connecting phrases is too huge
(usually more than one Terabyte), it is necessary
to decide the usefulness of connecting phrases for
SMT. The more useful connecting phrases can be
selected, by ranking the appearing probabilities of
the connecting phrases in SMT decoding.
Each line of a phrase table can be simplified
(without considering other unrelated scores in the
phrase table) as
f ||| e ||| P (e|f), (3)
where the P (e|f) means the translation probabili-
ty from f(source phrase) to e(target phrase),
which can be calculated using bilingual parallel
training data. In decoding, the probability of a tar-
get phrase e appearing in SMT should be
P
t
(e) =
?
f
P
s
(f) ? P (e|f), (4)
2
We are aware that connecting phrases can be applied to
not only two phrases, but also three or more. However the ap-
pearing probabilities (which will be discussed in Eq. 5 of next
subsection) of connecting phrases are approximately estimat-
ed. To estimate and compare probabilities of longer phrases
in different lengths will lead to serious bias, and the experi-
ments also showed using more than two connecting phrases
did not perform well (not shown for limited space), so only
two connecting phrases are applied in this paper.
where the P
s
(f) means the appearing probability
of a source phrase, which can be calculated using
source language part in the bilingual training data.
Using P
t
(e)
3
, we can select the connecting
phrases e with high appearing probabilities as
the n-grams to be added to the original n-
grams. These n-grams are called ?grown n-
grams?. Namely, we build all the connecting
phrases at first, and then we use the appearing
probabilities of the connecting phrases to decide
which connecting phrases should be selected. For
an i-gram connecting phrasew
k
1
w
i
k+1
, wherew
k
1
is
part of ?w
k
1
and w
i
k+1
is part of w
i
k+1
? (the ?w
k
1
and w
i
k+1
? are from the phrase table), the prob-
ability of the connecting phrases can be roughly
estimated as
P
con
(w
k
1
w
i
k+1
) =
i?1
?
k=1
(
?
?
P
t
(?w
k
1
)?
?
?
P
t
(w
i
k+1
?)).
(5)
A threshold for P
con
(w
k
1
w
i
k+1
) is set, and only
the connecting phrases whose appearing probabil-
ities are higher than the threshold will be selected
as the grown n-grams.
3.2 Calculating the Probabilities of Grown
N -grams Using CSLM
To our bilingual LM growing method, a 5-gram
LM and n-gram (n=2,3,4,5) CSLMs are built by
using the target language of the parallel corpus,
and the phrase table is learned from the parallel
corpus.
The probabilities of unigram in the original n-
gram LM will be maintained as they are. The
n-grams from the bilingual phrase table will be
grown by using the ?connecting phrases? method.
As the whole connecting phrases are too huge, we
use the ranking method to select the more useful
connecting phrases. The distribution of different
n-grams (n=2,3,4,5) of the grown LMs are set as
the same as the original LM.
The probabilities of the grown n-grams
(n=2,3,4,5) are calculated using the 2,3,4,5-
CSLM, respectively. If the tail (target) words of
the grown n-grams are not in the short-list of C-
SLM, the P
b
(?) in Eq. 1 will be applied to calcu-
late their probabilities.
3
This P
t
(e) hence provides more bilingual information,
in comparison with using monolingual target LMs only.
191
We combine the n-grams (n=1,2,3,4,5) togeth-
er and re-normalize the probabilities and backof-
f weights of the grown LM. Finally the original
BNLM and the grown LM are interpolated. The
entire process is illustrated in Figure 1.
Corpus
Phrase Table
Grown n-grams 
with Probabilities
Grown LM
Output
Input
Interpolate
Grown n-grams
CSLM
BNLM
Connecting
Phrases
Figure 1: NN based bilingual LM growing.
4 Experiments and Results
4.1 Experiment Setting up
The same setting up of the NTCIR-9 Chinese to
English translation baseline system (Goto et al.,
2011) was followed, only with various LMs to
compare them. The Moses phrase-based SMT
system was applied (Koehn et al., 2007), togeth-
er with GIZA++ (Och and Ney, 2003) for align-
ment and MERT (Och, 2003) for tuning on the de-
velopment data. Fourteen standard SMT features
were used: five translation model scores, one word
penalty score, seven distortion scores, and one LM
score. The translation performance was measured
by the case-insensitive BLEU on the tokenized test
data.
We used the patent data for the Chinese to En-
glish patent translation subtask from the NTCIR-9
patent translation task (Goto et al., 2011). The par-
allel training, development, and test data sets con-
sist of 1 million (M), 2,000, and 2,000 sentences,
respectively.
Using SRILM (Stolcke, 2002; Stolcke et al.,
2011), we trained a 5-gram LM with the interpo-
lated Kneser-Ney smoothing method using the 1M
English training sentences containing 42M words
without cutoff. The 2,3,4,5-CSLMs were trained
on the same 1M training sentences using CSLM
toolkit (Schwenk, 2007; Schwenk, 2010). The set-
tings for CSLMs were: input layer of the same
dimension as vocabulary size (456K), projection
layer of dimension 256 for each word, hidden lay-
er of dimension 384 and output layer (short-list) of
dimension 8192, which were recommended in the
CSLM toolkit and (Wang et al., 2013a)
4
.
4
Arsoy used around 55 M words as the corpus, including
4.2 Results
The experiment results were divided into four
groups: the original BNLMs (BN), the CSLM
Re-ranking (RE), our previous converting (WA),
the Arsoy?s growing, and our growing methods.
For our bilingual LM growing method, 5 bilingual
grown LMs (BI-1 to 5) were conducted in increas-
ing sizes. For the method of Arsoy, 5 grown LMs
(AR-1 to 5) with similar size of BI-1 to 5 were also
conducted, respectively.
For the CSLM re-ranking, we used CSLM to
re-rank the 100-best lists of SMT. Our previous
converted LM, Arsoy?s grown LMs and bilingual
grown LMs were interpolated with the original
BNLMs, using default setting of SRILM
5
. To re-
duce the randomness of MERT, we used twometh-
ods for tuning the weights of different SMT fea-
tures, and two BLEU scores are corresponding to
these twomethods. TheBLEU-s indicated that the
same weights of the BNLM (BN) features were
used for all the SMT systems. The BLEU-i indi-
cated that the MERT was run independently by
three times and the average BLEU scores were
taken.
We also performed the paired bootstrap re-
sampling test (Koehn, 2004)
6
. Two thousands
samples were sampled for each significance test.
The marks at the right of the BLEU score indicated
whether the LMs were significantly better/worse
than the Arsoy?s grown LMs with the same IDs
for SMT (?++/???: significantly better/worse at
? = 0.01, ?+/??: ? = 0.05, no mark: not signif-
icantly better/worse at ? = 0.05).
From the results shown in Table 1, we can get
the following observations:
(1) Nearly all the bilingual grown LMs outper-
formed both BNLM and our previous converted
LM on PPL and BLEU. As the size of grown LM-
s is increased, the PPL always decreased and the
BLEU scores trended to increase. These indicated
that our proposed method can give better probabil-
ity estimation for LM and better performance for
SMT.
(2) In comparison with the grown LMs in Ar-
84K words as vocabulary, and 20K words as short-list. In this
paper, we used the same setting as our previous work, which
covers 92.89% of the frequency of words in the training cor-
pus, for all the baselines and our method for fair comparison.
5
In our previous work, we used the development data to
tune the weights of interpolation. In this paper, we used the
default 0.5 as the interpolation weights for fair comparison.
6
We used the code available at http://www.ark.cs.
cmu.edu/MT
192
Table 1: Performance of the Grown LMs
LMs n-grams PPL BLEU-s BLEU-i ALH
BN 73.9M 108.8 32.19 32.19 3.03
RE N/A 97.5 32.34 32.42 N/A
WA 73.9M 104.4 32.60 32.62 3.03
AR-1 217.6M 103.3 32.55 32.75 3.14
AR-2 323.8M 103.1 32.61 32.64 3.18
AR-3 458.5M 103.0 32.39 32.71 3.20
AR-4 565.6M 102.8 32.67 32.51 3.21
AR-5 712.2M 102.5 32.49 32.60 3.22
BI-1 223.5M 101.9 32.81+ 33.02+ 3.20
BI-2 343.6M 101.0 32.92+ 33.11++ 3.24
BI-3 464.5M 100.6 33.08++ 33.25++ 3.26
BI-4 571.0M 100.3 33.15++ 33.12++ 3.28
BI-5 705.5M 100.1 33.11++ 33.24++ 3.31
soy?s method, our grown LMs obtained better P-
PL and significantly better BLEU with the sim-
ilar size. Furthermore, the improvement of PPL
and BLEU of the existing methods became satu-
rated much more quickly than ours did, as the LMs
grew.
(3) The last column was the Average Length of
the n-grams Hit (ALH) in SMT decoding for dif-
ferent LMs using the following function
ALH =
5
?
i=1
P
i?gram
? i, (6)
where the P
i?gram
means the ratio of the i-grams
hit in SMT decoding. There were also positive
correlations between ALH, PPL and BLEUs. The
ALH of bilingual grown LM was longer than that
of the Arsoy?s grown LM of the similar size. In
another word, less back-off was used for our pro-
posed grown LMs in SMT decoding.
4.3 Experiments on TED Corpus
The TED corpus is in special domain as discussed
in the introduction, where large extra monolingual
corpora are hard to find. In this subsection, we
conducted the SMT experiments on TED corpora
using our proposed LM growing method, to eval-
uate whether our method was adaptable to some
special domains.
We mainly followed the baselines of the IWSLT
2014 evaluation campaign
7
, only with a few mod-
ifications such as the LM toolkits and n-gram or-
der for constructing LMs. The Chinese (CN) to
English (EN) language pair was chosen, using de-
v2010 as development data and test2010 as evalu-
ation data. The same LM growing method was ap-
7
https://wit3.fbk.eu/
plied on TED corpora as on NTCIR corpora. The
results were shown in Table 2.
Table 2: CN-EN TED Experiments
LMs n-grams PPL BLEU-s
BN 7.8M 87.1 12.41
WA 7.8M 85.3 12.73
BI-1 23.1M 79.2 12.92
BI-2 49.7M 78.3 13.16
BI-3 73.4M 77.6 13.24
Table 2 indicated that our proposed LM grow-
ing method improved both PPL and BLEU in com-
parison with both BNLM and our previous CSLM
converting method, so it was suitable for domain
adaptation, which is one of focuses of the current
SMT research.
5 Conclusion
In this paper, we have proposed a neural network
based bilingual LM growing method by using the
bilingual parallel corpus only for SMT. The results
show that our proposed method can improve both
LM and SMT performance, and outperforms the
existing LM growing methods significantly with-
out extra corpus. The connecting phrase-based
method can also be applied to LM adaptation.
Acknowledgments
We appreciate the helpful discussion with Dr.
Isao Goto and Zhongye Jia, and three anony-
mous reviewers for valuable comments and sug-
gestions on our paper. Rui Wang, Hai Zhao
and Bao-Liang Lu were partially supported by
the National Natural Science Foundation of Chi-
na (No. 60903119, No. 61170114, and No.
61272248), the National Basic Research Program
of China (No. 2013CB329401), the Science and
Technology Commission of Shanghai Municipali-
ty (No. 13511500200), the European Union Sev-
enth Framework Program (No. 247619), the Cai
Yuanpei Program (CSC fund 201304490199 and
201304490171), and the art and science interdis-
cipline funds of Shanghai Jiao Tong University
(A study on mobilization mechanism and alerting
threshold setting for online community, and media
image and psychology evaluation: a computation-
al intelligence approach). The corresponding au-
thor of this paper, according to the meaning given
to this role by Shanghai Jiao Tong University, is
Hai Zhao.
193
References
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2013. Converting neural net-
work language models into back-off language mod-
els for efficient decoding in automatic speech recog-
nition. In Proceedings of ICASSP-2013, Vancouver,
Canada, May. IEEE.
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2014. Converting neural net-
work language models into back-off language mod-
els for efficient decoding in automatic speech recog-
nition. IEEE/ACM Transactions on Audio, Speech,
and Language, 22(1):184?192.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
cessings of EMNLP-2013, pages 1044?1054, Seat-
tle, Washington, USA, October. Association for
Computational Linguistics.
Jerome R Bellegarda. 2004. Statistical language mod-
el adaptation: review and perspectives. Speech
Communication, 42(1):93?108. Adaptation Meth-
ods for Speech Recognition.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search (JMLR), 3:1137?1155, March.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit
3
: Web inventory of transcribed
and translated talks. In Proceedings of EAMT-2012,
pages 261?268, Trento, Italy, May.
Philip Clarkson and A.J. Robinson. 1997. Lan-
guage model adaptation using mixtures and an ex-
ponentially decaying cache. In Proceedings of
ICASSP-1997, volume 2, pages 799?802 vol.2, Mu-
nich,Germany.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of ACL-
2014, pages 1370?1380, Baltimore, Maryland, June.
Association for Computational Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the paten-
t machine translation task at the NTCIR-9 work-
shop. In Proceedings of NTCIR-9 Workshop Meet-
ing, pages 559?578, Tokyo, Japan, December.
Rukmini Iyer, Mari Ostendorf, and Herbert Gish.
1997. Using out-of-domain data to improve in-
domain language models. Signal Processing Letter-
s, IEEE, 4(8):221?223.
Zhongye Jia and Hai Zhao. 2013. Kyss 1.0: a
framework for automatic evaluation of chinese input
method engines. In Proceedings of IJCNLP-2013,
pages 1195?1201, Nagoya, Japan, October. Asian
Federation of Natural Language Processing.
Zhongye Jia and Hai Zhao. 2014. A joint graph mod-
el for pinyin-to-chinese conversion with typo cor-
rection. In Proceedings of ACL-2014, pages 1512?
1523, Baltimore, Maryland, June. Association for
Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of ACL-2007 Workshop
on Statistical Machine Translation, pages 224?227,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL-2007, pages 177?180,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of EMNLP-2004, pages 388?395, Barcelona, Spain,
July. Association for Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gau-
vain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceed-
ings of ICASSP-2011, pages 5524?5527, Prague,
Czech Republic, May. IEEE.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proceedings of ACL-2014,
pages 1491?1500, Baltimore, Maryland, June. As-
sociation for Computational Linguistics.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Re-
current neural network based language model. In
Proceedings of INTERSPEECH-2010, pages 1045?
1048.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using restricted boltzman-
n machines. In Proceedings of IWSLT-2012, pages
311?318, Hong Kong.
Thomas Niesler and Phil Woodland. 1996. A variable-
length category-based n-gram language model. In
Proceedings of ICASSP-1996, volume 1, pages 164?
167 vol. 1.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignmen-
t models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of ACL-2003, pages 160?167, Sapporo, Japan, July.
Association for Computational Linguistics.
194
Eric Sven Ristad and Robert G. Thomas. 1995. New
techniques for context modeling. In Proceedings
of ACL-1995, pages 220?227, Cambridge, Mas-
sachusetts. Association for Computational Linguis-
tics.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of
COLING ACL-2006, pages 723?730, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
HLT, WLM ?12, pages 11?19, Montreal, Canada,
June. Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space lan-
guage models. Computer Speech and Language,
21(3):492?518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. The
Prague Bulletin of Mathematical Linguistics, pages
137?146.
Vesa Siivola, Teemu Hirsimki, and Sami Virpioja.
2007. On growing and pruning kneser-ney s-
moothed n-gram models. IEEE Transactions on Au-
dio, Speech, and Language, 15(5):1617?1624.
Manhung Siu and Mari Ostendorf. 2000. Variable n-
grams and extensions for conversational speech lan-
guage modeling. IEEE Transactions on Speech and
Audio, 8(1):63?75.
Le Hai Son, Alexandre Allauzen, Guillaume Wis-
niewski, and Franc?ois Yvon. 2010. Training con-
tinuous space language models: some practical is-
sues. In Proceedings of EMNLP-2010, pages 778?
788, Cambridge, Massachusetts, October. Associa-
tion for Computational Linguistics.
Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of NAACL HLT-
2012, pages 39?48, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Andreas Stolcke, Jing Zheng, Wen Wang, and Vic-
tor Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings of INTERSPEECH 2011,
Waikoloa, HI, USA, December.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270?274, Lansdowne, VA, USA.
Andreas Stolcke. 2002. Srilm-an extensible
language modeling toolkit. In Proceedings of
INTERSPEECH-2002, pages 257?286, Seattle, US-
A, November.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of EMNLP-2013, pages 1387?1392,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumi-
ta, Hai Zhao, and Bao-Liang Lu. 2013a. Convert-
ing continuous-space language models into n-gram
language models for statistical machine translation.
In Proceedings of EMNLP-2013, pages 845?850,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Xiaolin Wang, Hai Zhao, and Bao-Liang Lu. 2013b.
Labeled alignment for recognizing textual entail-
ment. In Proceedings of IJCNLP-2013, pages 605?
613, Nagoya, Japan, October. Asian Federation of
Natural Language Processing.
Xiao-Lin Wang, Yang-Yang Chen, Hai Zhao, and Bao-
Liang Lu. 2014. Parallelized extreme learning ma-
chine ensemble based on minmax modular network.
Neurocomputing, 128(0):31 ? 41.
Qiongkai Xu and Hai Zhao. 2012. Using deep lin-
guistic features for finding deceptive opinion spam.
In Proceedings of COLING-2012, pages 1341?1350,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Jingyi Zhang and Hai Zhao. 2013. Improving function
word alignment with frequency and syntactic infor-
mation. In Proceedings of IJCAI-2013, pages 2211?
2217. AAAI Press.
Xiaotian Zhang, Hai Zhao, and Cong Hui. 2012.
A machine learning approach to convert CCGbank
to Penn treebank. In Proceedings of COLING-
2012, pages 535?542, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word
segmentation for chinese machine translation. In
Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, volume 7817 of
Lecture Notes in Computer Science, pages 248?263.
Springer Berlin Heidelberg.
195
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1512?1523,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
A Joint Graph Model for Pinyin-to-Chinese Conversion
with Typo Correction?
Zhongye Jia and Hai Zhao?
MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems,
Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
800 Dongchuan Road, Shanghai 200240, China
jia.zhongye@gmail.com, zhaohai@cs.sjtu.edu.cn
Abstract
It is very import for Chinese language pro-
cessing with the aid of an efficient input
method engine (IME), of which pinyin-
to-Chinese (PTC) conversion is the core
part. Meanwhile, though typos are in-
evitable during user pinyin inputting, ex-
isting IMEs paid little attention to such big
inconvenience. In this paper, motivated by
a key equivalence of two decoding algo-
rithms, we propose a joint graph model to
globally optimize PTC and typo correction
for IME. The evaluation results show that
the proposed method outperforms both ex-
isting academic and commercial IMEs.
1 Introduction
1.1 Chinese Input Method
The daily life of Chinese people heavily depends
on Chinese input method engine (IME), no matter
whether one is composing an E-mail, writing an
article, or sending a text message. However, ev-
ery Chinese word inputted into computer or cell-
phone cannot be typed through one-to-one map-
ping of key-to-letter inputting directly, but has to
go through an IME as there are thousands of Chi-
nese characters for inputting while only 26 letter
keys are available in the keyboard. An IME is
an essential software interface that maps Chinese
characters into English letter combinations. An ef-
?This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No.60903119, Grant
No.61170114, and Grant No.61272248), the National Ba-
sic Research Program of China (Grant No.2013CB329401),
the Science and Technology Commission of Shanghai Mu-
nicipality (Grant No.13511500200), and the European Union
Seventh Framework Program (Grant No.247619).
?Corresponding author
ficient IME will largely improve the user experi-
ence of Chinese information processing.
Nowadays most of Chinese IMEs are pinyin
based. Pinyin is originally designed as the pho-
netic symbol of a Chinese character (based on the
standard modern Chinese, mandarin) , using Latin
letters as its syllable notation. For example, the
pinyin of the Chinese character ???(love) is ??i?.
Most characters usually have unique pinyin rep-
resentations, while a few Chinese characters may
be pronounced in several different ways, so they
may have multiple pinyin representations. The ad-
vantage of pinyin IME is that it only adopts the
pronunciation perspective of Chinese characters
so that it is simple and easy to learn. But there
are only less than 500 pinyin syllables in stan-
dard modern Chinese, compared with over 6,000
commonly used Chinese characters, which leads
to serious ambiguities for pinyin-to-charactermap-
ping. Modern pinyin IMEsmostly use a ?sentence-
based? decoding technique (Chen and Lee, 2000)
to alleviate the ambiguities. ?Sentence based?
means that IME generates a sequence of Chinese
characters upon a sequence of pinyin inputs with
respect to certain statistical criteria.
1.2 Typos and Chinese Spell Checking
Written in Chinese characters but not alphabets,
spell checking for Chinese language is quite dif-
ferent from the same task for other languages.
Since Chinese characters are entered via IME,
those user-made typos do not immediately lead to
spelling errors. When a user types a wrong letter,
IME will be very likely to fail to generate the ex-
pected Chinese character sequence. Normally, the
user may immediately notice the inputting error
and then make corrections, which usually means
doing a bunch of extra operations like cursor
1512
movement, deletion and re-typing. Thus there are
two separated sub-tasks for Chinese spell check-
ing: 1. typo checking for user typed pinyin se-
quences which should be a built-in module in
IME, and 2. spell checking for Chinese texts in
its narrow sense, which is typically a module of
word processing applications (Yang et al, 2012b).
These two terms are often confused especially in
IME related works such as (Chen and Lee, 2000)
and (Wu et al, 2009).
Pinyin typos have always been a serious prob-
lem for Chinese pinyin IMEs. The user may fail
to input the completely right pinyin simply be-
cause he/she is a dialect speaker and does not know
the exact pronunciation for the expected character.
This may be a very common situation since there
are about seven quite different dialects in Chinese,
among which being spoken languages, six are far
different from the standard modern Chinese, man-
darin. With the boom of smart-phones, pinyin ty-
pos worsen due to the limited size of soft key-
board, and the lack of physical feedback on the
touch screen. However, existing practical IMEs
only provide small patches to deal with typos such
as Fuzzy Pinyin (Wu and Chen, 2004) and other
language specific errors (Zheng et al, 2011b).
Typo checking and correction has an important
impact on IME performance. When IME fails to
correct a typo and generate the expected sentence,
the user will have to takemuch extra effort to move
the cursor back to the mistyped letter and correct it,
which leads to very poor user experience (Jia and
Zhao, 2013).
2 Related Works
The very first approach for Chinese input with
typo correction was made by (Chen and Lee,
2000), which was also the initial attempt of
?sentence-based? IME. The idea of ?statistical in-
put method? was proposed by modeling PTC con-
version as a hidden Markov model (HMM), and
using Viterbi (Viterbi, 1967) algorithm to decode
the sequence. They solved the typo correction
problem by decomposing the conditional proba-
bility P (H|P ) of Chinese character sequence H
given pinyin sequence P into a language model
P (w
i
|w
i?1
) and a typing model P (p
i
|w
i
). The
typing model that was estimated on real user input
data was for typo correction. However, real user
input data can be very noisy and not very conve-
nient to obtain. As we will propose a joint model
in this paper, such an individual typing model is
not necessarily built in our approach.
(Zheng et al, 2011a) developed an IME sys-
tem with typo correction called CHIME using
noisy channel error model and language-specific
features. However their model depended on a
very strong assumption that input pinyin sequence
should have been segmented into pinyin words by
the user. This assumption does not really hold in
modern ?sentence-based? IMEs. We release this
assumption since our model solves segmentation,
typo correction and PTC conversion jointly.
Besides the common HMM approach for PTC
conversion, there are also variousmethods such as:
support vector machine (Jiang et al, 2007), max-
imum entropy (ME) model (Wang et al, 2006),
conditional random field (CRF) (Li et al, 2009)
and statistical machine translation (SMT) (Yang et
al., 2012a; Wang et al, 2013c; Zhang and Zhao,
2013), etc.
Spell checking or typo checking was first pro-
posed for English (Peterson, 1980). (Mays et al,
1991) addressed that spell checking should be done
within a context, i.e., a sentence or a long phrase
with a certain meaning, instead of only in one
word. A recent spell correction work is (Li et al,
2006), where a distributional similarity was intro-
duced for spell correction of web queries.
Early attempts for Chinese spelling checking
could date back to (Chang, 1994) where charac-
ter tables for similar shape, pronunciation, mean-
ing, and input-method-code characters were pro-
posed. More recently, the 7th SIGHANWorkshop
on Chinese Language Processing (Yu et al, 2013)
held a shared task on Chinese spell checking. Var-
ious approaches were made for the task includ-
ing language model (LM) based methods (Chen
et al, 2013), ME model (Han and Chang, 2013),
CRF (Wang et al, 2013d; Wang et al, 2013a),
SMT (Chiu et al, 2013; Liu et al, 2013), and graph
model (Jia et al, 2013), etc.
3 Pinyin Input Method Model
3.1 From English Letter to Chinese Sentence
It is a rather long journey from the first English
letter typed on the keyboard to finally a completed
Chinese sentence generated by IME. We will first
take an overview of the entire process.
The average length of pinyin syllables is about 3
letters. There are about 410 pinyin syllables used
in the current pinyin system. Each pinyin sylla-
1513
ble has a bunch of corresponding Chinese char-
acters which share the same pronunciation repre-
sented by the syllable. The number of those homo-
phones ranges from 1 to over 300. Chinese char-
acters then form words. But word in Chinese is
a rather vague concept. Without word delimiters,
linguists have argued on what a Chinese word re-
ally is for a long time and that is why there is al-
ways a primary word segmentation treatment in
most Chinese language processing tasks (Zhao et
al., 2006; Huang and Zhao, 2007; Zhao and Kit,
2008; Zhao et al, 2010; Zhao and Kit, 2011; Zhao
et al, 2013). A Chinese word may contain from
1 to over 10 characters due to different word seg-
mentation conventions. Figure 1 demonstrates the
relationship of pinyin andword, from pinyin letters
?nihao? to the word ??? (hello)?. Typically, an
IME takes the pinyin input, segments it into sylla-
bles, looks up corresponding words in a dictionary
and generates a sentence with the candidate words.
nihao ??
ni hao ?? Pinyin syllablesChinese characters
Pinyin word
Chinese word
n i h a o Pinyin characters
Figure 1: Relationship of pinyin and words
3.2 Pinyin Segmentation and Typo
Correction
Non-Chinese users may feel confused or even
surprised if they know that when typing pinyin
through an IME, Chinese IME users will never en-
ter delimiters such as ?Space? key to segment ei-
ther pinyin syllables or pinyin words, but just in-
put the entire un-segmented pinyin sequence. For
example, if one wants to input ????? (Hello
world)?, he will just type ?nihaoshijie? instead of
segmented pinyin sequence ?ni hao shi jie?. Nev-
ertheless, pinyin syllable segmentation is a much
easier problem compared to Chinese word seg-
mentation. Since pinyin syllables have a very lim-
ited vocabulary and follow a set of regularities
strictly, it is convenient to perform pinyin sylla-
ble segmentation by using rules. But as the pinyin
input is not segmented, it is nearly impossible to
adopt previous spell checking methods for English
to pinyin typo checking, although techniques for
English spell checking have been well developed.
A bit confusing but interesting, pinyin typo cor-
rection and segmentation come as two sides of one
problem: when a pinyin sequence is mistyped, it
is unlikely to be correctly segmented; when it is
segmented in an awkward way, it is likely to be
mistyped.
Inspired by (Yang et al, 2012b) and (Jia et al,
2013), we adopt the graph model for Chinese spell
checking for pinyin segmentation and typo correc-
tion, which is based on the shortest path word seg-
mentation algorithm (Casey and Lecolinet, 1996).
The model has two major steps: segmentation and
correction.
3.2.1 Pinyin Segmentation
The shortest path segmentation algorithm is based
on the idea that a reasonable segmentation should
minimize the number of segmented units. For a
pinyin sequence p
1
p
2
. . . p
L
, where p
i
is a letter,
first a directed acyclic graph (DAG) G
S
= (V,E)
is built for pinyin segmentation step. The vertex
set V consists of the following parts:
? Virtual start vertex S
0
and end vertex S
E
;
? Possible legal syllables fetched from dictio-
nary D
p
according to the input pinyin se-
quence:
{S
i,j
|S
i,j
= p
i
. . . p
j
? D
p
};
? The letter itself as a fallback no matter if it is
a legal pinyin syllable or not:
{S
i
|S
i
= p
i
}.
The vertex weights w
S
are all set to 0. The edges
are from a syllable to all syllables next to it:
E = {E(S
i,j
? S
j+1,k
)|S
i,j
, S
j+1,k
? V}.
The edge weight the negative logarithm of con-
ditional probability P (S
j+1,k
|S
i,j
) that a syllable
S
i,j
is followed by S
j+1,k
, which is give by a bi-
gram language model of pinyin syllables:
W
E(S
i,j
?S
j+1,k
)
= ? logP (S
j+1,k
|S
i,j
)
The shortest path P ? on the graph is the path P
with the least sum of weights:
P
?
= argmin
(v,E)?G?(v,E)?P
?
v
w
v
+
?
E
W
E
.
1514
Computing the shortest path from S
0
to S
E
on
G
S
yields the best segmentation. This is the sin-
gle source shortest path (SSSP) problem on DAG
which has an efficient algorithm by preprocessing
the DAG with topology sort, then traversing ver-
tices and edges in topological order. It has the time
complexity of O(|V|+ |E|). For example, one in-
tends to input ????? (Hello world)? by typ-
ing ?nihaoshijie?, but mistyped as ?mihaoshijiw?.
The graph for this input is shown in Figure 2. The
shortest path, i.e., the best segmentation is ?mi hao
shi ji w?. We will continue to use this example in
the rest of this paper.
m i h a o s h i j i w
ha
hao
ao
shi
j i
m i
Figure 2: Graph model for pinyin segmentation
3.2.2 Pinyin Typo Correction
Next in the correction step, for the segmented
pinyin sequence S
1
, S
2
, . . . , S
M
, a graph G
c
is
constructed to perform typo correction. The ver-
tex set V consists of the following parts:
? Virtual start vertex S?
0
and end vertex S?
E
with
vertex weights of 0;
? All possible syllables similar to original syl-
lable in G
s
. If the adjacent syllables can be
merged into a legal syllable, the merged syl-
lable is also added into V:
{S
?
i,j
|S
?
i,j
= S
?
i
. . . S
?
j
? D
p
,
S
?
k
? S
k
, k = i ? j},
where the similarity ? is measured in Lev-
enshtein distance (Levenshtein, 1966). Sylla-
bles with Levenshtein distance under a certain
threshold are considered as similar:
L(S
i
, S
j
) < T ? S
i
? S
j
.
The vertex weight is the Levenshtein distance
multiply by a normalization parameter:
w
S
?
i,j
= ?
j
?
k?i
L(S
?
k
, S
k
).
Similar toG
s
, the edges are from one syllable to all
syllables next to it and edge weights are the condi-
tional probabilities between them. Computing the
shortest path from S?
0
to S?
E
on G
c
yields the best
typo correction result. In addition, the result has
been segmented so far. Considering our running
example, the graph G
c
is shown in Figure 3, and
the typo correction result is ?mi hao shi jie?.
whao shi j i
j ie
m i
ti
ni
m a
hai
hu o
p ao
shu
sai
z hi
j ia
. . .
. . .
. . .
. . . . . .
. . . . . .
. . .
a
e
. . .
shu ai
j u
Figure 3: Graph model for pinyin typo correction
Merely using the above model, the typo cor-
rection result is not satisfying yet, no matter how
much effort is paid. The major reason is that the
basic semantic unit of Chinese language is actu-
ally word (tough vaguely defined) which is usu-
ally composed of several characters. Thus the con-
ditional probability between characters does not
make much sense. In addition, a pinyin syllable
usually maps to dozens or even hundreds of cor-
responding homophonic characters, which makes
the conditional probability between syllablesmuch
more noisy. However, using pinyin words instead
of syllables is not a wise choice because pinyin
word segmentation is not so easy a task as syllable
segmentation. To make typo correction better, we
consider to integrate it with PTC conversion using
a joint model.
3.3 Hidden Markov Model for
Pinyin-to-Chinese Conversion
PTC conversion has long been viewed as a decod-
ing problem using HMM. We continue to follow
this formalization. The best Chinese character se-
quenceW ? for a given pinyin syllable sequence S
is the one with the highest conditional probability
P (W |S) that
W
?
= argmax
W
P (W |S)
= argmax
W
P (W )P (S|W )
P (S)
= argmax
W
P (W )P (S|W )
= argmax
w
1
,w
w
,...,w
M
?
w
i
P (w
i
|w
i?1
)
?
w
i
P (s
i
|w
i
)
1515
In the HMM for pinyin IME, observation states are
pinyin syllables, hidden states are Chinese words,
emission probability is P (s
i
|w
i
), and transition
probability is P (w
i
|w
i?1
). Note the transition
probability is the conditional probability between
words instead of characters. PTC conversion is to
decode the Chinese word sequence from the pinyin
sequence. The Viterbi algorithm (Viterbi, 1967) is
used for the decoding.
The shortest path algorithm for typo correction
and Viterbi algorithm for PTC conversion are very
closely related. It has been strictly proven by (For-
ney, 1973) that the sequence decoding problem on
HMM is formally identical to finding a shortest
path on a certain graph, which can be constructed
in the following manner.
Consider a first order HMM with all possi-
ble observations O = {o
1
, o
2
, . . . , o
M
}, hidden
states S = {s
1
, s
2
, . . . , s
N
}, a special start state
s
0
, emission probabilities (E
s
i
,o
k
) = P (o
k
|s
i
),
transition probabilities (T
s
i
,s
j
) = P (s
j
|s
i
), and
start probabilities (S
s
i
) = P (s
i
|s
0
). For an
observation sequence of T time periods Y =
{y
1
, y
2
, . . . , y
T
|y
t
? O, t = 1, . . . , T}, the de-
coding problem is to find the best corresponding
hidden state sequence X? with the highest proba-
bility, i.e.,
X
?
= argmax
x
1
,x
t
?S
S
x
1
E
x
1
,y
1
T
?
t=2
E
x
t
,y
t
T
x
t?1
,x
t
. (1)
Thenwewill construct a DAGG = (V,E) upon
the HMM. The vertex set V includes:
? Virtual start vertex v
0
and end vertex v
E
with
vertex weight of 0;
? Normal vertices v
x
t
, where t = 1, . . . , T , and
?x
t
? S. The vertex weight is the negative
logarithm of emission probability:
w
v
x
t
= ? log E
x
t
,y
t
.
The edge set E includes:
? Edges from the start vertexE(v
0
? v
x
1
)with
edge weight
W
E(v
0
?v
x
1
)
= ? logS
x
1
,
where ?x
1
? S;
? Edges to the end vertex E(v
x
T
? v
E
) with
vertex weights of 0;
? Edges between adjacent time periods
E(v
x
t?1
? v
x
t
) with edge weight
W
E(v
x
t?1
?v
x
t
)
= ? log T
x
t?1
,x
t
,
where t = 2, . . . , T , and ?x
t
, x
t?1
? S.
The shortest path P ? from v
0
to v
E
is the one with
the least sum of vertex and edge weights, i.e.,
P
?
= argmin
v
x
t
?V
T
?
t=1
(w
v
x
t
+ W
E(v
x
t?1
?v
x
t
)
)
= argmin
v
x
1
,v
x
t
?V
{? logS
x
1
? log E
x
1
,y
1
+
T
?
t=2
(? log E
x
t
,y
t
? log T
x
t?1
,x
t
)}
= argmax
v
x
1
,v
x
t
?V
S
x
1
E
x
1
,y
1
T
?
t=2
E
x
t
,y
t
T
x
t?1
,x
t
. (2)
The optimization goal of P ? in Equation (2) is
identical to that of X? in Equation (1).
3.4 Joint Graph Model For Pinyin IME
Given HMM decoding problem is identical to
SSSP problem on DAG, we propose a joint graph
model for PTC conversion with typo correction.
The joint graph model aims to find the global op-
timal for both PTC conversion and typo correction
on the entire input pinyin sequence. The graph
G = (V,E) is constructed based on graph G
c
for
typo correction in Section 3.2. The vertex set V
consists of the following parts:
? Virtual start vertex V
0
and end vertex V
E
with
vertex weight of 0;
? Adjacent pinyin syllables in G
c
are merged
into pinyin words. Corresponding Chinese
words are fetched from a PTC dictionary D
c
,
which is a dictionary maps pinyin words to
Chinese words, and added as vertices:
{V
i,j
|?V
i,j
? D
c
[S
?
i
. . . S
?
j
], i ? j};
The vertex weight consists of two parts: 1.
the vertex weights of syllables in G
c
, and 2.
the emission probability:
w
V
i,j
=?
j
?
k=i
L(S
?
k
, S
k
)
? ? logP (S?
i
. . . S
?
j
|V
i,j
);
1516
If the corresponding pinyin syllables inG
c
have an
edge between them, the vertices in G also have an
edge:
E = {E(V
i,j
? V
j+1,k
)|E(S
i,j
? S
j+1,k
) ? G
c
}.
The edge weights are the negative logarithm of the
transition probabilities:
W
E(V
i,j
?V
j+1,k
)
= ? logP (V
j+1,k
|V
i,j
)
Although the model is formulated on first order
HMM, i.e., the LM used for transition probabil-
ity is a bigram one, it is easy to extend the model
to take advantage of higher order n-gram LM, by
tracking longer history while traversing the graph.
Computing the shortest path from V
0
to V
E
onG
yields the best pinyin-to-Chinese conversion with
typo correction result. Considering our running
example, the graph G is shown in Figure 4.
ni?hao
whao shi j i
j iem i
ti
ni
m a hai
hu o
p ao
shu
sai
z hi
j ia
a
eshu ai
j u
m i?hu o
??...
z hi' j i
shi' j ie
??? ...
??? ...
?? ...
?? ...
?? ...
?? ...?? ?
...
?? ...
??? ...
??? ...
?? ...
?? ...?? ...
?? ...
??? ...
??? ...
??... ? .?..
??? ...
?? ...
?? ...
??...
Figure 4: Joint graph model
The joint graph is rather huge and density. Ac-
cording to our empirical statistics, when setting
threshold T = 2, for a sentence of M characters,
the joint graph will have |V| = M ? 1, 000, and
|E| = M ? 1, 000, 000.
3.5 K-Shortest Paths
To reduce the scale of graphG, we filter graphG
c
by searching itsK-shortest paths first to getG?
c
and
construct G on top of G?
c
. Figure 5 shows the 3-
shortest paths filtered graphG?
c
and Figure 6 shows
the correspondingG for our running example. The
scale of graph may be thus drastically reduced.
hao shi
j i
j ie
m i
ni
hu o z hi a
Figure 5: K-shortest paths in typo correction
An efficient heap data structure is required in
K-shortest paths algorithm (Eppstein, 1998) for
hao shi
j i
j ie
m i
ni
hu o z hi a
ni?hao
m i?hu o z hi' j i
shi' j ie
?? ...
??...
? .?..
?? ...
??? ...
?? ...
?? ...??? ...
??...
??? ...
?? ...
??...
??? ...
Figure 6: Filtered graph model
backtracking the best paths to current vertex while
traversing. The heap is implemented as a priority
queue of size K sorted according to path length
that should support efficient push and pop opera-
tions. Fibonacci heap (Fredman and Tarjan, 1987)
is adopted for the heap implementation since it has
a push complexity ofO(1) which is better than the
O(K) for other heap structures.
Another benefit provided by K-shortest paths
is that it can be used for generating N -best can-
didates of PTC conversion, which may be helpful
for further performance improvement.
4 Experiments
4.1 Corpora, Tools and Experiment Settings
The corpus for evaluation is the one provided
in (Yang et al, 2012a), which is originally ex-
tracted from the People?s Daily corpus and labeled
with pinyin. The corpus has already been split into
training T????, development D?? and test T???
sets as shown in Table 1.
T???? D?? T???
#Sentence 1M 2K 100K
#character 43,679,593 83,765 4,123,184
Table 1: Data set size
SRILM (Stolcke, 2002) is adopted for lan-
guagemodel training andKenLM (Heafield, 2011;
Heafield et al, 2013) for language model query.
The Chinese part of the corpus is segmented into
words before LM training. Maximum match-
ing word segmentation is used with a large word
vocabulary V extracted from web data provided
by (Wang et al, 2013b). The pinyin part is seg-
mented according to the Chinese part. This vo-
cabulary V also serves as the PTC dictionary. The
original vocabulary is not labeled with pinyin, thus
we use the PTC dictionary of sunpinyin1 which is
an open source Chinese pinyin IME, to label the
1http://code.google.com/p/sunpinyin/
1517
vocabulary V with pinyin. The emission proba-
bilities are estimated using the lexical translation
module of MOSES (Koehn et al, 2007) as ?trans-
lation probability? from pinyin to Chinese.
4.2 Evaluation Metrics
Wewill use conventional sequence labeling evalu-
ation metrics such as sequence accuracy and char-
acter accuracy2.
Chinese characters in a sentence may be sepa-
rated by digits, punctuation and alphabets which
are directly inputted without the IME. We fol-
low the so-called term Max Input Unit (MIU), the
longest consecutive Chinese character sequence
proposed by (Jia and Zhao, 2013). We will mainly
consider MIU accuracy (MIU-Acc) which is the
ratio of the number of completely corrected gen-
erated MIUs over the number of all MIUs, and
character accuracy (Ch-Acc), but the sentence ac-
curacy (S-Acc) will also be reported in evaluation
results.
We will also report the conversion error
rate (ConvER) proposed by (Zheng et al, 2011a),
which is the ratio of the number of mistyped pinyin
word that is not converted to the right Chinese
word over the total number of mistyped pinyin
words3.
4.3 Baseline System without Typo Correction
Firstly we build a baseline system without typo
correction which is a pipeline of pinyin syllable
segmentation and PTC conversion. The baseline
system takes a pinyin input sequence, segments it
into syllables, and then converts it to Chinese char-
acter sequence.
The pinyin syllable segmentation already has
very high (over 98%) accuracy with a trigram LM
using improved Kneser-Ney smoothing. Accord-
ing to our empirical observation, emission prob-
abilities are mostly 1 since most Chinese words
have unique pronunciation. So in this step we set
? = 0. We consider different LM smoothing
methods including Kneser-Ney (KN), improved
Kneser-Ney (IKN), and Witten-Bell (WB). All of
the three smoothing methods for bigram and tri-
gram LMs are examined both using back-off mod-
2We only work on the PTC conversion part of IME, thus
we are unable to use existing evaluation systems (Jia and
Zhao, 2013) for full Chinese IME functions.
3Other evaluation metrics are also proposed by (Zheng et
al., 2011a) which is only suitable for their system since our
system uses a joint model
els and interpolated models. The number of N -
best candidates for PTC conversion is set to 10.
The results on D?? are shown in Figure 7 in which
the ?-i? suffix indicates using interpolated model.
According to the results, we then choose the tri-
gram LM using Kneser-Ney smoothing with inter-
polation.
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
 0.74
KN KN-i IKN IKN-i WB WB-i
 0.944
 0.946
 0.948
 0.95
 0.952
 0.954
 0.956
 0.958
 0.96
 0.962
 0.964
MIU
-Acc Ch-A
cc
MIU-Acc-bigramCh-Acc-bigramMIU-Acc-trigramCh-Acc-trigram
Figure 7: MIU-Acc and Ch-Acc with different LM
smoothing
The choice of the number of N -best candidates
for PTC conversion also has a strong impact on the
results. Figure 8 shows the results onD??with dif-
ferentNs, of which theN axis is drawn in logarith-
mic scale. We can observe that MIU-Acc slightly
decreases whileN goes up, but Ch-Acc largely in-
creases. We therefore chooseN = 10 as trade-off.
 0.7265
 0.727
 0.7275
 0.728
 0.7285
 0.729
 0.7295
 0.73
 0.7305
 0.731
 0.7315
 0.732
 1  10  100  1000
 0.935
 0.94
 0.945
 0.95
 0.955
 0.96
 0.965
 0.97
 0.975
 0.98
 0.985
MIU
-Acc Ch-A
cc
MIU-AccCh-Acc
Figure 8: MIU-Acc and Ch-Acc with differentNs
The parameter ? determines emission probabil-
ity. Results with different ? on D?? is shown in
Figure 9, of which the ? axis is drawn in logarith-
mic scale. ? = 0.03 is chosen at last.
We compare our baseline system with several
practical pinyin IMEs including sunpinyin and
Google Input Tools (Online version)4. The results
on D?? are shown in Table 2.
4http://www.google.com/inputtools/try/
1518
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.001  0.01  0.1  1
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
MIU
-Acc Ch-A
cc
MIU-AccCh-Acc
Figure 9: MIU-Acc and Ch-Acc with different ?
MIU-Acc Ch-Acc S-Acc
Baseline 73.39 96.24 38.00
sunpinyin 52.37 87.51 13.95
Google 74.74 94.81 40.2
Yang-ME - 93.3 30.2
Yang-MT - 95.5 45.4
Table 2: Baseline system compared to other
IMEs (%)
4.4 PTC Conversion with Typo Correction
Based upon the baseline system, we build the joint
system of PTC conversion with typo correction.
We simulate user typos by randomly generating
errors automatically on the corpus. The typo rate
is set according to previous Human-Computer In-
teraction (HCI) studies. Due to few works have
been done on modeling Chinese text entry, we
have to refer to those corresponding results on
English (Wobbrock and Myers, 2006; MacKen-
zie and Soukoreff, 2002; Clarkson et al, 2005),
which show that the average typo rate is about 2%.
(Zheng et al, 2011a) performed an experiment that
2,000 sentences of 11,968 Chinese words were en-
tered by 5 native speakers. The collected data con-
sists of 775 mistyped pinyin words caused by one
edit operation, and 85 caused by two edit opera-
tions. As we observe on T???? that the average
pinyin word length is 5.24, then typo rate in the
experiment of (Zheng et al, 2011a) can be roughly
estimated as:
775 + 85? 2
11968? 5.24
= 1.51%,
which is similar to the conclusion on English. Thus
we generate corpora from D?? with typo rate of
0% (0-P), 2% (2-P), and 5% (5-P) to evaluate the
system.
According to (Zheng et al, 2011a) most
mistyped pinyin words are caused by one edit op-
eration. Since pinyin syllable is much shorter than
pinyin word, this ratio can be higher for pinyin
syllables. From our statistics on T????, with 2%
randomly generated typos, Pr(L(S?, S) < 2) =
99.86%. Thus we set the threshold T for L to 2.
We first set K-shortest paths filter to K = 10
and tune ?. Results with different ? are shown
in Figure 10. With ? = 3.5, we select K. Re-
 0.7
 0.705
 0.71
 0.715
 0.72
 0.725
 0.73
 0.735
 0.74
 0.745
 2  2.5  3  3.5  4  4.5  5
 0.954
 0.956
 0.958
 0.96
 0.962
 0.964
 0.966
 0.968
 0.97
MIU
-Acc Ch-A
ccMIU-AccCh-Acc
(a) 0-P
 0.665
 0.67
 0.675
 0.68
 0.685
 0.69
 0.695
 2  2.5  3  3.5  4  4.5  5
 0.932
 0.934
 0.936
 0.938
 0.94
 0.942
 0.944
 0.946
MIU
-Acc Ch-A
cc
MIU-AccCh-Acc
(b) 2-P
 0.585
 0.59
 0.595
 0.6
 0.605
 0.61
 0.615
 2  2.5  3  3.5  4  4.5  5
 0.872
 0.874
 0.876
 0.878
 0.88
 0.882
 0.884
 0.886
 0.888
MIU
-Acc Ch-A
cc
MIU-AccCh-Acc
(c) 5-P
Figure 10: MIU-Acc and Ch-Acc with different ?
sults with differentK are shown in Figure 11. We
choose K = 20 since there is no significant im-
provement when K > 20.
The selection of K also directly guarantees the
running time of the joint model. With K = 20,
on a normal PC with Intel Pentium Dual-Core
E6700 CPU, the PTC conversion rate is over 2000
characters-per-minute (cpm), which is much faster
than the normal typing rate of 200 cpm.
With all parameters optimized, results on T???
1519
 0.705
 0.71
 0.715
 0.72
 0.725
 0.73
 0.735
 0.74
 0.745
 0  10  20  30  40  50  60  70  80  90  100 0.954
 0.956
 0.958
 0.96
 0.962
 0.964
 0.966
 0.968
MIU
-Acc Ch-A
ccMIU-AccCh-Acc
(a) 0-P
 0.655
 0.66
 0.665
 0.67
 0.675
 0.68
 0.685
 0.69
 0.695
 0.7
 0  10  20  30  40  50  60  70  80  90  100 0.92
 0.925
 0.93
 0.935
 0.94
 0.945
MIU
-Acc Ch-A
ccMIU-AccCh-Acc
(b) 2-P
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0  10  20  30  40  50  60  70  80  90  100 0.85
 0.855
 0.86
 0.865
 0.87
 0.875
 0.88
 0.885
 0.89
MIU
-Acc Ch-A
ccMIU-AccCh-Acc
(c) 5-P
Figure 11: MIU-Acc and Ch-Acc with differentK
using the proposed joint model are shown in Ta-
ble 3 and Table 4. Our results are compared to
the baseline system without typo correction and
Google Input Tool. Since sunpinyin does not have
typo correction module and performs much poorer
than our baseline system, we do not include it in
the comparison. Though no direct proofs can be
found to indicate if Google Input Tool has an in-
dependent typo correction component, its outputs
show that such a component is unlikely available.
Since Google Input Tool has to be accessed
through a web interface and the network connec-
tion cannot be guaranteed. we only take a subset
of 10K sentences of T??? to perform the experi-
ments, and the results are shown in Table 3.
The scores reported in (Zheng et al, 2011a) are
not listed in Table 4 since the data set is differ-
ent. They reported a ConvER of 53.56%, which is
given here for reference.
Additionally, to further inspect the robustness of
ourmodel, performance with typo rate ranges from
0% to 5% is shown in Figure 12. Although the per-
formance decreases while typo rate goes up, it is
still quite satisfying around typo rate of 2% which
is assumed to be the real world situation.
MIU-Acc Ch-Acc S-Acc ConvER
Baseline 0-P 79.90 97.47 48.87 -
Baseline 2-P 50.47 90.53 11.12 99.95
Baseline 5-P 30.26 82.83 3.32 99.99
Google 0-P 79.08 95.26 46.83 -
Google 2-P 49.47 61.50 11.08 91.70
Google 5-P 29.18 36.20 3.29 94.64
Joint 0-P 79.90 97.52 49.27 -
Joint 2-P 75.55 95.40 40.69 18.45
Joint 5-P 67.76 90.17 27.86 24.68
Table 3: Test results on 10K sentences from T???
(%)
MIU-Acc Ch-Acc S-Acc ConvER
Baseline 0-P 74.46 96.42 40.50 -
Baseline 2-P 47.25 89.50 9.62 99.95
Baseline 5-P 28.28 81.74 2.63 99.98
Joint 2-P 74.22 96.39 40.34 -
Joint 2-P 69.91 94.14 33.11 21.35
Joint 5-P 62.14 88.49 22.62 27.79
Table 4: Test results on T??? (%)
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 0  1  2  3  4  5
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
MIU
-Acc Ch-A
cc
MIU-AccCh-Acc
Figure 12: MIU-Acc and Ch-Acc with different
typo rate (%)
5 Conclusion
In this paper, we have developed a joint graph
model for pinyin-to-Chinese conversion with typo
correction. This model finds a joint global opti-
mal for typo correction and PTC conversion on the
entire input pinyin sequence. The evaluation re-
sults show that our model outperforms both pre-
vious academic systems and existing commercial
products. In addition, the joint model is efficient
enough for practical use.
1520
References
Richard G. Casey and Eric Lecolinet. 1996. A Sur-
vey of Methods and Strategies in Character Segmen-
tation. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 18(7):690?706.
Chao-Huang Chang. 1994. A Pilot Study on Auto-
matic Chinese Spelling Error Correction. Journal of
Chinese Language and Computing, 4:143?149.
Zheng Chen and Kai-Fu Lee. 2000. A New Statis-
tical Approach To Chinese Pinyin Input. In Pro-
ceedings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 241?247,
Hong Kong, October.
Kuan-YuChen, Hung-Shin Lee, Chung-HanLee, Hsin-
Min Wang, and Hsin-Hsi Chen. 2013. A Study
of Language Modeling for Chinese Spelling Check.
In Proceedings of the Seventh SIGHAN Workshop
on Chinese Language Processing, pages 79?83,
Nagoya, Japan, October. Asian Federation of Nat-
ural Language Processing.
Hsun-wen Chiu, Jian-cheng Wu, and Jason S. Chang.
2013. Chinese Spelling Checker Based on Statis-
tical Machine Translation. In Proceedings of the
Seventh SIGHAN Workshop on Chinese Language
Processing, pages 49?53, Nagoya, Japan, October.
Asian Federation of Natural Language Processing.
Edward Clarkson, James Clawson, Kent Lyons, and
Thad Starner. 2005. An Empirical Study of Typ-
ing Rates onmini-QWERTYKeyboards. InCHI ?05
Extended Abstracts onHuman Factors in Computing
Systems, CHI EA ?05, pages 1288?1291, New York,
NY, USA. ACM.
David Eppstein. 1998. Finding the K Shortest Paths.
SIAM Journal on computing, 28(2):652?673.
Jr G. David Forney. 1973. The Viterbi Algorithm.
Proceedings of the IEEE, 61(3):268?278.
Michael L. Fredman and Robert Endre Tarjan. 1987.
Fibonacci Heaps and Their Uses in Improved Net-
work Optimization Algorithms. Journal of the ACM
(JACM), 34(3):596?615, July.
Dongxu Han and Baobao Chang. 2013. A Maxi-
mum Entropy Approach to Chinese Spelling Check.
In Proceedings of the Seventh SIGHAN Workshop
on Chinese Language Processing, pages 74?78,
Nagoya, Japan, October. Asian Federation of Nat-
ural Language Processing.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Changning Huang and Hai Zhao. 2007. Chinese Word
Segmentation: A Decade Review. Journal of Chi-
nese Information Processing, 21(3):8?20.
Zhongye Jia and Hai Zhao. 2013. KySS 1.0: a
Framework for Automatic Evaluation of Chinese In-
put Method Engines. In Proceedings of the Sixth In-
ternational Joint Conference on Natural Language
Processing, pages 1195?1201, Nagoya, Japan, Octo-
ber. Asian Federation of Natural Language Process-
ing.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013. Graph
Model for Chinese Spell Checking. In Proceedings
of the Seventh SIGHAN Workshop on Chinese Lan-
guage Processing, pages 88?92, Nagoya, Japan, Oc-
tober. Asian Federation of Natural Language Pro-
cessing.
Wei Jiang, Yi Guan, Xiaolong Wang, and BingQuan
Liu. 2007. PinYin-to-Character Conversion Model
based on Support Vector Machines. Journal of Chi-
nese information processing, 21(2):100?105.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Vladimir I. Levenshtein. 1966. Binary Codes Capable
of Correcting Deletions, Insertions and Reversals. In
Soviet physics doklady, volume 10, page 707.
Mu Li, Muhua Zhu, Yang Zhang, and Ming Zhou.
2006. Exploring Distributional Similarity Based
Models for Query Spelling Correction. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
1025?1032, Sydney, Australia, July. Association for
Computational Linguistics.
Lu Li, Xuan Wang, Xiao-Long Wang, and Yan-Bing
Yu. 2009. A Conditional Random Fields Approach
to Chinese Pinyin-to-Character Conversion. Journal
of Communication and Computer, 6(4):25?31.
Xiaodong Liu, Kevin Cheng, Yanyan Luo, Kevin Duh,
and Yuji Matsumoto. 2013. A Hybrid Chinese
Spelling Correction Using LanguageModel and Sta-
tisticalMachine Translation with Reranking. InPro-
ceedings of the Seventh SIGHAN Workshop on Chi-
nese Language Processing, pages 54?58, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
1521
I. Scott MacKenzie and R.William Soukoreff. 2002. A
Character-level Error Analysis Technique for Eval-
uating Text Entry Methods. In Proceedings of the
Second Nordic Conference on Human-computer In-
teraction, NordiCHI ?02, pages 243?246, NewYork,
NY, USA. ACM.
Eric Mays, Fred J Damerau, and Robert L Mercer.
1991. Context Based Spelling Correction. Informa-
tion Processing & Management, 27(5):517?522.
James L. Peterson. 1980. Computer Programs for De-
tecting and Correcting Spelling Errors. Commun.
ACM, 23(12):676?687, December.
Andreas Stolcke. 2002. SRILM-An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the in-
ternational conference on spoken language process-
ing, volume 2, pages 901?904.
Andrew J. Viterbi. 1967. Error Bounds for Con-
volutional Codes and an Asymptotically Optimum
Decoding Algorithm. Information Theory, IEEE
Transactions on, 13(2):260?269.
Xuan Wang, Lu Li, Lin Yao, and Waqas Anwar. 2006.
A Maximum Entropy Approach to Chinese Pin Yin-
To-Character Conversion. In Systems, Man and Cy-
bernetics, 2006. SMC?06. IEEE International Con-
ference on, volume 4, pages 2956?2959. IEEE.
Chun-Hung Wang, Jason S. Chang, and Jian-Cheng
Wu. 2013a. Automatic Chinese Confusion Words
ExtractionUsing Conditional RandomFields and the
Web. In Proceedings of the Seventh SIGHANWork-
shop on Chinese Language Processing, pages 64?
68, Nagoya, Japan, October. Asian Federation of
Natural Language Processing.
Peilu Wang, Ruihua Sun, Hai Zhao, and Kai Yu.
2013b. A New Word Language Model Evaluation
Metric for Character Based Languages. In Chinese
Computational Linguistics and Natural Language
Processing Based on Naturally Annotated Big Data,
pages 315?324. Springer.
Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumita,
Hai Zhao, and Bao-Liang Lu. 2013c. Converting
Continuous-Space Language Models into N-Gram
Language Models for Statistical Machine Transla-
tion. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 845?850, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Yih-Ru Wang, Yuan-Fu Liao, Yeh-Kuang Wu, and
Liang-Chun Chang. 2013d. Conditional Random
Field-based Parser and Language Model for Tradi-
tional Chinese Spelling Checker. In Proceedings
of the Seventh SIGHAN Workshop on Chinese Lan-
guage Processing, pages 69?73, Nagoya, Japan, Oc-
tober. Asian Federation of Natural Language Pro-
cessing.
Jacob O.Wobbrock and Brad A.Myers. 2006. Analyz-
ing the Input Stream for Character- Level Errors in
Unconstrained Text Entry Evaluations. ACM Trans.
Comput.-Hum. Interact., 13(4):458?489, December.
Jun Wu and Liren Chen. 2004. Fault-tolerant Roman-
ized Input Method for Non-roman Characters, Au-
gust 25. US Patent App. 10/928,131.
Jun Wu, Hulcan Zhu, and Hongjun Zhu. 2009. Sys-
tems and Methods for Translating Chinese Pinyin
to Chinese Characters, January 13. US Patent
7,478,033.
Shaohua Yang, Hai Zhao, and Bao-liang Lu. 2012a. A
Machine Translation Approach for Chinese Whole-
Sentence Pinyin-to-Character Conversion. In Pro-
ceedings of the 26th Pacific Asia Conference on Lan-
guage, Information, and Computation, pages 333?
342, Bali,Indonesia, November. Faculty of Com-
puter Science, Universitas Indonesia.
ShaohuaYang, Hai Zhao, XiaolinWang, and Bao-liang
Lu. 2012b. Spell Checking for Chinese. In Interna-
tional Conference on Language Resources and Eval-
uation, pages 730?736, Istanbul, Turkey, May.
Liang-Chih Yu, Yuen-Hsien Tseng, Jingbo Zhu, and
Fuji Ren, editors. 2013. Proceedings of the Seventh
SIGHAN Workshop on Chinese Language Process-
ing. Asian Federation of Natural Language Process-
ing, Nagoya, Japan, October.
Jingyi Zhang and Hai Zhao. 2013. Improving Func-
tion Word Alignment with Frequency and Syntac-
tic Information. In Proceedings of the Twenty-Third
international joint conference on Artificial Intelli-
gence, pages 2211?2217. AAAI Press.
Hai Zhao and Chunyu Kit. 2008. Exploiting Unlabeled
Text with Different Unsupervised Segmentation Cri-
teria for Chinese Word Segmentation. Research in
Computing Science, 33:93?104.
Hai Zhao and Chunyu Kit. 2011. Integrating Unsu-
pervised and Supervised Word Segmentation: The
Role of Goodness Measures. Information Sciences,
181(1):163?183.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
Improved Chinese Word Segmentation System with
Conditional Random Field. In Proceedings of the
Fifth SIGHANWorkshop on Chinese Language Pro-
cessing, pages 162?165, Sydney, Australia, July.
Association for Computational Linguistics.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A Unified Character-Based Tagging
Framework for Chinese Word Segmentation. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 9(2):5.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An Empirical Study on Word
Segmentation for Chinese Machine Translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, pages 248?263. Springer.
1522
Yabin Zheng, Chen Li, and Maosong Sun. 2011a.
CHIME: An Efficient Error-tolerant Chinese Pinyin
Input Method. In Proceedings of the Twenty-Second
International Joint Conference on Artificial Intel-
ligence - Volume Volume Three, IJCAI?11, pages
2551?2556. AAAI Press.
Yabin Zheng, Lixing Xie, Zhiyuan Liu, Maosong Sun,
Yang Zhang, and Liyun Ru. 2011b. Why Press
Backspace? Understanding User Input Behaviors in
Chinese Pinyin Input Method. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Techologies,
pages 485?490, Portland, Oregon, USA, June. Asso-
ciation for Computational Linguistics.
1523
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 203?207
Manchester, August 2008
Parsing Syntactic and Semantic Dependencies with
Two Single-Stage Maximum Entropy Models
?
Hai Zhao and Chunyu Kit
Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
haizhao@cityu.edu.hk, ctckit@cityu.edu.hk
Abstract
This paper describes our system to carry
out the joint parsing of syntactic and se-
mantic dependencies for our participation
in the shared task of CoNLL-2008. We il-
lustrate that both syntactic parsing and se-
mantic parsing can be transformed into a
word-pair classification problem and im-
plemented as a single-stage system with
the aid of maximum entropy modeling.
Our system ranks the fourth in the closed
track for the task with the following per-
formance on the WSJ+Brown test set:
81.44% labeled macro F1 for the overall
task, 86.66% labeled attachment for syn-
tactic dependencies, and 76.16% labeled
F1 for semantic dependencies.
1 Introduction
The joint parsing of syntactic and semantic depen-
dencies introduced by the shared task of CoNLL-
08 is more complicated than syntactic dependency
parsing or semantic role labeling alone (Surdeanu
et al, 2008). For semantic parsing, in particu-
lar, a dependency-based representation is given but
the predicates involved are unknown, and we also
have nominal predicates besides the verbal ones.
All these bring about more difficulties for learning.
This paper presents our research for participation
in the CoNLL-2008 shared task, with a highlight
on our strategy to select learning framework and
features for maximum entropy learning.
?
This study is supported by CERG grant 9040861 (CityU
1318/03H) and CityU Strategic Research Grant 7002037.
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The rest of the paper is organized as follows.
The next section presents the technical details of
our system and Section 3 its evaluation results.
Section 4 looks into a few issues concerning our
forthcoming work for this shared task, and Section
5 concludes the paper.
2 System Description
For the sake of efficiency, we opt for the maximum
entropy model with Gaussian prior as our learning
model for both the syntactic and semantic depen-
dency parsing. Our implementation of the model
adopts L-BFGS algorithm for parameter optimiza-
tion as usual (Liu and Nocedal, 1989). No addi-
tional feature selection techniques are applied.
Our system consists of three components to deal
with syntactic and semantic dependency parsing
and word sense determination, respectively. Both
parsing is formulated as a single-stage word-pair
classification problem, and the latter is carried out
by a search through the NomBank (Meyers et al,
2004) or the PropBank (Palmer et al, 2005)
1
.
2.1 Syntactic Dependency Parsing
We use a shift-reduce scheme to implement syn-
tactic dependency parsing as in (Nivre, 2003). It
takes a step-wised, history- or transition-based ap-
proach. It is basically a word-by-word method
with a projective constraint. In each step, the clas-
sifier checks a word pair, e.g., TOP, the top of a
stack for processed words, and, NEXT, the first
word in the unprocessed word sequence, in order
to determine if a dependent label should be as-
signed to them. Besides two arc-building actions,
a shift action and a reduce action are also defined
to meet the projective constraint, as follows.
1
These two dictionaries that we used are downloaded from
CoNLL-2008 official website.
203
Notation Meaning
s Clique in the top of stack
s
?1
,... The first clique below the top of stack, etc.
i, i
+1
,... The first (second) clique in the unprocessed
sequence, etc.
dprel Dependent label
h Head
lm Leftmost child
rm Rightmost child
rn Right nearest child
form Word form
lemma Word lemma
pos Predicted PoS tag
sp Y Split Y , which may be form, lemma or pos.
. ?s, e.g., ?s.dprel? means dependent label
of the clique in the top of stack
/ Feature combination, i.e., ?s.pos/i.pos?
means s.pos and i.pos together as a
feature function.
p The current predicate candidate
a The current argument candidate
Table 1: Feature Notations
1. Left-arc: Add an arc from NEXT to TOP and
pop the stack.
2. Right-arc: Add an arc from TOP to NEXT
and push NEXT onto the stack.
3. Reduce: Pop TOP from the stack.
4. Shift: Push NEXT onto the stack.
We implement a left-to-right arc-eager parsing
model in a way that the parser scan through an in-
put sequence from left to right and the right depen-
dents are attached to their heads as soon as possible
(Hall et al, 2007). To construct a single-stage sys-
tem, we extend the left-/right-arc actions to their
correspondent multi-label actions as necessary. In-
cluding 32 left-arc and 66 right-arc actions, alto-
gether a 100-class problem is yielded for the pars-
ing action classification for this shared task.
Since only projective sequences can be handled
by the shift-reduce scheme, we apply the pseudo-
projective transformation introduced by (Nivre and
Nilsson, 2005) to projectivize those non-projective
sequences. Our statistics show that only 7.6% se-
quences and less than 1% dependencies in the cor-
pus provided for training are non-projective. Thus,
we use a simplified strategy to projectivize an input
sequence. Firstly, we simply replace the head of a
non-projective dependency by its original head?s
head but without any additional dependent label
encoding for the purpose of deprojectivizing the
output during decoding. Secondly, if the above
standard projectivization step cannot eliminate all
Basic Extension
x.sp Y itself, its previous two and next two Y s, and
all bigrams within the five-clique window,
(x is s or i, and Y is form, lemma or pos.)
x.Y (x is s or i, and Y is form, lemma or pos.)
x.Y /i.Y (x is s or s
?1
and Y is pos, sp lemma
or sp pos)
s.h.sp form
s.dprel
s.lm.dprel
s.rn.dprel
i.lm.sp pos
s.lm.dprel/s.dprel
s.lm.sp pos/s.sp pos
s.h.sp pos/s.sp pos
x.sp pos|rootscore (x is s or i.)
s.sp pos/i.sp pos|pairscore
s.curroot.sp pos/i.sp pos
Table 2: Features for Syntactic Parsing
non-projective dependencies in a sequence, then
the word with the shortest sequence (rather than
dependent tree) distance to the original head will
be chosen as the head of a non-projective depen-
dency. In practice, the above two-step projectiviza-
tion procedure can eliminate all non-projective de-
pendencies in all sequences. Our purpose here is to
provide as much data as possible for training, and
only projective sequences are input for training and
output for decoding.
While memory-based and margin-based learn-
ing approaches such as support vector machines
are popularly applied to shift-reduce parsing, our
work provides evidence that the maximum en-
tropy model can achieve a comparative perfor-
mance with the aid of a suitable feature set. With
feature notations in Table 1, we use a feature set as
shown in Table 2 for syntactic parsing.
Here, we explain ?rootscore?, ?pairscore? and
curroot in Table 2. Both rootscore and pairscore
return the log frequency for an event in the training
corpus. The former counts a given split PoS occur-
ring as ROOT, and the latter two split PoS?s com-
bination associated with a dependency label. The
feature curroot returns the root of a partial parsing
tree that includes a specified node.
2.2 Semantic Dependency Parsing
Assuming no predicates overtly known, we keep
using a word-pair classifier to perform semantic
parsing through a single-stage processing. Specif-
ically, we specify the first word in a word pair as
a predicate candidate (i.e., a semantic head, and
noted as p in our feature representation) and the
next as an argument candidate (i.e., a semantic de-
204
Basic Extension
x.sp Y itself, its previous and next cliques, and
all bigrams within the three-clique window.
(Y is form or lemma.)
a
x.sp pos itself, its previous and next two cliques, and
all bigrams within the five-clique window.
x.Y (Y is form, lemma or pos.)
p.Y /i.Y (Y is sp lemma or sp pos.)
a is the same as p
x.is Verb or Noun
bankAdvice
b
a.h.sp form
x.dprel
x.lm.dprel
p.rm.dprel
p.lm.sp pos
a.lm.dprel/a.dprel
a.lm.sp pos/a.sp pos
a.sp Y/a.dprel (Y is lemma or pos.)
x.sp lemma/x.h.sp form
p.sp lemma/p.h.sp pos
p.sp pos/p.dprel
a.preddir
c
p.voice/a.preddir
d
x.posSeq
e
x.dprelSeq
f
a.dpTreeLevel
g
a/p|dpRelation
a/p|SharedPosPath
a/p|SharedDprelPath
a/p|x.posPath
a/p|x.dprelPath
a/p|dprelPath
a
x is p or a throughout the whole table.
b
This and the following features are all concerned with a
known syntactic dependency tree.
c
preddir: the direction to the current predicate candidate.
d
voice: if the syntactic head of p is be and p is not ended
with -ing, then p is passive.
e
posSeq: PoS tag sequence of all syntactic children
f
dprelSeq: syntactic dependent label sequence of all syn-
tactic children
g
dpTreeLevel: the level in the syntactic parse tree, counted
from the leaf node.
Table 3: Features for Semantic Parsing
pendent, and noted as a). We do not differenti-
ate between nominal and verbal predicates and our
system handles them in in exactly the same way.
If decoding outputs show that no arguments can
be found for a predicate candidate in the decoding
output, then this candidate will be naturally dis-
carded from the output predicate list.
When no constraint available, however, all word
pairs in the an input sequence must be considered,
leading to very poor efficiency in computation for
no gain in effectiveness. Thus, the training sample
needs to be pruned properly.
For predicate, only nouns and verbs are consid-
ered possible candidates. That is, all words with-
out a split PoS in these two categories are filtered
out. Many prepositions are also marked as pred-
icate in the training corpus, but their arguments?
roles are ?SU?, which are not counted the official
evaluation.
For argument, a dependency version of the prun-
ing algorithm in (Xue and Palmer, 2004) is used to
find, in an iterative way, the current syntactic head
and its siblings in a parse tree in a constituent-
based representation. In this representation, the
head of a phrase governs all its sisters in the tree,
as illustrated in the conversion of constituents to
dependencies in (Lin, 1995). In our implementa-
tion, the following equivalent algorithm is applied
to select argument candidates from a syntactic de-
pendency parse tree.
Initialization: Set the given predicate candi-
date as the current node;
(1) The current node and all of its syntactic chil-
dren are selected as argument candidates.
(2) Reset the current node to its syntactic head
and repeat step (1) until the root is reached.
This algorithm can cover 98.5% arguments while
reducing about 60% of the training samples, ac-
cording to our statistics. However, this is achieved
at the price of including a syntactic parse tree as
part of the input for semantic parsing.
The feature set listed in Table 3 is adopted for
our semantic parsing, some of which are borrowed
from (Hacioglu, 2004). Among them, dpTreeRela-
tion returns the relationship of a and p in a syntac-
tic parse tree. Its possible values include parent,
sibling, child, uncle, grand parent
etc. Note that there is always a path to the ROOT in
the syntactic parse tree for either a or p. Along the
common part of these two paths, SharedDprelPath
returns the sequence of dependent labels collected
from each node, and SharedPosPath returns the
corresponding sequence of PoS tags. x.dprelPath
and x.posPath return the PoS tag sequence from x
to the beginnings of SharedDprelPath and Shared-
PosPath, respectively. a/p|dprelPath returns the
concatenation of a.dprelPath and p.dprelPath.
We may have an example to show how the fea-
ture bankAdvice works. Firstly, the current pro-
cessed semantic role labels and argument candi-
date direction are checked. Specifically, they are
the arguments A0 and A1 that have been marked
before the predicate candidate p and the current ar-
gument identification direction after p. Secondly,
205
UAS LAS Label-Acc.
Development 88.78 85.85 91.14
WSJ 89.86 87.52 92.47
Brown 85.03 79.83 86.71
WSJ+Brown 89.32 86.66 91.83
Table 4: The Results of Syntactic Parsing (%)
Data Precision Recall F-score
Development 79.76 72.25 75.82
Label. WSJ 80.57 74.97 77.67
Brown 66.28 61.29 63.69
WSJ+Brown 79.03 73.49 76.16
Development 89.58 81.15 85.16
Unlab. WSJ 89.48 83.26 86.26
Brown 83.14 76.88 79.89
WSJ+Brown 88.79 82.57 85.57
Table 5: The Results of Semantic Parsing (%)
each example
2
of p in NomBank or PropBank that
depends on the split PoS tag of p is checked if
it partially matches the current processed role la-
bels. If a unique example exists in this form, e.g.,
Before:A0-A1; After:A3, then this feature
returns A3 as feature value. If no matched or mul-
tiple matched examples exist, then this feature re-
turns a default value.
2.3 Word Sense Determination
The shared task of CoNLL-2008 for word sense
disambiguation task is to determine the sense of an
output predicate. Our system carries out this task
by searching for a right example in the given Nom-
Bank or PropBank. The semantic role set scheme
of each example for an output predicate is checked.
If a scheme is found to match the output seman-
tic role set of a predicate, then the corresponding
sense for the first match is chosen; otherwise the
system outputs ?01? as the default sense.
3 Evaluation Results
Our evaluation is carried out on a 64-bit ubuntu
Linux installed server with double dual-core AMD
Opteron processors of 2.8GHz and 8GB memory.
The full training set for CoNLL-2008 is used to
train the maximum entropy model. The training
for the syntactic parser costs about 200 hours and
2
The term ?example? means a chunk in NomBank
or PropBank, which demonstrates how semantic roles
occur around a specified predicate. For example, for
a sense item of the predicate access in PropBank,
we first have <arg n="0">a computer</arg>
<rel>access</rel> <arg n="1">its
memory</arg>, and then a role set scheme for this
sense as Before:A0;After:A1.
Data Precision Recall F-score
Development 82.80 79.05 80.88
Label. WSJ 84.05 81.25 82.62
Macro Brown 73.05 70.56 71.78
WSJ+Brown 82.85 80.08 81.44
Development 89.18 84.97 87.02
Unlab. WSJ 89.67 86.56 88.09
Macro Brown 84.08 80.96 82.49
WSJ+Brown 89.06 85.94 87.47
Development 83.69 80.71 82.17
Label. WSJ 85.07 82.88 83.96
Micro Brown 75.14 73.09 74.10
WSJ+Brown 83.98 81.80 82.88
Development 89.06 85.90 87.45
Unlab. WSJ 89.72 87.42 88.56
Micro Brown 84.38 82.07 83.21
WSJ+Brown 89.14 86.83 87.97
Table 6: Overall Scores (%)
4.1GB memory and that for the semantic parser
costs about 170 hours and 4.9GB memory. The
running time in each case is the sum of all running
time for all threads involved. When a parallel opti-
mization technique is applied to speedup the train-
ing, the time can be reduced to about 1/3.5 of the
above.
The official evaluation results for our system are
presented in Tables 4, 5 and 6. Following the
official guideline of CoNLL-2008, we use unla-
beled attachment score (UAS), labeled attachment
score (LAS) and label accuracy to assess the per-
formance of syntactic dependency parsing. For
semantic parsing, the unlabeled scores metric the
identification performance and the labeled scores
the overall performance of semantic labeling.
4 To Do
Although we are unable to follow our plan to do
more than what we have done for this shared task,
because of the inadequate computational resource
and limited time, we have a number of techniques
in our anticipation to bring in further performance
improvement.
While expecting to accomplish the joint infer-
ence of syntactic and semantic parsing, we only
have time to complete a system with the former to
enhance the latter. But we did have experiments in
the early stage of our work to show that a syntactic
dependency parser can make use of available se-
mantic dependency information to enhance its per-
formance by 0.5-1%
3
.
Most errors in our syntactic parsing are related
3
We used the outputs of a semantic parser, either predicted
or gold-standard, as features for syntactic parsing.
206
to the dependencies of comma and prepositions.
We need to take care of them, for PP attachment
is also crucial to the success of semantic parsing.
Extra effort is paid, as illustrated in previous work
such as (Xue and Palmer, 2004), to handle such
cases, especially when a PP is involved. We find in
our data that about 1% arguments occur as a grand-
child of a predicate through PP attachment.
Syntactic parsing contributes crucially to the
overall performance of the joint parsing by pro-
viding a solid basis for further semantic parsing.
Thus there is reason to believe that improvement
of syntactic dependency parsing can be more in-
fluential than that of semantic parsing to the overall
improvement. Only one model was used for syn-
tactic parsing in our system, in contrast to the exist-
ing work using an ensemble technique for further
performance enhancement, e.g., (Hall et al, 2007).
Again, the latter means much more computational
cost should be taken.
Though it was not done before submission dead-
line, we also tried to enhance the semantic parsing
with some more sophisticated inputs from the syn-
tactic parsing. One is predicted syntactic parsed
tree input that may be created by cross-validation
rather than the gold-standard syntactic input that
our submitted semantic parser was actually trained
on. Another is the n-best outputs of the syntactic
parser. However, only the single-best output of the
syntactic parser was actually used.
5 Conclusion
As presented in the above sections, our system to
participate in the CoNLL-2008 shared task is im-
plemented as two single-stage maximum entropy
learning. We have tackled both syntactic and se-
mantic parsing as a word-pair classification prob-
lem. Despite the simplicity of this approach, our
system has produced promising results.
Acknowledgements
We wish to thank Dr. Wenliang Chen of NICT,
Japan for helpful discussions on dependency pars-
ing, and two anonymous reviewers for their valu-
able comments.
References
Hacioglu, Kadri. 2004. Semantic role labeling us-
ing dependency trees. In Proceedings of the 20th
international conference on Computational Linguis-
tics (COLING-2004), pages 1273?1276, Geneva,
Switzerland, August 23rd-27th.
Hall, Johan, Jens Nilsson, Joakim Nivre,
G?ulsen Eryi?git, Be?ata Megyesi, Mattias Nils-
son, and Markus Saers. 2007. Single malt or
blended? a study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 933?939,
Prague, Czech, June.
Lin, Dekang. 1995. A dependency-based method for
evaluating broad-coverage parser. In Proceedings
of the Fourteenth International Joint Conference on
Artificial Intelligence (IJCAI-95), pages 1420?1425,
Montr?eal, Qu?ebec, Canada, August 20-25.
Liu, Dong C. and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimiza-
tion. Mathematical Programming, 45:503?528.
Meyers, Adam, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In Proceedings of HLT/NAACL
Workshop on Frontiers in Corpus Annotation, pages
24?31, Boston, Massachusetts, USA, May 6.
Nivre, Joakim and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL-2005), pages 99?106, Ann
Arbor, Michigan, USA, June 25-30.
Nivre, Joakim. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT 03), pages 149?160, Nancy, France, April 23-
25.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
Xue, Nianwen and Martha Palmer. 2004. Cal-
ibrating features for semantic role labeling. In
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2004), pages 88?94,
Barcelona, Spain, July 25-26.
207
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 67?71,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
An Empirical Study on Development Set Selection Strategy
for Machine Translation Learning?
Cong Hui12, Hai Zhao12?, Yan Song3, Bao-Liang Lu12
1Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University, 800 Dong Chuan Rd., Shanghai 200240, China
3Department of Chinese, Translation and Linguistics, City University of Hong Kong
huicong@sjtu.edu.cn, {zhaohai,blu}@cs.sjtu.edu.cn
Abstract
This paper describes a statistical machine
translation system for our participation
for the WMT10 shared task. Based on
MOSES, our system is capable of translat-
ing German, French and Spanish into En-
glish. Our main contribution in this work
is about effective parameter tuning. We
discover that there is a significant perfor-
mance gap as different development sets
are adopted. Finally, ten groups of devel-
opment sets are used to optimize the model
weights, and this does help us obtain a sta-
ble evaluation result.
1 Introduction
We present a machine translation system that rep-
resents our participation for the WMT10 shared
task from Brain-like Computing and Machine In-
telligence Lab of Shanghai Jiao Tong University
(SJTU-BCMI Lab). The system is based on the
state-of-the-art SMT toolkit MOSES (Koehn et al,
2007). We use it to translate German, French and
Spanish into English. Though different develop-
ment sets used for training parameter tuning will
certainly lead to quite different performance, we
empirically find that the more sets we combine to-
gether, the more stable the performance is, and a
development set similar with test set will help the
performance improvement.
2 System Description
The basic model of the our system is a log-linear
model (Och and Ney, 2002). For given source lan-
?This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No. 60903119, Grant
No. 60773090 and Grant No. 90820018), the National Basic
Research Program of China (Grant No. 2009CB320901), and
the National High-Tech Research Program of China (Grant
No.2008AA02Z315).
?corresponding author
guage strings, the target language string t will be
obtained by the following equation,
t?I1 =argmaxtI1
{p?m1 (tI1 | sJ1 )}
=argmax
tI1
{ exp[
?M
m=1 ?mhm(tI1, sJ1 )]?
t?I1 exp[
?M
m=1 ?mhm(t?I1, sJ1 )]
},
where hm is the m-th feature function and ?m is
the m-th model weight. There are four main parts
of features in the model: translation model, lan-
guage model, reordering model and word penalty.
The whole model has been well implemented by
the state-of-the-art statistical machine translation
toolkit MOSES.
For each language that is required to translated
into English, two sets of bilingual corpora are pro-
vided by the shared task organizer. The first set
is the new release (version 5) of Europarl cor-
pus which is the smaller. The second is a com-
bination of other available data sets which is the
larger. In detail, two corpora, europarl-v5 and
news-commentary10 are for German, europarl-v5
and news-commentary10 plus undoc for French
and Spanish, respectively. Details of training data
are in Table 1. Only sentences with length 1 to 40
are acceptable for our task. We used the larger set
for our primary submission.
We adopt word alignment toolkit GIZA++ (Och
and Ney, 2003) to learn word-level alignment with
its default setting and grow-diag-final-and param-
eters. Given a sentence pair and its corresponding
word-level alignment, phrases will be extracted by
using the approach in (Och and Ney, 2004). Phrase
probability is estimated by its relative frequency
in the training corpus. Lexical reordering is deter-
mined by using the default setting of MOSES with
msd-bidirectional parameter.
For training the only language model (English),
the data sets are extracted from monolingual parts
of both europarl-v5 and news-commentary10,
67
sentences words(s) words(t)
de small 1540549 35.76M 38.53M
large 1640818 37.95M 40.64M
fr small 1683156 44.02M 44.20M
large 8997997 251.60M 228.50M
es small 1650152 43.17M 41.25M
large 7971200 236.24M 207.79M
Table 1: Bilingual training corpora from Ger-
man(de), French(fr) and Spanish(es) to English.
which include 1968914 sentences and 47.48M
words. And SRILM is adopted with 5-gram, in-
terpolate and kndiscount settings (Stolcke, 2002)
.
The next step is to estimate feature weights by
optimizing translation performance on a develop-
ment set. We consider various combinations of 10
development sets with 18207 sentences to get a
stable performance in our primary submission.
We use the default toolkits which are provided
by WMT10 organizers for preprocessing (i.e., to-
kenize) and postprocessing (i.e., detokenize, re-
caser).
3 Development Set Selection
3.1 Motivation
Given the previous feature functions, the model
weights will be obtained by optimizing the follow-
ing maximum mutual information criterion, which
can be derived from the maximum entropy princi-
ple:
??M1 = argmax?M1
{
S?
i=1
log p?M1 (ti | si)}
As usual, minimum error rate training (MERT) is
adopted for log-linear model parameter estimation
(Och, 2003). There are many improvements on
MERT in existing work (Bertoldi et al, 2009; Fos-
ter and Kuhn, 2009), but there is no demonstration
that the weights with better performance on the
development set would lead to a better result on
the unseen test set. In our experiments, we found
that different development sets will cause signifi-
cant BLEU score differences, even as high as one
percent. Thus the remained problem will be how
to effectively choose the development set to obtain
a better and more stable performance.
3.2 Experimental Settings
Our empirical study will be demonstrated through
German to English translation on the smaller cor-
pus. The development sets are all development
sets and test sets from the previous WMT shared
translation task as shown in Table 2, and labeled
as dev-0 to dev-9. Meanwhile, we denote 10 batch
sets from batch-0 to batch-9 where the batch-i set
is the combination of dev- sets from dev-0 to dev-i.
The test set is newstest2009, which includes 2525
sentences, 54K German words and 58K English
words, and news-test2008, which includes 2051
sentences, 41K German words and 43K English
words.
id name sent w(de) w(en)
dev-0 dev2006 2000 49K 53K
dev-1 devtest2006 2000 48K 52K
dev-2 nc-dev2007 1057 23K 23K
dev-3 nc-devtest2007 1064 24K 23K
dev-4 nc-test2007 2007 45K 44K
dev-5 nc-test2008 2028 45K 44K
dev-6 news-dev2009 2051 41K 43K
dev-7 test2006 2000 49K 54K
dev-8 test2007 2000 49K 54K
dev-9 test2008 2000 50K 54K
Table 2: Development data.
3.3 On the Scale of Development Set
Having 20 different development sets (10 dev- sets
and batch- sets), 20 models are correspondingly
trained.The decode results on the test set are sum-
marized in Table 3 and Figure 1. The dotted lines
are the performances of 10 different development
sets on the two test sets, we will see that there
is a huge gap between the highest and the lowest
score, and there is not an obvious rule to follow. It
will bring about unsatisfied results if a poor devel-
opment set is chosen. The solid lines represents
the performances of 10 incremental batch sets on
the two test sets, the batch processing still gives a
poor performance at the beginning, but the results
become better and more stable when the develop-
ment sets are continuously enlarged. This sort of
results suggest that a combined development set
may produce reliable results in the worst case. Our
primary submission used the combined develop-
ment set and the results as Table 4.
68
id 09-dev 09-batch 08-dev 08-batch
0 16.46 16.46 16.38 16.38
1 16.67 16.25 16.66 16.44
2 16.74 16.20 16.94 16.22
3 16.15 16.83 16.18 17.02
4 16.44 16.73 16.64 16.89
5 16.50 16.97 16.75 17.13
6 17.15 17.03 17.67 17.24
7 16.51 17.00 16.34 17.09
8 17.03 16.97 17.15 17.22
9 16.25 16.99 16.24 17.26
Table 3: BLEU scores on the two test
sets(newstest2009 & news-test2008), which use
two data set sequences(dev- sequence & batch- se-
quence) to optimize model weights.
de-en fr-en es-en
18.90 24.30 26.40
Table 4: BLEU scores of our primary submission.
3.4 On BLEU Score Difference
To compare BLEU score differences between test
set and development set, we consider two groups
of BLEU score differences, For each development
set, dev-i, the BLEU score difference will be com-
puted between b1 from which adopts itself as the
development set and b2 from which adopts test
set as the development set. For the test set, the
BLEU score difference will be computed between
b?1 from which adopts each development set, dev-i,
as the development set and b?2 from which adopts
itself as the development set.
These two groups of results are illustrated in
Figure 2 (the best score of the test set under self
tuning, newstest2009 is 17.91). The dotted lines
have the inverse trend with the dotted in Figure
1(because the addition of these two values is con-
stant), and the solid lines have the same trend
with the dotted, which means that the good per-
formance is mutual between test set and develop-
ment sets: if tuning using A set could make a good
result over B set, then vice versa.
3.5 On the Similarity between Development
Set and Test Set
This experiment is motivated by (Utiyama et al,
2009), where they used BLEU score to measure
the similarity of a sentences pair and then ex-
tracted sentences similar with those in test set to
0 1 2 3 4 5 6 7 8 90
0.5
1
1.5
2
2.5
DATA SET ID
DIF
F of
 BLE
U S
COR
E
 
 
09?Ddev09?Dtest
Figure 2: The trend of BLEU score differences
construct a specific tuning set. In our experiment,
we will try to measure data set similarity instead.
Given two sets of sentences, one is called as candi-
date(cnd) set and the other reference(ref) set. For
any cnd sentence, we let the whole ref set to be its
reference and then multi-references BLEU score is
computed for cnd set. There comes a problem that
the sentence penalty will be constant for any cnd
sentence, we turn to calculate the average length
of whose sentences which have common n-gram
with the given cnd sentence.
Now we may define three measures. The mea-
sure which uses dev- and batch- sets as cnd sets
and news-test2009 set as ref set is defined as
precision-BLEU , and the measure which uses the
above sets on the contrary way is defined as recall-
BLEU. Then F1-BLEU is defined as the harmonic
mean of precision-BLEU and recall-BLEU. These
results are illustrated in Figure 3. From the fig-
ure, we find that F1-BLEU plays an important
role to predict the goodness of a development set,
F1-BLEU scores of batch- sets have an ascending
curve and batch data set sequence will cause a sta-
ble good test performance, the point on dev- sets
which has high F1-BLEU(eg, dev-0,4,5) would
also has a good test performance.
3.6 Related Work
The special challenge of the WMT shared task is
domain adaptation, which is a hot topic in recent
years and more relative to our experiments. Many
existing works are about this topic (Koehn and
Schroeder, 2007; Nakov, 2008; Nakov and Ng,
2009; Paul et al, 2009; Haque et al, 2009). How-
ever, most of previous works focus on language
69
model, translation phrase table, lexicons model
and factored translation model, few of them pay
attention to the domain adaptation on the develop-
ment set. For future work we consider to use some
machine learning approaches to select sentences in
development sets more relevant with the test set in
order to further improve translation performance.
4 Conclusion
In this paper, we present our machine translation
system for the WMT10 shared task and perform an
empirical study on the development set selection.
According to our experimental results, Choosing
different development sets would play an impor-
tant role for translation performance. We find that
a development set with higher F1-BLEU yields
better and more stable results.
References
Nicola Bertoldi, Barry Haddow, and Jean Baptiste
Fouet. 2009. Improved Minimum Error Rate Train-
ing in Moses. The Prague Bulletin of Mathematical
Linguistics, 91:7?16.
George Foster and Roland Kuhn. 2009. Stabiliz-
ing minimum error rate training. In Proceedings
of the 4th Workshop on Statistical Machine Trans-
lation(WMT), Boulder, Colorado, USA.
Rejwanul Haque, Sudip Kumar Naskar, Josef Van Gen-
abith, and Andy Way. 2009. Experiments on Do-
main Adaptation for English?Hindi SMT. In 7th In-
ternational Conference on Natural Language Pro-
cessing(ICNLP), Hyderabad, India.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the 2nd Workshop on Sta-
tistical Machine Translation(WMT), Prague, Czech
Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics(ACL), Prague, Czech Republic.
Preslav Nakov and Hwee Tou Ng. 2009. NUS
at WMT09: domain adaptation experiments for
English-Spanish machine translation of news com-
mentary text. In Proceedings of the 4th Workshop on
Statistical Machine Translation(WMT), Singapore.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: Experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of the 3rd Workshop on
Statistical Machine Translation(WMT), Columbus,
Ohio, USA.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics(ACL), Philadelphia, Pennsylva-
nian, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41th Annual Meeting of the Association for Compu-
tational Linguistics(ACL), Sapporo, Japan.
Michael Paul, Andrew Finch, and Eiichiro Sumita.
2009. NICT@ WMT09: model adaptation and
transliteration for Spanish-English SMT. In Pro-
ceedings of the 4th Workshop on Statistical Machine
Translation(WMT), Singapore.
Andreas Stolcke. 2002. SRILM: an extensible lan-
guage modeling toolkit. In 7th International Con-
ference on Spoken Language Processing(ICSLP),
Denver, Colorado, USA.
Masao Utiyama, Hirofumi Yamamoto, and Eiichiro
Sumita. 2009. Two methods for stabilizing MERT:
NICT at IWSLT 2009. In Proceedings of Inter-
national Workshop on Spoken Language Transla-
tion(IWSLT), Tokyo, Japan.
70
0 1 2 3 4 5 6 7 8 915
15.5
16
16.5
17
17.5
18
DATA SET ID
BL
EU
 SC
OR
E
 
 
09?dev
09?batch
08?dev
08?batch
Figure 1: The BLEU score trend in Tabel 3, we will see that the batch lines output a stable and good
performance.
0 1 2 3 4 5 6 7 8 910
15
20
25
30
DATA SET ID
BL
EU
 VA
LU
E
 
 
pDev
pBatch
rDev
rBatch
fDev
fBatch
Figure 3: The precision(p), recall(r) and F1(f) BLEU score on the dev(Dev) and batch(Batch) sets based
on the comparison with news-test2009 set.
71
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 62?65,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Reranking with Multiple Features for Better Transliteration
Yan Song? Chunyu Kit? Hai Zhao??
?Department of Chinese, Translation and Linguistics
City University of Hong Kong, 83 Tat Chee Ave., Kowloon, Hong Kong
?Department of Computer Science and Engineering
Shanghai Jiao Tong University, #800, Dongchuan Rd, Shanghai, China
{yansong,ctckit}@cityu.edu.hk, zhaohai@cs.sjtu.edu.cn
Abstract
Effective transliteration of proper names
via grapheme conversion needs to find
transliteration patterns in training data,
and then generate optimized candidates
for testing samples accordingly. However,
the top-1 accuracy for the generated candi-
dates cannot be good if the right one is not
ranked at the top. To tackle this issue, we
propose to rerank the output candidates for
a better order using the averaged percep-
tron with multiple features. This paper de-
scribes our recent work in this direction for
our participation in NEWS2010 transliter-
ation evaluation. The official results con-
firm its effectiveness in English-Chinese
bidirectional transliteration.
1 Introduction
Since transliteration can be considered a direct or-
thographic mapping process, one may adopt gen-
eral statistical machine translation (SMT) proce-
dures for its implementation. Aimed at finding
phonetic equivalence in another language for a
given named entity, however, different translitera-
tion options with different syllabification may gen-
erate multiple choices with the symphonic form
for the same source text. Consequently, even the
overall results by SMT output are acceptable, it
is still unreliable to rank the candidates simply by
their statistical translation scores for the purpose
of selecting the best one. In order to make a proper
choice, the direct orthographic mapping requires a
precise alignment and a better transliteration op-
tion selection. Thus, powerful algorithms for ef-
fective use of the parallel data is indispensable, es-
pecially when the available data is limited in vol-
ume.
Interestingly, although an SMT based approach
could not achieve a precise top-1 transliteration re-
sult, it is found in (Song et al, 2009) that, in con-
trast to the ordinary top-1 accuracy (ACC) score,
its recall rate, which is defined in terms of whether
the correct answer is generated in the n-best output
list, is rather high. This observation suggests that
if we could rearrange those outputs into a better
order, especially, push the correct one to the top,
the overall performance could be enhanced signif-
icantly, without any further refinement of the orig-
inal generation process. This reranking strategy is
proved to be efficient in transliteration generation
with a multi-engine approach (Oh et al, 2009).
In this paper, we present our recent work on
reranking the transliteration candidates via an on-
line discriminative learning framework, namely,
the averaged perceptron. Multiple features are in-
corporated into it for performance enhancement.
The following sections will give the technical de-
tails of our method and present its results for
NEWS2010 shared task for named entity translit-
eration.
2 Generation
For the generation of transliteration candidates,
we follow the work (Song et al, 2009), using a
phrase-based SMT procedure with the log-linear
model
P (t|s) = exp[
?n
i=1 ?ihi(s, t)]?
t exp[
?n
i=1 ?ihi(s, t)]
(1)
for decoding. Originally we use two directional
phrase1 tables, which are learned for both direc-
tions of source-to-target and target-to-source, con-
taining different entries of transliteration options.
In order to facilitate the decoding by exploiting all
possible choices in a better way, we combine the
forward and backward directed phrase tables to-
gether, and recalculate the probability for each en-
1It herein refers to a character sequence as described in
(Song et al, 2009).
62
try in it. After that, we use a phoneme resource2 to
refine the phrase table by filtering out the wrongly
extracted phrases and cleaning up the noise in it.
In the decoding process, a dynamic pruning is per-
formed when generating the hypothesis in each
step, in which the threshold is variable according
to the current searching space, for we need to ob-
tain a good candidate list as precise as possible
for the next stage. The parameter for each fea-
ture function in log-linear model is optimized by
MERT training (Och, 2003). Finally, a maximum
number of 50 candidates are generated for each
source name.
3 Reranking
3.1 Learning Framework
For reranking training and prediction, we adopt
the averaged perceptron (Collins, 2002) as our
learning framework, which has a more stable per-
formance than the non-averaged version. It is pre-
sented in Algorithm 1. Where ~? is the vector of
parameters we want to optimize, x, y are the cor-
responding source (with different syllabification)
and target graphemes in the candidate list, and ?
represents the feature vector in the pair of x and
y. In this algorithm, reference y?i is the most ap-
propriate output in the candidate list according to
the true target named entity in the training data.
We use the Mean-F score to identify which candi-
date can be the reference, by locating the one with
the maximum Mean-F score value. This process
updates the parameters of the feature vector and
also relocate all of the candidates according to the
ranking scores, which are calculated in terms of
the resulted parameters in each round of training
as well as in the testing process. The number of
iteration for the final model is determined by the
development data.
3.2 Multiple Features
The following features are used in our reranking
process:
Transliteration correspondence feature, f(si, ti);
This feature describes the mapping between
source and target graphemes, similar to the
transliteration options in the phrase table in
our previous generation process, where s and
2In this work, we use Pinyin as the phonetic representa-
tion for Chinese.
Algorithm 1 Averaged perceptron training
Input: Candidate list with reference
{LIST (xj , yj)nj=1, y?i }Ni=1
Output: Averaged parameters
1: ~? ? 0, ~?a ? 0, c? 1
2: for t = 1 to T do
3: for i = 1 to N do
4: y?i ? argmaxy?LIST (xj ,yj)~? ? ?(xi, yi)
5: if y?i 6= y?i then
6: ~? ? ~? +?(x?i , y?i )? ?(x?i, y?i)
7: ~?a ? ~?a+ c ? {?(x?i , y?i )??(x?i, y?i)}
8: end if
9: c? c+ 1
10: end for
11: end for
12: return ~? ? ~?a/c
t refer to the source and target language re-
spectively, and i to the current position.
Source grapheme chain feature, f(sii?1);
It measures the syllabification for a given
source text. There are two types of units
in different levels. One is on syllable level,
e.g., ?aa/bye?, ?aa/gaar/d?, reflecting the
segmentation of the source text, and the other
on character level, such as ?a/b?, ?a/g?,
?r/d?, showing the combination power of
several characters. These features on differ-
ent source grapheme levels can help the sys-
tem to achieve a more reliable syllabification
result from the candidates. We only consider
bi-grams when using this feature.
Target grapheme chain feature, f(tii?2);
This feature measures the appropriateness of
the generated target graphemes on both char-
acter and syllables level. It performs in a
similar way as the language model for SMT
decoding. We use tri-gram syllables in this
learning framework.
Paired source-to-target transition feature, f(<
s, t >ii?1);
This type of feature is firstly proposed in
(Li et al, 2004), aiming at generating source
and target graphemes simultaneously under
a suitable constraint. We use this feature
to restrict the synchronous transition of both
source and target graphemes, measuring how
well are those transitions, such as for ?st?,
63
whether ?s? transliterated by ??? is followed
by ?t? transliterated by ???. In order to deal
with the data sparseness, only bi-gram transi-
tion relations are considered in this feature.
Hidden Markov model (HMM) style features;
There are a group of features with HMM
style constraint for evaluating the candi-
dates generated in previous SMT process,
including, previous syllable HMM features,
f(sii?n+1, ti), posterior syllable HMM fea-
tures, f(si+n?1i , ti), and posterior character
HMM features, f(si, l, ti), where l denotes
the character following the previous syllable
in the source language. For the last feature,
it is effective to use both the current sylla-
ble and the first letter of the next syllable
to bound the current target grapheme. The
reason for applying this feature in our learn-
ing framework is that, empirically, the letters
following many syllables strongly affect the
transliteration for them, e.g., Aves ? ??
?, ?a? followed by ?v? is always translated
into ??? rather than ???.
Target grapheme position feature, f(ti, p);
This feature is an improved version of that
proposed in (Song et al, 2009), where p
refers to the position of ti. We have a mea-
sure for the target graphemes according to
their source graphemes and the current posi-
tion of their correspondent target characters.
There are three categories of such position,
namely, start (S), mediate (M) and end (E). S
refers to the first character in a target name, E
to the final, and the others belong to M. This
feature is used to exploit the observation that
some characters are more likely to appear at
certain positions in the target name. Some are
always found at the beginning of a named en-
tity while others only at the middle or the end.
For example, ?re? associated to first charac-
ter in a target name is always transliterated as
???, such as Redd ???. When ?re? ap-
pears at the end of a source name, however,
its transliteration will be ??? in most cases,
just like Gore ???.
Target tone feature;
This feature is only applied to the translit-
eration task with Chinese as the target lan-
guage. It can be seen as a combination
of a target grapheme chain with some posi-
tion features, using tone instead of the target
grapheme itself for evaluation. There are 5
tones (0,1,2,3,4) for Chinese characters. It is
easy to conduct a comprehensive analysis for
the use of a higher ordered transition chain as
a better constraint. Many fixed tone patterns
can be identified in the Chinese translitera-
tion training data. The tone information can
also be extracted from the Pinyin resource we
used in the previous stage.
Besides the above string features, we also have
some numeric features, as listed below.
Transliteration score;
This score is the joint probabilities of all
transliteration options, included in the output
candidates generated by our decoder.
Target language model score;
This score is calculated from the probabilistic
tri-gram language model.
Source/target Pinyin feature;
This feature uses Pinyin representation for a
source or target name, depending on what
side the Chinese language is used. It mea-
sures how good the output candidates can be
in terms of the comparison between English
text and Pinyin representation. The resulted
score is updated according to the Levenshtein
distance for the two input letter strings of En-
glish and Pinyin.
For a task with English as the target language,
we add the following two additional features into
the learning framework.
Vowel feature;
It is noticed that when English is the target
language, vowels can sometimes be missing
in the generated candidates. This feature is
thus used to punish those outputs unqualified
to be a valid English word for carrying no
vowel.
Syllable consistent feature;
This feature measures whether an English tar-
get name generated in the previous step has
the same number of syllables as the source
name. In Chinese-to-English transliteration,
Chinese characters are single-syllabled, thus
64
Table 1: Evaluation results for our NEWS2010 task.
Task Source Target ACC Mean F MRR Map ref Recall ACCSMT
EnCh English Chinese 0.477 0.740 0.506 0.455 0.561 0.381
ChEn Chinese English 0.227 0.749 0.269 0.226 0.371 0.152
we can easily identify their number. For syl-
labification, we have an independent segmen-
tation process for calculating the syllables.
4 Results
For NEWS2010, we participated in all two
Chinese related transliteration tasks, namely,
EnCh (English-to-Chinese) and ChEn (Chinese-
to-English back transliteration). The official eval-
uation scores for our submissions are presented
in Table 1 with recall rate, and the ACC score
(ACCSMT ) for original SMT outputs. It is easy
to see the performance gain for the reranking, and
also from the recall rate that there is still some
room for improvement, in spite of the high ratio of
ACC/Recall3 calculated from Table 1. However, it
is also worth noting that, some of the source texts
cannot be correctly transliterated, due to many
multiple-word name entities with semantic com-
ponents in the test data, e.g., ?MANCHESTER
BRIDGE?, ?BRIGHAM CITY? etc. These seman-
tic parts are beyond our transliteration system?s ca-
pability to tackle, especially when the training data
is limited and the only focus of the system is on the
phonetic equivalent correspondence.
Compared to the EnCh transliteration, we get a
rather low ACC score for the ChEn back translit-
eration, suggesting that ChEn task is somewhat
harder than the EnCh (in which Chinese char-
acters are always limited). The ChEn task is a
one-to-many translation, involving a lot of pos-
sible choices and combinations of English sylla-
bles. This certainly makes it a more challenge-
able task than EnCh. However, looking into the
details of the outputs, we find that, in the ChEn
back transliteration, some characters in the test
corpus are unseen in the training and the devel-
opment data, resulting in incorrect transliterations
for many graphemes. This is another factor affect-
ing our final results for the ChEn task.
5 Conclusion
In this paper, we have presented our work on
multiple feature based reranking for transliteration
3Compared to the results from (Song et al, 2009)
generation. It NEWS2010 results show that this
approach is effective and promising, in the sense
that it ranks the best in EnCh and ChEn tasks. The
reranking used in this work can also be consid-
ered a regeneration process based on an existing
set, as part of our features are always used directly
to generate the initial transliteration output in other
researches. Though, those features are strongly
dependent on the nature of English and Chinese
languages, it is thus not an easy task to transplant
this model for other language pairs. It is an inter-
esting job to turn it into a language independent
model that can be applied to other languages.
Acknowledgments
The research described in this paper was par-
tially supported by City University of Hong Kong
through the Strategic Research Grants (SRG)
7002267 and 7008003. Dr. Hai Zhao was sup-
ported by the Natural Science Foundation of China
(NSFC) through the grant 60903119. We also
thank Mr. Wenbin Jiang for his helpful sugges-
tions on averaged perceptron learning.
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP-2002, pages 1?8, July.
Haizhou Li, Min Zhang, and Jian Su. 2004. A
joint source-channel model for machine transliter-
ation. In Proceedings of ACL-04, pages 159?166,
Barcelona, Spain, July.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL-03, pages 160?167, Sapporo, Japan, July.
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-
sawa. 2009. Machine transliteration using target-
language grapheme and phoneme: Multi-engine
transliteration approach. In Proceedings of NEWS
2009, pages 36?39, Suntec, Singapore, August.
Yan Song, Chunyu Kit, and Xiao Chen. 2009. Translit-
eration of name entity via improved statistical trans-
lation on character sequences. In Proceedings of
NEWS 2009, pages 57?60, Suntec, Singapore, Au-
gust.
65
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 92?99,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Hedge Detection and Scope Finding by Sequence Labeling
with Normalized Feature Selection?
Shaodian Zhang12, Hai Zhao123?, Guodong Zhou3 and Bao-Liang Lu12
1Center for Brain-Like Computing and Machine Intelligence
Dept of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University
3School of Computer Science and Technology, Soochow University
zhangsd.sjtu@gmail.com, zhaohai@cs.sjtu.edu.cn
gdzhou@suda.edu.cn, blu@cs.sjtu.edu.cn
Abstract
This paper presents a system which adopts
a standard sequence labeling technique for
hedge detection and scope finding. For
the first task, hedge detection, we formu-
late it as a hedge labeling problem, while
for the second task, we use a two-step la-
beling strategy, one for hedge cue label-
ing and the other for scope finding. In par-
ticular, various kinds of syntactic features
are systemically exploited and effectively
integrated using a large-scale normalized
feature selection method. Evaluation on
the CoNLL-2010 shared task shows that
our system achieves stable and competi-
tive results for all the closed tasks. Fur-
thermore, post-deadline experiments show
that the performance can be much further
improved using a sufficient feature selec-
tion.
1 Introduction
Hedges are linguistic devices representing spec-
ulative parts of articles. Previous works such as
(Hyland, 1996; Marco and Mercer, 2004; Light et
al., 2004; Thompson et al, 2008) present research
on hedge mainly as a linguistic phenomenon.
Meanwhile, detecting hedges and their scopes au-
tomatically are increasingly important tasks in nat-
ural language processing and information extrac-
tion, especially in biomedical community. The
shared task of CoNLL-2010 described in Farkas
et al (2010) aims at detecting hedges (task 1)
and finding their scopes (task 2) for the literature
? This work is partially supported by the National
Natural Science Foundation of China (Grants 60903119,
60773090, 90820018 and 90920004), the National Basic Re-
search Program of China (Grant No. 2009CB320901), and
the National High-Tech Research Program of China (Grant
No.2008AA02Z315).
?corresponding author
from BioScope corpus (Szarvas et al, 2008) and
Wikipedia. This paper describes a system adopt-
ing sequence labeling which performs competitive
in the official evaluation, as well as further test.
In addition, a large-scale feature selection proce-
dure is applied in training and development. Con-
sidering that BioScope corpus is annotated by two
independent linguists according to a formal guide-
line (Szarvas, 2008), while Wikipedia weasels are
tagged by netizens who are diverse in background
and various in evaluation criterion, it is needed to
handle them separately. Our system selects fea-
tures for Wikipedia and BioScope corpus indepen-
dently and evaluate them respectively, leading to
fine performances for all of them.
The rest of the paper is organized as follows.
The next section presents the technical details of
our system of hedge detection and scope finding.
Section 3 gives information of features. Section
4 shows the evaluation results, including official
results and further ones after official outputs col-
lection. Section 5 concludes the paper.
2 Methods
Basically, the tasks are formulated as sequence la-
beling in our approach. The available label set dif-
fers between task 1 and 2. In addition, it is needed
to introduce an indicator in order to find scopes for
the multi-hedge sentences properly.
2.1 Hedge detection
The valid label set of task 1, hedge detection, con-
tains only two labels: ?Hedge? and ? ?, which
represent that a word is in a hedge cue or not
respectively. Since results of hedge detection in
this shared task are evaluated at sentence level, a
sentence will be classified as ?uncertain? in the
post-process if it has one or more words labeled
?Hedge? in it and otherwise ?certain?.
92
2.2 Scope finding
The second task is divided into two steps in our
system. The first step is quite the same as what
the system does in task 1: labeling the words as in
hedge cues or not. Then the scope of each hedge
will be labeled by taking advantage of the result
of the first step. A scope can be denoted by a
beginning word and an ending word to represent
the first and the last element. In scope finding the
available label set contains ?Begin?, ?End?, ?Mid-
dle? and ? ?, representing the first and last word in
the scope, in-scope and out-of-scope. As an exam-
ple, a sentence with hedge cue and scope labeling
is given in Table 1. Hedge cue ?indicating? with
its scope from ?indicating? itself to ?transcription?
are labeled. While evaluating outputs, only ?Be-
gin?s and ?End?s will be taken into consideration
and be treated as the head and tail tokens of the
scopes of specific hedge cues.
Furthermore ...
, ...
inhibition ...
can ...
be ...
blocked ...
by ...
actinomycin ...
D ...
, ...
indicating ... Hedge Begin
a ... Middle
requirement ... Middle
for ... Middle
de ... Middle
novo ... Middle
transcription ... End
. ...
Table 1: A sentence with hedge cue and scope la-
beling
It seems that the best labeling result of task 1
can be used directly to be the proper intermediate
representation of task 2. However, the complexity
of scope finding for multi-hedge sentences forces
us to modify the intermediate result of task 2 for
the sake of handling the sentences with more than
one hedge cue correctly. Besides, since task 1 is
a sentence classification task essentially, while the
goal of the first step of task 2 is to label the words
as accurately as possible, it is easy to find that
the optimal labeling results of task 1 may not be
optimal to be the intermediate representations for
task 2. This problem can be solved if sentence-
level hedge detection and intermediate representa-
tion finding are treated as two separate tasks with
independent feature selection procedures. The de-
tails of feature selection will be given in section
3.
2.3 Scope finding for multi-hedge cases
Sentences with more than one hedge cue are quite
common in both datasets of BioScope corpus and
Wikipedia. By counting hedges in every sentence,
we find that about one fourth of the sentences with
hedges have more than one hedge cue in all three
data sources (Table 2). In Morante and Daele-
mans (2009), three classifiers predict whether each
token is Begin, End or None and a postprocess-
ing is needed to associate Begins and Ends with
their corresponding hedge cues. In our approach,
in order to decrease ambiguous or illegal outputs
e.g. inequivalent numbers of Begins and Ends, a
pair of Begin and End without their correspond-
ing hedge cue between them, etc., sentences with
more than one hedge cue will be preprocessed by
making copies as many as the number of hedges
and be handled separately.
The sentence which is selected as a sample has
two hedge cues: ?suggesting? and ?may?, so our
system preprocesses the sentence into two single-
hedge ones, which is illustrated in Table 3. Now it
comes to the problem of finding scope for single-
hedge sentence. The two copies are labeled sep-
arately, getting one scope from ?suggesting? to
?mitogenesis? for the hedge cue ?suggesting? and
the other from ?IFN-alpha? to ?mitogenesis? for
?may?. Merging the two results will give the final
scope resolution of the sentence.
However, compared with matching Begins and
Ends in postprocessing given by Morante and
Daelemans (2009), the above method gives rise
to out of control of projections of the scopes,
i.e. scopes of hedges may partially overlap after
copies are merged. Since scopes should be in-
tact constituents of sentences, namely, subtrees in
syntax tree which never partly overlap with each
other, results like this are linguistically illegal and
should be discarded. We solve this problem by in-
troducing an instructional feature called ?Indica-
tor?. For sentences with more than one hedge cue,
namely more than one copy while finding scopes,
words inside the union of existing (labeled) scopes
will be tagged as ?Indicator? in unhandled copies
before every labeling. For example, after finding
scope for the first copy in Table 3 and words from
93
Dataset # Sentence # No-hedge ratio # One-hedge ratio # Multi-hedge ratio
Biomedical Abstracts 11871 9770 82.3% 1603 13.5% 498 4.2%
Biomedical Fulltexts 2670 2151 80.6% 385 14.4% 134 5.0%
Wikipedia 11111 8627 77.6% 1936 17.4% 548 4.9%
Table 2: Statistics of hedge amount
IFN-alpha IFN-alpha
also also
sensitized sensitized
T T
cells cells
to to
IL-2-induced IL-2-induced
proliferation proliferation
, ,
further further
suggesting Hedge suggesting
that that
IFN-alpha IFN-alpha
may may Hedge
be be
involved involved
in in
the the
regulation regulation
of of
T-cell T-cell
mitogenesis mitogenesis
. .
Table 3: An example of 2-hedge sentence before
scope finding
?suggesting? to ?mitogenesis? are put in the scope
of cue ?suggesting?, these words should be tagged
?Indicator? in the second copy, whose result is il-
lustrated in Table 4. If not in a scope, any word is
tagged ? ? as the indicator. The ?Indicator?s tag-
ging from ?suggesting? to ?mitogenesis? in Table
4 mean that no other than the situations of a) ?Be-
gin? is after or at ?suggesting? and ?End? is before
or at ?mitogenesis? b) Both ?Begin? and ?End? are
before ?suggesting? c) Both next ?Begin? and next
?End? are after ?mitogenesis? can be accepted. In
other words, new labeling should keep the projec-
tions of scopes in the result. Although it is only
an instructional indicator and does not have any
coerciveness, the evaluation result of experiment
shows it effective.
3 Feature selection
Since hedge and scope finding are quite novel
tasks and it is not easy to determine the effective
features by experience, a greedy feature selection
is conducted. As it mentioned in section 2, our
system divides scope finding into two sub-tasks:
IFN-alpha ...
also ...
sensitized ...
T ...
cells ...
to ...
IL-2-induced ...
proliferation ...
, ...
further ...
suggesting ... Indicator
that ... Indicator
IFN-alpha ... Indicator Begin
may ... Indicator Hedge Middle
be ... Indicator Middle
involved ... Indicator Middle
in ... Indicator Middle
the ... Indicator Middle
regulation ... Indicator Middle
of ... Indicator Middle
T-cell ... Indicator Middle
mitogenesis ... Indicator End
. ...
Table 4: Scope resolution with instructional fea-
ture: ?Indicator?
a) Hedge cue labeling
b) Scope labeling
The first one is the same as hedge detection task
in strategy, but quite distinct in target of feature
set, because hedge detection is a task of sentence
classification while the first step of scope find-
ing aims at high accuracy of labeling hedge cues.
Therefore, three independent procedures of fea-
ture selection are conducted for BioScope corpus
dataset. AsWikipedia is not involved in the task of
scope finding, it only needs one final feature set.
About 200 feature templates are initially con-
sidered for each task. We mainly borrow ideas and
are enlightened by following sources while initial-
izing feature template sets:
a) Previous papers on hedge detection and
scope finding (Light et al, 2004; Medlock,
2008; Medlock and Briscoe, 2008; Kilicoglu
and Bergler, 2008; Szarvas, 2008; Ganter
and Strube, 2009; Morante and Daelemans,
2009);
94
b) Related works such as named entity recog-
nition (Collins, 1999) and text chunking
(Zhang et al, 2001);
c) Some literature on dependency parsing
(Nivre and Scholz, 2004; McDonald et al,
2005; Nivre, 2009; Zhao et al, 2009c; Zhao
et al, 2009a);
3.1 Notations of Feature Template
A large amount of advanced syntactic features in-
cluding syntactic connections, paths, families and
their concatenations are introduced. Many of these
features come from dependency parsing, which
aims at building syntactic tree expressed by depen-
dencies between words. More details about de-
pendency parsing are given in Nivre and Scholz
(2004) and McDonald et al (2005). The parser
in Zhao et al (2009a) is used to construct de-
pendency structures in our system, and some of
the notations in this paper adopt those presented
in Zhao et al (2009c). Feature templates are from
various combinations or integrations of the follow-
ing basic elements.
Word Property. This part of features includes
word form (form), lemma (lemma), part-of-speech
tag (pos), syntactic dependency (dp) , syntactic de-
pendency label (dprel).
Syntactic Connection. This includes syntactic
head (h), left(right) farthest(nearest) child (lm, ln,
rm and rn) and high (low) support verb, noun or
preposition. Here we specify the last one as an
example, support verb(noun/preposition). From a
given word to the syntactic root along the syntac-
tic tree, the first verb/noun/preposition that is met
is called its low support verb/noun/preposition,
and the nearest one to the root(farthest to
the given word) is called as its high support
verb/noun/preposition. The concept of support
verb was broadly used (Toutanova et al, 2005;
Xue, 2006; Jiang and Ng, 2006), and it is extended
to nouns and prepositions in Zhao et al (2009b).
In addition, a slightly modified syntactic head, pp-
head, is introduced, it returns the left most sibling
of a given word if the word is headed by a prepo-
sition, otherwise it returns the original head.
Path. There are two basic types of path. One
is the linear path (linePath) in the sequence, the
other is the path in the syntactic parsing tree (dp-
Path). For example, m:n|dpPath represents the
dependency path from word m to n. Assuming
that the two paths from m and n to the root are
pm and pn, m:n|dpPathShare, m:n|dpPathPred
and m:n|dpPathArgu represent the common part
of pm and pn, part of pm which does not belong
to pn and part of pn which does not belong to pm,
respectively.
Family. A children set includes all syntactic
children(children) are used in the template nota-
tions.
Concatenation of Elements. For all collected
elements according to dpPath, children and so on,
we use three strategies to concatenate all those
strings to produce the feature value. The first is
seq, which concatenates all collected strings with-
out doing anything. The second is bag, which
removes all duplicated strings and sort the rest.
The third is noDup, which removes all duplicated
neighbored strings.
Hedge Cue Dictionary and Scope Indicator.
Hedge cues in the training set are collected and put
in a dictionary. Whether a word in the training or
testing set is in the dictionary (dic) is introduced
into feature templates. As the evaluation is non-
open, we do not put in any additional hedge cues
from other resources. An indicator (indicator) is
given for multi-hedge scope finding, as specified
in section 2.At last, in feature set for scope label-
ing, hedge represents that the word is in a hedge
cue.
At last, we take x as current token to be labeled,
and xm to denote neighbor words. m > 0 repre-
sents that it is a word goes mth after current word
and m < 0 for word ?mth before current word.
3.2 Feature template sets for each task
As optimal feature template subsets cannot be ex-
pected to be extracted from so large sets by hand,
greedy feature selections according to Zhao et al
(2009b) are applied. The normalized feature selec-
tion has been proved to be effective in quite a lot
of NLP tasks and can often successfully select an
optimal or very close to optimal feature set from a
large-scale superset. Although usually it needs 3
to 4 loops denoted by ?While? in the Algorithm 1
of Zhao et al (2009b) to get the best template set,
we only complete one before official outputs col-
lection because of time limitation, which to a large
extent hinders the performance of the system.
Three template sets are selected for BioScope
corpus. One with the highest accuracy for
sentence-level hedge detection (Set B), one with
the best performance for word-level hedge cue la-
95
beling (Set H) and another one with the maximal
F-score for scope finding (Set S). In addition, one
set is discovered for sentence-level hedge detec-
tion of Wikipedia (Set W)1 . Table 52 lists some
selected feature templates which are basic word or
hedging properties for the three sets of BioScope
corpus and Wikipedia. From the table we can see
it is clear that the combinations of lemma, POS
and word form of words in context, which are usu-
ally basic and common elements in NLP, are also
effective for hedge detection. And as we expected,
the feature that represents whether the word is in
the hedge list or not is very useful especially in
hedge cue finding, indicating that methods based
on a hedge cue lists (Light et al, 2004) or keyword
selection (Szarvas, 2008) are quite significant way
to accomplish such tasks.
Some a little complicated syntactic features
based on dependencies are systemically exploited
as features for tasks. Table 6 enumerates some of
the syntactic features which proves to be highly
effective. We noticed that lowSupportNoun, high-
SupportNoun and features derived from dpPath is
notably useful. It can be explained by the aware-
ness that hedge labeling and scope finding are to
process literatures in the level of semantics where
syntactic features are often helpful.
We continue our feature selection procedures
for BioScope corpus after official outputs collec-
tion and obtain feature template sets that bring bet-
ter performance. Table 7 gives some of the fea-
tures in the optimized sets for BioScope corpus
resolution. One difference between the new sets
and the old ones is the former contain more syntac-
tic elements, indicating that exploiting syntactic
feature is a correct choice. Another difference is
the new sets assemble more information of words
before or after the current word, especially words
linearly far away but close in syntax tree. Appear-
ance of combination of these two factors such as
x?1.lm.form seems to provide an evidence of the
insufficiency training and development of our sys-
tem submitted to some extent.
4 Evaluation results
Two tracks (closed and open challenges) are pro-
vided for CoNLL-2010 shared task. We partici-
pated in the closed challenge, select features based
1num in the set of Wikipedia represents the sequential
number of word in the sentence
2Contact the authors to get the full feature lists, as well as
entire optimized sets in post-deadline experiment
- x.lemma + x1.lemma + x?1.lemma
+ x.dic + x1.dic + x?1.dic
- x.lemma + x1.pos + x?1.pos + x.pos
+ x1.lemma + x?1.lemma
- x.form
Set B x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x.dic + x1.dic + x?1.dic
- x1.pos
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic
- x.pos + x?1.pos
- x.dic
Set H x.dic + x.lemma + x.pos + x.form
- x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x?2.form + x?2.lemma
- x?1.form + x.form
- x.dic + x1.dic + x?1.dic
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic + x3.dic + x?3.dic
- x.indicator
- x.hedge + x1.hedge + x?1.hedge
Set S x.lemma + x1.pos + x?1.pos + x.pos
+ x1.lemma + x?1.lemma
- x.pos + x.hedge + x.dp + x.dprel
- x1.pos
- x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x.lemma + x1.lemma + x?1.lemma
- + x.dic + x1.dic + x?1.dic
- x.lemma + x1.lemma + x?1.lemma
+x2.lemma + x?2.lemma + x.dic
+ x1.dic + x?1.dic + x2.dic + x?2.dic
- x.lemma + x1.lemma
Set W x.hedge + x1.hedge + x?1.hedge
+ x2.hedge + x?2.hedge + x3.hedge
+ x?3.hedge
- x.pos + x1.pos + x?1.pos +x2.pos
+ x?2.pos + x.dic + x1.dic + x?1.dic
+ x2.dic + x?2.dic
- x.pos + x.dic
- x.num + x.dic
Table 5: Selected feature template sets
96
- x.lowSupportNoun:x | dpPathArgu.dprel.seq
- x.lowSupportNoun:x|dpPathArgu.dprel.seq
+ x.lowSupportProp:x|dpPathArgu.dprel.seq
- x.lowSupoortNoun.pos
- x.pos + x.children.dprel.bag
- x.rm.dprel + x.form
Set B x.pphead.lemma
- x.form + x.children.dprel.bag
- x.lowSupportNoun:x?dpTreeRelation
- x.lowSupportProp.lemma
- x.form + x.children.dprel.noDup
- x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportVerb.form
- x.lowSupportProp:x|dpPathShared.dprel.seq
- x.lowSupportProp:x|dpPathShared.pos.seq
- x.highSupportNoun.pos
- x.highSupportNoun:x|dpTreeRelation
- x.highSupportNoun:x|dpPathArgu.dprel.seq
Set H + x.highSupportProp:x|dpPathArgu.dprel.seq
- xlowSupportProp.lemma
- x.rm.dprel
- x.lm.form
- x.lemma + x.pphead.form
- x.lowSupportVerb.form
- x.rm.lemma + x.rm.form
- x.children.dprel.noDup
- x.children.dprel.bag
- x.highSupportNoun:x|dpTreeRelation
- x.lemma + x.pphead.form
Set S x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportVerb.form
- x.lowSupportVerb.lemma
- x.h.children.dprel.bag
- x.highSupportVerb.form
- x.lm.form
- x.lemma + x.pphead.form
- x.lm.dprel + x.pos
- x.lowSupportProp:x|dpPathPred.dprel.seq
- x.pphead.lemma
Set W x.rm.lemma
- x.lowSupportProp:x|dpTreeRelation
- x.lowSupportVerb:x|dpPathPred.dprel.seq
- x.lowSupportVerb:x|dpPathPred.pos.seq
- x.lowSupportVerb:x|dpPathShared.pos.seq
- x.lowSupportProp:x|dpPathShared.pos.seq
- x.lowSupportProp.form
Table 6: Syntactic features
- x?1.lemma
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic + x3.dic + x?3.dic
- x?1.pos + x1.pos
Set H x.rm.lemma
- x.rm.dprel
- x.lm.dprel + x.pos
- x.lowSupportNoun:x | dpPathArgu.dprel.seq
- x.lowSupportNoun:x|dpPathArgu.dprel.seq
+ x.lowSupportProp:x|dpPathArgu.dprel.seq
- x?1.lemma
- x.lemma + x1.lemma + x?1.lemma + x.dic
+ x1.dic + x?1.dic
- x.form + x.lemma + x.pos + x.dic
Set B x?2.form + x?1.form
- x.highSupportNoun:x|dpTreeRelation
- x.highSupportNoun:x|dpPathArgu.dprel.seq
- x.lowSupportProp:x|dpPathShared.dprel.seq
- x?1.lm.form
- x1.form
- x.pos + x.dic
- x.hedge + x1.hedge + x?1.hedge
- x.pos + x1.pos + x?1.pos + x2.pos + x?2.pos
Set S x.children.dprel.bag
- x.lemma + x.pphead.form
- x.highSupportVerb.form
- x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportNoun:x|dpTreeRelation + x.form
Table 7: Selected improved feature template sets
for BioScope corpus
on the in-domain data and evaluated our system
on the in-domain and cross-domain evaluation set.
All the experiments are implemented and run by
Maximum Entropy Markov Models (McCallum,
2000).
4.1 Official results
The official results for tasks are in Table 8, in
which three in-domain tests and cue matching
result for biomedical texts are listed. For the
first task for BioCorpus, our system gives F-score
0.8363 in in-domain test and for Wikipedia we
give F-score 0.5618 in closed evaluation. For the
second task, our system gives results in closed and
open test, with F-score 0.4425 and 0.4441 respec-
tively.
We compare the F-score of our system with the
best in the final result in Table 9. We rank pretty
high in Wikipedia hedge detection, while other
three are quite steady but not prominent. This is
mainly due to two reasons:
1. Feature selection procedures are not perfectly
conducted.
2. Abstracts and fulltexts in BioScope are mixed
to be the training set, which proves quite in-
appropriate when the evaluation set contains
97
only fulltext literature, since abstract and full-
text are quite different in terms of hedging.
Dataset F-score Best
Task1-closed 0.8363 0.8636
BioScope Task2-closed 0.4425 0.5732
Cue-matching 0.7853 0.8134
Wikipedia Task1-closed 0.5618 0.6017
Table 9: Comparing results with the best
4.2 Further results
Intact feature selection procedures for BioScope
corpus are conducted after official outputs collec-
tions. The results of evaluation with completely
selected features compared with the incomplete
one are given in Table 7. The system performs a
higher score on evaluation data (Table 10), which
is more competitive in both tasks on BioScope cor-
pus. The improvement for task 2 is significant, but
the increase of performance of hedge cue detec-
tion is less remarkable. We believe that a larger
fulltext training set and a more considerate train-
ing plan will help us to do better job in the future
work.
Dataset Complete Incomplete
Task1-closed 0.8522 0.8363
BioScope Task2-closed 0.5151 0.4425
Cue-matching 0.7990 0.7853
Table 10: Comparing improved outputs with the
best
5 Conclusion
We describe the system that uses sequence label-
ing with normalized feature selection and rich fea-
tures to detect hedges and find scopes for hedge
cues. Syntactic features which are derived from
dependencies are exploited, which prove to be
quite favorable. The evaluation results show that
our system is steady in performance and does
pretty good hedging and scope finding in both Bio-
Scope corpus and Wikipedia, especially when the
feature selection procedure is carefully and totally
conducted. The results suggest that sequence la-
beling and a feature-oriented method are effective
in such NLP tasks.
References
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore, 4,
August.
Ken Hyland. 1996. Writing without conviction: Hedg-
ing in science research articles. Applied Linguistics,
17:433?54.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic
role labeling of NomBank: A maximum entropy ap-
proach. In Proceedings of the EMNLP-2006, pages
138?145, Sydney, Australia.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9.
Marc Light, Xin Ying Qiu, and Padimini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proc. of the
BioLINK 2004, pages 17?24.
Chrysanne Di Marco and Robert E. Mercer. 2004.
Hedging in scientific articles as a means of classify-
ing citations. In Working Notes of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications, pages 50?54.
Andrew McCallum. 2000. Maximum entropy markov
models for information extraction and segmentation.
In Proceedings of ICML 2000, pages 591?598, Stan-
ford, California.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of HLT/EMNLP 05, pages 523?530, Vancouver,
Canada, October.
Ben Medlock and Ted Briscoe. 2008. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of 45th Annual Meeting
of the ACL, pages 992?999, Prague, Czech Repub-
lic, June.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41:636?654.
98
Dataset TP FP FN precision recall F-score
BioScope Task1-closed 669 141 121 0.8259 0.8468 0.8363
Task2-closed 441 519 592 0.4594 0.4269 0.4425
Cue-matching 788 172 259 0.8208 0.7526 0.7853
Wikipedia Task1-closed 991 303 1243 0.7658 0.4436 0.5618
Table 8: Official results of our submission for in-domain tasks
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the Workshop on BioNLP, pages 28?36,
Boulder, Colorado, June.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING-2004, pages 64?70, Geneva, Switzer-
land, August 23rd-27th.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of ACL-
IJCNLP 2009, pages 351?359, Suntec, Singapore,
2-7 August.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of BioNLP 2008,
pages 38?45, Columbus, Ohio, USA, June.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of ACL-08, pages 281?
289, Columbus, Ohio, USA, June.
Paul Thompson, Giulia Venturi, John McNaught,
Simonetta Montemagni, and Sophia Ananiadou.
2008. Categorising modality in biomedical texts. In
Proc. of the LREC 2008 Workshop on Building and
Evaluating Resources for Biomedical Text Mining,
pages 27?34, Marrakech.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-2005, pages
589?596, Ann Arbor, USA.
Nianwen Xue. 2006. Semantic role labeling of nom-
inalized predicates in Chinese. In Proceedings of
the Human Language Technology Conference of the
NAACL (NAACL-2006), pages 431?438, New York
City, USA, June.
Tong Zhang, Fred Damerau, and David Johnson. 2001.
Text chunking using regularized winnow. In Pro-
ceedings of the 39th Annual Meeting on Associa-
tion for Computational Linguistics, pages 539?546,
Toulouse, France.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009a. Multi-
lingual dependency learning: Exploiting rich fea-
tures for tagging syntactic and semantic dependen-
cies. In Proceedings of CoNLL-2009, June 4-5,
Boulder, Colorado, USA.
Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009b.
Semantic dependency parsing of NomBank and
PropBank: An efficient integrated approach via a
large-scale feature selection. In Proceedings of
EMNLP-2009, pages 30?39, Singapore.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009c. Multilingual dependency learning:
A huge feature engineering method to semantic de-
pendency parsing. In Proceedings of CoNLL-2009,
June 4-5, Boulder, Colorado, USA.
99
Dependency Parser for Chinese Constituent Parsing ?
Xuezhe Ma, Xiaotian Zhang, Hai Zhao, Bao-Liang Lu
1Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University, 800 Dong Chuan Rd., Shanghai 200240, China
{xuezhe.ma,xtian.zh}@gmail.com, {zhaohai,blu}@cs.sjtu.edu.cn
Abstract
This paper presents our work for participation
in the 2010 CIPS-ParsEval shared task on Chi-
nese syntactic constituent tree parsing. We
use dependency parsers for this constituent
parsing task based on a formal dependency-
constituent transformation method which con-
verts dependency to constituent structures us-
ing a machine learning approach. A condi-
tional random fields (CRF) tagger is adopted
for head information recognition. Our ex-
periments shows that acceptable parsing and
head tagging results are obtained on our ap-
proaches.
1 Introduction
Constituent parsing is a challenging but useful task
aiming at analyzing the constituent structure of a sen-
tence. Recently, it is widely adopted by the popular ap-
plications of natural language processing techniques,
such as machine translation (Ding and Palmer, 2005),
synonym generation (Shinyama et al, 2002), relation
extraction (Culotta and Sorensen, 2004) and lexical re-
source augmentation (Snow et al, 2004). A great deal
of researches have been conducted on this topic with
promising progress (Magerman, 1995; Collins, 1999;
Charniak, 2000; Charniak and Johnson, 2005; Sagae
and Lavie, 2006; Petrov and Klein, 2007; Finkel et al,
2008; Huang, 2008).
Recently, several effective dependency parsing al-
gorithms has been developed and shows excellent per-
formance in the responding parsing tasks (McDonald,
2006; Nivre and Scholz, 2004). Since graph struc-
tures of dependency and constituent parsing over a
sentence are strongly related, they should be benefited
from each other. It is true that constituent parsing may
be smoothly altered to fit dependency parsing. How-
ever, due to the inconvenience from dependency to
constituent structure, it is not so easy to adopt the latter
?This work is partially supported by the National Natu-
ral Science Foundation of China (Grant No. 60903119, Grant
No. 60773090 and Grant No. 90820018), the National Ba-
sic Research Program of China (Grant No. 2009CB320901),
and the National High-Tech Research Program of China
(Grant No.2008AA02Z315).
for the former. This means that most of these popular
and effective dependency parsing models can not be di-
rectly extended to constituents parsing. This paper pro-
poses an formal method for such a conversion which
adoptively solves the problem of ambiguity. Based on
the proposed method, a dependency parsing algorithm
can be used to solve tasks of constituent parsing.
A part of Tsinghua Chinese Treebank (TCT) (Zhou,
2004; Zhou, 2007; Chen et al, 2008) is used as
the training and test data for the 2010 CIPS-ParsEval
shared task. Being different from the annotation
scheme of the Penn Chinese Treebank (CTB), the TCT
has another annotation scheme, which combines both
the constituent tree structure and the head informa-
tion of each constituent. Specifically, there can be al-
ways multiple heads in a constituent. For the 2010
CIPS-ParsEval shared task, only segmented sentences
are given in test data without part-of-speech (POS)
tags, a POS tagger is required for this task. There-
fore, we divide our system into three major cascade
stages, namely POS tagging, constituent parsing and
head information recognition, which are connected as
a pipeline of processing. For the POS tagging, we
adopt the SVMTool tagger (Gimenez and Marquez,
2004); for the constituent parsing, we use the Maxi-
mum Spanning Tree (MST) (McDonald, 2006) parser
combined with a dependencies-to-constituents conver-
sion; and for the head information recognition, we ap-
ply a sequence labeling method to label head informa-
tion.
Section 2 presents the POS tagger in our approach.
The details of our parsing method is presented in sec-
tion 3. The head information recognition is described
in section 4. The data and experimental results are
shown in section 5. The last section is the conclusion
and future work.
2 POS Tagging
The SVMTool tagger (Gimenez and Marquez, 2004) is
used as our POS tagging tool for the first stage. It is a
POS tagger based on SVM classifier, written in Perl. It
can be trained on standardized collection of hand POS-
tagged sentences. It uses SVM-Light1 toolkit as the
1http://www.cs.cornell.edu/People/tj/
svm_light/.
implementation of SVM classifier and achieves 97.2%
accuracy on the Penn English Treebank. We test the
accuracy of the SVMTool tagger on the development
set of the TCT (see section 5.1) and achieve accuracy
of 94.98%.
3 Parsing Constituents Using
Dependency Parsing Algorithms
3.1 Convert Dependencies to Constituents
The conversion from constituent to dependency struc-
tures is straightforward with some specific rules based
on linguistic theory. However, there is not an effective
method which can accurately accomplish the opposite
transformation, from the dependency structures back
into constituent ones due to the existence of ambiguity
introduced by the former transformation.
Aimed at the above difficulty, our solution is to in-
troduce a formal dependency structure and a machine
learning method so that the ambiguity from depen-
dency structures to constituent structures can be dealt
with automatically.
3.1.1 Binarization
We first transform constituent trees into the form
that all productions for all subtrees are either unary or
binary, before converting them to dependency struc-
tures. Due to the binarization, the target constituent
trees of the conversion from dependency back to con-
stituent structures are binary branching.
This binarization is done by the left-factoring ap-
proach described in (Charniak et al, 1998; Petrov and
Klein, 2008), which converts each production with n
children, where n > 2, into n? 1 binary productions.
Additional non-terminal nodes introduced in this con-
version must be clearly marked. Transforming the bi-
nary branching trees into arbitrary branching trees is
accomplished by using the reverse process.
3.1.2 Using Binary Classifier
We train a classifier to decide which dependency
edges should be transformed first at each step of con-
version automatically. After the binarization described
in the previous section, only one dependency edge
should be transformed at each step. Therefore the
classifier only need to decide which dependency edge
should be transformed at each step during the conver-
sion.
As a result of the projective property of constituent
structures, this problem only happens in the cases that
modifiers are at both sides of their heads. And for these
cases that one head has multiple modifiers, only the
leftmost or the rightmost dependency edge could be
transformed first. Therefore, a binary classifier is al-
ways enough for the disambiguation at each step.
1. Word form of the parent
2. Part-of-speech (POS) tag of the parent
3. Word form of the leftmost child
4. POS tag of the leftmost child
5. Dependency label of the leftmost child
6. Word form of the rightmost child
7. POS tag of the rightmost child
8. Dependency label of the rightmost child
9. Distance between the leftmost child and
the parent
10. Distance between the rightmost child
and the parent
Table 1: Features used for conversion classifier.
Support Vector Machine (SVM) is adopted as the
learning algorithm for the binary classifier and the fea-
tures are in Table 1.
3.1.3 Convert Constituent Labels
The rest problem is that we should restore the label
for each constituent when dependency structure trees
are again converted to constituent structures. The prob-
lem is solved by storing constituent labels as labels of
dependency types. The label for each constituent is just
used as the label dependency type for each dependency
edge.
The conversion method is tested on the develop-
ment, too. Constituent trees are firstly converted into
dependency structures using the head rules described
in (Li and Zhou, 2009). Then, we transform those
trees back to constituent structure using our conversion
method and use the PARSEVAL (Black et al, 1991)
measures to evaluate the performance of the conver-
sion method. Our conversion method obtains 99.76%
precision and 99.76% recall, which is a great perfor-
mance.
3.2 Dependency Parser for Constituent Parsing
Based on the proposed conversion method, depen-
dency parsing algorithms can be used for constituent
parsing. This can be done by firstly transforming train-
ing data from constituents into dependencies and ex-
tract training instances to train a binary classifier for
dependency-constituent conversion, then training a de-
pendency parser using the transformed training data.
On the test step, parse the test data using the depen-
dency parser and convert output dependencies to con-
stituents using the binary classifier trained in advance.
In addition, since our conversion method needs depen-
dency types, labeled dependency parsing algorithms
are always required.
1. Constituent label of the constituent
2. Constituent label of each child of
the constituent.
3. Wether it is a terminal for each
child of the constituent
4. The leftmost word in the sentence
of each child of the constituent.
5. The leftmost word in the sentence
of each child of the constituent.
Table 2: CRF features for head information recogni-
tion.
1. Word form and POS tag of the parent.
2. Word form and POS tag of each child.
3. POS tag of the leftmost child of each child.
4. POS tag of the rightmost child of each child.
5. Dependency label between the parent and
its parent
Table 3: CRF features for dependency type labeling.
4 Head Information Recognition
Since head information of each constituent is always
determined by the syntactic label of its own and the
categories of the constituents in subtrees, the order and
relations between the productions of each constituent
strongly affects the head information labeling. It is
natural to apply a sequential labeling strategy to tackle
this problem. The linear chain CRF model is adopted
for the head information labeling, and the implemen-
tation of CRF model we used is the 0.53 version of
the CRF++ toolkit2. We assume that head information
is independent between different constituents, which
could decrease the length of sequence to be labeled for
the CRF model.
We use a binary tag set to determine whether a con-
stituent is a head, e.g. H for a head, O for a non-head,
which is the same as (Song and Kit, 2009). The fea-
tures in Table 2 are used for CRF model.
To test our CRF tagger, we remove all head informa-
tion from the development set, and use the CRF tagger
to retrieve the head. The result strongly proves its ef-
fectiveness by showing an accuracy of 99.52%.
5 Experiments
All experiments reported here were performed on a
Core 2 Quad 2.83Ghz CPU with 8GB of RAM.
2The CRF++ toolkit is publicly available from
http://crfpp.sourceforge.net/.
5.1 Data
There are 37,219 short sentences in official released
training data for the first sub-task and 17,744 long sen-
tences for the second sub-task (for the second sub-task,
one line in the training data set may contain more than
one sentence). We split one eighth of the data as our
development set. On the other hand, there are both
1,000 sentences in released test data for the first and
second sub-tasks.
5.2 Constituent Parsing
As mentioned in section 3, constituent parsing is
done by using a dependency parser combined with
our conversion method. We choose the second or-
der maximum spanning tree parser with k-best online
large-margin learning algorithm (Crammer and Singer,
2003; Crammer et al, 2003). The MST parser we use
is in the form of an open source program implemented
in C++3.
The features used for MST parser is the same as
(McDonald, 2006). Both the single-stage and two-
stage dependency type labeling approaches are applied
in our experiments. For the two-stage dependency type
labeling, The linear chain CRF model is adopted in-
stead of the first-order Markov model used in (McDon-
ald, 2006). The features in Table 3 are used for CRF
model. It takes about 7 hours for training the MST
parser, and about 24 hours for training the CRF model.
As mentioned in section 3.1.2, SVM is adopted as
the learning algorithm for the binary classifier. There
are about 40,000 training instances in the first sub-task
and about 80,000 in the second sub-task. Develop-
ment sets are used for tuning parameter C of SVM
and the training time of the SVM classifier for the first
and second sub-task is about 8 and 24 hours, respec-
tively. However, the conversion from dependencies to
constituents is extremely fast. Converting more than
2,000 trees takes less than 1 second.
To transform the constituent trees in training set into
dependency structures, we use the head rules of (Li and
Zhou, 2009).
5.3 Results
The evaluation metrics used in 2010 CIPS-ParsEval
shared task is shown in following:
1. syntactic parsing
Precision = number of correct constituents in proposed parsenumber of constituents in proposed parse
Recall = number of correct constituents in proposed parsenumber of constituents in standard parse
F1 = 2*Precision*RecallPrecision+Recall
3The Max-MSTParser is publicly available from
http://max-mstparser.sourceforge.net/.
without head with head
Precision Recall F1 Precision Recall F1
single-stage 77.78 78.13 77.96 75.78 76.13 75.95
two-stage 78.61 78.76 78.69 76.61 76.75 76.68
Table 4: Official scores of syntactic parsing. single-stage and two-stage are for single-stage and two-stage depen-
dency type labeling approached, respectively.
Micro-R Macro-R
single-stage 62.74 62.47
two-stage 63.14 62.48
Table 5: Official scores of event recognition
The correctness of syntactic constituents is judged
based on the following two criteria:
(a) the boundary, the POS tags of all the words
in the constituent and the constituent type la-
bel should match that of the constituent in
the gold standard data.
(b) the boundary, the POS tags of all the words
in the constituent, the constituent type la-
bel and head child index of the constituent
should match that of the constituent in the
gold standard data. (if the constituent con-
tains more than one head child index, at least
one of them should be correct.)
2. event pattern recognition
Micro-R = number of all correct events in proposed parsenumber of all events in standard parse
Macro-R = sum of recall of different target verbsnumber of target verbs
Here the event pattern of a sentence is defined to
be the sequence of event blocks controlled by the
target verb in a sentence. The criteria for judging
the correctness of event pattern recognition is:
? the event pattern should be completely con-
sistent with gold standard data (information
of each event block should completely match
and the order of event blocks should also
consistent).
There are both two submissions for the first and sec-
ond sub-tasks. One is using the single-stage depen-
dency type labeling and the other is two-stage. Since
there are some mistakes in our models for the second
sub-task, the results of our submissions are unexpect-
edly poor and are not shown in this paper. All the re-
sults in this paper is reported by the official organizer
of the 2010 CIPS-ParsEval shared task.
The accuracy of POS tagging on the official test data
is 92.77%. The results of syntactic parsing for the first
sub-task is shown in Table 4. And results of event
recognition is shown in Table 5.
From the Table 4 and 5, we can see that our system
achieves acceptable parsing and head tagging results,
and the results of event recognition is also reasonably
high.
5.4 Comparison with Previous Works
We comparison our approach with previous works of
2009 CIPS-ParsEval shared task. The data set and
evaluation measures of 2009 CIPS-ParsEval shared
task, which are quite different from that of 2010 CIPS-
ParsEval shared task, are used in this experiment for
the comparison purpose. Table 6 shows the compari-
son.
We compare our method with several main parsers
on the official data set of 2009 CIPS-ParsEval shared
task. All these results are evaluated with official
evaluation tool by the 2009 CIPS-ParsEval shared
task. Bikel?s parser4 (Bikel, 2004) in Table 6 is
a implementation of Collins? head-driven statistical
model (Collins, 2003). The Stanford parser5 is based
on the factored model described in (Klein and Man-
ning, 2002). The Charniak?s parser6 is based on the
parsing model described in (Charniak, 2000). Berke-
ley parser7 is based on unlexicalized parsing model de-
scribed in (Petrov and Klein, 2007). According to Ta-
ble 6, the performance of our method is better than all
the four parsers described above. Chen et al (2009)
and Jiang et al (2009) both make use of combination
of multiple parsers and achieve considerably high per-
formance.
4http://www.cis.upenn.edu/?dbikel/
software.html
5http://nlp.stanford.edu/software/
lex-parser.shtml/
6ftp://ftp.cs.brown.edu/pub/nlparser/
7http://nlp.cs.berkeley.edu/Main.html
F1
Bikel?s parser 81.8
Stanford parser 83.3
Charniak?s parser 83.9
Berkeley parser 85.2
this paper 85.6
Jiang et al(2009). 87.2
Chen et al(2009). 88.8
Table 6: Comparison with previous works
6 Conclusion
This paper describes our approaches for the parsing
task in CIPS-ParsEval 2010 shared task. A pipeline
system is used to solve the POS tagging, constituent
parsing and head information recognition. SVMTool
tagger is used for the POS tagging. For constituent
parsing, we proposes a conversion based method,
which can use dependency parsers for constituent pars-
ing. MST parser is chosen as our dependency parser.
A CRF tagger is used for head information recognition.
The official scores indicate that our system obtains ac-
ceptable results on constituent parsing and high perfor-
mance on head information tagging.
One of future work should apply parser combination
and reranking approaches to leverage this in producing
more accurate parsers.
References
Bikel, Daniel M. 2004. Intricacies of collins parsing
model. Computational Linguistics, 30(4):480?511.
Black, Ezra W., Steven P. Abney, Daniel P. Flickinger,
Cluadia Gdaniec, Ralph Grishman, Philio Harrison,
Donald Hindle, Robert J.P. Inqria, Frederick Jelinek,
Judith L. Klavans, Mark Y. Liberman, Mitchell P.
Marcus, Salim Roukos, and B Santorini. 1991. A
procedure for quantitatively comparing the syntac-
tic coverage of english grammars. In Proceedings
of the February 1991 DARPA Speech and Natural
Language Workshop.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the 43rd ACLL, pages
132?139.
Charniak, Eugene, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139, seattle, WA.
Chen, Yi, Qiang Zhou, and Hang Yu. 2008. Anal-
ysis of the hierarchical Chinese funcitional chunk
bank. Journal of Chinese Information Processing,
22(3):24?31.
Chen, Xiao, Changning Huang, Mu Li, and Chunyu
Kit. 2009. Better parser combination. In CIPS-
ParsEval-2009 shared task.
Collins, Michael. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learining.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2003. Online
passive aggressive algorithms. In Proceedings of
NIPS.
Culotta, Aron and Jeffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In Pro-
ceedings of ACL.
Ding, Yuan and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL.
Finkel, Jenny Rose, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. pages 959?967, The
Ohio State University, Columbus, Ohio, USA.
Gimenez and Marquez. 2004. Svmtool: A general
POS tagger generator based on support vector ma-
chines. In Proceedings of the 4th International Con-
ference of Language Resources and Evaluation, Lis-
bon, Portugal.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL/HLT.
Jiang, Wenbin, Hao Xiong, and Qun Liu. 2009. Muti-
path shift-reduce parsing with online training. In
CIPS-ParsEval-2009 shared task.
Klein, Dan and Christopher Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In In Advances in NIPS 2002, pages
3?10.
Li, Junhui and Guodong Zhou. 2009. Soochow uni-
versity report for the 1st china workshop on syntac-
tic parsing. In CIPS-ParsEval-2009 shared task.
Magerman, David M. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL, pages
276?283, MIT, Cambridge, Massachusetts, USA.
McDonald, Ryan. 2006. Discriminative Learning
Spanning Tree Algorithm for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Nivre, Joakim and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics (COLING-2004), pages 64?70,
Geneva, Switzerland, August 23rd-27th.
Petrov, Slav and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404?411, Rochester, New York.
Petrov, Slav and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. In Proceed-
ings of NIPS 20.
Sagae, Kenji and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of COL-
ING/ACL, pages 689?691, Sydney, Australia.
Shinyama, Yusuke, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In HLT-2002.
Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng.
2004. Learning syntactic patterns for automatic hy-
pernym discovery. In Proceedings of NIPS.
Song, Yan and Chunyu Kit. 2009. PCFG parsing with
crf tagging for head recognition. In CIPS-ParsEval-
2009 shared task.
Zhou, Qiang. 2004. Annotation scheme for Chinese
treebank. Journal of Chinese Information Process-
ing, 18(4):1?8.
Zhou, Qiang. 2007. Base chuck scheme for the Chi-
nese language. Journal of Chinese Information Pro-
cessing, 21(3):21?27.
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 152?156,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Regression with Phrase Indicators for Estimating MT Quality?
Chunyang Wu Hai Zhao?
Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering, Shanghai Jiao Tong University
chunyang506@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn
Abstract
We in this paper describe the regression sys-
tem for our participation in the quality estima-
tion task of WMT12. This paper focuses on
exploiting special phrases, or word sequences,
to estimate translation quality. Several feature
templates on this topic are put forward sub-
sequently. We train a SVM regression model
for predicting the scores and numerical results
show the effectiveness of our phrase indicators
and method in both ranking and scoring tasks.
1 Introduction
The performance of machine translation (MT) sys-
tems has been considerable promoted in the past two
decades. However, since the quality of the sentence
given by MT decoder is not guaranteed, an impor-
tant issue is to automatically predict or identify its
characteristics. Recent studies on quality estimation
or confidence estimation have focused on measuring
the translating quality at run-time, instead of involv-
ing reference corpus. Researches on this topic con-
tribute to offering advices or warnings for users even
without knowledge about either side of languages
?This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No. 60903119 and
Grant No. 61170114), the National Research Foundation for
the Doctoral Program of Higher Education of China under
Grant No. 20110073120022, the National Basic Research Pro-
gram of China (Grant No. 2009CB320901), the Science and
Technology Commission of Shanghai Municipality (Grant No.
09511502400), and the European Union Seventh Framework
Program (Grant No. 247619).
?corresponding author
and illuminating some other potential MT applica-
tions.
This paper describes the regression system for our
participation in the WMT12 quality estimation task.
In this shared task, we analyzed the pattern of trans-
lating errors and studied on capturing such patterns
among the corpus. The basic objective in this pa-
per is to recognize those phrases, or special word se-
quence combinations which can indicate the quality
of a translation instance. By introducing no exter-
nal NLP toolkits, we exploited several feasible tech-
niques to extract such patterns directly on the cor-
pus. One contribution of this paper is those feature
templates on the basis of this topic. Numerical re-
sults show their positive effects on both ranking and
scoring subtasks.
The rest of this paper is organized as follows: In
Section 2, we show the related work. In Section
3, we specify the details of our system architecture.
The experimental results are reported in Section 4.
Finally, the conclusion is given in Section 5.
2 Related Work
Compared with traditional MT metrics such as
BLEU (Papineni et al, 2002), the fundamental goal
of quality estimation (QE) is predicting the quality
of output sentences without involving reference sen-
tences.
Early works (Quirk, 2004; Gamon et al, 2005)
have demonstrated the consistency of the automatic
score and human evaluation. Several further works
aimed at predicting automatic scores in order to bet-
ter select MT n-best candidates (Specia and Farzin-
dar, 2010), measure post-editing effort (Specia et
152
al., 2011) or combine SMT and TM systems (He et
al., 2010). Instead of estimating on word or sen-
tence levels, Soricut and Echihabi (2010) proposed
a document-level ranking system which grants the
user to set quality threshold. Besides, recent stud-
ies on the QE topic introduced syntactic and linguis-
tic information for better estimating the quality such
as dependency preservation checking (Bach et al,
2011).
3 System Description
We specify the details of our system in this section.
Following previous approaches for quality estima-
tion, it is first trained on the corpus with labeled
quality scores and then it is able to predict the score
for unlabeled instances.
A major challenge for this estimating task is to ex-
ploit effective indicators, or features, to identify the
quality of the translating results. In this paper, all the
features are extracted from the official corpora, in-
volving no external tools such as pre-trained parsers
or POS taggers. Most of the feature templates focus
on special phrases or word sequences. Some of the
phrases could introduce translation errors and other-
s might declare the merit of the MT output. Their
weights are automatically given by the regressor.
3.1 Regression Model
For obtaining MT quality predictor, we utilize
SVM light (Joachims, 1999)1 to train this regression
model. The radial basis function kernel is chosen as
the kernel of this model. The label for each instance
is the score annotated manually and the input vector
consists of a large amount of indicators described in
Section 3.2.
3.2 Features
For training the regression model, we utilize the
17 baseline features: number of source/target to-
kens, average source token length, source/target
LM probability, target-side average of target word
occurrences, original/inverse frequency average of
translations per source word, source/target percent-
age of uni-/bi-/tri-grams in quartile 1 or 4, source
percentage of unigrams in the training corpus and
1http://svmlight.joachims.org/
source/target number of punctuation. Besides, sev-
eral features and templates are proposed as follows:
? Inverted Automatic Scores: For each Span-
ish system output sentence, we translate it to
English and get its scores of BLEU and ME-
TEOR (Denkowski and Lavie, 2011). These s-
cores are treated as features named inverted au-
tomatic scores. In order to obtain these numer-
als, we train a Spanish-to-English phrase-based
Moses2 (Koehn et al, 2007) decoder with de-
fault parameters on the official parallel corpus.
The original training corpus is split into a de-
veloping set containing the last 3000 sentence
pairs at the end of the corpus and a training set
with the remained pairs. The word alignment
information is generated by GIZA++ (Och and
Ney, 2003) and the feature weights are tuned on
the developing set by Z-MERT (Zaidan, 2009).
? Minimal/Maximal link likelihood of gener-
al language model: In the word graph of
each decoding instance, denote the minimal
and maximal general language model likeli-
hood of links as lmin and lmax. We treat
exp(lmin) and exp(lmax) as features respec-
tively.
? Trace Density: Define the trace density ?T as
the quotient of decoding trace length and sen-
tence length:
?T = TraceLength / SentenceLength. (1)
? Average of Phrase Length: This feature is
also obtained from the decoding trace informa-
tion.
? Number of Name Entity: This feature can-
not be obtained exactly due to the resource con-
strains. We in this task count the number of the
word whose first letter is capitalized, and that
is not the first word in the sentence.
We also extract several special phrases or se-
quences. The total of each phrase/sequence type
and each pattern are respectively defined as features.
When an instance matches a pattern, the entry rep-
resenting this pattern in its vector is set to |1/Z|. In
2http://www.statmt.org/moses/
153
this paper the regressor term Z is the size of the tem-
plate which the pattern belongs to. The detail de-
scription of such templates is presented as follows:
? Reference 2?5-grams: All the 2?5-grams of
the reference sentences provided in the official
data are generated as features.
? Bi-gram Source Splitting: This template
comes from the GIZA alignment document.
We scan the parallel corpus: for each bi-gram
in the source sentence, if its words? counter-
parts in the target side are separated, we add it
to the bi-gram source splitting feature template.
The part-of-speech tags of the words seem to be
effective to this task. Since it is not provided, we uti-
lize a trick design for obtaining similar information:
? Target Functional Word Patterns: On the
target corpus, we scan those words whose
length is smaller than or equal to three. Such
a word w is denoted as functional word. Any
bi-gram in the corpus starting or ending with w
is added to a dictionary D. For each system-
output translation instance, we compare the
analogous bi-grams in it with this dictionary,
all bi-grams not in D are extracted as features.
Denote the collection of 2?5-grams of the
system-output sentences scored lower than 1.5 as B;
that with scores higher than 4.5 as G. Here the ?s-
core? is the manual score provided in the official re-
source.
? Target Bad 2?5-grams: B?G
? Target Good 2?5-grams: G? B
? Source Bad/Good 2?5-grams: Analogous
phrases on the source side are also extracted
by the same methods as Target Bad/Good n-
grams.
For each output-postedit sentence pair, we con-
struct a bipartite graph by aligning the same words
between these two sentences. By giving a maximal
matching, the output sentence can be split to several
segments by the unmatched words.
? Output-Postedit Different 2?5-grams: For
each unaligned segments, we attach the previ-
ous word to the left side and the next word to
the right. 2?5-grams in this refined segment
are extracted as features.
? Output-Postedit Different Anchor: Denote
the refined unaligned segment as
sr = (prevWord, s1, s2, . . . , sn, nextWord).
A special sequence with two word segments
prevWord s1 . . . sn nextWord
is given as a feature.
In the source-side scenario with the inverted trans-
lations, similar feature templates are extracted as
well:
? Source-Invert Different 2?5-grams/Anchor
A significant issue to be considered in this shared
task is that the training data set is not a huge one,
containing about two thousand instances. Although
carefully designed, the feature templates however
cannot involve enough cases. In order to overcome
this drawback, we adopt the following strategy:
For any template T, we compare its patterns with
the items in the phrase table. If the phrase item p is
similar enough with the pattern g, p is added to the
template T. Two similarity metrics are utilized: De-
note the longest common sequence as LCSQ(p, g)
and the longest common segment as LCSG(p, g) 3,
LCSQ(p, g)2
|p||g|
> 0.6, (2)
LCSG(p, g) ? 3. (3)
Besides, when training the regression model or
testing, the entry representing the similar items in
the feature vector are also set to 1/|template size|.
4 Experiments
4.1 Data
In order to conduct this experiment, we randomly
divide the official training data into two parts: one
3To simplify, the sequence allows separation while the seg-
ment should be contiguous. For example, LCSQ(p, g) and
LCSG(p, g) for ?I am happy? and ?I am very happy? are ?I,
am, happy? and ?I, am?, respectively.
154
Data Set
Score Distribution
[1-2) [2-3) [3-4) [4-5)
Original 3.2% 24.1% 38.7% 34.0%
Train 3.2% 24.2% 38.3% 34.4%
Dev 3.3% 24.0% 40.3% 32.4%
Table 1: The comparison of the score distributions among
three data sets: Original, Training (Train) and Develop-
ment (Dev).
Ranking Scoring
DA SC MAE RMSE
Baseline 0.47 0.49 0.61 0.79
This paper 0.49 0.52 0.60 0.77
Table 2: The experiment results on the ranking and scor-
ing tasks. In this table, DA, SC, MAE and RMSE are
DeltaAvg, Spearman Correlation, Mean-Average-Error
and Root-Mean-Squared-Error respectively.
training set with about 3/4 items and one develop-
ment set with the other 1/4 items. The comparison
of the score distribution among these data sets is list-
ed in Table 1.
4.2 Results
The baseline of this experiment is the regression
model trained on the 17 baseline features. The pa-
rameters of the classifier are firstly tuned on the
baseline features. Then the settings for both the
baseline and our model remain unchanged. The
numerical results for the ranking and scoring tasks
are listed in Table 2. The ranking task is evalu-
ated on the DeltaAvg metric (primary) and Spear-
man correlation (secondary) and the scoring task is
evaluated on Mean-Average-Error and Root-Mean-
Squared-Error. For the ranking task, our system out-
performs 0.02 on DeltaAvg and 0.03 on Spearman
correlation; for the scoring task, 0.01 lower on MAE
and 0.02 lower on RMSE.
The official evaluation results are listed in Table 3.
The official LibSVM4 model is a bit better than our
submission. Our system was further improved af-
ter the official submission. Different combinations
of the rates defined in Equation 2?3 and regressor
parameter settings are tested. As a result, the ?Re-
fined? model in Table 3 is the results of the refined
4http://www.csie.ntu.edu.tw/ cjlin/libsvm/
Ranking Scoring
DA SC MAE RMSE
SJTU 0.53 0.53 0.69 0.83
Official SVM 0.55 0.58 0.69 0.82
Refined 0.55 0.57 0.68 0.81
Best Workshop 0.63 0.64 0.61 0.75
Table 3: The official evaluation results.
version. Compared with the official model, it gives
similar ranking results and performs better on the s-
coring task.
5 Conclusion
We presented the SJTU regression system for the
quality estimation task in WMT 2012. It utilized
a support vector machine approach with several fea-
tures or feature templates extracted from the decod-
ing and corpus documents. Numerical results show
the effectiveness of those features as indicators for
training the regression model. This work could be
extended by involving syntax information for ex-
tracting more effective indicators based on phrases
in the future.
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011.
Goodness: a method for measuring machine transla-
tion confidence. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, HLT
?11, pages 211?219, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level mt evaluation without reference
translations: Beyond language modeling. In In Euro-
pean Association for Machine Translation (EAMT).
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
155
Thorsten Joachims. 1999. Advances in kernel method-
s. chapter Making large-scale support vector machine
learning practical, pages 169?184. MIT Press, Cam-
bridge, MA, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the Association for Computational Lin-
guistics Companion Demo and Poster Sessions, pages
177?180, Prague, Czech Republic. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computation-
al Linguistics, ACL ?02, pages 311?318, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Christopher B. Quirk. 2004. Training a sentence-level
machine translation confidence metric. LREC, 4:2004.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations vi-
a ranking. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 612?621, Uppsala, Sweden, July. Association
for Computational Linguistics.
Lucia Specia and Atefeh Farzindar. 2010. Estimat-
ing machine translation post-editing effort with hter.
In AMTA 2010- workshop, Bringing MT to the User:
MT Research and the Translation Industry. The Ninth
Conference of the Association for Machine Transla-
tion in the Americas, nov.
Lucia Specia, Hajlaoui N., Hallett C., and Aziz W. 2011.
Predicting machine translation adequacy. In Machine
Translation Summit XIII.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
156
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 95?99,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Chinese Coreference Resolution via Ordered Filtering?
Xiaotian Zhang1,2 Chunyang Wu1,2 Hai Zhao1,2?
1Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University
xtian.zh@gmail.com, chunyang506@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn
Abstract
We in this paper present the model for our
participation (BCMI) in the CoNLL-2012
Shared Task. This paper describes a pure
rule-based method, which assembles dif-
ferent filters in a proper order. Different
filters handle different situations and the
filtering strategies are designed manually.
These filters are assigned to different or-
dered tiers from general to special cases.
We participated in the Chinese and En-
glish closed tracks, scored 51.83 and 59.24
respectively.
1 Introduction
In this paper, we describes the approaches we u-
tilized for our participation in the CoNLL-2012
Shared Task. This year?s shared task targets at
modeling coreference resolution for multiple lan-
guages. Following (Lee et al, 2011), we extend-
s the methodology of deterministic coreference
model, using manually designed rules to rec-
ognize expressions with corresponding entities.
The deterministic coreference model (Raghu-
? This work was partially supported by the Na-
tional Natural Science Foundation of China (Grant No.
60903119 and Grant No. 61170114), the National Re-
search Foundation for the Doctoral Program of Higher E-
ducation of China under Grant No. 20110073120022, the
National Basic Research Program of China (Grant No.
2009CB320901), the Science and Technology Commission
of Shanghai Municipality (Grant No. 09511502400), and
the European Union Seventh Framework Program (Grant
No. 247619).
? Corresponding author.
nathan et al, 2010) has shown good perfor-
mance in the shared task of CoNLL-2011. This
kind of model focuses on filtering with ordered
tiers: One filter is applied at one time, from
highest to lowest precision. However, compared
with learning approaches (Soon et al, 2001), s-
ince effective rules are quite heterogeneous in
different languages, several filtering methods
should be redesigned when different languages
are considered. We modified the original Stan-
ford English coreference system1 to adapt to the
Chinese scenario. For the English participation,
we implemented the full strategies and interface
of the semantic-based filters which are not ob-
tained from the open source toolkit.
The rest of this paper is organized as follows:
In Section 2, we review the related work; In Sec-
tion 3, we describe the detail of our model of
handling coreference resolution in Chinese; Ex-
periment results are reported in Section 4 and
the conclusion is presented in Section 5.
2 Related Work
Many existing works have been published on
learning relation extractors via supervised (Soon
et al, 2001) or unsupervised (Haghighi and K-
lein, 2010; Poon and Domingos, 2008) approach-
es. For involving semantics, (Rahman and Ng,
2011) proposed a coreference resolution model
with world knowledge; By using word associa-
tions, (Kobdani et al, 2011) showed its effec-
tiveness to coreference resolution. Compared
1http://nlp.stanford.edu/software/dcoref.shtml
95
with machine learning methods, (Raghunathan
et al, 2010) proposed rule-base models which
have been witnessed good performance.
Researchers began to work on Chinese coref-
erence resolution at a comparatively late date
and most of them adopt a machine learning
approach. (Guochen and Yunfei, 2005) based
their Chinese personal pronoun coreference res-
olution system on decision trees and (Naiquan et
al., 2009) realized a Chinese coreference resolu-
tion system based on maximum entropy model.
(Weixuan et al, 2010) proposes a SVM-based
approach to anaphora resolution of noun phrases
in Chinese and achieves the F-measure of 63.3%
in the evaluation on ACE 2005. (Guozhi et al,
2011) presented a model for personal pronouns
anaphora resolution based on corpus,which us-
ing rule pretreatment combined with maximum
entropy.
3 Model for Chinese
In general, we adapt Stanford English corefer-
ence system to Chinese by making necessary
changes. The sketch of this deterministic model
is to extract mentions and relevant information
firstly; then several manually designed rules, or
filtering sieves are applied to identify the corefer-
ence. Moreover, these sieves are utilized in a pre-
designed order, which are sorted from highest to
lowest precision. The ordered filtering sieves are
listed in Table 1.
Ordered Sieves
1. Mention Detection Sieve
2. Discourse Processing Sieve
3. Exact String Match Sieve
4. Relaxed String Match Sieve
5. Precise Constructs Sieve
6. Head Matching Sieves
7. Proper Head Word Match Sieve
8. Pronouns Sieve
9. Post-Processing Sieve
Table 1: Ordered filtering sieves for Chinese. Modi-
fied sieves are bold.
We remove the semantic-based sieves due to
the resource constraints. The simplified version
consists of nine filtering sieves. The bold ones
in Table 1 are the modified sieves for Chinese.
First of all, we adopt the head finding rules for
Chinese used in (Levy and Manning, 2003), and
this affects sieve 4, 6 and 7 which are all take
advantage of the head words. And our change
to other sieves are described as follows.
? Mention Detection Sieve: We in this
sieve first extract all the noun phrases,
pronouns (the words with part-of-speech
(POS) tag PN), proper nouns (the word-
s with POS tag NR) and named entities.
Thus a mention candidate set is produced.
We then refine this set by removing several
types of candidates listed as follows:
1. Themeasure words, a special word pat-
tern in Chinese such as ? ?? (a year
of), ???? (a ton of).
2. Cardinals, percents and money.
3. A mention if a larger mention with the
same head word exists.
? Discourse Processing & Pronouns
Sieve: In these two sieves, we adapt
the common pronouns to Chinese. It in-
cludes ?\? (you), ??? (I or me),?? (he
or him),??? (she or her),??? (it),?\??
(plural of ?you?), ???? (we or us),???
(they, gender: male),???? (they, gender:
female),???? (plural of ?it?) and relative
pronoun ?gC? (self). Besides these, we
enrich the pronouns set by adding ?4?, ?4
??, ? T? and ?T?? which are more often
to appear in spoken dialogs as first person
pronouns and ? s? which is used to show
respect for ?you? and the third person pro-
noun ???.
Besides, for mention processing of the original
system, whether a mention is singular or plural
should be given. Different from English POS
tags, in Chinese plural nouns couldn?t be distin-
guished from single nouns in terms of the POS.
Therefore, we add two rules to judge whether a
noun is plural or not.
? A noun that ends with ??? (plural marker
for pronouns and a few animate nouns), and
?? (and so on) is plural.
96
? A noun phrase that involves the coordinat-
ing conjunction words such as ? ?? (and)
is plural.
4 Experiments
4.1 Modification for the English system
We implement the semantic-similarity sieves
proposed in (Lee et al, 2011) with the WordNet.
These modifications consider the alias sieve and
lexical chain sieve. For the alias sieve, two men-
tions are marked as aliases if they appear in the
same synset in WordNet. For the lexical chain
sieve, two mentions are marked as coreference if
linked by a WordNet lexical chain that traverses
hypernymy or synonymy relations.
4.2 Numerical Results
Lang. Coref Anno. R P F
Ch
Before gold 87.78 40.63 55.55auto 80.37 38.95 52.47
After gold 69.56 62.77 65.99auto 65.02 59.76 62.28
En
Before gold 93.65 42.32 58.30auto 88.84 40.17 55.32
After gold 77.49 74.59 76.01auto 72.88 74.53 73.69
Table 2: Performance of the mention detection com-
ponent, before and after coreference resolution, with
both gold and auto linguistic annotations on devel-
opment set.
Lang. R P F
Ch 61.11 62.12 61.61
En 75.23 72.24 73.71
Table 3: Performance of the mention detection com-
ponent, after coreference resolution, with auto lin-
guistic annotations on test set.
Table 2 shows the performance of mention de-
tection both before and after the coreference res-
olution with gold and predicted linguistic anno-
tations on development set. The performance of
mention detection on test set is presented in Ta-
ble 3. The recall is much higher than the preci-
sion so as to make sure less mentions are missed,
Metric R P F1 avg F1
Ch
MUC 50.02 49.64 49.83
51.83BCUBED 65.81 65.50 65.66CEAF (M) 49.88 49.88 49.88
CEAF (E) 40.39 43.47 41.88
BLANC 67.12 65.83 66.45
En
MUC 64.08 63.57 63.82
59.24BCUBED 66.45 70.71 68.51CEAF (M) 57.24 57.24 57.24
CEAF (E) 45.13 45.67 45.40
BLANC 71.12 77.92 73.95
Table 5: Results on the official test set (closed track).
and because spurious mentions will be left as s-
ingletons and removed at last, a low precision
will not affect the final result. The performance
of mention detection for Chinese is worse than
that of English, and this is a direction for future
improvement for Chinese.
Our results on the development set for both
languages are listed in Table 4 and the official
test results are in Table 5. Avg F1 is the arith-
metic mean of MUC, B3, and CEAFE.
We further examine the performance by test-
ing on different data types (broadcast con-
versations, broadcast news, magazine articles,
newswire, conversational speech, and web da-
ta) of the development set, and the results are
shown in Table 6. The system do better on bn,
mz, tc than bc, nw, wb for both Chinese and
English. And it performs the worst on wb due
to a relative lower recall in mention detection.
For Chinese, we also compare the performance
when handling the three different mention types,
proper nominal, pronominal, and other nominal.
Table 7 shows the scores output by the official
scorer when only each kind of mentions are pro-
vided in the keys file and response file each time
and both the quality of the coreference links a-
mong the nominal of each mention type and the
corresponding performance of mention detection
are presented. The performance of coreference
resolution among proper nominal and pronomi-
nal is significant higher than that of other nom-
inal which highly coincides with the results in
Table 6.
97
MUC BCUBED CEAF (E) avg F1Lang. Setting R P F1 R P F1 R P F1
Ch
AUTO 52.38 47.44 49.79 68.25 62.36 65.17 37.43 41.89 39.54 51.50
GOLD 58.16 53.55 55.76 70.66 68.65 69.64 41.44 45.60 43.42 56.27
GMB 63.60 87.63 73.70 62.71 88.32 73.34 74.08 42.83 54.28 67.11
En
AUTO 64.24 64.95 64.59 68.22 73.16 70.60 47.03 46.29 46.66 60.61
GOLD 67.45 66.94 67.20 69.76 73.62 71.64 47.86 48.42 48.14 62.33
GMB 71.78 90.55 80.08 65.45 88.95 75.41 77.42 46.47 58.08 71.19
Table 4: Results on the official development set (closed track). GMB stands for Gold Mention Boundaries
Lang. Anno. bc bn mz nw pt tc wb
Ch AUTO 50.31 53.87 52.80 47.82 - 55.10 47.54GOLD 53.19 63.63 58.23 50.65 - 58.96 50.15
En AUTO 59.26 62.40 63.17 57.57 65.24 60.91 56.88GOLD 60.34 64.51 64.36 59.71 67.07 62.44 58.47
Table 6: Results (Avg F1) on different data types of the development set (closed track).
Proper nominal Pronominal Other nominal
Data Type MD (Recall) avg F1 MD (Recall) avg F1 MD (Recall) avg F1
bc 94.5 (550/582) 68.06 94.5 (1372/1452) 66.40 80.5 (1252/1555) 47.74
bn 96.7 (1213/1254) 67.46 97.8 (264/270) 77.39 83.7 (1494/1786) 53.51
mz 92.0 (526/572) 67.05 94.8 (91/96) 56.89 76.1 (834/1096) 53.68
nw 91.4 (402/440) 67.44 90.6 (29/32) 83.54 51.0 (1305/2559) 44.86
tc 100 (23/23) 95.68 84.5 (572/677) 61.96 71.2 (272/382) 53.88
wb 93.2 (218/234) 72.23 95.9 (397/414) 72.55 77.1 (585/759) 43.37
all 94.4 (2932/3105) 68.30 92.7 (2725/2941) 68.10 70.6 (5742/8137) 49.56
Table 7: Results ( Recall of mention detection and Avg F1) on different data types and different mention
types of the development set with linguistic annotations (closed track).
98
5 Conclusion
We presented the rule-base approach for the BC-
MI?s participation in the shared task of CoNLL-
2012. We extend the work by (Lee et al, 2011)
and modified several tiers to adapt to Chinese.
Numerical results show the effectiveness in the
evaluation for Chinese and English. For the
Chinese scenario, we firstly show it is possible
to consider special POS-tags and common pro-
nouns as indicators for improving the perfor-
mance. This work could be extended by involv-
ing more feasible filtering tiers or utilizing some
automatic rule generating methods.
References
Li Guochen and Luo Yunfei. 2005. ?^`k?J?
???<?????). (Personal pronoun
coreference resolution in Chinese using a priority
selection strategy). Journal of Chinese Informa-
tion Processings, pages 24?30.
Dong Guozhi, Zhu Yuquan, and Cheng Xianyi. 2011.
Research on personal pronoun anaphora resolution
in chinese. Application Research of Computers,
28:1774?1776.
Aria Haghighi and Dan Klein. 2010. Coreference
resolution in a modular, entity-centered model. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT
?10, pages 385?393, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Hamidreza Kobdani, Hinrich Schu?tze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrapping
coreference resolution using word associations. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
783?792, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2011. Stanford multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 28?34, Portland, Oregon,
USA, June. Association for Computational Lin-
guistics.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank?
In Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics - Volume
1, ACL ?03, pages 439?446, Stroudsburg, PA, US-
A. Association for Computational Linguistics.
Hu Naiquan, Kong Fang, Wang Haidong, and Zhou
Guodong. 2009. Realization on chinese corefer-
ence resolution system based on maximum entropy
model. Application Research of Computers, pages
2948?2951.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with markov log-
ic. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 650?659, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan
Rangarajan, Nate Chambers, Mihai Surdeanu,
Dan Jurafsky, and Christopher Manning. 2010.
A multi-pass sieve for coreference resolution. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
492?501, Cambridge, MA, October. Association
for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2011. Coreference
resolution with world knowledge. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 814?824,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Y-
ong Lim. 2001. A machine learning approach to
coreference resolution of noun phrases. Comput.
Linguist., 27(4):521?544, December.
Tan Weixuan, Kong Fang, Wang Haidong, and Zhou
Guodong. 2010. Svm-based approach to chinese
anaphora resolution. High Performance Comput-
ing Technology, pages 30?36.
99
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 118?121,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Hybrid Rule-based Algorithm for Coreference Resolution ?
Heming Shou1,2 Hai Zhao1,2?
1Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University
shouhm@gmail.com, zhaohai@cs.sjtu.edu.cn
Abstract
This paper describes our coreference resolu-
tion system for the CoNLL-2012 shared task.
Our system is based on the Stanford?s dcore-
f deterministic system which applies multiple
sieves with the order from high precision to
low precision to generate coreference chains.
We introduce the newly added constraints and
sieves and discuss the improvement on the o-
riginal system. We evaluate the system using
OntoNotes data set and report our results of
average F-score 58.25 in the closed track.
1 Introduction
In this paper, our coreference resolution system for
CoNLL-2012 shared task (Pradhan et al, 2012) is
summarized. Our system is an extension of Stan-
ford?s multi-pass sieve system, (Raghunathan et al,
2010) and (Lee et al, 2011), by adding novel con-
straints and sieves. In the original model , sieves are
sorted in decreasing order of precision. Initially each
mention is in its own cluster. Mention clusters are
combined by satisfying the condition of each sieve
in the scan pass. Through empirical studies, we pro-
posed some extensions and algorithms for further-
more enhancing the performance.
?This work was partially supported by the National Natural
Science Foundation of China (Grant No. 60903119 and Grant
No. 61170114), the National Research Foundation for the Doc-
toral Program of Higher Education of China under Grant No.
20110073120022, the National Basic Research Program of Chi-
na (Grant No. 2009CB320901) and the European Union Sev-
enth Framework Program (Grant No. 247619).
?corresponding author
Many other existing systems applied supervised
or unsupervised (Haghighi and Klein, 2010) learn-
ing models. The classical resolution algorithm was
proposed by (Soon et al, 2001). Semantic knowl-
edge like word associations was involved by (Kob-
dani et al, 2011). Most of the supervised learning
models in CoNLL-2011 shared task (Chang et al,
2011)(Bjo?rkelund and Nugues, 2011) used classi-
fiers (Maximum Entropy or SVM) to train the mod-
els for obtaining the pairwise mention scores. How-
ever, the training process usually takes much longer
time than unsupervised or deterministic systems. In
contrast, (Raghunathan et al, 2010) proposed a rule-
based model which obtained competitive result with
less time.
Two considerable extensions to the Stanford mod-
el in this paper are made to guarantee higher pre-
cision and recall. First, we recorded error pattern-
s from outputs of the original Stanford system and
found that the usual errors are mention boundary
mismatches, pronoun mismatches and so on. To
avoid the irrational coreference errors, we added
some constraints to the mention detection for elim-
inating some unreasonable mention boundary mis-
matches. Second, we added some constraints in the
coreference sieves based on the errors on the training
set and the development set.
We participated in the closed track and received
an official F-score (unweighted mean of MUC,
BCUBED and CEAF(E) metric) of 58.25 for En-
glish. The system with our extensions is briefly in-
troduced in Section 2. We report our evaluation re-
sults and discuss in Section 3.
118
2 System Architecture
The original Stanford system consists of three
stages: mention detection, coreference resolution
and post-processing. The mention detection stage
is for extracting mentions with a relative high re-
call. The coreference resolution stage uses multiple
sieves to generate coreference clusters. The post-
processing stage makes the output compatible with
the shared task and OntoNotes specifications (Prad-
han et al, 2007), e.g. removing singletons, apposi-
tive, predicate nominatives and relative pronouns.
2.1 Mention Detection
Our system mainly focuses on making extension-
s for mention detection and coreference resolution.
From error analysis, we found that mention bound-
aries caused many precision and recall errors. For
example, for the gold mention Robert H. Chandross,
an economist for Lloyd?s Bank in New York, the o-
riginal system only extracts Robert H. Chandross as
the mention and links it with he in the following sen-
tence. This mismatch leads to both precision and re-
call errors since the mention with longer boundary
is not detected but the shorter one is used. Another
example which omits today in the phrase for the pre-
dicted mention is mentioned in (Lee et al, 2011) and
this boundary mismatch also accounts for precision
and recall errors. Some other examples may be like
this: Auto prices had a big effect in the PPI, and at
the CPI level they won?t, the gold mentions are Au-
to prices, the PPI, the CPI level and they while the
original system only finds out auto prices. Consid-
ering these boundary mismatches, it is not hard for
us to categorize the error types.
By observation, most boundary problems happen
in the following cases:
? The predicted mention is embedded in the gold
mention.
? The gold mention is embedded in the predicted
mention.
? Some gold mentions are totally omitted.
It is very rare for the case that predicted mention
overlaps with the gold mention but no one includes
the other.
For the first and second cases, some analysis and
constraint about prefix and postfix of phrases are ap-
plied to get predicted mentions as precise as gold
mentions. For the example mentioned above, the
clause ,an economist ... which modifies the person
Robert H. Chandross is annexed to the person name
mention. We also append time and other modifiers
to the original mention. As for the third case, we al-
low more pronouns and proper nouns to be added to
the list of mentions.
2.2 Sieve Coreference
Like the constraints on the extension to the mention
detection stage, our system also generates error re-
ports for the sieve passes. While our system is rule-
based and it also works without training data sets,
some statistical information is also helpful to detect
and avoid errors.
The first extension we used is a direct way to uti-
lize the training data and the development data. We
simply record the erroneous mention pairs in the
train and development sets with distance and sieve
information. One of the most common errors is that
when mentions with particular types appear twice
in the same sentence, the original system often puts
them into the same cluster. For example, there are
often two or more you or person names in the dia-
logue, however, the different occurrences are treat-
ed as coreference which produces precision errors.
To address this problem, we convert proper noun-
s to type designator, e.g. Paul as Man Name. Then
we use the formatted error pairs as constraints on the
sieve passes since some pairs mostly cause precision
errors. If the checking pair matches up some records
in the errors with the same sieve information and the
error frequency is over a threshold, we must discard
this pair in this sieve pass.
Another difference between our system and the S-
tanford system is the semantic similarity sieve. For
each sieve pass, the current clusters are built by
stronger sieves ( sieves in the earlier passes ). The S-
tanford system selects the most representative men-
tion from a mention cluster to query for semantic
information. The preference order is:
1. mentions headed by proper nouns
2. mentions headed by common nouns
119
3. nominal mentions
4. pronominal mentions
In our system, we not only select the most rep-
resentative one but compare all the types above, i.e,
select the longest string in each type of this clus-
ter. When applying semantic sieves, we also com-
pare representative mention for each type and make
synthesized decisions by the number of types which
have similar semantic meanings.
We also made some modifications on the sieves
and their ordering in the original system. For Prop-
er HeadWordMatch mentioned in (Lee et al, 2011),
the Pronoun distance which indicates sentence dis-
tance limit between a pronoun and its antecedent.
We change the value from 3 to 2.
3 Experiments and Results
Table 1: CoNLL-2012 Shared Task Test Results
Metric Recall Precision F1
MD 75.35 72.08 73.68
MUC 63.46 62.39 62.92
BCUBED 65.31 68.90 67.05
CEAF(M) 55.68 55.68 55.68
CEAF(E) 44.20 45.35 44.77
BLANC 69.43 75.08 71.81
OFFICIAL - - 58.25
Table 2: Comparison between original system and our
system on the development set
metric original our system
MUC F 61.64 62.31
MUC P 58.65 59.58
MUC R 64.95 65.29
BCUBED F 68.61 69.87
BCUBED P 67.23 68.81
BCUBED R 70.04 70.97
Our system enhanced the precision and recall of
the original system of (Lee et al, 2011). The table 1.
shows the official result for the CoNLL-2012 shared
task. The recall of our mention detection approach
is 75.35% while the precision is 72.08%. The fi-
nal official score 58.25 is the unweighed mean of
MUC, BCUBED and CEAF(E). Although the test
set is different from that of the previous year, com-
paring with the original system, our result of MD
and MUC shows that our improvement is meaning-
ful. The table 2. indicates the improvement from
our system over the original system evaluated by the
development set. Since experiments with seman-
tic knowledge like WordNet and Wikipedia cannot
give better performance, we omit the semantic func-
tion for generating test result. Our explanation is
that the predicted mentions are still not precise e-
nough and the fuzziness of the semantic knowledge
might cause conflicts with our sieves. If the seman-
tic knowledge tells that two mentions are similar and
possibly can be combined while they do not satisfy
the sieve constraints, it will be very hard to make a
decision since we cannot find an appropriate thresh-
old to let the semantic suggestion pass through.
4 Conclusion
In this paper we made a series of improvements on
the existing Stanford system which only uses deter-
ministic rules. Since the rules are high dimensional,
i.e., the rules that are adopted in the system may de-
pend on the states of the ongoing clustering process,
it is not feasible to apply it in the statistical learning
methods since take the intermediate results into con-
sideration will be. The experimental results show
that our improvements are effective. For this task,
we added constraints on the mention detection stage
and the coreference resolution stage. We also added
new sieves and conduct a group of empirical studies
on semantic knowledge. Our results give a demon-
stration that the deterministic model for coreference
resolution is not only simple and competitive but al-
so has high extendibility.
References
Anders Bjo?rkelund and Pierre Nugues. 2011. Exploring
lexicalized features for coreference resolution. In Pro-
ceedings of the Fifteenth Conference on Computation-
al Natural Language Learning: Shared Task, pages
45?50, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Nick Rizzolo, Mark Sammons, and Dan Roth. 2011.
Inference protocols for coreference resolution. In Pro-
ceedings of the Fifteenth Conference on Computation-
120
al Natural Language Learning: Shared Task, pages
40?44, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June. Association for Computational
Linguistics.
Hamidreza Kobdani, Hinrich Schuetze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrapping
coreference resolution using word associations. In
Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 783?792, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Sameer S. Pradhan, Lance A. Ramshaw, Ralph M.
Weischedel, Jessica MacBride, and Linnea Micciulla.
2007. Unrestricted coreference: Identifying entities
and events in ontonotes. In ICSC, pages 446?453.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestrict-
ed coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012), Jeju, Korea.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceedings
of EMNLP 2010.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. Comput. Linguist.,
27(4):521?544, December.
121
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 74?81,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Grammatical Error Correction as Multiclass Classification
with Single Model?
Zhongye Jia, Peilu Wang and Hai Zhao?
MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems,
Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
800 Dongchuan Road, Shanghai 200240, China
{jia.zhongye,plwang1990}@gmail.com,zhaohai@cs.sjtu.edu.cn
Abstract
This paper describes our system in the shared
task of CoNLL-2013. We illustrate that gram-
matical error detection and correction can be
transformed into a multiclass classification
task and implemented as a single-model sys-
tem regardless of various error types with the
aid of maximum entropy modeling. Our sys-
tem achieves the F1 score of 17.13% on the
standard test set.
1 Introduction and Task Description
Grammatical error correction is the task of auto-
matically detecting and correcting erroneous word
usage and ill-formed grammatical constructions in
text (Dahlmeier et al, 2012). This task could be help-
ful for hundreds of millions of people around the world
that are learning English as a second language. Al-
though there have been much of work on grammatical
error correction, the current approaches mainly focus
on very limited error types and the result is far from
satisfactory.
The CoNLL-2013 shared task, compared with the
previous Help Our Own (HOO) tasks focusing on only
determiner and preposition errors, considers a more
comprehensive list of error types, including determiner,
preposition, noun number, verb form, and subject-
verb agreement errors. The evaluation metric used in
CoNLL-2013 is Max-Matching (M2) (Dahlmeier and
Ng, 2012) precision, recall and F1 between the system
edits and a manually created set of gold-standard ed-
its. The corpus used in CoNLL-2013 is NUS Corpus
of Learner English (NUCLE) of which the details are
described in (Dahlmeier et al, 2013).
In this paper, we describe the system submission
from the team 1 of Shanghai Jiao Tong Univer-
?This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No.60903119, Grant
No.61170114, and Grant No.61272248), and the National
Basic Research Program of China (Grant No.2009CB320901
and Grant No.2013CB329401).
?Corresponding author
sity (SJT1). Grammatical error detection and correc-
tion problem is treated as multiclass classification task.
Unlike previous works (Dahlmeier et al, 2012; Ro-
zovskaya et al, 2012; Kochmar et al, 2012) that train
a model upon each error type, we use one single model
for all error types. Instead of the original error type, a
more detailed version of error types is used as class la-
bels. A rule based system generates labels from the
golden edits utilizing an extended version of Leven-
shtein edit distance. We use maximum entropy (ME)
model as classifier to obtain the error types and use
rules to do the correction. Corrections are made us-
ing rules. Finally, the corrections are filtered using lan-
guage model (LM).
2 System Architecture
Our system is a pipeline of grammatical error detection
and correction. We treats grammatical error detection
as a classification task. First all the tokens are relabeled
according to the golden annotation and a sequence of
modified version of error types is generated. This re-
labeling task is rule based using an extended version
of Levenshtein edit distance which will be discussed
in the following section. Then with the modified error
types as the class labels, a classifier using ME model
is trained. The grammatical error correction is also
rule based, which is basically the reverse of the rela-
beling phase. The modefied version of error types that
we used is much more detailed than the original five
types so that it enables us to use one rule to do the cor-
rection for each modified error type. After all, the cor-
rections are filtered by LM, to remove those corrections
that seem much worse than the original sentence.
As typical classification task, we have a training step
and a test step. The training step consists three phases:
? Error types relabeling.
? Training data refinement.
? ME training.
The test step includes three phases:
? ME classification.
74
? Error correction according to lebels.
? LM filtering.
2.1 Rebeling by Levenshtein Edit Distance
with Inflection
In CoNLL-2013 there are 5 error types but they cannot
be used directly as class labels, since they are too gen-
eral for error correction. For example, the verb form
error includes all verb form inflections such as con-
verting a verb to its infinitive form, gerund form, paste
tense, paste participle, passive voice and so on. Previ-
ous approaches (Dahlmeier et al, 2012; Rozovskaya et
al., 2012; Kochmar et al, 2012) manually decompose
each error types to more detailed ones. For example,
in (Dahlmeier et al, 2012), the determinater error is
decomposed into:
? replacement determiner (RD): { a? the }
? missing determiner (MD): { ?? a }
? unwanted determiner (UD): { a? ? }
For limited error types such as merely determinatives
error and preposition error in HOO 2012, manually de-
composition may be sufficient. But for CoNLL-2013
with 5 error types including complicated verb inflec-
tion, an automatic method to decompose error types is
needed. We present an extended version of Levenshtein
edit distance to decompose error types into more de-
tailed class labels and relabel the input with the labels
generated.
The original Levenshtein edit distance has 4 edit
types: unchange (U), addition (A), deletion (D) and
substitution (S). We extend the ?substitution? edit
into two types of edits: inflection (I) and the orig-
nal substitution (S). To judge whether two words can
be inflected from each other, the extended algorithm
needs lemmas as input. If the two words have the
same lemma, they can be inflected from each other.
The extended Levenshtein edit distance with inflec-
tion is shown in Algorithm 1. It takes the source to-
kens toksrc, destination tokens tokdst and their lemmas
lemsrc, lemdst as input and returns the edits E and the
parameters of edits P. For example, for the golden edit
{look? have been looking at}, the edits E returned by
DISTANCE will be {A,A,U ,A}, and the parameters
P of edits returned by DISTANCE will be {have, been,
looking, at}.
Then with the output of DISTANCE, the labels can
be generated by calculating the edits between original
text and golden edits. For those tokens without errors,
we directly assign a special label ??? to them. The
tricky part of the relabeling algorithm is the problem
of the edit ?addition?, A. A new token can only be
added before or after an existing token. Thus for la-
bels with addition, we must find some token that the
label can be assigned to. That sort of token is defined
as pivot. A pivot can be a token that is not changed in
Algorithm 1 Levenshtein edit distance with inflection
1: function DISTANCE(toksrc, tokdst, lemsrc,
lemdst)
2: (lsrc, ldst)? (len(toksrc), len(tokdst))
3: D[0 . . . lsrc][0 . . . ldst]? 0
4: B[0 . . . lsrc][0 . . . ldst]? (0, 0)
5: E[0 . . . lsrc][0 . . . ldst]? ?
6: for i? 1 . . . lsrc do
7: D[i][0]? i
8: B[i][0]? (i? 1, 0)
9: E[i][0]? D
10: end for
11: for j ? 1 . . . ldst do
12: D[0][j]? j
13: B[0][j]? (0, j ? 1)
14: E[0][j]? A
15: end for
16: for i? 1 . . . lsrc; j ? 1 . . . ldst do
17: if toksrc[i? 1] = tokdst[j ? 1] then
18: D[i][j]? D[i? 1][j ? 1]
19: B[i][j]? (i? 1, j ? 1)
20: E[i][j]? U
21: else
22: m ? min(D[i ? 1][j ? 1], D[i ?
1][j], D[i][j ? 1])
23: if m = D[i? 1][j ? 1] then
24: D[i][j]? D[i? 1][j ? 1] + 1
25: B[i][j]? (i? 1, j ? 1)
26: if lemsrc[i ? 1] = lemdst[j ? 1]
then
27: E[i][j]? S
28: else
29: E[i][j]? I
30: end if
31: else if m = D[i? 1][j] then
32: D[i][j]? D[i? 1][j] + 1
33: B[i][j]? (i? 1, j)
34: E[i][j]? D
35: else if m = D[i][j ? 1] then
36: D[i][j]? D[i][j ? 1] + 1
37: B[i][j]? (i, j ? 1)
38: E[i][j]? A
39: end if
40: end if
41: end for
42: (i, j)? (lsrc, ldst)
43: while i > 0 ? j > 0 do
44: insert E[i][j] into head of E
45: insert tokdst[j ? 1] into head of P
46: (i, j)? B[i][j]
47: end while
48: return (E,P)
49: end function
an edit, such as the ?apple? in edit {apple ? an ap-
ple}, or some other types of edit such as the inflection
of ?look? to ?looking? in edit {look? have been look-
75
ing at}. In the CoNLL-2013 task, the addition edits are
mostly adding articles or determinaters, so when gener-
ating the label, adding before the pivot is preferred and
only those trailing edits are added after the last pivot.
At last, the label is normalized to upper case.
The BNF syntax of labels is defined in Figure 1. The
the non-terminal ?inflection-rules? can be substituted
by terminals of inflection rules that are used for cor-
recting the error types of noun number, verb form, and
subject-verb agreement errors. All the inflection rules
are listed in Table 1. The ?stop-word? can be subsi-
tuted by terminals of stop words which contains all ar-
ticles, determinnaters and prepositions for error types
of determiner and preposition, and a small set of other
common stop words. All the stop words are listed in
Table 2.
?label? ::= ?simple-label? | ?compound-label?
?simple-label? ::= ?pivot? | ?add-before? | ?add-after?
?compound-label? ::= ?add-before? ?pivot?
| ?pivot? ?add-after?
| ?add-before? ?pivot? ?add-after?
?pivot? ::= ?inflection? | ?unchange? | ?substitution?
| ?deletion?
?add-before? ::= ?stop-word??
| ?stop-word???add-before?
?add-after? ::= ??stop-word?
| ??stop-word??add-after?
?substitution? ::= ?stop-word?
?inflection? ::= ?inflection-rules?
?unchange? ::= ?
?deletion? ::= ?
Figure 1: BNF syntax of label
Algorithm 2 is used to generate the label from the
extended Levenshtein edits according to the syntax de-
fined in Figure 1. It takes the original tokens, tokorig
and golden edit tokens, tokgold in an annotation and
their lemmas, lemorig, lemgold as input and returns the
generated label L. For our previous example of edit
{looked ? have been looking at}, the L returned by
RELABEL is {HAVE?BEEN?GERUND?AT}. Some
other examples of relabeling are demonstrated in Ta-
ble 3.
The correction step is done by rules according to the
labels. The labels are parsed with a simple LL(1) parser
and edits are made according to labels. A bit of extra
work has to be taken to handle the upper/lower case
problem. For additions and substitutions, the words
added or substituted are normalized to lowercase. For
inflections, original case of words are reserved. Then
a bunch of regular expressions are applied to correct
upper/lower case for sentence head.
Catalog Rules
Noun Number LEMMA, NPLURAL
Verb Number VSINGULAR, LEMMA
Verb Tense LEMMA, GERUND, PAST, PART
Table 1: Inflection rules
Catalog Words
Determinater a an the my your his her its our
their this that these those
Preposition about along among around as at
beside besides between by down
during except for from in inside
into of off on onto outside over
through to toward towards un-
der underneath until up upon with
within without
Modal Verb can could will would shall should
must may might
BE be am is are was were been
HAVE have has had
Other many much
Table 2: Stop words
Tokens Edit Label
to {to reveal?revealing} ?reveal GERUND
a {a woman?women} ?woman NPLURAL
developing {developing world THE?
wold ?the developing world} ?
a {a? ?} ?
in {in?on} ON
apple {apple?an apple} AN?
Table 3: Examples of relabeling
2.2 Training Corpus Refinement
The corpus used to train the grammatical error recog-
nition model is highly imbalanced. The original train-
ing corpus has 57,151 sentences and only 3,716 of
them contain at least one grammatical error, and only
5,049 of the total 1,161K words are needed to be cor-
rected. This characteristic makes it hard to get a well-
performed machine learning model, since the samples
to be recognized are so sparse and those large amount
of correct data will severely affect the machine learn-
ing process as it is an optimization on the global train-
ing data. While developing our system, we found
that only using sentences containing grammatical er-
rors will lead to a notable improvement of the result.
76
Algorithm 2 Relabeling using the extended Leven-
shtein edit distance
1: function RELABEL(tokorig , tokgold, lemorig ,
lemgold)
2: (E,P) ? DISTANCE(tokorig, tokgold,
lemorig , lemgold)
3: pivot? number of edits in E that are not A
4: L? ?
5: L? ??
6: while i < length of E do
7: if E[i] = A then
8: L? L+ label of edit E[i] with P[i]
9: i? i + 1
10: else
11: l? L+ label of edit E[i] with P[i]
12: pivot? pivot? 1
13: if pivot = 0 then
14: i? i + 1
15: while i < length of E do
16: l? l +?+ P[i]
17: i? i + 1
18: end while
19: end if
20: push l into L
21: L? ??
22: end if
23: end while
24: L? upper case of L
25: return L
26: end function
Inspired by this phenomenon, we propose a method to
refine the training corpus which will reduce the error
sparsity of the training data and notably improve the
recall rate.
The refined training corpus is composed of contexts
containing grammatical errors. To keep the informa-
tion which may have effects on the error identification
and classification, those contexts are selected based on
both syntax tree and n-gram, of which the process is
shown in Algorithm 3. For a word with error, its syntax
context of size c is those words in the minimal subtree
in the syntax tree with no less than c leaf nodes; and its
n-gram context of size n is n? 1 words before and af-
ter itself. In the CORPUSREFINE algorithm, the input c
gives the size of syntax context and n provides the size
of the n-gram context. These two parameters decide
the amount of information that may affect the recogni-
tion of errors. To obtain the context, given a sentence
containing a grammatical error, we first get a minimum
syntax tree whose number of terminal nodes exceed the
c threshold, then check whether the achieved context
containing the error word?s n-gram, if not, add the n-
gram to the context. At last the context is returned by
CORPUSREFINE.
An example illustrating this process is presented in
Figure 2. In this example, n and c are both set to 5,
Algorithm 3 Training Corpus Refine Algorithm
1: function CORPUSREFINE(sentence, c, n)
2: context? ?
3: if sentence contains no error then
4: return ?
5: end if
6: build the syntax tree T of sentence
7: enode? the node with error in T
8: e? enode
9: while True do
10: pnode? parent node of e in T
11: cnodes ? all the children nodes of pnode
in T
12: if len(cnodes) > c then
13: context? cnodes
14: break
15: end if
16: e? pnode
17: end while
18: i? the position of enode in context
19: l? size of context
20: if i < n then
21: add (n-i) words before context at the head
of context
22: end if
23: if l-i<n then
24: append (l-i) words after context in
context
25: end if
26: return context
27: end function
the minimal syntax tree and the context decided by it
are colored in green. Since the syntax context in the
green frame does not contain the error word?s 5-gram,
therefore, the 5-gram context in the blue frame is added
to the syntax context and the final achieved context of
this sentence is ?have to stop all works for the develop-
ment?.
2.3 LM Filter
All corrections are filtered using a large LM. Only
those corrections that are not too much worse than the
original sentences are accepted. Perplexity (PPL) of the
n-gram is used to judge whether a sentence is good:
PPL = 2?
?
w?n-gram p(w) log p(w),
We use rPPL, the ratio between the PPL before and
after correction, as a metric of information gain.
rPPL =
PPLcorrected
PPLoriginal
,
Only those corrections with an rPPL lower than a cer-
tain threshold are accepted.
77
N-gram Context
Then the innovators have to stop all works for the development .
S
RB
NP
VP .
DT NNS VBP
S
VP
TO
VP
VB
NP
PP
DT NNS IN
NP
DT NN
Syntax Context
Figure 2: Example of training corpus refinement
3 Features
The single model approach enables us only to optimize
one feature set for all error type in the task, which can
drastically reduce the computational cost in feature se-
lection.
As many previous works have proposed various of
features, we first collected features from different pre-
vious works including (Dahlmeier et al, 2012; Ro-
zovskaya et al, 2012; HAN et al, 2006; Rozovskaya
et al, 2011; Tetreault, Joel R and Chodorow, Martin,
2008). Then experiments with different features were
built to test these features? effectiveness and only those
have positive contribution to the final performance
were preserved. The features we used are presented in
Table 4, where word0 is the word that we are generat-
ing features for, and word and POS is the word itself
and it?s POS tag for various components. NPHead de-
notes the head of the minimum Noun Phrase (NP) in
syntax tree. wordNP?1 represents the word appearing
before NP in the sentence. NC stands for noun com-
pound and is composed of the last n words (n ? 2)
in NP which are tagged as a noun. Verb feature is
the word that is tagged as a verb whose direct object
is the NP containing current word. Adj feature repre-
sents the first word in the NP whose POS is adjective.
Prep feature denotes the preposition word if it imme-
diately precedes the NP. DPHead is the parent of the
current word in the dependency tree, and DPRel is the
dependency relation with parent.
4 Experiments
4.1 Data Sets
The CoNLL-2013 training data consist of 1,397 arti-
cles together with gold-standard annotation. The docu-
ments are a subset of the NUS Corpus of Learner En-
glish (NUCLE) (Dahlmeier et al, 2013). The official
test data consists of 50 new essays which are also from
NUCLE. The first 25 essays were written in response to
one prompt. The remaining 25 essays were written in
response to a second prompt. One of the prompts had
been used for the training data, while the other prompt
is new. More details of the data set are described in (Ng
et al, 2013).
We split the the entire training corpus ALL by article.
For our training step, we randomly pick 90% articles of
ALL and build a training corpus TRAIN of 1,258 arti-
cles. The rest 10% of ALL with 139 articles are for
developing corpus DEV .
For the final submission, we use the entire corpus
ALL for relabeling and training.
78
Feature Example
Lexical features
word0 (current word) the
wordi, (i = ?1,?2,?3) man, stared, at, old, oak,
tree
n words before word0,
(n=2, 3, 4)
stared+at,
man+stared+at. . .
n words after word0, (n=2,
3, 4)
old+oak, old+oak+tree . . .
wordi + POSi, (i =
?1,?2,?3)
stared+VBD, at+IN. . .
First word in NP the
ith word before/after NP,
(i = ?1,?2,?3)
man, stared, at, period . . .
wordNP?1 + NP at + ( the + old + oak +
tree )
Bag of words in NP old+oak+the+tree
NC oak tree
Adj + NC old oak tree
Adj POS + NC JJ+oak tree
POS features
POS0 (current word POS) NNS
POSi, (i = ?1,?2,?3) NN, VBD, IN . . .
POS?n + POS?(n?1) +
? ? ?+POS?1, (n = 2, 3, 4)
VBD + IN, NN + VBD +
IN . . .
POS1 + POS2 + ? ? ? +
POSn, (n = 2, 3, 4)
JJ + NN, JJ + NN + NNS
. . .
Bag of POS in NP DT+JJ+NN+NN
Head word features
NPHead of NP tree
NPHead POS NN
NPHead +
NPHead POS
tree+NN
Bag of POS in NP +
NPHead
DT JJ NN NN+tree
wordNP?1 + NPHead at+tree
Adj + NPHead old+tree
Adj POS + NPHead JJ+tree
wordNP?1 + Adj +
NPHead
at+old+tree
wordNP?1 +Adj POS +
NPHead
at+JJ+tree
Preposition features
Prep at
Prep + NPHead at+tree
Prep + Adj + NPHead at+old+tree
Prep + Adj POS +
NPHead
at+JJ+tree
Prep + NC at+(oak+tree)
Prep + NP at + ( the + old + oak +
tree )
Prep + NPHead POS at+NN
Prep + Adj +
NPHead POS
at+old+NN
Prep + Adj POS +
NPHead POS
at + JJ + NN
Prep + Bag of NP POS at + ( DT + JJ + NN )
Prep + NPHead POS +
NPHead
at + NN + tree
Lemma features
Lemma the
Dependency features
DPHead word tree
DPHead POS NN
DPRel det
Table 4: Features for grammatical error recognition.
The example sentence is:?That man stared at the old
oak tree.? and the current word is ?the? .
Feature Example
Verb features
Verb stared
Verb POS VBD
Verb + NPHead stared+tree
Verb + Adj + NPHead stared+old+tree
Verb + Adj POS +
NPHead
stared+JJ+tree
Verb + NP stared+(the+old+oak+tree)
Verb + Bag of NP POS stared+(DT+JJ+NN)
Table 5: Features Continued
Count Label
1146000 ?
3369 ?
3088 NPLURAL
2766 THE?
1470 LEMMA
706 A?
200?300 IN AN? THE ARE FOR TO OF
100
?200
ON A IS GERUND PAST VSINGULAR
50?100 WITH ?THE AT AN BY THIS INTO
5?50 FROM TO? WAS ABOUT WERE ?A
THESE TO?LEMMA OF? ?OF ARE?
?TO THROUGH BE?PAST AS AMONG
IN? BE? THEIR THE?LEMMA OVER
?ON HAVE? DURING FOR? ?WITH
PART ?IN HAVE WITHIN BE MANY
?AN THE?NPLURAL MUCH IS? WITH?
TOWARDS ?FOR HAVE?PART ?ABOUT
WILL ?UPON THEIR? HAVE?PAST
HAS?PART HAS? HAS BY? BEEN?
BE?? AN?LEMMA THAT? ITS HAD
FROM? BETWEEN A?LEMMA
4 WERE? UPON THOSE ON? MANY?
IS?? ?FROM CAN AS?
3 WILL?LEMMA WILL? TOWARD THIS?
THAT ITS? HAVE?? ?BE AT? ?AS
ABOUT?
2 WOULD WAS? TO?BE? THE??
ONTO IS?PAST IS?GERUND INSIDE
HAVE?BEEN? CAN?LEMMA ?BEEN
?AT
1 WOULD?LEMMA WITHOUT UN-
DER TO?THE TO?THAT?OF
TO?HAVE THIS?WILL? THIS?MAY
THIS?HAS? THESE? THE???OF
THEIR?LEMMA THE?GERUND
THE?A? THAT?HAS?BEEN?
THAT?HAS? SHOULD? ?OVER
OF?THE? OF?THE OF?GERUND OFF
OF?A? MAY? MAY IS?TO IS?LEMMA
INTO? ?INTO IN?THE? HIS HAVE?AN
HAS?PAST HAS?BEEN?GERUND
HAS?BEEN? HAD?? HAD? DURING?
COULD CAN?BE? CAN? BY?GERUND
?BY ?BETWEEN BESIDES BEEN?PART
BEEN AT?AN AT?A AS?THE AS?HAS
AROUND ARE?PAST ARE?A ARE??
A???OF AM?
Table 6: All labels after relabeling
79
4.2 Resources
We use the following NLP resources in our sys-
tem. For relabeling and correction, perl mod-
ule Lingua::EN::Inflect1 (Conway, 1998) is used
for determining noun and verb number and Lin-
gua::EN::VerbTense2 is used for determining verb
tense. A revised and extended version of maxi-
mum entropy model3 is used for ME modeling. For
lemmatization, the Stanford CoreNLP lemma annota-
tor (Toutanova et al, 2003; Toutanova and Manning,
2000) is used. The language model is built by the
SRILM toolkit (Stolcke and others, 2002). The corpus
for building LM is the EuroParl corpus (Koehn, 2005).
The English part of the German-English parallel cor-
pus is actually used. We use such a corpus to build
LM for the following reasons: 1. LM for grammatical
error correction should be trained from corpus that it-
self is grammatically correct, and the EuroParl corpus
has very good quality of writing; 2. the NUCLE cor-
pus mainly contains essays on subjects such as environ-
ment, economics, society, politics and so on, which are
in the same dormain as those of the EuroParl corpus.
4.3 Relabeling the Corpus
There are some complicated edits in the annotations
that can not be represented by our rules, for example
substitution of non-stopwords such as {human? peo-
ple} or {are not short of? do not lack}. The relabel-
ing phase will ignore those ones thus it may not cover
all the edits. After all, we get 174 labels after relabel-
ing on the entire corpus as shown in Table 6. These
labels are generated following the syntax defined in
Figure1 and terminals defined in Table 1 and Table 2.
Directly applying these labels for correction receives an
M2 score of Precission = 91.43%,Recall = 86.92%
and F1 = 89.12%. If the non-stopwords non-inflection
edits are included, of course the labels will cover all the
golden annotations and directly applying labels for cor-
rection can receive a score up to almost F1 = 100%.
But that will get nearly 1,000 labels which is too com-
putationally expensive for a classification task. Cut out
labels with very low frequency such as those exists only
once reduces the training time, but does not give signif-
icant performance improvement, since it hurts the cov-
erage of error detection. So we use all the labels for
training.
4.4 LM Filter Threshold
To choose the threshold for rPPL, we run a series of
tests on the DEV set with different thresholds. From
our empirical observation on those right corrections
and those wrong ones, we find the right ones seldomly
1http://search.cpan.org/?dconway/
Lingua-EN-Inflect-1.89/
2http://search.cpan.org/?jjnapiork/
Lingua-EN-VerbTense-3.003/
3http://www.nactem.ac.uk/tsuruoka/
maxent/
have rPPL > 2, so we test thresholds ranging from 1.5
to 3. The curves are shown in Figure 3.
 0.14
 0.145
 0.15
 0.155
 0.16
 0.165
 0.17
 1.4  1.6  1.8  2  2.2  2.4  2.6  2.8  3
Prec
ision
, Re
call 
and
 F1 c
urve
s
LM Filter Threshold
PrecisionRecallF1
Figure 3: Different thresholds of LM filters
With higher threshold, more correction with lower
information gain will be obtained. Thus the recall
grows higher but the precission grows lower. We can
observe a peak of F1 arround 1.8 to 2.0, and the thresh-
old chosen for final submission is 1.91.
4.5 Evaluation and Result
The evaluation is done by calculating the M2 precis-
sion, recall and F1 score between the system output
and golden annotation. All the error types are evalu-
ated jointly. Only one run of a team is permitted to be
submitted. Table 7 shows our result on our DEV data
set and the official test data set.
Data Set Precission Recall F1
DEV 16.03% 15.88% 15.95%
Official 40.04% 10.89% 17.13%
Table 7: Evaluation Results
5 Conclusion
In this paper, we presented the system from team 1 of
Shanghai Jiao Tong University that participated in the
HOO 2012 shared task. Our system achieves an F1
score of 17.13% on the official test set based on gold-
standard edits.
References
DM Conway. 1998. An algorithmic approach to en-
glish pluralization. In Proceedings of the Second An-
nual Perl Conference. C. Salzenberg. San Jose, CA,
O?Reilly.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
Evaluation for Grammatical Error Correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 568?572, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
80
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng
Ng. 2012. NUS at the HOO 2012 Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 216?
224, Montre?al, Canada, June. Association for Com-
putational Linguistics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations, Atlanta, Georgia, USA.
NA-RAE HAN, MARTIN CHODOROW, and CLAU-
DIA LEACOCK. 2006. Detecting Errors in En-
glish Article Usage by Non-Native Speakers. Nat-
ural Language Engineering, 12:115?129, 5.
Ekaterina Kochmar, ?istein Andersen, and Ted
Briscoe. 2012. HOO 2012 Error Recognition and
Correction Shared Task: Cambridge University Sub-
mission Report. In Proceedings of the Seventh Work-
shop on Building Educational Applications Using
NLP, pages 242?250, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
For Statistical Machine Translation. In MT summit,
volume 5.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The conll-
2013 shared task on grammatical error correction. In
Proceedings of the Seventeenth Conference on Com-
putational Natural Language Learning.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the 13th European Workshop on Natural Lan-
guage Generation, pages 263?266. Association for
Computational Linguistics.
Alla Rozovskaya, Mark Sammons, and Dan Roth.
2012. The UI System in the HOO 2012 Shared Task
on Error Correction. In Proceedings of the Seventh
Workshop on Building Educational Applications Us-
ing NLP, pages 272?280, Montre?al, Canada, June.
Association for Computational Linguistics.
Andreas Stolcke et al 2002. SRILM-An Extensible
Language Modeling Toolkit. In Proceedings of the
international conference on spoken language pro-
cessing, volume 2, pages 901?904.
Tetreault, Joel R and Chodorow, Martin. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics-
Volume 1, pages 865?872. Association for Compu-
tational Linguistics.
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the Knowledge Sources Used in a Max-
imum Entropy Part-of-Speech Tagger. In Proceed-
ings of the 2000 Joint SIGDAT conference on Em-
pirical methods in natural language processing and
very large corpora: held in conjunction with the
38th Annual Meeting of the Association for Compu-
tational Linguistics-Volume 13, pages 63?70. Asso-
ciation for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich Part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
81
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 74?82,
Baltimore, Maryland, 26-27 July 2014.
c
?2014 Association for Computational Linguistics
Grammatical Error Detection and Correction
using a Single Maximum Entropy Model
?
Peilu Wang, Zhongye Jia and Hai Zhao
?
Key Laboratory of Shanghai Education Commission for
Intelligent Interaction and Cognitive Engineering,
Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
800 Dongchuan Road, Shanghai 200240, China
{plwang1990,jia.zhongye}@gmail.com,zhaohai@cs.sjtu.edu.cn
Abstract
This paper describes the system of Shang-
hai Jiao Tong Unvierity team in the
CoNLL-2014 shared task. Error correc-
tion operations are encoded as a group of
predefined labels and therefore the task
is formulized as a multi-label classifica-
tion task. For training, labels are obtained
through a strict rule-based approach. For
decoding, errors are detected and correct-
ed according to the classification results.
A single maximum entropy model is used
for the classification implementation in-
corporated with an improved feature selec-
tion algorithm. Our system achieved pre-
cision of 29.83, recall of 5.16 and F 0.5
of 15.24 in the official evaluation.
1 Introduction
The task of CoNLL-2014 is grammatical error cor-
rection which consists of detecting and correcting
the grammatical errors in English essays written
by non-native speakers (Ng et al., 2014). The re-
search of grammatical error correction can poten-
tially help millions of people in the world who are
learning English as foreign language. Although
there have been many works on grammatical error
correction, the current approaches mainly focus on
very limited error types and the result is far from
satisfactory.
The CoNLL-2014 shared task, compared with
the previous Help Our Own (HOO) tasks (Dale et
al., 2012) considering only determiner and prepo-
sition errors and the CoNLL-2013 shared task fo-
?
This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No.60903119, Grant
No.61170114, and Grant No.61272248), the National Ba-
sic Research Program of China (Grant No.2013CB329401),
the Science and Technology Commission of Shanghai Mu-
nicipality (Grant No.13511500200), and the European Union
Seventh Framework Program (Grant No.247619).
?
Corresponding author
cusing on five major types of errors, requires to
correct all 28 types of errors (Ng et al., 2014).
One traditional strategy is designing a system
combined of a set of sub-models, where each sub-
model is specialized for a specific subtask, for ex-
ample, correcting one type of errors. This strat-
egy is computationally efficient and can adopt d-
ifferent favorable features for each subtask. Top
ranked systems in CoNLL-2013 (Rozovskaya et
al., 2013; Kao et al., 2013; Xing et al., 2013;
Yoshimoto et al., 2013; Xiang et al., 2013) are
based on this strategy. However, the division of
the model relies on prior-knowledges and the de-
signing of different features for each sub-model
requires a large amount of manual works. This
shortage is especially notable in CoNLL-2014
shared task, since the number of error types is
much larger and the composition of errors is more
complicated than before.
In contrast, we follow the work in (Jia et al.,
2013a; Zhao et al., 2009a), integrating everything
into one model. This integrated system holds a
merit that a one-way feature selection benefits the
whole system and no additional process is needed
to deal with the conflict or error propagation of ev-
ery sub-models. Here is a glance of this method: A
set of more detailed error types are generated auto-
matically from the original 28 types of errors. The
detailed error type can be regarded as the label of
a word, thus the task of grammatical error detec-
tion is transformed to a multi-label classification
task using maximum entropy model (Berger et al.,
1996; Zhao et al., 2013). A feature selection ap-
proach is introduced to get effective features from
large amounts of feature candidates. Once errors
are detected through word label classification, a
rule-based method is used to make corrections ac-
cording to their labels.
The rest of the paper is organized as follows.
Section 2 describes the system architecture. Sec-
tion 3 introduces the feature selection approach
74
and the features we used. Experiments and result-
s are presented in section 5, followed by conclu-
sion.
2 System Architecture
In our approach, the grammatical error detection
is regarded as a multi-label classification task. At
first, each token in training corpus is assigned a la-
bel according to the golden annotation. The con-
struction of labels is rule based using an extend-
ed version of Levenshtein edit distance algorith-
m which will be discussed in the following sub-
section. Each label maps an edit operation to do
the correction, thus the generated labels are much
more detailed than the originial 28 error types.
Then, a maximum entropy (ME) model is adopted
as the classifier. With the labeled data, the process
of grammatical error correction is just applying the
edit operation mapped by each label, which is ba-
sically the reverse of the labeling phase.
2.1 Data Labeling
In CoNLL-2014 shared task, there are 28 error
types but they can not be used directly as class la-
bels, since these types are too general that they can
hardly be corrected by applying one rule-based
edit. For example, the correction of Vform (ver-
b form) error type includes all verb form inflec-
tions such as converting a verb to its infinitive for-
m, gerund form, past form and past participle and
so on. Previous works (Dahlmeier et al., 2012;
Rozovskaya et al., 2012; Kochmar et al., 2012)
manually decompose each error types to more de-
tailed subtypes. For example, in (Dahlmeier et al.,
2012), the determinater errors are decomposed in-
to:
? replacement determiner (RD): { a ? the }
? missing determiner (MD): { ? ? a }
? unwanted determiner (UD): { a ? ? }
For a task with a few error types such as merely
determinative and preposition error in HOO 2012,
manually decomposition may be sufficient. How-
ever, for CoNLL-2014, all 28 error types are re-
quired to be corrected and some of these types
such as Rloc- (Local redundancy) and Um (Un-
clear meaning) are quite complex that the manu-
al decomposition is time consuming and requires
lots of grammatical knowledges. Therefore, an au-
tomatica decomposition method is proposed. It is
extended from the Levenshtein edit distance algo-
rithm and can divide error types into more detailed
subtypes that each subtype can be corrected by ap-
plying one simple rule. How to calculate the ex-
tended Levenshtein edit distance is described in
Algorithm 1.
Algorithm 1 Extended Levenshtein Edit Distance
INPUT: toks
src
, toks
dst
OUTPUT: E,P
l
src
, l
dst
? len(toks
src
), len(toks
dst
)
D[0 . . . l
src
][0 . . . l
dst
]? 0
B[0 . . . l
src
][0 . . . l
dst
]? (0, 0)
E[0 . . . l
src
][0 . . . l
dst
]? ?
for i? 1 . . . l
src
do
D[i][0]? i
B[i][0]? (i-1, 0)
E[i][0]? D
end for
for j ? 1 . . . l
dst
do
D[0][j]? j
B[0][j]? (0, j-1)
E[0][j]? A
end for
for i? 1 . . . l
src
; do
for j ? 1 . . . l
dst
do
if toks
src
[i-1] = toks
dst
[j-1] then
D[i][j]? D[i-1][j-1]
B[i][j]? (i-1, j-1)
E[i][j]? U
else
m = min(D[i-1][j-1], D[i-1][j], D[i][j-1])
if m = D[i-1][j-1] then
D[i][j]? D[i-1][j-1] + 1
B[i][j]? (i-1, j-1)
if lemma(toks
src
[i-1])
= lemma(toks
dst
[j-1]) then
E[i][j]? S
else
E[i][j]? I
end if
else if m = D[i-1][j] then
D[i][j]? D[i-1][j] + 1
B[i][j]? (i-1, j)
E[i][j]? D
else if m = D[i][j-1] then
D[i][j]? D[i][j-1] + 1
B[i][j]? (i, j-1)
E[i][j]? A
end if
end if
end for
end for
i, j ? l
src
, l
dst
while i > 0 ? j > 0 do
insert E[i][j] into head of E
insert toks
dst
[j ? 1] into head of P
(i, j)? B[i][j]
end while
return (E,P)
In this algorithm, toks
src
represents the tokens
that are annotated with one grammatical error and
toks
dst
represents the corrected tokens of toks
src
.
At first, three two dimensional matrixes D, B and
75
E are initialized. For all i and j, D[i][j] holds
the Levenshtein distance between the first i tokens
of toks
src
and first j tokens of toks
dst
. B stores
the path of the Levenshtein distance and E stores
the edit operations in this path. The original Lev-
enshtein edit distance has 4 edit operations: un-
change (U ), addition (A), deletion (D) and substi-
tution (S). We extend the ?substitution? edit into
two types of edits: inflection (I) and the original
substitution (S). If two different words have the
same lemma, the substitution operation is I, else is
S. lemma(x) returns the lemma of token x. This
algorithm returns the edit operations E and the pa-
rameters of these operations P. Here is a simple
sample illustrating this algorithm. For the golden
edit {a red apple is ? red apples are}, toks
src
is
a red apple is, toks
dst
is red apples are, the output
edits E will be {D,U , I,S}, and the parameters P
will be {-, red, apples, are}.
Then with the output of this extended Leven-
shtein distance algorithm, labels can be generated
by transforming these edit operations into readable
symbols. For those tokens without errors, we di-
rectly assign a special label ??? to them. A tricky
part of the labeling process is the problem of the
edit ?addition?,A. A new token can only be added
before or after an existing token. Thus for edit op-
eration with addition, we must find an existing to-
ken that the label can be assigned to, and this sort
of token is defined as pivot. A pivot can be a token
that is not changed in an edit operation, such as the
?apple? in edit {apple ? an apple}, or some oth-
er types of edit such as the inflection of ?look? to
?looking? in edit {look ? have been looking at}.
The names of these labels are based on BNF
syntax which is defined in Figure 1. The non-
terminal ?word? can be substituted by all words
in the vocabulary. The non-terminal ?inflection-
rules? can be substituted by terminals of inflection
rules that are used for correcting the error types of
noun number, verb form, and subject-verb agree-
ment errors. All the inflection rules are listed in
Table 1.
With the output of extended Levenshtein edits
distance algorithm, Algorithm 2 gives the process
to generate labels whose names are based on the
syntax defined in Figure 1. It takes the output E, P
of Algorithm 1 as inputs and returns the generat-
ed set of labels L. Each label in L corresponds to
one token in toks
src
in order. For our previous ex-
ample of edit {a red apple is ? red apples are},
?label? ::= ?simple-label? | ?compound-label?
?simple-label? ::= ?pivot? | ?add-before? |
?add-after?
?compound-label? ::= ?add-before? ?pivot?
| ?pivot? ?add-after?
| ?add-before? ?pivot? ?add-after?
?pivot? ::= ?unchange? | ?substitution? |
?inflection?
| ?deletion?
?add-before? ::= ?word??
| ?word???add-before?
?add-after? ::= ??word?
| ??word??add-after?
?substitution? ::= ?word?
?inflection? ::= ?inflection-rules?
?unchange? ::= ?
?deletion? ::= ?
Figure 1: BNF syntax of label
Rules Description
LEMMA change word to its lemma
NPLURAL change noun to its plural form
VSINGULAR change verb to its singular form
GERUND change verb to its gerund form
PAST change verb to its past form
PART change verb to its past partici-
ple
Table 1: Inflection rules
the L returned by Algorithm 2 is {?, ?, NPLU-
RAL, ARE} corresponding to the tokens {a, red,
apple, is} in toks
src
. Some other examples of the
generated labels are presented in Table 2.
These labels are elaborately designed that each
of them can be interpreted easily as a series of ed-
it operations. Once the labels are determined by
classifier, the correction of the grammatical errors
is conducted by applying the edit operations inter-
preted from these labels.
76
Algorithm 2 Labeling Algorithm
1: INPUT: E,P
2: OUTPUT: L
3: pivot? number of edits in E that are not A
4: L? ?
5: L?
??
6: while i < length(E) do
7: if E[i] = A then
8: L? L+ label of edit E[i] with P[i]
9: i? i + 1
10: else
11: l? L+ label of edit E[i] with P[i]
12: pivot? pivot? 1
13: if pivot = 0 then
14: i? i + 1
15: while i < length of E do
16: l? l +?+ P[i]
17: i? i + 1
18: end while
19: end if
20: push l into L
21: L?
??
22: end if
23: end while
24: L? upper case of L
25: return L
Tokens Edit Label
to
{to reveal?revealing}
?
reveal GERUND
a
{a woman?women}
?
woman NPLURAL
developing {developing world THE?
wold ?the developing world} ?
a {a? ?} ?
in {in?on} ON
apple {apple?an apple} AN?
Table 2: Examples of labeling
2.2 Label Classification
Using the approach described above, the training
corpus is converted to a sequence of words with
labels. Maximum entropy model is used as the
classifier. It allows a very rich set of features to be
used in a model and has shown good performance
in similiar tasks (Zhao et al., 2013). The features
we used are discussed in the next section.
3 Feature Selection and Generation
One key factor affecting the performance of maxi-
mum entropy classifier is the features it used. A
good feature that contains useful information to
guide classification will significantly improve the
performance of the classifier. One direct way to
involve more good features is involving more fea-
tures.
In our approach, large amounts of candidate
features are collected at first. We carefully exam-
ine the factors involved in a wide range of fea-
tures that have been or can be used to the word
label classification task. Many features that are
considered effective in various of previous work-
s (Dahlmeier et al., 2012; Rozovskaya et al.,
2012; Han et al., 2006; Rozovskaya et al., 2011;
Tetreault, Joel R and Chodorow, Martin, 2008)
are included. Besides, features that are used in
the similar spell checking tasks (Jia et al., 2013b;
Yang et al., 2012) and some novel features show-
ing effectiveness in other NLP tasks (Wang et al.,
2013; Zhang and Zhao, 2013; Xu and Zhao, 2012;
Ma and Zhao, 2012; Zhao, 2009; Zhao et al.,
2009b) are also included. However, using too
many features is time consuming. Besides, it in-
creases the probability of overfitting and may lead
to a poor solution of the maximum-likelihood pa-
rameter estimate in the ME training.
Algorithm 3 Greedy Feature Selection
1: INPUT: all feature candidates F
2: OUTPUT: selected features S
3: S = {f
0
, f
1
, . . . , f
k
}, a random subset of F
4: while do
5: C = RECRUITMORE(S)
6: if C = {} then
7: return S
8: end if
9: S
?
= SHAKEOFF(S+C)
10: if scr(M(S)) ? scr(M(S
?
)) then
11: return S
12: end if
13: S = S
?
14: end while
15: function RECRUITMORE(S)
16: C = {}, and p = scr(M(S))
17: for each f ? F ? S do
18: if p < scr(M(S + {f})) then
19: C = C + {f}
20: end if
21: end for
22: end function
23: function SHAKEOFF(S)
24: while do
25: S
?
= S
0
= S
26: for each f ? S do
27: if scr(M(S
?
)) < scr(M(S
?
? {f})) then
28: S
?
= S
?
? {f}
29: end if
30: end for
31: S = S
?
32: if S
?
= S
0
then
33: return S
?
34: end if
35: end while
36: end function
Therefore a feature selection algorithm is intro-
duced to filter out ?bad? features at first and the re-
maining features will be used to generate new fea-
tures. The feature selection algorithm has shown
77
effectiveness in (Zhao et al., 2013) and is present-
ed in Algorithm 3.
In this algorithm, M(S) represents the model
using feature set S and scr(M) represents the e-
valuation score of model M on a development da-
ta set. It repeats two main steps until no further
performance gain is achievable:
1. Include any features from the rest of F into
the current set of candidate features if the in-
clusion would lead to a performance gain.
2. Exclude any features from the current set of
candidate templates if the exclusion would
lead to no deterioration in performance.
By repeatedly adding the useful and removing
the useless features, the algorithm aims to return
a better and smaller set of features for next round.
Only 55 of the 109 candidate features remain af-
ter using this algorithm and they are presented in
Table 4. Table 3 gives an interpretation of the ab-
breviations used in Table 4. Each feature of a word
is set to that listed in feature column if the word
satisfies the condition listed in current word col-
umn, else the feature is set to ?NULL?. For ex-
ample, if the current word satisfies the condition
in the first row of Table 4 which is the first word
in the left of a NC, feature 1 of this word is set to
all words in the NC, otherwise, feature 1 is set to
?NULL?.
4 Experiment
4.1 Data Sets
The CoNLL-2014 training data is a corpus of
learner English provided by (Dahlmeier et al.,
2013). This corpus consists of 1,397 articles, 12K
sentences and 116K tokens. The official blind test
data consists of 50 articles, 245 sentences and 30K
tokens. More detailed information about this data
is described in (Ng et al., 2014; Dahlmeier et al.,
2013).
In development phase, the entire training corpus
is splited by sentence. 80% sentences are picked
up randomly and used for training and the rest
20% are used as the developing corpus. For the fi-
nal submission, the entire corpus is used for train-
ing.
Abbreviation Description
NP Noun Phrase
NC Noun Compound and is ac-
tive if second to last word in
NP is tagged as noun
VP Verb Phrase
cw Current Word
pos part-of-speech of the current
word
X.l
i
the ith word in the left of X
X.r
i
the ith word in the right of X
NP[0] the first word of NP
NP.head the head word of NP
NP.(DT or
IN or TO)
word in NP whose pos is DT
or IN or TO
VP.verb word in VPwhose pos is ver-
b
VP.NP NP in VP
dp the dependency relation gen-
erated by standford depen-
dency parser
dp.dep the dependent in the depen-
dency relation
dp.head the head in the dependency
relation
dp.rel the type of the dependency
relation
Table 3: The interpretation of the abbrevations in
Table 4
4.2 Data Labeling
The labeling algorithm described in section 2.1 is
firstly applied to the training corpus. Total 7047
labels are generated and those whose count is larg-
er than 15 is presented in Table 5. Directly ap-
plying these 7047 labels for correction receives an
M
2
score of precision=90.2%, recall=87.0% and
F 0.5=89.5%. However, the number of labels
is too large that the training process is time con-
suming and those labels appears only few times
will hurt the generalization of the trained model.
Therefore, labels with low frequency which ap-
pear less than 30 times are cut out and 109 labels
remain. The M
2
score of the system using this re-
fined labels is precision=83.9%, recall=64.0% and
F 0.5=79.0%. Note that even applying all labels,
the F 0.5 is not 100%. It is because some annota-
tions in the training corpus are not consistency.
78
current word feature
NC.l
1
NC
NP.l
1
NP
NP[0] NP.l
1
.pos
NC.l
1
NC
NC.l
1
NC.l
1
.pos
NC.l
1
and pos=DT NC
NC.l
1
and pos=VB NC
NP.l
1
and pos=VB NP
pos=VB cw
pos=DT cw
the cw.r
1
a cw.r
1
an cw.r
1
NP[0] cw
NP[0] NP.l
1
NP[0] NP.l
2
NP[0] NP.l
3
NP[0] NP.l
1
.pos
NP[0] NP.l
2
.pos
NP[0] NP.l
3
.pos
NP.l
1
NP.head
NP.l
1
NP.head.pos
NP.head NP. head
NP.head NP. head.bag
NP.head NP. head.pos
NP.head NP. head.pos.bag
NP.head NP. (JJ or CC)
NP.(DT or IN or TO) NP
NP.(DT or IN or TO) NP.head
NP.(DT or IN or TO) NP.head.pos
dp.dep dp.head
dp.head dp.dep
dp.dep dp.head.pos
dp.head dp.dep.pos
dp.dep dp.rel
dp.head dp.rel
VP.verb VP.NP
VP.verb VP.NP.head
VP.NP.head VP.verb
VP.verb VP.NP.head.pos
VP.NP.head VP.verb.pos
cw cw.l
i
, i ? {0, 1, 2, 3}
cw cw.r
i
, i ? {1, 2, 3}
cw cw.l
i
.pos, i ? {0, 1, 2, 3}
cw cw.r
i
.pos, i ? {1, 2, 3}
Table 4: Remained features after the feature selec-
tion.
Count Label
1091911 ?
31507 ?
3637 NPLURAL
2822 THE?
2600 LEMMA
948 ,?
300?900 A? PAST THE IN TO . IS OF ARE FOR
GERUND ,
50?100 AND ON AN? A VSINGULAR WAS THEIR
20?50 ELDERLY IT OF? THEY WITH TO?
WERE THIS ; ITS .? THAT ?S? AND?
THAT? HAVE? CAN AS HAVE?PART
FROM BE WOULD BY
15?20 HAVE HAS?WILL HAS AT AN THESE ?,
THEM IN? INTO #? ARE? WHICH PEO-
PLE HAS?PART ECONOMIC IS? BE? SO
COULD TO?LEMMA MANY PART MAY
LESS IT? FOR? BEING?
15?20 NOT ABOUT WILL?LEMMA SHOULD
HIS BECAUSE AGED SUCH ALSO
WHICH? HAVE?PAST WILL? WHO
WHEN MUCH
15?20 ON? ? THROUGH BE?PAST MORE
IF HELP THE?ELDERLY ?S ONE AS?
THERE THEIR? WITH? HAVE??
ECONOMY DEVELOPMENT CON-
CERNED PEOPLE? PROBLEMS BUT
MEANS THEREFORE HOWEVER BE-
ING : UP PROBLEM ?? THE?LEMMA
IN?ADDITION HOWEVER?,? AMONG
;? WHERE THUS ONLY HEALTH
HAS?PAST FUNDING EXTENT ALSO?
TECHNOLOGICAL ? OR HAD WOULD?
VERY .?THIS ITS? IMPORTANT DEVEL-
OPED ?BEEN AGE ABOUT?WHO? USE
THEY? THAN NUMBER HOWEVER?,
GOVERNMENT FURTHERMORE DURING
BUT? YOUNGER RIGHT POPULATION
PERSON? FEWER ENVIRONMENTAL-
LY WOULD?LEMMA OTHER MAY?
LIMITED HE COULD?HAVE BEEN STIL-
L SPENDING SAFETY OVER ONE??S
MAKE MADE LIFE HUMAN HAD?
FUNDS CARE ARGUED ALL ?? WHEN?
TIME THOSE SOCIETY RESEARCH
PROVIDE OLD NEEDS INCREASING DE-
VELOPING BECOME BE?? ADDITION
Table 5: Labels whose count is larger than 15.
current word feature
NC.l
1
NC, cw, cw.l
1
, cw.l
1
.pos,
cw.r
1
, cw.r
1
.pos
NP[0] NP.head, NP.l
1
, NP.l
2
,
cw, cw.l
1
, cw.l
1
.pos,
NP.head NP[0], NP.l
1
, NP.l
2
, cw,
cw.l
1
, cw.l
1
.pos,
dp.head cw, cw.l
1
, cw.l
2
dp.dep,
dp.dep.pos, dp.rel
Table 6: Examples of the new generated features.
79
4.3 Data Refinement
The training corpus is refined before used that sen-
tences which do not contain errors are filtered out.
Only 38% of the total sentences remain. With less
training corpus, it takes less time to train the ME
model. Table 7 presents the performance of sys-
tems using the unrefined training corpus and re-
fined corpus.
System Presicion Recall F 0.5
unrefined 26.99% 1.67% 6.71%
refined 11.17% 3.1% 7.34%
Table 7: Comparison of systems with differen-
t training corpus.
All sets of these systems are kept the same ex-
cept the training corpus they use. It can be seen
that the refinement also improves the performance
of the system.
4.4 Feature Selection
Figure 2 shows the results of systems with dif-
ferent feature sets. sys 10 is the system with
Figure 2: Performance of systems with different
features.
10 randomly chosen features which are used as
the initial set of features in Algorithm 3, sys 55
is the system with the refined 55 features. With
these refined features, various of new features are
generated by combining different features. This
combination is conducted empirically that features
which are considered having relations are com-
bined to generate new features. Using this method,
165 new features are generated and total 220 fea-
tures are used in sys 220. Table 6 gives a few
of examples showing the combined features. The
performance is evaluated by the precision, recal-
l and F 0.5 score of the M
2
scorer according
to (Dahlmeier and Ng, 2012). It can be seen
that sys 220 with the most number of features
achieves the best performance.
4.5 Evaluation Result
The final system we use is sys 220 with refined
training data, the performance of our system on the
developing corpus and the blind official test data is
presented in Table 8. The score is calculated using
M
2
scorer.
Data Set Precision Recall F 0.5
DEV 13.52% 6.41% 11.07%
OFFICIAL 29.83% 5.16% 15.24%
Table 8: Evaluation Results
5 Conclusion
In this paper, we describe the system of Shang-
hai Jiao Tong Univerity team in the CoNLL-2014
shared task. The grammatical error detection is re-
garded as a multi-label classification task and the
correction is conducted by applying a rule-based
approach based on these labels. A single max-
imum entropy classifier is introduced to do the
multi-label classification. Various features are in-
volved and a feature selection algorithm is used
to refine these features. Finally, large amounts of
feature templates that are generated by the combi-
nation of the refined features are used. This system
achieved precision of 29.83%, recall of 5.16% and
F 0.5 of 15.24% in the official evaluation.
References
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39?71.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAA-
CL 2012), pages 568?572, Montreal, Canada.
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng
Ng. 2012. NUS at the HOO 2012 Shared Task. In
Proceedings of the Seventh Workshop on Building E-
ducational Applications Using NLP, pages 216?224,
Montr?eal, Canada, June. Association for Computa-
tional Linguistics.
80
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The nus corpus of learner english. In Pro-
ceedings of the Eighth Workshop on Innovative Use
of NLP for Building Educational Applications (BEA
2013), pages 22?31, Atlanta, Georgia, USA.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. Hoo 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the Second Workshop on Building Education-
al Applications Using NLP, pages 54?62, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
NA-RAE Han, Martin Chodorow, and Claudia Lea-
cock. 2006. Detecting Errors in English Article
Usage by Non-Native Speakers. Natural Language
Engineering, 12:115?129, 5.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013a.
Grammatical error correction as multiclass classi-
fication with single model. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 74?81,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013b. Graph
model for chinese spell checking. In Proceedings
of the Seventh SIGHAN Workshop on Chinese Lan-
guage Processing, pages 88?92, Nagoya, Japan, Oc-
tober. Asian Federation of Natural Language Pro-
cessing.
Ting-hui Kao, Yu-wei Chang, Hsun-wen Chiu, Tzu-Hsi
Yen, Joanne Boisson, Jian-cheng Wu, and Jason S.
Chang. 2013. Conll-2013 shared task: Grammati-
cal error correction nthu system description. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 20?25, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Ekaterina Kochmar, ?istein Andersen, and Ted
Briscoe. 2012. HOO 2012 Error Recognition and
Correction Shared Task: Cambridge University Sub-
mission Report. In Proceedings of the Seventh
Workshop on Building Educational Applications Us-
ing NLP, pages 242?250, Montr?eal, Canada, June.
Association for Computational Linguistics.
Xuezhe Ma and Hai Zhao. 2012. Fourth-order depen-
dency parsing. In Proceedings of COLING 2012:
Posters, pages 785?796, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The conll-2014 shared task
on grammatical error correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task (CoNLL-2014
Shared Task), Baltimore, Maryland, USA.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the 13th European Workshop on Natural Lan-
guage Generation, pages 263?266. Association for
Computational Linguistics.
Alla Rozovskaya, Mark Sammons, and Dan Roth.
2012. The UI System in the HOO 2012 Shared Task
on Error Correction. In Proceedings of the Seventh
Workshop on Building Educational Applications Us-
ing NLP, pages 272?280, Montr?eal, Canada, June.
Association for Computational Linguistics.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The university of illinois sys-
tem in the conll-2013 shared task. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 13?19,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Tetreault, Joel R and Chodorow, Martin. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics-
Volume 1, pages 865?872. Association for Compu-
tational Linguistics.
Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumi-
ta, Hai Zhao, and Bao-Liang Lu. 2013. Convert-
ing continuous-space language models into n-gram
language models for statistical machine translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
845?850, Seattle, Washington, USA, October. Asso-
ciation for Computational Linguistics.
Yang Xiang, Bo Yuan, Yaoyun Zhang, Xiaolong Wang,
Wen Zheng, and Chongqiang Wei. 2013. A hybrid
model for grammatical error correction. In Proceed-
ings of the Seventeenth Conference on Computation-
al Natural Language Learning: Shared Task, pages
115?122, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Junwen Xing, Longyue Wang, Derek F. Wong, Lidi-
a S. Chao, and Xiaodong Zeng. 2013. Um-checker:
A hybrid system for english grammatical error cor-
rection. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 34?42, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Qiongkai Xu and Hai Zhao. 2012. Using deep lin-
guistic features for finding deceptive opinion spam.
In Proceedings of COLING 2012: Posters, pages
1341?1350, Mumbai, India, December. The COL-
ING 2012 Organizing Committee.
Shaohua Yang, Hai Zhao, Xiaolin Wang, and Bao
liang Lu. 2012. Spell checking for chinese.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet U?gur Do?gan,
Bente Maegaard, Joseph Mariani, Jan Odijk, and
81
Stelios Piperidis, editors, Proceedings of the Eighth
International Conference on Language Resources
and Evaluation (LREC-2012), pages 730?736, Is-
tanbul, Turkey, May. European Language Resources
Association (ELRA). ACL Anthology Identifier:
L12-1423.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuza-
wa, Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumo-
to. 2013. Naist at 2013 conll grammatical error
correction shared task. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 26?33, Sofi-
a, Bulgaria, August. Association for Computational
Linguistics.
Jingyi Zhang and Hai Zhao. 2013. Improving function
word alignment with frequency and syntactic infor-
mation. In Proceedings of the Twenty-Third inter-
national joint conference on Artificial Intelligence,
pages 2211?2217. AAAI Press, August.
Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009a.
Semantic dependency parsing of nombank and prop-
bank: An efficient integrated approach via a large-
scale feature selection. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 30?39, Singapore, August.
Association for Computational Linguistics.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009b. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 55?63,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Hai Zhao, Xiaotian Zhang, and Chunyu Kit. 2013. In-
tegrative semantic dependency parsing via efficien-
t large-scale feature selection. Journal of Artificial
Intelligence Research, 46:203?233.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 879?887, Athens, Greece,
March. Association for Computational Linguistics.
82
