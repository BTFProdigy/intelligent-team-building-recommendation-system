Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1070?1079,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
An Analysis of Active Learning Strategies for Sequence Labeling Tasks
Burr Settles??
?Dept. of Computer Sciences
University of Wisconsin
Madison, WI 53706, USA
bsettles@cs.wisc.edu
Mark Craven??
?Dept. of Biostatistics & Medical Informatics
University of Wisconsin
Madison, WI 53706, USA
craven@biostat.wisc.edu
Abstract
Active learning is well-suited to many prob-
lems in natural language processing, where
unlabeled data may be abundant but annota-
tion is slow and expensive. This paper aims
to shed light on the best active learning ap-
proaches for sequence labeling tasks such as
information extraction and document segmen-
tation. We survey previously used query selec-
tion strategies for sequence models, and pro-
pose several novel algorithms to address their
shortcomings. We also conduct a large-scale
empirical comparison using multiple corpora,
which demonstrates that our proposed meth-
ods advance the state of the art.
1 Introduction
Traditional supervised learning algorithms use
whatever labeled data is provided to induce a model.
By contrast, active learning gives the learner a de-
gree of control by allowing it to select which in-
stances are labeled and added to the training set. A
typical active learner begins with a small labeled set
L, selects one or more informative query instances
from a large unlabeled pool U , learns from these la-
beled queries (which are then added to L), and re-
peats. In this way, the learner aims to achieve high
accuracy with as little labeling effort as possible.
Thus, active learning can be valuable in domains
where unlabeled data are readily available, but ob-
taining training labels is expensive.
Such is the case with many sequence labeling
tasks in natural language domains. For example,
part-of-speech tagging (Seung et al, 1992; Lafferty
et al, 2001), information extraction (Scheffer et al,
2001; Sang and DeMeulder, 2003; Kim et al, 2004),
and document segmentation (Carvalho and Cohen,
2004) are all typically treated as sequence labeling
problems. The source data for these tasks (i.e., text
documents in electronic form) are often easily ob-
tained. However, due to the nature of sequence la-
beling tasks, annotating these texts can be rather te-
dious and time-consuming, making active learning
an attractive technique.
While there has been much work on active learn-
ing for classification (Cohn et al, 1994; McCallum
and Nigam, 1998; Zhang and Oles, 2000; Zhu et
al., 2003), active learning for sequence labeling has
received considerably less attention. A few meth-
ods have been proposed, based mostly on the con-
ventions of uncertainty sampling, where the learner
queries the instance about which it has the least cer-
tainty (Scheffer et al, 2001; Culotta and McCallum,
2005; Kim et al, 2006), or query-by-committee,
where a ?committee? of models selects the instance
about which its members most disagree (Dagan and
Engelson, 1995). We provide more detail on these
and the new strategies we propose in Section 3.
The comparative effectiveness of these ap-
proaches, however, has not been studied. Further-
more, it has been suggested that uncertainty sam-
pling and query-by-committee fail on occasion (Roy
and McCallum, 2001; Zhu et al, 2003) by query-
ing outliers, e.g., instances considered informative
in isolation by the learner, but containing little infor-
mation about the rest of the distribution of instances.
Proposed methods for dealing with these shortcom-
ings have so far only considered classification tasks.
1070
This paper presents two major contributions for
active learning and sequence labeling tasks. First,
we motivate and introduce several new query strate-
gies for probabilistic sequence models. Second, we
conduct a thorough empirical analysis of previously
proposed methods with our algorithms on a variety
of benchmark corpora. The remainder of this pa-
per is organized as follows. Section 2 provides a
brief introduction to sequence labeling and condi-
tional random fields (the sequence model used in
our experiments). Section 3 describes in detail all
the query selection strategies we consider. Section 4
presents the results of our empirical study. Section 5
concludes with a summary of our findings.
2 Sequence Labeling and CRFs
In this paper, we are concerned with active learn-
ing for sequence labeling. Figure 1 illustrates
how, for example, an information extraction prob-
lem can be viewed as a sequence labeling task.
Let x = ?x1, . . . , xT ? be an observation sequence
of length T with a corresponding label sequence
y = ?y1, . . . , yT ?. Words in a sentence corre-
spond to tokens in the input sequence x, which are
mapped to labels in y. These labels indicate whether
the word belongs to a particular entity class of inter-
est (in this case, org and loc) or not (null). These
labels can be assigned by a sequence model based
on a finite state machine, such as the one shown to
the right in Figure 1.
We focus our discussion of active learning for
sequence labeling on conditional random fields, or
CRFs (Lafferty et al, 2001). The rest of this sec-
tion serves as a brief introduction. CRFs are sta-
tistical graphical models which have demonstrated
state-of-the-art accuracy on virtually all of the se-
quence labeling tasks mentioned in Section 1. We
use linear-chain CRFs, which correspond to condi-
tionally trained probabilistic finite state machines.
A linear-chain CRF model with parameters ? de-
fines the posterior probability of y given x to be1:
P (y|x; ?) =
1
Z(x)
exp
(
T?
t=1
K?
k=1
?kfk(yt?1, yt,xt)
)
.
(1)
1Our discussion assumes, without loss of generality, that
each label is uniquely represented by one state, thus each label
sequence y corresponds to exactly one path through the model.
loc
orgnull
x:
y:
...the
null
ACME
org
Inc.
org
offices
null
in
null
Chicago
loc
Figure 1: An information extraction example treated as
a sequence labeling task. Also shown is a corresponding
sequence model represented as a finite state machine.
Here Z(x) is a normalization factor over all pos-
sible labelings of x, and ?k is one of K model
parameter weights corresponding to some feature
fk(yt?1, yt,xt). Each feature fk describes the se-
quence x at position t with label yt, observed along
a transition from label states yt?1 to yt in the finite
state machine. Consider the example text from Fig-
ure 1. Here, fk might be the feature WORD=ACME
and have the value fk = 1 along a transition from
the null state to the org state (and 0 elsewhere).
Other features set to 1 here might be ALLCAPS and
NEXTWORD=Inc. The weights in ? are set to max-
imize the conditional log likelihood ` of training se-
quences in the labeled data set L:
`(L; ?) =
L?
l=1
logP (y(l)|x(l); ?) ?
K?
k=1
?2k
2?2
,
where L is the size of the labeled set L, and the sec-
ond term is a Gaussian regularization penalty on ???
to prevent over-fitting. After training, labels can be
predicted for new sequences using the Viterbi algo-
rithm. For more details on CRFs and their training
procedures, see Sutton and McCallum (2006).
Note that, while we describe the active learning
algorithms in the next section in terms of linear-
chain CRFs, they have analogs for other kinds of
sequence models, such as hidden Markov models,
or HMMs (Rabiner, 1989), probabilistic context-
free grammars (Lari and Young, 1990), and general
CRFs (Sutton and McCallum, 2006).
3 Active Learning with Sequence Models
In order to select queries, an active learner must have
a way of assessing how informative each instance is.
Let x? be the most informative instance according to
some query strategy ?(x), which is a function used
to evaluate each instance x in the unlabeled pool U .
1071
Given: Labeled set L, unlabeled pool U , query
strategy ?(?), query batch size B
repeat
// learn a model using the current L
? = train(L) ;
for b = 1 to B do
// query the most informative instance
x?b = argmaxx?U ?(x) ;
// move the labeled query from U to L
L = L ? ?x?b , label(x
?
b)? ;
U = U ? x?b ;
end
until some stopping criterion ;
Algorithm 1: Pool-based active learning.
Algorithm 1 provides a sketch of the generic pool-
based active learning scenario.
In the remainder of this section, we describe var-
ious query strategy formulations of ?(?) that have
been used for active learning with sequence mod-
els. We also point out where we think these ap-
proaches may be flawed, and propose several novel
query strategies to address these issues.
3.1 Uncertainty Sampling
One of the most common general frameworks for
measuring informativeness is uncertainty sampling
(Lewis and Catlett, 1994), where a learner queries
the instance that it is most uncertain how to la-
bel. Culotta and McCallum (2005) employ a sim-
ple uncertainty-based strategy for sequence models
called least confidence (LC):
?LC(x) = 1 ? P (y?|x; ?).
Here, y? is the most likely label sequence, i.e., the
Viterbi parse. This approach queries the instance
for which the current model has the least confidence
in its most likely labeling. For CRFs, this confi-
dence can be calculated using the posterior proba-
bility given by Equation (1).
Scheffer et al (2001) propose another uncertainty
strategy, which queries the instance with the smallest
margin between the posteriors for its two most likely
labelings. We call this approach margin (M):
?M (x) = ?
(
P (y?1|x; ?) ? P (y
?
2|x; ?)
)
.
Here, y?1 and y
?
2 are the first and second best la-
bel sequences, respectively. These can be efficiently
computed using the N -best algorithm (Schwartz
and Chow, 1990), a beam-search generalization of
Viterbi, with N = 2. The minus sign in front is sim-
ply to ensure that ?M acts as a maximizer for use
with Algorithm 1.
Another uncertainty-based measure of informa-
tiveness is entropy (Shannon, 1948). For a dis-
crete random variable Y , the entropy is given by
H(Y ) = ?
?
i P (yi) logP (yi), and represents the
information needed to ?encode? the distribution of
outcomes for Y . As such, is it often thought of as
a measure of uncertainty in machine learning. In
active learning, we wish to use the entropy of our
model?s posteriors over its labelings. One way this
has been done with probabilistic sequence models is
by computing what we call token entropy (TE):
?TE(x) = ?
1
T
T?
t=1
M?
m=1
P?(yt = m) logP?(yt = m),
(2)
where T is the length of x, m ranges over all pos-
sible token labels, and P?(yt = m) is shorthand
for the marginal probability that m is the label at
position t in the sequence, according to the model.
For CRFs and HMMs, these marginals can be effi-
ciently computed using the forward and backward
algorithms (Rabiner, 1989). The summed token en-
tropies have typically been normalized by sequence
length T , to avoid simply querying longer sequences
(Baldridge and Osborne, 2004; Hwa, 2004). How-
ever, we argue that querying long sequences should
not be explicitly discouraged, if in fact they contain
more information. Thus, we also propose the total
token entropy (TTE) measure:
?TTE(x) = T ? ?TE(x).
For most sequence labeling tasks, however, it is
more appropriate to consider the entropy of the la-
bel sequence y as a whole, rather than some aggre-
gate of individual token entropies. Thus an alternate
query strategy is sequence entropy (SE):
?SE(x) = ?
?
y?
P (y?|x; ?) logP (y?|x; ?), (3)
where y? ranges over all possible label sequences for
input sequence x. Note, however, that the number
1072
of possible labelings grows exponentially with the
length of x. To make this feasible, previous work
(Kim et al, 2006) has employed an approximation
we call N-best sequence entropy (NSE):
?NSE(x) = ?
?
y??N
P (y?|x; ?) logP (y?|x; ?),
where N = {y?1, . . . ,y
?
N}, the set of the N most
likely parses, and the posteriors are re-normalized
(i.e., Z(x) in Equation (1) only ranges over N ). For
N = 2, this approximation is equivalent to ?M , thus
N -best sequence entropy can be thought of as a gen-
eralization of the margin approach.
Recently, an efficient entropy calculation via dy-
namic programming was proposed for CRFs in the
context of semi-supervised learning (Mann and Mc-
Callum, 2007). We use this algorithm to compute
the true sequence entropy (3) for active learning in
a constant-time factor of Viterbi?s complexity. Hwa
(2004) employed a similar approach for active learn-
ing with probabilistic context-free grammars.
3.2 Query-By-Committee
Another general active learning framework is the
query-by-committee (QBC) approach (Seung et al,
1992). In this setting, we use a committee of models
C = {?(1), . . . , ?(C)} to represent C different hy-
potheses that are consistent with the labeled set L.
The most informative query, then, is the instance
over which the committee is in most disagreement
about how to label.
In particular, we use the query-by-bagging ap-
proach (Abe and Mamitsuka, 1998) to learn a com-
mittee of CRFs. In each round of active learning,
L is sampled (with replacement) L times to create
a unique, modified labeled set L(c). Each model
?(c) ? C is then trained using its own corresponding
labeled set L(c). To measure disagreement among
committee members, we consider two alternatives.
Dagan and Engelson (1995) introduced QBC with
HMMs for part-of-speech tagging using a measure
called vote entropy (VE):
?V E(x) = ?
1
T
T?
t=1
M?
m=1
V (yt,m)
C
log
V (yt,m)
C
,
where V (yt,m) is the number of ?votes? labelm re-
ceives from all the committee member?s Viterbi la-
belings at sequence position t.
McCallum and Nigam (1998) propose a QBC
strategy for classification based on Kullback-Leibler
(KL) divergence, an information-theoretic measure
of the difference between two probability distribu-
tions. The most informative query is considered to
be the one with the largest average KL divergence
between a committee member?s posterior label dis-
tribution and the consensus. We modify this ap-
proach for sequence models by summing the average
KL scores using the marginals at each token position
and, as with vote entropy, normalizing for length.
We call this approach Kullback-Leibler (KL):
?KL(x) =
1
T
T?
t=1
1
C
C?
c=1
D(?(c)?C),
where (using shorthand again):
D(?(c)?C) =
M?
m=1
P?(c)(yt = m) log
P?(c)(yt = m)
PC(yt = m)
.
Here PC(yt = m) = 1C
?C
c=1 P?(c)(yt = m), or the
?consensus? marginal probability that m is the label
at position t in the sequence.
Both of these disagreement measures are normal-
ized for sequence length T . As with token en-
tropy (2), this may bias the learner toward query-
ing shorter sequences. To study the effects of nor-
malization, we also conduct experiments with non-
normalized variants ?TV E and ?TKL.
Additionally, we argue that these token-level dis-
agreement measures may be less appropriate for
most tasks than measuring the committee?s disagree-
ment about the label sequence y as a whole. There-
fore, we propose sequence vote entropy (SVE):
?SV E(x) = ?
?
y??NC
P (y?|x; C) logP (y?|x; C),
where N C is the union of the N -best parses from
all models in the committee C, and P (y?|x; C) =
1
C
?C
c=1 P (y?|x; ?
(c)), or the ?consensus? posterior
probability for some label sequence y?. This can be
thought of as a QBC generalization of N -best en-
tropy, where each committee member casts a vote
for the posterior label distribution. We also explore
a sequence Kullback-Leibler (SKL) variant:
?SKL(x) =
1
C
C?
c=1
?
y??NC
P (y?|x; ?(c)) log
P (y?|x; ?(c))
P (y?|x; C)
.
1073
3.3 Expected Gradient Length
A third general active learning framework we con-
sider is to query the instance that would impart the
greatest change to the current model if we knew its
label. Since we train discriminative models like
CRFs using gradient-based optimization, this in-
volves querying the instance which, if labeled and
added to the training set, would create the greatest
change in the gradient of the objective function (i.e.,
the largest gradient vector used to re-estimate pa-
rameter values).
Let ?`(L; ?) be the gradient of the log-
likelihood ` with respect to the model parameters ?,
as given by Sutton and McCallum (2006). Now let
?`(L+?x,y?; ?) be the new gradient that would be
obtained by adding the training tuple ?x,y? to L.
Since the query algorithm does not know the true la-
bel sequence y in advance, we instead calculate the
expected gradient length (EGL):
?EGL(x) =
?
y??N
P (y?|x; ?)
?
?
??`(L+?x,y??; ?)
?
?
? ,
approximated as an expectation over the N -best la-
belings, where ? ? ? is the Euclidean norm of each
resulting gradient vector. We first introduced this ap-
proach in previous work on multiple-instance active
learning (Settles et al, 2008), and adapt it to query
selection with sequences here. Note that, at query
time, ?`(L; ?) should be nearly zero since ` con-
verged at the previous round of training. Thus, we
can approximate ?`(L+?x,y??; ?) ? ?`(?x, y??; ?)
for computational efficiency, because the training in-
stances are assumed to be independent.
3.4 Information Density
It has been suggested that uncertainty sampling and
QBC are prone to querying outliers (Roy and Mc-
Callum, 2001; Zhu et al, 2003). Figure 2 illus-
trates this problem for a binary linear classifier us-
ing uncertainty sampling. The least certain instance
lies on the classification boundary, but is not ?rep-
resentative? of other instances in the distribution, so
knowing its label is unlikely to improve accuracy on
the data as a whole. QBC and EGL exhibit similar
behavior, by spending time querying possible out-
liers simply because they are controversial, or are
expected to impart significant change in the model.
A
B
Figure 2: An illustration of when uncertainty sampling
can be a poor strategy for classification. Shaded poly-
gons represent labeled instances (L), and circles repre-
sent unlabeled instances (U). Since A is on the decision
boundary, it will be queried as the most uncertain. How-
ever, querying B is likely to result in more information
about the data as a whole.
We argue that this phenomenon can occur with se-
quence labeling tasks as well as with classification.
To address this, we propose a new active learning
approach called information density (ID):
?ID(x) = ?SE(x) ?
(
1
U
U?
u=1
sim(x,x(u))
)?
.
That is, the informativeness of x is weighted by its
average similarity to all other sequences in U , sub-
ject to a parameter ? that controls the relative im-
portance of the density term. In the formulation pre-
sented above, sequence entropy ?SE measures the
?base? informativeness, but we could just as easily
use any of the instance-level strategies presented in
the previous sections.
This density measure requires us to compute the
similarity of two sequences. To do this, we first
transform each x, which is a sequence of feature
vectors (tokens), into a single kernel vector ~x:
~x =
[
T?
t=1
f1(xt), . . . ,
T?
t=1
fJ(xt)
]
,
where fj(xt) is the value of feature fj for token xt,
and J is the number of features in the input represen-
tation2. In other words, sequence x is compressed
into a fixed-length feature vector ~x, for which each
element is the sum of the corresponding feature?s
values across all tokens. We can then use cosine
2Note that J 6= K, and fj(xt) here differs slightly from the
feature definition given in Section 2. Since the labels yt?1 and
yt are unknown before querying, the K features used for model
training are reduced down to the J input features here, which
factor out any label dependencies.
1074
similarity on this simplified representation:
simcos(x,x(u)) =
~x ? ~x(u)
?~x? ? ?~x(u)?
.
We have also investigated similarity functions
based on exponentiated Euclidean distance and KL-
divergence, the latter of which was also employed by
McCallum and Nigam (1998) for density-weighting
QBC in text classification. However, these measures
show no improvement over cosine similarity, and re-
quire setting additional hyper-parameters.
One potential drawback of information density is
that the number of required similarity calculations
grows quadratically with the number of instances
in U . For pool-based active learning, we often as-
sume that the size of U is very large. However,
these densities only need to be computed once, and
are independent of the base information measure.
Thus, when employing information density in a real-
world interactive learning setting, the density scores
can simply be pre-computed and cached for efficient
lookup during the actual active learning process.
3.5 Fisher Information
We also introduce a query selection strategy for se-
quence models based on Fisher information, build-
ing on the theoretical framework of Zhang and Oles
(2000). Fisher information I(?) represents the over-
all uncertainty about the estimated model parame-
ters ?, as given by:
I(?) = ?
?
x
P (x)
?
y
P (y|x; ?)
?2
??2
logP (y|x; ?).
For a model with K parameters, the Fisher infor-
mation takes the form of a K ? K covariance ma-
trix. Our goal in active learning is to select the query
that most efficiently minimizes the model variance
reflected in I(?). This can be accomplished by op-
timizing the Fisher information ratio (FIR):
?FIR(x) = ?tr
(
Ix(?)
?1IU (?)
)
, (4)
where Ix(?) and IU (?) are Fisher information ma-
trices for sequence x and the unlabeled pool U , re-
spectively. The leading minus sign again ensures
that ?FIR is a maximizer for use with Algorithm 1.
Previously, Fisher information for active learning
has only been investigated in the context of simple
binary classification. When employing FIR with se-
quence models like CRFs, there are two additional
computational challenges. First, we must integrate
over all possible labelings y, which can, as we have
seen, be approximated as an expectation over theN -
best labelings. Second, the inner product in the ratio
calculation (4) requires inverting a K ? K matrix
for each x. In most interesting natural language ap-
plications, K is very large, making this algorithm
intractable. However, it is common in similar situ-
ations to approximate the Fisher information matrix
with its diagonal (Nyffenegger et al, 2006). Thus
we estimate Ix(?) using:
Ix(?) =
?
y??N
P (y?|x; ?)
[(
? logP (y?|x; ?)
??1
)2
+ ?, . . . ,
(
? logP (y?|x; ?)
??K
)2
+ ?
]
,
and IU (?) using:
IU (?) =
1
U
U?
u=1
Ix(u)(?).
For CRFs, the partial derivative at the root of each
element in the diagonal vector is given by:
? logP (y?|x; ?)
??k
=
T?
t=1
fk(y?t?1, y?t,xt)
?
T?
t=1
?
y,y?
P (y, y?|x)fk(y, y?,xt),
which is similar to the equation used to compute the
training gradient, but without a regularization term.
A smoothing parameter ?  1 is added to prevent
division by zero when computing the ratio.
Notice that this method implicitly selects repre-
sentative instances by favoring queries with Fisher
information Ix(?) that is not only high, but similar
to that of the overall data distribution IU (?). This
is in contrast to information density, which tries to
query representative instances by explicitly model-
ing the distribution with a density weight.
1075
Corpus Entities Features Instances
CoNLL-03 4 78,644 19,959
NLPBA 5 128,401 18,854
BioCreative 1 175,331 10,000
FlySlip 1 31,353 1,220
CORA:Headers 15 22,077 935
CORA:References 13 4,208 500
Sig+Reply 2 25 617
SigIE 12 10,600 250
Table 1: Properties of the different evaluation corpora.
4 Empirical Evaluation
In this section we present a large-scale empirical
analysis of the query strategies described in Sec-
tion 3 on eight benchmark information extraction
and document segmentation corpora. The data sets
are summarized in Table 1.
4.1 Data and Methodology
CoNLL-03 (Sang and DeMeulder, 2003) is a col-
lection of newswire articles annotated with four en-
tities: person, organization, location, and misc.
NLPBA (Kim et al, 2004) is a large collection
of biomedical abstracts annotated with five entities
of interest, such as protein, RNA, and cell-type.
BioCreative (Yeh et al, 2005) and FlySlip (Vla-
chos, 2007) also comprise texts in the biomedical
domain, annotated for gene entity mentions in arti-
cles from the human and fruit fly literature, respec-
tively. CORA (Peng and McCallum, 2004) consists
of two collections: a set of research paper headers
annotated for entities such as title, author, and insti-
tution; and a collection of references annotated with
BibTeX fields such as journal, year, and publisher.
The Sig+Reply corpus (Carvalho and Cohen, 2004)
is a set of email messages annotated for signature
and quoted reply line segments. SigIE is a subset of
the signature blocks from Sig+Reply which we have
enhanced with several address book fields such as
name, email, and phone. All corpora are format-
ted in the ?IOB? sequence representation (Ramshaw
and Marcus, 1995).
We implement all fifteen query selection strate-
gies described in Section 3 for use with CRFs, and
evaluate them on all eight data sets. We also com-
pare against two baseline strategies: random in-
stance selection (i.e., passive learning), and na??vely
querying the longest sequence in terms of tokens.
We use a typical feature set for each corpus based on
the cited literature (including words, orthographic
patterns, part-of-speech, lexicons, etc.). Where the
N -best approximation is used N = 15, and for all
QBC methods C = 3; these figures exhibited a good
balance of accuracy and training speed in prelimi-
nary work. For information density, we arbitrarily
set ? = 1 (i.e., the information and density terms
have equal weight). In each experiment, L is ini-
tialized with five random labeled instances, and up
to 150 queries are subsequently selected from U in
batches of size B = 5. All results are averaged
across five folds using cross-validation.
We evaluate each query strategy by constructing
learning curves that plot the overall F1 measure (for
all entities or segments) as a function of the num-
ber of instances queried. Due to lack of space, we
cannot show learning curves for every experiment.
Instead, Table 2 summarizes our results by reporting
the area under the learning curve for all strategies
on all data. Figure 3 presents a few representative
learning curves for six of the corpora.
4.2 Discussion of Learning Curves
The first conclusion we can draw from these results
is that there is no single clear winner. However, in-
formation density (ID), which we introduce in this
paper, stands out. It usually improves upon the base
sequence entropy measure, never performs poorly,
and has the highest average area under the learning
curve across all tasks. It seems particularly effective
on large corpora, which is a typical assumption for
the active learning setting. Sequence vote entropy
(SVE), a QBCmethod we propose here, is also note-
worthy in that it is fairly consistently among the top
three strategies, although never the best.
Second, the top uncertainty sampling strategies
are least confidence (LC) and sequence entropy
(SE), the latter being the dominant entropy-based
method. Among the QBC strategies, sequence vote
entropy (SVE) is the clear winner. We conclude that
these three methods are the best base information
measures for use with information density.
Third, query strategies that evaluate the en-
tire sequence (SE, SVE, SKL) are generally su-
perior to those which aggregate token-level infor-
mation. Furthermore, the total token-level strate-
gies (TTE, TVE, TKL) outperform their length-
1076
Baselines Uncertainty Sampling Query-By-Committee Other
Corpus Rand Long LC M TE TTE SE NSE VE KL TVE TKL SVE SKL EGL ID FIR
CoNLL-03 78.8 79.4 89.4 84.5 38.9 89.7 90.1 89.1 45.9 62.0 86.7 81.7 89.0 87.9 87.3 89.6 81.7
NLPBA 59.9 67.6 71.0 62.9 53.4 70.9 71.5 68.9 52.4 53.1 66.9 63.5 71.8 68.5 69.3 73.1 73.6
BioCreative 34.6 26.9 54.8 46.8 37.8 53.0 56.0 50.5 35.2 37.4 49.2 45.1 56.6 50.8 51.5 59.1 58.8
FlySlip 112.1 121.0 125.1 119.5 110.3 124.9 125.4 124.1 113.3 109.4 124.1 119.5 122.7 120.7 125.9 126.8 118.2
Headers 76.0 78.2 81.4 78.6 78.5 78.5 80.8 80.4 72.8 78.5 79.7 78.5 80.7 78.4 79.6 80.2 79.1
References 90.0 86.0 89.8 91.5 84.4 88.6 88.4 89.4 85.1 89.1 88.7 88.2 89.9 86.9 88.2 88.7 87.1
Sig+Reply 129.1 129.6 132.1 132.3 131.7 131.6 131.4 133.1 131.4 130.7 132.1 130.6 132.8 132.3 130.5 131.5 133.2
SigIE 84.3 82.7 88.8 87.3 89.3 88.3 87.6 89.1 89.8 85.5 89.7 85.1 89.5 89.7 87.7 88.5 88.5
Average 83.1 83.9 91.6 87.9 78.0 90.7 91.4 90.6 78.2 80.7 89.6 86.5 91.6 89.4 90.0 92.2 90.0
Table 2: Detailed results for all query strategies on all evaluation corpora. Reported is the area under the F1 learning
curve for each strategy after 150 queries (maximum possible score is 150). For each row, the best method is shown
boxed in bold, the second best is shown underlined in bold, and the third best is shown in bold. The last row summa-
rizes the results across all eight tasks by reporting the average area for each strategy. Query strategy formulations for
sequence models introduced in this paper are indicated with italics along the top.
normalized counterparts (TE, VE, KL) in nearly all
cases. In fact, the normalized variants are often in-
ferior even to the baselines. While an argument can
be made that these shorter sequences might be eas-
ier to label from a human annotator?s perspective,
our ongoing work indicates that the relationship be-
tween instance length and actual labeling costs (e.g.,
elapsed annotation time) is not a simple one. Anal-
ysis of our experiment logs also shows that length-
normalized methods are occasionally biased toward
short sequences with little intuitive value (e.g., sen-
tences with few or no entities to label). In addition,
vote entropy appears to be a better disagreement
measure for QBC strategies than KL divergence.
Finally, Fisher information (FIR), while theoreti-
cally sound, exhibits behavior that is difficult to in-
terpret. It is sometimes the winning strategy, but oc-
casionally only on par with the baselines. When it
does show significant gains over the other strategies,
these gains appear to be only for the first several
queries (e.g., NLPBA and BioCreative in Figure 3).
This inconsistent performance may be a result of the
approximations made for computational efficiency.
Expected gradient length (EGL) also appears to ex-
hibit mediocre performance, and is likely not worth
its additional computational expense.
4.3 Discussion of Run Times
Here we discuss the execution times for each query
strategy using current hardware. The uncertainty
sampling methods are roughly comparable in run
time (token-based methods run slightly faster), each
routinely evaluating tens of thousands of sequences
in under a minute. The QBC methods, on the other
hand, must re-train multiple models with each query,
resulting in a lag of three to four minutes per query
batch (and up to 20 minutes for corpora with more
entity labels).
The expected gradient length and Fisher informa-
tion methods are the most computationally expen-
sive, because they must first perform inference over
the possible labelings and then calculate gradients
for each candidate label sequence. As a result, they
take eight to ten minutes (upwards of a half hour on
the larger corpora) for each query. Unlike the other
strategies, their time complexities also scale linearly
with the number of model parameters K which, in
turn, increases as new sequences are added to L.
As noted in Section 3.4, information density in-
curs a large computational cost to estimate the den-
sity weights, but these can be pre-computed and
cached for efficient lookup. In our experiments, this
pre-processing step takes less than a minute for the
smaller corpora, about a half hour for CoNLL-03
and BioCreative, and under two hours for NLPBA.
The density lookup causes no significant change in
the run time of the base information measure. Given
these results, we advocate information density with
an uncertainty sampling base measure in practice,
particularly for active learning with large corpora.
5 Conclusion
In this paper, we have presented a detailed analy-
sis of active learning for sequence labeling tasks.
In particular, we have described and criticized the
query selection strategies used with probabilistic se-
1077
F1 m
eas
ure
F1 m
eas
ure
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
CoNLL-03
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
NLPBA
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
BioCreative
number of instances queried
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
FlySlip
number of instances queried
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
Sig+Reply
number of instances queried
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  20  40  60  80  100  120  140
information density (ID)Fisher information (FIR)query-by-committee (SVE)random
SigIE
Figure 3: Learning curves for selected query strategies on six of the evaluation corpora.
quence models to date, and proposed several novel
strategies to address some of their shortcomings.
Our large-scale empirical evaluation demonstrates
that some of these newly proposed methods advance
the state of the art in active learning with sequence
models. These methods include information density
(which we recommend in practice), sequence vote
entropy, and sometimes Fisher information.
Acknowledgments
We would like to thank the anonymous reviewers for
their helpful feedback. This work was supported by
NIH grants T15-LM07359 and R01-LM07050.
References
N. Abe and H. Mamitsuka. 1998. Query learning strate-
gies using boosting and bagging. In Proceedings of
the International Conference on Machine Learning
(ICML), pages 1?9. Morgan Kaufmann.
J. Baldridge and M. Osborne. 2004. Active learning and
the total cost of annotation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 9?16. ACL Press.
V.R. Carvalho and W. Cohen. 2004. Learning to extract
signature and reply lines from email. In Proceedings
of the Conference on Email and Anti-Spam (CEAS).
D. Cohn, L. Atlas, and R. Ladner. 1994. Improving gen-
eralization with active learning. Machine Learning,
15(2):201?221.
A. Culotta and A. McCallum. 2005. Reducing labeling
effort for stuctured prediction tasks. In Proceedings
of the National Conference on Artificial Intelligence
(AAAI), pages 746?751. AAAI Press.
I. Dagan and S. Engelson. 1995. Committee-based
sampling for training probabilistic classifiers. In Pro-
ceedings of the International Conference on Machine
Learning (ICML), pages 150?157. Morgan Kaufmann.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3):73?77.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recognition
task at JNLPBA. In Proceedings of the International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (NLPBA), pages 70?
75.
S. Kim, Y. Song, K. Kim, J.W. Cha, and G.G. Lee.
2006. MMR-based active machine learning for bio
named entity recognition. In Proceedings of Human
Language Technology and the North American Asso-
ciation for Computational Linguistics (HLT-NAACL),
pages 69?72. ACL Press.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning
(ICML), pages 282?289. Morgan Kaufmann.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4:35?56.
D. Lewis and J. Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Pro-
1078
ceedings of the International Conference on Machine
Learning (ICML), pages 148?156. Morgan Kaufmann.
G. Mann and A. McCallum. 2007. Efficient computation
of entropy gradient for semi-supervised conditional
random fields. In Proceedings of the North American
Association for Computational Linguistics (NAACL),
pages 109?112. ACL Press.
A. McCallum and K. Nigam. 1998. Employing EM
in pool-based active learning for text classification.
In Proceedings of the International Conference on
Machine Learning (ICML), pages 359?367. Morgan
Kaufmann.
M. Nyffenegger, J.C. Chappelier, and E. Gaussier. 2006.
Revisiting Fisher kernels for document similarities. In
Proceedings of the European Conference on Machine
Learning (ECML), pages 727?734. Springer.
F. Peng and A. McCallum. 2004. Accurate information
extraction from research papers using conditional ran-
dom fields. In Proceedings of Human Language Tech-
nology and the North American Association for Com-
putational Linguistics (HLT-NAACL). ACL Press.
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257?286.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the ACL Workshop on Very Large Corpora.
N. Roy and A. McCallum. 2001. Toward optimal active
learning through sampling estimation of error reduc-
tion. In Proceedings of the International Conference
on Machine Learning (ICML), pages 441?448. Mor-
gan Kaufmann.
E.F.T.K. Sang and F. DeMeulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proceedings of the
Conference on Natural Language Learning (CoNLL),
pages 142?147.
T. Scheffer, C. Decomain, and S. Wrobel. 2001. Ac-
tive hidden Markov models for information extraction.
In Proceedings of the International Conference on Ad-
vances in Intelligent Data Analysis (CAIDA), pages
309?318. Springer-Verlag.
R. Schwartz and Y.-L. Chow. 1990. The N -best algo-
rithm: an efficient and exact procedure for finding the
N most likely sentence hypotheses. In Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 81?83. IEEE
Press.
B. Settles, M. Craven, and S. Ray. 2008. Multiple-
instance active learning. In Advances in Neural Infor-
mation Processing Systems (NIPS), volume 20, pages
1289?1296. MIT Press.
H.S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proceedings of the ACM
Workshop on Computational Learning Theory, pages
287?294.
C. E. Shannon. 1948. A mathematical theory of com-
munication. Bell System Technical Journal, 27:379?
423,623?656.
C. Sutton and A. McCallum. 2006. An introduction to
conditional random fields for relational learning. In
L. Getoor and B. Taskar, editors, Introduction to Sta-
tistical Relational Learning. MIT Press.
A. Vlachos. 2007. Evaluating and combining biomedical
named entity recognition systems. In BioNLP 2007:
Biological, translational, and clinical language pro-
cessing, pages 199?206.
A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman.
2005. Biocreative task 1a: gene mention finding eval-
uation. BMC Bioinformatics, 6(Suppl 1):S2.
T. Zhang and F.J. Oles. 2000. A probability analysis
on the value of unlabeled data for classification prob-
lems. In Proceedings of the International Conference
onMachine Learning (ICML), pages 1191?1198. Mor-
gan Kaufmann.
X. Zhu, J. Lafferty, and Z. Ghahramani. 2003. Combin-
ing active learning and semi-supervised learning using
gaussian fields and harmonic functions. In Proceed-
ings of the ICML Workshop on the Continuum from
Labeled to Unlabeled Data, pages 58?65.
1079
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 18?25,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Detecting Speculative Language Using Syntactic Dependencies
and Logistic Regression
Andreas Vlachos and Mark Craven
Department of Biostatistics and Medical Informatics
University of Wisconsin-Madison
{vlachos,craven}@biostat.wisc.edu
Abstract
In this paper we describe our approach
to the CoNLL-2010 shared task on de-
tecting speculative language in biomedical
text. We treat the detection of sentences
containing uncertain information (Task1)
as a token classification task since the
existence or absence of cues determines
the sentence label. We distinguish words
that have speculative and non-speculative
meaning by employing syntactic features
as a proxy for their semantic content. In
order to identify the scope of each cue
(Task2), we learn a classifier that predicts
whether each token of a sentence belongs
to the scope of a given cue. The features
in the classifier are based on the syntactic
dependency path between the cue and the
token. In both tasks, we use a Bayesian
logistic regression classifier incorporating
a sparsity-enforcing Laplace prior. Over-
all, the performance achieved is 85.21%
F-score and 44.11% F-score in Task1 and
Task2, respectively.
1 Introduction
The term speculative language, also known as
hedging, refers to expressions of uncertainty over
statements. Recognition of such statements is im-
portant for higher-level applications. For exam-
ple, a multi-document summarization system can
assign different weights to speculative and non-
speculative statements when aggregating informa-
tion on a particular issue.
The CoNLL-2010 shared task (Farkas et al,
2010) formulates speculative language detection
as two subtasks. In the first subtask (Task1), sys-
tems need to determine whether a sentence con-
tains uncertain information or not. In the sec-
ond subtask (Task2), systems need to identify the
hedge cues and their scope in the sentence. Table 1
provides an example from the training data.
The participants are provided with data from
two domains: biomedical scientific literature (both
abstracts and full articles) and Wikipedia. We
choose to focus on the former. The training data
for this domain are nine full articles and 1,273 ab-
stracts from the BioScope corpus (Szarvas et al,
2008) and the test data are 15 full articles.
Our approach to speculative language detection
relies on syntactic parsing and machine learning.
We give a description of the techniques used in
Sections 2 and 3. We treat the detection of sen-
tences containing uncertain information (Task1) as
a token classification task in which we learn a clas-
sifier to predict whether a token is a cue or not. In
order to handle words that have speculative and
non-speculative meaning (e.g. ?indicating? in the
example of Table 1), we employ syntactic features
as a proxy for their semantic content (Section 4).
For scope identification (Task2), we learn a clas-
sifier that predicts whether each token of the sen-
tence belongs to the scope of a particular cue (Sec-
tion 6). The features used are based on the syntac-
tic dependency path between the cue and the to-
ken. We report results and perform error analysis
for both tasks, pointing out annotation issues that
could be ameliorated (Sections 5 and 7). Based on
our experience we suggest improvements on the
task definition taking into account work from the
broader field (Section 8).
2 Syntactic parsing for the biomedical
domain
The syntactic parser we chose for our experi-
ments is the C&C Combinatory Categorial Gram-
mar (CCG) parser adapted to the biomedical do-
main (Rimell and Clark, 2009). In this frame-
work, parsing is performed in three stages: part-
of-speech (PoS) tagging, CCG supertagging and
parse selection. The parse selection module de-
18
The Orthology and Combined modules both have states that achieve likelihood ratios above 400 (as
high as 1207 for the Orthology module and 613 for the Combined module), {indicating that both these
modules {can, on their own, predict some interacting protein pairs with a posterior odds ratio above 1}}.
Table 1: Sentence annotated as speculative with two cues (in boldface) and their scopes (in brackets).
rives the actual parse tree using the information
from the other two components. The intermediate
CCG supertagging stage assigns each token to a
lexical category which attempts to capture its syn-
tactic role in the sentence. Lexical categories con-
tain more information than PoS tags (mainly on
subcategorization) and they are more numerous,
thereby making their assignment a relatively dif-
ficult task. Therefore, the parse selection module
takes into account multiple predictions per token
which allows recovery from supertagging errors
while still reducing the ambiguity propagated. An
interesting aspect of this three-stage parsing ap-
proach is that, if the parse selection module fails to
construct a parse tree for the sentence (a common
issue when syntactic parsers are ported to new do-
mains), the lexical categories obtained by the su-
pertagger preserve some of the syntactic informa-
tion that would not be found in PoS tags.
The adaptation to the biomedical domain by
Rimell and Clark (2009) involved re-training the
PoS tagger and the CCG supertagger using in-
domain resources, while the parse selection com-
ponent was left intact. As recent work in the
BioNLP 2009 shared task has shown (Kim et al,
2009), domain-adapted parsing benefits informa-
tion extraction systems.
The native output of the C&C parser is con-
verted into the Stanford Dependency (SD) col-
lapsed dependency format (de Marneffe and Man-
ning, 2008). These dependencies define binary re-
lations between tokens and the labels of these re-
lations are obtained from a hierarchy. While the
conversion is unlikely to be perfect given that the
native C&C output follows a different formalism,
we made this choice because it allows for the use
of different parsers with minimal adaptation.
Finally, an important pre-processing step we
take is tokenization of the original text. Since the
PoS tagger is trained on the GENIA corpus which
follows the Penn TreeBank tokenization scheme,
we use the tokenization script provided by the tree-
bank.1
1http://www.cis.upenn.edu/?treebank/tokenization.html
3 Bayesian logistic regression
In both tasks, we use a Bayesian logistic regres-
sion classifier incorporating a sparsity-enforcing
Laplace prior (Genkin et al, 2006). Logistic re-
gression models are of the form:
p(y = +1|?, x) = exp(x?
T )
1 + exp(x?T ) (1)
where y ? {+1,?1} is a binary class label, x
is the feature vector representation of the instance
to be classified and ? is the feature weight vec-
tor which is learnt from the training data. Since
feature interactions are not directly represented,
the interactions that are expected to matter for the
task considered must be specified as additional
features. In Bayesian logistic regression, a prior
distribution on ? is used which encodes our prior
beliefs on the feature weights. In this work, we
use the Laplace prior which encourages the fea-
ture weight vector to be sparse, reflecting our be-
lief that most features will be irrelevant to the task.
4 Detecting sentences containing
speculation
In Task1, systems need to determine whether a
sentence contains uncertain information (labeled
uncertain) or not (labeled certain). A sentence is
uncertain if one or more of its tokens denote un-
certainty. Such tokens are labeled as cues and they
are provided by the organizers for training. If a
cue is a present, any other (potentially ?unhedg-
ing?) token becomes irrelevant to the task. There-
fore, we cast the task as a binary token classifi-
cation problem and determine the sentence label
from the token-level decisions.
Words used as speculative cues do not always
denote speculation. For example, in BioScope ?if?
and ?appear? are annotated as cues 4% and 83%
of the times they are encountered. In order to
gain better understanding of the task, we build a
dictionary-based cue extractor. First we extract all
the cues from the training data and use their lem-
mas, obtained using morpha (Minnen et al, 2001),
to tag tokens in the test data. We keep only single-
token cues in order to avoid non-indicative lem-
19
token=indicating lemma=indicate
PoS=VBG lemma+PoS=indicate+VBG
CCG=(S[ng]\NP)/S[em]
lemma+CCG=indicate+(S[ng]\NP)/S[em]
Table 2: Features extracted for the token ?indicat-
ing? from the Example in Table 1. CCG supertag
(S[ng]\NP)/S[em] denotes that ?indicating? ex-
pects an embedded clause (S[em]) to its right (in-
dicated by the forward slash /) and a noun phrase
(NP) to its left (indicated by the backward slash \)
to form a present participle (S[ng]).
mas entering the dictionary (e.g. ?that? in ?in-
dicate that?). Since the test data consist of full
articles only, we evaluate the performance of the
dictionary-based approach using four-fold cross-
validation on the nine full articles of the training
data with the abstracts added as training data in
every fold, but not used as test data. The recall
achieved is 98.07%, but F-score is lower (59.53%)
demonstrating that the single-token cues in the
training data provide adequate coverage, but low
precision. The restricted domain helps precision
as it precludes some word meanings from appear-
ing. For example ?might? is unlikely to be encoun-
tered as a noun in the biomedical domain. Never-
theless, in order to achieve better performance it
is important to further refine the cue identification
procedure.
Determining whether a token is used as a specu-
lative cue or not resembles supervised word sense
disambiguation. The main difference is that in-
stead of having an inventory of senses for each
word, we have two senses applicable to all words.
As in most word sense disambiguation tasks, the
classification of a word as cue or not is dependent
on the other words in the sentence, which we take
into account using syntax. The syntactic context
of words is a useful proxy to their semantics, as
shown in recent work on verb sense disambigua-
tion (Chen and Palmer, 2009). Furthermore, it is
easy to obtain syntactic information automatically
using a parser, even though there will be some
noise due to parsing errors. Similar intuitions were
exploited by Kilicoglu and Bergler (2008) in refin-
ing a dictionary of cues with syntactic rules.
In what follows, we present the features ex-
tracted for each token for our final system, along
with an example of their application in Table 2.
Where appropriate we give the relevant labels in
the Stanford Dependency (SD) scheme in paren-
theses for reproducibility:
? We extract the token itself and its lemma as
features.
? To handle cases where word senses are identi-
fiable by the PoS of a token (?might result? vs
?the might?), we combine the latter with the
lemma and add it as a feature.
? We combine the lemma with the CCG supertag
and add it as a feature in order to capture cases
where the hedging function of a word is de-
termined by its syntactic role in the sentence.
For example, ?indicating? in the example of
Table 1 is followed by a clausal complement (a
very reliable predictor of hedging function for
epistemic verbs), which is captured by its CCG
supertag. As explained in Section 2, this in-
formation can be recovered even in sentences
where the parser fails to produce a parse.
? Passive voice is commonly employed to limit
commitment to the statement made, therefore
we add it as a feature combined with the
lemma to verbs in that voice (nsubjpass).
? Modal auxiliaries are prime hedging devices
but they are also among the most ambiguous.
For example, ?can? is annotated as a cue in
16% of its occurrences and it is the fifth most
frequent cue in the full articles. To resolve
this ambiguity, we add as features the lemma
of the main verb the auxiliary is dependent on
(aux) as well as the lemmas of any dependents
of the main verb. Thus we can capture some
stereotypical speculative expressions in scien-
tific articles (e.g ?could be due?), while avoid-
ing false positives that are distinguished by the
use of first person plural pronoun and/or ref-
erence to objective enabling conditions (Kil-
icoglu and Bergler, 2008).
? Speculation can be expressed via negation of
a word expressing certainty (e.g. ?we do not
know?), therefore we add the lemma of the to-
ken prefixed with ?not? (neg).
? In order to capture stereotypical hedging ex-
pressions such as ?raises the question? and
?on the assumption? we add as features the di-
rect object of verbs combined with the lemma
of their object (dobj) and the preposition for
nouns in a prepositional relation (prep *).
? In order to capture the effect of adverbs on the
hedging function of verbs (e.g. ?theoretically
20
features Recall Precision F-score
tokens, lemmas 75.92 81.07 78.41
+PoS, CCG 78.23 83.71 80.88
+syntax 81.00 81.31 81.15
+combs 79.58 84.98 82.19
Table 3: Performance of various feature sets on
Task1 using cross-validation on full articles.
considered?) we add the lemma of the adverb
as a feature to the verb (advmod).
? To distinguish the probabilistic/numerical
sense from the hedging sense of adjectives
such as ?possible?, we add the lemma and the
number of the noun they modify as features
(amod), since plural number tends to be as-
sociated with the probabilistic/numerical sense
(e.g. ?all possible combinations?).
Finally, given that this stage is meant to identify
cues in order to recover their scopes in Task2, we
attempt to resolve multi-token cues in the train-
ing data into single-token ones. This agrees with
the minimal strategy for marking cues stated in the
corpus guidelines (Szarvas et al, 2008) and it sim-
plifies scope detection. Therefore, during train-
ing multi-token cues are resolved to their syntactic
head according to the dependency output, e.g. in
Table 1 ?indicate that? is restricted to ?indicate?
only. There were two cases in which this process
failed; the cues being ?cannot? (S3.167) and ?not
clear? (S3.269). We argue that the former is in-
consistently annotated (the sentence reads ?cannot
be defined. . . ? and it would have been resolved to
?defined?), while the latter is headed syntactically
by the verb ?be? which is preceding it.
5 Task1 results and error analysis
Initially we experiment using the full-articles part
of the training data only divided in four folds. The
reason for this choice is that the language of the
abstracts is relatively restricted and phenomena
that appear only in full papers could be obscured
by the abstracts, especially since the latter con-
sist of more sentences in total (11,871 vs. 2,670).
Such phenomena include language related to fig-
ures and descriptions of probabilistic models.
Each row in Table 3 is produced by adding
extra features to the feature set represented on
the row directly above it. First we consider us-
ing only the tokens and their lemmas as features
features Recall Precision F-score
tokens, lemmas 79.19 80.43 79.81
+PoS, CCG 81.12 85.22 83.12
+syntax 83.43 84.57 84.00
+combs 85.16 85.99 85.58
Table 4: Performance of various feature sets on
Task1 using cross-validation on full articles incor-
porating the abstracts as training data.
which amounts to a weighted dictionary but which
achieves reasonable performance. The inclusion
of PoS tags and CCG supertags improves perfor-
mance, whereas syntactic context increases recall
while decreasing precision slightly. This is due
to the fact that logistic regression does not rep-
resent feature interactions and the effect of these
features varies across words. For example, clausal
complements affect epistemic verbs but not other
words (?indicate? vs. ?state? in the example of
Table 1) and negation affects only words express-
ing certainty. In order to ameliorate this limitation
we add the lexicalized features described in Sec-
tion 4, for example the combination of the lemma
with the negation syntactic dependency. These ad-
ditional features improved precision from 81.31%
to 84.98%.
Finally, we add the abstracts to the training data
which improves recall but harms precision slightly
(Table 4) when only tokens and lemmas are used
as features. Nevertheless, we decided to keep them
as they have a positive effect for all other feature
representations.
A misinterpretation of the BioScope paper
(Szarvas et al, 2008) led us to believe that five of
the nine full articles in the training data were anno-
tated using the guidelines of Medlock and Briscoe
(2007). After the shared task, the organizers clar-
ified to us that all the full articles were annotated
using the BioScope guidelines. Due to our misin-
terpretation, we change our experimental setup to
cross-validate on the four full articles annotated in
BioScope only, considering the other five full ar-
ticles and the abstracts only as training data. We
keep this setup for the remainder of the paper.
We repeat the cross-validation experiments with
the full feature set and this new experimental setup
and report the results in Table 5. Using the same
feature set, we experiment with the Gaussian prior
instead of the sparsity-enforcing Laplace prior
which results in decreased precision and F-score,
21
Recall Precision F-score
cross-Laplace 80.33 84.21 82.23
cross-Gaussian 81.59 80.58 81.08
test 84.94 85.48 85.21
Table 5: Performance of the final system in Task1.
therefore confirming our intuition that most fea-
tures extracted are irrelevant to the task and should
have zero weight. Finally, we report our perfor-
mance on the test data using the Laplace prior.
6 Detecting the scope of the hedges
In Task2, the systems need to identify speculative
cues and their respective scopes. Since our system
for Task1 identifies cues, our discussion of Task2
focuses on identifying the scope of a given cue.
It is a non-trivial task, since scopes can be nested
and can span over a large number of tokens of the
sentence.
An initial approach explored was to associate
each cue with the token representing the syntactic
head of its scope and then to infer the scope us-
ing syntactic parsing. In order to achieve this, we
resolved the (almost always multi-token) scopes
to their syntactic heads and then built a classi-
fier whose features are based on syntactic depen-
dency paths. Multi-token scopes which were not
headed syntactically by a single token (according
to the parser) were discarded in order to obtain a
cleaner dataset for training. This phenomenon oc-
curs rather frequently, therefore reducing the train-
ing instances. At testing, the classifier identifies
the syntactic head of the scope for each cue and
we infer the scope from the syntactic parser?s out-
put. If more than one scope head is identified for
a particular cue, then the scopes are concatenated.
The performance of this approach turned out to
be very low, 10.34% in F-score. We identified two
principal reasons for this. First, relying on the syn-
tactic parser?s output to infer the scope is unavoid-
ably affected by parsing errors. Second, the scope
annotation takes into account semantics instead of
syntax. For example bibliographic references are
excluded based on their semantics.
In order to handle these issues, we developed an
approach that predicts whether each token of the
sentence belongs to the scope of a given cue. The
overall scope for that cue becomes the string en-
closed by the left- and right-most tokens predicted
to belong to the scope. The features used by the
classifier to predict whether a token belongs to the
scope of a particular cue are based on the short-
est syntactic dependency path connecting them,
which is found using Dijkstra?s algorithm. If no
such path is found (commonly due to parsing fail-
ure), then the token is classified as not belonging
to the scope of that cue. The features we use are
the following:
? The dependency path between the cue and the
token, combined with both their lemmas.
? According to the guidelines, different cues
have different preferences in having their
scopes extended to their left or to their right.
For example modal auxiliaries like ?can? in
Table 1 extend their scope to their right. There-
fore we add the dependency path feature de-
fined above refined by whether the token is on
the left or the right of the cue in question.
? We combine the dependency path and the lem-
mas of the cue and the token with their PoS
tags and CCG supertags, since these tags re-
fine the syntactic function of the tokens.
The features defined above are very sparse, espe-
cially when longer dependency paths are involved.
This can affect performance substantially, as the
scopes can be rather long, in many cases spanning
over the whole sentence. An unseen dependency
path between a cue and a token during testing re-
sults in the token being excluded from the scope
of that cue. In turn, this causes predicted scopes to
be shorter than they should be. We attempt to al-
leviate this sparsity in two stages. First, we make
the following simplifications to the labels of the
dependencies:
? Adjectival, noun compound, relative clause
and participial modifiers (amod, nn, rcmod,
partmod) are converted to generic modifiers
(mod).
? Passive auxiliary (auxpass) and copula (cop)
relations are converted to auxiliary relations
(aux).
? Clausal complement relations with inter-
nal/external subject (ccomp/xcomp) are con-
verted to complement relations (comp).
? All subject relations in passive or active voice
(nsubj, nsubjpass, csubj, csubjpass) are con-
verted to subjects (subj).
? Direct and indirect object relations (iobj, dobj)
are converted to objects (obj).
22
? We de-lexicalize conjunct (conj *) and prepo-
sitional modifier relations (prep *).
Second, we shorten the dependency paths:
? Since the SD collapsed dependencies format
treats conjunctions asymmetrically (conj), we
propagate the subject and object dependencies
of the head of the conjunction to the depen-
dent. We process appositional and abbrevi-
ation modifiers (appos, abbrev) in the same
way.
? Determiner and predeterminer relations (det,
predet) in the end of the dependency path are
removed, since the determiners (e.g. ?the?)
and predeterminers (e.g. ?both?) are included
in/excluded from the scope following their
syntactic heads.
? Consecutive modifier and dependent relations
(mod, dep) are replaced by a single relation of
the same type.
? Auxiliary relations (aux) that are not in the be-
ginning or the end of the path are removed.
Despite these simplifications, it is still possible
during testing to encounter dependency paths un-
seen in the training data. In order to ameliorate
this issue, we implement a backoff strategy that
progressively shortens the dependency path until
it matches a dependency path seen in the training
data. For example, if the path from a cue to a token
is subj-mod-mod and it has not been seen in the
training data, we test if subj-mod has been seen.
If it has, we consider it as the dependency path to
define the features described earlier. If not, we test
for subj in the same way. This strategy relies on
the assumption that tokens that are likely to be in-
cluded in/excluded from the scope following the
tokens they are syntactically dependent on. For
example, modifiers are likely to follow the token
being modified.
7 Task2 results and error analysis
In order to evaluate the performance of our ap-
proach, we performed four-fold cross-validation
on the four BioScope full articles, using the re-
maining full articles and the abstracts as training
data only. The performance achieved using the
features mentioned in Section 6 is 28.62% F-score,
while using the simplified dependency paths in-
stead of the path extracted from the parser?s out-
put improves it to 34.35% F-score. Applying the
back-off strategy for unseen dependency paths to
features Recall Precision F-score
standard 27.54 29.79 28.62
simplified 33.11 35.69 34.35
+backoff 34.10 36.75 35.37
+post 40.98 44.17 42.52
Table 6: Performance on Task2 using cross-
validation on BioScope full articles.
the simplified paths results in 35.37% F-score (Ta-
ble 6).
Our system predicts only single token cues.
This agrees in spirit with the minimal cue an-
notation strategy stated in the BioScope guide-
lines. The guidelines allow for multi-token cues,
referred to as complex keywords, which are de-
fined as cases where the tokens of a phrase cannot
express uncertainty independently. We argue that
this definition is rather vague, and combined with
the requirement for contiguity, results in cue in-
stances such as ?indicating that? (multiple occur-
rences), ?looks as? (S4.232) and ?address a num-
ber of questions? (S4.36) annotated as cues. It is
unclear why ?suggesting that? or ?appears that?
are not annotated as cues as well, or why ?that?
contributes to the semantic content of ?indicate?.
?that? does help determine the sense of ?indicate?,
but we argue that it should not be part of the cue as
it does not contribute to its semantic content. ?in-
dicate that? is the only consistent multi-token cue
pattern in the training data. Therefore, when our
system identifies as a cue a token with the lemma
?indicate?, if this token is followed by ?that?,
?that? is added to the cue. Given the annotation
difficulties multi-token cues present, it would be
useful during evaluation to relax cue matching in
the same way as in the BioNLP 2009 shared task,
i.e. considering as correct those cues predicted
within one token of the gold standard annotation.
As explained in Section 6, bibliographic ref-
erences are excluded from scopes and cannot be
recognized by means of syntactic parsing only.
Additionally, in some cases the XML formatting
does not preserve the parentheses and/or brack-
ets around numerical references. We employ two
post-processing steps to deal with these issues.
First, if the ultimate token of a scope happens to
be the penultimate token of the sentence and a
number, then it is removed from the scope. This
step can have a negative effect when the last to-
ken of the scope and penultimate token of the sen-
23
Recall Precision F-score
Cues cross 74.52 81.63 77.91test 74.50 81.85 78.00
Task2 cross 40.98 44.17 42.52test 42.40 45.96 44.11
Table 7: Performance on cue identification and
cue/scope identification in Task2.
tence happens to be a genuine number, as in Fig-
ure 1. In our experiments however, this heuristic
always increased performance. Second, if a scope
contains an opening parenthesis but not its clos-
ing one, then the scope is limited to the token im-
mediately before the opening one. Note that the
training data annotation allows for partial paren-
thetical statements to be included in scopes, as a
result of terminating scopes at bibliographic ref-
erences which are not the only tokens in a paren-
theses. For example, in S7.259: ?expressed (ED,
unpublished)? the scope is terminated after ?ED?.
These post-processing steps improved the perfor-
mance substantially to 42.52% F-score (Table 6).
The requirement for contiguous scope spans
which include their cue(s) is not treated appropri-
ately by our system, since we predict each token of
the scope independently. Combined with the fact
that the guidelines frequently define scopes to ex-
tend either to the left or to the right of the cue, an
approach based on sequential tagging and/or pre-
dicting boundaries could perform better. However,
as mentioned in the guidelines, the contiguity re-
quirement sometimes forced the inclusion of to-
kens that should have been excluded given the pur-
pose of the task.
Our final performance on the test data is 44.11%
in F-score (Table 7). This is higher than the one re-
ported in the official results (38.37%) because we
subsequently increased the coverage of the C&C
parser (parse failures resulted in 63 cues not re-
ceiving a scope), the addition of the back-off strat-
egy for unseen dependency paths and the clarifica-
tion on the inclusion of bibliographic references in
the scopes which resulted in improving the paren-
theses post-processing steps.
8 Related work
The shared task uses only full articles for testing
while both abstracts and full articles are used for
training. We argue that this represents a realistic
scenario for system developers since annotated re-
sources consist mainly of abstracts, while most in-
formation extraction systems are applied to full ar-
ticles. Also, the shared task aimed at detecting the
scope of speculation, while most previous work
(Light et al, 2004; Medlock and Briscoe, 2007;
Kilicoglu and Bergler, 2008) considered only clas-
sification of sentences, possibly due to the lack of
appropriately annotated resources.
The increasing interest in detecting speculative
language in scientific text resulted in a number of
guidelines. Compared to the most recent previous
definition by Medlock and Briscoe (2007), Bio-
Scope differs in the following ways:
? BioScope does not annotate anaphoric hedge
references.
? BioScope annotates indications of experimen-
tally observed non-universal behaviour.
? BioScope annotates statements of explicitly
proposed alternatives.
The first difference is due to the requirement that
the scope of the speculation be annotated, which
is not possible when it is present in a different sen-
tence. The other two differences follow from the
stated purpose which is the detection of sentences
containing uncertain information.
In related work, Hyland (1996) associates the
use of speculative language in scholarly publica-
tions with the purpose for which they are em-
ployed by the authors. In particular, he dis-
tinguishes content-oriented hedges from reader-
oriented ones. The former are used to calibrate
the strength of the claims made, while the latter
are employed in order to soften anticipated crit-
icism on behalf of the reader. Content-oriented
hedges are further distinguished as accuracy-
oriented ones, used to express uncertain claims
more accurately, and writer-oriented ones, used
to limit the commitment of the author(s) to the
claims. While the boundaries between these dis-
tinctions are not clear-cut and instances of hedging
can serve more than one of these purposes simulta-
neously, it is worth bearing them in mind while ap-
proaching the task. With respect to the shared task,
taking into account that hedging is used to ex-
press statements more accurately can help resolve
the ambiguity when annotating certain statements
about uncertainty. Such statements, which involve
words such as ?estimate?, ?possible?, ?predict?,
occur frequently in full articles.
Wilson (2008) analyzes speculation detection
24
inside a general framework for sentiment analysis
centered around the notion of private states (emo-
tions, thoughts, intentions, etc.) that are not open
to objective observation or verification. Specu-
lation is annotated with a spec-span/spec-target
scheme by answering the questions what the spec-
ulation is and what the speculation is about. With
respect to the BioScope guidelines, spec-span is
similar to what scope attempts to capture. spec-
span and spec-target do not need to be present
at the same time, which could help annotating
anaphoric cues.
9 Conclusions
This paper describes our approach to the CoNLL-
2010 shared task on speculative language detec-
tion using logistic regression and syntactic depen-
dencies. We achieved competitive performance on
sentence level uncertainty classification (Task1),
but not on scope identification (Task2). Motivated
by our error analysis we suggest refinements to the
task definition that could improve annotation.
Our approach to detecting speculation cues suc-
cessfully employed syntax as a proxy for the se-
mantic content of words. In addition, we demon-
strated that performance gains can be obtained by
choosing an appropriate prior for feature weights
in logistic regression. Finally, our performance in
scope detection was improved substantially by the
simplification scheme used to reduce the sparsity
of the dependency paths. It was devised using hu-
man judgment, but as information extraction sys-
tems become increasingly reliant on syntax and
each task is likely to need a different scheme, fu-
ture work should investigate how this could be
achieved using machine learning.
Acknowledgements
We would like to thank the organizers for provid-
ing the infrastructure and the data for the shared
task, Laura Rimell for her help with the C&C
parser and Marina Terkourafi for useful discus-
sions. The authors were funded by NIH/NLM
grant R01/LM07050.
References
Jinying Chen and Martha Palmer. 2009. Improving
English Verb Sense Disambiguation Performance
with Linguistically Motivated Features and Clear
Sense Distinction Boundaries. Language Resources
and Evaluation, 43(2):143?172.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In CrossParser ?08: Coling 2008: Pro-
ceedings of the Workshop on Cross-Framework and
Cross-Domain Parser Evaluation, pages 1?8.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of CoNLL-2010 Shared Task, pages 1?12.
Alexander Genkin, David D. Lewis, and David Madi-
gan. 2006. Large-scale Bayesian Logistic Re-
gression for Text Classification. Technometrics,
49(3):291?304.
Ken Hyland. 1996. Writing Without Conviction?
Hedging in Science Research Articles. Applied Lin-
guistics, 17(4):433?454.
Halil Kilicoglu and Sabine Bergler. 2008. Recog-
nizing speculative language in biomedical research
articles: a linguistically motivated perspective. In
BioNLP ?08: Proceedings of the Workshop on Cur-
rent Trends in Biomedical Natural Language Pro-
cessing, pages 46?53.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 1?9.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The Language of Bioscience: Facts, Specu-
lations, and Statements In Between. In HLT-NAACL
2004 Workshop: BioLINK 2004, Linking Biological
Literature, Ontologies and Databases, pages 17?24.
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific
Literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Laura Rimell and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852?865.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In BioNLP ?08: Proceedings of
the Workshop on Current Trends in Biomedical Nat-
ural Language Processing, pages 38?45.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity
and Sentiment Analysis: Recognizing the Intensity,
Polarity, and Attitudes of Private States. Ph.D. the-
sis, University of Pittsburgh.
25
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 49?57,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Search-based Structured Prediction applied to Biomedical Event Extraction
Andreas Vlachos and Mark Craven
Department of Biostatistics and Medical Informatics
University of Wisconsin-Madison
{vlachos,craven}@biostat.wisc.edu
Abstract
We develop an approach to biomedical event
extraction using a search-based structured pre-
diction framework, SEARN, which converts
the task into cost-sensitive classification tasks
whose models are learned jointly. We show
that SEARN improves on a simple yet strong
pipeline by 8.6 points in F-score on the
BioNLP 2009 shared task, while achieving the
best reported performance by a joint inference
method. Additionally, we consider the issue of
cost estimation during learning and present an
approach called focused costing that improves
improves efficiency and predictive accuracy.
1 Introduction
The term biomedical event extraction is used to re-
fer to the task of extracting descriptions of actions
and relations involving one or more entities from
the biomedical literature. The recent BioNLP 2009
shared task (BioNLP09ST) on event extraction (Kim
et al, 2009) focused on event types of varying com-
plexity. Each event consists of a trigger and one or
more arguments, the latter being proteins or other
events. Any token in a sentence can be a trigger for
one of the nine event types and, depending on their
associated event types, triggers are assigned appro-
priate arguments. Thus, the task can be viewed as
a structured prediction problem in which the output
for a given instance is a (possibly disconnected) di-
rected acyclic graph (not necessarily a tree) in which
vertices correspond to triggers or protein arguments,
and edges represent relations between them.
Despite being a structured prediction task, most of
the systems that have been applied to BioNLP09ST
to date are pipelines that decompose event extrac-
tion into a set of simpler classification tasks. Clas-
sifiers for these tasks are typically learned indepen-
dently, thereby ignoring event structure during train-
ing. Typically in such systems, the relationships
among these tasks are taken into account by incor-
porating post-processing rules that enforce certain
constraints when combining their predictions, and
by tuning classification thresholds to improve the ac-
curacy of joint predictions. Pipelines are appealing
as they are relatively easy to implement and they of-
ten achieve state-of-the-art performance (Bjorne et
al., 2009; Miwa et al, 2010).
Because of the nature of the output space, the task
is not amenable to sequential or grammar-based ap-
proaches (e.g. linear CRFs, HMMs, PCFGs) which
employ dynamic programming in order to do ef-
ficient inference. The only joint inference frame-
work that has been applied to BioNLP09ST to date
is Markov Logic Networks (MLNs) (Riedel et al,
2009; Poon and Vanderwende, 2010). However,
MLNs require task-dependent approximate infer-
ence and substantial computational resources in or-
der to achieve state-of-the-art performance.
In this work we explore an alternative joint in-
ference approach to biomedical event extraction us-
ing a search-based structured prediction framework,
SEARN (Daume? III et al, 2009). SEARN is an
algorithm that converts the problem of learning a
model for structured prediction into learning a set
of models for cost-sensitive classification (CSC).
CSC is a task in which each training instance has
a vector of misclassification costs associated with it,
thus rendering some mistakes on some instances to
be more expensive than others (Domingos, 1999).
Compared to a standard pipeline, SEARN is able to
49
achieve better performance because its models are
learned jointly. Thus, each of them is able to use fea-
tures representing the predictions made by the oth-
ers, while taking into account possible mistakes.
In this paper, we make the following contribu-
tions. Using the SEARN framework, we develop a
joint inference approach to biomedical event extrac-
tion. We evaluate our approach on the BioNLP09ST
dataset and show that SEARN improves on a simple
yet strong pipeline by 8.6 points in F-score, while
achieving the best reported performance on the task
by a joint inference method. Additionally, we con-
sider the issue of cost estimation and present an ap-
proach called focused costing that improves perfor-
mance. We believe that these contributions are likely
to be relevant to applications of SEARN to other
natural language processing tasks that involve struc-
tured prediction in complex output spaces.
2 BioNLP 2009 shared task description
BioNLP09ST focused on the extraction of events
involving proteins whose names are annotated in
advance. Each event has two types of arguments,
Theme and Cause, which correspond respectively to
the Agent and Patient roles in semantic role label-
ing (Gildea and Jurafsky, 2002). Nine event types
are defined which can be broadly classified in three
categories, namely Simple, Binding and Regulation.
Simple events include Gene expression, Transcrip-
tion, Protein catabolism, Phosphorylation, and Lo-
calization events. These have only one Theme ar-
gument which is a protein. Binding events have
one or more protein Themes. Finally, Regulation
events, which include Positive regulation, Nega-
tive regulation and Regulation, have one obligatory
Theme and one optional Cause, each of which can
be either a protein or another event. Each event has
a trigger which is a contiguous string that can span
over one or more tokens. Triggers and arguments
can be shared across events. In an example demon-
strating the complexity of the task, given the passage
?. . . SQ 22536 suppressed gp41-induced IL-10 pro-
duction in monocytes?, systems should extract the
three appropriately nested events listed in Fig. 1d.
Performance is measured using Recall, Precision
and F-score over complete events, i.e. the trigger,
the event type and the arguments all must be correct
in order to obtain a true positive. It is important to
note that if either the trigger, the type, or an argu-
ment of a predicted event is incorrect then this event
will result in one false positive and one false nega-
tive. In the example of Fig. 1, if ?suppressed? is rec-
ognized incorrectly as a Regulation trigger then it is
better to not assign a Theme to it so that we avoid
a false positive due to extracting an event with in-
correct type. Finally, the evaluation ignores triggers
that do not form events.
3 Event extraction decomposition
Figure 1 describes the event extraction decomposi-
tion that we use throughout the paper. We assume
that the sentences to be processed are parsed into
syntactic dependencies and lemmatized. Each stage
has its own module, which is either a learned classi-
fier (trigger recognition, Theme/Cause assignment)
or a rule-based component (event construction).
3.1 Trigger recognition
In trigger recognition the system decides whether a
token acts as a trigger for one of the nine event types
or not. Thus it is a 10-way classification task. We
only consider tokens that are tagged as nouns, verbs
or adjectives by the parser, as they cover the majority
of the triggers in the BioNLP09ST data. The main
features used in the classifier represent the lemma
of the token which is sufficient to predict the event
type correctly in most cases. In addition, we include
features that conjoin each lemma with its part-of-
speech tag. This allows us to handle words with
the same nominal and verbal form that have differ-
ent meanings, such as ?lead?. While the domain
restricts most lemmas to one event type, there are
some whose event type is determined by the context,
e.g. ?regulation? on its own denotes a Regulation
event but in ?positive regulation? it denotes a Posi-
tive regulation event instead. In order to capture this
phenomenon, we add as features the conjunction of
each lemma with the lemma of the tokens immedi-
ately surrounding it, as well as with the lemmas of
the tokens with which it has syntactic dependencies.
3.2 Theme and Cause assignment
In Theme assignment, we form an agenda of can-
didate trigger-argument pairs for all trigger-protein
combinations in the sentence and classify them as
50
SQ 22536 suppressed
Neg reg
gp41-induced
Pos reg
IL-10 production
Gene exp
(a) Trigger recognition
SQ 22536 suppressed
Neg reg
gp41-induced
Pos reg
IL-10 production
Gene exp
Theme
ThemeTheme
(b) Theme assignment
SQ 22536 suppressed
Neg reg
gp41-induced
Pos reg
IL-10 production
Gene exp
Theme
Theme
Cause
Theme
(c) Cause assignment
ID type Trigger Theme Cause
E1 Neg reg suppressed E2
E2 Pos reg induced E3 gp41
E3 Gene exp production IL-10
(d) Event construction
Figure 1: The stages of our event extraction decomposition. Protein names are shown in bold.
Themes or not. Whenever a trigger is predicted to be
associated with a Theme, we form candidate pairs
between all the Regulation triggers in the sentence
and that trigger as the argument, thus allowing the
prediction of nested events. Also, we remove candi-
date pairs that could result in directed cycles, as they
are not allowed by the task.
The features used to predict whether a trigger-
argument pair should be classified as a Theme are
extracted from the syntactic dependency path and
the textual string between them. In particular, we
extract the shortest unlexicalized dependency path
connecting each trigger-argument pair, allowing the
paths to follow either dependency direction. One set
of features represents these paths, and in addition,
we have sets of features representing each path con-
joined with the lemma, the PoS tag and the event
type of the trigger, the type of the argument and
the first and last lemmas in the dependency path.
The latter help by providing some mild lexicaliza-
tion. We also add features representing the textual
string between the trigger and the argument, com-
bined with the event type of the trigger. While not as
informative as dependency paths, such features help
in sentences where the parse is incorrect, as triggers
and their arguments tend to appear near each other.
In Cause assignment, we form an agenda of can-
didate trigger-argument pairs using only the Regu-
lation class triggers that were assigned at least one
Theme. These are combined with protein names and
other triggers that were assigned a Theme. We ex-
tract features as in Theme assignment, further fea-
tures representing the conjunction of the dependency
path of the candidate pair with the path(s) from the
trigger to its Theme(s).
3.3 Event construction
In event construction, we convert the predictions of
the previous stages into a set of legal events. If
a Binding trigger is assigned multiple Themes, we
choose to form either one event per Theme or one
event with multiple Themes. Following Bjorne et
al. (2009), we group the arguments of each Binding
trigger according to the first label in their syntac-
tic dependency path and generate events using the
cross-product of these groups. For example, assum-
ing the parse was correct and all the Themes recog-
nized, ?interactions of A and B with C? results in
two Binding events with two Themes each, A with
C, and B with C respectively. We add the exception
that if two Themes are in the same token (e.g. ?A/B
interactions?) or the lemma of the trigger is ?bind?
then they form one Binding event with two Themes.
4 Structured prediction with SEARN
SEARN (Daume? III et al, 2009) forms the struc-
tured output prediction for an instance s as a se-
quence of T multiclass predictions y?1:T made by a
hypothesis h. The latter consists of a set of classi-
fiers that are learned jointly. Each prediction y?t can
use features from s as well as from all the previous
predictions y?1:t?1. These predictions are referred to
51
as actions and we adopt this term in order to distin-
guish them from the structured output predictions.
The SEARN algorithm is presented in Alg. 1. It
initializes hypothesis h to the optimal policy pi (step
2) which predicts the optimal action in each step
t according to the gold standard. The optimal ac-
tion at step t is the one that minimizes the overall
loss over s assuming that all future actions y?t+1:T
are also made optimally. The loss function ` is de-
fined by the structured prediction task considered.
Each iteration begins by making predictions for all
instances s in the training data S (step 6). For each
s and each action y?t, a cost-sensitive classification
(CSC) example is generated (steps 8-12). The fea-
tures are extracted from s and the previous actions
y?1:t?1 (step 8). The cost for each possible action
yit is estimated by predicting the remaining actions
y?t+1:T in s using h (step 10) and evaluating the cost
incurred given that action (step 11). Using a CSC
learning algorithm, a new hypothesis is learned (step
13) which is combined with the current one accord-
ing to the interpolation parameter ?.
Algorithm 1 SEARN
1: Input: labeled instances S , optimal policy pi, CSC
learning algorithm CSCL, loss function `
2: current policy h = pi
3: while h depends significantly on pi do
4: Examples E = ?
5: for s in S do
6: Predict h(s) = y?1:T
7: for y?t in h(s) do
8: Extract features ?t = f(s, y?1:t?1)
9: for each possible action yit do
10: Predict y?t+1:T = h(s|y?1:t?1, yit)
11: Estimate cit = `(y?1:t?1, y
i
t, y?t+1:T )
12: Add (?t, ct) to E
13: Learn a hypothesis hnew = CSCL(E)
14: h = ?hnew + (1? ?)h
15: Output: policy h without pi
In each iteration, SEARN moves away from the
optimal policy and uses the learned hypotheses in-
stead when predicting (steps 6 and 10). Thus, each
hnew is adapted to the actions chosen by h instead
of those of the optimal policy. When the depen-
dence on the latter becomes insignificant, the algo-
rithm terminates and returns the weighted ensemble
of learned hypotheses without the optimal policy.
Note though that the estimation of the costs in step
11 is always performed using the gold standard.
The interpolation parameter ? determines how
fast SEARN moves away from the optimal policy
and as a result how many iterations will be needed to
minimize the dependence on it. Dependence in this
context refers to the probability of using the optimal
policy instead of the learned hypothesis in choos-
ing an action during prediction. In each iteration,
the features extracted ?t are progressively corrupted
with the actions chosen by the learned hypotheses
instead of those of the optimal policy.
Structural information under SEARN is incorpo-
rated in two ways. First, via the costs that are es-
timated using the loss over the instance rather than
isolated actions (e.g. in PoS tagging, the loss would
be the number of incorrect PoS tags predicted in
a sentence if a token is tagged as noun). Second,
via the features extracted from the previous actions
(y?1:t?1) (e.g. the PoS tag predicted for the previ-
ous token can be a feature). These types of features
are possible in a standard pipeline as well, but dur-
ing training they would have to be extracted using
the gold standard instead of the actual predictions
made by the learned hypotheses, as during testing.
Since the prediction for each instance (y?1:T in step
6) changes in every iteration, the structure features
used to predict the actions have to be extracted anew.
The extraction of features from previous actions
implies a search order. For some tasks, such as PoS
tagging, there is a natural left-to-right order in which
the tokens are treated, however for many tasks this
is not the case.
Finally, SEARN can be used to learn a pipeline of
independently trained classifiers. This is achieved
using only one iteration in which the cost for each
action is set to 0 if it follows from the gold standard
and to 1 otherwise. This adaptation allows for a fair
comparison between SEARN and a pipeline.
5 SEARN for biomedical event extraction
In this section we discuss how we learn the event
extraction decomposition described in Sec. 3 under
SEARN. Each instance is a sentence consisting of
the tokens, the protein names and the syntactic pars-
ing output. The hypothesis learned in each iteration
consists of a classifier for each stage of the pipeline,
52
excluding event construction which is rule-based.
Unlike PoS tagging, there is no natural ordering
of the actions in event extraction. Ideally, the ac-
tions predicted earlier should be less dependent on
structural features and/or easier so that they can in-
form the more structure dependent/harder ones. In
trigger recognition, we process the tokens from left
to right since modifiers appearing before nouns tend
to affect the meaning of the latter, e.g. ?binding ac-
tivity?. In Theme and Cause assignment, we predict
trigger-argument pairs in order of increasing depen-
dency path length, assuming that since dependency
paths are the main source of features at this stage and
shorter paths are less sparse, pairs containing shorter
ones should be more reliable to predict.
In addition to the features mentioned in Sec. 3,
SEARN allows us to extract and learn weights for
structural features for each action from the previous
ones. During trigger recognition, we add as features
the combination of the lemma of the current token
combined with the event type (if any) assigned to
the previous and the next token, as well as to the to-
kens that have syntactic dependencies with it. Dur-
ing Theme assignment, when considering a trigger-
argument pair, we add features based on whether it
forms an undirected cycle with previously predicted
Themes, whether the trigger has been assigned a pro-
tein as a Theme and the candidate Theme is an event
trigger (and the reverse) and whether the argument
has become the Theme of a trigger with the same
event type. We also add a feature indicating whether
the trigger has three Themes predicted already. Dur-
ing Cause assignment, we add features representing
whether the trigger has been assigned a protein as a
Cause and the candidate Cause is an event trigger.
The loss function ` sums the number of false pos-
itive and false negative events, which is the evalua-
tion measure of BioNLP09ST. The optimal policy is
derived from the gold standard and returns the ac-
tion that minimizes this loss over the sentence given
the previous actions and assuming that all future ac-
tions are optimal. In trigger recognition, it returns
either the event type for tokens that are triggers or a
?notrigger? label otherwise. In Theme assignment,
for a given trigger-argument pair the optimal policy
returns Theme only if the trigger is recognized cor-
rectly and the argument is indeed a Theme for that
trigger according to the gold standard. In case the ar-
gument is another event, we require that at least one
of its Themes to be recognized correctly as well. In
Cause assignment, the requirements are the same as
those for the Themes, but we also require that at least
one Theme of the trigger in the trigger-argument pair
to be considered correct. These additional checks
follow from the task definition, under which events
must have all their elements identified correctly.
5.1 Cost estimation
Cost estimation (steps 5-12 in Alg. 1) is crucial to
the successful application of SEARN. In order to
highlight its importance, consider the example of
Fig. 2 focusing on trigger recognition.
In the first iteration (Fig. 2a), the actions for the
sentence will be made using the optimal policy only,
thus replicating the gold standard. During costing,
if a token is not a trigger according to the gold stan-
dard (e.g. ?SQ?), then the cost for incorrectly pre-
dicting that it is a trigger is 0, as the optimal policy
will not assign Themes to a trigger with incorrect
event type. Such instances are ignored by the cost-
sensitive learner. If a token is a trigger according to
the gold standard, then the cost for not predicting it
as such or predicting its type incorrectly is equal to
the number of the events that are dependent on it, as
they will become false negatives. False positives are
avoided as we are using the optimal policy in this
iteration.
In the second iteration (Fig. 2b), the optimal pol-
icy is interpolated with the learned hypothesis, thus
some of the actions are likely to be incorrect. As-
sume that ?SQ? is incorrectly predicted to be a
Neg reg trigger and assigned a Theme. During cost-
ing, the action of labeling ?SQ? as Neg reg has a
cost of 1, as it would result in a false positive event.
Thus the learned hypothesis will be informed that it
should not label ?SQ? as a trigger as it would assign
Themes to it incorrectly and it is adapted to handle
its own mistakes. Similarly, the action of labeling
?production? as Neg reg in this iteration would in-
cur a cost of 6, as the learned hypothesis would as-
sign a Theme incorrectly, thus resulting in 3 false
negative and 3 false positive events. Therefore, the
learned hypothesis will be informed that assigning
the wrong event type to ?production? is worse than
not predicting a trigger.
By evaluating the cost of each action according to
53
SQ 22536 suppressed
Neg reg
gp41-induced
Pos reg
IL-10 production
Gene exp
Theme
Theme
Cause
Theme
token No Gene exp Pos reg Neg reg
SQ 0 0 0 0
suppressed 1 1 1 0
-induced 2 2 0 2
production 3 0 3 3
(a) First iteration (optimal policy only)
SQ
Neg reg
22536 suppressed
Neg reg
gp41-induced
Pos reg
IL-10 production
Neg reg
Theme
Theme
Cause
ThemeTheme
token No Gene exp Pos reg Neg reg
SQ 0 0 0 1
suppressed 1 1 1 0
-induced 2 2 0 2
production 3 0 3 6
(b) Second iteration (interpolation)
Figure 2: Prediction (top) and CSC examples for trigger recognition actions (bottom) in the first two SEARN
iterations. Each CSC example has its own vector of misclassification costs.
its effect on the prediction for the whole sentence,
we are able to take into account steps in the pre-
diction process that are not learned as actions. For
example, if the Binding event construction heuris-
tic described in Sec. 3.3 cannot produce the correct
events for a token that is a Binding trigger despite
the Themes being assigned correctly, then this will
increase the cost for tagging that trigger as Binding.
The interpolation between the optimal policy and
the learned hypothesis is stochastic, thus affecting
the cost estimates obtained. In order to obtain more
reliable estimates, one can average multiple sam-
ples for each action by repeating steps 10 and 11
of Alg. 1. However, the computational cost is effec-
tively multiplied by the number of samples.
In step 11 of Alg. 1, the cost of each action is esti-
mated over the whole sentence. While this allows us
to take structure into account, it can result in costs
being affected by a part of the output that is not re-
lated to that action. This is likely to occur in event
extraction, as sentences can often be long and con-
tain disconnected event components in their output
graphs. For this reason, we refine the cost estimation
of each action to take into account only the events
that are connected to it through either gold standard
or predicted events. For example, in Fig. 2 the cost
estimation for ?SQ? will ignore the predicted events
in the first iteration and the gold standard, while it
will take them into account in the second one. We
refer to this refinement as focused costing.
A different approach proposed by Daume? III et
al. (2009) is to assume that all actions following the
one we are costing are going to be optimal and use
the optimal policy to approximate the prediction of
the learned hypothesis in step 10 of Alg. 1. In tasks
where the learned hypothesis is accurate enough,
this has no performance loss and it is computation-
ally efficient as the optimal policy is deterministic.
However, in event extraction the learned hypothesis
is likely to make mistakes, thus the optimal policy
does not provide a good approximation for it.
5.2 CSC learning with passive-aggressive
algorithms
The SEARN framework requires a multiclass CSC
algorithm to learn how to predict actions. This algo-
rithm must be computationally fast during parameter
learning and prediction, as in every iteration we need
to learn a new hypothesis and to consider each pos-
sible action for each instance in order to construct
the cost-sensitive examples. Daume? III et al (2009)
showed that any binary classification algorithm can
be used to perform multiclass CSC by employing an
appropriate conversion between the tasks. The main
drawback of this approach is its reliance on multi-
ple subsamplings of the training data, which can be
inefficient for large datasets and many classes.
With these considerations in mind, we implement
a multiclass CSC learning algorithm using the gen-
eralization of the online passive-aggressive (PA) al-
gorithm for binary classification proposed by Cram-
mer et al (2006). For each training example xt,
the K-class linear classifier with K weight vectors
w(k)t makes a prediction y?t and suffers a loss `t. In
54
the case of multiclass CSC learning, each example
has its own cost vector ct. If the loss is 0 then the
weight vectors of the classifier are not updated (pas-
sive). Otherwise, the weight vectors are updated
minimally so that the prediction on example xt is
corrected (aggressive). The update takes into ac-
count the loss and the aggressiveness parameter C.
Crammer et al (2006) describe three variants to per-
form the updates which differ in how the learning
rate ?t is set. In our experiments we use the variant
named PA-II with prediction-based updates (Alg. 2).
Since we are operating in a batch learning setting
(i.e. we have access to all the training examples and
their order is not meaningful), we perform multiple
rounds over the training examples shuffling their or-
der, and average the weight vectors obtained.
Algorithm 2 Passive-aggressive CSC learning
1: Input: training examples X = x1 . . . xT , cost vec-
tors c1 . . . cT ? 0, rounds R, aggressiveness C
2: Initialize weights w(k)0 = (0, ..., 0)
3: for r = 1, ..., R do
4: Shuffle X
5: for xt ? X do
6: Predict y?t = argmaxk(w
(k)
t ? xt)
7: Receive cost vector ct ? 0
8: if c(y?t)t > 0 then
9: Suffer loss `t = w
(y?t)
t ?xt?w
(yt)
t ?xt+
?
c(y?t)t
10: Set learning rate ?t =
`t
||xt||2+ 12C
11: Update w(yt)t+1 = wt + ?txt
12: Update w(y?t)t+1 = wt ? ?txt
13: Average wavg = 1T?R
?T?R
i=0 wi
6 Experiments
BioNLP09ST comprises three datasets ? training,
development and test ? which consist of 800, 150
and 260 abstracts respectively. After the end
of the shared task, an on-line evaluation server
was activated in order to allow the evaluation on
the test data once per day, without allowing ac-
cess to the data itself. We report results using
Recall/Precision/F-score over complete events using
the approximate span matching/approximate recur-
sive matching variant which was the primary perfor-
mance criterion in BioNLP09ST. This variant counts
a predicted event as a true positive if its trigger is
extracted within a one-token extension of the gold-
standard trigger. Also, in the case of nested events,
those events below the top-level need their trigger,
event type and Theme but not their Cause to be cor-
rectly identified for the top-level event to be consid-
ered correct. The same event matching variant was
used in defining the loss as described in Sec. 5.
A pre-processing step we perform on the train-
ing data is to reduce the multi-token triggers in the
gold standard to their syntactic heads. This proce-
dure simplifies the task of assigning arguments to
triggers and, as the evaluation variant used allows
approximate trigger matching, it does not result in
a performance loss. For syntactic parsing, we use
the output of the BLLIP re-ranking parser adapted to
the biomedical domain by McClosky and Charniak
(2008), as provided by the shared task organizers
in the Stanford collapsed dependency format with
conjunct dependency propagation. Lemmatization
is performed using morpha (Minnen et al, 2001).
In all our experiments, for CSC learning with PA,
the C parameter is set by tuning on 10% of the train-
ing data and the number of rounds is fixed to 10. For
SEARN, we set the interpolation parameter ? to 0.3
and the number of iterations to 12. The costs for
each action are obtained by averaging three samples
as described in Sec. 5.1. ? and the number of sam-
ples are the only parameters that need tuning and we
use the development data for this purpose.
First we compare against a pipeline of indepen-
dently learned classifiers obtained as described in
Sec. 4 in order to assess the benefits of joint learning
under SEARN using focused costing. The results
shown in Table 1 demonstrate that SEARN obtains
better event extraction performance on both the de-
velopment and test sets by 7.7 and 8.6 F-score points
respectively. The pipeline baseline employed in our
experiments is a strong one: it would have ranked
fifth in BioNLP09ST and it is 20 F-score points bet-
ter than the baseline MLN employed by Poon and
Vanderwende (2010). Nevertheless, the indepen-
dently learned classifier for triggers misses almost
half of the event triggers, from which the subsequent
stages cannot recover. On the other hand, the trig-
ger classifier learned with SEARN overpredicts, but
since the Theme and Cause classifiers are learned
jointly with it they maintain relatively high precision
with substantially higher recall compared to their in-
55
pipeline SEARN focus SEARN default
R P F R P F R P F
triggerdev 53.0 61.1 56.8 81.8 34.2 48.2 84.9 12.0 21.0
Themedev 44.2 79.6 56.9 62.0 69.1 65.4 59.0 65.1 61.9
Causedev 18.1 59.2 27.8 30.6 45.0 36.4 31.9 45.5 37.5
Eventdev 35.8 68.9 47.1 50.8 59.5 54.8 47.4 54.3 50.6
Eventtest 30.8 67.4 42.2 44.5 59.1 50.8 41.3 53.6 46.6
Table 1: Recall / Precision / F-score on BioNLP09ST development and test data. Left-to-right: pipeline of
independently learned classifiers, SEARN with focused costing, SEARN with default costing.
dependently learned counterparts. The benefits of
SEARN are more pronounced in Regulation events
which are more complex. For these events, it im-
proves on the pipeline on both the development and
test sets by 11 and 14.2 F-score points respectively.
The focused costing approach we proposed con-
tributes to the success of SEARN. If we replace it
with the default costing approach which uses the
whole sentence, the F-score drops by 4.2 points on
both development and test datasets. The default
costing approach mainly affects the trigger recog-
nition stage, which takes place first. Trigger over-
prediction is more extreme in this case and renders
the Theme assignment stage harder to learn. While
the joint learning of the classifiers ameliorates this
issue and the event extraction performance is even-
tually higher than that of the pipeline, the use of fo-
cused costing improves the performance even fur-
ther. Note that trigger overprediction also makes
training slower, as it results in evaluating more ac-
tions for each sentence. Finally, using one instead
of three samples per action decreases the F-score by
1.3 points on the development data.
Compared with the MLN approaches applied to
BioNLP09ST, our predictive accuracy is better than
that of Poon and Vanderwende (2010) which is the
best joint inference performance to date and substan-
tially better than that of Riedel et al (2009) (50 and
44.4 in F-score respectively). Recently, McClosky
et al (2011) combined multiple decoders for a de-
pendency parser with a reranker, achieving 48.6 in
F-score. While they also extracted structure fea-
tures for Theme and Cause assignment, their model
is restricted to trees (ours can output directed acyclic
graphs) and their trigger recognizer is learned inde-
pendently.
When we train SEARN combining the training
and the development sets, we reach 52.3 in F-score,
which is better than the performance of the top
system in BioNLP09ST (51.95) by Bjorne et al
(2009) which was trained in the same way. The
best performance to date is reported by Miwa et al
(2010) (56.3 in F-score), who experimented with six
parsers, three dependency representations and vari-
ous combinations of these. They found that different
parser/dependency combinations provided the best
results on the development and test sets.
A direct comparison between learning frame-
works is difficult due to the differences in task de-
composition and feature extraction. In particular,
event extraction results depend substantially on the
quality of the syntactic parsing. For example, Poon
and Vanderwende (2010) heuristically correct the
syntactic parsing used and report that this improved
their performance by four F-score points.
7 Conclusions
We developed a joint inference approach to biomed-
ical event extraction using the SEARN framework
which converts a structured prediction task into a set
of CSC tasks whose models are learned jointly. Our
approach employs the PA algorithm for CSC learn-
ing and a focused cost estimation procedure which
improves the efficiency and accuracy of the standard
cost estimation method. Our approach provides the
best reported results for a joint inference method on
the BioNLP09ST task. With respect to the experi-
ments presented by Daume? III et al (2009), we em-
pirically demonstrate the gains of using SEARN on
a problem harder than sequential tagging.
Acknowledgments
The authors were funded by NIH/NLM grant R01 /
LM07050.
56
References
Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP 2009Work-
shop Companion Volume for Shared Task, pages 10?
18.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Hal Daume? III, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75:297?325.
Pedro Domingos. 1999. Metacost: a general method for
making classifiers cost-sensitive. In Proceedings of
the 5th International Conference on Knowledge Dis-
covery and Data Mining, pages 155?164.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28:245?288.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the 46th Annual Meeting of the Association of Compu-
tational Linguistics: Human Language Technologies,
pages 101?104.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency pars-
ing. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating dependency repre-
sentation for event extraction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 779?787.
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In Proceedings of the Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 813?821.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A Markov logic approach
to bio-molecular event extraction. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
Shared Task, pages 41?49.
57
Proceedings of BioNLP Shared Task 2011 Workshop, pages 36?40,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Biomedical Event Extraction from Abstracts and Full Papers using
Search-based Structured Prediction
Andreas Vlachos and Mark Craven
Department of Biostatistics and Medical Informatics
University of Wisconsin-Madison
{vlachos,craven}@biostat.wisc.edu
Abstract
In this paper we describe our approach to
the BioNLP 2011 shared task on biomedical
event extraction from abstracts and full pa-
pers. We employ a joint inference system de-
veloped using the search-based structured pre-
diction framework and show that it improves
on a pipeline using the same features and it is
better able to handle the domain shift from ab-
stracts to full papers. In addition, we report on
experiments using a simple domain adaptation
method.
1 Introduction
The term biomedical event extraction is used to re-
fer to the task of extracting descriptions of actions
and relations among one or more entities from the
biomedical literature. The BioNLP 2011 shared
task GENIA Task1 (BioNLP11ST-GE1) (Kim et al,
2011) focuses on extracting events from abstracts
and full papers. The inclusion of full papers in the
datasets is the only difference from Task1 of the
BioNLP 2009 shared task (BioNLP09ST1) (Kim et
al., 2009), which used the same task definition and
abstracts dataset. Each event consists of a trigger
and one or more arguments, the latter being proteins
or other events. The protein names are annotated in
advance and any token in a sentence can be a trig-
ger for one of the nine event types. In an exam-
ple demonstrating the complexity of the task, given
the passage ?. . . SQ 22536 suppressed gp41-induced
IL-10 production in monocytes?, systems should ex-
tract the three nested events shown in Fig. 1d.
In our submission, we use the event extraction
system of Vlachos and Craven (2011) which em-
ploys the search-based structured prediction frame-
work (SEARN) (Daume? III et al, 2009). SEARN
converts the problem of learning a model for struc-
tured prediction into learning a set of models for
cost-sensitive classification (CSC). In CSC, each
training instance has a vector of misclassification
costs associated with it, thus rendering some mis-
takes in some instances to be more expensive than
others. Compared to other structured prediction
frameworks such as Markov Logic Networks (Poon
and Vanderwende, 2010), SEARN provides high
modeling flexibility but it does not requiring task-
dependent approximate inference.
In this work, we show that SEARN is more accu-
rate than a pipeline using the same features and it is
better able to handle the domain shift from abstracts
to full papers. Furthermore, we report on exper-
iments with the simple domain adaptation method
proposed by Daume? III (2007), which creates a ver-
sion of each feature for each domain. While the re-
sults were mixed, this method improves our perfor-
mance on full papers of the test set, for which little
training data is available.
2 Event extraction decomposition
Figure 1 describes the event extraction decomposi-
tion that is used throughout the paper. Each stage has
its own module to perform the classification needed.
In trigger recognition the system decides whether
a token acts as a trigger for one of the nine event
types or not. We only consider tokens that are tagged
as nouns, verbs or adjectives by the parser, as they
36
SQ 22536 suppressed
Neg reg
gp41 -induced
Pos reg
IL-10 production
Gene exp
(a) Trigger recognition
SQ 22536 suppressed
Neg reg
gp41 -induced
Pos reg
IL-10 production
Gene exp
Theme
ThemeTheme
(b) Theme assignment
SQ 22536 suppressed
Neg reg
gp41 -induced
Pos reg
IL-10 production
Gene exp
Theme
Theme
Cause
Theme
(c) Cause assignment
ID type Trigger Theme Cause
E1 Neg reg suppressed E2
E2 Pos reg induced E3 gp41
E3 Gene exp production IL-10
(d) Event construction
Figure 1: The stages of our biomedical event extraction system.
cover the majority of the triggers in the data. The
main features used in the classifier represent the
lemma of the token which is sufficient to predict
the event type correctly in most cases. In addition,
we include features that conjoin each lemma with
its part-of-speech tag and its immediate lexical and
syntactic context, which allows us to handle words
that can represent different event types, e.g. ?activ-
ity? often denotes a Regulation event but in ?binding
activity? it denotes a Binding event instead.
In Theme assignment, we form an agenda of can-
didate trigger-argument pairs for all trigger-protein
combinations in the sentence and classify them as
Themes or not. Whenever a trigger is predicted to be
associated with a Theme, we form candidate pairs
between all the Regulation triggers in the sentence
and that trigger as the argument, thus allowing the
prediction of nested events. Also, we remove candi-
date pairs that could result in directed cycles, as they
are not allowed by the task. In Cause assignment,
we form an agenda of candidate trigger-argument
pairs and classify them as Causes or not. We form
pairs between Regulation class triggers that were as-
signed at least one Theme, and protein names and
other triggers that were assigned at least one Theme.
The features used in these two stages are extracted
from the syntactic dependency path and the textual
string between the trigger and the argument. We
extract the shortest unlexicalized dependency path
connecting each trigger-argument pair using Dijk-
stra?s algorithm, allowing the paths to follow either
dependency direction. One set of features represents
the shortest unlexicalized path between the pair and
in addition we have sets of features representing
each path conjoined with the lemma, the PoS tag and
the event type of the trigger, the type of the argument
and the first and last lemmas in the dependency path.
In the event construction stage, we convert the
predictions of the previous stages into events. If
a Binding trigger is assigned multiple Themes, we
choose to form either one event per Theme or one
event with multiple Themes. For this purpose, we
group the arguments of each nominal Binding trig-
ger according to the first label in their dependency
path and generate events using the cross-product of
these groups. For example, assuming the parse was
correct and all the Themes recognized, ?interactions
of A and B with C? results in two Binding events
with two Themes each, A with C, and B with C re-
spectively. We add the exceptions that if two Themes
are part of the same token (e.g. ?A/B interactions?),
or the trigger and one of the Themes are part of the
same token, or the lemma of the trigger is ?bind?
then they form one Binding event with two Themes.
3 Structured prediction with SEARN
SEARN (Daume? III et al, 2009) forms the struc-
tured output prediction of an instance s as a se-
quence of T multiclass predictions y?1:T made by a
hypothesis h. The latter is a weighted ensemble of
classifiers that are learned jointly. Each prediction y?t
can use features from s as well as from all the pre-
vious predictions y?1:t?1, thus taking structure into
37
account. These predictions are referred to as actions
and we adopt this term in order to distinguish them
from the structured output predictions.
The SEARN algorithm is presented in Alg. 1. In
each iteration, SEARN uses the current hypothesis
h to generate a CSC example for each action y?t cho-
sen to form the prediction for each labeled instance
s (steps 6-12). The cost associated with each action
is estimated using the gold standard according to a
loss function l which corresponds to the task eval-
uation metric (step 11). Using a CSC learning al-
gorithm, a new hypothesis hnew is learned (step 13)
which is combined with the current one according to
the interpolation parameter ? (step 14). h is initial-
ized to the optimal policy (step 2) which is derived
from the gold standard. In each iteration SEARN
?corrupts? the optimal policy with the learned hy-
potheses. Thus, each hnew is adapted to the actions
chosen by h instead of the optimal policy. The algo-
rithm terminates when the dependence on the latter
becomes insignificant.
Algorithm 1 SEARN
1: Input: labeled instances S , optimal policy pi, CSC
learning algorithm CSCL, loss function `
2: current policy h = pi
3: while h depends significantly on pi do
4: Examples E = ?
5: for s in S do
6: Predict h(s) = y?1 . . . y?T
7: for y?t in h(s) do
8: Extract features ?t = f(s, y?1:t?1)
9: for each possible action yit do
10: Predict y?t+1:T = h(s|y?1:t?1, yit)
11: Estimate cit = `(y?1:t?1, y
i
t, y?t+1:T )
12: Add (?t, ct) to E
13: Learn a classifier hnew = CSCL(E)
14: h = ?hnew + (1? ?)h
15: Output: hypothesis h without pi
4 Biomedical event extraction with
SEARN
In this section we describe how we learn the event
extraction decomposition described in Sec. 2 under
SEARN. Each instance is a sentence and the hypoth-
esis learned in each iteration consists of a classifier
for each stage of the pipeline, excluding event con-
struction which is rule-based.
SEARN allows us to extract structural features for
each action from the previous ones. During trig-
ger recognition, we add as features the combination
of the lemma of the current token combined with
the event type (if any) assigned to the previous and
the next token, as well as to the tokens that have
syntactic dependencies with it. During Theme as-
signment, when considering a trigger-argument pair,
we add features based on whether the pair forms an
undirected cycle with previously predicted Themes,
whether the trigger has been assigned a protein as a
Theme and the candidate Theme is an event trigger
(and the reverse), and whether the argument is the
Theme of a trigger with the same event type. We
also add a feature indicating whether the trigger has
three Themes predicted already. During Cause as-
signment, we add features representing whether the
trigger has been assigned a protein as a Cause and
the candidate Cause is an event trigger.
Since the features extracted for an action depend
on previous ones, we need to define a prediction or-
der for the actions. In trigger recognition, we pro-
cess the tokens from left to right since modifiers
appearing before nouns tend to affect the meaning
of the latter, e.g. ?binding activity?. In Theme
and Cause assignment, we predict trigger-argument
pairs in order of increasing dependency path length,
assuming that, since they are the main source of fea-
tures in these stages and shorter paths are less sparse,
pairs containing shorter ones should be predicted
more reliably. The loss function sums the number of
false positive and false negative events, which is the
evaluation measure of the shared task. The optimal
policy is derived from the gold standard and returns
the action that minimizes the loss over the sentence
given the previous actions and assuming that all fu-
ture actions are optimal.
In step 11 of Alg. 1, the cost of each action is esti-
mated over the whole sentence. While this allows us
to take structure into account, it can result in costs
being affected by a part of the output that is not re-
lated to that action. This is likely to occur in event
extraction, as sentences can often be long and con-
tain disconnected event components in their output
graphs. For this reason we use focused costing (Vla-
chos and Craven, 2011), in which the cost estimation
for an action takes into account only the part of the
output graph connected with that action.
38
pipeline (R/P/F) SEARN (R/P/F)
trigger 49.1 64.0 55.6 83.2 28.6 42.6
Theme 43.7 78.6 56.2 63.8 72.0 67.6
Cause 13.9 61.0 22.6 33.9 53.8 41.6
Event 31.7 70.1 43.6 45.8 60.51 52.1
Table 1: Results on the development dataset.
5 Experiments
In our experiments, we perform multiclass CSC
learning using our implementation of the on-
line passive-aggressive (PA) algorithm proposed by
Crammer et al (2006). The aggressiveness param-
eter and the number of rounds in parameter learn-
ing are set by tuning on 10% of the training data
and we use the variant named PA-II with prediction-
based updates. For SEARN, we set the interpolation
parameter ? to 0.3. For syntactic parsing, we use
the output of the parser of Charniak and Johnson
(2005) adapted to the biomedical domain by Mc-
Closky (2010), as provided by the shared task orga-
nizers in the Stanford collapsed dependencies with
conjunct dependency propagation (Stenetorp et al,
2011). Lemmatization is performed using morpha
(Minnen et al, 2001). No other knowledge sources
or tools are used.
In order to assess the benefits of joint learning un-
der SEARN, we compare it against a pipeline of in-
dependently learned classifiers using the same fea-
tures and task decomposition. Table 1 reports the
Recall/Precision/F-score achieved in each stage, as
well as the overall performance. SEARN obtains
better performance on the development set by 8.5
F-score points. This increase is larger than the 7.3
points reported in Vlachos and Craven (2011) on
the BioNLP09ST1 datasets which contain only ab-
stracts. This result suggests that the gains of joint
inference under SEARN are greater when learning
from the additional data from full papers. Note
that while the classifier learned with SEARN over-
predicts triggers, the Theme and Cause classifiers
maintain relatively high precision with substantially
higher recall as they are learned jointly with it.
As triggers that do not form events are ignored by
the evaluation, trigger overprediction without event
overprediction does not result in performance loss.
The results of our submission on the test
dataset using SEARN were 42.6/61.2/50.2
(Recall/Precision/F-score) which ranked sixth
in the shared task. In the Regulation events which
are considered harder due to nesting, our submis-
sion was ranked fourth. This demonstrates the
potential of SEARN for structured prediction, as the
performance on regulation events depends partly on
the performance on the simple ones on which our
submission was ranked eighth.
After the end of the shared task, we experimented
with the domain adaptation method proposed by
Daume? III (2007), which creates multiple versions
for each feature by conjoining it with the domain la-
bel of the instance it is extracted from (abstracts or
full papers). While this improved the performance
of the pipeline baseline by 0.3 F-score points, the
performance under SEARN dropped by 0.4 points
on the development data. Using the online service
provided by the organizers, we evaluated the perfor-
mance of the domain adapted SEARN-based system
on the test set and the overall performance improved
to 50.72 in F-score (would have ranked 5th). In
particular, domain adaptation improved the perfor-
mance on full papers by 1.22 points, thus reaching
51.22 in F-score. This version of the system would
have ranked 3rd overall and 1st in the Regulation
events in this part of the corpus. We hypothesize
that these mixed results are due to the sparse fea-
tures used in the stages of the event extraction de-
composition, which become even sparser using this
domain adaptation method, thus rendering the learn-
ing of appropriate weights for them harder.
6 Conclusions
We presented a joint inference approach to the
BioNLP11ST-GE1 task using SEARN which con-
verts a structured prediction task into a set of CSC
tasks whose models are learned jointly. Our results
demonstrate that SEARN achieves substantial per-
formance gains over a standard pipeline using the
same features.
Acknowledgments
The authors were funded by NIH/NLM grant R01
LM07050.
39
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Hal Daume? III, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75:297?325.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
256?263.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Department of Computer Science,
Brown University.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In Proceedings of the Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 813?821.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task.
Andreas Vlachos and Mark Craven. 2011. Search-based
structured prediction applied to biomedical event ex-
traction. In Proceedings of the Fifteenth Conference
on Computational Natural Language Learning.
40
