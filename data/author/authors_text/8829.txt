LANGUAGE IDENTIFICATION 
IN 
UNKNOWN SIGNALS 
Contact author: John Elliott, jre@scs.leeds.ac.uk 
Co-authors: Eric Atwell, eric@scs.leeds.ac.uk 
Bill Whyte, billw@scs.leeds.ac.uk 
Organisation: Centre for Computer Analysis of Language and Speech, 
School of Computer Studies, University of Leeds, Leeds, Yorkshire, LS2 9JT England 
Abstract 
This paper describes algorithms and software developed to characterise and detect generic 
intelligent language-like features iu an input signal, using Natural Language Learning 
techniques: looking for characteristic statistical "language-signatures" in test corpora. As a 
first step towards such species-independent language-detection, we present a suite of 
programs to analyse digital representations of a range of data, and use the results to 
extrapolate whether or not there are language-like structures which distiuguish this data from 
other sources, such as nmsic, images, and white noise. We assume that generic species- 
independent commuuication can be detected by concentrating on localised patterns and 
rhythms, identifying segments at the level of characters, words and phrases, without 
necessarily having to "understand" the content. 
We assume that a language-like signal will be encoded symbolically, i.e. some kind of 
character-stream. Our language-detection algorithm for symbolic input uses a number of 
statistical clues: data compression ratio, "chunking" to find character bit-length and 
boundaries, and matching against a Zipfian type-token distribution for "letters" and "words". 
We do not claim extensive (let alne exhaustive) empirical evidence that our language- 
detection clues are "correct"; the only real test will come when the Search for Extra- 
Terrestrial Intelligence finds true alien signals. If and when true SETI signals are found, the 
first step to interpretation is to identify the language-like f atures, using techniques like the 
above. Our current research goal is to apply Natural Language Learning techniques to the 
identification of "higher-level" grammatical nd semantic structure in a linguistic signal. 
Introduction 
A useful thought experiment is to imagine 
eavesdropping on a signal from outer 
space. How can you decide that it is a 
message between intelligent life forms, 
without dialogue with the source? What is 
special about the language signal that 
separates it fiom non-language? What 
special 'zone' in the signal universe does 
language occupy? Is it, indeed, separable 
from other senti-structured sources, such 
as DNA and music (fig 1). 
Solving this problem might not only be 
useful in the event of detecting such 
signals fiom space, but also, by 
deliberately ignoring preconceptions based 
011 human texts, may provide us with some 
better understanding of what language 
really is. 
The Signal Universe 
ttowever, we ueed to start somewhere, and 
our initial investigations - which this paper 
summarises make some basic 
assumptions (which we would hope to 
relax in later research). Namely, that 
identifiable script will be a serial string, 
possessing a hierarchy of elements broadly 
equivalent to 'characters,' 'words', and 
1021 
'spaces', and possess something akin to  
human grammar .  
Identifying the 'Character Set' 
In 'real' decoding of unknown scripts it is 
accepted that identifying the correct set of 
discrete symbols is no mean feat 
(Chadwick 1967). To make life simple for 
ourselves we assume a digital signal with a 
fixed number of bits per character. Very 
different techniques are required to deal 
with audio or analogue equivalent 
waveforms (Elliott & Atweli 99, 00). We 
have reason to believe that the following 
method can be modified to relax this 
constraint, but this needs to be tested 
further. 
The task then reduces to trying to identify 
the number of bits per character. 
Suppose the probability of a bit is P~. Then 
the message ntropy of a string of length 
N will be given by: 
E = SUM \[PI In Pi\]; i =I,N 
If the signal contains merely a set of 
random digits, the expected wflue of this 
fnnctiou will rise monotonically as N 
increases. However, if the string contains a 
set of symbols of fixed length representing 
a character set used for communication, it 
is likely to show some decrease in entropy 
when analysed in blocks of this length, 
because the signal is 'less random' when 
thus blocked. Of course, we need to 
analyse blocks that begin and end at 
character boundaries. We simply carry out 
the measurements in sliding windows 
along the data. In figure 2 below, we see 
what happeus when we. apply this to 
samples of 8-bit ASCII text: 
i 
Enlropy Figure 2 
Lallgtlage 
{ I 
I 
4 5 6 7 8 9 
Bit Length 
Entropy profile as an indicalor of character bit-length 
We notice a clear drop, as predicted, for a 
bit length of 8.Modest progress though it 
may be, it is not unreasonable to assume 
that the first piece of evidence for the 
presence of language-like sO'ucture, 
would be the identification of a low- 
entropy, character set within the signal. 
Identifying 'Words' 
Again, work by crytopaleologists suggests 
that, once the character set has been found, 
the separation into word-like units, is not 
trivial and again we cheat, slightly: we 
assume that the language possesses 
something akin to a 'space' character. 
Taking our entropy measurement 
described above as a way of separating 
characters, we now try to identify the one, 
which represents 'space'. It is not 
unreasonable to believe that, in a word- 
based language, it is likely to be one of the 
most frequently used characters. 
Using a uumber of texts in a variety of 
languages, we first identified the top three 
most used characters. For each of these we 
hypothesised in turn that it represented 
'space'. This then allowed us to segment 
the signal into words-like units ('words' 
t'o1" simplicity). We coukl then compute the 
frequency distribution of words as a 
function of word length, for each of the 
three candidate 'space' characters (fig 3). 
Figure 3: Candidate word-lcnglh dishibulions 
using the 3 most frequent characters. 
400 
35O 
o ~ 3oo 
: ~ 250 
g 200 
i , ,  ~ 150 
i 1 O0 
l 50  
I 0 
I 
I 
It can be seen that  one 'separator' 
candidate (unsurprisingly, in fact, the most 
frequent character of all) results in a very 
varied distribution of word lengths. This is 
an interesting distribution, which, on the 
right hand side of the peak, approximately 
follows the well-known 'law' according to 
Zipf (Zipf, 1949), which predicts this 
1022 
behaviour o i l  the grounds of minimum 
efl'ort in a communication act. 
To ascertain whether the word-length 
frequency distribution holds for hmguage 
in general, nmltiple salnples from 20 
different hmguages fi'om Indo-European, 
Bantu, Semitic, Finno-Ugrian and Malayo- 
Polynesian groups were analysed (fig 4). 
Word lenglh dislribulions in mulliplc samples fl'om 
lndo-Eurol~ean, Semitic, l:inno-Ugrian, and Malayo- 
I'olynesian language groups 
20.00 :' 
15.oo 
10.00 
o 
5.00 
i:: ::y-: . . 
0 .00  :v  '1 ' ,~ , '  i - :  'i ' '~' -r ..... , 
? - "~  I'~ 0 03 r,D 03 C',l LO 
Od Cxl Figure 4 Word length 
Using statistical measures of signil'icance, 
it was found that most groups fell well 
within 5% limits - only two individual 
hmguages were near exceeding these 
limits -- of the proposed Human language 
word-length profile shown in fig 5. 
Figure 5: Itulnan language word-lengfl~ 
frequency distribution profile 
10.00  
t~ 5.00 f r l  
\[H . . . . . . . . . .  i 0.00  ~ , ~ ~ , 
Word l ength  l 
i 
Zipf's law is a strong indication of 
language-like behaviour. It can be used 
to segment the signal ptvvided a 'space' 
character exists. 
However, we shotdd not assume Zipf to be 
an infifllible language detector. Other 
natural phenomena such as molecular 
distribution in yeast DNA possess 
characteristics of power laws. Analyses of 
protein length distributions also display 
Poisson distributions where the number of 
proteins is plotted against the lengths of 
amino acids (Jenson 1998). 
Identifying 'Phrases' 
Although alien brains may be more oi less 
powerlhl than ours (Norris 1999), it is 
reasonable to assume that all intelligent 
Doblem solvers are subject to the same 
ultimate constraints of computatioual 
power and storage and their symbol 
systems will reflect his. 
Thus, language must use small sets of 
rules to generate a vast world of 
implications and consequences. Perhaps 
its most ilnportant single device is the use 
of embedded clauses and phrases (Minsky 
1984), with which to represent an 
expression or description, however 
complex, as a single component of another 
description. 
In serial languages, this appears to be 
achieved by clustering words into 'chunks' 
(phrases, sentences) of information, which 
are more-or-less consistent and self- 
contained elements of thought. 
lVurthermore, in human language at least, 
these 'chunks' tend to consist of contelzl 
terms, which describe what the chunk is 
'about' and .fimctional terms, which 
attribute references aud context by which 
tile content erms convey their information 
unambiguously. 'King' is usually a 
content erm; 'of' and 'the' are functional. 
We use 'term' rather than word, because 
many languages make far less use of full 
words for l'unctional operations than does 
English: in Latin the transformation 'rex' 
('king') to 'regis' (of the king) is one such 
example. 
Functional terms in a language tend to be 
short, probably attributable to the principle 
of least effort, as they are used frequently. 
A further distinguishing characteristic of 
functional and content terms is that 
different texts will often wtry in their 
content but tend to share a common 
linguistic structure and therefore make 
similar use of functional terms. That is, the 
probability distribution of content terms 
will vary from text to text, but the 
distribution of ftmction terms will not. 
Using English text, which had been 
enciphered using a simple substitution 
cipher (to avoid cheating), we identified 
1023 
across a variety of texts, the most common 
words, with least inter-text variation. 
These we call 'candidate function words'. 
Now, suppose these words occurred at 
random in the signal: we would expect o 
see the spacing between them to be merely 
a ftmction of their individual probabilities 
of occurrence. Analysing this statistically 
(as a Poisson distribution) or simply 
simulate it practically, we find that there 
are a non-insignificant number of cases 
wherein there are very large gaps (of the 
order of several tens of words) between 
successive occurrences. Compare this with 
the results from our analysis (fig 6). 
I\[ll,,. 
3 4 5 6 7 8 9 
Figure 6 
Function 
Word 
separation 
in 
English. 
Number of words between candidate flmclional words 
initial findings show that the frequency 
distribution of these lengths of text - our 
candidate phrases - follow a Zipfian 
distribution curve and rarely exceed 
lengths of more than eight. 
We might conclude from this, that our 
brains tend to "chunk' linguistic 
information into phrase-like structures of  
the order of seven or so word units long. 
Interestingly enough, this fits in well with 
human cognition theory (Ally & Bacon 
1991), which states that out: short-term 
mental capacity operates well only up to 7 
(+ or -  2) pieces of information, but any 
causal connection between this and our 
results must be considered highly 
speculative at this stage! 
Directions for Future Research 
We are familiar with parts of speech 
(commonly, 'nouns', verbs' etc) in 
language. Identification of patterns 
indicative of these would be further 
evidence of language-like characteristics 
and, by allowing us to group together the 
numerous word tokens in any language 
into smaller, more manageable collections 
would facilitate statistical analysis. Some 
attempts have been made in the past to use 
n-gram probabilities in order to define 
word classes or 'parts of speech' 
(Charniak 1993). 
In our own work we have begun the 
development of tools that measure the 
correlation profile between pairs of words, 
as a precursor to deducing general 
principles for 'typing' and clustering into 
syntactico-semantic classes. 
Correlation profile for word pair 
P(wl,w2 ) P(w2,wl) 
The figure 7 above shows the results for 
the relationship between a pair of 
unknown (because of tile substitution 
cipher approach) content and functional 
words, so identified by looking at their 
cross-corpus statistics as described above. 
It can be seen that the functional word has 
a very high probability of preceding the 
content word but has 11o instance of 
directly following it. At least 
metaphorically, the graph can be 
considered to show the 'binding force' 
between the two words varying with their 
separation. We are looking at how this 
metaphor might be used in order to 
describe language as a molecular structure, 
whose 'inter-molecular forces' can be 
related to part-of-speech interaction and 
the development of potential semantic 
categories for the unknown language. 
So far we have mainly been working with 
English, but we have begun to look at 
1024 
languages which represent their flmctional 
relationships by internal changes to words 
or by the addition of prefixes or suffixes. 
Although the process for separating into 
functional and content terms is more 
complex, we believe the fundamental 
results should be consistent. This will be 
one test of the theories presented above. 
In general, we realise that testing our 
hmguage detection algorithms will be a 
significant issue. We do not have 
examples that we know to be definitely 
from non-hulnan, but intelligent origins, 
and we need to look extensively at signals 
of non-intelligent origin which may mimic 
some of the language characteristics 
described above. This will form a 
significant part of our fllture work and we 
welcome discussion and suggestions. 
Conclusion 
Language in its written format has proved 
to be a rich source for a variety ot' 
statistical analyses - some more conclusive 
than others - which when combined, give a 
comprehensive algorithm for identifying 
the presence of language-like systems. 
Analysis stages include compression, 
entropy profile, type-token distribution, 
word-length Zipfian analysis, finding a 
fiequency distribution signature by 
successive chunking, stemming, cohesion 
analysis, phrase-length fiequency 
distribution and pattern comparison across 
smnples. 
REFERENCES 
Ally & Bacon, Cognitive Psychology, 
(third edition), Solso, Massachusetts, 
USA, 1991. 
Baldi, P., & Brunak, S., Bioinformatics - 
The Machine Learning Approach, MIT 
press, Cmnbridge Massaclmsetts, 1998. 
Chadwick, J. Tim Decipherment of Linear 
B, Cmnbridge University Press, 1967. 
Charniak E., Statistical language learning 
Bradford/MIT Press, Cambridge. 1993. 
Elliott, J & Atwell, E, Language in 
signals: the detection of generic species- 
independent intelligent language features 
in symbolic and oral communications, 
Proceedings of the 50 th International 
Astronautical Congress, paper IAA-99- 
IAA.9.1.08, International Astronautical 
Federation, Paris, 1999. 
Elliott, J & Atweli, E., Is anybody out 
there?: the detection of intelligent and 
generic language-like f atures, Journal of 
the British Interplanetary Society, Vo153 
No 1 &2. 
Elliott, J, Decoding the Martian 
Chronicles, MSc project report, School of 
Comlmter Studies, University of Leeds 
1999. 
Hughes J & Atwell E., The automated 
ewtluation of inferred word classifications 
in Proceedings of the European 
Conference on Artificial Intelligence 
(ECAI'94), pp550-554, John Wiley, 
Chichester. 1994. 
Jenson, H. Sell' Organised Criticality, 
Cambridge University Press, 1998. 
Minsky, M., Why Intelligent Aliens will 
be Intelligible, Cambridge University 
Press, 1984. 
Norris, R,. How old is ET?, Proceedings of 
50th International Ashonautical Congress, 
paper 1AA-99-IAA.9.1.04, International 
Astronautical Federation, Paris. 1999. 
Zipf, G. K., Human Behaviour and The 
Principle of Least Effort, Addison Wesley 
Press, New York, 1949 (1965 reprint) 
1025 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 25-30, Lisbon, Portugal, 2000. 
Increasing our Ignorance of Language: Identifying Language 
Structure in an Unknown 'Signal' 
J ohn  E l l i o t t  and Er i c  A twe l l  and Bi l l  Whyte  
Centre for Computer  Analysis of Language and Speech, School of Computer  Studies 
University of Leeds, Leeds, Yorkshire, LS2 9JT England 
{ j re ,  e r i c ,  b i l lw}?scs . leeds .ac .uk  
Abst rac t  
This paper describes algorithms and software 
developed to characterise and detect generic 
intelligent language-like features in an input 
signal, using natural language learning tech- 
niques: looking for characteristic statistical 
"language-signatures" in test corpora. As 
a first step towards such species-independent 
language-detection, we present a suite of pro- 
grams to analyse digital representations of a 
range of data, and use the results to extrap- 
olate whether or not there are language-like 
structures which distinguish this data from 
other sources, such as music, images, and white 
noise. Outside our own immediate NLP sphere, 
generic communication techniques are of par- 
ticular interest in the astronautical community, 
where two sessions are dedicated to SETI at 
their annual International conference with top- 
ics ranging from detecting ET technology to the 
ethics and logistics of message construction (E1- 
liott and Atwell, 1999; Ollongren, 2000; Vakoch, 
2000). 
1 In t roduct ion  
A useful thought experiment is to imagine 
eavesdropping on a signal from outer space. 
How can you decide that it is a message be- 
tween intelligent life forms? We need a 'lan- 
guage detector': or, to put it more accu- 
rately, something that separates language from 
non-language. But what is special about the 
language signal that separates it from non- 
language? Is it, indeed, separable? 
The problem goal is to separate language 
from non-language without dialogue, and learn 
something about the structure of language in 
the passing. The language may not be human 
(animals, aliens, computers...), the perceptual 
space can be unknown, and we cannot assume 
human language structure but must begin some- 
where. We need to approach the language signal 
from a naive viewpoint, in effect, increasing our 
ignorance and assuming as little as possible. 
Given this standpoint, an informal descrip- 
tion of 'language' might include that it: 
? has structure at several interrelated levels 
? is not random 
? has grammar 
? has letters/characters, words, phrases and 
sentences 
? has parts of speech 
? is recursive 
? has a theme with variations 
? is aperiodic but evolving 
? is generative 
? has transformation rules 
? is designed for communication 
? has Zipfian type-token distributions at sev- 
eral levels 
Language as a 'signal' 
? has some signalling elements (a 'script') 
? has a hierarchy of signalling elements? 
('Words', 'phrases' etc.) 
? is serial? 
? is correlated across a distance of several sig- 
nalling elements applying at various levels 
in the hierarchy 
? is usually not truly periodic 
? is quasi-stationary? 
? is non-ergodic? 
We assume that a language-like signal will be 
encoded symbolically, i.e. with some kind of 
character-stream. Our language-detection al- 
gorithm for symbolic input uses a number of 
25 
statistical clues such as entropy, "chunking" to 
find character bit-length and boundaries, and 
matching against a Zipfian type-token distribu- 
tion for "letters" and "words". 
2 Ident i fy ing  S t ructure  and  the  
'Character  Set '  
The initial task, given an incoming bit-stream, 
is to identify if a language-like structure ex- 
ists and if detected what are the unique pat- 
terns/symbols, which constitute its 'character 
set'. A visualisation of the alternative possible 
byte-lengths i gleaned by plotting the entropy 
calculated for a range of possible byte-lengths 
(fig 1). 
In 'real' decoding of unknown scripts it is ac- 
cepted that identifying the correct set of dis- 
crete symbols is no mean feat (Chadwick, 1967). 
To make life simple for ourselves we assume 
a digital signal with a fixed number of bits 
per character. Very different echniques are re- 
quired to deal with audio or analogue quivalent 
waveforms (Elliott and Atwell, 2000; Elliott and 
Atwell, 1999). We have reason to believe that 
the following method can be modified to relax 
this constraint, but this needs to be tested fur- 
ther. The task then reduces to trying to iden- 
tify the number of bits per character. Given the 
probability of a bit is Pi; the message ntropy 
of a string of length N will be given by the first 
order measure: 
E = SUM\[P i lnP i \ ] ; i  = 1, N 
If the signal contains merely a set of random dig- 
its, the expected value of this function will rise 
monotonically as N increases. However, if the 
string contains a set of symbols of fixed length 
representing a character set used for commu- 
nication, it is likely to show some decrease in 
entropy when analysed in blocks of this length, 
because the signal is 'less random' when thus 
blocked. Of course, we need to analyse blocks 
that begin and end at character boundaries. We 
simply carry out the measurements in sliding 
windows along the data. In figure 1, we see 
what happens when we applied this to samples 
of 8-bit ASCII text. We notice a clear drop, 
as predicted, for a bit length of 8. Modest 
progress though it may be, it is not unreason- 
able to assume that the first piece of ev- 
idence for the presence of language- l ike 
s t ruc ture ,  wou ld  be the  ident i f icat ion of  a 
low-entropy,  character  set w i th in  the  sig- 
nal. 
The next task, still below the stages normally 
tackled by NLL researchers, is to chunk the in- 
coming character-stream into words. Looking 
at a range of (admittedly human language) text, 
if the text includes a space-like word-separator 
character, this will be the most frequent charac- 
ter. So, a plausible hypothesis would be that the 
most frequent character is a word-separator1; 
then plot type-token frequency distributions for 
words, and for word-lengths. If the distribu- 
tions are Zipfian, and there are no significant 
'outliers' (very large gaps between 'spaces' sig- 
nifying very long words) then we have evidence 
corroborating our space hypothesis; this also 
corroborates our byte-length ypothesis, since 
the two are interdependent. 
3 Ident i fy ing  'Words '  
Again, work by crytopaleologists suggests that, 
once the character set has been found, the sep- 
aration into word-like units, is not trivial and 
again we cheat, slightly: we assume that the 
language possesses something akin to a 'space' 
character. Taking our entropy measurement de- 
scribed above as a way of separating characters, 
we now try to identify which character epre- 
sents 'space'. It is not unreasonable to believe 
that, in a word-based language, it is likely to be 
one of the most frequently used characters. 
Using a number of texts in a variety of lan- 
guages, we first identified the top three most 
used characters. For each of these we hy- 
pothesised in turn that it represented 'space'. 
This then allowed us to segment the signal into 
words-like units ('words' for simplicity). We 
could then compute the frequency distribution 
of words as a function of word length, for each 
of the three candidate 'space' characters (fig 2). 
It can be seen that one 'separator' candidate 
(unsurprisingly, in fact, the most frequent char- 
acter of all) results in a very varied distribu- 
tion of word lengths. This is an interesting 
distribution, which, on the right hand side of 
the peak, approximately follows the well-known 
'law' according to Zipf (1949), which predicts 
this behaviour on the grounds of minimum ef- 
1Work is currently progressing on techniques for un- 
supervised word separation without spaces. 
26 
fort in a communication act. Conversely, re- 
sults obtained similar to the 'flatter' distribu- 
tions above, when using the most frequent char- 
acter, is likely to indicate the absence of word 
separators in the signal. 
To ascertain whether the word-length fre- 
quency distribution holds for language in gen- 
eral, multiple samples from 20 different lan- 
guages from Indo-European, Bantu, Semitic, 
Finno-Ugrian and Malayo-Polynesian groups 
were analysed (fig 3). Using statistical measures 
of significance, it was found that most groups 
fell well within 5- only two individual languages 
were near exceeding these limits - of the pro- 
posed Human language word-length profile (E1- 
liott et al, 2000). 
Zipf 's  law is a s t rong  ind icat ion  of  
language- l ike behav iour .  I t  can be used 
to segment  the  signal p rov ided  a 'space'  
character  exists. However, we should not 
assume Zipf to be an infallible language detec- 
tor. Natural phenomena such as molecular dis- 
tribution in yeast DNA possess characteristics 
of power laws (Jenson, 1998). Nevertheless, it 
is worth noting, that such non-language posses- 
sors of power law characteristics generally dis- 
play distribution ranges far greater than lan- 
guage with long repeats far from each other 
(Baldi and Brunak, 1998); characteristics de- 
tectable at this level or at least higher order 
entropic evaluation. 
4 Ident i fy ing  'Phrase- l i ke '  chunks  
Having detected a signal which satisfies cri- 
teria indicating language-like structures at a 
physical evel (Elliott and Atwell, 2000; Elliott 
and Atwell, 1999), second stage analysis is re- 
quired to begin the process of identifying inter- 
nal grammatical components, which constitute 
the basic building blocks of the symbol system. 
With the use of embedded clauses and phrases, 
humans are able to represent an expression or 
description, however complex, as a single com- 
ponent of another description. This allows us to 
build up complex structures far beyond our oth- 
erwise restrictive cognitive capabilities (Minsky, 
1984). Without committing ourselves to a for- 
mal phrase structure approach, (in the Chom- 
skian sense) or even to a less formal 'chunk- 
ing' of language (Sparkle Project, 2000), it is 
this universal hierarchical structure, evident in 
all human languages and believed necessary for 
any advanced communicator ,  that constitutes 
the next phase in our signal analysis (Elliott and 
Atwell, 2000). It is f rom these 'discovered' ba- 
sic syntactic units that analysis of behavioural 
trends and inter-relationships amongst  termi- 
nals and non-terminals alike can begin to unlock 
the encoded internal grammatical  structure and 
indicate candidate parts of speech. To  do this, 
we  make use of a particular feature common to 
many known languages, the 'function' words, 
which occur in corpora with approximately the 
same statistics. These tend to act as bound- 
aries to fairly self-contained semantic/syntactic 
'chunks.' They  can be identified in corpora by 
their usually high frequency of occurrence and 
cross-corpora invariance, as opposed to 'con- 
tent' words which are usually less frequent and 
much more  context dependent. 
Now suppose the function words arrived in a 
text independent of the other words, then they 
would have a Poisson distribution, with some 
long tails (distance between successive function 
words.) But  this is NOT what  happens. In- 
stead, there is empirical evidence that function 
word  separation is constrained to within short 
limits, with very few more  than nine words 
apart (see fig 4). We conjecture that this is 
highly suggestive of chunking. 
5 C lus ter ing  in to  
syntact i co -semant ic  lasses 
Unlike traditional natural language process- 
ing, a solution cannot be assisted using vast 
amounts of training data with well-documented 
'legal' syntax and semantic interpretation or 
known statistical behaviour of speech cate- 
gories. Therefore, at this stage we are endeav- 
ouring to extract the syntactic elements with- 
out a 'Rossetta' stone and by making as few as- 
sumptions as possible. Given this, a generic sys- 
tem is required to facilitate the analysis of be- 
havioural trends amongst selected pairs of ter- 
minals and non-terminals alike, regardless of the 
target language. 
Therefore, an intermediate r search goal is to 
apply Natural Language Learning techniques to 
the identification of "higher-level" lexical and 
grammatical patterns and structure in a lin- 
guistic signal. We have begun the development 
of tools to visualise the correlation profiles be- 
27 
tween pairs of words or parts of speech, as a pre- 
cursor to deducing eneral principles for 'typing' 
and clustering into syntactico-semantic lexical 
classes. Linguists have long known that collo- 
cation and combinational patterns are charac- 
teristic features of natural anguages, which set 
them apart (Sinclair, 1991). Speech and lan- 
guage technology researchers have used word- 
bigram and n-gram models in speech recogni- 
tion, and variants of PoS-bigram models for 
Part-of-Speech tagging. In general, these mod- 
els focus on immediate neighbouring words, but 
pairs of words may have bonds despite sepa- 
ration by intervening words; this is more rele- 
vant in semantic analysis, eg Wilson and Rayson 
(1993), Demetriou (1997). We sought to in- 
vestigate possible bonding between type tokens 
(i.e., pairs of words or between parts of speech 
tags) at a range of separations, by mapping the 
correlation profile between a pair of words or 
tags. This can be computed for given word-pair 
type (wl,w2) by recording each word-pair token 
(wl,w2,d) in a corpus, where d is the distance or 
number of intervening words. The distribution 
of these word-pair tokens can be visualised by 
plotting d (distance between wl and w2) against 
frequency (how many (wl,w2,d) tokens found at 
this distance). Distance can be negative, mean- 
ing that w2 occurred be/ore wl and for any size 
window (i.e., 2 to n). In other words, we postu- 
late that it might be possible to deduce part-of- 
speech membership and, indeed, identify a set 
of part-of-speech classes, using the joint proba- 
bility of words themselves. But is this possible? 
One test would be to take an already tagged 
corpus and see if the parts-of-speech did indeed 
fall into separable clusters. 
Using a five thousand-word extract from the 
LOB corpus (Johansson et al, 1986) to test this 
tool, a number of parts-of-speech pairings were 
analysed for their cohesive profiles. The arbi- 
trary figure of five thousand was chosen, as it 
both represents a sample large enough to re- 
flect trends seen in samples much larger (with- 
out loosing any valuable data) and a sample 
size, which we see as at least plausible when 
analysing ancient or extra-terrestrial l nguages 
where data is at a premium. 
Figure 5 shows the results for the relationship 
between a pair of content and function words, so 
identified by looking at their cross-corpus statis- 
tics. It can be seen that the function word has a 
high probability of preceding the content word 
but has no instance of directly following it. At 
least metaphorically, the graph can be consid- 
ered to show the 'binding force' between the two 
words varying with their separation. We are 
looking at how this metaphor might be used in 
order to describe language as a molecular struc- 
ture, whose 'inter-molecular forces' can be re- 
lated to part-of-speech interaction and the de- 
velopment of potential semantic ategories for 
the unknown language. 
Examining language in such a manner also 
lends itself to summarising ('compressing') the 
behaviour to its more notable features when 
forming profiles. Figure 6 depicts a 3D repre- 
sentation of results obtained from profiling VB- 
tags with six other major syntactic ategories; 
figure 7 shows the main syntactic behavioural 
features found for the co-occurrence of some of 
the major syntactic lasses ranging over the cho- 
sen window of ten words. 
Such a tool may also be useful in other areas, 
such a lexico-grammatical analysis or tagging 
of corpora. Data-oriented approaches to cor- 
pus annotation use statistical n-grams and/or 
constraint-based models; n-grams or constraints 
with wider windows can improve error-rates, 
by examining the topology of the annotation- 
combination space. Such information could be 
used to guide development of Constraint Gram- 
mars. The English Constraint Grammar de- 
scribed in (1995) includes constraint rules up 
to 4 words either side of the current word (see 
Table 16, p352); the peaks and troughs in the 
visualisation tool might be used to find candi- 
date patterns for such long-distance constraints. 
Our research topic NLL4SETI (Natural Lan- 
guage Learning for the Search for Extra- 
Terrestrial Intelligence) is distinctive in that - 
it is potentially a VERY useful application of 
unsupervised NLL; - it starts from more ba- 
sic assumptions than most NLL research: we 
do not assume tokenisation i to characters and 
words, and have no tagged/parsed training cor- 
pus; - it focuses on utilising statistical distri- 
butional universals of language which are com- 
putable and diagnostic; - this focus has led us 
to develop distributional visualisation tools to 
explore type/token combination distributions; - 
the goal is NOT learning algorithms which anal- 
28 
yse/annotate human language in a way which 
human experts would approve of (eg phrase- 
chunking corresponding to a human linguist's 
parsing of English text); but algorithms which 
recognise language-like structuring in a poten- 
tially much wider range of digital data sets. 
6 Summary  and  fu ture  
deve lopments  
To summarise, our achievements to date include 
- a method for splitting a binary digit-stream 
into characters, by using entropy to diagnose 
byte-length; - a method for tokenising unknown 
character-streams into words of language; - 
an approach to chunking words into phrase- 
like sub-sequences, by assuming high-frequency 
function words act as phrase-delimiters; - a vi- 
sualisation tool for exploring word-combination 
patterns, where word-pairs need not be imme- 
diate neighbours but characteristically combine 
despite several intervening words. 
So far, our approaches have involved working 
with languages with which we are most familiar 
and, to a certain extent, making use of linguistic 
'knowns' such as pre-tagged corpora. It is early 
days yet and we make no apology for this initial 
approach. However, we feel that by deliberately 
reducing our dependence on prior knowledge 
('increasing our ignorance of language') and by 
treating language as a 'signal', we might be con- 
tributing a novel approach to natural anguage 
processing which might ultimately lead to a bet- 
ter, more fundamental understanding of what 
distinguishes language from the rest of the sig- 
nal universe. 
Re ferences  
P. Baldi and S. Brunak. 1998. Bioinformatics - The 
Machine Learning Approach. MIT press, Cam- 
bridge Massachusetts. 
J. Chadwick. 1967. The Decipherment ofLinear B. 
Cambridge University Press. 
George Demetriou. 1997. PhD thesis. School of 
Computer Studies, University of Leeds. 
John Elliott and Eric Atwell. 1999. Language insig- 
nals: the detection of generic species-independent 
intelligent language f atures in symbolic and oral 
communications. In Proceedings of the 50th In- 
ternational Astronautical Congress. paper IAA- 
99-IAA.9.1.08, International Astronautical Feder- 
ation, Paris. 
John Elliott and Eric Atwell. 2000. Is there any- 
body out there?: The detection of intelligent 
and generic language-like f atures. Journal of the 
British Interplanetary Society, 53:1/2:13-22. 
John Elliott, Eric Atwell, and Bill Whyte. 2000. 
Language identification in unknown signals. 
In Proceedings of COLING'2000 International 
Conference on Computational Linguistics. Saar- 
bruecken. 
H. Jenson. 1998. Self Organised Criticality. Cam- 
bridge University Press. 
Stig Johansson, Eric Atwell, Roger Garside, 
and Geoffrey Leech. 1986. The Tagged LOB 
corpus: users' manual. Bergen University, 
Norway: ICAME, The Norwegian Comput- 
ing Centre for the Humanities. Available 
from http://www.hit.uib.no/icame/lobman/lob- 
cont.html. 
Fred Karlsson, Atro Voutilainen, Juha Heikkila, 
and Arto Anttila. 1995. Constraint Grammar: 
a language-independent system for parsing unre- 
stricted text. Berlin: Mouton de Gruyter. 
Geoffrey Leech, Roger Garside, and Eric Atwell. 
1983. The automatic grammatical tagging of the 
lob corpus. ICAME Journal, 7:13-33. 
Christopher Manning and Hinrich Schutze. 1999. 
Foundations of Statistical Natural Language Pro- 
cessing. Cambridge: MIT Press. 
M. Minsky. 1984. Why Intelligent Aliens will be In- 
telligible. Cambridge University Press. 
Alexander Ollongren. 2000. Large-size message con- 
struction for eti. In Proceedings of the 50th In- 
ternational Astronautical Congress. paper IAA- 
99-IAA.9.1.09, International Astronautical Feder- 
ation, Paris. 
Sparkle Project. 2000. http://www.ilc.pi.cnr.it/spa 
rkle/wp 1-prefinal/node25.html. 
John Sinclair. 1991. Corpus, concordance, colloca- 
tion describing English language. Oxford Univer- 
sity Press. 
Doug Vakoch. 2000. Communicating scientifi- 
cally formulated spiritual principles in interstel- 
lar messages. In Proceedings of the 50th Inter- 
national Astronautical Congress. paper IAA-99- 
IAA.9.1.10, International Astronautical Federa- 
tion, Paris. 
Andrew Wilson and Paul Rayson. 1993. The au- 
tomatic ontent analysis of spoken discourse. In 
C. Souter and E. Atwell, editors, Corpus based 
computational linguistics. Rodopi, Amsterdam. 
G.K. Zipf. 1949. Human Behaviour and The Prin- 
ciple of Least Effort. Addison Wesley Press, New 
York. (1965 reprint). 
29 
I 
Entropy 
8 
7 
6 
5 
4 
3 
2 
1 
Figure 1 
Language 
I I I I I I 
4 5 6 7 8 9 
400 
350 
"~" 300 
~ 250 
20O 
150 
IO0 
5O 
O 
Figure 2: Candidate word-length distributions 
usin~ the 3 most freouent charaelers. 
Entropy profile as an indicator of character bit-length 
MulUple samples from Indo-European, Bantu, Semitic, Fin no- 
Ug rian. and M~ayo-Polyne$ian l guage groups 
25.00 
20.00 
=o 15.oo -?  ~" 
, , .  5.00 ~,t:;~ ',~%~,~ 
Figure 3 Word length 
F~quency  
Fig are 4 
Function 
Word 
separation i
English. 
I[ll,,. 
3 4 5 6 7 8 9 
Number of words between candidate functional words 
i l aa  - . - - t  . . . . . . . . . . . . . . . . . .  
foa l  .....q . . . . . . . . . . . . . . . . .  
9t~ . -4  . . . . . . . . . . . . . . . . . . . . . . . . .  
J too  ,.  , .  ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6a l  . . . . .4  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
l 
~ to  :- . . . . . . . . . . . . .  4 
. . . . . .  \[ 
Correlation profile tbr word pair 
i Figure 5 
I 
....... ............................. 1................................... 
. .  I 
t 
........................ i ................. i ... 
. . . . . . . . . . .  t . . . . .  i- 
ill .............. i J~, ......... 
P(w I !'unctional~w2 coalcn:) I 
tmquen~ 
Figure 6: VB-tag profile 
Figure 7 Cnoun 
Cnoun \[3, ~,3 
Jj 13. 
Rb Z, Xs 
Prep 6", )~2 
Cc 6", ~3:4 
Vb X2 
Art 13" 
Jj Rb Prep Cc Vb Art 
6" ~'2 13" 13', ~6 6, ~'2 6, ~'2 
\[3 6, ~,5,9 ~'2 ~2,4 6 6, ~,3 
~7 \[3 13" 6, ~9 13 ~2 
~2 6*, ~7 6, ~'3 ~'3 Z*,~9 \[3 
\[3 13, 26 ~,4 Z ~,5 13" 
k2 13 13" 6, z9 z 13. 
13" 6, ~,3,8 Z, ~,2 Z* Z Z, ~4 
Key: Z = Zero bigram - or at offset specified - occurrences. 6 = Very weak bonding - near zero - at 
bigram occurrences. \[3 = Strong bonding at bigram co-occurrences. * = Indicates opposing cohesive 
trend when P.O.S. reversed. Xn = High peak beyond bigram at offset distance of  'n'. @ = Flat 
distribution across offsets - bigram bonding evident. 
