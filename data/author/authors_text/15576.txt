Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 748?758,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Classifying Sentences as Speech Acts in Message Board Posts
Ashequl Qadir and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{asheq,riloff}@cs.utah.edu
Abstract
This research studies the text genre of mes-
sage board forums, which contain a mix-
ture of expository sentences that present fac-
tual information and conversational sentences
that include communicative acts between the
writer and readers. Our goal is to create
sentence classifiers that can identify whether
a sentence contains a speech act, and can
recognize sentences containing four different
speech act classes: Commissives, Directives,
Expressives, and Representatives. We con-
duct experiments using a wide variety of fea-
tures, including lexical and syntactic features,
speech act word lists from external resources,
and domain-specific semantic class features.
We evaluate our results on a collection of mes-
sage board posts in the domain of veterinary
medicine.
1 Introduction
In the 1990?s, the natural language processing com-
munity shifted much of its attention to corpus-based
learning techniques. Since then, most of the text cor-
pora that have been annotated and studied are collec-
tions of expository text (e.g., news articles, scientific
literature, etc.). The intent of expository text is to
present or explain information to the reader. In re-
cent years, there has been a growing interest in text
genres that originate from Web sources, such as we-
blogs and social media sites (e.g., tweets). These
text genres offer new challenges for NLP, such as
the need to handle informal and loosely grammatical
text, but they also pose new opportunities to study
discourse and pragmatic phenomena that are funda-
mentally different in these genres.
Message boards are common on the WWW as a
forum where people ask questions and post com-
ments to members of a community. They are typ-
ically devoted to a specific topic or domain, such as
finance, genealogy, or Alzheimer?s disease. Some
message boards offer the opportunity to pose ques-
tions to domain experts, while other communities
are open to anyone who has an interest in the topic.
From a natural language processing perspective,
message board posts are an interesting hybrid text
genre because they consist of both expository text
and conversational text. Most obviously, the conver-
sations appear as a thread, where different people
respond to each other?s questions in a sequence of
posts. Studying the conversational threads, however,
is not the focus of this paper. Our research addresses
the issue of conversational pragmatics within indi-
vidual message board posts.
Most message board posts contain both exposi-
tory sentences as well as speech acts. The person
posting a message (the ?writer?) often engages in
speech acts with the readers. The writer may explic-
itly greet the readers (?Hi everyone!?), request help
from the readers (?Anyone have a suggestion??), or
commit to a future action (?I promise I will report
back soon.?). But most posts contain factual infor-
mation as well, such as general knowledge or per-
sonal history describing a situation, experience, or
predicament.
Our research goals are twofold: (1) to distin-
guish between expository sentences and speech act
sentences in message board posts, and (2) to clas-
748
sify speech act sentences into four types: Com-
missives, Directives, Expressives, and Representa-
tives, following Searle?s original taxonomy (Searle,
1976). Speech act classification could be useful
for many applications. Information extraction sys-
tems could benefit from filtering speech act sen-
tences (e.g., promises and questions) so that facts are
only extracted from the expository text. Identifying
Directive sentences could be used to summarize the
questions being asked in a forum over a period of
time. Representative sentences could be extracted
to highlight the conclusions and beliefs of domain
experts in response to a question.
In this paper, we present sentence classifiers that
can identify speech act sentences and classify them
as Commissive, Directive, Expressive, and Repre-
sentative. First, we explain how each speech act
class is manifested in message board posts, which
can be different from how they occur in spoken dia-
logue. Second, we train classifiers to identify speech
act sentences using a variety of lexical, syntactic,
and semantic features. Finally, we evaluate our sys-
tem on a collection of message board posts in the
domain of veterinary medicine.
2 Related Work
There has been relatively little work on applying
speech act theory to written text genres, and most
of the previous work has focused on email classi-
fication. Cohen et al (2004) introduced the notion
of ?email speech acts? defined as specific verb-noun
pairs following a pre-designed ontology. They ap-
proached the problem as a document classification
task. Goldstein and Sabin (2006) adopted this no-
tion of email acts (Cohen et al, 2004) but focused
on verb lexicons to classify them. Carvalho and
Cohen (2005) presented a classification scheme us-
ing a dependency network, capturing the sequential
correlations with the context emails using transition
probabilities from or to a target email. Carvalho and
Cohen (2006) later employed N-gram sequence fea-
tures to determine which N-grams are meaningfully
related to different email speech acts with a goal
towards improving their earlier email classification
based on the writer?s intention.
Lampert et al (2006) performed speech act clas-
sification in email messages following a verbal re-
sponse modes (VRM) speech act taxonomy. They
also provided a comparison of VRM taxonomy with
Searle?s taxonomy (Searle, 1976) of speech act
classes. They evaluated several machine learning al-
gorithms using syntactic, morphological, and lexi-
cal features. Mildinhall and Noyes (2008) presented
a stochastic speech act model based on verbal re-
sponse modes (VRM) to classify email intentions.
Some research has considered speech act classes
in other means of online conversations. Twitchell
and Jr. (2004) and Twitchell et al (2004) employed
speech act profiling by plotting potential dialogue
categories in a radar graph to classify conversa-
tions in instant messages and chat rooms. Nas-
tri et al (2006) performed an empirical analysis of
speech acts in the away messages of instant mes-
senger services to achieve a better understanding of
the communication goals of such services. Ravi
and Kim (2007) employed speech act profiling in
online threaded discussions to determine message
roles and to identify threads with questions, answers,
and unanswered questions. They designed their own
speech act categories based on their analysis of stu-
dent interactions in discussion threads.
The work most closely related to ours is the re-
search of Jeong et al (2009) on semi-supervised
speech act recognition in both emails and forums.
Like our work, their research also classifies indi-
vidual sentences, as opposed to entire documents.
However, they trained their classifier on spoken
telephone (SWBD-DAMSL corpus) and meeting
(MRDA corpus) conversations and mapped the la-
belled dialog act classes of these corpora to 12 di-
alog act classes that they found suitable for email
and forum text genres. These dialog act classes (ad-
dressed as speech acts by them) are somewhat differ-
ent from Searle?s original speech act classes. They
also used substantially different types of features
than we do, focusing primarily on syntactic subtree
structures.
3 Classifying Speech Acts in Message
Board Posts
3.1 Speech Act Class Definitions
Searle?s (Searle, 1976) early research on speech acts
was seminal work in natural language processing
that opened up a new way of thinking about con-
749
versational dialogue and communication. Our goal
was to try and use Searle?s original speech act def-
initions and categories as the basis for our work to
the greatest extent possible, allowing for some inter-
pretation as warranted by the WWW message board
text genre.
For the purposes of defining and evaluating our
work, we created detailed annotation guidelines for
four of Searle?s speech act classes that commonly
occur in message board posts: Commissives, Direc-
tives, Expressives, and Representatives. We omitted
the fifth of Searle?s original speech act classes, Dec-
larations, because we virtually never saw declara-
tive speech acts in our data set.1 The data set used in
our study is a collection of message board posts in
the domain of veterinary medicine. We designed our
definitions and guidelines to reflect language use in
the text genre of message board posts, trying to be as
domain-independent as possible so that these defini-
tions should also apply to message board texts rep-
resenting other topics. However, we give examples
from the veterinary domain to illustrate how these
speech act classes are manifested in our data set.
Commissives: A Commissive speech act oc-
curs when the speaker commits to a future course
of action. In conversation, common Commissive
speech acts are promises and threats. In message
boards, these types of Commissives are relatively
rare. However, we found many statements where the
main purpose was to confirm to the readers that the
writer would perform some action in the future. For
example, a doctor may write ?I plan to do surgery on
this patient tomorrow? or ?I will post the test results
when I get them later today?. We viewed such state-
ments as implicit commitments to the reader about
intended actions. We also considered decisions not
to take an action as Commissive speech acts (e.g., ?I
will not do surgery on this cat because it would be
too risky.?). However, statements indicating that an
action will not occur because of circumstances be-
yond the writer?s control were considered to be fac-
tual statements and not speech acts (e.g., ?I cannot
do an ultrasound because my machine is broken.?).
Directives: A Directive speech act occurs when
1Searle defines Declarative speech acts as statements that
bring about a change in status or condition to an object by virtue
of the statement itself. For example, a statement declaring war
or a statement that someone is fired.
the speaker expects the listener to do something as
a response. For example, the speaker may ask a
question, make a request, or issue an invitation. Di-
rective speech acts are common in message board
posts, especially in the initial post of each thread
when the writer explicitly requests help or advice re-
garding a specific topic. Many Directive sentences
are posed as questions, so they are easy to identify
by the presence of a question mark. However, the
language in message board forums is informal and
often ungrammatical, so many Directives are posed
as a question but do not end in a question mark (e.g.,
?What do you think.?). Furthermore, many Direc-
tive speech acts are not stated as a question but as
a request for assistance. For example, a doctor may
write ?I need your opinion on what drug to give this
patient.? Finally, some sentences that end in ques-
tion marks are rhetorical in nature and do not repre-
sent a Directive speech act, such as ?Can you believe
that??.
Expressives: An Expressive speech act occurs in
conversation when a speaker expresses his or her
psychological state to the listener. Typical cases are
when the speaker thanks, apologizes, or welcomes
the listener. Expressive speech acts are common in
message boards because writers often greet readers
at the beginning of a post (?Hi everyone!?) or ex-
press gratitude for help from the readers (?I really
appreciate the suggestions.?). We also found Ex-
pressive speech acts in a variety of other contexts,
such as apologies.
Representatives: According to Searle, a Rep-
resentative speech act commits the speaker to the
truth of an expressed proposition. It represents the
speaker?s belief of something that can be evaluated
to be true or false. These types of speech acts were
less common in our data set, but some cases did ex-
ist. In the veterinary domain, we considered sen-
tences to be a Representative speech act when a
doctor explicitly confirmed a diagnosis or expressed
their suspicion or hypothesis about the presence (or
absence) of a disease or symptom. For example, if a
doctor writes that ?I suspect the patient has pancre-
atitis.? then this represents the doctor?s own propo-
sition/belief about what the disease might be.
Many sentences in our data set are stated as fact
but could be reasonably inferred to be speech acts.
For example, suppose a doctor writes ?The cat has
750
pancreatitis.?. It would be reasonable to infer that
the doctor writing the post diagnosed the cat with
pancreatitis. And in many cases, that is true. How-
ever, we saw many posts where that inference would
have been wrong. For example, the following sen-
tence might say ?The cat was diagnosed by a pre-
vious vet but brought to me due to new complica-
tions? or ?The cat was diagnosed with it 8 years
ago as a kitten in the animal shelter?. Consequently,
we were very conservative in labelling sentences as
Representative speech acts. Any sentence presented
as fact was not considered to be a speech act. A sen-
tence was only labelled as a Representative speech
act if the writer explicitly expressed his belief.
3.2 Features for Speech Act Classification
To create speech act classifiers, we designed a vari-
ety of lexical, syntactic, and semantic features. We
tried to capture linguistic properties associated with
speech act expressions as well as discourse prop-
erties associated with individual sentences and the
message board post as a whole. We also incorpo-
rated speech act word lists that were acquired from
external resources, and used two types of seman-
tic features to represent semantic entities associated
with the veterinary domain. Except for the semantic
features, all of our features are domain-independent
so should be able to recognize speech act sentences
across different domains. We experimented with
domain-specific semantic features to test our hy-
pothesis that Commissive speech acts can be asso-
ciated with domain-specific semantic entities.
For the purposes of analysis, we partition the fea-
ture set into three groups: Lexical and Syntactic
(LexSyn) Features, Speech Act Clue Features, and
Semantic Features. Unless otherwise noted, all of
the features had binary values indicating the pres-
ence or absence of that feature.
3.2.1 Lexical and Syntactic Features
We designed a variety of features to capture lexical
and syntactic properties of words and sentences. We
described the feature set below, with the features cat-
egorized based on the type of information that they
capture.
Unigrams: We created bag-of-word features rep-
resenting each unigram in the training set. Numbers
were replaced with a special # token.
Personal Pronouns: We defined three features to
look for the presence of a 1st person pronoun, 2nd
person pronoun, and 3rd person pronoun. We in-
cluded the subjective, objective, and possessive form
of each pronoun (e.g., he, him, and his).
Tense: Speech acts such as Commissives can be
related to tense. We created three features to iden-
tify verb phrases that occur in the past, present, or
future tense. To recognize tense, we followed the
rules defined by Allen (1995).
Tense + Person: We created four features that re-
quire the presence of a first person subjective pro-
noun (I, we) within a two word window on the left of
a verb phrase matching one of four tense representa-
tions: past, present, future, and present progressive
(a subset of the more general present tense represen-
tation).
Modals: One feature indicates whether the sen-
tence contains a modal (may, must, shall, will,
might, should, would, could).
Infinitive VP: One feature looks for an infinitive
verb phrase (?to? followed by a verb) that is preceded
by a first person pronoun (I, we) within a three word
window on the left. This feature tries to capture
common Commissive expressions (e.g., ?I definitely
plan to do the test tomorrow.?).
Plan Phrases: Commissives are often expressed
as a plan, so we created a feature that recognizes
four types of plan expressions: ?I am going to?, ?I
am planning to?, ?I plan to?, and ?My plan is to?.
Sentence contains Early Punctuation: One fea-
ture checks for the following punctuation marks
within the first three tokens of the sentence: , : ! This
feature was designed to recognize greetings, such as:
?Hi,? , or ?Hiya everyone !?.
Sentence begins with Modal/Verb: One feature
checks if a sentence begins with a modal or verb.
The intuition is to capture interrogative and impera-
tive sentences, since they are likely to be Directives.
Sentence begins with WH Question: One fea-
ture checks if a sentence begins with a WH question
word (Who, When, Where, What, Which, What,
How).
Neighboring Question: One feature checks
whether the following sentence contains a question
mark ???. We observed that in message boards, Di-
rectives often occur in clusters.
751
Sentence Position: Four binary features repre-
sent the relative position of the sentence in the post.
One feature indicates whether it is the first sentence,
one feature indicates whether it is the last sentence,
one feature indicates whether it is the second to last
sentence, and one feature indicates whether the sen-
tence occurs in the bottom 25% of the message. The
motivation for these features is that Expressives of-
ten occur at the beginning and end of the post, and
Directives tend to occur toward the end.
Number of Verbs: One feature represents the
number of verbs in the sentence using four possible
values: 0, 1, 2, >2. Some speech acts classes (e.g.,
Expressives) may occur with no verbs, and rarely
occur in long, complex sentences.
3.2.2 Speech Act Word Clues
We collected speech act word lists (mostly verbs)
from two external sources. In Searle?s original pa-
per (Searle, 1976), he listed words that he consid-
ered to be indicative of speech acts. We discarded
a few that we considered to be overly general, and
we added a few additional words. We also collected
a list of speech act verbs published in (Wierzbicka,
1987). The details for these speech act clue lists are
given below. Our system recognized all derivations
of these words.
Searle Keywords: We created one feature for
each speech act class. The Representative keywords
were: (hypothesize, insist, boast, complain, con-
clude, deduce, diagnose, and claim). We discarded 3
words from Searle?s list (suggest, call, believe) and
added 2 new words, assume and suspect. The Direc-
tive keywords were: (ask, order, command, request,
beg, plead, pray, entreat, invite, permit, advise,
dare, defy, challenge). We added the word please.
The Expressives keywords were: (thank, apolo-
gize, congratulate, condole, deplore, welcome). We
added the words appreciate and sorry. Searle did
not provide any hint on possible indicator words for
Commissives, so we manually defined five likely
Commissive keywords: (plan, commit, promise, to-
morrow, later).
Wierzbicka Verbs: We created one feature that
included 228 speech act verbs listed in the book
?English speech act verbs: a semantic dictionary?
(Wierzbicka, 1987)2.
3.2.3 Semantic Features
All of the previous features are domain-
independent and should be useful for identifying
speech acts sentences across many domains. How-
ever, we hypothesized that semantic entities may
correlate with speech acts within a particular do-
main. For example, consider medical domains. Rep-
resentative speech acts may involve diagnoses and
hypotheses regarding diseases and symptoms. Sim-
ilarly, Commissive speech acts may reveal a doc-
tor?s plan or intention regarding the administration
of drugs or tests. Thus, it may be beneficial for a
classifier to know whether a sentence contains cer-
tain semantic entities. We experimented with two
different sources of semantic information.
Semantic Lexicon: Basilisk (Thelen and Riloff,
2002) is a bootstrapping algorithm that has been
used to induce semantic lexicons for terrorist events
(Thelen and Riloff, 2002), biomedical concepts
(McIntosh, 2010), and subjective/objective nouns
for opinion analysis (Riloff et al, 2003). We
ran Basilisk over our collection of 15,383 veteri-
nary message board posts to create a semantic lex-
icon for veterinary medicine. As input, Basilisk
requires seed words for each semantic category.
To obtain seeds, we parsed the corpus using a
noun phrase chunker, sorted the head nouns by fre-
quency, and manually identified the 20 most fre-
quent nouns belonging to four semantic categories:
DISEASE/SYMPTOM, DRUG, TEST, and TREAT-
MENT.
However, the induced TREATMENT lexicon was
of relatively poor quality so we did not use it. The
DISEASE/SYMPTOM lexicon appeared to be of good
quality, but it did not improve the performance of
our speech act classifiers. We suspect that this is due
to the fact that diseases were not distinguised from
symptoms in our lexicon.3 Representative speech
acts are typically associated with disease diagnoses
2openlibrary.org/b/OL2413134M/English_
speech_act_verbs
3We induced a single lexicon for diseases and symptoms be-
cause it is difficult to draw a clear line between them seman-
tically. A veterinary consultant explained to us that the same
term (e.g., diabetes) may be considered a symptom in one con-
text if it is secondary to another condition (e.g., pancreatitis) but
a disease in a different context if it is the primary diagnosis.
752
and hypotheses, rather than individual symptoms.
In the end, we only used the DRUG and TEST se-
mantic lexicon in our classifiers. We used all 1000
terms in the DRUG lexicon, but only used the top
200 TEST words because the quality of the lexicon
seemed questionable after that point.
Semantic Tags: We also used bootstrapped con-
textual semantic taggers (Huang and Riloff, 2010)
that had been previously trained for the domain of
veterinary medicine. These taggers assign seman-
tic class labels to noun phrase instances based on
the surrounding context in a sentence. The tag-
gers were trained on 4,629 veterinary message board
posts using 10 seed words for each semantic cate-
gory (see (Huang and Riloff, 2010) for details). To
ensure good precision, only tags that have a confi-
dence value ? 1.0 were used. Our speech act classi-
fiers used the tags associated with two semantic cat-
egories: DRUG and TEST.
3.3 Classification
To create our classifiers, we used the Weka (Hall et
al., 2009) machine learning toolkit. We used Sup-
port Vector Machines (SVMs) with a polynomial
kernel and the default settings supplied by Weka.
Because a sentence can include multiple speech acts,
we created a set of binary classifiers, one for each of
the four speech act classes. All four classifiers were
applied to each sentence, so a sentence could be as-
signed multiple speech act classes.
4 Evaluation
4.1 Data Set
Our data set consists of message board posts from
the Veterinary Information Network (VIN), which is
a web site (www.vin.com) for professionals in vet-
erinary medicine. Among other things, VIN hosts
message board forums where veterinarians and other
veterinary professionals can discuss issues and pose
questions to each other. Over half of the small an-
imal veterinarians in the U.S. and Canada use the
VIN web site.
We obtained 15,383 VIN message board threads
representing three topics: cardiology, endocrinol-
ogy, and feline internal medicine. We did basic
cleaning, removing html tags and tokenizing num-
bers. We then applied the Stanford part-of-speech
tagger (Toutanova et al, 2003) to each sentence to
obtain part-of-speech tags for the words. For our ex-
periments, we randomly selected 150 message board
threads from this collection. Since the goal of our
work was to study speech acts in sentences, and not
the conversational dialogue between different writ-
ers, we used only the initial post of each thread.
These 150 message board posts contained a total of
1,956 sentences, with an average of 13.04 sentences
per post. In the next section, we explain how we
manually annotated each sentence in our data set to
create gold standard speech act labels.
4.2 Gold Standard Annotations
To create training and evaluation data for our re-
search, we asked two human annotators to manually
label sentences in our message board posts. Iden-
tifying speech acts is not always obvious, even to
people, so we gave them detailed annotation guide-
lines describing the four speech act classes discussed
in Section 3.1. Then we gave them the same set of
50 message board posts from our collection to an-
notate independently. Each annotator was told to
assign one or more speech act classes to each sen-
tence (COM, DIR, EXP, REP), or to label the sen-
tence as having no speech acts (NONE). The vast
majority of sentences had either no speech acts or
at most one speech act, but a small number of sen-
tences contained multiple types of speech acts.
We measured the inter-annotator agreement of the
two human judges using the kappa (?) score (Car-
letta, 1996). However, kappa agreement scores are
only applicable to labelling schemes where each in-
stance receives a single label. Therefore we com-
puted kappa agreement in two different ways to look
at the results from two different perspectives. In the
first scheme, we discarded the small number of sen-
tences that had multiple speech act labels and com-
puted kappa on the rest.4 This produced a kappa
score of .95, suggesting extremely high agreement.
However, over 70% of the sentences in our data set
have no speech act at all, so NONE was by far the
most common label. Consequently, this agreement
score does not necessarily reflect how consistently
the judges agreed on the four speech act classes.
4Of the 594 sentences in these 50 posts, only 22 sentences
contained multiple speech act classes.
753
In the second scheme, we computed kappa for
each speech act category independently. For each
category C, the judges were considered to be in
agreement if both of them assigned category C to
the sentence or if neither of the judges assigned cat-
egory C to the sentence. Table 1 shows the ? agree-
ment scores using this approach.
Speech Act Kappa (?) score
Expressive .97
Directive .94
Commissive .81
Representative .77
Table 1: Inter-annotator (?) agreement
Inter-annotator agreement was very high for both
the Expressive and Directive classes. Agreement
was lower for the Commissive and Representative
classes, but still relatively good so we felt comfort-
able that we had high-quality annotations.
To create our final data set, the two judges adjudi-
cated their disagreements on this set of 50 posts. We
then asked each annotator to label an additional (dif-
ferent) set of 50 posts each. All together, this gave
us a gold standard data set consisting of 150 anno-
tated message board posts. Table 2 shows the distri-
bution of speech act labels in our data set. 71% of
the sentences did not include any speech acts. These
were usually expository sentences containing factual
information. 29% of the sentences included one or
more speech acts, so nearly 13 of the sentences wereconversational in nature. Directive and Expressive
speech acts are by far the most common, with nearly
26% of all sentences containing one of these speech
acts. Commissive and Representative speech acts
are less common, each occurring in less than 3% of
the sentences.5
4.3 Experimental Results
4.3.1 Speech Act Filtering
For our first experiment, we created a speech act
filtering classifier to distinguish sentences that con-
tain one or more speech acts from sentences that do
not contain any speech acts. Sentences labelled as
5These numbers do not add up to 100% because some sen-
tences contain multiple speech acts.
Speech Act # sentences distribution
None 1397 71.42%
Directive 311 15.90%
Expressive 194 9.92%
Representative 57 2.91%
Commissive 51 2.61%
Table 2: Speech act class distribution in our data set.
having one or more speech acts were positive in-
stances, and sentences labelled as NONE were neg-
ative instances. Speech act filtering could be useful
for many applications, such as information extrac-
tion systems that only seek to extract facts. For ex-
ample, information may be posed as a question (in
a Directive) rather than a fact, information may be
mentioned as part of a future plan (in a Commis-
sive) that has not actually happened yet, or informa-
tion may be stated as a hypothesis or suspicion (in a
Representative) rather than as a fact.
We performed 10-fold cross validation on our set
of 150 annotated message board posts. Initially, we
used all of the features defined in Section 3.2. How-
ever, during the course of our research we discov-
ered that only a small subset of the lexical and syn-
tactic features seemed to be useful, and that remov-
ing the unnecessary features improved performance.
So we created a subset of minimal lexsyn features,
which will be described in Section 4.3.2. For speech
act filtering, we used the minimal lexsyn features
plus the speech act clues and semantic features.6
Class P R F
Speech Act .86 .83 .84
No Speech Act .93 .95 .94
Table 3: Precision, Recall, F-measure for speech act fil-
tering.
Table 3 shows the performance for speech act
filtering with respect to Precision (P), Recall (R),
and F-measure score (F).7 The classifier performed
well, recognizing 83% of the speech act sentences
with 86% precision, and 95% of the expository (no
6This is the same feature set used to produce the results for
row E of Table 4.
7We computed an F1 score with equal weighting of preci-
sion and recall.
754
Commissives Directives Expressives Representatives
Features P R F P R F P R F P R F
Baselines
Com baseline .45 .08 .14 - - - - - - - - -
Dir baseline - - - .97 .73 .83 - - - - - -
Exp baseline 1 - - - - - - .58 .18 .28 - - -
Exp baseline 2 - - - - - - .97 .86 .91 - - -
Rep baseline - - - - - - - - - 1.0 .05 .10
Classifiers
U Unigram .45 .20 .27 .87 .84 .85 .97 .88 .92 .32 .12 .18
A U+all lexsyn .52 .33 .40 .87 .84 .86 .98 .88 .92 .30 .14 .19
B U+minimal lexsyn .59 .33 .42 .87 .85 .86 .98 .88 .92 .32 .14 .20
C B+speechActClues .57 .31 .41 .86 .84 .85 .97 .91 .94 .33 .16 .21
D C+semTest .64 .35 .46 .87 .84 .85 .97 .91 .94 .33 .16 .21
E D+semDrug .63 .39 .48 .86 .84 .85 .97 .91 .94 .32 .16 .21
Table 4: Precision, Recall, F-measure for four speech act classes. The highest F score for each category appears in
boldface.
speech act) sentences with 93% precision.
4.3.2 Speech Act Categorization
BASELINES
Our next set of experiments focused on labelling
sentences with the four specific speech act classes:
Commissive, Directive. Expressive, and Represen-
tative. To assess the difficulty of identifying each
speech act category, we created several simple base-
lines using our intuitions about each category.
For Commissives, we created a heuristic to cap-
ture the most obvious cases of future tense (because
Commissive speech acts represent a writer?s com-
mitment toward a future course of action). For ex-
ample, the presence of the phrases ?I will? and ?I
shall? were hypothesized by Cohen et al (2004) to
be useful bigram clues for Commissives. This base-
line looks for future tense verb phrases with a 1st
person pronoun within one or two words preceding
the verb phrase. The Com baseline row of Table 4
shows the results for this heuristic, which obtained
8% recall with 45% precision. The heuristic applied
to only 9 sentences in our test set, 4 of which con-
tained a Commissive speech act.
Directive speech acts are often questions, so we
created a baseline system that labels all sentences
containing a question mark as a Directive. The Dir
baseline row of Table 4 shows that 97% of sentences
with a question mark were indeed Directives.8 But
only 73% of the Directive sentences contained a
question mark. The remaining 27% of Directives
did not contain a question mark and generally fell
into two categories. Some sentences asked a ques-
tion but the writer ended the sentence with a period
(e.g., ?Has anyone seen this before.?). And many di-
rectives were expressed as requests rather than ques-
tions (e.g., ?Let me know if anyone has a sugges-
tion.?).
For Expressives, we implemented two baselines.
Exp baseline 1 simply looks for an exclamation
mark, but this heuristic did not work well (18% re-
call with 58% precision) because exclamation marks
were often used for general emphasis (e.g., ?The
owner is frustrated with cleaning up urine!?). Exp
baseline 2 looks for the presence of four common
expressive words (appreciate, hi, hello, thank), in-
cluding morphological variations of appreciate and
thank. This baseline produced very good results,
86% recall with 97% precision. Obviously a small
set of common expressions account for most of the
Expressive speech acts in our corpus. However, the
word ?hi? did produce some false hits because it was
used as a shorthand for ?high?, usually when report-
ing test results (e.g., ?hi calcium?).
8235 sentences contained a question mark, and 227 of them
were Directives.
755
Finally, as a baseline for the Representative class
we simply looked for the words diagnose(d) and sus-
pect(ed). The Rep baseline row of Table 4 shows
that this heuristic was 100% accurate, but only pro-
duced 5% recall (matching 3 of the 57 Representa-
tive sentences in our test set).
CLASSIFIER RESULTS
The bottom portion of Table 4 shows the results
for our classifiers. As we explained in Section 3.3,
we created one classifier for each speech act cate-
gory, and all four classifiers were applied to each
sentence. So a sentence could receive anywhere
from 0-4 speech act labels indicating how many dif-
ferent types of speech acts appeared in the sentence.
We trained and evaluated each classifier using 10-
fold cross-validation on our gold standard data set.
The Unigram (U) row shows the performance of
classifiers that use only unigram features. For Di-
rectives, we see a 2% F-score improvement over the
baseline, which reflects a recall gain of 11% but
a corresponding precision loss of 10%. The uni-
grams are clearly helpful in identifying many Direc-
tive sentences that do not end in a question mark,
but at some cost to accuracy. For Expressives, the
unigram classifier achieves an F score of 92%, iden-
tifying slightly more Expressive sentences than the
baseline with the same level of precision. For Com-
missives and Representatives, the unigram classi-
fiers performed susbtantially better than their corre-
sponding baseline systems, but performance is still
relatively weak.
Row A (U+ all lexsyn) in Table 4 shows the re-
sults using unigram features plus all of the lexical
and syntactic features described in Section 3.2.1.
The lexical and syntactic features dramatically im-
prove performance on Commissives, increasing F
score from 27% to 40%, and they produce a 2% re-
call gain for Representatives but with a correspond-
ing loss of precision.
However, we observed that only a few of the lex-
ical and syntactic features had much impact on per-
formance. We experimented with different subsets
of the features and obtained even better performance
when using just 10 of them, which we will refer to as
the minimal lexsyn features. The minimal lexsyn fea-
ture set consists of the 4 Tense+Person features, the
Early Punctuation feature, the Sentence begins with
Modal/Verb feature, and the 4 Sentence Position fea-
tures. Row B shows the results using unigram fea-
tures plus only these minimal lexsyn features. Preci-
sion improves for Commissives by an additional 7%
and Representatives by 2% when using only these
lexical and syntactic features. Consequently, we use
the minimal lexsyn features for the rest of our exper-
iments.
Row C shows the results of adding the speech act
clue words (see Section 3.2.2) to the feature set used
in Row B. The speech act clue words produced an
additional recall gain of 3% for Expressives and 2%
for Representatives, although performance on Com-
missives dropped 2% in both recall and precision.
Rows D and E show the results of adding the se-
mantic features. We added one semantic category
at a time to measure the impact of them separately.
Row D adds two semantic features for the TEST cat-
egory, one from the Basilisk lexicon and one from
the semantic tagger. The TEST semantic features
produced an F-score gain of 5% for Commissives,
improving recall by 4% and precision by 7%. Row
E adds two semantic features for the DRUG category.
The DRUG features produced an additional F-score
gain of 2% for Commissives, improving recall by
4% with a slight drop in precision.
4.4 Analysis
Together, the TEST and DRUG semantic features dra-
matically improved the classifier?s ability to recog-
nize Commissive speech acts, increasing its F score
from 41% ? 48%. This result demonstrates that
in the domain of veterinary medicine, some types
of semantic entities are associated with speech acts.
Our intuition behind this result is that commitments
are usually related to future actions. In veterinary
medicine, TESTS and DRUGS are associated with ac-
tions performed by doctors. Doctors help their pa-
tients by prescribing or administering drugs and by
conducting tests. So these semantic entities may
serve as a proxy to implicitly represent actions that
the doctor has done or may do. In future work, ex-
plicitly recognizing actions and events many be a
worthwhile avenue to further improve results.
We achieved good success at identifying both Di-
rectives and Expressives, although simple heuristics
also perform well on these categories. We showed
that training a Directive classifier can help to iden-
756
tify Directive sentences that do not end with a ques-
tion mark, although at the cost of some precision.
The Commissive speech act class benefitted the
most from the rich feature set. Unigrams are clearly
not sufficient to identify Commissive sentences.
Many different types of clues seem to be important
for recognizing these sentences. The improvements
obtained from adding semantic features also sug-
gests that domain-specific semantics can be useful
for recognizing some speech acts. However, there is
still ample room for improvement, illustrating that
speech act classification is a challenging problem.
Representative speech acts were by far the most
difficult to recognize. We believe that there are
several reasons for their low performance. First,
Representatives were sparse in the data set, occur-
ring in only 2.91% of the sentences. Consequently,
the classifier had relatively few positive training
instances. Second, Representatives had the low-
est inter-annotator agreement, indicating that human
judges had difficulty recognizing these speech acts
too. The judges often disagreed about whether a
hypothesis or suspicion was the writer?s own belief
or whether it was stated as a fact reflecting general
medical knowledge. The message board text genre
is especially challenging in this regard because the
writer is often presumed to be expressing his/her be-
liefs even when the writer does not explicitly say so.
Finally, our semantic features could not distinguish
between diseases and symptoms. Access to a re-
source that can reliably identify disease terms could
potentially improve performance in this domain.
5 Conclusions
Our goal was to identify speech act sentences in
message board posts and to classify the sentences
with respect to four categories in Searle?s (1976)
speech act taxonomy. We achieved good results for
speech act filtering and the identification of Direc-
tive and Expressive speech act sentences. We found
that Representative and Commissive speech acts are
much more difficult to identify, although the per-
formance of our Commissive classifier substantially
improved with the addition of lexical, syntactic, and
semantic features. Except for the semantic class
information, our feature set is domain-independent
and could be used to recognize speech act sentences
in message boards for any domain. Furthermore, our
features only rely on part-of-speech tags and do not
require parsing, which is of practical importance for
text genres such as message boards that are littered
with ungrammatical text, typos, and shorthand nota-
tions.
In future work, we believe that segmenting sen-
tences into clauses may help to train classifiers more
precisely. Ultimately, we would like to identify
the speech act expressions themselves because some
sentences contain speech acts as well as factual in-
formation. Extracting the speech act expressions
and clauses from message boards and similar text
genres could provide better tracking of questions
and answers in web forums and be used for sum-
marization.
6 Acknowledgments
We gratefully acknowledge that this research was
supported in part by the National Science Founda-
tion under grant IIS-1018314. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of the U.S. government.
References
James Allen. 1995. Natural language understanding
(2nd ed.). Benjamin-Cummings Publishing Co., Inc.,
Redwood City, CA, USA.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22:249?254, June.
Vitor R. Carvalho and William W. Cohen. 2005. On the
collective classification of email ?speech acts?. In SI-
GIR ?05: Proceedings of the 28th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 345?352, New York,
NY, USA. ACM Press.
Vitor R. Carvalho and William W. Cohen. 2006. Improv-
ing ?email speech acts? analysis via n-gram selection.
In Proceedings of the HLT-NAACL 2006 Workshop on
Analyzing Conversations in Text and Speech, ACTS
?09, pages 35?41, Stroudsburg, PA, USA. Association
for Computational Linguistics.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In EMNLP, pages 309?316. ACL.
Jade Goldstein and Roberta Evans Sabin. 2006. Using
speech acts to categorize email and identify email gen-
757
res. In Proceedings of the 39th Annual Hawaii Inter-
national Conference on System Sciences - Volume 03,
pages 50.2?, Washington, DC, USA. IEEE Computer
Society.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18, November.
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
?10, pages 275?285, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Semi-supervised speech act recognition in
emails and forums. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 3 - Volume 3, EMNLP ?09, pages
1250?1259, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Andrew Lampert, Robert Dale, and Cecile Paris. 2006.
Classifying speech acts using verbal response modes.
In Proceedings of the 2006 Australasian Language
Technology Workshop (ALTW2006), pages 34?41.
Sydney Australia : ALTA.
Tara McIntosh. 2010. Unsupervised discovery of neg-
ative categories in lexicon bootstrapping. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 356?365, Stroudsburg, PA, USA. Association
for Computational Linguistics.
John Mildinhall and Jan Noyes. 2008. Toward a stochas-
tic speech act model of email behavior. In CEAS.
Jacqueline Nastri, Jorge Pena, and Jeffrey T. Hancock.
2006. The construction of away messages: A speech
act analysis. J. Computer-Mediated Communication,
pages 1025?1045.
Sujith Ravi and Jihie Kim. 2007. Profiling student inter-
actions in threaded discussions with speech act classi-
fiers. In Proceeding of the 2007 conference on Arti-
ficial Intelligence in Education: Building Technology
Rich Learning Contexts That Work, pages 357?364,
Amsterdam, The Netherlands, The Netherlands. IOS
Press.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the seventh confer-
ence on Natural language learning at HLT-NAACL
2003 - Volume 4, CONLL ?03, pages 25?32, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
John R. Searle. 1976. A classification of illocutionary
acts. Language in Society, 5(1):pp. 1?23.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing - Volume 10, EMNLP ?02, pages 214?221,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Douglas P. Twitchell and Jay F. Nunamaker Jr. 2004.
Speech act profiling: a probabilistic method for ana-
lyzing persistent conversations and their participants.
In System Sciences, 2004. Proceedings of the 37th An-
nual Hawaii International Conference on, pages 1?10,
January.
Douglas P. Twitchell, Mark Adkins, Jay F. Nunamaker
Jr., and Judee K. Burgoon. 2004. Using speech act
theory to model conversations for automated classi-
fication and retrieval. In Proceedings of the Inter-
national Working Conference Language Action Per-
spective Communication Modelling (LAP 2004), pages
121?130.
A. Wierzbicka. 1987. English speech act verbs: a se-
mantic dictionary. Academic Press, Sydney, Orlando.
758
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 704?714,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Sarcasm as Contrast between a Positive Sentiment and Negative Situation
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva,
Nathan Gilbert, Ruihong Huang
School Of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,asheq,alnds,ngilbert,huangrh}@cs.utah.edu, prafulla.surve@gmail.com
Abstract
A common form of sarcasm on Twitter con-
sists of a positive sentiment contrasted with a
negative situation. For example, many sarcas-
tic tweets include a positive sentiment, such as
?love? or ?enjoy?, followed by an expression
that describes an undesirable activity or state
(e.g., ?taking exams? or ?being ignored?). We
have developed a sarcasm recognizer to iden-
tify this type of sarcasm in tweets. We present
a novel bootstrapping algorithm that automati-
cally learns lists of positive sentiment phrases
and negative situation phrases from sarcastic
tweets. We show that identifying contrast-
ing contexts using the phrases learned through
bootstrapping yields improved recall for sar-
casm recognition.
1 Introduction
Sarcasm is generally characterized as ironic or satir-
ical wit that is intended to insult, mock, or amuse.
Sarcasm can be manifested in many different ways,
but recognizing sarcasm is important for natural lan-
guage processing to avoid misinterpreting sarcastic
statements as literal. For example, sentiment anal-
ysis can be easily misled by the presence of words
that have a strong polarity but are used sarcastically,
which means that the opposite polarity was intended.
Consider the following tweet on Twitter, which in-
cludes the words ?yay? and ?thrilled? but actually
expresses a negative sentiment: ?yay! it?s a holi-
day weekend and i?m on call for work! couldn?t be
more thrilled! #sarcasm.? In this case, the hashtag
#sarcasm reveals the intended sarcasm, but we don?t
always have the benefit of an explicit sarcasm label.
In the realm of Twitter, we observed that many
sarcastic tweets have a common structure that
creates a positive/negative contrast between a senti-
ment and a situation. Specifically, sarcastic tweets
often express a positive sentiment in reference to a
negative activity or state. For example, consider the
tweets below, where the positive sentiment terms
are underlined and the negative activity/state terms
are italicized.
(a) Oh how I love being ignored. #sarcasm
(b) Thoroughly enjoyed shoveling the driveway
today! :) #sarcasm
(c) Absolutely adore it when my bus is late
#sarcasm
(d) I?m so pleased mom woke me up with
vacuuming my room this morning. :) #sarcasm
The sarcasm in these tweets arises from the jux-
taposition of a positive sentiment word (e.g., love,
enjoyed, adore, pleased) with a negative activity or
state (e.g., being ignored, bus is late, shoveling, and
being woken up).
The goal of our research is to identify sarcasm
that arises from the contrast between a positive sen-
timent referring to a negative situation. A key chal-
lenge is to automatically recognize the stereotypi-
cally negative ?situations?, which are activities and
states that most people consider to be unenjoyable or
undesirable. For example, stereotypically unenjoy-
able activities include going to the dentist, taking an
exam, and having to work on holidays. Stereotypi-
cally undesirable states include being ignored, hav-
ing no friends, and feeling sick. People recognize
704
these situations as being negative through cultural
norms and stereotypes, so they are rarely accompa-
nied by an explicit negative sentiment. For example,
?I feel sick? is universally understood to be a nega-
tive situation, even without an explicit expression of
negative sentiment. Consequently, we must learn to
recognize phrases that correspond to stereotypically
negative situations.
We present a bootstrapping algorithm that auto-
matically learns phrases corresponding to positive
sentiments and phrases corresponding to negative
situations. We use tweets that contain a sarcasm
hashtag as positive instances for the learning pro-
cess. The bootstrapping algorithm begins with a sin-
gle seed word, ?love?, and a large set of sarcastic
tweets. First, we learn negative situation phrases
that follow a positive sentiment (initially, the seed
word ?love?). Second, we learn positive sentiment
phrases that occur near a negative situation phrase.
The bootstrapping process iterates, alternately learn-
ing new negative situations and new positive sen-
timent phrases. Finally, we use the learned lists
of sentiment and situation phrases to recognize sar-
casm in new tweets by identifying contexts that con-
tain a positive sentiment in close proximity to a neg-
ative situation phrase.
2 Related Work
Researchers have investigated the use of lexical
and syntactic features to recognize sarcasm in text.
Kreuz and Caucci (2007) studied the role that dif-
ferent lexical factors play, such as interjections (e.g.,
?gee? or ?gosh?) and punctuation symbols (e.g., ???)
in recognizing sarcasm in narratives. Lukin and
Walker (2013) explored the potential of a bootstrap-
ping method for sarcasm classification in social di-
alogue to learn lexical N-gram cues associated with
sarcasm (e.g., ?oh really?, ?I get it?, ?no way?, etc.)
as well as lexico-syntactic patterns.
In opinionated user posts, Carvalho et al (2009)
found oral or gestural expressions, represented us-
ing punctuation and other keyboard characters, to
be more predictive of irony1 in contrast to features
representing structured linguistic knowledge in Por-
1They adopted the term ?irony? instead of ?sarcasm? to re-
fer to the case when a word or expression with prior positive
polarity is figuratively used to express a negative opinion.
tuguese. Filatova (2012) presented a detailed de-
scription of sarcasm corpus creation with sarcasm
annotations of Amazon product reviews. Their an-
notations capture sarcasm both at the document level
and the text utterance level. Tsur et al (2010) pre-
sented a semi-supervised learning framework that
exploits syntactic and pattern based features in sar-
castic sentences of Amazon product reviews. They
observed correlated sentiment words such as ?yay!?
or ?great!? often occurring in their most useful pat-
terns.
Davidov et al (2010) used sarcastic tweets and
sarcastic Amazon product reviews to train a sarcasm
classifier with syntactic and pattern-based features.
They examined whether tweets with a sarcasm hash-
tag are reliable enough indicators of sarcasm to be
used as a gold standard for evaluation, but found that
sarcasm hashtags are noisy and possibly biased to-
wards the hardest form of sarcasm (where even hu-
mans have difficulty). Gonza?lez-Iba?n?ez et al (2011)
explored the usefulness of lexical and pragmatic fea-
tures for sarcasm detection in tweets. They used sar-
casm hashtags as gold labels. They found positive
and negative emotions in tweets, determined through
fixed word dictionaries, to have a strong correlation
with sarcasm. Liebrecht et al (2013) explored N-
gram features from 1 to 3-grams to build a classifier
to recognize sarcasm in Dutch tweets. They made an
interesting observation from their most effective N-
gram features that people tend to be more sarcastic
towards specific topics such as school, homework,
weather, returning from vacation, public transport,
the church, the dentist, etc. This observation has
some overlap with our observation that stereotypi-
cally negative situations often occur in sarcasm.
The cues for recognizing sarcasm may come from
a variety of sources. There exists a line of work
that tries to identify facial and vocal cues in speech
(e.g., (Gina M. Caucci, 2012; Rankin et al, 2009)).
Cheang and Pell (2009) and Cheang and Pell (2008)
performed studies to identify acoustic cues in sarcas-
tic utterances by analyzing speech features such as
speech rate, mean amplitude, amplitude range, etc.
Tepperman et al (2006) worked on sarcasm recog-
nition in spoken dialogue using prosodic and spec-
tral cues (e.g., average pitch, pitch slope, etc.) as
well as contextual cues (e.g., laughter or response to
questions) as features.
705
While some of the previous work has identi-
fied specific expressions that correlate with sarcasm,
none has tried to identify contrast between positive
sentiments and negative situations. The novel con-
tributions of our work include explicitly recogniz-
ing contexts that contrast a positive sentiment with a
negative activity or state, as well as a bootstrapped
learning framework to automatically acquire posi-
tive sentiment and negative situation phrases.
3 Bootstrapped Learning of Positive
Sentiments and Negative Situations
Sarcasm is often defined in terms of contrast or ?say-
ing the opposite of what you mean?. Our work fo-
cuses on one specific type of contrast that is common
on Twitter: the expression of a positive sentiment
(e.g., ?love? or ?enjoy?) in reference to a negative
activity or state (e.g., ?taking an exam? or ?being ig-
nored?). Our goal is to create a sarcasm classifier for
tweets that explicitly recognizes contexts that con-
tain a positive sentiment contrasted with a negative
situation.
Our approach learns rich phrasal lexicons of pos-
itive sentiments and negative situations using only
the seed word ?love? and a collection of sarcastic
tweets as input. A key factor that makes the algo-
rithm work is the presumption that if you find a pos-
itive sentiment or a negative situation in a sarcastic
tweet, then you have found the source of the sar-
casm. We further assume that the sarcasm probably
arises from positive/negative contrast and we exploit
syntactic structure to extract phrases that are likely
to have contrasting polarity. Another key factor is
that we focus specifically on tweets. The short na-
ture of tweets limits the search space for the source
of the sarcasm. The brevity of tweets also probably
contributes to the prevalence of this relatively com-
pact form of sarcasm.
3.1 Overview of the Learning Process
Our bootstrapping algorithm operates on the as-
sumption that many sarcastic tweets contain both a
positive sentiment and a negative situation in close
proximity, which is the source of the sarcasm.2 Al-
though sentiments and situations can be expressed
2Sarcasm can arise from a negative sentiment contrasted
with a positive situation too, but our observation is that this is
much less common, at least on Twitter.
Positive
Sentiment
Phrases
Negative
Situation
Phrases
Seed Word
"love"
Sarcastic Tweets
1 2
34
Figure 1: Bootstrapped Learning of Positive Sentiment
and Negative Situation Phrases
in numerous ways, we focus on positive sentiments
that are expressed as a verb phrase or as a predicative
expression (predicate adjective or predicate nomi-
nal), and negative activities or states that can be a
complement to a verb phrase. Ideally, we would
like to parse the text and extract verb complement
phrase structures, but tweets are often informally
written and ungrammatical. Therefore we try to rec-
ognize these syntactic structures heuristically using
only part-of-speech tags and proximity.
The learning process relies on an assumption that
a positive sentiment verb phrase usually appears to
the left of a negative situation phrase and in close
proximity (usually, but not always, adjacent). Picto-
rially, we assume that many sarcastic tweets contain
this structure:
[+ VERB PHRASE] [? SITUATION PHRASE]
This structural assumption drives our bootstrap-
ping algorithm, which is illustrated in Figure 1.
The bootstrapping process begins with a single seed
word, ?love?, which seems to be the most common
positive sentiment term in sarcastic tweets. Given
a sarcastic tweet containing the word ?love?, our
structural assumption infers that ?love? is probably
followed by an expression that refers to a negative
situation. So we harvest the n-grams that follow the
word ?love? as negative situation candidates. We se-
lect the best candidates using a scoring metric, and
add them to a list of negative situation phrases.
Next, we exploit the structural assumption in the
opposite direction. Given a sarcastic tweet that con-
tains a negative situation phrase, we infer that the
negative situation phrase is preceded by a positive
sentiment. We harvest the n-grams that precede the
negative situation phrases as positive sentiment can-
didates, score and select the best candidates, and
706
add them to a list of positive sentiment phrases.
The bootstrapping process then iterates, alternately
learning more positive sentiment phrases and more
negative situation phrases.
We also observed that positive sentiments are fre-
quently expressed as predicative phrases (i.e., pred-
icate adjectives and predicate nominals). For ex-
ample: ?I?m taking calculus. It is awesome. #sar-
casm?. Wiegand et al (2013) offered a related ob-
servation that adjectives occurring in predicate ad-
jective constructions are more likely to convey sub-
jectivity than adjectives occurring in non-predicative
structures. Therefore we also include a step in
the learning process to harvest predicative phrases
that occur in close proximity to a negative situation
phrase. In the following sections, we explain each
step of the bootstrapping process in more detail.
3.2 Bootstrapping Data
For the learning process, we used Twitter?s stream-
ing API to obtain a large set of tweets. We col-
lected 35,000 tweets that contain the hashtag #sar-
casm or #sarcastic to use as positive instances of sar-
casm. We also collected 140,000 additional tweets
from Twitter?s random daily stream. We removed
the tweets that contain a sarcasm hashtag, and con-
sidered the rest to be negative instances of sarcasm.
Of course, there will be some sarcastic tweets that do
not have a sarcasm hashtag, so the negative instances
will contain some noise. But we expect that a very
small percentage of these tweets will be sarcastic, so
the noise should not be a major issue. There will also
be noise in the positive instances because a sarcasm
hashtag does not guarantee that there is sarcasm in
the body of the tweet (e.g., the sarcastic content may
be in a linked url, or in a prior tweet). But again, we
expect the amount of noise to be relatively small.
Our tweet collection therefore contains a total of
175,000 tweets: 20% are labeled as sarcastic and
80% are labeled as not sarcastic. We applied CMU?s
part-of-speech tagger designed for tweets (Owoputi
et al, 2013) to this data set.
3.3 Seeding
The bootstrapping process begins by initializing the
positive sentiment lexicon with one seed word: love.
We chose this seed because it seems to be the most
common positive sentiment word in sarcastic tweets.
3.4 Learning Negative Situation Phrases
The first stage of bootstrapping learns new phrases
that correspond to negative situations. The learning
process consists of two steps: (1) harvesting candi-
date phrases, and (2) scoring and selecting the best
candidates.
To collect candidate phrases for negative situa-
tions, we extract n-grams that follow a positive senti-
ment phrase in a sarcastic tweet. We extract every 1-
gram, 2-gram, and 3-gram that occurs immediately
after (on the right-hand side) of a positive sentiment
phrase. As an example, consider the tweet in Figure
2, where ?love? is the positive sentiment:
I love waiting forever for the doctor #sarcasm
Figure 2: Example Sarcastic Tweet
We extract three n-grams as candidate negative situ-
ation phrases:
waiting, waiting forever, waiting forever for
Next, we apply the part-of-speech (POS) tagger
and filter the candidate list based on POS patterns so
we only keep n-grams that have a desired syntactic
structure. For negative situation phrases, our goal
is to learn possible verb phrase (VP) complements
that are themselves verb phrases because they should
represent activities and states. So we require a can-
didate phrase to be either a unigram tagged as a verb
(V) or the phrase must match one of 7 POS-based
bigram patterns or 20 POS-based trigram patterns
that we created to try to approximate the recogni-
tion of verbal complement structures. The 7 POS bi-
gram patterns are: V+V, V+ADV, ADV+V, ?to?+V,
V+NOUN, V+PRO, V+ADJ. Note that we used
a POS tagger designed for Twitter, which has a
smaller set of POS tags than more traditional POS
taggers. For example there is just a single V tag
that covers all types of verbs. The V+V pattern will
therefore capture negative situation phrases that con-
sist of a present participle verb followed by a past
participle verb, such as ?being ignored? or ?getting
hit?.3 We also allow verb particles to match a V tag
in our patterns. The remaining bigram patterns cap-
ture verb phrases that include a verb and adverb, an
3In some cases it may be more appropriate to consider the
second verb to be an adjective, but in practice they were usually
tagged as verbs.
707
infinitive form (e.g., ?to clean?), a verb and noun
phrase (e.g., ?shoveling snow?), or a verb and ad-
jective (e.g., ?being alone?). We use some simple
heuristics to try to ensure that we are at the end of an
adjective or noun phrase (e.g., if the following word
is tagged as an adjective or noun, then we assume
we are not at the end).
The 20 POS trigram patterns are similar in nature
and are designed to capture seven general types of
verb phrases: verb and adverb mixtures, an infini-
tive VP that includes an adverb, a verb phrase fol-
lowed by a noun phrase, a verb phrase followed by a
prepositional phrase, a verb followed by an adjective
phrase, or an infinitive VP followed by an adjective,
noun, or pronoun.
Returning to Figure 2, only two of the n-grams
match our POS patterns, so we are left with two can-
didate phrases for negative situations:
waiting, waiting forever
Next, we score each negative situation candidate
by estimating the probability that a tweet is sarcastic
given that it contains the candidate phrase following
a positive sentiment phrase:
| follows(?candidate, +sentiment) & sarcastic |
| follows(?candidate, +sentiment) |
We compute the number of times that the negative
situation candidate immediately follows a positive
sentiment in sarcastic tweets divided by the number
of times that the candidate immediately follows a
positive sentiment in all tweets. We discard phrases
that have a frequency < 3 in the tweet collection
since they are too sparse.
Finally, we rank the candidate phrases based on
this probability, using their frequency as a secondary
key in case of ties. The top 20 phrases with a prob-
ability ? .80 are added to the negative situation
phrase list.4 When we add a phrase to the nega-
tive situation list, we immediately remove all other
candidates that are subsumed by the selected phrase.
For example, if we add the phrase ?waiting?, then
the phrase ?waiting forever? would be removed from
the candidate list because it is subsumed by ?wait-
ing?. This process reduces redundancy in the set of
4Fewer than 20 phrases will be learned if < 20 phrases pass
this threshold.
phrases that we add during each bootstrapping itera-
tion. The bootstrapping process stops when no more
candidate phrases pass the probability threshold.
3.5 Learning Positive Verb Phrases
The procedure for learning positive sentiment
phrases is analogous. First, we collect phrases that
potentially convey a positive sentiment by extract-
ing n-grams that precede a negative situation phrase
in a sarcastic tweet. To learn positive sentiment verb
phrases, we extract every 1-gram and 2-gram that
occurs immediately before (on the left-hand side of)
a negative situation phrase.
Next, we apply the POS tagger and filter the n-
grams using POS tag patterns so that we only keep
n-grams that have a desired syntactic structure. Here
our goal is to learn simple verb phrases (VPs) so we
only retain n-grams that contain at least one verb and
consist only of verbs and (optionally) adverbs. Fi-
nally, we score each candidate sentiment verb phrase
by estimating the probability that a tweet is sarcastic
given that it contains the candidate phrase preceding
a negative situation phrase:
| precedes(+candidateVP,?situation) & sarcastic |
| precedes(+candidateVP,?situation) |
3.6 Learning Positive Predicative Phrases
We also use the negative situation phrases to harvest
predicative expressions (predicate adjective or pred-
icate nominal structures) that occur nearby. Based
on the same assumption that sarcasm often arises
from the contrast between a positive sentiment and
a negative situation, we identify tweets that contain
a negative situation and a predicative expression in
close proximity. We then assume that the predicative
expression is likely to convey a positive sentiment.
To learn predicative expressions, we use 24 copu-
lar verbs from Wikipedia5 and their inflections. We
extract positive sentiment candidates by extracting
1-grams, 2-grams, and 3-grams that appear immedi-
ately after a copular verb and occur within 5 words
of the negative situation phrase, on either side. This
constraint only enforces proximity because predica-
tive expressions often appear in a separate clause or
sentence (e.g., ?It is just great that my iphone was
stolen? or ?My iphone was stolen. This is great.?)
5http://en.wikipedia.org/wiki/List of English copulae
708
We then apply POS patterns to identify n-grams
that correspond to predicate adjective and predicate
nominal phrases. For predicate adjectives, we re-
tain ADJ and ADV+ADJ n-grams. We use a few
heuristics to check that the adjective is not part of a
noun phrase (e.g., we check that the following word
is not a noun). For predicate nominals, we retain
ADV+ADJ+N, DET+ADJ+N and ADJ+N n-grams.
We excluded noun phrases consisting only of nouns
because they rarely seemed to represent a sentiment.
The sentiment in predicate nominals was usually
conveyed by the adjective. We discard all candidates
with frequency < 3 as being too sparse. Finally,
we score each remaining candidate by estimating the
probability that a tweet is sarcastic given that it con-
tains the predicative expression near (within 5 words
of) a negative situation phrase:
| near(+candidatePRED,?situation) & sarcastic |
| near(+candidatePRED,?situation) |
We found that the diversity of positive senti-
ment verb phrases and predicative expressions is
much lower than the diversity of negative situation
phrases. As a result, we sort the candidates by their
probability and conservatively add only the top 5
positive verb phrases and top 5 positive predicative
expressions in each bootstrapping iteration. Both
types of sentiment phrases must pass a probability
threshold of ? .70.
3.7 The Learned Phrase Lists
The bootstrapping process alternately learns pos-
itive sentiments and negative situations until no
more phrases can be learned. In our experiments,
we learned 26 positive sentiment verb phrases, 20
predicative expressions and 239 negative situation
phrases.
Table 1 shows the first 15 positive verb phrases,
the first 15 positive predicative expressions, and the
first 40 negative situation phrases learned by the
bootstrapping algorithm. Some of the negative sit-
uation phrases are not complete expressions, but it
is clear that they will often match negative activities
and states. For example, ?getting yelled? was gener-
ated from sarcastic comments such as ?I love getting
yelled at?, ?being home? occurred in tweets about
?being home alone?, and ?being told? is often be-
ing told what to do. Shorter phrases often outranked
longer phrases because they are more general, and
will therefore match more contexts. But an avenue
for future work is to learn linguistic expressions that
more precisely characterize specific negative situa-
tions.
Positive Verb Phrases (26): missed, loves,
enjoy, cant wait, excited, wanted, can?t wait,
get, appreciate, decided, loving, really like,
looooove, just keeps, loveee, ...
Positive Predicative Expressions (20): great,
so much fun, good, so happy, better, my
favorite thing, cool, funny, nice, always fun,
fun, awesome, the best feeling, amazing,
happy, ...
Negative Situations (239): being ignored, be-
ing sick, waiting, feeling, waking up early, be-
ing woken, fighting, staying, writing, being
home, cleaning, not getting, crying, sitting at
home, being stuck, starting, being told, be-
ing left, getting ignored, being treated, doing
homework, learning, getting up early, going to
bed, getting sick, riding, being ditched, get-
ting ditched, missing, not sleeping, not talking,
trying, falling, walking home, getting yelled,
being awake, being talked, taking care, doing
nothing, wasting, ...
Table 1: Examples of Learned Phrases
4 Evaluation
4.1 Data
For evaluation purposes, we created a gold stan-
dard data set of manually annotated tweets. Even
for people, it is not always easy to identify sarcasm
in tweets because sarcasm often depends on con-
versational context that spans more than a single
tweet. Extracting conversational threads from Twit-
ter, and analyzing conversational exchanges, has its
own challenges and is beyond the scope of this re-
search. We focus on identifying sarcasm that is self-
contained in one tweet and does not depend on prior
conversational context.
We defined annotation guidelines that instructed
human annotators to read isolated tweets and label
709
a tweet as sarcastic if it contains comments judged
to be sarcastic based solely on the content of that
tweet. Tweets that do not contain sarcasm, or where
potential sarcasm is unclear without seeing the prior
conversational context, were labeled as not sarcas-
tic. For example, a tweet such as ?Yes, I meant that
sarcastically.? should be labeled as not sarcastic be-
cause the sarcastic content was (presumably) in a
previous tweet. The guidelines did not contain any
instructions that required positive/negative contrast
to be present in the tweet, so all forms of sarcasm
were considered to be positive examples.
To ensure that our evaluation data had a healthy
mix of both sarcastic and non-sarcastic tweets, we
collected 1,600 tweets with a sarcasm hashtag (#sar-
casm or #sarcastic), and 1,600 tweets without these
sarcasm hashtags from Twitter?s random streaming
API. When presenting the tweets to the annotators,
the sarcasm hashtags were removed so the annota-
tors had to judge whether a tweet was sarcastic or
not without seeing those hashtags.
To ensure that we had high-quality annotations,
three annotators were asked to annotate the same set
of 200 tweets (100 sarcastic + 100 not sarcastic).
We computed inter-annotator agreement (IAA) be-
tween each pair of annotators using Cohen?s kappa
(?). The pairwise IAA scores were ?=0.80, ?=0.81,
and ?=0.82. We then gave each annotator an addi-
tional 1,000 tweets to annotate, yielding a total of
3,200 annotated tweets. We used the first 200 tweets
as our Tuning Set, and the remaining 3000 tweets as
our Test Set.
Our annotators judged 742 of the 3,200 tweets
(23%) to be sarcastic. Only 713 of the 1,600 tweets
with sarcasm hashtags (45%) were judged to be sar-
castic based on our annotation guidelines. There are
several reasons why a tweet with a sarcasm hash-
tag might not have been judged to be sarcastic. Sar-
casm may not be apparent without prior conversa-
tional context (i.e., multiple tweets), or the sarcastic
content may be in a URL and not the tweet itself, or
the tweet?s content may not obviously be sarcastic
without seeing the sarcasm hashtag (e.g., ?The most
boring hockey game ever #sarcasm?).
Of the 1,600 tweets in our data set that were ob-
tained from the random stream and did not have a
sarcasm hashtag, 29 (1.8%) were judged to be sar-
castic based on our annotation guidelines.
4.2 Baselines
Overall, 693 of the 3,000 tweets in our Test Set
were annotated as sarcastic, so a system that classi-
fies every tweet as sarcastic will have 23% precision.
To assess the difficulty of recognizing the sarcastic
tweets in our data set, we evaluated a variety of base-
line systems.
We created two baseline systems that use n-gram
features with supervised machine learning to create
a sarcasm classifier. We used the LIBSVM (Chang
and Lin, 2011) library to train two support vector
machine (SVM) classifiers: one with just unigram
features and one with both unigrams and bigrams.
The features had binary values indicating the pres-
ence or absence of each n-gram in a tweet. The clas-
sifiers were evaluated using 10-fold cross-validation.
We used the RBF kernel, and the cost and gamma
parameters were optimized for accuracy using un-
igram features and 10-fold cross-validation on our
Tuning Set. The first two rows of Table 2 show the
results for these SVM classifiers, which achieved F
scores of 46-48%.
We also conducted experiments with existing sen-
timent and subjectivity lexicons to see whether they
could be leveraged to recognize sarcasm. We exper-
imented with three resources:
Liu05 : A positive and negative opinion lexicon
from (Liu et al, 2005). This lexicon contains
2,007 positive sentiment words and 4,783 neg-
ative sentiment words.
MPQA05 : The MPQA Subjectivity Lexicon that
is part of the OpinionFinder system (Wilson et
al., 2005a; Wilson et al, 2005b). This lexicon
contains 2,718 subjective words with positive
polarity and 4,910 subjective words with nega-
tive polarity.
AFINN11 The AFINN sentiment lexicon designed
for microblogs (Nielsen, 2011; Hansen et al,
2011) contains 2,477 manually labeled words
and phrases with integer values ranging from -5
(negativity) to 5 (positivity). We considered all
words with negative values to have negative po-
larity (1598 words), and all words with positive
values to have positive polarity (879 words).
We performed four sets of experiments with each
resource to see how beneficial existing sentiment
710
System Recall Precision F score
Supervised SVM Classifiers
1grams .35 .64 .46
1+2grams .39 .64 .48
Positive Sentiment Only
Liu05 .77 .34 .47
MPQA05 .78 .30 .43
AFINN11 .75 .32 .44
Negative Sentiment Only
Liu05 .26 .23 .24
MPQA05 .34 .24 .28
AFINN11 .24 .22 .23
Positive and Negative Sentiment, Unordered
Liu05 .19 .37 .25
MPQA05 .27 .30 .29
AFINN11 .17 .30 .22
Positive and Negative Sentiment, Ordered
Liu05 .09 .40 .14
MPQA05 .13 .30 .18
AFINN11 .09 .35 .14
Our Bootstrapped Lexicons
Positive VPs .28 .45 .35
Negative Situations .29 .38 .33
Contrast(+VPs, ?Situations), Unordered .11 .56 .18
Contrast(+VPs, ?Situations), Ordered .09 .70 .15
& Contrast(+Preds, ?Situations) .13 .63 .22
Our Bootstrapped Lexicons ? SVM Classifier
Contrast(+VPs, ?Situations), Ordered .42 .63 .50
& Contrast(+Preds, ?Situations) .44 .62 .51
Table 2: Experimental results on the test set
lexicons could be for sarcasm recognition in tweets.
Since our hypothesis is that sarcasm often arises
from the contrast between something positive and
something negative, we systematically evaluated the
positive and negative phrases individually, jointly,
and jointly in a specific order (a positive phrase fol-
lowed by a negative phrase).
First, we labeled a tweet as sarcastic if it con-
tains any positive term in each resource. The Pos-
itive Sentiment Only section of Table 2 shows that
all three sentiment lexicons achieved high recall (75-
78%) but low precision (30-34%). Second, we la-
beled a tweet as sarcastic if it contains any negative
term from each resource. The Negative Sentiment
Only section of Table 2 shows that this approach
yields much lower recall and also lower precision
of 22-24%, which is what would be expected of a
random classifier since 23% of the tweets are sar-
castic. These results suggest that explicit negative
sentiments are not generally indicative of sarcasm.
Third, we labeled a tweet as sarcastic if it contains
both a positive sentiment term and a negative senti-
ment term, in any order. The Positive and Negative
Sentiment, Unordered section of Table 2 shows that
this approach yields low recall, indicating that rela-
tively few sarcastic tweets contain both positive and
negative sentiments, and low precision as well.
Fourth, we required the contrasting sentiments to
occur in a specific order (the positive term must pre-
cede the negative term) and near each other (no more
than 5 words apart). This criteria reflects our obser-
vation that positive sentiments often closely precede
negative situations in sarcastic tweets, so we wanted
to see if the same ordering tendency holds for neg-
ative sentiments. The Positive and Negative Senti-
ment, Ordered section of Table 2 shows that this or-
dering constraint further decreases recall and only
slightly improves precision, if at all. Our hypothe-
711
sis is that when positive and negative sentiments are
expressed in the same tweet, they are referring to
different things (e.g., different aspects of a product).
Expressing positive and negative sentiments about
the same thing would usually sound contradictory
rather than sarcastic.
4.3 Evaluation of Bootstrapped Phrase Lists
The next set of experiments evaluates the effective-
ness of the positive sentiment and negative situa-
tion phrases learned by our bootstrapping algorithm.
The results are shown in the Our Bootstrapped Lex-
icons section of Table 2. For the sake of compar-
ison with other sentiment resources, we first eval-
uated our positive sentiment verb phrases and neg-
ative situation phrases independently. Our positive
verb phrases achieved much lower recall than the
positive sentiment phrases in the other resources, but
they had higher precision (45%). The low recall
is undoubtedly because our bootstrapped lexicon is
small and contains only verb phrases, while the other
resources are much larger and contain terms with
additional parts-of-speech, such as adjectives and
nouns.
Despite its relatively small size, our list of neg-
ative situation phrases achieved 29% recall, which
is comparable to the negative sentiments, but higher
precision (38%).
Next, we classified a tweet as sarcastic if it con-
tains both a positive verb phrase and a negative sit-
uation phrase from our bootstrapped lists, in any
order. This approach produced low recall (11%)
but higher precision (56%) than the sentiment lex-
icons. Finally, we enforced an ordering constraint
so a tweet is labeled as sarcastic only if it contains
a positive verb phrase that precedes a negative situa-
tion in close proximity (no more than 5 words apart).
This ordering constraint further increased precision
from 56% to 70%, with a decrease of only 2 points
in recall. This precision gain supports our claim that
this particular structure (positive verb phrase fol-
lowed by a negative situation) is strongly indicative
of sarcasm. Note that the same ordering constraint
applied to a positive verb phrase followed by a neg-
ative sentiment produced much lower precision (at
best 40% precision using the Liu05 lexicon). Con-
trasting a positive sentiment with a negative situa-
tion seems to be a key element of sarcasm.
In the last experiment, we added the positive pred-
icative expressions and also labeled a tweet as sar-
castic if a positive predicative appeared in close
proximity to (within 5 words of) a negative situa-
tion. The positive predicatives improved recall to
13%, but decreased precision to 63%, which is com-
parable to the SVM classifiers.
4.4 A Hybrid Approach
Thus far, we have used the bootstrapped lexicons
to recognize sarcasm by looking for phrases in our
lists. We will refer to our approach as the Contrast
method, which labels a tweet as sarcastic if it con-
tains a positive sentiment phrase in close proximity
to a negative situation phrase.
The Contrast method achieved 63% precision but
with low recall (13%). The SVM classifier with un-
igram and bigram features achieved 64% precision
with 39% recall. Since neither approach has high
recall, we decided to see whether they are comple-
mentary and the Contrast method is finding sarcastic
tweets that the SVM classifier overlooks.
In this hybrid approach, a tweet is labeled as sar-
castic if either the SVM classifier or the Contrast
method identifies it as sarcastic. This approach im-
proves recall from 39% to 42% using the Contrast
method with only positive verb phrases. Recall im-
proves to 44% using the Contrast method with both
positive verb phrases and predicative phrases. This
hybrid approach has only a slight drop in precision,
yielding an F score of 51%. This result shows that
our bootstrapped phrase lists are recognizing sarcas-
tic tweets that the SVM classifier misses.
Finally, we ran tests to see if the performance of
the hybrid approach (Contrast ? SVM) is statisti-
cally significantly better than the performance of the
SVM classifier alone. We used paired bootstrap sig-
nificance testing as described in Berg-Kirkpatrick
et al (2012) by drawing 106 samples with repeti-
tion from the test set. These results showed that the
Contrast ? SVM system is statistically significantly
better than the SVM classifier at the p < .01 level
(i.e., the null hypothesis was rejected with 99% con-
fidence).
4.5 Analysis
To get a better sense of the strength and limitations
of our approach, we manually inspected some of the
712
tweets that were labeled as sarcastic using our boot-
strapped phrase lists. Table 3 shows some of the sar-
castic tweets found by the Contrast method but not
by the SVM classifier.
i love fighting with the one i love
love working on my last day of summer
i enjoy tweeting [user] and not getting a reply
working during vacation is awesome .
can?t wait to wake up early to babysit !
Table 3: Five sarcastic tweets found by the Contrast
method but not the SVM
These tweets are good examples of a positive sen-
timent (love, enjoy, awesome, can?t wait) contrast-
ing with a negative situation. However, the negative
situation phrases are not always as specific as they
should be. For example, ?working? was learned as
a negative situation phrase because it is often neg-
ative when it follows a positive sentiment (?I love
working...?). But the attached prepositional phrases
(?on my last day of summer? and ?during vacation?)
should ideally have been captured as well.
We also examined tweets that were incorrectly la-
beled as sarcastic by the Contrast method. Some
false hits come from situations that are frequently
negative but not always negative (e.g., some peo-
ple genuinely like waking up early). However, most
false hits were due to overly general negative situa-
tion phrases (e.g., ?I love working there? was labeled
as sarcastic). We believe that an important direction
for future work will be to learn longer phrases that
represent more specific situations.
5 Conclusions
Sarcasm is a complex and rich linguistic phe-
nomenon. Our work identifies just one type of sar-
casm that is common in tweets: contrast between a
positive sentiment and negative situation. We pre-
sented a bootstrapped learning method to acquire
lists of positive sentiment phrases and negative ac-
tivities and states, and show that these lists can be
used to recognize sarcastic tweets.
This work has only scratched the surface of pos-
sibilities for identifying sarcasm arising from posi-
tive/negative contrast. The phrases that we learned
were limited to specific syntactic structures and we
required the contrasting phrases to appear in a highly
constrained context. We plan to explore methods for
allowing more flexibility and for learning additional
types of phrases and contrasting structures.
We also would like to explore new ways to iden-
tify stereotypically negative activities and states be-
cause we believe this type of world knowledge is
essential to recognize many instances of sarcasm.
For example, sarcasm often arises from a descrip-
tion of a negative event followed by a positive emo-
tion but in a separate clause or sentence, such as:
?Going to the dentist for a root canal this after-
noon. Yay, I can?t wait.? Recognizing the intensity
of the negativity may also be useful to distinguish
strong contrast from weak contrast. Having knowl-
edge about stereotypically undesirable activities and
states could also be important for other natural lan-
guage understanding tasks, such as text summariza-
tion and narrative plot analysis.
6 Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Interior National Business Center (DoI
/ NBC) contract number D12PC00285. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
DoI/NBE, or the U.S. Government.
References
Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.
2012. An empirical investigation of statistical signifi-
cance in nlp. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?12, pages 995?1005.
Paula Carvalho, Lu??s Sarmento, Ma?rio J. Silva, and
Euge?nio de Oliveira. 2009. Clues for detecting irony
in user-generated contents: oh...!! it?s ?so easy? ;-). In
Proceedings of the 1st international CIKM workshop
on Topic-sentiment analysis for mass opinion, TSA
2009.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
713
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Henry S. Cheang and Marc D. Pell. 2008. The sound of
sarcasm. Speech Commun., 50(5):366?381, May.
Henry S. Cheang and Marc D. Pell. 2009. Acous-
tic markers of sarcasm in cantonese and english.
The Journal of the Acoustical Society of America,
126(3):1394?1405.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL 2010.
Elena Filatova. 2012. Irony and sarcasm: Corpus gener-
ation and analysis using crowdsourcing. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12).
Roger J. Kreuz Gina M. Caucci. 2012. Social and par-
alinguistic cues to sarcasm. online 08/02/2012, 25:1?
22, February.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies.
Lars Kai Hansen, Adam Arvidsson, Finn Arup Nielsen,
Elanor Colleoni, and Michael Etter. 2011. Good
friends, bad news - affect and virality in twitter. In
The 2011 International Workshop on Social Comput-
ing, Network, and Services (SocialComNet 2011).
Roger Kreuz and Gina Caucci. 2007. Lexical influences
on the perception of sarcasm. In Proceedings of the
Workshop on Computational Approaches to Figurative
Language.
Christine Liebrecht, Florian Kunneman, and Antal
Van den Bosch. 2013. The perfect solution for detect-
ing sarcasm in tweets #not. In Proceedings of the 4th
Workshop on Computational Approaches to Subjec-
tivity, Sentiment and Social Media Analysis, WASSA
2013.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opinions
on the web. In Proceedings of the 14th International
World Wide Web conference (WWW-2005).
Stephanie Lukin and Marilyn Walker. 2013. Really?
well. apparently bootstrapping improves the perfor-
mance of sarcasm and nastiness classifiers for online
dialogue. In Proceedings of the Workshop on Lan-
guage Analysis in Social Media.
Finn Arup Nielsen. 2011. A new anew: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ?Making
Sense of Microposts?: Big things come in small pack-
ages (http://arxiv.org/abs/1103.2903).
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In The 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL 2013).
Katherine P. Rankin, Andrea Salazar, Maria Luisa Gorno-
Tempini, Marc Sollberger, Stephen M. Wilson, Dani-
jela Pavlic, Christine M. Stanley, Shenly Glenn,
Michael W. Weiner, and Bruce L. Miller. 2009. De-
tecting sarcasm from paralinguistic cues: Anatomic
and cognitive correlates in neurodegenerative disease.
Neuroimage, 47:2005?2015.
Joseph Tepperman, David Traum, and Shrikanth
Narayanan. 2006. ?Yeah right?: Sarcasm recogni-
tion for spoken dialogue systems. In Proceedings of
the INTERSPEECH 2006 - ICSLP, Ninth International
Conference on Spoken Language Processing.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
ICWSM - A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Product
Reviews. In Proceedings of the Fourth International
Conference on Weblogs and Social Media (ICWSM-
2010), ICWSM 2010.
Michael Wiegand, Josef Ruppenhofer, and Dietrich
Klakow. 2013. Predicative adjectives: An unsuper-
vised criterion to extract subjective adjectives. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 534?
539, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005a. OpinionFinder: A System for Subjec-
tivity Analysis. In Proceedings of HLT/EMNLP 2005
Interactive Demonstrations, pages 34?35, Vancouver,
Canada, October.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the 2005
Human Language Technology Conference / Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
714
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203?1209,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Learning Emotion Indicators from Tweets: Hashtags, Hashtag Patterns,
and Phrases
Ashequl Qadir
School of Computing
University of Utah
Salt Lake City, UT 84112, USA
asheq@cs.utah.edu
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112, USA
riloff@cs.utah.edu
Abstract
We present a weakly supervised approach
for learning hashtags, hashtag patterns, and
phrases associated with five emotions: AFFEC-
TION, ANGER/RAGE, FEAR/ANXIETY, JOY,
and SADNESS/DISAPPOINTMENT. Starting
with seed hashtags to label an initial set of
tweets, we train emotion classifiers and use
them to learn new emotion hashtags and hash-
tag patterns. This process then repeats in a
bootstrapping framework. Emotion phrases
are also extracted from the learned hashtags
and used to create phrase-based emotion clas-
sifiers. We show that the learned set of emo-
tion indicators yields a substantial improve-
ment in F-scores, ranging from +%5 to +%18
over baseline classifiers.
1 Introduction
Identifying emotions in social media text can be benefi-
cial for many applications, for example to help compa-
nies understand how people feel about their products,
to assist governments in recognizing growing anger or
fear associated with an event, or to help media outlets
understand people?s emotional response toward contro-
versial issues or international affairs. On the Twitter
micro-blogging platform, people often use hashtags to
express an emotional state (e.g., #happyasalways, #an-
gryattheworld). While some hashtags consist of a sin-
gle word (e.g., #angry), many hashtags include multi-
ple words and creative spellings (e.g., #cantwait4tmrw,
#Youredabest), which can not be easily recognized us-
ing sentiment or emotion lexicons.
Our research learns three types of emotion in-
dicators for tweets: hashtags, hashtag patterns,
and phrases for one of five emotions: AFFEC-
TION, ANGER/RAGE, FEAR/ANXIETY, JOY, or SAD-
NESS/DISAPPOINTMENT. We present a bootstrapping
framework for learning emotion hashtags and extend
the framework to also learn more general hashtag pat-
terns. We then harvest emotion phrases from the hash-
tags and hashtag patterns for contextual emotion clas-
sification.
First, we make the observation that emotion hashtags
often share a common prefix. For example, #angry-
attheworld and #angryatlife both have the prefix ?an-
gry at?, which suggests the emotion ANGER. Conse-
quently, we generalize beyond specific hashtags to cre-
ate hashtag patterns that will match all hashtags with
the same prefix, such as the pattern #angryat* which
will match both #angryattheworld and #angryatlife.
A key challenge is that a seemingly strong emotion
word or phrase can have a different meaning depending
upon the following words. For example, #angry* may
seem like an obvious pattern to identify ANGER tweets.
But #angrybirds is a popular hashtag that refers to a
game, not the writer?s emotion. Similarly, ?love you?
usually expresses AFFECTION when it is followed by
a person (e.g., #loveyoumom). But it may express JOY
in other contexts (e.g., #loveyoulife). We use probabil-
ity estimates to determine which hashtag patterns are
reliable indicators for an emotion.
Our second observation is that hashtags can also be
used to harvest emotion phrases. For example, if we
learn that the hashtag #lovelife is associated with JOY,
then we can extract the phrase ?love life? from the
hashtag and use it to recognize emotion in the body
of tweets. However, unlike hashtags, which are self-
contained, the words surrounding a phrase in a tweet
must also be considered. For example, negation can
toggle polarity (?don?t love life? may suggest SAD-
NESS, not JOY) and the aspectual context may indicate
that no emotion is being expressed (e.g., ?I would love
life if ...?). Consequently, we train classifiers to deter-
mine if a tweet contains an emotion based on both an
emotion phrase and its context.
2 Related Work
In addition to sentiment analysis, which has been
widely studied (e.g., (Barbosa and Feng, 2010; Brody
and Diakopoulos, 2011; Kouloumpis et al., 2011;
Mitchell et al., 2013)), recognizing emotions in social
media text has also become a popular research topic in
recent years. Researchers have studied feature sets and
linguistic styles (Roberts et al., 2012), emotion influ-
encing behaviors (Kim et al., 2012), sentence contexts
(Yang et al., 2007b), hierarchical emotion classifica-
tion (Ghazi et al., 2010; Esmin et al., 2012) and emo-
tion lexicon creation (Yang et al., 2007a; Mohammad,
2012a; Staiano and Guerini, 2014). Researchers have
also started to utilize the hashtags of tweets, but pri-
marily to collect labeled data (e.g., for sarcasm (Davi-
1203
Figure 1: Bootstrapped Learning. (HT = hashtag; HP = hashtag pattern)
dov et al., 2010; Riloff et al., 2013) and for senti-
ment/emotion data (Wang et al., 2012; Mohammad et
al., 2013; Choudhury et al., 2012; Purver and Bat-
tersby, 2012; Mohammad, 2012a)).
Wang et al. (2011) investigated several graph based
algorithms to collectively classify hashtag sentiments,
but their work is focused on positive versus nega-
tive polarity classification. Our research extends the
preliminary work on bootstrapped learning of emo-
tion hashtags (Qadir and Riloff, 2013) to additionally
learn patterns corresponding to hashtag prefix expres-
sions and to extract emotion phrases from the hashtags,
which are used to train phrase-based emotion classi-
fiers.
3 Learning Emotion Hashtags, Hashtag
Patterns and Phrases
For our research, we collapsed Parrot?s emo-
tion taxonomy (Parrott, 2001)
1
into 5 emotion
classes that frequently occur in tweets and min-
imally overlap with each other: AFFECTION,
ANGER/RAGE, FEAR/ANXIETY, JOY, and SAD-
NESS/DISAPPOINTMENT. We also used a NONE OF
THE ABOVE class for tweets that do not express any
emotion or express an emotion different from our five
classes. For each of these categories, we identified 5
common hashtags that are strongly associated with the
emotion and used them as seeds. Table 1 shows the
seed hashtags.
Compared to the Ekman emotion classes (Ekman,
1992), one of the emotion taxonomies frequently used
in NLP research (Strapparava and Mihalcea, 2007; Mo-
hammad, 2012b), JOY, ANGER, SADNESS and FEAR
are comparable to 4 of our 5 emotion classes. We do
not study Ekman?s SURPRISE and DISGUST classes,
but include AFFECTION.
3.1 Learning Hashtags
Figure 1 presents the framework of the bootstrapping
algorithm for hashtag learning. The process begins by
1
There were other emotions in Parrott?s taxonomy such
as SURPRISE, NEGLECT, etc. that we did not use for this
research.
Emotion Classes Seed Hashtags
AFFECTION #loveyou, #sweetheart, #bff
#romantic, #soulmate
ANGER & RAGE #angry, #mad, #hateyou
#pissedoff, #furious
FEAR & ANXIETY #afraid, #petrified, #scared
#anxious, #worried
JOY #happy, #excited, #yay
#blessed, #thrilled
SADNESS & #sad, #depressed
DISAPPOINTMENT #disappointed, #unhappy
#foreveralone
Table 1: Emotion Classes and Seed Hashtags
collecting tweets that contain the seed hashtags and la-
beling them with the corresponding emotion. For this
purpose, we collected 323,000 tweets in total that con-
tain at least one of our seed hashtags. We also exploit a
large pool of unlabeled tweets to use during bootstrap-
ping, consisting of 2.3 million tweets with at least one
hashtag per tweet (because we want to learn hashtags),
collected using Twitter?s streaming API. We did not in-
clude retweets or tweets with URLs, to reduce duplica-
tion and focus on tweets with original content. The un-
labeled tweets dataset had 1.29 average hashtags-per-
tweet and 3.95 average tweets-per-hashtag. We prepro-
cessed the tweets with CMU?s tokenizer (Owoputi et
al., 2013) and normalized with respect to case.
The labeled tweets are then used to train a set of
emotion classifiers. We trained one logistic regression
classifier for each emotion class using the LIBLINEAR
package (Fan et al., 2008). We chose logistic regression
because it produces probabilities with its predictions,
which are used to assign scores to hashtags. As fea-
tures, we used unigrams and bigrams with frequency>
1. We removed the seed hashtags from the tweets so
the classifiers could not use them as features.
For each emotion class e ? E, the tweets contain-
ing a seed hashtag for e were used as positive training
instances. The negative training instances consisted of
the tweets containing seed hashtags for the competing
emotions as well as 100,000 randomly selected tweets
1204
Affection Anger & Fear & Joy Sadness &
Rage Anxiety Disappointment
#yourthebest #godie #hatespiders #tripleblessed #leftout
#myotherhalf #donttalktome #haunted #tgfad #foreverugly
#bestfriendforever #pieceofshit #shittingmyself #greatmood #singleprobs
#loveyoulots #irritated #worstfear #thankful #lonerlyfe
#flyhigh #fuming #scaresme #atlast #teamlonely
#comehomesoon #hateliars #nightmares #feelinggood #unloved
#wuvyou #heated #paranoid #happygirl #friendless
#alwaysandforever #getoutofmylife #hateneedles #godisgreat #heartbroken
#missyousomuch #angrytweet #frightened #superhappy #needalife
#loveyougirl #dontbothermewhen #freakedout #ecstatic #letdown
Table 2: Examples of Learned Hashtags
from our unlabeled tweets. Although some of the unla-
beled tweets may correspond to emotion e, we expect
that most will have no emotion or an emotion different
from e, giving us a slightly noisy but large, diverse set
of negative instances.
We then apply each emotion classifier to the un-
labeled tweets. For each emotion e, we collect the
tweets classified as e and extract the hashtags from
those tweets to create a candidate pool H
e
of hashtags
for emotion e. To limit the number of candidates, we
discard hashtags that occur < 10 times, have just one
character, or have> 20 characters. Next, we score each
candidate hashtag h by computing the average proba-
bility assigned by the logistic regression classifier for
emotion e over all of the tweets containing hashtag h.
For each emotion class, we select the 10 hashtags with
the highest scores. From the unlabeled tweets, we then
add all tweets with one of the learned hashtags to the
training instances, and the bootstrapping process con-
tinues. Table 2 shows examples of the learned hashtags.
3.2 Learning Hashtag Patterns
We learn hashtag patterns in a similar but separate boot-
strapping process. We first expand each hashtag into a
sequence of words using an N-gram based word seg-
mentation algorithm
2
supplied with corpus statistics
from our tweet collection. For example, #angryatlife
expands
3
to the phrase ?angry at life?. We use a Prefix
Tree (Trie) data structure to represent all possible pre-
fixes of the expanded hashtag phrases, but the prefixes
consist of words instead of characters.
Next, we traverse the tries and consider all possi-
ble prefix paths as candidate hashtag patterns. We
only consider prefixes that have occurred with at least
one following word. For example, #angryashell, #an-
gryasalways, #angrybird, #angryatlife, #angryatyou
would produce patterns: #angry*, #angryas*, #an-
gryat* as shown in Figure 2.
We score each pattern by applying the classifier for
2
http://norvig.com/ngrams/
3
On a random sample of 100 hashtags, we found expan-
sion accuracy to be 76% (+8% partially correct expansions).
Figure 2: Trie of example hashtags with prefix angry.
Dotted lines lead to non-terminal nodes where patterns
are extracted.
emotion e (trained in the same way as hashtag learn-
ing) to all tweets having hashtags that match the pat-
tern. We compute the average probability produced by
the classifier, and for each emotion class, we select the
10 hashtag patterns with the highest scores. From the
unlabeled tweets, we then add all tweets with hashtags
that match one of the learned hashtag patterns to the
training instances, and the bootstrapping process con-
tinues. Table 3 shows examples of learned hashtag pat-
terns and matched hashtags.
3.3 Creating Phrase-based Classifiers
The third type of emotion indicator that we acquire are
emotion phrases. At the end of the bootstrapping pro-
cess, we apply the word segmentation algorithm to all
of the learned hashtags and hashtag patterns to expand
them into phrases (e.g., #lovemylife ? ?love my life?).
Each phrase is assumed to express the same emotion as
the original hashtag. However, as we will see in Sec-
tion 4, just the presence of a phrase yields low preci-
sion, and surrounding context must also be taken into
account.
Consequently, we train a logistic regression classi-
fier for each emotion e, which classifies a tweet with
respect to emotion e based on the presence of a learned
phrase for e as well as a context window of size 6
around the phrase (set of 3 words on its left and set of 3
1205
Emotion Hashtag Pattern Examples of Matching Hashtags
AFFECTION #bestie* #bestiefolyfe, #bestienight, #bestielove
#missedyou* #missedyoutoomuch, #missedyouguys, #missedyoubabies
ANGER & RAGE #godie* #godieoldman, #godieyou, #godieinahole
#pissedoff* #pissedofffather, #pissedoffnow, #pissedoffmood
FEAR & ANXIETY #tooscared* #tooscaredtogoalone, #tooscaredformama, #tooscaredtomove
#nightmares* #nightmaresfordays, #nightmaresforlife, #nightmarestonight
JOY #feelinggood* #feelinggoodnow, #feelinggoodforme, #feelinggoodabout
#goodmood* #goodmooditsgameday, #goodmoodmode, #goodmoodnight
SADNESS & #bummed* #bummedout, #bummedaf, #bummednow
DISAPPOINTMENT #singlelife* #singlelifeblows, #singlelifeforme, #singlelifesucks
Table 3: Examples of Learned Hashtag Patterns and Matching Hashtags
words on its right). Tweets containing a learned phrase
for e and a seed hashtag for e are the positive training
instances. Tweets containing a learned phrase for e and
a seed hashtag for a different emotion are used as the
negative training instances. For example, when ?love
my life? is learned as an emotion phrase for JOY, the
tweet, ?how can I love my life when everybody leaves
me! #sad? will have one feature each for the left words
?how?, ?can?, and ?I?, one feature each for the right
words ?when?, ?everybody? and ?leaves?, and one
feature for the phrase ?love my life?. The tweet will
then be considered a negative instance for JOY because
?#sad? indicates a different emotion.
4 Experimental Results
To evaluate our learned emotion indicators, we manu-
ally selected 25 topic keywords/phrases
4
that we con-
sidered to be strongly associated with emotions, but
not necessarily with any specific emotions of our study.
We then searched in Twitter using Twitter Search API
for any of these topic phrases and their correspond-
ing hashtags. These 25 topic phrases are: Prom,
Exam, Graduation, Marriage, Divorce, Husband, Wife,
Boyfriend, Girlfriend, Job, Hire, Laid Off, Retirement,
Win, Lose, Accident, Failure, Success, Spider, Loud
Noise, Chest Pain, Storm, Home Alone, No Sleep and
Interview. Since the purpose is to evaluate the qual-
ity and coverage of the emotion hashtags that we learn,
we filtered out any tweet that did not have at least one
hashtag.
Two annotators were given annotation guidelines
and were instructed to label each tweet with up to
two emotions. The instructions specified that the emo-
tion must be felt by the writer. The annotators an-
notated 500 tweets with an inter-annotator agreement
level of 0.79 Kappa (?) (Carletta, 1996). The an-
notation disagreements in these 500 tweets were then
adjudicated, and each annotator labeled an additional
2,500 tweets. Altogether this gave us an emotion an-
notated dataset of 5,500 tweets. We randomly sepa-
rated out 1,000 tweets from this collection as a tuning
4
This data collection process is similar to the emotion
tweet dataset creation by Roberts et al. (2012)
set, and used the remaining 4,500 tweets as evaluation
data. The distribution of emotions in the evaluation
data was 6% for AFFECTION, 9% for ANGER/RAGE,
13% for FEAR/ANXIETY, 22% for JOY, and 12% for
SADNESS/DISAPPOINTMENT. 42% of the tweets had
none of the 5 emotions and 4% of the tweets had more
than one emotions in the same tweet.
We created two baseline systems to assess the diffi-
culty of the emotion classification task. First, we cre-
ated SVM classifiers for each emotion using N-gram
features and performed 10-fold cross-validation on the
test data. We used LIBSVM (Chang and Lin, 2011)
and set the cost and gamma parameters based on the
tuning data. Second, we acquired the NRC Emotional
Tweets Lexicon (Mohammad, 2012a), which contains
emotion unigrams and bigrams for 8 emotions, 4 that
are comparable to ours: ANGER, FEAR, JOY and SAD-
NESS. We created a hashtag from each term in the lexi-
con by appending a # symbol on the front and removing
whitespace. For each term, we chose the emotion with
the highest score in the lexicon.
Table 4 shows our experimental results. The baseline
classifiers (SVM
1
uses unigrams, SVM
1+2
uses uni-
grams and bigrams) have low recall but 63-78% pre-
cision. The hashtags created from the NRC Lexicon
have low precision. This could be due to possible en-
tries (e.g., ?candy? or ?idea?), which without context
are not much indicative of any specific emotion.
The second section of Table 4 shows the results when
we label a tweet based on the presence of a hash-
tag or hashtag pattern. First, we use just the 5 seed
hashtags to assess their coverage (as expected, high
precision but low recall). Next, we add the hashtags
learned during bootstrapping. For most emotions, the
hashtags achieve performance similar to the supervised
SVMs. The following row shows results for our learned
hashtag patterns. Recall improves by +14% for AF-
FECTION, which illustrates the benefit of more general
hashtag patterns, and at least maintains similar level of
precision for other emotions. When the hashtags and
hashtag patterns are combined (HTs+HPs), we see the
best of both worlds with improved recall as high as
+17% in AFFECTION and +10% in FEAR/ANXIETY
1206
AFFECTION ANGER & FEAR & JOY SADNESS &
Method RAGE ANXIETY DISAPPOINT.
P R F P R F P R F P R F P R F
Baselines
SVM
1
78 40 53 66 17 27 68 33 44 66 47 55 63 26 37
SVM
1+2
78 35 48 67 10 17 68 29 41 65 43 52 63 21 32
NRC Lexicon HTs n/a 26 16 20 39 12 18 36 13 19 28 18 22
Learned Hashtags (HTs) and Hashtag Patterns (HPs)
Seed HTs 94 06 11 75 01 03 100 06 11 93 04 08 81 02 05
All HTs 82 34 48 63 23 34 60 37 46 81 13 22 72 28 40
All HPs 76 48 59 60 22 32 57 42 48 84 09 16 73 16 26
All HTs+HPs 74 51 60 56 27 36 55 47 51 80 15 25 70 29 41
Learned Emotion Phrases
Emotion Phrases 32 28 30 17 46 25 28 45 35 50 23 32 26 30 28
Phrase-based Classifier (PC) 54 07 12 48 05 09 63 17 27 69 12 20 50 06 11
SVM
1
+PC 79 42 55 63 18 28 70 35 47 68 48 56 62 27 38
Hybrid Approach
SVM
1
+PC ? HTs+HPs 69 64 66 55 38 45 54 61 57 68 54 60 62 44 51
Table 4: Emotion Classification Results (P = Precision, R = Recall, F = F-score)
compared to All HTs, as well as improved F-scores
across the board.
The third section of Table 4 presents the results for
the emotion phrases. The first row (Emotion Phrases)
shows that labeling a tweet based solely on the pres-
ence of a phrase is not very accurate. Next, we applied
the trained models of the phrase-based classifiers (de-
scribed in Section 3.3) to each tweet of the evaluation
data. This provided us with probability of an emotion
for each of the 5 emotions. The phrase-based classifiers
(PC) yield higher precision, albeit with low recall. Fi-
nally, we use these probabilities as 5 additional features
to SVM
1
. The corresponding SVM
1
+PC row shows
a consistent 1-2 point F score gain over the original
SVM
1
baseline.
The last section of Table 4 shows the best results with
a hybrid system, which labels a tweet with emotion e if
EITHER the enhanced SVM labels it as e OR the tweet
contains a hashtag or hashtag pattern associated with e.
This combined approach achieves substantially higher
performance than any individual method across all 5
emotion classes, with improved F-scores ranging from
+%5 to +%18 over the baseline classifiers, demonstrat-
ing that the different types of emotion indicators are
complementary.
5 Conclusions
We have shown that three types of emotion indicators
can be learned from tweets with weakly supervised
bootstrapping: hashtags, hashtag patterns, and phrases.
Our findings suggest that emotion hashtags are strong
indicators for recognizing writer?s emotion in tweets,
and can be further generalized into hashtag patterns by
learning prefix expressions corresponding to an emo-
tion. Phrases learned from the hashtags and patterns
are not always reliable by themselves, but training ad-
ditional classifiers with the emotion phrases and their
surrounding context provides added benefits to emotion
classification in tweets. Our results showed that com-
bining the learned emotion indicators with an N-gram
classifier in a hybrid approach substantially improves
performance across 5 emotion classes.
Acknowledgments
This work was supported by the Intelligence Advanced
Research Projects Activity (IARPA) via Department of
Interior National Business Center (DoI/NBC) contract
number D12PC00285. The U.S. Government is autho-
rized to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright anno-
tation thereon. The views and conclusions contained
herein are those of the authors and should not be in-
terpreted as necessarily representing the official poli-
cies or endorsements, either expressed or implied, of
IARPA, DoI/NBC, or the U.S. Government.
References
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING
?10.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22:249?254, June.
1207
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1?27:27, May.
Munmun De Choudhury, Michael Gamon, and Scott
Counts. 2012. Happy, nervous or surprised? clas-
sification of human affective states in social media.
In Proceedings of the Sixth International Conference
on Weblogs and Social Media.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10.
Paul Ekman. 1992. An argument for basic emotions.
Cognition and Emotion, 6(3):169200.
Ahmed Ali Abdalla Esmin, Roberto L. De Oliveira Jr.,
and Stan Matwin. 2012. Hierarchical classifica-
tion approach to emotion recognition in twitter. In
Proceedings of the 11th International Conference on
Machine Learning and Applications, ICMLA, Boca
Raton, FL, USA, December 12-15, 2012. Volume 2,
pages 381?385. IEEE.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. J. Mach. Learn.
Res., 9:1871?1874, June.
Diman Ghazi, Diana Inkpen, and Stan Szpakowicz.
2010. Hierarchical versus flat classification of
emotions in text. In Proceedings of the NAACL
HLT 2010 Workshop on Computational Approaches
to Analysis and Generation of Emotion in Text,
CAAGET ?10.
Suin Kim, JinYeong Bak, and Alice Oh. 2012.
Discovering emotion influence patterns in online
social network conversations. SIGWEB Newsl.,
(Autumn):3:1?3:6, September.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In Proceedings of the Fifth In-
ternational Conference on Weblogs and Social Me-
dia.
Margaret Mitchell, Jacqui Aguilar, Theresa Wilson,
and Benjamin Van Durme. 2013. Open domain tar-
geted sentiment. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Semantic
Evaluation Exercises (SemEval-2013).
Saif Mohammad. 2012a. #emotional tweets. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics.
Saif Mohammad. 2012b. Portable features for clas-
sifying emotional text. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics (NAACL-
2013).
W. Gerrod Parrott, editor. 2001. Emotions in Social
Psychology. Psychology Press.
Matthew Purver and Stuart Battersby. 2012. Experi-
menting with distant supervision for emotion classi-
fication. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, EACL ?12, pages 482?491.
Ashequl Qadir and Ellen Riloff. 2013. Bootstrapped
learning of emotion hashtags #hashtags4you. In
Proceedings of the 4th Workshop on Computational
Approaches to Subjectivity, Sentiment and Social
Media Analysis.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive senti-
ment and negative situation. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?13.
Kirk Roberts, Michael A. Roach, Joseph Johnson, Josh
Guthrie, and Sanda M. Harabagiu. 2012. Em-
patweet: Annotating and detecting emotions on twit-
ter. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC-2012). ACL Anthology Identifier: L12-
1059.
Jacopo Staiano and Marco Guerini. 2014. De-
pechemood: a lexicon for emotion analysis from
crowd-annotated news. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers).
Carlo Strapparava and Rada Mihalcea. 2007.
SemEval-2007 Task 14: Affective Text. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007).
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou,
and Ming Zhang. 2011. Topic sentiment analysis
in twitter: a graph-based hashtag sentiment classifi-
cation approach. In Proceedings of the 20th ACM
international conference on Information and knowl-
edge management, CIKM ?11.
Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P. Sheth. 2012. Harnessing twitter ?big
data? for automatic emotion identification. In Pro-
ceedings of the 2012 ASE/IEEE International Con-
ference on Social Computing and 2012 ASE/IEEE
1208
International Conference on Privacy, Security, Risk
and Trust, SOCIALCOM-PASSAT ?12.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi
Chen. 2007a. Building emotion lexicon from
weblog corpora. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, ACL ?07.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi
Chen. 2007b. Emotion classification using web blog
corpora. In Proceedings of the IEEE/WIC/ACM In-
ternational Conference on Web Intelligence, WI ?07,
pages 275?278.
1209
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 199?208,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Ensemble-based Semantic Lexicon Induction for Semantic Tagging
Ashequl Qadir
University of Utah
School of Computing
Salt Lake City, UT 84112, USA
asheq@cs.utah.edu
Ellen Riloff
University of Utah
School of Computing
Salt Lake City, UT 84112, USA
riloff@cs.utah.edu
Abstract
We present an ensemble-based framework
for semantic lexicon induction that incorpo-
rates three diverse approaches for semantic
class identification. Our architecture brings
together previous bootstrapping methods for
pattern-based semantic lexicon induction and
contextual semantic tagging, and incorpo-
rates a novel approach for inducing semantic
classes from coreference chains. The three
methods are embedded in a bootstrapping ar-
chitecture where they produce independent
hypotheses, consensus words are added to the
lexicon, and the process repeats. Our results
show that the ensemble outperforms individ-
ual methods in terms of both lexicon quality
and instance-based semantic tagging.
1 Introduction
One of the most fundamental aspects of meaning is
the association between words and semantic cate-
gories, which allows us to understand that a ?cow?
is an animal and a ?house? is a structure. We will
use the term semantic lexicon to refer to a dictionary
that associates words with semantic classes. Se-
mantic dictionaries are useful for many NLP tasks,
as evidenced by the widespread use of WordNet
(Miller, 1990). However, off-the-shelf resources are
not always sufficient for specialized domains, such
as medicine, chemistry, or microelectronics. Fur-
thermore, in virtually every domain, texts contain
lexical variations that are often missing from dic-
tionaries, such as acronyms, abbreviations, spelling
variants, informal shorthand terms (e.g., ?abx? for
?antibiotics?), and composite terms (e.g., ?may-
december? or ?virus/worm?). To address this prob-
lem, techniques have been developed to automate
the construction of semantic lexicons from text cor-
pora using bootstrapping methods (Riloff and Shep-
herd, 1997; Roark and Charniak, 1998; Phillips and
Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007;
McIntosh and Curran, 2009; McIntosh, 2010), but
accuracy is still far from perfect.
Our research explores the use of ensemble meth-
ods to improve the accuracy of semantic lexicon in-
duction. Our observation is that semantic class as-
sociations can be learned using several fundamen-
tally different types of corpus analysis. Bootstrap-
ping methods for semantic lexicon induction (e.g.,
(Riloff and Jones, 1999; Thelen and Riloff, 2002;
McIntosh and Curran, 2009)) collect corpus-wide
statistics for individual words based on shared con-
textual patterns. In contrast, classifiers for semantic
tagging (e.g., (Collins and Singer, 1999; Niu et al,
2003; Huang and Riloff, 2010)) label word instances
and focus on the local context surrounding each in-
stance. The difference between these approaches is
that semantic taggers make decisions based on a sin-
gle context and can assign different labels to differ-
ent instances, whereas lexicon induction algorithms
compile corpus statistics from multiple instances of
a word and typically assign each word to a single
semantic category.1 We also hypothesize that coref-
erence resolution can be exploited to infer semantic
1This approach would be untenable for broad-coverage se-
mantic knowledge acquisition, but within a specialized domain
most words have a dominant word sense. Our experimental re-
sults support this assumption.
199
class labels. Intuitively, if we know that two noun
phrases are coreferent, then they probably belong to
the same high-level semantic category (e.g., ?dog?
and ?terrier? are both animals).
In this paper, we present an ensemble-based
framework for semantic lexicon induction. We in-
corporate a pattern-based bootstrapping method for
lexicon induction, a contextual semantic tagger, and
a new coreference-based method for lexicon induc-
tion. Our results show that coalescing the decisions
produced by diverse methods produces a better dic-
tionary than any individual method alone.
A second contribution of this paper is an analysis
of the effectiveness of dictionaries for semantic tag-
ging. In principle, an NLP system should be able to
assign different semantic labels to different senses
of a word. But within a specialized domain, most
words have a dominant sense and we argue that us-
ing domain-specific dictionaries for tagging may be
equally, if not more, effective. We analyze the trade-
offs between using an instance-based semantic tag-
ger versus dictionary lookup on a collection of dis-
ease outbreak articles. Our results show that the in-
duced dictionaries yield better performance than an
instance-based semantic tagger, achieving higher ac-
curacy with comparable levels of recall.
2 Related Work
Several techniques have been developed for seman-
tic class induction (also called set expansion) using
bootstrapping methods that consider co-occurrence
statistics based on nouns (Riloff and Shepherd,
1997), syntactic structures (Roark and Charniak,
1998; Phillips and Riloff, 2002), and contextual pat-
terns (Riloff and Jones, 1999; Thelen and Riloff,
2002; McIntosh and Curran, 2008; McIntosh and
Curran, 2009). To improve the accuracy of in-
duced lexicons, some research has incorporated neg-
ative information from human judgements (Vyas
and Pantel, 2009), automatically discovered neg-
ative classes (McIntosh, 2010), and distributional
similarity metrics to recognize concept drift (McIn-
tosh and Curran, 2009). Phillips and Riloff (2002)
used co-training (Blum and Mitchell, 1998) to ex-
ploit three simple classifiers that each recognized a
different type of syntactic structure. The research
most closely related to ours is an ensemble-based
method for automatic thesaurus construction (Cur-
ran, 2002). However, that goal was to acquire fine-
grained semantic information that is more akin to
synonymy (e.g., words similar to ?house?), whereas
we associate words with high-level semantic classes
(e.g., a ?house? is a transient structure).
Semantic class tagging is closely related to named
entity recognition (NER) (e.g., (Bikel et al, 1997;
Collins and Singer, 1999; Cucerzan and Yarowsky,
1999; Fleischman and Hovy, 2002)). Some boot-
strapping methods have been used for NER (e.g.,
(Collins and Singer, 1999; Niu et al, 2003) to
learn from unannotated texts. However, most NER
systems will not label nominal noun phrases (e.g.,
they will not identify ?the dentist? as a person)
or recognize semantic classes that are not associ-
ated with proper named entities (e.g., symptoms).2
ACE mention detection systems (e.g., (ACE, 2007;
ACE, 2008)) can label noun phrases that are asso-
ciated with 5-7 semantic classes and are typically
trained with supervised learning. Recently, (Huang
and Riloff, 2010) developed a bootstrapping tech-
nique that induces a semantic tagger from unanno-
tated texts. We use their system in our ensemble.
There has also been work on extracting semantic
class members from the Web (e.g., (Pas?ca, 2004; Et-
zioni et al, 2005; Kozareva et al, 2008; Carlson et
al., 2009)). This line of research is fundamentally
different from ours because these techniques benefit
from the vast repository of information available on
the Web and are therefore designed to harvest a wide
swath of general-purpose semantic information. Our
research is aimed at acquiring domain-specific se-
mantic dictionaries using a collection of documents
representing a specialized domain.
3 Ensemble-based Semantic Lexicon
Induction
3.1 Motivation
Our research combines three fundamentally differ-
ent techniques into an ensemble-based bootstrap-
ping framework for semantic lexicon induction:
pattern-based dictionary induction, contextual se-
mantic tagging, and coreference resolution. Our
motivation for using an ensemble of different tech-
2Some NER systems will handle special constructions such
as dates and monetary amounts.
200
niques is driven by the observation that these meth-
ods exploit different types of information to infer se-
mantic class knowledge. The coreference resolver
uses features associated with coreference, such as
syntactic constructions (e.g., appositives, predicate
nominals), word overlap, semantic similarity, prox-
imity, etc. The pattern-based lexicon induction al-
gorithm uses corpus-wide statistics gathered from
the contexts of all instances of a word and compares
them with the contexts of known category members.
The contextual semantic tagger uses local context
windows around words and classifies each word in-
stance independently from the others.
Since each technique draws its conclusions from
different types of information, they represent inde-
pendent sources of evidence to confirm whether a
word belongs to a semantic class. Our hypothe-
sis is that, combining these different sources of ev-
idence in an ensemble-based learning framework
should produce better accuracy than using any one
method alone. Based on this intuition, we create
an ensemble-based bootstrapping framework that it-
eratively collects the hypotheses produced by each
individual learner and selects the words that were
hypothesized by at least 2 of the 3 learners. This
approach produces a bootstrapping process with
improved precision, both at the critical beginning
stages of the bootstrapping process and during sub-
sequent bootstrapping iterations.
3.2 Component Systems in the Ensemble
In the following sections, we describe each of the
component systems used in our ensemble.
3.2.1 Pattern-based Lexicon Induction
The first component of our ensemble is Basilisk
(Thelen and Riloff, 2002), which identifies nouns
belonging to a semantic class based on collec-
tive information over lexico-syntactic pattern con-
texts. The patterns are automatically generated us-
ing AutoSlog-TS (Riloff, 1996). Basilisk begins
with a small set of seed words for each seman-
tic class and a collection of unannotated documents
for the domain. In an iterative bootstrapping pro-
cess, Basilisk identifies candidate nouns, ranks them
based on its scoring criteria, selects the 5 most confi-
dent words for inclusion in the lexicon, and this pro-
cess repeats using the new words as additional seeds
in subsequent iterations.
3.2.2 Lexicon Induction with a Contextual
Semantic Tagger
The second component in our ensemble is a con-
textual semantic tagger (Huang and Riloff, 2010).
Like Basilisk, the semantic tagger also begins with
seed nouns, trains itself on a large collection of
unannotated documents using bootstrapping, and it-
eratively labels new instances. This tagger labels
noun instances and does not produce a dictionary.
To adapt it for our purposes, we ran the bootstrap-
ping process over the training texts to induce a se-
mantic classifier. We then applied the classifier to
the same set of training documents and compiled a
lexicon by collecting the set of nouns that were as-
signed to each semantic class. We ignored words
that were assigned different labels in different con-
texts to avoid conflicts in the lexicons. We used
the identical configuration described by (Huang and
Riloff, 2010) that applies a 1.0 confidence threshold
for semantic class assignment.
3.2.3 Coreference-Based Lexicon Construction
The third component of our ensemble is a new
method for semantic lexicon induction that exploits
coreference resolution. Members of a coreference
chain represent the same entity, so all references to
the entity should belong to the same semantic class.
For example, suppose ?Paris? and ?the city? are in
the same coreference chain. If we know that city is
a Fixed Location, then we can infer that Paris is also
a Fixed Location.
We induced lexicons from coreference chains us-
ing a similar bootstrapping framework that begins
with seed nouns and unannotated texts. Let S de-
note a set of semantic classes and W denote a set of
unknown words. For any s ? S and w ? W , let
Ns,w denote the number of instances of s in the cur-
rent lexicon3 that are coreferent with w in the text
corpus. Then we estimate the probability that word
w belongs to semantic class s as:
P (s|w) = Ns,w?
s??S Ns?,w
We hypothesize the semantic class of w,
SemClass(w) by:
SemClass(w) = argmaxs P (s|w)
3In the first iteration, the lexicon is initialized with the seeds.
201
To ensure high precision for the induced lexicons,
we use a threshold of 0.5. All words with a prob-
ability above this thresold are added to the lexicon,
and the bootstrapping process repeats. Although the
coreference chains remain the same throughout the
process, the lexicon grows so more words in the
chains have semantic class labels as bootstrapping
progresses. Bootstrapping ends when fewer than 5
words are learned for each of the semantic classes.
Many noun phrases are singletons (i.e., they are
not coreferent with any other NPs), which limits the
set of words that can be learned using coreference
chains. Furthermore, coreference resolvers make
mistakes, so the accuracy of the induced lexicons
depends on the quality of the chains. For our experi-
ments, we used Reconcile (Stoyanov et al, 2010), a
freely available supervised coreference resolver.
3.3 Ensemble-based Bootstrapping
Framework
Figure 1 shows the architecture of our ensemble-
based bootstrapping framework. Initially, each lexi-
con only contains the seed nouns. Each component
hypothesizes a set of candidate words for each se-
mantic class, based on its own criteria. The word
lists produced by the three systems are then com-
pared, and we retain only the words that were hy-
pothesized with the same class label by at least two
of the three systems. The remaining words are dis-
carded. The consenus words are added to the lexi-
con, and the bootstrapping process repeats. As soon
as fewer than 5 words are learned for each of the
semantic classes, bootstrapping stops.
Figure 1: Ensemble-based bootstrapping framework
We ran each individual system with the same seed
words. Since bootstrapping typically yields the best
precision during the earliest stages, we used the se-
mantic tagger?s trained model immediately after its
first bootstrapping iteration. Basilisk generates 5
words per cycle, so we report results for lexicons
generated after 20 bootstrapping cycles (100 words)
and after 80 bootstrapping cycles (400 words).
3.4 Co-Training Framework
The three components in our ensemble use different
types of features (views) to identify semantic class
members, so we also experimented with co-training.
Our co-training model uses an identical framework,
but the hypotheses produced by the different meth-
ods are all added to the lexicon, so each method can
benefit from the hypotheses produced by the others.
To be conservative, each time we added only the 10
most confident words hypothesized by each method.
In contrast, the ensemble approach only adds
words to the lexicon if they are hypothesized by two
different methods. As we will see in Section 4.4,
the ensemble performs much better than co-training.
The reason is that the individual methods do not con-
sistently achieve high precision on their own. Con-
sequently, many mistakes are added to the lexicon,
which is used as training data for subsequent boot-
strapping. The benefit of the ensemble is that con-
sensus is required across two methods, which serves
as a form of cross-checking to boost precision and
maintain a high-quality lexicon.
4 Evaluation
4.1 Semantic Class Definitions
We evaluated our approach on nine semantic cate-
gories associated with disease outbreaks. The se-
mantic classes are defined below.
Animal: Mammals, birds, fish, insects and other
animal groups. (e.g., cow, crow, mosquito, herd)
4http://www.nlm.nih.gov/research/umls/
5http://www.maxmind.com/app/worldcities
6http://www.listofcountriesoftheworld.
com/
7http://names.mongabay.com/most_common_
surnames.htm
8http://www.sec.gov/rules/other/
4-460list.htm
9http://www.utexas.edu/world/univ/state/
10http://www.uta.fi/FAST/GC/usabacro.
html/
202
Semantic External Word List Sources
Class
Animal WordNet: [animal], [mammal family], [animal group]
Body Part WordNet: [body part], [body substance], [body covering], [body waste]
DisSym WordNet: [symptom], [physical condition], [infectious agent]; Wikipedia: common and infectious
diseases, symptoms, disease acronyms; UMLS Thesaurus4: diseases, abnormalities, microorganisms
(Archaea, Bacteria, Fungus, Virus)
Fixed Loc. WordNet: [geographic area], [land], [district, territory], [region]; Wiki:US-states; Other:cities5, countries6
Human WordNet: [person], [people], [personnel]; Wikipedia: people names, office holder titles, nationalities,
occupations, medical personnels & acronyms, players; Other: common people names & surnames7
Org WordNet: [organization], [assembly]; Wikipedia: acronyms in healthcare, medical organization acronyms,
news agencies, pharmaceutical companies; Other: companies8, US-universities9, organizations10
Plant & Food WordNet: [food], [plant, flora], [plant part]
Temp. Ref. WordNet: [time], [time interval], [time unit],[time period]
TimeBank: TimeBank1.2 (Pustejovsky et al, 2003) TIMEX3 expressions
Trans. Struct. WordNet: [structure, construction], [road, route], [facility, installation], [work place]
Table 1: External Word List Sources
Body Part: A part of a human or animal body, in-
cluding organs, bodily fluids, and microscopic parts.
(e.g., hand, heart, blood, DNA)
Diseases and Symptoms (DisSym): Diseases
and symptoms. We also include fungi and disease
carriers because, in this domain, they almost always
refer to the disease that they carry. (e.g. FMD, An-
thrax, fever, virus)
Fixed Location (Fixed Loc.): Named locations,
including countries, cities, states, etc. We also in-
clude directions and well-defined geographic areas
or geo-political entities. (e.g., Brazil, north, valley)
Human: All references to people, including
names, titles, professions, and groups. (e.g., John,
farmer, traders)
Organization (Org.): An entity that represents a
group of people acting as a single recognized body,
including named organizations, departments, gov-
ernments, and their acronyms. (e.g., department,
WHO, commission, council)
Temporal Reference (Temp. Ref.): Any refer-
ence to a time or duration, including months, days,
seasons, etc. (e.g., night, May, summer, week)
Plants & Food11: plants, plant parts, or any type
of food. (e.g., seed, mango, beef, milk)
Transient Structures (Trans. Struct.): Transient
physical structures. (e.g., hospital, building, home)
Additionally, we defined a Miscellaneous class
for words that do not belong to any of the other cat-
11We merged plants and food into a single category as it is
difficult to separate them because many food items are plants.
egories. (e.g., output, information, media, point).
4.2 Data Set
We ran our experiments on ProMED-mail12 articles.
ProMED-mail is an internet based reporting system
for infectious disease outbreaks, which can involve
people, animals, and plants grown for food. Our
ProMED corpus contains 5004 documents. We used
4959 documents as (unannotated) training data for
bootstrapping. For the remaining 45 documents,
we used 22 documents to train the coreference re-
solver (Reconcile) and 23 documents as our test set.
The coreference training set contains MUC-7 style
(Hirschman, 1997) coreference annotations13. Once
trained, Reconcile was applied to the 4959 unanno-
tated documents to produce coreference chains.
4.3 Gold Standard Semantic Class Annotations
To obtain gold standard annotations for the test set,
two annotators assigned one of the 9 semantic class
labels, or Miscellaneous, to each head noun based on
its surrounding context. A noun with multiple senses
could get assigned different semantic class labels in
different contexts. The annotators first annotated 13
of the 23 documents, and discussed the cases where
they disagreed. Then they independelty annotated
12http://www.promedmail.org/
13We omit the details of the coreference annotations since
it is not the focus of this research. However, the annotators
measured their agreement on 10 documents and achieved MUC
scores of Precision = .82, Recall = .86, F-measure = .84.
203
the remaining 10 documents and measured inter-
annotator agreement with Cohen?s Kappa (?) (Car-
letta, 1996). The ? score for these 10 documents was
0.91, indicating a high level of agreement. The an-
notators then adjudicated their disagreements on all
23 documents to create the gold standard.
4.4 Dictionary Evaluation
To assess the quality of the lexicons, we estimated
their accuracy by compiling external word lists
from freely available sources such as Wikipedia14
and WordNet (Miller, 1990). Table 1 shows the
sources that we used, where the bracketed items re-
fer to WordNet hypernym categories. We searched
each WordNet hypernym tree (also, instance-
relationship) for all senses of the word. Addition-
ally, we collected the manually labeled words in our
test set and included them in our gold standard lists.
Since the induced lexicons contain individual
nouns, we extracted only the head nouns of multi-
word phrases in the external resources. This
can produce incorrect entries for non-compositional
phrases, but we found this issue to be relatively rare
and we manually removed obviously wrong entries.
We adopted a conservative strategy and assumed that
any lexicon entries not present in our gold standard
lists are incorrect. But we observed many correct en-
tries that were missing from the external resources,
so our results should be interpreted as a lower bound
on the true accuracy of the induced lexicons.
We generated lexicons for each method sepa-
rately, and also for the ensemble and co-training
models. We ran Basilisk for 100 iterations (500
words). We refer to a Basilisk lexicon of size N
using the notation B[N ]. For example, B400 refers
to a lexicon containing 400 words, which was gen-
erated from 80 bootstrapping cycles. We refer to the
lexicon obtained from the semantic tagger as ST Lex.
Figure 2 shows the dictionary evaluation results.
We plotted Basilisk?s accuracy after every 5 boot-
strapping cycles (25 words). For ST Lex, we sorted
the words by their confidence scores and plotted the
accuracy of the top-ranked words in increments of
50. The plots for Coref, Co-Training, and Ensemble
B[N] are based on the lexicons produced after each
bootstrapping cycle.
14www.wikipedia.org/
The ensemble-based framework yields consis-
tently better accuracy than the individual methods
for Animal, Body Part, Human and Temporal Refer-
ence, and similar if not better for Disease & Symp-
tom, Fixed Location, Organization, Plant & Food.
However, relying on consensus from multiple mod-
els produce smaller dictionaries. Big dictionaries are
not always better than small dictionaries in practice,
though. We believe, it matters more whether a dic-
tionary contains the most frequent words for a do-
main, because they account for a disproportionate
number of instances. Basilisk, for example, often
learns infrequent words, so its dictionaries may have
high accuracy but often fail to recognize common
words. We investigate this issue in the next section.
4.5 Instance-based Tagging Evaluation
We also evaluated the effectiveness of the induced
lexicons with respect to instance-based semantic
tagging. Our goal was to determine how useful the
dictionaries are in two respects: (1) do the lexicons
contain words that appear frequently in the domain,
and (2) is dictionary look-up sufficient for instance-
based labeling? Our bootstrapping processes en-
force a constraint that a word can only belong to one
semantic class, so if polysemy is common, then dic-
tionary look-up will be problematic.15
The instance-based evaluation assigns a semantic
label to each instance of a head noun. When using a
lexicon, all instances of the same noun are assigned
the same semantic class via dictionary look-up. The
semantic tagger (SemTag), however, is applied di-
rectly since it was designed to label instances.
Table 2 presents the results. As a baseline, the
W.Net row shows the performance of WordNet for
instance tagging. For words with multiple senses,
we only used the first sense listed in WordNet.
The Seeds row shows the results when perform-
ing dictionary look-up using only the seed words.
The remaining rows show the results for Basilisk
(B100 and B400), coreference-based lexicon induc-
tion (Coref), lexicon induction using the semantic
tagger (ST Lex), and the original instance-based tag-
ger (SemTag). The following rows show the results
for co-training (after 4 iterations and 20 iterations)
15Only coarse polysemy across semantic classes is an issue
(e.g., ?plant? as a living thing vs. a factory).
204
Figure 2: Dictionary Evaluation Results
and for the ensemble (using Basilisk size 100 and
size 400). Table 3 shows the micro & macro average
results across all semantic categories.
Table 3 shows that the dictionaries produced by
the Ensemble w/B100 achieved better results than
the individual methods and co-training with an F
score of 80%. Table 2 shows that the ensemble
achieved better performance than the other methods
for 4 of the 9 classes, and was usually competitive
on the remaining 5 classes. WordNet (W.Net) con-
sistently produced high precision, but with compar-
atively lower recall, indicating that WordNet does
not have sufficient coverage for this domain.
4.6 Analysis
Table 4 shows the performance of our ensemble
when using only 2 of the 3 component methods.
Removing any one method decreases the average
F-measure by at least 3-5%. Component pairs
that include induced lexicons from coreference (ST
Lex+Coref and B100+Coref) yield high precision
but low recall. The component pair ST Lex+B100
produces higher recall but with slightly lower accu-
racy. The ensemble framework boosted recall even
more, while maintaining the same precision.
We observe that some of the smallest lexicons
produced the best results for instance-based seman-
tic tagging (e.g., Organization). Our hypothesis is
that consensus decisions across different methods
helps to promote the acquisition of high frequency
domain words, which are crucial to have in the dic-
tionary. The fact that dictionary look-up performed
better than an instance-based semantic tagger also
suggests that coarse polysemy (different senses that
205
Method Animal Body DisSym Fixed Human Org. Plant & Temp. Trans.
Part Loc. Food Ref. Struct.
P R F P R F P R F P R F P R F P R F P R F P R F P R F
Individual Methods
W.Net 92 88 90 93 59 72 99 77 87 86 58 69 83 55 66 86 44 59 65 79 71 93 85 89 85 64 73
Seeds 100 54 70 92 55 69 100 59 74 95 10 18 100 22 36 100 41 58 100 61 76 100 52 69 100 09 17
B100 99 77 86 94 73 82 100 66 80 96 23 37 96 31 47 91 58 71 82 64 72 68 83 75 67 22 33
B400 94 90 92 51 86 64 100 69 81 97 35 51 91 51 65 79 77 78 46 82 59 49 94 64 83 78 80
Coref 90 67 77 92 55 69 66 83 73 65 46 54 57 50 53 54 68 60 81 61 69 60 74 67 45 09 15
ST Lex 94 89 91 68 77 72 80 91 85 91 74 82 79 43 55 84 62 71 51 68 58 73 91 81 82 49 61
SemTag 91 90 90 52 68 59 77 90 83 91 78 84 81 48 60 80 63 70 43 82 56 77 93 84 83 53 64
Co-Training
pass4 64 76 70 67 73 70 91 79 85 91 39 54 98 44 61 83 69 76 43 68 53 73 94 82 49 36 42
pass20 60 89 71 56 91 69 88 91 90 83 64 72 92 54 68 72 77 74 28 71 40 65 98 78 46 40 43
Ensembles
w/B100 93 94 94 74 77 76 93 81 86 92 73 81 94 55 70 90 78 84 56 89 68 55 94 70 79 75 77
w/B400 94 93 93 65 91 75 96 87 91 89 75 81 92 56 70 79 79 79 47 86 61 53 94 68 63 55 58
Table 2: Instance-based Semantic Tagging Results (P = Precision, R = Recall, F = F-measure)
Method Micro Average Macro Average
P R F P R F
Individual Systems
W.Net 88 66 75 87 68 76
Seeds 99 35 52 99 40 57
B100 89 50 64 88 55 68
B400 77 66 71 77 74 75
Coref 65 59 62 68 57 62
ST Lex 82 72 77 78 72 75
SemTag 80 74 77 75 74 74
Co-Training
pass4 77 61 68 73 64 68
pass20 69 74 71 65 75 70
Ensembles
w/B100 83 77 80 81 80 80
w/B400 79 78 78 75 79 77
Table 3: Micro & Macro Average for Semantic Tagging
cut across semantic classes) is a relatively minor is-
sue within a specialized domain.
5 Conclusions
Our research combined three diverse methods
for semantic lexicon induction in a bootstrapped
ensemble-based framework, including a novel ap-
proach for lexicon induction based on coreference
chains. Our ensemble-based approach performed
better than the individual methods, in terms of
both dictionary accuracy and instance-based seman-
tic tagging. In future work, we believe this ap-
proach could be enhanced further by adding new
types of techniques to the ensemble and by investi-
Method Micro Average Macro Average
P R F P R F
Ensemble with component pairs
ST Lex+Coref 92 59 72 92 57 70
B100+Coref 92 40 56 94 44 60
ST Lex+B100 82 69 75 81 75 77
Ensemble with all components
ST Lex+B100+Coref 83 77 80 81 80 80
Table 4: Ablation Study of the Ensemble Framework for
Semantic Tagging
gating better methods for estimating the confidence
scores from the individual components.
Acknowledgments
We are grateful to Lalindra de Silva for manually
annotating data, Nathan Gilbert for help with Rec-
oncile, and Ruihong Huang for help with the se-
mantic tagger. We gratefully acknowledge the sup-
port of the National Science Foundation under grant
IIS-1018314 and the Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0172. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
DARPA, AFRL, or the U.S. government.
206
References
ACE. 2007. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2007.
ACE. 2008. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2008.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
ANLP-97, pages 194?201.
A. Blum and T. Mitchell. 1998. Combining Labeled and
Unlabeled Data with Co-Training. In Proceedings of
the 11th Annual Conference on Computational Learn-
ing Theory (COLT-98).
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22:249?254, June.
Andrew Carlson, Justin Betteridge, Estevam R. Hr-
uschka Jr., and Tom M. Mitchell. 2009. Coupling
semi-supervised learning of categories and relations.
In HLT-NAACL 2009 Workshop on Semi-Supervised
Learning for NLP.
M. Collins and Y. Singer. 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-99).
S. Cucerzan and D. Yarowsky. 1999. Language Inde-
pendent Named Entity Recognition Combining Mor-
phologi cal and Contextual Evidence. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-99).
J. Curran. 2002. Ensemble Methods for Automatic The-
saurus Extraction. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91?134, June.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proceedings of the
COLING conference, August.
L. Hirschman. 1997. MUC-7 Coreference Task Defini-
tion.
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-08).
T. McIntosh and J. Curran. 2008. Weighted mutual
exclusion bootstrapping for domain independent lex-
icon and template acquisition. In Proceedings of the
Australasian Language Technology Association Work-
shop.
T. McIntosh and J. Curran. 2009. Reducing Semantic
Drift with Bagging and Distributional Similarity. In
Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics.
T. McIntosh. 2010. Unsupervised Discovery of Negative
Categories in Lexicon Bootstrapping. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
V. Ng. 2007. Semantic Class Induction and Coreference
Resolution. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics.
Cheng Niu, Wei Li, Jihong Ding, and Rohini K. Srihari.
2003. A bootstrapping approach to named entity clas-
sification using successive learners. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics (ACL-03), pages 335?342.
M. Pas?ca. 2004. Acquisition of categorized named en-
tities for web search. In Proc. of the Thirteenth ACM
International Conference on Information and Knowl-
edge Management, pages 137?145.
W. Phillips and E. Riloff. 2002. Exploiting Strong Syn-
tactic Heuristics and Co-Training to Learn Semantic
Lexicons. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 125?132.
J. Pustejovsky, P. Hanks, R. Saur??, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics 2003, pages 647?
656.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044?1049. The AAAI Press/MIT Press.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
207
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110?1116.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with Reconcile. In Proceedings of
the ACL 2010 Conference Short Papers, pages 156?
161.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214?221.
V. Vyas and P. Pantel. 2009. Semi-automatic entity set
refinement. In Proceedings of North American Asso-
ciation for Computational Linguistics / Human Lan-
guage Technology (NAACL/HLT-09).
208
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 2?11,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Bootstrapped Learning of Emotion Hashtags #hashtags4you
Ashequl Qadir
School of Computing
University of Utah
Salt Lake City, UT 84112, USA
asheq@cs.utah.edu
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112, USA
riloff@cs.utah.edu
Abstract
We present a bootstrapping algorithm to au-
tomatically learn hashtags that convey emo-
tion. Using the bootstrapping framework, we
learn lists of emotion hashtags from unlabeled
tweets. Our approach starts with a small num-
ber of seed hashtags for each emotion, which
we use to automatically label tweets as initial
training data. We then train emotion classi-
fiers and use them to identify and score candi-
date emotion hashtags. We select the hashtags
with the highest scores, use them to automat-
ically harvest new tweets from Twitter, and
repeat the bootstrapping process. We show
that the learned hashtag lists help to improve
emotion classification performance compared
to an N-gram classifier, obtaining 8% micro-
average and 9% macro-average improvements
in F-measure.
1 Introduction
The increasing popularity of social media has given
birth to new genres of text that have been the
focus of NLP research for applications such as
event discovery (Benson et al, 2011), election out-
come prediction (Tumasjan et al, 2011; Berming-
ham and Smeaton, 2011), user profile classification
(De Choudhury et al, 2012), conversation model-
ing (Ritter et al, 2010), consumer insight discovery
(Chamlertwat et al, 2012), etc. A hallmark of so-
cial media is that people tend to share their personal
feelings, often in publicly visible forums. As a re-
sult, social media has also been the focus of NLP
research on sentiment analysis (Kouloumpis et al,
2011), emotion classification and lexicon generation
(Mohammad, 2012), and sarcasm detection (Davi-
dov et al, 2010). Identifying emotion in social me-
dia text could be beneficial for many application ar-
eas, for example to help companies understand how
people feel about their products, to assist govern-
ments in recognizing growing anger or fear associ-
ated with an event, and to help media outlets under-
stand the public?s emotional response toward con-
troversial issues or international affairs.
Twitter, a micro-blogging platform, is particularly
well-known for its use by people who like to in-
stantly express thoughts within a limited length of
140 characters. These status updates, known as
tweets, are often emotional. Hashtags are a distinc-
tive characteristic of tweets, which are a community-
created convention for providing meta-information
about a tweet. Hashtags are created by adding the ?#?
symbol as a prefix to a word or a multi-word phrase
that consists of concatenated words without whites-
pace (e.g., #welovehashtags). People use hashtags
in many ways, for example to represent the topic of
a tweet (e.g., #graduation), to convey additional in-
formation (e.g., #mybirthdaytoday), or to express an
emotion (e.g., #pissedoff).
The usage of hashtags in tweets is common, as
reflected in the study of a sample of 0.6 million
tweets by Wang et al (2011) which found that
14.6% of tweets in their sample had at least one
hashtag. In tweets that express emotion, it is com-
mon to find hashtags representing the emotion felt
by the tweeter, such as ?the new iphone is a waste
of money! nothing new! #angry? denoting anger or
?buying a new sweater for my mom for her birthday!
#loveyoumom? denoting affection.
2
Identifying the emotion conveyed by a hashtag
has not yet been studied by the natural language pro-
cessing community. The goal of our research is to
automatically identify hashtags that express one of
five emotions: affection, anger/rage, fear/anxiety,
joy, or sadness/disappointment. The learned hash-
tags are then used to recognize tweets that express
one of these emotions. We use a bootstrapping ap-
proach that begins with 5 seed hashtags for each
emotion class and iteratively learns more hashtags
from unlabeled tweets. We show that the learned
hashtags can accurately identify tweets that convey
emotion and yield additional coverage beyond the
recall of an N-gram classifier.
The rest of the paper is divided into the following
sections. In Section 2, we present a brief overview
of previous research related to emotion classification
in social media and the use of hashtags. In Sec-
tion 3, we describe our bootstrapping approach for
learning lists of emotion hashtags. In Section 4 we
discuss the data collection process and our experi-
mental design. In Section 5, we present the results
of our experiments. Finally, we conclude by sum-
marizing our findings and presenting directions for
future work.
2 Related Work
Recognizing emotions in social media texts has
grown popular among researchers in recent years.
Roberts et al (2012) investigated feature sets to clas-
sify emotions in Twitter and presented an analysis
of different linguistic styles people use to express
emotions. The research of Kim et al (2012a) is fo-
cused on discovering emotion influencing patterns
to classify emotions in social network conversations.
Esmin et al (2012) presented a 3-level hierarchi-
cal emotion classification approach by differentiat-
ing between emotion vs. non-emotion text, positive
vs. negative emotion, and then classified different
emotions. Yang et al (2007b) investigated sentence
contexts to classify emotions in blogs at the doc-
ument level. Some researchers have also worked
on analyzing the correlation of emotions with topics
and trends. Kim et al (2012b) analyzed correlations
between topics and emotions in Twitter using topic
modeling. Gilbert and Karahalios (2010) analyzed
correlation of anxiety, worry and fear with down-
ward trends in the stock market. Bollen et al (2011)
modeled public mood and emotion by creating six-
dimensional mood vectors to correlate with popular
events that happened in the timeframe of the dataset.
On the other hand, researchers have recently
started to pay attention to the hashtags of tweets, but
mostly to use them to collect labeled data. Davi-
dov et al (2010) used #sarcasm to collect sarcastic
tweets from twitter. Choudhury et al (2012) used
hashtags of 172 mood words to collect training data
to find associations between mood and human af-
fective states, and trained classifiers with unigram
and bigram features to classify these states. Purver
and Battersby (2012) used emotion class name hash-
tags and emoticons as distant supervision in emotion
classification. Mohammad (2012) also used emotion
class names as hashtags to collect labeled data from
Twitter, and used these tweets to generate emotion
lexicons. Wang et al (2012) used a selection of emo-
tion hashtags as the means to acquire labeled data
from twitter, and found that a combination of uni-
grams, bigrams, sentiment/emotion-bearing words,
and parts-of-speech information to be the most ef-
fective in classifying emotions. A study by Wang
et al (2012) also shows that hashtags can be used
to create a high quality emotion dataset. They found
about 93.16% of the tweets having emotion hashtags
were relevant to the corresponding emotion.
However, none of this work investigated the use
of emotion hashtag lists to help classify emotions in
tweets. In cases where hashtags were used to collect
training data, the hashtags were manually selected
for each emotion class. In many cases, only the
name of the emotion classes were used for this pur-
pose. The work most closely related to our research
focus is the work of Wang et al (2011) where they
investigated several graph based algorithms to col-
lectively classify hashtag sentiments. However, their
work is focused on classifying hashtags of positive
and negative sentiment polarities, and they made use
of sentiment polarity of the individual tweets to clas-
sify hashtag sentiments. On the contrary, we learn
emotion hashtags and use the learned hashtag lists
to classify emotion tweets. To the best of our knowl-
edge, we are the first to present a bootstrapped learn-
ing framework to automatically learn emotion hash-
tags from unlabeled data.
3
3 Learning Emotion Hashtags via
Bootstrapping
3.1 Motivation
The hashtags that people use in tweets are often very
creative. While it is common to use just single word
hashtags (e.g., #angry), many hashtags are multi-
word phrases (e.g., #LoveHimSoMuch). People also
use elongated1 forms of words (e.g., #yaaaaay,
#goawaaay) to put emphasis on their emotional
state. In addition, words are often spelled creatively
by replacing a word with a number or replacing
some characters with phonetically similar characters
(e.g., #only4you, #YoureDaBest). While many of
these hashtags convey emotions, these stylistic vari-
ations in the use of hashtags make it very difficult
to create a repository of emotion hashtags manu-
ally. While emotion word lexicons exist (Yang et al,
2007a; Mohammad, 2012), and adding a ?#? symbol
as a prefix to these lexicon entries could potentially
give us lists of emotion hashtags, it would be un-
likely to find multi-word phrases or stylistic varia-
tions frequently used in tweets. This drives our mo-
tivation to automatically learn hashtags that are com-
monly used to express emotion in tweets.
3.2 Emotion Classes
For this research, we selected 5 prominent emo-
tion classes that are frequent in tweets: Af-
fection, Anger/Rage, Fear/Anxiety, Joy and Sad-
ness/Disappointment. We started by analyzing Par-
rott?s (Parrott, 2001) emotion taxonomy and how
these emotions are expressed in tweets. We also
wanted to ensure that the selected emotion classes
would have minimal overlap with each other. We
took Parrott?s primary emotion Joy and Fear2 di-
rectly. We merged Parrott?s secondary emotion Af-
fection and Lust into our Affection class and merged
Parrott?s secondary emotion Sadness and Disap-
pointment into our Sadness/Disappointment class,
since these emotions are often difficult to distinguish
from each other. Lastly, we mapped Parrott?s sec-
ondary emotion Rage to our Anger/Rage class di-
rectly. There were other emotions in Parrott?s tax-
onomy such as Surprise, Neglect, etc. that we did
1This feature has also been found to have a strong associa-
tion with sentiment polarities (Brody and Diakopoulos, 2011)
2we renamed the Fear class as Fear/Anxiety
not use for this research. In addition to the five emo-
tion classes, we used a None of the Above class for
tweets that do not carry any emotion or that carry an
emotion other than one of our five emotion classes.
3.3 Overview of Bootstrapping Framework
Figure 1: Bootstrapping Architecture
Figure 1 presents the framework of our bootstrap-
ping algorithm for learning emotion hashtags. The
algorithm runs in two steps. In the first step, the
bootstrapping process begins with five manually de-
fined ?seed? hashtags for each emotion class. For
each seed hashtag, we search Twitter for tweets that
contain the hashtag and label these tweets with the
emotion class associated with the hashtag. We use
these labeled tweets to train a supervised N-gram
classifier for every emotion e ? E, where E is the
set of emotion classes we are classifying.
In the next step, the emotion classifiers are applied
to a large pool of unlabeled tweets and we collect
the tweets that are labeled by the classifiers. From
these labeled tweets, we extract the hashtags found
in these tweets to create a candidate pool of emo-
tion hashtags. The hashtags in the candidate pool
are then scored and ranked and we select the most
highly ranked hashtags to add to a hashtag reposi-
tory for each emotion class.
Finally, we then search for tweets that contain
the learned hashtags in a pool of unlabeled tweets
and label each of these with the appropriate emotion
class. These newly labeled tweets are added to the
4
set of training instances. The emotion classifiers are
retrained using the larger set of training instances,
and the bootstrapping process continues.
3.4 Seeding
For each of the 5 emotion classes, we manually
selected 5 seed hashtags that we determined to be
strongly representative of the emotion. Before col-
lecting the initial training tweets containing the seed
hashtags, we manually searched in Twitter to en-
sure that these seed hashtags are frequently used by
tweeters. Table 1 presents our seed hashtags.
Emotion Classes Seed Hashtags
AFFECTION #loveyou, #sweetheart, #bff
#romantic, #soulmate
ANGER & #angry, #mad, #hateyou
RAGE #pissedoff, #furious
FEAR & #afraid, #petrified, #scared
ANXIETY #anxious, #worried
JOY #happy, #excited, #yay
#blessed, #thrilled
SADNESS & #sad, #depressed
DISAPPOINT- #disappointed, #unhappy
MENT #foreveralone
Table 1: Seed Emotion Hashtags
3.5 N-gram Tweet Classifier
The tweets acquired using the seed hashtags are used
as training instances to create emotion classifiers
with supervised learning. We first pre-process the
training instances by tokenizing the tweets with a
freely available tokenizer for Twitter (Owoputi et
al., 2013). Although it is not uncommon to express
emotion states in tweets with capitalized characters
inside words, the unique writing styles of the tweet-
ers often create many variations of the same words
and hashtags. We, therefore, normalized case to en-
sure generalization.
We trained one logistic regression classifier for
each emotion class. We chose logistic regression
as the classification algorithm because it produces
probabilities along with each prediction that we later
use to assign scores to candidate emotion hashtags.
As features, we used unigrams to represent all of
the words and hashtags in a tweet, but we removed
the seed hashtags that were used to select the tweets
(or the classifier would simply learn to recognize the
seed hashtags). Our hypothesis is that the seed hash-
tag will not be the only emotion indicator in a tweet,
most of the time. The goal is for the classifier to
learn to recognize words and/or additional hashtags
that are also indicative of the emotion. Additionally,
we removed from the feature set any user mentions
(by looking for words with ?@? prefix). We also re-
moved any word or hashtag from the feature set that
appeared only once in the training data.
For emotion e, we used the tweets containing
seed hashtags for e as the positive training instances
and the tweets containing hashtags for the other
emotions as negative instances. However, we also
needed to provide negative training instances that
do not belong to any of the 5 emotion classes. For
this purpose, we added 100,000 randomly collected
tweets to the training data. While it is possible that
some of these tweets are actually positive instances
for e, our hope is that the vast majority of them will
not belong to emotion e.
We experimented with feature options such as bi-
grams, unigrams with the ?#? symbol stripped off
from hashtags, etc., but the combination of unigrams
and hashtags as features worked the best. We used
the freely available java version of the LIBLINEAR
(Fan et al, 2008) package with its default parameter
settings for logistic regression.
3.6 Learning Emotion Hashtags
The next step is to learn emotion hashtags. We apply
the emotion classifiers to a pool of unlabeled tweets
and collect all of the tweets that the classifier can
label. For each emotion e ? E, we first create a can-
didate pool of emotion hashtags He, by collecting
all of the hashtags in the labeled tweets for emotion
e. To limit the size of the candidate pool, we dis-
carded hashtags with just one character or more than
20 characters, and imposed a frequency threshold of
10. We then score these hashtags to select the top N
emotion hashtags we feel most confident about.
To score each candidate hashtag h ? He, we com-
pute the average of the probabilities assigned by the
logistic regression classifier to all the tweets con-
taining hashtag h. We expect the classifier to as-
sign higher probabilities only to tweets it feels confi-
dent about. Therefore, if h conveys e, we expect that
5
the average probability of all the tweets containing
h will also be high. We select the top 10 emotion
hashtags for each emotion class e, and add them to
our list of learned hashtags for e.
3.7 Adding New Training Instances for
Bootstrapping
To facilitate the next stage of bootstrapping, we col-
lect all tweets from the unlabeled data that contain
hashtag h and label them with the emotion associ-
ated with h. By adding more training instances, we
expect to provide the classifiers with new tweets that
will contain a potentially more diverse set of words
that the classifiers can consider in the next stage of
the bootstrapping.
When the new tweets are added to the training set,
we remove the hashtags from them that we used for
labelling to avoid bias, and the bootstrapping pro-
cess continues. We ran the bootstrapped learning for
100 iterations. Since we learned 10 hashtags during
each iteration, we ended up with emotion hashtag
lists consisting of 1000 hashtags for each emotion.
4 Experimental Setup
4.1 Data Collection
To collect our initial training data, we searched Twit-
ter for the seed hashtags mentioned in Section 3.4
using Twitter?s Search API3 over a period of time.
To ensure that the collected tweets are written in En-
glish, we used a freely available language recognizer
trained for tweets (Carter et al, 2013). We filtered
out tweets that were marked as re-tweets using #rt or
beginning with ?rt?4 because re-tweets are in many
cases exactly the same or very similar to the origi-
nal. We also filtered out any tweet containing a URL
because if such a tweet contains emotion, it is pos-
sible that the emotion indicator may be present only
on the linked website (e.g., a link to a comic strip
followed by an emotion hashtag). After these filter-
ing steps, we ended up with a seed labeled training
dataset of 325,343 tweets.
In addition to the seed labeled data, we collected
random tweets using Twitter?s Streaming API5 over
a period of time to use as our pool of unlabeled
3https://dev.twitter.com/docs/api/1/get/search
4a typical convention to mark a tweet as a re-tweet
5https://dev.twitter.com/docs/streaming-apis
tweets. Like the training data, we filtered out re-
tweets and tweets containing a URL as well as
tweets containing any of the seed hashtags. Since
our research focus is on learning emotion hashtags,
we also filtered out any tweet that did not have at
least one hashtag. After filtering, we ended up with
roughly 2.3 million unlabeled tweets.
4.2 Test Data
Since manual annotation is time consuming, to en-
sure that many tweets in our test data have at least
one of our 5 emotions, we manually selected 25
topic keywords/phrases6 that we considered to be
strongly associated with emotions, but not neces-
sarily any specific emotion. We then searched in
Twitter for any of these topic phrases and their cor-
responding hashtags. These 25 topic phrases are:
Prom, Exam, Graduation, Marriage, Divorce, Hus-
band, Wife, Boyfriend, Girlfriend, Job, Hire, Laid
Off, Retirement, Win, Lose, Accident, Failure, Suc-
cess, Spider, Loud Noise, Chest Pain, Storm, Home
Alone, No Sleep and Interview. Since the purpose of
collecting these tweets is to evaluate the quality and
coverage of the emotion hashtags that we learn, we
filtered out any tweet that did not have at least one
hashtag (other than the topic hashtag).
To annotate tweets with respect to emotion, two
annotators were given definitions of the 5 emotion
classes from Collins English Dictionary7, Parrott?s
(Parrott, 2001) emotion taxonomy of these 5 emo-
tions and additional annotation guidelines. The an-
notators were instructed to label each tweet with up
to two emotions. The instructions specified that the
emotion must be felt by the tweeter at the time the
tweet was written. After several trials and discus-
sions, the annotators reached a satisfactory agree-
ment level of 0.79 Kappa (?) (Carletta, 1996). The
annotation disagreements in these 500 tweets were
then adjudicated, and each annotator labeled an ad-
ditional 2,500 tweets. Altogether this gave us an
emotion annotated dataset of 5,500 tweets. We ran-
domly separated out 1,000 tweets from this collec-
tion as a tuning set, and used the remaining 4,500
tweets as evaluation data.
In Table 2, we present the emotion distribution in
6This data collection process is similar to the emotion tweet
dataset creation by Roberts et al (2012)
7http://www.collinsdictionary.com/
6
tweets that were labeled using the seed hashtags in
the second column. In the next column, we present
the emotion distribution in the tweets that were an-
notated for evaluation by the human annotators.
Emotion Tweets with Evaluation
Seed Hashtags Tweets
AFFECTION 14.38% 6.42%
ANGER/RAGE 14.01% 8.91%
FEAR/ANXIETY 11.42% 13.16%
JOY 37.47% 22.33%
SADNESS/ 23.69% 12.45%
DISAPPOINTMENT
NONE OF THE ABOVE - 42.38%
Table 2: Distribution of emotions in tweets with seed
hashtags and evaluation tweets
4.3 Evaluating Emotion Hashtags
For comparison, we trained logistic regression clas-
sifiers with word unigrams and hashtags as features
for each emotion class, and performed 10-fold cross-
validation on the evaluation data. As a second base-
line for comparison, we added bigrams to the feature
set of the classifiers.
To decide on the optimum size of the lists for
each emotion class, we performed list lookup on the
tuning data that we had set aside before evaluation.
For any hashtag in a tweet in the tuning dataset, we
looked up that hashtag in our learned lists, and if
found, assigned the corresponding emotion as the
label for that tweet. We did this experiment starting
with only seeds in our lists, and incrementally in-
creased the sizes of the lists by 50 hashtags at each
experiment. We decided on the optimum size based
on the best F-measure obtained for each emotion
class. In Table 3, we show the list sizes we found to
achieve the best F-measure for each emotion class in
the tuning dataset.
Emotion List Sizes
AFFECTION 500
ANGER/RAGE 1000
FEAR/ANXIETY 850
JOY 1000
SADNESS/DISAPPOINTMENT 400
Table 3: Optimum list sizes decided from tuning dataset
To use the learned lists of emotion hashtags for
classifying emotions in tweets, we first used them as
features for the logistic regression classifiers. We
created 5 list features with binary values, one for
each emotion class. Whenever a tweet in the evalua-
tion data contained a hashtag from one of the learned
emotion hashtags lists, we set the value of that list
feature to be 1, and 0 otherwise. We used these
5 new features in addition to the word unigrams
and hashtag features, and evaluated the classification
performance of the logistic regression classifiers in
a 10-fold cross-validation setup by calculating pre-
cision, recall and F-measure.
Since the more confident hashtags are added to
the lists at the beginning stages of bootstrapping, we
also tried creating subsets from each list by group-
ing hashtags together that were learned after each 5
iterations of bootstrapping (50 hashtags in each sub-
set). We then created 20 list subset features for each
emotion with binary values, yielding 100 additional
features in total. We also evaluated this feature rep-
resentation of the hashtag lists in a 10-fold cross-
validation setup.
As a different approach, we also used the lists in-
dependently from the logistic regression classifiers.
For any hashtag in the evaluation tweets, we looked
up the hashtag in our learned lists. If the hashtag was
found, we assigned the corresponding emotion class
label to the tweet containing the hashtag. Lastly,
we combined the list lookup decisions with the de-
cisions of the baseline logistic regression classifiers
by taking a union of the decisions, i.e., if either as-
signed an emotion to a tweet, we assigned that emo-
tion as the label for the tweet. We present the results
of these different approaches in Section 5.
5 Results and Analysis
Table 4 shows the precision, recall and F-measure
of the N-gram classifier as well as several different
utilizations of the learned hashtag lists. The first and
the second row in Table 4 correspond to the results
for the baseline unigram classifier (UC) alone and
when bigrams are added to the feature set. These
baseline classifiers had low recall for most emotion
classes, suggesting that the N-grams and hashtags
are not adequate as features to recognize the emotion
classes.
Results of using the hashtag lists as 5 additional
features for the classifier are shown in the third row
7
Affection Anger Fear Joy Sadness
Evaluation Rage Anxiety Disappointment
P R F P R F P R F P R F P R F
Baseline Classifiers
Unigram Classifier (UC) 67 43 52 51 19 28 63 33 43 65 48 55 57 29 39
UC + Bigram Features 70 38 50 52 15 23 64 29 40 65 45 53 57 25 34
Baseline Classifier with List Features
UC + List Features 71 49 58 56 28 37 67 41 51 66 50 57 61 34 44
UC + List Subset Features 73 45 56 58 23 33 69 38 49 66 48 55 61 32 42
List Lookup
Seed Lookup 94 06 11 75 01 03 100 06 11 93 04 08 81 02 05
List Lookup 73 40 52 59 25 35 61 36 45 70 16 26 80 17 28
Baseline Classifier with List Lookup
UC ? Seed Lookup 68 45 54 52 21 30 63 33 44 66 49 56 58 31 40
UC ? List Lookup 63 60 61 52 38 44 56 53 54 64 54 59 59 38 46
Table 4: Emotion classification result (P = Precision, R = Recall, F = F-measure)
of Table 4. The hashtag lists consistently improve
precision and recall across all five emotions. Com-
pared to the unigram classifier, F-measure improved
by 6% for AFFECTION, by 9% for ANGER/RAGE,
by 8% for FEAR/ANXIETY, by 2% for JOY, and by
5% for SADNESS/DISAPPOINTMENT. The next
row presents the results when the list subset fea-
tures were used. Using this feature representation
as opposed to using each list as a whole shows pre-
cision recall tradeoff as the classifier learns to rely
on the subsets of hashtags that are good, resulting in
improved precision for several emotion classes, but
recognizes emotions in fewer tweets, which resulted
in less recall.
The fifth and the sixth rows of Table 4 show re-
sults of list lookup only. As expected, seed lookup
recognizes emotions in tweets with high precision,
but does not recognize the emotions in many tweets
because the seed lists have only 5 hashtags per emo-
tion class. Comparatively, using learned hashtag
lists shows substantial improvement in recall as the
learned lists contain a lot more emotion hashtags
than the initial seeds.
Finally, the last two rows of Table 4 show classi-
fication performance of taking the union of the de-
cisions made by the unigram classifier and the deci-
sions made by matching against just the seed hash-
tags or the lists of learned hashtags. The union with
the seed hashtags lookup shows consistent improve-
ment across all emotion classes compared to the un-
igram baseline but the improvements are small. The
Evaluation Micro Macro
Average Average
P R F P R F
Baseline Classifiers
Unigram Classifier (UC) 62 37 46 61 34 44
UC + Bigram Features 63 33 43 62 30 41
Baseline Classifier with List Features
UC + List Features 65 42 51 64 40 49
UC + List Subset Features 66 39 49 65 37 48
List Lookup
Seed Lookup 93 04 08 89 04 08
List Lookup 67 24 35 68 27 38
Baseline Classifier with List Lookup
UC ? Seed Lookup 63 38 47 61 36 45
UC ? List Lookup 60 49 54 59 49 53
Table 5: Micro and Macro averages
union with the lookup in the learned lists of emo-
tion hashtags shows substantial recall gains. This
approach improves recall over the unigram baseline
by 17% for AFFECTION, 19% for ANGER/RAGE,
20% for FEAR/ANXIETY, 6% for JOY, and 9% for
SADNESS/DISAPPOINTMENT. At the same time,
we observe that despite this large recall gain, preci-
sion is about the same or just a little lower. As a re-
sult, we observe an overall F-measure improvement
of 9% for AFFECTION, 16% for ANGER/RAGE,
11% for FEAR/ANXIETY, 4% for JOY, and 7% for
SADNESS/DISAPPOINTMENT.
Table 5 shows the overall performance improve-
ment of the classifiers, averaged across all five emo-
tion classes, measured as micro and macro aver-
8
AFFECTION ANGER FEAR JOY SADNESS
RAGE ANXIETY DISAPPOINT-
MENT
#youthebest #godie #hatespiders #thankinggod #catlady
#yourthebest #donttalktome #freakedout #thankyoulord #buttrue
#hyc #fuckyourself #creepedout #thankful #singleprobs
#yourethebest #getoutofmylife #sinister #superexcited #singleproblems
#alwaysandforever #irritated #wimp #tripleblessed #lonelytweet
#missyou #pieceofshit #shittingmyself #24hours #lonely
#loveyoumore #ruinedmyday #frightened #ecstatic #crushed
#loveyoulots #notfriends #paranoid #happyme #lonerproblems
#thanksforeverything #yourgross #haunted #lifesgood #unloved
#flyhigh #madtweet #phobia #can?twait #friendless
#comehomesoon #stupidbitch #shittingbricks #grateful #singlepringle
#yougotthis #sofuckingannoying #hateneedles #goodmood #brokenheart
#missyoutoo #annoyed #biggestfear #superhappy #singleforever
#youdabest #fuming #worstfear #missedthem #nosociallife
#otherhalf #wankers #concerned #greatmood #teamnofriends
#youramazing #asshole #waitinggame #studio #foreverugly
#cutiepie #dontbothermewhen #mama #tgfl #nofriends
#bestfriendforever #fu #prayforme #exicted #leftout
#alwayshereforyou #fuckyou #nightmares #smiles #singleforlife
#howimetmybestfriend #yousuck #baddriver #liein #:?(
Table 6: Top 20 hashtags learned for each emotion class
age precision, recall and F-measure scores. We see
both types of feature representations of the hashtag
lists improve precision and recall across all emo-
tion classes over the N-gram classifier baselines.
Using the union of the classifier and list lookup,
we see a 12% recall gain with only 2% precision
drop in micro-average over the unigram baseline,
and 15% recall gain with only 2% precision drop
in macro-average. As a result, we see an overall
8% micro-average F-measure improvement and 9%
macro-average F-measure improvement.
In Table 6, we show the top 20 hashtags learned
in each emotion class by our bootstrapped learning.
While many of these hashtags express emotion, we
also notice a few hashtags representing reasons (e.g.,
#baddriver in FEAR/ANXIETY) that are strongly
associated with the corresponding emotion, as well
as common misspellings (e.g., #exicted in JOY).
6 Conclusions
In this research we have presented a bootstrapped
learning framework to automatically learn emotion
hashtags. Our approach makes use of supervi-
sion from seed hashtag labeled tweets, and through
a bootstrapping process, iteratively learns emotion
hashtags. We have experimented with several ap-
proaches to use the lists of emotion hashtags for
emotion classification and have found that the hash-
tag lists consistently improve emotion classification
performance in tweets. In future research, since our
bootstrapped learning approach does not rely on any
language specific techniques, we plan to learn emo-
tion hashtags in other prominent languages such as
Spanish, Portuguese, etc.
7 Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Interior National Business Center (DoI
/ NBC) contract number D12PC00285. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
DoI/NBE, or the U.S. Government.
9
References
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 389?398.
Adam Bermingham and Alan Smeaton. 2011. On using
twitter to monitor political sentiment and predict elec-
tion results. In Proceedings of the Workshop on Sen-
timent Analysis where AI meets Psychology (SAAIP
2011), pages 2?10.
Johan Bollen, Huina Mao, and Alberto Pepe. 2011.
Modeling public mood and emotion: Twitter sentiment
and socio-economic phenomena. In Proceedings of
the Fifth International Conference on Weblogs and So-
cial Media.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 562?570.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22:249?254, June.
S. Carter, W. Weerkamp, and E. Tsagkias. 2013. Mi-
croblog language identification: Overcoming the limi-
tations of short, unedited and idiomatic text. Language
Resources and Evaluation Journal, 47(1).
Wilas Chamlertwat, Pattarasinee Bhattarakosol, Tip-
pakorn Rungkasiri, and Choochart Haruechaiyasak.
2012. Discovering consumer insight from twitter via
sentiment analysis. Journal of Universal Computer
Science, 18(8):973?992, apr.
Munmun De Choudhury, Michael Gamon, and Scott
Counts. 2012. Happy, nervous or surprised? classi-
fication of human affective states in social media. In
Proceedings of the Sixth International Conference on
Weblogs and Social Media.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 107?116.
Munmun De Choudhury, Nicholas Diakopoulos, and Mor
Naaman. 2012. Unfolding the event landscape on
twitter: classification and exploration of user cate-
gories. In Proceedings of the ACM 2012 conference on
Computer Supported Cooperative Work, CSCW ?12,
pages 241?244.
Ahmed Ali Abdalla Esmin, Roberto L. De Oliveira Jr.,
and Stan Matwin. 2012. Hierarchical classification
approach to emotion recognition in twitter. In Pro-
ceedings of the 11th International Conference on Ma-
chine Learning and Applications, ICMLA, Boca Ra-
ton, FL, USA, December 12-15, 2012. Volume 2, pages
381?385. IEEE.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. J. Mach. Learn. Res.,
9:1871?1874, June.
Eric Gilbert and Karrie Karahalios. 2010. Widespread
worry and the stock market. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media.
Suin Kim, JinYeong Bak, and Alice Oh. 2012a. Discov-
ering emotion influence patterns in online social net-
work conversations. SIGWEB Newsl., (Autumn):3:1?
3:6, September.
Suin Kim, JinYeong Bak, and Alice Oh. 2012b. Do you
feel what i feel? social aspects of emotions in twitter
conversations. In International AAAI Conference on
Weblogs and Social Media.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In Proceedings of the Fifth In-
ternational Conference on Weblogs and Social Media.
Saif Mohammad. 2012. #emotional tweets. In *SEM
2012: The First Joint Conference on Lexical and Com-
putational Semantics, pages 246?255.
Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL-2013).
W. Gerrod Parrott, editor. 2001. Emotions in Social Psy-
chology. Psychology Press.
Matthew Purver and Stuart Battersby. 2012. Experi-
menting with distant supervision for emotion classi-
fication. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, EACL ?12, pages 482?491.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 172?180.
Kirk Roberts, Michael A. Roach, Joseph Johnson, Josh
Guthrie, and Sanda M. Harabagiu. 2012. Empatweet:
Annotating and detecting emotions on twitter. In Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 3806?3813. ACL Anthology Identifier: L12-
1059.
10
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2011. Election fore-
casts with twitter: How 140 characters reflect the po-
litical landscape. Social Science Computer Review,
29(4):402?418, November.
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou,
and Ming Zhang. 2011. Topic sentiment analysis in
twitter: a graph-based hashtag sentiment classification
approach. In Proceedings of the 20th ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ?11, pages 1031?1040.
Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P. Sheth. 2012. Harnessing twitter ?big
data? for automatic emotion identification. In Pro-
ceedings of the 2012 ASE/IEEE International Confer-
ence on Social Computing and 2012 ASE/IEEE In-
ternational Conference on Privacy, Security, Risk and
Trust, SOCIALCOM-PASSAT ?12, pages 587?592.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi
Chen. 2007a. Building emotion lexicon from weblog
corpora. In Proceedings of the 45th Annual Meeting
of the ACL on Interactive Poster and Demonstration
Sessions, ACL ?07, pages 133?136.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi
Chen. 2007b. Emotion classification using web blog
corpora. In Proceedings of the IEEE/WIC/ACM In-
ternational Conference on Web Intelligence, WI ?07,
pages 275?278.
11
