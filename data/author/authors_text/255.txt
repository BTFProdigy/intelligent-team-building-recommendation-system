Development of a machine learnable discourse tagging tool
Masahiro Araki?, Yukihiko Kimura?, Takuya Nishimoto?, and Yasuhisa Niimi?
?Department of Electronics and Information Science
Kyoto Institute of Technology
Matsugasaki Sakyo-ku Kyoto 606-8585, Japan
{araki,kimu,nishi,niimi}@dj.kit.ac.jp
Abstract
We have developed a discourse level
tagging tool for spoken dialogue cor-
pus using machine learning meth-
ods. As discourse level informa-
tion, we focused on dialogue act, rel-
evance and discourse segment. In
dialogue act tagging, we have im-
plemented a transformation-based
learning procedure and resulted in
70% accuracy in open test. In
relevance and discourse segment
tagging, we have implemented a
decision-tree based learning proce-
dure and resulted in about 75% and
72% accuracy respectively.
1 Introduction
In dialogue research communities, the need of
dialogue corpora with various level of anno-
tation is recognized. However, creating an-
notated dialogue corpora needs considerable
cost in recording, transcribing, annotating,
and checking the consistency and reliability
of the annotated data.
Considering such situation, we focused on
annotation step and developed discourse level
tagging tool for spoken dialogue corpus using
machine learning methods. In this paper, we
explain the detail of tagging scheme and de-
scribe machine learning algorithm suitable for
each level of tagging.
2 Multiple level tagging scheme
for Japanese dialogue
It is widely recognized that making annotated
spoken dialogue corpora is labor-intensive ef-
fort. To this end, the Discourse Research
Initiative (DRI) was set up in March of
1996 by US, European, and Japanese re-
searchers to develop standard discourse anno-
tation schemes (Carletta et al, 1997; Core et
al., 1998). In line with the effort of this initia-
tive, Japanese Discourse Research Initiative
has started and created annotation scheme for
various level of information of dialogue, that
is JDTAG (Japanese Dialogue TAG) (JDRI,
2000). Our aim is to develop tagging tools in
line with the JDTAG.
In the following of this section, we explain
the part of tagging scheme which are relevant
to our tools.
2.1 Dialogue act
In JDTAG, slash unit is defined following
Meteer and Taylor (Meteer and Taylor, 1995).
Dialogue act tagging scheme is a set of rules to
identify the function of each slash unit from
the viewpoint of speech act theory (Searle,
1969) and discourse analysis (Coulthhard,
1992; Stenstrom, 1994). These dialogue act
tag reflect a local structure of the dialogue.
To improve the agreement score among the
annotators, we assume basic structure of dia-
logue shown in Figure 1.
Typical exchange pattern is shown in Fig-
ure 2.
In this scheme, the tags (Figure 3) need to
be an element of exchange structure except
for those of dialogue management.
2.2 Relevance
Dialogue act tag can be regarded as a function
of utterance. Therefore, we can see the se-
quence of dialogue act tag as the flat structure
of the dialogue. It is insufficient to express
? Task-orientedDialogue ? (Opening) ProblemSolving (Closing)
? ProblemSolving ? Exchange+
? Exchange ? Initiate (Response)/Initiate* (Response)* (FollowUp) (FollowUp)
?*?:repeat more than 0 time!$?+?:repeat more than 1 time, ( ): the element can be omitted.
Figure 1: Exchange structure
-------------------------------------------
(I) 0041 A: chikatetsu no ekimei ha?
(What?s the name of the subway station?)
(R) 0042 B: chikatetsy no teramachi
eki ni narimasu.
(The subway station is Teramachi.)
(F) 0043 A: hai.
(I see.)
-------------------------------------------
I: Initiate, R: Response, F:Follow-up
Figure 2: Typical exchange pattern
? Dialogue management
Open, Close
? Initiate
Request, Suggest, Persuade, Propose,
Confirm, Yes-no question, Wh-question,
Promise, Demand, Inform,
Other assert, Other initiate.
? Response
Positive, Negative, Answer, Other response.
? Follow up
Understand
? Response with Initiate
The element of this category is represented as
Response Type / Initiate Type.
Figure 3: Tag set of dialogue act
tree-like structure, such as embedded subdia-
logue. In order to represent such higher level
information, we use a relevance tag.
There are two types of relevance between
slash units. The one is the relevance of the
inside of exchange. The other is the relevance
of between neighboring exchanges. We call
the former one as relevance type 1, and the
latter one as relevance type 2.
Relevance type 1 represents the relation of
initiate utterance and its response utterance
by showing the ID number of the initiate ut-
terance at the response utterance. By us-
ing this tag, the initiate-response pair which
strides over embedded subdialogue can be
grasped.
Relevance type 2 represents the meso-
structure of the dialogue such as chaining,
coupling, elliptical coupling as introduced in
(Stenstrom, 1994). Chaining is a pattern of
[A:I B:R] [A:I B:R] (speaker A initiates the
exchange and speaker B responds it). Cou-
pling is a pattern of [A:I B:R] [B:I A:R].
Elliptical coupling is a pattern of [A:I] [B:I
A:R] which omits the response in the first ex-
change. Relevance type 2 tag is attached to
the each initiate response in showing whether
such meso-level dialogue structure can be ob-
served (yes) or not (no).
The follow-up utterance has no relevance
tag. It is because follow-up necessarily has a
relevance to the preceded response utterance.
The example of dialogue act tagging (first
element of tag) and relevance tagging (second
element) is shown Figure 4.
----------------------------------------
[<Yes-no question> <relevance no>]
0027 A: hatsuka no jyuuji kara
ha aite irun de syou ka
(Is it available from 10 at 20th?)
[<Yes-no question> <relevance yes>]
0028 B: kousyuu shitsu desu ka?
(Are you mentioning the seminar room?)
[<Positive> <0028>]
0029 A: hai
(Yes.)
[<Negative> <0027>]
0030 B: hatsuka ha aite orimasen
(It is not available in 20th.)
[<Understand>]
0031 A: soudesu ka
(OK.)
----------------------------------------
Figure 4: An example dialogue with the dia-
logue act and relevance tags
----------------------------------------
[2: room for a lecture: ]
38 A: {F e} heya wa dou simashou ka?
(How about meeting room?)
[1: small-sized meeting room: clarification]
39 B: heya wa shou-kaigishitsu wa aite masu ka?
(Can I use the small-sized meeting room?)
40 A: {F to} kayoubi no {F e} 14 ji han kara
wa {F e} shou-kaigisitsu wa aite imasen
(The small meeting room is not available
from 14:30 on Tuesday.)
[1:the large-sized meeting room: ]
41 A: dai-kaigishitsu ga tukae masu
(You can use the large meeting room.)
[1: room for a lecture: return]
42 B: {D soreja} dai-kaigishitsu de onegai
shimasu
(Ok. Please book the large meeting room.)
----------------------------------------
[TBI:topic name:segment relation]
Figure 5: An example dialogue with the dia-
logue segment tags
2.3 Dialogue segment
Dialogue segment of JDTAG indicates bound-
ary of discourse segment introduced in (Grosz
and Sidner, 1986). A dialogue segment is
identified based on the exchange structure ex-
plained above. A dialogue segment tag is first
inserted before each initiating utterance. Af-
ter that, a topic break index, a topic name,
and a segment relation are identified.
Topic break index (TBI) takes the value of
1 or 2: the boundary with TBI=2 is less con-
tinuous than the one with TBI=1 with regard
to the topic. The topic name is labeled by an-
notators? subjective judgment for the topics
of that segment. The segment relation indi-
cates the one between the preceding and the
following segments, which is classified as clar-
ification, interruption, and return.
Figure 5 shows an example dialogue with
the dialogue segment tags.
3 Dialogue act tagger
Considering the limitation of amount of cor-
pus with dialogue level annotations, a promis-
ing dialogue act tagger is based on ma-
chine learning method with limited amount of
training data rather than statistical method,
which needs large amount of training data.
Rule-based and example-based learning algo-
rithms are suitable to this purpose. In this
section, we compare our implementation of
transformation-based rule learning algorithm
and example-based tagging algorithm.
3.1 Transformation-based learning
Transformation-based learning is a simple
rule-based learning algorithm. Figure 6 illus-
trates the learning process.
???????????
??????
?????????
??????
??????? ????
?????????
?????? ????
???????? ?????? ???????? ?????? ???????? ??????
? ? ?????? ????????? ? ?
?????????
????
??????? ????
???
??????? ????
???
????????
???? ???

????? ?????
??? ?????
????? ???? ????
????????? ?? ??? ??????? ????
????
?? ?????????
???? ????????
Figure 6: Learning procedure of dialogue act
tagging rule by TBL
First, initial tagged data was made from
unannotated corpus by using bootstrapping
method. In our implementation, we use de-
fault rule which assigns the most frequent
tag to all the utterance as a bootstrapping
method. All the possible rules are constructed
from annotated corpus by combining condi-
tional parts and their consequence. All the
possible rule are applied to the data and
the rule whose transformation results in the
greatest improvement is selected. This rule is
added to the current rule set and this itera-
tion is continued until no improvement is ob-
served. In the previous research, TBL showed
successful performance in many annotation
task, e.g. (Brill, 1995), (Samuel et al, 1998).
In our experiment, the selected features in
the conditional part of the rule are words
(the notation in the rule is include), sentence
length (length) and previous dialogue act tag
(prev). Although each feature is not enough
to use as a clue in determining dialogue act,
the combination of these features works well.
We used four types of combinations, that is,
include + include, include + length, include
+ prev and length + prev.
The result of the learning process is a se-
quence of rules. For example, in dialogue act
tagging, acquired rules in scheduling domain
are shown in Figure 7.
#1 condition: default,
new_tag: wh-question
#2 condition: include="yoroshii(good)"
& include="ka(*1)",
new_tag: yes-no-question
#3 condition: include="hai(yes)"
& prev=yes-no-question,
new_tag: affirmative
#4 condition: include="understand"
& length < 4,
new_tag: follow-up
...
(*1 "ka" is a functional word
for interrogative sentence)
Figure 7: Acquired dialogue act tagging rules
3.2 Example-based learning
Example-based learning is suitable for classi-
fication task. It stores up example of input
and corresponding class, calculates the dis-
tance between these examples and new input,
and classifies it to the nearest class.
In our dialogue act tagging, the example
is consists of word sequence (partitioned by
slash unit tag) and part of speech information.
Corresponding dialogue act tag is attached to
all the example.
The distance between example and new in-
put is calculated using the weighted agree-
ment of elements shown in Table 1.
Table 1: Elements for calculating a distance.
element weight
dialogue act of before two sentence 1
dialogue act of previous sentence 3
postpositional word of end of sentence 3
clue word for dialogue act 3
another word 2
3.3 Experimental results
We have compared above two dialogue act
tagging algorithms in two different tasks: a
route direction task and a car trouble shoot-
ing task. We used 4 dialogues for each task
(268 and 184 sentences) as a training data
and 2 dialogues as a test data (113 and 63
sentences). The results are shown in Table 2.
Table 2: Comparison of TBL and example-
based method.
algorithm task closed open
route direction 85.1 72.6
TBL car trouble shooting 90.2 66.7
average 87.7 69.7
route direction 93.8 62.6
ex-based car trouble shooting 89.7 52.4
average 91.8 57.5
We got similar average score for closed test.
Therefore, we regard the tuning level of pa-
rameter of each algorithm as a comparable
level. In open test in the same task, we got
69.7% in TBL and 57.5 % in example-based
method. As a result, we can conclude TBL is
more suitable method for dialogue act tagging
learning in limited amount of training data.
4 Relevance tagger using decision
tree
4.1 Decision tree learning
Decision tree learning algorithm is one of clas-
sification rule learning algorithm. The train-
ing data is a list of attribute-value pair. The
output of this algorithm is a decision tree
whose nodes are regarded as set of rules. Each
rule tests the value of an attribute and indi-
cates the next node.
A basic algorithm is as follows:
1. create root node.
2. if all the data belong to the same class,
create a class node and exit.
otherwise,
? choose one attribute which has
the maximum mutual informa-
tion and create nodes correspond-
ing values.
? divide and assign the training
data according to the values and
create link to the new node.
3. apply this algorithm to all the new nodes recur-
sively
We also used post-pruning rule hired in
C4.5 (Quinlan, 1992).
4.2 Relevance tagging algorithm and
results
Relevance type 1
Relevance type 1 tag is automatically an-
notated according to the exchange structure
which is identified in dialogue act tagging
stage. The accuracy of the relevance type
1 tag is depend on whether a given dialogue
or task domain is follow the assumption of
exchange structure explained above. In well
formed dialogue, the accuracy is above 95%.
However, in ill-formed case, it is around 70%.
Relevance type 2
We have used decision tree method in
identifying relevance type 2, which identi-
fies whether neighboring exchange structures
have a certain kind of relevance. The at-
tributes of training data are as follows.
1. relevance type 2 tag of previous exchange
2. initiative dialogue act tag of previous exchange
3. response dialogue act tag of previous exchange
4. initiative dialogue act tag of current exchange
We used 9 dialogue (hotel reservation, route
direction, scheduling, and telephone shop-
ping) as training and test data. The results
are shown in Table 3. We got this results after
10 cross validation. In cross domain experi-
ment, we got 84% accuracy in closed test (av-
erage 47 nodes) and 75% in open test. Using
post-pruning method, we got 82% of accuracy
(average 22 nodes; estimated accuracy 76%)
in closed test and 77% in open test.
5 Dialogue segment tagger
5.1 TBI tagger
We used decision tree method in identifying
the value of topic break index because the tar-
get attribute have only two values; 1 (small
topic change) or 2 (large topic change). In
case of target attribute has small number of
values, decision tree method can be estimated
to outperform transformation-based learning.
The attributes of training data are as fol-
lows.
1. relevance type 2 tag of previous exchange
2. relevance type 2 tag of current exchange
3. topic break index tag of previous exchange
4. dialogue act tag of previous slash unit
5. dialogue act tag of current slash unit
We used same data set with the experiment
of dialogue act tagging. We got 87% accuracy
in closed test (average 61 nodes) and 80% in
open test. Using post-pruning method, we got
82% of accuracy (average 12 nodes; estimated
accuracy 76%) in closed test and 78% in open
test (see Table 4).
5.2 Topic name tagger
In JDTAG topic name tagging scheme, anno-
tators can assign a topic name subjectively
to the given dialogue segment. Certainly it
is an appropriate method for this scheme to
use for the dialogue of any task domain. But,
even in the almost same pattern of exchange,
different annotators might annotate different
topic names. It prevent the data from a prac-
tical usage, e.g. extracting exchange pattern
in asking a route to certain place.
We prepare a candidate topic name list and
assign to dialogue segment as a topic name.
Because candidate topic name is around 10
to 30 according to the task domain, we use
transformation-based learning method for ac-
quiring a topic name tagging rule set.
The selected features in the conditional
part of the rule are words of current segment
(up to 2), dialogue act tag of the first slash
unit of the segment, topic name tag of previ-
ous segment.
As a result, in the above data set, the candi-
date rules are 5588. And we got 98% accuracy
in the closed test and 56% in open test.
5.3 Segment relation tagger
The number of segment relation types are 4
(clarification, interruption, return, and none).
Therefore, we used decision tree for acquiring
rules for identifying segment relation types.
In making decision tree, we did not use
post-pruning because a great many of seg-
ment relation tag is none (about 85%). Post-
pruning makes a tree too general (only one
top node which identifies none or else).
Table 3: Results of relevance type 2 tagging
not pruned pruned
# of nodes accuracy # of nodes accuracy estimated error rate
Training 47.3 83.7% 22.0 81.9% 23.7%
Test 47.3 75.4% 22.0 77.2% 23.7%
Table 4: Results of topic break index tagging
not pruned pruned
# of nodes accuracy # of nodes accuracy estimated error rate
Training
trouble shooting 53.0 90.0% 10.2 82.4% 22.6%
Test
trouble shooting 53.0 85.2% 10.2 78.2% 22.6%
Training
route direction 69.0 84.1% 14.5 82.4% 25.5%
Test
route direction 69.0 73.8% 14.5 77.5% 25.5%
As a result, also in the same data set, we
got 92% accuracy in the closed test.
6 Conclusion
We have developed a discourse level tagging
tool for spoken dialogue corpus using machine
learning methods. We use transformation-
based learning method in case of many target
values, and decision tree method otherwise.
Our future work is to develop an environ-
ment in which annotators can easily browse
and post-edit the output of the tool.
Acknowledgement
This work has been supported by the NEDO
Industrial Technology Research Grant Pro-
gram (No. 00A18004b)
References
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: A
case study in part-of-speech tagging. Compu-
tational Linguistics, 21(4):543?566.
J. Carletta, N. Dahlback, N. Reithinger, and
M. A. Walker. 1997. Standards for di-
alogue coding in natural language pro-
cessing. Dagstuhl-Seminar-Report:167
(ftp://ftp.cs.uni-sb.de/pub/dagstuhl/ re-
porte/97/9706.ps.gz).
M. Core, M. Ishizaki, J. Moore, C. Nakatani,
N. Reithinger, D. Traum, and S. Tutiya. 1998.
The Report of the Third Workshop of the
Discourse Research Initiative. Chiba Corpus
Project. Technical Report 3, Chiba University.
M. Coulthhard, editor. 1992. Advances in Spoken
Discourse Analysis. Routledge.
B. J. Grosz and C. L. Sidner. 1986. Attention,
intention and the structure of discourse. Com-
putational Linguistics, 12:175?204.
The Japanese Discourse Research Initiative JDRI.
2000. Japanese dialogue corpus of multi-level
annotation. In Proc. of the 1st SIGDIAL Work-
shop on discourse and dialogue.
M. Meteer and A. Taylor. 1995. Dysflu-
ency annotation stylebook for the switch-
board corpus. Linguistic Data Consor-
tium (ftp://ftp.cis.upenn.edu/pub/treebank/
swbd/doc/DFL-book.ps.gz).
J. R. Quinlan. 1992. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
K. Samuel, S. Carberry, and K. Vijay-
Shanker. 1998. Dialogue act tagging with
transformation-based learning. In Proc. of
COLING-ACL 98, pages 1150?1156.
J. R. Searle. 1969. Speech Acts. Cambridge Uni-
versity Press.
A. B. Stenstrom. 1994. An Introduction to Spoken
Interaction. Addison-Wesley.
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 109?116,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multimodal Dialog Description Language 
for Rapid System Development 
 
 
Masahiro Araki               Kenji Tachibana 
Kyoto Institute of Technology 
Graduate School of Science and Technology, Department of Information Science 
Matsugasaki Sakyo-ku Kyoto 606-8585 Japan 
araki@dj.kit.ac.jp 
 
  
 
Abstract 
In this paper, we explain a rapid devel-
opment method of multimodal dialogue 
sys-tem using MIML (Multimodal Inter-
action Markup Language), which defines 
dialogue patterns between human and 
various types of interactive agents. The 
feature of this language is three-layered 
description of agent-based interactive 
systems which separates task level de-
scription, interaction description and de-
vice dependent realization. MIML has 
advantages in high-level interaction de-
scription, modality extensibility and 
compatibility with standardized tech-
nologies. 
1 Introduction 
In recent years, various types of interactive 
agents, such as personal robots, life-like agents 
(Kawamoto et al 2004), and animated agents are 
developed for many purposes. Such interactive 
agents have an ability of speech communication 
with human by using automatic speech recog-
nizer and speech synthesizer as a main modality 
of communication. The purpose of these interac-
tive agents is to realize a user-friendly interface 
for information seeking, remote operation task, 
entertainment, etc. 
Each agent system is controlled by different 
description language. For example, Microsoft 
agent is controlled by JavaScript / VBScript em-
bedded in HTML files, Galatea (Kawamoto et al. 
2004) is controlled by extended VoiceXML (in 
Linux version) and XISL (Katsurada et al 2003) 
(in Windows version). In addition to this differ-
ence, these languages do not have the ability of 
higher level task definition because the main 
elements of these languages are the control of 
modality functions for each agent. These make 
rapid development of multimodal system diffi-
cult.  
In order to deal with these problems, we pro-
pose a multimodal interaction description lan-
guage, MIML (Multimodal Interaction Markup 
Language), which defines dialogue patterns be-
tween human and various types of interactive 
agents by abstracting their functions. The feature 
of this language is three-layered description of 
agent-based interactive systems.  
The high-level description is a task definition 
that can easily construct typical agent-based in-
teractive task control information. The middle-
level description is an interaction description that 
defines agent?s behavior and user?s input at the 
granularity of dialogue segment. The low-level 
description is a platform dependent description 
that can override the pre-defined function in the 
interaction description.  
The connection between task-level and inter-
action-level is realized by generation of interac-
tion description templates from the task level 
description. The connection between interaction-
level and platform-level is realized by a binding 
mechanism of XML.  
The rest of this paper consists as follows. Sec-
tion 2 describes the specification of the proposed 
language. Section 3 explains a process of rapid 
multimodal dialogue system development. Sec-
tion 4 gives a comparison with existing multi-
modal languages. Section 5 states conclusions 
and future works. 
109
2 Specification of MIML 
2.1 Task level markup language 
2.1.1 Task classification 
In spoken dialogue system development, we pro-
posed task classification based on the direction 
of information flow (Araki et al 1999). We con-
sider that the same analysis can be applied to 
agent based interactive systems (see Table 1).  
Table 1: Task classification of agent-based inter-
active systems 
Class Direction of Info. flow Typical task 
Information 
assistant 
 
user  agent Interactive presentation 
User agent 
 
user                                  agent 
control of home 
network equip-
ments 
Question 
and Answer 
 
user                 agent daily life in-
formation query
 
In the information assistant class, the agent 
has information to be presented to the user. 
Typically, the information contents are Web 
pages, an instruction of consumer product usage, 
an educational content, etc. Sometimes the con-
tents are too long to deliver all the information to 
the user. Therefore, it needs user model that can 
manage user?s preference and past interaction 
records in order to select or filter out the contents. 
In the user agent class, the user has informa-
tion to be delivered to the agent in order to 
achieve a user?s goal. Typically, the information 
is a command to control networked home 
equipments, travel schedule to reserve a train 
ticket, etc. The agent mediates between user and 
target application in order to make user?s input 
appropriate and easy at the client side process 
(e.g. checking a mandatory filed to be filled, 
automatic filling with personal data (name, ad-
dress, e-mail, etc.)). 
In the Question and Answer class, the user has 
an intention to acquire some information from 
the agent that can access to the Web or a data-
base. First, the user makes a query in natural lan-
guage, and then the agent makes a response ac-
cording to the result of the information retrieval. 
If too much information is retrieved, the agent 
makes a narrowing down subdialogue. If there is 
no information that matches user?s query, the 
agent makes a request to reformulate an initial 
query. If the amount of retrieved information is 
appropriate to deliver to the user by using current 
modality, the agent reports the results to the user. 
The appropriate amount of information differs in 
the main interaction modality of the target device, 
such as small display, normal graphic display or 
speech. Therefore, it needs the information of 
media capability of the target device. 
2.1.2 Overview of task markup language 
As a result of above investigation, we specify 
the task level interaction description language 
shown in Figure 1. 
 
taskml
bodyhead
userModel deviceModel
section*
xforms
qa
searchquery result
model input
 
 
Figure. 1  Structure of the Task Markup Lan-
guage. 
 
The features of this language are (1) the ability 
to model each participant of dialogue (i.e. user 
and agent) and (2) to provide an execution 
framework of each class of task.  
The task markup language <taskml> consists 
of two parts corresponding to above mentioned 
features: <head> part and <body> part. The 
<head> part specifies models of the user (by 
<userModel> element) and the agent (by <de-
viceModel> element). The content of each model 
is described in section 2.1.3. The <body> part 
specifies a class of interaction task. The content 
of each task is declaratively specified under the 
<section>, <xforms> and <qa> elements, which 
are explained in section 2.1.4. 
2.1.3 Head part of task markup language 
In the <head> element of the task markup lan-
guage, the developer can specify user model in 
<userModel> element and agent model in <de-
viceModel> element.  
In the <userModel> element, the developer 
declares variables which represent user?s infor-
mation, such as expertise to domain, expertise to 
dialogue system, interest level to the contents, 
etc.  
In the <deviceModel> element, the developer 
can specify the type of interactive agent and 
main modality of interaction. This information is 
(* means the 
element can 
repeat more 
than 1 time) 
110
used for generating template from this task de-
scription to interaction descriptions. 
2.1.4 Body part of task markup language 
According to the class of the task, the <body> 
element consists of a sequence of <section> ele-
ments, a <xforms> element or a <qa> element. 
The <section> element represents a piece of 
information in the task of the information assis-
tant class. The attributes of this element are id, 
start time and end time of the presentation mate-
rial and declared user model variable which indi-
cates whether this section meets the user?s needs 
or knowledge level. The child elements of the 
<section> element specify multimodal presenta-
tion. These elements are the same set of the child 
elements of <output> element in the interaction 
level description explained in the next subsection. 
Also, there is a <interaction> element as a child 
element of the <section> element which specifies 
agent interaction pattern description as an exter-
nal pointer. It is used for additional comment 
generated by the agent to the presented contents. 
For the sake of this separation of contents and 
additional comments, the developer can easily 
add agent?s behavior in accordance with the user 
model. The interaction flow of this class is 
shown in Figure 2. 
 
start
interaction
presentation
question and
answer
subdialog
yes
end
no
end of
sections?
Multimedia
contents
matches
user model?
yes
next section
no
. 
Figure. 2  Interaction flow of Information Assist 
class 
The <xforms> element represents a group of 
information in the task of the user agent class. It 
specifies a data model, constraint of the value 
and submission action following the notation of 
XForms 1.0.  
In the task of user agent class, the role of in-
teractive agent is to collect information from the 
user in order to achieve a specific task, such as 
hotel reservation. XForms is designed to separate 
the data structure of information and the appear-
ance at the user?s client, such as using text field 
input, radio button, pull-down menu, etc. because 
such interface appearances are different in de-
vices even in GUI-based systems. If the devel-
oper wants to use multimodal input for the user?s 
client, such separation of the data structure and 
the appearance, i.e. how to show the necessary 
information and how to get user?s input, is very 
important.  
In MIML, such device dependent ?appearance? 
information is defined in interaction level. There-
fore, in this user agent class, the task description 
is only to define data structure because interac-
tion flows of this task can be limited to the typi-
cal patterns. For example, in hotel reservation, as 
a result of AP (application) access, if there is no 
available room at the requested date, the user?s 
reservation request is rejected. If the system rec-
ommends an alternative choice to the user, the 
interaction branches to subdialogue of recom-
mendation, after the first user?s request is proc-
essed (see Figure 3). The interaction pattern of 
each subdialogue is described in the interaction 
level markup language. 
 
start
slot filling
AP access
all required
slots are filled?
confirmation
dialogue
rejection
dialogue
yes
no
end
application
recommendation
dialogue
accept?
yes
no  
Figure. 3  Interaction flow of User Agent class 
 
The <qa> element consists of three children: 
<query>, <search> and <result>.  
The content of <query> element is the same as 
the <xforms> element explained above. However, 
generated interaction patterns are different in 
user agent class and question and answer class. 
In user agent class, all the values (except for op-
tional slots indicated explicitly) are expected to 
be filled. On the contrary, in question and answer 
class, a subset of slots defined by form descrip-
tion can make a query. Therefore, the first ex-
111
change of the question and answer class task is 
system?s prompt and user?s query input.  
The <search> element represents application 
command using the variable defined in the 
<query> element. Such application command 
can be a database access command or SPARQL 
(Simple Protocol And RDF Query Language)1 in 
case of Semantic Web search.  
The <result> element specifies which informa-
tion to be delivered to the user from the query 
result. The behavior of back-end application of 
this class is not as simple as user agent class. If 
too many results are searched, the system transits 
to narrowing down subdialogue. If no result is 
searched, the system transits to subdialogue that 
relaxes initial user?s query. If appropriate num-
ber (it depends on presentation media) of results 
are searched, the presentation subdialogue begins. 
The flow of interaction is shown in Figure 4. 
 
Figure. 4  Interaction flow of Question and An-
swer class 
2.2 Interaction level markup language 
2.2.1 Overview of interaction markup lan-
guage 
Previously, we proposed a multimodal interac-
tion markup language (Araki et al 2004) as an 
extension of VoiceXML2. In this paper, we mod-
ify the previous proposal for specializing human-
agent interaction and for realizing interaction 
pattern defined in the task level markup language.  
The main extension is a definition of modality 
independent elements for input and output. In 
VoiceXML, system?s audio prompt is defined in 
<prompt> element as a child of <field> element 
                                                 
1 http://www.w3.org/TR/rdf-sparql-query/ 
2 http://www.w3.org/TR/voicexml20/ 
that defines atomic interaction acquiring the 
value of the variable. User?s speech input pattern 
is defined by <grammar> element under <field> 
element. In our MIML, <grammar> element is 
replaced by the <input> element which specifies 
active input modalities and their input pattern to 
be bund to the variable that is indicated as name 
attribute of the <field> element. Also, <prompt> 
element is replaced by the <output> element 
which specifies active output modalities and a 
source media file or contents to be presented to 
the user. In <output> element, the developer can 
specify agent?s behavior by using <agent> ele-
ment. The outline of this interaction level 
markup language is shown in Figure 5. 
 
mmvxml
formlink
block
*
field filled
outputinput filled
**
*
initial
input
catch
*
audio
video
page
agent
smil
speech
image
touch
 
Figure. 5  Structure of Interaction level Markup 
Language 
2.2.2 Input and output control in agent 
The <input> element and the <output> element 
are designed for implementing various types of 
interactive agent systems. 
The <input> element specifies the input proc-
essing of each modality. For speech input, 
grammar attribute of <speech> element specifies 
user?s input pattern by SRGS (Speech Recogni-
tion Grammar Specification)3 , or alternatively, 
type attribute specifies built-in grammar such as 
Boolean, date, digit, etc. For image input, type 
attribute of <image> element specifies built-in 
behavior for camera input, such as nod, faceRec-
ognition, etc. For touch input, the value of the 
variable is given by referring external definition 
of the relation between displayed object and its 
value. 
The <output> element specifies the output 
control of each modality. Each child element of 
                                                 
3 http://www.w3.org/TR/speech-grammar/ 
start
initial query
input
searchDB
# of
results
relaxation
dialogue
report
dialogue
narrowing
down
subdialog
0
appropriate
too many
end
Web
112
this element is performed in parallel. If the de-
veloper wants to make sequential output, it 
should be written in <smil> element (Synchro-
nized Multimedia Integration Language) 4 , For 
audio output, <audio> element works as the 
same way as VoiceXML, that is, the content of 
the element is passed to TTS (Text-to-Speech 
module) and if the audio file is specified by the 
src attribute, it is a prior output. In <video>, 
<page> (e.g. HTML) and <smil> (for rich mul-
timedia presentation) output, each element speci-
fies the contents file by src attribute. In <agent> 
element, the agent?s behavior definition, such as 
move, emotion, status attribute specifies the pa-
rameter for each action. 
2.3 Platform level description 
The differences of agent and other devices for 
input/output are absorbed in this level. In interac-
tion level markup language, <agent> element 
specifies agent?s behavior. However, some agent 
can move in a real world (e.g. personal robot), 
some agent can move on a computer screen (e.g. 
Microsoft Agent), and some cannot move but 
display their face (e.g. life-like agent). 
One solution for dealing with such variety of 
behavior is to define many attributes at <agent> 
element, for example, move, facial expression, 
gesture, point, etc. However, the defects of this 
solution are inflexibility of correspondence to 
progress of agent technology (if an agent adds 
new ability to its behavior, the specification of 
language should be changed) and interference of 
reusability of interaction description (description 
for one agent cannot apply to another agent).  
Our solution is to use the binding mechanism 
in XML language between interaction level and 
platform dependent level. We assume default 
behavior for each value of the move, emotion 
and status attributes of the <agent> element. If 
such default behavior is not enough for some 
purpose, the developer can override the agent?s 
behavior using binding mechanism and the 
agent?s native control language. As a result, the 
platform level description is embedded in bind-
ing language described in next section. 
3 Rapid system development 
3.1 Usage of application framework 
Each task class has a typical execution steps as 
investigated in previous section. Therefore a sys-
tem developer has to specify a data model and 
                                                 
4 http://www.w3.org/AudioVideo/ 
specific information for each task execution. 
Web application framework can drive interactive 
task using these declarative parameters. 
As an application framework, we use Struts5 
which is based on Model-View-Controller (MVC) 
model. It clearly separates application logic 
(model part), transition of interaction (controller 
part) and user interface (view part). Although 
MVC model is popular in GUI-based Web appli-
cation, it can be applied in speech-based applica-
tion because any modality dependent information 
can be excluded from the view part. Struts pro-
vides (1) a controller mechanism and (2) integra-
tion mechanism with the back-end application 
part and the user interface part. In driving Struts, 
a developer has to (1) define a data class which 
stores the user?s input and responding results, (2) 
make action mapping rules which defines a tran-
sition pattern of the target interactive system, and 
(3) make the view part which defines human-
computer interaction patterns. The process of 
Struts begins by the request from the user client 
(typically in HTML, form data is submitted to 
the Web server via HTTP post method).  
The controller catches the request and stores 
the submitted data to the data class, and then 
calls the action class specified by the request fol-
lowing the definition of action mapping rules.  
The action class communicates with the back-
end application, such as database management 
system or outside Web servers by referring the 
data class, and returns the status of the process-
ing to the controller. According to the status, the 
controller refers the action mapping rules and 
selects the view file which is passed to the user?s 
client. Basically, this view file is written in Java 
Server Pages, which can be any XML file that 
includes Java code or useful tag libraries. Using 
this embedded programming method, the results 
of the application processing is reflected to the 
response. The flow of processing in the Struts is 
shown in Figure 6. 
Figure. 6  MVC model. 
                                                 
5 http:// struts.apache.org 
user
interface controller
application
logic
data
class
Action
mapping
request
results
call
status
lookup
view modelcontroller
113
The first step of rapid development is to pre-
pare backend application (Typically using Data-
base Management System) and their application 
logic code. The action mapping file and data 
class file are created automatically from the task 
level description described next subsection. 
3.2 Task definition 
Figure 7 shows an example description of the 
information assistant task. In this task setting, 
video contents which are divided into sections 
are presented to the user one by one. At the end 
of a section, a robot agent put in a word in order 
to help user?s understanding and to measure the 
user?s preference (e.g. by the recognition of ac-
knowledging, nodding, etc.) . If low user?s pref-
erence is observed, unimportant parts of the 
presentation are skipped and comments of the 
robot are adjusted to beginner?s level. The im-
portance of the section is indicated by interes-
tLevel attribute and knowledgeLevel attribute 
that are introduced in the <userModel> element. 
If one of the values of these attribute is below the 
current value of the user model, the relevant sec-
tion is skipped. The skipping mechanism using 
user model variables is automatically inserted 
into an interaction level description. 
Figure. 7  An Example of Task Markup Lan-
guage. 
3.3 Describing Interaction 
The connection between task-level and interac-
tion-level is realized by generation of interaction 
description templates from the task level descrip-
tion. The interaction level description corre-
sponds to the view part of the MVC model on 
which task level description is based. From this 
point of view, task level language specification 
gives higher level parameters over MVC frame-
work which restricts behavior of the model for 
typical interactive application patterns. Therefore, 
from this pattern information, the skeletons of 
the view part of each typical pattern can be gen-
erated based on the device model information in 
task markup language.  
For example, by the task level description 
shown in Figure 7, data class is generated from 
<userModel> element by mapping the field of 
the class to user model variable, and action map-
ping rule set is generated using the sequence in-
formation of <section> elements. The branch is 
realized by calling application logic which com-
pares the attribute variables of the <section> and 
user model data class. Following action mapping 
rule, the interaction level description is generated 
for each <section> element. In information assis-
tant class, a <section> element corresponds to 
two interaction level descriptions: the one is pre-
senting contents which transform <video> ele-
ment to the <output> elements and the other is 
interacting with user, such as shown in Figure 8.  
The latter file is merely a skeleton. Therefore, 
the developer has to fill the system?s prompt, 
specify user?s input and add corresponding ac-
tions. 
Figure 8 describes an interaction as follows: at 
the end of some segment, the agent asks the user 
whether the contents are interesting or not. The 
user can reply by speech or by nodding gesture. 
If the user?s response is affirmative, the global 
variable of interest level in user model is incre-
mented. 
 
<taskml type="infoAssist"> 
  <head> 
    <userModel> 
      <interestLevel/> 
      <knowledgeLevel/> 
    </userModel> 
    <deviceModel 
mainMode="speech" agentType="robot"/> 
  </head> 
  <body> 
    <section id="001" 
  s_time="00:00:00" e_time="00:00:50"  
intersetLevel="1"  knowledgeLevel="1"> 
        <video src="vtr1.avi" /> 
        <interaction name="interest1.mmi" 
 s_time="00:00:30"/> 
    </section> 
     ... 
  </body> 
</taskml> 
114
Bool speak
(String message){
Module m
=Call TTS-module;
m.set(message);
m.speak(message);
release m;
}
Bool speak
(String message){
Module m
=Call TTS-module;
m.set(message);
m.speak(message);
release m;
}
<message>
<head>
<to>TTS-module</to>
<from>DM</fro >
<head>
<body>
Set Text ?hello?
</body>
</message>
<audio>
Hello
</audio>
?
? ?
? ?
n
Child Place
+
?
?
? ?
n
 
Figure. 8  An Example of Interaction level 
Markup Language. 
3.4 Adaptation to multiple interaction de-
vices 
The connection between interaction-level and 
platform-level is realized by binding mechanism 
of XML. XBL (XML Binding Language)6 was 
originally defined for smart user interface de-
scription, extended for SVG afterwards, and fur-
thermore, for general XML language. The con-
cept of binding in XBL is a tree extension by 
inheriting the value of attributes to the sub tree 
(see Figure 9). As a result of this mechanism, the 
base language, in this the case interaction 
markup language, can keep its simplicity but 
does not loose flexibility. 
 
 
 
Figure. 9  Concept of XML binding. 
 
By using this mechanism, we implemented 
various types of weather information system, 
                                                 
6 http://www.w3.org/TR/xbl/ 
such as Microsoft agent (Figure 10), Galatea 
(Figure 11) and a personal robot. The platform 
change is made only by modifying agentType 
attribute of <deviceModel> element of taskML. 
 
Figure. 10 Interaction with Microsoft agent. 
 
Figure. 11 Interaction with Galatea. 
4 Comparison with existing multimodal 
language 
There are several multimodal interaction systems, 
mainly in research level (L?pez-C?zar and Araki 
2005). XHTML+Voice 7  and SALT 8  are most 
popular multimodal interaction description lan-
guages. These two languages concentrate on how 
to add speech interaction on graphical Web 
pages by adding spoken dialogue description to 
(X)HTML codes. These are not suitable for a 
description of virtual agent interactions. 
(Fernando D?Haro et al 2005) proposes new 
multimodal languages for several layers. Their 
proposal is mainly on development environment 
which supports development steps but for lan-
guage itself. In contrary to that, our proposal is a 
                                                 
7 http://www-306.ibm.com/software/pervasive/ 
multimodal/x%2Bv/11/spec.htm 
8 http://www.saltforum.org/ 
<mmvxml> 
<form> 
   <field name=?question?> 
     <input> 
         <speech type=?boolean?/> 
         <image type=?nod?/> 
     </input> 
     <output> 
         <audio> Is it interesting? </audio> 
     </output> 
     <filled> 
         <if cond=?question==true?>  
<assign name=?intersestLevel? 
                          expr=? intersestLevel+1?/> 
</if> 
<submit src=?http://localhost:8080/step2/> 
      </filled> 
    </field> 
 </form> 
</mmvxml> 
115
simplified language and framework that auto-
mate several steps for system development. 
5 Conclusion and future works 
In this paper, we explained a rapid development 
method of multimodal dialogue system using 
MIML. This language can be extended for more 
complex task settings, such as multi-scenario 
presentation and multiple-task agents. Although 
it is difficult to realize multi-scenario presenta-
tion by the proposed filtering method, it can be 
treated by extending filtering concept to discrete 
variable and enriching the data type of <user-
Model> variables. For example, if the value of 
<knowledgeLevel> variable in Figure 7 can take 
one of ?expert?, ?moderate? and ?novice?, and 
each scenario in multi-scenario presentation is 
marked with these values, multi-scenario presen-
tation can be realized by filtering with discrete 
variables. In case of multiple-task agents, we can 
implement such agents by adding one additional 
interaction description which guides to branch 
various tasks. 
Acknowledgments 
Authors would like to thank the members of 
ISTC/MMI markup language working group for 
their useful discussions.  
 
References 
M. Araki, K. Komatani, T. Hirata and S. Doshita. 
1999. A Dialogue Library for Task-oriented Spo-
ken Dialogue Systems, Proc. IJCAI Workshop on 
Knowledge and Reasoning in Practical Dialogue 
Systems, pp.1-7. 
M. Araki, K. Ueda, M. Akita, T. Nishimoto and Y. 
Niimi. 2002. Proposal of a Multimodal Dialogue 
Description Language, In Proc. of PRICAI 02. 
L. Fernando D?Haro et al 2005. An advanced plat-
form to speed up the design of multilingual dialog 
applications for multiple modalities, Speech Com-
munication, in Press. 
R. L?pez-C?zar Delgado, M Araki. 2005. Spoken, 
Multilingual and Multimodal Dialogue Systems: 
Development and Assessment, Wiley. 
K. Katsurada, Y. Nakamura, H. Yamada, T. Nitta. 
2003.  XISL: A Language for Describing Multimo-
dal Interaction Scenarios, Proc. of ICMI'03, 
pp.281-284. 
S. Kawamoto, H. Shimodaira, T. Nitta, T. Nishimoto, 
S. Nakamura, K. Itou, S. Morishima, T. Yotsukura, 
A. Kai, A. Lee, Y. Yamashita, T. Kobayashi, K. 
Tokuda, K. Hirose, N. Minematsu, A. Yamada, Y. 
Den, T. Utsuro and S. Sagayama. 2004. Galatea: 
Open-Source Software for Developing Anthropo-
morphic Spoken Dialog Agents, In Life-Like Char-
acters. Tools, Affective   Functions, and Applica-
tions. ed. H. Prendinger and M. Ishizuka, pp.187-
212, Springer. 
116
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 70?73,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Rapid Development Process of Spoken Dialogue Systems using 
Collaboratively Constructed Semantic Resources 
 
Masahiro Araki 
Department of Information Science 
Kyoto Institute of Technology 
Matsugasaki, Sakyo-ku, Kyoto 606-8585, Japan
araki@kit.ac.jp 
 
 
 
 
 
Abstract 
We herein propose a method for the rapid 
development of a spoken dialogue system 
based on collaboratively constructed 
semantic resources and compare the 
proposed method with a conventional 
method that is based on a relational 
database. Previous development 
frameworks of spoken dialogue systems, 
which presuppose a relational database 
management system as a background 
application, require complex data definition, 
such as making entries in a task-dependent 
language dictionary, templates of semantic 
frames, and conversion rules from user 
utterances to the query language of the 
database. We demonstrate that a semantic 
web oriented approach based on 
collaboratively constructed semantic 
resources significantly reduces troublesome 
rule descriptions and complex 
configurations in the rapid development 
process of spoken dialogue systems. 
1 Introduction 
There has been continuing interest in the 
development methodology of spoken dialogue 
systems (SDS). In recent years, statistical methods, 
such as Williams et al (2007) and Hori et al 
(2009), have attracted a great deal of attention as a 
data-driven (i.e., corpus-driven) approach, which 
can reduce the troublesome manual coding of 
dialogue management rules. Statistical methods 
can also be applied to other components of SDS, 
such as semi-automatic construction of semantic 
interpreters and response generators. However the 
overall SDS development process still requires 
some hand coding, for example to establish the 
connection to the underlying application. 
Another data-driven approach was designed to 
provide all of the SDS components with the goal of 
rapidly constructing the entire system (Kogure et 
al., 2001; Heinroth et al, 2009). This approach 
starts from a data model definition (and so can be 
regarded as a data-modeling driven approach) and 
adds rules and templates, which are used as task-
dependent knowledge in an SDS. As a data model 
definition, Kogure et al (2001) used a relational 
database (RDB) schema and Heinroth et al (2009) 
used OWL, which is an ontology definition 
language in semantic web applications. Although 
these data-modeling schemata are familiar to 
developers of web applications, additional 
definition of rules and templates needed for an 
SDS is troublesome for ordinary web developers 
because such SDS-related rules require specialized 
knowledge of linguistics and speech application 
development. 
We herein propose a new data-modeling driven 
approach for rapid development of SDS that is 
based on collaboratively constructed semantic 
resources (CSRs). We present an automatic 
generation mechanism of code and data for a 
simple SDS. In addition, we compare the proposed 
approach with an ordinary data-modeling driven 
approach that is based on a RDB. By using CSRs 
and the Rails framework of web application 
development, the troublesome definitions of rules 
and templates for SDS can be reduced significantly. 
70
The remainder of the present paper is organized 
as follows. Section 2 describes the proposed 
approach to a data-modeling driven development 
process for SDS based on CSRs. Section 3 
compares the proposed approach with the previous 
RDB-based approach. In Section 4, the paper 
concludes with a discussion of future research. 
2 Data-modeling driven approach based 
on CSRs 
In this section, we explain our previous data-
modeling driven approach and describe additional 
new functionality based on CSRs. 
2.1 Object-oriented SDS development 
framework 
We previously proposed a data-modeling driven 
framework for rapid prototyping of SDS (Araki et 
al., 2011). This includes a class library that is 
based on the class hierarchy and the attribute 
definitions of an existing semantic web ontology, 
i.e., Schema.org1. This class library is used as a 
base class of an application-specific class 
definition. An example class definition is shown in 
Figure 1. 
 
 Figure 1: Example of class definition extending 
existing class library. 
 
In this example, the MyBook class inherits all of 
the attributes of the Book class of Schema.org in 
the same manner as object-oriented programming 
languages. The developer can limit the attributes 
that are used in the target application by listing 
them in the constraints section. On the other hand, 
the developer can add additional attributes (in this 
class, ranking attributes as the type of integer) in 
the definition of the class. 
                                                          
1 http://schema.org/ 
The task type and dialogue initiative type are 
indicated as annotations at the beginning of the 
class definition. In this example, the task type is 
DB search and the initiative type is user initiative. 
This information is used in generating the 
controller code and view code of the target SDS. 
Using Grails2, which is a Rails web application 
framework, the proposed framework generates the 
dialogue controller code of the indicated task type 
and the view code, which have speech interaction 
capability on the HTML5 code from this class 
definition. The overall concept of the object-
oriented framework is shown in Figure 2. 
 
Data?model
definition
Mix?in?of
traits
embed
application
logic
State
definition
generate
convert
Grails
Data?model
definition
Groovy
generate
HTML5
code
Model
Controller
View
 Figure 2: Overview of the object-oriented SDS 
development framework. 
2.2 Usage of CSRs 
The disadvantage of our previous framework, 
described in the previous subsection, is the high 
dependence on the dictation performance of the 
speech recognition component. The automatically 
generated HTML5 code invokes dictation API, 
irrespective of the state of the dialogue and 
initiative type. In order to improve speech 
recognition accuracy, grammar rules (in system 
initiative dialogue) and/or the use of a 
task/domain-dependent language model (LM) (in 
mixed/user initiative dialogue) are necessary. In 
our previous framework, the developer had to 
prepare these ASR-related components using 
language resources, which are beyond the 
proposed data-driven framework. 
In order to overcome this defect, we add the 
Freebase3 class library, which is based on large-
scale CSRs, because Freebase already includes the 
                                                          
2 http://grails.org/ 
3 http://www.freebase.com/ 
@DBSearch 
@SystemInitiative 
class MyBook extends Book { 
  int ranking 
  static constraints = { 
    name(onsearch:"like") 
    author(onsearch:"like") 
    publisher() 
    ranking(number:true) 
  } 
} 
71
contents of the data. These contents and a large-
scale web corpus facilitate the construction of 
grammar rules and a LM that is specific to the 
target task/domain. For example, the Film class of 
Freebase has more than 191 thousand entries (as of 
May 2012). These real data can be used as 
resources to improve SDS accuracy. 
In system initiative type dialogue, the contents 
of each attribute can construct word entries of the 
grammar rule for each attribute slot. For example, 
the grammar rule for the user's response to "Which 
genre of movie are you searching for?" can be 
constructed from the contents of the genres 
attribute of the Film class. We implemented a 
generator of the set of content words specified in 
the data model definition from the data of Freebase. 
The generator is embedded as one component of 
the proposed rapid prototyping system. 
In the mixed/user initiative type tasks, since 
content words and functional words make up the 
user's utterance, we need a LM for speech 
recognition and a semantic frame extractor for the 
construction of semantic data storage queries. We 
designed and implemented a LM generator and a 
semantic frame extractor using a functional 
expression dictionary that corresponds to the 
attributes of Freebase (Araki, submitted). An 
example entry of the function expression 
dictionary is shown in Figure 3 and the flow of the 
LM generation is shown in Figure 4. 
 
item value 
property fb:film.performance.actor 
phrase pattern X "ga de te iru" Y 
constraints X rdf:type "/film/actor" 
partial graph Y fb:film.performance.actor X 
Figure 3: An entry of function expression 
dictionary. 
Freebase
data
Web
corpus
Data?model
definition
content
words
in?domain
entries
domain
dependent
LM
example
sentences
 Figure 4: Construction process of LM. 
3 Comparison with the RDB-based 
approach 
3.1 Overview of the RDB-based method 
As an example of the RDB-based SDS prototyping 
method, we review the method described in 
Kogure et al (2001) (see Figure 5). 
 
ASR NLU Search NLG TTS
AM
LM
dictionary
functional
noun.
grammar
general
query DB
rule format
pronounce
input output
domain
indep.
dep. task
indep.
dep.  Figure 5: Modules and knowledge of the RDB-
based method. 
 
They examined the domain dependency and task 
dependency of the knowledge that drives SDS. 
Domain/task-independent knowledge, such as an 
acoustic model, a general function word dictionary, 
and a pronunciation dictionary, are prepared in 
advance for all of the systems. Both domain-
dependent/task-independent knowledge, such as 
the language model, the noun/verb dictionary, and 
the database schema, and domain/task-dependent 
knowledge, such as the rule of query generation 
obtained from the results of semantic analysis and 
format for output, must be specified by the 
developer. If the developer wants to change a task 
within the same domain, the developer can reuse 
domain-dependent/task-independent knowledge 
(everything above the dotted line in Figure 4) and 
must specify task-dependent knowledge 
(everything below the dotted line in Figure 4). 
3.2 Comparison of the data-modeling stage 
In the data modeling of the RDB-based method, 
the developer must specify field names (e.g., title, 
year), their corresponding data types (e.g., string, 
integer), and the labels of the fields (i.e., the labels 
for the language used in the SDS), as in the usual 
web application with RDB. Since the data model 
definitions differ from one another, it is difficult to 
integrate similar systems even if these systems deal 
with the same domain. 
In the CSRs-based approach, the data-modeling 
process involves selecting necessary attributes of 
the inherited class and, if needed, adding fields for 
72
additional domain/task-specific information. The 
data type has already been set in the existing data 
schema, and language-dependent label information 
can be acquired by the value of rdfs:label, where 
the value of the lang attribute is the target language. 
3.3 Comparison of code generation stage 
In the RDB-based method, the developer must 
specify the noun and verb dictionary, grammar for 
parsing, and rules for query generation. In addition, 
the RDB-based approach must either stick to a 
fixed dialogue pattern for DB search or make the 
developer write dialogue management rules. 
By combining the CSRs-based approach with 
the Rails framework, the task dependent dictionary 
is automatically generated from the data and 
grammar rules are easily constructed with the 
functional expression entries of properties. Also in 
this approach, typical dialogue management 
patterns are already prepared and can be specified 
as annotations. For the sake of this setting, all of 
the basic codes for SDS are automatically 
generated from the data model definition. 
3.4 Comparison of functionality 
In the RDB-based method, the developer must 
make a domain/task dependent LM using language 
resources outside of the development process. 
However, in general, it is difficult to acquire a 
domain/task-dependent corpus. In addition, 
although the RDB-based method is designed to be 
robust with respect to the task modification, this 
method is not robust with respect to porting to 
different languages. Language specific code tends 
to be embedded in every component of an SDS. 
In the CSRs-based approach, the domain/task-
dependent LM is automatically generated, as 
described in Subsection 2.2. For the sake of this 
data-modeling driven method and native 
multilinguality of CSRs, the developer can easily 
implement multilingual SDS (Araki et al, 2012). 
Multilingual contents are already prepared in 
Freebase (although English resources are 
dominant) and a multilingual web speech API is 
already implemented, e.g., in the Google Chrome 
browser, the developer can implement a prototype 
of other language SDS by dictation. If the 
developer wants to use domain/task-dependent 
LMs, he/she must prepare example sentences for 
the target domain/task in the target language. 
4 Conclusions and future research 
We have proposed a method for rapid development 
of a spoken dialogue system based on CSRs and 
have compared the proposed method with the 
conventional method, which is based on RDB. 
In the current implementation, our system cannot 
handle the problem of the variation of the named 
entity which is dealt with by e.g. Hillard et al 
(2011). We are planning to examine the 
extensibility of the proposed framework by 
combining such refinement methods. 
Acknowledgments 
The present research was supported in part by the 
Ministry of Education, Science, Sports, and 
Culture through a Grant-in-Aid for Scientific 
Research (C), 22500153, 2010. 
References  
Masahiro Araki and Yuko Mizukami. 2011. 
Development of a Data-driven Framework for 
Multimodal Interactive Systems. In Proc. of IWSDS 
2011, 91-101. 
Masahiro Araki. submitted. An Automatic Construction 
Method of Spoken Query Understanding Component 
from Data Model Definition.  
Masahiro Araki and Daisuke Takegoshi. 2012. A Rapid 
Development Framework for Multilingual Spoken 
Dialogue Systems. In Proc. of COMPSAC 2012. 
Tobias Heinroth, Dan Denich and Gregor Bertrand. 
2009. Ontology-based Spoken Dialogue Modeling. In 
Proc. of the IWSDS 2009. 
Dustin Hillard, Asli ?elikyilmaz, Dilek Z. Hakkani-T?r, 
and G?khan T?r. 2011. Learning Weighted Entity 
Lists from Web Click Logs for Spoken Language 
Understanding. In Proc. of Interspeech 2011, 705-
708. 
Chiori Hori, Kiyonori Ohtake, Teruhisa Misu,. Hideki 
Kashioka and Satoshi Nakamura. 2009. Statistical 
Dialog Management Applied to WFST-based Dialog 
Systems. In Proc. of ICASSP 2009, 4793-4796. 
Satoru Kogure and Seiichi Nakagawa. 2001. A 
Development Tool for Spoken Dialogue Systems and 
Its Evaluation. In Proc. of TSD2001, 373-380. 
Jason D. Williams and Steve Young. 2007. Partially 
Observable Markov Decision Processes for Spoken 
Dialog Systems. Computer Speech and Language, 
21(2), 393-422. 
73
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 25?28,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Framework for the Development of Spoken Dialogue System 
based on Collaboratively Constructed Semantic Resources 
 
 
Masahiro Araki       Daisuke Takegoshi 
Department of Information Science 
Kyoto Institute of Technology 
Matsugasaki Sakyo-ku Kyoto 6068585 Japan 
araki@kit.ac.jp 
 
 
 
 
 
 
Abstract 
We herein introduce our project of realizing a 
framework for the development of a spoken 
dialogue system based on collaboratively con-
structed semantic resources. We demonstrate 
that a semantic Web-oriented approach based 
on collaboratively constructed semantic re-
sources significantly reduces troublesome rule 
descriptions and complex configurations, 
which are caused by the previous relational 
database-based approach, in the development 
process of spoken dialogue systems. In addi-
tion, we show that the proposed framework 
enables multilingual spoken dialogue system 
development due to clear separation of model, 
view and controller components. 
1 Introduction 
In recent years, some large scale repositories of 
collaboratively constructed semantic resources 
(CSRs), such as Freebase1 , are available online. 
Those semantically structured data enable more 
precise search than simple text matching (e.g. 
"Find a dental clinic near Kyoto station opens at 
Saturday night.") and more complex search than 
simple query to relational database (RDB) (e.g. a 
query "Find machine learning books written by a 
researcher of NLP." needs cross search on a book 
                                                          
1 http://www.freebase.com/ 
DB and a researcher DB). Since search conditions 
of such queries to the structured data become com-
plex, natural language, especially speech, for smart 
phone and tablet PC, is a promising method of que-
ry input. 
There are some previous researches on convert-
ing natural language input to the query of struc-
tured data (Lopez et al, 2006) (Tablan et al, 2008). 
These researches basically concentrated on the in-
put sentence analysis and the query construction. If 
the developer want to apply existing natural lan-
guage understanding methods to spoken dialogue 
system (SDS) for structured data search, there re-
mains fair amount of components that need to be 
implemented, such as speech input component, 
dialogue flow management, backend interface, etc. 
In order to realize a development environment 
of SDS for structured data search, we designed a 
data model driven framework for rapid prototyping 
of SDS based on CSRs. The proposed framework 
can be regarded as an extension of existing Rails 
framework of Web application to (1) enabling 
speech interaction and (2) utilizing a benefit of 
CSRs. By using CSRs and the extended Rails 
framework, the troublesome definitions of rules 
and templates for SDS prototyping can be reduced 
significantly compared with the ordinary RDB-
based approach.  
As this data model driven approach is independ-
ent of language for interaction, the proposed 
framework has a capability of easily implementing 
multilingual SDS. 
25
The remainder of the present paper is organized 
as follows. Section 2 describes the proposed ap-
proach to a data modeling driven development 
process for SDS based on CSRs and explains the 
automatic construction of the spoken query under-
standing component. Section 3 demonstrates the 
multilingual capability of the proposed framework. 
In Section 4, the present paper is concluded, and a 
discussion of future research is presented. 
2 Data modeling driven approach based 
on CSRs 
2.1 Object-oriented SDS development 
framework 
We previously proposed a data modeling driven 
framework for rapid prototyping of SDS (Araki 
2011). We designed a class library that is based on 
class hierarchy and attribute definitions of an exist-
ing semantic Web ontology, i.e., Schema.org2. This 
class library is used as a base class of an applica-
tion-specific class definition. An example of class 
definition is shown in Figure 1. 
 
 
Figure 1: Example of class definition. 
 
In this example, the "MyBook" class inherits all 
of the attributes of the "Book" class of Schema.org 
in the same manner as object-oriented program-
ming languages. The developer can limit the at-
tributes that are used in the target application by 
listing them in the constraints section of the class 
definition. On the other hand, the developer can 
add additional attributes (ranking attributes as the 
type of Integer, which is not defined in original 
"Book" class) in the definition of the class. 
The task type and dialogue initiative type are in-
dicated as annotations at the beginning of the class 
                                                          
2 http://schama.org/ 
definition. In this example, the task type is DB 
search and the initiative type is user initiative. This 
information is used in generating the controller 
code (state transition code, which is equivalent to 
Figure 2) and view codes of the target SDS. 
 Input
query
Display
result
Help
submit
help
modify
query
exit
Home
 
Figure 2: Control flow of the DB search task. 
 
Using Grails3, which is a Rails Web application 
framework, the proposed framework generates the 
dialogue controller code of the indicated task type 
and the view codes, which have speech interaction 
capability on the HTML5 code from this class def-
inition. The overall concept of the data modeling 
driven framework is shown in Figure 3. 
 Data model
definition
Mix-in of
traits
embed
application
logic
State
definition
generate
convert
Grails
Data model
definition
Groovy
generate
HTML5
code
Model
Controller
View
 
Figure 3: Overview of the data modeling driven 
SDS development framework. 
2.2 Using CSRs 
The disadvantage of our previous framework, de-
scribed in the previous subsection, is the high de-
pendence on the dictation performance of the 
speech recognition component. The automatically 
generated HTML5 code invokes dictation API, 
irrespective of the state of the dialogue and initia-
tive type. In order to improve speech recognition 
accuracy, grammar rules (in system initiative dia-
logue) and/or the use of a task/domain-dependent 
language model (in mixed/user initiative dialogue) 
                                                          
3 http://grails.org/ 
@DBSearch 
@SystemInitiative 
class MyBook extends Book { 
  Integer ranking 
  static constraints = { 
    name(onsearch:"like") 
    author(onsearch:"like") 
    publisher() 
    ranking(number:true) 
  } 
} 
26
are key factors. In our previous framework, the 
developer had to prepare these ASR-related com-
ponents using language resources, which are be-
yond the proposed data-driven framework. 
In order to overcome this defect, we add the 
Freebase class library, which is based on large-
scale CSRs, because Freebase already includes the 
contents of the data. These contents and a large-
scale Web corpus facilitate the construction of 
grammar rules and a language model that is specif-
ic to the target task/domain. 
For example, the Film class of Freebase has 
more than 191 thousand entries (as of May 2012), 
most of which have information about directors, 
cast members, genres, etc. These real data can be 
used as resources to improve ASR accuracy. 
In system initiative type dialogue, the contents 
of each attribute of the target class can construct 
word entries of the grammar rule for each attribute 
slot. For example, the grammar rule for the user's 
response to "Which genre of movie are you search-
ing for?" can be constructed from the contents of 
the genres attribute of the Film class. We imple-
mented a generator of the set of content words 
specified in the data model definition from the data 
of Freebase. The generator is embedded as one 
component of the proposed framework. 
In the mixed/user initiative type tasks, since 
content words and functional words make up the 
user's utterance, we need a language model for 
speech recognition and a semantic frame extractor 
for the construction of query to semantic data. We 
designed and implemented a language model gen-
erator and a semantic frame extractor using a func-
tional expression dictionary that corresponds to the 
attributes of Freebase (Araki submitted). The flow 
of the language model generation is shown in Fig-
ure 4. 
Freebase
data
Web
corpus
Data model
inition
content
words
in-domain
entries
domain
dependent
LM
example
sentences
 
Figure 4: Construction process of LM. 
2.3 Helper application for data definition 
In order to facilitate the data-model definition pro-
cess, we implemented a helper application called 
MrailsBuilder. A screenshot of one phase of the 
definition process is shown in Figure 5, which 
shows the necessary slots for data definition in the 
GUI and a list of properties once the developer 
selects the parent class of the target class. 
 
 
Figure 5: Screenshot of MrailsBuilder. 
3 Multilingual extension of the framework 
With the internationalization capability of the 
Grails base framework and multilingual data re-
sources provided as CSRs, we can generate a mul-
tilingual SDS from the data model definition. All 
of the language-dependent information is stored in 
separated property files and is called at the time of 
the dynamic view code generation process in the 
interaction, as shown in Figure 6. 
 
Please input 
search 
condit ns.
B ok of AI.
Below items are 
found.
 
 
Figure 6: Example of realized interaction. 
27
We also implemented a contents extractor from 
Freebase data. In Freebase, each class (called 
"type") belongs to one domain. For example, the 
"Dish" type belongs to the "Food & Drink" domain 
(see Figure 7). Although it assigned to a two-level 
hierarchy, each type has no inherited properties. 
Therefore, it is easy for Freebase data to represent 
a set of property values as a string instead of a uni-
form resource identifier (URI). Each instance has 
the name property and its value is written in Eng-
lish. For some instances, it also has the name de-
scription in another language with the language 
code. Therefore, we can extract the name of the 
instance in various languages. 
 
? Books
? Business
? Film
? Food&Drink
? ...
Domain
? Ingredient
? Restaurant
? Dish
? ...
Type
? 2,421 Instances
? Properties
Dish
Property Expected Type
Type of dish /food/type_of_dish
Cuisine /dining/cuisine
Typical ingredients /food/ingredient
Recipes /food/recipe
 
Figure 7: Domain and type of Freebase. 
 
The input of the contents extractor is the model 
definition code as in Figure 1 and the language 
code (e.g., "ja" for Japanese). As an example, the 
"MyDish" class is defined as shown in Figure 8. 
 
 
Figure 8: Model definition of the "MyDish" class. 
 
The contents extractor outputs the instance rec-
ords of the given language code and this instance 
can be used for LM generator explained in section 
2.2. For example, the extracted words in the case 
of "de" (German) is shown in Figures 9. 
 
Figure 9: German contents of the "MyDish" class. 
4 Conclusions and future research 
We have proposed a framework for development 
of a SDS on CSRs and have explained rapid con-
struction method of spoken query understanding 
component and showed its multilingual capability. 
In future research, we plan to evaluate the quan-
titative productivity of the proposed framework. 
Acknowledgments 
The present research was supported in part by the 
Ministry of Education, Science, Sports, and Cul-
ture through a Grant-in-Aid for Scientific Research 
(C), 22500153, 2010. 
References  
Masahiro Araki and Yuko Mizukami. 2011. Develop-
ment of a Data-driven Framework for Multimodal In-
teractive Systems. In Proc. of IWSDS 2011, 91-101. 
Masahiro Araki and Daisuke Takegoshi. accepted. A 
Rapid Development Framework for Multilingual 
Spoken Dialogue Systems. In Proc. of IEEE 
COMPSAC 2012. 
Masahiro Araki. submitted. An Automatic Construction 
Method of Spoken Query Understanding Component 
from Data Model Definition.  
Vanessa Lopez, Enrico Motta, and Victoria S. Uren. 
2006, AquaLog: An ontology-driven Question An-
swering System to interface the Semantic Web. In 
Proc. of HLT-NAACL 2006, 269-272. 
Valentin Tablan, Danica Damljanovic, and Kalina 
Bontcheva. 2008, A natural language query interface 
to structured information. In Proc. of the 5h Europe-
an Semantic Web Conference (ESWC 2008). 
@DBSearch 
@UserInitiative 
class MyDish extends Dish { 
  static constraints = { 
    name() 
    type_of_dish1(nullable:true) 
    cuisine(nullable:true) 
    ingredients(nullable:true) 
    recipes(nullable:true) 
  } 
} 
28
