371
372
373
374
375
376
377
378
Automatic Measuring of English Language Proficiency using MT
Evaluation Technology
Keiji Yasuda
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
keiji.yasuda@atr.jp
Fumiaki Sugaya
KDDI R&D Laboratories
2-1-15, Ohara, Kamifukuoka-city,
Saitama, 356-8502, Japan
fsugaya@kddilabs.jp
Eiichiro Sumita
ATR Spoken Language Translation
Research Laboratories
Department of NLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
eiichiro.sumita@atr.jp
Toshiyuki Takezawa
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
toshiyuki.takezawa@atr.jp
Genichiro Kikui
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
genichiro.kikui@atr.jp
Seiichi Yamamoto
ATR Spoken Language Translation
Research Laboratories
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
seiichi.yamamoto@atr.jp
Abstract
Assisting in foreign language learning is one of
the major areas in which natural language pro-
cessing technology can contribute. This paper
proposes a computerized method of measuring
communicative skill in English as a foreign lan-
guage. The proposed method consists of two
parts. The first part involves a test sentence
selection part to achieve precise measurement
with a small test set. The second part is the ac-
tual measurement, which has three steps. Step
one asks proficiency-known human subjects to
translate Japanese sentences into English. Step
two gauges the match between the translations
of the subjects and correct translations based
on the n-gram overlap or the edit distance be-
tween translations. Step three learns the rela-
tionship between proficiency and match. By re-
gression it finds a straight-line fitting for the
scatter plot representing the proficiency and
matches of the subjects. Then, it estimates pro-
ficiency of proficiency-unknown users by using
the line and the match. Based on this approach,
we conducted experiments on estimating the
Test of English for International Communica-
tion (TOEIC) score. We collected two sets of
data consisting of English sentences translated
from Japanese. The first set consists of 330 sen-
tences, each translated to English by 29 subjects
with varied English proficiency. The second set
consists of 510 sentences translated in a similar
manner by a separate group of 18 subjects. We
found that the estimated scores correlated with
the actual scores.
1 Introduction
For effective second language learning, it is ab-
solutely necessary to test proficiency in the sec-
ond language. This testing can help in selecting
educational materials before learning, checking
learners? understanding after learning, and so
on.
To make learning efficient, it is important to
achieve testing with a short turnaround time.
Computer-based testing is one solution for this,
and several kinds of tests have been developed,
including CASEC (CASEC, 2004) and TOEFL-
CBT (TOEFL, 2004). However, these tests are
mainly based on cloze testing or multiple-choice
questions. Consequently, they require labour
costs for expert examination designers to make
the questions and the alternative ?detractor?
answers.
In this paper, we propose a method for the au-
tomatic measurement of English language pro-
ficiency by applying automatic evaluation tech-
niques. The proposed method selects adequate
test sentences from an existing corpus. Then,
it automatically evaluates the translations of
test sentences done by users. The core tech-
nology of the proposed method, i.e., the auto-
matic evaluation of translations, was developed
in research aiming at the efficient development
of Machine Translation (MT) technology (Su et
al., 1992; Papineni et al, 2002; NIST, 2002).
In the proposed method, we apply these MT
evaluation technologies to the measurement of
human English language proficiency. The pro-
posed method focuses on measuring the commu-
nicative skill of structuring sentences, which is
indispensable for writing and speaking. It does
not measure elementary capabilities including
vocabulary or grammar. This method also pro-
poses a test sentence selection scheme to enable
efficient testing.
Section 2 describes several automatic evalua-
tion methods applied to the proposed method.
Section 3 introduces the proposed evaluation
scheme. Section 4 shows the evaluation results
obtained by the proposed method. Section 5
concludes the paper.
2 MT Evaluation Technologies
In this section, we briefly describe automatic
evaluation methods of translation. These meth-
ods were proposed to evaluate MT output, but
they are applicable to translation by humans.
All of these methods are based on the same
idea, that is, to compare the target transla-
tion for evaluation with high-quality reference
translations that are usually done by skilled
translators. Therefore, these methods require a
corpus of high-quality human reference transla-
tions. We call these translations as ?references?.
2.1 DP-based Method
The DP score between a translation output and
references can be calculated by DP matching
(Su et al, 1992; Takezawa et al, 1999). First,
we define the DP score between sentence (i.e.,
word array) Wa and sentence Wb by the follow-
ing formula.
SDP (Wa,Wb) = T ? S ? I ?DT (1)
where T is the total number of words in Wa, S is
the number of substitution words for comparing
Wa to Wb, I is the number of inserted words for
comparing Wa to Wb, and D is the number of
deleted words for comparing Wa to Wb.
Using Equation 1, (Si(j)), that is, the test
sentence unit DP-score of the translation of test
sentence j done by subject i, can be calculated
by the following formula.
SDPi(j) =
max
k=1 to Nref
{
SDP (Wref(k)(j),Wsub(i)(j)), 0
}
(2)
where Nref is the number of references,
Wref(k)(j) is the k-th reference of the test sen-
tence j, and Wsub(i)(j) is the translation of the
test sentence j done by subject i.
Finally, SDPi , which is the test set unit DP-
score of subject i, can be calculated by the fol-
lowing formula.
SDPi =
1
Nsent
Nsent?
j=1
SDPi(j) (3)
where Nsent is the number of test sentences.
2.2 N-gram-based Method
Papineni et al (2002) proposed BLEU, which is
an automatic method for evaluating MT qual-
ity using N -gram matching. The National Insti-
tute of Standards and Technology also proposed
an automatic evaluation method called NIST
(2002), which is a modified method of BLEU.
In this research we use two kinds of units to
apply BLEU and NIST. One is a test sentence
unit and the other is a test set unit. The unit of
utterance corresponds to the unit of ?segment?
in the original BLEU and NIST studies (Pap-
ineni et al, 2002; NIST, 2002).
Equation 4 is the test sentence unit BLEU
score formulation of the translation of test sen-
tence j done by subject i.
SBLEUi (j) =
exp
{ N?
n=1
wn log(pn)?max
(L?ref
Lsys ? 1, 0
)}
(4)
where
pn =?
C?{Candidates}
?
n?gram?{C} Countclip (n?gram)?
C?{Candidates}
?
n?gram?{C} Count(n?gram)
wn = N?1
and
L?ref = the number of words in the reference
translation that is closest in length to the
translation being scored
Lsys = the number of words in the transla-
tion being scored
Equation 5 is the test sentence unit NIST
score formulation of the translation of test sen-
tence j done by subject i.
SNISTi(j) =
?N
n=1
{?
all w1...wn in sys output
info(w1...wn)?
all w1...wn in sys output
(1)
}
?exp
{
? log2
[
min
(
Lsys
Lref , 1
)]}
(5)
where
info(w1 . . . wn) =
log2
( the number of occurence of w1...wn?1
the number of occurence of w1...wn
)
Lref = the average number of words in a ref-
erence translation, averaged over all refer-
ence translations
Lsys = the number of words in the transla-
tion being scored
and ? is chosen to make the brevity penalty fac-
tor=0.5 when the number of words in the sys-
tem translation is 2/3 of the average number
of words in the reference translation. For Equa-
tions 4 and 7, N indicates the maximum n-gram
length. In this research we set N to 4 for BLEU
and to 5 for NIST.
We may consider the unit of the test set cor-
responding to the unit of ?document? or ?sys-
tem? in BLEU and NIST. However, we use for-
mulations for the test set unit scores that are
different from those of the original BLEU and
NIST.
Calculate correlation 
between TOEIC score and 
sentence unit automatic score
References translated
by bilinguals
English writing by 
proficiency-known
human subjects
English sentences
by proficiency
Japanese test set
Automatic evaluation
(sentence unit evaluation)
Corpus
Select test sentences
based on correlation
Figure 1: Flow of Test Set Selection
The test set unit scores of BLEU and NIST
are calculated by Equations 6 and 7.
SBLEUi =
1
Nsent
Nsent?
j=1
SBLEUi(j) (6)
SNISTi =
1
Nsent
Nsent?
j=1
SNISTi(j) (7)
3 The Proposed Method
The proposed method described in this paper
consists of two parts. One is the test set selec-
tion part and the other is the actual measure-
ment part. The measurement part is divided
into two phases: a parameter-estimation phase
and a testing phase. Here, we use the term ?sub-
jects? to refer to the human subjects in the test
set selection part and the parameter-estimation
phase of the measurement part; we use ?users?
to refer to the humans in the testing phase of
the measurement part.
Regression analysis using
proficiency and automatic
scores
References translated
by bilinguals
English writing by 
proficiency-known 
human subjects
English sentences
by proficiency
Japanese test set
Regression 
coefficient
Automatic evaluation
(Test set unit evaluation)
English writing by a user
Automatic evaluation
Estimation of English
proficiency
English sentences
Automatic score
English
proficiency
?Testing phase?
Corpus
?Parameter-estimation phase?
Figure 2: Flow of English Proficiency Measurment
We employ the Test of English for Interna-
tional Communication (TOEIC, 2004) as an ob-
jective measure of English proficiency.
3.1 Test Sentence Selection Method
Figure 1 shows the flow of the test sentence se-
lection. We first calculate the test sentence
unit automatic score by using Equation 2, 4 or
5 for each test sentence and subject. Second,
for each test sentence, we calculate the correla-
tion between the automatic scores and subjects?
TOEIC scores. Finally, using the above results,
we choose the test sentences that give high cor-
relation.
3.2 Method of Measuring English
Proficiency
Figure 2 shows the flow of measuring English
proficiency. In the parameter-estimation phase,
for each subject, we first calculate the test set
unit automatic score by using Equation 3, 6 or
7. Next, we apply regression analysis using the
automatic scores and subjects? TOEIC scores.
In the testing phase, we calculate a user?s
TOEIC score using the automatic score of the
user and the regression line calculated in the
parameter-estimation phase.
4 Experiments
4.1 Experimental Conditions
4.1.1 Test sets
For the experiments, we employ two differ-
ent test sets. One is BTEC (Basic Travel
Expression Corpus) (Takezawa et al, 2002)
and the other is SLTA1 (Takezawa, 1999).
Both BTEC and SLTA1 are parts of bilingual
corpora that have been collected for research
on speech translation systems. However, they
have different features. A detailed analysis
of these corpora was done by Kikui et al
(2003). Here, we briefly explain these test sets.
In this study, we use the Japanese side as a
test set and the English side as a reference for
automatic evaluation.
BTEC
BTEC was designed to cover expressions for
every potential subject in travel conversation.
This test set was collected by investigating
?phrasebooks? that contain Japanese/English
sentence pairs that experts consider useful for
tourists traveling abroad. One sentence con-
tains 8 words on average. The test set for this
experiment consists of 510 sentences from the
BTEC corpus.
The total number of examinees is 18, and
the range of their TOEIC scores is between the
400s and 900s. Every hundred-point range has
3 examinees.
SLTA1
SLTA1 consists of 330 sentences in 23 conver-
sations from the ATR bilingual travel conver-
sation database (Takezawa, 1999). One sen-
tence contains 13 words on average. This corpus
was collected by simulated dialogues between
Japanese and English speakers through a pro-
fessional interpreter. The topics of the conver-
sations are mainly hotel conversations, such as
reservations, enquiries and so on.
The total number of examinees is 29, and the
range of their TOEIC score is between the 300s
and 800s. Excluding the 600s, every hundred-
point range has 5 examinees.
4.1.2 Reference
For the automatic evaluation, we collected 16
references for each test sentence. One of them
is from the English side of the test set, and the
remaining 15 were translated by 5 bilinguals (3
references by 1 bilingual).
4.2 Experimental Results
4.2.1 Experimental Results of Test Set
Selection
Figures 3 and 4 show the correlation between
the test sentence unit automatic score and the
subjects? TOEIC score. Here, the automatic
score is calculated using Equation 2, 4 or 5. Fig-
ure 3 shows the results on BTEC, and Fig. 4
shows the results on SLTA1. In these fig-
ures, the ordinate represents the correlation.
The filled circles indicate the results using the
DP-based automatic evaluation method. The
gray circles indicate the results using BLEU.
The empty circles indicate the results using
NIST. Looking at these figures, we find that
the three automatic evaluation methods show
a similar tendency. Comparing BTEC and
SLTA1, BTEC contains more cumbersome test
sentences. In BTEC, about 20% of the test sen-
tences give a correlation of less than 0. Mean-
while, in the SLTA1, this percentage is about
10%.
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Test sentence (sorted by correlation)
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 3: Correlation between test sentence unit
automatic scores and subjects? TOEIC scores
(BTEC)
Table 1 shows examples of low-correlated test
sentences. As shown in the table, BTEC con-
tains more short and frequently used expres-
sions than does SLTA1. This kind of expres-
sion is thought to be too easy for testing, so
this low-correlation phenomenon is thought to
occur. SLTA1 still contains a few sentences of
this kind (?Example 1? of SLTA1 in the ta-
ble). Additionally, there is another contributing
factor explaining the low correlation in SLTA1.
Looking at ?Example 2? of SLTA1 in the ta-
ble, this expression is not very easy to translate.
For this test sentence, several expressions can
be produced as an English translation. Thus,
automatic evaluation methods cannot evaluate
correctly due to the insufficient variety of ref-
erences. Considering these results, this method
can remove inadequate test sentences due not
only to the easiness of the test sentence but
also to the difficulty of the automatic evalua-
tion. Figures 5 and 6 show the relationship
between the number of test sentences and cor-
relation. This correlation is calculated between
the test set unit automatic scores and the sub-
jects? TOEIC scores. Here, the automatic score
is calculated using Equation 3, 6 or 7. Figure
5 shows the results on BTEC, and Fig. 6 shows
the results on SLTA1.
In these figures, the abscissa represents the
number of test sentences, i.e., Nsent in Equa-
tions 3, 6 and 7, and the ordinate represents
the correlation. Definitions of the circles are
the same as those in the previous figure. Here,
the test sentence selection is based on the cor-
relation shown in Figs. 3 and 4.
Comparing Fig. 5 to Fig. 6, in the case of
Table 1: Example of low-correlated test sentences
Japanese English
Example 1
???????
Good night.
Example 2
????????????
Can I see a menu, please?
Example 1
??????????????????
Yes, with my Mastercard please
Example 2
???????????????????????
????????????????????????
I wish I could take that but we have a limited budget so
how much will that cost?
S
L
T
A
1
B
T
E
C
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120 150 180 210 240 270 300 330
Test sentence (sorted by correlation)
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 4: Correlation between test sentence unit
automatic scores and subjects? TOEIC scores
(SLTA1)
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Number of test sentences
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 5: Correlation between test set unit
automatic scores and subjects? TOEIC scores
(BTEC)
using the full test set (510 test sentences for
BTEC, 330 test sentences for SLTA1), the cor-
relation of BTEC is lower than that of SLTA1.
As we mentioned above, the ratio of the low-
correlated test sentences in BTEC is higher than
that of SLTA1 (See Figs. 3 and 4). This issue
is thought to cause a decrease in the correlation
shown in Fig. 5. However, by applying the se-
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 30 60 90 120 150 180 210 240 270 300 330
Number of test sentences
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 6: Correlation between test set unit
automatic scores and subjects? TOEIC scores
(SLTA1)
50
100
150
200
250
300
350
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Number of test sentences
S
t
a
n
d
a
r
d
 
e
r
r
o
r
DP
BLEU
NIST
Figure 7: Standard error (BTEC)
lection based on sentence unit correlation, these
obstructive test sentences can be removed. This
permits the selection of high-correlated small-
sized test sets. In these figures, the highest cor-
relations are around 0.95.
4.2.2 Experimental Results of English
Proficiency Measurement
For the experiments on English proficiency mea-
surement, we carried out a leave-one-out cross
validation test. The leave-one-out cross valida-
50
100
150
200
250
300
350
0 30 60 90 120 150 180 210 240 270 300 330
Number of test sentences
S
t
a
n
d
a
r
d
 
e
r
r
o
r
DP
BLEU
NIST
Figure 8: Standard error (SLTA1)
tion test is conducted not only for the measure-
ment of the English proficiency but also for the
test set selection.
To evaluate the proficiency measurement by
the proposed method, we calculate the standard
error of the results of a leave-one-out cross val-
idation test. The following formula is the defi-
nition of the standard error.
?E =
???? 1
Nuser
Nuser?
i=1
(Ti ?Ai)2 (8)
where Nuser is the number of users, Ti is the
actual TOEIC score of user i, and Ai is user i?s
estimated TOEIC score by using the proposed
method.
Figures 7 and 8 show the relationship between
the number of test sentences and the standard
error.
In these figures, the abscissa represents the
number of test sentences, and the ordinate rep-
resents the standard error. Definitions of the
circles are the same as in the previous figure.
Here, the test sentence selection is based on the
correlation shown in Figs. 3 and 4.
Looking at Figs. 7 and 8, we can observe dif-
ferences between the standard errors of BTEC
and SLTA1. This is thought to be due to the
difference of the number of subjects in the ex-
periments (for the leave-one-out cross valida-
tion test, 17 subjects with BTEC and 28 sub-
jects with SLTA1). Even though these were
closed experiments, the results in Figs. 5 and
6 show an even higher correlation with BTEC
than with SLTA1 at the highest point. There-
fore, there is room for improvement by increas-
ing the number of subjects with BTEC.
In the test using 30 to 60 test sentences in
Figs. 7 and 8, the standard errors are much
smaller than in the test using the full test set
(510 test sentences for BTEC, 330 test sentences
for SLTA1). These results imply that the test
set selection works very well and that it enables
precise testing using a smaller size test set.
5 Conclusion
We proposed an automatic measurement
method for English language proficiency. The
proposed method applies automatic MT evalu-
ation to measure human English language pro-
ficiency. This method focuses on measuring the
communicative skill of structuring sentences,
which is indispensable in writing and speaking.
However, it does not measure elementary capa-
bilities such as vocabulary and grammar. The
method also involves a new test sentence selec-
tion scheme to enable efficient testing.
In the experiments, we used TOEIC as an ob-
jective measure of English language proficiency.
We then applied some currently available auto-
matic evaluation methods: BLEU, NIST and a
DP-based method. We carried out experiments
on two test sets: BTEC and SLTA1. Accord-
ing to the experimental results, the proposed
method gave a good measurement result on a
small-sized test set. The standard error of mea-
surement is around 120 points on the TOEIC
score with BTEC and less than 100 TOEIC
points score with SLTA1. In both cases, the
optimum size of the test set is 30 to 60 test sen-
tences.
The proposed method still needs human
labour to make the references. To obtain higher
portability, we will apply an automatic para-
phrase scheme (Finch et al, 2002; Shimohata
and Sumita, 2002) to make the references auto-
matically.
6 Acknowledgements
The research reported here was supported in
part by a contract with the National Institute
of Information and Communications Technol-
ogy entitled ?A study of speech dialogue trans-
lation technology based on a large corpus?.
References
CASEC. 2004. Computer Assessment
System for English Communication.
http://www.ets.org/toefl/.
A. Finch, T. Watanabe, and E. Sumita. 2002.
?Paraphrasing by Statistical Machine Trans-
lation?. In Proceedings of the 1st Forum on
Information Technology (FIT2002), volume
E-53, pages 187?188.
G. Kikui, E. Sumita, T. Takezawa, and
S. Yamamoto. 2003. ?Creating Corpora for
Speech-to-Speech Translation?. In Proceed-
ings of EUROSPEECH, pages 381?384.
NIST. 2002. Automatic Evaluation
of Machine Translation Quality Us-
ing N-gram Co-Occurence Statistics.
http://www.nist.gov/speech/tests/mt
/mt2001/resource/.
K. Papineni, S. Roukos, T. Ward, and W.-
J. Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics (ACL), pages 311?318.
M. Shimohata and E. Sumita. 2002. ?Auto-
matic Paraphrasing Based on Parallel Corpus
for Normalization?. In Proceedings of Inter-
national Conference on Language Resources
and Evaluation (LREC), pages 453?457.
K.-Y. Su, M.-W. Wu, and J.-S. Chang. 1992.
A new quantitative quality measure for ma-
chine translation systems. In Proceedings of
the 14th International Conference on Com-
putational Linguistics(COLING), pages 433?
439.
T. Takezawa, F. Sugaya, A. Yokoo, and S. Ya-
mamoto. 1999. A new evaluation method for
speech translation systems and a case study
on ATR-MATRIX from Japanese to English.
In Proceeding of Machine Translation Summit
(MT Summit), pages 299?307.
T. Takezawa, E. Sumita, F. Sugaya, H. Ya-
mamoto, and S. Yamamoto. 2002. ?Toward a
Broad-Coverage Bilingual Corpus for Speech
Translation of Travel Conversations in the
Real World?. In Proceedings of International
Conference on Language Resources and Eval-
uation (LREC), pages 147?152.
T. Takezawa. 1999. Building a bilingual travel
conversation database for speech translation
research. In Proceedings of the 2nd Inter-
national Workshop on East-Asian Language
Resources and Evaluation ? Oriental CO-
COSDA Workshop ?99 ?, pages 17?20.
TOEFL. 2004. Test of English as a Foreign
Language. http://www.ets.org/toefl/.
TOEIC. 2004. Test of English
for International Communication.
http://www.ets.org/toeic/.
Proceedings of the Third Workshop on Statistical Machine Translation, pages 216?223,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improved Statistical Machine Translation by Multiple Chinese Word
Segmentation
Ruiqiang Zhang1,2 and Keiji Yasuda1,2 and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Science City, Kyoto, 619-0288, Japan
{ruiqiang.zhang,keiji.yasuda,eiichiro.sumita}@atr.jp
Abstract
Chinese word segmentation (CWS) is a
necessary step in Chinese-English statisti-
cal machine translation (SMT) and its per-
formance has an impact on the results of
SMT. However, there are many settings in-
volved in creating a CWS system such as
various specifications and CWS methods.
This paper investigates the effect of these
settings to SMT. We tested dictionary-
based and CRF-based approaches and
found there was no significant difference
between the two in the qualty of the result-
ing translations. We also found the corre-
lation between the CWS F-score and SMT
BLEU score was very weak. This paper
also proposes two methods of combining
advantages of different specifications: a
simple concatenation of training data and
a feature interpolation approach in which
the same types of features of translation
models from various CWS schemes are
linearly interpolated. We found these ap-
proaches were very effective in improving
quality of translations.
1 Introduction
Chinese word segmentation (CWS) is a necessary
step in Chinese-English statistical machine transla-
tion (SMT). The research on CWS independently
from SMT has been conducted for decades. As an
evidence, the CWS evaluation campaign, the Sighan
Bakeoff (Emerson, 2005),1, has been held four times
since 2004. However, works on relations between
CWS and SMT are scarce.
Generally, two factors need to be considered in
constructing a CWS system. The first one is the
specifications for CWS, i.e., the rules or guidelines
for word segmentation, and the second one is the
CWS methods. There are many CWS specifications
used by different organizations. Unfortunately, these
organizations do not seem to have any intention of
reaching a unified specification. More than five or
six specifications have been used in the four Sighan
Bakeoffs. There is also significant disagreement on
the specifications, although much of their contents is
the same. One of the aims of this work was therefore
to establish whether inconsistencies in specifications
significantly affect the quality of SMT.
The second factor is CWS methods. We grouped
all of the CWS methods into two classes: the class
without out-of-vocabulary (OOV) recognition and
the class with OOV recognition, represented by the
dictionary-based CWS and the CRF-based CWS, re-
spectively. Out-of-vocabulary recognition may have
two-sided effects on SMT performance. The CRF-
based CWS that supports OOV recognition produces
word segmentations with a higher F-score, but a
huge number of new words recognized correctly and
incorrectly that can incur data sparseness in training
the SMT models. On the other hand, the dictionary-
based approach that does not support OOV recogni-
tion produced a lower F-score, but with a relatively
weak data spareness problem. Which approach pro-
1A CWS competition organized by the ACL special interest
group on Chinese.
216
Table 1: Examples of disagreement in segmentation guidelines
ChineseName EnglishName Time
AS DENGXIAOPING GEORGE BUSH 1997YEAR 7MONTH 1DAY
CITYU DENGXIAOPING GEORGEBUSH 1997 YEAR 7 MONTH 1 DAY
MSR DENGXIAOPING GEORGEBUSH 1997YEAR7MONTH1DAY
PKU DENG XIAOPING GEORGEBUSH 1997YEAR 7MONTH 1DAY
Table 2: A second example of disagreement in segmentation guidelines
Composite words Composite words
AS FUJITSUCOMPANY EUROZONE
CITYU FUJITSU COMPANY EUROZONE
MSR FUJITSUCOMPANY EURO ZONE
PKU FUJITSU COMPANY EUROZONE
duces a better SMT result is our research interest in
this work.
The performance of CWS is usually measured by
the F-score, while that of SMT is measured using
the BLEU score. Does a CWS with a higher F-
score produce a better translation? In this paper
we answer this question by comparing F-scores with
BLEU scores.
In this work, we also propose approaches to make
use of all the Sighan training data regardless of the
specifications. Two methods are proposed: (1) a
simple combination of all the training data, and (2)
implementing linear interpolation of multiple trans-
lation models. Linear interpolation is widely used in
language modeling for speech recognition. We in-
terpolated multiple translation models generated by
the CWS schemes and found our approaches were
very effective in improving the translations.
2 CWS specifications and corpora from
the second Sighan Bakeoff
A Chinese word is composed of one or more char-
acters. There are no spaces between the words.
Automatic word segmentation is required for ma-
chine translation. Usually a specification is needed
to carry out word segmentation. Unfortunately, there
are many different versions of specifications. Differ-
ent tasks give rise to different requirements and the
CWS specifications must be adjusted accordingly.
For example, shorter segmentation has been shown
to be better for speech recognition. A composite
word (numbers, dates, times, etc.) is split into char-
acters even if it is one word defined by linguists. In
contrast, longer segmentation is preferred for named
entity recognition consisting of longer character se-
quences, such as the name of people, places, and or-
ganizations.
This work investigated four well-known spec-
ifications created by four different organizations:
Academia Sinica (AS), City University of Hong
Kong (CITYU), Microsoft Research (Beijing)
(MSR), and Beijing University (PKU). These specs
were used in the second Sighan Bakeoff (Emerson,
2005). When we compared the four specifications
and the manual segmentations in the Sighan Bakeoff
training data, we found there were many inconsis-
tencies among the four specifications. Some exam-
ples are shown in Table 1 and 2. For instance, the
AS and PKU specifications are distinct in splitting
both Chinese and English names. We also found the
MSR specification generated more composite words
and grouped longer character sequences into a word.
Using this specification could generate tens of thou-
sands of new words, which can cause data sparse-
ness for SMT.
In addition to using the four specifications, we
also downloaded the training and test corpora of the
second Sighan Bakeoff. We used each of the train-
ing corpora provided to create a CWS scheme and
evaluated the performance of the schemes on our test
217
data. This enabled us to examine the effect of CWS
specifications on SMT.
We used a Chinese word segmentation tool,
Achilles, to implement word segmentation. Part of
the work using this tool was described by (Zhang
et al, 2006). The approach was reported to achieve
the highest word segmentation accuracy using the
data from the second Sighan Bakeoff. Moreover,
this tool meets our need to test the effect of the two
kinds of CWS approaches for SMT. We can easily
train a dictionary-based and a CRF-based CWS by
using this tool. By turning the program?s option for
the CRF model on and off, we can use the Achilles
as a dictionary-based approach and as a CRF-based
CWS. In fact, the dictionary-based approach is the
default approach for Achilles.
3 Experiments
3.1 SMT resources
We followed the instructions for the 2005 NIST MT
evaluation campaign. Training the translation mod-
els for our SMT system used the available LDC par-
allel data except the UN corpus. To train the lan-
guage models for English, we used all the avail-
able English parallel data plus Xinhua News of the
LDC Gigaword English corpus, LDC2005T12. In
summary, we used 2.4 million parallel sentences for
training the translation model. We used the test data
defined in the NIST MT05 evaluation which is de-
fined in the LDC corpus as LDC2006E38. We used
the corpus, LDC2006E43, as the development data
for loglinear model optimization.
We used a phrase-based SMT system that is based
on a log-linear model incorporating multiple fea-
tures. The training and decoding system of our SMT
used the publicly available Pharaoh (Koehn et al,
2003)2. GIZA++ was used for word alignment.
The Pharaoh decoder was used exclusively in
all the experiments. No additional features but
the defaults defined by Pharaoh were used. The
feature weights were optimized against the BLEU
scores (Och, 2003).
We chose automatic metrics to evaluate CWS and
SMT. We used the F-score for CWS and BLEU for
SMT. The BLEU is BLEU4, computed using the
NIST-provided ?mt-eval? script.
2http://www.iccs.informatics.ed.ac.uk/?pkoehn
3.2 Implementation of CWS schemes
To determine the effect of CWS on SMT, we cre-
ated 14 CWS schemes which are shown in Ta-
ble 3. Schemes 1 to 12 were implemented using
the in-house tool, Achilles, and schemes 13 and 14
using off-the-shelf tools. The CWS schemes are
named according to the specifications (AS, CITYU,
MSR, PKU), implementing methods (CRF-based or
dictionary-based), and lexicon sources (Sighan or
LDC corpus). The table also shows the results of
segmentation on the SMT training and test data, i.e.,
number of total tokens, unique words, and OOV
words.
We divided the schemes into two groups for sim-
plicity. The first group includes schemes 1 to 12,
which were trained using a specific Sighan corpus.
For example, schemes 1 to 3 were trained using the
AS corpus, schemes 4 to 6 using the CITYU cor-
pus, and so on. The meaning of the name of the
CWS scheme can be derived from the table ? the
name is defined by specifications, methods and lexi-
con sources. For example, the CRF-AS scheme per-
forms CRF-based segmentation; and its lexicon is
from the AS corpus provided by the Sighan. The
CRF-AS segmenter can be easily trained, as de-
scribed by Achilles.
The second group contains two schemes 13 and
14. The ICTCLAS is a HHMM-based hierarchical
HMM segmenter (Zhang et al, 2003) that uses the
specifications of PKU. This segmenter incorporates
parts-of-speech information in the probability mod-
els and generates multiple HMM models for solving
segmentation ambiguities. The MSRSEG was de-
veloped by Gao et al (Gao et al, 2004). This seg-
menter is based on the MSR specifications. It uses a
log-linear model that integrates multiple features.
The segmenters of the first group, dict-AS
and dict-LDC-AS, are two dictionary-based CWS
schemes. They differ in lexicon size and lexicon
extracting source. The former used a lexicon ex-
tracted directly from the Sighan AS training data
while the latter used a lexicon from LDC parallel
corpora. It took some efforts to get the lexicon. First,
we used the CRF-AS to segment the LDC corpora.
We extracted a unique word list from the segmented
data and sorted it in decreasing order according to
word frequency. Because OOV was recognized by
218
Table 3: Analysis of results of segmentation on LDC training and test data for all CWS schemes
No. CWS schemes Specifications Methods Lexicon Tokens Unique words OOVs
1 CRF-AS AS CRF Sighan 47,934,088 413,588 1,193
2 dict-AS AS Dict Sighan 51,664,675 89,346 237
3 dict-LDC-AS AS Dict LDC 48,665,364 102,919 273
4 CRF-CITYU CITYU CRF Sighan 47,963,541 426,273 1,155
5 dict-CITYU CITYU Dict Sighan 51,251,729 56,996 362
6 dict-LDC-CITYU CITYU Dict LDC 48,787,154 102,754 217
7 CRF-MSR MSR CRF Sighan 46,483,923 523,788 1,297
8 dict-MSR MSR Dict Sighan 51,302,509 60,247 248
9 dict-LDC-MSR MSR Dict LDC 47,469,271 102,390 217
10 CRF-PKU PKU CRF Sighan 48,022,697 440,114 1,136
11 dict-PKU PKU Dict Sighan 52,721,809 47,176 211
12 dict-LDC-PKU PKU Dict LDC 48,721,795 102,213 256
13 ICTCLAS PKU HHMM - 50,751,402 162,222 835
14 MSRSEG MSR - - ?48,734,113 274,411 1,443
the CRF-AS, a huge word list was generated(see Ta-
ble 3). We chose the most frequent 100,000 words
as the dictionary for the dict-LDC-AS 3. The LM for
the dict-AS was trained using the AS corpus while
the LM for the dict-LDC-AS was trained using the
segmented SMT training corpus.
Therefore, the dict-LDC-AS used a larger lexicon
than the dict-AS. This lexicon contained the most
frequent OOV words recognized by the CRF-AS.
Our aim was to investigate whether the dict-LDC-
AS, whose lexicon consisted of the lexicon of dict-
AS and new words recognized by CRF-AS, could
improve SMT.
As shown in Table 3, using CRF-AS generated a
huge number of unique words for the training data
and OOV words for the test data. We found that
the CRF-AS generated three times more OOVs for
the test data than the dictionary-based CWS,dict-AS
(see OOVs in Table 3).
Other schemes in the first group were imple-
mented similarly to the ?AS?.
Table 3 lists the segmentation statistics for the
training and test data of all the tested CWS schemes,
where ?Tokens? indicates the total number of words
in the training data. ?Unique words? and ?OOVs?
3Only those words that appeared at least five times in the
lexicon were considered.
Table 4: BLEU scores for CWS schemes
CWS AS CITYU MSR PKU
CRF 23.70 23.55 22.50 23.61
dict 23.46 23.72 23.33 23.61
dict-LDC 23.52 23.36 23.16 23.74
ICTCLAS - - - 24.12
MSRSEG - - 19.72 -
BEST 23.70 23.72 23.33 23.74 (24.12)
mean the lexicon size of the segmented training data
and the unknown words in the test data, respectively.
3.3 Effect of CWS specifications on SMT
Our first concern was the effect of CWS specifica-
tions on SMT. The results in Table 4 show the rela-
tionships that were found. The last row gives the
best BLEU scores obtained for each of the CWS
specifications. The scores for AS, CITYU, MSR and
PKU were 23.70 (CRF-AS), 23.72 (dict-CITYU),
23.33 (dict-MSR) and 23.74 (dict-PKU-LDC), re-
spectively. We found there were no observable dif-
ferences between AS, CITYU, and PKU. However,
the specification that produced the worst transla-
tions was the MSR. The MSR specification appears
219
to have been designed for recognizing named enti-
ties (NE) (See the examples of segmentation in Ta-
ble 1). Many NEs are regarded as words by MSR,
while they are more appropriately split into sepa-
rate words by other specifications. For example, the
long word, ?1997YEAR7MONTH1DAY? (?July 1,
1997?). As a result, the CRF-MSR generated 20%
more words in the vocabulary than the other CWS
schemes in segmenting the SMT training data. The
larger vocabulary can trigger data sparseness prob-
lems and result in SMT degradation. The segmenter,
MSRSEG, produced an even lower BLEU score
(19.72) than the Achilles.
The results were verified by significance
test (Zhang et al, 2004). We found the systems
with the BLEU scores higher than 23.70 were
significantly better than those lower than 23.70.
3.4 Correlation between BLEU score and
F-score
The values of the F-scores and BLEU scores are
listed in parallel in Table 5. We tied the F-scores
and specifications together because comparing the
value of the F-score across specs is meaningless. We
separated the F-score and BLEU score for different
corpus. The F-score was calculated using the Sighan
test data. The CRF-based approach usually gives a
higher F-score, but its corresponding BLEU scores
were not always higher. The F-score and BLEU
score correlated well for ICTCLAS and CRF-AS
but less well for CRF-CITYU, CRF-PKU and CRF-
MSR. Obviously, there is no strong correlation be-
tween the F-score and BLEU score.
4 Effect of combining multiple CWS
schemes
We used the Sighan Bakeoff corpora of different
CWS specifications separately in the previous ex-
periments. Here, we propose two approaches to us-
ing all the resources combined. The first approach
is to concatenate all the training data of the Sighan
Bakeoff, regardless of the specifications and train-
ing a new CWS for segmenting SMT training data.
The second approach involves linear integration of
translation models. We found that both approaches
produced an improvement in translation quality.
4.1 Effect of combining training data from
multiple CWS specifications
The CWS specifications are very different and the
corresponding Sighan training data are segmented
in different ways. We used these data separately
in the previous work as if they were incompatible.
However, creating data manually is laborious and
costly. It would therefore be a significant advan-
tage if all the data could be used, regardless of the
different specifications. We therefore created a new
CWS scheme, called ?dict-hybrid?. This CWS was
trained by concatenating all the Sighan Bakeoff cor-
pora regardless of the different specifications. The
?dict-hybrid? was trained using Achilles. It uses a
dictionary-based approach, and its lexicon and lan-
guage model were obtained as follows.
First, we created a hybrid corpus by combining
all the Sighan training corpora: AS, CITYU, MSR,
PKU. The hybrid corpus was used to train a CRF-
based CWS. This CWS was then used to segment
the SMT training corpus and then we extracted a
lexicon of 100,000 from the top frequent words of
the segmented SMT corpus. This lexicon was used
as the lexicon of the ?dict-hybrid.? The LM of ?dict-
hybrid? was also trained on the segmented corpus.
Note a lexicon and a LM are the only needed re-
sources for building a dictionary-based CWS, like
the ?dict-hybrid.? (Zhang et al, 2006)
We used the ?dict-hybrid? to segment the SMT
training corpus and test data. This segmentation
generated 49,546,231 tokens, 112,072 unique words
for the training data and 693 OOVs for the test data.
The segmentation data were used for training a
new SMT model. We tested the model using the
same approach and found the BLEU score obtained
by this CWS scheme was 23.91. This score was
better than those in Table 4 obtained by any of the
Achilles CWS schemes except ICTCLAS. There-
fore, the CWS scheme ?dict-hybrid? produced better
translations than other schemes implemented using
Achilles, indicating that using multiple CWS cor-
pora can improve SMT even if their specifications
are different.
Significance testing also showed that the results
for ICTCLAS and ?dict-hybrid? were not signifi-
cantly different. The results of ?dict-hybrid? are sig-
nificantly better than those in the Table 4 which have
220
Table 5: Correlation between F-score and BLEU
PKU MSR
F-score BLEU F-score BLEU
CRF 0.939 23.61 CRF 0.954 22.50
dict 0.930 23.61 dict 0.947 23.22
dict-LDC 0.931 23.74 dict-LDC 0.928 23.16
ICTCLAS 0.948 24.12 MSRSEG 0.969 19.72
CITYU AS
F-score BLEU F-score BLEU
CRF 0.920 23.55 CRF 0.922 23.70
dict 0.873 23.72 dict 0.896 23.46
dict-LDC 0.886 23.36 dict-LDC 0.878 23.52
a BLEU score lower than 23.70.
4.2 Effect of feature interpolation of
translation models
We investigated the effect of linearly integrating
multiple features of the same type. We generated
multiple translation models by using different word
segmenters. Each translation model corresponded to
a word segmenter. The same type of features as in
the log-linear model were added linearly. For exam-
ple, the phrase translation model p(e| f ) can be lin-
early interpolated as, p(e| f ) = ?Si=1 ?i pi(e| f ) where
pi(e| f ) is the phrase translation model correspond-
ing to the i-th CWSs. ?i is the weight, and S is the
total number of models. ?Si=1 ?i = 1.
?s can be obtained by maximizing the likelihood
or BLEU scores of the development data. Optimiz-
ing the ? has been described elsewhere (Foster and
Kuhn, 2007). p(e| f ) is the phrase translation model
generated.
In addition to the phrase translation model, we
used the same approach to integrate three other
features: phrase inverse probability p( f |e), lexical
probability lex(e| f , a), and lexical inverse probabil-
ity lex( f |e, a).
We integrated the CWS schemes ranked in the
top five in Table 4: ICTCLAS, dict-hybrid, dict-
LDC-PKU, dict-CITYU, and CRF-AS. We labeled
the five schemes A, B, C, D, and E, respectively,
as shown in Table 6. The first line of Table 6 rep-
resents the test data segmented by the five CWS
schemes. ?tst-A? means the test data was segmented
by ICTCLAS. ?tst-B? means the test data segmented
by ?dict-hybrid?, and so on. The second line gives
baseline results showing the original results with-
out the use of feature integration. For different test
data, the baseline is different. The baseline of ICT-
CLAS was tested on ?tst-A? only. The baseline of
?dict-hybrid? was tested on ?tst-B? only. From the
third line we gradually added a translation model
to the models used in the baseline. For example,
?A+B? integrates models made using ICTCLAS and
?dict-hybrid.? Each integration models were tested
only on the test data participated in the integration.
Hence, some slots in Table 6 are blank.
We did not carry out parameter optimization with
regards to the ?s. Instead, we used equal ?s for all
the features. For example, all ?s equal 0.5 for A+B,
and 0.25 for A+B+C+D. Each cell in Table 6 indi-
cates the BLEU score of the integration in relation
to the test data. We found our approach improved
the baseline results significantly. The more models
integrated, the better the results. The improvement
was positive for all of the test data. With regards to
the integration, if a phrase pair exists in one model
only, we suppose the values of probabilities are zero
in other models.
To better understand the effects of feature inter-
polation, we blended the features of the translation
models, as shown in Table 7, by simply combining
the phrase pairs without probability interpolation.
When we merged two models, we defined one model
as the master model and the other as the supple-
mentary model. Only phrase pairs that were in the
221
supplementary models but not in the master model
were appended to the master model. Their feature
probabilities were not changed. Hence, the com-
bined model was a blend of phrase pairs from the
master model and supplementary model. There was
no probability integration, that was significantly dif-
ferent from the feature interpolation approach. For
each set of test data in Table 7, the master model
was the model using the same CWS as the test data.
While there was one row for each type of combina-
tion, the cells in the row contained different models.
For example, ?A+B? for test data ?A? uses ?A? as the
master model and ?B? as the supplementary model,
while the opposite holds for test data ?B?.
Comparing Table 6 and 7 showed that feature
interpolation outperformed feature blending. Fea-
ture interpolation yielded surprisingly good results.
The performance consistently improved when more
models were integrated, but this was not the case
for feature blending. This shows that probability
integration is very effective. Increasing the size of
phrase pairs, as feature blending does, is not as ef-
fective.
We used equal values for the ?s. Optimal values
may be obtained using the optimization approach
of maximizing BLEU or the likelihood of develop-
ment data as has been reported previously (Foster
and Kuhn, 2007). However, optimization is compu-
tationally expensive and the effect was not satisfac-
tory. Therefore, we decided not optimizing the ?s in
this work.
5 Related work and Discussions
CWS has been the subject of intensive research
in recent years, as is evident from the last
four international evaluations, the Sighan Bake-
offs, and many approaches have been proposed
over the past decade. Segmentation performance
has been improved significantly, from the earli-
est maximal match (dictionary-based) approaches to
CRF (Peng and McCallum, 2004) approach. We
used dictionary-based and CRF-based CWS ap-
proaches to demonstrate the effect of CWS on SMT,
both without and with OOV recognition.
SMT is a very complicated system to study. Its
response to CWS schemes is intractable and it is
very hard to use one or two measures to describe
the relationship between CWS and SMT, in a similar
way to describing the relationship between the align-
ment error rate (AER) and SMT (Fraser and Marcu,
2007). The CWS and SMT are related by a series of
factors such as the specifications, OOVs, lexicons,
and F-scores. None of these factors can be directly
related to the SMT. While we have completed many
experiments, based on changing the CWS specifica-
tions and methods used, to determine the relation-
ship between CWS and SMT, we have not estab-
lished any overwhelming rules. However, we be-
lieve the following guidelines are appropriate in con-
sidering a CWS system for SMT. Firstly, the F-score
is not a reliable guide to SMT quality. A very high
F-score may produce the lowest quality translations,
as was found for the MSRSEG. Secondly, it is better
to design a specification with smaller word units to
reduce data sparseness. Specifications like those for
MSR will produce an inferior translation. Thirdly,
do not use a huge lexicon for word segmentation.
A huge lexicon will result in data sparseness and
segmentation complexity. And lastly, using multi-
ple word segmentation results and approaches does
work. We used two approaches that combined mul-
tiple word segmentation - dict-hybrid and feature in-
tegration - and both improved the translations signif-
icantly.
The BLEU scores in our experiments were rela-
tively low in comparison with current state-of-the art
results. However, our system was very similar to the
system (Koehn et al, 2005) that gave a BLEU score
of 24.3, comparable to ours. The BLEU score can
be raised if we do post-editing, use more data for
language modeling and other methods.
6 Conclusions
We investigated the effect of CWS on SMT from
two points of view. Firstly, we analyzed multiple
CWS specifications and built a CWS for each one to
examine how they affected translations. Secondly,
we investigated the advantages and disadvantages of
various CWS approaches, both dictionary-based and
CRF-based, and built CWSs using these approaches
to examine their effect on translations.
We proposed a new approach to linear interpo-
lation of translation features. This approach pro-
duced a significant improvement in translation and
222
Table 6: Feature interpolation of translation models: A=ICTCLAS, B=dict-hybrid, C=dict-PKU-LDC, D=dict-CITYU, E=CRF-AS
Model tst-A tst-B tst-C tst-D tst-E
Baseline 24.12 23.91 23.74 23.72 23.70
A+B 24.25 24.20
A+B+C 24.49 24.31 23.84
A+B+C+D 24.60 24.43 24.05 24.27
A+B+C+D+E 24.61 24.55 24.16 24.39 24.17
Table 7: Feature blending of translation models
Model tst-A tst-B tst-C tst-D tst-E
Baseline 24.12 23.91 23.74 23.72 23.70
A+B 24.20 24.24
A+B+C 24.27 24.14 23.69
A+B+C+D 23.92 24.29 23.61 24.00
A+B+C+D+E 23.86 24.31 23.69 24.05 23.76
achieved the best BLEU score of all the CWS
schemes.
We have published a much more detailed pa-
per (Zhang et al, 2008) to describe the relations be-
tween CWS and SMT.
References
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, Jeju, Korea.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128?135, Prague, Czech Republic, June. Association
for Computational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. In Computational linguistics, Squibs Discussion,
volume 33 of 3, pages 293?303, September.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In ACL-2004,
pages 462?469, Barcelona, July.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL
2003: Main Proceedings, pages 127?133.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Miles Osborne Chris Callison-Burch, David
Talbot, and Michael White. 2005. Edinburgh sys-
tem description for the 2005 nist mt evaluation. In
Proceedings of Machine Translation Evaluation Work-
shop.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of the 41st
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 160?167.
Fuchun Peng and Andrew McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Proc. of Coling-2004, pages
562?568, Geneva, Switzerland.
Huaping Zhang, HongKui Yu, Deyi xiong, and Qun Liu.
2003. HHMM-based Chinese lexical analyzer ICT-
CLAS. In Proceedings of the Second SIGHAN Work-
shop on Chinese Language Processing, pages 184?
187.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of the LREC.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. In Proceedings
of the HLT-NAACL, Companion Volume: Short Pa-
pers, pages 193?196, New York City, USA, June.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Chinese word segmentation and statistical ma-
chine translation. ACM Trans. Speech Lang. Process.,
5(2), May.
223
Method of Selecting Training Data to Build a Compact and Efficient
Translation Model
Keiji Yasuda?,?, Ruiqiang Zhang?,?, Hirofumi Yamamoto?,? and Eiichiro Sumita?,?
?National Institute of Communications Technology
?ATR Spoken Language Translation Research Laboratories
2?2?2, Hikaridai, ?Keihanna Science City?, Kyoto, 619?0288 Japan
{keiji.yasuda,ruiqiang.zhang}@nict.go.jp
{hirofumi.yamamoto,eiichiro.sumita}@nict.go.jp
Abstract
Target task matched parallel corpora are re-
quired for statistical translation model train-
ing. However, training corpora sometimes
include both target task matched and un-
matched sentences. In such a case, train-
ing set selection can reduce the size of the
translation model. In this paper, we propose
a training set selection method for transla-
tion model training using linear translation
model interpolation and a language model
technique. According to the experimental
results, the proposed method reduces the
translation model size by 50% and improves
BLEU score by 1.76% in comparison with a
baseline training corpus usage.
1 Introduction
Parallel corpus is one of the most important compo-
nents in statistical machine translation (SMT), and
there are two main factors contributing to its perfor-
mance. The first is the quality of the parallel corpus,
and the second is its quantity.
A parallel corpus that has similar statistical char-
acteristics to the target domain should yield a more
efficient translation model. However, domain-
mismatched training data might reduce the transla-
tion model?s performance. A large training corpus
obviously produces better quality than a small one.
However, increasing the size of the training corpus
causes another problem, which is increased compu-
tational processing load. This problem not only af-
fects the training of the translation model, but also
its applications. The reason for this is that a large
amount of training data tends to yield a large trans-
lation model and applications then have to deal with
this model.
We propose a method of selecting translation
pairs as the training set from a training parallel
corpus to solve the problem of an expanded trans-
lation model with increased training load. This
method enables an adequate training set to be se-
lected from a large parallel corpus by using a small
in-domain parallel corpus. We can make the transla-
tion model compact without degrading performance
because this method effectively reduces the size of
the set for training the translation model. This com-
pact translation model can outperform a translation
model trained on the entire original corpus.
This method is especially effective for domains
where it is difficult to enlarge the corpus, such as
in spoken language parallel corpora (Kikui et al,
2006). The main approach to recovering from an un-
dersupply of the in-domain corpus has been to use
a very large domain-close or out-of-domain paral-
lel corpus for the translation model training (NIST,
2006). In such case, the proposed method effectively
reduces the size of the training set and translation
model.
Section 2 describes the method of selecting the
training set. Section 3 details the experimental re-
sults for selecting the training set and actual trans-
lation from the International Workshop on Spoken
Language Translation 2006 (IWSLT2006). Section
4 compares the results of the proposed method with
those of the conventional method. Section 5 con-
cludes the paper.
655
Target language
Targetlanguage Sourcelanguage
Source language
Large out-of-domain parallel corpus 
Small in-domain parallel corpus
1. Train translationmodel
3. Calculate perplexity
LMTarget LMSource
2. Train languagemodels
TMin-domain
4. Select translation pairs based on the perplexity
6. Integrate TMs using linear interpolation
5. Train translationmodel TMselected
TMfinal
Figure 1: Framework of method.
2 Method
Our method use a small in-domain parallel corpus
and a large out-of-domain parallel corpus, and it
selects a number of appropriate training translation
pairs from the out-of-domain parallel corpus. Fig-
ure 1 is a flow diagram of the method. The proce-
dure is as follows:
1. Train a translation model using the in-domain
parallel corpus.
2. Train a language model using the source lan-
guage side or/and target language side of the
in-domain corpus.
3. Calculate the word perplexity for each sentence
(in source language side or/and target language
side) in the out-of-domain corpus by using the
following formulas.
PPe = Pe(Se)?
1
ne (1)
where PPe is the target language side perplex-
ity, and Pe is the probability given by the target
side language model. Se is the target language
sentence in the parallel corpus, and ne is the
number of words in the sentence.
We can also calculate the perplexity in the
source language (PPf ) in the same way.
PPf = Pf (Sf )
? 1nf (2)
If we use perplexities in both languages, we can
calculate average perplexity (PPe+f ) by using
the following formula.
PPe+f = (PPe ? PPf )
1
2 (3)
656
Table 1: Size of parallel corpora
English Chinese English Chinese
In-domain
parallel corpus
40 K 40 K 320 K 301 K Basic Travel Expressions Corpus
Out-of-domain
parallel corpus
2.5 M 2.5 M 62 M 54 M
LDC corpus (LDC 2002T01, LDC2003T17, LDC2004T07,
LDC2004T08, LDC2005T06 and LDC2005T10)
# of sentences # of words
Explanation
4. Select translation pairs from the out-of-domain
parallel corpus. If the perplexity is smaller than
the threshold, use translation pairs as the train-
ing set. Otherwise, discard the translation pairs.
5. Train a translation model by using the selected
translation pairs.
6. Integrate the translation model obtained in 1
and 6 by linear interpolation.
3 Experiments
We carried out statistical machine translation experi-
ments using the translation models obtained with the
proposed method.
3.1 Framework of SMT
We employed a log-linear model as a phrase-based
statistical machine translation framework. This
model expresses the probability of a target-language
word sequence (e) of a given source language word
sequence (f ) given by
P (e|f) =
exp
(?M
i=1 ?ihi(e, f)
)
?
e? exp
(?M
i=1 ?ihi(e?, f)
) (4)
where hi(e, f) is the feature function, ?i is the fea-
ture function?s weight, and M is the number of fea-
tures. We can approximate Eq. 4 by regarding its
denominator as constant. The translation results (e?)
are then obtained by
e?(f, ?M1 ) = argmaxe
M?
i=1
?ihi(e, f) (5)
3.2 Experimental conditions
3.2.1 Corpus
We used data from the Chinese-to-English trans-
lation track of the IWSLT 2006(IWSLT, 2006) for
the experiments. The small in-domain parallel cor-
pus was from the IWSLT workshop. This corpus
was part of the ATR Bilingual Travel Expression
Corpus (ATR-BTEC) (Kikui et al, 2006). The large
out-of-domain parallel corpus was from the LDC
corpus (LDC, 2007). Details on the data are listed
in Table 1. We used the test set of the IWSLT2006
workshop for the evaluation. This test set consisted
of 500 Chinese sentences with eight English refer-
ence translations per Chinese sentence.
For the statistical machine-translation experi-
ments, we first aligned the bilingual sentences
for preprocessing using the Champollion tool (Ma,
2006). We then segmented the Chinese words us-
ing Achilles (Zhang et al, 2006). After the seg-
mentation, we removed all punctuation from both
English and Chinese corpuses and decapitalized the
English corpus. We used the preprocessed data to
train the phrase-based translation model by using
GIZA++ (Och and Ney, 2003) and the Pharaoh tool
kit (Koehn et al, 2003).
3.2.2 Features
We used eight features (Och and Ney, 2003;
Koehn et al, 2003) and their weights for the transla-
tions.
1. Phrase translation probability from source lan-
guage to target language (weight = 0.2)
2. Phrase translation probability from target lan-
guage to source language (weight = 0.2)
3. Lexical weighting probability from source lan-
guage to target language (weight = 0.2)
4. Lexical weighting probability from source tar-
get to language weight = 0.2)
5. Phrase penalty (weight = 0.2)
657
6. Word penalty (weight = ?1.0)
7. Distortion weight (weight = 0.5)
8. Target language model probability (weight =
0.5)
According to a previous study, the minimum er-
ror rate training (MERT) (Och, 2003), which is the
optimization of feature weights by maximizing the
BLEU score on the development set, can improve
the performance of a system. However, the range
of improvement is not stable because the MERT al-
gorithm uses random numbers while searching for
the optimum weights. As previously mentioned, we
used fixed weights instead of weights optimized by
MERT to remove its unstable effects and simplify
the evaluation.
3.2.3 Linear interpolation of translation
models
The experiments used four features (Feature # 1
to 4 in 3.2.2) as targets for integration. For each fea-
ture, we applied linear interpolation by using the fol-
lowing formula.
h(e, f) = ?outhout(e, f)+(1??out)hin(e, f) (6)
Here, hin(e, f) and hout(e, f) are features trained
on the in-domain parallel corpus and out-of-domain
corpus, respectively. ?out is the weight for the fea-
ture trained on the out-of-domain parallel corpus.
3.2.4 Language model
We used a Good-Turing (Good, 1953) 3-gram lan-
guage model for data selection.
For the actual translation, we used a modi-
fied Kneser-Ney (Chen and Goodman, 1998) 3-
gram language model because modified Kneser-Ney
smoothing tended to perform better than the Good-
Turing language model in this translation task. For
training of the language model, only the English side
of the in-domain corpus was used. We used the
same language model for the entire translation ex-
periment.
3.3 Experimental results
3.3.1 Translation performance
Figure 2 and 3 plot the results of the experiments.
The horizontal axis represents the weight for the out-
of-domain translation model, and the vertical axis
15%
16%
17%
18%
19%
20%
21%
22%
23%
24%
25%
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Weight for out-of-domain translation model
B
L
E
U
 s
c
o
re
400 K
800 K
1.2 M
1.6 M
2.5 M
Figure 2: Results of data selection and linear inter-
polation (BLEU)
represents the automatic metric of translation qual-
ity (BLEU score (Papineni et al, 2002) in Fig. 2,
and NIST score (NIST, 2002) in Fig. 3). Thick
straight broken lines in the figures indicate auto-
matic scores of a baseline system. This base line sys-
tem was trained on the in-domain and all of the out-
of-domain corpus (2.5M sentence pairs). These data
were concatenated before training; then one model
was trained without linear interpolation. The five
symbols in the figures represent the sizes (# of sen-
tence pairs) of the selected parallel corpus. Here,
the selection was carried out by using Eq. 1. For
automatic evaluation, we used the reference transla-
tion with a case unsensitive and no-punctuation set-
ting. Hence, higher automatic scores indicate better
translations; the selected corpus size of 1.2M (?)
indicates the best translation quality in Fig. 2 at the
point where the weight for the out-of-domain trans-
lation model is 0.7.
In contrast to Fig. 2, Fig. 3 shows no improve-
ments to the NIST score by using the baseline out-
of-domain usage. The optimal weights for each cor-
pus size are different from those in Fig. 2. How-
ever, there is no difference in optimal corpus size;
i.e., the selected corpus size of 1.2M gives the best
NIST score.
658
5.4
5.5
5.6
5.7
5.8
5.9
6
6.1
6.2
6.3
6.4
6.5
6.6
6.7
6.8
6.9
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Weight for out-of-domain translation model
N
IS
T
 s
c
o
re
400 K
800 K
1.2 M
1.6 M
2.5 M
Figure 3: Results of data selection and linear inter-
polation (BLEU)
Table 2: Size of integrated phrase tables
In-domain Out-of-domain
40 K 0 14 M
40 K 1.2 M 917 M
40 K 2.5 M 1.8 G
Size of phrase table
(Bytes)
Corpus size
(Sentence pairs)
3.3.2 Size of the translation models
Table 2 lists the sizes of the translation models
of the baseline and optimum-size training corpus.
The size of the phrase table is the uncompressed file
size of the phrase table trained by the Pharaoh tool
kit. As the table indicates, our method reduced the
model sizes by 50%.
This reduction had a positive effect on the com-
putational load of decoding.
3.3.3 Equations for the selection
The experiments described above used only target
language side information, i.e., Eq. 1, for the data
selection. Here, we compare selection performances
of Eqs. 1, 2, and 3. Table 3 shows the results.
The first row shows the results of using only the in-
domain parallel corpus. The second row shows re-
sults of the baseline. The third row shows the results
of using linear interpolation without data selection.
Comparing the results for the three equations, we
see that Eq. 1 gives the best performance. It out-
performs not only the baseline but also the results
obtained by using all of the (2.5M) out-of-domain
data and linear interpolation.
The results of using source language side infor-
mation (Eq. 2) and information from both language
sides (Eq. 3) still showed better performance than
the baseline system did.
4 Comparison with conventional method
There are few studies on data selection for trans-
lation model training. Most successful and recent
study was that of (Lu et al, 2007). They applied
the TF*IDF framework to translation model train-
ing corpus selection. According to their study, they
obtained a 28% translation model size reduction (A
2.41G byte model was reduced to a 1.74G byte
model) and 1% BLEU score improvement (BLEU
score increased from 23.63% to 24.63%). Although
there results are not directly comparable to ours [??]
because of the differences in the experimental set-
ting, our method outperforms theirs for both aspects
of model size reduction and translation performance
improvement (50% model size reduction and 1.76%
BLEU score improvement).
5 Conclusions
We proposed a method of selecting training sets for
training translation models that dramatically reduces
the sizes of the training set and translation models.
We carried out experiments using data from the
Chinese-to-English translation track of the IWSLT
evaluation campaign. The experimental results indi-
cated that our method reduced the size of the training
set by 48%. The obtained translation models were
half the size of the baseline.
The proposed method also had good translation
performance. Our experimental results demon-
strated that an SMT system with a half-size transla-
tion model obtained with our method improved the
BLEU score by 1.76%. (Linear interpolation im-
proved BLEU score by 1.61% and data selection im-
proved BLEU score by an additional 0.15%.)
659
Table 3: Results of data selection by using Eqs. 1, 2, and 3
In-domain Out-of-domain
40 K 0 N/A N/A 21.68%
40 K 2.5 M N/A N/A 23.16%
40 K 2.5 M N/A 0.7 24.77%
40 K 1.2 M Eq. 1 0.7 24.92%
40 K 1.2 M Eq. 2 0.8 24.76%
40 K 1.2 M Eq. 3 0.6 24.56%
Optimal weight for
out-of-domain model
BLEU score
Corpus size (Sentence pairs)
Selection method
We also compared the selections using source lan-
guage side information, target language side infor-
mation and information from both language sides.
The experimental results show that target language
side information gives the best performance in the
experimental setting. However, there are no large
differences among the different selection results.
The results are encouraging because they show that
the in-domain mono-lingual corpus is sufficient to
select training data from the out-of-domain parallel
corpus.
References
S. F. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. In Tech-
nical report TR-10-98, Center for Research in Com-
puting Technology (Harvard University).
I. J Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3):237?264.
IWSLT. 2006. IWSLT: International Work-
shop on Spoken Language Translation.
http://www.slc.atr.jp/IWSLT2006/.
G. Kikui, S. Yamamoto, T. Takezawa, and E. Sumita.
2006. Comparative study on corpora for speech trans-
lation. In IEEE Transactions on Audio, Speech and
Language Processing, volume 14(5), pages 1674?
1682.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. Proc. of Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL), pages 127?133.
LDC. 2007. Linguistic Data Consortium.
http://www.ldc.upenn.edu/.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by training
data selection and optimization. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 343?
350.
X Ma. 2006. Champollion: A Robust Parallel Text Sen-
tence Aligner. In Proc. of international conference on
Language Resources and Evaluation (LREC), pages
489?492.
NIST. 2002. Automatic Evaluation of Machine Trans-
lation Quality Using N-gram Co-Occurence Statistics.
http://www.nist.gov/speech/tests/mt/
mt2001/resource/.
NIST. 2006. The 2006 NIST Machine
Translation Evaluation Plan (MT06).
http://www.nist.gov/speech/tests/mt/
doc/mt06 evalplan.v3.pdf.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum Error Rate Training for Sta-
tistical Machine Translation. Proc. of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318.
R. Zhang, G. Kikui, and E. Sumita. 2006. Subword-
based Tagging by Conditional Random Fields for Chi-
nese Word Segmentation. Proc. of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL), Short Paper:193?196.
660
Quality-Sensitive Test Set Selection for a Speech Translation System 
Fumiaki Sugaya1, Keiji Yasuda2, Toshiyuki Takezawa and Seiichi Yamamoto 
ATR Spoken Language Translation Research Laboratories 
2-2-2 Hikari-dai Seika-cho, Soraku-gun, Kyoto, 619-0288, Japan 
{fumiaki.sugaya, keiji.yasuda, toshiyuki.takezawa,
seiichi.yamamoto}@atr.co.jp
 
 
                                                          
1 
2 
1Current affiliation: KDDI R&D Laboratories. Also at Graduate School of Science and Technology, Kobe University. 
2Also at Graduate School of Engineering, Doshisha University. 
  
Abstract 
We propose a test set selection method to 
sensitively evaluate the performance of a 
speech translation system. The proposed 
method chooses the most sensitive test 
sentences by removing insensitive 
sentences iteratively. Experiments are 
conducted on the ATR-MATRIX speech 
translation system, developed at ATR 
Interpreting Telecommunications 
Research Laboratories. The results show 
the effectiveness of the proposed method. 
According to the results, the proposed 
method can reduce the test set size to less 
than 40% of the original size while 
improving evaluation reliability. 
Introduction 
The translation paired comparison method 
precisely measures the capability of a speech 
translation system.  In this method, native speakers 
compare a system?s translation and the translations, 
made by examinees who have various TOEIC 
scores. The method requires two human costs: the 
data collection of examinees? translations and the 
comparison by native speakers.  In this paper, we 
propose a test set size reduction method that 
reduces the number of test set utterances.  The 
method chooses the most sensitive test utterances 
by removing the most insensitive utterances 
iteratively.    
In section 2, the translation paired comparison 
method is described. Section 3 explains the 
proposed method. In section 4, evaluation results 
for ATR-MATRIX are shown. Section 5 discusses 
the experimental results. In section 6, we state our 
conclusions. 
Translation paired comparison method 
The translation paired comparison method  
(Sugaya, 2000) is an effective evaluation method 
for precisely measuring the capability of a speech 
translation system. In this section, a description of 
the method is given. 
2.1 Methodology of the translation paired 
comparison method 
Figure 1 shows a diagram of the translation paired 
comparison method in the case of Japanese to 
English translation. The Japanese native-speaking 
examinees are asked to listen to Japanese text and 
provide an English translation on paper.  The 
Japanese text is spoken twice within one minute, 
with a pause in-between. To measure the English 
capability of the Japanese native speakers, the 
TOEIC score is used. The examinees are requested 
to present an official TOEIC score certificate 
showing that they have taken the test within the 
past six months. A questionnaire is given to them 
and the results show that the answer time is 
moderately difficult for the examinees. 
The test text is the SLTA1 test set, which 
consists of 330 utterances in 23 conversations from 
a bilingual travel conversation database (Morimoto, 
1994; Takezawa, 1999). The SLTA1 test set is 
                                            Association for Computational Linguistics.
                         Algorithms and Systems, Philadelphia, July 2002, pp. 109-116.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
open for both speech recognition and language 
translation. The answers written on paper are typed. 
In the proposed method, the typed translations 
made by the examinees and the outputs of the 
system are merged into evaluation sheets and are 
then compared by an evaluator who is a native 
English speaker. Each utterance information is 
shown on the evaluation sheets as the Japanese test 
text and the two translation results, i.e., translations 
by an examinee and by the system.  The two 
translations are presented in random order to 
eliminate bias by the evaluator.  The evaluator is 
asked to follow the procedure illustrated in Figure 
2. The four ranks in Figure 2 are the same as those 
used in Sumita (1999). The ranks A, B, C, and D 
indicate: (A) Perfect: no problems in both 
information and grammar; (B) Fair: easy-to-
understand with some unimportant information 
missing or flawed grammar; (C) Acceptable: 
broken but understandable with effort; (D) 
Nonsense: important information has been 
translated incorrectly. 
2.2 Evaluation result using the translation 
paired comparison method 
Figure 3 shows the result of a comparison between 
a language translation subsystem (TDMT) and the 
examinees. The input for TDMT included accurate 
transcriptions. The total number of examinees was 
thirty, with five people having scores in every 
hundred-point TOEIC range between the 300s and 
800s. In Figure 3, the horizontal axis represents the 
TOEIC score and the vertical axis the system 
winning rate (SWR) given by following equation: 
Translation 
Result by 
Human 
Evaluation 
Sheet 
Japanese Test 
Text Typing Paired Comparison 
Accurate Text 
 
 
 
 
where NTOTAL denotes the total number of 
utterances in the test set, NTDMT represents the 
number of  "TDMT won" utterances,  and NEVEN, 
indicates the number of  even (non-winner) 
utterances, i.e., no difference between the results of 
the TDMT and humans. The SWR ranges from 0 
to 1.0, signifying the degree of capability of the 
MT system relative to that of the examinee.  An 
SWR of 0.5 means that the TDMT has the same 
capability as the human examinee. 
Figure 3 shows that the SWR of TDMT is 
greater than 0.5 at TOEIC scores of around 300 
and 400, i.e., the TDMT system wins over humans 
with TOEIC scores of 300 and 400. Examinees, in 
contrast, win at scores of around 800. The 
capability balanced area is around a score of 600 to 
(1)                   
0.5
TOTAL
EVENTDMT
N
NNSWR
?+
=
Figure 1: Diagram of translation pair comparison method 
Japanese-to-English 
Language Translation 
(J-E TDMT) 
Japanese Recognition
(Japanese SPREC) 
Choose A, B, C, or D rank 
 
No 
Same rank?
 
Yes 
Consider naturalness 
 
YesNo 
Same? 
 
Select better result 
 
EVEN 
 
Figure 2: Procedure of comparison 
by native speaker 
 300 400 500 600 700 800 900
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
TO EIC score
SW
R
Figure 3: Evaluation results using translation 
paired comparison method 
Under the above condition, the standard deviation 
of the system's TOEIC score is calculated by 
 
(4)          
)(
)(1
2
2
0
2 ? ?
?
+=
XX
XC
n i
t ?
?
?
 
 
 
where n is the number of examinees, C0 is the 
system's TOEIC score, and X  is the average of 
the examinees' TOEIC scores. Equation (4) 
indicates that the minimum error is given when the 
system's TOEIC score equals the average of the 
examinees' TOEIC scores. 
By using a t-distribution, the confidence 
interval (CI) of the system's TOEIC score with 
confidence coefficient 1-?  is given by 
 700. To precisely determine the balanced point, we 
used regression analysis. The straight line in Figure 
3 is the regression line. The capability balanced 
point between the TDMT subsystem and the 
examinees is 0.5 of SWR. In Figure 3, the exact 
point is a TOEIC score of 708. We call this point 
the system's TOEIC score. Consequently, the 
translation capability of the language translation 
system equals that of the examinees at around a 
score of 700 points on the TOEIC.  
 
 
 
[ ]
(5)                                   )2;
2
( 
   , 00
??=
+?=
ntI
ICICCI
t
?
?
 
In the current study, we employ 0.01 for the 
value of ? .  
2.4 Costs for the translation paired comparison 
method 
The experimental result for ATR-MATRIX, 
which consists of a speech recognition subsystem 
and TDMT, has been also reported (Sugaya, 2000). 
This system?s TOEIC score is 548, where the 
number of speech recognition errors is a factor in 
the degradation of the score. 
The translation paired comparison method is an 
effective evaluation method because it can clearly 
express a system?s performance as a TOEIC (Test 
of English for International Communication)   
score. However, this method has excessive 
evaluation costs.    
Roughly speaking, one of these costs is the need 
to collect translations made by examinees of 
various TOEIC scores. As shown in Equations (4) 
and (5), n, the number of examinees, affects the 
confidence interval of the system?s TOEIC score. 
Therefore, a reduction in this number makes it 
difficult to obtain a reliable evaluation result. 
2.3 Error in the system?s TOEIC score 
The SWR (Yi) and TOEIC scores for the examinees 
(Xi) are assumed to satisfy the population 
regression equation:  
 (2)          ),...,2,1(    21 niXY iii =++= ??? 
The other cost is for the evaluation. Compared 
to a conventional evaluation method, such as a 
simple rank evaluation method, the translation 
paired comparison method uses a larger amount of 
labor because the evaluator must work on n 
evaluation sheets. Each sheet consists of 330 pairs 
of translation results to be evaluated. Even for an 
accomplished evaluator, it takes more than two 
weeks to finish the work, following the method 
explained in section 2.2. 
where 1? and 2? are population regression 
coefficients.  The error term ( i? ) is assumed to 
satisfy the following condition: 
 
 
0    (d)
   if     0),(),(    (c)
(3)                     ,...,2,1     ,)(    (b)
0)(    (a)
22
?
?==
==
=
i
jiji
i
i
jiECov
niV
E
?
????
??
?
 
 
 
 
3 Proposed method 
Yes 
No 
?
?
No
Yes
All candidates 
are calculated?
Set the number of iterations 
 
Remove worst utterances from 
candidates 
Is iteration 
achieved? 
Calculate iteration 
 
Update worst sentence, 
which causes maximum 
iteration 
Get next candidate 
 
As explained in the previous section, the 
translation paired comparison method has an 
excessive evaluation cost. Nevertheless, it is an 
effective evaluation method for measuring the 
capability of a speech translation system. 
Therefore, cost reduction for this evaluation 
method is an important subject for study. 
The proposed method reduces the evaluation 
cost by removing insensitive test utterances from 
the test set. In this section, we explain the 
optimization procedure of the proposed method.  
3.1 Optimization basis 
In the proposed method, the basis of test set 
optimization is the minimization of ? . As shown 
in Equations (4) and (5), this value has an 
influence on the confidence interval of the system's 
TOEIC score. Therefore, minimizing ?  brings 
about a reliable evaluation result.  
We introduce ? iteration, which is calculated in 
each iteration step. ? iteration is also calculated by 
using Equations (2) and (3). The difference 
between ? iteration and?  is the test set to be used 
for calculation. ? iteration is calculated using 
residual test utterances in each iteration step. 
However, the values of 1?  and 2?  are fixed, i.e., 
for the calculation of ? iteration, these 1?  and 2?  
are calculated using the original test set consisting 
of 330 test utterances. 
Optimization is conducted iteratively by 
picking up the test utterance that causes maximum 
?  iteration in each iteration step. The details of this 
procedure is explained in the next subsection. 
3.2 Methodology of the proposed method 
Figure 4 shows a diagram of the proposed method. 
In the first step, the number of iterations is set. 
This number is an actual number of removed test 
utterances. During the iterations, test utterances are 
removed one-by-one. To decide which test 
utterance to remove in each iteration, ? iteration is 
calculated for the condition of removing each test 
utterance. This calculation is done for all 
candidates, i.e., all constituents of residual test 
utterances.  
Figure 4: Procedure of proposed method 
 
At the end of each iteration step, the test 
utterance to be removed is decided. The removed 
test utterance is the one that maximizes ? iteration. 
We regard the utterance as maximizing? iteration if 
removing it from the test set gives minimum 
? iteration. 
70
720
740
760
TO
EI
C
 
sco
re
0 50 100 150 200 250 300
660
680
Iteration
   (upper)  C0 opt  +  Iopt 
  C0 opt
   (lower)  C0 opt   -  Iopt
20
30
40
?
t
 
o
p
t
      Random selection (Averaging of 10 trials)
      Optimzed  (Open)
      Optimzed  (Closed)
0 50 100 150 200 250 300
0
10
Iteration
 
Figure 6: Relationship between iteration 
and ? t opt 
Figure 5: Relationship between iteration  
and system?s TOEIC score 
 As shown in the figure, from iteration 1 to 
iteration 250, the value of C0 opt is stable and does 
not deviate from C0, which is 708. Furthermore, 
until around iteration 200, the value of Iopt 
decreases concurrently with the iteration. 
4 Experimental results 
In this section, we show experimental results of the 
proposed method. Here, we introduce the suffix 
?opt? to distinguish a variable calculated with the 
optimized test set from a variable calculated with 
the original test set. All of the above variables are 
calculated with the original test set. By joining the 
suffix ?opt? to these variables, we refer to variables 
calculated with the optimized test set, e.g., ?  opt 3, 
? t opt, Iopt, C0 opt, CI opt, and so on. 
This result suggests that the proposed may 
provide low-cost evaluation with high reliability. 
 
4.2 Experiment opened for examinees 
In the result shown in the previous subsection, the 
optimization and evaluation were conducted on the 
same examinees, i.e., the evaluation is closed for 
examinees. In this subsection, we look into the 
robustness of the proposed method against 
different examinees. We divided the group, 
consisting of 30 examinees, into two groups: a 
group of odd-numbered examinees and a group of 
even-numbered examinees. Individuals were sorted 
by TOEIC score from lowest to highest.  
4.1 Closed experiment 
This  subsection discusses an experimental result    
obtained for the same test set and examinees 
described in Section 2. Namely, the target test set 
for optimization consists of 330 utterances and the 
number of examinees is 30. 
Figure 5 shows the relationship between 
iteration and the system?s TOEIC score (C0 opt). In 
this figure, the horizontal axis represents the 
iteration number and the vertical axis the TOEIC 
score. The solid line represents C0 opt, which is the 
system?s TOEIC score using the optimized test in 
each iteration. The dotted line above the solid line 
represents the value of C0 opt + Iopt, and the dotted 
line below the solid line C0 opt - Iopt. 
One of the groups is used to optimize the test set. 
The other group is used for the translation paired 
comparison method. We use the term 
?optimization group? to refer to the first group and 
?evaluation group? to refer to the second group. 
Figure 6 shows the relationship between 
iteration and ? t opt. In this figure, the horizontal 
axis represents the iteration and the vertical axis 
shows? t opt. Three kinds of experimental results 
are shown in this figure. In each of three 
experiments, the translation paired comparison is 
conducted by the evaluation group. The differences 
                                                          
3 ? opt is different from? iteration. ? opt is calculated based on 
1?  opt and 2?  opt (not 1?  and 2? ) for the optimized test set.  
Figure 8: Relationship between iteration and 
t opt ?
0 50 100 150 200 250 300
0
5
10
15
20
25
30
Iteration
?
t
 
o
p
t
    Random selection (Averaging of 10 trials)
   Optimzed for TDM T
   Optimzed for ATR-M ATRIX
0 50 100 150 200 250 300
550
60
650
70
750
800
850
Iteration
C 0
 
opt
Random selection (Averaging of 10 trials) 
Optimized (Open)
Optimized (Closed)
Figure 7: Relationship between iteration and 
C0 opt 
among the three experiments are in the group to be 
used for optimization of the test set or the method 
used to reduce it. The double line represents the 
closed result using the test set, optimized on the 
evaluation group. The solid line represents the 
open result using the test set, optimized on the 
optimization group. The broken line represents the 
result using the test set, which is reduced by 
randomly removing test utterances one-by-one. 
The actual plotted broken line is averaged over 10 
random trials.  
0 50 100 150 200 250 300
460
480
500
520
540
560
580
Iteration
C 0
 
opt
   Optimized for TDM T
   Optimzed for ATR-M ATRIX
As shown in Figure 6, in the random selection 
result, t opt is on the rise. On the other hand, the 
open result is on the decline. 
?
Figure 7 shows the relationship between 
iteration and the system?s TOEIC score. In this 
figure, the horizontal axis represents the iteration 
and the vertical axis the TOEIC score. The 
denotation of each line is the same as that in Figure 
6. The error bar from the broken line represents 
? random, which is the standard deviation of the 
system?s TOEIC score over 10 random trials. 
Figure 9: Relationship between iteration and 
C0 opt 
In Figure 7, considering ? random, C0 opt of the 
open evaluation is more approximate to C0 than 
that of random selection, whereas C0 opt of the 
closed evaluation is much more approximate to C0. 
4.3 Experiment on ATR-MATRIX 
To be of actual use, the test set optimized for some 
system must be applicable for evaluation of other 
systems. In this subsection, we show the results of 
an experiment aimed at verifying this requirement 
is met. In this experiment, we apply the test set, 
which is optimized for TDMT, to evaluate ATR-
MATRIX. The experimental conditions are the 
same as in Section 4.1, except for the evaluation 
target. The results are shown in Figure 8 and 
Figure 9. 
Figure 8 shows the relationship between 
iteration and ? t opt. In this figure, the horizontal 
axis represents the iteration and the vertical axis 
shows ? t opt. The double line represents the result 
using the test set, optimized for ATR-MATRIX. 
The solid line represents the result using the test 
set, optimized for TDMT. The broken line 
represents the result using the test set, which is 
reduced by randomly removing test utterances one- 
6 Conclusions by-one. The actual plotted broken line is averaged 
over 10 random trials. 
We proposed a test set selection method for 
evaluating a speech translation system.  This 
method optimizes and drastically reduces the test 
set required by the translation paired comparison 
method. 
Figure 9 shows the relationship between 
iteration and the system?s TOEIC score. In this 
figure, the horizontal axis represents the iteration, 
and the vertical axis TOEIC score. The broken line 
and the solid line are plotted using the same 
denotation as that in Figure 8. Translation paired comparison is an effective 
method for measuring a system?s performance as a 
TOEIC score. However, this method has excessive 
evaluation costs. Therefore, cost reduction for this 
evaluation method is an important subject for study. 
In Figure 8, the solid line always lies on a lower 
position than the broken line. In Figure 9, from 
iteration 1 to around iteration 200, the broken line 
does not deviate from the actual system?s TOEIC 
score, which is 548. We applied the proposed method in an evaluation 
of ATR-MATRIX. Experimental results showed 
the effectiveness of the proposed method. This 
method reduced evaluation costs by more than  
60% and also improved the reliability of the 
evaluation result. 
Considering these results, the test set optimized 
for TDMT is shown to be applicable for evaluating 
ATR-MATRIX. 
5 Discussion 
Acknowledgement In this section, we discuss the experimental results 
shown in Section 4.  
The research reported here was supported in part 
by a contract with the Telecommunications 
Advancement Organization of Japan entitled, "A 
study of speech dialogue translation technology 
based on a large corpus." 
Looking at the broken lines in Figure 6 and 
Figure 8, test set reduction using random selection 
always causes an increase of ? t opt i.e., an increase 
in the scale of confidence interval. Therefore, this 
method causes the reliability of the evaluation 
result to deteriorate. Meanwhile, in the case of 
using the proposed method, looking at the solid 
lines on these figures, ? t opt is on the decline until 
around iteration 200. This means that we can 
achieve a more reliable evaluation result with a 
lower evaluation cost than when using the original 
test set. Here, looking at the solid lines in Figure 7 
and Figure 9, the Co opt system?s TOEIC score is 
nearly stable until iteration 200, and it does not 
deviate from Co. As mentioned before, Co for 
Figure 7 is 708 and Co for Figure 9 is 548. 
References 
Morimoto, T., Uratani, N., Takezawa, T., Furuse, 
O., Sobashima, Y., Iida, H., Nakamura, A., 
Sagisaka, Y., Higuchi, N. and Yamazaki, Y.  
1994. A speech and language database for 
speech translation research. In Proceedings of 
ICSLP `94, pages 1791-1794. 
Sugaya, F., Takezawa, T., Yokoo, A., Sagisaka, Y. 
and Yamamoto, S. 2000. Evaluation of the 
ATR-MATRIX Speech Translation System with 
a Pair Comparison Method between the System 
and Humans. In Proceedings of ICSLP 2000, 
pages 1105-1108. 
Considering these results, the proposed method 
can reduce the 330-utterance test set to a 130- 
utterance test set while reducing the scale of 
confidence interval. In other words, the proposed 
method both reduces evaluation costs by 60% and 
improves  reliability of the evaluation result. 
Sumita, E., Yamada, S., Yamamoto K., Paul, M., 
Kashioka, H., Ishikawa, K. and Shirai, S. 1999. 
Solutions to Problems Inherent in Spoken-
language Translation: The ATR-MATRIX 
Approach. In Proceedings of MT Summit `99, 
pages 229-235. 
Looking at Equations (4) and (5), the scale of 
confidence interval is also influenced by n.  When 
we allow the scale of confidence interval obtained 
from the original test set, we can use the proposed 
method?s reduction effect of ? t to compensate the 
? t 's increase by reducing n.  In this case, the 
actual achievable cost reduction will be more than 
60%.  
Takezawa, T. 1999. Building a bilingual travel 
conversation database for speech recognition 
research. In Proceedings of Oriental COCOSDA 
Workshop, pages 17-20. 
Takezawa, T., Morimoto, T., Sagisaka, Y., 
Campbell, N., Iida., H., Sugaya, F., Yokoo, A. 
and Yamamoto, S. 1998. A Japanese-to-English 
speech translation system: ATR-MATRIX. In 
Proceedings of ICSLP 1998, pages 2779-2782. 
