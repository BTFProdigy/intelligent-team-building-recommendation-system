In: Proceedings of CoNLL-2000 and LLL-2000, pages 209-218, Lisbon, Portugal, 2000. 
The Acquisition of Word Order by 
a Computational Learning System 
Aline Villavicencio 
Computer  Laboratory, University of Cambridge 
New Museums Site, Cambridge, CB2 3QG, England, UK 
Aline. Vil lavicencio@cl. cam. ac. uk 
Abst rac t  
The purpose of this work is to investigate the 
process of grammatical acquisition from data. 
We are using a computational learning sys- 
tern that is composed of a Universal Grammar 
with associated parameters, and a learning al- 
gorithm, following the Principles and Parame- 
ters Theory. The Universal Grammar is imple- 
mented as a Unification-Based Generalised Cat- 
egorial Grammar, embedded in a default inher- 
itance network of lexical types. The learning al- 
gorithm receives input from a corpus annotated 
with logical forms and sets the parameters based 
on this input. This framework is used as basis 
to investigate several aspects of language acqui- 
sition. In this paper we are concentrating on the 
acquisition of word order for different learners. 
The results obtained show the different learners 
having a similar performance and converging to- 
wards the target grammar given the input data 
available, regardless of their starting points. It 
also shows how the amount of noise present in 
the input data affects the speed of convergence 
of the learners towards the target. 
1 In t roduct ion  
In trying to solve the question of how to get a 
machine to automatically earn linguistic infor- 
mation from data, we can look at the way people 
do it. Gold (1967) when investigating language 
identification i  the limit, obtained results that 
implied that natural languages could not be 
learned only on the basis of positive evidence. 
These results were used as a confirmation for the 
proposal that children must have some innate 
knowledge about language, the Universal Gram- 
mar (UG), to help them overcome the prob- 
lem of the poverty of the stimulus and acquire 
a grammar on the basis of positive evidence 
only. According to Chomsky's Principles and 
Parameters Theory (Chomsky 1981), the UG 
is composed of principles and parameters, and 
the process of learning a language is regarded 
as the setting of values of a number of parame- 
ters, given exposure to this particular language. 
We employ this idea in the learning framework 
implemented. 
In this work we are interested in investigating 
the acquisition of grammatical knowledge from 
data, focusing on the acquisition of word or- 
der, that reflects the underlying order in which 
constituents occur in different languages (e.g. 
SVO and SOV languages). The learning sys- 
tem is equipped with a UG and associated pa- 
rameters, encoded as a Unification-Based Gen- 
eralised Categorial Grammar, and a learning al- 
gorithm that fixes the values of the parameters 
to a particular language. The learning algo- 
rithm follows the Bayesian Incremental Param- 
eter Setting (BIPS) algorithm (Briscoe 1999), 
and when setting the parameters it uses a Mini- 
mum Description Length (MDL) style bias to 
choose the most probable grammar that de- 
scribes the data well, given the goal of converg- 
ing to the target grammar. In section 2 we de- 
scribe the components of the learning system. 
In section 3, we investigate the acquisition of 
word order within this framework and discuss 
the results obtained by different learners. Fi- 
nally we present some conclusions and future 
work. 
2 The Learning System 
The learning system is composed of a language 
learner equipped with a UG and a learning al- 
gorithm that updates the initial parameter set- 
tings, based on exposure to a corpus of utter- 
ances. Each of these components i  discussed in 
209 
more detail in the following sections. 
2.1 The  Universa l  Grammar  
The UG consists of pr inc ip les  and  parame-  
te rs ,  and the latter are set according to the 
linguistic environment (Chomsky 1981). This 
proposal suggests that human languages follow 
a common set of principles and differ among 
one another only in finitely many respects, rep- 
resented by a finite number of parameters that 
can vary according to a finite number of val- 
ues (which makes them learnable in Gold's 
paradigm). In this section, we discuss the 
UG and associated parameters, which are for- 
malised in terms of a Unification-Based Gen- 
eralised Categorial Grammar (UB-GCG), em- 
bedded in a default inheritance network of lex- 
ical types. We concentrate on the description 
of word order parameters, which reiiect the ba- 
sic order in which constituents occur in different 
languages. 
UB-GCGs extend the basic Categorial Gram- 
mars ((Bar Hillel, 1964)) by including the use of 
attribute-value pairs associated with each cate- 
gory and by using a larger set of rules and op- 
erators. Words, categories and rules are repre- 
sented in terms of typed default feature struc- 
tures (TDFSS), that encode orthographic, syn- 
tactic and semantic information. There are 
two types of categories: atomic categories (s 
- sentence- ,  np  - noun  phrase- ,  and  n - 
noun) ,  that are saturated, and complex cat- 
egories, that are unsaturated. Complex cate- 
gories have a functor category (defined in RE- 
SUET), and a list of subcategorised lements (de- 
fined in ACTIVE), with each element in the list 
defined in terms of two features: SIGN, encoding 
the category, and DIRECTION, encoding the di- 
rection in which the category is to be combined 
(where VALUE can be either forward or back- 
ward).  As an example, in English an intransi- 
tire verb (s\np) is encoded as shown in figure 
1, where only the relevant attributes are shown. 
In this work, we employ the rules of (forward 
and backward) application, (forward and back- 
ward) composition and generalised weak permu- 
ration. A more detailed description of the UB- 
GCG used can be found in (Villavicencio 2000). 
The UG is implemented as a UB-GCG, era- 
bedded in a default inheritance network of lex- 
ical types (Villavicencio 1999), implemented in
the YADU framework (Lascarides and Copes- 
take 1999). The categories and rules in the 
grammar are defined as types in the hierarchy, 
represented in terms of TDFSS and the feature- 
structures associated with any given category 
or rule are defined by the inheritance chain. 
With different sub-networks used to encode dif- 
ferent kinds of linguistic knowledge, linguistic 
regularities are encoded near the top of a net- 
work, while types further down the network are 
used to represent sub-regularities or exceptions. 
Thus, types are concisely defined, with only 
specific information being described, since more 
general information is inherited from the super- 
types. The resulting UB-GCG is compact, since 
it avoids redundant specifications and the infor- 
mation is structured in a clear and concise way 
through the specification of linguistic regulari- 
ties and sub-regularities and exceptions. 
Regarding the categories of  the UB-GCG, 
word order parameters are those that specify 
the direction of each element in the subcate- 
gorisation list of a complex category. In figure 
1, sub jd i r  is a parameter specifying that the 
np subject is to be combined backwards. As the 
categories are defined in terms of an inheritance 
hierarchy, the parameters (and their values) in 
these categories are propagated throughout the 
hierarchy, from supertypes to subtypes, which 
inherit his information by default. There are 28 
parameters defined, and they are also in a hier- 
archical relationship, with the supertype being 
gendir ,  which specifies, by default, the general 
direction for a language, and from which all the 
other parameters inherit. Among the subtypes, 
we have subjd i r ,  which specifies the direction 
of the subject, vargdir ,  which specifies the di- 
rection of the other verbal arguments and ndir,  
which specifies the direction of nominal care- 
gories. A fragment of the parameters hierarchy 
can be seen in figure 2. With these 28 binary- 
valued parameters the UG defines a space of 
almost 800 grammars. 
The parameters are set based on exposure to 
a particular language, and while they are un- 
set, they inherit their value by default, from 
their supertypes. Then, when they are set, they 
can either continue to inherit by default, in case 
they have the same value as the supertype, or 
they can override this default and specify their 
own value, breaking the inheritance chain. For 
instance, in the case of English, the value of 
210 
intransitive 
RESULT : SIGN : s / 
J 
ACTIVE : SIGN :np 
\[ subjdir DIRECTION : \[VALUE backw d\]\] 
Figure 1: Intransitive Verb type 
gendir 
subjdir vargdir ndir 
/ \  
nmdir detdir 
Figure 2: Fragment of The Parameters Hierar- 
chy 
a default inheritance schema reduces the pieces 
of information to be acquired by the learner, 
since the information is structured and what it 
learns is not a single isolated category, but a 
structure that represents his information in a 
general manner. This is a clear and concise way 
of defining the UG with the parameters being 
straightforwardly defined in the categories, in 
a way  that takes advantage of the default in- 
heritance mechanism, to propagate information 
about parameters, throughout the lexical inher- 
itance network. 
gendir  is defined, by default, as forward, cap- 
turing the fact that it is a predominantly right- 
branching language, and all its subtypes, like 
subjd ir  and vargdir  inherit this default in- 
formation. Then an intransitive verb, which 
has the direction of the subject specified by 
subjdir,  will be defined as S/NP, with sub- 
jd ir  having default value forward. However, 
as in English, the subject NP occurs to the left 
of the verb, utterances with the subject o the 
left will trigger a change in subjd i r  to back- 
ward, which overrides the default value, break- 
ing the inheritance chain, figure 3. As a re- 
sult, intransitive verbs are defined as S\NP, fig- 
ure 1, for the grammar to account for these 
sentences. In the syntactic dimension of this 
network, intransitive verbs can be considered 
the general case of verbs, and the information 
defined in this node is propagated through the 
hierarchy to its subtypes, such as the transitive 
verbs, figure 3. For the learner, the information 
about subjects (subjdir  = backward) has al- 
ready been acquired while learning intransitive 
verbs, and the learner does not need to learn 
it again for transitive verbs, which not only in- 
herit this information, but also have the direc- 
tion for the object defined by vargdir  (vargdir 
= forward), as shown in figure 3. The use of 
2.2 The Corpus 
The UG has to be general enough to capture 
the grammar for any language, and the param- 
eters have to be set to account for a particular 
language, based on exposure to that language. 
This can be obtained by means of a corpus of 
utterances, annotated with logical forms, which 
is described in this section. Among these sen- 
tences, some will be triggers for certain param- 
eters, in the sense that, to parse that sentence, 
some of the parameters will have to be set to 
a given value. We are using the Sachs cor- 
pus (Sachs 1983) from the CHILDES project 
(MacWhinney 1995), that contains interactions 
between only one child and her parents, from 
the age of 1 year and 1 month to 5 years and 1 
month. From the resulting corpus, we extracted 
material for generating two different corpora: 
one containing only the child's sentences and 
the other containing the caretakers' entences. 
The caretakers' corpus is given as input to the 
learner to mirror the input to which a child 
learning a language is exposed. And the child's 
corpus is used for comparative purposes. 
In order to annotate the caretakers' corpus 
with the associated logical forms, a UB-GCG 
for English was built, that covers all the con- 
structions in the corpus: several verbal con- 
211 
top 
.._._~_~' = / complex 
I ndir var.gdiir subjdir = \ intransitive 
,2 ............................. nrndir detdir . . . . .  ,~::::: ....... """"f"'"'~~.'"'"''. 
............ tra'n's~tiv.e..~ oblique intransitive-control 
.............. ~p) /~p (s~np)/pp (s~np)/(s\np) 
d i t ra~s i t~=~'~-~e_cont ro l  
((s\np)/np)/np ((s~np)/np)/pp ((s~np)/np)/(s~np) 
Figure 3: A Fragment of the Network of Types 
structions (intransitives, transitives, ditransi- 
tives, obliques, control verbs, verbs with senten- 
tim complements, etc), declarative, imperative 
and interrogative sentences, and unbounded de- 
pendencies (wh-questions and relative clauses), 
among others. Thus the caretakers' corpus con- 
tains sentences annotated with logical forms, 
and an example can be seen in figure 4, for the 
sentence I wil l  take him, where a simplified ver- 
sion of the relevant attributes is shown, for rea- 
sons of clarity. Each predicate in the semantics 
list is associated with a word in the sentence, 
and, among other things, it contains informa- 
tion about the identifier of the predicate (SIT), 
the required arguments (e.g. ACTOR and UN- 
DERGOER for the verb take), as well as about 
the interaction with other predicates, specified 
by the boxed indices (e.g. take:ACTOR = \[\] = 
/:SIT). This grammar is not only used for anno- 
tating the corpus, but is also the target o which 
the learner has to converge. At the moment 
around 1,300 utterances were annotated with 
corresponding logical forms, with data ranging 
from when the child is 14 months old to 20 
months old. 
2.3 The  Learn ing  A lgor i thm 
The learning algorithm implernents the 
Bayesian Incremental Parameter Setting 
(BIPS) algorithm defined by Briscoe (1999). 
The parameters are binary-valued, where each 
possible value in a parameter is associated with 
a prior and a posterior probability. The value 
with highest posterior probability is used as 
the current value. Initially, in the learning 
process, the posterior probability associated 
with each parameter is initialised to the prior 
probability, and these values are going to define 
the parameter settings used. Then, as trigger 
sentences are successfully parsed, the posterior 
probabilities of the parameter settings that al- 
lowed the sentence to be parsed are reinforced. 
Otherwise, when a sentence cannot be parsed 
(with the correct logical form) the learning 
algorithm checks if a successful parse can be 
achieved by changing the values of some of the 
parameters, in constrained ways. If that is the 
case, the posterior probability of the values 
used are reinforced in each of the parameters, 
and if they achieve a certain threshold, they 
are retained as the current values, otherwise 
the previous values are kept. This constraint 
on the setting of the parameters ensures that a 
trigger does not cause an immediate change to 
a different grammar. The learner, instead, has 
to wait for enough evidence in the data before 
it can change the value of any parameter. As 
a consequence, the learner behaves in a more 
conservative way, being robust to noise present 
in the input data. 
Following Briscoe (1999) the probabilities as- 
sociated with the parameter values correspond 
to weights represented in terms of fractions, 
with the denominator storing the total evidence 
for a parameter and the numerator storing the 
evidence for a particular value of that param- 
eter. For instance, if the value backward  of 
the sub jd i r  parameter has a weight of 9/10, 
it means that from 10 times that evidence was 
provided for subjd i r ,  9 times it was for the 
value backward,  and only once for the other 
value, forward.  Table 1 shows a possible ini- 
tialisation for the sub jd i r  parameter, where the 
prior has a weight of 1/10 for forward,  corre- 
sponding to a probability of 0.1, and a weight of 
212 
s ign  
ORTH : <i, will, take, him> 
CAT : s 
SEM \[ will \] 
: S IT :  \[5\] 
ARGU~IENT : \[\] 
? take \] 
SIT: \[\] 
ACTOR: \[\] 
UNDERGOER : \[\] 
L S IT  : \[\] \] 
Figure 4: Sentence: I will take him 
9/10 for backward,  corresponding to a proba- 
bility of 0.9. The posterior is initialised with the 
same values as the prior, and as backward has 
a higher posterior probability it is used as the 
current value for the parameter. These initial 
parameter values determine the initial gram- 
mar for the learner. As triggers are processed, 
they provide evidence for certain parameters 
and these are represented as additions to the 
denominator and/or numerator of each of the 
posterior weights of the parameter values. Ta- 
ble 2 shows the status of the parameter after 
5 triggers that provided evidence for the value 
backward.  Initially, the learner uses the evi- 
dence provided by the triggers to choose certain 
parameter values, in order to be able to parse 
these triggers uccessfully while generating the 
appropriate logical form. After that, the trig- 
gers are used to reinforce these values, or to 
negate them. 
Table 1: Initialisation of a Parameter 
Value Prior Posterior 
Prob. \[Weight Prob. \[Weight 
1 0.1 1 Forward O. 1 1-o 1o 
Backward 0.9 ~ 0.9 10 10 
and specifies its own value, breaking the inheri- 
tance chain. For instance, in figure 3, subjd ir  
overrides the default value specified by gendir, 
breaking the inheritance chain. Unset subtype 
parameters inherit, by default, the current value 
of their supertypes, and while they are unset 
they do not influence the values of their super- 
types. 
As the parameters are defined in a default 
inheritance hierarchy, each time the posterior 
probability of a given parameter is updated, it 
is necessary to update the posterior probabili- 
ties of its supertypes and examine the current 
parameter settings to determine what the most 
appropriate hierarchy for these settings is, given 
the goal of converging to the target. The learner 
has a preference for grammars (and thus hi- 
erarchies) that not only model the data (rep- 
resented by the current settings) well, but are 
also compact, following the Minimum Descrip- 
tion Length (MDL) Principle. In this case, the 
most probable grammar in the grammar space, 
among the ones consistent with the parameter 
settings, is the one where the default inheritance 
hierarchy is the more concise, having the min- 
imum number of non-default parameter values 
specified, as described in (Villavicencio 2000). 
Table 2: Status of the Parameter 
The 28 word order parameters are defined in 
a hierarchical relation, with the supertype pa- 
rameters being set in accordance with the sub- 
types, to reflect he value of the majority of the 
subtypes. In this way, as the values of the sub- 
types are being set, they influence the value of 
the supertypes. If the value of a given sub- 
type differs from the value of the supertype, 
the subtype overrides the inherited efault value 
Value 
Forward 
I Prior Posterior 
Prob. I Weight Prob. I Weight 
1 0.07 1 O. 1 1-o 1-5 
Backward 0.9 0.93 1j I0 15 
213 
3 The Acquisition of  Word  Order  
We are investigating the acquisition of word 
order, which reflects the underlying order in 
which constituents occur in different languages. 
In this section we describe one experiment, 
where we compare the performance, of differ- 
ent learners under four conditions. Each learner 
is given as input the annotated corpus of sen- 
tences paired with logical forms, and they have 
to change the values of the parameters corre- 
sponding to the relevant constituents oaccount 
for the order in which these constituents ap- 
pear in the input sentences. We defined five 
different learners corresponding to five differ- 
ent initialisations of the parameter settings of 
the UG, to investigate how the init~alisations, 
or starting points, of the learners influence con- 
vergence to the target grammar. The first one, 
the unset learner, is initialised with all param- 
eters unset, and the others, the default learn- 
ers, are each initialised with default parameter 
values corresponding to one of four basic word 
orders, defined in terms of the canonical order 
of the verb (V), subject (S) and objects (O): 
SVO,  SOV,  VSO and OVS.  We initialised the 
parameters subjdir, vargdir and gendir  of the 
default learners according to each of the basic 
orders, with gendir  having the same direction 
as vargdir, and all the other parameters hav- 
ing unset values. These parameters have the 
prior and posterior probabilities initialised with 
0.1 for one value and 0.9 for the other. In this 
way, an SVO learner, for example, is initialised 
with subjd i r  having as current value backward 
(0.9), vargdir  forward (0.9) and gendir  for- 
ward (0.9). 
The sentences in the input corpus are pre- 
sented to a learner only once, sequentially, in 
the original order. The input to a learner is 
pre-processed by a system \[Waldron, 2000\] that 
assigns categories to each word in a sentence. 
The sentences with their putative category as- 
signments are given as input to the learner. The 
learner then evaluates the category assignments 
for each sentence and only uses those that are 
valid according to the UG to set the parame- 
ters; the others are discarded. The corpus con- 
tains 1,041 English sentences (which follow the 
SVO order), but from these only a small propor- 
tion are triggers for the parameters, in the sense 
that, for the learner to process them, it has to 
select certain parameter values. As each trigger- 
ing sentence is processed, the learner changes or 
reinforces its parameter values to reflect he or- 
der of constituents in these sentences. 
We wanted to check how the different learners 
performed in a normal noisy environment, with 
a limited corpus as input, and also to check if 
there is an interaction between the different ini- 
tialisations and the noise in the input data. To 
do that we tested how the learners performed 
under four conditions. Each condition was run 
10 times for each learner, and we report here 
the average results obtained. 
3.1 Condi t ion 1 :Learners -10  in a 
Noisy Environment 
In the first condition, we initialised the param- 
eters subjdir ,  vargdir  and gendir  of the de- 
fault learners with the prior and posterior prob- 
abilities of 0.1 corresponding to a weight of 
1/10, and probabilities of 0.9 to a weight of 
9/10. Results from the first experiment can be 
seen in table 3, where the learners are specified 
in the first column, the number of input triggers 
in the second, the number of correct parameters 
in relation to the target is in the third, and the 
number of parameters that are set with these 
triggers is in the fourth column. 
Table 3: Convergence of the different learners - 
Learners-10 
Learners Triggers Parameters Parameters 
Correct Set 
-Unset 179 22.3 10.5 
SVO-10 211.4 22.5 11 
-SOV-10 205.4 22.2 10.2 
OVS-10 271.5 22.5 11 
VSO-10 198.7 22.1 10.2 
The results show no significant variation in 
the performance of the different Learners. This 
is the case with the number of parameters that 
are correct in relation to the target, with an av- 
erage of 22.3 parameters out of 28, and also with 
the number of parameters that are set given the 
triggers available, with an average of 10.5 pa- 
rameters out of 28. 
The only difference between the learners was 
214 
Sub jd i r  - No isy  Env i ronment  
1 
= 0.9 \[- - - Unset 
svo- o 
~" I sov - lo  a 
'~ 0.6 \[ VSO-IO 
0.5 
Triggers 
Figure 5: Convergence of Subjdir- Learners-10 - Noisy Environment 
the time needed for each learner to converge: 
the closer the starting point of the learner was 
to the target, the faster it converged, as can 
be seen in figure 5, for the subjd i r  parame- 
ter. This figure shows all the learners converg- 
ing to the target value, with high probability, 
and with a convergence pattern very similar to 
the one presented by the unset learner. Even 
those default learners that were initialised with 
values incompatible with the target soon over- 
came this initial bias and converged to the tar- 
get. The same thing happens for vargdir  and 
gendir. This figure also shows some sharp falls 
in the convergence to the target value, for these 
learners. For example, the unset learner had a 
sharp drop in probability, which fell from 0.94 
to 0.85, around trigger 16. These declines were 
caused by noise in the category assignments of
the input triggers, which provided incorrect ev- 
idence for the parameter values. 
3.2 Condition 2:Learners-10 in a 
Noise-free Environment 
In order to test if and how much of the learn- 
ers' performance was affected by the presence 
of noisy triggers, using the same initialisations 
as the ones in condition 1, we tested how the 
learners performed in a noise-free nvironment. 
To obtain such an environment, aseach trigger 
was processed, a module was used for correcting 
the category assignment, if noise was detected. 
The results are shown in table 4. 
These learners have performances similar to 
Table 4: Convergence of the different learners - 
Learners-10 - Noise-free 
Learners Triggers Parameters Parameters 
Correct Set 
Unset 235.1 22.3 10.6 
SVO-10 227.9 22.3 10.6 
SOV-10 213.9 22.6 11.2 
OVS-10 212.2 22.3 10.6 
VSO-10 172.4 22 10 
those in condition 1 (section 3.1), with an av- 
erage of 22.3 of the 28 parameters correct in 
relation to the target, and an average of 10.6 pa- 
rameters that can be set with the triggers avail- 
able. But, in this condition the convergence was 
slightly faster for all learners, as can be seen in 
figure 6. These results show that, indeed, the 
presence of noise slows down the convergence of 
the learners, because they need more triggers to 
compensate for the effect produced by the noisy 
triggers. 
3.3 Condition 3:Learners-50 in a 
Noisy Environment 
We then tested if the use of stronger weights 
to initialise the learners would affect the learn- 
ers performance. The parameters ubjdir,  
vargdir and gendir were initialised with a 
weight of 5/50 for the probability of 0.1 and a 
215 
1 
= 0.9 
'~ 0.8 
0.7 
~ 0.6 
0. 
0.5 
Subjdir- Noise-free Environment 
i i i i i  I I I - -  . . . . . . . . . . . . . . . . . . . .  
// 
!a 
I 
r!  ! i ! i ; i i 
Triggers 
- - Unset 
-- -- SVO-IO 
SOV- 10 
. . . . . . . . . . . . . . . . . . . . . .  OVS-IO 
. . . . . .  VSO-IO 
Figure 6: Convergence of Subjdir- Learners-10 - Noise-free Environment 
weight of 45/50 for the probability ot70.9. These 
weights provide an extreme bias for each of the 
learners. In this condition, the learners were 
tested again in a normal noisy environment. 
Figure 7 shows the convergence patterns pre- 
sented by these learners for the subjd i r  param- 
eter. The effect produced by the noise was in- 
creased with these stronger weights, such that 
all the learners had a slower convergence to the 
target. Even those default leaxners initialised 
with values compatible with the target had a 
slightly slower convergence when compared to 
those in condition 1, with weaker weights, be- 
cause they had to overcome the stronger initial 
bias before converging to the target values. But, 
in spite of that, the performance of the learners 
is only slightly affected by the stronger weights, 
as shown in table 5. They had a performance 
similar to the ones obtained by the learners in 
the previous conditions, as shown in figure 8, 
comparing these learners with those in condi- 
tion 1. 
3.4 Condi t ion 4 :Learners -50  in a 
Noise-free Envi ronment  
When the noise-free nvironment was used with 
these stronger weights, the convergence pattern 
was slightly faster for all learners, when com- 
pared to condition 3 (which used a noisy envi- 
ronment), but still slower than conditions 1and 
2, as shown in figure 9. These learners had a 
similar performance to those obtained in all the 
previous conditions, as can be seen in table 6, 
Noisy Environment 
28 
i 20 \[ \ ]  Set 16
12 I \ [ \ ]  Correct 
8 
4 
0 J i ~-  
"~ y j  ~ y J  : J  ; J  
Learners 
Figure 8: Learners in Noisy Environment 
and in figure 10, which also shows the results 
obtained by the learners in condition 2, which 
Table 5: Convergence of the different learners - 
Learners-50 - Noise 
Learners Triggers Parameters Parameters 
Correct Set 
SVO-50 230.3 22.9 11.8 
SOV-50 168.1 22.4 10.4 
OVS-50 221.4 22.1 10.1 
VSO-50 154.6 21.9 9.7 
216 
1 ., 
,,~ 0.9 
"~ 0.8 
.~ 0.7 
"~ 0.6 
O 
I1. 
0.5 
Subjdir - Noisy Environment 
Triggers 
l SVO-50 soy-50 
- ................ OVS-50 
- - VSO-50 
Figure 7: Convergence of Subjdir - Learners-50 - Noisy Environment 
Subjdir-  Noise-free Environment 
~o. 
"~0. 
30.  "E 
1 
, r -  
Triggers 
I ~SVO-50  SOV-50 ~.  ........ OVS-50 - - VSO-50 
Figure 9: Convergence of Subjdir - Learners-50 - Noise-free Environment 
used weaker weights. 
Table 6: Convergence of the different learners - 
Learners-50 - Noise Free 
Learners Triggers Parameters Parameters 
Correct Set 
SVO-50 221.7 23.2 11.5 
SOV-50 195.4 23.2 11.8 
OVS-50 223.2 22.1 9.9 
VSO-50 223.4 21.8 9.8 
3.5 D iscuss ion 
As confirmed by these results, there is a strong 
interaction between the different starting points 
and the presence of noise. The noise has a 
strong influence on the convergence of the learn- 
ers, slowing down the learning process, since the 
learners need more triggers to compensate for 
the effect caused by the noisy ones. The dif- 
ferent initialisations caused little impact in the 
learners' performance, in spite of noticeably de- 
laying the convergence to the target of those 
learners that have values incompatible with the 
target. Thus, when combining the presence of 
noise with the use of stronger weights , there was 
217 
a significant delay in convergence, w\]~ere the fi- 
nal posterior probability was up to 10% lower 
than in the noise-free case (e.g. for the OVS 
learner), as can be seen in figures 7 and 9. 
Nonetheless, these learners were robust to the 
presence of noise in the input data, only select- 
ing or changing a value for a given parameter 
when there was enough evidence for that. As 
a consequence, all the learners were converging 
towards the target, even with the small amount 
of available triggers, regardless of the initialisa- 
tions and the presence of noise. This is the case 
even with an extreme bias in the initial values. 
Moreover, the learners make effective use of the 
inheritance mechanism topropagate default val- 
ues, with an average of around 4.2 non-default 
specifications for these learners. 
4 Conc lus ion  and Future  Work  
The purpose of this work is to investigate the 
process of grammatical cquisition from a com- 
putational perspective, focusing on the acqui- 
sition of word order from data. Five different 
learners were implemented in this framework 
and we investigated how the starting point for 
the learners affects their performance in con- 
verging to the target and its interaction with 
noise. The learners were all converging towards 
the target grammar, where the different start- 
ing points and the presence of noise affected 
only convergence times, with learners more far 
away from the target having a slower conver- 
gence pattern. Future works include annotat- 
ing more data to have a bigger corpus, and run- 
ning more experiments with this corpus, testing 
how much data is required for all the triggers 
Noise-Free Environment 
o4 ,,__., . . . . . . .  
Learners 
to converge, with high probability to the tar- 
get grammar. After that, we will concentrate 
on investigating the acquisition of subcategori- 
sation frames and argument structure, using the 
same framework for learning. Although this is 
primarily a cognitive computational model, it is 
potentially relevant to the development of more 
adaptive NLP technology. 
5 Acknowledgements  
I would like to thank Ted Briscoe for his com- 
ments and advice on this paper, and Fabio 
Nemetz for his support. Thanks also to the 
anonymous reviewers for their comments. The 
research reported on this paper is supported by 
doctoral studentship from CAPES/Brazil. 
References  
Bar Hillel, Y. Language and Information. Wesley, 
Reading, Mass. 1964. 
Briscoe, T. The Acquisition of Grammar in an 
Evolving Population of Language Agents. Linkop- 
ing Electronic Articles in Computer and Informa- 
tion Science, http://www.ep.liu.se/ea/cis/1999. 
Chomsky, N. Lectures on Government and Binding. 
Foris Publications, 1981. 
Gold, E.M. Language Identification in the Limit. In- 
formation and Control, v.10, p.447-474, 1967. 
Lascarides, A. and Copestake, A. Default Represen- 
tation in Constraint-based Frameworks. Compu- 
tational Linguistics, v.25 n.1, p.55-105, 1999. 
MacWhinney, B. The CHILDES Project: Tools for 
Analyzing Talk. Second Edition, 1995. 
Sachs, J. Talking about the there and then: the emer- 
gence of displaced reference in parent-child is- 
course. In K. E. Nelson editor, Children's lan- 
guage, v.4, 1983. 
Villavicencio, A. Representing a System of Lexical 
Types Using Default Unification. Proceedings of 
EACL, 1999. 
Villavicencio, A. The Acquisition of a Unification- 
Based Generalised Categorial Grammar. Proceed- 
ings of the Third CLUK Colloquium, 2000. 
Waldron, B. Learning Natural Language within the 
framework of categorial grammar. Proceedings of
the Third CLUK Colloquium, 2000. 
Figure 10: Learners in Noise Free Environment 
218 
Extracting the Unextractable: A Case Study on Verb-particles
Timothy Baldwin? and Aline Villavicencio?
? CSLI, Ventura Hall, Stanford University
Stanford, CA 94305-4115 USA
tbaldwin@csli.stanford.edu
? University of Cambridge, Computer Laboratory, William Gates Building
JJ Thomson Avenue, Cambridge CB3 OFD, UK
Aline.Villavicencio@cl.cam.ac.uk
Abstract
This paper proposes a series of techniques for ex-
tracting English verb?particle constructions from
raw text corpora. We initially propose three basic
methods, based on tagger output, chunker output
and a chunk grammar, respectively, with the chunk
grammar method optionally combining with an at-
tachment resolution module to determine the syn-
tactic structure of verb?preposition pairs in ambigu-
ous constructs. We then combine the three methods
together into a single classifier, and add in a number
of extra lexical and frequentistic features, producing
a final F-score of 0.865 over the WSJ.
1 Introduction
There is growing awareness of the pervasiveness and
idiosyncrasy of multiword expressions (MWEs),
and the need for a robust, structured handling
thereof (Sag et al, 2002; Calzolari et al, 2002;
Copestake et al, 2002). Examples of MWEs are
lexically fixed expressions (e.g. ad hoc), idioms (e.g.
see double), light verb constructions (e.g. make a
mistake) and institutionalised phrases (e.g. kindle
excitement).
MWEs pose a challenge to NLP due to their syn-
tactic and semantic idiosyncrasies, which are often
unpredictable from their component parts. Large-
scale manual annotation of MWEs is infeasible due
to their sheer volume (at least equivalent to the num-
ber of simplex words (Jackendoff, 1997)), produc-
tivity and domain-specificity. Ideally, therefore, we
would like to have some means of automatically ex-
tracting MWEs from a given domain or corpus, al-
lowing us to pre-tune our grammar prior to deploy-
ment. It is this task of extraction that we target in
this paper. This research represents a component of
the LinGO multiword expression project,1 which is
targeted at extracting, adequately handling and rep-
resenting MWEs of all types. As a research testbed
and target resource to expand/domain-tune, we use
the LinGO English Resource Grammar (LinGO-
ERG), a linguistically-precise HPSG-based gram-
mar under development at CSLI (Copestake and
Flickinger, 2000; Flickinger, 2000).
The particular MWE type we target for extrac-
tion is the English verb-particle construction.
Verb-particle constructions (?VPCs?) consist of a
1http://lingo.stanford.edu/mwe
head verb and one or more obligatory particles,
in the form of intransitive prepositions (e.g. hand
in), adjectives (e.g. cut short) or verbs (e.g. let
go) (Villavicencio and Copestake, 2002a; Villavicen-
cio and Copestake, 2002b; Huddleston and Pullum,
2002); for the purposes of this paper, we will fo-
cus exclusively on prepositional particles?by far the
most common and productive of the three types?
and further restrict our attention to single-particle
VPCs (i.e. we ignore VPCs such as get alng to-
gether). We define VPCs to optionally select for an
NP complement, i.e. to occur both transitively (e.g.
hand in the paper) and intransitively (e.g. battle on).
One aspect of VPCs that makes them difficult to
extract (cited in, e.g., Smadja (1993)) is that the
verb and particle can be non-contiguous, e.g. hand
the paper in and battle right on. This sets them apart
from conventional collocations and terminology (see,
e.g., Manning and Schu?tze (1999) and McKeown and
Radev (2000)) in that they cannot be captured ef-
fectively using N-grams, due to the variability in the
number and type of words potentially interceding
between the verb and particle.
We are aiming for an extraction technique which
is applicable to any raw text corpus, allowing us to
tune grammars to novel domains. Any linguistic
annotation required during the extraction process,
therefore, is produced through automatic means,
and it is only for reasons of accessibility and compa-
rability with other research that we choose to work
over the Wall Street Journal section of the Penn
Treebank (Marcus et al, 1993). That is, other than
in establishing upper bounds on the performance of
the different extraction methods, we use only the
raw text component of the treebank.
In this paper, we first outline distinguishing fea-
tures of VPCs relevant to the extraction process
(? 2). We then present and evaluate a number of
simple methods for extracting VPCs based on, re-
spectively, POS tagging (? 3), the output of a full
text chunk parser (? 4), and a chunk grammar (? 5).
Finally, we detail enhancements to the basic meth-
ods (? 6) and give a brief description of related re-
search (? 7) before concluding the paper (? 8).
2 Distinguishing Features of VPCs
Here, we review a number of features of VPCs per-
tinent to the extraction task. First, we describe lin-
guistic qualities that characterise VPCs, and second
we analyse the actual occurrence of VPCs in the
WSJ.
2.1 Linguistic features
Given an arbitrary verb?preposition pair, where the
preposition is governed by the verb, a number of
analyses are possible. If the preposition is intransi-
tive, a VPC (either intransitive or transitive) results.
If the preposition is transitive, it must select for an
NP, producing either a prepositional verb (e.g. re-
fer to) or a free verb?preposition combination
(e.g. put it on the table, climb up the ladder).
A number of diagnostics can be used to distinguish
VPCs from both prepositional verbs and free verb?
preposition combinations (Huddleston and Pullum,
2002):
1. transitive VPCs undergo the particle alterna-
tion
2. with transitive VPCs, pronominal objects must
be expressed in the ?split? configuration
3. manner adverbs cannot occur between the verb
and particle
The first two diagnostics are restricted to transitive
VPCs, while the third applies to both intransitive
and transitive VPCs.
The first diagnostic is the canonical test for par-
ticlehood, and states that transitive VPCs take two
word orders: the joined configuration whereby the
verb and particle are adjacent and the NP comple-
ment follows the particle (e.g. hand in the paper),
and the split configuration whereby the NP com-
plement occurs between the verb and particle (e.g.
hand the paper in). Note that prepositional verbs
and free verb?preposition combinations can occur
only in the joined configuration (e.g. refer to the book
vs. *refer the book to). Therefore, the existence of
a verb?preposition pair in the split configuration is
sufficient evidence for a VPC analysis. It is impor-
tant to realise that compatibility with the particle
alternation is a sufficient but not necessary condi-
tion on verb?particlehood. That is, a small number
of VPCs do not readily occur in the split configu-
ration, including carry out (a threat) (cf. ?carry a
threat out).
The second diagnostic stipulates that pronominal
NPs can occur only in the split configuration (hand
it in vs. *hand in it). Note also that heavy NPs tend
to occur in the joined configuration, and that various
other factors interact to determine which configura-
tion a given VPC in context will occur in (see, e.g.,
Gries (2000)).
The third diagnostic states that manner adverbs
cannot intercede between the verb and particle (e.g.
*hand quickly the paper in). Note that this con-
straint is restricted to manner adverbs, and that
there is a small set of adverbs which can pre-modify
particles and hence occur between the verb and par-
ticle (e.g. well in jump well up).
2.2 Corpus occurrence
In order to get a feel for the relative frequency of
VPCs in the corpus targeted for extraction, namely
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0  10  20  30  40  50  60  70
V
P
C
 t
yp
es
 (
%
)
Corpus frequency
Figure 1: Frequency distribution of VPCs in the
WSJ
Tagger correctextracted Prec Rec F?=1
Brill 135135 1.000 0.177 0.301
Penn 667800 0.834 0.565 0.673
Table 1: POS-based extraction results
the WSJ section of the Penn Treebank, we took a
random sample of 200 VPCs from the Alvey Natu-
ral Language Tools grammar (Grover et al, 1993)
and did a manual corpus search for each. In the
case that a VPC was found attested in the WSJ,
we made a note of the frequency of occurrence as:
(a) an intransitive VPC, (b) a transitive VPC in the
joined configuration, and (c) a transitive VPC in the
split configuration. Of the 200 VPCs, only 62 were
attested in the Wall Street Journal corpus (WSJ),
at a mean token frequency of 5.1 and median to-
ken frequency of 2 (frequencies totalled over all 3
usages). Figure 1 indicates the relative proportion
of the 62 attested VPC types which occur with the
indicated frequencies. From this, it is apparent that
two-thirds of VPCs occur at most three times in the
overall corpus, meaning that any extraction method
must be able to handle extremely sparse data.
Of the 62 attested VPCs, 29 have intransitive us-
ages and 45 have transitive usages. Of the 45 at-
tested transitive VPCs, 12 occur in both the joined
and split configurations and can hence be unambigu-
ously identified as VPCs based on the first diagnostic
from above. For the remaining 33 transitive VPCs,
we have only the joined usage, and must find some
alternate means of ruling out a prepositional verb
or free verb?preposition combination analysis. Note
that for the split VPCs, the mean number of words
occurring between the verb and particle was 1.6 and
the maximum 3.
In the evaluation of the various extraction tech-
niques below, recall is determined relative to this
limited set of 62 VPCs attested in the WSJ. That
is, recall is an indication of the proportion of the 62
VPCs contained within the set of extracted VPCs.
3 Method-1: Simple POS-based
Extraction
One obvious method for extracting VPCs is to run a
simple regular expression over the output of a part-
of-speech (POS) tagger, based on the observation
that the Penn Treebank POS tagset, e.g., contains a
dedicated particle tag (RP). Given that all particles
are governed by a verb, extraction consists of simply
locating each particle and searching back (to the left
of the particle, as particles cannot be passivised or
otherwise extraposed) for the head verb of the VPC.
Here and for the subsequent methods, we assume
that the maximum word length for NP complements
in the split configuration for transitive VPCs is 5,2
i.e. that an NP ?heavier? than this would occur more
naturally in the joined configuration. We thus dis-
count all particles which are more than 5 words from
their governing verb. Additionally, we extracted a
set of 73 canonical particles from the LinGO-ERG,
and used this to filter out extraneous particles in the
POS data.
In line with our assumption of raw text to extract
over, we use the Brill tagger (Brill, 1995) to auto-
matically tag the WSJ, rather than making use of
the manual POS annotation provided in the Penn
Treebank. We further lemmatise the data using
morph (Minnen et al, 2001) and extract VPCs based
on the Brill tags. This produces a total of 135 VPCs,
which we evaluate according to the standard metrics
of precision (Prec), recall (Rec) and F-score (F?=1).
Note that here and for the remainder of this pa-
per, precision is calculated according to the man-
ual annotation for the combined total of 4,173 VPC
candidate types extracted by the various methods
described in this paper, whereas recall is relative to
the 62 attested VPCs from the Alvey Tools data as
described above.
As indicated in the first line of Table 1 (?Brill?),
the simple POS-based method results in a precision
of 1.000, recall of 0.177 and F-score of 0.301.
In order to determine the upper bound on per-
formance for this method, we ran the extraction
method over the original tagging from the Penn
Treebank. This resulted in an F-score of 0.774
(?Penn? in Table 1). The primary reason for the
large disparity between the Brill tagger output and
original Penn Treebank annotation is that it is no-
toriously difficult to differentiate between particles,
prepositions and adverbs (Toutanova and Manning,
2000). Over the WSJ, the Brill tagger achieves a
modest tag recall of 0.103 for particles, and tag pre-
cision of 0.838. That is, it is highly conservative in
allocating particle tags, to the extent that it recog-
nises only two particle types for the whole of the
WSJ: out and down.
4 Method-2: Simple Chunk-based
Extraction
To overcome the shortcomings of the Brill tagger
in identifying particles, we next look to full chunk
2Note, this is the same as the maximum span length of 5
used by Smadja (1993), and above the maximum attested NP
length of 3 from our corpus study (see Section 2.2).
WSJ CoNLL
Prec Rec F?=1 Prec Rec F?=1
0.889 0.911 0.900 0.912 0.925 0.919
Table 2: Chunking performance
parsing. Full chunk parsing involves partitioning
up a text into syntactically-cohesive, head-final seg-
ments (?chunks?), without attempting to resolve
inter-chunk dependencies. In the chunk inventory
devised for the CoNLL-2000 test chunking shared
task (Tjong Kim Sang and Buchholz, 2000), a ded-
icated particle chunk type once again exists. It is
therefore possible to adopt an analogous approach to
that from Method-1, in identifying particle chunks
then working back to locate the verb each particle
chunk is associated with.
4.1 Chunk parsing method
In order to chunk parse the WSJ, we first tagged
the full WSJ and Brown corpora using the Brill tag-
ger, and then converted them into chunks based on
the original Penn Treebank parse trees, with the
aid of the conversion script used in preparing the
CoNLL-2000 shared task data.3 We next lemma-
tised the data using morph (Minnen et al, 2000),
and chunk parsed the WSJ with TiMBL 4.1 (Daele-
mans et al, 2001) using the Brown corpus as train-
ing data. TiMBL is a memory-based classification
system based on the k-nearest neighbour algorithm,
which takes as training data a set of fixed-length
feature vectors pre-classified according to an infor-
mation field. For each test instance described over
the same feature vector, it then returns the ?neigh-
bours? at the k-nearest distances to the test instance
and classifies the test instance according to the class
distribution over those neighbours. TiMBL provides
powerful functionality for determining the relative
distance between different values of a given feature
in the form of MVDM, and also supports weighted
voting between neighbours in classifying inputs, e.g.
in the form of inverse distance weighting.
We ran TiMBL based on the feature set described
in Veenstra and van den Bosch (2000), that is using
the 5 word lemmata and POS tags to the left and
3 word lemmata and POS tags to the right of each
focus word, along with the POS tag and lemma for
the focus word. We set k to 5, ran MVDM over only
the POS tags4 and used inverse distance weighting,
but otherwise ran TiMBL with the default settings.
We evaluated the basic TiMBL method over both
the full WSJ data, training on the Brown section
of the Penn Treebank, and over the original shared
task data from CoNLL-2000, the results for which
are presented in Table 2. Note that, similarly to
the CoNLL-2000 shared task, precision, recall and
3Note that the gold standard chunk data for the WSJ was
used only in evaluation of chunking performance, and to es-
tablish upper bounds on the performance of the various ex-
traction methods.
4Based on the results of Veenstra and van den Bosch
(2000) and the observation that MVDM is temperamental
over sparse data (i.e. word lemmata).
Chunker correctextracted Prec Rec F?=1
TiMBL 695854 0.772 0.548 0.641
Penn 651760 0.857 0.694 0.766
Table 3: Chunk tag-based extraction results
F-score are all evaluated at the chunk rather than
the word level. The F-score of 0.919 for the CoNLL-
2000 data is roughly the median score attained by
systems performing in the original task, and slightly
higher than the F-score of 0.915 reported by Veen-
stra and van den Bosch (2000), due to the use of
word lemmata rather than surface forms, and also
inverse distance weighting. The reason for the drop-
off in performance between the CoNLL data and the
full WSJ is due to the CoNLL training and test data
coming from a homogeneous data source, namely a
subsection of the WSJ, but the Brown corpus being
used as the training data in chunking the full extent
of the WSJ.
4.2 Extraction method
Having chunk-parsed the WSJ in the manner de-
scribed above, we next set about extracting VPCs by
identifying each particle chunk, and searching back
for the governing verb. As for Method-1, we allow a
maximum of 5 words to intercede between a particle
and its governing verb, and we apply the additional
stipulation that the only chunks that can occur be-
tween the verb and the particle are: (a) noun chunks,
(b) preposition chunks adjoining noun chunks, and
(c) adverb chunks found in our closed set of particle
pre-modifiers (see ? 2.1). Additionally, we used the
gold standard set of 73 particles to filter out extra-
neous particle chunks, as for Method-1 above.
The results for chunk-based extraction are pre-
sented in Table 3, evaluated over the chunk parser
output (?TiMBL?) and also the gold-standard chunk
data for the WSJ (?Penn?). These results are signifi-
cantly better than those for Method-1 over the Brill
output and Penn data, respectively, both in terms
of the raw number of VPCs extracted and F-score.
One reason for the relative success of extracting over
chunker as compared to tagger output is that our
chunker was considerably more successful than the
Brill tagger at annotating particles, returning an F-
score of 0.737 over particle chunks (precision=0.786,
recall=0.693). The stipulations on particle type and
what could occur between a verb and particle chunk
were crucial in maintaining a high VPC extraction
precision, relative to both particle chunk precision
and the gold standard extraction precision. As can
be seen from the upper bound on recall (i.e. recall
over the gold standard chunk data), however, this
method has limited applicability.
5 Method-3: Chunk Grammar-based
Extraction
The principle weakness of Method-2 was recall, lead-
ing us to implement a rule-based chunk sequencer
which searches for particles in prepositional and ad-
verbial chunks as well as particle chunks. In essence,
Method correctextracted Prec Rec F?=1
Rule?att 6761119 0.604 0.694 0.646
Timbl?att 615823 0.747 0.661 0.702
Penn?att 694927 0.749 0.823 0.784
Rule+att 9513126 0.304 0.823 0.444
Timbl+att 7391049 0.704 0.710 0.707
Penn+att 7501079 0.695 0.871 0.773
Table 4: Chunk grammar-based extraction results
we take each verb chunk in turn, and search to the
right for a single-word particle, prepositional or ad-
verbial chunk which is contained in the gold stan-
dard set of 73 particles. For each such chunk pair,
it then analyses: (a) the chunks which occur be-
tween them to ensure that, maximally, an NP and
particle pre-modifier adverb chunk are found; (b)
the chunks that occur immediately after the parti-
cle/preposition/adverb chunk to check for a clause
boundary or NP; and (c) the clause context of the
verb chunk for possible extraposition of an NP ver-
bal complement, through passivisation or relativisa-
tion. The objective of this analysis is to both deter-
mine the valence of the VPC candidate (intransitive
or transitive) and identify evidence either support-
ing or rejecting a VPC analysis. Evidence for or
against a VPC analysis is in the form of congruence
with the known linguistic properties of VPCs, as de-
scribed in Section 2.1. For example, if a pronominal
noun chunk were found to occur immediately after
the (possibly) particle chunk (e.g. *see off him), a
VPC analysis would not be possible. Alternatively,
if a punctuation mark (e.g. a full stop) were found
to occur immediately after the ?particle? chunk and
nothing interceded between the verb and particle
chunk, then this would be evidence for an intran-
sitive VPC analysis.
The chunk sequencer is not able to furnish posi-
tive or negative evidence for a VPC analysis in all
cases. Indeed, in a high proportion of instances, a
noun chunk (=NP) was found to follow the ?parti-
cle? chunk, leading to ambiguity between analysis as
a VPC, prepositional verb or free verb?preposition
combination (see Section 2.1), or in the case that
an NP occurs between the verb and particle, the
?particle? being the head of a PP post-modifying
an NP. As a case in point, the VP hand the paper in
here could take any of the following structures: (1)
hand [the paper] [in] [here] (transitive VPC hand
in with adjunct NP here), (2) hand [the paper] [in
here] (transitive prepositional verb hand in or sim-
ple transitive verb with PP adjunct), and (3) hand
[the paper in here] (simple transitive verb). In such
cases, we can choose to either (a) avoid committing
ourselves to any one analysis, and ignore all such
ambiguous cases, or (b) use some means to resolve
the attachment ambiguity (i.e. whether the NP is
governed by the verb, resulting in a VPC, or the
preposition, resulting in a prepositional verb or free
verb?preposition combination). In the latter case,
we use an unsupervised attachment disambiguation
method, based on the log-likelihood ratio (?LLR?,
Dunning (1993)). That is, we use the chunker output
to enumerate all the verb?preposition, preposition?
noun and verb?noun bigrams in theWSJ data, based
on chunk heads rather than strict word bigrams. We
then use frequency data to pre-calculate the LLR for
each such type. In the case that the verb and ?par-
ticle? are joined (i.e. no NP occurs between them),
we simply compare the LLR of the verb?noun and
particle?noun pairs, and assume a VPC analysis in
the case that the former is strictly larger than the
latter. In the case that the verb and ?particle? are
split (i.e. we have the chunk sequence VC NC1 PC
NC2),5 we calculate three scores: (1) the product
of the LLR for (the heads of) VC-PC and VC-NC2
(analysis as VPC, with NC2 as an NP adjunct of
the verb); (2) the product of the LLR for NC1-PC
and PC-NC2 (transitive verb analysis, with the PP
modifying NC1); and (3) the product of the LLR for
VC-PC and PC-NC2 (analysis as prepositional verb or
free verb?preposition combination). Only in the case
that the first of these scores is strictly greater than
the other two, do we favour a (transitive) VPC anal-
ysis.
Based on the positive and negative grammatical
evidence from above, for both intransitive and tran-
sitive VPC analyses, we generate four frequency-
based features. The optional advent of data derived
through attachment resolution, again for both in-
transitive and transitive VPC analyses, provides an-
other two features. These features can be combined
in either of two ways: (1) in a rule-based fashion,
where a given verb?preposition pair is extracted out
as a VPC only in the case that there is positive and
no negative evidence for either an intransitive or
transitive VPC analysis (?Rule? in Table 4); and
(2) according to a classifier, using TiMBL to train
over the auto-chunked Brown data, with the same
basic settings as for chunking (with the exception
that each feature is numeric and MVDM is not used
? results presented as ?Timbl? in Table 4). We also
present upper bound results for the classifier-based
method using gold standard chunk data, rather than
the chunker output (?Penn?). For each of these
three basic methods, we present results with and
without the attachment-resolved data (??att?).
Based on the results in Table 4, the classifier-based
method (?Timbl?) is superior to not only the rule-
based method (?Rule?), but also Method-1 and
Method-2. While the rule-based method degrades
significantly when the attachment data is factored
in, the classifier-based method remains at the same
basic F-score value, undergoing a drop in precision
but equivalent gain in recall and gaining more than
120 correct VPCs in the process. Rule+att returns
the highest recall value of all the automatic meth-
ods to date at 0.823, at the cost of low precision at
0.304. This points to the attachment disambigua-
tion method having high recall but low precision.
Timbl?att and Penn?att are equivalent in terms
5Here, VC = verb chunk, NC = noun chunk and PC = (in-
transitive or transitive) preposition chunk.
Method correctextracted Prec Rec F?=1
Combine 719953 0.754 0.710 0.731
M?2 686778 0.882 0.677 0.766
M3?att? 684788 0.868 0.694 0.771
M3+att? 8711020 0.854 0.823 0.838
Combine? 10001164 0.859 0.871 0.865
Combine?Penn 9311047 0.889 0.903 0.896
Table 5: Consolidated extraction results
of precision, but the Penn data leads to considerably
better recall.
6 Improving on the Basic Methods
Comparing the results for the three basic methods,
it is apparent that Method-1 and Method-2 offer
higher precision while Method-3 offers higher recall.
In order to capitalise on the respective strengths of
the different methods, in this section, we investigate
the possibility of combining the outputs of the four
methods into a single consolidated classifier. Sys-
tem combination is achieved by taking the union of
all VPC outputs from all systems, and having a vec-
tor of frequency-based features for each, based on
the outputs of the different methods for the VPC
in question. For each of Method-1 and Method-2,
a single feature is used describing the total number
of occurrences of the given VPC detected by that
method. For Method-3, we retain the 6 features used
as input to Timbl?att, namely the frequency with
which positive and negative evidence was detected
and also the frequency of VPCs detected through at-
tachment resolution, for both intransitive and tran-
sitive VPCs. Training data comes from the output
of the different methods over the Brown corpus, and
the chunking data for Method-2 and Method-3 was
generated using the WSJ gold standard chunk data
as training data, analogously to the method used to
chunk parse the WSJ.
The result of this simple combination process is
presented in the first line of Table 5 (?Combine?).
Encouragingly, we achieved the exact same recall
as the best of the simple methods (Timbl+att) at
0.710, and significantly higher F-score than any in-
dividual method at 0.731.
Steeled by this initial success, we further augment
the feature space with features describing the fre-
quency of occurrence of: (a) the particle in the cor-
pus, and (b) deverbal noun and adjective forms of
the VPC in the corpus (e.g. turnaround, dried-up),
determined through a simple concatenation opera-
tion optionally inserting a hyphen. The first of these
is attempted to reflect the fact that high-frequency
particles (e.g. up, over) are more productive (i.e.
are found in novel VPCs more readily) than low-
frequency particles.6 The deverbal feature is in-
tended to reflect the fact that VPCs have the po-
6We also experimented with a similar feature describing
verb frequency, but found it to either degrade or have no
effect on classifier performance.
tential to undergo deverbalisation whereas prepo-
sitional verbs and free verb?preposition combina-
tions do not.7 We additionally added in features
describing: (a) the number of letters in the verb
lemma, (b) the verb lemma, and (c) the particle
lemma. The first feature was intended to capture
the informal observation that shorter verbs tend
to be more productive than longer verbs (which
offers one possible explanation for the anomalous
call/ring/phone/*telephone up). The second and
third features are intended to capture this same pro-
ductivity effect, but on a individual word-level. Note
that as TiMBL treats all features as fully indepen-
dent, it is not able to directly pick up on the gold
standard verb?particle pairs in the training data to
select in the test data.
The expanded set of features was used to re-
evaluate each of: Method-2 (M?2 in Table 5); theclassifier version of Method-3 with and without
attachment-resolved data (M3?ATT?); and the
simple system combination method (Combine?).
Additionally, we calculated an upper bound for the
expanded feature set based on the gold standard
data for each of the methods (Combine?Penn in Ta-
ble 5). The results for these five consolidated meth-
ods are presented in Table 5.
The addition of the 7 new features leads to an
appreciable gain in both precision and recall for all
methods, with the system combination method once
again proving to be the best performer, at an F-score
of 0.865. The differential between the system com-
bination method when trained over auto-generated
POS and chunk data (Combine?) and that trained
over gold standard data (Combine?Penn) is still tan-gible, but considerably less than for any of the in-
dividual methods. Importantly, Combine? outper-
forms the gold standard results for each of the in-
dividual methods. Examples of false positives (i.e.
verb?prepositions misclassified as VPCs) returned
by this final system configuration are firm away, base
on and very off.
In Section 1, we made the claim that VPCs are
highly productive and domain-specific. We validate
this claim by comparing the 1000 VPCs correctly
extracted by the Combine? method against both
the LinGO-ERG and the relatively broad-coverage
Alvey Tools VPC inventory. The 28 March, 2002
version of the LinGO-ERG contains a total of 300
intransitive and transitive VPC types, of which
195 were contained in the 1000 correctly-extracted
VPCs. Feeding the remaining 805 VPCs into the
grammar (with a lexical type describing their tran-
sitivity) would therefore result in an almost four-
fold increase in the total number of VPCs, and in-
crease the chances of the grammar being able to
parse WSJ-style text. The Alvey Tools data con-
tains a total of 2254 VPC types. Of the 1000 ex-
tracted VPCs, 284 or slightly over 28%, were not
contained in the Alvey data, with examples includ-
ing head down, blend together and bid up. Combin-
ing this result with that for the LinGO-ERG, one can
7Note that only a limited number of VPCs can be dever-
balised in this manner: of the 62 VPCs attested in the WSJ,
only 8 had a deverbal usage.
see that we are not simply extracting information al-
ready at our fingertips, but are accessing significant
numbers of novel VPC types.
7 Related research
There is a moderate amount of research related to
the extraction of VPCs, or more generally phrasal
verbs, which we briefly describe here.
One of the earliest attempts at extracting ?in-
terrupted collocations? (i.e. non-contiguous colloca-
tions, including VPCs), was that of Smadja (1993).
Smadja based his method on bigrams, but unlike
conventional collocation work, described bigrams by
way of the triple of ?word1,word2,posn?, where posn
is the number of words occurring between word 1 and
word2 (up to 4). For VPCs, we can reasonably ex-
pect from 0 to 4 words to occur between the verb
and the particle, leading to 5 distinct variants of
the same VPC and no motivated way of selecting
between them. Smadja did not attempt to evalu-
ate his method other than anecdotally, making any
comparison with our research impossible.
The work of Blaheta and Johnson (2001) is closer
in its objectives to our research, in that it takes a
parsed corpus and extracts out multiword verbs (i.e.
VPCs and prepositional verbs) through the use of
log-linear models. Once again, direct comparison
with our results is difficult, as Blaheta and Johnson
output a ranked list of all verb?preposition pairs,
and subjectively evaluate the quality of different sec-
tions of the list. Additionally, they make no attempt
to distinguish VPCs from prepositional verbs.
The method which is perhaps closest to ours is
that of Kaalep and Muischnek (2002) in extracting
Estonian multiword verbs (which are similar to En-
glish VPCs in that the components of the multiword
verb can be separated by other words). Kaalep and
Muischnek apply the ?mutual expectation? test over
a range of ?positioned bigrams?, similar to those
used by Smadja. They test their method over three
different corpora, with results ranging from a preci-
sion of 0.21 and recall of 0.86 (F-score=0.34) for the
smallest corpus, to a precision of 0.03 and recall of
0.85 (F-score=0.06) for the largest corpus. That is,
high levels of noise are evident in the system output,
and the F-score values are well below those achieved
by our method for English VPCs.
8 Conclusion
In conclusion, this paper has been concerned with
the extraction of English verb?particle construc-
tions from raw text corpora. Three basic meth-
ods were proposed, based on tagger output, chunker
output and a chunk grammar; the chunk grammar
method was optionally combined with attachment
resolution to determine the syntactic structure of
verb?preposition pairs in ambiguous constructs. We
then experimented with combining the output of the
three methods together into a single classifier, and
further complemented the feature space with a num-
ber of lexical and frequentistic features, culminating
in an F-score of 0.865 over the WSJ.
It is relatively simple to adapt the meth-
ods described here to output subcategorisation
types, rather than a binary judgement on verb?
particlehood. This would allow the extracted out-
put to be fed directly into the LinGO-ERG for use
in parsing. We are also interested in extending
the method to extract prepositional verbs, many
of which appear in the attachment resolution data
and are subsequently filtered out by the consolidated
classifier.
Acknowledgements
This research was supported in part by NSF grant
BCS-0094638 and also the Research Collaboration
between NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation and
CSLI, Stanford University. We would like to thank
Francis Bond, Ann Copestake, Dan Flickinger, Di-
ana McCarthy and the three anonymous reviewers
for their valuable input on this research.
References
Don Blaheta and Mark Johnson. 2001. Unsuper-
vised learning of multi-word verbs. In Proc. of
the ACL/EACL 2001 Workshop on the Compu-
tational Extraction, Analysis and Exploitation of
Collocations, pages 54?60.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational
Linguistics, 21:543?65.
Nicoletta Calzolari, Charles J. Fillmore, Ralph Gr-
ishman, Nancy Ide, Alessandro Lenci, Catherine
MacLeod, and Antonio Zampolli. 2002. Towards
best practice for multiword expressions in compu-
tational lexicons. In Proc. of the 3rd International
Conference on Language Resources and Evalua-
tion (LREC 2002), pages 1934?40.
Ann Copestake and Dan Flickinger. 2000. Open
source grammar development environment and
broad-coverage English grammar using HPSG. In
Proc. of the 2nd International Conference on Lan-
guage Resources and Evaluation (LREC 2000),
pages 591?8.
Ann Copestake, Fabre Lambeau, Aline Villavicen-
cio, Francis Bond, Timothy Baldwin, Ivan Sag,
and Dan Flickinger. 2002. Multiword expres-
sions: linguistic precision and reusability. In Proc.
of the 3rd International Conference on Language
Resources and Evaluation (LREC 2002), pages
1941?7.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2001. TiMBL: Tilburg
Memory Based Learner, version 4.1, reference
guide. ILK technical report 01-04.
Ted Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Computa-
tional Linguistics, 19(1):61?74.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
Stefan T. Gries. 2000. Towards multifactorial anal-
yses of syntactic variation: The case of particle
placement. Ph.D. thesis, University of Hamburg.
Claire Grover, John Carroll, and Edward Briscoe.
1993. The Alvey Natural Language Tools gram-
mar (4th release). Technical Report 284, Com-
puter Laboratory, Cambridge University, UK.
Rodney Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Lan-
guage. Cambridge: Cambridge University Press.
Ray Jackendoff. 1997. The Architecture of the Lan-
guage Faculty. Cambridge, MA: MIT Press.
Heiki-Jaan Kaalep and Kadri Muischnek. 2002. Us-
ing the text corpus to create a comprehensive list
of phrasal verbs. In Proc. of the 3rd International
Conference on Language Resources and Evalua-
tion (LREC 2002), pages 101?5.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary
Ann Marcinkiewicz. 1993. Building a large an-
notated corpus of English: the Penn Treebank.
Computational Linguistics, 19(2):313?30.
Kathleen R. McKeown and Dragomir R. Radev.
2000. Collocations. In Robert Dale, Hermann
Moisl, and Harold Somers, editors, Handbook of
Natural Language Processing, chapter 21. Marcel
Dekker.
Guido Minnen, John Carroll, and Darren Pearce.
2000. Robust, applied morphological generation.
In Proc. of the First International Natural Lan-
guage Generation Conference (INLG), pages 201?
8.
Guido Minnen, John Carroll, and Darren Pearce.
2001. Applied morphological processing of En-
glish. 7(3).
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In
Proc. of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-2002), pages 1?15.
Frank Smadja. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,
19(1):143?78.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared
task: Chunking. In Proc. of the 4th Confer-
ence on Computational Natural Language Learn-
ing (CoNLL-2000), pages 127?132.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Proc.
of the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and Very
Large Corpora (EMNLP/VLC-2000).
Jorn Veenstra and Antal van den Bosch. 2000.
Single-classifier memory-based phrase chunking.
In Proc. of the 4th Conference on Computational
Natural Language Learning (CoNLL-2000), pages
157?9.
Aline Villavicencio and Ann Copestake. 2002a.
Phrasal verbs and the LinGO-ERG. LinGO
Working Paper No. 2002-01.
Aline Villavicencio and Ann Copestake. 2002b.
Verb-particle constructions in a computational
grammar. In Proc. of the 9th International Con-
ference on Head-Driven Phrase Structure Gram-
mar (HPSG-2002).
Learning to Distinguish PP Arguments from Adjuncts
Aline Villavicencio
Computer Laboratory, University of Cambridge
J.J Thomson Avenue, Cambridge, CB3 OFD, UK
Phone: +44-1223-763642
Fax: +44-1223-334678
Aline.Villavicencio@cl.cam.ac.uk
Abstract
Words differ in the subcategorisation frames in
which they occur, and there is a strong cor-
relation between the semantic arguments of a
given word and its subcategorisation frame, so
that all its arguments should be included in its
subcategorisation frame. One problem is posed
by the ambiguity between locative prepositional
phrases as arguments of a verb or adjuncts.
As the semantics for the verb is the same in
both cases, it is difficult to differentiate them,
and to learn the appropriate subcategorisation
frame. We propose an approach that uses se-
mantically motivated preposition selection and
frequency information to determine if a locative
PP is an argument or an adjunct. In order to
test this approach, we perform an experiment
using a computational learning system that re-
ceives as input utterances annotated with log-
ical forms. The results obtained indicate that
the learner successfully distinguishes between
arguments (obligatory and optional) and ad-
juncts.
1 Introduction
Words differ in the subcategorisation frames
that realise their semantic arguments, and a
given word may have several different subcate-
gorisation frames. The subcategorisation frame
includes all the complements of a given word.
For instance, the sentences:
?(1)John ate
?(2)John ate the apple
represent the intransitive and transitive frames,
respectively, and both are valid frames associ-
ated with the word eat. Given that the subcat-
egorisation frame of a given word should only
include a given constituent if it is an argument,
one problem is caused by the ambiguous nature
of some constituents, that can be either argu-
ments or adjuncts.
The ability to distinguish between subcate-
gorised arguments and non-subcategorised ad-
juncts is of great importance for several applica-
tions, such as automatic acquisition of subcat-
egorisation lexicons from data, and this prob-
lem has been widely investigated. For instance,
Buchholz (1998) investigates this task using a
memory-based learning approach, where the use
of syntactic and contextual features results in
a 91.6% accuracy in distinguishing arguments
from adjuncts. Brent (1994) looks at the prob-
lem from a more psychologically oriented per-
spective, trying to simulate the environment
available to a human language learner, and us-
ing binomial error estimation to derive subcat-
egorisation frames for verbs, based on imper-
fectly reliable local syntactic cues. This tech-
nique is able to capture the fact that the rel-
ative frequency of a verb-argument sequence is
likely to be higher than that of a verb-adjunct
sequence. However, the cues used in the sim-
ulations are too simple to achieve high accu-
racy. Steedman (1994) suggests the use of se-
mantic information to deal with this ambigu-
ity, given that syntax should be as close as
possible to semantics. Then, given that for
a particular language there is a strong cor-
relation between the subcategorisation frames
and predicate-argument structure of a given
word, from the predicate-argument structure of
a word it is possible to infer its subcategorisa-
tion frame.
In terms of the difficulty of this task, Buch-
holz (1998) found that in the experiments con-
ducted the ambiguity presented by Preposi-
tional Phrases (PPs) was the most difficult case
to classify, accounting for 23% of the errors.
Moreover, Brent (1994) also found in his sim-
ulations that locative adjuncts were sometimes
mistaken for arguments. In this paper we focus
on the problem of distinguishing between loca-
tive PPs as arguments or adjuncts, where only
if a given locative PP is an argument is that
it should be included in the subcategorisation
frame of the verb. The approach proposed here
is to use semantically motivated preposition se-
lection and frequency information to determine
if a locative PP is an argument of the verb or if
it is an adjunct. In order to test this approach,
we use a computational learning system, and
the results obtained indicate the effectiveness
of the approach.
The wider goal of this project is to inves-
tigate the process of grammatical acquisition
from data. Thus, in section 2 we start by giving
some background in language acquisition em-
ployed in the learning model, which is described
in section 3. Characteristics of the ambiguity
between arguments and adjuncts are discussed
in section 4 together with the approach used to
distinguish them. In section 5 we describe an
experiment conducted to test the approach. We
finish with some conclusions and a discussion of
future work.
2 Language Acquisition
In trying to solve the question of how to get a
machine to automatically learn language from
data, we can look at the way people do it. When
we acquire our mother language we are exposed
to an environment that includes noisy and un-
grammatical sentences, the potential influence
of other languages, and many other linguistic
phenomena. In spite of that, most children are
successful in the acquisition of a grammar in a
relatively short time, acquiring a sophisticated
mechanism for expressing their ideas, based on
data that is said to be too impoverished to gen-
erate such a complex capacity. One approach
to explain the acquisition of languages proposes
that children must have some innate knowledge
about language, a Universal Grammar (UG), to
help them overcome the problem of the poverty
of the stimulus and acquire a grammar on the
basis of positive evidence only (Chomsky 1965).
According to Chomsky?s Principles and Param-
eters Theory (Chomsky 1981), the UG is com-
posed of principles and parameters, and the pro-
cess of learning a language is regarded as the set-
ting of values of a number of parameters, given
exposure to this particular language. Another
likely source of information that is available to
children when learning a language is the seman-
tic interpretation or related conceptual repre-
sentation. Indeed, as Steedman (1994) puts it:
?Since the main thing that syntax is for is
passing concepts around, the belief that syntac-
tic structure keeps as close as possible to seman-
tics, and that in both evolutionary and child lan-
guage acquisition terms, the early development
of syntax amounts to little more than hanging
words onto the preexisting armatures of concep-
tual structure is so simple and probable as to
amount to the null hypothesis?.
A third source of information can be found
in the statistical properties of the input data to
which children seem to be sensitive, as observed
in recent work in psycholinguistics.
3 The Learning System
These ideas about human language acquisition
are employed, in this work, in the construc-
tion of a computational learning system that
can learn from its linguistic environment, which
may contain noise and ambiguities (Villavicen-
cio 2002).
Studies like this can not only be used to pro-
vide clues about possible directions to follow in
the automatic acquisition of information from
data, but also to help us understand better the
process of human language learning. However,
if that is to be achieved, we need to concentrate
only on algorithms and resources that a human
learner could employ. Thus, there are signif-
icant constraints on the assumptions that can
be made in the learning system implemented.
In this way, the learner cannot have access to
negative information; it also cannot start with
information specific to a particular language,
and can only assume information that is gen-
eral among human languages. Another aspect
is that learning has to be on-line and incremen-
tal, with the system only processing one sen-
tence at a time, without the possibility of stor-
ing sentences and reprocessing previously seen
sentences, or doing multiple passes through the
corpus. Moreover, the kind of data given to the
learner must be compatible with the linguistic
environment of a child.
In this work the linguistic environment of the
learner is simulated to a certain extent by us-
ing spontaneous child-directed sentences in En-
glish, which were extracted from the Sachs cor-
pus (MacWhinney 1995) (Sachs 1983). Some of
the semantic and contextual information avail-
able to children is introduced in the corpus by
annotating the sentences with logical forms. At
the moment around 1,500 parents? sentences are
annotated with the corresponding logical forms.
The computational learning system employed
in this investigation is composed of a UG and
associated parameters, and a learning algorithm
(Villavicencio 2002). The UG is represented
as a Unification-Based Generalised Categorial
Grammar, and it provides the core knowledge
about grammars that the learner has. A learn-
ing algorithm fixes the parameters of the UG to
the target language based on exposure to it. In
this work, this is in the form of the annotated
parents? sentences to simulates some of the char-
acteristics of the environment in which a child
acquires her language. Finally, children?s sensi-
tivity to statistical properties of the data is also
simulated to some extent in the learning system.
4 Learning from Ambiguous Triggers
The learning environment to which the learner
is exposed contains noise and ambiguity and the
learner has to be able to deal with these prob-
lems if it is to set its parameters correctly and
converge to the target grammar. In this work
we concentrate on the ambiguity in the form of
locative PP that can occur either as arguments
to a verb or as adjuncts.
When processing a sentence the learner needs
to determine appropriate syntactic categories
for the semantic predicates used as input in or-
der to correctly set its parameters. In most
cases, the learner is able to find the required
syntactic categories, using the Categorial Prin-
ciples (Steedman 2000). According to these
principles from the semantic interpretation of
a word and some directional information for a
language, it is possible to determine the syntac-
tic form of the corresponding category. 1 These
principles help the learner to determine the sub-
categorisation frame for a given word based on
1These principles are closely related to the Projection
Principle (Chomsky 1981) that states that the selectional
requirements of a word are projected onto every level of
syntactic representation.
its semantic predicate. Then, for instance, in
the sentence:
?(3)John talks to Mary
with logical form
?(4) talk-communicative-act(e,x,y), john(x),
comm-to(y), mary(y)
the verb talks has two arguments, the NP sub-
ject John, and the PP to Mary, as represented
in the logical form associated with the verb,
where the PP is the second argument and as
such should be included in the subcategorisation
frame of the verb: (S\NP)/PP. On the other
hand, in the sentence:
?(5)Bob eats with a fork
with logical form
?(6) eat-ingest-act(e,x), bob(x), instr-
with(e,y), a(y), fork(y)
the PP with a fork is not an argument of the
verb eat as reflected in its logical form and
should not be included in its subcategorisation
frame, which is S\NP.
It means that from the logical form associ-
ated with a verb, the learner can decide whether
a given constituent is an argument of the verb,
and should be included as its complement in
the subcategorisation frame or not. However,
one exception to this case is that of verbs oc-
curring with locative PPs, which can be either
arguments or adjuncts. The ambiguity between
these cases arises because in this logical form
representation the logical form describing the
verb with an argument locative PP is similar to
that describing the verb with an adjunct loca-
tive PP. For example, the sentence:
? (7) Bill kisses Mary in the park,
with logical form:
?(8) kiss-contact-act(e,x,y), bill(x), mary(y),
loc-in(e,z), the(z), park(z)
exemplifies a case where the locative PP is an
adjunct. Thus it should not be included in the
subcategorisation frame of the transitive verb
kiss, which is (S\NP)/NP. On the other hand,
the sentence:
?(9)Bill swims across the river
with logical form:
?(10) swim-motion-act(e,x), bill(x), motion-
across(e,y), the(y), river(y)
shows a case where the PP is an (optional) argu-
ment of the verb swim, and where the appropri-
ate subcategorisation frame for the verb should
include it ((S\NP)/PP), even though the PP is
not included in the logical form of the verb.
For both sentences, the logical form has a sim-
ilar structure, with both a verbal and a loca-
tive predicate, with the PP not being included
in the logical form of the verb. As a con-
sequence, the logical form cannot be used to
help the learner resolve the ambiguity: given
the logical forms {kiss-contact-act(e,x,y), loc-
in(e,z)} and {swim-motion-act(e,x), motion-
across(e,y)}, which syntactic category should
the learner choose for each of these verbs? This
ambiguity constitutes a significant problem for
the learner, since it has to decide whether a
given PP is functioning as a complement of a
verb or if it is working as an adjunct. Three
different cases to which the learner is exposed
are identified, based on Pustejovsky (1995) and
Wechsler (1995), with the PP occurring as an
obligatory argument, as an optional argument,
or as an adjunct2:
1. The PP is an obligatory argument of
the verb. For certain verbs the PP is an
obligatory argument of the verb and should
be included in its subcategorisation frame.
An instance of this case is the verb put, in
sentence 11:
? (11) Mary put the book on the shelf,
where the verb occurs with a locative PP.
Also, as the ungrammaticality of sentence
12 suggests, this verb requires a locative
PP:
? (12)* Mary put the book
The appropriate syntactic category for the
verb3 is ((S\NP)/PP)/NP.
2. The PP is an optional semantic argu-
ment of the verb. For example, a verb
such as swim can occur as in sentence 9,
where it is modified by a directional PP
which is an optional argument of the verb,
but this verb may also occur without the
PP, as in sentence 13:
? (13) Bill swims.
2In this work we classify PPs in terms of these three
cases, even though more fine-grained classifications can
be used as by Pustejovsky (1995).
3This work does not include, in its investigation,
elliptical or noisy constructions. Therefore, the sen-
tences analysed and the frequencies reported exclude
these cases.
This is a case of a verb that can occur in
both constructions with the PP being a se-
mantic argument, which, when occurring,
must be included in the subcategorisation
frame of the verb. Consequently, the ap-
propriate category for the verb swim in sen-
tence 9 is (S\NP)/PP, and in 13 is S\NP.
3. The PP is an adjunct. Adjuncts modify
the logical form of the sentence, but are not
part of the subcategorisation frame of the
verb. The PP in the park in sentence 7 is an
example of an adjunct that is neither part
of the semantic argument structure of the
verb kiss nor part of its subcategorisation
frame. This verb can also occur without
the PP, as in sentence 14:
? (14) Bill kisses Mary.
The appropriate syntactic category for the
verb in both sentences is (S\NP)/NP.
When faced with a locative PP, the learner
has to identify which of these cases is appro-
priate. The required subcategorisation frame is
determined independently for each verb sense,
depending on the semantic type of the verb,
and on its frequency of occurrence with a par-
ticular subcategorisation frame and predicate
argument-structure combination.
In order to determine if a locative PP is an
obligatory argument of the verb, the learner
uses frequency information about the occur-
rence of each verb with locative PPs. If the fre-
quency with which they occur together is above
a certain threshold, the PP is considered to be
an obligatory argument of the verb and included
in its subcategorisation frame. In this case, the
threshold is set to 80% of the total occurrences
of a verb. This is high enough for discarding
adjuncts and optional arguments that occur oc-
casionally, and at the same time is not high
enough to be affected by the occurrence of noise.
In an analysis of all the mother?s sentences in
the entire Sachs corpus, only two occurrences
of put without the locative PP were found: one
seems to be an instance of an elliptical construc-
tion, and the other a derived sense. The fre-
quency with which put occurs with a locative
PP correctly indicates that the PP is an argu-
ment of the verb, and it needs to be included
in the subcategorisation frame of the verb. On
the other hand, for verbs like kiss and swim in
sentences like 7 and 9, the locative PP is an oc-
casional constituent, with the semantics of the
sentence including the location predicate only in
these cases. The occasional occurrence of PPs
with these verbs correctly indicates that they
are not obligatory arguments of the verbs.
If the frequency of occurrence is not above the
threshold, then the PP can be either an optional
argument or an adjunct. To determine if a PP is
an optional argument, the learner uses informa-
tion about the kind of semantic event denoted
by the verb. As Steedman (1994) notes
?... if we are asking ourselves why children
do not classify meet as subcategorising for NP
PP on the basis of sentences like (1b), we met
Harry on the bus, then we are simply asking
the wrong question. A child who learns this in-
stance of the verb from this sentence must start
from the knowledge that the denoted event is a
meeting, and that this involves a transitive event
concept?.
Thus, when the learner receives an input sen-
tence, it uses semantic information about the
kind of event denoted by the verb and prepo-
sition given in the logical form associated with
the sentence to check if the preposition can be
selected by the verb. This approach to iden-
tify non-obligatory argument PPs is based on
Wechsler?s proposal of semantically motivated
preposition selection (Wechsler 1995), where a
PP is an argument of a verb if it can be selected
by the verb on pragmatic grounds. The learner
represents pragmatic knowledge in terms of a
hierarchy of types and words are classified ac-
cording to these types, based on the seman-
tics associated with them.4 A verb can select
a preposition as an argument if the latter is of
the same type as the verb, or of one of its sub-
types in the hierarchy. A fragment of such a
hierarchy is shown in figure 1. Then, a verb
such as talk (in John talks to Mary), which as
specified in the logical form (in 4) denotes a
communicative event and is an instance of type
communicative-act, can select as its optional
argument a preposition such as to, which is of
type comm-to, because the latter type is a
subtype of the former on the world knowledge
4In this work we do not address the issue of how such
a pragmatic hierarchy would be constructed and we as-
sume that it is already in place. However, for a related
task, see Green (1997).
 	  
 	  
 
 Verb-Particle Constructions and Lexical Resources
Aline Villavicencio
University of Cambridge Computer Laboratory,
William Gates Building, JJ Thomson Avenue,
Cambridge, CB3 0FD, UK
Aline.Villavicencio@cl.cam.ac.uk
Abstract
In this paper we investigate the phe-
nomenon of verb-particle constructions,
discussing their characteristics and their
availability for use with NLP systems. We
concentrate in particular on the coverage
provided by some electronic resources.
Given the constantly growing number of
verb-particle combinations, possible ways
of extending the coverage of the available
resources are investigated, taking into ac-
count regular patterns found in some pro-
ductive combinations of verbs and parti-
cles. We discuss, in particular, the use
of Levin?s (1993) classes of verbs as a
means to obtain productive verb-particle
constructions, and discuss the issues in-
volved in adopting such an approach.
1 Introduction
In this paper we discuss verb-particle constructions
(VPCs) in English and analyse some of the avail-
able sources of information about them for use in
NLP systems. VPCs can range from idiosyncratic
or semi-idiosyncratic combinations, such as get on
(in e.g. Bill got on well with his new colleagues), to
more regular ones, such as tear up (in e.g. In a rage
she tore up the letter Jack gave her). However, ex-
amples of ?idiomatic? VPCs like get on, meaning to
be on friendly terms with someone, where the mean-
ing of the combination cannot be straightforwardly
inferred from the meaning of the verb and the par-
ticle, fortunately seem to be a small minority (Side,
1990). Most cases seem to be more regular, with the
particle compositionally adding a specific meaning
to the construction and following a productive pat-
tern (e.g. in tear up, cut up and split up, where the
verbs are semantically related and up adds a sense of
completion the action of these verbs).
VPCs have been the subject of a considerable
amount of interest, and some investigation has been
done on the subject of productive VPCs. For in-
stance, even though the particle up occurs with a
wide range of verbs, it only combines productively
with some classes. Bame (1999) discusses two such
cases: the resultative and the aspectual up. For ex-
ample Kim carried the television up uses a resulta-
tive up and Kim ate the sandwich up an aspectual
up. With the resultative up, the argument is affected
(i.e., at the end of the action the television is up).
In contrast, the aspectual or completive up suggests
that the action is taken to some conclusion (i.e., the
sandwich is totally consumed at the end of the ac-
tion).
Fraser (1976) points out that semantic proper-
ties of verbs can affect their possibilities of com-
bining with particles. Thus, semantic properties
can influence the patterns of verb-particle combina-
tions that verbs follow. For example, in the case
of bolt/cement/clam/glue/paste/nail all are seman-
tically similar verbs where the objects specified by
the verbs are used to join material and they can all
combine with down. There is clearly a common se-
mantic thread running through this list, so that a new
verb that is semantically similar to them can also be
reasonably assumed to combine with down. Indeed
Side notes that frequently new VPCs are formed by
analogy with existing ones, with often the verb be-
ing varied and the particle remaining (e.g. hang on,
hold on and wait on).
By identifying classes of verbs that follow pat-
terns such as these in VPCs, we are able to max-
imise the use of the information contained in lexical
resources. For instance, we can make use of regular
patterns to productively generate VPCs from verbs
already listed in a lexical resource, according to their
verbal classes (e.g. the resultative combinations
walk up/down/out/in/away/around/... from walk and
the directional/locative particles up, down, out, in,
away, around, ...). We consider how we can use pro-
ductive patterns to extend the coverage of current
lexical resources, in the next sections. We start by
characterising VPCs, and investigating the coverage
provided by some available electronic dictionaries,
in section 3. We also discuss the use of corpora to
extend the coverage provided by these dictionaries.
After that we investigate how more productive com-
binations can be generated from a semantic classifi-
cation of verbs such as Levin?s (1993).
2 Characterizing VPCs
VPCs are combinations of verbs and prepositional
or adverbial particles, such as break down in The
old truck broke down. In these constructions par-
ticles are characterised by containing features of
motion-through-location and of completion or result
in their core meaning (Bolinger, 1971). In syntactic
terms in the example above we have an intransitive
VPC, where no other verbal complement is required.
Other VPCs may have further subcategorisation re-
quirements, and in, for example, They came across
an old manuscript we have a transitive VPC which
has a further NP complement.
In this work we are looking exclusively at cases of
VPCs, thus excluding prepositional verbs, where a
verb subcategorises for a prepositional phrase (PP),
such as rely on, in He relies on his wife for every-
thing. Cases like this and others of adverbial mod-
ification need to be distinguished from VPCs. This
difference may be quite subtle and, in order to dis-
tinguish VPCs from other constructions we use the
following criteria:
  The particle may come either before or after the
NP in transitive VPCs (e.g. He backed up the
team vs He backed the team up). Whether a
particle can be separated or not from the verb
may depend on the degree of bondage of the
particle with the verb, on the size of the NP,
and on the kind of NP.
  In transitive VPCs unstressed personal pro-
nouns must precede the particle (e.g. They ate
it up but not *They ate up it).
  The particle, in transitive VPCs, comes before
a simple definite NP without taking it as its ob-
ject (e.g. He brought along his girlfriend but
not It consists of two parts).
  In VPCs subcategorising for other verbal com-
plements, like PPs and sentential complements,
the particle must come immediately after the
verb.
  Verbs that subcategorise for an optional goal ar-
gument that is fullfilled by a locative or direc-
tional particle are considered to be VPCs with
the particle further specifying the meaning of
the verb (e.g. walk up in Bill walked up the
hill).
As discussed by Bolinger (1971), many of the cri-
teria proposed for diagnosing VPCs give different
results for the same combination frequently includ-
ing unwanted combinations and excluding genuine
VPCs. Nonetheless, they provide us with at least the
basis for this decision.
3 Dictionaries and VPCs
Dictionaries are a major source of information about
VPCs. In Table 1 we can see the coverage of
phrasal verbs (PVs) in several dictionaries and lexi-
cons: Collins Cobuild Dictionary of Phrasal Verbs
(Collins-PV), Cambridge International Dictionary
of Phrasal Verbs (CIDE-PV), the electronic versions
of the Alvey Natural Language Tools (ANLT) lexi-
con (Carroll and Grover, 1989) (which was derived
from the Longman Dictionary of Contemporary En-
glish, LDOCE), the Comlex lexicon (Macleod and
Grishman, 1998), and the LinGO English Resource
Grammar (ERG) (Copestake and Flickinger, 2000)
version of November 2001. This table shows in the
second column the number of PV entries for each of
Top Particles
0
100
200
300
400
500
600
ANLT Comlex ERG A+C A+C+E
Dictionaries
VP
Cs
up
out
off
down
away
Figure 1: Top Ranked Particles I
these dictionaries, including not only VPCs but also
other kinds of PV. The third column shows the num-
ber of VPC entries (available only for the electronic
dictionaries).
Table 1: Phrasal Verb Entries in Dictionaries
Dictionary PVs VPCs
ANLT 6,439 2,906
CIDE-PV over 4,500 -
Collins-PV over 3,000 -
Comlex 12,564 4,039
ERG 533 337
As we can see from these numbers, each of these
dictionaries has a considerable number of PV entries
potentially providing us with a good starting point
for handling VPCs. Table 2 shows some of the char-
acteristics of each dictionary, in more detail, with
respect to VPCs, where the seventh column shows
the proportion of verbs used in VPCs (sixth column)
from all verbs in a dictionary (second column).
Each of these dictionaries uses a different set of
verbs and particles in its VPCs. However, with re-
spect to the verbs listed in these dictionaries there
is a high level of agreement among them with, for
example, 93.26% of the verbs in Comlex being also
listed in ANLT. In Table 2 we can see the increase in
the number of verbs obtained by the union of the dic-
tionaries, where A+C represents the union of ANLT
and Comlex, A  C their intersection and A+C+E the
union of ANLT, Comlex and ERG. Because of the
high level of agreement for their verbs, when joined
together the contribution made by each dictionary is
relatively small, so that the combination of the three
(A+C+E) has only 7.3% more verbs than the ANLT
alone, for example.
In relation to VPCs, ANLT uses the largest num-
ber of particles, and with one exception all the par-
ticles contained in the ERG and Comlex are already
contained in ANLT. When we rank the particles ac-
cording to the frequency with which they occur in
the VPCs, we get similar patterns for all of the dic-
tionaries, as can be seen in Figure 1. This figure
shows the 5 top ranked particles for each of the dic-
tionaries, and for all of them up is the particle in-
volved in the largest number of combinations. By
analysing the VPCs in each of these dictionaries, we
can also see that only a small proportion of the total
number of verbs in a dictionary is used in its VPCs,
Table 2. For example, only 20% of the verbs listed
in ANLT form at least one VPC. For the other dic-
tionaries this proportion is even lower. These tend
to be very widely used and general verbs, such as
come, go, get, put, bring and take. Whether the re-
maining verbs do not form valid VPCs or whether
the combinations were simply omitted remains to be
investigated.
Even though only a subset of verbs in dictionaries
are used in VPCs, this subset generates a large num-
ber of combinations, as shown in Table 2. Each of
these dictionaries specialises in a subset of VPCs.
Because of this difference in coverage, when the
dictionaries are combined, as each one is added it
helps to significantly extend the coverage of VPCs.
Although there is a significant number of entries1
that are common among the different dictionaries,
it seems to correspond only to a subset of the to-
tal number of entries each dictionary has. For in-
stance, from the total number of entries obtained by
combining ANLT and Comlex, Table 2, only 34%
of the entries are listed in both dictionaries with the
remaining 66% of the total number of entries being
exclusive to one or the other of these dictionaries.
Moreover, even with the large number of entries al-
ready obtained by combining these two dictionaries,
a considerable proportion (16%) of the entries in the
LinGO ERG lexicon are not listed in any of these
two dictionaries (this proportion would increase if
we took subcategorization etc into account).2 Most
1These gures do not take into account subcategorisation
information, where a given verb-particle construction can occur
with more than one subcategorisation frame.
2The LinGO ERG lexicon was manually constructed with
Table 2: VPCs in Dictionaries
Dictionary Verbs VPC Distinct Particles Verbs Proportion of
Entries VPCs in VPCs Verbs in VPCs
ANLT 5,667 2,906 2,250 44 1,135 20%
Comlex 5,577 4,039 1,909 23 990 17.75%
ERG 1,223 337 270 25 176 14.39%
A+C 6,043 - 3,111 44 1,394 23.07%
A  C 5,201 - 1,052 23 731 14.05%
A+C+E 6,113 - 3,156 45 1,400 22.90%
of these are at least semi-compositional, e.g., crisp
up, come together, tie on, and were probably omit-
ted from the dictionaries for that reason,3 though
some others, such as hack up, are probably recent
coinages. The coverage of these resources is quite
limited and possible ways of extending it are a ne-
cessity for successful NLP systems.
4 VPCs in Corpora
The use of corpora to extract verb-particle com-
binations can contribute to extending the coverage
of dictionaries. An investigation of the automatic
extraction of VPCs from corpora is described in
Baldwin and Villavicencio (2002). In this section
we use VPCs extracted from the British National
Corpus (BNC), comparing these VPCs with those
contained in the combined A+C+E-VPCs, and dis-
cussing how the former can be used to complement
the coverage provided by the latter.
The BNC is a 100 million word corpus contain-
ing samples of written text from a wide variety of
sources, designed to represent as wide a range of
modern British English as possible. Using the meth-
ods described in Baldwin and Villavicencio (2002),
8,751 VPC entries were extracted from the BNC.
These entries are classified into intransitive and/or
transitive VPCs, depending on their subcategorisa-
tion frame, and they result in 7,078 distinct VPCs.
Some of these entries are not VPCs but rather noise,
such as **** off, ?s down, etc. After removing the
most obvious cases of noise, there were 7,070 VPCs
most of the verb-particle entries being empirically motivated by
the Verbmobil corpus. It is thus probably reasonably represen-
tative of a moderate-size domain-specic lexicon.
3The Cobuild Dictionary explicitly states that literal mean-
ings and combinations are not given for all verbs.
left. These are formed by 2,542 verbs and 48 parti-
cles, as shown in Table 3.
Table 3: VPCs from Dictionaries and from BNC
Resources VPCs Verbs Particles
A+C+E 3,156 1,400 45
BNC 7,070 2,542 48
A+C+E  BNC 2,014 1,149 28
A+C+E - BNC 1,138 251 17
BNC - A+C+E 5,056 1,393 20
A+C+E+BNC 8,208 2,793 65
When comparing the VPCs in BNC (BNC-VPCs)
with those in the combined dictionaries (A+C+E-
VPCs) there are 1,149 verbs in common, corre-
sponding to 82.1% of the verbs in the combined dic-
tionaries. When these resources are joined together,
there is a significant increase in the number of verbs
and particles, with a total of 2,793 different verbs
and 65 particles used in VPCs, Table 3. The verbs
that appear in the largest number of VPCs are again
general and widely used (e.g. move, come, go, get
and pull). For these, the five particles that occur in
the highest number of VPCs are shown in Figure 2,
and they are basically the same as those in the dic-
tionaries.
In terms of the VPCs, by joining A+C+E-VPCs
with BNC-VPCs there is an increase of 160.30% in
the number of VPCs. Among the extracted VPCs
many form productive combinations: some contain-
ing a more informal or a recent use of verbs (e.g. hop
off, kangaroo down and skateboard away). These
VPCs provide a useful addition to those contained
in the dictionaries. However, we are still able to ob-
Top Particles
0
300
600
900
1200
1500
1800
AN
LT
Co
ml
ex
ER
G
A+
C
A+
C+
E
BN
C
A+
C+
E+
BN
C
Dictionaries
V
P
C
s
up
out
off
down
away
Figure 2: Top Ranked Particles II
tain only a subset of the existing VPCs, and plau-
sible combinations such as hoover up are not found
in these combined resources. In the next section we
discuss how to extend even further their coverage by
making use of productive patterns found in classes
of semantically related verbs.
5 VPC Patterns in Levin?s Verb Classes
Fraser (1976) noted how semantic properties of
verbs can affect their possibilities of combination
with particles (e.g. hunt/track/trail/follow down
and bake/cook/fry/broil up). Semantic properties of
verbs can influence the patterns of combination that
they follow (e.g. verbs of hunting and the resulta-
tive down and verbs of cooking and the aspectual
up). By having a semantic classification of verbs we
can determine how they combine with certain par-
ticles, and this can be used to extend the coverage
of the available resources by productively generat-
ing VPCs from classes of related verbs according to
the patterns that they follow. One such classification
was proposed by Levin (1993). In Levin?s classifica-
tion, verbs are grouped into classes in terms of their
syntactic and semantic properties. These classes
were not developed specifically for VPCs, but it may
be the case that some productive patterns of combi-
nations correspond to certain classes of verbs. We
investigated the possibility of using Levin?s classes
of verbs to generate a set of candidate VPCs, and in
this section, we briefly discuss Levin?s classes and
describe how they can be used to predict productive
verb-particle combinations.
There are 190 fine grained subclasses that cap-
ture 3,100 different verbs listed, resulting in 4,167
entries, since each verb can belong to more than
one class. For example, the verb to run belongs
to classes 26.3 (Verbs of Preparing), 47.5.1 (Swarm
Verbs), 47.7 (Meander Verbs) and 51.3.2 (Run
Verbs). The number of elements in each class varies
considerably, so that 60% of all of these classes have
more than 10 elements, accounting for 88% of the
verbs, while the other 40% of the classes have 10 or
less elements, capturing the remaining 22% of the
verbs. The 5 larger classes are shown in Table 4.
Table 4: Verb Entries in Levin?s Classes
Class Name Entries
45.4 Other alternating 257
verbs of change of state
31.1 Amuse 220
51.3.2 Run 124
43.2 Sound emission 119
9.9 Butter 109
It is possible that some productive patterns found
in VPCs may be mapped onto the classes defined.
In this case, some classes may be good predic-
tors of productive VPCs, and to test this possibility
we analysed the combinations generated by Levin?s
classes and a subset of four particles (down, in,
out, up). To test the validity of a resulting com-
bination, we searched for it first among the VPCs
from the combined dictionaries, A+C+E-VPCs, and
then among the much more numerous but potentially
noisy A+C+E+BNC-VPCs.
All combinations of verbs in Levin?s classes and
these four particles were generated and tested for va-
lidity. We use the proportion of valid VPCs as a met-
ric to determine the degree of productivity of a given
class, so that the higher the proportion, the more
productive the class, according to the combined re-
sources. The classes are then ranked according to
their productivity degree.
There are 16,668 possible combinations that can
be generated, from the 4,167 entries in Levin?s
classes and four particles. However, from the 4,167
only 3,914 entries have verbs that are in A+C+E, so
we will consider only 15,656 possible VPCs, when
evaluating these results against the combined dictio-
naries.
When we compare the 15,656 possible VPCs
with those in A+C+E, 2,456 were considered valid
Top 10 Classes 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
11
.3 
- B
rin
g/t
ak
e
39
.1 
- E
at
37
.5 
- T
alk
23
.2 
- S
pli
t
15
.2 
- K
ee
p
54
.1 
- R
eg
ist
er
12
 - P
us
h/P
ull
9.2
 - P
ut 
in 
Sp
ati
al 
Co
nfi
g
35
.1 
- H
un
t
11
.2 
- S
lid
e
Classes
P
ro
po
rt
io
n
VPCs
Figure 3: Levin?s VPCs in Dictionaries
(15.69%). In Figure 3, we can see the degree of pro-
ductivity of a class, for the 10 highest ranked classes,
according to A+C+E-VPCs. From these classes, we
can see two basic patterns:
 verbs that can form aspectual combinations,
with the particle giving a sense of completion
and/or increase/improvement to the action de-
noted by the verb, e.g. verbs of Eating (39.1)
and Splitting (23.2),
 verbs that imply some motion or take a loca-
tion, e.g. verbs of Bring and Take (11.3), Push
and Pull (12) and Putting in spatial configura-
tion (9.2), and can form resultative combina-
tions.
However, apart from class 11.3, where all verbs
form good combinations with all four particles, ac-
cording to the dictionaries, the other classes have a
lower proportion of valid combinations. As these
results may be due to the coverage of the dictionar-
ies, we compared these results with those obtained
by also using BNC-VPCs to test the validity of a
combination. In this case, from the 4,167 entries in
Levin?s classification, 3,925 have verbs that are in
A+C+E+BNC-VPCs, generating 15,700 candidate
VPCs, against which we perform the evaluation. Us-
ing this larger set of VPCs, further combinations are
considered valid: 4,733 VPCs out of 15,700 candi-
dates (30.15%). This time a considerable improve-
ment in the results was verified, with a larger num-
ber of classes having the majority of its VPCs being
considered valid. Figure 4 shows the ten top ranked
classes found with A+C+E+BNC-VPCs. Confirm-
ing the trends suggested with the dictionaries, most
of the top ranked classes have verbs implying some
kind of motion or taking a location (e.g. 11.3 - Bring
and Take- and 53.2 - Rushing) forming resultative
VPCs, or forming aspectual VPCs (e.g. 23.2 - Split).
All of the classes in Figure 4 have 70% or more
of their verbs forming valid combinations, according
to A+C+E+BNC-VPCs. For these classes a man-
ual analysis of the VPCs generated was performed
to test the predicted productivity of the class. All
those combinations that were not attested were sub-
ject to human judgement. Cases of these are:
 catapult down/up - e.g. More victories followed
including a hard-fought points win over Lizo
Matayi which should have catapulted him up
for a national title challenge,
 split/tear in - e.g. The end of the square stick
was then split in for a few inches.
where all examples are from Google. This analy-
sis revealed that all of the candidate VPCs in these
classes are valid, which comes as a confirmation
of the degree of productivity of these high ranked
classes.
The classes that have a degree of productivity of
40% or more form 4,344 candidate VPCs, which
when joined together with the combined resources
obtain a total of 9,919 VPCs. This represents an in-
crease of 20.74% in the coverage of A+C+E+BNC-
VPCs, by making use of productive patterns found
in VPCs.
As each of these particles occurs with a certain
proportion of the verbs in a class, and this propor-
tion varies considerably from class to class, and
from particle to particle, further investigation was
conducted to see the degree of productivity of in-
dividual class-particle pairs. The degree of pro-
ductivity of each class-particle pair is determined
by the proportion of verbs in that class that form
valid combinations with that particle. Moreover, the
larger the number of classes where the majority of
verbs form valid VPCs with that particle, the more
productive the particle is. Table 5 shows for each
particle, the 5 classes that had the higher propor-
tion of valid VPCs with that particle, according to
A+C+E+BNC-VPCs. From these particles, the one
that is involved in the larger number of combinations
throughout more classes is up, which occurs with
40% or more of the verbs in a class for 54.7% of the
Top 10 Classes 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
11
.3 
- B
rin
g/t
ak
e
11
.2 
- S
lid
e
12
 - P
us
h/P
ull
53
.2 
- R
us
hin
g
23
.2 
- S
pli
t
37
.2 
- T
ell
37
.5 
- T
alk
54
.2 
- C
os
t
17
.1 
- T
hro
w
19
 - P
ok
e
Class
P
ro
po
rt
io
n
VPCs
Figure 4: Levin?s VPCs in Dictionaries + BNC
classes, and it is followed closely by out, as shown
in Table 6. Thus up is the best predictor of valid
verb-particle combinations, for most classes. On the
other hand, the weakest predictor of valid combina-
tions is in, which occurs in only a few classes, with
40% or more of the verbs. Class 11.3 is the best
class predictor, allowing all verbs to combine with
all particles.
The classes that have a degree of productivity
of 40% or more with a given particle using this
more specific measure, generate 4,719 VPCs, and
these were used to extend the coverage of these re-
sources obtaining a total of 9,896 VPCs. This rep-
resents an increase of 20.46% in the coverage of
A+C+E+BNC-VPCs, by making use of productive
patterns found in VPCs, and a very restricted set of
particles.
Table 6: Classes with 40% or more of valid VPCs
Particle Classes
up 54.7 %
out 53.7 %
down 46.7 %
in 9.9 %
These results suggest that patterns of productiv-
ity of VPCs can be mapped into Levin?s classes.
Whether choosing the more productive classes over-
all or the more productive class-particle pair the
result is a significant increase in coverage of the
lexical resources, when VPCs are generated from
these classes. More investigation is needed to ver-
ify whether the unattested combinations, specially
in the lower ranked classes are invalid or simply
did not occur in the dictionaries or in the corpus,
because the problem of data sparseness is espe-
cially accute for VPCs. Moreover, it is also nec-
essary to determine the precise semantics of these
VPCs, even though we expect that the more produc-
tive classes generate VPCs compositionally, com-
bining the semantics of the verb and particle to-
gether. Possible alternatives for dealing with this
issue are discussed by both Bannard et al (2003)
and McCarthy et al (2003). Furthermore, although
there are some cases where it appears reason-
able to treat VPCs as fully productive, there are
also cases of semi-productivity (e.g. verbs denot-
ing cooking processes and aspectual up: boil up
and heat up, but not ?saut?e up), as discussed by
Villavicencio and Copestake (2002), so it is impor-
tant to determine which classes are fully productive
and which are not.
6 Discussion
We investigated the identification of regular patterns
among verb-particle constructions using dictionar-
ies, corpora and Levin?s classes. These results sug-
gest that Levin?s classes provide us with productive
patterns of VPCs. Candidate VPCs generated from
these classes can help us improve the coverage of
current lexical resources, as shown in this investi-
gation. We used the available lexical resources and
corpus data to give us an indication of class produc-
tivity, and we used this information to rank these
classes. We took a sample of those classes that were
considered to be good predictors of valid VPCs,
and these were further investigated, through human
judgements, confirming their correspondence with
productive patterns in VPCs. Some of the patterns
can also be applied to other related particles (e.g. the
resultative pattern and locative/directional particles),
but even using a small set of particles it was possi-
ble to considerably extended the coverage of these
lexical resources.
More investigation into the productivity of the
lower ranked classes is needed since the domain be-
ing considered was restricted to the combined re-
sources, and we only considered a candidate VPC
to be valid if it was listed in them. For instance,
in a manual analysis of the combinations involving
the class of Roll verbs (class 51.3.1, bounce, drift,
Table 5: Top 5 Classes for up,down, out and in
Class UP Class DOWN Class OUT Class IN
11.2 100% 10.3 100% 10.3 100% 11.3 100%
11.3 100% 11.2 100% 11.2 100% 11.5 54%
12 100% 11.3 100% 11.3 100% 15.2 50%
19 100% 12 100% 10.9 100% 39.1 50%
37.5 100% 18.4 100% 37.2 100% 35.6 50%
drop, oat, glide, move, roll, slide, swing) most of
the verb-particles generated were considered accept-
able.4 In relation to the A+C+E-VPCs, we found
that 64% of these combinations are not listed. The
use of corpora significantly reduces this problem,
so that when we also consider the BNC-VPCs, the
results are much better, with 80.5% of the combi-
nations being listed. But for some classes, such as
those involving motion, not even the addition of cor-
pus data helps, and a great proportion of the VPCs
are not attested, even though most of the combi-
nations are considered acceptable by native speak-
ers. Thus, a more wide investigation using human
judgement and a larger set of VPCs would be nec-
essary, also using the World Wide Web as corpus.
Nonetheless, these results are encouraging and con-
firm that these classes provide us with good predic-
tors of VPC acceptability. Thus, the use of these
classes to automatically generate verb-particle con-
structions, based on groups of verbs and particles
presents a reasonable way of improving coverage of
existing lexical resources.
Acknowledgments
I?d like to thank Ann Copestake and Francis Bond
for their comments and Timothy Baldwin for all his
help with this work. This research was supported in
part by the NTT/Stanford Research Collaboration,
research project on multiword expressions.
References
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: A case study on verb-
particles. In Proc. of the 6th Conference on Natural
Language Learning (CoNLL-2002), Taipei, Taiwan.
4
*drop up is presumably disallowed because of contradic-
tory directional properties.
Ken Bame. 1999. Aspectual and resultative verb-particle
constructions with Up. Handout for talk presented at
the Ohio State University Linguistics Graduate Stu-
dent Colloquium, May.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proc. of the Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment, Sap-
poro, Japan.
Dwight Bolinger. 1971. The Phrasal Verb in English.
Harvard University Press, Harvard, USA.
John Carroll and Claire Grover. 1989. The derivation of
a large computational lexicon of English from LDOCE.
In B. Boguraev and E. Briscoe, editors, Computa-
tional Lexicography for Natural Language Processing.
Longman.
Ann Copestake and Dan Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Proc. of
the 2nd International Conference on Language Re-
sources and Evaluation (LREC 2000).
Bruce Fraser. 1976. The verb-Particle Combination in
English. Academic Press, New York, USA.
Beth Levin. 1993. English Verb Classes and Alterna-
tions - A Preliminary Investigation. The University of
Chicago Press.
Catherine Macleod and Ralph Grishman. 1998.
Comlex syntax reference manual, Proteus Project.
http://nlp.cs.nyu.edu/comlex.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proc. of the Workshop on Multiword Expres-
sions: Analysis, Acquisition and Treatment, Sapporo,
Japan.
Richard Side. 1990. Phrasal verbs: sorting them out.
ELT Journal, 44(2):144?52.
Aline Villavicencio and Ann Copestake. 2002. Verb-
particle constructions in a computational grammar of
English. In Proc. of HPSG 2002, Seoul, Korea.
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 36?44,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automated Multiword Expression Prediction for Grammar Engineering
Yi Zhang & Valia Kordoni
Dept. of Computational Linguistics
Saarland University
D-66041 Saarbru?cken, Germany
{yzhang,kordoni}@coli.uni-sb.de
Aline Villavicencio & Marco Idiart
Institutes of Informatics & Physics
Federal University of Rio Grande do Sul
Av. Bento Gonc?alves, 9500
Porto Alegre - RS, Brazil
avillavicencio@inf.ufrgs.br
idiart@if.ufrgs.br
Abstract
However large a hand-crafted wide-
coverage grammar is, there are always go-
ing to be words and constructions that
are not included in it and are going to
cause parse failure. Due to their hetero-
geneous and flexible nature, Multiword
Expressions (MWEs) provide an endless
source of parse failures. As the number
of such expressions in a speaker?s lexi-
con is equiparable to the number of single
word units (Jackendoff, 1997), one ma-
jor challenge for robust natural language
processing systems is to be able to deal
with MWEs. In this paper we propose
to semi-automatically detect MWE can-
didates in texts using some error mining
techniques and validating them using a
combination of the World Wide Web as a
corpus and some statistical measures. For
the remaining candidates possible lexico-
syntactic types are predicted, and they are
subsequently added to the grammar as new
lexical entries. This approach provides
a significant increase in the coverage of
these expressions.
1 Introduction
Hand-crafted large-scale grammars like the En-
glish Resource Grammar (Flickinger, 2000), the
Pargram grammars (Butt et al, 1999) and the
Dutch Alpino Grammar (Bouma et al, 2001)
are extremely valuable resources that have been
used in many NLP applications. However, due
to the open-ended and dynamic nature of lan-
guages, and the difficulties of grammar engineer-
ing, such grammars are likely to contain errors
and be incomplete. An error can be roughly clas-
sified as under-generating (if it prevents a gram-
matical sentence to be generated/parsed) or over-
generating (if it allows an ungrammatical sen-
tence to be generated/parsed). In the context of
wide-coverage parsing, we focus on the under-
generating errors which normally lead to parsing
failure.
Traditionally, the errors of the grammar are to
be detected manually by the grammar develop-
ers. This is usually done by running the grammar
over a carefully designed test suite and inspecting
the outputs. This procedure becomes less reliable
as the grammar gets larger, and is especially dif-
ficult when the grammar is developed in a dis-
tributed manner. Baldwin et al (2004), among
many others, for instance, have investigated the
main causes of parse failure, parsing a random
sample of 20,000 strings from the written com-
ponent of the British National Corpus (hencefor-
ward BNC) using the English Resource Gram-
mar (Flickinger, 2000), a broad-coverage preci-
sion HPSG grammar for English. They have found
that the large majority of failures are caused by
missing lexical entries, with 40% of the cases, and
missing constructions, with 39%.
To this effect, as mentioned above, in recent
years, some approaches have been developed in
order to (semi)automatically detect and/or repair
the errors in linguistic grammars. van Noord
(2004), for instance, takes a statistical approach
towards semi-automated error detection using the
parsability metric for word sequences. He reports
on a simple yet practical way of identifying gram-
mar errors. The method is particularly useful for
discovering systematic problems in a large gram-
mar with reasonable coverage. The idea behind it
is that each (under-generating) error in the gram-
36
mar leads to the parsing failure of some specific
grammatical sentences. By running the grammar
over a large corpus, the corpus can be split into
two subsets: the set of sentences covered by the
grammar and the set of sentences that failed to
parse. The errors can be identified by comparing
the statistical difference between these two sets
of sentences. By statistical difference, any kind
of uneven distribution of linguistic phenomena is
meant. In the case of van Noord (2004), the word
sequences are used, mainly because the cost to
compute and count the word sequences is mini-
mum. The parsability of a sequence wi . . . wj is
defined as:
R(wi . . . wj) =
C(wi . . . wj, OK)
C(wi . . . wj)
(1)
where C(wi . . . wj) is the number of sentences
in which the sequence wi . . . wj occurs, and
C(wi . . . wj , OK) is the number of sentences with
a successful parse which contain the sequence.
A frequency cut is used to eliminate the infre-
quent sequences. With suffix arrays and perfect
hashing automata, the parsability of all word se-
quences (with arbitrary length) can be computed
efficiently. The word sequences are then sorted
according to their parsabilities. Those sequences
with the lowest parsabilities are taken as direct in-
dication of grammar errors.
Among them, one common error, and sub-
sequently very common cause of parse failure
is due to Multiword Expressions (MWEs), like
phrasal verbs (break down), collocations (bread
and butter), compound nouns (coffee machine),
determiner-less PPs (in hospital), as well as so-
called ?frozen expressions? (by and large), as dis-
cussed by both Baldwin et al (2004) and van No-
ord (2004). Indicatively, in the experiments re-
ported in Baldwin et al (2004), for instance, from
all the errors due to missing lexical entries, one
fifth were due to missing MWEs (8% of total er-
rors). If an MWE is syntactically marked, the stan-
dard grammatical rules and lexical entries cannot
generate the string, as for instance in the case of
a phrasal verb like take off, even if the individual
words that make up the MWE are contained in the
lexicon.
In this paper we investigate semi-automatic
methods for error mining and detection of miss-
ing lexical entries, following van Noord (2004),
with the subsequent handling of the MWEs among
them. The output of the error mining phase pro-
poses a set of n-grams, which also contain MWEs.
Therefore, the task is to distinguish the MWEs
from the other cases. To do this, first we propose
to use the World Wide Web as a very large corpus
from which we collect evidence that enables us to
rule out noisy cases (due to spelling errors, for in-
stance), following Grefenstette (1999), Keller et
al. (2002), Kilgarriff and Grefenstette (2003) and
Villavicencio (2005). The candidates that are kept
can be semi-automatically included in the gram-
mar, by employing a lexical type predictor, whose
output we use in order to add lexical entries to the
lexicon, with a possible manual check by a gram-
mar writer. This procedure significantly speeds up
the process of grammar development, relieving the
grammar developer of some of the burden by au-
tomatically detecting parse failures and providing
semi-automatic means for handling them.
The paper starts with a discussion of MWEs and
of some of the characteristics that make them so
challenging for NLP, in section 2. This is followed
by a more detailed discussion of the technique
employed for error detection, in section 3. The
approach used for distinguishing noisy sequences
from MWE-related constructions using the World
Wide Web is then presented. How this information
is used for extending the grammar and the results
obtained are then addressed in section 5.
2 Multiword Expressions
The term Multiword Expressions (MWEs) has
been used to describe expressions for which the
syntactic or semantic properties of the whole ex-
pression cannot be derived from its parts ((Sag et
al., 2002), (Villavicencio et al, 2005)), including
a large number of related but distinct phenomena,
such as phrasal verbs (e.g. come along), nomi-
nal compounds (e.g. frying pan), institutionalised
phrases (e.g. bread and butter), and many oth-
ers. They are used frequently in language, and
in English, Jackendoff (1997) estimates the num-
ber of MWES in a speaker?s lexicon to be com-
parable to the number of single words. This is re-
flected in several existing grammars and lexical re-
sources, where almost half of the entries are Mul-
tiword Expressions. However, due to their hetero-
geneous characteristics, MWEs present a tough
challenge for both linguistic and computational
work (Sag et al, 2002). Some MWEs are fixed,
and do not present internal variation, such as ad
37
hoc, while others allow different degrees of inter-
nal variability and modification, such as touch a
nerve (touch/find a nerve) and spill beans (spill
several/musical/mountains of beans). In terms of
semantics, some MWEs are more opaque in their
meaning (e.g. to kick the bucket as to die), while
others have more transparent meanings that can be
inferred from the words in the MWE (e.g. eat up,
where the particle up adds a completive sense to
eat). Therefore, to provide a unified account for
the detection of these distinct but related phenom-
ena is a real challenge for NLP systems.
3 Detection of Errors: Overview
van Noord (2004) reports on various errors that
have been discovered for the Dutch Alpino Gram-
mar (Bouma et al, 2001) semi-automatically, us-
ing the Twente Nieuws Corpus. The idea pur-
sued by van Noord (2004) has been to locate those
n-grams in the input that might be the cause of
parsing failure. By processing a huge amount
of data, the parsability metrics briefly presented
in section 1 have been used to successfully lo-
cate various errors introduced by the tokenizer,
erroneous/incomplete lexical descriptions, frozen
expressions with idiosyncratic syntax, or incom-
plete grammatical descriptions. However, the re-
covery of these errors has been shown to still re-
quire significant efforts from the grammar devel-
oper. Moreover, there is no concrete data given
about the distribution of the different types of er-
rors discovered.
As also mentioned before, among the n-grams
that usually cause parse failures, there is a large
number of missing MWEs in the lexicon such
as phrasal verbs, collocations, compound nouns,
frozen expressions (e.g. by and large, centre of
attention, put forward by, etc).
For the purpose of the detection of MWEs, we
are interested in seeing what the major types of er-
ror for a typical large-scale deep grammar are. In
this context, we have run the error mining experi-
ment reported by van Noord with the English Re-
source Grammar (ERG; (Flickinger, 2000))1 and
the British National Corpus 2.0 (BNC; (Burnard,
2000)).
We have used a subset of the BNC written com-
ponent. The sentences in this collection contain
no more than 20 words and only ASCII characters.
1ERG is a large-scale HPSG grammar for English. In this
paper, we have used the January 2006 release of the grammar.
That is about 1.8M distinct sentences.
These sentences have then be fed into an effi-
cient HPSG parser (PET; (Callmeier, 2000)) with
ERG loaded. The parser has been configured with
a maximum edge number limit of 100K and has
run in the best-only mode so that it does not ex-
haustively find all the possible parses. The result
of each sentence is marked as one of the following
four cases:
? P means at least one parse is found for the
sentence;
? L means the parser halted after the morpho-
logical analysis and has not been able to con-
struct any lexical item for the input token;
? N means the search has finished normally
and there is no parse found for the sentence;
? E means the search has finished abnormally
by exceeding the edge number limit.
It is interesting to notice that when the ambigu-
ity packing mechanism (Oepen and Carroll, 2000)
is used and the unpacking is turned off 2, E does
not occur at all for our test corpus. Running the
parsability checking over the entire collection of
sentences has taken the parser less than 2 days on
a 64bit machine with 3GHz CPU. The results are
shown in Table 1.
Result # Sentences Percentage
P 644,940 35.80%
L 969,452 53.82%
N 186,883 10.38%
Table 1: Distribution of Parsing Results
?From the results shown in Table 1, one can see
that ERG has full lexical span for less than half of
the sentences. For these sentences, about 80% are
successfully parsed. These numbers show that the
grammar coverage has a significant improvement
as compared to results reported by Baldwin et al
(2004) and Zhang and Kordoni (2006), mainly at-
tributed to the increase in the size of the lexicon
and the new rules to handle punctuations and frag-
ments.
Obviously, L indicates the unknown words in
the input sentence. But for N , it is not clear where
2For the experiment of error mining, only the parsability
checking is necessary. There is no need to record the exact
parses.
38
and what kind of error has occurred. In order
to pinpoint the errors, we used the error mining
techniques proposed by van Noord (2004) on the
grammar and corpus. We have taken the sentences
marked as N (because the errors in L sentences
are already determined) and calculate the word se-
quence parsabilities against the sentences marked
as P . The frequency cut is set to be 5. The whole
process has taken no more than 20 minutes, result-
ing in total the parsability scores for 35K n-grams
(word sequences). The distribution of n-grams in
length with parsability below 0.1 is shown in Ta-
ble 2.
Number Percentage
uni-gram 798 20.84%
bi-gram 2,011 52.52%
tri-gram 937 24.47%
Table 2: Distribution of N-gram in Length in Error
Mining Results (R(x) < 0.1)
Although pinpointing the problematic n-grams
still does not tell us what the exact errors are, it
does shed some light on the cause. From Table 2
we see quite a lot of uni-grams with low parsabil-
ities. Table 3 gives some examples of the word
sequences. By intuition, we make the bold as-
sumption that the low parsability of uni-grams is
caused by the missing appropriate lexical entries
for the corresponding word.3
For the bi-grams and tri-grams, we do see a lot
of cases where the error can be repaired by just
adding a multiword lexical entry into the grammar.
N-gram Count
professionals 248
the flat 62
indication of 21
tone of voice 19
as always is 7
Table 3: Some Examples of the N-grams in Error
Mining Results
In order to distinguish those n-grams that can
be added into the grammar as MWE lexical en-
tries from the other cases, we propose to vali-
date them using evidence collected from the World
Wide Web.
3It has later been confirmed with the grammar developer
that almost all of the errors detected by these low parsability
uni-grams can be fixed by adding correct lexical entries.
4 Detection of MWEs and related
constructions
Recently, many researchers have started using the
World Wide Web as an extremely large corpus,
since, as pointed out by Grefenstette (1999), the
Web is the largest data set available for NLP
((Grefenstette, 1999), (Keller et al, 2002), (Kil-
garriff and Grefenstette, 2003) and (Villavicencio,
2005)). For instance, Grefenstette employs the
Web to do example-based machine translation of
compounds from French into English. The method
he employs would suffer considerably from data
sparseness, if it were to rely only on corpus data.
So for compounds that are sparse in the BNC he
also obtains frequencies from the Web. The scale
of the Web can help to minimise the problem of
data sparseness, that is especially acute for MWEs,
and Villavicencio (2005) uses the Web to find ev-
idence to verify automatically generated VPCs.
This work is built on these, in that we propose
to employ the Web as a corpus, using frequencies
collected from the Web to detect MWEs among
the n-grams that cause parse failure. We concen-
trate on the 482 most frequent candidates, to verify
t he method.
The candidate list has been pre-processed to re-
move systematic unrelated entries, like those in-
cluding acronyms, names, dates and numbers, fol-
lowing Bouma and Villada (2002). Using Google
as a search engine, we have looked for evidence
on the Web for each of the candidate MWEs, that
have occurred as an exact match in a webpage. For
each candidate searched, Google has provided us
with a measure of frequency in the form of the
number of pages in which it appears. Table 4
shows the 10 most frequent candidates, and among
these there are parts of formulae, frozen expres-
sions and collocations. Table 5 on the other hand,
shows the 10 least frequent candidates. From the
total of candidates, 311 have been kept while the
other have been discarded as noise.
A manual inspection of the candidates has re-
vealed that indeed the list contains a large amount
of MWEs and frozen expressions like taking into
account the, good and evil, by and large, put for-
ward by and breach of contract. Some of these
cases, like come into effect in, have very spe-
cific subcategorisation requirements, and this is re-
flected by the presence of the prepositions into and
in in the ngram. Other cases seem to be part of
formulae, like but also in, as part of not only X but
39
Table 4: Top 10 Candidate Multiword Expressions
MWE Pages Entropy Prob(%)
the burden of 36600000 0.366 79.4
and cost effective 34400000 0.372 70.7
the likes of 34400000 0.163 93.1
but also in 27100000 0.038 98.9
to bring together 25700000 0.086 96.6
points of view 24500000 0.017 99.6
and the more 23700000 0.512 61.5
with and without 23100000 0.074 97.4
can do for 22300000 0.003 99.9
taking into account the 22100000 0.009 99.6
but what about 21000000 0.045 98.7
the ultimate in 17400000 0.199 90.0
Table 5: Bottom 10 Candidate Multiword Expressions
MWE Pages Entropy Prob (%)
stand by and 1350000 0.399 65.5
discharged from hospital 553000 0.001 99.9
shock of it 92300 0.541 44.6
was woken by 91400 0.001 99.9
telephone rang and 43700 0.026 99.2
glanced across at 36900 0.003 99.9
the citizens charter 22900 0.070 97.9
input is complete 13900 0.086 97.2
from of government 706 0.345 0.1
the to infinitive 561 0.445 1.4
40
also Y, but what about, and the more the (part of
the more the Yer).
However, among the candidates there still re-
main those that are not genuine MWEs, like of al-
cohol and and than that in, which contain very fre-
quent words that enable them to obtain a very high
frequency count without being an MWE. There-
fore, to detect these cases, the remainder of the
candidates could be further analysed using some
statistical techniques to try to distinguish them
from the more likely MWEs among the candi-
dates. This is done by Bouma and Villada (2002)
who investigated some measures that have been
used to identify certain kinds of MWEs, focusing
on collocational prepositional phrases, and on the
tests of mutual information, log likelihood and ?2.
One significant difference here is that this work is
not constrained to a particular type of MWEs, but
has to deal with them in general. Moreover, the
statistical measures used by Bouma and Villada
demand the knowledge of single word frequencies
which can be a problem when using Google espe-
cially for common words like of and a.
In Tables 4 and 5 we present two alternative
measures that combined can help to detect false
candidates. The rational is similar to the statis-
tical tests, without the need of searching for the
frequency of each of the words that make up the
MWE. We assume that if a candidate is just a
result of the random occurrence of very frequent
words most probably the order of the words in the
ngram is not important. Therefore, given a can-
didate, such as the likes of, we measure the fre-
quency of occurrence of all its permutations (e.g.
the of likes, likes the of, etc) and we calculate the
candidate?s entropy as
S = ? 1logN
N
?
k=1
Pi logPi (2)
where Pi is the probability of occurrence of a
given permutation, and N the total number of per-
mutations. The entropy above defined has its max-
imum at S = 1 when all permutations are equally
probably, which indicates a clear signature of a
random nature. On the other hand, when order is
very important and only a single configuration is
allowed the entropy has its minimum, S = 0. An
ngram with low entropy has good chances of being
an MWE. A close inspection on Table 4 shows that
the top two candidate ngrams have relatively high
entropies ( here we consider high entropy when
S > 0.3 ). In the first case this can be explained
by the fact that the word the can appear after the
word of without compromising the MWE mean-
ing as in the burden of the job. In the second case
it shows that the real MWE is cost effective and
the word and can be either in the beginning or in
the end of the trigram. In fact for a trigram with
only two acceptable permutations the entropy is
S = log 2/ log 6 ' 0.39, very close to what is
obtained .
We also show the probability of occurrence
of each candidate ngram among its permutations
(P1). Most of the candidates in the list are more
frequent than their permutations. In Table 4 we
find two exceptions which are clearly spelling er-
rors in the last 2 ngrams. Therefore low P1 can
be a good indicative of a noisy candidate. Another
good predictor is the relative frequency between
the candidates. Given the occurrence values for
the most frequent candidates, we consider that by
using a threshold of 20,000 occurrences, it is pos-
sible to remove the more noisy cases.
We note that the grammar can also impose some
restrictions in the order of the elements in the
ngram, in the sense that some of the generated
permutations are ungrammatical (e.g. the of likes)
and will most probably have null or very low fre-
quencies. Therefore, on top of the constraints on
the lexical order there are also constraints on the
constituent order of a candidate which will be re-
flected in these measures.4
The remainder candidates can be semi-
automatically included in the grammar, by using
a lexical type predictor, as described in the next
section. With this information, each candidate is
added as a lexical entry, with a possible manual
check by a grammar writer prior to inclusion in
the grammar.
4Google ignores punctuation between the elements of the
ngram. This can lead to some hits being returned for some
of the ungrammatical permuted ngrams, such as one one by
in the sentence We?re going to catch people one by one. One
day,... from www.beertravelers.com/lists/drafttech.html. On
the other hand, Google only returns the number of pages
where a given ngram occurred, but not the number of times it
occurred in that page. This can result in a huge underestima-
tion especially for very frequent ngrams and words, which
can be used mo re than once in a given page. Therefore,
a conservative view of these frequencies must be adopted,
given that for some ngrams they might be inflated and for
others deflated.
41
5 Automated Deep Lexical Acquisition
In section (3), we have seen that more than 50%
of the sentences contain one or more unknown
words. And about half of the other parsing failures
are also due to lexicon missing. In this section, we
propose a statistical approach towards lexical type
prediction for unknown words, including multi-
word expressions.
5.1 Atomic Lexical Types
Lexicalist grammars are normally composed of a
limited number of rules and a lexicon with rich
linguistic features attached to each entry. Some
grammar formalisms have a type inheriting system
to encode various constraints, and a flat structure
of the lexicon with each entry mapped onto one
type in the inheritance hierarchy. The following
discussion is based on Head-driven Phrase Struc-
ture Grammar (HPSG) (Pollard and Sag, 1994),
but should be easily adapted to other formalisms,
as well.
The lexicon of HPSG consists of a list of well-
formed Typed Feature Structures (TFSs) (Carpen-
ter, 1992), which convey the constraints on spe-
cific words by two ways: the type compatibility,
and the feature-value consistency. Although it is
possible to use both features and types to con-
vey the constraints on lexical entries, large gram-
mars prefer the use of types in the lexicon because
the inheritance system prevents the redundant def-
inition of feature-values. And the feature-value
constraints in the lexicon can be avoided by ex-
tending the types. Say we have n lexical entries
Li :t
[
F a1
] . . . Ln :t
[
F an
]
. They share the same
lexical type t, but take different values for the fea-
ture F . If a1, . . . , an are the only possible values
for F in the context of type t, we can extend the
type t with subtypes ta1 :t
[
F a1
] . . . tan :t
[
F an
]
and modify the lexical entries to use these new
types, respectively. Based on the fact that large
grammars normally have a very restricted num-
ber of feature-values constraints for each lexical
type, the increase of the types is acceptable. It is
also typical that the types assigned to lexical en-
tries are maximum on the type hierarchy, which
means that they have no further subtypes. We will
call the maximum lexical types after extension the
atomic lexical types. Then the lexicon will be a
multi-valued mapping from the word stems to the
atomic lexical types.
Needless to underline here that all we have
mentioned above is not applicable exclusively to
HPSG, but to many other formalisms based on
TFSs, which makes our assumptions about atomic
lexical types all the more relevant for a wide range
of systems and applications.
5.2 Statistical Lexical Type Predictor
Given that the lexicon of deep grammars can be
modelled by a mapping from word stems to atomic
lexical types, we now go on designing the statisti-
cal methods that can automatically ?guess? such
mappings for unknown words.
Similar to Baldwin (2005), we also treat the
problem as a classification task. But there is an im-
portant difference. While Baldwin (2005) makes
predictions for each unknown word, we create a
new lexical entry for each occurrence of the un-
known word. The assumption behind this is that
there should be exactly one lexical entry that cor-
responds to the occurrence of the word in the given
context5.
We use a single classifier to predict the atomic
lexical type. There are normally hundreds of
atomic lexical types for a large grammar. So the
classification model should be able to handle a
large number of output classes. We choose the
Maximum Entropy-based model because it can
easily handle thousands of features and a large
number of possible outputs. It also has the ad-
vantages of general feature representation and no
independence assumption between features. With
the efficient parameter estimation algorithms dis-
cussed by Malouf (2002), the training of the model
is now very fast.
For our prediction model, the probability of a
lexical type t given an unknown word and its con-
text c is:
p(t|c) = exp(
?
i ?ifi(t, c))
?
t??T exp(
?
i ?ifi(t?, c))
(3)
where feature fi(t, c) may encode arbitrary char-
acteristics of the context. The parameters <
?1, ?2, . . . > can be evaluated by maximising the
pseudo-likelihood on a training corpus (Malouf,
2002). The detailed design and feature selec-
tion for the lexical type predictor are described in
Zhang and Kordoni (2006).
5Lexical ambiguity is not considered here for the un-
knowns. In principle, this constraint can be relaxed by allow-
ing the classifier to return more than one results by, setting a
confidence threshold, for example.
42
In the experiment described here, we have used
the latest version of the Redwoods Treebank in or-
der to train the lexical type predictor with morpho-
logical features and context words/POS tags fea-
tures 6. We have then extracted from the BNC
6248 sentences, which contain at least one of the
311 MWE candidates verified with World Wide
Web in the way described in the previous section.
For each occurrence of the MWE candidates in
this set of sentences, our lexical type predictor has
predicted a lexical entry candidate. This has re-
sulted in 1936 distinct entries. Only those entries
with at least 5 counts have been added into the
grammar. This has resulted in an extra 373 MWE
lexical entries for the grammar.
This addition to the grammar has resulted in a
significant increase in coverage (table 6) of 14.4%.
This result is very promising, as only a subset of
the candidate MWEs has been analysed, and could
result in an even greater increase in coverage, if
these techniques were applied to the complete set
of candidates.
However, we should also point out that the cov-
erage numbers reported in Table 6 are for a set
of ?difficult? sentences which contains a lot of
MWEs. When compared to the numbers reported
in Table 1, the coverage of the parser on this data
set after adding the MWE entries is still signifi-
cantly lower. This indicates that not all the MWEs
can be correctly handled by simply adding more
lexical entries. Further investigation is still re-
quired.
6 Conclusions
One of the important challenges for robust natural
language processing systems is to be able to deal
with the systematic parse failures caused in great
part by Multiword Expressions and related con-
structions. Therefore, in this paper we have pro-
posed an approach for the semi-automatic exten-
sion of grammars by using an error mining tech-
nique for the detection of MWE candidates in texts
and for predicting possible lexico-syntactic types
for them. The approach presented is based on that
of van Noord (2004) and proposes a set of MWE
candidates. For this set of candidates, using the
World Wide Web as a large corpus, frequencies are
gathered for each candidate. These in conjunction
with some statistical measures are employed for
ruling out noisy cases like spelling mistakes (from
6The POS tags are produced with the TnT tagger.
of government) and frequent non-MWE sequences
like input is complete.
With this information the remaining sequences
are analysed by a statistical type predictor that as-
signs the most likely lexical type for each of the
candidates in a given context. By adding these to
the grammar as new lexical entries, a considerable
increase in coverage of 14.4% was obtained.
The approach proposed employs simple and
self-contained techniques that are language-
independent and can help to semi-automatically
extend the coverage of a grammar without rely-
ing on external resources, like electronic dictio-
naries and ontologies that are expensive to obtain
and not available for all languages. Therefore, it
provides an inexpensive and reusable manner of
helping and speeding up the grammar engineer-
ing process, by relieving the grammar developer
of some of the burden of extending the coverage
of the grammar.
As future work we intend to investigate further
statistical measures that can be applied robustly to
different types of MWEs for refining even more
the list of candidates and distinguishing false pos-
itives, like of alcohol and from MWEs, like put
forward by. The high frequency with which the
former occur in corpora and the more accute prob-
lem of data sparseness that affects the latter make
this a difficult task.
References
Timothy Baldwin, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephan Oepen. 2004. Road-testing
the English Resource Grammar over the British Na-
tional Corpus. In Proceedings of the Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004), Lisbon, Portugal.
Timothy Baldwin. 2005. Bootstrapping deep lexical
resources: Resources for courses. In Proceedings
of the ACL-SIGLEX Workshop on Deep Lexical Ac-
quisition, pages 67?76, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Gosse Bouma and Begon?a Villada. 2002. Corpus-
based acquisition of collocational prepositional
phrases. In Proceedings of the Computational Lin-
guistics in the Netherlands (CLIN) 2001, University
of Twente.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage computational anal-
ysis of dutch. In Computational Linguistics in The
Netherlands 2000.
43
Entries Added Item # Covered # Coverage
ERG 0 6246 268 4.3%
ERG+MWE(Web) 373 6246 1168 18.7%
Table 6: Parser coverage on ?difficult? sentences before/after adding MWE lexical entries
Lou Burnard. 2000. User Reference Guide for the
British National Corpus. Technical report, Oxford
University Computing Services.
M. Butt, S. Dipper, A. Frank, and T.H. King. 1999.
Writing large-scale parallel grammars for english,
french, and german. In Proceedings of the LFG99
Conference. CSLI Publications.
Ulrich Callmeier. 2000. PET ? a platform for ex-
perimentation with efficient HPSG processing tech-
niques. Journal of Natural Language Engineering,
6(1):99?108.
Bob Carpenter. 1992. The Logic of Typed Fea-
ture Structures. Cambridge University Press, Cam-
bridge, England.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
Gregory Grefenstette. 1999. The World Wide Web
as a resource for example-based machine transla-
tion tasks. In Proceedings of ASLIB, Conference on
Translating and the Computer, London.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?59.
Frank Keller, Maria Lapata, and Olga Ourioupina.
2002. Using the Web to overcome data sparse-
ness. In Jan Hajic? and Yuji Matsumoto, editors, Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 230?237,
Philadelphia.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on web as corpus.
Computational Linguistics, 29.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Conferencde on Natural Lan-
guage Learning (CoNLL-2002), pages 49?55.
Stephan Oepen and John Carroll. 2000. Ambiguity
packing in constraint-based parsing ? practical re-
sults. In Proceedings of the 1st Conference of the
North American Chapter of the ACL, pages 162?
169, Seattle, WA.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press, Chicago, Illinois.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proceed-
ings of the 3rd International Conference on Intelli-
gent Text Processing and Computational Linguistics
(CICLing-2002), pages 1?15, Mexico City, Mexico.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of
the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
446?453, Barcelona, Spain, July.
Aline Villavicencio, Francis Bond, Anna Korhonen,
and Diana McCarthy. 2005. Introduction to the spe-
cial issue on multiword expressions: having a crack
at a hard nut. Journal of Computer Speech and Lan-
guage Processing, 19.
Aline Villavicencio. 2005. The availability of verb-
particle constructions in lexical resources: How
much is enough? Journal of Computer Speech and
Language Processing, 19.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts processing.
In Proceedings of the Fifth International Confer-
ence on Language Resources and Evaluation (LREC
2006), Genoa, Italy.
44
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1034?1043, Prague, June 2007. c?2007 Association for Computational Linguistics
Validation and Evaluation of Automatically Acquired Multiword
Expressions for Grammar Engineering
Aline Villavicencio??, Valia Kordoni?, Yi Zhang?,
Marco Idiart? and Carlos Ramisch?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?Department of Computer Sciences, Bath University (UK)
?Department of Computational Linguistics, Saarland University, and DFKI GmbH (Germany)
?Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
avillavicencio@inf.ufrgs.br, {yzhang,kordoni}@coli.uni-sb.de
idiart@if.ufrgs.br, ceramisch@inf.ufrgs.br
Abstract
This paper focuses on the evaluation of meth-
ods for the automatic acquisition of Multiword
Expressions (MWEs) for robust grammar engi-
neering. First we investigate the hypothesis that
MWEs can be detected by the distinct statistical
properties of their component words, regardless
of their type, comparing 3 statistical measures:
mutual information (MI), ?2 and permutation
entropy (PE). Our overall conclusion is that at
least two measures, MI and PE, seem to differen-
tiate MWEs from non-MWEs. We then investi-
gate the influence of the size and quality of differ-
ent corpora, using the BNC and the Web search
engines Google and Yahoo. We conclude that, in
terms of language usage, web generated corpora
are fairly similar to more carefully built corpora,
like the BNC, indicating that the lack of con-
trol and balance of these corpora are probably
compensated by their size. Finally, we show a
qualitative evaluation of the results of automat-
ically adding extracted MWEs to existing lin-
guistic resources. We argue that such a process
improves qualitatively, if a more compositional
approach to grammar/lexicon automated exten-
sion is adopted.
1 Introduction
The task of automatically identifying Multiword
Expressions (MWEs) like phrasal verbs (break
down) and compound nouns (coffee machine)
using statistical measures has been the focus
of considerable investigative effort, (e.g. Pearce
(2002), Evert and Krenn (2005) and Zhang et
al. (2006)). Given the heterogeneousness of
the different phenomena that are considered to
be MWEs, there is no consensus about which
method is best suited for which type of MWE,
and if there is a single method that can be suc-
cessfully used for any kind of MWE.
Another difficulty for work on MWE identifi-
cation is that of the evaluation of the results ob-
tained (Pearce, 2002; Evert and Krenn, 2005),
starting from the lack of consensus about a pre-
cise definition for MWEs (Villavicencio et al,
2005).
In this paper we investigate some of the is-
sues involved in the evaluation of automatically
extracted MWEs, from their extraction to their
subsequent use in an NLP task. In order to do
that, we present a discussion of different statisti-
cal measures, and the influence that the size and
quality of different data sources have. We then
perform a comparison of these measures and dis-
cuss whether there is a single measure that has
good overall performance for MWEs in general,
regardless of their type. Finally, we perform a
qualitative evaluation of the results of adding
automatically extracted MWEs to a linguistic
resource, taking as basis for the evaluation the
approach proposed by Zhang et al (2006). We
argue that such results can improve in quality
if a more compositional approach to MWE en-
coding is adopted for the grammar extension.
Having more accurate means of deciding for an
appropriate method for identifying and incor-
porating MWEs is critical for maintaining the
quality of linguistic resources for precise NLP.
This paper starts with a discussion of MWEs
(? 2), of their coverage in linguistic resources
(? 3), and of some methods proposed for auto-
matically identifying them (? 4). This is fol-
lowed by a detailed investigation and compar-
ison of measures for MWE identification (? 5).
1034
After that we present an approach for predicting
appropriate lexico-syntactic categories for their
inclusion in a linguistic resource, and an evalu-
ation of the results in a parsing task(? 7). We
finish with some conclusions and discussion of
future work.
2 Multiword Expressions
The term Multiword Expressions has been used
to describe expressions for which the syntactic or
semantic properties of the whole expression can-
not be derived from its parts (Sag et al, 2002),
including a large number of related but distinct
phenomena, such as phrasal verbs (e.g. come
along), nominal compounds (e.g. frying pan),
institutionalised phrases (e.g. bread and butter),
and many others. Jackendoff (1997) estimates
the number of MWEs in a speaker?s lexicon to
be comparable to the number of single words.
However, due to their heterogeneous character-
istics,MWEs present a tough challenge for both
linguistic and computational work (Sag et al,
2002). For instance, some MWEs are fixed, and
do not present internal variation, such as ad hoc,
while others allow different degrees of internal
variability and modification, such as spill beans
(spill several/musical/mountains of beans).
Sag et al (2002) discuss two main ap-
proaches commonly employed in NLP for treat-
ing MWEs: the words-with-spaces approach
models an MWE as a single lexical entry and it
can adequately capture fixed MWEs like by and
large. A compositional approach treats MWEs
by general and compositional methods of lin-
guistic analysis, being able to capture more syn-
tactically flexible MWEs, like rock boat, which
cannot be satisfactorily captured by a words-
with-spaces approach, since it would require lex-
ical entries to be added for all the possible
variations of an MWE (e.g. rock/rocks/rocking
this/that/his... boat). Therefore, to provide a
unified account for the detection and encoding
of these distinct but related phenomena is a real
challenge for NLP systems.
3 Grammar and Lexicon Coverage in
Deep Processing
Many NLP tasks and applications, like Parsing
and Machine Translation, depend on large-scale
linguistic resources, such as electronic dictionar-
ies and grammars for precise results. Several
substantial resources exist: e.g., hand-crafted
large-scale grammars like the English Resource
Grammar (ERG - Flickinger (2000)) and the
Dutch Alpino Grammar (Bouma et al, 2001).
Unfortunately, the construction of these re-
sources is the manual result of human efforts and
therefore likely to contain errors of omission and
commission (Briscoe and Carroll, 1997). Fur-
thermore, due to the open-ended and dynamic
nature of languages, such linguistic resources are
likely to be incomplete, and manual encoding of
new entries and constructions is labour-intensive
and costly.
Take, for instance, the coverage test results
for the ERG (a broad-coverage precision HPSG
grammar for English) on the British National
Corpus (BNC). Baldwin et al (2004), among
many others, have investigated the main causes
of parse failure, parsing a random sample of
20,000 strings from the written component of
the BNC using the ERG. They have found that
the large majority of failures is caused by miss-
ing lexical entries, with 40% of the cases, and
missing constructions, with 39%, where missing
MWEs accounted for 8% of total errors. That is,
even by a margin, the lexical coverage is lower
than the grammar construction coverage.
This indicates the acute need for robust (semi-
)automated ways of acquiring lexical informa-
tion for MWEs, and this is the one of the goals
of this work. In the next section we discuss
some approaches that have been developed in re-
cent years to (semi-)automatically detect and/or
repair lexical and grammar errors in linguistic
grammars and/or extend their coverage.
4 Acquiring MWEs
The automatic acquisition of specific types of
MWE has attracted much interest (Pearce,
2002; Baldwin and Villavicencio, 2002; Evert
and Krenn, 2005; Villavicencio, 2005; van der
1035
Beek, 2005; Nicholson and Baldwin, 2006). For
instance, Baldwin and Villavicencio (2002) pro-
posed a combination of methods to extract Verb-
Particle Constructions (VPCs) from unanno-
tated corpora, that in an evaluation on the
Wall Street Journal achieved 85.9% precision
and 87.1% recall. Nicholson and Baldwin (2006)
investigated the prediction of the inherent se-
mantic relation of a given compound nominaliza-
tion using as statistical measure the confidence
interval.
On the other hand, Zhang et al (2006) looked
at MWEs in general investigating the semi-
automated detection of MWE candidates in
texts using error mining techniques and vali-
dating them using a combination of the World
Wide Web as a corpus and some statistical mea-
sures. 6248 sentences were then extracted from
the BNC; these contained at least one of the 311
MWE candidates verified with World Wide Web
in the way described in Zhang et al (2006). For
each occurrence of the MWE candidates in this
set of sentences, the lexical type predictor pro-
posed in Zhang and Kordoni (2006) predicted a
lexical entry candidate. This resulted in 373 ad-
ditional MWE lexical entries for the ERG gram-
mar using a words-with-spaces approach. As re-
ported in Zhang et al (2006), this addition to
the grammar resulted in a significant increase in
grammar coverage of 14.4%. However, no fur-
ther evaluation was done of the results of the
measures used on the identification of MWEs or
of the resulting grammar, as not all MWEs can
be correctly handled by the simple words-with-
spaces approach (Sag et al, 2002). And these
are the starting points of the work we are re-
porting on here.
5 Evaluation of the Identification of
MWEs
One way of viewing the MWE identification task
is, given a list of sequences of words, to distin-
guish those that are genuine MWEs (e.g. in the
red), from those that are just sequences of words
that do not form any kind of meaningful unit
(e.g. of alcohol and). In order to do that, one
commonly used approach is to employ statisti-
cal measures (e.g. Pearce (2002) for collocations
and Zhang et al (2006) for MWEs in general).
When dealing with statistical analysis there are
two important statistical questions that should
be addressed: How reliable is the corpus used?
and How precise is the chosen statistical measure
to distinguish the phenomena studied?.
In this section we look at these issues, for the
particular case of trigrams, by testing different
corpora and different statistical measures. For
that we use 1039 trigrams that are the output
of Zhang et al (2006) error mining system, and
frequencies collected from the BNC and from
the World Wide Web. The former were col-
lected from two different portions of the BNC,
namely the fragment of the BNC (BNCf ) used
in the error-mining experiments, and the com-
plete BNC (from the site http://pie.usna.edu/),
to test whether a larger sample of a more ho-
mogeneous and well balanced corpus improves
results significantly. For the latter we used two
different search engines: Google and Yahoo, and
the frequencies collected reflect the number of
pages that had exact matches of the n-grams
searched, using the API tools for each engine.
5.1 Comparing Corpora
A corpus for NLP related work should be a re-
liable sample of the linguistic output of a given
language. For this work in particular, we expect
that the relative ordering in frequency for differ-
ent n-grams is preserved across corpora, in the
same domain (e.g. a corpus of chemistry arti-
cles). For, if this is not the case, different con-
clusions are certain to be drawn from different
corpora.
The first test we performed was a direct com-
parison of the rank plots of the relative fre-
quency of trigrams for the four corpora. We
ranked 1039 MWE-candidate trigrams accord-
ing to their occurrence in each corpus and we
normalised this value by the total number of
times any one of the 1039 trigrams appeared
for each corpus. These normalisation values
were: 66,101 times in BNCf , 322,325 in BNC,
224,479,065 in Google and 6,081,786,313 in Ya-
hoo. It is possible to have an estimate of the size
of each corpus from these numbers: the trigrams
1036
account for something like 0.3% of the BNC cor-
pora, while for Google and Yahoo nothing can
be said since their sizes are not reliable numbers.
Figure 1 displays the results. The overall rank-
ing distribution is very similar for these corpora
showing the expected Zipf like behaviour in spite
of their different sizes.
10-5
10-4
10-3
10-2
10-1
1 10 100 1000
re
la
tiv
e 
fre
qu
en
cy
rank
BNCf
BNC
Google
Yahoo
Figure 1: Relative frequency rank for the 1039
trigrams analysed.
Of course, the information coming from Fig-
ure 1 is not sufficient for our purposes. The or-
der of the trigrams could be very different inside
each corpus. Therefore a second test is needed
to compare the rankings of the n-grams in each
corpus. In order to do that we measure the
Kendall?s ? scores between corpora. Kendall?s ?
is a non-parametric method for estimating cor-
relation between datasets (Press et al, 1992).
For the number of trigrams studied here the
Kendall?s scores obtained imply a significant cor-
relation between the corpora with p<0.000001.
The significance indicates that the data are cor-
related and the null hypothesis of statistical
independence is certainly disproved. Unfortu-
nately disproving the null hypothesis does not
give much information about the degree of cor-
relation; it only asserts that it exists. Thus, it
could be a very insignificant correlation. In ta-
ble 1, we display a more intuitive measure to
estimate the correlation, the probability Q that
any 2 trigrams chosen from two corpora have
the same relative ordering in frequency. This
probability is related to Kendall?s ? through the
expression Q = (1 + ?)/2 .
BNC Google Yahoo
BNCf 0.81 0.73 0.78
BNC 0.73 0.77
Google 0.86
Table 1: The probability Q of 2 trigrams hav-
ing the same frequency rank order for different
corpora.
The results show that the four corpora are
certainly correlated, and can probably be used
interchangeably to access most of the statisti-
cal properties of the trigrams. Interestingly, a
higher correlation was observed between Yahoo
and Google than between BNCf and BNC, even
though BNCf is a fragment of BNC, and there-
fore would be expected to have a very high cor-
relation. This suggests that as corpora sizes
increase, so do the correlations between them,
meaning that they are more likely to agree on
the ranking of a given MWE.
5.2 Comparing statistical measures -
are they equivalent?
Here we concentrate on a single corpus, BNCf ,
and compare the three statistical measures for
MWE identification: Mutual Information (MI),
?2 and Permutation Entropy (PE)(Zhang et al,
2006), to investigate if they order the trigrams
in the same fashion.
MI and ?2 are typical measures of associa-
tion that compare the joint probability of occur-
rence of a certain group of events p(abc) with
a prediction derived from the null hypothesis
of statistical independence between these events
p?(abc) = p(a)p(b)p(c) (Press et al, 1992). In
our case the events are the occurrences of words
in a given position in an n-gram. For a trigram
with words w1w2w3, ?2 is calculated as:
?2 =
?
a,b,c
[ n(abc)? n?(abc) ]2
n?(abc)
where a corresponds either to the word w1 or to
?w1 (all but the word w1) and so on. n(abc)
is the number of trigrams abc in the corpus,
n?(abc) = n(a)n(b)n(c)/N2 is the predicted
number from the null hypothesis, n(a) is the
1037
number of unigrams a, and N the number of
words in the corpus. Mutual Information, in
terms of these numbers, is:
MI =
?
a,b,c
n(abc)
N log2
[ n(abc)
n?(abc)
]
The third measure, permutation entropy, is a
measure of order association. Given the words
w1, w2, and w3, PE is calculated in this work as:
PE = ?
?
(i,j,k)
p(wiwjwk) ln [ p(wiwjwk) ]
where the sum runs over all the permutations
of the indexes and, therefore, over all possible
positions of the selected words in the trigram.
The probabilities are estimated from the number
of occurrences of each permutation of a trigram
(e.g. by and large, large by and, and large by,
and by large, large and by, and by large and) as:
p(w1w2w3) =
n(w1w2w3)
?
(i,j,k)
n(wiwjwk)
PE was proposed by Zhang et al (2006) as a
possible measure to detect MWEs, under the
hypothesis that MWEs are more rigid to per-
mutations and therefore present smaller PEs.
Even though it is quite different from MI and
?2, PE can also be thought as an indirect mea-
sure of statistical independence, since the more
independent the words are the closer PE is from
its maximal value (ln 6, for trigrams). One pos-
sible advantage of this measure over the others
is that it does not rely on single word counts,
which are less accurate in Web based corpora.
Given the rankings produced for each one of
these three measures we again use Kendall?s ?
test to assess correlation and its significance.
Table 2 displays the Q probability of finding
the same ordering in these three measures. The
general conclusion from the table is that even
though there is statistical significance in the cor-
relations found (the p values are not displayed,
but they are very low as before) the differ-
ent measures order the trigrams very differently.
There is a 70% chance of getting the same order
from MI and ?2, but it is safe to say that these
measures are very different from the PE, since
their Q values are very close to pure chance.
MI??2 MI?PE ?2?PE
Q 0.71 0.55 0.45
Table 2: The probability Q of having 2 trigrams
with the same rank order for different statistical
measures.
5.3 Comparing Statistical Measures -
are they useful?
The use of statistical measures is widespread in
NLP but there is no consensus about how good
these measures are for describing natural lan-
guage phenomena. It is not clear what exactly
they capture when analysing the data.
In order to evaluate if they would make good
predictors for MWEs, we compare the measures
distributions for MWEs and non-MWEs. For
that we selected as gold standard a set of around
400 MWE candidates annotated by a native
speaker1 as MWEs or not. We then calculated
the histograms for the values of MI, ?2 and
PE for the two groups. MI and ?2 were cal-
culated only for BNCf . Table 3 displays the re-
sults of the Kolmogorov-Smirnof test (Press et
al., 1992) for these histograms, where the first
value is Kolmogorov-Smirnov D value (D?[0,1]
and large D values indicate large differences be-
tween distributions) and the second is the signif-
icance probability (p) associated to D given the
sizes of the data sets, in this case 90 for MWEs
and 292 for non-MWEs.
MIBNCf ?2BNCf PEY ahoo PEGoogle
D 0.27 0.13 0.27 0.24
p< 0.0001 0.154 0.0001 0.0005
Table 3: Comparison of MI, ?2 and PE
The surprising result is that there is no statis-
tical significance, at least using the Kolmogorov-
Smirnov test, that indicates that being or not
an MWE has some effect in the value of the tri-
gram?s ?2. The same does not happen for MI
or PE. They do seem to differentiate between
MWEs and non-MWEs. As discussed before the
statistical significance implies the existence of an
1The native speaker is a linguist expert in MWEs.
1038
effect but has very little to say about the inten-
sity of the effect. As in the case of this work our
interest is to use the effect to predict MWEs,
the intensity is very important. In the figures
that follow we show the normalised histograms
for MI, ?2(for the BNCf ) and PE (for the case
of Yahoo) for MWEs and non-MWEs. The ideal
scenario would be to have non overlapping dis-
tributions for the two cases, so a simple thresh-
old operation would be enough to distinguish
MWEs. This is not the case in any of the plots.
Starting from Figure 3 it clearly illustrates the
negative result for ?2 in table 3. The other two
distributions show a visible effect in the form of
a slight displacement of the distributions to the
left for MWEs. In particular for the distribution
of PE, the large peak on the right, representing
the n-grams whose word order is irrelevant with
respect to its occurrence, has an important re-
duction for MWEs.
The statistical measures discussed here are
all different forms of measuring correlations be-
tween the component words of MWEs. There-
fore, as some types of MWEs may have stronger
constraints on word order, we believe that more
visible effects can be seen in these measures if we
look at their application for individual types of
MWEs, which is planned for future work. This
will bring an improvement to the power of MWE
prediction of these measures.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
-5.5 -5 -4.5 -4 -3.5 -3 -2.5 -2
Pr
ob
ab
ilit
y
log(MI)
MWEs
non-MWEs
Figure 2: Normalised histograms of MI values
for MWEs and non-MWEs in BNCf .
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 2  3  4  5  6  7  8
Pr
ob
ab
ilit
y
log(?2)
MWEs
non-MWEs
Figure 3: Normalised histograms of ?2 values
for MWEs and non-MWEs in BNCf .
 0
 0.05
 0.1
 0.15
 0.2
 0.25
-3.5 -3 -2.5 -2 -1.5 -1 -0.5  0  0.5
Pr
ob
ab
ilit
y
log(PE(Yahoo))
MWEs
non-MWEs
Figure 4: Normalised histograms of PE values
for MWEs and non-MWEs in Yahoo.
6 Evaluation of the Extensions to
the Grammar
Our ultimate goal is to maximally automate
the process of discovering and handling MWEs.
With good statistical measures, we are able
to distinguish genuine MWE from non-MWEs
among the n-gram candidates. However, from
the perspective of grammar engineering, even
with a good candidate list of MWEs, great ef-
fort is still required in order to incorporate such
word units into a given grammar automatically
and in a precise way.
Zhang et al (2006) tried a simple ?word with
spaces? approach. By acquiring new lexical en-
tries for the MWEs candidates validated by the
statistical measures, the grammar coverage was
shown to improve significantly. However, no fur-
ther investigation on the parser accuracy was re-
ported there.
Taking a closer look at the MWE candidates
1039
proposed, we find that only a small proportion of
them can be handled appropriately by the?word
with spaces? approach of Zhang et al (2006).
Simply adding new lexical entries for all MWEs
can be a workaround for enhancing the parser
coverage, but the quality of the parser output is
clearly linguistically less interesting.
On the other hand, we also find that a large
proportion of MWEs that cannot be correctly
handled by the grammar can be covered prop-
erly in a constructional way by adding one lex-
ical entry for the head (governing) word of the
MWE. For example, the expression foot the bill
will be correctly handled with a standard head-
complement rule, if there is a transitive verb
reading for the word foot in the lexicon. Some
other examples are: to put forward, the good of,
in combination with, . . . , where lexical exten-
sion to the words in bold will allow the gram-
mar to cover these MWEs. In this paper, we
employ a constructional approach for the acqui-
sition of new lexical entries for the head words
of the MWEs.2
It is arguable that such an approach may lead
to some potential grammar overgeneration, as
there is no selectional restriction expressed in
the new lexical entry. However, as far as the
parsing task is concerned, such overgeneration
is not likely to reduce the accuracy of the gram-
mar significantly as we show later in this paper
through a thorough evaluation.
6.1 Experimental Setup
With the complete list of 1039 MWE candidates
discussed in section 5, we rank each n-gram
according to each of the three statistical mea-
sures. The average of all the rankings is used
as the combined measure of the MWE candi-
dates. Since we are only interested in acquiring
new lexical entries for MWEs which are not cov-
ered by the grammar, we used the error mining
results (Zhang et al, 2006; van Noord, 2004)
to only keep those candidates with parsability
? 0.1. The top 30 MWE candidates are used in
2The combination of the ?word with space? approach
of Zhang et al (2006) with the constructional approach
we propose here is an interesting topic that we want to
investigate in future research.
this experiment.
We used simple heuristics in order to extract
the head words from these MWEs:
? the n-grams are POS-tagged with an auto-
matic tagger;
? finite verbs in the n-grams are extracted as
head words;
? nouns are also extracted if there is no verb
in the n-gram.
Occasionally, the tagger errors might introduce
wrong head words. However, the lexical type
predictor of Zhang and Kordoni (2006) that we
used in our experiments did not generate inter-
esting new entries for them in the subsequent
steps, and they were thus discarded, as discussed
below.
With the 30 MWE candidates, we extracted
a sub-corpus from the BNC with 674 sentences
which included at least one of these MWEs. The
lexical acquisition technique described in Zhang
and Kordoni (2006) was used with this sub-
corpus in order to acquire new lexical entries for
the head words. The lexical acquisition model
was trained with the Redwoods treebank (Oepen
et al, 2002), following Zhang et al (2006).
The lexical prediction model predicted for
each occurrence of the head words a most plau-
sible lexical type in that context. Only those
predictions that occurred 5 times or more were
taken into consideration for the generation of the
new lexical entries. As a result, we obtained 21
new lexical entries.
These new lexical entries were later merged
into the ERG lexicon. To evaluate the grammar
performance with and without these new lexical
entries, we
1. parsed the sub-corpus with/without new
lexical entries and compared the grammar
coverage;
2. inspected the parser output manually and
evaluated the grammar accuracy.
In parsing the sub-corpus, we used the PET
parser (Callmeier, 2001). For the manual eval-
1040
uation of the parser output, we used the tree-
banking tools of the [incr tsdb()] system (Oepen,
2001).
6.2 Grammar Performance
Table 4 shows that the grammar coverage im-
proved significantly (from 7.1% to 22.7%) with
the acquired lexical entries for the head words
of the MWEs. This improvement in coverage
is largely comparable to the result reported in
(Zhang et al, 2006), where the coverage was re-
ported to raise from 5% to 18% with the ?word
with spaces? approach (see also section 4).
It is also worth mentioning that Zhang et al
(2006) added 373 new lexical entries for a to-
tal of 311 MWE candidates, with an average
of 1.2 entries per MWE. In our experiment, we
achieved a similar coverage improvement with
only 21 new entries for 30 different MWE candi-
dates, with an average of 0.7 entries per MWE.
This suggests that the lexical entries acquired
in our experiment are of much higher linguistic
generality.
To evaluate the grammar accuracy, we man-
ually checked the parser outputs for the sen-
tences in the sub-corpus which received at least
one analysis from the grammar before and af-
ter the lexical extension. Before the lexical ex-
tension, 48 sentences are parsed, among which
32 (66.7%) sentences contain at least one cor-
rect reading (table 4). After adding the 21 new
lexical entries, 153 sentences are parsed, out of
which 124 (81.0%) sentences contain at least one
correct reading.
Baldwin et al (2004) reported in an earlier
study that for BNC data, about 83% of the sen-
tences covered by the ERG have a correct parse.
In our experiment, we observed a much lower
accuracy on the sub-corpus of BNC which con-
tains a lot of MWEs. However, after the lexical
extension, the accuracy of the grammar recovers
to the normal level.
It is also worth noticing that we did not re-
ceive a larger average number of analyses per
sentence (table 4), as it was largely balanced
by the significant increase of sentences covered
by the new lexical entries. We also found
that the disambiguation model as described by
Toutanova et al (2002) performed reasonably
well, and the best analysis is ranked among top-
5 for 66% of the cases, and top-10 for 75%.
All of these indicate that our approach of lexi-
cal acquisition for head words of MWEs achieves
a significant improvement in grammar coverage
without damaging the grammar accuracy. Op-
tionally, the grammar developers can check the
validity of the lexical entries before they are
added into the lexicon. Nonetheless, even a
semi-automatic procedure like this can largely
reduce the manual work of grammar writers.
7 Conclusions
In this paper we looked at some of the issues
involved in the evaluation of the identification
of MWEs. In particular we evaluated the use
of three statistical measures for automatically
identifying MWEs. The results suggest that at
least two of them (MI and PE) can distinguish
MWEs. In terms of the corpora used, a sur-
prisingly higher level of agreement was found
between different corpora (Google and Yahoo)
than between two fragments of the same one.
This tells us two lessons. First that even though
Google and Yahoo were not carefully built to be
language corpora their sizes compensate for that
making them fairly good samples of language
usage. Second, a fraction of a smaller well bal-
anced corpus may not necessarily be as balanced
as the whole.
Furthermore, we argued that for precise gram-
mar engineering it is important to perform a
careful evaluation of the effects of including au-
tomatically acquired MWEs to a grammar. We
looked at the evaluation of the effects in cover-
age, size of the grammar and accuracy of the
parses after adding the MWE-candidates. We
adopted a compositional approach to the en-
coding of MWEs, using some heuristics to de-
tect the head of an MWE, and this resulted in
a smaller grammar than that by Zhang et al
(2006), still achieving a similar increase in cov-
erage and maintaining a high level of accuracy of
parses, comparable to that reported by Baldwin
et al (2004).
The statistical measures are currently only
1041
item # parsed # avg. analysis # coverage %
ERG 674 48 335.08 7.1%
ERG + MWE 674 153 285.01 22.7%
Table 4: ERG coverage with/without lexical acquisition for the head words of MWEs
used in a preprocessing step to filter the non-
MWEs for the lexical type predictor. Alterna-
tively, the statistical outcomes can be incorpo-
rated more tightly, i.e. to combine with the lex-
ical type predictor and give confidence scores on
the resulting lexical entries. These possibilities
will be explored in future work.
References
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: A case study on verb-
particles. In Proc. of the 6th Conference on Nat-
ural Language Learning (CoNLL-2002), Taipei,
Taiwan.
Timothy Baldwin, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephan Oepen. 2004. Road-testing
the English Resource Grammar over the British
National Corpus. In Proceedings of the Fourth
International Conference on Language Resources
and Evaluation (LREC 2004), Lisbon, Portugal.
Gosse Bouma, Gertjan van Noord, and Robert Mal-
ouf. 2001. Alpino: Wide-coverage computational
analysis of dutch. In Computational Linguistics in
The Netherlands 2000.
Ted Briscoe and John Carroll. 1997. Automatic
extraction of subcategorization from corpora. In
Fifth Conference on Applied Natural Language
Processing, Washington, USA.
Ulrich Callmeier. 2001. Efficient parsing with large-
scale unification grammars. Master?s thesis, Uni-
versita?t des Saarlandes, Saarbru?cken, Germany.
Stefan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of sta-
tistical association measures. Computer Speech
and Language, 19(4):450?466.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?59.
Jeremy Nicholson and Timothy Baldwin. 2006. In-
terpretation of compound nominalisations using
corpus and web statistics. In Proceedings of the
Workshop on Multiword Expressions: Identifying
and Exploiting Underlying Properties, pages 54?
61, Sydney, Australia. Association for Computa-
tional Linguistics.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and
Thorsten Brants. 2002. The LinGO Redwoods
treebank: Motivation and preliminary applica-
tions. In Proceedings of COLING 2002: The 17th
International Conference on Computational Lin-
guistics: Project Notes, Taipei.
Stephan Oepen. 2001. [incr tsdb()] ? competence
and performance laboratory. User manual. Tech-
nical report, Computational Linguistics, Saarland
University, Saarbru?cken, Germany.
Darren Pearce. 2002. A comparative evaluation of
collocation extraction techniques. In Third Inter-
national Conference on Language Resources and
Evaluation, Las Palmas, Canary Islands, Spain.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1992. Numerical
Recipes in C: The Art of Scientific Computing.
Second edition. Cambridge University Press.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-2002), pages 1?15, Mexico City,
Mexico.
Kristina Toutanova, Christoper D. Manning, Stu-
art M. Shieber, Dan Flickinger, and Stephan
Oepen. 2002. Parse ranking for a rich HPSG
grammar. In Proceedings of the First Workshop
on Treebanks and Linguistic Theories (TLT2002),
pages 253?263, Sozopol, Bulgaria.
Leonoor van der Beek. 2005. The extraction of
determinerless pps. In Proceedings of the ACL-
SIGSEM Workshop on The Linguistic Dimensions
of Prepositions and their Use in Computational
Linguistics Formalisms and Applications, Colch-
ester, UK.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of
1042
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages
446?453, Barcelona, Spain, July.
Aline Villavicencio, Francis Bond, Anna Korhonen,
and Diana McCarthy. 2005. Introduction to the
special issue on multiword expressions: having a
crack at a hard nut. Journal of Computer Speech
and Language Processing, 19(4):365?377.
Aline Villavicencio. 2005. The availability of verb-
particle constructions in lexical resources: How
much is enough? Journal of Computer Speech
and Language Processing, 19.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts process-
ing. In Proceedings of the Fifth International
Conference on Language Resources and Evalua-
tion (LREC 2006), Genoa, Italy.
Yi Zhang, Valia Kordoni, Aline Villavicencio, and
Marco Idiart. 2006. Automated multiword ex-
pression prediction for grammar engineering. In
Proceedings of the Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underlying
Properties, pages 36?44, Sydney, Australia. Asso-
ciation for Computational Linguistics.
1043
Prepositions in Applications: A Survey and
Introduction to the Special Issue
Timothy Baldwin
University of Melbourne, Australia
Valia Kordoni
DFKI GmbH and Saarland University,
Germany
Aline Villavicencio
Federal University of Rio Grande do Sul,
Brazil, and University of Bath, UK
1. Introduction
Prepositions1?as well as prepositional phrases (PPs) and markers of various sorts?
have a mixed history in computational linguistics (CL), as well as related fields such as
artificial intelligence, information retrieval (IR), and computational psycholinguistics:
On the one hand they have been championed as being vital to precise language un-
derstanding (e.g., in information extraction), and on the other they have been ignored
on the grounds of being syntactically promiscuous and semantically vacuous, and
relegated to the ignominious rank of ?stop word? (e.g., in text classification and IR).
Although NLP in general has benefitted from advances in those areas where prepo-
sitions have received attention, there are still many issues to be addressed. For example,
in machine translation, generating a preposition (or ?case marker? in languages such
as Japanese) incorrectly in the target language can lead to critical semantic divergences
over the source language string. Equivalently in information retrieval and information
extraction, it would seem desirable to be able to predict that book on NLP and book about
NLPmean largely the same thing, but paranoid about drugs and paranoid on drugs suggest
very different things.
Prepositions are often among the most frequent words in a language. For example,
based on the British National Corpus (BNC; Burnard 2000), four out of the top-ten
most-frequent words in English are prepositions (of, to, in, and for). In terms of both
parsing and generation, therefore, accurate models of preposition usage are essential to
avoid repeatedly making errors. Despite their frequency, however, they are notoriously
difficult to master, even for humans (Chodorow, Tetreault, and Han 2007). For example,
Lindstromberg (2001) estimates that less than 10% of upper-level English as a Second
1 Our discussion will focus primarily on prepositions in English, but many of the comments we make
apply equally to adpositions (prepositions and postpositions) and case markers in various languages. For
definitions and general discussion of prepositions in English and other languages, we refer the reader to
Lindstromberg (1998), Huddleston and Pullum (2002), and Che?liz (2002), inter alia.
Submission received: 9 December 2008; revised submission received: 17 January 2009; accepted for
publication: 17 January 2009.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 2
Language (ESL) students can use and understand prepositions correctly, and Izumi et al
(2003) reported error rates of English preposition usage by Japanese speakers of up
to 10%.
The purpose of this special issue is to showcase recent research on prepositions
across the spectrum of computational linguistics, focusing on computational syntax and
semantics. More importantly, however, we hope to reignite interest in the systematic
treatment of prepositions in applications. To this end, this article is intended to present
a cross-section view of research on prepositions and their use in NLP applications. We
begin by outlining the syntax of prepositions and its relevance to NLP applications,
focusing on PP attachment and prepositions in multiword expressions (Section 2). Next,
we discuss formal and lexical semantic aspects of prepositions, and again their rele-
vance to NLP applications (Section 3), and describe instances of applied research where
prepositions have featured prominently (Section 4). Finally, we outline the contributions
of the papers included in this special issue (Section 5) and conclude with a discussion of
research areas relevant to prepositions which we believe are ripe for further exploration
(Section 6).
2. Syntax
There has been a tendency for prepositions to be largely ignored in the area of syntactic
research as ?an annoying little surface peculiarity? (Jackendoff 1973, page 345). In com-
putational terms, the two most important syntactic considerations with prepositions
are: (1) selection (Fillmore 1968; Bennett 1975; Tseng 2000; Kracht 2003), and (2) valence
(Huddleston and Pullum 2002).
Selection is the property of a preposition being subcategorized/specified by the
governor (usually a verb) as part of its argument structure. An example of a selected
preposition is with in dispense with introductions, where introductions is the object of the
verb dispense, butmust be realized in a prepositional phrase headed bywith (c.f. *dispense
introductions). Conventionally, selected prepositions are specified uniquely (e.g., dispense
with) or as well-defined clusters (e.g., chuckle over/at), and have bleached semantics.
Selected prepositions contrast with unselected prepositions, which do not form part
of the argument structure of a governor and do have semantic import (e.g., live in
Japan). Unsurprisingly, there is not a strict dichotomy between selected and unselected
prepositions (Tseng 2000). For example, in the case of rely on Kim, on is specified in
the argument structure of rely but preserves its directional semantics; similarly, in put it
down , put requires a locative adjunct (c.f. *put it) but is entirely agnostic as to its identity,
and the preposition is semantically transparent.
Preposition selection is important in any NLP application that operates at the
syntax?semantics interface, that is, that overtly translates surface strings onto semantic
representations, or vice versa (e.g., information extraction or machine translation using
some form of interlingua). It forms a core component of subcategorization learning
(Manning 1993; Briscoe and Carroll 1997; Korhonen 2002), and poses considerable chal-
lenges when developing language resources with syntactico-semantic mark-up (Kipper,
Snyder, and Palmer 2004).
Prepositions can occur with either intransitive or transitive valence. Intransitive
prepositions (often referred to as ?particles?) are valence-saturated, and as such do not
take arguments. They occur most commonly as: (a) components of larger multiword
expressions (e.g., verb particle constructions, such as pick it up), (b) copular predicates
(e.g., the doctor is in), or (c) prenominal modifiers (e.g., an off day). Transitive prepositions,
120
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
on the other hand, select for (usually noun phrase) complements to form PPs (e.g., at
home). The bare term ?preposition? traditionally refers to a transitive preposition, but in
this article is used as a catch-all for prepositions of all valences.
Preposition valence has received relatively little direct exposure in the NLP liter-
ature but has been a latent feature of all work on part of speech (POS) tagging and
parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the
Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive
prepositions (RP), and unselected intransitive prepositions (RB, along with a range of
other adverbials).2 There have been only isolated cases of research where a dedicated
approach has been used to distinguish between these three sub-usages, in the interests
of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and
Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005).
Two large areas of research on the syntactic aspects of prepositions are (a) PP-
attachment and (b) prepositions in multiword expressions, which are discussed in the
following sections. Although this article will focus largely on the syntax of preposi-
tions in English, prepositions in other languages naturally have their own challenges.
Notable examples which have been the target of research in computational linguistics
are adpositions in Estonian (Muischnek, Mu?u?risep, and Puolakainen 2005), adpositions
in Finnish (Lestrade 2006), short and long prepositions in Polish (Tseng 2004), and the
French a` and de (Abeille? et al 2003).
2.1 PP Attachment
PP attachment is the task of finding the governor for a given PP. For example, in the
following sentence:
(1) Kim eats pizza with chopsticks
there is syntactic ambiguity, with the PP with chopsticks being governed by either the
noun pizza (i.e., as part of the NP pizza with chopsticks, as indicated in Example (2)), or
the verb eats (i.e., as a modifier of the verb, as indicated in Example (3)).
(2) S
NP
N
Kim
VP
V
eats
NP
N
pizza
PP
P
with
NP
N
chopsticks
2 It also includes the enigmatic TO tag, which is exclusively used for occurrences of to, over all infinitive,
transitive preposition, and intransitive preposition usages.
121
Computational Linguistics Volume 35, Number 2
(3) S
NP
N
Kim
VP
V
eats
NP
N
pizza
PP
P
with
NP
N
chopsticks
Of these, the latter case of verb attachment (i.e., Example (3)) is, of course, the correct
analysis.
Naturally the number of PP contexts with attachment ambiguity is theoretically
unbounded. One special case of note is a sequence of PPs such as Kim eats pizza
with chopsticks on Wednesdays at Papa Gino?s, where the number of discrete analyses for a
sequence of n PPs is defined by the corresponding Catalan number Cn (Church and Patil
1982). The bulk of PP attachment research, however, has focused exclusively on the case
of a single PP occurring immediately after anNP, which in turn is immediately preceded
by a verb. As such, wewill focus our discussion predominantly on this syntactic context.
To simplify discussion, we will refer to the verb as v, the head noun of the immediately
proceeding NP as n1, the preposition as p, and the head noun of the NP object of
the preposition as n2. Returning to our earlier example, the corresponding 4-tuple is
?eatv, pizzan1 ,withp, chopsticksn2?.
The high degree of interest in PP attachment stems from it being a common phe-
nomenon when parsing languages such as English, and hence a major cause of parser
errors (Lin 2003). As such, it has implications for any task requiring full syntactic
analysis or a notion of constituency, such as prosodic phrasing (van Herwijnen et al
2003). Languages other than English with PP attachment ambiguity which have been
the target of research include Dutch (van Herwijnen et al 2003), French (Gaussier and
Cancedda 2001; Gala and Lafourcade 2005), German (Hartrumpf 1999; Volk 2001, 2003;
Foth and Menzel 2006), Spanish (Calvo, Gelbukh, and Kilgarriff 2005), and Swedish
(Kokkinakis 2000; Aasa 2004; Volk 2006).
PP attachment research has undergone a number of significant paradigm shifts over
the course of the last three decades, and been the target of interest of theoretical syntax,
AI, psycholinguistics, statistical NLP, and statistical parsing.
Early research on PP attachment focused on the development of heuristics intended
to model human processing strategies, based on analysis of the competing parse trees
independent of lexical or discourse context (Frazier 1979; Schu?tze 1995). For example,
Minimal Attachment was the strategy of choosing the attachment site which ?mini-
mizes? the parse tree, as calculated by its node membership; assuming the parse trees
provided for Example (1), this would be unable to disambiguate between Examples (2)
and (3) as they both contain the same number of nodes. Late Attachment, on the other
hand, was the strategy of attaching ?low? in the parse tree, corresponding to Exam-
ple (2). Ford, Bresnan, and Kaplan (1982) proposed an alternative heuristic strategy,
based on the existence of p in a subcategorization frame for v. In later research, Pereira
(1985) described amethod for incorporating Right Association andMinimal Attachment
122
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
into a shift-reduce parser, and Whittemore and Ferrara (1990) developed a rule-based
algorithm to combine various attachment preferences on the basis of empirical evalua-
tion of the predictive power of each.
Syntactic preferences were of course a blunt instrument in dealing with PP attach-
ment, and largely ineffectual in predicting the difference in PP attachment between
Kim eats pizza with chopsticks (verb attachment) and Kim eats pizza with anchovies (noun
attachment), for example. This led to a shift away from syntactic methods in the 1980s
towards AI-inspired techniques which used world knowledge to resolve PP attachment
ambiguity. In the case of Example (1), for example, the knowledge that chopsticks are
an eating implement would suggest a preference for verb attachment, and similarly the
knowledge that they are not a foodstuff would suggest a dispreference for noun attach-
ment. Wilks, Huang, and Fass (1985) attempted to capture this type of world knowledge
using hand-coded ?preferential semantics? of each of v, n1, p, and n2. Dahlgren and
McDowell (1986) simplified this analysis to use only p and n2, still in the form of
hand-coded rules. Hirst (1987) used his model of ?commonsense semantics? to resolve
PP attachment ambiguity, based on verb-guided preferences (i.e., valence properties
of the verb) and plausibility relative to a knowledge base. In this same vein, Jensen
and Binot (1987) used the dictionary definitions of v and n2 as a ?knowledge base? to
resolve PP attachment based on approximate reasoning, for example, by determining
that chopsticks is an instrument which is compatible with eat.
In parallel, in the area of psycholinguistics, Altmann and Steedman (1988) provided
experimental evidence to suggest that PP attachment resolution interacts dynamically
with discourse context, and that ambiguity is resolved on the basis of the interdepen-
dence between structure and the mental representation of that context. This suggested
that computational research on PP attachment resolution needed to take into account
the discourse context, in addition to syntax and world knowledge. Spivey-Knowlton
and Sedivy (1995) later used psycholinguistic experiments to demonstrate that verb-
specific attachment properties and NP definiteness both play important roles in the
human processing of PP attachment. Importantly, this workmotivated the need for verb
classes to resolve PP attachment ambiguity.
The next significant shift in NLP research on PP attachment was brought about by
Hindle and Rooth (1993), who were the harbingers of statistical NLP and large-scale
empirical evaluation. Hindle and Rooth challenged the prevailing view at the time that
lexical semantics and/or discourse modeling were needed to resolve PP attachment, in
proposing a distributional approach, based simply on estimation of the probability of
p attaching high or low given v and n1 (ignoring n2). The method uses unambiguous
cases of PP attachment (e.g., cases of n1 being a pronoun [high attachment], or the PP
post-modifying n1 in subject position [low attachment]) to derive smoothed estimates
of Prhigh(p|v), Prhigh(NULL|n) (i.e., the probability of n not being post-modified by
a PP), and Prlow(p|n), which then form the basis of Prhigh(p|v, n) and Prlow(p|v, n),
respectively. The proposedmethodwas significant in demonstrating the effectiveness of
simple co-occurrence probabilities, without explicit semantics or discourse processing,
and also in its ability to operate without explicitly annotated training data.
Resnik and Hearst (1993) observed that PP attachment preferences are also condi-
tioned on the semantics of the noun object of the preposition in the PP, as can be seen
in our earlier example of Kim eats pizza with chopsticks/anchovies where chopsticks leads
to verb attachment and anchovies to noun attachment. Although they were unable to
come up with a model which was empirically superior to existing methods which did
not represent the semantics of the noun object, this paved the way for a new wave of
research using the full 4-tuple of ?v, n1, p, n2?.
123
Computational Linguistics Volume 35, Number 2
The first to successfully apply the full 4-tuple were Ratnaparkhi, Reynar, and
Roukos (1994), who in the process established a benchmark PP attachment data set
upon which most of the subsequent research has been based. The data set, colloquially
known as the ?RRR? data set, was automatically extracted from the Wall Street Journal
section of the Penn Treebank, and is made up of 20,801 training and 3,097 test 4-tuples
of type ?v, n1, p, n2?, each of which is annotated with a binary label for verb or noun
attachment.3 Also of significance was the fact that Ratnaparkhi, Reynar, and Roukos
were able to come up with a ?class? representation for words, based on distributional
similarity, that outperformed a simple word-based model.
The general framework established by Ratnaparkhi, Reynar, and Roukos (1994),
and the RRR data set, defined the task of PP attachment for over a decade. It has been
tackled using a variety of smoothing methods andmachine learning algorithms, includ-
ing backed-off estimation (Collins and Brooks 1995), instance-based learning (Zavrel,
Daelemans, and Veenstra 1997; Zhao and Lin 2004), log-linear models (Franz 1996),
maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees
(Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, andMoliner
1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer
1999). In addition to the four lexical and class-based features provided by the 4-tuple
?v, n1, p, n2?, researchers have used noun definiteness, distributional similarity, noun
number, subcategorization categories, word proximity in corpus data, and PP semantic
class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The
empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who
used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics.
In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that
Japanese translations of English verb and noun attachment involve distinct construc-
tions. This allowed them to automatically identify instances of PP attachment ambiguity
in a parallel English?Japanese corpus, complete with attachment information. They
used this data to disambiguate PP attachment in English inputs, and demonstrated that,
the quality of English?Japanese machine translation improved significantly as a result.
More recently, Atterer and Schu?tze (2007) challenged the real-world utility of meth-
ods based on the RRR data set, on the grounds that it is based on the availability of a
gold-standard parse tree for a given input. They proposed that, instead, PP attachment
be evaluated as a means of post-processing over the raw output of an actual parser,
and produced results to indicate: (a) that a state-of-the-art parser (Bikel 2004) does
remarkably well at PP attachment without a dedicated PP attachment module; but
also (b) that post-processing based on a range of methods developed over the RRR
data set (Collins and Brooks 1995; Toutanova, Manning, and Ng 2004; Olteanu and
Moldovan 2005) generally improves parser accuracy. In addition, they developed a
variant of the RRR data set (RRR-sent) which contains full sentential contexts of possible
PP attachment ambiguity. Others who have successfully built PP re-attachment models
for specific parsers are Olteanu (2004) and Foth and Menzel (2006). Agirre, Baldwin,
and Martinez (2008) used the evaluation methodology of Atterer and Schu?tze (2007) to
confirm the finding from the original RRR data set that lexical semantics (in various
guises) can enhance PP attachment accuracy relative to a baseline parser. As part of this
effort, they developed a standardized data set for exploration of the interaction between
lexical semantics and parsing/PP attachment accuracy.
3 Because it was automatically extracted, the RRR data set is notoriously noisy. For instance, Pantel and
Lin (2000) observed that 133 tuples contain the as either n1 or n2.
124
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
One significant variation on the classic binary PP attachment task which attempts
to generate a richer semantic characterisation of the PP is the work of Merlo (2003) and
Merlo and Esteve Ferrer (2006), who included classification of the PP as an argument
or adjunct, making for a four-way classification task. In this context, they found that
PP attachment resolution for argument PPs is considerably easier than is the case for
adjunct PPs.
Returning to our observation that PP attachment can occur in multiple syntactic
configurations, Merlo, Crocker, and Berthouzoz (1997) applied backed-off estimation
to the problem of multiple PP attachment, in the form of 14 discrete syntactic config-
urations. Unsurprisingly, they found the task considerably harder than the basic V NP
PP case, due to increased ambiguity and data sparseness. Mitchell (2004) similarly per-
formed an extensive analysis of the Penn Treebank to investigate the different contexts
PP attachment ambiguities occur in, and the relative ability of different PP attachment
methods to disambiguate each.
There have also been domain-specific methods proposed for PP attachment, for
example, in the area of biomedicine (Hahn, Romacker, and Schulz 2002; Pustejovsky
et al 2002; Leroy, Chen, and Martinez 2003; Schuman and Bergler 2006).
2.2 The Syntax of Prepositional Multiword Expressions
Prepositions are also often found as part of multiword expressions (MWEs), such as
verb-particle constructions (break down), prepositional verbs (rely on), determinerless
PPs (in hospital), complex prepositions (by means of ) and compound nominals (affairs
of state). MWEs are lexical items which are composed of more than one word and are
lexically, syntactically, semantically, pragmatically, and/or statistically idiosyncratic in
some way (Sag et al 2002). In this section, we present a brief overview of the syntax
of the key prepositional MWEs in English, and discuss a cross-section of some of the
recent research done in the area.
Prepositional MWEs span the full spectrum of morphosyntactic variation. Some
complex prepositions undergo no inflection, internal modification, or word order vari-
ation (e.g., in addition to) and are best analyzed as ?words with spaces? (Sag et al
2002). Others optionally allow internal modification (e.g., with [due/particular/special/...]
regard to) or determiner insertion (e.g., on [the] top of ) and are considered to be semi-
fixed expressions (Villada Moiro?n 2005). Compound nominals are similarly semi-fixed
expressions, in that they have rigid constraints on word order and lexical composition,
but allow morphological inflection (e.g., part(s) of speech).
The three prepositional MWE types that cause the greatest syntactic problems in
English, in terms of their relative frequency and tendency for syntactic variation, are:
1. verb-particle constructions (VPCs), where the verb selects for an
intransitive preposition (e.g., chicken out or hand in: Dehe? et al [2002]);
2. prepositional verbs (PVs), where the verb selects for a transitive
preposition (e.g., rely on or refer to: Huddleston and Pullum [2002]);
3. determinerless PPs (PP?Ds), where a PP is made up of a preposition and
singular noun without a determiner (e.g., at school, off screen: Baldwin et al
[2006]).
All three MWE types undergo limited syntactic variation (Sag et al 2002). For exam-
ple, transitive verb particle constructions generally undergo the particle alternation,
125
Computational Linguistics Volume 35, Number 2
whereby the particle may occur either adjacent to the verb (e.g., tear up the letter), or
be separated from the verb by the NP complement (e.g., tear the letter up). Some VPCs
readily occur with both orders (like tear up), while others have a strong preference for
a particular order (e.g., take off?under the interpretation of having the day off?tends
to occur in the particle-final configuration in usages such as take Friday off vs. ?take off
Friday).4 In addition, many VPCs undergo limited internal modification by adverbials,
where the adverb pre-modifies the particle (e.g., come straight over).
The syntactic variability of prepositional verbs is more subtle. PVs occur in two
basic forms: (1) fixed preposition PVs (e.g., come across), where the verb and selected
preposition must be strictly adjacent; and (2) mobile preposition PVs (e.g., refer to),
where the selected preposition is adjacent to the verb in the canonical word order, but
undergoes limited syntactic alternation. For example, mobile preposition PVs allow
limited coordination of PP objects (e.g., refer to the book and to the DVD), and the NP
object of the selected preposition can be passivized (e.g., the book they referred to).
Even subtler are the syntactic effects observed for determinerless PPs. The singular
noun in the PP?D is often strictly countable (e.g., off screen, on break), resulting in
syntactic markedness as, without a determiner, the noun does not constitute a saturated
NP. This in turn dictates the need for a dedicated analysis in a linguistically motivated
grammar in order to be able to avoid parse failures (Baldwin et al 2004; van der Beek
2005). Additionally, there is considerable variation in the internal modifiability of deter-
minerless PPs, with some not permitting any internal modification (e.g., of course) and
others allowing optional internal modification (e.g., at considerable length). There are also,
however, cases of obligatory internal modification (e.g., at considerable/great expense vs.
*at expense) and highly restricted internal modification (e.g., at long last vs. *at great/short
last). Balancing up these different possibilities in terms of over- and undergeneration in
a grammar is far from trivial (Baldwin et al 2006).
Naturally there are other prepositional MWE types in languages other than English
with their own syntactic complexities. Notable examples to have received attention in
the computational linguistics literature are Dutch ?collocational prepositional phrases?
(VilladaMoiro?n 2005), German complex prepositions (Trawin?ski 2003; Trawinski, Sailer,
and Soehn 2006), German particle verbs (Schulte im Walde 2004; Rehbein and van
Genabith 2006), and Polish preposition?pronoun contractions (Trawin?ski 2005, 2006).
2.2.1 Prepositional MWEs in NLP. The syntactic variation of prepositional MWEs leads
to difficulties for NLP applications. To start with, there is the problem of identifying
their token occurrences, for example, for semantic indexing purposes. As with simplex
words, a given MWE may appear with different subcategorization frames (e.g., give up
vs. give up [something]), but added to that, the order of the elements may be flexible
or internally modified (see previous discussion), and some elements may be optional
(e.g., out in make a big thing (out) of [something]). Additionally, they conspire with PP
attachment ambiguity to compound structural ambiguity. For example, hand the paper in
today is ambiguous between a V NP PP analysis ([V hand] [NP the paper] [PP in today]), a
V NP analysis ([V hand] [NP the paper in today]), and a transitive VPC analysis ([V hand]
[NP the paper] [P in] [NP today]); of these, the final analysis is, of course, correct.
4 The question of whether or not a particle can be separated from the verb depends on factors such as the
degree of bonding of the particle with the verb, the size of the NP, and the type of the NP, as discussed by
Wasow (2002).
126
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Related to this is the issue of resource development, especially lexicon development
for linguistically motivated parsers. First, a representation must be arrived at which is
sufficiently expressive to encode the syntactic subtleties of eachMWE instance (Sag et al
2002; Calzolari et al 2002; Copestake et al 2002; Odijk 2004). Second, the lexiconmust be
populated in order to ensure adequate parser coverage over prepositional MWEs. Due
to the high productivity and domain specificity of prepositional MWEs, a number of
lexical acquisition approaches have been developed, usually customized to a particular
prepositional MWE type in a specific language.
For VPCs in English, for instance, Baldwin and Villavicencio (2002) focused on
their automatic extraction from raw text corpora, on the basis of a POS tagger, chunker,
and chunk grammar, and finally the combined output of the three along with various
linguistic and frequency features. Baldwin (2005a) expanded on this work to extract
VPCs complete with valence information, to produce a fully specified lexical item. In
both cases, it was found that 100% recall was extremely hard to achieve due to the
power-law distribution of VPC token frequencies in corpus data. That is, around half
of the VPC lexical items found in pre-existing lexical resources occurred in the BNC at
most 2 times.
Villavicencio (2005, 2006) took a different approach, in observing that VPCs occur
productively in semantically coherent clusters, based on (near-)synonymy of the head
verb (e.g., clear/clean/drain up and break/rip/cut/tear up). As a result, she expanded a
seed set of VPCs based on class-based verb semantic information, and used Web-based
statistics to filter false positives out of the resultant VPC candidate set.
Li et al (2003) looked at the task of VPC identification rather than extraction,
namely, identifying each individual VPC token instance in corpus data, based on a
set of hand-crafted regular expressions. Although they report very high precision and
recall for their method, it has the obvious disadvantage that the regular expressions
must be manually encoded, and it is not clear that the proposed method is superior
to an off-the-shelf parser. Kim and Baldwin (2006, in press) attempted to automate
the process of identification by post-processing the output of the RASP parser, and
demonstrated (a) that the RASP parser (Briscoe, Carroll, and Watson 2006) is highly
effective at VPC identification, and (b) that the incorporation of lexicalized models of
selectional preferences can lead to modest improvements in parser accuracy.
In terms of English PV extraction, Baldwin (2005b) proposed a method based on a
combination of statistical measures and linguistic diagnostics, and demonstrated that
the combination of statistics with linguistic diagnostics achieved the best extraction
performance.
Research on prepositional MWEs in languages other than English includes Krenn
and Evert (2001) and Evert and Krenn (2005) on the extraction of German PP?verb
collocations (which are similar to verbal idioms/verb?noun combinations in English
[Fazly, Cook, and Stevenson 2009]) based on a range of lexical association measures.
Pecina (2008) further extended this work using a much broader set of lexical association
measures and classifier combination. Looking at German, Do?mges et al (2007) analyzed
the productivity of PP?Ds headed by unter, and used their results to motivate a syntactic
analysis of the phenomenon. For Dutch, van der Beek (2005) worked on the extraction
of PP?Ds from the output of a parser, once again using a range of statistical measures.
Sharoff (2004) described a semi-automatic approach to classifying prepositional MWEs
in Russian, based on statistical measures and manual filtering using knowledge about
the structure of Russian prepositional phrases.
A recent development which we expect will further catalyze research on preposi-
tional (and general) MWE extraction was the release of a number of standardized data
127
Computational Linguistics Volume 35, Number 2
sets for MWE extraction evaluation as part of an LREC 2008 Workshop. This includes
data sets for English VPCs (Baldwin 2008) and German PP?verbs (Krenn 2008).
3. Semantics
There are three diametrically opposed views to the semantics of prepositions: (1) prepo-
sitions are semantically vacuous and unworthy of semantic representation (a view
commonly subscribed to in the information retrieval community: Baeza-Yates and
Ribeiro-Neto [1999], Manning, Raghavan, and Schu?tze [2008]); (2) preposition semantics
is a function of the words that select them and they in turn select, such that it is impos-
sible to devise a standalone semantic characterization of preposition semantics (Tseng
2000; Old 2003); and (3) prepositional semantics is complex but can be captured in a
standalone resource (Kipper, Dang, and Palmer 2000; Saint-Dizier and Vazquez 2001;
Litkowski and Hargraves 2005). Kipper, Snyder, and Palmer (2004, page 23) elegantly
summarized this third position as ?it is precisely because the preposition?semantics
relationship is so complex that properly accounting for it will lead to a more robust
natural language resource.? Turning the clock back 16 years, Zelinski-Wibbelt (1993,
page 1) observed that ?we are now witnessing a veritable plethora of investigations
into the semantics of preposition semantics? and speculated that ?the time has come to
see how natural language processing (NLP) can benefit from the insights of theoretical
linguistics.? Although these prognostications were perhaps overly optimistic at the
time, they are now being progressively fulfilled.
In discussing preposition semantics, there is a basic distinction between composi-
tional (or regular/productive) and non-compositional (or irregular/collocational) se-
mantics. Preposition usages with compositional semantics transparently preserve the
standalone semantics of the preposition (e.g., They met on Friday), whereas those with
non-compositional semantics involve some level of semantic specialization or diver-
gence from the standalone semantics (e.g., They got on famously). In terms of formal
semantics, the complement vs. adjunct distinction5 is also relevant for determining the
logical form for a given input.
Subsequently, we review research on the formal and lexical semantics of preposi-
tions, and the semantics of prepositional MWEs.
3.1 Formal Semantic Approaches to Prepositions
Research on the formal semantics of prepositions has focused predominantly on devis-
ing representations for temporal, spatial, and locative usages, the three most productive
and coherent classes of prepositions.
Prepositions can be used in order to convey temporal information relevant to the
duration of a proposition. Normally, three types of information are identified: (1) the
duration of the preposition, (2) its duration relative to the time of reference of the dis-
course, and (3) its absolute location on the time axis. Some prepositions convey all three
types of information, and others convey only one. Within each information type, the
5 In talking about the complement/adjunct distinction, we put aside the well-known issue of borderline
cases (Rauh 1993), and ignore language-specific phenomena such as Funktionsverbgefu?ge, where
German PPs can also occur as invariant syntagmas in light verb constructions (e.g., in Beschlag nehmen ?to
occupy?) and the complement?adjunct distinction does not apply.
128
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
semantic input of temporal prepositions can vary considerably: They can distinguish
between existential and universal quantification over time, indicate whether or not the
extremes of the period are to be included, and mark the motion through time.
Themain challenge for formal approaches to the semantics of temporal prepositions
is coming upwith a representation which can adequately encode the different semantics
of temporal prepositions (Bennett 1975; Miller and Johnson-Laird 1976; Ro?hrer 1977;
Kamp 1981; Allen 1984; Richards et al 1989; Bre?e and Smit 1986; Durrell and Bre?e 1993).
Another concern has been the ability to support compositional semantic interpretation
of temporal PPs, with emphasis on their quantificational role. For English, Pratt and
Francez (1997), for instance, proposed generalized temporal quantifiers to represent
temporal NPs, temporal PPs, and sentences.
With respect to spatial semantics, the focus of research has been on devising formal
semantic representations that capture both the functional relationships between the
objects, and human interaction with those objects. Along these lines, Kelleher and
Costello (2009) propose computational models of spatial preposition semantics for use
in visually situated dialogue systems.
Finally, with locative prepositions, formal semantic approaches have proposed that
they should decompose into three semantic elements: a locative relation, a reference
entity, and a place value. Each locative expression (e.g., a PP orNP) is generally assumed
to have a unique locative relation and place value. Thus, combinations of pure locative
relationmarkers and prepositions which are specified for place values are ruled out, and
no embedded locative relations are allowed in the semantic structure of a single locative
expression. Some locative prepositions are underspecified for a place value, however,
and thus are able to co-occur with a second preposition that specifies a place value
(e.g., out from under the table). The specific locative relation denoted by a preposition
is generally considered not to be specified in the lexicon, but instead to vary with
eventuality types.
J?rgensen and L?nning (2009) used the Minimal Recursion Semantics (MRS)
framework (Copestake et al 2005) as the basis for a language-independent represen-
tation of semantics of locative prepositions. They applied the proposed representa-
tion to English and Norwegian locative prepositions, and demonstrated its utility for
Norwegian?English machine translation. Hellan and Beermann (2005) similarly used
the MRS framework to capture the locative and spatial semantics of Norwegian prepo-
sitions. Ramsay (2005) proposed a unified theory of preposition semantics, where the
semantics of temporal usages of prepositions are predicted naturally from abstract
relational definitions. Arsenijevic? (2005) developed a formal semantic analysis of prepo-
sitions in terms of event structure and applied it to a natural language generation
task.
Denis, Kuhn, and Wechsler (2003) analyzed the syntactico-semantics of V-PP goal
motion complexes in English (e.g., Kim ran to the library), and developed an analysis
based on the conclusion that the preposition is often semantically rather than syn-
tactically determined. This lends support to arguments for a systematic account of
preposition semantics.
Kordoni (2003b, 2003a, 2006) focused instead on the role of prepositions in diathesis
alternations, and proposed an MRS analysis of indirect prepositional arguments based
on English, German, and Modern Greek data. Specifically, she showed that a robust
formal semantic framework like MRS provides an appropriate theoretical basis for
a linguistically motivated account of indirect prepositional arguments, and also the
necessary formal generalizations for the analysis of such arguments in a multilingual
context (as a result of MRS structures being easily comparable across languages).
129
Computational Linguistics Volume 35, Number 2
3.2 Lexical Semantic Resources for Prepositions
A number of lexical semantic resources have been developed specifically for prepo-
sitions, four of which are outlined here: (1) the English LCS Lexicon, (2) the Preposition
Project, (3) PrepNet, and (4) VerbNet.
The English LCS Lexicon (Dorr 1993, 1997) uses the formalism of Lexical Conceptual
Structure (based on work by Jackendoff [1983, 1990]6) to encode lexical knowledge
using a typed directed graph of semantic primitives and fields. In addition to a large
lexicon of verbs, it includes 165 English prepositions classified into 122 intransitive and
375 transitive senses. For example, the LCS representation for the directional sense of
up (as in up the stairs) is:
(4) (toward Loc (nil 2) (UP Loc (nil 2) (? Thing 6)))
where the numbers indicate the logical arguments of the predicates. This representation
indicates that the logical subject of the PP (indexed by ?2?; e.g., the piano in move the
piano up the stairs) is relocated up in the direction of the logical argument (indexed by
?6?; e.g., the stairs in our example), which is in turn a concrete thing. The LCS Lexicon
was developed from a theoretical point of view and isn?t directly tied to corpus usage.
The Preposition Project (Litkowski 2002; Litkowski and Hargraves 2005, 2006) is
an attempt to develop a comprehensive semantic database for English prepositions,
intended for NLP applications. The project took the New Oxford Dictionary of English
(Pearsall 1998) as its source of preposition sense definitions, which it then fine-tuned
based on cross-comparison with both functionally tagged prepositions in FrameNet
(Baker, Fillmore, and Lowe 1998) and the account of preposition semantics in a de-
scriptive grammar of English (Quirk et al 1985); it also draws partially on Dorr?s
LCS definitions of prepositions. Importantly, the Preposition Project is building up a
significant number of tagged preposition instances through analysis of the preposition
data in FrameNet, which it then uses to characterize the noun selectional preferences of
each preposition sense and also the attachment properties to verbs of different types. At
the time of writing, 673 preposition senses for 334 prepositions (mostly phrasal prepo-
sitions) have been annotated. In tandemwith developing type-level sense definitions of
prepositions, the Preposition Project has sense annotated over 27,000 occurrences of the
56 most common prepositions, based on functionally tagged prepositions in FrameNet.
To date, the primary application of the Preposition Project has been in a SemEval 2007
task on the word sense disambiguation of prepositions (see Section 3.3).
PrepNet (Saint-Dizier and Vazquez 2001; Cannesson and Saint-Dizier 2002; Saint-
Dizier 2005, 2008) is an attempt to develop a compositional account of preposition
semantics which interfaces with the semantics of the predicate (e.g., verb or predicative
noun). Similarly to the English LCS Lexicon, it uses LCS as the descriptive language,
in conjunction with typed ?-calculus and underspecified representations. Noteworthy
elements of PrepNet are that it attempts to capture selectional constraints, metaphor-
ical sense extension, and complex arguments. PrepNet was originally developed over
French prepositions, but has since been applied to the analysis of instrumentals across
a range of languages (Saint-Dizier 2006b).
VerbNet (Kipper, Dang, and Palmer 2000; Kipper Schuler 2005) contains a shallow
hierarchy of 50 spatial prepositions, classified into five categories. The hierarchy is
6 See Levin and Rappaport-Hovav (in press) for a recent review of this style of semantics.
130
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
derived from pioneering work by Spa?rck Jones and Boguraev (1987), which is in turn
derived from Wood (1979). The preposition sense inventory was used as the basis for
extending VerbNet and led to significant redevelopment of the verb class set (Kipper,
Snyder, and Palmer 2004), in a poignant illustration of how preposition semantics
impinges on verb semantics.
In other work, Sablayrolles (1995) classified 199 simple and complex spatial prepo-
sitions into 16 classes. Lersundi and Agirre (2003) applied a similar methodology to
Dorr and Habash (2002) in developing a multilingual sense inventory for Basque post-
positions and English and Spanish prepositions. Fort and Guillaume (2007) developed
a syntactico-semantic lexicon of French prepositions, partly based on PrepNet; their
particular interest was in enhancing parsing performance. Old (2003) analyzed Roget?s
Thesaurus and arrived at the conclusion that it was not a good source of standalone
preposition semantics. Beavers (2003) analyzed the aspectual and path properties of
goal-marking postpositions in Japanese, and proposed an analysis based on predicate
and event restrictions. Boonthum, Toida, and Levinstein (2005, 2006) defined a general-
purpose sense inventory of seven prepositions (but purportedly applicable to all prepo-
sitions), taking the LCS lexicon as a starting point and expanding the sense inventory
by consulting Quirk et al (1985) and Barker (1996). Finally, as part of a more general
attempt to capture lexical semantics in the formalism of Multilayered Extended Seman-
tic Networks (MultiNet), Helbig (2006) developed a semantic treatment of prepositions,
focusing primarily on German.
There has historically been a strong interest in preposition semantics in the field of
cognitive linguistics (Talmy 1988), in the form of ?schemas.? Schemas are an attempt to
visualize the relation between the object of the preposition (the Trajector, or TR) and its
cognitive context (the Landmark, or LM). They provide a language-independent repre-
sentation, and have been used to analyze crosslinguistic correspondences in preposition
semantics. For example, Tyler and Evans (2003)used schemas to capture the semantics of
English prepositions, arguing that all meanings are grounded in human spatio-physical
experience. Schemas have also been used as the basis of crosslinguistic analysis of
preposition semantics, for example, by Brala (2000) to describe the senses of on and in
in English and Croatian, Knas? (2006) to motivate a sense inventory for at in English and
Polish, and Cosme and Gilquin (2008) to contrast with and avec in English and French.
Trujillo (1995) also developed a language-independent classification of spatial
prepositions for machine translation purposes, in a lexicalist framework.
3.3 Automatic Classification of Preposition Sense
Only a modest amount of research has been carried out on the sense disambiguation
of prepositions, largely because until recently, there haven?t been lexical semantic re-
sources and sense-tagged corpora for prepositions that could be used for this purpose.
Classification of preposition sense has been motivated as a standalone task in appli-
cations such as machine translation, and also as a means of improving the general
performance of semantic tasks such as semantic role labeling.
O?Hara and Wiebe (2003) were the first to perform a standalone preposition word
sense disambiguation (WSD) task, based on the semantic roles in the Penn Treebank.
They collapsed the semantic roles into seven basic semantic classes, and built a decision
tree classifier based on a set of contextual features similar to those used inWSD systems.
O?Hara andWiebe (2009) is an updated version of this original research, using a broader
range of resources. Ye and Baldwin (2006) also built on the earlier research, in attempt-
ing to enhance the accuracy of semantic role labelingwith dedicated PP disambiguation.
131
Computational Linguistics Volume 35, Number 2
They demonstrated the potential for accurate preposition labeling to contribute to large-
scale improvements in overall semantic role labeling performance.
As mentioned in Section 3.2, Litkowski and Hargraves (2007) ran a task on the
WSD of prepositions at SemEval 2007, as a spinoff of the Preposition Project. The task
focused on 34 prepositions, with a combined total of 332 senses. Similarly to a lexical
sample WSD task, participants were required to disambiguate token instances of each
preposition relative to the provided discrete sense inventory. Three teams participated
in the task (Popescu, Tonelli, and Pianta 2007; Ye and Baldwin 2007; Yuret 2007), with all
systems outperforming two baselines over both fine- and coarse-grained sense invento-
ries, through various combinations of lexical, syntactic, and semantic features. The best-
performing system achieved F-scores of 0.818 and 0.861 over fine- and coarse-grained
senses, respectively (Ye and Baldwin 2007).
In other papers dedicated to prepositional WSD, Boonthum, Toida, and Levinstein
(2005, 2006) proposed a semantic collocation-based approach to preposition interpre-
tation, and demonstrated the import of the method in a paraphrase recognition task.
Alam (2003, 2004) used decision trees to disambiguate 12 senses of over, distinguishing
between senses which are determined by their governor and those which are deter-
mined by their NP complement.
Baldwin (2006) explored the interaction between preposition valence and the dis-
tributional hypothesis, based on latent semantic analysis (LSA; Deerwester et al 1990).
He derived gold-standard preposition-to-preposition similarities using each of Dorr?s
LCS lexicon (based on the method of Resnik and Diab [2000]) and Roget?s Thesaurus,
and compared them to the similarities predicted by LSA, in each case using either
valence specification (considering intransitive and transitive prepositions separately) or
valence underspecification (considering both preposition valences together). His results
indicated higher correlation when valence specification is used, suggesting not only
that there is a significant difference in semantics between transitive and intransitive
usages of a given preposition, but that it is sufficiently marked in the context of use that
distributional methods are able to pick up on it.
Cook and Stevenson (2006) developed a four-way classification of the semantics
of up in VPCs based on cognitive grammar, and classified token instances based on a
combination of linguistic and word co-occurrence features.
In a machine translation context, Trujillo (1995) used selectional preferences and
unification to perform target language disambiguation over his classification of spatial
prepositions.
Srihari, Niu, and Li (2000) used manual rules to disambiguate prepositions in
named entities.
3.4 The Semantics of Prepositional MWEs
Prepositional MWEs?focusing primarily on the English MWE types of VPCs, PVs, PP?
Ds, and compound nominals?populate the spectrum from fully compositional to fully
non-compositional (Dixon 1982; McCarthy, Keller, and Carroll 2003). For instance, put
up (as in put the picture up) is fully compositional, whereas make out (as in Kim and Sandy
made out) is fully non-compositional. Compositionality can be viewed relative to each
component word (Bannard 2005) or holistically for the MWE as a single unit (McCarthy,
Keller, and Carroll 2003). With play out (as in see how the match plays out), for example,
we might claim that play is non-compositional but out is (semi-)compositional, and that
as a whole the VPC is non-compositional. The question of compositionality is confused
132
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
somewhat by productive constructions such as the resultative up (e.g., eat/beat/finish/...
up) and the manner by (e.g., by train/car/broomstick/...), which have specialized semantics
relative to their simplex usages but occur relatively freely with this semantics within a
given construction (VPC and PP?D, respectively, in our examples).
There have been a number of attempts to model the compositionality of VPCs. Ban-
nard (2005, 2006) considered VPC compositionality at the component word level, and
proposed a distributional approach that assumes there is a positive correlation between
compositionality and the distributional similarity of each component to simplex usages
of that same word. McCarthy, Keller, and Carroll (2003) opted for a holistic notion
of compositionality and used a range of approaches based on the distributional ?the-
saurus? of Lin (1998) to model compositionality, for example, in calculating the overlap
in the top-N similar words for a given VPC and its head verb. They also examined
the use of statistical tests such as mutual information in modeling compositionality,
and found the similarity-based methods to correlate more highly with the human
judgments. Baldwin et al (2003) used LSA to analyze the compositionality of VPCs
(and compound nouns), and once again demonstrated that there is a positive correlation
between compositionality and the distributional similarity between a given VPC and
its head word. Kim and Baldwin (2007) predicted the compositionality of VPCs based
on a combination of the McCarthy, Keller, and Carroll (2003) and Bannard (2006) data
sets, and analysis of verb?particle co-occurrence patterns. Taking a different approach to
the task, Patrick and Fletcher (2005) classified token instances of verb?preposition pairs
according to the three classes of decomposable (syntactic dependence between the P and
V, with compositional semantics), non-decomposable (syntactic dependence between
the P and V, with idiomatic semantics), and independent (no syntactic dependence
between the P and V).
Levi (1978) pioneered the use of prepositions as a means of interpreting compound
nouns, through the notion of compatibility with paraphrases incorporating preposi-
tions. For example, baby chair can be paraphrased as chair for (a) baby, indicating com-
patibility with the FOR class. In her original research, Levi used four prepositions, in
combination with a set of verbs (for relative clause paraphrases) and semantic roles
(for nominalizations). Lauer (1995) extended this research in developing an exclusively
preposition-based set of seven semantic classes. For example, Levi would interpret truck
driver as PATIENT, in the sense that truck is the patient of the underlying verb of the
head noun to drive, whereas Lauer would interpret it as OF (c.f., driver of (the) truck).
Girju (2007, 2009) leveraged translation data to improve the accuracy of compound
noun interpretation, based on the observation that the choice of preposition in Ro-
mance languages is often indicative of the semantics of the compound noun. Con-
versely, Johnston and Busa (1996) used Qualia structure from the Generative Lexicon
(Pustejovsky 1995) to interpret the prepositions in Italian complex nominals, such as
macchina da corsa ?race car?. Jensen and Nilsson (2003) used (Danish) prepositions as
the basis of a finite set of role relations with which to describe the meaning content
of nominals, and demonstrated the utility of the resultant ontology in disambiguating
nominal phrases.
4. Applications
Prepositions have tended to be overlooked in NLP applications, but there have been
isolated examples of prepositions being shown to be worthy of dedicated treatment. In
particular, applications requiring some level of syntactic abstraction tend to benefit from
133
Computational Linguistics Volume 35, Number 2
the inclusion of prepositions. Similarly, applications which incorporate an element of
natural language generation or realization need to preserve prepositions in the interests
of producing well-formed outputs.
Riloff (1995) challenged the validity of the stop-word philosophy for text classi-
fication, and demonstrated that dependency tuples incorporating prepositions are a
more effective document representation than simple words. In a direct challenge to
the prevalent ?stop word? perception of prepositions in information retrieval, Hansen
(2005) and Lassen (2006) placed emphasis on not only prepositions but preposi-
tion semantics in a music retrieval system and ontology-based text search system,
respectively.
Information extraction is one application where prepositions are uncontrover-
sially crucial to system accuracy, in terms of the role they play in named entities
(Cucchiarelli and Velardi 2001; Toral 2005; Kozareva 2006) and in IE patterns, in linking
the elements in a text (Appelt et al 1993; Muslea 1999; Ono et al 2001; Leroy and Chen
2002).
Benamara (2005) used preposition semantics in a cooperative question answering
system. In the context of cross-language question answering (CLQA), Hartrumpf,
Helbig, and Osswald (2006) used MultiNet to interpret the semantics of German
prepositions, and demonstrated that in instances where the answer passage contained
a different preposition to that included in the original question, preposition semantics
boosted the performance of their CLQA system.
Boonthum, Toida, and Levinstein (2006) successfully applied their prepositionWSD
method in a paraphrase recognition task, namely, predicting that Kim covered the baby in
blankets and Kim covered the baby with blankets have essentially the same semantics. They
proposed seven general senses of prepositions (e.g., PARTICIPANT, INSTRUMENT, and
QUALITY), and annotated prepositions occurring in 120 sentences for each of 10 prepo-
sitions. They evaluated aWSDmethod over this data, and sketched how the preposition
sense information could then be used for paraphrase recognition.
Prepositions have deservedly received a moderate amount of attention in appli-
cations which require explicit representation of 3D space, such as robotics, animated
agents, and virtual reality, in the context of interpreting the spatial information of
prepositions. For example, Xu and Badler (2000) developed a geometric definition of
the motion trajectories of prepositions, whereas Tokunaga, Koyama, and Saito (2005)
use potential functions to estimate the spatial extent of Japanese spatial nouns (which
combine with postpositions to have a similar syntactic and semantic profile to Eng-
lish spatial prepositions). Kelleher and van Genabith (2003) proposed a method for
interpreting in front of and behind in a virtual reality environment based on different
frames of reference. Hying (2007) carried out an analysis of preposition semantics
in the HRCR Map Task corpus, and used it to evaluate two models of projective
prepositions. Kelleher and Kruijff (2005) developed a model for grounding spatial
expressions in visual perception and also for modeling proximity, and Reichelt and
Verleih (2005) developed the B3D system for generating a computational representation
of prepositions in geospatial applications. Furlan, Baldwin, and Klippel (2007) used
preposition occurrence in Web data as a means of classifying landmarks for use in
route directions. Finally, Kelleher and Costello (2009) proposed computational models
of topological and projective spatial prepositions for use in a visually situated dialogue
system.
In the field of machine translation (MT), spatial and temporal prepositions have
received a moderate amount of attention, particularly in the context of interlingua-
and transfer-based MT. Dorr and Voss (1993) mapped prepositions onto 5-place spatial
134
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
predicates in the context of interlingua-based MT, building on the work of Talmy (1985)
and Jackendoff (1983, 1990). Nu?bel (1996) developed a set of interlingual predicates for
adjunct PPs in English?German MT, incorporating a dialogue component which can
be used to disambiguate prepositions relative to the discourse context. Bond, Ogura,
and Uchino (1997) developed a dedicated type hierarchy for the generation of preposi-
tions associated with temporal expressions in Japanese?English machine translation.
Kumar Naskar and Bandyopadhyay (2006) proposed a transfer-based method for trans-
lating English prepositions into Bengali postpositions/inflectional markers, focusing
on spatial, temporal, and also idiomatic usages. Bond (1998) proposed an algorithm
for translating Japanese spatial nouns into English prepositions based on semantic
fields. Trujillo (1995) used his classification of spatial prepositions as the basis of an
English?Spanish translation system using bilingual lexical rules. Hajic? et al (2002)
proposed a method for inserting prepositions into tectogrammatical representations in
Czech?English MT, although they did not manage to integrate the predictions into
their final MT system. Husain, Sharma, and Reddy (2007) achieved promising results
using an explicit model of preposition semantics as the basis for preposition selection in
Hindi/Telugu?English machine translation.
In the context of statistical machine translation, Toutanova and Suzuki (2007) iden-
tified that case marker (= postposition) generation poses a significant challenge for stan-
dard phrase-based methods in English?Japanese translation, identifying case marker
errors in 16% of outputs. They proposed an n-best reranking method to improve case
marker generation performance, whereby they expand the n-best list to include extra
case marker variations, and perform case marker prediction for each bunsetsu.7 In eval-
uation, they demonstrated that their proposed method significantly outperforms both
the baseline SMT system (Quirk, Menezes, and Cherry 2005) and a comparable n-best
rerankingmethodwithout dedicated casemarker candidate expansion. The significance
of this research is that it demonstrates that dedicated handling of postpositions can
enhance SMT performance, a result which has promise for adpositions and markers in
other languages.
As stated in Section 1, prepositions are notoriously hard for non-native speakers to
master, and are a frequent source of errors in English as a Second Language (ESL) prose.
Errors can take the form of incorrect preposition selection (e.g., *Kim stayed in home),
erroneous preposition insertion (e.g., *Kim played at outside), or erroneous preposition
omission (e.g., *Kim went ? the conference) (Tetreault and Chodorow 2008b). Tetreault
and Chodorow (2008b) proposed a combined detection/correction method based on a
supervised model. For each preposition in a text, they predict the most likely candidate
from 34 candidates, based on local word context. If the most likely candidate differs
from the original preposition selection, an error is predicted and correction proposed.
In evaluation over ESL texts, they found that their method performs at high precision
but low recall. Separately, they found that the same method performs considerably
better when applied to preposition selection over native English text, and also that it
is important to have multiple annotators correct ESL text in order to avoid skewing the
data (Tetreault and Chodorow 2008a). In other research, Gamon et al (2008) performed
preposition selection in terms of both selection and insertion, but over a smaller
set of prepositions. De Felice and Pulman (2007) similarly proposed a method for
7 Roughly speaking, a bunsetsu is a case-marked chunk.
135
Computational Linguistics Volume 35, Number 2
preposition correction, but only evaluated their model over five prepositions and native
English text.
5. Introduction to the Articles in This Special Issue
For this special issue we invited submissions that brought a theoretical basis to research
on prepositions in lexical resources and NLP tasks. The number of submissions re-
ceived reflects the interest in prepositions at this time, with a total of 16 submissions.
On the basis of a rigorous review process, we selected four articles for inclusion in
the special issue, covering: the use of semantic resources to disambiguate preposition
semantics, for use in lexical acquisition (O?Hara and Wiebe 2009); a crosslingual lexical
semantic analysis of prepositions to interpret nominal compounds (Girju 2009); a formal
semantic analysis of preposition semantics, and its possible application in Norwegian?
English machine translation (J?rgensen and L?nning 2009); and a computational model
for preposition semantics for use in a dialogue system (Kelleher and Costello 2009). We
now outline each of these articles.
The first two articles look at the semantic interpretation of prepositions, as they tend
to be highly polysemous and have a number of closely related senses.
O?Hara and Wiebe look at the disambiguation of preposition semantics for use
in lexical acquisition, in semi-automatically extending lexical resources. They investi-
gate the utility of information learned from resources such as the Penn Treebank and
FrameNet, to perform semantic role disambiguation of PPs. The proposedmethodology
is evaluated in a series of experiments, and different sense granularities are contrasted
in task-based evaluation.
Prepositions are highly frequent in many languages, but also highly idiomatic in
usage in a given language (hence the difficulty of non-native speakers in learning to use
prepositions correctly). However, there are instances of cross-linguistic regularities in
their linguistic realizations. For example, when translating English nominal compounds
of the type Noun Preposition Noun (N P N) and Noun Noun (N N) into Romance
languages, these are often translated into N PN compounds, where the choice of prepo-
sition is (semi-)predictable from the semantics of the compound. Girju investigates
the role of the syntactic and semantic properties of prepositions in English and Romance
languages in the automatic semantic interpretation of English nominal compounds.
On the basis of an extensive corpus analysis of the distribution of semantic relations
in nominal compounds in English and five Romance languages, she attempts to dis-
ambiguate the semantic relations in English nominal compounds. She focuses on non-
equative compositional nominal compounds, and empirically tests the contribution of
prepositions to the task of semantic interpretation, using a supervised, knowledge-
intensive model.
Identification of crosslinguistic and possibly universal properties of prepositions (or
equivalent constructions) could potentially impact on a number of NLP applications. In
machine translation, for example, a formal description of the crosslinguistic semantic
properties of prepositions could provide the basis for an interlingua. This is the topic of
the third article in this special issue: J?rgensen and L?nning propose a unification-based
grammar implementation of a formal semantic analysis of preposition semantics, and
demonstrate its application in Norwegian?English MT.
A correct handling of prepositions, particularly spatial prepositions, is also impor-
tant in dialogue systems, as prepositions are often used to refer to entities in the physical
environment of system interaction. The last article in this special issue addresses this
136
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
topic: Kelleher and Costello present computational models of spatial preposition
semantics for use in visually situated dialogue systems. The proposed models of
topological and projective spatial prepositions can be used for both interpretation and
generation of prepositional expressions in complex visual environments containing
multiple objects, and are able to account for the contextual effect which other distractor
objects can have on the region described by a preposition. The evaluation is done in
terms of psycholinguistic tests evaluating the approach to distractor interference on
prepositional semantics. The models are employed in a human?robot dialogue system
to interpret locative expressions containing a topological preposition and to generate
spatial references in visually situated contexts.
6. Discussion and Conclusion
As we hope to have demonstrated in this introduction, prepositions have led a mixed
existence in computational linguistics and related fields, particularly in the context
of applications. In applications requiring spatial interpretation (e.g., situated dialogue
systems), they have been the focus of dedicated research, and in applications which
incorporate a language generation component such as MT, there have been isolated
instances exemplifying the need for dedicated handling of prepositions/case markers.
In general, however, they tend to have been relegated to the sidelines in applied NLP
research.
Research on prepositions has tended to concentrate on a specific subset of preposi-
tions in a particular language, often as a one-off research task which has failed to make
broader impact in the field of computational linguistics. PP attachment?especially in
English?has been an exception, in the sense that the RRR data set has given rise to an
active strand of research centered around prepositions.We hope that variants of the RRR
data set from Atterer and Schu?tze (2007) and Agirre, Baldwin, and Martinez (2008) will
reinvigorate interest in PP attachment, in a situated parsing context. Similarly, the emer-
gence of resources such as the Preposition Project, and the data set made available for
the SemEval 2007 task on the word sense disambiguation of prepositions (Litkowski
and Hargraves 2007), provide the means for more detailed analysis of preposition
semantics. In addition to standalone word sense disambiguation tasks, however, there
needs to be more research on the interaction of preposition semantics with other
semantic tasks, such as semantic role labeling and the word sense disambiguation of
content words. The increasing availability of large-scale parallel corpora paves the way
for crosslinguistic research on preposition syntax. Areas of particular promise in this
regard are methods for generating prepositions inMT, and automatic error correction of
preposition usage in non-native speaker text. Following the lead of Saint-Dizier (2006b),
J?rgensen and L?nning (2009), and others, we also hope to see more crosslinguistic
and typological research on the lexical semantics of prepositions. Although there has
been a steady proliferation of WordNets for different languages, linked variously to
English WordNet (e.g., EuroWordNet for several European languages [Vossen 1998],
BALKANET for Balkan languages [Stamou et al 2002], HowNet for Chinese [Dong and
Dong 2006], and Japanese WordNet for Japanese [Isahara et al 2008]), they have tended
to follow the lead of English WordNet and focus exclusively on content words. Given
the increasing maturity of resources such as the Preposition Project and PrepNet, the
time seems right to develop preposition sense inventories for more languages, linked
back to English. On the basis of currently available resources and future efforts such
as these, we believe there will be a steady lowering of the barrier to including a more
systematic handling of prepositions in NLP applications.
137
Computational Linguistics Volume 35, Number 2
The purpose of this article has been to highlight the theoretical and applied re-
search that has been done on prepositions in computational linguistics, focusing on
computational syntax and semantics. In particular, we have aimed to highlight applied
research which has focused specifically on prepositions. It is our hope that through this
special issue, we will rekindle interest in prepositions and motivate greater awareness
of prepositions in various applications.
Acknowledgments
Thanks to Ken Litkowski, Paola Merlo,
Patrick Saint-Dizier, and Martin Volk for
feedback on an earlier draft of this paper,
and Robert Dale and Mary Gardiner for their
unfailing support and encouragement
throughout the editorial process. We also
acknowledge the efforts of the considerable
number of reviewers who devoted time to
reviewing the original submissions to the
special issue, and fine-tuning the accepted
articles. The first author of this article was
supported in part by the Australian Research
Council through Discovery Project grant
DP0663879.
References
Aasa, Jo?rgen. 2004. Unsupervised resolution
of PP attachment ambiguities in Swedish.
Master?s thesis, Stockholm University.
Abeille?, Anne, Olivier Bonami, Danie`le
Godard, and Jesse Tseng. 2003. The
syntax of a` and de: an HPSG analysis. In
Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 133?144,
Toulouse.
Abney, Steven, Robert E. Schapire, and
Yoram Singer. 1999. Boosting applied to
tagging and PP attachment. In Proceedings
of the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC-99),
pages 38?45, College Park, MD.
Agirre, Eneko, Timothy Baldwin, and
David Martinez. 2008. Improving parsing
and PP attachment performance with
sense information. In Proceedings of the
46th Annual Meeting of the ACL: Human
Language Technologies, pages 317?325,
Columbus, OH.
Alam, Yukiko Sasaki. 2003. For
computational treatment of the polysemy
of prepositional uses of Over. In
Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 66?76,
Toulouse.
Alam, Yukiko Sasaki. 2004. Decision trees
for sense disambiguation of prepositions:
Case of over. In Proceedings of the
HLT-NAACL 2004 Workshop on
Computational Lexical Semantics,
pages 52?59, Boston, MA.
Alegre, Martha A., Josep M. Sopena, and
Agusti Lloberas. 1999. PP-attachment: A
committee machine approach. In
Proceedings of the Joint SIGDAT Conference
on Empirical Methods in Natural Language
Processing and Very Large Corpora
(EMNLP/VLC-99), pages 231?238,
College Park, MD.
Allen, James F. 1984. Towards a general
theory of action and time. Artificial
Intelligence, 23:123?154.
Altmann, Gerry and Mark Steedman. 1988.
Interaction with context during human
sentence processing. Cognition,
30(3):191?238.
Appelt, Douglas E., Jerry R. Hobbs, John
Bear, David Israel, Megumi Kameyama,
and Mabry Tyson. 1993. FASTUS: A
finite-state processor for information
extraction from real-world text. In
Proceedings of the 13th International Joint
Conference on Artificial Intelligence
(IJCAI-93), pages 1172?1181, Chambery.
Arsenijevic?, Boban. 2005. Subevents and
prepositions. In Proceedings of the Second
ACL-SIGSEMWorkshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and
Applications, pages 39?46, Colchester.
Atterer, Michaela and Hinrich Schu?tze. 2007.
Prepositional phrase attachment without
oracles. Computational Linguistics,
33(4):469?476.
Baeza-Yates, Ricardo and Berthier
Ribeiro-Neto. 1999.Modern Information
Retrieval. Addison Wesley/ACM press,
Harlow, UK.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of the
36th Annual Meeting of the ACL and 17th
International Conference on Computational
138
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Linguistics: COLING/ACL-98, pages 86?90,
Montreal.
Baldwin, Timothy. 2005a. Deep lexical
acquisition of verb-particle constructions.
Computer Speech and Language,
19(4):398?414.
Baldwin, Timothy. 2005b. Looking for
prepositional verbs in corpus data. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 180?189, Colchester.
Baldwin, Timothy. 2006. Distributional
similarity and preposition semantics. In
Saint-Dizier (Saint-Dizier 2006a).
Baldwin, Timothy. 2008. A resource for
evaluating the deep lexical acquisition of
English verb-particle constructions. In
Proceedings of the LREC 2008 Workshop:
Towards a Shared Task for Multiword
Expressions (MWE 2008), pages 1?2,
Marrakech.
Baldwin, Timothy, Colin Bannard, Takaaki
Tanaka, and Dominic Widdows. 2003.
An empirical model of multiword
expression decomposability. In
Proceedings of the ACL-2003 Workshop
on Multiword Expressions: Analysis,
Acquisition and Treatment, pages 89?96,
Sapporo.
Baldwin, Timothy, John Beavers, Leonoor
van der Beek, Francis Bond, Dan
Flickinger, and Ivan A. Sag. 2006. In search
of a systematic treatment of determinerless
PPs. In Saint-Dizier (Saint-Dizier 2006a).
Baldwin, Timothy, Emily M. Bender, Dan
Flickinger, Ara Kim, and Stephan Oepen.
2004. Road-testing the English Resource
Grammar over the British National
Corpus. In Proceedings of the 4th
International Conference on Language
Resources and Evaluation (LREC 2004),
pages 2047?2050, Lisbon.
Baldwin, Timothy and Aline Villavicencio.
2002. Extracting the unextractable: A case
study on verb-particles. In Proceedings of
the 6th Conference on Natural Language
Learning (CoNLL-2002), pages 98?104,
Taipei.
Bannard, Colin. 2005. Learning about the
meaning of verb-particle constructions
from corpora. Computer Speech and
Language, 19(4):467?478.
Bannard, Colin. 2006. Acquiring Phrasal
Lexicons from Corpora. Ph.D. thesis,
University of Edinburgh.
Barker, Ken. 1996. The assessment of
semantic cases using English positional,
prepositional and adverbial case markers.
Technical Report TR-96-08, Computer
Science, University of Ottawa.
Beavers, John. 2003. The semantics and
polysemy of goal marking prepositions in
Japanese. In Proceedings of the
ACL-SIGSEMWorkshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and
Applications, pages 205?216, Toulouse.
Benamara, Farah. 2005. Reasoning with
prepositions within a cooperative
question-answering framework. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 145?152, Colchester.
Bennett, David C. 1975. Spatial and Temporal
uses of English Prepositions: As Essay in
Stratified Semantics. Longman, London.
Bikel, Daniel M. 2004. Intricacies of Collins?
parsing model. Computational Linguistics,
30(4):479?511.
Bond, Francis. 1998. Interpretation of
Japanese ?spatial? nouns in
Japanese-to-English machine translation.
Presentation at the 1998 Annual General
Meeting of the Australian Linguistics
Society: ALS-98.
Bond, Francis, Kentaro Ogura, and Hajime
Uchino. 1997. Temporal expressions in
Japanese-to-English machine translation.
In Proceedings of the 7th International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-97),
pages 55?62, Santa Fe, NM.
Boonthum, Chutima, Shunichi Toida, and
Irwin Levinstein. 2005. Sense
disambiguation for preposition ?with?. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 153?162, Colchester.
Boonthum, Chutima, Shunichi Toida, and
Irwin Levinstein. 2006. Preposition senses:
Generalized disambiguation model. In
Proceedings of the 7th International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-2006),
pages 196?207, Mexico City.
Brala, Marija M. 2000. Understanding and
translating (spatial) prepositions: An
exercise in cognitive semantics for
lexicographic purposes.Working Papers of
the University of Cambridge Research Centre
for English and Applied Linguistics, 7.
University of Cambridge.
139
Computational Linguistics Volume 35, Number 2
Bre?e, D. S. and R. A. Smit. 1986. Temporal
relations. Journal of Semantics, 5(4):345?384.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the 5th
Conference on Applied Natural Language
Processing (ANLP), pages 356?363,
Washington, DC.
Briscoe, Ted, John Carroll, and Rebecca
Watson. 2006. The second release of the
RASP system. In Proceedings of the Poster
Session of the 21st International Conference on
Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics (COLING/ACL 2006),
pages 77?80, Sydney.
Burnard, Lou. 2000. Reference Guide for the
British National Corpus. Oxford University
Computing Services, Oxford, UK.
Calvo, Hiram, Alexander Gelbukh, and
Adam Kilgarriff. 2005. Distributional
thesaurus versus WordNet: A comparison
of backoff techniques for unsupervised PP
attachment. In Proceedings of the 6th
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLing-2005), pages 177?188,
Mexico City.
Calzolari, Nicoletta, Charles Fillmore,
Ralph Grishman, Nancy Ide, Alessandro
Lenci, Catherine MacLeod, and
Antonio Zampolli. 2002. Towards
best practice for multiword expressions
in computational lexicons. In
Proceedings of the 3rd International
Conference on Language Resources and
Evaluation (LREC 2002), pages 1934?1940,
Las Palmas.
Cannesson, Emmanuelle and Patrick
Saint-Dizier. 2002. Defining and
representing preposition senses: A
preliminary analysis. In Proceedings
of the ACL-02 Workshop on Word Sense
Disambiguation: Recent Successes
and Future Directions, pages 25?31,
Philadelphia, PA.
Che?liz, Mar??a del Carmen Horno. 2002. Lo
que la Preposicio?n Esconde: Estudio Sobre la
Argumentalidad Preposicional en el Predicado
Verbal. University of Zaragoza Press,
Zaragoza, Spain. (In Spanish).
Chodorow, Martin, Joel Tetreault, and
Na-Rae Han. 2007. Detection of
grammatical errors involving prepositions.
In Proceedings of the 4th ACL-SIGSEM
Workshop on Prepositions, pages 25?30,
Prague.
Church, Kenneth and Ramesh Patil. 1982.
Coping with syntactic ambiguity or how
to put the block in the box on the table.
American Journal of Computational
Linguistics, 8(3?4):139?149.
Collins, Michael and James Brooks. 1995.
Prepositional phrase attachment through a
backed-off model. In Proceedings of the 3rd
Annual Workshop on Very Large Corpora,
pages 27?38, Cambridge, MA.
Cook, Paul and Suzanne Stevenson. 2006.
Classifying particle semantics in English
verb-particle constructions. In Proceedings
of the COLING/ACL 2006 Workshop on
Multiword Expressions: Identifying and
Exploiting Underlying Properties,
pages 45?53, Sydney.
Copestake, Ann, Dan Flickinger, Ivan A. Sag,
and Carl Pollard. 2005. Minimal recursion
semantics: An introduction. Journal of
Research on Language and Computation,
3(2?3):281?332.
Copestake, Ann, Fabre Lambeau, Aline
Villavicencio, Francis Bond, Timothy
Baldwin, Ivan Sag, and Dan Flickinger.
2002. Multiword expressions: Linguistic
precision and reusability. In Proceedings of
the 3rd International Conference on Language
Resources and Evaluation (LREC 2002),
pages 1941?1947, Las Palmas.
Cosme, Christelle and Gae?tanelle Gilquin.
2008. Free and bound prepositions
in a contrastive perspective: The
case of with and avec. In Sylviane
Granger and Fanny Meunier, editors,
Phraseology: An Interdisciplinary
Perspective. John Benjamins, Amsterdam,
pages 259?274.
Cucchiarelli, Alessandro and Paola Velardi.
2001. Unsupervised named entity
recognition using syntactic and semantic
contextual evidence. Computational
Linguistics, 27(1):123?131.
Dahlgren, Kathleen and Joyce McDowell.
1986. Using commonsense knowledge
to disambiguate prepositional
phrase modifiers. In Proceedings of
the 6th Conference on Artificial
Intelligence (AAAI-86), pages 589?593,
Philadelphia, PA.
De Felice, Rachele and Stephen G. Pulman.
2007. Automatically acquiring models of
preposition use. In Proceedings of the 4th
ACL-SIGSEMWorkshop on Prepositions,
pages 45?50, Prague.
Deerwester, Scott, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer,
and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
American Society of Information Science,
41(6):391?407.
140
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Dehe?, Nicole, Ray Jackendoff, Andrew
McIntyre, and Silke Urban, editors. 2002.
Verb-particle explorations. Mouton de
Gruyter, Berlin/New York.
Denis, Pascal, Jonas Kuhn, and Stephen
Wechsler. 2003. V-PP goal motion
complexes in English: An HPSG account.
In Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 121?132,
Toulouse.
Dixon, Robert M. W. 1982. The grammar of
English phrasal verbs. Australian Journal of
Linguistics, 2:149?247.
Do?mges, Florian, Tibor Kiss, Antje Mu?ller,
and Claudia Roch. 2007. Measuring the
productivity of determinerless PPs. In
Proceedings of the 4th ACL-SIGSEM
Workshop on Prepositions, pages 31?37,
Prague.
Dong, Zhendong and Qiang Dong. 2006.
HowNet And the Computation of Meaning.
World Scientific Publishing, River
Edge, NJ.
Dorr, Bonnie. 1993.Machine Translation: A
View from the Lexicon. MIT Press,
Cambridge, MA.
Dorr, Bonnie and Nizar Habash. 2002.
Interlingua approximation: A
generation-heavy approach. In Proceedings
of the AMTA-2002 Interlingua Reliability
Workshop, Tiburon, CA.
Dorr, Bonnie J. 1997. Large-scale dictionary
construction for foreign language tutoring
and interlingual machine translation.
Machine Translation, 12(4):271?322.
Dorr, Bonnie J. and Clare R. Voss. 1993.
Machine translation of spatial expressions:
Defining the relation between an
interlingua and a knowledge
representation system. In Proceedings of the
11th Annual Conference on Artificial
Intelligence (AAAI-93), pages 374?379,
Washington, DC.
Durrell, Martin and David Bre?e. 1993.
German temporal prepositions from an
English perspective. In Zelinski-Wibbelt
(Zelinski-Wibbelt 1993), pages 295?326.
Evert, Stefan and Brigitte Krenn. 2005. Using
small random samples for the manual
evaluation of statistical association
measures. Computer Speech and Language,
19(4):450?466.
Fazly, Afsaneh, Paul Cook, and Suzanne
Stevenson. 2009. Unsupervised type and
token identification of idiomatic
expressions. Computational Linguistics,
35(1):61?103.
Fellbaum, Christiane, editor. 1998.WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fillmore, Charles J. 1968. The case for case. In
Emmon Bach and Robert T. Harms,
editors, Universals in Linguistic Theory.
Holt, Rinehart, and Winston, New York,
pages 1?88.
Ford, Marilyn, Joan Bresnan, and Ronald
Kaplan. 1982. A competence-based theory
of syntactic closure. In Joan Bresnan,
editor, The Mental Representation of
Grammatical Relations. MIT Press,
Cambridge, MA, pages 727?796.
Fort, Kare?n and Bruno Guillaume. 2007.
PrepLex: A lexicon of French prepositions
for parsing. In Proceedings of the 4th
ACL-SIGSEMWorkshop on Prepositions,
pages 17?24, Prague.
Foth, Kilian A. and Wolfgang Menzel. 2006.
The benefit of stochastic PP attachment to
a rule-based parser. In Proceedings of the
Poster Session of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL
2006), pages 223?230, Sydney.
Franz, Alexander. 1996. Learning PP
attachment from corpus statistics. In Stefan
Wermter, Ellen Riloff, and Gabriele
Scheler, editors, Connectionist, Statistical,
and Symbolic Approaches to Learning for
Natural Language Processing. Springer,
Berlin, Germany, pages 188?202.
Frazier, Lyn. 1979. On Comprehending
Sentences: Syntactic Parsing Strategies.
Ph.D. thesis, University of Connecticut.
Furlan, Aidan, Timothy Baldwin, and Alex
Klippel. 2007. Landmark classification for
route description generation. In Proceedings
of the 4th ACL-SIGSEMWorkshop on
Prepositions, pages 9?16, Prague.
Gala, Nuria and Mathieu Lafourcade.
2005. Combining corpus-based pattern
distributions with lexical signatures
for PP attachment ambiguity resolution.
In Proceedings of the 6th Symposium on
Natural Language Processing (SNLP-05),
Chiang Rai.
Gamon, Michael, Jianfeng Gao, Chris
Brockett, Alexandre Klementiev,
William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using
contextual speller techniques and
language modeling for ESL error
correction. In Proceedings of the 3rd
International Joint Conference on Natural
Language Processing (IJCNLP-08),
pages 449?456, Hyderabad.
141
Computational Linguistics Volume 35, Number 2
Gaussier, Eric and Nicola Cancedda. 2001.
Probabilistic models for PP-attachment
resolution and NP analysis. In Proceedings
of the ACL/EACL-2001 Workshop on
Computational Natural Language Learning
(CoNLL-2001), pages 1?8, Toulouse.
Girju, Roxana. 2007. Improving the
interpretation of noun phrases with
cross-linguistic information. In Proceedings
of the 45th Annual Meeting of the ACL,
pages 568?575, Prague.
Girju, Roxana. 2009. The syntax and
semantics of prepositions in the task of
automatic interpretation of nominal
phrases and compounds: A cross-linguistic
study. Computational Linguistics,
35(2):185?228.
Hahn, Udo, Martin Romacker, and Stefan
Schulz. 2002. Creating knowledge
repositories from biomedical reports: The
MEDSYNDIKATE system. In Proceedings of
the Pacific Symposium on Biocomputing 2002,
pages 338?349, Lihue, HI.
Hajic?, Jan, Martin C?mejrek, Jason Eisner,
Gerald Penn, Owen Rambow, Drago
Radev, Yuan Ding, Terry Koo, and Kristen
Parton. 2002. Natural language generation
in the context of machine translation.
Technical report, CSLP, Johns Hopkins
University. Available at www.clsp.jhu.
edu/ws02/groups/mt/?MT final rpt.pdf.
Hansen, Steffen Leo. 2005. Concept-based
representation of prepositions. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 138?144, Colchester.
Hartrumpf, Sven. 1999. Hybrid
disambiguation of prepositional phrase
attachment and interpretation. In
Proceedings of the Joint SIGDAT Conference
on Empirical Methods in Natural Language
Processing and Very Large Corpora
(EMNLP/VLC-99), pages 111?120,
College Park, MD.
Hartrumpf, Sven, Hermann Helbig, and
Rainer Osswald. 2006. Semantic
interpretation of prepositions for NLP
applications. In Proceedings of the Third
ACL-SIGSEMWorkshop on Prepositions,
pages 29?36, Trento.
Helbig, Hermann. 2006. Knowledge
Representation and the Semantics of
Natural Language. Springer, Dordrecht,
Netherlands.
Hellan, Lars and Dorothee Beermann. 2005.
Classification of prepositional senses for
deep grammar applications. In Proceedings
of the Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 74?83,
Colchester.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Hirst, Graeme. 1987. Semantic Interpretation
and the Resolution of Ambiguity. Cambridge
University Press, Cambridge, UK.
Huddleston, Rodney and Geoffrey K.
Pullum. 2002. The Cambridge Grammar of
the English Language. Cambridge
University Press, Cambridge, UK.
Husain, Samar, Dipti Misra Sharma, and
Manohar Reddy. 2007. Simple preposition
correspondence: A problem in English to
Indian language machine translation. In
Proceedings of the 4th ACL-SIGSEM
Workshop on Prepositions, pages 51?58,
Prague.
Hying, Christian. 2007. A corpus-based
analysis of geometric constraints on
projective prepositions. In Proceedings
of the 4th ACL-SIGSEMWorkshop on
Prepositions, pages 1?8, Prague.
Isahara, Hitoshi, Francis Bond, Kiyotaka
Uchimoto, Masao Utiyama, and Kyoko
Kanzaki. 2008. Development of the
Japanese WordNet. In Proceedings of the
6th International Conference on Language
Resources and Evaluation (LREC 2008),
Marrakech.
Izumi, Emi, Kiyotaka Uchimoto, Toyomi
Saiga, Thepchai Supnithi, and Hitoshi
Isahara. 2003. Automatic error detection
in the Japanese learners? English spoken
data. In Proceedings of the 41st Annual
Meeting of the ACL, pages 145?148,
Sapporo.
Jackendoff, Ray. 1973. The base rules for
prepositional phrases. In Stephen R.
Anderson and Paul Kiparsky, editors, A
Festschrift for Morris Halle. Holt, Rinehart,
and Winston Inc., New York,
pages 345?356.
Jackendoff, Ray. 1983. Semantics and
Cognition. MIT Press, Cambridge, MA.
Jackendoff, Ray. 1990. Semantic Structures.
MIT Press, Cambridge, MA.
Jensen, Karen and Jean-Louis Binot. 1987.
Disambiguating prepositional phrase
attachments by using on-line dictionary
definitions. Computational Linguistics,
13(3?4):251?260.
Jensen, Per Anker and J. Fischer Nilsson.
2003. Ontology-based semantics for
prepositions. In Proceedings of the
142
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
ACL-SIGSEMWorkshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and
Applications, pages 193?204, Toulouse.
Johnston, Michael and Frederica Busa. 1996.
Qualia structure and the compositional
interpretation of compounds. In
Proceedings of the ACL SIGLEX Workshop on
Breadth and Depth of Semantic Lexicons,
pages 77?88, Santa Cruz, CA.
J?rgensen, Fredrik and Jan Tore L?nning.
2009. A minimal recursion semantic
analysis of locatives. Computational
Linguistics, 35(2):229?270.
Kamp, Hans. 1981. Theory of truth and
semantic representation. In B. J. Janssen
and T. M. V. Stokhof, editors, Formal
Methods in the Study of Language.
Mathematisch Centrum, Amsterdam,
pages 277?322.
Kelleher, John and Josef van Genabith. 2003.
A computational model of the referential
semantics of projective prepositions. In
Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 181?192,
Toulouse.
Kelleher, John D. and Fintan J. Costello.
2009. Applying computational models of
spatial prepositions to visually situated
dialog. Computational Linguistics,
35(2):271?306.
Kelleher, John D. and Geert-Jan M. Kruijff.
2005. A context-dependent model of
proximity in physically situated
environments. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 128?137,
Colchester.
Kim, Su Nam and Timothy Baldwin. 2006.
Automatic identification of English verb
particle constructions using linguistic
features. In Proceedings of the Third
ACL-SIGSEMWorkshop on Prepositions,
pages 65?72, Trento.
Kim, Su Nam and Timothy Baldwin. 2007.
Detecting compositionality of English
verb-particle constructions using semantic
similarity. In Proceedings of the 7th Meeting
of the Pacific Association for Computational
Linguistics (PACLING 2007), pages 40?48,
Melbourne.
Kim, Su Nam and Timothy Baldwin (in
press). How to pick out token instances of
English verb-particle constructions.
Language Resources and Evaluation.
Kipper, Karin, Hoa Trang Dang, and Martha
Palmer. 2000. Class-based construction of a
verb lexicon. In Proceedings of the 18th
Annual Conference on Artificial Intelligence
(AAAI-2000), pages 691?696, Austin, TX.
Kipper, Karin, Benjamin Snyder, and Martha
Palmer. 2004. Using prepositions to extend
a verb lexicon. In Proceedings of the
HLT-NAACL 2004 Workshop on
Computational Lexical Semantics,
pages 23?29, Boston, MA.
Kipper Schuler, Karin. 2005. VerbNet: A
Broad-coverage, Comprehensive Verb Lexicon.
Ph.D. thesis, University of Pennsylvania.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
ACL, pages 423?430, Sapporo.
Knas?, Iwona. 2006. Polish equivalents of
spatial at. In Proceedings of the Third
ACL-SIGSEMWorkshop on Prepositions,
pages 9?16, Trento.
Kokkinakis, Dimitris. 2000. Supervised
PP-attachment for Swedish: Combining
unsupervised and supervised training
data. Nordic Journal of Linguistics,
3(2):191?213.
Kordoni, Valia. 2003a. The key role of
semantics in the development of
large-scale grammars of natural language.
In Proceedings of the 10th Conference of the
EACL (EACL 2003), pages 111?14,
Budapest.
Kordoni, Valia. 2003b. A robust deep analysis
of indirect prepositional arguments. In
Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 112?120,
Toulouse.
Kordoni, Valia. 2006. Prepositional
arguments in a multilingual context. In
Saint-Dizier (Saint-Dizier 2006a).
Korhonen, Anna. 2002. Subcategorization
Acquisition. Ph.D. thesis, University of
Cambridge.
Kozareva, Zornitsa. 2006. Bootstrapping
named entity recognition with
automatically generated gazetteer lists. In
Proceedings of the EACL 2006 Student
Research Workshop, pages 15?21, Trento.
Kracht, Marcus. 2003. Directionality
selection. In Proceedings of the
ACL-SIGSEMWorkshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and
Applications, pages 89?100, Toulouse.
Krenn, Brigitte. 2008. Description of
evaluation resource?German PP-verb
143
Computational Linguistics Volume 35, Number 2
data. In Proceedings of the LREC 2008
Workshop: Towards a Shared Task for
Multiword Expressions (MWE 2008),
pages 7?10, Marrakech.
Krenn, Brigitte and Stefan Evert. 2001. Can
we do better than frequency? A case study
on extracting PP-verb collocations. In
Proceedings of the ACL/EACL 2001 Workshop
on the Computational Extraction, Analysis
and Exploitation of Collocations, pages 39?46,
Toulouse.
Kumar Naskar, Sudip and Sivaji
Bandyopadhyay. 2006. Handling of
prepositions in English to Bengali machine
translation. In Proceedings of the Third
ACL-SIGSEMWorkshop on Prepositions,
pages 89?94, Trento.
Lassen, Tine. 2006. An ontology-based view
of prepositional senses. In Proceedings of the
Third ACL-SIGSEMWorkshop on
Prepositions, pages 45?50, Trento.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on Noun
Compounds. Ph.D. thesis, Macquarie
University.
Leroy, Gondy and Hsinchun Chen. 2002.
Filling preposition-based templates to
capture information from medical
abstracts. In Proceedings of the Pacific
Symposium on Biocomputing 2002,
pages 350?61, Lihue, HI.
Leroy, Gondy, Hsinchun Chen, and Jesse D.
Martinez. 2003. A shallow parser based on
closed-class words to capture relations in
biomedical text. Journal of Biomedical
Informatics, 36:145?158.
Lersundi, Mikel and Eneko Agirre. 2003.
Semantic interpretations of postpositions
and prepositions: A multilingual inventory
for Basque, English and Spanish. In
Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and Their Use in Computational Linguistics
Formalisms and Applications, pages 56?65,
Toulouse.
Lestrade, S. A. M. 2006. Marked adpositions.
In Proceedings of the Third ACL-SIGSEM
Workshop on Prepositions, pages 23?28,
Trento.
Levi, Judith N. 1978. The Syntax and Semantics
of Complex Nominals. Academic Press,
New York.
Levin, Beth and Malka Rappaport-Hovav (in
press). Lexical conceptual structure. In
Claudia Maienborn, Klaus von Heusinger,
and Paul Portner, editors, Semantics: An
International Handbook of Natural Language
Meaning. Mouton de Gruyter, Berlin,
Germany.
Li, Wei, Xiuhong Zhang, Cheng Niu, Yuankai
Jiang, and Rohini K. Srihari. 2003. An
expert lexicon approach to identifying
English phrasal verbs. In Proceedings of the
41st Annual Meeting of the ACL,
pages 513?520, Sapporo.
Lin, Dekang. 1998. Automatic retrieval
and clustering of similar words. In
Proceedings of the 36th Annual Meeting
of the ACL and 17th International
Conference on Computational Linguistics
(COLING/ACL-98), pages 768?774,
Montreal.
Lin, Dekang. 2003. Dependency-based
evaluation of MINIPAR. In Anne Abeille?,
editor, Building and Using Syntactically
Annotated Corpora. Kluwer, Dordrecht,
Netherlands, pages 317?330.
Lindstromberg, Seth. 1998. English
Prepositions Explained. John Benjamins,
Amsterdam, Netherlands.
Lindstromberg, Seth. 2001. Preposition
entries in UK monolingual learner?s
dictionaries: Problems and possible
solutions. Applied Linguistics, 22(1):79?103.
Litkowski, Ken and Orin Hargraves. 2005.
The Preposition Project. In Proceedings of
the Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 171?179,
Colchester.
Litkowski, Kenneth C. 2002. Digraph
analysis of dictionary preposition
definitions. In Proceedings of the ACL-02
Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions,
pages 9?16, Philadelphia, PA.
Litkowski, Kenneth C. and Orin Hargraves.
2006. Coverage and inheritance in the
preposition project. In Proceedings of the
Third ACL-SIGSEMWorkshop on
Prepositions, pages 37?44, Trento.
Litkowski, Kenneth C. and Orin Hargraves.
2007. SemEval-2007 task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the 4th International Workshop
on Semantic Evaluations, pages 24?29,
Prague.
MacKinlay, Andrew and Timothy Baldwin.
2005. POS tagging with a more informative
tagset. In Proceedings of the Australasian
Language Technology Workshop 2005,
pages 40?48, Sydney.
Manning, Christopher D. 1993. Automatic
acquisition of a large subcategorization
dictionary from corpora. In Proceedings of
the 31st Annual Meeting of the ACL,
pages 235?242, Columbus, OH.
144
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Manning, Christopher D., Prabhakar
Raghavan, and Hinrich Schu?tze. 2008.
Introduction to Information Retrieval.
Cambridge University Press, Cambridge,
UK.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: the
Penn treebank. Computational Linguistics,
19(2):313?330.
McCarthy, Diana, Bill Keller, and John
Carroll. 2003. Detecting a continuum of
compositionality in phrasal verbs. In
Proceedings of the ACL-2003 Workshop on
Multiword Expressions: Analysis,
Acquisition and Treatment, pages 73?80,
Sapporo.
Merlo, Paola. 2003. Generalised
PP-attachment disambiguation using
corpus-based linguistic diagnostics. In
Proceedings of the 10th Conference of the
EACL (EACL 2003), pages 251?258,
Budapest.
Merlo, Paola, Matthew W. Crocker, and
Cathy Berthouzoz. 1997. Attaching
multiple prepositional phrases:
Generalized backed-off estimation. In
Proceedings of the 2nd Conference on
Empirical Methods in Natural Language
Processing (EMNLP-97), pages 149?155,
Providence, RI.
Merlo, Paola and Eva Esteve Ferrer. 2006.
The notion of argument in prepositional
phrase attachment. Computational
Linguistics, 32(3):341?377.
Miller, George A. and Philip N.
Johnson-Laird. 1976. Language and
Perception. Cambridge University Press,
Cambridge, UK.
Mitchell, Brian. 2004. Prepositional Phrase
Attachment Using Machine Learning
Algorithms. Ph.D. thesis, University of
Sheffield.
Muischnek, Kadri, Kaili Mu?u?risep, and Tiina
Puolakainen. 2005. Adpositions in
Estonian computational syntax. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 2?10, Colchester.
Muslea, Ion. 1999. Extraction patterns for
information extraction tasks: A survey. In
Proceedings of the AAAI-99 Workshop on
Machine Learning for Information Extraction,
pages 1?6, Orlando, FL.
Nu?bel, Rita. 1996. Knowledge sources for the
disambiguation of prepositions in machine
translation. In Proceedings of the PRICAI-96
Workshop on Future Issues for Multi-lingual
Text Processing, Cairns. Available at
ftp://ftp.mpce.mq.edu.au/pub/comp/
mri/nlp/fimtp/nuebel.ps.gz.
Odijk, Jan. 2004. Reusable lexical
representations for idioms. In Proceedings
of the 4th International Conference on
Language Resources and Evaluation (LREC
2004), pages 903?906, Lisbon.
O?Hara, Tom and Janyce Wiebe. 2003.
Preposition semantic classification via
Treebank and FrameNet. In Proceedings of
the 7th Conference on Natural Language
Learning (CoNLL-2003), pages 79?86,
Edmonton.
O?Hara, Tom and Janyce Wiebe. 2009.
Exploiting semantic role resources for
preposition disambiguation. Computational
Linguistics, 35(2):151?184.
Old, L. John. 2003. An analysis of semantic
overlap among English prepositions
from the Roget?s Thesaurus. In Proceedings
of the ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 13?19,
Toulouse.
Olteanu, Marian and Dan Moldovan. 2005.
PP-attachment disambiguation using large
context. In Proceedings of the 2005
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005),
pages 273?280, Vancouver.
Olteanu, Marian G. 2004. Prepositional
phrase attachment ambiguity resolution
through a rich syntactic, lexical and
semantic set of features applied in support
vector machines learner. Master?s thesis,
University of Texas, Dallas.
Ono, Toshihide, Haretsugu Hishigaki, Akira
Tanigami, and Toshihisa Takagi. 2001.
Automated extraction of information on
protein-protein interactions from the
biological literature. Bioinformatics,
17(2):155?161.
Pantel, Patrick and Dekang Lin. 2000. An
unsupervised approach to prepositional
phrase attachment using contextually
similar words. In Proceedings of the 38th
Annual Meeting of the ACL, pages 101?108,
Hong Kong.
Patrick, Jon and Jeremy Fletcher. 2005.
Classifying verb particle constructions by
verb arguments. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 200?209,
Colchester.
145
Computational Linguistics Volume 35, Number 2
Pearsall, Judy, editor. 1998. The New Oxford
Dictionary of English. Clarendon Press,
Oxford, UK.
Pecina, Pavel. 2008. A machine learning
approach to multiword expression
extraction. In Proceedings of the LREC 2008
Workshop: Towards a Shared Task for
Multiword Expressions (MWE 2008),
pages 54?57, Marrakech.
Pereira, Fernando C. N. 1985. A new
characterization of attachment preferences.
In David R. Dowty, Lauri Karttunen, and
Arnold M. Zwicky, editors, Natural
Language Parsing: Psychological,
Computational and Theoretical Perspectives.
Cambridge University Press, Cambridge,
UK, pages 307?319.
Popescu, Octavian, Sara Tonelli, and
Emanuele Pianta. 2007. IRST-BP:
Preposition disambiguation based on
chain clarifying relationships contexts. In
Proceedings of the 4th International Workshop
on Semantic Evaluations, pages 191?194,
Prague.
Pratt, Ian and Nissim Francez. 1997. On the
semantics of temporal prepositions and
preposition phrases. Technical Report
LCL9701, Computer Science Department,
Technion, Haifa, Israel.
Pustejovsky, J., J. Castano, R. Saur??,
A. Rumshinsky, J. Zhang, and W. Luo.
2002. Medstract: Creating large-scale
information servers for biomedical
libraries. In Proceedings of the ACL 2002
Workshop on Natural Language Processing in
the Biomedical Domain, pages 85?92,
Philadelphia, PA.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press, Cambridge, MA.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translation: Syntactically informed phrasal
SMT. In Proceedings of the 43rd Annual
Meeting of the ACL, pages 271?279, Ann
Arbor, MI.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartvik. 1985. A
Comprehensive Grammar of the English
Language. Longman, London.
Ramsay, Allan. 2005. Prepositions as
abstract relations. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 30?38,
Colchester.
Ratnaparkhi, Adwait, Jeff Reynar, and Salim
Roukos. 1994. A maximum entropy model
for prepositional phrase attachment. In
Proceedings of the ARPA Human Language
Technology Workshop, pages 250?255,
Princeton, NJ.
Rauh, Gisa. 1993. On the grammar of lexical
and non-lexical prepositions in English. In
Zelinski-Wibbelt (Zelinski-Wibbelt 1993),
pages 99?150.
Rehbein, Ines and Josef van Genabith. 2006.
German particle verbs and pleonastic
prepositions. In Proceedings of the Third
ACL-SIGSEMWorkshop on Prepositions,
pages 57?64, Trento.
Reichelt, Thorsten and Etienne Verleih. 2005.
B3D?a system for the description and
calculation of spatial prepositions. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 101?109, Colchester.
Resnik, Philip and Mona Diab. 2000.
Measuring verb similarity. In Proceedings of
the 22nd Annual Meeting of the Cognitive
Science Society (CogSci 2000),
pages 399?404, Philadelphia, PA.
Resnik, Philip and Marti A. Hearst. 1993.
Structural ambiguity and conceptual
relations. In Proceedings of the Workshop on
Very Large Corpora: Academic and Industrial
Perspectives, pages 58?64, Columbus, OH.
Richards, Barry, Inge Bethke, Jaap van der
Does, and Jon Oberlander. 1989. Temporal
Representation and Inference. Academic
Press, London.
Riloff, Ellen. 1995. Little words can make a
big difference for text classification. In
Proceedings of 18th International ACM-SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR?95),
pages 130?136, Seattle, WA.
Ro?hrer, Christian. 1977. How to define
temporal conjunctions. Linguistische
Berichte Braunschweig, 51:1?11.
Sablayrolles, Pierre. 1995. The semantics of
motion. In Proceedings of the 7th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL?95),
pages 281?283, Dublin.
Sag, Ivan, Timothy Baldwin, Francis Bond,
Ann Copestake, and Dan Flickinger.
2002. Multiword expressions: A pain
in the neck for NLP. In Proceedings of the
3rd International Conference on Intelligent
Text Processing and Computational
Linguistics (CICLing-2002), pages 1?15,
Mexico City.
Saint-Dizier, Patrick. 2005. An overview of
PrepNet: abstract notions, frames and
inferential patterns. In Proceedings of the
146
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 155?169,
Colchester.
Saint-Dizier, Patrick, editor. 2006a.
Computational Linguistics Dimensions of
Syntax and Semantics of Prepositions.
Kluwer Academic, Dordrecht,
Netherlands.
Saint-Dizier, Patrick. 2006b. PrepNet: a
multilingual lexical description of
prepositions. In Proceedings of the 5th
International Conference on Language
Resources and Evaluation (LREC 2006),
pages 877?885, Genoa.
Saint-Dizier, Patrick. 2008. Syntactic and
semantic frames in PrepNet. In Proceedings
of the 3rd International Joint Conference on
Natural Language Processing (IJCNLP-08),
pages 763?768, Hyderabad.
Saint-Dizier, Patrick and Gloria Vazquez.
2001. A compositional framework for
prepositions. In Proceedings of the Fourth
International Workshop on Computational
Semantics (IWCS-4), pages 165?179,
Tilburg.
Schulte im Walde, Sabine. 2004.
Identification, quantitative description,
and preliminary distributional analysis of
German particle verbs. In Proceedings of the
COLING Workshop on Enhancing and Using
Electronic Dictionaries, pages 85?88, Geneva.
Schuman, Jonathan and Sabine Bergler. 2006.
Postnominal prepositional phrase
attachment in proteomics. In Proceedings of
the HLT-NAACL 06 BioNLP Workshop on
Linking Natural Language Processing and
Biology, pages 82?89, New York.
Schu?tze, Carson. 1995. PP attachment and
argumenthood. In Carson T. Schu?tze,
Jennifer B. Ganger, and Kevin Broihier,
editors, Papers on Language Processing and
Acquisition, volume 26. MIT Working
Papers in Linguistics, Cambridge, MA,
pages 95?152.
Schwartz, Lee, Takako Aikawa, and Chris
Quirk. 2003. Disambiguation of English PP
attachment using multilingual aligned
data. In Proceedings of the Ninth Machine
Translation Summit (MT Summit IX), New
Orleans, LA. Available at www.amtaweb.
org/summit/MTSummit/FinalPapers/
39-Aikawa-final.pdf.
Shaked, Nava A. 1993. How do we count?
The problem of tagging phrasal verbs in
parts. In Proceedings of the 31st Annual
Meeting of the ACL, pages 289?291,
Columbus, OH.
Sharoff, Serge. 2004. What is at stake: A case
study of Russian expressions starting with
a preposition. In Proceedings of the ACL
2004 Workshop on Multiword Expressions:
Integrating Processing, pages 17?23,
Barcelona.
Sopena, Josep M., Agusti Lloberas, and
Joan L. Moliner. 1998. A connectionist
approach to prepositional phrase
attachment for real world texts. In
Proceedings of the 36th Annual Meeting of the
ACL and 17th International Conference on
Computational Linguistics
(COLING/ACL-98), pages 1233?1237,
Montreal.
Spa?rck Jones, Karen and Branimir Boguraev.
1987. A note on a study of cases.
Computational Linguistics, 13(1?2):65?8.
Spivey-Knowlton, Michael and Julie C.
Sedivy. 1995. Resolving attachment
ambiguities with multiple constraints.
Cognition, 55:227?267.
Srihari, Rohini, Cheng Niu, and Wei Li. 2000.
A hybrid approach for named entity and
sub-type tagging. In Proceedings of the 6th
Conference on Applied Natural Language
Processing (ANLP), pages 247?254, Seattle,
WA.
Stamou, Sofia, Kemal Oflazer, Karel Pala,
Dimitris Christoudoulakis, Dan Cristea,
Dan Tufi Svetla Koeva, George Totkov,
Dominique Dutoit, and Maria
Grigoriadou. 2002. BALKANET: A
multilingual semantic network for the
Balkan languages. In Proceedings of the
International Wordnet Conference,
pages 12?14, Mysore.
Stetina, Jiri and Makoto Nagao. 1997.
Corpus based PP attachment ambiguity
resolution with a semantic dictionary.
In Proceedings of the 5th Annual Workshop
on Very Large Corpora, pages 66?80,
Hong Kong.
Talmy, Len. 1985. Lexicalization patterns:
Semantic structure in lexical forms. In
Timothy Shopen, editor, Grammatical
Categories and the Lexicon. Cambridge
University Press, Cambridge, UK,
pages 57?149.
Talmy, Leonard. 1988. Force dynamics in
language and cognition. Cognitive Science,
12:49?100.
Tetreault, Joel R. and Martin Chodorow.
2008a. Native judgments of non-native
usage: Experiments in preposition error
detection. In Coling 2008: Proceedings of the
Workshop on Human Judgements in
Computational Linguistics, pages 24?32,
Manchester.
147
Computational Linguistics Volume 35, Number 2
Tetreault, Joel R. and Martin Chodorow.
2008b. The ups and downs of preposition
error detection in ESL writing. In
Proceedings of the 22nd International
Conference on Computational Linguistics
(COLING 2008), pages 865?872,
Manchester.
Tokunaga, Takenobu, Tomofumi Koyama,
and Suguru Saito. 2005. Meaning of
Japanese spatial nouns. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 93?100,
Colchester.
Toral, Antonio. 2005. DRAMNERI: A free
knowledge based tool to named entity
recognition. In Proceedings of the First Free
Software Technologies Conference,
pages 27?32, Corunna.
Toutanova, Kristina and Christoper D.
Manning. 2000. Enriching the knowledge
sources used in a maximum entropy
part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC-2000),
pages 63?70, Hong Kong.
Toutanova, Kristina, Christopher D.
Manning, and Andrew Y. Ng. 2004.
Learning random walk models for
inducing word dependency distributions.
In Proceedings of the 21st International
Conference on Machine Learning,
pages 815?822, Banff.
Toutanova, Kristina and Hisami Suzuki.
2007. Generating case markers in
machine translation. In Proceedings of
Human Language Technologies 2007: The
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 49?56, Rochester, NY.
Trawin?ski, Beata. 2003. Combinatorial
aspects of PPs headed by raising
prepositions. In Proceedings of the
ACL-SIGSEMWorkshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and
Applications, pages 157?168, Toulouse.
Trawin?ski, Beata. 2005. Preposition?pronoun
contraction in Polish. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 20?29,
Colchester.
Trawin?ski, Beata. 2006. A quantitative
approach to preposition-pronoun
contraction in Polish. In Proceedings of the
Third ACL-SIGSEMWorkshop on
Prepositions, pages 17?22, Trento.
Trawinski, Beata, Manfred Sailer, and
Jan-Philipp Soehn. 2006. Combinatorial
aspects of collocational prepositional
phrases. In Saint-Dizier (Saint-Dizier
2006a).
Trujillo, Arturo. 1995. Lexicalist Machine
Translation of Spatial Prepositions. Ph.D.
thesis, University of Cambridge.
Tseng, Jesse. 2004. Prepositions and
complement selection. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 11?19,
Colchester.
Tseng, Jesse L. 2000. The Representation and
Selection of Prepositions. Ph.D. thesis,
University of Edinburgh.
Tyler, Andrea and Vyvyan Evans. 2003. The
Semantics of English Prepositions: Spatial
Scenes, Embodied Meaning, and Cognition.
Cambridge University Press, Cambridge,
UK.
van der Beek, Leonoor. 2005. The extraction
of determinerless PPs. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 190?199,
Colchester.
van Herwijnen, Olga, Jacques Terken, Antal
van den Bosch, and Erwin Marsi. 2003.
Learning PP attachment for filtering
prosodic phrasing. In Proceedings of the 10th
Conference of the EACL (EACL 2003),
pages 139?146, Budapest.
Villada Moiro?n, Begon?a. 2005. Data-driven
identification of fixed expressions and their
modifiability. Ph.D. thesis, Alfa-Informatica,
University of Groningen.
Villavicencio, Aline. 2005. The availability of
verb-particle constructions in lexical
resources: How much is enough? Computer
Speech and Language, 19(4):415?432.
Villavicencio, Aline. 2006. Verb-particle
constructions in the World Wide Web. In
Saint-Dizier (2006a).
Volk, Martin. 2001. Exploiting the WWW as a
corpus to resolve PP attachment
ambiguities. In Proceedings of Corpus
Linguistics 2001, pages 601?606, Lancaster.
Volk, Martin. 2002. Combining unsupervised
and supervised methods for PP
attachment disambiguation. In Proceedings
of the 19th International Conference on
Computational Linguistics (COLING 2002),
pages 1?7, Taipei.
148
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Volk, Martin. 2003. German prepositions and
their kin. A survey with respect to the
resolution of PP attachment ambiguities.
In Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 77?85,
Toulouse.
Volk, Martin. 2006. How bad is the problem
of PP-attachment? A comparison of English,
German, and Swedish. In Proceedings of the
Third ACL-SIGSEMWorkshop on
Prepositions, pages 81?88, Trento.
Vossen, Piek, editor. 1998. EuroWordNet: A
Multilingual Database with Lexical Semantic
Networks. Kluwer Academic, Dordrecht,
Netherlands.
Wasow, Thomas. 2002. Postverbal Behavior.
CSLI Publications, Stanford, CA.
Whittemore, Greg and Kathleen Ferrara.
1990. Empirical study of predictive powers
of simple attachment schemes for
post-modifier prepositional phrases. In
Proceedings of the 28th Annual Meeting of the
ACL, pages 23?30, Pittsburgh, PA.
Wilks, Yorick, Xiuming Huang, and Dan
Fass. 1985. Syntax, preference and right
attachment. In Proceedings of the 9th
International Joint Conference on Artificial
Intelligence (IJCAI-85), pages 779?784, Los
Angeles, CA.
Wood, Frederick T. 1979. English Prepositional
Idioms. Macmillan, London.
Xu, Yilun Dianna and Norman I. Badler.
2000. Algorithms for generating motion
trajectories described by prepositions. In
Proceedings of Computer Animation 2000
(CA?00), pages 30?35, Washington, DC.
Ye, Patrick and Timothy Baldwin. 2006.
Semantic role labeling of prepositional
phrases. ACM Transactions on Asian
Language Information Processing,
5(3):228?244.
Ye, Patrick and Timothy Baldwin. 2007.
MELB-YB: Preposition sense
disambiguation using rich semantic
features. In Proceedings of the 4th
International Workshop on Semantic
Evaluations, pages 241?244, Prague.
Yeh, Alexander S. and Marc B. Vilain.
1998. Some properties of preposition
and subordinate conjunction
attachments. In Proceedings of the 36th
Annual Meeting of the ACL and 17th
International Conference on Computational
Linguistics: COLING/ACL-98,
pages 1436?1442, Montreal.
Yuret, Deniz. 2007. KU: Word sense
disambiguation by substitution. In
Proceedings of the 4th International Workshop
on Semantic Evaluations, pages 207?214,
Prague.
Zavrel, Jakub, Walter Daelemans, and
Jorn Veenstra. 1997. Resolving PP
attachment ambiguities with
memory-based learning. In Proceedings
of the Conference on Computational
Natural Language Learning (CoNLL-97),
pages 136?144, Madrid.
Zelinski-Wibbelt, Cornelia, editor. 1993.
The Semantics of Prepositions: From
Mental Processing to Natural Language
Processing. Mouton de Gruyter, Berlin,
Germany.
Zhao, Shaojun and Dekang Lin. 2004. A
nearest-neighbor method for resolving
PP-attachment ambiguity. In Proceedings of
the First International Joint Conference on
Natural Language Processing (IJCNLP-04),
pages 545?554, Hainan Island.
149

Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 1?8,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Statistically-Driven Alignment-Based Multiword Expression
Identification for Technical Domains
Helena de Medeiros Caseli?, Aline Villavicencio??, Andre? Machado?, Maria Jose? Finatto?
?Department of Computer Science, Federal University of Sa?o Carlos (Brazil)
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?Department of Computer Sciences, Bath University (UK)
?Institute of Language and Linguistics, Federal University of Rio Grande do Sul (Brazil)
helenacaseli@dc.ufscar.br, avillavicencio@inf.ufrgs.br,
ammachado@inf.ufrgs.br, mfinatto@terra.com.br
Abstract
Multiword Expressions (MWEs) are one
of the stumbling blocks for more precise
Natural Language Processing (NLP) sys-
tems. Particularly, the lack of coverage
of MWEs in resources can impact nega-
tively on the performance of tasks and ap-
plications, and can lead to loss of informa-
tion or communication errors. This is es-
pecially problematic in technical domains,
where a significant portion of the vocab-
ulary is composed of MWEs. This pa-
per investigates the use of a statistically-
driven alignment-based approach to the
identification of MWEs in technical cor-
pora. We look at the use of several sources
of data, including parallel corpora, using
English and Portuguese data from a corpus
of Pediatrics, and examining how a sec-
ond language can provide relevant cues for
this tasks. We report results obtained by
a combination of statistical measures and
linguistic information, and compare these
to the reported in the literature. Such an
approach to the (semi-)automatic identifi-
cation of MWEs can considerably speed
up lexicographic work, providing a more
targeted list of MWE candidates.
1 Introduction
A multiword expression (MWE) can be defined
as any word combination for which the syntac-
tic or semantic properties of the whole expres-
sion cannot be obtained from its parts (Sag et
al., 2002). Examples of MWEs are phrasal verbs
(break down, rely on), compounds (police car, cof-
fee machine), idioms (rock the boat, let the cat
out of the bag). They are very numerous in lan-
guages, as Biber et al (1999) note, accouting for
between 30% and 45% of spoken English and 21%
of academic prose, and for Jackendoff (1997) the
number of MWEs in a speaker?s lexicon is of the
same order of magnitude as the number of single
words. However, these estimates are likely to be
underestimates if we consider that for language
from a specific domain the specialized vocabulary
is going to consist largely of MWEs (global warm-
ing, protein sequencing) and new MWEs are con-
stantly appearing (weapons of mass destruction,
axis of evil).
Multiword expressions play an important role
in Natural Language Processing (NLP) applica-
tions, which should not only identify the MWEs
but also be able to deal with them when they are
found (Fazly and Stevenson, 2007). Failing to
identify MWEs may cause serious problems for
many NLP tasks, especially those envolving some
kind of semantic processing. For parsing, for in-
stance, Baldwin et al (2004), found that for a ran-
dom sample of 20,000 strings from the British Na-
tional Corpus (BNC) even with a broad-coverage
grammar for English (Flickinger, 2000) missing
MWEs accounted for 8% of total parsing errors.
Therefore, there is an enormous need for robust
(semi-)automated ways of acquiring lexical infor-
mation for MWEs (Villavicencio et al, 2007) that
can significantly extend the coverage of resources.
For example, one can more than double the num-
ber of verb-particle constructions (VPCs) entries
in a dictionary, such as the Alvey Natural Lan-
guage Tools (Carroll and Grover, 1989), just ex-
tracting VPCs from a corpus like the BNC (Bald-
win, 2005). Furthermore, as MWEs are language
dependent and culturally motivated, identifying
the adequate translation of MWE occurrences is an
important challenge for machine translation meth-
ods.
In this paper, we investigate experimentally the
use of an alignment-based approach for the iden-
tification of MWEs in technical corpora. We look
at the use of several sources of data, including par-
1
allel corpora, using English and Portuguese data
from a corpus of Pediatrics, and examining how
a second language can provide relevant cues for
this tasks. In this way, cost-effective tools for the
automatic alignment of texts can generate a list
of MWE candidates with their appropriate trans-
lations. Such an approach to the (semi-)automatic
identification of MWEs can considerably speed up
lexicographic work, providing a more targeted list
of MWE candidates and their translations, for the
construction of bilingual resources, and/or with
some semantic information for monolingual re-
sources.
The remainder of this paper is structured as fol-
lows. Section 2 briefly discusses MWEs and some
previous works on methods for automatically ex-
tracting them. Section 3 presents the resources
used while section 4 describes the methods pro-
posed to extract MWEs as a statistically-driven by-
product of an automatic word alignment process.
Section 5 presents the evaluation methodology and
analyses the results and section 6 finishes this pa-
per with some conclusions and proposals for fu-
ture work.
2 Related Work
The term Multiword Expression has been used to
describe a large number of distinct but related phe-
nomena, such as phrasal verbs (e.g. come along),
nominal compounds (e.g. frying pan), institution-
alised phrases (e.g. bread and butter), and many
others (Sag et al, 2002). They are very frequent in
everyday language and this is reflected in several
existing grammars and lexical resources, where
almost half of the entries are Multiword Expres-
sions.
However, due to their heterogeneous charac-
teristics, MWEs present a tough challenge for
both linguistic and computational work (Sag et
al., 2002). Some MWEs are fixed, and do not
present internal variation, such as ad hoc, while
others allow different degrees of internal vari-
ability and modification, such as touch a nerve
(touch/find a nerve) and spill beans (spill sev-
eral/musical/mountains of beans). In terms of se-
mantics, some MWEs are more opaque in their
meaning (e.g. to kick the bucket as to die), while
others have more transparent meanings that can be
inferred from the words in the MWE (e.g. eat up,
where the particle up adds a completive sense to
eat). Therefore, providing appropriate methods
for the automatic identification and treatment of
these phenomena is a real challenge for NLP sys-
tems.
A variety of approaches has been proposed for
automatically identifying MWEs, differing basi-
cally in terms of the type of MWE and lan-
guage to which they apply, and the sources of
information they use. Although some work on
MWEs is type independent (e.g. (Zhang et al,
2006; Villavicencio et al, 2007)), given the het-
erogeneity of MWEs much of the work looks in-
stead at specific types of MWE like collocations
(Pearce, 2002), compounds (Keller and Lapata,
2003) and VPCs (Baldwin, 2005; Villavicencio,
2005; Carlos Ramisch and Aline Villavicencio and
Leonardo Moura and Marco Idiart, 2008). Some
of these works concentrate on particular languages
(e.g. (Pearce, 2002; Baldwin, 2005) for English
and (Piao et al, 2006) for Chinese), but some
work has also benefitted from asymmetries in lan-
guages, using information from one language to
help deal with MWEs in the other (e.g. (na Vil-
lada Moiro?n and Tiedemann, 2006; Caseli et al,
2009)).
As basis for helping to determine whether a
given sequence of words is in fact an MWE (e.g.
ad hoc vs the small boy) some of these works em-
ploy linguistic knowledge for the task (Villavicen-
cio, 2005), while others employ statistical meth-
ods (Pearce, 2002; Evert and Krenn, 2005; Zhang
et al, 2006; Villavicencio et al, 2007) or combine
them with some kinds of linguistic information
such as syntactic and semantic properties (Bald-
win and Villavicencio, 2002; Van de Cruys and na
Villada Moiro?n, 2007) or automatic word align-
ment (na Villada Moiro?n and Tiedemann, 2006).
Statistical measures of association have been
commonly used for this task, as they can be demo-
cratically applied to any language and MWE type.
However, there is no consensus about which mea-
sure is best suited for identifying MWEs in gen-
eral. Villavicencio et al (2007) compared some of
these measures (mutual information, permutation
entropy and ?2) for the type-independent detec-
tion of MWEs and found that Mutual Information
seemed to differentiate MWEs from non-MWEs,
but the same was not true of ?2. In addition, Ev-
ert and Krenn (2005) found that for MWE iden-
tification the efficacy of a given measure depends
on factors like the type of MWEs being targeted
for identification, the domain and size of the cor-
2
pora used, and the amount of low-frequency data
excluded by adopting a threshold. Nonetheless,
Villavicencio et al (2007), discussing the influ-
ence of the corpus size and nature over the meth-
ods, found that these different measures have a
high level of agreement about MWEs, whether
in carefully constructed corpora or in more het-
erogeneous web-based ones. They also discuss
the results obtained from adopting approaches like
these for extending the coverage of resources, ar-
guing that grammar coverage can be significantly
increased if MWEs are properly identified and
treated (Villavicencio et al, 2007).
Among the methods that use additional infor-
mation along with statistics to extract MWE, the
one proposed by na Villada Moiro?n and Tiede-
mann (2006) seems to be the most similar to our
approach. The main difference between them is
the way in which word alignment is used in the
MWE extraction process. In this paper, the word
alignment is the basis for the MWE extraction
process while Villada Moiro?n and Tiedemann?s
method uses the alignment just for ranking the
MWE candidates which were extracted on the ba-
sis of association measures (log-likelihood and
salience) and head dependence heuristic (in parsed
data).
Our approach, as described in details by Caseli
et al (2009), also follows to some extent that
of Zhang et al (2006), as missing lexical en-
tries for MWEs and related constructions are de-
tected via error mining methods, and this paper fo-
cuses on the extraction of generic MWEs as a by-
product of an automatic word alignment. Another
related work is the automatic detection of non-
compositional compounds (NCC) by Melamed
(1997) in which NCCs are identified by analyz-
ing statistical translation models trained in a huge
corpus by a time-demanding process.
Given this context, our approach proposes
the use of alignment techniques for identifying
MWEs, looking at sequences detected by the
aligner as containing more than one word, which
form the MWE candidates. As a result, sequences
of two or more consecutive source words are
treated as MWE candidates regardless of whether
they are translated as one or more target words.
3 The Corpus and Reference Lists
The Corpus of Pediatrics used in these experi-
ments contains 283 texts in Portuguese with a total
of 785,448 words, extracted from the Jornal de Pe-
diatria. From this corpus, the Pediatrics Glossary,
a reference list containing multiword terms and re-
curring expressions, was semi-automatically con-
structed, and manually checked.1 The primary aim
of the Pediatrics Glossary, as an online resource
for long-distance education, was to train, qualify
and support translation students on the domain of
pediatrics texts.
The Pediatrics Glossary was built from the
36,741 ngrams that occurred at least 5 times in the
corpus. These were automatically cleaned or re-
moved using some POS tag patterns (e.g. remov-
ing prepositions from terms that began or ended
with them). In addition, if an ngram was part of a
larger ngram, only the latter appeared in the Glos-
sary, as is the case of aleitamento materno (mater-
nal breastfeeding) which is excluded as it is con-
tained in aleitamento materno exclusivo (exclusive
maternal breastfeeding). This post-processing re-
sulted in 3,645 ngrams, which were manually
checked by translation students, and resulted in
2,407 terms, with 1,421 bigrams, 730 trigrams and
339 ngrams with n larger than 3 (not considered in
the experiments presented in this paper).
4 Statistically-Driven and
Alignment-Based methods
4.1 Statistically-Driven method
Statistical measures of association have been
widely employed in the identification of MWEs.
The idea behind their use is that they are an in-
expensive language and type independent means
of detecting recurrent patterns. As Firth famously
said a word is characterized by the company it
keeps and since we expect the component words
of an MWE to occur frequently together, then
these measures can give an indication of MWE-
ness. In this way, if a group of words co-occurs
with significantly high frequency when compared
to the frequencies of the individual words, then
they may form an MWE. Indeed, measures such
as Pointwise Mutual Information (PMI), Mutual
Information (MI), ?2, log-likelihood (Press et al,
1992) and others have been employed for this
task, and some of them seem to provide more ac-
curate predictions of MWEness than others. In
fact, in a comparison of some measures for the
type-independent detection of MWEs, MI seemed
1Available in the TEXTQUIM/UFRGS website: http:
//www.ufrgs.br/textquim
3
to differentiate MWEs from non-MWEs, but the
same was not true of ?2 (Villavicencio et al,
2007). In this work we use two commonly em-
ployed measures for this task: PMI and MI,
as implemented in the Ngram Statistics Package
(Banerjee and Pedersen, 2003).
From the Portuguese portion of the Corpus of
Pediatrics, 196,105 bigram and 362,663 trigram
MWE candidates were generated, after filtering
ngrams containing punctuation and numbers. In
order to evaluate how these methods perform with-
out any linguistic filtering, the only threshold em-
ployed was a frequency cut-off of 2 occurrences,
resulting in 64,839 bigrams and 54,548 trigrams.
Each of the four measures were then calculated for
these ngrams, and we ranked each n-gram accord-
ing to each of these measures. The average of all
the rankings is used as the combined measure of
the MWE candidates.
4.2 Alignment-Based method
The second of the MWE extraction approaches
to be investigated in this paper is the alignment-
based method. The automatic word alignment of
two parallel texts ? a text written in one (source)
language and its translation to another (target) lan-
guage ? is the process of searching for correspon-
dences between source and target words and se-
quences of words. For each word in a source sen-
tence equivalences in the parallel target sentence
are looked for. Therefore, taking into account a
word alignment between a source word sequence
S (S = s1 . . . sn with n ? 2) and a target word
sequence T (T = t1 . . . tm with m ? 1), that
is S ? T , the alignmet-based MWE extracion
method assumes that: (a) S and T share some se-
mantic features, and (b) S may be a MWE.
In other words, the alignment-based MWE ex-
traction method states that the sequence S will be
a MWE candidate if it is aligned with a sequence
T composed of one or more words (a n : m align-
ment with n ? 2 and m ? 1). For example,
the sequence of two Portuguese words aleitamento
materno ? which occurs 202 times in the cor-
pus used in our experiments ? is a MWE can-
didate because these two words were joined to be
aligned 184 times with the word breastfeeding (a
2 : 1 alignment), 8 times with the word breast-
fed (a 2 : 1 alignment), 2 times with breastfeeding
practice (a 2 : 2 alignment) and so on.
Thus, notice that the alignment-based MWE ex-
traction method does not rely on the conceptual
asymmetries between languages since it does not
expect that a source sequence of words be aligned
with a single target word. The method looks
for the sequences of source words that are fre-
quently joined together during the alignment de-
spite the number of target words involved. These
features indicate that the method priorizes preci-
sion in spite of recall.
It is also important to say that although the se-
quences of source and target words resemble the
phrases used in the phrase-based statistical ma-
chine translation (SMT), they are indeed a re-
finement of them. More specifically, although
both approaches rely on word alignments per-
formed by GIZA++2 (Och and Ney, 2000), in
the alignment-based approach not all sequences of
words are considered as phrases (and MWE can-
didates) but just those with an alignment n : m
(n >= 2) with a target sequence. To confirm
this assumption a phrase-based SMT system was
trained with the same corpus used in our exper-
iments and the number of phrases extracted fol-
lowing both approaches were compared. While
the SMT extracted 819,208 source phrases, our
alignment-based approach (without applying any
part-of-speech or frequency filter) extracted only
34,277. These results show that the alignment-
based approach refines in some way the phrases
of SMT systems.
In this paper, we investigate experimentally
whether MWEs can be identified as a by-product
of the automatic word alignment of parallel texts.
We focus on Portuguese MWEs from the Corpus
of Pediatrics and the evaluation is performed us-
ing the bigrams and trigrams from the Pediatrics
Glossary as gold standard.
To perform the extraction of MWE candi-
dates following the alignment-based approach,
first, the original corpus had to be sentence and
word aligned and Part-of-Speech (POS) tagged.
For these preprocessing steps were used, re-
spectively: a version of the Translation Cor-
pus Aligner (TCA) (Hofland, 1996), the statisti-
cal word aligner GIZA++ (Och and Ney, 2000)
and the morphological analysers and POS taggers
from Apertium3 (Armentano-Oller et al, 2006).
2GIZA++ is a well-known statistical word aligner that can
be found at: http://www.fjoch.com/GIZA++.html
3Apertium is an open-source machine translation en-
gine and toolbox available at: http://www.apertium.
org.
4
From the preprocessed corpus, the MWE candi-
dates are extracted as those in which two or more
words have the same alignment, that is, they are
linked to the same target unit. This initial list of
MWE candidates is, then, filtered to remove those
candidates that: (a) match some sequences of POS
tags or words (patterns) defined in previous exper-
iments (Caseli et al, 2009) or (b) whose frequency
is below a certain threshold. The remaining units
in the candidate list are considered to be MWEs.
Several filtering patterns and minimum fre-
quency thresholds were tested and three of them
are presented in details here. The first one (F1)
is the same used during the manual building of
the reference lists of MWEs: (a) patterns begin-
ning with Article + Noun and beginning or finish-
ing with verbs and (b) with a minimum frequency
threshold of 5.
The second one (F2) is the same used in the
(Caseli et al, 2009), mainly: (a) patterns begin-
ning with determiner, auxiliary verb, pronoun, ad-
verb, conjunction and surface forms such as those
of the verb to be (are, is, was, were), relatives
(that, what, when, which, who, why) and prepo-
sitions (from, to, of ) and (b) with a minimum fre-
quency threshold of 2.
And the third one (F3) is the same as (Caseli et
al., 2009) plus: (a) patterns beginning or finishing
with determiner, adverb, conjunction, preposition,
verb, pronoun and numeral and (b) with a mini-
mum frequency threshold of 2.
5 Experiments and Results
Table 1 shows the top 5 and the bottom 5 ranked
candidates returned by PMI and the alignment-
based approach. Although some of the results are
good, especially the top candidates, there is still
considerable noise among the candidates, as for
instance jogar video game (lit. play video game).
From table 1 it is also possible to notice that the
alignment-based approach indeed extracts Pedi-
atrics terms such as aleitamento materno (breast-
feeding) and also other possible MWE that are not
Pediatrics terms such as estados unidos (United
States).
In table 2 we show the precision (number of
correct candidates among the proposed ones), re-
call (number of correct candidates among those in
reference lists) and F-measure ((2 ? precision ?
recall)/(precision + recall)) figures for the as-
sociation measures using all the candidates (on the
PMI alignment-based
Online Mendelian Inheritance faixa eta?ria
Beta Technology Incorporated aleitamento materno
Lange Beta Technology estados unidos
Oxido Nitrico Inalatorio hipertensa?o arterial
jogar video game leite materno
... ...
e um de couro cabeludo
e a do bloqueio lact??feros
se que de emocional anatomia
e a da neonato a termo
e de nao duplas ma?es bebe?s
Table 1: Top 5 and Bottom 5 MWE candidates
ranked by PMI and alignment-based approach
pt MWE candidates PMI MI
# proposed bigrams 64,839 64,839
# correct MWEs 1403 1403
precision 2.16% 2.16%
recall 98.73% 98.73%
F 4.23% 4.23%
# proposed trigrams 54,548 54,548
# correct MWEs 701 701
precision 1.29% 1.29%
recall 96.03% 96.03%
F 2.55% 2.55%
# proposed bigrams 1,421 1,421
# correct MWEs 155 261
precision 10.91% 18.37%
recall 10.91% 18.37%
F 10.91% 18.37%
# proposed trigrams 730 730
# correct MWEs 44 20
precision 6.03% 2.74%
recall 6.03% 2.74%
F 6.03% 2.74%
Table 2: Evaluation of MWE candidates - PMI and
MI
first half of the table) and using the top 1,421 bi-
gram and 730 trigram candidates (on the second
half). From these latter results, we can see that the
top candidates produced by these measures do not
agree with the Pediatrics Glossary, since there are
only at most 18.37% bigram and 6.03% trigram
MWEs among the top candidates, as ranked by
MI and PMI respectively. Interestingly, MI had a
better performance for bigrams while for trigrams
PMI performed better.
On the other hand, looking at the alignment-
based method, 34,277 pt MWE candidates were
extracted and Table 3 sumarizes the number of
candidates filtered following the three filters de-
scribed in 4.2: F1, F2 and F3.
To evaluate the efficacy of the alignment-based
method in identifying multiword terms of Pedi-
atrics, an automatic comparison was performed
using the Pediatrics Glossary. In this auto-
5
pt MWE candidates F1 F2 F3
# filtered by POS patterns 24,996 21,544 32,644
# filtered by frequency 9,012 11,855 1,442
# final Set 269 878 191
Table 3: Number of pt MWE candidates filtered
in the alignment-based approach
pt MWE candidates F1 F2 F3
# proposed bigrams 250 754 169
# correct MWEs 48 95 65
precision 19.20% 12.60% 38.46%
recall 3.38% 6.69% 4.57%
F 5.75% 8.74% 8.18%
# proposed trigrams 19 110 20
# correct MWEs 1 9 4
precision 5.26% 8.18% 20.00%
recall 0.14% 1.23% 0.55%
F 0.27% 2.14% 1.07%
# proposed bi/trigrams 269 864 189
# correct MWEs 49 104 69
precision 18.22% 12.04% 36,51%
recall 2.28% 4.83% 3.21%
F 4.05% 6.90% 5.90%
Table 4: Evaluation of MWE candidates
matic comparision we considered the final lists of
MWEs candidates generated by each filter in table
3. The number of matching entries and the values
for precision, recall and F-measure are showed in
table 4.
The different values of extracted MWEs (in ta-
ble 3) and evaluated ones (in table 4) are due to
the restriction of considering only bigrams and tri-
grams in the Pediatrics Glossary. Then, longer
MWEs ? such as doenc?a arterial coronariana
prematura (premature coronary artery disease)
and pequenos para idade gestacional (small for
gestational age) ? extracted by the alignment-
based method are not being considered at the mo-
ment.
After the automatic comparison using the Pedi-
atrics Glossary, an analysis by human experts was
performed on one of the derived lists ? that with
the best precision values so far (from filter F3).
The human analysis was necessary since, as stated
in (Caseli et al, 2009), the coverage of reference
lists may be low, and it is likely that a lot of MWE
candidates that were not found in the Pediatrics
Glossary are nonetheless true MWEs. In this pa-
per only the pt MWE candidates extracted using
filter F3 (as described in section 4.2) were manu-
ally evaluated.
From the 191 pt MWE candidates extracted af-
ter F3, 69 candidates (36.1% of the total amount)
were found in the bigrams or trigrams in the
Glossary (see table 4). Then, the remaining 122
candidates (63.9%) were analysed by two native-
speakers human judges, who classified each of the
122 candidates as true, if it is a multiword expres-
sion, or false, otherwise independently of being
a Pediatrics term. For the judges, a sequence of
words was considered a MWE mainly if it was:
(1) a proper name or (2) a sequence of words for
which the meaning cannot be obtained by com-
pounding the meanings of its component words.
The judgments of both judges were compared
and a disagreement of approximately 12% on mul-
tiwords was verified. This disagreement was also
measured by the kappa (K) measure (Carletta,
1996), with k = 0.73, which does not prevent
conclusions to be drawn. According to Carletta
(1996), among other authors, a value of k between
0.67 and 0.8 indicates a good agreement.
In order to calculate the percentage of true can-
didates among the 122, two approaches can be
followed, depending on what criteria one wants
to emphasize: precision or coverage (not recall
because we are not calculating regarding a refer-
ence list). To emphasize the precision, one should
consider as genuine MWEs only those candidates
classified as true by both judges, on the other hand,
to emphasize the coverage, one should consider
also those candidates classified as true by just one
of them. So, from 191 MWE candidates, 126
(65.97%) were classified as true by both judges
and 145 (75.92%) by at least one of them.
6 Conclusions and Future Work
MWEs are a complex and heterogeneous set of
phenomena that defy attempts to capture them
fully, but due to their role in communication they
need to be properly accounted for in NLP applica-
tions and tasks.
In this paper we investigated the identifica-
tion of MWEs from technical domain, test-
ing statistically-driven and alignment-based ap-
proaches for identifying MWEs from a Pediatrics
parallel corpus. The alignment-based method gen-
erates a targeted precision-oriented list of MWE
candidates, while the statistical methods produce
recall-oriented results at the expense of precision.
Therefore, the combination of these methods can
produce a set of MWE candidates that is both more
precise than the latter and has more coverage than
the former. This can significantly speed up lex-
icographic work. Moreover, the results obtained
6
show that in comparison with the manual extrac-
tion of MWEs, this approach can provide also a
general set of MWE candidates in addition to the
manually selected technical terms.
Using the alignment-based extraction method
we notice that it is possible to extract MWEs that
are Pediatrics terms with a precision of 38% for
bigrams and 20% for trigrams, but with very low
recall since only the MWEs in the Pediatrics Glos-
sary were considered correct. However, after a
manual analysis carried out by two native speakers
of Portuguese we found that the percentage of true
MWEs considered by both or at least one of them
were, respectively, 65.97% and 75.92%. This was
a significative improvement but it is important to
say that, in this manual analysis, the human ex-
perts classified the MWEs as true independently of
them being Pediatrics terms. So, as future work we
intend to carry out a more carefull analysis with
experts in Pediatrics to evaluate how many MWEs
candidates are also Pediatrics terms.
In addition, we plan to investigate a weighted
combination of these methods, favouring those
that have better precision. Finally, we also in-
tend to apply the results obtained in to the semi-
automatic construction of ontologies.
Acknowledgments
We would like to thank the TEXTQUIM/UFRGS
group for making the Corpus of Pediatrics and Pe-
diatrics Glossary available to us. We also thank the
financial support of FAPESP in bulding the paral-
lel corpus. This research has been partly funded
by the FINEP project COMUNICA.
References
Carme Armentano-Oller, Rafael C. Carrasco, Anto-
nio M. Corb??-Bellot, Mikel L. Forcada, Mireia
Ginest??-Rosell, Sergio Ortiz-Rojas, Juan Anto-
nio Pe?rez-Ortiz, Gema Ram??rez-Sa?nchez, Felipe
Sa?nchez-Mart??nez, and Miriam A. Scalco. 2006.
Open-source Portuguese-Spanish machine transla-
tion. In R. Vieira, P. Quaresma, M.G.V. Nunes, N.J.
Mamede, C. Oliveira, and M.C. Dias, editors, Pro-
ceedings of the 7th International Workshop on Com-
putational Processing of Written and Spoken Por-
tuguese, (PROPOR 2006), volume 3960 of Lecture
Notes in Computer Science, pages 50?59. Springer-
Verlag, May.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the Unextractable: A Case Study on Verb-
particles. In Proceedings of the 6th Conference on
Natural Language Learning (CoNLL-2002), pages
98?104, Taipei, Taiwan.
Timothy Baldwin, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephan Oepen. 2004. Road-testing
the English Resource Grammar over the British Na-
tional Corpus. In Proceedings of the Fourth In-
ternational Conference on Language Resources and
Evaluation (LREC 2004), pages 2047?2050, Lisbon,
Portugal.
Timothy Baldwin. 2005. The deep lexical acquisition
of English verb-particles. Computer Speech and
Language, Special Issue on Multiword Expressions,
19(4):398?414.
Satanjeev Banerjee and Ted Pedersen. 2003. The De-
sign, Implementation and Use of the Ngram Statis-
tics Package. In In Proceedings of the Fourth Inter-
national Conference on Intelligent Text Processing
and Computational Linguistics, pages 370?381.
Douglas Biber, Stig Johansson, Geoffrey Leech, Susan
Conrad, and Edward Finegan. 1999. Grammar of
Spoken and Written English. Longman, Harlow.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistics. Computational
Linguistics, 22(2):249?254.
Carlos Ramisch and Aline Villavicencio and Leonardo
Moura and Marco Idiart. 2008. Picking them
up and Figuring them out: Verb-Particle Construc-
tions, Noise and Idiomaticity. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL 2008), pages 49?56.
John Carroll and Claire Grover. 1989. The derivation
of a large computational lexicon of English from
LDOCE. In Bran Boguraev and Ted Briscoe, editors,
Computational Lexicography for Natural Language
Processing, pages 117?134. Longman, Harlow, UK.
Helena M. Caseli, Carlos Ramisch, Maria G. V. Nunes,
and Aline Villavicencio. 2009. Alignment-based
extraction of multiword expressions. Language Re-
sources and Evaluation, to appear.
Stefan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statis-
tical association measures. Computer Speech and
Language, 19(4):450?466.
Afsaneh Fazly and Suzanne Stevenson. 2007. Distin-
guishing Subtypes of Multiword Expressions Using
Linguistically-Motivated Statistical Measures. In
Proceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pages 9?16, Prague,
June.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
7
Knut Hofland. 1996. A program for aligning English
and Norwegian sentences. In S. Hockey, N. Ide,
and G. Perissinotto, editors, Research in Humanities
Computing, pages 165?178, Oxford. Oxford Univer-
sity Press.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?59.
Frank Keller and Mirella Lapata. 2003. Using the Web
to Obtain Frequencies for Unseen Bigrams. Compu-
tational Linguistics, 29(3):459?484.
I. Dan Melamed. 1997. Automatic Discovery of
Non-Compositional Compounds in Parallel Data. In
eprint arXiv:cmp-lg/9706027, pages 6027?+, June.
Bego na Villada Moiro?n and Jorg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the Workshop
on Multi-word-expressions in a Multilingual Context
(EACL-2006), pages 33?40, Trento, Italy.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the ACL, pages 440?447,
Hong Kong, China, October.
Darren Pearce. 2002. A Comparative Evaluation of
Collocation Extraction Techniques. In Proceedings
of the Third International Conference on Language
Resources and Evaluation, pages 1?7, Las Palmas,
Canary Islands, Spain.
Scott S. L. Piao, Guangfan Sun, Paul Rayson, and
Qi Yuan. 2006. Automatic Extraction of Chi-
nese Multiword Expressions with a Statistical Tool.
In Proceedings of the Workshop on Multi-word-
expressions in a Multilingual Context (EACL-2006),
pages 17?24, Trento, Italy, April.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1992. Numerical
Recipes in C: The Art of Scientific Computing. Sec-
ond edition. Cambridge University Press.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In Pro-
ceedings of the Third International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing-2002), volume 2276 of (Lecture
Notes in Computer Science), pages 1?15, London,
UK. Springer-Verlag.
Tim Van de Cruys and Bego na Villada Moiro?n. 2007.
Semantics-based Multiword Expression Extraction.
In Proceedings of the Workshop on A Broader Pre-
spective on Multiword Expressions, pages 25?32,
Prague, June.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
Evaluation of Automatically Acquired Multiword
Expressions for Grammar Engineering. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1034?1043, Prague, June.
Aline Villavicencio. 2005. The Availability of Verb-
Particle Constructions in Lexical Resources: How
Much is Enough? Journal of Computer Speech and
Language Processing, 19(4):415?432.
Yi Zhang, Valia Kordoni, Aline Villavicencio, and
Marco Idiart. 2006. Automated Multiword Ex-
pression Prediction for Grammar Engineering. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties, pages 36?44, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
8
Lexical Encoding of MWEs
Aline Villavicencio
Department of Language and Linguistics
University of Essex
Wivenhoe Park
Colchester, CO4 3SQ, UK
and
Computer Laboratory, University of Cambridge
avill@essex.ac.uk
Ann Copestake, Benjamin Waldron, Fabre Lambeau
Computer Laboratory
University of Cambridge
William Gates Building, JJ Thomson Avenue
Cambridge, CB3 0FD, UK
 
aac10,bmw20,faml2  @cl.cam.ac.uk
Abstract
Multiword Expressions present a challenge for lan-
guage technology, given their flexible nature. Each
type of multiword expression has its own charac-
teristics, and providing a uniform lexical encoding
for them is a difficult task to undertake. Nonethe-
less, in this paper we present an architecture for the
lexical encoding of these expressions in a database,
that takes into account their flexibility. This encod-
ing extends in a straightforward manner the one re-
quired for simplex (single) words, and maximises
the information contained for them in the descrip-
tion of multiwords.
1 Introduction
Multiword Expressions (MWEs) can be defined as
idiosyncratic interpretations that cross word bound-
aries (or spaces) (from Sag et al (2002). They
comprise a wide-range of distinct but related phe-
nomena like idioms, phrasal verbs, noun-noun com-
pounds and many others, that due to their flexible
nature, are considered to be a challenge for many
areas of current language technology. Even though
some MWEs are fixed, and do not present inter-
nal variation, such as ad hoc, others are much more
flexible and allow different degrees of internal vari-
ability and modification, as, for instance, touch a
nerve (touch/find a nerve) and spill beans (spill sev-
eral/musical/mountains of beans). In terms of se-
mantics, some MWEs are opaque and their seman-
tics cannot be straightforwardly inferred from the
meanings of the component words (e.g. to kick the
bucket as to die). In other cases the meaning is more
transparent and can be inferred from the words in
the MWE (e.g. eat up, where the particle up adds a
completive sense to eat).
Given the flexibility and variation in form of
MWEs and the complex interrelations that may be
found between their components, an encoding that
treats them as invariant strings (a words with spaces
approach), will not be adequate to fully describe any
such expression appropriately with the exception of
the simplest fixed cases such as ad hoc ((Sag et al,
2002), (Calzolari et al, 2002)). Different strate-
gies for encoding MWEs have been employed by
different lexical resources with varying degrees of
success, depending on the type of MWE. One case
is the Alvey Tools Lexicon (Carroll and Grover,
1989), which has a good coverage of phrasal verbs,
providing extensive information about their syntac-
tic aspects (variation in word order, subcategorisa-
tion, etc), but it does not distinguish compositional
from non-compositional entries neither does it spec-
ify entries that can be productively formed. Word-
Net, on the other hand, covers a large number of
MWEs (Fellbaum, 1998), but does not provide in-
formation about their variability. Neither of these
resources covers idioms. The challenge in design-
ing adequate lexical resources for MWEs, is to en-
sure that the variability and the extra dimensions re-
quired by the different types of MWE can be cap-
tured. Such a move is called for by Calzolari et al
(2002) and Copestake et al (2002). Calzolari et al
(2002) discuss these problems while attempting to
establish the standards for MWE description in the
context of multilingual lexical resources. Their fo-
cus is on MWEs that are productive and that present
regularities that can be generalised and applied to
other classes of words that have similar properties.
Copestake et al (2002) present an initial schema for
MWE description and we build on these ideas here,
by proposing an architecture for a lexical encoding
of MWEs, which allows for a unified treatment of
different kinds of MWE.
In what follows, we start by laying out the min-
imal encoding needed for simplex (single) words.
Then, we analyse two different types of MWE (id-
ioms and verb-particle constructions), and discuss
their requirements for a lexical encoding. Given
these requirements, we present a possible encoding
for MWEs, that uniformly captures different types
of expressions. This database encoding minimises
the amount of information that needs to be specified
for MWE entries, by maximising the information
that can be obtained from simplex words, while re-
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 80-87
quiring only minimal modification to the encoding
used for simplex words. We finish with some dis-
cussion and conclusions.
2 Simplex Entries
Simplex entries, in this context, refer to simple stan-
dalone words that are defined independently of oth-
ers, and form the bulk of most lexical resources.
For these entries, it is necessary to define at least
their orthography, and syntactic and semantic char-
acteristics, but more information can also be spec-
ified, such as particular dialect, register, and so on,
and table 1 shows one such encoding. In this min-
imal encoding a lexical entry has an identifier (to
uniquely distinguish between the different entries
defining different combinations of parts-of-speech
and senses for a given word), the word?s orthog-
raphy, grammatical (syntactic and semantic) type
and predicate name.1 In the case of this example,
the identifier is like tv 1, which is an entry for the
verb like, with type trans-verb for transitive verbs,
and predicate name like v rel. A type like trans-
verb embodies the constraints defined for a given
construction (in this case transitive verbs), in a par-
ticular grammar, and these vary from grammar to
grammar. Thus, these words can be expanded into
full feature structures during processing according
to the constraints defined in a specific grammar.
Table 1: LINGO ERG lexical database encoding
identifier orthography type predicate
like tv 1 like trans-verb like v rel
This table shows a minimal encoding for simplex
words, but it can serve as basis for a more complete
one. That is the case of the LinGO ERG (Copes-
take and Flickinger, 2000) lexicon, which adopts for
its database version, a compatible but more com-
plex encoding which is successfully used to de-
scribe simplex words (Copestake et al, 2004). In
the next sections, we investigate what would be nec-
essary for extending this encoding for successfully
capturing MWEs.
3 Idioms
Idioms constitute a complex case of MWEs, al-
lowing a great deal of variation. Some idioms are
1The identifier and semantic relation names follow the stan-
dard adopted by the LinGO ERG (Copestake and Flickinger,
2000), while the grammatical type names are also compatible
with it.
very flexible and can be passivised, topicalised, in-
ternally modified, and/or have optional elements
(e.g. spill beans in those beans were spilt, users
spilt password beans and judges spill their musical
beans), while others are more inflexible and only ac-
cept morphological inflection (e.g. kick/kicks/kicked
the bucket).
In order to verify empirically the possible space
of variation that idioms allow, we analysed a sam-
ple of some of the most frequent idioms in English.
This sample was used for determining the require-
ments that an encoding needs in order to provide the
means of adequately capturing idioms.
The Collins Cobuild Dictionary of Idioms lists
approximately 4,400 idioms in English, and 750 of
them are marked as the most frequent listed.2 From
these, 100 idioms were randomly selected and anal-
ysed as described by Villavicencio and Copestake
(2002).
A great part of the idioms in this sample seems
to form natural classes that follow similar patterns
(e.g. the class of verb-object idioms, where an id-
iom consists of a specific verb that takes a specific
object such as rock boat and spill beans). The re-
maining idioms, on the other hand, cannot so eas-
ily be grouped together, forming a large tail of
classes often containing only one or two idioms (e.g.
thumbs up and quote, unquote).
Most of the idioms in this sample present a
large degree of variability, especially in terms of
their syntax, also allowing variable elements (throw
SOMEONE to the lions), and optional ones (in a
(tight) corner). The type of variation that these
MWEs allow seems to be linked to their decom-
posability (Nunberg et al, 1994) in the sense that
many idioms seem to be compositional if we con-
sider that some of their component words have non-
standard meanings. Then, using compositional pro-
cesses, the meaning of an idiom can be derived from
the meanings of its elements. Thus, in these idioms,
referred to as semantically decomposable idioms,
a meaning can be assigned to individual words (even
if some of them are non-standard meanings) from
where the meaning of the idiom can be composi-
tionally constructed. One example is spill the beans,
where if spill is paraphrased as reveal and beans as
secrets, the idiom can be interpreted as reveal se-
crets. On the other hand, an idiom like to kick the
bucket, meaning to die, according to this approach
is non decomposable.
When semantic decomposability is used as ba-
sis for the classification, the majority of the idioms
2These idioms have at least one occurrence in every 2 mil-
lion words of the corpus employed to build this dictionary.
in this sample is classified as decomposable, and a
few cases as non-decomposable. The decompos-
able cases correspond to the flexible idioms, and
the non-decomposable to the fixed ones, provid-
ing a clear cut division for their treatment. For the
non-decomposable idioms, a treatment of idioms as
words with space can be adopted similar to that of
simplex words, where in a single entry the orthog-
raphy of the component words is specified, along
with the syntactic and semantic type of the idiom,
and a corresponding predicate name. In addition,
for the cases that allow morphological inflection, it
is also important to define which of the elements
of the MWE can be inflected. In this case, an id-
iom like kick the bucket, is given the type of a nor-
mal intransitive verb, except that it is composed of
more than one word, and only the verb can be in-
flected (e.g. kick/kicked/kicks the bucket,...). Conse-
quently, an encoding for non-decomposable idioms
needs to allow the definition of several orthographic
elements for an entry, as well as the specification of
the entry?s orthographic element that allows inflec-
tion.
In order to capture the flexibility of decompos-
able idioms, a treatment using normal composi-
tional processes can be employed as discussed by
Copestake (1994). In this approach, each idiomatic
component of an idiom could be defined as a sep-
arate entry similar to that of a simplex word, ex-
cept that it would also be possible to specify a para-
phrase for its meaning. In the case of spill beans, it
would mean defining an entry for the idiomatic spill,
which can be paraphrased as reveal and another for
the idiomatic beans paraphrased as secrets. More-
over, as an idiomatic entry for a word may share
many of the properties of (one of) the word?s non-
idiomatic entries (sometimes differing from the lat-
ter only in terms of their semantics), it is important
to define also for each idiomatic element a corre-
sponding non-idiomatic one, from which many as-
pects will be inherited by default. For example, in
an idiom such as spill beans, the idiomatic entry for
spill shares with the non-idiomatic entry the mor-
phology (spilled or spilt) and the syntax (as a transi-
tive verb), and so does the idiomatic beans with the
non-idiomatic one. In addition, as there is a vari-
ability in the status of the words that form MWEs,
with some words having a more literal interpretation
and others a more idiomatic one, only the idiomatic
words need to have separate entries defined. For ex-
ample in the case of the idiom pull the plug, pull
can be interpreted as contributing one of its non-
idiomatic senses (that of removing), while plug has
an idiomatic interpretation (that can be understood
as meaning support). Thus, only an idiomatic en-
try (like that for plug) needs to be defined, while the
contribution of a non-idiomatic entry (like that for
pull) to the idiom comes from the standard entry for
that word.
Having idiomatic and non-idiomatic entries avail-
able for use in idioms is just the first step in being
able to capture this type of MWE. For a precise en-
coding of idioms, it is also necessary to define a very
specific context of use for the idiomatic entries, to
avoid the possibility of overgeneration. Thus, the
verb spill has its idiomatic meaning of reveal only in
the context of spilt the beans but not otherwise (e.g.
in spill the water). The definition of these idiomatic
contexts is important to ensure that idiomatic entries
are used only in the context of the idiom, and that
outside the idiom these entries are disallowed. Con-
versely, it is important to be able to define for each
idiom, all the elements that need to be present for
the idiomatic interpretation to be available. An id-
iom is only going to be understood as such if all of
its obligatory components are present. In addition, it
is necessary to ensure that the appropriate relation-
ship among the components of an idiom is found,
for the idiomatic meaning to be available, in order
to avoid the case of false positives, where all the
elements of an idiom are found, but not with the rel-
evant interrelations. Thus, a sentence like He threw
the cat among the pigeons has a possible idiomatic
interpretation available, but this interpretation is not
available in a sentence like He held the cat and she
threw the bread among the pigeons, even though it
has all the obligatory elements for the idiom (throw,
cat, among, pigeons), because cat did not occur as
a semantic argument (the agent) of throw. Many
idioms also present some slight variation in their
components, accepting any one of a restricted set of
words, as for example on home ground and on home
turf. Each of these possibilities corresponds to the
same idiom realised in a slightly different way, but
which nonetheless has the same meaning. Some id-
ioms have also optional elements (such as in a cor-
ner and in a tight corner), and for these it is nec-
essary to indicate which are the optional and which
are the obligatory elements.
Idioms also present variation in the number of
(obligatory) components they have, with some as
short as two words (e.g. pull strings) to others as
long as 10 words (e.g. six of one and half a dozen
of the other) or more, but with no lower and upper
bound, or standard size. Consequently, an adequate
treatment of idioms cannot assume that idioms will
have a specific pre-defined size, but instead it needs
to be able to deal with this variability.
4 Verb Particle Constructions
Verb Particle Constructions (VPCs) are combina-
tions of verbs and prepositional or adverbial par-
ticles, such as break down in The old truck broke
down. In syntactic terms, VPCs can be used in
several different subcategorisation frames (e.g. eat
up as intransitive or transitive VPC). In semantic
terms VPCs can range from idiosyncratic or semi-
idiosyncratic combinations, such as get alng mean-
ing to be in friendly terms, where the meaning of the
combination cannot be straightforwardly inferred
from the meaning of the verb and the particle, (in
e.g. He got along well with his colleagues), to more
regular ones, such as tear up (in e.g. In a rage she
tore up the letter Jack gave her). The latter is a case
where the particle compositionally adds a specific
meaning to the construction and follows a produc-
tive pattern (e.g. as in tear up, cut up and split up,
where the verbs are semantically related and up adds
a sense of completion to the action of these verbs).
In terms of inflectional morphology, the verb-
particle verb follows the same pattern as the simplex
verb (e.g. split up and split). Other characteristics,
like register and dialect are also shared between the
verb in a VPC and the simplex verb. If the VPC and
corresponding simplex verb are defined as indepen-
dent unrelated entries, these generalisations about
what is common between them would be lost. One
option to avoid this problem is to define the VPC
entry in a lexical encoding in terms of the corre-
sponding simplex verb entry.
As discussed earlier for many VPCs the particle
compositionally adds to the meaning of the verb
to form the meaning of the VPC, and this pro-
vides one more reason for keeping the link between
the VPC entry (e.g. wander up) and the simplex
verb entry (e.g. wander), which share the seman-
tics of the verb. Moreover, some of the compo-
sitional VPCs seem to follow productive patterns
(e.g. the resultative combinations walk/jump/run
up/down/out/in/away/around/... from joining these
verbs and the directional/locative particles up,
down, out, in, away, around, ...). This is dis-
cussed in Fraser (1976), who notes that the seman-
tic properties of verbs seem to affect their possi-
bility of combination with particles. For produc-
tive VPCs, one possibility is then to use the en-
tries of verbs already listed in a lexical resource
to productively generate VPC entries by combin-
ing them with particles according to their seman-
tic classes, as discussed by Villavicencio (2003).
However, there are also cases of semi-productivity,
since the possibilities of combinations are not fully
predictable from a particular verb and particle (e.g.
phone/ring/call/*telephone up). Thus, although
some classes of VPCs can be productively gener-
ated from verb entries, to avoid overgeneration we
adopt an approach where the remaining VPCs need
to be explicitly licensed by the specification of the
appropriate VPC entry.
To sum up, for VPC entries an appropriate en-
coding needs to maintain the link between a VPC
and the corresponding simplex form, from where
the VPC inherits many of its characteristics, includ-
ing inflectional morphology and for compositional
cases, the semantics of the verb. On the other hand,
for a non-compositional entry, like get alng, it is
necessary to specify the resulting semantics. In this
case, the semantics defined in the VPC entry over-
rides that inherited by default from its components.
5 A Possible Encoding for MWEs
Taking the encoding of simplex entries as basis for
an MWE encoding, we now discuss the necessary
extensions to the former, to be able to provide the
means of capturing the extra dimensions required
by the latter. While taking these requirements into
account, it is also desirable to define a very gen-
eral architecture, in which simplex and MWE en-
tries can be defined quite similarly, and in which
different types of MWE can be captured in a uni-
form encoding.
In the proposed encoding, simplex entries are still
defined in terms of orthography, grammatical type
and semantic predicate, in the Simplex table (ta-
ble 2). The same encoding can be used for fixed
MWEs, which are treated as words with space, ex-
cept that it also allows for the definition of the ele-
ment in the MWE that can be inflected. This is the
case of kick the bucket, which is defined as an in-
transitive construction whose first orthographic el-
ement (kick) is marked as allowing inflection, and
from where variations such as kicks the bucket can
be derived, table 2.
The encoding of flexible MWEs, on the other
hand, is done in 3 stages. In the first one, the id-
iomatic components of an MWE are defined in a
similar way to simplex words, in terms of an identi-
fier, grammatical type and semantic predicate, in the
MWE table (table 3). In addition, they also make
reference to a non-idiomatic simplex entry (base
form in table 3) from where they inherit by de-
fault many of their characteristics, including orthog-
raphy. This is done by means of the non-idiomatic
entry?s identifier. In the case of e.g. the idiomatic
spill (i spill tv 1), the corresponding non-idiomatic
entry is the transitive spill defined in the simplex ta-
ble, and whose identifier is spill tv 1. Moreover,
when appropriate, a non-idiomatic paraphrase for
the idiomatic element can also be defined. This is
achieved by specifying, in paraphrase the equiv-
alent non-idiomatic element?s semantic predicate.
The idiomatic spill, for example, is assigned as
corresponding paraphrase the non-idiomatic reveal
(reveal tv rel) defined in the simplex table. This
can be used to generate a non-idiomatic paraphrase
for the whole MWE (e.g. reveal secrets as para-
phrase of spill beans, as defined in table 3).
However, in order to be able to encode precisely
an MWE, in the second stage its context is speci-
fied, where all the elements that make that MWE
are listed. This ensures that only when all the core
elements defined for an MWE are present, is that the
MWE is recognised as such (e.g. spill and beans for
the MWE spill beans), preventing the case of false
positives (e.g. spill the milk) from being treated as
an instance of this MWE. Likewise, this prevents id-
iomatic entries from being used outside the context
of the MWE (e.g. the idiomatic spill being inter-
preted as reveal in spill some water). This is done
in the table known as MWE Components, table 4.
In this table each entry is defined in terms of an
identifier for the MWE (e.g. i spill beans 1), and
identifiers for each of the MWE components (e.g.
i spill tv 1 and i bean n 1), that provide the link to
the lexical specification of these components either
in the simplex table (table 2), or in the MWE table
(table 3). In order to allow MWEs with any number
of elements to be uniformly defined, (from shorter
ones like spill beans, rows 1 to 2 in table 4, to longer
ones like pull the curtain down on) we propose an
encoding where each element of the MWE is speci-
fied as a separate contextual entry (row). Thus, what
links all the components of an MWE together, spec-
ified each as an entry, is that they have the same
MWE identifier (e.g. i spill beans 1). Moreover,
to account for MWEs with optional elements, like
in a corner and in a tight corner where tight is op-
tional, each of the elements of the MWE needs to be
marked as obligatory or optional in this table.
For some MWEs, such as VPCs, one of the com-
ponents may be contributing a very specific mean-
ing in the context of that particular MWE, and often
the meaning is more specific than the one defined in
the corresponding base form entry for the compo-
nent, from when the meaning is obtained by default.
Thus, for non-compositional VPCs, such as look
up, the particles can be assumed to have a vacuous
semantic contribution, and the semantics of these
VPCs are contributed solely by the verbs. For look
up, the verbal component, look tv 1, defines the
meaning of the VPC as look-up tv rel while up is
assigned a vacuous relation (up-vacuous prt rel).
Similarly, up in a VPC such as wander up has either
a directional or locational/aspectual interpretation,
which in both cases can be regarded as qualifying
the event of wandering and can be compositionally
added to the meaning of the verb to generate the
meaning of the combination. For these cases, it is
important to allow the semantics of the component
in question to be further refined in its entry for that
MWE (e.g. up with semantics up-end-pt prt rel in
table 4). The approach taken means that the com-
monality in the directional interpretation between
wander up and walk up, where the semantics of the
particle is shared, is captured by means of the spe-
cific semantic type defined for the particle, which
means that generalizations can be made in an infer-
ence component or in semantic transfer for Machine
Translation. Similarly, by defining a VPC from the
base form of the corresponding verb, it is possible to
capture the fact that the semantics of verb is shared
between the verb wander and the VPC wander up.
Finally, in order to specify the appropriate rela-
tionships between the elements of the MWE, a set
of labels is used (PRED1, PRED2,...), which refer
to the position of the element in the logical form
for the MWE. This can be seen in the MWE Type
table (table 5). The basic idea behind the use of
these labels, defined in the column slot, is that they
can be employed as place holders in the semantic
predicate associated with that particular MWE. The
precise correspondences between these place hold-
ers and the predicates are specified in meta-types
defined for each different class of MWE. Thus the
particular meta-type verb-object-idiom is for idioms
with two obligatory elements, where PRED1 cor-
responds to pred1(X,Y) and PRED2 to pred2(Y),
and PRED1 (corresponding to the verb) is a pred-
icate whose second semantic argument (Y) is coin-
dexed with the second predicate (the object). When
this meta-type is instantiated with the entries for an
MWE like spill beans (i spill beans 1) the slots are
instantiated as i spill rel(X,Y), and i bean rel(Y).3
These meta-types act as interface between the
database and a specific grammar system. As men-
tioned before MWEs can be grouped together in
classes according to the patterns they follow (in
terms of syntactic and semantic characteristics).
Therefore, for each particular class of MWE, a
specific meta-type is defined, which contains the
precise interrelation between the components of
the MWE. This means that for a particular gram-
mar, for each meta-type there must be a (grammar-
3For reasons of clarity, in this paper we are using a simpli-
fied but equivalent notation for the meta-type description.
Table 5: MWE Type Table
mwe meta-type
i find nerve 1 verb-object-idiom
i spill beans 1 verb-object-idiom
walk up 1 verb-particle-np
wander up 1 verb-particle-np
look up 1 verb-particle-np
dependent) type that maps the semantic relations be-
tween the elements of the MWE into the appropri-
ate grammar dependent features. Thus, in the third
stage, it is necessary to specify the meta-types for
the MWEs encoded.
In order to test the generality of the meta-types
defined, a further sample of 25 idioms was ran-
domly selected, and an attempt was made to clas-
sify them according to the meta-types defined. The
majority of these idioms could be successfully de-
scribed by the available types, with only a few for
which further meta-types needed to be defined.
The same mechanisms are also used for defining
MWEs which have an element that can be realised
in different ways, but as one of a restricted set of
words like touch a nerve and find a nerve which
are instances of the same MWE. For these cases,
it is necessary to define each of the possible variants
and the position in the idiom in which they occur.
This is done in table 4, where find and touch, the
variants of the idiom find/touch a nerve are defined
as occurring in a particular slot, PRED1 (and nerve
as PRED2): i touch rel(X,Y) i nerve rel(Y) and
i find rel(X,Y) i nerve rel(Y). By using the same
identifier (i find nerve 1) and slot (PRED1) in both
cases, find and touch are specified as two possible
distinct realizations of the slot for that same idiom.
6 Discussion
Multiword Expressions present a challenge for lan-
guage technology, given their flexible nature. In this
paper we described a possible architecture for the
lexical encoding of these expressions. Even though
different types of MWEs have their own character-
istics, this proposal provides a uniform lexical en-
coding for defining them. This architecture takes
into account the flexibility of MWEs extending in a
straightforward manner the one required for simplex
words, and maximises the information contained for
them in the description of MWEs while minimising
the amount of information that needs to be defined
in the description of these expressions.
This encoding provides a clear way to capture
both fixed (and semi-fixed) MWEs and flexible
ones. The former are treated in the same manner
as simplex words, but with the possibility of speci-
fying the inflectional element of the MWE. For flex-
ible MWEs, on the other hand, the encoding is done
in three stages. The first one is the definition of
the idiomatic elements, in the MWE table, the sec-
ond the definition of an MWE?s components, in the
MWE Components table, and the third is the spec-
ification of a class (or meta-type) for the MWE, in
the MWE Type table. Different types of MWEs can
be straightforwardly described using this encoding,
as discussed in terms of idioms and VPCs.
A database employing this encoding can be in-
tegrated with a particular grammar, providing the
grammar system with a useful repertoire of MWEs.
This is the case of the MWE grammar (Villavicen-
cio, 2003) and of the wide-coverage LinGO ERG
(Flickinger, 2004), both implemented on the frame-
work of HPSG and successfully integrated with this
database. This encoding is also used as basis of the
architecture for a multilingual database of MWEs
defined by Villavicencio et al (2004), which has
the added complexity of having to record the cor-
respondences and differences in MWEs in different
languages: different word orders, different lexical
and syntactic constructions, etc. In terms of usage,
this encoding means that the search facilities pro-
vided by the database can help the user investigate
MWEs with particular properties. This in turn can
be used to aid the addition of new MWEs to the
database by analogy with existing MWEs with sim-
ilar characteristics.
7 Acknowledgements
This research was supported in part by the
NTT/Stanford Research Collaboration, research
project on multiword expressions and by the Noun
Phrase Agreement and Coordination AHRB Project
MRG-AN10939/APN17606. This document was
generated partly in the context of the DeepThought
project, funded under the Thematic Programme
User-friendly Information Society of the 5th Frame-
work Programme of the European Community
(Contract No IST-2001-37836).
References
Nicoletta Calzolari, Charles Fillmore, Ralph Gr-
ishman, Nancy Ide, Alessandro Lenci, Cather-
ine MacLeod, and Antonio Zampolli. 2002. To-
wards best practice for multiword expressions in
computational lexicons. In Proceedings of the
3rd International Conference on Language Re-
sources and Evaluation (LREC 2002), Las Pal-
mas, Canary Islands.
John Carroll and Claire Grover. 1989. The deriva-
tion of a large computational lexicon of English
from LDOCE. In B. Boguraev and E. Briscoe,
editors, Computational Lexicography for Natural
Language Processing. Longman.
Ann Copestake and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage English grammar using
HPSG. In Proceedings of the 2nd International
Conference on Language Resources and Evalua-
tion (LREC 2000).
Ann Copestake, Fabre Lambeau, Aline Villavicen-
cio, Francis Bond, Timothy Baldwin, Ivan Sag,
and Dan Flickinger. 2002. Multiword expres-
sions: Linguistic precision and reusability. In
Proceedings of the 3rd International Conference
on Language Resources and Evaluation (LREC
2002), Las Palmas, Canary Islands.
Ann Copestake, Fabre Lambeau, Benjamin Wal-
dron, Francis Bond, Dan Flickinger, and Stephan
Oepen. 2004. A lexicon module for a grammar
development environment. In To appear in Pro-
ceedings of the International Conference on Lan-
guage Resources and Evaluation (LREC 2004),
Lisbon, Portugal.
Ann Copestake. 1994. Representing idioms. Paper
presented at the HPSG Conference.
Christiane Fellbaum. 1998. Towards a representa-
tion of idioms in WordNet. In Proceedings of the
workshop on the use of WordNet in Natural Lan-
guage Processing Systems (Coling-ACL 1998),
Montreal.
Dan Flickinger. 2004. Personal Communication.
Bruce Fraser. 1976. The Verb-Particle Combina-
tion in English. Academic Press, New York,
USA.
Geoffrey Nunberg, Ivan A. Sag, and Tom Wasow.
1994. Idioms. Language, 70:491?538.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for NLP. In
Proceedings of the 3rd International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing-2002), pages 1?15, Mexico
City, Mexico.
Aline Villavicencio and Ann Copestake. 2002. As-
pectual on the nature of idioms. LinGO Working
Paper No. 2002-04.
Aline Villavicencio, Timothy Baldwin, and Ben-
jamin Waldron. 2004. A multilingual database of
idioms. In To appear in Proceedings of the Inter-
national Conference on Language Resources and
Evaluation (LREC 2004), Lisbon, Portugal.
Aline Villavicencio. 2003. Verb-particle construc-
tions and lexical resources. In Francis Bond,
Anna Korhonen, Diana McCarthy, and Aline
Villavicencio, editors, Proceedings of the ACL
2003 Workshop on Multiword Expressions: Anal-
ysis, Acquisition and Treatment, pages 57?64,
Sapporo, Japan.
Table 2: Simplex Table: Extended Encoding for Simplex Entries
identifier orthography type predicate inflectional
position
find tv 1 find trans-verb find tv rel
look tv 1 look trans-verb look tv rel
mention tv 1 mention trans-verb mention tv rel
pull tv 1 pull trans-verb pull tv rel
reveal tv 1 reveal trans-verb reveal tv rel
spill tv 1 spill trans-verb spill tv rel
touch tv 1 touch trans-verb touch tv rel
wander tv 1 wander trans-verb wander tv rel
up prt 1 up particle up prt rel
bean n 1 bean noun bean n rel
nerve n 1 nerve noun nerve n rel
secret n 1 secret noun secret n rel
unmentionable n 1 unmentionable noun unmentionable n rel
kick-the-bucket iv 1 kick, the, bucket intrans-verb kick-the-bucket iv rel 1
walk tv 1 walk intrans-verb walk iv rel
Table 3: MWE Table:Encoding for Idiomatic Entries
identifier base form type predicate paraphrase
i find tv 1 find tv 1 idiomatic-trans-verb i find tv rel mention tv rel
i spill tv 1 spill tv 1 idiomatic-trans-verb i spill tv rel reveal tv rel
i touch tv 1 touch tv 1 idiomatic-trans-verb i touch tv rel mention tv rel
i bean n 1 bean n 1 idiomatic noun i bean n rel secret n rel
i nerve n 1 nerve n 1 idiomatic noun i nerve n rel unmentionable n rel
Table 4: MWE Components
Phrase Component Predicate Slot Optional
i spill beans 1 i spill tv 1 PRED1 no
i spill beans 1 i bean n 1 PRED2 no
i find nerve 1 i find tv 1 PRED1 no
i find nerve 1 i touch tv 1 PRED1 no
i find nerve 1 i nerve n 1 PRED2 no
walk up 1 walk iv 1 PRED1 no
walk up 1 up prt 1 up-end-pt prt rel PRED2 no
wander up 1 wander tv 1 PRED1 no
wander up 1 up prt 1 up-end-pt prt rel PRED2 no
look up 1 look tv 1 look-up tv rel PRED1 no
look up 1 up prt 1 up vacous prt rel PRED2 no
Coling 2010: Poster Volume, pages 1041?1049,
Beijing, August 2010
Web-based and combined language models:
a case study on noun compound identification
Carlos Ramisch?? Aline Villavicencio? Christian Boitet?
? GETALP ? Laboratory of Informatics of Grenoble, University of Grenoble
? Institute of Informatics, Federal University of Rio Grande do Sul
{ceramisch,avillavicencio}@inf.ufrgs.br Christian.Boitet@imag.fr
Abstract
This paper looks at the web as a corpus
and at the effects of using web counts
to model language, particularly when we
consider them as a domain-specific versus
a general-purpose resource. We first com-
pare three vocabularies that were ranked
according to frequencies drawn from
general-purpose, specialised and web cor-
pora. Then, we look at methods to com-
bine heterogeneous corpora and evaluate
the individual and combined counts in the
automatic extraction of noun compounds
from English general-purpose and spe-
cialised texts. Better n-gram counts can
help improve the performance of empiri-
cal NLP systems that rely on n-gram lan-
guage models.
1 Introduction
Corpora have been extensively employed in sev-
eral NLP tasks as the basis for automatically
learning models for language analysis and gener-
ation. In theory, data-driven (empirical or statis-
tical) approaches are well suited to take intrinsic
characteristics of human language into account. In
practice, external factors also determine to what
extent they will be popular and/or effective for a
given task, so that they have shown different per-
formances according to the availability of corpora,
to the linguistic complexity of the task, etc.
An essential component of most empirical sys-
tems is the language model (LM) and, in partic-
ular, n-gram language models. It is the LM that
tells the system how likely a word or n-gram is in
that language, based on the counts obtained from
corpora. However, corpora represent a sample of
a language and will be sparse, i.e. certain words or
expressions will not occur. One alternative to min-
imise the negative effects of data sparseness and
account for the probability of out-of-vocabulary
words is to use discounting techniques, where a
constant probability mass is discounted from each
n-gram and assigned to unseen n-grams. Another
strategy is to estimate the probability of an un-
seen n-gram by backing off to the probability of
the smaller n-grams that compose it.
In recent years, there has also been some ef-
fort in using the web to overcome data sparseness,
given that the web is several orders of magnitude
larger than any available corpus. However, it is
not straightforward to decide whether (a) it is bet-
ter to use the web than a standard corpus for a
given task or not, and (b) whether corpus and web
counts should be combined and how this should
be done (e.g. using interpolation or back-off tech-
niques). As a consequence there is a strong need
for better understanding of the impacts of web fre-
quencies in NLP systems and tasks.
More reliable ways of combining word counts
could improve the quality of empirical NLP sys-
tems. Thus, in this paper we discuss web-based
word frequency distributions (? 2) and investigate
to what extent ?web-as-a-corpus? approaches can
be employed in NLP tasks compared to standard
corpora (? 3). Then, we present the results of
two experiments. First, we compare word counts
drawn from general-purpose corpora, from spe-
cialised corpora and from the web (? 4). Second,
we propose several methods to combine data from
heterogeneous corpora (? 5), and evaluate their ef-
fectiveness in the context of a specific multiword
1041
expression task: automatic noun compound iden-
tification. We close this paper with some conclu-
sions and future work (? 6).
2 The web as a corpus
Conventional and, in particular, domain-specific
corpora, are valuable resources which provide a
closed-world environment where precise n-gram
counts can be obtained. As they tend to be smaller
than general purpose corpora, data sparseness can
considerably hinder the results of statistical meth-
ods. For instance, in the biomedical Genia cor-
pus (Ohta et al, 2002), 45% of the words occur
only once (so-called hapax legomena), and this is
a very poor basis for a statistical method to decide
whether this is a significant event or just random
noise.
One possible solution is to see the web as a
very large corpus containing pages written in sev-
eral languages and being representative of a large
fraction of human knowledge. However, there are
some differences between using regular corpora
and the web as a corpus, as discussed by Kilgar-
riff (2003). One assumption, in particular, is that
page counts can approximate word counts, so that
the total number of pages is used as an estimator
of the n-gram count, regardless of how many oc-
currences of the n-gram they contain.
This simple underlying assumption has been
employed for several tasks. For example, Grefen-
stette (1999), in the context of example-based ma-
chine translation, uses web counts to decide which
of a set of possible translations is the most natural
one for a given sequence of words (e.g. groupe de
travail as work group vs labour collective). Like-
wise, Keller and Lapata (2003) use the web to esti-
mate the frequencies of unseen nominal bigrams,
while Nicholson and Baldwin (2006) look at the
interpretation of noun compounds based on the
individual counts of the nouns and on the global
count of the compound estimated from the web as
a large corpus.
Villavicencio et al (2007) show that the web
and the British National Corpus (BNC) could be
used interchangeably to identify general-purpose
and type-independent multiword expressions. La-
pata and Keller (2005) perform a careful and
systematic evaluation of the web as a corpus in
other general-purpose tasks both for analysis and
generation, comparing it with a standard corpus
(the BNC) and using two different techniques to
combine them: linear interpolation and back-off.
Their results show that, while web counts are not
as effective for some tasks as standard counts, the
combined counts can generate results, for most
tasks, that are as good as the results produced by
the best individual corpus between the BNC and
the web. Nakov (2007) further investigates these
tasks and finds that, for many of them, effective
attribute selection can produce results that are at
least comparable to those from the BNC using
counts obtained from the web.
On the one hand, the web can minimise the
problem of sparse data, helping distinguish rare
from invalid cases. Moreover, a search engine al-
lows access to ever increasing quantities of data,
even for rare constructions and words, which
counts are usually equated to the number of pages
in which they occur. On the other hand, n-
grams in the highest frequency ranges, such as
the words the, up and down, are often assigned
the estimated size of the web, uniformly. While
this still gives an idea of their massive occur-
rence, it does not provide a finer grained distinc-
tion among them (e.g. in the BNC, the, down and
up occur 6,187,267, 84,446 and 195,426 times,
respectively, while in Yahoo! they all occur in
2,147,483,647 pages).
3 Standard vs web corpora
When we compare n-gram counts estimated from
the web with counts taken from a well-formed
standard corpus, we notice that web counts are
?estimated? or ?approximated? as page counts,
whereas standard corpus counts are the exact
number of occurrences of the n-gram. In this way,
web counts are dependent on the particular search
engine?s algorithms and representations, and these
may perform approximations to handle the large
size of their indexing structures and procedures,
such as ignoring punctuation and using stopword
lists (Kilgarriff, 2007). This assumption, as well
as the following discussion, are not valid for for
controlled data sets derived from Web data, such
1042
as the Google 1 trillion n-grams1. Thus, our re-
sults cannot be compared to those using this kind
of data (Bergsma et al, 2009).
In data-driven techniques, some statistical mea-
sures are based on contingency tables, and the
counts for each of the table cells can be straight-
forwardly computed from a standard corpus.
However, this is not the case for the web, where
the occurrences of an n-gram are not precisely
calculated in relation to the occurrences of the
(n? 1)-grams composing it. For instance, the
n-gram the man may appear in 200,000 pages,
while the words the and man appear in respec-
tively 1,000,000 and 200,000 pages, implying that
the word man occurs with no other word than the2.
In addition, the distribution of words in a stan-
dard corpus follows the well known Zipfian dis-
tribution (Baayen, 2001) while, in the web, it is
very difficult to distinguish frequent words or n-
grams as they are often estimated as the size of the
web. For instance, the Yahoo! frequencies plotted
in figure 1(a) are flattened in the upper part, giv-
ing the same page counts for more than 700 of the
most frequent words. Another issue is the size of
the corpus, which is an important information, of-
ten needed to compute frequencies from counts or
to estimate probabilities in n-gram models. Un-
like the size of a standard corpus, which is easily
obtained, it is very difficult to estimate how many
pages exist on the web, especially as this number
is always increasing.
But perhaps the biggest advantage of the web is
its availability, even for resource-poor languages
and domains. It is a free, expanding and easily ac-
cessible resource that is representative of language
use, in the sense that it contains a great variability
of writing styles, text genres, language levels and
knowledge domains.
4 Analysing n-gram frequencies
In this section, we describe an experiment to com-
pare the probability distribution of the vocabulary
of two corpora, Europarl (Koehn, 2005) and Ge-
nia (Ohta et al, 2002), that represent a sample
of general-purpose and specialised English. In
1This dataset is released through LDC and is not freely
available. Therefore, we do not consider it in our evaluation.
2In practice, this procedure can lead to negative counts.
Vep Vgenia Vinter
types 104,144 20,876 6,798
hapax 41,377 9,410 ?
tokens 39,595,352 486,823 ?
Table 1: Some characteristics of general vs
domain-specific corpora.
addition to both corpora, we also considered the
counts from the web as a corpus, using Google
and Yahoo! APIs, and these four corpora act as n-
gram count sources. To do that, we preprocessed
the data (? 4.1), extracted the vocabularies from
each corpus and calculated their counts in our
four n-gram count sources (? 4.2), analysing their
rank plots to compare how each of these sources
models general-purpose and specialised language
(? 4.3). The experiments described in this sec-
tion were implemented in the mwetoolkit and
are available at http://sf.net/projects/
mwetoolkit/.
4.1 Preprocessing
The Europarl corpus v3.0 (ep) contains transcrip-
tions of the speeches held at the European Par-
liament, with more than 1.4M sentences and
39,595,352 words. The Genia corpus (genia) con-
tains abstracts of scientific articles in biomedicine,
with around 1.8K sentences and 486,823 words.
These standard corpora were preprocessed in the
following way:
1. conversion to XML, lemmatisation and POS
tagging3;
2. case homogenisation, based on the following
criteria:
? all-uppercase and mixed case words
were normalised to their predominant
form, if it accounts for at least 80% of
the occurrences;
? uppercase words at the beginning of
sentences were lowercased;
? other words were not modified.
3Genia contains manual POS tag annota-
tion. Europarl was tagged using the TreeTagger
(www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger).
1043
This lowercasing algorithm helps to deal with the
massive use of abbreviations, acronyms, named
entities, and formulae found in specialised cor-
pora, such as those containing biomedical (and
other specialised) scientific articles.
For calculating arbitrary-sized n-grams in large
textual corpora efficiently, we implemented a
structure based on suffix arrays (Yamamoto and
Church, 2001). While suffix trees are often used
in LM tools, where n-grams have a fixed size, they
are not fit for arbitrary length n-gram searches and
can consume quite large amounts of memory to
store all the node pointers. Suffix arrays, on the
other hand, allow for arbitrary length n-grams to
be counted in a time that is proportional to log(N),
where N is the number of words (which is equiva-
lent to the number of suffixes) in the corpus. Suf-
fix arrays use a constant amount of memory pro-
portional to N. In our implementation, where ev-
ery word and every word position in the corpus are
encoded as a 4-byte integer, it corresponds pre-
cisely to 4?2?N plus the size of the vocabulary,
which is generally very small if compared to N,
given a typical token/type ratio. The construction
of the suffix array takes O(N log2 N) operations,
due to a sorting step at the end of the process.
4.2 Vocabulary creation
After preprocessing, we extracted all the unigram
surface forms (i.e. all words) from ep and from ge-
nia, generating two vocabularies, Vep and Vgenia,
where the words are ranked in descending fre-
quency order with respect to the corpus itself seen
as a n-gram count source. Formally, we can model
a vocabulary as a set V of words vi ?V taken from
a corpus. A word count is the value c(vi) = n of a
function that goes from words to natural numbers,
c : V ? N. Therefore, there is always an implicit
word order relation?r in a vocabulary, that can be
generated from V and c by using the order relation
? in N4. Thus, a rank is defined as a partially-
ordered set formed by a vocabulary?word order
pair relation: ?V,?r?.
Table 1 summarises some measures of the ex-
tracted vocabularies, where Vinter denotes the in-
tersection of Vep and Vgenia. Notice that Vinter
4That is, ?v1,v2 ?V , suppose c(v1) = n1 and c(v2) = n2,
then v1 ?r v2 if and only if n1 ? n2.
n-gram genia ep google yahoo
642 1 4 8090K 220M
African 2 2028 15400K 916M
fatty 16 22 2550K 59700K
medicine 4 643 21900K 934M
Mac 15 3 34500K 1910M
SH2 27 1 113K 3270K
advances 4 646 6200K 173M
thereby 29 2370 8210K 145M
Table 2: Distribution of some words in Vinter.
contains considerably less entries than the small-
est vocabulary (Vgenia). This shows to what ex-
tent both types of text differ and how important
it is to use the correct techniques when work-
ing with domain-specific data in empirical ap-
proaches. The table also shows the number of ha-
pax legomena (i.e. words that occur only once) in
each corpus, and in this aspect both corpora are
similar5. It also shows how sparseness affects lan-
guage, since a vocabulary that is 400% bigger has
only 5% less hapax legomena.
For each entry in each vocabulary, we ob-
tained a count estimated from four different n-
gram count sources: ep, genia, Google as a cor-
pus (google) and Yahoo! as a corpus (yahoo). The
latter were configured to return only results for
pages in English. Table 2 shows an example of
entries extracted from Vinter. Notice that there are
no zeroes in columns genia and ep, since this vo-
cabulary only contains words that occur at least
once in these corpora. Also, some words like Mac
and SH2, that are probably specialised terms, oc-
cur more in genia than in ep even if the latter is
more than 80 times larger than the former.
4.3 Rank analyses
For each vocabulary, we want to estimate how
similar the ranks generated by each of the four
count sources are. Figure 1 shows the rank po-
sition (x) against the frequency (y) of words in
Vgenia, Vep and Vinter, where each plotted point rep-
resents a rank position according to corpus fre-
5The percentual difference in the proportion of hapax
legomena can be explained by the fact that genia is much
smaller than ep.
1044
10-5
10-4
10-3
10-2
10-1
1 10 100 1000 10000
No
rm
ali
ze
d f
req
ue
nc
y
Rank
Rankplot of vgenia
genia
epgoogleyahoo
(a) Rank plot of Vgenia.
10-5
10-4
10-3
10-2
10-1
1 10 100 1000 10000
No
rm
ali
ze
d f
req
ue
nc
y
Rank
Rankplot of vep
genia
epgoogleyahoo
(b) Rank plot of Vep.
10-5
10-4
10-3
10-2
10-1
1 10 100 1000 10000
No
rm
ali
ze
d f
req
ue
nc
y
Rank
Rankplot of vinter
genia
epgoogleyahoo
(c) Rank plot of Vinter.
Figure 1: Plot of normalised frequencies of vocabularies according to rank positions, log-log scale.
quencies and may correspond to several different
words.6 The four sources have similar shaped
curves for each of the three vocabularies: ep
and genia could be reasonably approximated by
a linear regression curve (in the log-log domain).
google and yahoo present Zipfian curves for low
frequency ranges but have a flat line for higher
frequencies, and the phenomenon seems consis-
tent in all vocabularies and more intense on yahoo.
This is related to the problem discussed in sec-
tion 3 which is that web-based frequencies are not
accurate to model common words because web
counts correspond to page counts and not to word
counts, and that a common word will probably ap-
pear dozens of times in a single page. Nonethe-
less, google seems more robust to this effect,
and indeed yahoo returns exactly the same value
(roughly 2 billion pages) for a large number of
common words, producing the perfectly straight
line in the rank plots. Moreover, the problem
seems less serious in Vinter, but this could be due
to its much smaller size. These results show that
google is incapable of distinguishing among the
top-100 words while yahoo is incapable of distin-
guishing among the top-1000 words, and this can
be a serious drawback for web-based counts both
in general-purpose and specialised NLP tasks.
The curves agree in a large portion of the fre-
quency range, and the only interval in which ge-
nia and ep disagree is in lower frequencies (shown
in the bottom right corner). This happens be-
6Given the Zipfian behaviour of word probability distri-
butions, a log-log scale was used to plot the curves.
cause general-purpose ep frequencies are much
less accurate to model the specialised genia vo-
cabulary, specially in low frequency ranges when
sparseness becomes more marked (figure 1(a)),
and vice-versa (figure 1(b)). This effect is min-
imised in figure 1(c), corresponding to Vinter.
Although both vocabularies present the same
word frequency distributions, it does not mean
that their ranks are similar for the four count
sources. Tables 3 and 4 show the correlation
scores for the compared count sources and for the
two vocabularies, using Kendall?s ? . The ? corre-
lation index estimates the probability that a word
pair in a given rank has the same respective po-
sition in another rank, in spite of the distance be-
tween the words7.
In the two vocabularies, correlation is low,
which indicates that the ranks tend to order words
differently even if there are some similarities in
terms of the shape of the frequency distribution.
When we compare genia with google and with
yahoo, we observe that yahoo is slightly less cor-
related with genia than google, probably because
of its uniform count estimates for frequent words.
However, both seem to be more similar to genia
than ep.
A comparison of ep with google and with yahoo
shows that web frequencies are much more similar
to a general-purpose count source like ep than to
a specialised source like genia. Additionally, both
yahoo and google seem equally correlated to ep.
7For all correlation values, p < 0.001 for the alternative
hypothesis that ? is greater than 0.
1045
Vgenia Vgenia Vgenia Vgenia
top middle bottom
genia-ep 0.26 0.24 0.13 0.06
genia-google 0.28 0.24 0.18 0.09
genia-yahoo 0.27 0.22 0.17 0.09
ep-google 0.57 0.68 0.53 0.49
ep-yahoo 0.57 0.68 0.53 0.49
google-yahoo 0.90 0.90 0.89 0.89
Table 3: Kendall?s ? for count sources in Vgenia.
Vep Vep Vep Vep
top middle bottom
genia-ep 0.26 0.36 0.07 0.04
genia-google 0.27 0.39 0.15 0.12
genia-yahoo 0.24 0.35 0.12 0.10
ep-google 0.40 0.45 0.22 0.09
ep-yahoo 0.38 0.44 0.20 0.08
google-yahoo 0.86 0.89 0.84 0.83
Table 4: Kendall?s ? for count sources in Vep.
Surprisingly, this correlation is higher for Vgenia
than for Vep, as web frequencies and ep frequen-
cies are more similar for a specialised vocabulary
than for a general-purpose vocabulary. This could
mean that the three perform similarly (poorly) at
estimating frequencies for the biomedical vocab-
ulary (Vgenia) whereas they differ considerably at
estimating general-purpose frequencies.
The correlation of the rank (first column) is also
decomposed into the correlation for top words
(more than 10 occurrences), middle words (10 to
3 occurrences) and bottom words (2 and 1 occur-
rences). Except for the pair google-yahoo, the cor-
relation is much higher in the top portion of the
vocabulary and is close to zero in the long tail.
In spite of the logarithmic scale of the graphics
in figure 1, that show the largest difference in the
top part, the bottom part is actually the most ir-
regular. The only exception is ep compared with
the web count sources in Vgenia: these two pairs do
not present the high variability of the other com-
pared pairs, and this means that using ep counts
(general-purpose) to estimate genia counts (spe-
cialised) is similar to using web counts, indepen-
dently of the position of the word in the rank.
Counts from google and from yahoo are also very
similar, specially if we also consider Spearman?s
? , that is very close to total correlation. Web ranks
are also more similar for a specialised vocabulary
than for a general-purpose one, providing further
evidence for the hypothesis that the higher corre-
lation is a consequence of both sources being poor
frequency estimators. That is, for a given vocabu-
lary, when web count sources are good estimators,
they will be more distinct (e.g. having less zero
frequencies).
5 Combining corpora frequencies
In our second experiment, the goal is to propose
and to evaluate techniques for the combination
of n-gram counts from heterogeneous sources.
Therefore, we will use the insights about the vo-
cabulary differences presented in the previous sec-
tion. In this evaluation, we measure the impact
of the suggested techniques in the identification
of noun?noun compounds in corpora. Noun com-
pounds are very frequent in general-purpose and
specialised texts (e.g. bus stop, European Union
and gene activation). We extract them automat-
ically from ep and from genia using a standard
method based on POS patterns and association
measures (Evert and Krenn, 2005; Pecina, 2008;
Ramisch et al, 2010).
5.1 Experimental setup
The evaluation task consists of, given a corpus
of N words, extract all occurrences of adjacent
pairs of nouns8 and then rank them using a stan-
dard statistical measure that estimates the asso-
ciation strength between the two nouns. Analo-
gously to the formalism adopted in section 4.2,
we assume that, for each corpus, we generate a
set NN containing n-grams v1...n ? NN9 for which
we obtain n-gram counts from four sources. The
elements in NN are generated by comparing the
POS pattern noun?noun against all the bigrams in
the corpus and keeping only those pairs of adja-
cent words that match the pattern. The calculation
of the association measure, considering a bigram
v1v2, is based on a contingency table which cells
8We ignore other types of compounds, e.g. adjective?
noun pairs.
9We abbreviate a sequence v1 . . .vn as v1...n.
1046
contain all possible outcomes a1a2,ai ? {vi,?vi}.
For web-based counts, we corrected up to 2% of
them by forcing the frequency of a unigram to be
at least equal to the frequency of the bigram in
which it occurs. Such inconsistencies are incom-
patible with statistical approaches based on con-
tingency table, as discussed in section 2.
The log-likelihood association measure (LL, al-
ternatively called expected mutual information),
estimates the difference between the observed ta-
ble and the expected table under the assumption of
independent events, where E(a1 . . .an) =
n?
i=1
c(ai)
Nn?1is calculated using maximum likelihood:
LL(v1v2) = ?
a1a2
c(a1a2)? log2
c(a1a2)
E(a1a2)
The evaluation of the NN lists is performed au-
tomatically with the help of existing noun com-
pound dictionaries. The general-purpose gold
standard, used to evaluate NNep, is composed of
bigram noun compounds extracted from several
resources: 6,212 entries from the Cambridge In-
ternational Dictionary of English, 22,981 from
Wordnet and 2,849 from the data sets of MWE
200810. Those were merged into a single general-
purpose gold standard that contains 28,622 bi-
gram noun compounds. The specialised gold stan-
dard, used to evaluate NNgenia, is composed of
7,441 bigrams extracted from constituent annota-
tion of the genia corpus with respect to concepts
in the Genia ontology (Kim et al, 2006).
True positives (TPs) are the n-grams of NN
that are contained in the respective gold standard,
while n-grams that do not appear in the gold stan-
dard are considered false positives11. While this
is a simplification that underestimates the perfor-
mance of the method, it is appropriate for the pur-
pose of this evaluation because we compare only
the mean average precision (MAP) between two
NN ranks, in order to verify whether improve-
ments obtained by the combined frequencies are
10420 entries provided by Timothy Baldwin, 2,169 en-
tries provided by Su Nam Kim and 250 entries provided by
Preslav Nakov, freely available at http://multiword.
sf.net/
11In fact, nothing can be said about an n-gram that is not
in a (limited-coverage) dictionary, further manual annotation
would be necessary to asses its relevance.
significant. Additionaly, MWEs are complex lin-
guistic phenomena, and their annotation, specially
in a domain corpus, is a difficult task that reaches
low agreement rates, sometimes even for expert
native speakers. Therefore, not only for theo-
retical reasons but also for practical reasons, we
adopted an automatic evaluation procedure rath-
ern than annotating the top candidates in the lists
by hand.
Since the log-likelihood measure is a function
that assigns a real value to each n-gram, there is
a rank relation ?r that will be used to calculate
MAP as follows:
MAP(NN,?r) =
?
v1...n?NN
P(v1...n)? p(v1...n)
|TPs in NN| ,
where p = 1 if v1...n is a TP, 0 else, and the preci-
sion P(v1...n) of a given n-gram corresponds to the
number of TPs before v1...n in ?NN,?r? over the
total number of n-grams before v1...n in ?NN,?r?.
5.2 Combination heuristics
From the initial list of 176,552 lemmatised n-
grams in NNep and 14,594 in NNgenia, we fil-
tered out all hapax legomena in order to remove
noise and avoid useless computations. Then, we
counted the occurrences of v1, v2 and v1v2 in our
four sources, and those were used to calculate the
four LL values of n-grams in both lists. We also
propose three heuristics to combine a set of m
count sources c1 through cm into a single count
source ccomb:
ccomb(v1...n) =
m?
i=1
wi(v1...n)? ci(v1...n),
where w(v1...n) is a function that assigns a weight
between 0 and 1 for each count source accord-
ing to the n-gram v1...n. Three different func-
tions were used in our experiments: uniform
linear interpolation assumes a constant and uni-
form weight w(v1...n) = 1/m for all n-grams; pro-
portional linear interpolation assumes a constant
weight wi(v1...n) = ((?mj=1 N j)?Ni)/?mj=1 N j that
is proportional to the inverse size of the corpus;
and back-off uses the uniform interpolation of
web frequencies whenever the n-gram count in the
original corpus falls below a threshold (empiri-
cally defined as log2(N/100,000)).
1047
MAP of rank NNgenia NNep
LLgenia 0.4400 0.0462
LLep 0.4351 0.0371
LLgoogle 0.4297 0.0532
LLyahoo 0.4209 0.0508
LLuni f orm 0.4254 0.0508
LLproportional 0.4262 0.0520
LLbacko f f 0.3719 0.0370
Table 5: Performance of compound extraction.
Table 5 shows that the performance of back-
off is below all other techniques for both vocab-
ularies, thus excluding it as a successful combina-
tion heuristic. The large difference between MAP
scores for NNep and for NNgenia is explained by
the relative size of the gold standards: while the
general-purpose reference accounts for 16% of the
size of the NNep set, the specialised reference has
as many entries as 50% of NNgenia. Moreover, the
former was created by joining heterogeneous re-
sources while the latter was compiled by human
annotators from the Genia corpus itself. The goal
of our evaluation, however, is not to compare the
difficulty of each task, but to compare the com-
bination heuristics presented in each row of the
table.
The best MAP for NNgenia was obtained with
genia, that significantly outperforms all other
sources except ep12. On the other hand, the use
of web-based or interpolated counts in extracting
specialised noun?noun compounds does not im-
prove the performance of results based on sparse
but reliable counts drawn from well-formed cor-
pora. Nonetheless, the performance of ep in spe-
cialised extraction is surprising and could only be
explained by some overlap between the corpora.
Moreover, the interpolated counts are not signif-
icantly different from google counts, even if this
corpus should have the weakest weight in propor-
tional interpolation.
General-purpose compound extraction, how-
ever, benefits from the counts drawn from large
corpora as google and yahoo. Indeed, the former
12Significance was assessed through a standard one-tailed
t test for equal sample sizes and variances, ? = 0.005.
significantly outperforms all other count sources,
closely followed by proportional counts. In
both vocabularies, proportional interpolation per-
forms very similar to the best count source, but,
strangely enough, it still does not outperform
google. Further data inspection would be needed
to explain these results for the interpolated combi-
nation and to try to shed some light on the reason
why the backoff method performs so poorly.
6 Future perspectives
In this work, we presented a detailed evalua-
tion of the use of web frequencies as estima-
tors of corpus frequencies in general-purpose and
specialised tasks, discussing some important as-
pects of corpus-based versus web-based n-gram
frequencies. The results indicate that they are
not only very distinct but they are so in different
ways. The importance of domain-specific data for
modelling a specialised vocabulary is discussed in
terms of using ep to get Vgenia counts. Further-
more, the web corpora were more similar to genia
than to ep, which can be explained by the fact that
?similar? is different from ?good?, i.e. they might
be equally bad in modelling genia while they are
distinctly better for ep.
We also proposed heuristics to combine count
sources inspired by standard interpolation and
back-off techniques. Results show that we can-
not use web-based or combined counts to identify
specialised noun compounds, since they do not
help minimise data sparseness. However, general-
purpose extraction is improved with the use of
web counts instead of counts drawn from standard
corpora.
Future work includes extending this research
to other languages and domains in order to es-
timate how much of these results depend on the
corpora sizes. Moreover, as current interpolation
techniques usually combine two corpora, weights
are estimated in a more or less ad hoc proce-
dure (Lapata and Keller, 2005). Interpolating sev-
eral corpora would need a more controlled learn-
ing technique to obtain optimal weights for each
frequency function. Additionally, the evaluation
shows that corpora perform differently according
to the frequency range. This insight could be used
to define weight functions for interpolation.
1048
Acknowledgements
This research was partly supported by CNPq
(Projects 479824/2009-6 and 309569/2009-5),
FINEP and SEBRAE (COMUNICA project
FINEP/SEBRAE 1194/07). Special thanks to
Fl?vio Brun for his thorough work as volunteer
proofreader.
References
Baayen, R. Harald. 2001. Word Frequency Distri-
butions, volume 18 of Text, Speech and Language
Technology. Springer.
Bergsma, Shane, Dekang Lin, and Randy Goebel.
2009. Web-scale N-gram models for lexical disam-
biguation. In Boutilier, Craig, editor, Proceedings
of the 21st International Joint Conference on Arti-
ficial Intelligence (IJCAI 2009), pages 1507?1512,
Pasadena, CA, USA, July.
Evert, Stefan and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of sta-
tistical association measures. Computer Speech &
Language Special issue on Multiword Expression,
19(4):450?466.
Grefenstette, Gregory. 1999. The World Wide Web
as a resource for example-based machine translation
tasks. In Proceedings of the Twenty-First Interna-
tional Conference on Translating and the Computer,
London, UK, November. ASLIB.
Keller, Frank and Mirella Lapata. 2003. Using
the web to obtain frequencies for unseen bigrams.
Computational Linguistics Special Issue on the Web
as Corpus, 29(3):459?484.
Kilgarriff, Adam and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistics Special Issue on the Web
as Corpus, 29(3):333?347.
Kilgarriff, Adam. 2007. Googleology is bad science.
Computational Linguistics, 33(1):147?151.
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2006. GENIA ontology. Techni-
cal report, Tsujii Laboratory, University of Tokyo.
Koehn, Philipp. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the Tenth Machine Translation Summit(MT Summit
2005), Phuket, Thailand, September. Asian-Pacific
Association for Machine Translation.
Lapata, Mirella and Frank Keller. 2005. Web-
based models for natural language processing. ACM
Transactions on Speech and Language Processing
(TSLP), 2(1):1?31.
Nakov, Preslav. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syn-
tax and Semantics. Ph.D. thesis, EECS Department,
University of California, Berkeley, CA, USA.
Nicholson, Jeremy and Timothy Baldwin. 2006. Inter-
pretation of compound nominalisations using cor-
pus and web statistics. In Moir?n, Bego?a Villada,
Aline Villavicencio, Diana McCarthy, Stefan Ev-
ert, and Suzanne Stevenson, editors, Proceedings of
the COLING/ACL Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties (MWE 2006), pages 54?61, Sidney, Australia,
July. Association for Computational Linguistics.
Ohta, Tomoko, Yuka Tateishi, and Jin-Dong Kim.
2002. The GENIA corpus: an annotated research
abstract corpus in molecular biology domain. In
Proceedings of the Second Human Language Tech-
nology Conference (HLT 2002), pages 82?86, San
Diego, CA, USA, March. Morgan Kaufmann Pub-
lishers.
Pecina, Pavel. 2008. Reference data for czech collo-
cation extraction. In Gregoire, Nicole, Stefan Ev-
ert, and Brigitte Krenn, editors, Proceedings of the
LREC Workshop Towards a Shared Task for Multi-
word Expressions (MWE 2008), pages 11?14, Mar-
rakech, Morocco, June.
Ramisch, Carlos, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for mul-
tiword expression identification. In Calzolari, Nico-
letta, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odjik, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC 2010), Valetta, Malta, May.
European Language Resources Association.
Villavicencio, Aline, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation
and evaluation of automatically acquired multi-
word expressions for grammar engineering. In
Eisner, Jason, editor, Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2007), pages
1034?1043, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Yamamoto, Mikio and Kenneth W. Church. 2001. Us-
ing suffix arrays to compute term frequency and
document frequency for all substrings in a corpus.
Computational Linguistics, 27(1):1?30.
1049
Coling 2010: Demonstration Volume, pages 21?24,
Beijing, August 2010
COMUNICA - A Question Answering System for Brazilian
Portuguese
Rodrigo Wilkens?, Aline Villavicencio?, Daniel Muller?, Leandro Wives?,
Fabio da Silva?, Stanley Loh?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil),
?Conexum (Brazil), ?DFL (Brazil), ?IntextMining (Brazil)
{rwilkens,avillavicencio,wives}@inf.ufrgs.br, conexum@conexum.com.br, fabio@dfl.psi.br, sloh@terra.com.br
Abstract
COMUNICA is a voice QA system for
Brazilian Portuguese with search ca-
pabilities for consulting both struc-
tured and unstructured datasets.
One of the goals of this work is to help
address digital inclusion by providing
an alternative way to accessing writ-
ten information, which users can em-
ploy regardless of available computa-
tional resources or computational lit-
eracy.
1 Introduction
A crucial social problem in many countries
is functional illiteracy, and in Latin Amer-
ica, according to UNESCO, the process of
literacy is only effectively achieved for people
who completed at least four years of school-
ing. Among those who have not completed
this cycle of education, there has been high
rates of return to illiteracy. According to this
definition, in 2002 Brazil had a total of 32.1
million functionally illiterate citizens, repre-
senting 26% of the population aged 15 or
older1. This may have a significant effect
on digital inclusion, preventing a consider-
able part of the population from accessing
massive amounts of information such as that
available on the Web, or benefitting from
advances in technology. Although these fig-
ures do not include digital iliteracy, or lack
of computational resources, they can give an
idea of the magnitude of the problem.
In this context, voice question answering
systems (QA) have the potential to make
written information more easily accessible to
1IBGE: http://www.ibge.gov.br/ibgeteen/
pesquisas/educacao.html
wider audiences as they allow users to ask
questions in their own native language and
especially if this includes spoken language,
sometimes without the need even for a com-
puter (e.g. using the phone). This paper de-
scribes COMUNICA, a voice QA system for
Brazilian Portuguese with search capabilities
for consulting both structured and unstruc-
tured datasets. The domain chosen to eval-
uate the system is that of municipal infor-
mation from the FAMURS database.2 One
of the goals of this work is to help address
digital inclusion by providing a way to over-
come (a) difficulties in accessing written in-
formation (for visually challenged users), (b)
lack of computational resources (for users in
remote or computerless areas) and (c) com-
putational illiteracy.
2 QA systems
In recent years, QA has received consider-
able attention, as can be seen by the initia-
tives devoted to the task, such as the TREC3
and CLEF4. The task of a QA system is
to automatically answer a question in nat-
ural language, searching for information in a
given data source (e.g. a database, or cor-
pora from a given domain). This is a chal-
lenging task as question types can range from
lists to facts and definitions, while answers
may come from small data sets such as doc-
ument collections, to the World Wide Web.
Moreover, the difficulty of the task is also
influenced by whether the questions are re-
stricted to a particular domain (e.g. sports,
genes) or not, which additional sources of in-
2http://www.famurs.com.br
3http://trec.nist.gov
4http://www.clef-campaign.org
21
formation are available for a given language
(e.g. ontologies of domain-specific knowl-
edge, general ontologies), their coverage, and
which tools can be used to help the task (e.g.
named entity recognisers, parsers, word sense
disambiguation tools). Furthermore, there is
no concensus as to the amount of resources
and tools that are needed in order to build a
working QA system with reasonable perfor-
mance.
For a resource rich language like English,
there is a consistent body of work exempli-
fied by systems such as JAVELIN (Nyberg et
al., 2002) and QuALiM (Kaisser, 2005). For
other languages, like Portuguese, and partic-
ularly the Brazilian variety, QA systems are
not as numerous. Over the years, there was
an increase in the number of participating
systems and data sources in the CLEF evalu-
ation. For instance, in 2004 there were 2 par-
ticipating systems, and in 2006 it had 4 sys-
tems and the best performance was obtained
by Priberam (Amaral et al, 2005) with 67%
accuracy (Magnini et al, 2006). Figure 1
summarizes the performance of the QA sys-
tems for Portuguese for QA@CLEF over the
years.
3 COMUNICA Architecture
The Comunica system is composed of five
modules: a manager module and four pro-
cessing modules, as shown in figure 2. The
manager is responsible for the integration
and communication with the speech recog-
nition, text processing, database access, and
speech synthesis modules.
Figure 2: Architecture of the system.
3.1 Speech Recognition
For continuous speech recognition of the
users? requests we use an automated phone
service. This module uses two research
fronts signal analysis (Fourier transform and
Wavelets). The coefficients obtained are
sequenced on three fronts for continuous
speech recognition: HMMs (Becerikli and
Oysal, 2007) TDDNN and NESTOR (Nasuto
et al, 2009). To train the models, a corpus
of FAMURS callcentre telephone interactions
has been recorded. The recognition focuses
on the vocabulary employed in the domain,
in this case municipal information related to
taxes from FAMURS. In order to do that,
it uses 2 ontologies to validate the candidate
words in the input: (a) a general purpose
and (b) a domain ontology. The recognised
transcribed input is passed to the manager
for further processing.
3.2 Text Processing
The manager sends the transcribed input
to be processed by the natural language
processing module. The natural language
queries are processed using shallow and deep
tools and accessing both a general and a do-
main specific ontologies (illustrated in Figure
3). This module needs to determine which
type of query the user performed and what
is the likely type of answer, based on mostly
lexical and syntactic information. This pro-
cess is divided into 3 mains steps: parsing,
concept identification and pattern selection.
In the first step, the input is parsed using
the PALAVRAS parser (Bick, 2002), and
the output provides information about the
particular pronoun (wh-word), subject and
other verbal complements in the sentence.
For concept identification, the system uses
the domain ontology, which contains the rel-
evant concepts to be used in next steps. The
ontologies also provide additional informa-
tion about nouns (such as hyperonymy and
synonymy) for determining which instances
of the concepts were present in the input.
For example, ?Gramado? is an instance of
22
Figure 1: Performance of QA systems for Portuguese QA-CLEF.
the concept ?city?. Both absolute and rela-
tive dates and periods (e.g. last quarter, first
week) need to be treated.
Finally, based on this information this
module selects from a set of pre-defined ques-
tion patterns linking concepts of the domain
ontology with SQL commands, the one which
contains the largest number of concepts in
common with the input, and sends it to the
manager in an XML format. If there is no
complete frame, this module identifies which
concepts are missing and returns this in the
XML output.
Figure 3: The domain ontology
3.3 Database Access
The search module is divided in two sub-
modules: one for searching information in a
structured database and the other for search-
ing in an unstructured knowledge base. It
receives as entry an XML file, containing
the original input in natural language and
the concepts identified in the question. The
structured search module receives the input
tagged with concepts of the ontology and an
identified search pattern, and selects a struc-
tured SQL query. These queries are prede-
fined according to the search patterns and
the structure of the database. For example,
in the case of the FAMURS domain, there
are concepts related to time period, cities
and taxes. When these 3 concepts are found
in the input, a special pattern is selected
which defines the kind of information that
must be retrieved from the database. An
SQL command is then executed in the struc-
tured database. All possible patterns are
mapped to a specific SQL command. These
commands have slots that are filled with in-
stances of the concepts identified in the sen-
tence. For example, names of cities are in-
stances of the concept ?city?. The retrieved
values are used for producing the answer in
natural language, using some predefined an-
swer patterns.
Otherwise, the system uses the ADS
Digital Company Virtual Assistant (VA)
(Duizith et al, 2004) to search the unstruc-
tured data (e.g. Frequently Asked Ques-
23
tions), using the lexical information to lo-
cate the answer associated to the most simi-
lar question. This answer is written in natu-
ral language and will be returned to the main
module of the system. If no similar question
is found according to a predefined degree of
similarity, the VA returns a standard answer.
3.4 Speech Synthesis
The text output to the user is synthesized,
resulting in an audio file that is transmitted
through the server.
3.5 Manager
The manager is responsible for the integra-
tion and communication of the modules. It
processes requests, interpreting the actions
to be taken and dispatching the requests to
specific modules. To start the interaction the
manager activates the speech recogniser, and
if no problem is detected with the input, it
is passed to to the text processing module.
In the case of missing information, the man-
ager informs the user that more information
is needed. Othwerise, the query is passed to
the database module. The database module
then returns the result of the query to the
manager, which sends this information to the
interface component.
All the components are SOA compliant
and designed as Web services. This allows
us to use a common and simple way of com-
munication among components, allowing a
certain degree of independence. Then com-
ponents can be implemented using different
technologies and may be distributed among
different servers, if needed.
4 System Demonstration
This is an ongoing project, and a working
version of the system will be demonstrated
through some text example interactions from
the FAMURS domain as the speech recog-
nizer and synthesizer are currently under de-
velopment. However, users will be able to
interact with the other modules, and experi-
ence the benefits of natural language inter-
action for accessing database information.
Acknowledgments
This research was partly supported by CNPq
(Projects 479824/2009-6 and 309569/2009-
5), FINEP and SEBRAE (COMUNICA
project FI- NEP/SEBRAE 1194/07).
References
Amaral, Carlos, Helena Figueira, Andre? F. T. Mar-
tins, Afonso Mendes, Pedro Mendes, and Cla?udia
Pinto. 2005. Priberam?s question answering sys-
tem for portuguese. In Peters, Carol, Fredric C.
Gey, Julio Gonzalo, Henning Mu?ller, Gareth J. F.
Jones, Michael Kluck, Bernardo Magnini, and
Maarten de Rijke, editors, CLEF, pages 410?419.
Springer.
Becerikli, Yasar and Yusuf Oysal. 2007. Modeling
and prediction with a class of time delay dynamic
neural networks. Applied Soft Computing, 7:1164?
1169.
Bick, Eckhard. 2002. The Parsing System Palavras -
Automatic Grammatical Analysis of Portuguese in
a Constraint Grammar Famework. Ph.D. thesis,
Aarhus University.
Duizith, Jose? Luiz, Lizandro Kirst da Silva, Daniel
Brahm, Gustavo Tagliassuchi, and Stanley Loh.
2004. A virtual assistant for websites. Revista
Eletronica de Sistemas de Informac?a?o, 3.
Kaisser, Michael. 2005. Qualim at trec 2005: Web-
question answering with framenet. In Proceedings
of the 2005 Edition of the Text REtrieval Confer-
ence, TREC 2005.
Magnini, Bernardo, Danilo Giampiccolo, Pamela
Forner, Christelle Ayache, Valentin Jijkoun, Petya
Osenova, Anselmo Pen?as, Paulo Rocha, Bogdan
Sacaleanu, and Richard F. E. Sutcliffe. 2006.
Overview of the clef 2006 multilingual question
answering track. In Peters, Carol, Paul Clough,
Fredric C. Gey, Jussi Karlgren, Bernardo Magnini,
Douglas W. Oard, Maarten de Rijke, and Maxi-
milian Stempfhuber, editors, CLEF, volume 4730
of Lecture Notes in Computer Science, pages 223?
256. Springer.
Nasuto, S.J., J.M. Bishop, and K. DeMeyerc. 2009.
Communicating neurons: A connectionist spik-
ing neuron implementation of stochastic diffusion
search. Neurocomputing, (72):704?712.
Nyberg, Eric, Teruko Mitamura, Jaime G. Carbonell,
James P. Callan, Kevyn Collins-Thompson,
Krzysztof Czuba, Michael Duggan, Laurie Hiyaku-
moto, N. Hu, Yifen Huang, Jeongwoo Ko, Lu-
cian Vlad Lita, S. Murtagh, Vasco Pedro, and
David Svoboda. 2002. The javelin question-
answering system at trec 2002. In TREC.
24
Coling 2010: Demonstration Volume, pages 57?60,
Beijing, August 2010
Multiword Expressions in the wild?
The mwetoolkit comes in handy
Carlos Ramisch?? Aline Villavicencio? Christian Boitet?
? GETALP ? Laboratory of Informatics of Grenoble, University of Grenoble
? Institute of Informatics, Federal University of Rio Grande do Sul
{ceramisch,avillavicencio}@inf.ufrgs.br Christian.Boitet@imag.fr
Abstract
The mwetoolkit is a tool for auto-
matic extraction of Multiword Expres-
sions (MWEs) from monolingual corpora.
It both generates and validates MWE can-
didates. The generation is based on sur-
face forms, while for the validation, a se-
ries of criteria for removing noise are pro-
vided, such as some (language indepen-
dent) association measures.1 In this paper,
we present the use of the mwetoolkit
in a standard configuration, for extracting
MWEs from a corpus of general-purpose
English. The functionalities of the toolkit
are discussed in terms of a set of selected
examples, comparing it with related work
on MWE extraction.
1 MWEs in a nutshell
One of the factors that makes Natural Language
Processing (NLP) a challenging area is the fact
that some linguistic phenomena are not entirely
compositional or predictable. For instance, why
do we prefer to say full moon instead of total moon
or entire moon if all these words can be consid-
ered synonyms to transmit the idea of complete-
ness? This is an example of a collocation, i.e. a
sequence of words that tend to occur together and
whose interpretation generally crosses the bound-
aries between words (Smadja, 1993). More gen-
erally, collocations are a frequent type of mul-
tiword expression (MWE), a sequence of words
that presents some lexical, syntactic, semantic,
pragmatic or statistical idiosyncrasies (Sag et al,
2002). The definition of MWE also includes a
wide range of constructions like phrasal verbs (go
1The first version of the toolkit was presented in
(Ramisch et al, 2010b), where we described a language- and
type-independent methodology.
ahead, give up), noun compounds (ground speed),
fixed expressions (a priori) and multiword termi-
nology (design pattern). Due to their heterogene-
ity, MWEs vary in terms of syntactic flexibility
(let alne vs the moon is at the full) and semantic
opaqueness (wheel chair vs pass away).
While fairly studied and analysed in general
Linguistics, MWEs are a weakness in current
computational approaches to language. This is
understandable, since the manual creation of lan-
guage resources for NLP applications is expen-
sive and demands a considerable amount of ef-
fort. However, next-generation NLP systems need
to take MWEs into account, because they corre-
spond to a large fraction of the lexicon of a na-
tive speaker (Jackendoff, 1997). Particularly in
the context of domain adaptation, where we would
like to minimise the effort of porting a given sys-
tem to a new domain, MWEs are likely to play a
capital role. Indeed, theoretical estimations show
that specialised lexica may contain between 50%
and 70% of multiword entries (Sag et al, 2002).
Empirical evidence confirms these estimations: as
an example, we found that 56.7% of the terms
annotated in the Genia corpus are composed by
two or more words, and this is an underestimation
since it does not include general-purpose MWEs
such as phrasal verbs and fixed expressions.
The goal of mwetoolkit is to aid lexicog-
raphers and terminographers in the task of creat-
ing language resources that include multiword en-
tries. Therefore, we assume that, whenever a tex-
tual corpus of the target language/domain is avail-
able, it is possible to automatically extract inter-
esting sequences of words that can be regarded as
candidate MWEs.
2 Inside the black box
MWE identification is composed of two phases:
first, we automatically generate a list of candi-
57
mle = c(w1 . . .wn)N
dice = n? c(w1 . . .wn)?ni=1 c(wi)
pmi = log2
c(w1 . . .wn)
E(w1 . . .wn)
t-score = c(w1 . . .wn)?E(w1 . . .wn)?c(w1 . . .wn)
Figure 1: A candidate is a sequence of words w1 to
wn, with word counts c(w1) . . .c(wn) and n-gram
count c(w1 . . .wn) in a corpus with N words. The
expected count if words co-occurred by chance is
E(w1 . . .wn)? c(w1)...c(wn)Nn?1 .
dates from the corpus; then we filter them, so that
we can discard as much noise as possible. Can-
didate generation uses flat linguistic information
such as surface forms, lemmas and parts of speech
(POS).2 We can then define target sequences of
POS, such as VERB NOUN sequences, or even
more fine-grained constraints which use lemmas,
like take NOUN and give NOUN, or POS patterns
that include wildcards that stand for any word or
POS.3 The optimal POS patterns for a given do-
main, language and MWE type can be defined
based on the analysis of the data.
For the candidate filtering a set of association
measures (AMs), listed in figure 1, are calculated
for each candidate. A simple threshold can sub-
sequently be applied to filter out all the candidates
for which the AMs fall below a user-defined value.
If a gold standard is available, the toolkit can build
a classifier, automatically annotating each candi-
date to indicate whether it is contained in the gold
standard (i.e. it is regarded as a true MWE) or
not (i.e. it is regarded as a non-MWE).4 This
annotation is not used to filter the lists, but only
2If tools like a POS tagger are not available for a lan-
guage/domain, it is possible to generate simple n-gram lists
(n = 1..10), but the quality will be inferior. A possible solu-
tion is to filter out candidates on a keyword basis, e.g. from
a list of stopwords).
3Although syntactic information can provide better re-
sults for some types of MWEs, like collocations (Seretan,
2008), currently no syntactic information is allowed as a cri-
terion for candidate generation, keeping the toolkit as simple
and language independent as possible.
4The gold standard can be a dictionary or a manually an-
notated list of candidates.
candidate fEP fgoogle class
status quo 137 1940K True
US navy 4 1320K False
International Cooperation 2 1150K False
Cooperation Agreement 188 115K True
Panama Canal 2 753K True
security institution 5 8190 False
lending institution 4 54800 True
human right 2 251K True
Human Rights 3067 3400K False
pro-human right 2 34 False
Table 1: Example of MWE candidates extracted
by mwetoolkit.
by the classifier to learn the relation between the
AMs and the MWE class of the candidate. This
is particularly useful because, to date, it remains
unclear which AM performs better for a partic-
ular type or language, and the classifier applies
measures according to their efficacy in filtering
the candidates.Some examples of output are pre-
sented in table 1.
3 Getting started
The toolkit is open source software that can
be freely downloaded (sf.net/projects/
mwetoolkit). As a demonstration, we present
the extraction of noun-noun compounds from the
general-purpose English Europarl (EP) corpus5.
To preprocess the corpus, we used the sen-
tence splitter and tokeniser provided with EP, fol-
lowed by a lowercasing treatment (integrated in
the toolkit), and lemmatisation and POS tagging
using the TreeTagger6. The tagset was simplified
since some distinctions among plural/singular and
proper nouns were irrelevant.
From the preprocessed corpus, we obtained all
sequences of 2 nouns, which resulted in 176,552
unique noun compound candidates. Then, we ob-
tained the corpus counts for the bigrams and their
component unigrams in the EP corpus. Adopt-
ing the web as a corpus, we also use the number
of pages retrieved by Google and by Yahoo! as
5www.statmt.org/europarl.
6http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/.
58
raw EN 
Europar
l
sentenc
e-split, 
lowerca
sed,
tokenis
ed, POS
-tagged
,
lemmat
ised Eu
roparl  Preproce
ssing
noun-no
un 
candida
tes
filtered-
1 
candida
tes
filtered-
2 
candida
tes  Count ca
ndidate
s
  Thresh
old
 Associa
tion me
as.
 Sort an
d thresh
old
mwetoo
lkit
Figure 2: Step-by-step demonstration on the EP
corpus.
counts. The mwetoolkit implements a cache
mechanism to avoid redundant queries, but to
speed up the process7, we filtered out all candi-
dates occurring less than two times in EP, which
reduced the list of candidates to 64,551 entries
(filtered-1 candidates in figure 2).
For the second filtering step, we calculated
four AMs for each of the three frequency sources
(EP, Google and Yahoo!). Some results on ma-
chine learning applied to the candidate lists of
the mwetoolkit can be found in Ramisch et al
(2010b). Here, we will limit ourselves to a dis-
cussion on some advantages and inconvenients of
the chosen approach by analysing a list of selected
examples.
4 Pros and cons
One of the biggest advantages of our approach is
that, since it is language independent, it is straight-
forward to apply it on corpora in virtually any
language. Moreover, it is not dependent on a
specific type of construction or syntactic formal-
ism. Of course, since it only uses limited linguis-
tic information, the accuracy of the resulting lists
can always be further improved with language-
dependent tools. In sum, the toolkit allows users
to perform systematic MWE extraction with con-
sistent intermediary files and well defined scripts
and arguments (avoiding the need for a series of ad
hoc separate scripts). Even if some basic knowl-
edge about how to run Python scripts and how to
7Yahoo! limits the queries to 5,000/day.
pass arguments to the command line is necessary,
the user is not required to be a programmer.
Nested MWEs are a problem in the current
approach. Table 1 shows two bigrams Interna-
tional Cooperation and Cooperation Agreement,
both evaluated as False candidates. However, they
could be considered as parts of a larger MWE In-
ternational Cooperation Agreement, but with the
current methodology it is not possible to detect
this kind of situation. Another case where the
candidate contains a MWE is the example pro-
human right, and in this case it would be neces-
sary to separate the prefix from the MWE, i.e. to
re-tokenise the words around the MWE candidate.
Indeed, tools for consistent tokenisation, specially
concerning dashes and slashes, could improve the
quality of the results, in particular for specialised
corpora.
The toolkit provides full integration with web
search engine APIs. The latter, however, are of
limited utility because search engines are not only
slow but also return more or less arbitrary num-
bers, some times even inconsistent (Ramisch et
al., 2010c). When large corpora like EP are avail-
able, we suggest that it is better to use its counts
rather than web counts. The toolkit provides an
efficient indexing mechanism, allowing for arbi-
trary n-grams to be counted in linear time.
The automatic evaluation of the candidates will
always be limited by the coverage of the reference
list. In the examples, Panama Canal is consid-
ered as a true MWE whereas US navy is not, but
both are proper names and the latter should also
be included as a true candidate. The same happens
for the candidates Human Rights and human right.
The mwetoolkit is an early prototype whose
simple design allows fine tuning of knowledge-
poor methods for MWE extraction. However, we
believe that there is room for improvement at sev-
eral points of the extraction methodology.
5 From now on
One of our goals for future versions is to be able
to extract bilingual MWEs from parallel or com-
parable corpora automatically. This could be done
through the inclusion of automatic word align-
ment information. Some previous experiments
show, however, that this may not be enough, as
59
automatic word alignment uses almost no lin-
guistic information and its output is often quite
noisy (Ramisch et al, 2010a). Combining align-
ment and shallow linguistic information seems a
promising solution for the automatic extraction
of bilingual MWEs. The potential uses of these
lexica are multiple, but the most obvious appli-
cation is machine translation. On the one hand,
MWEs could be used to guide the word align-
ment process. For instance, this could solve the
problem of aligning a language where compounds
are separate words, like French, with a language
that joins compound words together, like Ger-
man. In statistical machine translation systems,
MWEs could help to filter phrase tables or to boost
the scores of phrases which words are likely to
be multiwords.Some types of MWE (e.g. collo-
cations) could help in the semantic disambigua-
tion of words in the source language. The sense
of a word defined by its collocate can allow to
chose the correct target word or expression (Sere-
tan, 2008).
We would also like to improve the techniques
implemented for candidate filtering. Related work
showed that association measures based on con-
tingency tables are more robust to data sparseness
(Evert and Krenn, 2005). However, they are pair-
wise comparisons and their application on arbi-
trarily long n-grams is not straightforward. An
heuristics to adapt these measures is to apply them
recursively over increasing n-gram length. Other
features that could provide better classification
are context words, linguistic information coming
from simple word lexica, syntax, semantic classes
and domain-specific keywords. While for poor-
resourced languages we can only count on shallow
linguistic information, it is unreasonable to ignore
available information for other languages. In gen-
eral, machine learning performs better when more
information is available (Pecina, 2008).
We would like to evaluate our toolkit on several
data sets, varying the languages, domains and tar-
get MWE types. This would allow us to assign
its quantitative performance and to compare it to
other tools performing similar tasks. Additionally,
we could evaluate how well the classifiers perform
across languages and domains. In short, we be-
lieve that the mwetoolkit is an important first
step toward robust and reliable MWE treatment.
It is a freely available core application providing
flexible tools and coherent up-to-date documenta-
tion, and these are essential characteristics for the
extension and support of any computer system.
Acknowledgements
This research was partly supported by CNPq
(Projects 479824/2009-6 and 309569/2009-5),
FINEP and SEBRAE (COMUNICA project
FINEP/SEBRAE 1194/07).
References
Evert, Stefan and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statis-
tical association measures. Comp. Speech & Lang.
Special issue on MWEs, 19(4):450?466.
Jackendoff, Ray. 1997. Twistin? the night away. Lan-
guage, 73:534?559.
Pecina, Pavel. 2008. Reference data for czech colloca-
tion extraction. In Proc. of the LREC Workshop To-
wards a Shared Task for MWEs (MWE 2008), pages
11?14, Marrakech, Morocco, Jun.
Ramisch, Carlos, Helena de Medeiros Caseli, Aline
Villavicencio, Andr? Machado, and Maria Jos? Fi-
natto. 2010a. A hybrid approach for multiword ex-
pression identification. In Proc. of the 9th PROPOR
(PROPOR 2010), volume 6001 of LNCS (LNAI),
pages 65?74, Porto Alegre, RS, Brazil. Springer.
Ramisch, Carlos, Aline Villavicencio, and Christian
Boitet. 2010b. mwetoolkit: a framework for multi-
word expression identification. In Proc. of the Sev-
enth LREC (LREC 2010), Malta, May. ELRA.
Ramisch, Carlos, Aline Villavicencio, and Christian
Boitet. 2010c. Web-based and combined language
models: a case study on noun compound identifica-
tion. In Proc. of the 23th COLING (COLING 2010),
Beijing, China, Aug.
Sag, Ivan, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for NLP.
In Proc. of the 3rd CICLing (CICLing-2002), vol-
ume 2276/2010 of LNCS, pages 1?15, Mexico City,
Mexico, Feb. Springer.
Seretan, Violeta. 2008. Collocation extraction based
on syntactic parsing. Ph.D. thesis, University of
Geneva, Geneva, Switzerland.
Smadja, Frank A. 1993. Retrieving collocations from
text: Xtract. Comp. Ling., 19(1):143?177.
60
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419?424,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Nothing like Good Old Frequency:
Studying Context Filters for Distributional Thesauri
Muntsa Padr
?
o,
?
Marco Idiart
?
, Carlos Ramisch
?
, Aline Villavicencio
?
?
Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?
Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
?
Aix Marseille Universit?e, CNRS, LIF UMR 7279, 13288, Marseille (France)
muntsa.padro@inf.ufrgs.br, marco.idiart@gmail.com,
carlos.ramisch@lif.univ-mrs.fr, avillavicencio@inf.ufrgs.br
Abstract
Much attention has been given to the
impact of informativeness and similar-
ity measures on distributional thesauri.
We investigate the effects of context fil-
ters on thesaurus quality and propose the
use of cooccurrence frequency as a sim-
ple and inexpensive criterion. For eval-
uation, we measure thesaurus agreement
with WordNet and performance in answer-
ing TOEFL-like questions. Results illus-
trate the sensitivity of distributional the-
sauri to filters.
1 Introduction
Large-scale distributional thesauri created auto-
matically from corpora (Grefenstette, 1994; Lin,
1998; Weeds et al., 2004; Ferret, 2012) are an
inexpensive and fast alternative for representing
semantic relatedness between words, when man-
ually constructed resources like WordNet (Fell-
baum, 1998) are unavailable or lack coverage. To
construct a distributional thesaurus, the (colloca-
tional or syntactic) contexts in which a target word
occurs are used as the basis for calculating its sim-
ilarity with other words. That is, two words are
similar if they share a large proportion of contexts.
Much attention has been devoted to refin-
ing thesaurus quality, improving informativeness
and similarity measures (Lin, 1998; Curran and
Moens, 2002; Ferret, 2010), identifying and de-
moting bad neighbors (Ferret, 2013), or using
more relevant contexts (Broda et al., 2009; Bie-
mann and Riedl, 2013). For the latter in particular,
as words vary in their collocational tendencies, it
is difficult to determine how informative a given
context is. To remove uninformative and noisy
contexts, filters have often been applied like point-
wise mutual information (PMI), lexicographer?s
mutual information (LMI) (Biemann and Riedl,
2013), t-score (Piasecki et al., 2007) and z-score
(Broda et al., 2009). However, the selection of a
measure and of a threshold value for these filters
is generally empirically determined. We argue that
these filtering parameters have a great influence on
the quality of the generated thesauri.
The goal of this paper is to quantify the im-
pact of context filters on distributional thesauri.
We experiment with different filter methods and
measures to assess context significance. We pro-
pose the use of simple cooccurrence frequency as
a filter and show that it leads to better results than
more expensive measures such as LMI or PMI.
Thus we propose a cheap and effective way of fil-
tering contexts while maintaining quality.
This paper is organized as follows: in ?2 we
discuss evaluation of distributional thesauri. The
methodology adopted in the work and the results
are discussed in ?3 and ?4. We finish with some
conclusions and discussion of future work.
2 Related Work
In a nutshell, the standard approach to build a dis-
tributional thesaurus consists of: (i) the extraction
of contexts for the target words from corpora, (ii)
the application of an informativeness measure to
represent these contexts and (iii) the application of
a similarity measure to compare sets of contexts.
The contexts in which a target word appears can
be extracted in terms of a window of cooccurring
(content) words surrounding the target (Freitag et
al., 2005; Ferret, 2012; Erk and Pado, 2010) or in
terms of the syntactic dependencies in which the
target appears (Lin, 1998; McCarthy et al., 2003;
Weeds et al., 2004). The informativeness of each
context is calculated using measures like PMI, and
t-test while the similarity between contexts is cal-
culated using measures like Lin?s (1998), cosine,
Jensen-Shannon divergence, Dice or Jaccard.
Evaluation of the quality of distributional the-
sauri is a well know problem in the area (Lin,
419
1998; Curran and Moens, 2002). For instance, for
intrinsic evaluation, the agreement between the-
sauri has been examined, looking at the average
similarity of a word in the thesauri (Lin, 1998),
and at the overlap and rank agreement between the
thesauri for target words like nouns (Weeds et al.,
2004). Although much attention has been given to
the evaluation of various informativeness and sim-
ilarity measures, a careful assessment of the ef-
fects of filtering on the resulting thesauri is also
needed. For instance, Biemann and Riedl (2013)
found that filtering a subset of contexts based on
LMI increased the similarity of a thesaurus with
WordNet. In this work, we compare the impact of
using different types of filters in terms of thesaurus
agreement with WordNet, focusing on a distribu-
tional thesaurus of English verbs. We also propose
a frequency-based saliency measure to rank and
filter contexts and compare it with PMI and LMI.
Extrinsic evaluation of distributional thesauri
has been carried out for tasks such as En-
glish lexical substitution (McCarthy and Navigli,
2009), phrasal verb compositionality detection
(McCarthy et al., 2003) and the WordNet-based
synonymy test (WBST) (Freitag et al., 2005). For
comparative purposes in this work we adopt the
latter.
3 Methodology
We focus on thesauri of English verbs constructed
from the BNC (Burnard, 2007)
1
. Contexts are ex-
tracted from syntactic dependencies generated by
RASP (Briscoe et al., 2006), using nouns (heads
of NPs) which have subject and direct object rela-
tions with the target verb. Thus, each target verb
is represented by a set of triples containing (i) the
verb itself, (ii) a context noun and (iii) a syntac-
tic relation (object, subject). The thesauri were
constructed using Lin?s (1998) method. Lin?s ver-
sion of the distributional hypothesis states that two
words (verbs v
1
and v
2
in our case) are similar if
they share a large proportion of contexts weighted
by their information content, assessed with PMI
(Bansal et al., 2012; Turney, 2013).
In the literature, little attention is paid to context
filters. To investigate their impact, we compare
two kinds of filters, and before calculating similar-
ity using Lin?s measure, we apply them to remove
1
Even though larger corpora are available, we use a tradi-
tional carefully constructed corpus with representative sam-
ples of written English to control the quality of the thesaurus.
potentially noisy triples:
? Threshold (th): we remove triples that oc-
cur less than a threshold th. Threshold values
vary from 1 to 50 counts per triple.
? Relevance (p): we keep only the top p most
relevant contexts for each verb, were rele-
vance is defined according to the following
measures: (a) frequency, (b) PMI, and (c)
LMI (Biemann and Riedl, 2013). Values of
p vary between 10 and 1000.
In this work, we want to answer two ques-
tions: (a) Do more selective filters improve intrin-
sic evaluation of thesaurus? and (b) Do they also
help in extrinsic evaluation?
For intrinsic evaluation, we determine agree-
ment between a distributional thesaurus and Word-
Net as the path similarities for the first k distri-
butional neighbors of a verb. A single score is
obtained by averaging the similarities of all verbs
with their k first neighbors. The higher this score
is, the closer the neighbors are to the target in
WordNet, and the better the thesaurus. Several
values of k were tested and the results showed ex-
actly the same curve shapes for all values, with
WordNet similarity decreasing linearly with k. For
the remainder of the paper we adopt k = 10, as it
is widely used in the literature.
For extrinsic evaluation, we use the WBST set
for verbs (Freitag et al., 2005) with 7,398 ques-
tions and an average polysemy of 10.4. The task
consists of choosing the most suitable synonym
for a word among a set of four options. The the-
saurus is used to rank the candidate answers by
similarity scores, and select the first one as the
correct synonym. As discussed by Freitag et al.
(2005), the upper bound reached by English na-
tive speakers is 88.4% accuracy, and simple lower
bounds are 25% (random choice) and 34.5% (al-
ways choosing the most frequent option).
4 Results
Figure 1 shows average WordNet similarities for
thesauri built filtering by frequency threshold th
and by p most frequent contexts. Table 1 sum-
marizes the parametrization leading to the best
WordNet similarity for each kind of filter. In all
cases we show the results obtained for different
frequency ranges
2
as well as the results when av-
eraging over all verbs.
2
In order to study the influence of verb frequency on the
results, we divide the verbs in three groups: high-frequency
420
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 1  10
W
N
 
s
i
m
i
l
a
r
i
t
y
th 
WordNet path Similarity for different frequency ranges, k=10Filtering triples with frequency under th
all verbshigh frequent verbsmid frequent verbslow frequent verbs  0
 0.05
 0.1
 0.15
 0.2
 0.25
 10  100  1000
W
N
 
s
i
m
i
l
a
r
i
t
y
p 
WordNet path Similarity for different frequency ranges, k=10Keeping p most frequent triples per verb
all verbshigh frequent verbsmid frequent verbslow frequent verbs
Figure 1: WordNet scores for verb frequency ranges, filtering by frequency threshold th (left) and p most
frequent contexts (right).
Filter All verbs Frequency range
Low Mid High
No filter - 0.148 - 0.101 - 0.144 - 0.198
Filter low freq. contexts th = 50 0.164 th = 50 0.202 th = 50 0.154 th = 1 0.200
Keep p contexts (freq.) p = 200 0.158 p = 500 0.138 p = 200 0.149 p = 200 0.206
Keep p contexts (PMI) p = 1000 0.139 p = 1000 0.101 p = 1000 0.136 p = 1000 0.181
Keep p contexts (LMI) p = 200 0.155 p = 100 0.112 p = 200 0.147 p = 200 0.208
Table 1: Best scores obtained for each filter for all verbs and frequency ranges. Scores are given in terms
of WordNet path. Confidence interval is arround ? 0.002 in all cases.
When using a threshold filter (Figure 1 left),
high values lead to better performance for mid-
and low-frequency verbs. This is because, for high
th values, there are few low and mid-frequency
verbs left, since a verb that occurs less has less
chances to be seen often in the same context. The
similarity for verbs with no contexts over the fre-
quency threshold cannot be assessed and as a con-
sequence those verbs are not included in the fi-
nal thesaurus. As Figure 2 shows, the number
of verbs decreases much faster for low and mid
frequency verbs when th increases.
3
For exam-
ple, for th = 50, there are only 7 remaining low-
frequency verbs in the thesaurus and these tend
to be idiosyncratic multiword expressions. One
example is wreak, and the only triple contain-
ing this verb that appeared more than 50 times is
wreak havoc (71 occurrences). The neighbors of
this verb are cause and play, which yield a good
similarity score in WordNet. Therefore, although
higher thresholds result in higher similarities for
low and mid-frequency verbs, this comes at a cost,
as the number of verbs included in the thesaurus
decreases considerably.
(||v|| ? 500), mid-frequency (150 ? ||v|| < 500) and low-
frequency (||v|| < 150).
3
For p most salient contexts, the number of verbs does not
vary and is the same shown in Figure 2 for th = 1 (no filter).
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 1  10
N
u
m
b
e
r
 
o
f
 
v
e
r
b
s
th 
Number of verbs in WordNetFiltering triples with frequency under th
all verbshigh frequent verbsmid frequent verbslow frequent verbs
Figure 2: Number of verbs per frequency ranges
when filtering by context frequency threshold th
As expected, the best performance is obtained
for high-frequency verbs and no filter, since it re-
sults in more context information per verb. In-
creasing th decreases similarity due to the removal
of some of these contexts. In average, higher th
values lead to better overall similarity among the
frequency ranges (from 0.148 with th = 1 to
0.164 with th = 50). The higher the threshold,
the more high-frequency verbs will prevail in the
thesauri, for which the WordNet path similarities
are higher.
On the other hand, when adopting a relevance
421
 0
 0.2
 0.4
 0.6
 0.8
 1
 1  10
P
,
 
R
,
 
F
1
th 
WBST task: P, R and F1Filtering triples with frequency under th
PrecisionRecallF1  0
 0.2
 0.4
 0.6
 0.8
 1
 10  100  1000
P
,
 
R
,
 
F
1
p 
WBST task: P, R and F1Keeping p most frequent triples per verb
PrecisionRecallF1
Figure 3: WBST task scores filtering by frequency threshold th (left) and p most frequent contexts
(right).
filter of keeping the p most relevant contexts for
each verb (Figure 1 right), we obtain similar re-
sults, but more stable thesauri. The number of
verbs remains constant, since we keep a fixed
number of contexts for each verb and verbs are not
removed when the threshold is modified. Word-
Net similarity increases as more contexts are taken
into account, for all frequency ranges. There is a
maximum around p = 200, though larger values
do not lead to a drastic drop in quality. This sug-
gests that the noise introduced by low-frequency
contexts is compensated by the increase of infor-
mativeness for other contexts. An ideal balance
is reached by the lowest possible p that maintains
high WordNet similarity, since the lower the p the
faster the thesaurus construction.
In terms of saliency measure, when keeping
only the p most relevant contexts, sorting them
with PMI leads to much worse results than LMI
or frequency, as PMI gives too much weight to
infrequent combinations. This is consistent with
results of Biemann and Riedl (2013). Regarding
LMI versus frequency, the results using the latter
are slightly better (or with no significant differ-
ence, depending on the frequency range). The ad-
vantage of using frequency instead of LMI is that
it makes the process simpler and faster while lead-
ing to equal or better performance in all frequency
ranges. Therefore for the extrinsic evaluation us-
ing WBST task, we use frequency to select the
p most relevant contexts and then compute Lin?s
similarity using only those contexts.
Figure 3 shows the performance of the thesauri
in the WBST task in terms of precision, recall and
F1.
4
For precision, the best filter is to remove con-
4
Filters based on LMI and PMI were also tested with the
texts occurring less than th times, but, this also
leads to poor recall, since many verbs are left out
of the thesauri and their WSBT questions cannot
be answered. On the other hand, keeping the most
relevant p contexts leads to more stable results and
when p is high (right plot), they are similar to those
shown in the left plot of Figure 3.
4.1 Discussion
The answer to our questions in Section 3 is yes,
more selective filters improve intrinsic and extrin-
sic thesaurus quality. The use of both filtering
methods results in thesauri in which the neighbors
of target verbs are closer in WordNet and get better
scores in TOEFL-like tests. However, the fact that
filtering contexts with frequency under th removes
verbs in the final thesaurus is a drawback, as high-
lighted in the extrinsic evaluation on the WBST
task.
Furthermore, we demonstrated that competitive
results can be obtained keeping only the p most
relevant contexts per verb. On the one hand, this
method leads to much more stable thesauri, with
the same verbs for all values of p. On the other
hand, it is important to highlight that the best re-
sults to assess the relevance of the contexts are ob-
tained using frequency while more sophisticated
filters such as LMI do not improve thesaurus qual-
ity. Although an LMI filter is relatively fast com-
pared to dimensionality reduction techniques such
as singular value decomposition (Landauer and
Dumais, 1997), it is still considerably more expen-
sive than a simple frequency filter.
In short, our experiments indicate that a reason-
same results as intrinsic evaluation: sorting contexts by fre-
quency leads to better results.
422
able trade-off between noise, coverage and com-
putational efficiency is obtained for p = 200 most
frequent contexts, as confirmed by intrinsic and
extrinsic evaluation. Frequency threshold th is
not recommended: it degrades recall because the
contexts for many verbs are not frequent enough.
This result is useful for extracting distributional
thesauri from very large corpora like the UKWaC
(Ferraresi et al., 2008) by proposing an alterna-
tive that minimizes the required computational re-
sources while efficiently removing a significant
amount of noise.
5 Conclusions and Future Work
In this paper we addressed the impact of filters
on the quality of distributional thesauri, evaluat-
ing a set of standard thesauri and different filtering
methods. The results suggest that the use of fil-
ters and their parameters greatly affect the thesauri
generated. We show that it is better to use a filter
that selects the most relevant contexts for a verb
than to simply remove rare contexts. Furthermore,
the best performance was obtained with the sim-
plest method: frequency was found to be a simple
and inexpensive measure of context salience. This
is especially important when dealing with large
amounts of data, since computing LMI for all con-
texts would be computationally costly. With our
proposal to keep just the p most frequent contexts
per verb, a great deal of contexts are cheaply re-
moved and thus the computational power required
for assessing similarity is drastically reduced.
As future work, we plan to use these filters to
build thesauri from larger corpora. We would like
to generalize our findings to other syntactic con-
figurations (e.g. noun-adjective) as well as to other
similarity and informativeness measures. For in-
stance, ongoing experiments indicate that the same
parameters apply when Lin?s similarity is replaced
by cosine. Finally, we would like to compare the
proposed heuristics with more sophisticated filter-
ing strategies like singular value decomposition
(Landauer and Dumais, 1997) and non-negative
matrix factorization (Van de Cruys, 2009).
Acknowledgments
We would like to thank the support of projects
CAPES/COFECUB 707/11, PNPD 2484/2009,
FAPERGS-INRIA 1706-2551/13-7, CNPq
312184/2012-3, 551964/2011-1, 482520/2012-4
and 312077/2012-2.
References
Mohit Bansal, John DeNero, and Dekang Lin. 2012.
Unsupervised translation sense clustering. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
773?782, Montr?eal, Canada, June. Association for
Computational Linguistics.
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2D! a framework for lexical expansion with con-
textual similarity. Journal of Language Modelling,
1(1).
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In James
Curran, editor, Proc. of the COLING/ACL 2006 In-
teractive Presentation Sessions, pages 77?80, Sid-
ney, Australia, Jul. ACL.
Bartosz Broda, Maciej Piasecki, and Stan Szpakow-
icz. 2009. Rank-based transformation in mea-
suring semantic relatedness. In Proceedings of
the 22nd Canadian Conference on Artificial Intel-
ligence: Advances in Artificial Intelligence, Cana-
dian AI ?09, pages 187?190, Berlin, Heidelberg.
Springer-Verlag.
Lou Burnard. 2007. User Reference Guide for the
British National Corpus. Technical report, Oxford
University Computing Services, Feb.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Proc.of
the ACL 2002 Workshop on Unsupervised Lexical
Acquisition, pages 59?66, Philadelphia, Pennsylva-
nia, USA. ACL.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In Proc. of the
ACL 2010 Conference Short Papers, pages 92?97,
Uppsala, Sweden, Jun. ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). MIT Press, May. 423 p.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluat-
ing UKWaC, a very large web-derived corpus of En-
glish. In In Proceedings of the 4th Web as Corpus
Workshop (WAC-4.
Olivier Ferret. 2010. Testing semantic similarity mea-
sures for extracting synonyms from a corpus. In
Proc. of the Seventh LREC (LREC 2010), pages
3338?3343, Valetta, Malta, May. ELRA.
Olivier Ferret. 2012. Combining bootstrapping and
feature selection for improving a distributional the-
saurus. In ECAI, pages 336?341.
Olivier Ferret. 2013. Identifying bad semantic neigh-
bors for improving distributional thesauri. In Proc.
of the 51st ACL (Volume 1: Long Papers), pages
561?571, Sofia, Bulgaria, Aug. ACL.
423
Dayne Freitag, Matthias Blume, John Byrnes, Ed-
mond Chow, Sadik Kapadia, Richard Rohwer, and
Zhiqiang Wang. 2005. New experiments in distri-
butional representations of synonymy. In Ido Dagan
and Dan Gildea, editors, Proc. of the Ninth CoNLL
(CoNLL-2005), pages 25?32, University of Michi-
gan, MI, USA, Jun. ACL.
Gregory Grefenstette. 1994. Explorations in Au-
tomatic Thesaurus Discovery. Springer, Norwell,
MA, USA.
Thomas K Landauer and Susan T. Dumais. 1997. A
solution to platos problem: The latent semantic anal-
ysis theory of acquisition, induction, and represen-
tation of knowledge. Psychological review, pages
211?240.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proc. of the 36th ACL and
17th COLING, Volume 2, pages 768?774, Montreal,
Quebec, Canada, Aug. ACL.
Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Francis Bond, Anna Korhonen,
Diana McCarthy, and Aline Villavicencio, editors,
Proc. of the ACL Workshop on MWEs: Analysis, Ac-
quisition and Treatment (MWE 2003), pages 73?80,
Sapporo, Japan, Jul. ACL.
Maciej Piasecki, Stanislaw Szpakowicz, and Bartosz
Broda. 2007. Automatic selection of heterogeneous
syntactic features in semantic similarity of polish
nouns. In Proceedings of the 10th international
conference on Text, speech and dialogue, TSD?07,
pages 99?106, Berlin, Heidelberg. Springer-Verlag.
Peter D. Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and
paraphrase. 1:353?366.
Tim Van de Cruys. 2009. A non-negative tensor factor-
ization model for selectional preference induction.
In Proceedings of the Workshop on Geometrical
Models of Natural Language Semantics, pages 83?
90, Athens, Greece, March. Association for Compu-
tational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proc. of the 20th COLING (COL-
ING 2004), pages 1015?1021, Geneva, Switzerland,
Aug. ICCL.
424
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1321?1330,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Language Acquisition and Probabilistic Models: keeping it simple
Aline Villavicencio?, Marco Idiart?Robert Berwick?, Igor Malioutov?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
?LIDS, Dept. of EECS, Massachusetts Institute of Technology (USA)
? CSAIL, Dept. of EECS, Massachusetts Institute of Technology (USA)
avillavicencio@inf.ufrgs.br, marco.idiart@if.ufrgs.br
berwick@csail.mit.edu, igorm@mit.edu
Abstract
Hierarchical Bayesian Models (HBMs)
have been used with some success
to capture empirically observed pat-
terns of under- and overgeneralization
in child language acquisition. How-
ever, as is well known, HBMs are
?ideal? learning systems, assuming ac-
cess to unlimited computational re-
sources that may not be available
to child language learners. Conse-
quently, it remains crucial to carefully
assess the use of HBMs along with al-
ternative, possibly simpler, candidate
models. This paper presents such
an evaluation for a language acquisi-
tion domain where explicit HBMs have
been proposed: the acquisition of En-
glish dative constructions. In particu-
lar, we present a detailed, empirically-
grounded model-selection compari-
son of HBMs vs. a simpler alternative
based on clustering along with max-
imum likelihood estimation that we
call linear competition learning (LCL).
Our results demonstrate that LCL can
match HBM model performance with-
out incurring on the high computa-
tional costs associated with HBMs.
1 Introduction
In recent years, with advances in probability
and estimation theory, there has been much
interest in Bayesian models (BMs) (Chater,
Tenenbaum, and Yuille, 2006; Jones and
Love, 2011) and their application to child lan-
guage acquisition with its challenging com-
bination of structured information and in-
complete knowledge, (Perfors, Tenenbaum,
and Wonnacott, 2010; Hsu and Chater, 2010;
Parisien, Fazly, and Stevenson, 2008; Parisien
and Stevenson, 2010) as they offer several ad-
vantages in this domain. They can readily
handle the evident noise and ambiguity of ac-
quisition input, while at the same time pro-
viding efficiency via priors that mirror known
pre-existing language biases. Further, hierar-
chical Bayesian Models (HBMs) can combine
distinct abstraction levels of linguistic knowl-
edge, from variation at the level of individ-
ual lexical items, to cross-item variation, using
hyper-parameters to capture observed pat-
terns of both under- and over-generalization
as in the acquisition of e.g. dative alterna-
tions in English (Hsu and Chater, 2010; Per-
fors, Tenenbaum, and Wonnacott, 2010), and
verb frames in a controlled artificial language
(Wonnacott, Newport, and Tanenhaus, 2008).
HBMs can thus be viewed as providing a
?rational? upper bound on language learn-
ability, yielding optimal models that account
for observed data while minimizing any re-
quired prior information. In addition, the
clustering implicit in HBM modeling intro-
duces additional parameters that can be tuned
to specific data patterns. However, this comes
at a well-known price: HBMs generally are
also ideal learning systems, known to be
computationally infeasible (Kwisthout, Ware-
ham, and van Rooij, 2011). Approximations
proposed to ensure computational tractabil-
ity, like reducing the number of classes that
need to be learned may also be linguisti-
cally and cognitively implausible. For in-
stance, in terms of verb learning, this could
1321
take the form of reducing the number of sub-
categorization frames to the relevant subset,
as in (Perfors, Tenenbaum, and Wonnacott,
2010), where only 2 frames are considered for
?take?, when in fact it is listed in 6 frames
by Levin (1993). Finally, comparison of vari-
ous Bayesian models of the same task is rare
(Jones and Love, 2011) and Bayesian infer-
ence generally can be demonstrated as sim-
ply one class of regularization or smooth-
ing techniques among many others; given the
problem at hand, there may well be other,
equally compelling regularization methods
for dealing with the bias-variance dilemma
(e.g., SVMs (Shalizi, 2009)). Consequently, the
relevance of HBMs for cognitively accurate ac-
counts of human learning remains uncertain
and needs to be carefully assessed.
Here we argue that the strengths of HBMs
for a given task must be evaluated in light of
their computational and cognitive costs, and
compared to other viable alternatives. The fo-
cus should be on finding the simplest statis-
tical models consistent with a given behav-
ior, particularly one that aligns with known
cognitive limitations. In the case of many
language acquisition tasks this behavior often
takes the form of overgeneralization, but with
eventual convergence to some target language
given exposure to more data.
In particular, in this paper we consider how
children acquire English dative verb construc-
tions, comparing HBMs to a simpler alterna-
tive, a linear competition learning (LCL) al-
gorithm that models the behavior of a given
verb as the linear competition between the ev-
idence for that verb, and the average behav-
ior of verbs belonging to its same class. The
results show that combining simple cluster-
ing methods along with ordinary maximum
likelihood estimation yields a result compara-
ble to HBM performance, providing an alter-
native account of the same facts, without the
computational costs incurred by HBM models
that must rely, for example, on Markov Chain
Monte Carlo (MCMC) methods for numeri-
cally integrating complex likelihood integrals,
or on Chinese Restaurant Process (CRP) for
producing partitions.
In terms of Marr?s hierarchy (Marr, 1982)
learning verb alternations is an abstract com-
putational problem (Marr?s type I), solvable
by many type II methods combining repre-
sentations (models, viz. HBMs or LCLs) with
particular algorithms. The HBM convention
of adopting ideal learning amounts to invok-
ing unbounded algorithmic resources, solv-
ability in principle, even though in practice
such methods, even approximate ones, are
provably NP-hard (cf. (Kwisthout, Wareham,
and van Rooij, 2011)). Assuming cognitive
plausibility as a desideratum, we therefore ex-
amine whether HBMs can also be approxi-
mated by another type II method (LCLs) that
does not demand such intensive computa-
tion. Any algorithm that approximates an
HBM can be viewed as implementing a some-
what different underlying model; if it repli-
cates HBM prediction performance but is sim-
pler and less computationally complex then
we assume it is preferable.
This paper is organized as follows: we start
with a discussion of formalizations of lan-
guage acquisition tasks, ?2. We present our
experimental framework for the dative acqui-
sition task, formalizing a range of learning
models from simple MLE methods to HBM
techniques, ?3, and a computational evalua-
tion of each model, ?4. We finish with conclu-
sions and possibilities for future work, ?5.
2 Evidence in Language Acquisition
A familiar problem for language acquisition is
how children learn which verbs participate in
so-called dative alternations, exemplified by
the child-produced sentences 1 to 3, from the
Brown (1973) corpus in CHILDES (MacWhin-
ney, 1995).
1. you took me three scrambled eggs (a direct object da-
tive (DOD) from Adam at age 3;6)
2. Mommy can you fix dis for me ? (a prepositional da-
tive (PD) from Adam at age 4;7)
3. *Mommy, fix me my tiger (from Adam at age 5;2)
Examples like these show that children gen-
eralize their use of verbs. For example, in sen-
tence (1), the child Adam uses take as a DOD
before any recorded occurrence of a similar
use of take in adult speech to Adam. Such
verbs alternate because they can also occur
with a prepositional form, as in sentence (2).
However, sometimes a child?s use of verbs like
1322
these amounts to an overgeneralization ? that
is, their productive use of a verb in a pattern
that does not occur in the adult grammar, as in
sentence (3), above. Faced with these two verb
frames the task for the learner is to decide for a
particular verb if it is a non-alternating DOD
only verb, a PD only verb, or an alternating
verb that allows both forms.
This ambiguity raises an important learn-
ability question, conventionally known as
Baker?s paradox (Baker, 1979). On the as-
sumption that children only receive positive
examples of verb forms, then it is not clear
how they might recover from the overgener-
alization exhibited in sentence (3) above, be-
cause they will never receive positive sen-
tences from adults like (3), using fix in a DOD
form. As has long been noted, if negative ex-
amples were systematically available to learn-
ers, then this problem would be solved, since
the child would be given evidence that the
DOD form is not possible in the adult gram-
mar. However, although parental correction
could be considered to be a source of negative
evidence, it is neither systematic nor generally
available to all children (Marcus, 1993). Even
when it does occur, all careful studies have in-
dicated that it seems mostly concerned with
semantic appropriateness rather than syntax.
In the cases where it is related to syntax, it
is often difficult to determine what the cor-
rection refers to in the utterance and besides
children seem to be oblivious to the correction
(Brown and Hanlon, 1970; Ingram, 1989).
One alternative solution to Baker?s paradox
that has been widely discussed at least since
Chomsky (1981) is the use of indirect negative
evidence. On the indirect negative evidence
model, if a verb is not found where it would
be expected to occur, the learner may con-
clude it is not part of the adult grammar. Cru-
cially, the indirect evidence model is inher-
ently statistical. Different formalizations of in-
direct negative evidence have been incorpo-
rated in several computational learning mod-
els for learning e.g. grammars (Briscoe, 1997;
Villavicencio, 2002; Kwiatkowski et al, 2010);
dative verbs (Perfors, Tenenbaum, and Won-
nacott, 2010; Hsu and Chater, 2010); and mul-
tiword verbs (Nematzadeh, Fazly, and Steven-
son, 2013). Since a number of closely related
models can all implement the indirect nega-
tive evidence approach, the decision of which
one to choose for a given task may not be en-
tirely clear. In this paper we compare a range
of statistical models consistent with a certain
behavior: early overgeneralization, with even-
tual convergence to the correct target on the
basis of exposure to more data.
3 Materials and Methods
3.1 Dative Corpora
To emulate a child language acquisition en-
vironment we use naturalistic longitudinal
child-directed data, from the Brown corpus in
CHILDES, for one child (Adam) for a subset
of 19 verbs in the DOD and PD verb frames,
figure 1. This dataset was originally reported
in Perfors, Tenenbaum, and Wonnacott (2010),
and longitudinal and incremental aspects to
acquisition are approximated by dividing the
data available into 5 incremental epochs (E1 to
E5 in the figures), where at the final epoch the
learner has seen the full corpus.
Model comparison requires a gold standard
database for acquisition, reporting which
frames have been learned for which verbs at
each stage, and how likely a child is of mak-
ing creative uses of a particular verb in a new
frame. An independent gold standard with
developmental information (e.g. Gropen et
al. (1989)) would clearly be ideal. Absent
this, a first step is demonstrating that sim-
pler alternative models can replicate HBM
performance on their own terms. Therefore,
the gold standard we use for evaluation is
the classification predicted by Perfors, Tenen-
baum, and Wonnacott (2010). The evaluations
reported in our analysis take into account in-
trinsic characteristics of each model in rela-
tion to the likelihoods of the verbs, to deter-
mine the extent to which the models go be-
yond the data they were exposed to, discussed
in section 2. Further, since it has been ar-
gued that very low frequency verbs may not
yet be firmly placed in a child?s lexicon (Yang,
2010; Gropen et al, 1989), at each epoch we
also impose a low-frequency threshold of 5
occurrences, considering only verbs that the
learner has seen at least 5 times. This use of a
low-frequency threshold for learning has ex-
tensive support in the literature for learning
1323
of all kinds in both human and non-human
animals, e.g. (Gallistel, 2002). A cut-off fre-
quency in this range has also commonly been
used in NLP tasks like POS tagging (Ratna-
parkhi, 1999).
3.2 The learners
We selected a set of representative statistical
models that are capable in principle of solv-
ing this classification task, ranging from what
is perhaps the simplest possible, a simple bi-
nomial, all the way to multi-level hierarchical
Bayesian approaches.
A Binomial distribution serves as the sim-
plest model for capturing the behavior of a
verb occurring in either DOD or PD frame.
Representing the probability of DOD as ?, af-
ter n occurrences of the verb the probability
that y of them are DOD is:
p( y| ?,n) =
(n
y
)
?y (1 ? ?)n?y (1)
Considering that p(y| ?,n) is the likelihood
in a Bayesian framework, the simplest and the
most intuitive estimator of ?, given y in n verb
occurrences, is the Maximum Likelihood Esti-
mator (MLE):
?MLE =
y
n (2)
?MLE is viable as a learning model in the sensethat its accuracy increases as the amount of ev-
idence for a verb grows (n ? ?), reflecting
the incremental, on-line character of language
learning. However, one well known limita-
tion of MLE is that it assigns zero probability
mass to unseen events. Ruling out events on
the grounds that they did not occur in a finite
data set early in learning may be too strong ?
though it should be noted that this is simply
one (overly strong) version of the indirect neg-
ative evidence position.
Again as is familiar, to overcome zero
count problem, models adopt one or another
method of smoothing to assign a small prob-
ability mass to unseen events. In a Bayesian
formulation, this amounts to assigning non-
zero probability mass to some set of priors;
smoothing also captures the notion of gener-
alization, making predictions about data that
has never been seen by the learner. In the
context of verb learning smoothing could be
based on several principles:
? an (innate) expectation as to how verbs in
general should behave;
? an acquired class-based expectation of
the behavior of a verb, based on its associ-
ation to similar but more frequent verbs.
The former can be readily implemented
in terms of prior probability estimates. As
we discuss below, class-based estimates arise
from one or another clustering method, and
can produce more accurate estimates for less
frequent verbs based on patterns already
learned for more frequent verbs in the same
class; see (Perfors, Tenenbaum, and Wonna-
cott, 2010). In this case, smoothing is a side-
effect of the behavior of a class as a whole.
When learning begins, the prior probability
is the only source of information for a learner
and, as such, dominates the value of the poste-
rior probability. However, in the large sample
limit, it is the likelihood that dominates the
posterior distribution regardless of the prior.
In Hierarchical Bayesian Models both effects
are naturally incorporated. The prior distri-
bution is structured as a chain of distributions
of parameters and hyper-parameters, and the
data may be divided into classes that share
some of the hyper-parameters, as defined be-
low for the case of a three levels model:
? ? Exponential(1)
? ? Exponential(1)
?k ? Exponential(?)
?k ? Beta(?, ?)
?ik ? Beta(?k?k, ?k(1 ? ?k))
yi|ni ? Binomial(?ik)
The indices refer to the possible hierarchies
among the hyper-parameters. ? and ? are in
the top, and they are shared by all verbs. Then
there are classes of different ?k, ?k, and theprobabilities for the DOD frame for the dif-
ferent verbs (?ik) are drawn according to theclasses k assigned to them. An estimate for
(?ik) for a given configuration of clusters isgiven by
1324
Figure 1: Verb tokens per epoch (E1 to E5)
Figure 2: Verb tokens ? 5 per epoch (E1 to E5)
where P(Y) is the evidence of the data,
the unnormalized posterior for the hyper-
parameters is
and the likelihood for ? and ? is
The Hierarchical Bayesian Model prediction
for?i is the average of the estimate?ikHBM overall possible partitions of the verbs in the task.
To simplify the notation we can write
?HBM = E
[ y + ??
n + ?
]
(3)
where in the expression E[. . . ] are included
the integrals described above and the average
of all possible class partitions. Due to this
complexity, in practice even small data sets re-
quire the use of MCMC methods, and statisti-
cal models for partitions, like CRP (Gelman et
al., 2003; Perfors, Tenenbaum, and Wonnacott,
2010). This complexity also calls into question
the cognitive fidelity of such approaches.
Eq.3 is particularly interesting because by
fixing? and ? (instead of averaging over them)
it is possible to deduce simpler (and classical)
models: MLE corresponds to ? = 0; the so
called ?add-one? smoothing (referred in this
paper as L1) corresponds to ? = 2 and ? = 1/2.
From Eq.3 it is also clear that if ? and ? (or
their distributions) are unchanged, as the evi-
dence of a verb grows (n??), the HBM esti-
mate approaches MLE?s, (?HBM ? ?MLE). Onthe other hand, when ? >> n, ?HBM ? ?, sothat ? can be interpreted as a prior value for ?
in the low frequency limit.
Following this reasoning, we propose an
alternative approach, a linear competition
learner (LCL), that explicitly models the be-
havior of a given verb as the linear competi-
tion between the evidence for the verb, and
the average behavior of verbs of the same
class. As clustering is defined independently
from parameter estimation, the advantages of
the proposed approach are twofold. First, it
is computationally much simpler, not requir-
ing approximations by Monte Carlo meth-
ods. Second, differently from HBMs where
the same attributes are used for clustering and
parameter estimation (in this case the DOD
and PD counts for each verb), in LCL cluster-
1325
ing may be done using more general contexts
that employ a variety of linguistic and envi-
ronmental attributes.
For LCL the prior and class-based informa-
tion are incorporated as:
?LCL =
yi + ?C?C
ni + ?C (4)
where ?C and ?C are defined via justifiableheuristic expressions dependent solely on the
statistics of the class attributed to each verb i.
The strength of the prior (?C) is a mono-tonic function of the number of elements (mC)in the class C, excluding the target verb vi.To approximate the gold standard behavior of
the HBM for this task (Perfors, Tenenbaum,
and Wonnacott, 2010) we chose the following
function for ?C:
?C = mC3/2(1 ?mC?1/5) + 0.1 (5)
with the strength of the prior for the LCL
model depending on the number of verbs in
the class, not on their frequency. Eq.5 was
chosen as a good fit to HBMs, without incur-
ring their complexity. The powers are simple
fractions, not arbitrary numbers. A best fit
was not attempted due to the lack of assess-
ment of how accurate HBMs are on real data.
The prior value (?C) is a smoothed estima-tion of the probability of DOD in a given class,
combining the evidence for all verbs in that
class:
?C =
YC + 1/2
NC + 1 (6)
in this case YC is the number of DOD occur-rences in the class, and NC the total numberof verb occurrences in the class, in both cases
excluding the target verb vi.The interpretation of these parameters is
as follows: ?C is the estimate of ? in the ab-sence of any data for a verb; and ?C controlsthe crossover between this estimate and MLE,
with a large ?C requiring a larger sample (ni)to overcome the bias given by ?C.For comparative purposes, in this paper we
examine alternative models for (a) probability
estimation and (b) clustering. The models are
the following:
? two models without clusters: MLE and
L1;
? two models where clusters are performed
independently: LCL and MLE??; and
? the full HBM described before.
MLE?? corresponds to replacing ?, ? in eq.3by their maximal likelihood values calculated
from P({yi,ni}i?k|?, ?) described before.For models without clustering, estimation
is based solely on the observed behavior of
verbs. With clustering, same-cluster verbs
share some parameters, influencing one an-
other. HBMs place distributions over pos-
sible clusters, with estimation derived from
averages over distributions. In HBMs, clus-
tering and probability estimation are calcu-
lated jointly. In the other models these two
estimates are calculated separately, permit-
ting ?plug-and-play? use of external cluster-
ing methods, like X-means (Pelleg and Moore,
2000)1. However, to further assess the impact
of cluster assignment on alternative model
performance, we also used the clusters that
maximize the evidence of the HBM for the
DOD and PD counts of the target verbs, and
we refer to these as Maximum Evidence (ME)
clusters. In MWE clusters, verbs are separated
into 3 classes: one if they have counts for both
frames; another for only the DOD frame; and
a final for only the PD frame.
4 Evaluation
The learning task consists of estimating the
probability that a given verb occurs in a partic-
ular frame, using previous occurrences as the
basis for this estimation. In this context, over-
generalization can be viewed as the model?s
predictions that a given verb seen only in one
frame (say, a PD) can also occur in the other
(say, a DOD) as well, and it decreases as the
learner receives more data. In one extreme
we have MLE, which does not overgeneralize,
and in the other the L1 model, which assigns
uniform probability for all unseen cases. The
other 3 models fall somewhere in between,
overgeneralizing beyond the observed data,
using the prior and class-based smoothing to
assign some (low) probability mass to an un-
seen verb-frame pair. The relevant models?
1Other clustering algorithms were also used; here
we report X-means results as representative of these
models. X-means is available from http://www.cs.
waikato.ac.nz/ml/weka/
1326
predictions for each of the target verbs in the
DOD frame, given the full corpus, are in fig-
ure 3. In either end of the figure are the verbs
that were attested in only one of the frames
(PD only at the left-hand end, and DOD only
at the right-hand end). For these verbs, LCL
and HBM exhibit similar behavior. When the
low-frequency threshold is applied, MLE??,HBM and LCL work equally well, figure 4.
Figure 4: Probability of verbs in DOD frame,
Low Frequency Threshold.
To examine how overgeneralization pro-
gresses during the course of learning as the
models were exposed to increasing amounts
of data, we used the corpus divided by cumu-
lative epochs, as described in ?3.1. For each
epoch, verbs seen in only one of the frames
were divided in 5 frequency bins, and the
models were assessed as to how much over-
generalization they displayed for each of these
verbs. Following Perfors, Tenenbaum, and
Wonnacott (2010) overgeneralization is calcu-
lated as the absolute difference between the
models predicted ? and ?MLE, for each of theepochs, figure 5, and for comparative pur-
poses their alternating/non-alternating clas-
sification is also adopted. For non-alternating
verbs, overgeneralization reflects the degree
of smoothing of each model. As expected, the
more frequent a verb is, the more confident
the model is in the indirect negative evidence
it has for that verb, and the less it overgeneral-
izes, shown in the lighter bars in all epochs. In
addition, the overall effect of larger amounts
of data are indicated by a reduction in over-
generalization epoch by epoch. The effects of
class-based smoothing can be assessed com-
paring L1, a model without clustering which
displays a constant degree of overgeneraliza-
tion regardless of the epoch, while HBM uses
a distribution over clusters and the other mod-
els X-means. If a low-frequency threshold is
applied, the differences between the models
decrease significantly and so does the degree
of overgeneralization in the models? predic-
tions, as shown in the 3 lighter bars in the fig-
ure.
Figure 5: Overgeneralization, per epoch, per
frequency bin, where 0.5 corresponds to the
maximum overgeneralization.
While the models differ somewhat in their
predictions, the quantitative differences need
to be assessed more carefully. To compare
the models and provide an overall difference
measure, we use the predictions of the more
complex model, HBM, as a baseline and then
calculate the difference between its predic-
tions and those of the other models. We
used three different measures for comparing
models, one for their standard difference; one
that prioritizes agreement for high frequency
verbs; and one that focuses more on low fre-
quency verbs.
The first measure, denoted Difference, cap-
tures a direct comparison between two mod-
els, M1 and M2 as the average prediction dif-ference among the verbs, and is defined as:
This measure treats all differences uniformly,
regardless of whether they relate to high or
low frequency verbs in the learning sample
(e.g. for bring with 150 counts and serve with
only 1 have the same weight). To focus on high
frequency verbs, we also define the Weighted
Difference between two models as:
Here we expect Dn < D since models tend to
1327
Figure 3: Probability of verbs in DOD frame.
agree as the amount of evidence for each verb
increases. Conversely, our third measure, de-
noted Inverted, prioritizes the agreement be-
tween two models on low frequency verbs, de-
fined as follows:
D1/n captures the degree of similarity in over-generalization between two models. The re-
sults of applying these three difference mea-
sures are shown in figure 6 for the relevant
models, where grey is for D(M1,M2), blackfor Dn(M1,M2) and white for D1/n(M1,M2).Given the probabilistic nature of Monte Carlo
methods, there is also a variation between dif-
ferent runs of the HBM model (HBM to HBM-
2), and this indicates that models that per-
form within these bounds can be considered
to be equivalent (e.g. HBMs and ME-MLE??for Weighted Difference, and the HBMs and
X-MLE?? for the Inverted Difference).
Comparing the prediction agreement, the
strong influence of clustering is clear: the
models that have compatible clusters have
similar performances. For instance, all the
models that adopt the ME clusters for the
data perform closest to HBMs. Moreover, the
weighted differences tend to be smaller than
0.01 and around 0.02 for the inverted differ-
ences. The results for these measures become
even closer in most cases when the low fre-
quency threshold is adopted, figure 7, as the
Figure 6: Model Comparisons.
Figure 7: Model Comparison - Low Frequency
Threshold.
0 5 10 15 20 25 30 35 40 45 500.5
0.6
0.7
0.8
0.9
1
number of examples
DO
D p
rob
abi
lity
 
 
MLE
L1
HBM
LCLMLE
L1
HBM
LCL
Figure 8: DOD probability evolution for mod-
els with increase in evidence
evidence reduces the influence of the prior.
To examine the decay of overgeneralization
with the increase in evidence for these mod-
els, two simulated scenarios are defined for a
single generic verb: one where the evidence
for DOD amounts to 75% of the data (dashed
lines) and in the other to 100% (solid lines),
figures 9 and 8. Unsurprisingly, the perfor-
mance of the models is dependent on the
amount of evidence available. This is a con-
sequence of the decrease in the influence of
the priors as the sample size increases in a rate
of 1/N, as shown in figure 9 for the decrease
in overgeneralization. Ultimately it is the ev-
1328
100 101 102
10?4
10?3
10?2
10?1
100
number of examples
ove
rge
ner
aliz
atio
n
 
 L1HBMLCLL1HBMLCL
Figure 9: Overgeneralization reduction with
increase in evidence
idence that dominates the posterior probabil-
ity. Although the Bayesian model exhibits fast
convergence, after 10 examples, the simpler
model L1 is only approximately 3% below the
Bayesian model in performance for scenario 1
and is still 90% accurate in scenario 2, figure 8.
These results suggest that while these mod-
els all differ slightly in the degree of overgen-
eralization for low frequency data and noise,
these differences are small, and as evidence
reaches approximately 10 examples per verb,
the overall performance for all models ap-
proaches that of MLE.
5 Conclusions and Future Work
HBMs have been successfully used for a
number of language acquisition tasks captur-
ing both patterns of under- and overgeneral-
ization found in child language acquisition.
Their (hyper)parameters provide robustness
for dealing with low frequency events, noise,
and uncertainty and a good fit to the data,
but this fidelity comes at the cost of complex
computation. Here we have examined HBMs
against computationally simpler approaches
to dative alternation acquisition, which imple-
ment the indirect negative approach. We also
advanced several measures for model com-
parison in order to quantify their agreement
to assist in the task of model selection. The re-
sults show that the proposed LCL model, in
particular, that combines class-based smooth-
ing with maximum likelihood estimation, ob-
tains results comparable to those of HBMs,
in a much simpler framework. Moreover,
when a cognitively-viable frequency thresh-
old is adopted, differences in the performance
of all models decrease, and quite rapidly ap-
proach the performance of MLE.
In this paper we used standard clustering
techniques grounded solely on verb counts to
enable comparison with previous work. How-
ever, a variety of additional linguistic and dis-
tributional features could be used for cluster-
ing verbs into more semantically motivated
classes, using a larger number of frames and
verbs. This will be examined in future work.
We also plan to investigate the use of cluster-
ing methods more targeted to language tasks
(Sun and Korhonen, 2009).
Acknowledgements
We would like to thank the support of
projects CAPES/COFECUB 707/11, CNPq
482520/2012-4, 478222/2011-4, 312184/2012-
3, 551964/2011-1 and 312077/2012-2. We also
want to thank Amy Perfors for kindly sharing
the input data.
References
Baker, Carl L. 1979. Syntactic Theory and the Pro-
jection Problem. Linguistic Inquiry, 10(4):533?
581.
Briscoe, Ted. 1997. Co-evolution of language and
the language acquisition device. In Proceedings
of the 35th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 418?427.
Morgan Kaufmann.
Brown, Roger. 1973. A first language: Ehe early
stages. Harvard University Press, Cambridge,
Massachusetts.
Brown, Roger and Camille Hanlon. 1970. Deriva-
tional complexity and the order of acquisition of
child?s speech. In J. Hays, editor, Cognition and
the Development of Language. NY: John Wiley.
Chater, Nick, Joshua B. Tenenbaum, and Alan
Yuille. 2006. Probabilistic models of cogni-
tion: where next? Trends in Cognitive Sciences,
10(7):292 ? 293.
Chomsky, Noam. 1981. Lectures on government and
binding. Mouton de Gruyter.
1329
Gallistel, Charles R. 2002. Frequency, contin-
gency, and the information processing theory of
conditioning. In P.Sedlmeier and T. Betsch, ed-
itors, Frequency processing and cognition. Oxford
University Press, pages 153?171.
Gelman, Andrew, John B. Carlin, Hal S. Stern, and
Donald B. Rubin. 2003. Bayesian Data Analy-
sis, Second Edition (Chapman & Hall/CRC Texts in
Statistical Science). Chapman and Hall/CRC, 2
edition.
Gropen, Jess, Steve Pinker, Michael Hollander,
Richard Goldberg, and Ronald Wilson. 1989.
The learnability and acquisition of the dative al-
ternation in English. Language, 65(2):203?257.
Hsu, Anne S. and Nick Chater. 2010. The logi-
cal problem of language acquisition: A proba-
bilistic perspective. Cognitive Science, 34(6):972?
1016.
Ingram, David. 1989. First Language Acquisition:
Method, Description and Explanation. Cambridge
University Press.
Jones, Matt and Bradley C. Love. 2011. Bayesian
Fundamentalism or Enlightenment? On the ex-
planatory status and theoretical contributions
of Bayesian models of cognition. Behavioral and
Brain Sciences, 34(04):169?188.
Kwiatkowski, Tom, Luke Zettlemoyer, Sharon
Goldwater, and Mark Steedman. 2010. Induc-
ing probabilistic CCG grammars from logical
form with higher-order unification. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1223?1233.
Kwisthout, Johan, Todd Wareham, and Iris van
Rooij. 2011. Bayesian intractability is not an
ailment that approximation can cure. Cognitive
Science, 35(5):779?1007.
Levin, B. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
MacWhinney, Brian. 1995. The CHILDES project:
tools for analyzing talk. Hillsdale, NJ: Lawrence
Erlbaum Associates, second edition.
Marcus, Gary F. 1993. Negative evidence in lan-
guage acquisition. Cognition, 46:53?85.
Marr, D. 1982. Vision. San Francisco, CA: W. H.
Freeman.
Nematzadeh, Aida, Afsaneh Fazly, and Suzanne
Stevenson. 2013. Child acquisition of multi-
word verbs: A computational investigation. In
A. Villavicencio, T. Poibeau, A. Korhonen, and
A. Alishahi, editors, Cognitive Aspects of Com-
putational Language Acquisition. Springer, pages
235?256.
Parisien, Christopher, Afsaneh Fazly, and Suzanne
Stevenson. 2008. An incremental bayesian
model for learning syntactic categories. In Pro-
ceedings of the Twelfth Conference on Computational
Natural Language Learning, CoNLL ?08, pages
89?96, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Parisien, Christopher and Suzanne Stevenson.
2010. Learning verb alternations in a usage-
based bayesian model. In Proceedings of the 32nd
Annual Conference of the Cognitive Science Society.
Pelleg, Dan and Andrew Moore. 2000. X-means:
Extending k-means with efficient estimation of
the number of clusters. In Proceedings of the
Seventeenth International Conference on Machine
Learning, pages 727?734, San Francisco. Morgan
Kaufmann.
Perfors, Amy, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, nega-
tive evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
(37):607?642.
Ratnaparkhi, Adwait. 1999. Learning to parse nat-
ural language with maximum entropy models.
Machine Learning, pages 151?175.
Shalizi, Cosma R. 2009. Dynamics of bayesian
updating with dependent data and misspeci-
fied models. ElectroCosmanic Journal of Statistics,
3:1039?1074.
Sun, Lin and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired se-
lectional preferences. In EMNLP, pages 638?
647.
Villavicencio, Aline. 2002. The Acquisition of a
Unification-Based Generalised Categorial Grammar.
Ph.D. thesis, Computer Laboratory, University
of Cambridge.
Wonnacott, Elizabeth, Elissa L. Newport, and
Michael K. Tanenhaus. 2008. Acquiring and
processing verb argument structure: Distribu-
tional learning in a miniature language. Cogni-
tive Psychology, 56:165?209.
Yang, Charles. 2010. Three factors in language
variation. Lingua, 120:1160?1177.
1330
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 49?56
Manchester, August 2008
Picking them up and Figuring them out:
Verb-Particle Constructions, Noise and Idiomaticity
Carlos Ramisch
??
, Aline Villavicencio
??
, Leonardo Moura
?
and Marco Idiart
?
?
Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?
GETALP Laboratory, Joseph Fourier University - Grenoble INP (France)
?
Department of Computer Sciences, Bath University (UK)
?
Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
{ceramisch,avillavicencio,lfsmoura}@inf.ufrgs.br, idiart@if.ufrgs.br
Abstract
This paper investigates, in a first stage,
some methods for the automatic acquisi-
tion of verb-particle constructions (VPCs)
taking into account their statistical prop-
erties and some regular patterns found in
productive combinations of verbs and par-
ticles. Given the limited coverage pro-
vided by lexical resources, such as dictio-
naries, and the constantly growing number
of VPCs, possible ways of automatically
identifying them are crucial for any NLP
task that requires some degree of semantic
interpretation. In a second stage we also
study whether the combination of statis-
tical and linguistic properties can provide
some indication of the degree of idiomatic-
ity of a given VPC. The results obtained
show that such combination can success-
fully be used to detect VPCs and distin-
guish idiomatic from compositional cases.
1 Introduction
Considerable investigative effort has focused on
the automatic identification of Multiword Expres-
sions (MWEs), like compound nouns (science fic-
tion) and phrasal verbs (carry out) (e.g. Pearce
(2002), Evert and Krenn (2005) and Zhang et
al. (2006)). Some of them employ language
and/or type dependent linguistic knowledge for
the task, while others employ independent statis-
tical methods, such as Mutual Information and
Log-likelihood (e.g. Pearce (2002) and, Zhang et
al. (2006)), or even a combination of them (e.g.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Baldwin (2005) and Sharoff (2004)), as basis for
helping to determine whether a given sequence
of words is in fact an MWE. Although some re-
search aims at developing methods for dealing
with MWEs in general (e.g. Zhang et al (2006),
Ramisch et al (2008)), there is also some work that
deals with specific types of MWEs (e.g. Pearce
(2002) on collocations and Villavicencio (2005)
on verb-particle constructions (VPCs)) as each of
these MWE types has distinct distributional and
linguistic characteristics.
VPCs are combinations of verbs and particles,
such as take off in Our plane took off late, that due
to their complex characteristics and flexible na-
ture, provide a real challenge for NLP. In particu-
lar, there is a lack of adequate resources to identify
and treat them, and those that are available provide
only limited coverage, in face of the huge number
of combinations in use. For tasks like parsing and
generation, it is essential to know whether a given
VPC is possible or not, to avoid for example us-
ing combinations that sound unnatural or ungram-
matical to native speakers (e.g. give/lend/?grant
out for the conveying of something to someone or
some place - (Fraser, 1976)).
1
Thus, the knowl-
edge of which combinations are possible is cru-
cial for precision grammar engineering. In ad-
dition, as the semantics of VPCs varies from the
idiomatic to the more compositional cases, meth-
ods for the automatic detection and handling of id-
iomaticity are very important for any NLP task that
involves some degree of semantic interpretation
such as Machine Translation (in this case avoiding
the problem of producing an unrelated translation
for a source sentence). Automatic methods for the
identification of idiomaticity in MWEs have been
1
See Baldwin et al (2004) for a discussion of the effects of
multiword expressions like VPCs on a parser?s performance.
49
proposed using a variety of approaches such as
statistical, substitutional, distributional, etc. (e.g.
McCarthy et al (2003), Bannard (2005) and Fa-
zly and Stevenson (2006)). In particular, Fazly
and Stevenson (2006) look at the correlation be-
tween syntactic fixedness (in terms of e.g. pas-
sivisation, choice of determiner type and pluralisa-
tion) and non-compositionality of verb-noun com-
pounds such as shoot the breeze.
In this work we investigate the automatic extrac-
tion of VPCs, looking into a variety of methods,
combining linguistic with statistical information,
ranging from frequencies to association measures:
Mutual Information (MI), ?
2
and Entropy. We also
investigate the determination of compositionality
of VPCs verifying whether the degree of semantic
flexibility of a VPC combined with some statisti-
cal information can be used to determine if it is
idiomatic or compositional.
This paper starts with a brief description of
VPCs, research on their automatic identification
and determination of their semantics (? 2). We then
explain the research questions and the assumptions
that serve as the basis for the application of statis-
tical measures (? 3) on the dataset (? 4). Our meth-
ods and experiments are then detailed (? 5), and
the results obtained are analysed (? 6). We con-
clude with a discussion of the contributions that
this work brings to the research on verb-particle
constructions (? 7).
2 Verb-Particle Constructions in Theory
and Practice
Particles in VPCs are characterised by containing
features of motion-through-location and of com-
pletion or result in their core meaning (Bolinger,
1971). VPCs can range from idiosyncratic or semi-
idiosyncratic combinations, such as get on (in e.g.
Bill got on well with his new colleagues), to more
regular ones, such as tear up (e.g. in In a rage she
tore up the letter Jack gave her). A three way clas-
sification is adopted by (Deh?e, 2002) and (Jack-
endoff, 2002), where a VPC can be classified as
compositional, idiomatic or aspectual, depending
on its sense. In compositional VPCs the meaning
of the construction is determined by the literal in-
terpretations of the particle and the verb. These
VPCs usually involve particles with directional or
spatial meaning, and these can often be replaced
by the appropriate directional PPs (e.g. carry in
in Sheila carried the bags in/into the house Deh?e
(2002)). Idiomatic VPCs, on the other hand, can-
not have their meaning determined by interpreting
their components literally (e.g. get on, meaning to
be on friendly terms with someone). The third class
is that of aspectual VPCs, which have the parti-
cle providing the verb with an endpoint, suggesting
that the action described by the verb is performed
completely, thoroughly or continuously (e.g. tear
up meaning to tear something into a lot of small
pieces).
From a syntactic point of view, a given combi-
nation can occur in several different subcategorisa-
tion frames. For example, give up can occur as an
intransitive VPC (e.g. in I give up! Tell me the an-
swer), where no other complement is required, or
it may occur as a transitive VPC which requires a
further NP complement (e.g. in She gave up alco-
hol while she was pregnant ). Since in English par-
ticles tend to be homographs with prepositions (up,
out, in), a verb followed by a preposition/particle
and an NP can be ambiguous between a transitive
VPC and a prepositional verb (e.g. rely on, in He
relies on his wife for everything). Some criteria
that characterise VPCs are discussed by Bolinger
(1971):
2
C1 In a transitive VPC the particle may come ei-
ther before or after the NP (e.g. He backed
up the team vs. He backed the team up).
However, whether a particle can be separated
or not from the verb may depend on the de-
gree of bonding between them, the size of the
NP, and the kind of NP. This is considered by
many to be sufficient condition for diagnos-
ing a VPC, as prepositions can only appear in
a position contiguous to the verb (e.g. *He
got the bus off ).
C2 Unstressed personal pronouns must precede
the particle (e.g. They ate it up but not *They
ate up it).
C3 If the particle precedes a simple definite NP,
the particle does not take the NP as its object
(e.g. in He brought along his girlfriend) un-
like with PP complements or modifiers (e.g.
in He slept in the hotel). This means that in
the first example the NP is not a complement
of the particle along, while in the second it is.
2
The distinction between a VPC and a prepositional verb
may be quite subtle, and as pointed out by Bolinger, many
of the criteria proposed for diagnosing VPCs give different
results for the same combination, frequently including un-
wanted combinations and excluding genuine VPCs.
50
In this paper we use the first two criteria, therefore
the candidates may contain noise (in the form of
prepositional verbs and related constructions).
VPCs have been the subject of a considerable
amount of interest, and some analysis has been
done on the subject of productive VPCs. In many
cases the particle seems to be compositionally
adding a specific meaning to the construction and
following a productive pattern (e.g. in tear up,
cut up and split up, where the verbs are seman-
tically related and up adds a sense of completion
to the action of these verbs). Fraser (1976) points
out that semantic properties of verbs can affect
their ability to combine with particles: for exam-
ple, bolt/cement/clamp/glue/paste/nail are seman-
tically similar verbs where the objects represented
by the verbs are used to join material, and they can
all combine with down. There is clearly a com-
mon semantic thread running through this list, so
that a new verb that is semantically similar to them
can also be reasonably assumed to combine with
down. Indeed, frequently new VPCs are formed by
analogy with existing ones, where often the verb is
varied and the particle remains (e.g. hang on, hold
on and wait on). Similarly, particles from a given
semantic class can be replaced by other particles
from the same class in compositional combina-
tions: send up/in/back/away (Wurmbrand, 2000).
By identifying classes of verbs that follow patterns
such as these in VPCs, we can help in the identi-
fication of a new unknown candidate combination,
using the degree of productivity of a class to which
the verb belongs as a back-off strategy.
In terms of methods for automatic identifica-
tion of VPCs from corpora, Baldwin (2005) pro-
poses the extraction of VPCs with valence infor-
mation from raw text, exploring a range of tech-
niques (using (a) a POS tagger, (b) a chunker, (c) a
chunk grammar, (d) a dependency parser, and (e) a
combination of all methods). Villavicencio (2005)
uses the Web as a corpus and productive patterns
of combination to generate and validate candidate
VPCs. The identification of compositionality in
VPCs is addressed by McCarthy et al (2003) who
examine the overlap of similar words in an auto-
matically acquired distributional thesaurus for verb
and VPCs, and by Bannard (2005) who uses a
distributional approach to determine when and to
what extent the components of a VPC contribute
their simplex meanings to the interpretation of the
VPC. Both report a correlation between some of
the measures and compositionality judgements.
3 The Underlying Hypotheses
The problem of the automatic detection and classi-
fication of VPCs can be summarised as, for a given
VPC candidate, to answer to the questions:
Q1 Is it a real VPC or some free combination
of verb and preposition/adverb or a preposi-
tional verb?
Q2 If it is a true VPC, is it idiomatic or composi-
tional?
In order to answer the first question, we use two
assumptions. Firstly, we consider that the elements
of a true VPC co-occur above chance. The greater
the correlation between the verb and the particle
the greater the chance that the candidate is a true
VPC. Secondly, based on criterion C1 we also as-
sume that VPCs have more flexible syntax and are
more productive than non-VPCs. This second as-
sumption goes against what is usually adopted for
general MWEs, since it is the prepositional verbs
that allow less syntactic configurations than VPCs
and are therefore more rigid (? 2). To further dis-
tinguish VPCs from prepositional verbs and other
related constructions we also verify the possibil-
ity of the particle to be immediately followed by
an indirect prepositional complement (like in The
plane took off from London), which is a good in-
dicator/delimiter of a VPC since in non-VPC con-
structions like prepositional verbs the preposition
needs to have an NP complement. Therefore, we
will assume that a true VPC occurs in the following
configurations, according to Villavicencio (2005)
and Ramisch et al (2008):
S1 VERB + PARTICLE + DELIMITER, for intran-
sitive VPCs;
S2 VERB + NP + PARTICLE + DELIMITER, for
transitive split VPCs and;
S3 VERB + PARTICLE + NP + DELIMITER, for
transitive joint VPCs.
In order to answer Q2, we look at the link be-
tween productivity and compositionality and as-
sume that a compositional VPC accepts the sub-
stitution of one of its members by a semantically
related term. This is in accordance to Fraser
(1976), who shows that semantic properties of
51
verbs can affect their ability to combine with par-
ticles: for example verbs of hunting combining
with the resultative down (hunt/track/trail/follow
down) and verbs of cooking with the aspectual up
(bake/cook/fry/broil up), forming essentially pro-
ductive VPCs. Idiomatic VPCs, however, will
not accept the substitution of one of its members
by a related term (e.g. get and its synonyms in
get/*obtain/*receive over), even if at first glance
this could seem natural. In our experiments, we
will consider that a VPC is compositional if it ac-
cepts: the replacement of the verb by a synonym,
or of the preposition by another preposition. Sum-
marising our hypothesis, we get:
? For Q1: Is the candidate syntactically flexi-
ble, i.e. does it allow the configurations S1
through S3?
? NO: non-VPC
? YES: VPC
? For Q2: Is the candidate semantically flexi-
ble, allowing the substitution of a member by
a related word?
? NO: idiomatic VPC
? YES: compositional VPC
4 Data Sources
To generate a gold standard, we used the Bald-
win VPC candidates dataset (henceforth Baldwin
CD)
3
, which contains 3,078 English VPC candi-
dates annotated with information about idiomatic-
ity (14.5% are considered idiomatic). We fur-
ther annotated this dataset with information about
whether each candidate is a genuine VPC or not,
where a candidate is consider genuine if it be-
longs to at least one of a set of machine-readable
dictionaries: the Alvey Natural Language Tools
(ANLT) lexicon (Carroll and Grover, 1989), the
Comlex lexicon (Macleod and Grishman, 1998),
and the LinGO English Resource Grammar (ERG)
(Copestake and Flickinger, 2000)
4
. With this crite-
rion 81.8% of them are considered genuine VPCs.
To gather information about the candidates in
this work we employ both a fragment of 1.8M
sentences from the British National Corpus (BNC
Burnard (2000)) and the Web as corpora. The
BNC fragment is used to calculate the correlation
3
This dataset was provided by Timothy Baldwin for the
MWE2008 Workshop.
4
Version of November 2001.
measures since they require a corpus with known
size. The Web is used to generate frequencies
for the entropy measures, as discussed in ? 5.2.
Web frequencies are approximated by the number
of pages containing a candidate and indexed by
Yahoo Search API. In order to keep the searches
as simple and self-sufficient as possible, no addi-
tional sources of information are used (Villavicen-
cio, 2005). Therefore, the frequencies are quite
conservative in the sense that by employing in-
flected forms of verbs, potentially much more evi-
dence could be gathered.
For the generation of semantic variational pat-
terns, we use both Wordnet 3.0 (Fellbaum, 1998)
and Levin?s English Verb Classes and Alternations
(Levin, 1993). Wordnet is organised as a graph of
concepts, called synsets, linked by relations of syn-
onymy, hyponymy, etc. Each synset contains a list
of words that represent the concept. The verbs in
a synset and its synonym synsets are used to gen-
erate variations of a VPC candidate. Likewise we
use Levin?s classes, which define 190 fine-grained
classes for English verbs, based on their syntactic
and semantic features.
It is important to highlight that the generation
of the semantic variations strongly relies on these
resources. Therefore, cross-language extension
would depend on the availability of similar tools
for the target language.
5 Carrying out the experiments
Our experiments are composed of two stages, each
one consisting of three steps (corresponding to the
next three sections). The first stage filters out ev-
ery candidate that is evaluated as not being a VPC,
while the second one intends to identify the id-
iomatic VPCs among the remaining candidates of
the previous stage.
5.1 Generating candidates
For each of the 3,078 items in the Baldwin CD we
generated 2 sets of variations, syntactic and seman-
tic, and we will refer to these as alternative forms
or variations of a candidate.
The syntactic variations are generated using the
patterns S1 to S3 described in section 3. Following
the work of Villavicencio (2005) 3 frequently used
prepositions for, from and with are used as delim-
iters and we search for NPs in the form of pronouns
like this and definite NPs like the boy. The use of
alternative search patterns also helps to give an in-
52
dication about the syntactic distribution of a can-
didate VPC, and consequently if it has a preferred
syntactic realisation. For instance, for eat up and
the delimiter with, we propose a list of Web search
queries for its respective variations v
i
, shown with
their corresponding Web frequencies in table 1.
5
Variation (v
i
) Frequency (n
Y ahoo
(v
i
))
eat up with 49200
eat the * up with 2240
eat this up with 1120
eat up the * with 3110
Table 1: Distribution of syntactic variations for the
candidate eat up.
For the semantic variations, in order to capture
the idiomaticity of VPCs we generate the alterna-
tive forms by replacing the verb by its synonym
verbs as follows:
WNS Wordnet Strict variations. When using Word-
net, we consider any verb that belongs to the
same synset of the candidate as a synonym.
WNL Wordnet Loose variations. This is an indi-
rect synonymy relation capturing any verb
in Wordnet that belongs either to the same
synset or to a synset that is synonym of the
synset in which the candidate verb is con-
tained.
Levin These include all verbs in the same Levin
class as the candidate.
Multiword synonyms are ignored in this step to
avoid noisy search patterns, (e.g. *eat up up). The
examples for these variations are shown in table 2
for the candidate act in.
Wordnet and Levin are considered ambiguous
resources because one verb is potentially contained
in several synsets or classes. However, as Word
Sense Disambiguation is not within the scope of
this work we employ some heuristics to select a
given sense for the candidate verb. In order to test
the effect of frequency, the first heuristic adopts the
first synset in the list, as Wordnet organises synsets
in descending order of frequency (denoted as first).
To study the influence of the number of synonyms,
the second and third heuristics use respectively the
biggest (max) and smallest (min) synsets. The last
5
The Yahoo wildcard used in these searches matches any
word occurring in that particular position.
Variation (v
i
) Source n
Y ahoo
(v
i
)
act in ? 2690
playact in WNS 0
play in WNS 167000
behave in WNL 98
do in WNL 24600
pose in Levin 1610
qualify in Levin 358
rank in Levin 706
rate in Levin 16700
serve in Levin 2240
Table 2: Distribution of syntactic variations for the
candidate eat up.
heuristic is the union of all synonyms (all). These
heuristics are indicated using a subscript notation,
where e.g. WNS
all
symbolizes the WNS varia-
tions set using the union of all synsets as disam-
biguation heuristic. Finally, we generated two
additional sets of candidates by replacing the par-
ticle by one of the 48 prepositions listed in the
ANLT dictionary (prep) and also by one of 9 cho-
sen locative prepositions (loc-prep). It is impor-
tant to also verify possible variations of the prepo-
sition because compositional VPCs combine pro-
ductively with one or more groups of particles, e.g.
locatives, and present consequently a wider prob-
ability distribution among the variations, while an
idiomatic VPC presents a higher frequency for a
chosen preposition.
5.2 Working the statistical measures out
The classifications of the candidate VPCs are done
using a set of measures: the frequencies of the
VPC candidates and of their individual words,
their Mutual Information (MI), ?
2
and Entropies.
We calculate the MI and ?
2
indices of a candidate
formed by a verb and a particle based on their in-
dividual frequencies and on their co-occurrence in
the BNC fragment.
The Entropy measure is given by
H(V ) = ?
n
?
i=1
p(v
i
) ln [ p(v
i
) ]
where
p(v
i
) =
n(v
i
)
?
? v
j
?V
n(v
j
)
is the probability of the variation v
i
to occur
among the set of all possible variations V =
53
H(V ) ? 0.001081
n
BNC
(p) ? 51611
n
Y ahoo
(v
transitive
) ? 1
n
Y ahoo
(v) ? 2020000000 : yes
n
Y ahoo
(v) > 2020000000
?
2
? 25.99
? ? ?
Figure 1: Fragment of the decision tree that filters
out non-VPCs.
{v
1
, v
2
, . . . , v
n
}, and n(v
i
) is the Web frequency
for the variation v
i
.
The entropy of a probability distribution gives
us some clues about its shape. A very low en-
tropy is a sign of a heterogeneous distribution that
contains a peak. On the other hand, a distribution
that presents uniformity will lead to a high entropy
value.
The interest of H(V ) for the detection of VPCs
is in that true instances are more likely to not prefer
a canonical form, more widely distributing proba-
bilities over all alternative syntactic frames (S1 to
S3), while non-VPCs are more likely to choose one
frame and present low frequencies for the proposed
variations.
For the semantic variations, the entropy is cal-
culated from a set V of variations generated by the
Wordnet synset, Levin class and preposition sub-
stitutions described in ? 5.1. The interpretation of
the entropy at this point is that high entropy indi-
cates compositionality while low entropy indicates
idiomaticity, since compositional VPCs are more
productive and distribute well over a class of verbs
or a class of prepositions and idiomatic VPCs pre-
fer a specific verb or preposition.
5.3 Bringing estimations together
Once we got a set of measures to predict
VPCs and another to predict their idiomatic-
ity/compositionality, we would like to know which
measures are useful. Therefore, we combine our
measures automatically by building a decision tree
with the J48 algorithm, a version of the traditional
entropy-based C4.5 algorithm implemented in the
Weka package.
6
6 Weighting the results up
The first stage of our experiments applied to the
3,078 VPC candidates generated a decision tree us-
6
http://www.cs.waikato.ac.nz/ml/weka/
ing 10-fold cross validation that is partially repro-
duced in figure 1. From these, 2,848 candidates
were considered genuine VPCs, with 2,419 true
positives, 100 false negatives and 429 false posi-
tives. This leads to a recall of 96% of the VPCs
being kept in the list with a precision of 84.9%,
and an f-measure of 90.1%. We interpret this as a
very positive result since although some false neg-
atives have been filtered out, the remaining candi-
dates are now less noisy.
Figure 1 shows that the entropy of the variations
is the best predictor since it is at the root of the
tree. We can also see that there are several types
of raw frequencies being used before a correlation
measure appears (?
2
). We can conclude that the
frequency of each transitive, intransitive and split
configurations are also good predictors to detect
false from true VPCs. At this point, MI does not
seem to contribute to the classification task.
For our second stage, we generated Wordnet
synonym, Levin class and preposition variations
for a list of the 2,867 VPC candidates classified
as genuine cases. We also took into account the
proportion of synonyms that are MWEs (vpc-syn)
and the proportion of synonyms that contain the
candidate itself (self-syn).
In order to know what kind of contribution each
measure gives to the construction of the decision
tree, we used a simple iterative algorithm that con-
structs the set U of useful attributes. It first ini-
tialises U with all attributes, then calculates the
precision for each class (yes and no)
7
on a cross
validation using all attributes in U . For each at-
tribute a ? U , it ignores a and recalculates preci-
sions. If both precisions decrease, the contribution
of a is positive, if both increase then a is negative,
else its contribution remains unknown. All fea-
tures that contribute negatively are removed from
U , and the algorithm is repeated until there is no
negative attribute left.
The step-by-step execution of the algorithm
can be observed in table 3, where the inconclu-
sive steps are hidden. We found out that the
optimal features are U
?
= {self-syn, H(prep),
H(Levin
first
), H(WNS
first
), H(WNS
min
),
H(Levin
max
), H(Levin
min
).} The self-syn in-
formation seems to be very important, as without
it precisions of both classes decrease considerably
7
We use the precision as a quality estimator since it gives
a good idea of the amount of work that a grammar engineer
or lexicographer must perform in order to clear the list from
false positives.
54
Precision
# Ignored No Yes +/?
1
st
iteration
0 ? 86.6% 54.9%
1 vpc-syn 86.7% 56.6% ?
2 self-syn 85.2% 28.7% +
4 H(loc-prep) 86.7% 56.1% ?
6 H(WNS
max
) 87.5% 57.4% ?
9 H(WNLfirst) 86.7% 57.9% ?
10 H(WNL
max
) 86.7% 57.8% ?
11 H(WNL
min
) 86.9% 57.6% ?
16 H(Levin
all
) 86.7% 55.1% ?
2
nd
iteration
17 ? 87.7% 60.3%
18 H(prep) 87.6% 59.2% +
21 H(WNS
all
) 87.8% 61.6% ?
22 H(WNL
all
) 87.8% 61.0% ?
23 H(Levin
first
) 87.5% 60.2% +
3
rd
iteration
26 ? 87.8% 61.9%
27 H(WNS
first
) 87.8% 61.9% ?
28 H(WNS
min
) 87.7% 61.1% +
29 H(Levin
max
) 87.8% 61.6 ?
30 H(Levin
min
) 87.7% 61.5% +
Table 3: Iterative attributes selection process. Pre-
cision in each class is used as quality estimator.
(experiment #2).
All entropies of the WNL heuristics are of little
or no utility. This could probably be explained by
either the choice of simple WSD heuristics for se-
lecting synsets, or because the indirect synonymy
information is too far related to the original verb to
be used in variational patterns. Inspecting the gen-
erated variations, we notice that most of the syn-
onym synsets are related to secondary senses or
very specific uses of a verb and are thus not cor-
rectly disambiguated.
In what concerns the WNS sets, only the small-
est and first synset were kept, suggesting again that
it may not be a good idea to maximise the syn-
onyms set and for future work, we intent to es-
tablish a threshold for a synset to be taken into
account. In addition, we can also infer a posi-
tive contribution of the frequency of a sense with
the choice of the first synset returned by Word-
net resulting in a reasonable WSD heuristic (which
is compatible with the results by McCarthy et al
(2004)).
On the other hand, the algorithm selected the
first, the smallest and the biggest of the Levin?s
sets. This probably happens because the major-
ity of these verbs belongs only to one or two, but
never to a great number of classes. Since the gran-
ularity of the classes is coarser than for synsets,
the heuristics often offer four equal or very close
entropies and thus redundant information. As an
overall result, the last iteration shown in table 3
indicates a precision of 61.9% for the classifier in
detecting idiomatic VPCs, that is to say that we au-
tomatically retrieved 176 VPCs where 67 are false
positives and 109 are truly idiomatic. This value is
a quality estimator for the resulting VPCs that will
potentially be used in the construction of a lexi-
con. Recall of idiomatic VPCs goes from 16.7%
to 24.9%.
7 Conclusions
One of the important challenges for robust natu-
ral language processing systems is to be able to
successfully deal with Multiword Expressions and
related constructions. We investigated the identifi-
cation of VPCs using a combination of statistical
methods and linguistic information, and whether
there is a correlation between the productivity of
VPCs and their semantics that could help us detect
if a VPC is idiomatic or compositional.
The results confirm that the use of statistical
and linguistic information to automatically iden-
tify verb-particle constructions presents a reason-
able way of improving coverage of existing lexi-
cal resources in a very simple and straightforward
manner. In terms of grammar engineering, the in-
formation about compositional candidates belong-
ing to productive classes provides us with the ba-
sis for constructing a family of fine-grained redun-
dancy rules for these classes. These rules are ap-
plied in a constrained way to verbs already in the
lexicon, according to their semantic classes. The
VPCs identified as idiomatic, on the other hand,
need to be explicitly added to the lexicon, after
their semantic is determined. This study can also
be complemented with the results of investigations
into the semantics of VPCs, as discussed by both
Bannard (2005) and McCarthy et al (2003).
In addition, the use of clustering methods is an
interesting possibility for automatically identify-
ing clusters of productive classes of both verbs and
of particles that combine well together.
55
Acknowledgments
This research was partly supported by the CNPq
research project Recuperac??ao de Informac??oes
Multil??ng?ues (CNPq Universal 484585/2007-0).
References
Baldwin, Timothy, Emily M. Bender, Dan Flickinger, Ara
Kim, and Stephan Oepen. 2004. Road-testing the English
Resource Grammar over the British National Corpus. In
Fourth International Conference on Language Resources
and Evaluation (LREC 2004), Lisbon, Portugal.
Baldwin, Timothy. 2005. Deep lexical acquisition of verb-
particle constructions. Computer Speech and Language,
19(4):398?414.
Bannard, Colin J. 2005. Learning about the meaning of verb-
particle constructions from corpora. Computer Speech and
Language, 19(4):467?478.
Bolinger, Dwight. 1971. The phrasal verb in English. Har-
vard University Press, Harvard, USA.
Burnard, Lou. 2000. User reference guide for the British Na-
tional Corpus. Technical report, Oxford University Com-
puting Services.
Carroll, John and Claire Grover. 1989. The derivation of a
large computational lexicon of English from LDOCE. In
Boguraev, B. and E. Briscoe, editors, Computational Lexi-
cography for Natural Language Processing. Longman.
Copestake, Ann and Dan Flickinger. 2000. An open-source
grammar development environment and broad-coverage
English grammar using HPSG. In Proceedings of the
2nd International Conference on Language Resources and
Evaluation (LREC 2000).
Deh?e, Nicole. 2002. Particle verbs in English: syntax, in-
formation structure and intonation. John Benjamins, Am-
sterdam/Philadelphia.
Evert, Stefan and Brigitte Krenn. 2005. Using small random
samples for the manual evaluation of statistical association
measures. Computer Speech and Language, 19(4):450?
466.
Fazly, Afsaneh and Suzanne Stevenson. 2006. Automatically
constructing a lexicon of verb phrase idiomatic combina-
tions. In EACL. The Association for Computer Linguis-
tics.
Fellbaum, Christiane, editor. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Communica-
tion). The MIT Press, May.
Fraser, Bruce. 1976. The Verb-Particle Combination in En-
glish. Academic Press, New York, USA.
Jackendoff, Ray. 2002. English particle constructions, the
lexicon, and the autonomy of syntax. In N. Deh?e, R. Jack-
endoff, A. McIntyre and S. Urban, editors, Verb-Particle
Explorations. Berlin: Mouton de Gruyter.
Levin, Beth. 1993. English Verb Classes and Alternations:
a preliminary investigation. University of Chicago Press,
Chicago and London.
Macleod, Catherine and Ralph Grishman. 1998. Comlex syn-
tax reference manual, Proteus Project.
McCarthy, Diana, Bill Keller, and John Carroll. 2003. De-
tecting a continuum of compositionality in phrasal verbs.
In Proceedings of the ACL 2003 workshop on Multiword
expressions, pages 73?80, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Finding predominant word senses in untagged
text. In Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics, page 279. Associa-
tion for Computational Linguistics.
Pearce, Darren. 2002. A comparative evaluation of colloca-
tion extraction techniques. In Third International Confer-
ence on Language Resources and Evaluation, Las Palmas,
Canary Islands, Spain.
Ramisch, Carlos, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for the ex-
traction of multiword expressions. In Proceedings of the
LREC Workshop - Towards a Shared Task for Multiword
Expressions (MWE 2008), pages 50?53, Marrakech, Mo-
rocco, June.
Sharoff, Serge. 2004. What is at stake: a case study of rus-
sian expressions starting with a preposition. pages 17?23,
Barcelona, Spain.
Villavicencio, Aline. 2005. The availability of verb-particle
constructions in lexical resources: How much is enough?
Journal of Computer Speech and Language Processing,
19(4):415?432.
Wurmbrand, S. 2000. The structure(s) of particle verbs. Ms.,
McGill University.
Zhang, Yi, Valia Kordoni, Aline Villavicencio, and Marco
Idiart. 2006. Automated multiword expression prediction
for grammar engineering. In Proceedings of the Workshop
on Multiword Expressions: Identifying and Exploiting Un-
derlying Properties, pages 36?44, Sydney, Australia. As-
sociation for Computational Linguistics.
56
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 52?60,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Investigation on Polysemy and Lexical Organization of Verbs  
 
Daniel Cerato Germann1  Aline Villavicencio12 Maity Siqueira3 
1Institute of Informatics, Federal University of  Rio Grande do Sul (Brazil) 
2Department of Computer Sciences, Bath University (UK) 
3Institute of Language Studies, Federal University of  Rio Grande do Sul (Brazil) 
{dcgermann,avillavicencio}@inf.ufrgs.br, maitysiqueira@hotmail.com 
  
 
 
 
Abstract 
This work investigates lexical organizat ion of 
verbs looking at the influence of some linguis-
tic factors on the process of lexical acquisition  
and use. Among the factors that may play a 
role in acquisition, in this paper we investigate 
the influence of polysemy. We examine data 
obtained from psycholinguistic action naming  
tasks performed by children and adults 
(speakers of Brazilian Portuguese), and ana-
lyze some characteristics of the verbs used by 
each group in terms of similarity of content, 
using Jaccard?s coefficient, and of topology, 
using graph theory. The experiments suggest 
that younger children tend to use more poly-
semic verbs than adults to describe events in 
the world. 
1 Introduction 
Lexical acquisition is restrained by perception and 
comprehension difficulties, which are associated 
with a number of linguistic and psycholinguistic 
factors. Among these we can cite age of acquis i-
tion (Ellis and Morrison, 1998; Ellis and Ralph, 
2000), frequency (Morrison and Ellis, 1995) , syn-
tactic (Ferrer-i-Cancho et al, 2004; Goldberg, 
1999; Thompson et. al, 2003) and semantic (Bree-
din et. al, 1998; Barde et al, 2006) characteristics 
of words. In terms of semantic features, acquisition 
may be influenced by the polysemy and generality 
of a word, among others. 
In terms of semantic features, acquisition may 
be influenced by the generality and polysemy of a 
word, among others. For instance, considering 
acquisition of verbs in particular, Goldberg (1999) 
observes that verbs such as go, put and give are 
among those to be acquired first, for they are more 
general and frequent, and have lower ?semantic 
weight? (a relative measure of complexity; Breedin 
et. al, 1998; Barde et al, 2006). These verbs, 
known as light verbs, not only are acquired first: 
they are also known to be more easily used by 
aphasics (Breedin et al, 1998; Thompson, 2003; 
Thompson et al 2003; Barde et al 2006; but see 
Kim and Thompson, 2004), which suggest their 
great importance for human cognition. The prefe-
rence for light verbs may be explained by the more 
general meanings they tend to present and their 
more polysemic nature, that is their ability to con-
vey multiple meanings, since the more polysemic 
the verb is, the more contexts in which it can be 
used (Kim and Thompson, 2004; Barde et al, 
2006). The importance of the number of relation-
ships a word has in the learning environment has 
been pointed out by Hills et al (2009), regardless 
of generality. Several factors may influence acqui-
sition, but in this paper we will focus on polysemy.   
Understanding how characteristics like polyse-
my influence acquisition is essential for the con-
struction of more precise theories. Therefore, the 
hypothesis we investigate is that more polysemous 
words have a higher chance of earlier acquisition. 
For this purpose, we compare data from children 
and adults from the same linguistic community, 
native speakers of Brazilian Portuguese, in an ac-
tion naming task, looking at lexical evolution by 
using statistical and topological analysis of the data 
modeled as graphs (following Steyvers and Tenen-
baum, 2005, and Gorman and Curran, 2007). This 
approach innovates in the sense that it directly 
simulates the influence of a linguistic factor over 
the process of lexical evolution.  
52
This paper is structured as follows. Section 2 de-
scribes relevant work on computational modeling 
of language acquisition. Section 3 presents the 
materials and methods employed in the exper i-
ments of the present work. Sections 4 and 5 present 
the results, and section 6 concludes and presents 
future work. 
2 Related Work   
In recent years, there has been growing interest in 
the investigation of language acquisition using 
computational models. For instance, some work 
has investigated language properties such as age-
of-acquisition effects (Ellis and Ralph, 2000; Li et 
al., 2004). Others have simulated aspects of the 
acquisition process (Siskind, 1996; Yu, 2005; Yu, 
2006; Xu and Tenenbaum, 2007; Fazly et al 2008) 
and lexical growth (Steyvers and Tenenbaum, 
2005; Gorman and Curran, 2007).  
Some authors employ graph theory metrics to 
directly analyze word senses (Sinha and Mihalcea, 
2007; Navigli and Lapata, 2007). In this paper, 
word senses are implicitly expressed by graph 
edges, thus being considered indirectly. Graph 
theory has also been successfully used in more 
theoretical fields, like the characterization and 
comparison of languages (Motter et al, 2002; Fer-
rer-i-Cancho et al, 2004; Masucci and Rodgers, 
2006). For example, the works by Sigman and 
Cecchi (2002), and Gorman and Curran (2007) use 
graph measures to extensively analyze WordNet 
properties. Steyvers and Tenenbaum (2005) use 
some properties of language networks to propose a 
model of semantic growth, which is compatible 
with the effects of learning history variables, such 
as age of acquisition and frequency, in semantic 
processing tasks. The approach proposed  in this 
work follows Steyvers and Tenenbaum (2005), and 
Gorman and Curran (2007) in the sense of iterative 
modifications of graphs, but differs in method (we 
use involutions instead of evolutions) and objec-
tive: modifications are motivated by the study of 
polysemy instead of production of a given topolog-
ical arrangement. It also follows Deyne and Storms 
(2008), in the sense that it directly relates linguistic 
factors and graph theory metrics, and Coronges et 
al. (2007), in the sense that it compares networks 
of different populations with the given approach. 
As to Brazilian Portuguese, in particular, Anti-
queira et al (2007) relate graph theory metrics and 
text quality measurement, while Soares et al(2005) 
report on a phonetic study. Tonietto et al (2008) 
analyze the influence of pragmatic aspects, such as 
conventionality of use, over the lexical organiza-
tion of verbs, and observe that adults tend to prefer 
more conventional labels than children.  
In this context, this study follows Tonietto et al
(2008) in using data from a psycholinguistic action 
naming task. However, the analysis is done in 
terms of lexical evolution, by using graph and set 
theory metrics (explained below) to understand the 
influence of some linguistic characteristics of 
words, especially polysemy.  
3 Materials and Methods  
3.1 The Data  
This paper investigates the lexical evolution of 
verbs by using data from an action naming task 
performed by different age groups: 55 children and 
55 young adults. In order to study the evolution of 
the lexicon in children, children?s data are longitu-
dinal; participants of the first data collection (G1) 
aged between 2;0 and 3;11 (average 3;1), and in 
the second collection (G2), between 4;1 and 6;6 
(average 5;5) as described by Tonietto et al 
(2008). The adult group is unrelated to the child-
ren, and aged between 17;0 and 34;0 (average 
21;8). The longitudinal data enabled the compari-
son across the lexical evolution of children at age 
of acquisition (G1), two years later (G2), and the 
reference group of adults (G3). Participants were 
shown 17 actions of destruction or division (To-
nietto et al 2008); answers were preprocessed in 
order to eliminate both invalid answers (like ?I 
don't know?) and answers with only 1 occurrence 
per group. The selection of this particular domain 
(destruction and division) is due to its cognitive 
importance: it was found to be one of the four con-
ceptual zones, grouping a great amount of verbs1 
(Tonietto, 2009).  
There were a total of 935 answers per group, out 
of which 785, 911 and 917 were valid answers to 
G1, G2 and G3, respectively. These made averages 
of 46.18, 53.59 and 53.94 valid answers per action, 
respectively. The average numbers of distinct valid 
answers per action, before merging (explained in 
section 3.2), were 6.76, 5.53 and 4, respectively.   
                                                                 
1 The others are evasion, excitation, and union. 
53
The answers given by each participant were col-
lected and annotated two polysemy scores, each 
calculated from a different source:  
 Wscore is the polysemy score of a verb ac-
cording to its number of synsets (synonym 
sets) in WordNetBR (Dias-da-Silva et al, 
2000, Maziero, 2008), the Brazilian Portu-
guese version of Wordnet (Fellbaum, 1998). 
 Hscore is the number of different entries for 
a verb in the Houaiss dictionary (Houaiss, 
2007). 
 
Information about these two scores for each 
group is shown in Table 1. 
 
 G1 G2 G3 
Average type Wscore 10.55 10.64 10.48 
Average token Wscore 16.25 14.66 11.13 
Average type Hscore 21.59 20.84 16.26 
Average token Hscore 26.93 23.02 17.82 
Table 1: Score per group and per participant.  
 
We notice that most scores, i.e., type and token 
Hscores, and token Wscore, decrease as age in-
creases, which is compatible with the hypothesis 
investigated. However, due to the limited coverage 
of WordNetBR2, some verbs had a null value, and 
this is reflected in type Wscore. This is the case of 
?serrar? (to saw) which appears in both G1 and 
G2, but not in G3.  
A comparative analysis of linguistic production 
across the different groups is presented in Table 2. 
There is a significant similarity across the groups, 
with 12 verbs (out of a total of 44) being common 
to all of them. In each column, the second graph is 
compared to the first. In the ?G1-G2? column, 
there are 16 verbs common to both graphs, which 
represents 64% of the verbs in G2 (with 36% of the 
verbs in G2 not appearing in G1). As expected, due 
to the proximity in age, results show a higher simi-
larity between G1 and G2 than between G2 and 
G3. 
 
 
 
 
 
 
                                                                 
2 WordNetBR was still under construction when annotation 
was performed. 
 G1-G2 G2-G3 G1-G3 All 
Common verbs 16 17 12 12 
Verbs only in 
older group (%) 
36 45.16 58.06 - 
? 
Table 2: Comparisons between groups3. 
3.2 Simulation Dynamics 
Linguistic production of each group was 
represented in terms of graphs, whose nodes 
represent the verbs mentioned in the task. Verbs 
uttered for the same action were assumed to share 
semantic information, thus being related to each 
other. The existence of conceptual relationships 
due to semantic association is in accordance with 
Nelson et al (1998), where implicit semantic rela-
tions were shown to influence on recall and recog-
nition. Therefore, for each age group, all the verbs 
uttered for a given action were linked together, 
forming a (clique) subgraph. The subgraphs for the 
different actions were then connected in a merging 
step, through the polysemic words uttered for more 
than one action.  
As the goal of this research is to investigate 
whether a factor such as polysemy has any influ-
ence on language acquisition, we examine the ef-
fects of using it to incrementally change the 
network over time. Strategies for network modif i-
cation, such as network growth (Albert and Ba-
rab?si, 2002), have been used to help evaluate the 
effects of particular factors by iteratively changing 
the network (e.g., Steyvers and Tenenbaum, 2005; 
Gorman and Curran, 2007). Network growth in-
crementally adds nodes to an initial state of the 
network, by means of some criteria, allowing ana l-
ysis of its convergence to a final state. The longi-
tudinal data used in this paper provides references 
to both an initial and a final state. However, due to 
differences in vocabulary size and content between 
the groups, network growth would require com-
plete knowledge of the vocabulary of both the 
source and target groups to precisely decide on the 
nodes to include and where. Network involution, 
the strategy adopted, works in the opposite way 
than network growth. It takes an older group graph 
as the source and decides on the nodes to iterative-
ly remove, regardless of the younger group graph, 
and uses the latter only as a reference for compari-
                                                                 
3 Relevant comparisons are for G1-G2 and G2-G3 pairs. Val-
ues for G1-G3 are only presented for reference.  
54
son of the structure and content of the resulting 
graph.   
For comparison, graph theory metrics allow us 
to measure structural similarity, abstracting away 
from the particular verbs in the graphs. Since 
graphs represent vocabularies, by these metrics we 
aim to analyze vocabulary structure, verifying 
whether it is possible for structures to approximate 
each other. The graphs were measured in relation 
to the following:    
? number of vertices (n), 
? number of edges (M), 
? average minimal path length (L), 
? density (D), 
? average node connectivity (k), 
? average clustering coefficient (C/s) 4, 
? average number of repetitions (r). 
L assesses structure in the sense of positioning: 
how far the nodes are from one another. D and k 
express the relation between number of edges and 
number of nodes in different ways; they are a 
measure of edge proportion. C/s measures the dis-
tribution of edges among the nodes, assessing the 
structure per se. The division by the number of 
disconnected subgraphs extends the concept to 
account for partitioning. Finally, r captures the 
number of different actions for which the same 
verb was employed.  
Although all metrics are useful for analyzing the 
graphs, a subset of four was selected to be used in 
the involution process: k , D, L and C/s. With k  and 
D, we measure semantic share, since that is what 
relations among nodes are supposed to mean (see 
above). L and C/s are intended to measure vocabu-
lary uniformity, since greater distances and lower 
clusterization are related to the presence of subcen-
ters of meaning (again, taking relations as effect of 
semantic share).    
  In order to compare the contents of each graph 
as well, we employed a measure of set similarity; 
in this case, Jaccard?s similarity coefficient (Jac-
card, 1901). With these measures, we analyze how 
close vocabularies of each two groups are in re-
spect to their content. Given two sets A and B, the 
Jaccard?s coefficient J can be calculated as fol-
lows:  
                                                                 
4 We adopt the local clustering coefficient of Watts and Stro-
gatz (1998), but as the graphs may become disconnected 
during network modification, this value is further divided by 
the number of subgraphs. 
  , 
where ?x? is the number of elements in both A and 
B, ?y? is the number of elements only in A, and 
?z? is the number of elements only in B. For this 
purpose, graphs were taken as verb sets, regardless 
of their inner relations. 
To verify the hypothesis that more polysemic 
verbs are more likely to be acquired, by node eli-
mination, verbs were ranked in increasing order of 
polysemy (from less to more polysemic verbs). 
Therefore, at each step of graph involution, a verb 
was selected to be removed, and the resulting 
graph was measured. In case of a tie, verbs with 
the same polysemy value were randomly selected 
until all of them have been removed. Results are 
reported in terms of the averages of 10-fold cross-
validation. 
4 Results 
A topological analysis of the graphs is shown in 
Table 3. As expected, vocabulary size, represented 
by n, increases with age, with G1 and G2 being 
closer in age and size than G2 and G3. A concomi-
tant decrease in the average connectivity (k) of the 
nodes with age suggests vocabulary specialization. 
This decrease is even more clearly shown by densi-
ty (D), since it measures the proportion of edges 
against the theoretical maximum. As age increases, 
so does the average minimal path length (L), with 
less paths through each node, which leads to a 
more structured and distributed network. Speciali-
zation is again represented by a decrease in r, the 
average number of actions for which each verb was 
mentioned (the more repeatedly it is mentioned, 
the less specialized the vocabulary tends to be). 
 
 G1 G2 G3 
n 22 25 31 
L 1.46 1.6 1.98 
D 0.55 0.42 0.27 
M 128 126 126 
C/s 0.84 0.78 0.78 
k 
? = 11.64, 
SD = 6.73 
? = 10.08, 
SD = 4.86 
? = 8.13, 
SD = 4.76 
r 
? = 5.23, 
SD = 4.41 
? = 3.76, 
SD = 3.15 
? = 2.19, 
SD = 1.58 
Table 3: Properties of graphs. 
 
 
55
 
Figure 1. Graphs G1, G2 and G3 respectively.  
 
Results suggest a greater similarity between G1 
and G2 than between G2 and G3. Jaccard?s coeffi-
cient reinforces this result, with a score of 0.52 
between G1 and G2, and of 0.44 between G2 and 
G3. 
Figure 1 shows the graphs for each group, where 
progressive structuring and decentralization can be 
seen. 
The effect of polysemy is observed in the pro-
portion of verbs with a higher degree: G1 is struc-
tured by highly connected verbs (there is a low 
proportion of verbs with low degree), while in G3 
more than 80% of the nodes have a degree of 11 or 
less (Figure 2). 
 
 
Figure 2. Cumulative histogram of node degree. 
5 Simulation Results 
This research investigates the relation between the 
number of meanings and ease of learning, hypothe-
sizing that the more meanings a verb has, the easier 
it is to be learned, and the earlier children will use 
it. Particularly considering graph theory metrics, if 
we remove the verbs with fewer meanings from the 
graph of an older group, the overall structure will 
approximate to that of a younger group. Consider-
ing set theory metrics, as we remove these verbs, 
there should be an increase in the similarity be-
tween the contents of the graphs.  
Therefore, the most relevant part of each chart is 
its initial state. The verbs to be first removed are 
expected to be those that differentiate graphs con-
cerning both structure and content.  
Although the previous results in section 4 sug-
gest an influence of polysemy on the lexical organ-
ization of verbs, we intend to use involutions to 
confirm these tendencies. Each involution is com-
pared to a random counterpart, making the inter-
pretation easy. 
5.1 Network Involution Topology 
The graph theory metrics (k , L, C/s and D) of the 
collected data are shown in Figures 3 and 4 in 
terms of 2 lines: network involution with node 
removal (a) by using the selected criterion, and (b) 
by using random selection (10-fold cross valida-
tion). In addition, each figure also shows the meas-
ure for the younger group as reference (a dashed, 
straight, thick line).   
In each figure, charts are displayed in four col-
umns and two rows. Each column represents a 
graph theory metric, and each row refers to the use 
of a different score. For example, the first chart of 
each figure is the result of average connectivity (k) 
in a complete involution, using Wscore. Each le-
gend refers to all eight charts in the figure. 
The results of the simulations from G2 to G1 
(Figure 3) show that the four metrics are clearly 
distinct from random elimination from the begin-
ning, indicating that polysemy plays a role in the 
process. C/s is particularly distinct from random 
elimination: while the former remains constant 
almost to the end, indicating a highly structured 
(clustered) graph, even during node removal, the 
random elimination shows effects of graph part i-
tioning. The remaining metrics presented their 
greatest approximations to the reference line before 
the middle of the chart, suggesting that the initial 
verbs were actually the ones differentiating both 
graphs. These results suggest an initial increase in 
semantic share, as the proportion of edges by node 
increases (k and D), and in uniformity, as nodes get 
closer to one another (L) and remain clustered 
(C/s). 
         
56
 Simulation Random Reference - G2
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Network involution from G2 to G1 using two scores for node removal: graph theory metrics 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
Figure 4. Network involution from G3 to G2 using two scores for node removal: graph theory metrics 
 
Looking at the involution charts of G3, taking G2 
as reference, the same tendencies are maintained, 
although not as clearly as the previous results (Fig-
ure 4). The greatest approximations between k and 
D happen in the first half of the chart, but much 
closer to the middle when compared with Figure 3. 
C/s still behaves steadily, remaining stable during 
most of the simulation, suggesting maintenance of 
the clustered structure.  
The quasi-random behavior of L can be ex-
plained by the initial structure of the graphs. They 
become progressively sparser as age increases, but 
the difference between G3 and G2 is greater than 
between G2 and G1 (this was both visually and 
statically confirmed). Therefore, G3 would require 
too many removals until the most distant nodes 
were eliminated, even in an ideal elimination s imu-
lation, thus preventing a descent from the begin-
ning. The same can be said about average 
connectivity: since G3 has such a low initial score, 
and low deviation, even if the nodes with the low-
est degrees were eliminated, it would not result in a 
much better result.  
5.2 Network Involution Similarity 
The main metric to analyze set similarity is Jac-
card?s coefficient. There are two important factors 
 Simulation Random Reference - G1
               Average connectivity (k)          Average minimal path (L)        Clustering coefficient (C/s )                    Density (D) 
  
   
   
   
   
   
   
  
H
sc
o
re
  
  
   
   
   
   
  
W
sc
or
e 
               Average connectivity (k)          Average minimal path (L)        Clustering coefficient (C/s )                    Density (D) 
  
   
   
   
   
   
   
  
H
sc
o
re
  
  
   
   
   
   
  
W
sc
or
e 
 
 
0
5
10
15
Iteration
0
0,5
1
1,5
2
Iteration
0
0,2
0,4
0,6
0,8
1
1,2
Iteration
0
0,2
0,4
0,6
0,8
1
1,2
Iteration
0
5
10
15
Iteration
0
0,5
1
1,5
2
Iteration
0
0,2
0,4
0,6
0,8
1
1,2
Iteration
0
0,2
0,4
0,6
0,8
1
1,2
Iteration
 
 
 
0
2
4
6
8
10
12
Iteration
0
0,5
1
1,5
2
2,5
Iteration
0
0,2
0,4
0,6
0,8
1
1,2
Iteration
0
0,2
0,4
0,6
0,8
Iteration
0
2
4
6
8
10
12
Iteration
0
0,5
1
1,5
2
2,5
Iteration
0
0,2
0,4
0,6
0,8
1
1,2
Iteration
0
0,1
0,2
0,3
0,4
0,5
Iteration
57
influencing it: the number of verbs common to 
both sets (the ?x? component of the formula), 
?common verbs? hereby; and the number of verbs 
which are exclusive for the older group, the ?dif-
ferent verbs? (the ?z? component of the formula, 
where the older group is represented by ?B?). In 
the charts, a rise means that ?different verbs? were 
eliminated one by one (increasing set similarity),  
 
    Jaccard?s Coefficient                Excluded Verbs                 
      
      
 
   
 
 
Figure 5. Network involution from G2 to G1 using 
two scores for node removal: set theory metrics. 
 
    Jaccard?s Coefficient                Excluded Verbs                 
          
      
 
   
 
 
Figure 6. Network involution from G3 to G2 using 
two scores for node removal: set theory metrics. 
 
and a descent means that ?common verbs? were 
eliminated instead. 
In addition to Jaccard?s coefficient, we included 
the measures for ?excluded different? verbs and 
?excluded common? verbs (and their random coun-
terparts) in percentage. In this sense, the ?Excluded 
Different? line presents the percentage of the ?dif-
ferent verbs? excluded so far, and similarly in the 
?Excluded Common? line. By doing so, it is possi-
ble to measure the exact evolution of both sets 
despite the proportion between them (there are 
much more ?common? than ?different? verbs). A 
rise in the ?Excluded Different? line means that 
sets are getting similar, while stabilization (since 
descents are not possible) means that they are get-
ting different. The opposite applies to the ?Ex-
cluded Common? line. All lines start at 0% and 
end at 100%. 
In the figures, charts are arranged in columns 
(the parameter being measured) and rows (the 
score being used). This time, each legend is partic-
ular to each parameter (one to Jaccard?s coefficient 
and another to the excluded verbs). 
Both simulation sets (Figures 5 and 6) confirm 
the expected pattern: an initial increase in the pro-
portion between "different" and "common" verbs. 
Jaccard?s coefficient behaves more satisfactorily in 
the second simulation set (Figure 6), where a sharp 
rise is observed before the middle of the chart, thus 
indicating that many ?different verbs? were ex-
cluded. In the first set (Figure 5), Wscore behaves 
ambiguously with two rises: one before and anoth-
er after de middle of the chart. Hscore behaves the 
same way, but the second rise is much sharper than 
the first. Even so, the positive effect of polysemy is 
clear in the ?Excluded Different? and ?Excluded 
Common? lines. We notice that the ?Excluded 
Different? line is usually above the ?Excluded 
Common? in the beginning and far from the ran-
dom values. Wscore in Figure 5 is an exception, 
although a significant rise is observed in the begin-
ning. 
5.3 Discussion 
Results show that metrics behaved in a consistent 
manner, considering the natural variation of differ-
ent sources of information.5 Concerning graph 
                                                                 
5 Since the measures were taken from the whole graph, it was 
not possible to determine a measure of significance without 
other graph configurations to compare to. However, the com-
0
0,1
0,2
0,3
0,4
0,5
0,6
Iteration
0
20
40
60
80
100
120
Iteration
0
0,1
0,2
0,3
0,4
0,5
0,6
Iteration
0
20
40
60
80
100
120
Iteration
0
0,1
0,2
0,3
0,4
0,5
Iteration
0
20
40
60
80
100
120
Iteration
0
0,1
0,2
0,3
0,4
0,5
Iteration
0
20
40
60
80
100
120
Iteration
 
Jaccard's Coefficient
Jaccard's Coefficient -
Random
Excluded Different
Excluded Common
Excluded Different - Random
Excluded Common - Random
 
H
sc
or
e 
 
 
  
   
   
 W
sc
o
re
  
 
Jaccard's Coefficient
Jaccard's Coefficient -
Random
Excluded Different
Excluded Common
Excluded Different - Random
Excluded Common - Random
 
H
sc
or
e 
 
 
  
   
   
 W
sc
o
re
  
58
theory metrics, the early graph disconnection in the 
random simulation alone (in the C/s metric) con-
firmed a structural stability by using polysemy. 
The regular behavior of the Jaccard?s coeffi-
cient in the simulations may be attributed to a high 
similarity between the pair of sets: just 45.16% of 
the verbs in G3 were able to increase the index, 
and just 36% of the verbs in G2 (Table 2). Even so, 
an analysis of the ?Excluded Different? curves 
made it clear that the results were better than they 
appeared to be.  
6 Conclusions and Future Work 
This study investigated the influence of polysemy 
on verb acquisition and organization using both 
graph and set theory metrics. In general, results 
from the topological analysis showed a tendency 
towards the reference value, and the greatest simi-
larities were mostly collected in the beginning, as 
expected, pointing for a preference of children to 
use more polysemous verbs. The static analysis of 
the initial graphs (Tables 1, 2 and 3) corroborate 
the hypothesis. As a result, we note that not only 
does the evolution of human vocabulary lead to a 
decrease in the average polysemy measure, but its 
structure also evolves according to this linguistic 
factor. So we conclude that both the model of invo-
lution and the given analysis are appropriate for 
linguistic studies concerning vocabulary evolution. 
The analyses highlighted also some interesting 
properties reflected in the graphs, such as vocabu-
lary growth and specialization with the increase of 
participants? age. In addition, the analysis was 
useful in showing that the graphs of the two groups 
of children were more similar to each other than to 
that of adults, both in structure and content. 
For future work, we intend to apply the same 
approach to other parameters, such as frequency, 
concreteness, and syntactic complexity. As they 
may simultaneously influence acquisition, we also 
plan to investigate possible combinations of these 
factors. We also intend to apply this methodology 
to investigate lexical dissolution in the context of 
pathologies, such as Alzheimer?s disease, and in 
                                                                                                      
parisons with random elimination can be seen as a tendency. 
Additionally, the experiments consist of two simulations, over 
three different data sets, by using two different sets of polyse-
my, two kinds of metrics, and five different metrics, which 
provide robustness to the results. 
larger data sets, in order to further confirm the 
results obtained so far. 
Acknowledgements  
This research was partly supported by CNPq 
(Projects 479824/2009-6 and 309569/2009-5), 
FINEP and SEBRAE (COMUNICA project 
FINEP/SEBRAE 1194/07). We would also like to 
thank Maria Alice Parente, Lauren Tonietto, Bruno 
Menegola and Gustavo Valdez for providing the 
data.  
References  
R?ka Albert and Albert-L?szl? Barab?si. 2002. Statis-
tical mechanics of complex networks. Reviews of 
modern physics, 74(1):47-97. 
L. Antiqueira, M.G.V. Nunes, O. N. Oliveira Jr., and L. 
da F. Costa. 2007. Strong correlat ions between text 
quality and complex networks features. Physica A: 
Statistical Mechanics and its Applications, 373:811-
820. 
Laura H. F. Barde, Myrna F. Schwartz, and Consuelo B. 
Boronat. 2006. Semantic weight and verb retrieval in 
aphasia. Brain and Language, 97(3):266-278. 
Sarah D. Breedin, Eleanor M. Saffran, and Myrna F. 
Schwartz. 1998. Semantic Factors in Verb Retrieval: 
An Effect of Complexity. Brain and Language, 
63(1):1-31. 
Kathryn A. Coronges, Alan W. Stacy, and Thomas W. 
Valente. 2007. Structural Comparison of Cognitive 
Associative Networks in  Two Populations. Journal of 
Applied Social Psychology, 37(9): 2097-2129. 
Simon de Deyne and Gert Storms. 2008. Word associa-
tions: Network and semantic properties. Behavior 
Research Methods, 40(1): 213-231.  
Bento C. Dias da Silva et al 2000. Constru??o de um 
thesaurus eletr?nico para o portugu?s do Brasil. In  
Proceedings of the 4th Processamento Computacio-
nal do Portugu?s Escrito e Falado (PROPOR) , 1-10. 
Ant?nio Houaiss. 2007. Dicion?rio Eletr?nico Houaiss 
da L?ngua Portuguesa, version 2.0a. Ed itora Objet i-
va. 
Andrew W. Ellis and Catriona M. Morrison. 1998. Real 
Age-of-Acquisition Effects in Lexical Retrieval. 
Journal of Experimental Psychology: Learning, 
Memory, and Cognition, 24(2):515-523 
Andrew W. Ellis and Matthew A. L. Ralph. 2000. Age 
of Acquisition Effects in Adult Lexical Processing 
Reflect Loss of Plasticity in Maturing Systems: In-
sights From Connectionist Networks. Journal of Ex-
perimental Psychology: Learning, Memory, and 
Cognition, 26(5):1103-1123.  
59
Afsaneh Fazly, Afra A lishahi and Suzanne Stevenson. 
2008. A Probabilistic Incremental Model of Word 
Learn ing in the Presence of Referential Uncertainty. 
In Proceedings of the 30th Annual Conference of the 
Cognitive Society (CogSci). 
Christian Fellbaum. 1998. WordNet: An Electronic 
Lexical Database. MIT Press. 
Ramon Ferrer i Cancho, Ricard V. Sol?, and Reinhard 
K?hler. 2004. Patterns in syntactic dependency net-
works. Phys. Rev. E, 69(5).  
Adele E. Goldberg. The Emergence of the Semantics of 
Argument Structure Constructions. 1999. In Emer-
gence of Language. Lawrence Erlbaum Associates , 
Mahwah, NJ. 
James Gorman and James R. Curran. 2007. The Topol-
ogy of Synonymy and Homonymy Networks. In 
Proceedings Workshop on Cognitive Aspects of 
Computational Language Acquisition . 
Thomas T. Hills, Mounir Maouene, Josita Maouene, 
Adam Sheya, and Linda Smith. 2009. Longitudinal 
Analysis of Early Semantic Networks: Preferential 
Attachment or Preferential Acquisition , 20(6): 729-
739.  
Paul Jaccard. 1901. Distribution de la flore alpine dans 
le Bassin des Drouces et dans quelques regions voi-
sines. Bulletin de la Soci?t? Vaudoise des Sciences 
Naturelles, 37(140): 241?272. 
Mikyong Kim and Cynthia K, Thompson. 2004.Verb 
deficits in Alzheimer?s disease and agrammatism: 
Implications for lexical organization. Brain and Lan-
guage, 88(1): 1-20. 
Ping Li, Igor Farkas, and Brian MacWhinney. 2004. 
Early lexical development in a self-organizing neural 
network. Neural Networks, 17(8-9): 1345-1362.  
A. P. Masucci and G. J. Rodgers. 2006. Network prop-
erties of written human language. Physical Review E, 
74(2). 
Erick Galani Maziero, E.G. et al 2008. A Base de Da-
dos Lexical e a Interface Web do TeP 2.0 - Thesaurus 
Eletr?nico para o Portugu?s do Brasil. In Procee-
dings of the 6th Workshop em Tecnologia da Infor-
ma??o e da Linguagem Humana . 
Catriona M. Morrison and Andrew W. Ellis. 1995. 
Roles of Word Frequency and Age of Acquisition in 
Word Naming and Lexical Decision. Journal of Ex-
perimental Psychology: Learning, Memory, and 
Cognition, 21(1): 116-133. 
Adilson E. Motter et al 2002. Topology of the concep-
tual network of language. Physical Review E, 65. 
Roberto Navig li and Mirella Lapata. 2007. Graph Con-
nectivity Measures for Unsupervised Word Sense 
Disambiguation. In  Proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence. 
Douglas L. Nelson, Vanesa M. McKinney, Nancy R. 
Gee, and Gerson A. Janczura. 1998. Interpreting the 
influence of implicitly activated memories on recall 
and recognition. Psychological Review, 105:299-324. 
Mariano Sigman and Guillermo A. Cecchi. 2002. Glob-
al organization of the WordNet lexicon. Proceedings 
of the National Academy of Sciences of the United 
States of America, 99(3). 
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised 
Graph-based Word Sense Disambiguation Using 
Measures of Word Semantic Similarity. In Proceed-
ings of the IEEE International Conference on Seman-
tic Computing (ICSC 2007) .  
Jeffrey M. Siskind. 1996. A computational study of 
cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1): 1-38. 
M. Medeiros Soares, G. Corso, and L. S. Lucena. 2005. 
The network of syllables in Portuguese. Physica A: 
Statistical Mechanics and its Applications, 355(2-4): 
678-684. 
Mark Steyvers and Joshua B. Tenenbaum. 2005. The 
Large-Scale Structure of Semantic Networks: Statis-
tical Analyses and a Model of Semantic Growth. 
Cognitive Science: A Multidisciplinary Journal , 
29(1): 41-78.  
Cynthia K. Thompson. 2003. Unaccusative verb pro-
duction in agrammatic aphasia: the argument struc-
ture complexity hypothesis. Journal of 
Neurolinguistics, 16(2-3). 
Cynthia K. Thompson, Lewis P. Shapiro and Swathi 
Kiran and Jana Sobecks. 2003. The Role of Syntactic 
Complexity in Treatment of Sentence Deficits in 
Agrammat ic Aphasia: The Complexity Account of 
Treatment Efficacy (CATE). Journal of Speech, 
Language, and Hearing Research, 46(3): 591-607. 
Lauren Tonietto. 2009. Desenvolvimento da convencio-
nalidade e especificidade na aquisi??o de verbos: re-
la??es com complexidade sint?tica e categoriza??o . 
Ph.D. thesis, Federal University of Rio Grande do 
Sul. 
Lauren Tonietto, Aline Villavicencio, Maity Siqueira, 
Maria A lice de Mattos Pimenta Parente, Tan ia Mara 
Sperb. 2008. A especificidade sem?ntica como fator 
determinante na aquisi??o de verbos. Psico, 39(3): 
343-351. 
Duncan J. Watts and Steven H. Strogatz. 1998. Collec-
tive dynamics of ?small-world? networks. Nature, 
6684(393):440-442. 
Fei Xu and Joshua B. Tenenbaum. 2007. Word learning 
as Bayesian inference. Psychological Review, 114(2): 
245-272. 
Chen Yu.  2005. The emergence of links between lexi-
cal acquisition and object categorizat ion: A computa-
tional study. Connection Science, 17(3-4): 381-397. 
Chen Yu. 2006. Learn ing syntax?semantics mappings to 
bootstrap word learning. In Proceedings of the 28th 
Conference of the Cognitive Science Society. 
60
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 19?23,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
An Investigation on the Influence of Frequency on the  
Lexical Organization of Verbs 
 
 
Daniel Cerato Germann1  Aline Villavicencio2 Maity Siqueira3 
1Institute of Informatics, Federal University of  Rio Grande do Sul (Brazil) 
2Department of Computer Sciences, Bath University (UK) 
3Institute of Language Studies, Federal University of  Rio Grande do Sul (Brazil) 
{dcgermann,avillavicencio}@inf.ufrgs.br, maitysiqueira@hotmail.com 
 
 
 
  
 
Abstract 
 
This work extends the study of Germann et al 
(2010) in investigating the lexical organization 
of verbs. Particularly, we look at the influence 
of frequency on the process of lexical acquis i-
tion and use. We examine data obtained from 
psycholinguistic action naming  tasks per-
formed by children and adults (speakers of 
Brazilian Portuguese), and analyze some cha-
racteristics of the verbs used by each group in 
terms of similarity of content, using Jaccard?s 
coefficient, and of topology, using graph 
theory. The experiments suggest that younger 
children tend to use more frequent verbs than 
adults to describe events in the world.  
1 Introduction 
The cognitive influence of frequency has been 
proven strong in the learning process of both 
sense and nonsense words (Howes and Solomon, 
1951; Solomon and Postman, 1952). Frequency 
has also been shown to highly correlate with se-
mantic factors, endorsing its importance, through 
the so called ?light verbs? (Goldberg, 1999).  
In this study, we investigate whether words 
that are more frequent have a higher chance of 
earlier acquisition. For this purpose, we com-
pare data from children and adults, native speak-
ers of Brazilian Portuguese, on an action naming 
task, looking at lexical evolution, using statistical 
and topological analysis of the data modeled as 
graphs. Our approach innovates in the sense that 
it directly simulates the influence of a linguistic 
factor over the process of lexical evolution.  
This paper is structured as follows. Section 2 
describes related work. Section 3 presents the 
materials and methods employed. Section 4 
presents the results and section 5 concludes.  
2 Related Work  
Steyvers and Tenenbaum (2005), use some prop-
erties of language networks to propose a model 
of semantic growth, which is compatible with the 
effects of age of acquisition and frequency, in 
semantic processing tasks. The approach pro-
posed in this paper follows Steyvers and Tenen-
baum in the sense of iterative modifications of 
graphs, but differs in method (we use involutions 
instead of evolutions) and objective: modifica-
tions are motivated by the study of frequency 
instead of production of a topological arrange-
ment. It also follows Deyne and Storms (2008), 
in directly relating linguistic factors and graph 
theory metrics, and Coronges et al (2007), in 
comparing networks of different populations. 
This study also follows Tonietto et al (2008) 
in using data from a psycholinguistic action nam-
ing task. However, the analysis is done in terms 
of graph manipulation, instead of pure statistics.  
3 Materials and Methods  
3.1 The Data 
The action naming task was performed by differ-
ent age groups: 55 children and 55 young adults. 
Children?s data are longitudinal; partic ipants of 
the first data collection (G1) aged between 2;0 
and 3;11 (average 3;1), and in the second collec-
tion (G2), between 4;1 and 6;6 (average 5;5) as 
described by Tonietto et al (2008). The adult 
group is unrelated to the children, and aged be-
tween 17;0 and 34;0 (average 21;8). Participants 
were shown 17 actions of destruction or division 
(Tonietto et al 2008) and asked to describe it. 
19
Data processing and justification of the chosen 
domain are described in Germann et al (2010).  
The answers given by each participant were 
collected and annotated with two frequency 
scores, each calculated from a different source. 
The first, Fscore, is the number of occurrences of 
the verb in the ?Florian?polis? corpus (Scliar-
Cabral, 1993; MacWhinney, 2000). The second, 
Yscore, is the number of given results searching 
for the infinitive form of the verb in the ?Ya-
hoo!" Searcher (http://br.yahoo.com). In the ad-
vanced settings, ?Brazil? was selected as country 
and ?Portuguese? as language. Information about 
these two scores for each group is shown in Ta-
ble 1.  
 
 G1 G2 G3 
Average  
type Fscore 44.05 35.92 17.84 
Average  
token  Fscore 
43.44 35.71 21.22 
Average 
type Yscore 
15441904 18443193 10419263 
Average  
token Yscore 10788194 9277047 8927866 
a 
Table 1: Type and token scores1. 
All scores but type Yscore, decrease as age in-
creases, which is compatible with the hypothesis 
investigated. 
3.2 Simulation Dynamics 
Linguistic production of each group was ex-
pressed in terms of graphs, whose nodes 
represent the mentioned verbs. All verbs uttered 
for the same video were assumed share semantic 
information, and then linked together, forming a 
(clique) subgraph. The subgraphs were then con-
nected in a merging step, through the words ut-
tered for more than one video.  
To investigate the influence of frequency on 
the language acquisition process, we used it to 
change the network over time. Network involu-
tion, the strategy adopted, works in the opposite 
way than network growth (Albert and Barab?si, 
2002). Instead of adding nodes, it takes an older 
group graph as the source and decides on the 
nodes to iteratively remove (taking the younger 
group graph only as a reference for comparison).  
Verbs were ranked in increasing order of fre-
quency. At each step of graph involution, the less 
frequent verb was selected to be removed, and 
                                                 
1 Given the measure magnitude, values of Yscore were pre-
sented without the decimal fraction.  
the resulting graph was measured. Results are 
reported in terms of the averages of 10-fold 
cross-validation (because ties imply in random 
selection).  
Graph theory metrics were used to measure 
structural similarity: average minimal path length 
(L), density (D), average node connectivity (k) 
and average clustering coefficient (C/s)2. In the 
involution, k and D, measure semantic share, 
since that is what relations among nodes are sup-
posed to mean (see above). L and C/s are in-
tended to measure vocabulary uniformity, since 
greater distances and lower clusterization are 
related to the presence of subcenters of meaning.  
In order to compare the contents of each graph 
as well, we employed a measure of set similarity: 
Jaccard?s similarity coefficient (Jaccard, 1901). 
Given two sets A and B, the Jaccard?s coefficient 
J can be calculated as follows:  
  , 
where ?x? is the number of elements in both A 
and B, ?y? is the number of elements only in A, 
and ?z? is the number of elements only in B.  
4 Simulation Results  
As we remove the verbs with lower frequency 
from the graph of an older group, the overall 
structure should approximate to that of a younger 
group, and both should get more similar concern-
ing content. Therefore, the most relevant part of 
each chart is the begging: the first removed verbs 
are expected to be those that differentiate graphs. 
4.1 Network Involution Topology 
The graph theory metrics are shown in Figures 1 
and 2 in terms of 2 lines: network involution (a) 
by using the selected criterion, and (b) by using 
random selection (10-fold cross validation). In 
addition, each figure also shows the measure for 
the younger group as reference (a dashed, 
straight, thick line). 
In Figure 1, columns represent a graph theory 
metric, and rows represent the use of a different 
score. Each legend refers to all charts. 
The results for the simulations from G2 to G1, 
(Figure 1) show that the four metrics are clearly 
distinct from random elimination from the be-
ginning, indicating that frequency plays a role in 
the process. C/s is particularly distinct from ran-
                                                 
2 We adopted the local clustering coefficient of Watts and 
Strogatz (1998), but as the graphs may become discon-
nected during network modification, this value is further 
divided by the number of disconnected subgraphs. 
20
dom: while the former remains constant almost 
to the end, indicating a highly structured (clus-
tered) graph, the later shows effects of graph par-
titioning. The remaining metrics presented their 
greatest approximations to the reference line be-
fore the middle of the chart, suggesting that the 
initial verbs were actually the ones differentiat-
ing both graphs. These results suggest an initial 
increase in semantic share, as k and D increase, 
and in uniformity, as nodes get closer to one 
another (L) and remain clustered (C/s). In Figure 
2, the same tendencies are maintained, although 
not as clearly as the previous results. The great-
est approximations of k and D happen in the first 
half of the chart, but in a smoother way. C/s still 
behaves steadily, remaining stable during most 
of the simulation. Yscore resembles Fscore (the 
same way as in Figure 1), and was not presented 
due to space restrictions. 
4.2 Network Involution Set Similarity 
In the Jaccard?s coefficient charts, a rise or stabi-
lization means that ?different verbs? (present 
only in the older graph) were eliminated (in-
creasing set similarity), and a descent means that 
?common verbs? (present in both graphs) were 
eliminated instead. 
Charts for ?excluded different? and ?excluded 
common? verbs (and their random counterparts) 
are presented in percentage. By doing so, it is 
possible to measure the exact evolution of both, 
despite the proportion between them (there are 
much more ?common? than ?different? verbs). A 
rise in the ?Excluded Different? line means that 
sets are getting similar, while stabilization (des-
cents are not possible) means that they are get-
ting different. The opposite applies to the ?Ex-
cluded Common? line.  
In the figures, charts are arranged in columns 
(the score being used) and rows (the parameter 
being measured). Each legend is particular to 
each row (one to Jaccard?s coefficient and anoth-
er to the excluded verbs). 
Both simulation sets (Figures 3 and 4) confirm 
the expected pattern in general: an initial in-
crease in the proportion between "different" and 
?common? verbs. In Figure 3, Yscore presents an 
unexpected descent just before the middle, fol-
lowed by a sharp rise. Since the greatest descent 
happens just in the end, we interpret this middle 
descent as data noise. In Figure 4, Fscore 
presents an almost random result, indicating that 
the score had low impact in content similarity for 
this simulation. Fscore in Figure 3 and Yscore in 
Figure 4 behaved as expected, with most ?differ-
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Involution from G2 to G1 using three scores for node removal: graph theory metrics.  
 
 
 
 
 
 
 
 
 
Figure 2. Involution from G3 to G2 using three scores for node removal: graph theory metrics.  
 Simulation Random Reference - G1
  
   
   
   
  Y
sc
or
e 
 
F
sc
or
e 
 
 
 
0
5
10
15
Iteration
0
0,5
1
1,5
2
Iteration
0
0,2
0,4
0,6
0,8
1
1,2
Iteration
0
0,2
0,4
0,6
0,8
1
1,2
Iteration
0
5
10
15
Iteration
0
0,5
1
1,5
2
Iteration
0
0,2
0,4
0,6
0,8
1
1,2
Iteration
0
0,2
0,4
0,6
0,8
Iteration
              Average connectivity (k)        Average minimal path (L)        Clustering coefficient (C/s )                    Density (D) 
 
 
 
0
2
4
6
8
10
12
Iteration
0
0,5
1
1,5
2
2,5
Iteration
0
0,2
0,4
0,6
0,8
1
1,2
Iteration
0
0,1
0,2
0,3
0,4
0,5
0,6
Iteration
 Simulation Random Reference - G2
              Average connectivity (k)        Average minimal path (L)        Clustering coefficient (C/s )                    Density (D) 
  
   
   
   
 F
sc
or
e
   
F
sc
or
e 
21
 
 
 
   
  
 
  
   
   
 
 
 
 
 
 
Jaccard's Coefficient
Jaccard's Coefficient - Random
Excluded Different
Excluded Common
Excluded Different - Random
Excluded Common - Random
 
 
 
   
  
 
  
   
   
 
 
 
 
 
 
Jaccard's Coefficient
Jaccard's Coefficient - Random
Excluded Different
Excluded Common
Excluded Different - Random
Excluded Common - Random
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Involution from G2 to G1 using three scores for node removal: set theory metrics.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Involution from G3 to G2 using three scores for node removal: set theory metrics. 
 
ent? verbs being excluded before the middle of 
the chart. Jaccard?s coefficient follows the same 
pattern. 
5 Conclusions and Future Work 
This study has investigated the influence of fre-
quency on verb acquisition and organization us-
ing both graph and set theory metrics. In general, 
results from the topological analysis showed a 
tendency towards the reference value, and the 
greatest similarities were mostly collected in the 
beginning, pointing for a preference of children 
to use verbs more frequently perceived in the 
language. So we conclude that both the model of 
involution and the given analysis are appropriate 
for linguistic studies concerning vocabulary evo-
lution3. 
                                                 
3 Since the measures were taken from the whole graph, it is 
not possible to determine a measure of significance. How-
ever, the comparisons with random elimination can be seen 
 For future work, we intend to apply the same 
approach to other parameters, such as concrete-
ness, and syntactic complexity (and combina-
tions, and to investigate lexical dissolution in the 
context of pathologies, such as Alzheimer?s dis-
ease, and in larger data sets, in order to further 
confirm the results obtained so far.  
 
Acknowledgments 
This research was partly supported by CNPq 
(Projects 479824/2009-6 and 309569/2009-5), 
FINEP and SEBRAE (COMUNICA project FI-
NEP/SEBRAE 1194/07). We would also like to 
thank Maria Alice Parente, Lauren Tonietto, 
Bruno Menegola and Gustavo Valdez for provid-
ing the data. 
                                                                          
as a tendency. Additionally, the experiments consist of two 
simulations, over three different data sets, using two differ-
ent sets of frequency (and a combination with polysemy) 
and two kinds of metrics, which provide robustness to the 
results. 
                Fscore                                     Yscore 
  
   
   
E
x
cl
u
d
ed
 V
er
b
s  
  
   
  J
a
cc
a
rd
?s
 C
o
ef
fi
ci
en
t 
  
   
   
  (
P
er
ce
nt
ag
e)
 
  
   
   
   
   
   
  
 
 
 
0
0,1
0,2
0,3
0,4
0,5
0,6
Iteration
0
0,1
0,2
0,3
0,4
0,5
0,6
Iteration
0
20
40
60
80
100
120
Iteration
0
20
40
60
80
100
120
Iteration
 
 
 
0
0,1
0,2
0,3
0,4
0,5
Iteration
0
0,1
0,2
0,3
0,4
0,5
0,6
Iteration
0
20
40
60
80
100
120
Iteration
0
20
40
60
80
100
120
Iteration
                Fscore                                     Yscore
  
  
   
   
E
x
cl
u
d
ed
 V
er
b
s  
  
   
  J
a
cc
a
rd
?s
 C
o
ef
fi
ci
en
t 
  
   
   
  (
P
er
ce
nt
ag
e)
 
  
   
   
   
   
   
  
 
22
References  
R?ka Albert  and Albert-L?s zl?  Barab?si. 2002. Stat is-
tical mechanics of complex networks. Reviews o f 
modern physics, 74(1):47-97. 
Kathryn A. Coronges, Alan W. Stacy and Thomas W. 
Valente. 2007. Structural Comparison of Cognitive 
Associative Networks in  Two Populations. Journal 
of Applied Social Psychology, 37(9): 2097-2129. 
Simon de Deyne and Gert Storms. 2008. Word asso-
ciations: Network and semantic properties. Behavi-
or Research Methods, 40(1): 213-231.  
Daniel Cerato Germann, AlineVillavicencio and 
Maity Siqueira. In press. An Investigation on Poly-
semy and Lexical Organization of Verbs . In  Proce-
edings of the NAALHT - Workshop on Computa-
cional Linguistics 2010. 
Adele E. Goldberg. The Emergence of the Semantics 
of Argument Structure Constructions. 1999. In  
Emergence of Language. Lawrence Erlbaum Asso-
ciates , Mahwah, NJ. 
Davis H. Howes and Richard L. Solomon. 1952. Vis-
ual duration threshold as a function of word-
probability. Journal of Experimental Psychology, 
41(6): 401-410. 
Paul Jaccard. 1901. Distribution de la flo re alp ine 
dans le Bassin des Drouces et dans quelques re-
gions voisines. Bulletin de la Soci?t? Vaudoise des 
Sciences Naturelles, 37(140): 241?272. 
B. MacWhinney. 2000. The CHILDES project: Tools 
for analyzing talk . Lawrence Erlbaum Associates , 
Mahwah, NJ. 
Scliar-Cabral . 1993. Corpus Florian?polis. Retrieved 
January 10, 2009, from 
http://childes.psy.cmu.edu/data/Romance/Portugue
se/Florianopolis.zip  
Richard L. Solomon and Leo  Postman. 1952. Fre-
quency of usage as a determinant of recognition 
thresholds for words. Journal of Experimental Psy-
chology, 43(3): 195-201. 
Mark Steyvers and Joshua B. Tenenbaum. 2005. The 
Large-Scale Structure of Semantic Networks: Sta-
tistical Analyses and a Model o f Semantic Growth. 
Cognitive Science: A Multidisciplinary Journal , 
29(1): 41-78.  
Lauren Tonietto et al 2008. A especificidade sem?n-
tica como fator determinante na aquisi??o de ver-
bos. Psico, 39(3): 343-351. 
Duncan J. Watts and Steven H. Strogatz. 1998. Co l-
lective dynamics of ?small-world? networks. Na-
ture, 6684(393):440-442. 
 
23
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 74?82,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Identifying and Analyzing
Brazilian Portuguese Complex Predicates
Magali Sanches Duran? Carlos Ramisch? ? Sandra Maria Alu??sio? Aline Villavicencio?
? Center of Computational Linguistics (NILC), ICMC, University of Sa?o Paulo, Brazil
? Institute of Informatics, Federal University of Rio Grande do Sul, Brazil
? GETALP ? LIG, University of Grenoble, France
magali.duran@uol.com.br ceramisch@inf.ufrgs.br
sandra@icmc.usp.br avillavicencio@inf.ufrgs.br
Abstract
Semantic Role Labeling annotation task de-
pends on the correct identification of pred-
icates, before identifying arguments and as-
signing them role labels. However, most pred-
icates are not constituted only by a verb: they
constitute Complex Predicates (CPs) not yet
available in a computational lexicon. In order
to create a dictionary of CPs, this study em-
ploys a corpus-based methodology. Searches
are guided by POS tags instead of a limited list
of verbs or nouns, in contrast to similar stud-
ies. Results include (but are not limited to)
light and support verb constructions. These
CPs are classified into idiomatic and less id-
iomatic. This paper presents an in-depth anal-
ysis of this phenomenon, as well as an original
resource containing a set of 773 annotated ex-
pressions. Both constitute an original and rich
contribution for NLP tools in Brazilian Por-
tuguese that perform tasks involving seman-
tics.
1 Introduction
Semantic Role Labeling (SRL), independently of
the approach adopted, comprehends two steps be-
fore the assignment of role labels: (a) the delimita-
tion of argument takers and (b) the delimitation of
arguments. If the argument taker is not correctly
identified, the argument identification will propa-
gate the error and SRL will fail. Argument tak-
ers are predicates, frequently represented only by a
verb and occasionally by Complex Predicates (CPs),
that is, ?predicates which are multi-headed: they are
composed of more than one grammatical element?
(Alsina et al, 1997, p. 1), like give a try, take care,
take a shower. In SRL, the verbal phrases (VPs)
identified by a parser are usually used to automat-
ically identify argument takers, but do no suffice.
A lexicon of CPs, as well as the knowledge about
verbal chains composition, would complete a fully
automatic identification of argument takers. Con-
sequently, the possibility of disagreement between
SRL annotators would rely only on the assignment
of role labels to arguments. This paper reports the
investigation of such multi-word units, in order to
meet the needs arisen from an SRL annotation task
in a corpus of Brazilian Portuguese1.
To stress the importance of these CPs for SRL,
consider the sentence John takes care of his business
in three alternatives of annotation:
The first annotation shows care of his business as
a unique argument, masking the fact that this seg-
ment is constituted of a predicative noun, care, and
its internal argument, of his business. The second
annotation shows care and of his business as argu-
ments of take, which is incorrect because of his busi-
ness is clearly an argument of care. The third an-
notation is the best for SRL purposes: as a unique
predicate ? take care, take shares its external argu-
1CPs constituted by verbal chains (e.g. have been working)
are not focused here.
74
ment with care and care shares its internal argument
with take.
The goal of this paper is twofold: first, we briefly
describe our computer-aided corpus-based method
used to build a comprehensive machine-readable
dictionary of such expressions. Second and most
important, we analyze these expressions and their
behavior in order to shed some light on the most ad-
equate lexical representation for further integration
of our resource into an SRL annotation task. The re-
sult is a database of 773 annotated CPs, that can be
used to inform SRL and other NLP applications.
In this study we classify CPs into two groups: id-
iomatic CPs and less idiomatic CPs. Idiomatic CPs
are those whose sense may not be inferred from their
parts. Examples in Portuguese are fazer questa?o
(make a point), ir embora (go away), dar o fora (get
out), tomar conta (take care), dar para tra?s (give
up), dar de ombros (shrug), passar mal (get sick).
On the other hand, we use ?less idiomatic CPs? to
refer to those CPs that vary in a continuum of differ-
ent levels of compositionality, from fully composi-
tional to semi-compositional sense, that is, at least
one of their lexical components may be litterally
understood and/or translated. Examples of less id-
iomatic CPs in Portuguese are: dar instruc?a?o (give
instructions), fazer menc?a?o (make mention), tomar
banho (take a shower), tirar foto (take a photo), en-
trar em depressa?o (get depressed), ficar triste (be-
come sad).
Less idiomatic CPs headed by a predicative noun
have been called in the literature ?light verb con-
structions? (LVC) or ?support verb constructions?
(SVC). Although both terms have been employed as
synonyms, ?light verb? is, in fact, a semantic con-
cept and ?support verb? is a syntactic concept. The
term ?light verb? is attributed to Jespersen (1965)
and the term ?support verb? was already used by
Gross in 1981. A light verb is the use of a poly-
semous verb in a non prototypical sense or ?with a
subset of their [its] full semantic features?, North
(2005). On the other hand, a support verb is the
verb that combines with a noun to enable it to fully
predicate, given that some nouns and adjectives may
evoke internal arguments, but need to be associated
with a verb to evoke the external argument, that is,
the subject. As the function of support verb is almost
always performed by a light verb, attributes of LVCs
and SVCs have been merged, making them near syn-
onyms. Against this tendency, this study will show
cases of SVCs without light verbs (trazer preju??zo =
damage, lit. bring damage) and cases of LVCs with-
out support verbs (dar certo = work well, lit. give
correct).
To the best of our knowledge, to date, there is no
similar study regarding these complex predicates in
Brazilian Portuguese, focusing on the development
of a lexical resource for NLP tasks, such as SRL.
The remainder of this paper is organized as follows:
in ?2 we discuss related work, in ?3 we present the
corpus and the details about our methodology, in ?4
we present and discuss the resulting lists of candi-
dates, in ?5 we envisage further work and draw our
conclusions.
2 Related Work
Part of the CPs focused on here are represented by
LVCs and SVCs. These CPs have been studied in
several languages from different points of view: di-
acronic (Ranchhod, 1999; Marchello-Nizia, 1996),
language contrastive (Danlos and Samvelian, 1992;
Athayde, 2001), descriptive (Butt, 2003; Langer,
2004; Langer, 2005) and for NLP purposes (Salkoff,
1990; Stevenson et al, 2004; Barreiro and Cabral,
2009; Hwang et al, 2010). Closer to our study,
Hendrickx et al (2010) annotated a Treebank of 1M
tokens of European Portuguese with almost 2,000
CPs, which include LVCs and verbal chains. This
lexicon is relevant for many NLP applications, no-
tably for automatic translation, since in any task in-
volving language generation they confer fluency and
naturalness to the output of the system.
Work focusing on the automatic extraction of
LVCs or SVCs often take as starting point a list of re-
current light verbs (Hendrickx et al, 2010) or a list
of nominalizations (Teufel and Grefenstette, 1995;
Dras, 1995; Hwang et al, 2010). These approaches
are not adopted here because our goal is precisely
to identify which are the verbs, the nouns and other
lexical elements that take part in CPs.
Similar motivation to study LVCs/SVCs (for
SRL) is found within the scope of Framenet (Atkins
et al, 2003) and Propbank (Hwang et al, 2010).
These projects have taken different decisions on how
to annotate such constructions. Framenet annotates
75
the head of the construction (noun or adjective) as
argument taker (or frame evoker) and the light verb
separately; Propbank, on its turn, first annotates sep-
arately light verbs and the predicative nouns (as
ARG-PRX) and then merges them, annotating the
whole construction as an argument taker.
We found studies regarding Portuguese
LVCs/SVCs in both European (Athayde, 2001;
Rio-Torto, 2006; Barreiro and Cabral, 2009; Duarte
et al, 2010) and Brazilian Portuguese (Neves,
1996; Conejo, 2008; Silva, 2009; Abreu, 2011). In
addition to the variations due to dialectal aspects, a
brief comparison between these papers enabled us
to verify differences in combination patterns of both
variants. In addition, Brazilian Portuguese studies
do not aim at providing data for NLP applications,
whereas in European Portuguese there are at least
two studies focusing on NLP applications: Barreiro
and Cabral (2009), for automatic translation and
Hendrickx et al (2010) for corpus annotation.
3 Corpus, Extraction Tool and Methods
We employ a corpus-based methodology in order to
create a dictionary of CPs. After a first step in which
we use a computer software to automatically extract
candidate n-grams from a corpus, the candidate lists
have been analyzed by a linguist to distinguish CPs
from fully compositional word sequences.
For the automatic extraction, the PLN-BR-FULL2
corpus was used, consisting of news texts from
Folha de Sa?o Paulo from 1994 to 2005, with
29,014,089 tokens. The corpus was first prepro-
cessed for sentence splitting, case homogeniza-
tion, lemmatization and POS tagging using the
PALAVRAS parser (Bick, 2000).
Differently from the studies referred to in Sec-
tion 2, we did not presume any closed list of light
verbs or nouns as starting point to our searches. The
search criteria we used contain seven POS patterns
observed in examples collected during previous cor-
pus annotation tasks3:
1. V + N + PRP: abrir ma?o de (give up, lit. open
hand of );
2www.nilc.icmc.usp.br/plnbr
3V = VERB, N = NOUN, PRP = PREPOSITION, DET =
DETERMINER, ADV = ADVERB, ADJ = ADJECTIVE.
2. V + PRP + N: deixar de lado (ignore, lit. leave
at side);
3. V + DET + N + PRP: virar as costas para
(ignore, lit. turn the back to);
4. V + DET + ADV: dar o fora (get out, lit. give
the out);
5. V + ADV: ir atra?s (follow, lit. go behind);
6. V + PRP + ADV: dar para tra?s (give up, lit.
give to back);
7. V + ADJ: dar duro (work hard, lit. give hard).
This strategy is suitable to extract occurrences
from active sentences, both affirmative and negative.
Cases which present intervening material between
the verb and the other element of the CP are not cap-
tured, but this is not a serious problem considering
the size of our corpus, although it influences the fre-
quencies used in candidate selection. In order to fa-
cilitate human analysis of candidate lists, we used
the mwetoolkit4: a tool that has been developed
specifically to extract MWEs from corpora, which
encompasses candidate extraction through pattern
matching, candidate filtering (e.g. through associa-
tion measures) and evaluation tools (Ramisch et al,
2010). After generating separate lists of candidates
for each pattern, we filtered out all those occurring
less than 10 times in the corpus. The entries re-
sulting of automatic identification were classified by
their frequency and their annotation is discussed in
the following section.
4 Discussion
Each pattern of POS tags returned a large number
of candidates. Our expectation was to identify CPs
among the most frequent candidates. First we an-
notated ?interesting? candidates and then, in a deep
analysis, we judged their idiomaticity. In the Table
1, we show the total number of candidates extracted
before applying any threshold, the number of an-
alyzed candidates using a threshold of 10 and the
number of CPs by pattern divided into two columns:
idiomatic and less idiomatic CPs. Additionally, each
CP was annotated with one or more single-verb
4www.sf.net/projects/mwetoolkit
76
Pattern Extracted Analyzed Less idiomatic Idiomatic
V + N + PRP 69,264 2,140 327 8
V + PRP + N 74,086 1,238 77 8
V + DET + N + PRP 178,956 3,187 131 4
V + DET + ADV 1,537 32 0 0
V + ADV 51,552 3,626 19 41
V + PREP + ADV 5,916 182 0 2
V + ADJ 25,703 2,140 145 11
Total 407,014 12,545 699 74
Table 1: Statistics for the Patterns.
paraphrases. Sometimes it is not a simple task to
decide whether a candidate constitutes a CP, spe-
cially when the verb is a very polysemous one and
is often used as support verb. For example, fazer
exame em/de algue?m/alguma coisa (lit. make exam
in/of something/somebody) is a CP corresponding to
examinar (exam). But fazer exame in another use is
not a CP and means to submit oneself to someone
else?s exam or to perform a test to pass examina-
tions (take an exam). In the following sections, we
comment the results of our analysis of each of the
patterns.
4.1 VERB + NOUN + PREPOSITION
The pattern V + N is very productive, as every com-
plement of a transitive verb not introduced by prepo-
sition takes this form. For this reason, we restricted
the pattern, adding a preposition after the noun with
the aim of capturing only nouns that have their own
complements.
We identified 335 complex predicates, including
both idiomatic and less idiomatic ones. For exam-
ple, bater papo (shoot the breeze, lit. hit chat) or
bater boca (have an argument, lit. hit mouth) are
idiomatic, as their sense is not compositional. On
the other side, tomar conscie?ncia (become aware, lit.
take conscience) and tirar proveito (take advantage)
are less idiomatic, because their sense is more com-
positional. The candidates selected with the pattern
V + N + PRP presented 29 different verbs, as shown
in Figure 15.
Sometimes, causative verbs, like causar (cause)
5We provide one possible (most frequent sense) English
translation for each Portuguese verb.
and provocar (provoke) give origin to constructions
paraphrasable by a single verb. In spite of taking
them into consideration, we cannot call them LVCs,
as they are used in their full sense. Examples:
? provocar alterac?a?o (provoke alteration)= al-
terar (alter);
? causar tumulto (cause riot) = tumultuar (riot).
Some of the candidates returned by this pattern
take a deverbal noun, that is, a noun created from
the verb, as stated by most works on LVCs and
SVCs; but the opposite may also occur: some con-
structions present denominal verbs as paraphrases,
like ter simpatia por (have sympathy for) = simpati-
zar com (sympathize with) and fazer visita (lit. make
visit) = visitar (visit). These results oppose the idea
about LVCs resulting only from the combination of a
deverbal noun and a light verb. In addition, we have
identified idiomatic LVCs that are not paraphrasable
by verbs of the same word root, like fazer jus a (lit.
make right to) = merecer (deserve).
Moreover, we have found some constructions
that have no correspondent paraphrases, like fazer
sucesso (lit. make success) and abrir excec?a?o (lit.
open exception). These findings evidence that, the
most used test to identify LVCs and SVC ? the ex-
istence of a paraphrase formed by a single verb, has
several exceptions.
We have also observed that, when the CP has a
paraphrase by a single verb, the prepositions that in-
troduce the arguments may change or even be sup-
pressed, like in:
? Dar apoio a algue?m = apoiar algue?m (give sup-
port to somebody = support somebody);
77
atear (set (on fire))botar (put)
levar (carry)tornar-se (become)
tra?ar (trace)achar (find)
chamar (call)colocar (put)
ganhar (receive/win)lan?ar (throw)
pegar (take/grab)tirar (remove)
trazer (bring)bater (beat)
ficar (stay)p?r (put)
sentir (feel)firmar (firm)
pedir (ask)abrir (open)
causar (cause)fechar (close)
prestar (provide)provocar (provoke)
tomar (take)ser (be)
dar (give)ter (have)
fazer (make/do)
0 10 20 30 40 50 60 70 80
Idiomatic Non idiomatic
Figure 1: Distribution of verbs involved in CPs, consid-
ering the pattern V + N + PRP.
? Dar cabo de algue?m ou de alguma coisa =
acabar com algue?m ou com alguma coisa (give
end of somebody or of something = end with
somebody or with something).
Finally, some constructions are polysemic, like:
? Dar satisfac?a?o a algue?m (lit. give satisfaction
to somebody) = make somebody happy or pro-
vide explanations to somebody;
? Chamar atenc?a?o de algue?m (lit. call the at-
tention of somebody) = attract the attention of
somebody or reprehend somebody.
4.2 VERB + PREPOSITION + NOUN
The results of this pattern have too much noise, as
many transitive verbs share with this CP class the
same POS tags sequence. We found constructions
with 12 verbs, as shown in Figure 2. We classi-
fied seven of these constructions as idiomatic CPs:
dar de ombro (shrug), deixar de lado (ignore), po?r
de lado (put aside), estar de olho (be alert), ficar
de olho (stay alert), sair de fe?rias (go out on vaca-
tion). The later example is very interesting, as sair
de fe?rias is synonym of entrar em fe?rias (enter on
vacation), that is, two antonym verbs are used to ex-
press the same idea, with the same syntactic frame.
In the remaining constructions, the more frequent
ater (sonfansiter )ntonb)npf
nupter lnfvscter uptyf
ctser vt))funer lnf
)noter cteeyfc-n(ter teesonf
m?er mhpfpner -tonf
cd)dcter mhpfngpeter ngpnef
/ w k/ kw z/ zw ?/ ?w 0/
1asd2tpsc 3dgrsasd2tpsc
Figure 2: Distribution of verbs involved in CPs, consid-
ering the pattern V + PRP + N.
ateere (esonfieio)be (fsieio)rrn
pruie (aieeln)tvie ()icrn
)teare (yebofnuipre (-r yte)mn
ubeie ()seonamivie (aippn
i-ebe (t?ronhre (-r yte)mn
)re (miurndigre (vicr/wtn
k zk ?k 0k 1k 2k 3k
4wbtvi)ba 5to bwbtvi)ba
Figure 3: Distribution of verbs involved in CPs, consid-
ering the pattern V + DET + N + PRP.
verbs are used to give an aspectual meaning to the
noun: cair em, entrar em, colocar em, po?r em (fall
in, enter in, put in) have inchoative meaning, that is,
indicate an action starting, while chegar a (arrive at)
has a resultative meaning.
4.3 VERB + DETERMINER + NOUN +
PREPOSITION
This pattern gave us results very similar to the pat-
tern V + N + PRP, evidencing that it is possible
to have determiners as intervening material between
the verb and the noun in less idiomatic CPs. The
verbs involved in the candidates validated for this
pattern are presented in Figure 3.
The verbs ser (be) and ter (have) are special cases.
Some ter expressions are paraphrasable by an ex-
pression with ser + ADJ, for example:
? Ter a responsabilidade por = ser responsa?vel
por (have the responsibility for = be responsi-
ble for);
? Ter a fama de = ser famoso por (have the fame
of = be famous for);
78
? Ter a garantia de = ser garantido por (have the
guarantee of = be guaranteed for).
Some ter expressions may be paraphrased by a
single verb:
? Ter a esperanc?a de = esperar (have the hope of
= hope);
? Ter a intenc?a?o de = tencionar (have the inten-
tion of = intend);
? Ter a durac?a?o de = durar (have the duration of
= last).
Most of the ser expressions may be paraphrased
by a single verb, as in ser uma homenagem para =
homenagear (be a homage to = pay homage to). The
verb ser, in these cases, seems to mean ?to consti-
tute?. These remarks indicate that the patterns ser +
DET + N and ter + DET + N deserve further anal-
ysis, given that they are less compositional than they
are usually assumed in Portuguese.
4.4 VERB + DETERMINER + ADVERB
We have not identified any CP following this pattern.
It was inspired by the complex predicate dar o fora
(escape, lit. give the out). Probably this is typical in
spoken language and has no similar occurrences in
our newspaper corpus.
4.5 VERB + ADVERB
This pattern is the only one that returned more id-
iomatic than less idiomatic CPs, for instance:
? Vir abaixo = desmoronar (lit. come down =
crumble);
? Cair bem = ser adequado (lit. fall well = be
suitable);
? Pegar mal = na?o ser socialmente adequado (lit.
pick up bad = be inadequate);
? Estar de pe?6 = estar em vigor (lit. be on foot =
be in effect);
? Ir atra?s (de algue?m) = perseguir (lit. go behind
(somebody) = pursue);
6The POS tagger classifies de pe? as ADV.
? Partir para cima (de algue?m) = agredir (lit.
leave upwards = attack);
? Dar-se bem = ter sucesso (lit. give oneself well
= succeed);
? Dar-se mal = fracassar (lit. give oneself bad =
fail).
In addition, some CPs identified through this pat-
tern present a pragmatic meaning: olhar la? (look
there), ver la? (see there), saber la? (know there), ver
so? (see only), olhar so? (look only), provided they are
employed in restricted situations. The adverbials in
these expressions are expletives, not contributing to
the meaning, exception made for saber la?, (lit. know
there) which is only used in present tense and in first
and third persons. When somebody says ?Eu sei la??
the meaning is ?I don?t know?.
4.6 VERB + PREPOSITION + ADVERB
This is not a productive pattern, but revealed two
verbal expressions: deixar para la? (put aside) and
achar por bem (decide).
4.7 VERB + ADJECTIVE
Here we identified three interesting clusters:
1. Verbs of double object, that is, an object
and an attribute assigned to the object. These
verbs are: achar (find), considerar (con-
sider), deixar (let/leave), julgar (judge), man-
ter (keep), tornar (make) as in: Ele acha voce?
inteligente (lit. He finds you intelligent = He
considers you intelligent). For SRL annotation,
we will consider them as full verbs with two in-
ternal arguments. The adjective, in these cases,
will be labeled as an argument. However, con-
structions with the verbs fazer and tornar fol-
lowed by adjectives may give origin to some
deadjectival verbs, like possibilitar = tornar
poss??vel (possibilitate = make possible). Other
examples of the same type are: celebrizar
(make famous), esclarecer (make clear), evi-
denciar (make evident), inviabilizar (make un-
feasible), popularizar (make popular), respon-
sabilizar (hold responsible), viabilizar (make
feasible).
79
2. Expressions involving predicative adjectives,
in which the verb performs a functional role, in
the same way as support verbs do in relation to
nouns. In contrast to predicative nouns, pred-
icative adjectives do not select their ?support?
verbs: they combine with any verb of a restrict
set of verbs called copula. Examples of copula
verbs are: acabar (finish), andar (walk), con-
tinuar (continue), estar (be), ficar (stay), pare-
cer (seem), permanecer (remain), sair (go out),
ser (be), tornar-se (become), viver (live). Some
of these verbs add an aspect to the predica-
tive adjective: durative (andar, continuar, es-
tar, permanecer, viver) and resultative (acabar,
ficar, tornar-se, sair).
? The resultative aspect may be expressed
by an infix, substituting the combina-
tion of V + ADJ by a full verb: ficar
triste = entristecer (become sad) or by
the verbalization of the adjective in reflex-
ive form: ficar tranquilo = tranquilizar-se
(calm down); estar inclu??do = incluir-se
(be included).
? In most cases, adjectives preceded by cop-
ula verbs are formed by past participles
and inherit the argument structure of the
verb: estar arrependido de = arrepender-
se de (lit. be regretful of = regret).
3. Idiomatic CPs, like dar duro (lit. give hard =
make an effort), dar errado (lit. give wrong =
go wrong), fazer bonito (lit. make beautiful =
do well), fazer feio (make ugly = fail), pegar
leve (lit. pick up light = go easy), sair errado
(lit. go out wrong = go wrong), dar certo (lit.
give correct = work well).
4.8 Summary
We identified a total of 699 less idiomatic CPs
and observed the following recurrent pairs of para-
phrases:
? V = V + DEVERBAL N, e.g. tratar = dar trata-
mento (treat = give treatment);
? DENOMINAL V = V + N, e.g. amedrontar =
dar medo (frighten = give fear);
atear (set (on fire))botar (put)
lorrer (run)varantir (vuarantee)
cayer (be-caye)soar (sounm)
tornar?se (belohe)tradar (trale)
tratar (treat)lceirar (shegg)
fagar (shegg)ihavinar (spea/)
partir (ihavine)saber (geaye)
torler (/now)yager (wrinv)
yirar (be wortc)yogtar (turn)
vancar (vo bal/)gandar (releiye-win)
tirar (tcrow)traker (rehoye)
passar (brinv)alcar (pass-spenm)
sevuir (foggow)yer (see)
sentir (feeg)lair (fagg)
yir (lohe)bater (beat)
sair (vo out)firhar (firh)
pemir (as/)lcahar (lagg)
lcevar (arriye)ogcar (goo/)
lausar (lause)felcar (lgose)
geyar (geaye-get)ir (vo)
abrir (open)meizar (geaye-get)
pevar (ta/e-vrab)prestar (proyime)
proyolar (proyo/e)p?r (put)
tohar (ta/e)estar (be)
logolar (put)tornar (turn)
entrar (enter)ser (be)
mar (viye)filar (sta0)
ter (caye)faker (mo-ha/e)
1 21 31 41 51 611 621 631
7miohatil 8on imiohatil
Figure 4: Distribution of verbs involved in CPs, consid-
ering the total number of CPs (i.e. all patterns).
? DEADJECTIVAL V = V + ADJ, e.g. res-
ponsabilizar = tornar responsa?vel (lit. respon-
sibilize = hold responsible).
This will help our further surveys, as we may
search for denominal and deadjectival verbs (which
may be automatically recognized through infix and
suffix rules) to manually identify corresponding
CPs. Moreover, the large set of verbs involved in the
analyzed CPs, summarized in Figure 4, shows that
any study based on a closed set of light verbs will
be limited, as it cannot capture common exceptions
and non-prototypical constructions.
5 Conclusions and Future Work
This study revealed a large number of CPs and pro-
vided us insights into how to capture them with more
precision. Our approach proved to be very useful to
identify verbal MWEs, notably with POS tag pat-
80
terns that have not been explored by other studies
(patterns not used to identify LVCs/SVCs). How-
ever, due to the onus of manual annotation, we as-
sume an arbitrary threshold of 10 occurrences that
removes potentially interesting candidates. Our hy-
pothesis is that, in a machine-readable dictionary,
as well as in traditional lexicography, rare entries
are more useful than common ones, and we would
like to explore two alternatives to address this is-
sue. First, it would be straightforward to apply more
sophisticated filtering techniques like lexical asso-
ciation measures to our candidates. Second, we
strongly believe that our patterns are sensitive to
corpus genre, because the CPs identified are typical
of colloquial register. Therefore, the same patterns
should be applied on a corpus of spoken Brazilian
Portuguese, as well as other written genres like web-
crawled corpora. Due to its size and availability, the
latter would also allow us to obtain better frequency
estimators.
We underline, however, that we should not un-
derestimate the value of our original corpus, as it
contains a large amount of unexplored material. We
observed that only the context can tell us whether
a given verb is being used as a full verb or as a
light and/or support verb7. As a consequence, it
is not possible to build a comprehensive lexicon of
light and support verbs, because there are full verbs
that function as light and/or support verbs in spe-
cific constructions, like correr (run) in correr risco
(run risk). As we discarded a considerable number
of infrequent lexical items, it is possible that other
unusual verbs participate in similar CPs which have
not been identified by our study.
For the moment, it is difficult to assess a quan-
titative measure for the quality and usefulness of
our resource, as no similar work exists for Por-
tuguese. Moreover, the lexical resource presented
here is not complete. Productive patterns, the ones
involving nouns, must be further explored to enlarge
the aimed lexicon. A standard resource for English
like DANTE8, for example, contains 497 support
verb constructions involving a fixed set of 5 support
verbs, and was evaluated extrinsically with regard
to its contribution in complementing the FrameNet
7A verb is not light or support in the lexicon, it is light and/or
support depending on the combinations in which it participates.
8www.webdante.com
data (Atkins, 2010). Likewise, we intend to evalu-
ate our resource in the context of SRL annotation, to
measure its contribution in automatic argument taker
identification. The selected CPs will be employed in
an SRL project and, as soon as we receive feedback
from this experience, we will be able to report how
many CPs have been annotated as argument takers,
which will represent an improvement in relation to
the present heuristic based only on parsed VPs.
Our final goal is to build a broad-coverage lexicon
of CPs in Brazilian Portuguese that may contribute
to different NLP applications, in addition to SRL.
We believe that computer-assisted language learning
systems and other Portuguese as second language
learning material may take great profit from it. Anal-
ysis systems like automatic textual entailment may
use the relationship between CPs and paraphrases to
infer equivalences between propositions. Computa-
tional language generation systems may also want
to choose the most natural verbal construction to use
when generating texts in Portuguese. Finally, we be-
lieve that, in the future, it will be possible to enhance
our resource by adding more languages and by link-
ing the entries in each language, thus developing a
valuable resource for automatic machine translation.
Acknowledgements
We thank the Brazilian research foundation FAPESP
for financial support.
References
De?bora Ta??s Batista Abreu. 2011. A sema?ntica
de construc?o?es com verbos-suporte e o paradigma
Framenet. Master?s thesis, Sa?o Leopoldo, RS, Brazil.
1997. Complex Predicates. CSLI Publications, Stanford,
CA, USA.
Maria Francisca Athayde. 2001. Construc?o?es com
verbo-suporte (funktionsverbgefu?ge) do portugue?s e
do alema?o. Number 1 in Cadernos do CIEG Centro
Interuniversita?rio de Estudos German??sticos. Universi-
dade de Coimbra, Coimbra, Portugal.
Sue Atkins, Charles Fillmore, and Christopher R. John-
son. 2003. Lexicographic relevance: Selecting infor-
mation from corpus evidence. International Journal
of Lexicography, 16(3):251?280.
Sue Atkins, 2010. The DANTE Database: Its Contribu-
tion to English Lexical Research, and in Particular to
Complementing the FrameNet Data. Menha Publish-
ers, Kampala, Uganda.
81
Anabela Barreiro and Lu??s Miguel Cabral. 2009. ReE-
screve: a translator-friendly multi-purpose paraphras-
ing software tool. In Proceedings of the Workshop Be-
yond Translation Memories: New Tools for Transla-
tors, The Twelfth Machine Translation Summit, pages
1?8, Ottawa, Canada, Aug.
Eckhard Bick. 2000. The parsing system Palavras.
Aarhus University Press.
Miriam Butt. 2003. The light verb jungle. In Proceed-
ings of the Workshop on Multi-Verb Constructions,
pages 243?246, Trondheim, Norway.
Ca?ssia Rita Conejo. 2008. O verbo-suporte fazer na
l??ngua portuguesa: um exerc??cio de ana?lise de base
funcionalista. Master?s thesis, Maringa?, PR, Brazil.
Laurence Danlos and Pollet Samvelian. 1992. Transla-
tion of the predicative element of a sentence: category
switching, aspect and diathesis. In Proceedings of the
Fourth International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI),
pages 21?34, Montre?al, Canada.
Mark Dras. 1995. Automatic identification of support
verbs: A step towards a definition of semantic weight.
In Proceedings of the Eighth Australian Joint Confer-
ence on Artificial Intelligence, pages 451?458, Can-
berra, Australia. World Scientific Press.
Ine?s Duarte, Anabela Gonc?alves, Matilde Miguel,
Ama?lia Mendes, Iris Hendrickx, Fa?tima Oliveira,
Lu??s Filipe Cunha, Fa?tima Silva, and Purificac?a?o Sil-
vano. 2010. Light verbs features in European Por-
tuguese. In Proceedings of the Interdisciplinary Work-
shop on Verbs: The Identification and Representation
of Verb Features (Verb 2010), Pisa, Italy, Nov.
Iris Hendrickx, Ama?lia Mendes, S??lvia Pereira, Anabela
Gonc?alves, and Ine?s Duarte. 2010. Complex predi-
cates annotation in a corpus of Portuguese. In Pro-
ceedings of the ACL 2010 Fourth Linguistic Annota-
tion Workshop, pages 100?108, Uppsala, Sweden.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Yuping Zhou, Nianwen
Xue, and Martha Palmer. 2010. Propbank annota-
tion of multilingual light verb constructions. In Pro-
ceedings of the ACL 2010 Fourth Linguistic Annota-
tion Workshop, pages 82?90, Uppsala, Sweden.
Otto Jespersen. 1965. A Modern English Grammar on
Historical Principles. George Allen and Unwin Ltd.,
London, UK.
Stefan Langer. 2004. A linguistic test battery for sup-
port verb constructions. Special issue of Linguisticae
Investigationes, 27(2):171?184.
Stefan Langer, 2005. Semantik im Lexikon, chapter
A formal specification of support verb constructions,
pages 179?202. Gunter Naar Verlag, Tu?bingen, Ger-
many.
Christiane Marchello-Nizia. 1996. A diachronic survey
of support verbs: the case of old French. Langages,
30(121):91?98.
Maria Helena Moura Neves, 1996. Grama?tica do por-
tugue?s falado VI: Desenvolvimentos, chapter Estudo
das construc?o?es com verbos-suporte em portugue?s,
pages 201?231. Unicamp FAPESP, Campinas, SP,
Brazil.
Ryan North. 2005. Computational measures of the ac-
ceptability of light verb constructions. Master?s thesis,
Toronto, Canada.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. Multiword expressions in the wild? the
mwetoolkit comes in handy. In Proc. of the 23rd COL-
ING (COLING 2010) ? Demonstrations, pages 57?
60, Beijing, China, Aug. The Coling 2010 Organizing
Committee.
Elisabete Ranchhod, 1999. Lindley Cintra. Home-
nagem ao Homem, ao Mestre e ao Cidada?o, chap-
ter Construc?o?es com Nomes Predicativos na Cro?nica
Geral de Espanha de 1344, pages 667?682. Cosmos,
Lisbon, Portugal.
Grac?a Rio-Torto. 2006. O Le?xico: sema?ntica e
grama?tica das unidades lexicais. In Estudos sobre
le?xico e grama?tica, pages 11?34, Coimbra, Portugal.
CIEG/FLUL.
Morris Salkoff. 1990. Automatic translation of sup-
port verb constructions. In Proc. of the 13th COLING
(COLING 1990), pages 243?246, Helsinki, Finland,
Aug. ACL.
Hilda Monetto Flores Silva. 2009. Verbos-suporte ou
expresso?es cristalizadas? Soletras, 9(17):175?182.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.
2004. Statistical measures of the semi-productivity of
light verb constructions. In , Proc. of the ACL Work-
shop on MWEs: Integrating Processing (MWE 2004),
pages 1?8, Barcelona, Spain, Jul. ACL.
Simone Teufel and Gregory Grefenstette. 1995. Corpus-
based method for automatic identification of support
verbs for nominalizations. In Proc. of the 7th Conf.
of the EACL (EACL 1995), pages 98?103, Dublin, Ire-
land, Mar.
82
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101?109,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Identification and Treatment of Multiword Expressions applied to
Information Retrieval
Otavio Costa Acosta, Aline Villavicencio, Viviane P. Moreira
Institute of Informatics
Federal University of Rio Grande do Sul (Brazil)
{ocacosta,avillavicencio,viviane}@inf.ufrgs.br
Abstract
The extensive use of Multiword Expressions
(MWE) in natural language texts prompts
more detailed studies that aim for a more ade-
quate treatment of these expressions. A MWE
typically expresses concepts and ideas that
usually cannot be expressed by a single word.
Intuitively, with the appropriate treatment of
MWEs, the results of an Information Retrieval
(IR) system could be improved. The aim of
this paper is to apply techniques for the au-
tomatic extraction of MWEs from corpora to
index them as a single unit. Experimental re-
sults show improvements on the retrieval of
relevant documents when identifying MWEs
and treating them as a single indexing unit.
1 Introduction
One of the motivations of this work is to investi-
gate if the identification and appropriate treatment
of Multiword Expressions (MWEs) in an applica-
tion contributes to improve results and ultimately
lead to more precise man-machine interaction. The
term ?multiword expression? has been used to de-
scribe a large set of distinct constructions, for in-
stance support verbs, noun compounds, institution-
alized phrases and so on. Calzolari et al (2002) de-
fines MWEs as a sequence of words that acts as a
single unit at some level of linguistic analysis.
The nature of MWEs can be quite heterogeneous
and each of the different classes has specific char-
acteristics, posing a challenge to the implementa-
tion of mechanisms that provide unified treatment
for them. For instance, even if a standard system ca-
pable of identifying boundaries between words, i.e.
a tokenizer, may nevertheless be incapable of recog-
nizing a sequence of words as an MWEs and treat-
ing them as a single unit if necessary (e.g. to kick the
bucket meaning to die). For an NLP application to
be effective, it requires mechanisms that are able to
identify MWEs, handle them and make use of them
in a meaningful way (Sag et al, 2002; Baldwin et
al., 2003). It is estimated that the number of MWEs
in the lexicon of a native speaker of a language has
the same order of magnitude as the number of sin-
gle words (Jackendoff, 1997). However, these ra-
tios are probably underestimated when considering
domain-specific language, in which the specialized
vocabulary and terminology are composed mostly
by MWEs.
In this paper, we perform an application-oriented
evaluation of the inclusion of MWE treatment into
an Information Retrieval (IR) system. IR systems
aim to provide users with quick access to data
they are interested (Baeza-Yates and Ribeiro-Neto,
1999). Although language processing is not vi-
tal to modern IR systems, it may be convenient
(Sparck Jones, 1997) and in this scenario, NLP tech-
niques may contribute in the selection of MWEs for
indexing as single units in the IR system. The se-
lection of appropriate indexing terms is a key factor
for the quality of IR systems. In an ideal system,
the index terms should correspond to the concepts
found in the documents. If indexing is performed
only with the atomic terms, there may be a loss of
semantic content of the documents. For example, if
the query was pop star meaning celebrity, and the
terms were indexed individually, the relevant docu-
ments may not be retrieved and the system would
101
return instead irrelevant documents about celestial
bodies or carbonated drinks. In order to investigate
the effects of indexing of MWEs for IR, the results
of queries are analyzed using IR quality metrics.
This paper is structured as follows: in section
2 we discuss briefly MWEs and some of the chal-
lenges they represent. This is followed in section 3
by a discussion of the materials and methods em-
ployed in this paper, and in section 4 of the evalu-
ation performed. We finish with some conclusions
and future work.
2 Multiword Expressions
The concept of Multiword Expression has been
widely viewed as a sequence of words that acts as a
single unit at some level of linguistic analysis (Cal-
zolari et al, 2002), or as Idiosyncratic interpreta-
tions that cross word boundaries (or spaces) (Sag
et al, 2002).
One of the great challenges of NLP is the identifi-
cation of such expressions, ?hidden? in texts of var-
ious genres. The difficulties encountered for identi-
fying Multiword Expressions arise for reasons like:
? the difficulty to find the boundaries of a multi-
word, because the number of component words
may vary, or they may not always occur in a
canonical sequence (e.g. rock the boat, rock the
seemingly intransigent boat and the bourgeois
boat was rocked);
? even some of the core components of an MWE
may present some variation (e.g. throw NP to
the lions/wolves/dogs/?birds/?butterflies);
? in a multilingual perspective, MWEs of a
source language are often not equivalent to
their word-by-word translation in the target lan-
guage (e.g. guarda-chuva in Portuguese as um-
brella in English and not as ?store rain).
The automatic discovery of specific types of
MWEs has attracted the attention of many re-
searchers in NLP over the past years. With the recent
increase in efficiency and accuracy of techniques for
preprocessing texts, such as tagging and parsing,
these can become an aid in improving the perfor-
mance of MWE detection techniques. In terms of
practical MWE identification systems, a well known
approach is that of Smadja (1993), who uses a set
of techniques based on statistical methods, calcu-
lated from word frequencies, to identify MWEs in
corpora. This approach is implemented in a lexico-
graphic tool called Xtract. More recently there has
been the release of the mwetoolkit (Ramisch et al,
2010) for the automatic extraction of MWEs from
monolingual corpora, that both generates and vali-
dates MWE candidates. As generation is based on
surface forms, for the validation, a series of crite-
ria for removing noise are provided, including some
(language independent) association measures such
as mutual information, dice coefficient and maxi-
mum likelihood. Several other researchers have pro-
posed a number of computational techniques that
deal with the discovery of MWEs: Baldwin and
Villavicencio (2002) for verb-particle constructions,
Pearce (2002) and Evert and Krenn (2005) for col-
locations, Nicholson and Baldwin (2006) for com-
pound nouns and many others.
For our experiments, we used some standard sta-
tistical measures such as mutual information, point-
wise mutual information, chi-square, permutation
entropy (Zhang et al, 2006), dice coefficient, and
t-test to extract MWEs from a collection of docu-
ments (i.e. we consider the collection of documents
indexed by the IR system as our corpus).
3 Materials and Methods
Based on the hypothesis that the MWEs can improve
the results of IR systems, we carried out an evalua-
tion experiment. The goal of our evaluation is to
detect differences between the quality of the stan-
dard IR system, without any treatment for MWEs,
and the same system improved with the identifica-
tion of MWEs in the queries and in the documents.
In this section we describe the different resources
and methods used in the experiments.
3.1 Resources and Tools
For this evaluation we used two large newspaper cor-
pora, containing a high diversity of terms:
? Los Angeles Times (Los Angeles, USA - 1994)
? The Herald (Glasgow, Scotland - 1995)
Together, both corpora cover a large set of sub-
jects present in the news published by these newspa-
102
pers in the years listed. The language used is Amer-
ican English, in the case of the Los Angeles Times
and British English, in the case of The Herald. Here-
after, the corpus of the Los Angeles Times will be re-
ferred as LA94 and The Herald as GH95. Together,
they contain over 160,000 news articles (Table 1)
and each news article is considered as a document.
Corpus Documents
LA94 110.245
GH95 56.472
Total 166.717
Table 1: Total documents
The collection of documents, as well as the query
topics and the list of relevance judgments (which
will be discussed afterwards), were prepared in the
context of the CLEF 2008 (Cross Language Eval-
uation Forum), for the task entitled Robust-WSD
(Acosta et al, 2008). This task aimed to explore
the contribution of the disambiguation of words to
bilingual or monolingual IR. The task was to as-
sess the validity of word-sense disambiguation for
IR. Thus, the documents in the corpus have been an-
notated by a disambiguation system. The structure
of a document contains information about the identi-
fier of a term in a document (TERM ID), the lemma
of a term (LEMA) and also its morphosyntactic tag
(POS). In addition, it contains the form in which the
term appeared in the text (WF) and information of the
term in the WordNet (Miller, 1995; Fellbaum, 1998)
as SYNSET SCORE and CODE, both not used for
the experiment. An example of the representation of
a term in the document is shown in Figure 1.
<TE
RM
 
ID=
"GH
950
102
-
000
000
-
126
"
LEM
A="
un
derw
orld
" 
POS
=
"NN
">
<W
F>u
nde
rwo
rld<
/WF
>
<SY
NSE
T S
COR
E="
0.5"
 
COD
E="
061
201
71-
n"/>
<SY
NSE
T S
COR
E="
0.5"
 
COD
E="
063
275
98-n
"/>
</TE
RM
> 
Figure 1: Structure of a term in the original documents
In this paper, we extracted the terms located in
the LEMA attribute, in other words, in their canonical
form (e.g. letter bomb for letter bombs). The use of
lemmas and not the words (e.g. write for wrote, writ-
ten, etc.) to the formation of the corpus, avoids lin-
guistic variations that can affect the results of the ex-
periments. As a results, our documents were formed
only by lemmas and the next step is the indexing
of documents using an IR system. For this task we
used a tool called Zettair (Zettair, 2008), which is a
compact textual search engine that can be used both
for the indexing and for querying text collections.
Porter?s Stemmer (Porter, 1997) as implemented in
Zettair was also used. Stemming can provide further
conflation of related terms. For example, bomb and
bombing were not merged in the lemmatized texts
but after stemming they are conflated to a single rep-
resentation.
After indexing, the next step is the preparation of
the query topics. Just as the corpus, only the lemmas
of the query topics were extracted and used. The test
collection has a total of 310 query topics. The judg-
ment of whether a document is relevant to a query
was assigned according to a list of relevant docu-
ments, manually prepared and supplied with the ma-
terial provided by CLEF. We used Zettair to generate
the ranked list of documents retrieved in response
to each query. For each query topic, the 1,000 top
scoring documents were selected. We used the co-
sine metric to calculate the scores and rank the doc-
uments.
Finally, to calculate the retrieval evaluation met-
rics (detailed in Section 3.5) we used the tool trec
eval. This tool compares the list of retrieved docu-
ments (obtained from Zettair) against the list of rel-
evant documents (provided by CLEF).
3.2 Multiword Expression as Single Terms
In this work, we focused on MWEs composed of
exactly two words (i.e. bigrams). In order to incor-
porate MWEs as units for the IR system to index,
we adopted a very simple heuristics that concate-
nated together all terms composing an MWE using
? ? (e.g. letter bomb as letter bomb). Figure 2 ex-
emplifies this concatenation. Each bigram present in
a predefined dictionary and occurring in a document
is treated as a single term, for indexing and retrieval
purposes. The rationale was that documents contain-
ing specific MWEs can be indexed more adequately
than those containing the words of the expression
separately. As a result, retrieval quality should in-
crease.
103
<TE
RM
 
ID=
"GH
950
102
-
000
000
-
126
"
LEM
A="
un
derw
orld
" 
POS
=
"NN
">
<W
F>u
nde
rwo
rld<
/WF
>
<SY
NSE
T S
COR
E="
0.5"
 
COD
E="
061
201
71-
n"/>
<SY
NSE
T S
COR
E="
0.5"
 
COD
E="
063
275
98-n
"/>
</TE
RM
> 
Orig
inal
Top
ic:
-
Wh
atw
as
the
role
ofth
eH
ubb
lete
lesc
ope
inp
rov
ing
the
exis
ten
ce
of
blac
kho
les?
Mod
ified
Top
ic:
-
wha
tbe
the
role
oft
he
hub
ble
tele
sco
pe
inp
rov
eth
ee
xist
enc
eo
f
blac
kho
le?
blac
k_h
ole
<n
um
>14
1</n
um
>
<titl
e>
lette
rbo
mb
for
kies
bau
er
find
info
rm
atio
no
nth
ee
xplo
sion
ofa
lette
r
bom
bin
the
stud
ioo
fthe
tvc
han
nel
pro7
pres
ent
er
ara
bell
aki
esb
au
er
.
lette
r_bo
mb
lette
r_bo
mb
tv_c
han
ne
l
</tit
le>
Figure 2: Modified query.
3.3 Multiword Expressions Dictionaries
In order to determine the impact of the quality of
the dictionary used in the performance of the IR sys-
tem, we examined several different sources of MWE
of varying quality. The dictionaries containing the
MWEs to be inserted into the corpus as a single
term, are created by a number of techniques involv-
ing automatic and manual extraction. Below we de-
scribe how these MWE dictionaries were created.
? Compound Nouns (CN) - for the creation of
this dictionary, we extracted all bigrams con-
tained in the corpus. Since the number of avail-
able bigrams was very large (99,744,811 bi-
grams) we filtered them using the information
in the original documents, the morphosyntactic
tags. Along with the LEMA field, extracted in
the previous procedure, we also extracted the
value of the field POS (part-of-speech). In or-
der to make the experiment feasible, we used
only bigrams formed by compound nouns, in
other words, when the POS of both words was
NN (Noun). Thus, with bigrams consisting
of sequences of NN as a preprocessing step
to eliminate noise that could affect the exper-
iment, the number of bigrams with MWE can-
didates was reduced to 308,871. The next step
was the selection of bigrams that had the high-
est frequency in the text, so we chose candi-
dates occurring at least ten times in the whole
corpus. As a result, the first list of MWEs was
composed by 15,001 bigrams, called D1.
? Best Compound Nouns - after D1, we re-
fined the list with the use of statistical methods.
The methods used were the mutual information
and chi-square. It was necessary to obtain fre-
quency values from Web using the search tool
Yahoo!, because despite the number of terms
in the corpus, it was possible that the newspa-
per genre of our corpus would bias the counts.
For this work we used the number of pages
in which a term occurs as a measure of fre-
quency. With the association measures based
on web frequencies, we generated a ranking in
decreasing order of score for each entry. We
merged the rankings by calculating the average
rank between the positions of each MWE; the
first 7,500 entries composed the second dictio-
nary, called D2.
? Worst Compound Nouns - this dictionary was
created from bigrams that have between five
and nine occurrences and are more likely to co-
occur by chance. It was created in order to
evaluate whether the choice of the potentially
more noisy MWEs entailed a negative effect in
the results of IR, compared to the previous dic-
tionaries. The third dictionary, with 17,328 bi-
grams, is called D3.
? Gold Standard - this was created from a sub-
list of the Cambridge International Dictionary
of English (Procter, 1995), containing MWEs.
Since this list contains all types of MWEs,
it was necessary to further filter these to ob-
tain compound nouns only, using morphosyn-
tactic information obtained by the TreeTagger
(Schmid, 1994), which for English is reported
to have an accuracy of 96.36%? (Schmid,
1994). Formed by 568 MWEs, the fourth dic-
tionary will be called D4.
? Decision Tree - created from the use of the
J48 algorithm (Witten and Frank, 2000) from
Weka (Hall et al, 2009), a data mining tool.
With this algorithm it is possible to make a
MWE classifier in terms of a decision tree. This
requires providing training data with true and
false examples of MWE. The training set con-
tained 1,136 instances, half true (D4) and half
false MWEs (taken from D3). After combining
several statistical methods, the best result for
classification was obtained with the use of mu-
tual information, chi-square, pointwise mutual
information, and Dice. The model obtained
from Weka was applied to test data containing
15,001 MWE candidates (D1). The 12,782 bi-
grams classified as true compose the fifth dic-
104
tionary, called D5.
? Manual - for comparative purposes, we also
created two dictionaries by manually evaluat-
ing the text of the 310 query topics. The first
dictionary contained all bigrams which would
achieve a different meaning if the words were
concatenated (e.g. space shuttle). This dictio-
nary, was called D6 and contains 254 expres-
sions. The other one was created by a spe-
cialist (linguist) who classified as true or false
a list of MWE candidates from the query top-
ics. The linguist selection of MWEs formed D7
with 178 bigrams.
3.4 Creating Indices
For the experiments, we needed to manipulate the
corpus in different ways, using previously built dic-
tionaries. The MWEs from dictionaries have been
inserted in the corpus as single terms, as described
before. For each dictionary, an index was created in
the IR system. These indices are described below:
1. Baseline (BL) - corpus without MWE.
2. Compound Nouns (CN) - with 15 MWEs of
D1.
3. Best CN (BCN) - with 7,500 MWEs of D2.
4. Worst CN (WCN) - with 17,328 MWEs of D3.
5. Gold Standard (GS) - with 568 MWEs of D4.
6. Decision Tree (DT) - with 12,782 MWEs of
D5.
7. Manual 1 (M1) - with 254 MWEs of D6.
8. Manual 2 (M2) - with 178 MWEs of D7.
3.5 Evaluation Metrics
To evaluate the results of the IR system, we need
to use metrics that estimate how well a user?s query
was satisfied by the system. IR evaluation is based
on recall and precision. Precision (Eq. 1) is the por-
tion of the retrieved documents which is actually rel-
evant to the query. Recall (Eq. 2) is the fraction
of the relevant documents which is retrieved by the
IRS.
Precision(P ) =
#Relevant
?
#Retrieved
#Retrieved
(1)
Recall(R) =
#Relevant
?
#Retrieved
#Relevant
(2)
Precision and Recall are set-based measures,
therefore, they do not take into consideration the or-
dering in which the relevant items were retrieved.
In order to evaluate ranked retrieval results the most
widely used measurement is the average precision
(AvP ). AvP emphasizes returning more relevant
documents earlier in the ranking. For a set of
queries, we calculate the Mean Average Precision
(MAP) according to Equation 3 (Manning et al,
2008).
MAP (Q) =
1
|Q|
|Q|?
j=1
1
mj
mj?
k=1
P (Rjk) (3)
where |Q| is the number of queries, Rjk is the set
of ranked retrieval results from the top result until
document dk, and mj is the number of relevant doc-
uments for query j.
4 Experiment and Evaluations
The experiments performed evaluate the insertion of
MWEs in results obtained in the IR system. The
analysis is divided into two evaluations: (A) total
set of query topics, where an overview is given of
the MWE insertion effects and (B) topics modified
by MWEs, where we evaluate only the query topics
that contain MWEs.
4.1 Evaluation A
This evaluation investigates the effects of inserting
MWEs in documents and queries. After each type
of index was generated, MWEs were also included
in the query topics, in accordance to the dictionar-
ies used for each index (for Baseline BL, the query
topics had no modifications).
With eight corpus variations, we obtained indi-
vidual results for each one of them. The results
presented in Table 2 were summarized by the ab-
solute number of relevant documents retrieved and
105
the MAP for the entire set of query topics. In total,
6,379 relevant documents are returned for the 310
query topics.
Index Rel. Retrieved MAP
BL 3,967 0.1170
CN 4,007 0.1179
BCN 3,972 0.1156
WCN 3,982 0.1150
GS 3,980 0.1193
DT 4,002 0.1178
M1 4,064 0.1217
M2 4,044 0.1205
Table 2: Results ? Evaluation A.
It is possible to see a small improvement in the
results for the indices M1 and M2 in relation to the
baseline (BL). This happens because the choice of
candidate MWEs was made from the contents of the
document topics and not, as with other indices, from
the whole corpus. Considering the indices built with
MWEs extracted from the corpus, the best result is
index GS.In second place, comes the CN index, with
a subtle improvement over the Baseline. BL surpris-
ingly got a better result than the Best and Worst CN.
The loss in retrieval quality as a result from MWE
identification for BCN was not expected.
When comparing the gain or loss in MAP of indi-
vidual query topics, we can see how the index BCN
compares to the Baseline: BCN had better MAP in
149 and worse MAP in 108 cases. However, the av-
erage loss is higher than the average gain, this ex-
plains why BL obtains a better result overall. In or-
der do decide if one run is indeed superior to an-
other, instead of using the absolute MAP value, we
chose to calculate a margin of 5%. The intuition
behind this is that in IR, a difference of less than
5% between the results being compared is not con-
sidered significant (Buckley and Voorhees, 2000).
To be considered as gain the difference between the
values resulting from two different indices for the
same query topic should be greater than 5%. Differ-
ences of less than 5% are considered ties. This way,
MAP values of 0.1111 and 0.1122 are considered
ties. Given this margin, we can see in Tables 3 and
4 that the indices BCN and WCN are better com-
pared to the baseline. In the case of BCN, the gain
is almost 20% of cases and the WCN, the difference
between gain and loss is less than 2%.
Gain 60 19.35%
Loss 35 11.29%
Ties 215 69.35%
Total 310 100.00%
Difference between Gain and Loss 8,06%
Table 3: BCN x Baseline
Gain 26 8.39%
Loss 21 6.77%
Ties 263 84.84%
Total 310 100.00%
Difference between Gain and Loss 1.61%
Table 4: WCN x Baseline
Finally, this first experiment guided us toward
a deeper evaluation of the query topics that have
MWEs, because there is a possibility that the MWE
insertions in documents can decrease the accuracy
of the system on topics that have no MWE.
4.2 Evaluation B
This evaluation studies in detail the effects on the
document retrieval in response to topics in which
there were MWEs. For this purpose, we used the
same indices used before and we performed an in-
dividual evaluation of the topics, to obtain a better
understanding on where the identification of MWEs
improves or degrades the results.
As each dictionary was created using a different
methodology, the number of expressions contained
in each dictionary is also different. Thus, for each
method, the number of query topics considered as
having MWEs varies according to the dictionary
used. Table 5 shows the number of query topics
containing MWEs for each dictionary used, and as a
consequence, the percentage of modified query top-
ics over the complete set of 310 topics.
First, it is interesting to observe the values of
MAP for all topics that have been altered by the
identification of MWEs. These values are shown in
Table 6.
As shown in Table 6 we verified that the GS in-
dex obtained the best result compared to others. This
106
Index Topics with MWEs % Modified
BL 0 0.00%
CN 75 24.19%
BCN 41 13.23%
WCN 28 9.03%
GS 9 2.90%
DT 51 16.45%
M1 195 62.90%
M2 152 49.03%
Table 5: Topics with MWEs
Index MAP
CN 0.1011
BCN 0.0939
WCN 0.1224
GS 0.2393
DT 0.1193
M1 0.1262
M2 0.1236
Table 6: Results - Evaluation B
was somewhat expected since the MWEs in that dic-
tionary are considered ?real? MWEs. After GS, best
results were obtained from the manual indices M1
and M2. The index that we consider as containing
the lowest confident MWEs (WCN), obtained better
results than Decision Trees, Nominal Compounds
and Best Nominal Compounds, in this order. One
possible reason for this to happen is that the number
of MWEs inserted is higher than in the other indices.
Compared with the BL, all indices with MWE inser-
tion have improved more than degraded the results,
in quantitative terms. Our largest gain was with
the index GS, where 55.56% of the topics have im-
proved, but the same index showed the highest per-
centage of loss, 22.22%. Analyzing the WCN, we
can identify that this index has the lowest gain com-
pared to all other indices: 32.14%, although having
also the lowest loss. But, 60.71 % of the topics mod-
ified had no significant differences compared to the
Baseline. Thus, we can conclude that the WCN in-
dex is the one that modifies the least the result of a
query. The indices CN and BCN had a similar result,
and knowing that a dictionary used to create BCN is
a subset of the dictionary CN, we can conclude that
the gain values, choosing the best MWE candidates,
does not affect the accuracy, which only improves
subtly. But the computational cost for the insertion
of these MWEs in the corpus was reduced by half. In
terms of gain percentage, indices M1 and M2 were
superior only to WCN, but they are close to other
results, including the DT index, which obtained an
intermediate result between manual dictionaries and
CN. Analyzing some topics in depth, like topic 141
(Figure 3), the best the result among all the indices
was obtained by the CN.
<TE
RM
 
ID=
"GH
950
102
-
000
000
-
126
"
LEM
A="
un
derw
orld
" 
POS
=
"NN
">
<W
F>u
nde
rwo
rld<
/WF
>
<SY
NSE
T S
COR
E="
0.5"
 
COD
E="
061
201
71-
n"/>
<SY
NSE
T S
COR
E="
0.5"
 
COD
E="
063
275
98-n
"/>
</TE
RM
> 
Orig
inal
Top
ic:
-
Wh
atw
as
the
role
ofth
eH
ubb
lete
lesc
ope
inp
rov
ing
the
exis
ten
ce
of
blac
kho
les?
Mod
ified
Top
ic:
-
wha
tbe
the
role
oft
he
hub
ble
tele
sco
pe
inp
rov
eth
ee
xist
enc
eo
f
blac
kho
le?
blac
k_h
ole
<n
um
>14
1</n
um
>
<titl
e>
lette
rbo
mb
for
kies
bau
er
find
info
rm
atio
no
nth
ee
xplo
sion
ofa
lette
r
bom
bin
the
stud
ioo
fthe
tvc
han
nel
pro7
pres
ent
er
ara
bell
aki
esb
au
er
.
lette
r_bo
mb
lette
r_bo
mb
tv_c
han
ne
l
</tit
le>
Figure 3: Topic #141
Table 7 shows the top ten scoring documents re-
trieved for query topic 141 in the baseline. The rele-
vant document (in bold) is the fourth position in the
Baseline. After inserting the expression letter bomb
twice (because it occurs twice in the original topic),
and tv channel that were in dictionary D1 used by the
CN index, the relevant document is scored higher
and as a consequence is returned in the first posi-
tion of the ranking(Table 8) . The MAP of this topic
has increased 75 percentage points, from 0.2500 in
Baseline to 1.000 in the CN index. We see also that
the document that was in first position in the Base-
line ranking, has its score decreased and was ranked
in fourth position in the ranking given by the CN.
This document contained information on a ?small
bomb located outside the of the Russian embassy?
and has is not relevant to topic 141, being properly
relegated to a lower position.
An interesting fact about this topic is that only the
MWE letter bomb influences the result. This was
verified as in the index BCN, whose dictionary does
not have this MWE, the topic was changed only be-
cause of the MWE tv channel and there was no gain
or loss for the result.
The second highest gain was of M1 index, in topic
173. The gain was of 28 percentage points. On the
other hand, we found a downside in M1 and M2
indices, although they improved results on average,
they have reached very high values of loss in some
topics.
107
Position Document Score
P1 LA043094-0230 0.470900
P2 GH950823-000105 0.459994
P3 GH951120-000182 0.439536
P4 GH950610-000164 0.430784
P5 GH950614-000122 0.428766
P6 LA091894-0425 0.428429
P7 GH950829-000082 0.422941
P8 GH950220-000162 0.411968
P9 GH950318-000131 0.406006
P10 GH950829-000037 0.402806
Table 7: Ranking for Topic #141 - Baseline
Position Document Score
P1 GH950610-000164 0.457950
P2 GH950614-000122 0.436753
P3 GH950823-000105 0.423938
P4 LA043094-0230 0.421757
P5 GH951120-000182 0.400123
P6 GH950829-000082 0.393195
P7 LA091894-0425 0.386613
P8 GH950705-000100 0.384116
P9 GH950220-000162 0.382157
P10 GH950318-000131 0.380471
Table 8: Ranking for Topic #141 - CN
In sum, the MWEs insertion seems to improve re-
trieval bringing more relevant documents, due to a
more precise indexing of specific terms. However,
the use of these expressions also brought a negative
impact for some cases, because some topics require
a semantic analysis to return relevant documents (as
for example topic 130, which requires relevant doc-
uments to mention the causes of the death of Kurt
Cobain ? documents which mention his death with-
out mentioning the causes were not considered rele-
vant).
5 Conclusions and Future Work
This work consists in investigating the impact of
Multiword Expressions on applications, focusing on
compound nouns in Information Retrieval systems,
and whether a more adequate treatment for these ex-
pressions can bring possible improvements in the in-
dexing these expressions. MWEs are found in all
genres of texts and their appropriate use is being tar-
geted for study, both in linguistics and computing,
due to the different characteristic variations of this
type of expression, which ends up causing problems
for the success of computational methods that aim
their processing.
In this work we aimed at achieving a better under-
standing of several important points associated with
the use of Multiword Expressions in IR systems. In
general, the MWEs insertion improves the results of
retrieval for relevant documents, because the index-
ing of specific terms makes it easier to retrieve spe-
cific documents related to these terms. Nevertheless,
the use of these expressions made the results worse
in some c]ases, because some topics require a se-
mantic analysis to return relevant documents. Some
of these documents are related to the query, but do
not satisfy all criteria in the query topic. We con-
clude also that the quality of MWEs used directly
influenced the results.
For future work, we would like to use other MWE
types and not just compound nouns as used in this
work. Other methods of extraction and a further
study in Named Entities are good themes to comple-
ment this subject. A variation of corpora, different
from newspaper articles, because each domain has a
specific terminology, can also be an interesting sub-
ject for further evaluation.
References
Otavio Acosta, Andre Geraldo, Viviane Moreira Orengo,
and Aline Villavicencio. 2008. Ufrgs@clef2008:
Indexing multiword expressions for information re-
trieval. Aarhus, Denmark. Working Notes of the
Workshop of the Cross-Language Evaluation Forum -
CLEF.
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. ACM Press / Addison-
Wesley.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: A case study on verb-
particles. Sixth Conference on Computational Natural
Language Learning - CoNLL 2002.
Timothy Baldwin, C. Bannard, T. Tanaka, and D. Wid-
dows. 2003. An empirical model of multiword ex-
pression decomposability. ACL 2003 Workshop on
Multiword Expressions: Analysis, Acquisition and
Treatment.
108
Chris Buckley and Ellen M. Voorhees. 2000. Evaluating
evaluation measure stability. In SIGIR ?00: Proceed-
ings of the 23rd annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 33?40, New York, NY, USA. ACM.
Nicoletta Calzolari, Charles Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine MacLeod,
and Antonio Zampolli. 2002. Towards best prac-
tice for multiword expressions in computational lex-
icons. Third International Conference on Language
Resources and Evaluation - LREC.
Stefan Evert and Brigitte Krenn. 2005. Using small ran-
dom samples for the manual evaluation of statistical
association measures. Computer Speech & Language
- Special Issue on Multiword Expression - Volume 19,
Issue 4, p. 450-466.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
Ray Jackendoff. 1997. The architecture of the language
faculty. MIT Press.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press. 1394399.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38:39?41, November.
Jeremy Nicholson and Timothy Baldwin. 2006. Interpre-
tation of compound nominalisations using corpus and
web statistic. Workshop on Multiword Expressions:
Identifying and Exploiting Underlying Properties.
Darren Pearce. 2002. A comparative evaluation of collo-
cation extraction techniques. Third International Con-
ference on Language Resources and Evaluation.
Martin F. Porter. 1997. An algorithm for suffix strip-
ping. pages 313?316, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Paul Procter. 1995. Cambridge international dictionary
of English. Cambridge University Press, Cambridge,
New York.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. Multiword expressions in the wild? the
mwetoolkit comes in handy. In Coling 2010: Demon-
strations, pages 57?60, Beijing, China, August. Coling
2010 Organizing Committee.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickiger. 2002. Multiword expres-
sions. a pain in the neck for nlp. Third International
Conference on Computational Linguistics and intelli-
gent Text Processing.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics.
Karen Sparck Jones. 1997. What is the role of nlp in text
retrieval? University of Cambridge.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann, San Fran-
cisco.
Zettair. 2008. The zettair search engine. (dispon??vel via
WWW em http://www.seg.rmit.edu.au/zettair).
Yi Zhang, Valia Kordoni, Aline. Villavicencio, and
Marco Idiart. 2006. Automated multiword expression
prediction for grammar engineering. COLING/ACL
2006 Workshop on Multiword Expressions: Identify-
ing and Exploiting Underlying Properties.
109
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 134?136,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Fast and Flexible MWE Candidate Generation
with the mwetoolkit
Vitor De Araujo? Carlos Ramisch? ? Aline Villavicencio?
? Institute of Informatics, Federal University of Rio Grande do Sul, Brazil
? GETALP ? LIG, University of Grenoble, France
{vbuaraujo,ceramisch,avillavicencio}@inf.ufrgs.br
Abstract
We present an experimental environment for
computer-assisted extraction of Multiword
Expressions (MWEs) from corpora. Candi-
date extraction works in two steps: generation
and filtering. We focus on recent improve-
ments in the former, for which we increased
speed and flexibility. We present examples
that show the potential gains for users and ap-
plications.
1 Project Description
The mwetoolkit was presented and demonstrated
in Ramisch et al (2010b) and in Ramisch et al
(2010a), and applied to several languages (Linardaki
et al, 2010) and domains (Ramisch et al, 2010c).
It is a downloadable open-source1 set of command-
line tools mostly written in Python. Our target users
are researchers with a background in computational
linguistics. The system performs language- and
type-independent candidate extraction in two steps2:
1. Candidate generation
? Pattern matching3
? n-gram counting
2. Candidate filtering
? Thresholds, stopwords and patterns
? Association measures, classifiers
1sf.net/projects/mwetoolkit
2For details, see previous papers and documentation
3The following attributes, if present, are supported for pat-
terns: surface form, lemma, POS, syntactic annotation.
The main contribution of our tool, rather than a
novel approach to MWE extraction, is an environ-
ment that systematically integrates the functionali-
ties found in other tools, that is, sophisticated cor-
pus queries like in CQP (Christ, 1994) and Manatee
(Rychly? and Smrz, 2004), candidate generation like
in Text::NSP (Banerjee and Pedersen, 2003), and fil-
tering like in UCS (Evert, 2004). The pattern match-
ing and n-gram counting steps are the focus of the
improvements described in this paper.
2 An Example
Our toy corpus, consisting of the first 20K sentences
of English Europarl v34, was POS-tagged and lem-
matized using the TreeTagger5 and converted into
XML. 6 As MWEs encompass several phenomena
(Sag et al, 2002), we define our target word se-
quences through the patterns shown in figure 1. The
first represents sequences with an optional (?) deter-
miner DET, any number (*) of adjectives A and one
or more (+) nouns N. This shallow pattern roughly
corresponds to noun phrases in English. The sec-
ond defines expressions in which a repeated noun is
linked by a preposition PRP. The backw element
matches a previous word, in this example the same
lemma as the noun identified as noun1.
After corpus indexing and n-gram pattern match-
ing, the resulting unique candidates are returned.
Examples of candidates captured by the first pattern
4statmt.org/europarl
5http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger
6For large corpora, XML imposes considerable overhead.
As corpora do not require the full flexibility of XML, we are
currently experimenting with plain-text, which is already in use
with the new C indexing routines.
134
<pat id="1">
<pat repeat="?"><w pos="DET"/></pat>
<pat repeat="*"><w pos="A"/></pat>
<pat repeat="+"><w pos="N"/></pat>
</pat>
<pat id="2">
<w pos="N" id="noun1"/>
<w pos="PRP"/>
<backw lemma="noun1" pos="noun1"/>
</pat>
Figure 1: Pattern 1 matches NPs, pattern 2 matches se-
quences N1 PRP N1.
are complicated administrative process, the clock,
the War Crimes Tribunal. The second pattern cap-
tures hand in hand, eye to eye, word for word. 7
3 New Features
Friendlier User Interface In the previous ver-
sion, one needed to manually invoke the Python
scripts passing the correct options. The current ver-
sion provides an interactive command-based inter-
face which allows simple commands to be run on
data files, while keeping the generation of interme-
diary files and the pipelining between the different
phases of MWE extraction implicit. At the end, a
user may want to save the session and restart the
work later.8
Regular Expression Support While in the previ-
ous version only wildcard words were possible, now
we support all the operators shown in figure 1 plus
repetition interval (2,3), multiple choice (either)
and in-word wildcards like writ* matching written,
writing, etc. All these extensions allow for much
more powerful candidate patterns to be expressed.
This means that one can also use syntax annotation if
the text is parsed: if two words separated by n words
share a syntactic head, they are extracted. Multi-
attribute patterns are correctly handled during pat-
tern matching, in spite of individual per-attribute in-
dices. Some scripts may fuse the individual indices
on the fly, producing a combined index (e.g. n-gram
counting).
7Currently only contiguous n-grams can be captured; non-
contiguous extraction (e.g., verb-noun pairs, with intervening
material, not part of the expression) is planned.
8Although it is not a graphical interface some users request,
it is far easier to use than the previous version.
Faster processing Candidate generation was not
able to deal with large corpora such as Europarl
and the BNC. The first optimization concerns pat-
tern matching: instead of using the XML corpus and
external matching procedures, now we match candi-
dates using Python?s builtin regular expressions di-
rectly on the corpus index. On a small corpus the
current implementation takes about 72% the origi-
nal time to perform pattern-based generation. On the
BNC, extraction of the two example patterns shown
before took about 4.5 hours and 1 hour, respectively.
The second optimization concerns the creation of
the index. The previous script allowed a static in-
dex to be created from the XML corpus, but it was
not scalable. Thus, we have rewritten index routines
in C. We still assume that the index must fit in main
memory, but the new routines provide faster index-
ing with reasonable memory consumption, propor-
tional to the corpus size. These scripts are still ex-
perimental and need extensive testing. With the C
index routines, indexing the BNC corpus took about
5 minutes per attribute on a 3GB RAM computer.
4 Future Improvements
Additionally to evaluation on several tasks and lan-
guages, we intend to develop several improvements
to the tool. First, we would like to rewrite the pattern
matching routines in C to speed the process up and
reduce memory consumption. Second, we would
like to test several heuristics to handle nested candi-
dates (current strategy returns all possible matches).
Third, we would like to perform more tests on us-
ing regular expressions to extract candidates based
on their syntax annotation. Fourth, we would like
to improve candidate filtering (not emphasized in
this paper) by testing new association measures, fil-
ters, context-based measures, etc. Last but most im-
portant, we are planning a new release version and
therefore we need extensive testing and documenta-
tion.
References
Satanjeev Banerjee and Ted Pedersen. 2003. The de-
sign, implementation, and use of the Ngram Statistic
Package. In Proceedings of the Fourth International
Conference on Intelligent Text Processing and Com-
135
putational Linguistics, pages 370?381, Mexico City,
Mexico, Feb.
Oli Christ. 1994. A modular and flexible architecture
for an integrated corpus query system. In COMPLEX
1994, Budapest, Hungary.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, In-
stitut fu?r maschinelle Sprachverarbeitung, University
of Stuttgart, Stuttgart, Germany.
Evita Linardaki, Carlos Ramisch, Aline Villavicencio,
and Aggeliki Fotopoulou. 2010. Towards the con-
struction of language resources for greek multiword
expressions: Extraction and evaluation. In Stelios
Piperidis, Milena Slavcheva, and Cristina Vertan, ed-
itors, Proc. of the LREC Workshop on Exploitation
of multilingual resources and tools for Central and
(South) Eastern European Languages, pages 31?40,
Valetta, Malta. May.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010a. Multiword expressions in the wild?
the mwetoolkit comes in handy. In Proc. of the 23rd
COLING (COLING 2010) ? Demonstrations, pages
57?60, Beijing, China, Aug. The Coling 2010 Orga-
nizing Committee.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010b. mwetoolkit: a framework for multi-
word expression identification. In Proc. of the Seventh
LREC (LREC 2010), Malta, May. ELRA.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010c. Web-based and combined language
models: a case study on noun compound identification.
In Proc. of the 23rd COLING (COLING 2010), pages
1041?1049, Beijing, China, Aug. The Coling 2010 Or-
ganizing Committee.
Pavel Rychly? and Pavel Smrz. 2004. Manatee, bonito
and word sketches for czech. In Proceedings of
the Second International Conference on Corpus Lin-
guisitcs, pages 124?131, Saint-Petersburg, Russia.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proc. of
the 3rd CICLing (CICLing-2002), volume 2276/2010
of LNCS, pages 1?15, Mexico City, Mexico, Feb.
Springer.
136
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 23?25,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
An annotated English child language database
Aline Villavicencio??, Beracah Yankama?, Rodrigo Wilkens?,
Marco A. P. Idiart?, Robert Berwick?
?Federal University of Rio Grande do Sul (Brazil)
?MIT (USA)
alinev@gmail.com, beracah@mit.edu, rswilkens@gmail.com, marco.idiart@gmail.com, berwick@csail.mit.edu
1 Introduction
The use of large-scale naturalistic data has been
opening up new investigative possibilities for lan-
guage acquisition studies, providing a basis for
empirical predictions and for evaluations of alter-
native acquisition hypotheses. One widely used
resource is CHILDES (MacWhinney, 1995) with
transcriptions for over 25 languages of interac-
tions involving children, with the English corpora
available in raw, part-of-speech tagged, lemma-
tized and parsed formats (Sagae et al, 2010; But-
tery and Korhonen, 2005). With a recent increase
in the availability of lexical and psycholinguistic
resources and robust natural language processing
tools, it is now possible to further enrich child-
language corpora with additional sources of infor-
mation.
In this paper we describe the English CHILDES
Verb Database (ECVD), which extends the orig-
inal lexical and syntactic annotation of verbs
in CHILDES with information about frequency,
grammatical relations, semantic classes, and other
psycholinguistic and statistical information. In
addition, these corpora are organized in a search-
able database that allows the retrieval of data ac-
cording to complex queries that combine different
sources of information. This database is also mod-
ular and can be straightforwardly extended with
additional annotation levels. In what follows, we
discuss the tools and resources used for the anno-
tation (?2), and conclude with a discussion of the
implications of this initial work along with direc-
tions for future research (?3).
2 Linguistic and Statistical
Properties
The English CHILDES Verb Database con-
tains information about the English corpora in
CHILDES parsed using three different pipelines:
(1) MEGRASP; (2) RASP; and (3) the CHILDES
Treebank. In the first, made available as part of
the CHILDES distribution1, the corpora are POS
1http://childes.psy.cmu.edu/
tagged (in %mor), and parsed using MEGRASP
(Sagae et al, 2010) which provides information
about dependency parses and grammatical rela-
tions (in %gra):2
*MOT: I said (.) Adam you could have a banana
and offer Robin and Ursula one (.)would you
?
%mor: pro|I v|say&PAST n:prop|Adam pro|you
aux|could v|have det|a n|banana ...
%gra: 1|2|SUBJ 2|6|CJCT 3|2|OBJ 4|6|SUBJ
5|6|AUX 6|9|COORD 7|8|DET 8|6|OBJ ...
In the second pipeline, the RASP system
(Briscoe et al, 2006) is used for tokenisation,
tagging, lemmatization and parsing of the input
sentences, outputting syntactic trees (in %ST)
and grammatical relations (%GR).3 In both
examples each GR denotes a relation, along with
its head and dependent:
*MOT: oh no # he didn?t say anything about win-
dow .
%ST: (T Oh:1 no:2 ,:3 (S he:4 (VP do+ed:5
not+:6 say:7 anything:8 (PP about:9 (N1
window:10)))) .:11)
%GR: (|ncsubj| |say:7 VV0| |he:4 PPHS1| )
(|aux| |say:7 VV0| |do+ed:5 VDD|)
(|ncmod| |say:7 VV0| |not+:6 XX|)
(|iobj| |say:7 VV0| |about:9 II|) (|dobj|
|say:7 VV0| |anything:8 PN1|) (|dobj|
|about:9 II| |window:10 NN1|)
The third focuses on the Adam corpus from
the Brown data set (Brown, 1973) and uses
the Charniak parser with Penn Treebank style
part of speech tags and output, followed by
hand-curation, as described by Pearl and Sprouse
(2012):
(S1 (SBARQ (WHNP (WP who)) (SQ (VP (COP is)
(NP (NN that)))) (. ?)))
2In an evaluation MEGRASP produced correct depen-
dency relations for 96% of the relations in the gold stan-
dard, with the dependency relations being labelled with the
correct GR 94% of the time.
3The data was kindly provided by P. Buttery and A.
Korhonen and generated as described in (Buttery and Ko-
rhonen, 2005).
23
The use of annotations from multiple parsers
enables the combination of the complementary
strengths of each in terms of coverage and ac-
curacy, similar to inter-annotator agreement ap-
proaches. These differences are also useful for op-
timizing search patterns in terms of the source
which produces the best accuracy for a particu-
lar case. Information about corpora sizes and the
annotated portions for each of the parsers is dis-
played in table 1.
Information Sentences
Total Raw 4.84 million
MEGRASP & RASP Raw 2.5 million
MEGRASP Parsed 109,629
RASP Parsed 2.21 million
CHILDES Treebank 26,280
MEGRASP & RASP Parsed 98,456
Table 1: Parsed Sentences
The verbs in each sentence are also annotated
with information about shared patterns of mean-
ing and syntactic behavior from 190 fine-grained
subclasses that cover 3,100 verb types (Levin,
1993). This annotation allows searches defined
in terms of verb classes, and include all sentences
that contain verbs that belong to a given class.
For instance, searching for verbs of running would
return sentences containing not only run but also
related verbs like slide, roll and stroll.
Additional annotation of properties linked to
language use and recognition include extrinsic fac-
tors such as word frequency and intrinsic factors
such as the length of a word in terms of sylla-
bles; age of acquisition; imageability; and familiar-
ity. Some of this annotation is obtained from the
MRC Psycholinguistic Database (Coltheart, 1981)
which contains 150,837 entries with information
about 26 properties, although not all properties
are available for every word (e.g. IMAG is only
available for 9,240 words).
For enabling complex search functionalities
that potentially combine information from several
sources, the annotated sentences were organized
in a database, and Tables 2 and 3 list some of the
available annotations. Given the focus on verbs,
for search efficiency each sentence is indexed ac-
cording to the verbs it contains. In addition, verbs
and nouns are further annotated with information
shown in table 3 whenever it is available in the
existing resources.
These levels of annotation allow for complex
searches involving for example, a combination of
information about a verb?s lemma, target gram-
matical relations, and occurrence of Levin?s classes
in the corpora.
Not all sentences have been successfully ana-
lyzed, and the comments field contains informa-
Fields
Sentence ID
Corpus
Speaker
File
Raw sentence
MOR and POST tags
MEGRASP dep. and GRs
RASP syntactic tree
RASP dep. and GRs
Comments
Table 2: Information about Sentences
Fields
Word ID
Sentence ID
Levin?s classes
Age of acquisition
Familiarity
Concreteness
Frequency
Imageability
Number of syllables
Table 3: Information about Words
tion about the missing annotations and cases of
near perfect matches that arise from the parsers
using different heuristics for e.g. non-words, meta-
characters and punctuation. These required more
complex matching procedures for identifying the
corresponding cases in the annotations of the
parsers.
3 Conclusions and future work
This paper describes the construction of the En-
glish CHILDES Verb Database. It combines in-
formation from different parsing systems to capi-
talize on their complementary recall and precision
strengths and ensure the accuracy of the searches.
It also includes information about Levin?s classes
for verbs, and some psycholinguistic information
for some of the words, like age of acquisition,
familiarity and imageability. The result is a
large-scale integrated resource that allows com-
plex searches involving different annotation lev-
els. This database can be used to inform analysis,
for instance, about the complexity of the language
employed with and by a child as her age increases,
that can shed some light on discussions about the
poverty of the stimulus. This is an ongoing project
to make the annotated data available to the re-
search community in a user-friendly interface that
allows complex patterns to be specified in a simple
way.
Acknowledgements
This research was partly supported by CNPq
Projects 551964/2011-1, 202007/2010-3,
24
305256/2008-4 and 309569/2009-5.
References
E. Briscoe, J. Carroll, and R. Watson. 2006. The
second release of the rasp system. In Proceedings
of the COLING/ACL 2006 Interactive Presentation
Sessions, Sydney, Australia.
R. Brown. 1973. A first language: The early
stages. Harvard University Press, Cambridge, Mas-
sachusetts.
P. Buttery and A. Korhonen. 2005. Large-scale anal-
ysis of verb subcategorization differences between
child directed speech and adult speech. In Proceed-
ings of the Interdisciplinary Workshop on the Iden-
tification and Representation of Verb Features and
Verb Classes.
M. Coltheart. 1981. The MRC psycholinguistic
database. Quarterly Journal of Experimental Psy-
chology, 33A:497?505.
B. Levin. 1993. English verb classes and alterna-
tions - a preliminary investigation. The University
of Chicago Press.
B. MacWhinney. 1995. The CHILDES project: tools
for analyzing talk. Hillsdale, NJ: Lawrence Erlbaum
Associates, second edition.
L. Pearl and J. Sprouse, 2012. Experimental Syntax
and Islands Effects, chapter Computational Models
of Acquisition for Islands. Cambridge University
Press.
K. Sagae, E. Davis, A. Lavie, B. MacWhinney, and
S. Wintner. 2010. Morphosyntactic annotation of
CHILDES transcripts. Journal of Child Language,
37(03):705?729.
25
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 38?42,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
I say have you say tem:
profiling verbs in children data in English and Portuguese
Rodrigo Wilkens
Institute of Informatics
Federal University of Rio Grande do Sul
Brazil
rswilkens@inf.ufrgs.br
Aline Villavicencio
Institute of Informatics
Federal University of Rio Grande do Sul
Brazil
avillavicencio@inf.ufrgs.br
Abstract
In this paper we present a profile of verb us-
age across ages in child-produced sentences
in English and Portuguese. We examine
in particular lexical and syntactic character-
istics of verbs and find common trends in
these languages as children?s ages increase,
such as the prominence of general and poly-
semic verbs, as well as divergences such as
the proportion of subject dropping. We also
find a correlation between the age of acqui-
sition and the number of complements of a
verb for English.
1 Introduction
In this paper we report on a large scale investiga-
tion of some linguistic and distributional patterns
of verbs in child-produced sentences for two lan-
guages, Portuguese and English. We compare the
characteristics that emerge for two languages that,
in spite of similarities in terms of verb usages also
have important differences, in particular in allow-
ing subject pro-drop, and examine to what degree
these are reflected in the data. This is particularly
relevant given the sparseness (and in some cases
lack) of the Portuguese data, in particular for cer-
tain ages, which may not provide as clear indica-
tions as the English data, but existing analysis for
the latter can also benefit the former and be used
to help assess results obtained for similar trends
found in it.
As such our work is related to that of Buttery
and Korhonen (2007) who perform a large scale
investigation of the subcategorization frames in
the English corpora in CHILDES (MacWhin-
ney, 2000), a database containing transcriptions
of child-directed and child-produced sentences,
comparing preferences in child and adult lan-
guage to provide support for child language ac-
quisition studies. These preferences are found us-
ing large amounts of automatically annotated data
that would be otherwise too costly and time con-
suming to manually annotate.
At present, CHILDES contains data for more
than 25 languages including English and Por-
tuguese. For English, the corpora are currently
available with annotations in raw, part-of-speech-
tagged, lemmatized and parsed formats (Sagae et
al., 2010) (Buttery and Korhonen, 2005) (Buttery
and Korhonen, 2007). Although there are similar
initiatives for other languages, like Spanish and
Hebrew (Sagae et al, 2010), for Portuguese, there
is a lack of such annotations on a large scale. In
this work we address this issue and automatically
annotate the Portuguese corpora with linguistic
and distributional information using a robust sta-
tistical parser, providing the possibility of deeper
analysis of language acquisition data.
Crosslinguistic investigations of child-
produced language have also highlighted the
important role of very general and frequent verbs,
light verbs like go, put and give which are among
the first to be acquired for languages like English
and Italian as discussed by Goldberg (1999). In
this paper we compare patterns found in child
verb usage in English and Portuguese, in one of
the first large scale investigations of syntactically
annotated child-produced Portuguese data. Using
this level of annotation we are able to examine
patterns in verb usage in particular in terms of
subjects and complements. Thus, this work is
also related to the that of Valian (1991) who found
a subject pro-drop rate of around 70% for 2 to 3
year old children in Italian, a pro-drop language,
38
and even a significant number of subject omission
for English, which is not a pro-drop language.
This investigation aims at producing a large-
coverage profile of child verb usage that can in-
form computational models of language acquisi-
tion, by both reporting on preferences in child lan-
guage as a whole and on a developmental level.
This paper is structured as follows: in section 2
we report on the resources used for this investiga-
tion, and the results are discussed in section 3. We
finish with some conclusions and future work.
2 Resources
For examining child-produced data we use the
English and Portuguese corpora from CHILDES
(MacWhinney, 2000). The English corpora in
CHILDES have been parsed using at least three
different pipelines: MOR, POST and MEGRASP
(available as part of the CHILDES distribution,
the corpora are POS tagged using the MOR and
POST programs (Parisse and Normand, 2000)).
In addition we use a version annotated with the
RASP system (Briscoe et al, 2006), that tok-
enizes, tags, lemmatizes and parses the input sen-
tences, outputting syntactic trees and then adding
grammatical relations (GR) as described by (But-
tery and Korhonen, 2005). This corpus contains
16,649 types and 76,386,369 tokens in 3,031,217
sentences distributed by age as shown in Table 1.
Table 1: Frequency of words and sentences by age
in years in CHILDES for English and Portuguese
Age English Portuguese
Words (k) Sent (k) Words (k) Sent (k)
0 4,944 130 0 0
1 12,124 604 7 2
2 19,481 1,367 8 1
3 17,962 468 0 0
4 16,725 249 1 61
5 3,266 121 38 1
6 782 19 47 1
7 1,088 63 56 1
8 12 5 56 1
The Portuguese, CHILDES contains 3 corpora:
(1) Batore?o, with 60 narratives, 30 from adults
and 30 from children, about two stories; (2)
Porto Alegre with data from 5 to 9 year old chil-
dren, collected both cross-sectionally and longitu-
dinally; and (3) Floriano?polis with the longitudi-
nal data for one Brazilian child: 5530 utterances
in broad phonetic transcription.
0 1 2 3 4 5 6 7 800.05
0.10.15
0.20.25
0.3
 
 
noun (en) verb (pt) preposition (pt) noun (pt)
(a) Portuguese
0 1 2 3 4 5 6 7 800.05
0.10.15
0.20.25
0.3
 
 
verb (en) preposition (en) noun (en) verb (pt) preposition (pt) noun (pt)
(b) English
Figure 1: Verbs in relation to other frequent Parts-
of-speech in English (1b) and Portuguese (1a)
The combined size of the Portuguese corpora
in sentences and words is in Table 1. These were
annotated with the PALAVRAS parser, a robust
parser, which has a reported accuracy of 99%
for part-of-speech tagging, 96-97% for syntactic
trees, and 91.8% for multiword expressions (Bick,
2000)1. The childes annotation were first normal-
ized to deal with incomplete words and remove
transcription annotations, and then automatically
lemmatized, POS tagged, parsed and assigned se-
mantic tags for nouns, verbs and adjectives.
3 Verbs in children data
To characterize verb usage in each of these lan-
guages we examined the distribution of verbs
across the ages in terms of their relative frequen-
cies, the number of syntactic complements with
which they occur, and looking at possible links
between these and age of acquisition, as reported
by Gilhooly and Logie (1980).
Figure 1 focuses on the relative distributions of
verbs in relation to other frequent parts-of-speech:
prepositions and nouns. For both languages verbs
account for around 20% of the words used, and
this proportion remains constant as age increases,
with the exception of the discontinuity for years 3
1The PALAVRAS parser was evaluated using European
and Brazilian Portuguese newspaper corpora (CETENFolha
and CETEMPblico) composed of 9,368 sentences.
39
0 1 2 3 4 5 6 7 80
0.050.1
0.150.2
0.25
 
 
have say ir ser estar ter ver
(a) Portuguese
0 1 2 3 4 5 6 7 80
0.050.1
0.150.2
 
 
be get go have say ir ser estar ter ver
(b) English
Figure 2: 5 most frequent verbs in Portuguese (2a)
and in English (2b)
to 5 due to the lack of data for children with these
ages in the Portuguese corpora in CHILDES.
Table 2: Verb types and tokens for English and
Portuguese
Language Types Tokens
English 34,693 17,830,777
Portuguese 62,048 888,234
Table 2 shows the number of verb types and to-
kens in these two languages. Among these verbs,
the top 5 most frequent verbs2 for each language
are: be, get, go, have and say for English and ir
(go) ser (be) estar (be), ter (have) and ver (see)
for Portuguese. These correspond to very general
and polysemous verbs, and their relative propor-
tions in the two languages remain high throughout
the ages for children, figure 2. The frequencies
for English are consistent with those reported by
Goldberg (1999) and the Portuguese data is com-
patible with the crosslinguistic trends for related
languages.
In terms of the syntactic characteristics of verbs
in child-produced data, we examine separately
2The reported frequency for each verb is for the lemma-
tized form, including all its inflected forms.
0 1 2 3 4 5 6 7 80.20.3
0.40.5
0.60.7
0.80.9
 
 
subj+verb (pt) only verb (pt)
(a) Portuguese
0 1 2 3 4 5 6 7 800.2
0.40.6
0.81
 
 
subj+verb (pt) only verb (pt)
(b) English
Figure 3: Percentage of sentences of verb with
and without subject in Portuguese (3a) and in En-
glish (3b)
the occurrence of subjects and other comple-
ments in these languages, using the syntactic an-
notation provided by the RASP and PALAVRAS
parsers. In the RASP annotation (Briscoe, 2006)
we search for 3 types of complements in English:
a direct object (dobj), the second NP comple-
ment in a double object construction (obj2) and
an indirect PP object (iobj). For Portuguese, we
search the PALAVRAS annotation for the follow-
ing types of objects: a direct (accusative) object
(ACC), a dative object (DAT), an indirect prepo-
sitional object (PIV) and an object complement
(OC).3
For subjects figure 3 shows the occurrences of
overt (subj verb) and omitted subjects (only verb)
in sentences in relation to the total number of
verbs (verb) for the two languages. These are a
source of divergence between them as in the En-
glish data most of the verb usages consistently
have an overt subject, and only around 10-20%
omit the subject, but these tend to occur less as the
age increases, with a peak for 2 year old children.
In Portuguese, on the other hand, initially most of
the verb usages omit the subject, and only later
3http://beta.visl.sdu.dk/visl/pt/info/symbolset-
manual.html
40
0 1 2 3 4 5 6 7 800.2
0.40.6
0.81
 
 
0NP (pt) 1NP (pt) 2NP (pt) 3NP(pt)
(a) Portuguese
0 1 2 3 4 5 6 7 800.2
0.40.6
0.81
 
 
0NP (en) 1NP (en) 2NP (en) 3NP(en)
(b) English
Figure 4: Percentage of occurrence of objects in
Portuguese (4a) and in English (4b)
this trend is reversed, but still maintaining a high
proportion of subject dropping, around 40% of
verb usages, and around 60% including an overt
subject. The precise age for this change cannot
be assessed from this data, due to the lack of sen-
tences for 3-5 year old children in the Portuguese
data. This difference between the two languages
can be explained as a result of Portuguese being
a (subject) pro-drop language and children being
consistently exposed to subject dropping in their
linguistic environment. Although English is not a
pro-drop language, children, especially at an early
age, still produce sentences without overt sub-
jects, as much discussed in the literature (Valian,
1991) and more recently (Yang, 2010). Children
learning pro-drop languages seem to adopt it from
an early age and use it with a frequency much
closer to adult usage (Valian, 1991).
In relation to other verb complements, we ex-
amine the changes in the distribution of verbs
and their subcategorization frames in the corpus
across children?s ages. Figure 4 shows the distri-
bution per age for verbs with one, two and three
complements for both languages. As expected in
general verbs with fewer complements are more
frequently used and as the number of comple-
ments increases, the frequency decreases, for all
ages and for both languages. Moreover, as age in-
creases, there is a slight but constant increase in
the presence of verbs with 2 and 3 complements
in the corpus, with a small decrease in those with
only 1, which nonetheless still account for the ma-
jority of the cases. These patterns are more clearly
visible for English, as more data is available than
for Portuguese for all ages.
To further investigate this we analyzed whether
a relation between the number of complements of
a verb and its age of acquisition could be found.
For English we used the age of acquisition (AoA)
scores from Gilhooly and Logie (1980) which is
available for 22 of the verbs in the English data,
but from these two verbs were removed from the
set, as they did not occur in all the ages. For Por-
tuguese, the scores from Marques et al (2007)
are available for only four verbs in the CHILDES
corpora, and were therefore not considered in this
analysis. Using the total frequency for a verb in
the corpus, we calculated the relative frequencies
for each number of complements (0, 1, 2 and 3)
per age. For each verb and each age the number of
complements with maximum frequency was used
as the basis for checking if a correlation with the
AoA scores for the verb could be found. In terms
of the number of complements per age these verbs
can be divided into 3 groups, apart from 2 of the
verbs (lock and burn) that do not have any clear
pattern:
0-obj: for verbs that are used predominantly with-
out complements throughout the ages, think,
speak, swim, lie, turn, fly, try;
1-obj: for verbs that appear consistently with 1
complement for all ages, drive, chop, hate,
find, win, tear;
0-to-1: for verbs initially used mostly without com-
plements but then consistently with 1 com-
plement, hurt, guess, throw, kick, hide.
In terms of the age of acquisition, verbs in the
0-obj group tend to have lower scores than those
in the second group, with a 0.72 Spearman?s rank
correlation coefficient indicating a high correla-
tion between AoA and predominant number of
complements of a verb. As the third group had
both patterns, it was not considered in the anal-
ysis. These results suggest that the number of
syntactic objects tends to increase with the age
of acquisition. This may be partly explained by
41
a potential increase in complexity as the num-
ber of obligatory arguments for a verb increase
(Boynton-Hauerwas, 1998). However, more in-
vestigation is needed to confirm this trend.
4 Conclusions
In this paper we presented a wide-coverage pro-
file of verbs in child-produced data, for English
and Portuguese. We examined the distribution
of some lexical and syntactic characteristics of
verbs in these languages. Common trends, such
as the prominent role of very general and poly-
semic verbs among the most frequently used and
a preference for smaller number of complements
were found throughout the ages in both languages.
Divergences between them such as the proportion
of subject dropping in each language were also
found: a lower proportion for English which de-
creases with age and a higher proportion for Por-
tuguese which remains relatively high. These re-
sults are compatible with those reported by e.g.
Goldberg (1999) and Valian (1991), respectively.
Furthermore, for English we found a high cor-
relation between a lower age of acquisition of a
verb and a lower predominant number of com-
plements. Given the size of the Portuguese data,
for some of these analyses further investigation
is needed with more data to confirm the trends
found.
For future work we intend to extend these anal-
yses for other parts-of-speech, particularly nouns,
also looking at other semantic and pragmatic fac-
tors, such as polysemy, concreteness and famil-
iarity. In addition, we plan to examine intrinsic
(e.g. length of words; imageability; and famil-
iarity) and and extrinsic factors (e.g. frequency),
and their effect in groups with typical develop-
ment and with specific linguistic impairments.
References
Bick, E. 2000. The Parsing System Palavras. Au-
tomatic Grammatical Analysis of Portuguese in a
Constraint Grammar Framework. [S.l.]: University
of Arhus.
Bick, E. 2003. Multi-level NER for Portuguese in a
CG framework. Proceedings of the Computational
Processing of the Portuguese Language.
Boynton-Hauerwas, L. S. 1998. The role of general
all purpose verbs in language acquisition: A com-
parison of children with specific language impair-
ments and their language-matched peers. North-
western University
Briscoe, E., Carroll, J., and Watson, R. 2006. The sec-
ond release of the rasp system. COLING/ACL 2006
Interactive Presentation Sessions, Sydney, Aus-
tralia.
Briscoe, T. 2006. An introduction to tag sequence
grammars and the RASP system parser. Technical
report in University of Cambridge, Computer Lab-
oratory.
Buttery, P., Korhonen, A. 2005. Large Scale Anal-
ysis of Verb Subcategorization differences between
Child Directed Speech and Adult Speech. Interdis-
ciplinary Workshop on the Identification and Rep-
resentation of Verb Features and Verb Classes.
Buttery, P., Korhonen, A. 2007. I will shoot your
shopping down and you can shoot all my tins?
Automatic Lexical Acquisition from the CHILDES
Database. Proceedings of the Workshop on Cogni-
tive Aspects of Computational Language Acquisi-
tion. Association for Computational Linguistics.
Gilhooly, K.J. and Logie, R.H. 1980. Age of acqui-
sition, imagery, concreteness, familiarity and am-
biguity measures for 1944 words. Behaviour Re-
search Methods and Instrumentation.
Goldberg, Adele E. . The Emergence of Language,
chapter Emergence of the semantics of argument
structure constructions, pages 197?212. Carnegie
Mellon Symposia on Cognition Series.
Hsu, A. S., Chater, N. 2010. Aspects of the Theory of
Syntax. MIT Press.
MacWhinney, B. 2000. The CHILDES project: tools
for analyzing talk. Lawrence Erlbaum Associates,
second edition.
Marques, J. F., Fonseca, F. L., Morais, A. S., Pinto,
I. A. 2007. Estimated age of acquisition norms
for 834 Portuguese nouns and their relation with
other psycholinguistic variables. Behavior Re-
search Methods.
Parisse, C. and Normand, M. T. Le. 2000. Automatic
disambiguation of the morphosyntax in spoken lan-
guage corpora. Behavior Research Methods, In-
struments, and Computers.
Pavio, A., Yuille, J.C., and Madigan, S.A. 1968. Con-
creteness, imagery and meaningfulness values for
925 words. Journal of Experimental Psychology
Monograph Supplement.
Sagae, K., Davis, E., Lavie, A., MacWhinney, B. and
Wintner, S. 2010. Morphosyntactic annotation of
CHILDES transcripts. Journal of Child Language.
Toglia, M.P. and Battig, W.R. 1978. Handbook of
Semantic Word Norms. New York: Erlbaum.
Valian, V. 1991. Syntactic subjects in the early speech
of American and Italian Children. Journal of Cog-
nition.
Yang, Charles 2010. Three factors in language varia-
tion. Lingua.
42
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 43?50,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Get out but don?t fall down: verb-particle constructions in child language
Aline Villavicencio??, Marco A. P. Idiart?, Carlos Ramisch?,
V??tor Arau?jo?, , Beracah Yankama?, Robert Berwick?
?Federal University of Rio Grande do Sul (Brazil)
?MIT (USA)
alinev@gmail.com, marco.idiart@gmail.com, ceramisch@inf.ufrgs.br,
vbuaraujo@inf.ufrgs.br, beracah@mit.edu, berwick@csail.mit.edu
Abstract
Much has been discussed about the chal-
lenges posed by Multiword Expressions
(MWEs) given their idiosyncratic, flexi-
ble and heterogeneous nature. Nonethe-
less, children successfully learn to use them
and eventually acquire a number of Mul-
tiword Expressions comparable to that of
simplex words. In this paper we report
a wide-coverage investigation of a partic-
ular type of MWE: verb-particle construc-
tions (VPCs) in English and their usage
in child-produced and child-directed sen-
tences. Given their potentially higher com-
plexity in relation to simplex verbs, we
examine whether they appear less promi-
nently in child-produced than in child-
directed speech, and whether the VPCs
that children produce are more conserva-
tive than adults, displaying proportionally
reduced lexical repertoire of VPCs or of
verbs in these combinations. The results
obtained indicate that regardless of any ad-
ditional complexity VPCs feature widely in
children data following closely adult usage.
Studies like these can inform the develop-
ment of computational models for language
acquisition.
1 Introduction
There has been considerable discussion about
the challenges imposed by Multiword Expres-
sions (MWEs) which in addition to crossing word
boundaries act as a single lexical unit at some lev-
els of linguistic analysis (Calzolari et al, 2002;
Sag et al, 2002; Fillmore, 2003). They include a
wide range of grammatical constructions such as
verb-particle constructions (VPCs), idioms, com-
pound nouns and listable word configurations,
such as terminology and formulaic linguistic units
(Wray, 2009). Depending on the definition, they
may also include less traditional sequences like
copy of in They gave me a copy of the book (Fill-
more et al, 1988), greeting formulae like how
do you do?, and lexical bundles such as I dont
know whether or memorized poems and famil-
iar phrases from TV commercials (Jackendoff,
1997). These expressions may have reduced syn-
tactic flexibility, and be semantically more opaque
so that their semantics may not be easily inferred
from their component words. For instance, to play
down X means to (try to) make X seem less im-
portant than it really is and not literally a playing
event.
These expressions may also breach general
syntactic rules, sometimes spanning phrasal
boundaries and often having a high degree of lex-
icalisation and conventionality. They form a com-
plex of features that interact in various, often un-
tidy, ways and represent a broad continuum be-
tween non-compositional (or idiomatic) and com-
positional groups of words (Moon, 1998). In ad-
dition, they are usually sequences or groups of
words that co-occur more often than would be ex-
pected by chance, and have been argued to appear
in the same order of magnitude in a speaker?s lex-
icon as the simplex words (Jackendoff, 1997).
In terms of language acquisition difficulties
may arise as the interpretation of these expres-
sions often demands more knowledge than just
about (1) unitary words and (2) word-to-word re-
lations. This introduces a distinction between
what a learner is able to computationally disam-
biguate or figure out automatically from language
and what must be explicitly stored/memorized
and retrieved whole from memory at the time of
43
use, rather than being subject to generation or
analysis by the language grammar (Wray, 2009,
p. 9). Yet, according to Fillmore et al (1988),
in an ideal learning environment, most of the
knowledge about how to use a language should
be computable while explicitly memorized se-
quences should be kept to a minimum.
Due to these idiosyncrasies they have been
noted as easily phonetically mislearned: e.g. by
and large mistaken for by in large, to all in-
tents and purposes for to all intensive purposes,
and an arm and a leg for a nominal egg (Fill-
more, 2003). For second language (L2) learn-
ers in particular (Wray, 2002) MWEs are in-
deed a well-known cause of problems and less
likely to be used by them than by native speak-
ers in informal spoken contexts (Siyanova and
Schmitt, 2007). Even if L2 learners may be capa-
ble of producing a large number of MWEs, their
underlying intuitions and fluency do not match
those of native speakers (Siyanova and Schmitt,
2008) and they may produce marked combina-
tions that are not conventionally used together
(e.g. plastic surgery/?operation, strong/?powerful
tea) (Pearce, 2002; Siyanova and Schmitt, 2007).
Given the potential additional sources of com-
plexity of MWEs for learning, in this paper we
investigate whether children shy away from us-
ing them when they communicate. We focus on
a particular type of MWEs, VPCs, which present
a wide range of syntactic and semantic idyosin-
crasies examining whether children produce pro-
portionally less VPCs than adults. In addition, we
analyze whether any potential added processing
costs for VPCs are reflected in a reduced choice
of VPCs or verbs to form these combinations in
child-produced sentences compared to adult us-
age. Finally, given the possibility of flexible word
orders in VPCs with the verb and particle not only
occurring adjacently but also with an NP object
between them, we compare these two groups in
terms of distances between the verb and the par-
ticle in these combinations, to determine whether
there is a preference for a joint or a split config-
uration and if children and adults adopt distinct
strategies for their usage. By profiling the VPC
usage by children our aim is to provide the basis
for a computational modeling of the acquisition of
these constructions.
This paper is structured as follows: in sec-
tion 2 describes VPCs and related works; sec-
tion 3 presents the resources and methods used in
this paper. The analyses of VPCs in children and
adults sentences are in section 4. We finish with
conclusions and possibilities of future works.
2 Related Work
VPCs are combinations of verbs and prepositional
(up, down, ...), adverbial (away, back,...), adjecti-
val (short,...) or verbal (go, be,...) particles, and in
this work we focus on VPCs with prepositional or
adverbial particles like put off and move on. From
a language acquisition perspective, the complex-
ity of VPCs arises from their wide syntactic as
semantic variability.
Syntactically, like simplex verbs, VPCs can oc-
cur in different subcategorisation frames (e.g. in-
transitive in break down and transitive in print NP
up). However, the type of verb and the num-
ber of arguments of a VPC seem to have an
impact in learning as both children with typical
development and with specific language impair-
ments (SLI) seem to use obligatory arguments and
inflectional morphology more consistently with
general all purpose verbs, like make, go, do, put,
than with more specific verbs. Moreover, as the
number of obligatory arguments increases chil-
dren with SLI seem to produce more general and
fewer specific verbs (Boynton-Hauerwas, 1998).
Goldberg (1999b) refers to these verbs as light
verbs, suggesting that due to their frequency of
use, they are acquired earlier by children, and sub-
sequently act as centers of gravity from which
more specific instances can be learnt. These verbs
are very common and frequent in the everyday
communication, that could be used in place of
more specialized instances (e.g. make instead of
build).
In transitive VPCs there is the additional diffi-
culty of the particle appearing in different word
orders in relation to the verb: in a joint configu-
ration, adjacent to the verb (e.g. make up NP) or
in a split configuration after the NP complement
(make NP up) (Lohse et al, 2004). While some
VPCs can appear in both configurations, others
are inseparable (run across NP), and a learner has
to successfully account for these. Gries (2002)
using a multifactorial analysis to investigate 25
variables that could be linked to particle place-
ment like size of the direct object (in syllables
and words), type of NP (pronoun or lexical), type
of determiner (indefinite or definite). For a set
44
of 403 VPCs from the British National Corpus
he obtains 84% success in predicting (adult) na-
tive speakers? choice. Lohse et al (2004) propose
that these factors can be explained by consider-
ations of processing efficiency based on the size
of the object NP and on semantic dependencies
among the verb, the particle, and the object. In a
similar study for children Diessel and Tomasello
(2005) found that the type of the NP (pronoun vs
lexical NP) and semantics of the particle (spatial
vs non-spatial) were good predictors of placement
on child language data.
Semantically, one source of difficulties for
learners comes from the wide spectrum of compo-
sitionality that VPCs present. On one end of the
spectrum some combinations like take away com-
positionally combine the meaning of a verb with
the core meaning of a particle giving a sense of
motion-through-location (Bolinger, 1971). Other
VPCs like boil up are semi-idiomatic (or aspec-
tual) and the particle modifies the meaning of the
verb adding a sense of completion or result. At the
other end of the spectrum, idiomatic VPCs like
take off, meaning to imitate have an opaque mean-
ing that cannot be straightforwardly inferred from
the meanings of each of the components literally.
Moreover, even if some verbs form combinations
with almost every particle (e.g., get, fall, go,...),
others are selectively combined with only a few
particles (e.g., book and sober with up), or do not
combine well with them at all (e.g., know, want,
resemble,...) (Fraser, 1976). Although there are
some semi-productive patterns in these combina-
tions, like verbs of cooking and the aspectual up
(cook up, boil up, bake up), and stative verbs not
forming VPCs, for a learner it may not be clear
whether an unseen combination of verb and parti-
cle is indeed a valid VPC that can be produced or
not. Sawyer (1999) longitudinal analysis of VPCs
in child language found that children seem to treat
aspectual and compositional combinations differ-
ently, with the former being more frequent and
employing a larger variety of types than the lat-
ter. The sources of errors also differ and while
for compositional cases the errors tend to be lexi-
cal, for aspectuals there is a predominance of syn-
tactic errors such as object dropping, which ac-
counts for 92% of the errors in split configura-
tion for children under 5 (Sawyer, 1999). Chil-
dren with SLI tended to produce even more object
dropping errors for VPCs than children with typ-
ical development, despite both groups producing
equivalent numbers of VPCs (Juhasz and Grela,
2008). Given that compositionality seems to have
an impact on learning, to help reduce avoidance
of phrasal verbs Sawyer (2000) proposes a seman-
tic driven approach for second language learning
where transparent compositional cases would be
presented first to help familiarization with word
order variation, semi-idiomatic cases would be
taught next in groups according to the contribu-
tion of the particle (e.g telicity or completive-
ness), and lastly the idiomatic cases that need to
be memorized.
In this paper we present a wide coverage ex-
amination of VPC distributions in child produced
and child-directed sentences, comparing whether
children reproduce the linguistic environment to
which they are exposed or whether they present
distinct preferences in VPC usage.
3 Materials and Methods
For this work we use the English corpora from
the CHILDES database (MacWhinney, 1995)
containing transcriptions of child-produced and
child-directed speech from interactions involving
children of different age groups and in a variety
of settings, from naturalistic longitudinal studies
to task oriented latitudinal cases. These corpora
are available in raw, part-of-speech-tagged, lem-
matized and parsed formats (Sagae et al, 2010).
Moreover the English CHILDES Verb Construc-
tion Database (ECVCD) (Villavicencio et al,
2012) also adds for each sentence the RASP pars-
ing and grammatical relations (Briscoe and Car-
roll, 2006), verb semantic classes (Levin, 1993),
age of acquisition, familiarity, frequency (Colt-
heart, 1981) and other psycholinguistic and dis-
tributional characteristics. These annotated sen-
tences are divided into two groups according to
the speaker annotation available in CHILDES, the
Adults Set and the Children Set contain respec-
tively all the sentences spoken by adults and by
children1, as shown in table 1 as Parsed.
VPCs in these corpora are detected by look-
ing in the RASP annotation for all occurrences
of verbs followed by particles, prepositions and
adverbs up to 5 words to the right, following
Baldwin (2005), shown as Sentences with VPCs
1For the latter sentences which did not contain informa-
tion about age were removed.
45
Sentences Children Set Adults Set
Parsed 482,137 988,101
with VPCs 44,305 83,098
with VPCs Cleaned 38,326 82,796
% with VPCs 7.95 8.38
Table 1: VPCs in English Corpora in the Children
and Adults Sets
in table 1. The resulting sentences are subse-
quently automatically processed to remove noise
and words mistagged as verbs. For these candi-
dates with non-alphabetic characters, like @ in
a@l up, were removed as were those that did not
involve verbs (e.g. di, dat,), using the Comlex
Lexicon as reference for verb validity (Macleod
and Grishman, 1998). The resulting sets are listed
as Sentences with VPCs Cleaned in table 1. The
analyses reported in this paper use these sen-
tences, and the distribution of VPCs per children
age group is shown in table 2. Given the non-
uniform amounts of VPC for each age group, and
the larger proportion of VPC sentences in younger
ages in these corpora, we consider children as a
unique group. For these, the individual frequen-
cies of the verb, the particle and the VPC are col-
lected separately in the children set and in the
adult set, using the mwetoolkit (Ramisch et al,
2010).
Age in months VPC Sentences
0-24 2,799
24-48 26,152
48-72 8,038
72-96 1,337
>96 514
No age 4,841
Table 2: VPCs in Children Set per Age
To evaluate the VPCs in these sets, we use:
? English VPC dataset (Baldwin, 2008); which
lists 3,078 VPCs with valency (intransitive
and transitive) information;
? Comlex lexicon (Macleod and Grishman,
1998) containing 10,478 phrasal verbs;
? the Alvey Natural Language Tools (ANLT)
lexicon (Carroll and Grover, 1989) with
6,351 phrasal verbs.
4 VPCs in Child Language
To investigate whether any extra complexity in the
acquisition of VPCs is reflected in their reduced
presence in child-produced than in child-directed
sentences, we compare the proportion of VPCs in
the Children and Adults Sets, table 3. In absolute
terms adults produced more than double the num-
ber of VPCs that children did. However, given
the differences in size of the two sets, in relative
terms there was a similar proportion of VPC us-
age in these corpora for each of the groups: 7.95%
of the sentences produced by children contained
VPCs vs 8.38% of those by adults. Moreover, the
frequencies with which these VPCs are used by
both children and adults reflects the Zipfian distri-
bution found for the use of words in natural lan-
guages, with a large part of the VPCs occurring
just once in the data, table 4. In addition, in terms
of frequency, children?s production of VPCs re-
sembles that of the adults.
Total VPC Children Set Adults Set
Tokens 38,326 82,796
Types 1,579 2,468
Table 3: VPC usage in CHILDES
Frequency Children Set Adults Set
1 42.62% 43.03%
2 13.05% 15%
3 8.36% 6.48%
4 4.05% 4.5%
?5 31.92% 31%
Table 4: VPC types per frequency
Another possible source of divergence between
children and adults is in the lexical variety found
in VPCs. The potential difficulties with VPCs
may be manifested in children producing a re-
duced repertoire of VPCs or using a smaller set
of verbs to form these combinations. As shown in
table 3, adults, as expected, employ a larger VPC
vocabulary with 1.56 more types than children.
However, an examination of the distributions of
types reveals that they only differ by a scale. As
a result when children frequencies are multiplied
by a factor of 2.16, which corresponds to the ra-
tio between VPC tokens used by adults and chil-
dren (table 3), the resulting distribution has a very
46
good match with the adult distribution, see fig-
ure 1. Therefore, the lower number of VPC types
used by children can be explained totally by the
lower number of sentences they produced, and the
hypothesis that difficulties in VPCs would lead to
their avoidance is not confirmed by the data.
Nonetheless, there is a discrepancy between
the distributions found for the higher frequency
VPCs. Children have a more uniform distribution
and adults tend to repeat more often the higher
frequency combinations (top left corner of fig-
ure 1). An evidence that this discrepancy is partic-
ular for high frequency VPCs, and not their con-
stituent verbs, is shown in figure 2. This figure
displays the rank plot for the verbs present in the
VPCs, for both adults and children. The same
scale factor used in figure 1 is applied to compen-
sate for the lower number of VPC sentences in the
children set. This time the match is extraordinary,
spanning the whole vocabulary.
100 102 10410
0
101
102
103
104
105
rank
freq
uen
cy
VPC Usage
 
 adults
children*
Figure 1: VPC Usage Frequency vs Ranking. The
children frequency is scaled to match adult total
VPC usage.
Ranks however, might not tell the whole story.
It is important to verify if the same VPCs and
verbs are present in the both vocabularies, and fur-
ther if their orders in the ranks are similar. The
two groups have very similar preferences for VPC
usage, with a Kendall ? score of 0.63 which indi-
cates that they are highly correlated, as Kendall
? ranges from -1 to 1. Furthermore they use a
very similar set of verbs in VPCs, with a Kendall
100 102 10410
0
101
102
103
104
105
rank
freq
uen
cy
Verbs in VPCs Usage
 
 adultschildren*
Figure 2: Verbs in VPCs Usage Frequency vs
Ranking. The children frequency is scaled to
match adult total VPC usage.
? score of 0.84 pointing to a very strong corre-
lation. We find less agreement between the or-
ders of VPCs and verbs for both children and
adults, indicating that the order of the verbs in
the data is not predictive of the relative frequen-
cies of VPCs. We examined (a) if children?s VPC
ranks followed their verb ranks, (b) if adults VPC
ranks followed their verb ranks and (c) if chil-
dren?s VPC ranks followed adults? verb ranks.
The resulting Kendall scores were around 0.2 for
all three cases. Moreover, if the lower frequency
VPCs are removed to avoid potential cases of
noise, the Kendall ? score for VPCs by adults and
children increases with the threshold, second line
from the top in Figure 3, while it remains constant
for all the other cases. As an example, the top 10
VPC types used by children and adults are listed
in table 5. From these, 9 out of the 10 are the
same differing only in the order in which they ap-
pear. Most of these combinations are listed in one
of the dictionaries used for evaluation: 72% for
adults and 75.87% for children. When a thresh-
old of at least 5 counts is applied these values go
up to 87.72% for adults and 79.82% for children,
as would be expected. This indicates that besides
any possible lack of coverage for child-directed
VPCs in the lexicons or noise, it is in the lower
frequency combinations that novel and domains
specific non-standard usages can be found. Some
47
Rank Chidren Children Adult Adult Child
VPC Freq VPC Freq Rank
1 put on 2005 come on 6244 7
2 go in 1608 put on 4217 1
3 get out 1542 go on 2660 9
4 take off 1525 get out 2251 3
5 fall down 1329 take off 2249 4
6 put in 1284 put in 2177 6
7 come on 1001 sit down 2133 8
8 sit down 981 go in 1661 2
9 go on 933 come out 1654 10
10 come out 872 pick up 1650 18
Table 5: Top VPCs for Children and Adults
of the combinations not found in these dictionar-
ies include crawl in and creep up by adults and
erase off and crash down by children.
0
0.2
0.4
0.6
0.8
1
0 5 10 20
Lexical Choices for VPCs
K
e
n
d
a
l
l
 
t
a
u
threshold
Children / Adults VPCs Children VPCs / Verbs
Adults VPC / Verbs Children VPCs / Adult Verbs
Children /Adult Verbs
Figure 3: Kendall ? score per VPC frequency
threshold
Finally, despite adults having a larger verb vo-
cabulary used in VPCs than children, the two
groups have similar ratios of verb per VPCs: 2.81
VPCs for children and 2.79 for adults, table 6.
The top verbs used in VPCs types are also respon-
sible for very frequent VPC tokens (e.g. go, get,
come, take, put, make and move) accounting for
5.83% VPC types and 43.76% tokens for adults
and 7.02% of the types and 47.81% of the to-
kens for children, confirming the discrepancy dis-
cussed earlier. These are very general verbs and
some of the most frequent in the data, reported
among the first to be learned (Goldberg, 1999a)
which may facilitate their acquisition and use in
VPCs.
Comparing VPC types used by children and by
adults, this trend is confirmed: a large proportion
(72.32%) of the VPC types that children use is
also used by adults, Children ? Adult in table 6.
When low frequency VPCs types are removed,
this proportion increases (89.48%). Moreover,
when the VPCs used only by the adults are con-
sidered, most of these (93.44%) occur with fre-
quency lower than 5. This suggests that children
tend to follow quite closely the combinations em-
ployed by adults, and the lower frequency cases
may not yet be incorporated in their active vocab-
ulary.
In terms of the distance between verb and par-
ticle, there is a strong preference in the data for
joint combinations for both children and adults,
table 7. For the split cases, the majority contains
only one word between the verb and the particle.
Children in particular display a slight disprefer-
ence for longer distances between verbs and parti-
cles, and over 97% of VPCs have at most 2 words
between them.
Distance Children Set Adults Set
0 65.13% 64.14%
1 23.48% 22.15%
2 9.33% 10.90%
3 1.65% 2.15%
4 0.29% 0.47%
5 0.09% 0.16%
Table 7: Distance between verb and particle
5 Conclusions and future work
In this paper we presented an investigation of
VPCs in child-produced and child-directed sen-
tences in English to determine whether potential
complexities in the nature of these combinations
48
Children Adult Children ?Adult Children Adult
VPCs VPCs VPCs only VPCs only VPCs
VPCs 1579 2468 1142 437 1243
Verb in VPCs 561 884 401 160 483
Particle in VPCs 28 35 24 4 9
VPCs ? 5 504 766 451 53 278
Verb in VPCs ? 5 207 282 183 24 99
Particle in VPCs ? 5 18 20 17 1 3
Table 6: Number of VPC, Verb and Particle types by group, common usages
are reflected in their reduced usage by children.
The combination of these results shows that, de-
spite any additional difficulties, VPCs are as much
a feature in children?s data as in adults?. Children
follow very closely adult usage in terms of the
types and are sensitive to their frequencies, dis-
playing similar distributions to adults. They also
seem to use them in a similar manner in terms of
particle placement. Therefore no correction for
VPC complexity was found in this data.
Despite these striking similarities in many of
the distributions, there are still some discrepan-
cies between these two groups. In particular in the
VPC ranks, children present a more uniform dis-
tribution for higher frequency VPCs when com-
pared to adults. Moreover, there is a modest but
significant dispreference for longer distances be-
tween verb and particle for children. Whether
these reflect different strategies or efficiency con-
siderations deserves to be further investigated.
Acknowledgements
This research was partly supported by CNPq
Projects 551964/2011-1, 202007/2010-3,
305256/2008-4 and 309569/2009-5.
References
Timothy Baldwin. 2005. Deep lexical acquisition
of verb-particle constructions. Computer Speech &
Language Special issue on MWEs, 19(4):398?414.
Timothy Baldwin. 2008. A resource for evaluating
the deep lexical acquisition of english verb-particle
constructions. In Proceedings of the LREC Work-
shop Towards a Shared Task for Multiword Expres-
sions (MWE 2008), pages 1?2, Marrakech, Mo-
rocco, June.
Dwight Bolinger. 1971. The phrasal verb in English.
Harvard University Press, Harvard, USA.
L. S. Boynton-Hauerwas. 1998. The role of general
all purpose verbs in language acquisition: A com-
parison of children with specific language impair-
ments and their language-matched peers. 59.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on
the PARC depbank. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL 2006),
pages 41?48, Sidney, Australia, July. Association
for Computational Linguistics.
Nicoleta Calzolari, Charles Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine Macleod,
and Antonio Zampolli. 2002. Towards best prac-
tice for multiword expressions in computational
lexicons. In Third International Conference on
Language Resources and Evaluation (LREC 2002),
pages 1934?1940, Las Palmas, Canary Islands,
Spain. European Language Resources Association.
John Carroll and Claire Grover. 1989. The derivation
of a large computational lexicon of English from
LDOCE. In B. Boguraev and E. Briscoe, editors,
Computational Lexicography for Natural Language
Processing. Longman.
M. Coltheart. 1981. The MRC psycholinguistic
database. Quarterly Journal of Experimental Psy-
chology, 33A:497?505.
Holger Diessel and Michael Tomasello. 2005. Particle
placement in early child language : A multifactorial
analysis. Corpus Linguistics and Linguistic Theory,
1(1):89?112.
Charles J. Fillmore, Paul Kay, and Mary C. O?Connor.
1988. Regularity and idiomaticity in grammatical
constructions: The case of Let Alone. Language,
64(3):510?538.
Charles Fillmore. 2003. Multiword expressions: An
extremist approach. Presented at Collocations and
idioms 2003: linguistic, computational, and psy-
cholinguistic perspectives.
Bruce Fraser. 1976. The Verb-Particle Combination
in English. Academic Press, New York, USA.
49
Adele E. Goldberg, 1999a. The Emergence of Lan-
guage, chapter Emergence of the semantics of
argument structure constructions, pages 197?212.
Carnegie Mellon Symposia on Cognition Series.
Adele E. Goldberg. 1999b. The emergence of the
semantics of argument structure constructions. In
B. MacWhinney, editor, Emergence of language.
Lawrence Erlbaum Associates, Hillsdale, NJ.
Stefan Gries. 2002. The influence of processing on
syntactic variation: Particle placement in english.
In Nicole Dehe?, Ray Jackendoff, Andrew McIn-
tyre, and Silke Urban, editors, Verb-Particle Ex-
plorations, pages 269?288. New York: Mouton de
Gruyter.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?559.
C. R. Juhasz and B. Grela. 2008. Verb particle errors
in preschool children with specific language impair-
ment. Contemporary Issues in Communication Sci-
ence & Disorders, 35:76?83.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago, USA.
Barbara Lohse, John A Hawkins, and Thomas Wa-
sow. 2004. Domain minimization in english verb-
particle constructions. Language, 80(2):238?261.
Catherine Macleod and Ralph Grishman. 1998.
COMLEX syntax reference manual, Proteus
Project.
B. MacWhinney. 1995. The CHILDES project: tools
for analyzing talk. Hillsdale, NJ: Lawrence Erl-
baum Associates, second edition.
Rosamund E. Moon. 1998. Fixed Expressions and
Idioms in English: A Corpus-based Approach. Ox-
ford University Press.
Darren Pearce. 2002. A comparative evaluation of
collocation extraction techniques. In Third Inter-
national Conference on Language Resources and
Evaluation (LREC 2002), Las Palmas, Canary Is-
lands, Spain. European Language Resources Asso-
ciation.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for mul-
tiword expression identification. In Proceedings of
the Seventh International Conference on Language
Resources and Evaluation (LREC 2010), Malta,
May. European Language Resources Association.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for NLP.
In Proceedings of the 3rd International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing-2002), volume 2276/2010 of
Lecture Notes in Computer Science, pages 1?15,
Mexico City, Mexico, February. Springer.
K. Sagae, E. Davis, A. Lavie, B. MacWhinney, and
S. Wintner. 2010. Morphosyntactic annotation of
CHILDES transcripts. Journal of Child Language,
37(03):705?729.
J.H. Sawyer. 1999. Verb adverb and verb particle
constructions: their syntax and acquisition. s.n.
Joan H. Sawyer. 2000. Comments on clayton m. dar-
win and loretta s. gray?s ?going after the phrasal
verb: An alternative approach to classification?. a
reader reacts. TESOL Quarterly, 34(1):151?159.
Anna Siyanova and Norbert Schmitt. 2007. Na-
tive and nonnative use of multi-word vs. one-word
verbs. International Review of Applied Linguistics,
45:109139.
Anna Siyanova and Norbert Schmitt. 2008. L2 learner
production and processing of collocation: A multi-
study perspective. Canadian Modern Language Re-
view, 64(3):429458.
Aline Villavicencio, Beracah Yankama, Robert
Berwick, and Marco Idiart. 2012. A large scale
annotated child language construction database. In
Proceedings of the 8th LREC, Istanbul, Turkey.
Alison Wray. 2002. Formulaic Language and the Lex-
icon. Cambridge University Press, Cambridge, UK.
Alison Wray. 2009. Formulaic language in learn-
ers and native speakers. Language Teaching,
32(04):213?231.
50
Proceedings of the 2012 Student Research Workshop, pages 1?6,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Broad Evaluation of Techniques for Automatic
Acquisition of Multiword Expressions
Carlos Ramisch?, ?, Vitor De Araujo?, Aline Villavicencio?
?Federal University of Rio Grande do Sul (Brazil)
? GETALP ? LIG, University of Grenoble (France)
{ceramisch, vbuaraujo, avillavicencio}@inf.ufrgs.br
Abstract
Several approaches have been proposed for the au-
tomatic acquisition of multiword expressions from
corpora. However, there is no agreement about
which of them presents the best cost-benefit ratio, as
they have been evaluated on distinct datasets and/or
languages. To address this issue, we investigate
these techniques analysing the following dimen-
sions: expression type (compound nouns, phrasal
verbs), language (English, French) and corpus size.
Results show that these techniques tend to extract
similar candidate lists with high recall (? 80%) for
nominals and high precision (? 70%) for verbals.
The use of association measures for candidate filter-
ing is useful but some of them are more onerous and
not significantly better than raw counts. We finish
with an evaluation of flexibility and an indication of
which technique is recommended for each language-
type-size context.
1 Introduction
Taking into account multiword expressions (MWEs) is
important to confer naturalness to the output of NLP sys-
tems. An MT system, for instance, needs to be aware of
idiomatic expressions like raining cats and dogs to avoid
literal translations.1 Likewise, a parser needs to deal with
verb-particle expressions like take off from Paris and with
light verb constructions like take a walk along the river
in order to avoid PP-attachment errors.
Even though the last decade has seen considerable re-
search in the automatic acquisition of MWEs, both in
theoretical and in computational linguistics, to date there
are few NLP applications integrating explicit MWE treat-
ment. This may be partly explained by the complexity of
MWEs: as they are heterogeneous and flexible, there is
no unique push-button approach to identify all types of
MWEs in all languages (Sag et al, 2002). Existing ap-
proaches are either generic but present relatively low pre-
1The equivalent expressions in French would be raining ropes, in
German raining young dogs, in Portuguese raining Swiss knives, etc.
cision or they require a large amount of language-specific
resources to yield good results.
The goal of this paper is to evaluate approaches for the
automatic acquisition of MWEs from corpora (?2), exam-
ining as parameters of the experimental context the lan-
guage (English and French), type of target MWE (verbal
and nominal) and size of corpus (small, medium, large).
We focus on 4 approaches2 and the experimental setup is
presented in ?3. In ?4 we evaluate the following acqui-
sition dimensions: quality of extracted candidates and of
association measures, use of computational resources and
flexibility. Thus, this research presents a comparative in-
vestigation of available approaches and indicates the best
cost-benefit ratio in a given context (language, type, cor-
pus size), pointing out current limitations and suggesting
future avenues of research for the field.
2 MWE Acquisition Approaches
Efforts for the evaluation of MWE acquisition approaches
usually focus on a single technique or compare the qual-
ity of association measures (AMs) used to rank a fixed
annotated list of MWEs. For instance, Evert and Krenn
(2005) and Seretan (2008) specifically evaluate and anal-
yse the lexical AMs used in MWE extraction on small
samples of bigram candidates. Pearce (2002), systemat-
ically evaluates a set of techniques for MWE extraction
on a small test set of English collocations. Analogously,
Pecina (2005) and Ramisch et al (2008) present exten-
sive comparisons of individual AMs and of their combi-
nation for MWE extraction in Czech, German and En-
glish. There have also been efforts for the extrinsic eval-
uation of MWEs for NLP applications such as informa-
tion retrieval (Xu et al, 2010), word sense disambigua-
tion (Finlayson and Kulkarni, 2011) and MT (Carpuat
and Diab, 2010).
One recent initiative aiming at more comparable eval-
2We consider only freely available, downloadable and openly docu-
mented tools. Therefore, outside the scope of this work are proprietary
tools, terminology and lexicography tools, translation aid tools and pub-
lished techniques for which no available implementation is provided.
1
uations of MWE acquisition approaches was in the form
of a shared task (Gr?goire et al, 2008). However, the
present work differs from the shared task in its aims. The
latter considered only the ranking of precompiled MWE
lists using AMs or linguistic filters at the end of extrac-
tion. However, for many languages and domains, no such
lists are available. In addition, the evaluation results pro-
duced for the shared task may be difficult to generalise,
as some of the evaluations prioritized the precision of the
techniques without considering the recall or the novelty
of the extracted MWEs. To date little has been said about
the practical concerns involving MWE acquisition, like
computational resources, flexibility or availability. With
this work, we hope to help filling this gap by performing
a broad evaluation of the acquisition process as a whole,
considering many different parameters.
We focus on 4 approaches for MWE acquisition from
corpora, which follow the general trend in the area of us-
ing shallow linguistic (lemmas, POS, stopwords) and/or
statistical (counts, AMs) information to distinguishing
ordinary sequences (e.g. yellow dress, go to a concert)
from MWEs (e.g. black box, go by a name). In addition
to the brief description below, Section 4.4 underlines the
main differences between the approaches.
1. LocalMaxs3 extracts MWEs by generating all pos-
sible n-grams from a sentence and then filtering
them based on the local maxima of the AM?s dis-
tribution (Silva and Lopes, 1999). It is based
purely on word counts and is completely language
independent, but it is not possible to directly in-
tegrate linguistic information in order to target a
specific type of construction.4 The evaluation
includes both LocalMaxs Strict which prioritizes
high precision (henceforth LocMax-S) and Local-
Maxs Relaxed which focuses on high recall (hence-
forth LocMax-R). A variation of the original algo-
rithm, SENTA, has been proposed to deal with non-
contiguous expressions (da Silva et al, 1999). How-
ever, it is computationally costly5 and there is no
freely available implementation.
2. MWE toolkit6 (mwetk) is an environment for
type and language-independent MWE acquisition,
integrating linguistic and frequency information
(Ramisch et al, 2010). It generates a targeted list
of MWE candidates extracted and filtered according
to user-defined criteria like POS sequences and a set
3http://hlt.di.fct.unl.pt/luis/multiwords/
index.html
4Although this can be simulated by concatenating words and POS
tags together in order to form a token.
5It is based on the calculation of all possible n-grams in a sen-
tence, which explode in number when going from contiguous to non-
contiguous n-grams.
6http://mwetoolkit.sourceforge.net
Small Medium Large
# sentences 5,000 50,000 500,000
# en words 133,859 1,355,482 13,164,654
# fr words 145,888 1,483,428 14,584,617
Table 1: Number of sentences and of words of each fragment of
the Europarl corpus in fr and in en.
of statistical AMs. It is an integrated framework for
MWE treatment, providing from corpus preprocess-
ing facilities to the automatic evaluation of the re-
sulting list with respect to a reference. Its input is
a corpus annotated with POS, lemmas and depen-
dency syntax, or if these are not available, raw text.
3. Ngram Statistics Package7 (NSP) is a traditional
approach for the statistical analysis of n-grams in
texts (Pedersen et al, 2011). It provides tools for
counting n-grams and calculating AMs, where an n-
gram is a sequence of n words occurring either con-
tiguously or within a window of w words in a sen-
tence. While most of the measures are only appli-
cable to bigrams, some of them are also extended to
trigrams and 4-grams. The set of available AMs in-
cludes robust and theoretically sound measures such
as log-likelihood and Fischer?s exact test. Although
there is no direct support to linguistic information
such as POS, it is possible to simulate them to some
extent using the same workaround as for LocMax.
4. UCS toolkit8 provides a large set of sophisticated
AMs. It focuses on high accuracy calculations for
bigram AMs, but unlike the other approaches, it
starts from a list of candidates and their respec-
tive frequencies, relying on external tools for corpus
preprocessing and candidate extraction. Therefore,
questions concerning contiguous n-grams and sup-
port of linguistic filters are not dealt with by UCS. In
our experiments, we will use the list of candidates
generated by mwetk as input for UCS.
As the focus of this work is on MWE acquisition (iden-
tification and extraction), other tasks related to MWE
treatment, namely interpretation, classification and appli-
cations (Anastasiou et al, 2009), are not considered in
this paper. This is the case, for instance, of approaches
for dictionary-based in-context MWE token identification
requiring an initial dictionary of valid MWEs, like jMWE
(Kulkarni and Finlayson, 2011).
3 Experimental Setup
For comparative purposes, we investigate the acquisition
of MWEs in two languages, English (en) and French
7http://search.cpan.org/dist/Text-NSP
8http://www.collocations.de/software.html
2
(fr), analysing nominal and verbal expressions in en and
nominal in fr,9 obtained with the following rules:
? Nominal expressions en: a noun preceded by a se-
quence of one or more nouns or adjectives, e.g. Eu-
ropean Union, clock radio, clown anemone fish.
? Nominal expressions fr: a noun followed by either
an adjective or a prepositional complement (with the
prepositions de, ? and en) followed by an option-
ally determined noun, e.g. algue verte, ali?nation de
bien, allergie ? la poussi?re.
? Verbal expressions en: verb-particle constructions
formed by a verb (except be and have) followed by
a prepositional particle10 not further than 5 words
after it, e.g. give up, switch the old computer off.
To test the influence of corpus size on performance,
three fragments of the en and fr parts of the Eu-
roparl corpus v311 were used as test corpora: (S)mall,
(M)edium and (L)arge, summarised in Table 1.
The extracted MWEs were automatically evaluated
against the following gold standards: WordNet 3, the
Cambridge Dictionary of Phrasal Verbs, and the VPC
(Baldwin, 2008) and CN (Kim and Baldwin, 2008)
datasets 12 for en; the Lexique-Grammaire13 for fr. The
total number of entries is listed below, along with the
number of entries occurring at least twice in each cor-
pus (in parentheses), which was the denominator used to
calculate recall in ? 4.1:
? Nominal expressions en: 59,683 entries (S: 122, M:
764, L: 2,710);
? Nominal expressions fr: 69,118 entries (S: 220, M:
1,406, L: 4,747);
? Verbal expressions en: 1,846 entries (S: 699, M:
1,846, L: 1,846).
4 Evaluation Results
The evaluation of MWE acquisition is an open problem.
While classical measures like precision and recall assume
that a complete (or at least broad-coverage) gold standard
exists, manual annotation of top-n candidates and mean
average precision (MAP) are labour-intensive even when
applied to a small sample, emphasizing precision regard-
less of the number of acquired new expressions. As ap-
proaches differ in the way they allow the description of
extraction criteria, we evaluate candidate extraction sep-
arately from AMs.
9As fr does not present many verb-particle constructions and due
to the lack of availability of resource for other types of fr verbal ex-
pressions (e.g. light verb constructions), only nominal expressions are
considered.
10up, off, down, back, away, in, on.
11http://www.statmt.org/europarl/
12The latter are available from http://multiword.sf.net/
13http://infolingu.univ-mlv.fr/
LocM
ax-S
LocM
ax-R mwe
tk NSP UCS
LocM
ax-S
LocM
ax-R mwe
tk NSP UCS
LocM
ax-S
LocM
ax-R mwe
tk NSP UCS
en-noun                     fr-noun                     en-verb
0%10%
20%30%
40%50%
60%70%
80%90%
100%
Precision Recall F-measure
Figure 1: Quality of candidates extracted from medium corpus,
comparison across languages/MWE types.
4.1 Extracted Candidates
We consider as MWE candidates the initial set of se-
quences before any AM is applied. Candidate extraction
is performed through the application of patterns describ-
ing the target MWEs in terms of POS sequences, as de-
scribed in ? 3. To minimise potential cases of noise, can-
didates occurring only once in the corpus were discarded.
We compare the quality of these candidates in terms of
(P)recision, (R)ecall and (F)-measure using the gold stan-
dard references described in ? 3. These measures are un-
derestimations as they assume that candidates not in the
gold standard are false MWEs, whereas they may simply
be absent due to coverage limitations.
The quality of candidates extracted from the medium-
size corpus (M) varies across MWE types/languages, as
shown in Figure 1. The candidates for UCS are obtained
by keeping only the bigrams in the candidate list returned
by the mwetk. For nominal MWEs, the approaches have
similar patterns of performance in the two languages,
with high recall and low precision yielding an F-measure
of around 10 to 15%. The variation between en and fr
can be partly explained by the differences in size of the
gold standards for each of these languages. Further re-
search would be needed to determine to what degree the
characteristics of these languages and the set of extraction
patterns influence these results. For verbal expressions,
LocMax has high precision (around 70%) but low recall
while the other approaches have more balanced P and R
values around 20%. This is partly due to the need for
simulating POS filters for extraction of verbal MWE can-
didates with LocMax. The filter consists of keeping only
contiguous n-grams in which the first and the last words
matched verb+particle pattern and removing intervening
words.
The techniques differ in terms of extraction strategy:
(i) mwetk and NSP allow the definition of linguistic fil-
ters while LocMax only allows the application of grep-
3
S M L
LocMax-S
P 7.53% 6.18% 4.50%
R 42.62% 38.48% 37.42%
LocMax-R
P 7.46% 6.02% ?
R 42.62% 38.48% ?
P-mwetk
P 6.50% 4.40% 2.35%
R 83.61% 86.78% 89.23%
NSP
P 6.61% 4.46% 2.48%
R 83.61% 85.73% 89.41%
UCS
P 6.96% 4.91% 2.77%
R 96.19% 95.65% 96.88%
Table 2: (P)recision and (R)ecall of en nominal candidates,
comparison across corpus sizes (S)mall, (M)edium and (L)arge.
like filters after extraction; (ii) there is no preliminary fil-
tering in mwetk and NSP, they simply return all candi-
dates matching a pattern, while LocMax filters the candi-
dates based on the local maxima criterion; (iii) LocMax
only extracts contiguous candidates while the others al-
low discontiguous candidates. The way mwetk and NSP
extract discontiguous candidates differs: the former ex-
tracts all verbs with particles no further than 5 positions to
the right. NSP extracts bigrams in a window of 5 words,
and then filters the list keeping only those in which the
first word is a verb and that contain a particle. However,
the results are similar, with slightly better values for NSP.
The evaluation of en nominal candidates according to
corpus size is shown in Table 2.14 For all approaches,
precision decreases when the corpus size increases as
more noise is returned, while recall increases for all ex-
cept LocMax. This may be due to the latter ignoring
smaller n-grams when larger candidates containing them
become sufficiently frequent, as is the case when the cor-
pus increases. Table 3 shows that the candidates extracted
by LocMax are almost completely covered by the candi-
dates extracted by the other approaches. The relaxed ver-
sion extracts slighly more candidates, but still much less
than mwetk, NSP and UCS, which all extract a similar
set of candidates. In order to distinguish the performance
of the approaches, we need to analyse the AMs they use
to rank the candidates.
4.2 Association Measures
Traditionally, to evaluate an AM, the candidates are
ranked according to it and a threshold value is applied,
below which the candidates are discarded. However, if
we average the precision considering all true MWEs as
14It was not possible to evaluate LocMax-R on the large corpus as
the provided implementation did not support corpora of this magnitude.
L
o
c
M
a
x
-
S
L
o
c
M
a
x
-
R
m
w
e
t
k
N
S
P
U
C
S
To
ta
lv
er
bs
LocMax-S ? 124 124 122 124 124
LocMax-R 4747 ? 156 153 156 156
mwetk 4738 4862 ? 1565 1926 1926
NSP 4756 4879 14611 ? 1565 1629
UCS 4377 4364 13407 13045 ? 1926
Total nouns 4760 4884 15064 14682 13418
Table 3: Intersection of the candidate lists extracted from
medium corpus. Nominal candidates en in bottom left, verbal
candidates en in top right.
threshold points, we obtain the mean average precision
(MAP) of the measure without setting a hard threshold.
Table 4 presents the MAP values for the tested AMs15
applied to the candidates extracted from the large cor-
pus (L), where the larger the value, the better the perfor-
mance. We used as baseline the assignment of a random
score and the use of the raw frequency for the candidates.
Except for mwetk:t and mwetk:pmi, all MAP values
are significantly different from the two baselines, with a
two-tailed t test for difference of means assuming unequal
sample sizes and variances (p-value < 0.005).
The LocMax:glue AM performs best for all types
of MWEs, suggesting local maxima as a good generic
MWE indicator and glue as an efficient AM to generate
highly precise results (considering the difficulty of this
task). On the other hand this approach returns a small set
of candidates and this may be problematic depending on
the task (e.g. for building a wide-coverage lexicon). For
mwetk, the best overall AM is the Dice coefficient; the
other measures are not consistently better than the base-
line, or perform better for one MWE type than for the
other. The Poisson-Stirling (ps) measure performed quite
well, while the other two measures tested for NSP per-
formed below baseline for some cases. Finally, as we ex-
pected, the AMs applied by UCS perform all above base-
line and, for nominal MWEs, are comparable to the best
AM (e.g. Poisson.pv and local.MI). The MAP for verbal
expressions varies much for UCS (from 30% to 53% ), but
none of the measures comes close to the MAP of the glue
(87.06%). None of the approaches provides a straightfor-
ward method to choose or combine different AMs.
4.3 Computational resources
In the decision of which AM to adopt, factors like the de-
gree of MWE flexibility and computational performance
may be taken into account. For instance, the Dice coef-
ficient can be applied to any length of n-gram quite fast
15Due to length limitations, we cannot detail the calculation of the
evaluated AMs; please refer to the documentation of each approach,
cited in ? 2, for more details.
4
en noun fr noun en verb
Baseline
random 2.749 6.1072 17.2079
freq 4.7478 8.7946 22.7155
LocMax-S
glue 6.9901 12.9383 87.0614
mwetk
dice 5.7783 9.5419 46.3609
t-test 5.0907 8.6373 26.4185
pmi 2.7589 2.9173 53.5591
log-lik. 3.166 5.5176 45.8837
NSP
pmi 2.9902 7.6782 62.1689
ps 5.3985 12.3791 57.6238
tmi 2.108 4.8928 19.8009
UCS
z.score 6.1202 11.7657 46.8707
Poisson.pv 6.5858 12.8226 32.7737
MI 5.1465 9.3363 53.5591
relative.risk 5.0999 9.2919 46.6702
odds.ratio 5.0364 9.2104 50.2201
gmean 6.0101 11.524 45.6089
local.MI 6.4294 12.7779 29.9858
Table 4: Mean average precision of AMs in large corpus.
while more sophisticated measures like Poisson.pv can be
applied only to 2-grams and sometimes use much com-
putational resources. Even if one could argue that we can
be lenient towards a slow offline extraction process, the
extra waiting may not be worth a slight quality improve-
ment. Moreover, memory limitations are an issue if no
large computer clusters are available.
In Figure 2, we plotted in log-scale the time in sec-
onds used by each approach to extract nominal and ver-
bal expressions in en, using a dedicated 2.4GHz quad-
core Linux machine with 4Gb RAM. For nominal expres-
sions, time increases linearly with the size of the corpus,
whereas for verbal expressions it seems to increase faster
than the size of the corpus. UCS is the slowest approach
for both MWE types while NSP and LocMax-S are the
fastest. However, it is important to emphasize that NSP
consumed more than 3Gb memory to extract 4- and 5-
grams from the large corpus and LocMax-R could not
handle the large corpus at all. In theory, all techniques can
be applied to arbitrarily large corpora if we used a map-
reduce approach (e.g. NSP provides tools to split and join
the corpus). However, the goal of this evaluation is to dis-
cover the performance of the techniques with no manual
optimization. In this sense, mwetk seems to provide an
average trade-off between quality and resources used.
4.4 Flexibility
Table 5 summarises the characteristics of the approaches.
Among them, UCS does not extract candidates from cor-
pora but takes as input a list of bigrams and their counts.
S M LLocMax-xSRmwe
wee
weee
weeee
tSkm
-NPoU
-xmC
onux
 
mn-fm
cr-----
----mn
-noan voCb0%12voCb0%13k4m567289L2voCb0%12voCb0%13k4m567289L2
Figure 2: Time (seconds, log scale) to extract en nouns (bold
line) and verbs (dashed line) from corpora.
LocMax mwetk NSP UCS
Candidate extraction Yes Yes Yes No
N-grams with n > 2 Yes Yes Yes No
Discontiguous MWE No Yes Yes ?
Linguistic filter No Yes No No
Robust AMs No No Yes Yes
Large corpora Partly Yes Yes No
Availability Free Free Free Free
Table 5: Summary of tools for MWE acquisition.
While it only supports n-grams of size 2, NSP imple-
ments some of the AMs for 3 and 4-grams and mwetk
and LocMax have no constraint on the number of words.
LocMax extracts only contiguous MWEs while mwetk
allows the extraction of unrestrictedly distant words and
NSP allows the specification of a window of maximum w
ignored words between each two words of the candidate.
Only mwetk integrates linguistic filters on the lemma,
POS and syntactic annotation, but this was performed us-
ing external tools (sed/grep) for the other approaches with
similar results. The AMs implemented by LocMax and
mwetk are conceived for any size of n-gram and are thus
less statistically sound than the clearly designed measures
used by UCS and, to some extent, by NSP (Fisher test).
The large corpus used in our experiments was not sup-
ported by LocMax-R version, but LocMax-S has a ver-
sion that deals with large corpora, as well as mwetk and
NSP. Finally, all of these approaches are freely available
for download and documented on the web.
5 Conclusions and future work
We evaluated the automatic acquisition of MWEs from
corpora. The dimensions evaluated were type of
construction (for flexibility and contiguity), language
and corpus size. We evaluated two steps separately:
candidate extraction and filtering with AMs. Can-
didate lists are very similar, with approaches like
5
mwetk and NSP returning more candidates (they cover
most of the nominal MWEs in the corpus) but hav-
ing lower precision. LocMax-S presented a remark-
ably high precision for verbal expressions. However,
the choice of an AM may not only take into ac-
count its MAP but also its flexibility and the compu-
tational resources used. Our results suggest that the
approaches could be combined using machine learn-
ing (Pecina, 2005). The data used in our experi-
ments is available at http://www.inf.ufrgs.br/
~ceramisch/?page=downloads/mwecompare.
In the future, we would like to develop this evaluation
further by taking into account other characteristics such
as the domain and genre of the source corpus. Such eval-
uation would be useful to guide future research on spe-
cialised multiword terminology extraction, determining
differences with respect to generic MWE extraction. We
would also like to evaluate other MWE-related tasks (e.g.
classification, interpretation) and also dictionary-based
identification (Kulkarni and Finlayson, 2011) and bilin-
gual MWE acquisition (Carpuat and Diab, 2010). Fi-
nally, we believe that an application-based extrinsic eval-
uation involving manual validation of candidates would
ultimately demonstrate the usefulness of current MWE
acquisition techniques.
Acknowledgements
This work was partly funded by the CAMELEON project
(CAPES?COFECUB 707-11).
References
Dimitra Anastasiou, Chikara Hashimoto, Preslav Nakov, and
Su Nam Kim, editors. 2009. Proc. of the ACL Workshop on
MWEs: Identification, Interpretation, Disambiguation, Ap-
plications (MWE 2009), Suntec, Singapore, Aug. ACL.
Timothy Baldwin. 2008. A resource for evaluating the deep
lexical acquisition of english verb-particle constructions. In
Gr?goire et al (Gr?goire et al, 2008), pages 1?2.
Marine Carpuat and Mona Diab. 2010. Task-based evaluation
of multiword expressions: a pilot study in statistical machine
translation. In Proc. of HLT: The 2010 Annual Conf. of the
NAACL (NAACL 2003), pages 242?245, Los Angeles, Cali-
fornia, Jun. ACL.
Joaquim Ferreira da Silva, Ga?l Dias, Sylvie Guillor?, and Jos?
Gabriel Pereira Lopes. 1999. Using localmaxs algorithm for
the extraction of contiguous and non-contiguous multiword
lexical units. In Proceedings of the 9th Portuguese Confer-
ence on Artificial Intelligence: Progress in Artificial Intelli-
gence, EPIA ?99, pages 113?132, London, UK. Springer.
Stefan Evert and Brigitte Krenn. 2005. Using small random
samples for the manual evaluation of statistical association
measures. Comp. Speech & Lang. Special issue on MWEs,
19(4):450?466.
Mark Finlayson and Nidhi Kulkarni. 2011. Detecting multi-
word expressions improves word sense disambiguation. In
Kordoni et al (Kordoni et al, 2011), pages 20?24.
Nicole Gr?goire, Stefan Evert, and Brigitte Krenn, editors.
2008. Proc. of the LREC Workshop Towards a Shared Task
for MWEs (MWE 2008), Marrakech, Morocco, Jun.
Su Nam Kim and Timothy Baldwin. 2008. Standardised evalu-
ation of english noun compound interpretation. In Gr?goire
et al (Gr?goire et al, 2008), pages 39?42.
Valia Kordoni, Carlos Ramisch, and Aline Villavicencio, edi-
tors. 2011. Proc.of the ACL Workshop on MWEs: from Pars-
ing and Generation to the Real World (MWE 2011), Portland,
OR, USA, Jun. ACL.
Nidhi Kulkarni and Mark Finlayson. 2011. jMWE: A java
toolkit for detecting multi-word expressions. In Kordoni
et al (Kordoni et al, 2011), pages 122?124.
?ric Laporte, Preslav Nakov, Carlos Ramisch, and Aline Villav-
icencio, editors. 2010. Proc.of the COLING Workshop on
MWEs: from Theory to Applications (MWE 2010), Beijing,
China, Aug. ACL.
Darren Pearce. 2002. A comparative evaluation of collocation
extraction techniques. In Proc. of the Third LREC (LREC
2002), Las Palmas, Canary Islands, Spain, May. ELRA.
Pavel Pecina. 2005. An extensive empirical study of collo-
cation extraction methods. In Proc. of the ACL 2005 SRW,
pages 13?18, Ann Arbor, MI, USA, Jun. ACL.
Ted Pedersen, Satanjeev Banerjee, Bridget McInnes, Saiyam
Kohli, Mahesh Joshi, and Ying Liu. 2011. The ngram
statistics package (text::NSP) : A flexible tool for identify-
ing ngrams, collocations, and word associations. In Kordoni
et al (Kordoni et al, 2011), pages 131?133.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for the ex-
traction of multiword expressions. In Gr?goire et al (Gr?-
goire et al, 2008), pages 50?53.
Carlos Ramisch, Aline Villavicencio, and Christian Boitet.
2010. Multiword expressions in the wild? the mwetoolkit
comes in handy. In Yang Liu and Ting Liu, editors, Proc.
of the 23rd COLING (COLING 2010) ? Demonstrations,
pages 57?60, Beijing, China, Aug. The Coling 2010 Orga-
nizing Committee.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and
Dan Flickinger. 2002. Multiword expressions: A pain in the
neck for NLP. In Proc. of the 3rd CICLing (CICLing-2002),
volume 2276/2010 of LNCS, pages 1?15, Mexico City, Mex-
ico, Feb. Springer.
Violeta Seretan. 2008. Collocation extraction based on syn-
tactic parsing. Ph.D. thesis, University of Geneva, Geneva,
Switzerland.
Joaquim Silva and Gabriel Lopes. 1999. A local maxima
method and a fair dispersion normalization for extracting
multi-word units from corpora. In Proceedings of the Sixth
Meeting on Mathematics of Language (MOL6), pages 369?
381, Orlando, FL, USA, Jul.
Ying Xu, Randy Goebel, Christoph Ringlstetter, and Grzegorz
Kondrak. 2010. Application of the tightness continuum
measure to chinese information retrieval. In Laporte et al
(Laporte et al, 2010), pages 54?62.
6
Proceedings of the First Workshop on Multilingual Modeling, pages 25?31,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparable Corpus Based on Aligned Multilingual Ontologies
Roger Granada
PUCRS (Brazil)
roger.granada@acad.pucrs.br
Lucelene Lopes
PUCRS (Brazil)
lucelene.lopes@pucrs.br
Carlos Ramisch
University of Grenoble (France)
ceramisch@inf.ufrgs.br
Cassia Trojahn
University of Grenoble (France)
cassia.trojahn@inria.fr
Renata Vieira
PUCRS (Brazil)
renata.vieira@pucrs.br
Aline Villavicencio
UFRGS (Brazil)
alinev@gmail.com
Abstract
In this paper we present a methodology for
building comparable corpus, using multilin-
gual ontologies of a scpecific domain. This
resource can be exploited to foster research on
multilingual corpus-based ontology learning,
population and matching. The building re-
source process is exemplified by the construc-
tion of annotated comparable corpora in En-
glish, Portuguese, and French. The corpora,
from the conference organization domain, are
built using the multilingual ontology concept
labels as seeds for crawling relevant docu-
ments from the web through a search engine.
Using ontologies allows a better coverage of
the domain. The main goal of this paper is
to describe the design methodology followed
by the creation of the corpora. We present a
preliminary evaluation and discuss their char-
acteristics and potential applications.
1 Introduction
Ontological resources provide a symbolic model of
the concepts of a scientific, technical or general
domain (e.g. Chemistry, automotive industry, aca-
demic conferences), and of how these concepts are
related to one another. However, ontology creation
is labour intensive and error prone, and its mainte-
nance is crucial for ensuring the accuracy and util-
ity of a given resource. In multilingual contexts, it
is hard to keep the coherence among ontologies de-
scribed in different languages and to align them ac-
curately. These difficulties motivate the use of semi-
automatic approaches for cross-lingual ontology en-
richment and population, along with intensive reuse
and interoperability between ontologies. For that, it
is crucial to have domain-specific corpora available,
or the means of automatically gathering them.
Therefore, this paper describes an ontology-based
approach for the generation of multilingual compa-
rable corpora. We use a set of multilingual domain-
dependent ontologies, which cover different aspects
of the conference domain. These ontologies provide
the seeds for building the domain specific corpora
from the web. Using high-level background knowl-
edge expressed in concepts and relations, which are
represented as natural language descriptions in the
labels of the ontologies, allow focused web crawl-
ing with a semantic and contextual coverage of the
domain. This approach makes web crawling more
precise, which is crucial when exploiting the web as
a huge corpus.
Our motivation is the need of such resources
in tasks related to semi-automatic ontology cre-
ation and maintenance in multilingual domains.
We exemplify our methodology focusing on the
construction of three corpora, one in English,
one in Portuguese, and one in French. This
effort is done in the context of a larger re-
search project which aims at investigating meth-
ods for the construction of lexical resources, in-
tegrating multilingual lexica and ontologies, fo-
cusing on collaborative and automatic techniques
(http://cameleon.imag.fr/xwiki/bin/view/Main/).
In the next section, we present some relevant re-
lated work (?2). This is followed by a description
of the methodology used to build the corpora (?3).
Finally, the application example expressed by the
resulting corpora are evaluated (?4) and discussed
25
(?5). We conclude by outlining their future applica-
tions (? 6).
2 Related Work
Web as corpus (WAC) approaches have been suc-
cessfully adopted in many cases where data sparse-
ness plays a major limiting role, either in specific
linguistic constructions and words in a language
(e.g. compounds and multiword expressions), or for
less resourced languages in general1.
For instance, Grefenstette (1999) uses WAC for
machine translation of compounds from French into
English, Keller et al (2002) for adjective-noun,
noun-noun and verb-object bigram discovery, and
Kim and Nakov (2011) for compound interpretation.
Although a corpus derived from the web may con-
tain noise, the sheer size of data available should
compensate for that. Baroni and Ueyama (2006)
discuss in details the process of corpus construc-
tion from web pages for both generic and domain-
specific corpora. In particular, they focus on the
cleaning process applied to filter the crawled web
pages. Much of the methodology applied in our
work is similar to their proposed approach (see ?3).
Moreover, when access to parallel corpora is lim-
ited, comparable corpora can minimize data sparse-
ness, as discussed by Skadina et al (2010). They
create bilingual comparable corpora for a variety of
languages, including under-resourced ones, with 1
million words per language. This is used as ba-
sis for the definition of metrics for comparability of
texts. Forsyth and Sharoff (2011) compile compa-
rable corpora for terminological lexicon construc-
tion. An initial verification of monolingual compa-
rability is done by partitioning the crawled collec-
tion into groups. Those are further extended through
the identification of representative archetypal texts
to be used as seeds for finding documents of the
same type.
Comparable corpora is a very active research sub-
ject, being in the core of several European projects
(e.g. TTC2, Accurat3). Nonetheless, to date most of
1Kilgarriff (2007) warns about the dangers of statistics heav-
ily based on a search engine. However, since we use the down-
loaded texts of web pages instead of search engine count esti-
mators, this does not affect the results obtained in this work.
2www.ttc-project.eu
3www.accurat-project.eu
the research on comparable corpora seems to focus
on lexicographic tasks (Forsyth and Sharoff, 2011;
Sharoff, 2006), bilingual lexicon extraction (Morin
and Prochasson, 2011), and more generally on ma-
chine translation and related applications (Ion et al,
2011). Likewise, there is much to be gained from
the potential mutual benefits of comparable corpora
and ontology-related tasks.
Regarding multilingually aligned ontologies, very
few data sets have been made available for use in
the research community. Examples include the vlcr4
and the mldirectory5 datasets. The former con-
tains a reduced set of alignments between the the-
saurus of the Netherlands Institute for Sound and
Vision and two other resources, English WordNet
and DBpedia. The latter consists of a set of align-
ments between web site directories in English and
in Japanese. However, these data sets provide sub-
sets of bilingual alignments and are not fully pub-
licly available. The MultiFarm dataset6, a multilin-
gual version of the OntoFarm dataset (S?va?b et al,
2005), has been designed in order to overcome the
lack of multilingual aligned ontologies. MultiFarm
is composed of a set of seven ontologies that cover
the different aspects of the domain of organizing sci-
entific conferences. We have used this dataset as the
basis for generating our corpora.
3 Methodology
The main contribution of this paper is the proposal
of the methodology to build corpora. This sec-
tion describes the proposed methodology present-
ing our own corpus crawler, but also its application
to construct three corpora, in English, Portuguese,
and French. These corpora are constructed from the
MultiFarm dataset.
3.1 Tools and Resources
Instead of using an off-the-shelf web corpus tool
such as BootCaT (Baroni and Bernardini, 2004), we
implemented our own corpus crawler. This allowed
us to have more control on query and corpus con-
struction process. Even though our corpus construc-
4www.cs.vu.nl/?laurah/oaei/2009
5oaei.ontologymatching.org/2008/
mldirectory
6web.informatik.uni-mannheim.de/
multifarm
26
tion strategy is similar to the one implemented in
BootCaT, there are some significant practical issues
to take into account, such as:
? The predominance of multiword keywords;
? The use of the fixed keyword conference;
? The expert tuning of the cleaning process;
? The use of a long term support search AP[b].
Besides, BootCaT uses the Bing search API,
which will no longer work in 2012. As our work
is part of a long-term project, we preferred to use
Google?s search API as part of the University Re-
search Program.
The set of seed domain concepts comes from
the MultiFarm dataset. Seven ontologies from the
OntoFarm project (Table 1), together with the align-
ments between them, have been translated from En-
glish into eight languages (Chinese, Czech, Dutch,
French, German, Portuguese, Russian, and Span-
ish). As shown in Table 1, the ontologies differ
in numbers of classes, properties, and in their log-
ical expressivity. Overall, the ontologies have a high
variance with respect to structure and size and they
were based upon three types of resources:
? actual conferences and their web pages (type
?web?),
? actual software tools for conference organisa-
tion support (type ?tool?), and
? experience of people with personal participa-
tion in organisation of actual conferences (type
?insider?).
Currently, our comparable corpus generation ap-
proach focuses on a subset of languages, namely En-
glish (en), Portuguese (pt) and French (fr). The
labels of the ontology concepts, like conference and
call for papers, are used to generate queries and re-
trieve the pages in our corpus. In the current imple-
mentation, the structure and relational properties of
the ontologies were ignored. Concept labels were
our choice of seed keywords since we intended to
have comparable, heterogeneous and multilingual
domain resources. This means that we need a corpus
and an ontology referring to the same set of terms or
concepts. We want to ensure that the concept labels
Name Type C DP OP
Ekaw insider 74 0 33
Sofsem insider 60 18 46
Sigkdd web 49 11 17
Iasted web 140 3 38
ConfTool tool 38 23 13
Cmt tool 36 10 49
Edas tool 104 20 30
Table 1: Ontologies from the OntoFarm dataset in terms
of number of classes (C), datatype properties (DP) and
object properties (OP).
are present in the corresponding natural language,
textual sources. This combination of resources is es-
sential for our goals, which involve problems such as
ontology learning and enriching from corpus. Thus,
the original ontology can serve as a reference for
automatically extracted resources. Moreover, we
intend to use the corpus as an additional resource
for ontology (multilingual) matching, and again the
presence of the labels in the corpus is of great rele-
vance.
3.2 Crawling and Preprocessing
In each language, a concept label that occurs in
two or more ontologies provides a seed keyword
for query construction. This results in 49 en key-
words, 54 pt keywords and 43 fr keywords. Be-
cause many of our keywords are formed by more
than one word (average length of keywords is re-
spectively 1.42, 1.81 and 1.91 words), we combine
three keywords regardless of their sizes to form a
query. The first keyword is static, and corresponds
to the word conference in each language. The query
set is thus formed by permuting keywords two by
two and concatenating the static keyword to them
(e.g. conference reviewer program committee). This
results in 1 ? 48 ? 47 = 2, 256 en queries, 2,756
pt queries and 1,892 fr queries. Average query
length is 3.83 words for en, 4.62 words for pt and
4.91 words for fr. This methodology is in line with
the work of Sharoff (2006), who suggests to build
queries by combining 4 keywords and downloading
the top 10 URLs returned for each query.
The top 10 results returned by Google?s search
27
API7 are downloaded and cleaned. Duplicate URLs
are automatically removed. We did not filter out
URLs coming from social networks or Wikipedia
pages because they are not frequent in the corpus.
Results in formats other than html pages (like .doc
and .pdf documents) are ignored. The first clean-
ing step is the extraction of raw text from the html
pages. In some cases, the page must be discarded for
containing malformed html which our page cleaner
is not able to parse. In the future, we intend to im-
prove the robustness of the HTML parser.
3.3 Filtering and Linguistic Annotation
After being downloaded and converted to raw text,
each page undergoes a two-step processing. In the
first step, markup characters as interpunctuation,
quotation marks, etc. are removed leaving only let-
ters, numbers and punctuation. Further heuristics
are applied to remove very short sentences (less than
3 words), email addresses, URLs and dates, since
the main purpose of the corpus is related to concept,
instance and relations extraction. Finally, heuristics
to filter out page menus and footnotes are included,
leaving only the text of the body of the page. The
raw version of the text still contains those expres-
sions in case they are needed for other purposes.
In the second step, the text undergoes linguistic
annotation, where sentences are automatically lem-
matized, POS tagged and parsed. Three well-known
parsers were employed: Stanford parser (Klein and
Manning, 2003) for texts in English, PALAVRAS
(Bick, 2000) for texts in Portuguese, and Berkeley
parser (Petrov et al, 2006) for texts in French.
4 Evaluation
The characteristics of the resulting corpora are sum-
marized in tables 2 and 3. Column D of table 2
shows that the number of documents retrieved is
much higher in en than in pt and fr, and this is
not proportional to the number of queries (Q). In-
deed, if we look in table 3 at the average ratio of
documents retrieved per query (D/Q), the en queries
return much more documents than queries in other
languages. This indicates that the search engine re-
turns more distinct results in en and more duplicate
URLs in fr and in pt. The high discrepancy in
7research.google.com/university/search
Q D W token W type
en 2,256 10,127 15,852,650 459,501
pt 2,756 5,342 12,876,344 405,623
fr 1,892 5,154 9,482,156 362,548
Table 2: Raw corpus dimensions: number of queries (Q),
documents (D), and words (W).
D/Q S/D W/S TTR
en 4.49 110.59 14.15 2.90%
pt 1.94 120.08 20.07 3.15%
fr 2.72 115.63 15.91 3.82%
Table 3: Raw corpus statistics: average documents per
query (D/Q), sentences per document (S/D), words per
sentence (W/S) and type-token ration (TTR).
the number of documents has a direct impact in the
size of the corpus in each language. However, this
is counterbalanced by the average longer documents
(S/D) and longer sentences (W/S) in pt and fr with
respect to en. The raw corpus contains from 9.48
million words in fr, 12.88 million words in pt to
15.85 million words in en, constituting a large re-
source for research on ontology-related tasks.
A preliminary semi-automated analysis of the cor-
pus quality was made by extracting the top-100 most
frequent n-grams and unigrams for each language.
Using the parsed corpora, the extraction of the top-
100 most frequent n-grams for each language fo-
cused on the most frequent noun phrases composed
by at least two words. The lists with the top-100
most frequent unigrams was generated by extract-
ing the most frequent nouns contained in the parsed
corpus for each language. Four annotators manually
judged the semantic adherence of these lists to the
conference domain.
We are aware that semantic adherence is a vague
notion, and not a straightforward binary classifica-
tion problem. However, such a vague notion was
considered useful at this point of the research, which
is ongoing work, to give us an initial indication
of the quality of the resulting corpus. Examples
of what we consider adherent terms are appel a?
communication (call for papers), conference pro-
gram and texto completo (complete text), examples
28
# of adherent terms
Lower Upper
en words 46 85
en n-grams 57 94
fr words 21 69
fr n-grams 24 45
pt words 32 70
pt n-grams 11 45
Table 4: Number of words and n-grams judged as seman-
tically adherent to the domain.
of nonadherent terms extracted from the corpus were
produits chimiques (chemical products), following
case, projeto de lei (law project). In the three lan-
guages, the annotation of terms included misparsed
and mistagged words (ad hoc), places and dates typ-
ical of the genre (but not necessarily of the domain),
general-purpose terms frequent in conference web-
sites (email, website) and person names.
Table 4 shows the results of the annotation. The
lower bound considers an n-gram as semantically
adherent if all the judges agree on it. The upper
bound, on the other hand, considers as relevant n-
grams all those for which at least one of the four
judges rated it as relevant. As a result of our anal-
ysis, we found indications that the English corpus
was more adherent, followed by French and Por-
tuguese. This can be explained by the fact that
the amount of internet content is larger for English,
and that the number of international conferences
is higher than national conferences adopting Por-
tuguese and French as their official languages. We
considered the adherence of Portuguese and French
corpora rather low. There are indications that mate-
rial related to political meetings, law and public in-
stitutions was also retrieved on the basis of the seed
terms.
The next step in our evaluation is verifying its
comparable nature, by counting the proportion of
translatable words. Thus, we will use existing bilin-
gual dictionaries and measure the rank correlation of
equivalent words in each language pair.
5 Discussion
The first version of the corpus containing the to-
tality of the raw pages, the tools used to process
them, and a sample of 1,000 annotated texts for
each language are freely available for download at
the CAMELEON project website8. For the raw
files, each page is represented by an URL, a lan-
guage code, a title, a snippet and the text of the
page segmented into paragraphs, as in the original
HTML file. A companion log file contains informa-
tion about the download dates and queries used to re-
trieve each URL. The processed files contain the fil-
tered and parsed texts. The annotation format varies
for each language according to the parser used. The
final version of this resource will be available with
the totality of pages parsed.
Since the texts were extracted from web pages,
there is room for improvement concerning some im-
portant issues in effective corpus cleaning. Some of
these issues were dealt with as described in the ? 3,
but other issues are still open and are good candi-
dates for future refinements. Examples already fore-
seen are the removal of foreign words, special char-
acters, and usual web page expressions like ?site un-
der construction?, ?follow us on twitter?, and ?click
here to download?. However, the relevance of some
of these issues depends on the target application. For
some domains, foreign expressions may be genuine
part of the vocabulary (e.g. parking or weekend in
colloquial French and deadline in Portuguese), and
as such, should be kept, while for other domains
these expressions may need to be removed, since
they do not really belong to the domain. Therefore,
the decision of whether to implement these filters
or not, and to deal with truly multilingual texts, de-
pends on the target application.
Another aspect that was not taken into account in
this preliminary version is related to the use of the
relations between concepts in the ontologies to guide
the construction of the queries. Exploiting the con-
textual and semantic information expressed in these
relations may have an impact in the set of retrieved
documents and will be exploited in future versions
of the corpus.
6 Conclusions and Future Work
This paper has described an ontology-based ap-
proach for the generation of a multilingual compara-
8cameleon.imag.fr/xwiki/bin/view/Main/
Resources
29
ble corpus in English, Portuguese and French. The
corpus constructed and discussed here is an impor-
tant resource for ontology learning research, freely
available to the research community. The work on
term extraction that we are doing for the initial as-
sessment of the corpus is indeed the initial step to-
wards more ambitious research goals such as multi-
lingual ontology learning and matching in the con-
text of our long-term research project.
The initial ontologies (originally built by hand)
and resulting corpora can serve as a reference, a re-
search resource, for information extraction tasks re-
lated to ontology learning (term extraction, concept
formation, instantiation, etc). The resource also al-
lows the investigation of ontology enriching tech-
niques, due to dynamic and open-ended nature of
language, by which relevant terms found in the cor-
pus may not be part of the original ontology. We can
also assess the frequencies (relevance) of the labels
of the ontology element with respect to the corpus,
thus assessing the quality of the ontology itself. An-
other research that can be developed on the basis of
our resource is to evaluate the usefulness of a corpus
in the improvement of existing multilingual ontol-
ogy matching techniques9.
Regarding to our own crawler implementation,
we plan to work on its evaluation by using other
web crawlers, as BootCaT, and compare both ap-
proaches, specially on what concerns the use of on-
tologies.
From the point of view of NLP, several techniques
can be compared showing the impact of adopting
different tools in terms of depth of analysis, from
POS tagging to parsing. This is also an important re-
source for comparable corpora research, which can
be exploited for other tasks such as natural language
translation and ontology-based translation. So far
this corpus contains English, Portuguese and French
versions, but the ontology data set includes 8 lan-
guages, to which this corpus may be extended in the
future.
9An advantage of this resource is that the Multilingual Onto-
Farm is to be included in the OAEI (Ontology Alignment Eval-
uation Initiative) evaluation campaign.
References
Marco Baroni and Silvia Bernardini. 2004. BootcaT:
Bootstrapping corpora and terms from the web. In
Proc. of the Fourth LREC (LREC 2004), Lisbon, Por-
tugal, May. ELRA.
Marco Baroni and Motoko Ueyama. 2006. Building
general- and special-purpose corpora by web crawling.
In Proceedings of the 13th NIJL International Sympo-
sium on Language Corpora: Their Compilation and
Application, pages 31?40.
Eckhard Bick. 2000. The parsing system Palavras.
Aarhus University Press.
Richard Forsyth and Serge Sharoff. 2011. From crawled
collections to comparable corpora: an approach based
on automatic archetype identification. In Proc. of Cor-
pus Linguistics Conference, Birmingham, UK.
Gregory Grefenstette. 1999. The World Wide Web as a
resource for example-based machine translation tasks.
In Proc. of the Twenty-First Translating and the Com-
puter, London, UK, Nov. ASLIB.
Radu Ion, Alexandru Ceaus?u, and Elena Irimia. 2011.
An expectation maximization algorithm for textual
unit alignment. In Zweigenbaum et al (Zweigenbaum
et al, 2011), pages 128?135.
Frank Keller, Maria Lapata, and Olga Ourioupina. 2002.
Using the Web to overcome data sparseness. In Jan
Hajic? and Yuji Matsumoto, editors, Proc. of the 2002
EMNLP (EMNLP 2002), pages 230?237, Philadel-
phia, PA, USA, Jul. ACL.
Adam Kilgarriff. 2007. Googleology is bad science.
Comp. Ling., 33(1):147?151.
Su Nam Kim and Preslav Nakov. 2011. Large-scale noun
compound interpretation using bootstrapping and the
web as a corpus. In Proc. of the 2011 EMNLP
(EMNLP 2011), pages 648?658, Edinburgh, Scotland,
UK, Jul. ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proc. of the 41st ACL (ACL
2003), pages 423?430, Sapporo, Japan, Jul. ACL.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable corpora
enhanced with parallel corpora. In Zweigenbaum et al
(Zweigenbaum et al, 2011), pages 27?34.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st COLING
and 44th ACL (COLING/ACL 2006), pages 433?440,
Sidney, Australia, Jul. ACL.
Serge Sharoff, 2006. Creating general-purpose cor-
pora using automated search engine queries. Gedit,
Bologna, Italy.
30
Inguna Skadina, Ahmed Aker, Voula Giouli, Dan Tufis?,
Robert Gaizauskas, Madara Mieirina, and Nikos Mas-
tropavlos. 2010. A Collection of Comparable Cor-
pora for Under-resourced Languages. In Inguna Skad-
ina and Andrejs Vasiljevs, editors, Frontiers in Artifi-
cial Intelligence and Applications, volume 219, pages
161?168, Riga, Latvia, Oct. IOS Press.
Ondr?ej S?va?b, Vojte?ch Sva?tek, Petr Berka, Dus?an Rak,
and Petr Toma?s?ek. 2005. Ontofarm: Towards an ex-
perimental collection of parallel ontologies. In Poster
Track of ISWC 2005.
Pierre Zweigenbaum, Reinhard Rapp, and Serge Sharoff,
editors. 2011. Proc.of the 4th Workshop on Building
and Using Comparable Corpora: Comparable Cor-
pora and the Web (BUCC 2011), Portland, OR, USA,
Jun. ACL.
31
