BioNLP 2007: Biological, translational, and clinical language processing, pages 129?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Code Assignment to Medical Text
Koby Crammer and Mark Dredze and Kuzman Ganchev and Partha Pratim Talukdar
Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA
{crammer|mdredze|kuzman|partha}@seas.upenn.edu
Steven Carroll
Division of Oncology, The Children?s Hospital of Philadelphia, Philadelphia, PA
carroll@genome.chop.edu
Abstract
Code assignment is important for handling
large amounts of electronic medical data in
the modern hospital. However, only expert
annotators with extensive training can as-
sign codes. We present a system for the
assignment of ICD-9-CM clinical codes to
free text radiology reports. Our system as-
signs a code configuration, predicting one or
more codes for each document. We com-
bine three coding systems into a single learn-
ing system for higher accuracy. We compare
our system on a real world medical dataset
with both human annotators and other auto-
mated systems, achieving nearly the maxi-
mum score on the Computational Medicine
Center?s challenge.
1 Introduction
The modern hospital generates tremendous amounts
of data: medical records, lab reports, doctor notes,
and numerous other sources of information. As hos-
pitals move towards fully electronic record keeping,
the volume of this data only increases. While many
medical systems encourage the use of structured in-
formation, including assigning standardized codes,
most medical data, and often times the most impor-
tant information, is stored as unstructured text.
This daunting amount of medical text creates
exciting opportunities for applications of learning
methods, such as search, document classification,
data mining, information extraction, and relation ex-
traction (Shortliffe and Cimino, 2006). These ap-
plications have the potential for considerable bene-
fit to the medical community as they can leverage
information collected by hospitals and provide in-
centives for electronic record storage. Much of the
data generated by medical personnel is unused past
the clinical visit, often times because there is no way
to simply and quickly apply the wealth of informa-
tion. Medical NLP holds the promise of both greater
care for individual patients and enhanced knowledge
about health care.
In this work we explore the assignment of ICD-9-
CM codes to clinical reports. We focus on this prac-
tical problem since it is representative of the type
of task faced by medical personnel on a daily ba-
sis. Many hospitals organize and code documents
for later retrieval using different coding standards.
Often times, these standards are extremely complex
and only trained expert coders can properly perform
the task, making the process of coding documents
both expensive and unreliable since a coder must se-
lect from thousands of codes a small number for a
given report. An accurate automated system would
reduce costs, simplify the task for coders, and create
a greater consensus and standardization of hospital
data.
This paper addresses some of the challenges asso-
ciated with ICD-9-CM code assignment to clinical
free text, as well as general issues facing applica-
tions of NLP to medical text. We present our auto-
mated system for code assignment developed for the
Computational Medicine Center?s challenge. Our
approach uses several classification systems, each
with the goal of predicting the exact code configu-
ration for a medical report. We then use a learning
129
system to combine our predictions for superior per-
formance.
This paper is organized as follows. First, we ex-
plain our task and difficulties in detail. Next we de-
scribe our three automated systems and features. We
combine the three approaches to create a single su-
perior system. We evaluate our system on clinical
reports and show accuracy approaching human per-
formance and the challenge?s best score.
2 Task Overview
The health care system employs a large number of
categorization and classification systems to assist
data management for a variety of tasks, including
patient care, record storage and retrieval, statistical
analysis, insurance, and billing. One of these sys-
tems is the International Classification of Diseases,
Ninth Revision, Clinical Modification (ICD-9-CM)
which is the official system of assigning codes to di-
agnoses and procedures associated with hospital uti-
lization in the United States. 1 The coding system
is based on World Health Organization guidelines.
An ICD-9-CM code indicates a classification of a
disease, symptom, procedure, injury, or information
from the personal history. Codes are organized hier-
archically, where top level entries are general group-
ings (e.g. ?diseases of the respiratory system?) and
bottom level codes indicate specific symptoms or
diseases and their location (e.g. ?pneumonia in as-
pergillosis?). Each specific, low-level code consists
of 4 or 5 digits, with a decimal after the third. Higher
level codes typically include only 3 digits. Overall,
there are thousands of codes that cover a broad range
of medical conditions.
Codes are assigned to medical reports by doc-
tors, nurses and other trained experts based on com-
plex coding guidelines (National Center for Health
Statistics, 2006). A particular medical report can be
assigned any number of relevant codes. For exam-
ple, if a patient exhibits a cough, fever and wheez-
ing, all three codes should be assigned. In addi-
tion to finding appropriate codes for each condition,
complex rules guide code assignment. For exam-
ple, a diagnosis code should always be assigned if a
diagnosis is reached, a diagnosis code should never
1http://www.cdc.gov/nchs/about/otheract/
icd9/abticd9.htm
be assigned when the diagnosis is unclear, a symp-
tom should never be assigned when a diagnosis is
present, and the most specific code is preferred. This
means that codes that seem appropriate to a report
should be omitted in specific cases. For example,
a patient with hallucinations should be coded 780.1
(hallucinations) but for visual hallucinations, the
correct code is 368.16. The large number of codes
and complexity of assignment rules make this a diffi-
cult problem for humans (inter-annotator agreement
is low). Therefore, an automated system that sug-
gested or assigned codes could make medical data
more consistent.
These complexities make the problem difficult
for NLP systems. Consider the task as multi-class,
multi-label. For a given document, many codes may
seem appropriate but it may not be clear to the algo-
rithm how many to assign. Furthermore, the codes
are not independent and different labels can inter-
act to either increase or decrease the likelihood of
the other. Consider a report that says, ?patient re-
ports cough and fever.? The presence of the words
cough and fever indicate codes 786.2 (cough) and
780.6 (fever). However, if the report continues to
state that ?patient has pneumonia? then these codes
are dropped in favor of 486 (pneumonia). Further-
more, if the report then says ?verify clinically?, then
the diagnosis is uncertain and only codes 786.2 and
780.6 apply. Clearly, this is a challenging problem,
especially for an automated system.
2.1 Corpus
We built and evaluated our system in accordance
with the Computational Medicine Center?s (CMC)
2007 Medical Natural Language Processing Chal-
lenge.2 Since release of medical data must strictly
follow HIPAA standards, the challenge corpus un-
derwent extensive treatment for disambiguation,
anonymization, and careful scrubbing. A detailed
description of data preparation is found in Compu-
tational Medicine Center (2007). We describe the
corpus here to provide context for our task.
The training corpus is comprised of 978 radiolog-
ical reports taken from real medical records. A test
corpus contains 976 unlabeled documents. Radiol-
ogy reports have two text fields, clinical history and
2www.computationalmedicine.org/challenge
130
impression. The physician ordering the x-ray writes
the clinical history, which contains patient informa-
tion for the radiologist, including history and current
symptoms. Sometimes a guess as to the diagnosis
appears (?evaluate for asthma?). The descriptions
are sometimes whole sentences and other times sin-
gle words (?cough?). The radiologist writes the im-
pression to summarize his or her findings. It con-
tains a short analysis and often times a best guess as
to the diagnosis. At times this field is terse, (?pneu-
monia? or ?normal kidneys?) and at others it con-
tains an entire paragraph of text. Together, these two
fields are used to assign ICD-9-CM codes, which
justify a certain procedure, possibly for reimburse-
ment by the insurance company.
Only a small percentage of ICD-9-CM codes ap-
pear in the challenge. In total, the reports include 45
different codes arranged in 94 configurations (com-
binations). Some of these codes appear frequently,
while others are rare, appearing only a single time.
The test set is restricted so that each configuration
appears at least once in the training set, although
there is no further guarantee as to the test set?s distri-
bution over codes. Therefore, in addition to a large
number of codes, there is variability in the amount
of data for each code. Four codes have over 100
examples each and 24 codes have 10 or fewer doc-
uments, with 10 of these codes having only a single
document.
Since code annotation is a difficult task, each doc-
ument in the corpus was evaluated by three expert
annotators. A gold annotation was created by tak-
ing the majority of the annotators; if two of the three
annotators provided a code, that code is used in the
gold configuration. This approach means that a doc-
ument?s configuration may be a construction of mul-
tiple annotators and may not match any of the three
annotators exactly. Both the individual and the ma-
jority annotations are included with the training cor-
pus.
While others have attempted ICD-9 code classi-
fication, our task differs in two respects (Section 7
provides an overview of previous work). First, pre-
vious work has used discharge reports, which are
typically longer with more text fields. Second, while
most systems are evaluated as a recommendation
system, offering the top k codes and then scoring
recall at k, our task is to provide the exact configu-
ration. The CMC challenge evaluated systems using
an F1 score, so we are penalized if we suggest any
label that does not appear in the majority annotation.
To estimate task difficulty we measured the inter-
annotator score for the training set using the three
annotations provided. We scored two annotations
with the micro average F1, which weighs each code
assignment equally (see Section 5 for details on
evaluation metrics). If an annotator omitted a code
and included an extra code, he or she is penalized
with a false positive (omitting a code) and a false
negative (adding an extra code). We measured anno-
tators against each other; the average f-measure was
74.85 (standard deviation of .06). These scores were
low since annotators chose from an unrestricted set
of codes, many of which were not included in the fi-
nal majority annotation. However, these scores still
indicate the human accuracy for this task using an
unrestricted label set. 3
3 Code Assignment System
We developed three automated systems guided by
our above analysis. First, we designed a learning
system that used natural language features from the
official code descriptions and the text of each re-
port. It is general purpose and labels all 45 codes
and 94 configurations (labels). Second, we built a
rule based system that assigned codes based on the
overlap between the reports and code descriptions,
similar to how an annotator may search code de-
scriptions for appropriate labels. Finally, a special-
ized system aimed at the most common codes imple-
mented a policy that mimics the guidelines a medical
staffer would use to assign these codes.
3.1 Learning System
We begin with some notational definitions. In what
follows, x denotes the generic input document (ra-
diology report), Y denotes the set of possible label-
ings (code configurations) of x, and y?(x) the cor-
rect labeling of x. For each pair of document x
and labeling y ? Y , we compute a vector-valued
feature representation f(x, y). A linear model is
3We also measured each annotator with the majority codes,
taking the average score (87.48), and the best annotator with
the majority label (92.8). However, these numbers are highly
biased since the annotator influences the majority labeling. We
observe that our final system still exceeds the average score.
131
given by a weight vector w. Given this weight vec-
tor w, the score w ? f(x, y) ranks possible labelings
of x, and we denote by Yk,w(x) the set of k top
scoring labelings for x. For some structured prob-
lems, a factorization of f(x, y) is required to enable
a dynamic program for inference. For our problem,
we know all the possible configurations in advance
(there are 94 of them) so we can pick the highest
scoring y ? Y by trying them all. For each docu-
ment x and possible labeling y, we compute a score
using w and the feature representation f(x, y). The
top scoring y is output as the correct label. Section
3.1.1 describes our feature function f(x, y) while
Section 3.1.2 describes how we find a good weight
vector w.
3.1.1 Features
Problem representation is one of the most impor-
tant aspects of a learning system. In our case, this
is defined by the set of features f(x, y). Ideally we
would like a linear combination of our features to ex-
actly specify the true labeling of all the instances, but
we want to have a small total number of features so
that we can accurately estimate their values. We sep-
arate our features into two classes: label specific fea-
tures and transfer features. For simplicity, we index
features by their name. Label specific features are
only present for a single label. For example, a simple
class of label specific features is the conjunction of a
word in the document with an ICD-9-CM code in the
label. Thus, for each word we create 94 features, i.e.
the word conjoined with every label. These features
tend to be very powerful, since weights for them can
encode very specific information about the way doc-
tors talk about a disease, such as the feature ?con-
tains word pneumonia and label contains code 486?.
Unfortunately, the cost of this power is that there are
a large number of these features, making parameter
estimation difficult for rare labels. In contrast, trans-
fer features can be present in multiple labels. An
example of a transfer feature might be ?the impres-
sion contains all the words in the code descriptions
of the codes in this label?. Transfer features allow us
to generalize from one label to another by learning
things like ?if all the words of the label description
occur in the impression, then this label is likely? but
have the drawback that we cannot learn specific de-
tails about common labels. For example, we cannot
learn that the word ?pneumonia? in the impression
is negatively correlated with the code cough. The
inclusion of both label specific and transfer features
allows us to learn specificity where we have a large
number of examples and generality for rare codes.
Before feature extraction we normalized the re-
ports? text by converting it to lower case and by
replacing all numbers (and digit sequences) with a
single token ?NUM?. We also prepared a synonym
dictionary for a subset of the tokens and n-grams
present in the training data. The synonym dictionary
was based onMeSH4, the Medical Subject Headings
vocabulary, in which synonyms are listed as terms
under the same concept. All ngrams and tokens
in the training data which had mappings defined in
the synonym dictionary were then replaced by their
normalized token; e.g. all mentions of ?nocturnal
enuresis? or ?nighttime urinary incontinence? were
replaced by the token ?bedwetting?. Additionally,
we constructed descriptions for each code automati-
cally from the official ICD-9-CM code descriptions
in National Center for Health Statistics (2006). We
also created a mapping between code and code type
(diagnosis or symptom) using the guidelines.
Our system used the following features. The de-
scriptions of particular features are in quotes, while
schemes for constructing features are not.
? ?this configuration contains a disease code?,
?this configuration contains a symptom code?,
?this configuration contains an ambiguous
code? and ?this configuration contains both dis-
ease and symptom codes?.5
? With the exception of stop-words, all words of
the impression and history conjoined with each
label in the configuration; pairs of words con-
joined with each label; words conjoined with
pairs of labels. For example, ?the impression
contains ?pneumonia? and the label contains
codes 786.2 and 780.6?.
? A feature indicating when the history or im-
pression contains a complete code description
4www.nlm.nih.gov/mesh
5We included a feature for configurations that had both dis-
ease and symptom codes because they appeared in the training
data, even though coding guidelines prohibit these configura-
tions.
132
for the label; one for a word in common with
the code description for one of the codes in the
label; a common word conjoined with the pres-
ence of a negation word nearby (?no?, ?not?,
etc.); a word in common with a code descrip-
tion not present in the label. We applied similar
features using negative words associated with
each code.
? A feature indicating when a soft negation word
appears in the text (?probable?, ?possible?,
?suspected?, etc.) conjoined with words that
follow; the token length of a text field (?im-
pression length=3?); a conjunction of a feature
indicating a short text field with the words in
the field (?impression length=1 and ?pneumo-
nia? ?)
? A feature indicating each n-gram sequence that
appears in both the impression and clinical his-
tory; the conjunction of certain terms where
one appears in the history and the other in the
impression (e.g. ?cough in history and pneu-
monia in impression?).
3.1.2 Learning Technique
Using these feature representations, we now learn
a weight vector w that scores the correct labelings
of the data higher than incorrect labelings. We used
a k-best version of the MIRA algorithm (Crammer,
2004; McDonald et al, 2005). MIRA is an online
learning algorithm that for each training document
x updates the weight vector w according to the rule:
wnew = argmin
w
?w ? wold?
s.t. ?y ? Yk,wold(x) :
w ? f(x, y?(x)) ? w ? f(x, y) ? L(y?(x), y)
where L(y?(x), y) is a measure of the loss of label-
ing y with respect to the correct labeling y?(x). For
our experiments, we set k to 30 and iterated over the
training data 10 times. Two standard modifications
to this approach also helped. First, rather than using
just the final weight vector, we average all weight
vectors. This has a smoothing effect that improves
performance on most problems. The second modifi-
cation is the introduction of slack variables:
wnew = argmin
w
?w ? wold? + ?
?
i
?i
s.t. ?y ? Yk,wold(x) :
w ? f(x, y?(x)) ? w ? f(x, y) ? L(y?(x), y) ? ?i
?i ? {1 . . . k} : ?i ? 0.
We used a ? of 10?3 in our experiments.
The most straightforward loss function is the 0/1
loss, which is one if y does not equal y?(x) and zero
otherwise. Since we are evaluated based on the num-
ber of false negative and false positive ICD-9-CM
codes assigned to all the documents, we used a loss
that is the sum of the number of false positive and the
number of false negative labels that y assigns with
respect to y?(x).
Finally, we only used features that were possi-
ble for some labeling of the test data by using only
the test data to construct our feature alphabet. This
forced the learner to focus on hypotheses that could
be used at test time and resulted in a 1% increase in
F-measure in our final system on the test data.
3.2 Rule Based System
Since some of the configurations appear a small
number of times in our corpus (some only once),
we built a rule based system that requires no train-
ing. The system uses a description of the ICD-9-CM
codes and their types, similar to the list used by our
learning system (Section 3.1.1). The code descrip-
tions include between one and four short descrip-
tions, such as ?reactive airway disease?, ?asthma?,
and ?chronic obstructive pulmonary disease?. We
treat each of these descriptions as a bag of words.
For a given report, the system parses both the clini-
cal history and impression into sentences, using ?.?
as a sentence divider. Each sentence is the checked
to see if all of the words in a code description appear
in the sentence. If a match is found, we set a flag
corresponding to the code. However, if the code is
a disease, we search for a negation word in the sen-
tence, removing the flag if a negation word is found.
Once all code descriptions have been evaluated, we
check if there are any flags set for disease codes. If
so, we remove all symptom code flags. We then emit
a code corresponding to each set flag. This simple
system does not enforce configuration restrictions;
133
we may predict a code configuration that does not
appear in our training data. Adding this restriction
improved precision but hurt recall, leading to a slight
decrease in F1 score. We therefore omitted the re-
striction from our system.
3.3 Automatic Coding Policies
As we described in Section 2, enforcing coding
guidelines can be a complex task. While a learning
system may have trouble coding a document, a hu-
man may be able to define a simple policy for cod-
ing. Since some of the most frequent codes in our
dataset have this property, we decided to implement
such an automatic coding policy. We selected two
related sets of codes to target with a rule based sys-
tem, a set of codes found in pneumonia reports and
a set for urinary tract infection/reflux reports.
Reports related to pneumonia are the most com-
mon in our dataset and include codes for pneumo-
nia, asthma, fever, cough and wheezing; we handle
them with a single policy. Our policy is as follows:
? Search for a small set of keywords (e.g.
?cough?, ?fever?) to determine if a code should
be applied.
? If ?pneumonia? appears unnegated in the im-
pression and the impression is short, or if it oc-
curs in the clinical history and is not preceded
by phrases such as ?evaluate for? or ?history
of?, apply pneumonia code and stop.
? Use the same rule to code asthma by looking
for ?asthma? or ?reactive airway disease?.
? If no diagnosis is found, code all non-negated
symptoms (cough, fever, wheezing).
We selected 80% of the training set to evaluate in the
construction of our rules. We then ran the finished
system on both this training set and the held out 20%
of the data. The system achieved F1 scores of 87%
on the training set and 84% on the held out data for
these five codes. The comparable scores indicates
that we did not over-fit the training data.
We designed a similar policy for two other related
codes, urinary tract infection and vesicoureteral re-
flux. We found these codes to be more complex as
they included a wide range of kidney disorders. On
these two codes, our system achieved 78% on the
train set and 76% on the held out data. Overall, au-
tomatically applying our two policies yielded high
confidence predictions for a significant subset of the
corpus.
4 Combined System
Since our three systems take complimentary ap-
proaches to the problem, we combined them to im-
prove performance. First, we took our automatic
policy and rule based systems and cascaded them; if
the automatic policy system does not apply a code,
the rule based system classifies the report. We used
a cascaded approach since the automatic policy sys-
tem was very accurate when it was able to assign
a code. Therefore, the rule based system defers to
the policy system when it is triggered. Next, we in-
cluded the prediction of the cascaded system as a
feature for our learning system. We used two fea-
ture rules: ?cascaded-system predicted exactly this
label? and ?cascaded-system predicted one of the
codes in this label?. As we show, this yielded our
most accurate system. While we could have used a
meta-classifier to combine the three systems, includ-
ing the rule based systems as features to the learning
system allowed it to learn the appropriate weights
for the rule based predictions.
5 Evaluation Metric
Evaluation metrics for this task are often based on
recommendation systems, where the system returns
a list of the top k codes for selection by the user. As
a result, typical metrics are ?recall at k? and aver-
age precision (Larkey and Croft, 1995). Instead, our
goal was to predict the exact configuration, returning
exactly the number of codes predicted to be on the
report. The competition used a micro-averaged F1
score to evaluate predictions. A contingency table
(confusion matrix) is computed by summing over
each predicted code for each document by predic-
tion type (true positive, false positive, false negative)
weighing each code assignment equally. F1 score
is computed based on the resultant table. If specific
codes or under-coding is favored, we can modify our
learning loss function as described in Section 3.1.2.
A detailed treatment of this evaluation metric can be
found in Computational Medicine Center (2007).
134
System Precision Recall F1
BL 61.86 72.58 66.79
RULE 81.9 82.0 82.0
CASCADE 86.04 84.56 85.3
LEARN 85.5 83.6 84.6
CASCADE+LEARN 87.1 85.9 86.5
Table 1: Performance of our systems on the provided
labeled training data (F1 score). The learning sys-
tems (CASCADE+LEARN and LEARN ) were eval-
uated on ten random split of the data while RULE
was evaluated on all of the training data. We include
a simple rule based system (BL ) as a baseline.
6 Results
We evaluated our systems on the labeled training
data of 978 radiology reports. For each report, each
system predicted an exact configuration of codes
(i.e. one of 94 possible labels). We score each sys-
tem using a micro-averaged F1 score. Since we only
had labels for the training data, we divided the data
using an 80/20 training test split and averaged results
over 10 runs for our learning systems. We evaluated
the following systems:
? RULE : The rule based system based on ICD-
9-CM code descriptions (Section 3.2).
? CASCADE : The automatic code policy system
(Section 3.3) cascaded with RULE (Section 4).
? LEARN : The learning system with both label
specific and transfer features (Section 3.1).
? CASCADE+LEARN : Our combined system
that incorporates CASCADE predictions as a
feature to LEARN (Section 4).
For a baseline, we built a simple system that ap-
plies the official ICD-9-CM code descriptions to find
the correct labels (BL ). For each code in the train-
ing set, the system generates text-segments related to
it. During testing, for each new document, the sys-
tem checks if any text-segment (as discovered dur-
ing training) appears in the document. If so, the cor-
responding code is predicted. The results from our
four systems and baseline are shown in Table 1.
System Train Test
CASCADE 85.3 84
CASCADE+LEARN 86.5 87.60
Average - 76.6
Best - 89.08
Table 2: Performance of two systems on the train
and test data. Results obtained from the web sub-
mission interface were rounded. Average and Best
are the average and best f-measures of the 44 sub-
mitted systems (standard deviation 13.40).
Each of our systems easily beats the baseline, and
the average inter-annotator score for this task. Ad-
ditionally, we were able to evaluate two of our sys-
tems on the test data using a web interface as pro-
vided by the competition. The test set contains 976
documents (about the same as the training set) and
is drawn the from same distribution as the training
data. Our test results were comparable to perfor-
mance on the training data, showing that we did
not over-fit to the training data (Table 2). Addi-
tionally, our combined system (CASCADE+LEARN
) achieved a score of 87.60%, beating our training
data performance and exceeding the average inter-
annotator score. Out of 44 submitted systems, the
average score on test data was 76.7% (standard devi-
ation of 13.40) and the maximum score was 89.08%.
Our system scored 4th overall and was less than
1.5% behind the best system. Overall, in comparison
with our baselines and over 40 systems, we perform
very well on this task.
7 Related Work
There have been several attempts at ICD-9-CM
code classification and related problems for med-
ical records. The specific problem of ICD-9-CM
code assignment was studied by Lussier et al (2000)
through an exploratory study. Larkey and Croft
(1995) designed classifiers for the automatic assign-
ment of ICD-9 codes to discharge summaries. Dis-
charge summaries tend to be considerably longer
than our data and contain multiple text fields. Ad-
ditionally, the number of codes per document has
a larger range, varying between 1 and 15 codes.
Larkey and Croft use three classifiers: K-nearest
neighbors, relevance feedback, and bayesian inde-
135
pendence. Similar to our approach, they tag items
as negated and try to identify diagnosis and symp-
tom terms. Additionally, their final system combines
all three models. A direct comparison is not possi-
ble due to the difference in data and evaluation met-
rics; they use average precision and recall at k. On
a comparable metric, ?principal code is top candi-
date?, their best system achieves 59.9% accuracy. de
Lima et al (1998) rely on the hierarchical nature of
medical codes to design a hierarchical classification
scheme. This approach is likely to help on our task
as well but we were unable to test this since the lim-
ited number of codes removes any hierarchy. Other
approaches have used a variety of NLP techniques
(Satomura and Amaral, 1992).
Others have used natural language systems for the
analysis of medical records (Zweigenbaum, 1994).
Chapman and Haug (1999) studied radiology re-
ports looking for cases of pneumonia, a goal sim-
ilar to that of our automatic coding policy system.
Meystre and Haug (2005) processed medical records
to harvest potential entries for a medical problem
list, an important part of electronic medical records.
Chuang et al (2002) studied Charlson comorbidi-
ties derived from processing discharge reports and
chest x-ray reports and compared them with admin-
istrative data. Additionally, Friedman et al (1994)
applies NLP techniques to radiology reports.
8 Conclusion
We have presented a learning system that processes
radiology reports and assigns ICD-9-CM codes.
Each of our systems achieves results comparable
with an inter-annotator baseline for our training data.
A combined system improves over each individ-
ual system. Finally, we show that on test data un-
available during system development, our final sys-
tem continues to perform well, exceeding the inter-
annotator baseline and achieving the 4th best score
out of 44 systems entered in the CMC challenge.
9 Acknowledgements
We thank Andrew Lippa for his extensive medical
wisdom. Dredze is supported by an NDSEG fel-
lowship; Ganchev and Talukdar by NSF ITR EIA-
0205448; and Crammer by DARPA under Contract
No. NBCHD03001. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the DARPA or the De-
partment of Interior-National Business Center (DOI-
NBC).
References
W.W. Chapman and P.J. Haug. 1999. Comparing expert sys-
tems for identifying chest x-ray reports that support pneu-
monia. In AMIA Symposium, pages 216?20.
JH Chuang, C Friedman, and G Hripcsak. 2002. A com-
parison of the charlson comorbidities derived from medical
language processing and administrative data. AMIA Sympo-
sium, pages 160?4.
Computational Medicine Center. 2007. The
computational medicine center?s 2007 med-
ical natural language processing challenge.
http://computationalmedicine.org/challenge/index.php.
Koby Crammer. 2004. Online Learning of Complex Categorial
Problems. Ph.D. thesis, Hebrew Univeristy of Jerusalem.
Luciano R. S. de Lima, Alberto H. F. Laender, and Berthier A.
Ribeiro-Neto. 1998. A hierarchical approach to the auto-
matic categorization of medical documents. In CIKM.
C Friedman, PO Alderson, JH Austin, JJ Cimino, and SB John-
son. 1994. A general natural-language text processor for
clinical radiology. Journal of the American Medical Infor-
matics Association, 1:161?74.
Leah S. Larkey and W. Bruce Croft. 1995. Automatic assign-
ment of icd9 codes to discharge summaries. Technical re-
port, University of Massachusetts at Amherst, Amherst, MA.
YA Lussier, C Friedman, L Shagina, and P Eng. 2000. Au-
tomating icd-9-cm encoding using medical language pro-
cessing: A feasibility study.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Flexible text segmentation with structured multilabel classi-
fication. In HLT/EMNLP.
Stephane Meystre and Peter J Haug. 2005. Automation of a
problem list using natural language processing. BMC Medi-
cal Informatics and Decision Making.
National Center for Health Statistics. 2006. Icd-
9-cm official guidelines for coding and reporting.
http://www.cdc.gov/nchs/datawh/ftpserv/ftpicd9/ftpicd9.htm.
Y Satomura and MB Amaral. 1992. Automated diagnostic in-
dexing by natural language processing. Medical Informat-
ics, 17:149?163.
Edward H. Shortliffe and James J. Cimino, editors. 2006.
Biomedical Informatics: Computer Applications in Health
Care and Biomedicine. Springer.
P. Zweigenbaum. 1994. Menelas: an access system for medical
records using natural language. Comput Methods Programs
Biomed, 45:117?20.
136
Proceedings of the Linguistic Annotation Workshop, pages 53?56,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semi-Automated Named Entity Annotation
Kuzman Ganchev and Fernando Pereira
Computer and Information Science,
University of Pennsylvania,
Philadelphia PA
{ kuzman and pereira } @cis.upenn.edu
Mark Mandel
Linguistic Data Consortium,
University of Pennsylvania, Philadelphia PA
mamandel@ldc.upenn.edu
Steven Carroll and Peter White
Division of Oncology, Children?s Hospital of Philadelphia Philadelphia PA
{ carroll and white }@genome.chop.edu
Abstract
We investigate a way to partially automate
corpus annotation for named entity recogni-
tion, by requiring only binary decisions from
an annotator. Our approach is based on a lin-
ear sequence model trained using a k-best
MIRA learning algorithm. We ask an an-
notator to decide whether each mention pro-
duced by a high recall tagger is a true men-
tion or a false positive. We conclude that our
approach can reduce the effort of extending
a seed training corpus by up to 58%.
1 Introduction
Semi-automated text annotation has been the subject
of several previous studies. Typically, a human an-
notator corrects the output of an automatic system.
The idea behind our approach is to start annota-
tion manually and to partially automate the process
in the later stages. We assume that some data has
already been manually tagged and use it to train a
tagger specifically for high recall. We then run this
tagger on the rest of our corpus and ask an annotator
to filter the list of suggested gene names.
The rest of this paper is organized as follows. Sec-
tion 2 describes the model and learning algorithm.
Section 3 relates our approach to previous work.
Section 4 describes our experiments and Section 5
concludes the paper.
2 Methods
Throughout this work, we use a linear sequence
model. This class of models includes popular tag-
ging models for named entities such as conditional
random fields, maximum entropy Markov models
and max-margin Markov networks. Linear sequence
models score possible tag sequences for a given in-
put as the dot product between a learned weight vec-
tor and a feature vector derived from the input and
proposed tas sequence. Linear sequence models dif-
fer principally on how the weight vector is learned.
Our experiments use the MIRA algorithm (Cram-
mer et al, 2006; McDonald et al, 2005) to learn
the weight vector.
2.1 Notation
In what follows, x denotes the generic input sen-
tence, Y (x) the set of possible labelings of x, and
Y +(x) the set of correct labelings of x. There is
also a distinguished ?gold? labeling y(x) ? Y +(x).
For each pair of a sentence x and labeling y ?
Y (x), we compute a vector-valued feature represen-
tation f(x, y). Given a weight vector w, the score
w ? f(x, y) ranks possible labelings of x, and we de-
note by Yk,w(x) the set of k top scoring labelings for
x.
We use the standard B,I,O encoding for named
entities (Ramshaw and Marcus, 1995). Thus Y (x)
for x of length n is the set of all sequences of length
n matching the regular expression (O|(BI?))?. In a
linear sequence model, for suitable feature functions
f , Yk,w(x) can be computed efficiently with Viterbi
decoding.
2.2 k-best MIRA and Loss Functions
The learning portion of our method finds a weight
vector w that scores the correct labelings of the test
data higher than incorrect labelings. We used a k-
53
best version of the MIRA algorithm (Crammer et
al., 2006; McDonald et al, 2005). This is an online
learning algorithm that starts with a zero weight vec-
tor and for each training sentence makes the small-
est possible update that would score the correct la-
bel higher than the old top k labels. That is, for each
training sentence x we update the weight vector w
according to the rule:
wnew = argminw ?w ? wold?
s. t. w ? f(x, y(x)) ? w ? f(x, y) ? L(Y +(x), y)
?y ? Yk,wold(x)
where L(Y +(x), y) is the loss, which measures the
errors in labeling y relative to the set of correct la-
belings Y +(x).
An advantage of the MIRA algorithm (over many
other learning algorithms such as conditional ran-
dom fields) is that it allows the use of arbitrary loss
functions. For our experiments, the loss of a label-
ing is a weighted combination of the number of false
positive mentions and the number of false negative
mentions in that labeling.
2.3 Semi-Automated Tagging
For our semi-automated annotation experiments, we
imagine the following scenario: We have already an-
notated half of our training corpus and want to anno-
tate the remaining half. The goal is to save annotator
effort by using a semi-automated approach instead
of annotating the rest entirely manually.
In particular we investigate the following method:
train a high-recall named entity tagger on the anno-
tated data and use that to tag the remaining corpus.
Now ask a human annotator to filter the resulting
mentions. The mentions rejected by the annotator
are simply dropped from the annotation, leaving the
remaining mentions.
3 Relation to Previous Work
This section relates our approach to previous work
on semi-automated approaches. First we discuss
how semi-automated annotation is different from ac-
tive learning and then discuss some previous semi-
automated annotation work.
3.1 Semi-Automated versus Active Learning
It is important not to confuse semi-automated anno-
tation with active learning. While they both attempt
to alleviate the burden of creating an annotated cor-
pus, they do so in a completely orthogonal manner.
Active learning tries to select which instances should
be labeled in order to make the most impact on learn-
ing. Semi-automated annotation tries to make the
annotation of each instance faster or easier. In par-
ticular, it is possible to combine active learning and
semi-automated annotation by using an active learn-
ing method to select which sentences to label and
then using a semi-automated labeling method.
3.2 Previous work on semi-automated
annotation
The most common approach to semi-automatic an-
notation is to automatically tag an instance and then
ask an annotator to correct the results. We restrict
our discussion to this paradigm due to space con-
straints. Marcus et al (1994), Chiou et al (2001)
and Xue et al (2002) apply this approach with some
minor modifications to part of speech tagging and
phrase structure parsing. The automatic system of
Marcus et al only produces partial parses that are
then assembled by the annotators, while Chiou et al
modified their automatic parser specifically for use
in annotation. Chou et al (2006) use this tag and
correct approach to create a corpus of predicate ar-
gument structures in the biomedical domain. Culota
et al (2006) use a refinement of the tag and correct
approach to extract addressbook information from e-
mail messages. They modify the system?s best guess
as the user makes corrections, resulting in less anno-
tation actions.
4 Experiments
We now evaluate to what extent our semi-automated
annotation framework can be useful, and how much
effort it requires. For both questions we compare
semi-automatic to fully manual annotation. In our
first set of experiments, we measured the usefulness
of semi-automatically annotated corpora for training
a gene mention tagger. In the second set of exper-
iments, we measured the annotation effort for gene
mentions with the standard fully manual method and
with the semi-automated methods.
4.1 Measuring Effectiveness
The experiments in this section use the training data
from the the Biocreative II competition (Tanabe et
54
Sentence Expression of SREBP-1a stimulated StAR promoter activity in the context of COS-1 cells
gold label Expression of SREBP-1a stimulated StAR promoter activity in . . .
alternative Expression of SREBP-1a stimulated StAR promoter activity in . . .
alternative Expression of SREBP-1a stimulated StAR promoter activity in . . .
Figure 1: An example sentence and its annotation in Biocreative II. The evaluation metric would give full
credit for guessing one of the alternative labels rather than the ?gold? label.
al., 2005). The data is supplied as a set of sentences
chosen randomly fromMEDLINE and annotated for
gene mentions.
Each sentence in the corpus is provided as a list of
?gold? gene mentions as well as a set of alternatives
for each mention. The alternatives are generated by
the annotators and count as true positives. Figure 1
shows an example sentence with its gold and alter-
native mentions. The evaluation metric for these ex-
periments is F-score augmented with the possibility
of alternatives (Yeh et al, 2005).
We used 5992 sentences as the data that has al-
ready been annotated manually (set Data-1), and
simulated different ways of annotating the remain-
ing 5982 sentences (set Data-2). We compare the
quality of annotation by testing taggers trained us-
ing these corpora on a 1493 sentence test set.
We trained a high-recall tagger (recall of 89.6%)
on Data-1, and ran it on Data-2. Since we have
labels available for Data-2, we simulated an anno-
tator filtering these proposed mentions by accepting
them only if they exactly match a ?gold? or alterna-
tive mention. This gave us an F-score of 94.7% on
Data-2 and required 9981 binary decisions.
Figure 2 shows F1 score as a function of the num-
ber of extra sentences annotated. Without any ad-
ditional data, the F-measure of the tagger is 81.0%.
The two curves correspond to annotation with and
without alternatives. The horizontal line at 82.8%
shows the level achieved by the semi-automatic
method (when using all of Data-2).
From the figure, we can see that to get compa-
rable performance to the semi-automatic approach,
we need to fully manually annotate roughly a third
as much data with alternatives, or about two thirds as
much data without alternatives. The following sec-
tion examines what this means in terms of annotator
time by providing timing results for semi-automatic
and fully-manual annotation without alternatives.
 81 81.5 82 82.5 83 83.5 84 84.5 85
 0
 1000
 2000
 3000
 4000
 5000
 6000
Extra
 Anno
tated 
Sente
nces (
from 
Data-
2)
Manu
al Wi
th Alt
ernati
ves
Manu
al w/o
 Alter
native
s
Semi-
Autom
atic (o
n all o
f Data
-2)
Figure 2: Effect of the number of annotated in-
stances on F1 score. In all cases the original 5992
instances were used; the curves show manual an-
notation while the level line is the semi-automatic
method. The curves are averages over 3 trials.
4.2 Measuring Effort
The second set of experiments compares annotator
effort between fully manual and semi-automatic an-
notation. Because we did not have access to an expe-
rienced annotator from the Biocreative project, and
gene mention annotations vary subtly among anno-
tation efforts, we evaluated annotator effort on on the
PennBioIE named entity corpus.1 Furthermore, we
have not yet annotated enough data locally to per-
form both effectiveness and effort experiments on
the local corpus alone. However, both corpora an-
notate gene mentions in MEDLINE abstracts, so we
expect that the timing results will not be significantly
different.
We asked an experienced annotator to tag 194
MEDLINE abstracts: 96 manually and 98 using the
semi-automated method. Manual annotation was
done using annotation software familiar to the an-
notator. Semi-automatic annotation was done with a
1Available from http://bioie.ldc.upenn.edu/
55
Web-based tool developed for the task. The new tool
highlights potential gene mentions in the text and al-
lows the annotator to filter them with a mouse click.
The annotator had been involved in the creation of
the local manually annotated corpus, and had a lot of
experience annotating named entities. The abstracts
for annotation were selected randomly so that they
did not contain any abstracts tagged earlier. There-
fore, we did not expect the annotator to have seen
any of them before the experiment.
To generate potential gene mentions for the semi-
automated annotation, we ran two taggers on the
data: a high recall tagger trained on the local corpus
and a high recall tagger trained on the Biocreative
corpus. At decode time, we took the gene mentions
from the top two predictions of each of these taggers
whenever there were any gene mentions predicted.
As a result, the annotator had to make more binary
decisions per sentence than they would have for ei-
ther training corpus alone. For the semi-automated
annotation, the annotator had to examine 682 sen-
tences and took on average 10 seconds per sentence.
For the fully-manual annotation, they examined 667
sentences and took 40 seconds per sentence on av-
erage. We did not ask the annotator to tag alterna-
tives because they did not have any experience with
tagging alternatives and we do not have a tool that
makes the annotation of alternatives easy. Conse-
quently, effort totals for annotation with alternatives
would have been skewed in our favor. The four-fold
speedup should be compared to the lower curve in
Figure 2.
5 Discussion and Further Work
We can use the effort results to estimate the relative
effort of annotating without alternatives and of semi-
automated annotation. To obtain the same improve-
ment in F-score, we need to semi-automatically an-
notate roughly a factor of 1.67 more data than using
the fully manual approach. Multiplying that by the
0.25 factor reduction in annotation time, we get that
the time required for a comparable improvement in
F-score is 0.42 times as long ? a 58% reduction in
annotator time.
We do not have any experiments on annotating
alternatives, but the main difference between semi-
automated and fully-manual annotation is that the
former does not require the annotator to decide on
boundaries. Consequently, we expect that annota-
tion with alternatives will be considerably more ex-
pensive than without alternatives, since more bound-
aries have to be outlined.
In future work, it would be interesting to compare
this approach to the traditional approach of manually
correcting output of a system. Due to constraints
on annotator time, it was not possible to do these
experiments as part of the current work.
References
Fu-Dong Chiou, David Chiang, and Martha Palmer.
2001. Facilitating treebank annotation using a statisti-
cal parser. In HLT ?01. ACL.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su,
Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu. 2006.
A semi-automatic method for annotating a biomedical
proposition bank. In FLAC?06. ACL.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 7.
Aron Culota, Trausti Kristjansson, Andrew McCallum,
and Paul Viola. 2006. Corrective feedback and per-
sistent learning for information extraction. Artificial
Intelligence, 170:1101?1122.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL?05. ACL.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora. ACL.
Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne
Matten, and W. John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics, 6(Suppl. 1).
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese corpus.
In Proceedings of the 19th international conference on
Computational linguistics. ACL.
Alexander Yeh, Alexander Morgan, Marc Colosimo, and
Lynette Hirschman. 2005. BioCreAtIvE Task 1A:
gene mention finding evaluation . BMC Bioinformat-
ics, 6(Suppl. 1).
56
