Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 125?130,
Prague, June 2007. c?2007 Association for Computational Linguistics
Hypothesis Transformation and Semantic Variability Rules Used in 
Recognizing Textual Entailment 
Adrian Iftene 
?Al. I. Cuza? University, Faculty of 
Computer Science, Iasi, Romania 
 adiftene@info.uaic.ro 
Alexandra Balahur-Dobrescu 
?Al. I. Cuza? University, Faculty of 
Computer Science, Iasi, Romania 
 abalahur@info.uaic.ro 
 
Abstract 
Based on the core approach of the tree edit 
distance algorithm, the system central mod-
ule is designed to target the scope of TE ? 
semantic variability. The main idea is to 
transform the hypothesis making use of ex-
tensive semantic knowledge from sources 
like DIRT, WordNet, Wikipedia, acronyms 
database. Additionally, we built a system to 
acquire the extra background knowledge 
needed and applied complex grammar rules 
for rephrasing in English. 
1 Introduction 
Many NLP applications need to recognize when 
the meaning of one text can be expressed by, or 
inferred from, another text. Information Retrieval 
(IR), Question Answering (QA), Information Ex-
traction (IE), Text Summarization (SUM) are ex-
amples of applications that need to assess such a 
semantic relationship between text segments. Tex-
tual Entailment Recognition (RTE) (Dagan et al, 
2006) has recently been proposed as an application 
independent task to capture such inferences. 
This year our textual entailment system partici-
pated for the first time in the RTE1 competition. 
Next chapters present its main parts, the detailed 
results obtained and some possible future im-
provements. 
2 System description 
The process requires an initial pre-processing, fol-
lowed by the execution of a core module which 
uses the output of the first phase and obtains in the 
end the answers for all pairs. Figure 1 shows how 
                                                          
1
 http://www.pascal-network.org/Challenges/RTE3/ 
the pre-processing is realized with the MINIPAR 
(Lin, 1998) and LingPipe2 modules which provide 
the input for the core module. This one uses four 
databases: DIRT, Acronyms, Background knowl-
edge and WordNet. 
 
Figure 1: System architecture 
The system architecture is based on a peer-to-
peer networks design, in which neighboring com-
puters collaborate in order to obtain the global fit-
ness for every text-hypothesis pair. Eventually, 
based on the computed score, we decide for which 
pairs we have entailment. This type of architecture 
was used in order to increase the computation 
speed. 
3 Initial pre-processing    
The first step splits the initial file into pairs of files 
for text and hypothesis. All these files are then sent 
to the LingPipe module in order to find the Named 
entities.  
                                                          
2
 http://www.alias-i.com/lingpipe/ 
Initial 
data 
DIRT 
Minipar 
module 
Dependency 
trees for   
(T, H) pairs 
LingPipe 
module 
Named 
entities for     
(T, H) pairs 
Final 
result 
Core 
Module3 
Core 
Module2 
Core 
Module1 
Acronyms 
Background 
knowledge 
Wordnet 
P2P  
Computers 
Wikipedia 
125
In parallel, we transform with MINIPAR both 
the text and the hypothesis into dependency trees. 
Figure 2 shows the output associated with the sen-
tence: ?Le Beau Serge was directed by Chabrol.?. 
 
Figure 2: MINIPAR output ? dependency tree 
For every node from the MINIPAR output, we 
consider a stamp called entity with three main fea-
tures: the node lemma, the father lemma and the 
edge label (which represents the relation between 
words) (like in Figure 3). 
 
Figure 3: Entity components 
Using this stamp, we can easily distinguish be-
tween nodes of the trees, even if these have the 
same lemma and the same father. In the example 
from Figure 1, for the ?son? nodes we have two 
entities (Le_Beau_Serge, direct, s) and 
(Le_Beau_Serge, direct, obj). 
4 The hypothesis tree transformation 
Presently, the core of our approach is based on a 
tree edit distance algorithm applied on the depend-
ency trees of both the text and the hypothesis 
(Kouylekov, Magnini 2005). If the distance (i.e. the 
cost of the editing operations) among the two trees 
is below a certain threshold, empirically estimated 
on the training data, then we assign an entailment 
relation between the two texts. 
The main goal is to map every entity in the de-
pendency tree associated with the hypothesis 
(called from now on hypothesis tree) to an entity in 
the dependency tree associated with the text (called 
from now on text tree).  
For every mapping we calculate a local fitness 
value which indicates the appropriateness between 
entities. Subsequently, the global fitness is calcu-
lated from these partial values. 
For every node (refers to the word contained in 
the node) which can be mapped directly to a node 
from the text tree, we consider the local fitness 
value to be 1. When we cannot map one word of 
the hypothesis to one node from the text, we have 
the following possibilities: 
? If the word is a verb in the hypothesis tree, we 
use the DIRT resource (Lin and Pantel, 2001) 
in order to transform the hypothesis tree into an 
equivalent one, with the same nodes except the 
verb. Our aim in performing this transforma-
tion is to find a new value for the verb which 
can be better mapped in the text tree.  
? If the word is marked as named entity by Ling-
Pipe, we try to use an acronyms? database3 or if 
the word is a number we try to obtain informa-
tion related to it from the background knowl-
edge. In the event that even after these 
operations we cannot map the word from the 
hypothesis tree to one node from the text tree, 
no fitness values are computed for this case 
and we decide the final result: No entailment.  
? Else, we use WordNet (Fellbaum, 1998) to 
look up synonyms for this word and try to map 
them to nodes from the text tree.  
Following this procedure, for every transforma-
tion with DIRT or WordNet, we consider for local 
fitness the similarity value indicated by these re-
sources. If after all checks, one node from the hy-
pothesis tree cannot be mapped, some penalty is 
inserted in the value of the node local fitness.  
4.1 The DIRT resource 
For the verbs in the MINIPAR output, we extract 
templates with DIRT- like format. For the sample 
output in Figure 2, where we have a single verb 
?direct?, we obtain the following list of ?full? tem-
plates:N:s:V<direct>V:by:N and N:obj:V<direct> 
V:by:N. To this list we add a list of ?partial? tem-
plates: N:s:V<direct>V:, :V<direct>V:by:N, 
:V<direct>V:by:N, and N:obj:V<direct>V:. 
In the same way, we build a list with templates 
for the verbs in the text tree. With these two lists 
we perform a search in the DIRT database and ex-
tract the ?best? trimming, considering the template 
type (full or partial) and the DIRT score. 
According to the search results, we have the fol-
lowing situations: 
                                                          
3
 http://www.acronym-guide.com 
direct (V) 
Le_Beau_Serge (N) be (be) Chabrol 
Le_Beau_Serge (N) 
Le (U) Beau (U) 
s 
be by 
obj 
lex-mod 
node lemma 
edge label 
father lemma 
lex-mod
126
a) left ? left relations similarity 
This case is described by the following two tem-
plates for the hypothesis and the text: 
relation1 HypothesisVerb relation2 
relation1 TextVerb relation3  
This is the most frequent case, in which a verb is 
replaced by one of its synonyms or equivalent ex-
pressions  
The transformation of the hypothesis tree is done 
in two steps:  
1. Replace the relation2 with relation3, 
2. Replace the verb from the hypothesis with 
the corresponding verb from the text. (see 
Figure 4). 
 
Figure 4: Left-left relation similarity 
b) right ? right relations similarity: the same 
idea from the previous case. 
c) left ? right relations similarity 
This case can be described by the following two 
templates for the hypothesis and the text: 
relation1 HypothesisVerb relation2 
relation3 TextVerb relation1  
The transformation of the hypothesis tree is:  
1. Replace the relation2 with relation3, 
2. Replace the verb from the hypothesis with 
the corresponding verb from the text. 
3. Rotate the subtrees accordingly: left sub-
tree will be right subtree and vice-versa 
right subtree will become left-subtree (as it 
can be observed in Figure 5). 
 
Figure 5: Left-right relation similarity 
This case appears for pair 161 with the verb ?at-
tack?: 
T: ?The demonstrators, convoked by the solidarity 
with Latin America committee, verbally attacked 
Salvadoran President Alfredo Cristiani.? 
H: ?President Alfredo Cristiani was attacked by 
demonstrators.? 
In this case, for the text we have the template 
N:subj:V<attack>V:obj:N, and for the hypothesis 
the template N:obj:V<attack>V:by:N. Using DIRT, 
hypothesis H is transformed into:  
H?: Demonstrators attacked President Alfredo 
Cristiani.  
Under this new form, H is easier comparable to T. 
d) right ? left relations similarity: the same 
idea from the previous case 
For every node transformed with DIRT, we con-
sider its local fitness as being the similarity value 
indicated by DIRT. 
4.2 Extended WordNet 
For non-verbs nodes from the hypothesis tree, if in 
the text tree we do not have nodes with the same 
lemma, we search for their synonyms in the ex-
tended WordNet4. For every synonym, we check to 
see if it appears in the text tree, and select the map-
ping with the best value according to the values 
from Extended WordNet. Subsequently, we change 
the word from the hypothesis tree with the word 
from WordNet and also its fitness with its indicated 
similarity value. For example, the relation between 
?relative? and ?niece? is accomplished with a score 
of 0.078652. 
                                                          
4
 http://xwn.hlt.utdallas.edu/downloads.html  
HypothesisVerb 
 
relation1 relation2 
TextVerb 
 
relation3 relation1 
Left 
Subtree 
Right 
Subtree 
Right 
Subtree 
Left 
Subtree 
HypothesisVerb 
 
relation1 
relation2 
TextVerb 
 relation1 relation3 
Left 
Subtree 
Right 
Subtree 
Right 
Subtree 
Left 
Subtree 
127
4.3 Acronyms 
The acronyms? database helps our program find 
relations between the acronym and its meaning: 
?US - United States?, and ?EU - European Union?. 
We change the word with the corresponding ex-
pression from this database. Since the meaning is 
the same, the local fitness is considered maximum, 
i.e. 1. 
4.4 Background Knowledge 
Some information cannot be deduced from the al-
ready used databases and thus we require addi-
tional means of gathering extra information of the 
form: 
Argentine [is] Argentina 
Netherlands [is] Holland 
2 [is] two 
Los Angeles [in] California 
Chinese [in] China 
Table 1: Background knowledge 
Background knowledge was built semi-
automatically, for the named entities (NEs) and for 
numbers from the hypothesis without correspon-
dence in the text. For these NEs, we used a module 
to extract from Wikipedia5 snippets with informa-
tion related to them. Subsequently, we use this file 
with snippets and some previously set patterns of 
relations between NEs, with the goal to identify a 
known relation between the NE for which we have 
a problem and another NE.  
If such a relation is found, we save it to an out-
put file. Usually, not all relations are correct, but 
those that are will help us at the next run.  
Our patterns identify two kinds of relations be-
tween words: 
? ?is?, when the module extracts information of 
the form: ?Argentine Republic? (Spanish: 'Re-
publica Argentina', IPA)? or when explanations 
about the word are given in brackets, or when 
the extracted information contains one verb 
used to define something, like ?is?, ?define?, 
?represent?: '2' ('two') is a number. 
? ?in? when information is of the form: 'Chinese' 
refers to anything pertaining to China or in the 
form Los Angeles County, California, etc. 
                                                          
5
 http://en.wikipedia.org/wiki/Main_Page  
In this case, the local fitness for the node is set to 
the maximum value for the [is]-type relations, and 
it receives some penalties for the [in]-type relation. 
5 Determination of entailment  
After transforming the hypothesis tree, we calcu-
late a global fitness score using the extended local 
fitness value for every node from the hypothesis - 
which is calculated as sum of the following values: 
1. local fitness obtained after the tree trans-
formation and node mapping, 
2. parent fitness after parent mapping, 
3. mapping of the node edge label from the 
hypothesis tree onto the text tree, 
4. node position (left, right) towards its father 
in the hypothesis and position of the map-
ping nodes from the text. 
After calculating this extended local fitness score, 
the system computes a total fitness for all the nodes 
in the hypothesis tree and a negation value associ-
ated to the hypothesis tree. Tests have shown that 
out of these parameters, some are more important 
(the parameter at 1.) and some less (the parameter 
at 3.). Below you can observe an example of how 
the calculations for 3 and 4 are performed and what 
the negation rules are. 
5.1 Edge label mapping 
After the process of mapping between nodes, we 
check how edge labels from the hypothesis tree are 
mapped onto the text tree. Thus, having two adja-
cent nodes in the hypothesis, which are linked by 
an edge with a certain label, we search on the path 
between the nodes? mappings in the text tree this 
label. (see Figure 6) 
 
Figure 6: Entity mapping 
Text tree 
node 
mapping 
father 
mapping 
edge label 
mapping  
Hypothesis tree 
128
It is possible that more nodes until the label of the 
edge linking the nodes in the hypothesis exist, or it 
is possible that this label is not even found on this 
path. According to the distance or to the case in 
which the label is missing, we insert some penalties 
in the extended local fitness. 
5.2 Node position 
After mapping the nodes, one of the two following 
possible situations may be encountered: 
? The position of the node towards its father and 
the position of the mapping node towards its 
father?s mapping are the same (left-left or 
right-right). In this case, the extended local fit-
ness is incremented. 
? The positions are different (left-right or right-
left) and in this case a penalty is applied ac-
cordingly. 
5.3 Negation rules 
For every verb from the hypothesis we consider a 
Boolean value which indicates whether the verb 
has a negation or not, or, equivalently, if it is re-
lated to a verb or adverb ?diminishing? its sense or 
not. Consequently, we check in its tree on its de-
scending branches to see whether one or more of 
the following words are to be found (pure form of 
negation or modal verb in indicative or conditional 
form): ?not, may, might, cannot, should, could, 
etc.?. For each of these words we successively ne-
gate the initial truth value of the verb, which by 
default is ?false?. The final value depends on the 
number of such words. 
Since the mapping is done for all verbs in the 
text and hypothesis, regardless of their original 
form in the snippet, we also focused on studying 
the impact of the original form of the verb on its 
overall meaning within the text. Infinitives can be 
identified when preceded by the particle ?to?. Ob-
serving this behavior, one complex rule for nega-
tion was built for the particle ?to? when it precedes 
a verb. In this case, the sense of the infinitive is 
strongly influenced by the active verb, adverb or 
noun before the particle ?to?, as follows: if it is 
being preceded by a verb like ?allow, impose, gal-
vanize? or their synonyms, or adjective like ?nec-
essary, compulsory, free? or their synonyms or 
noun like ?attempt?, ?trial? and their synonyms, the 
meaning of the verb in infinitive form is stressed 
upon and becomes ?certain?. For all other cases, 
the particle ?to? diminish the certainty of the action 
expressed in the infinitive-form verb. Based on the 
synonyms database with the English thesaurus6, we 
built two separate lists ? one of ?certainty stressing 
(preserving)? ? ?positive? and one of ?certainty 
diminishing? ? ?negative? words. Some examples 
of these words are ?probably?, ?likely? ? from the 
list of ?negative? words and ?certainly?, ?abso-
lutely? ? from the list of ?positive? words. 
5.4 Global fitness calculation 
We calculate for every node from the hypothesis 
tree the value of the extended local fitness, and af-
terwards consider the normalized value relative to 
the number of nodes from the hypothesis tree. We 
denote this result by TF (total fitness): 
rNodesNumbeHypothesis
calFitnessExtendedLo
TF Hnode
node
?
=  
After calculating this value, we compute a value 
NV (the negation value) indicating the number of 
verbs with the same value of negation, using the 
following formula: 
rOfVerbsTotalNumbe
rVerbsNumbePositiveNV _=  
where the Positive_VerbsNumber is the number of 
non-negated  verbs from the hypothesis using the 
negation rules, and TotalNumberOfVerbs is the 
total number of verbs from the hypothesis. 
Because the maximum value for the extended 
fitness is 4, the complementary value of the TF is 
4-TF and the formula for the global fitness used is: 
)4(*)1(* TFNVTFNVessGlobalFitn ??+=  
For pair 518 we have the following: 
Initial entity Node 
Fitness 
Extended 
local fitness 
(the, company, det) 1 3.125 
(French, company, nn) 1 3.125 
(railway, company, nn) 1 3.125 
(company, call, s) 1 2.5 
(be, call, be) 1 4 
(call, -, -) 0.096 3.048 
(company, call, obj) 1 1.125 
(SNCF, call, desc) 1 2.625 
Table 2: Entities extended fitness 
                                                          
6
 http://thesaurus.reference.com/ 
129
TF = (3.125 + 3.125 + 3.125 + 2.5 + 4 + 3.048 + 
1.125 + 2.625)/8 = 22.673/8 = 2.834 
NV = 1/1 = 1 
GlobalFitness = 1*2.834+(1?1)*(4-2.834) = 2.834 
Using the development data, we establish a 
threshold value of 2.06. Thus, pair 518 will have 
the answer ?yes?. 
6 Results 
Our system has a different behavior on different 
existing tasks, with higher results on Question An-
swering (0.87) and lower results on Information 
Extraction (0.57). We submitted two runs for our 
system, with different parameters used in calculat-
ing the extended local fitness. However, the results 
are almost the same (see Table 3).  
 IE IR QA SUM Global 
Run01 0.57 0.69 0.87 0.635 0.6913 
Run02 0.57 0.685 0.865 0.645 0.6913 
Table 3: Test results 
To be able to see each component?s relevance, the 
system was run in turn with each component re-
moved. The results in the table below show that the 
system part verifying the NEs is the most impor-
tant.    
System Description Precision Relevance 
Without DIRT 0.6876 0.54 % 
Without WordNet 0.6800 1.63 % 
Without Acronyms 0.6838  1.08 % 
Without BK 0.6775 2.00 % 
Without Negations 0.6763 2.17 % 
Without NEs 0.5758 16.71 % 
Table 4: Components relevance 
7 Conclusions 
The system?s core algorithm is based on the tree 
edit distance approach, however, focused on trans-
forming the hypothesis. It presently uses wide-
spread syntactic analysis tools like Minipar, lexical 
resources like WordNet and LingPipe for Named 
Entities recognition and semantic resources like 
DIRT. The system?s originality resides firstly in 
creating a part-of and equivalence ontology using 
an extraction module for Wikipedia data on NEs 
(the background knowledge), secondly in using a 
distinct database of acronyms from different do-
mains, thirdly acquiring a set of important context 
influencing terms and creating a semantic equiva-
lence set of rules based on English rephrasing con-
cepts and last, but not least, on the technical side, 
using a distributed architecture for time perform-
ance enhancement.   
The approach unveiled some issues related to the 
dependency to parsing tools, for example separat-
ing the verb and the preposition in the case of 
phrasal verbs, resulting in the change of meaning.  
Another issue was identifying expressions that 
change context nuances, which we denoted by 
?positive? or ?negative? words. Although we ap-
plied rules for them, we still require analysis to 
determine their accurate quantification. 
For the future, our first concern is to search for a 
method to establish more precise values for penal-
ties, in order to obtain lower values for pairs with 
No entailment. Furthermore, we will develop a new 
method to determine the multiplication coefficients 
for the parameters in the extended local fitness and 
the global threshold.  
8 Acknowledgements 
The authors thank the members of the NLP group 
in Iasi for their help and support at different stages 
of the system development. Special thanks go to 
Daniel Matei which was responsible for preparing 
all the input data. 
The work on this project is partially financed by 
Siemens VDO Iai and by the CEEX Rotel project 
number 29. 
References 
Dagan, I., Glickman, O., and Magnini, B. 2006. The 
PASCAL Recognising Textual Entailment Challenge. 
In Qui?onero-Candela et al, editors, MLCW 2005, 
LNAI Volume 3944, pages 177-190. Springer-Verlag. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, Mass. 
Kouylekov, M. and Magnini, B. 2005. Recognizing Tex-
tual Entailment with Tree Edit Distance Algorithms. 
In Proceedings of the First Challenge Workshop Rec-
ognising Textual Entailment, Pages 17-20, 25?28 
April, 2005, Southampton, U.K. 
Lin, D. 1998. Dependency-based Evaluation of 
MINIPAR. In Workshop on the Evaluation of Parsing 
Systems, Granada, Spain, May, 1998. 
Lin, D., and Pantel, P. 2001. DIRT - Discovery of Infer-
ence Rules from Text. In Proceedings of ACM Con-
ference on Knowledge Discovery and Data Mining 
(KDD-01). pp. 323-328. San Francisco, CA. 
130
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 189?195,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Sentimatrix ? Multilingual Sentiment Analysis Service 
 
Alexandru-Lucian G?nsc?1, Emanuela Boro?1, Adrian Iftene1, Diana Trandab??1,  
Mihai Toader2, Marius Cor?ci2, Cenel-Augusto Perez1, Dan Cristea1, 3 
1
?Al. I. Cuza? University, Faculty of Computer Science, Iasi, Romania 
2Intelligentics, Cluj-Napoca, Romania 
3Institute of Computer Science, Romanian Academy, Iasi, Romania 
{lucian.ginsca, emanuela.boros, adiftene, dtrandabat, augusto.perez, 
dcristea}@info.uaic.ro, {mtoader, marius}@intelligentics.ro 
 
 
Abstract 
This paper describes the preliminary results 
of a system for extracting sentiments 
opinioned with regard with named entities. 
It also combines rule-based classification, 
statistics and machine learning in a new 
method. The accuracy and speed of 
extraction and classification are crucial. 
The service oriented architecture permits 
the end-user to work with a flexible 
interface in order to produce applications 
that range from aggregating consumer 
feedback on commercial products to 
measuring public opinion on political 
issues from blog and forums. The 
experiment has two versions available for 
testing, one with concrete extraction results 
and sentiment calculus and the other with 
internal metrics validation results. 
1 Motivation 
Nowadays, big companies and organizations spend 
time and money in order to find users? opinions 
about their products, the impact of their marketing 
decisions, or the overall feeling about their support 
and maintenance services. This analysis helps in 
the process of establishing new trends and policies 
and determines in which areas investments must be 
made. One of the focuses of our work is helping 
companies build such analysis in the context of 
users? sentiment identification. Therefore, the 
corpus we work on consists of articles of 
newspapers, blogs, various entries of forums, and 
posts in social networks. 
Sentiment analysis, i.e. the analysis and 
classification of the opinion expressed by a text on 
its subject matter, is a form of information 
extraction from text, which recently focused a lot 
of research and growing commercial interest. 
This paper describes Sentimatrix, a sentiment 
analysis service, doing sentiment extraction and 
associating these analyses with named entities, in 
different languages. We seek to explore how 
sentiment analysis methods perform across 
languages, especially Romanian. The main 
applications that this system experiments with are 
monitoring the Internet before, during and after a 
campaign/message release and obtaining consumer 
feedback on different topics/products. 
In Section 2 we briefly discuss a state of the art 
in sentiment analysis, the system?s architecture is 
described in Section 3 and in Section 4 we focus 
on identifying opinions on Romanian. 
Subsequently, we present the experiment results, 
analysis and discussion in Sections 5 and 6. Future 
work and conclusions are briefly described in 
Section 7. 
2 Sentimatrix compared with state-of-
the-art 
A comprehensive state of the art in the field of 
sentiment analysis, together with potential 
applications of such opinion identification tools, is 
presented in (Pang and Lee, 2008). 
Starting from the early 1990s, the research on 
sentiment-analysis and point of views generally 
assumed the existence of sub-systems for rather 
sophisticated NLP tasks, ranging from parsing to 
the resolution of pragmatic ambiguities (Hearst, 
1992; Wiebe 1990 and 1994). In Sentimatrix, in 
order to identify the sentiment a user expresses 
about a specific product or company, the company 
name must be first identified in the text. Named 
189
entity recognition (NER) systems typically use 
linguistic grammar-based techniques or statistical 
models (an overview is presented in (Nadeau and 
Satoshi Sekine. 2007)). Hand-crafted grammar-
based systems typically obtain better precision, but 
at the cost of lower recall and months of work by 
experienced computational linguists. Besides, the 
task is hard to adapt to new domains. Various 
sentiment types and levels have been considered, 
starting from the ?universal? six level of emotions 
considered in (Ovesdotter Alm, 2005; Liu et al, 
2003; Subasic and Huettner, 2001): anger, disgust, 
fear, happiness, sadness, and surprise. For 
Sentimatrix, we adapted this approach to five 
levels of sentiments: strong positive, positive, 
neutral, negative and strong negative.  
The first known systems relied on relatively 
shallow analysis based on manually built 
discriminative word lexicons (Tong 2001), used to 
classify a text unit by trigger terms or phrases 
contained in a lexicon. The lack of sufficient 
amounts of sentiment annotated corpora led the 
researchers to incorporate learning components 
into their sentiment analysis tools, usually 
supervised classification modules, (e.g., 
categorization according to affect), as initiated in 
(Wiebe and Bruce 1995). 
Much of the literature on sentiment analysis has 
focused on text written in English. Sentimatrix is 
designed to be, as much as possible, language 
independent, the resources used being easily 
adaptable for any language. 
Some of the most known tools available 
nowadays for NER and Opinion Mining are: 
Clarabridge (www.clarabridge.com), RavenPack 
(ravenpack.com), Lexalytics (www.lexalytics.com) 
OpenAmplify (openamplify.com), Radian6 
(www.radian6.com), Limbix (lymbix.com), but 
companies like Google, Microsoft, Oracle, SAS, 
are also deeply involved in this task. 
3 System components 
In Figure 1, the architecture and the main modules 
of our system are presented: preprocessing, named 
entity extraction and opinion identification 
(sentiment extraction per fragment).  
The final production system is based on service 
oriented architecture in order to allow users 
flexible customization and to enable an easier way 
for marketing technology. Each module of the 
system (Segmenter, Tokenizer, Language Detector, 
Entity Extractor, and Sentiment Extractor) can be 
exposed in a user-friendly interface.  
 
Figure 1. System architecture 
3.1 Preprocessing 
The preprocessing phase is made out of a text 
segmentator and a tokenizer. Given a text, we 
divide it into paragraphs, every paragraph is split 
into sentences, and every phrase is tokenized. Each 
token is annotated with two pieces of information: 
its lemma (for Romanian it is obtained from our 
resource with 76,760 word lemmas corresponding 
to 633,444 derived forms) and the normalized form 
(translated into the proper diacritics1). 
3.2 Language Detection 
Language detection is a preprocessing step 
problem of classifying a sample of characters 
based on its features (language-specific models). 
Currently, the system supports English, Romanian 
and Romanian without Diacritics. This step is 
needed in order to correctly identify a sentiment or 
a sentiment modifier, as the named entity detection 
depends on this. We combined three methods for 
                                                          
1
 In Romanian online texts, two diacritics are commonly used, 
but only one is accepted by the official grammar. 
190
identifying the language: N-grams detection, 
strictly 3-grams detection and lemma correction.  
The 3-grams classification method uses corpus 
from Apache Tika for several languages. The 
Romanian 3-gram profile for this method was 
developed from scratch, using our articles archive. 
The language detection in this case performs 
simple distance measurement between every 
language profile that we have and the test 
document profile. The N-grams classification 
method implies, along with computing frequencies, 
a posterior Naive Bayes implementation. The third 
method solves the problematic issue of short 
phrases language detection and it implies looking 
through the lemmas of several words to obtain the 
specificity of the test document. 
3.3 Named Entity Recognition 
The Named Entity Recognition component for 
Romanian language is created using linguistic 
grammar-based techniques and a set of resources. 
Our component is based on two modules, the 
named entity identification module and the named 
entity classification module. After the named entity 
candidates are marked for each input text, each 
candidate is classified into one of the considered 
categories, such as Person, Organization, Place, 
Country, etc. 
 
Named Entity Extraction: After the pre-
processing step, every token written with a capital 
letter is considered to be a named entity candidate.  
For tokens with capital letters which are the first 
tokens in phrases, we consider two situations:  
1. this first token of a phrase is in our stop word 
list (in this case we eliminate it from the 
named entities candidate list),  
2. the first token of a phrase is in our common 
word list. In the second situation there are 
considered two cases:  
a. this common word is followed by lowercase 
words (then we check if the common word 
can be found in the list of trigger words, like 
university, city, doctor, etc.),  
b. this common word is followed by uppercase 
words (in this case the first word of the 
sentence is kept in the NEs candidate list, 
and in a further step it will be decided if it 
will be combined with the following word in 
order to create a composed named entity).  
Named Entities Classification: In the 
classification process we use some of rules utilized 
in the unification of NEs candidates along with the 
resource of NEs and several rules specifically 
tailored for classification. Thus, after all NEs in the 
input text are identified and, if possible, compound 
NEs have been created, we apply the following 
classification rules: contextual rules (using 
contextual information, we are able to classify 
candidate NEs in one of the categories 
Organization, Company, Person, City and Country 
by considering a mix between regular expressions 
and trigger words) and resource-based rules (if no 
triggers were found to indicate what type of entity 
we have, we start searching our databases for the 
candidate entity). 
 
Evaluation: The system?s Upper Bound and its 
performance in real context are evaluated for each 
of the two modules (identification and 
classification) and for each named entity type. The 
first part of the evaluation shows an upper bound 
of 95.76% for F-measure at named entity 
extraction and 95.71% for named entity 
classification. In real context the evaluation shows 
a value of 90.72% for F-measure at named entity 
extraction and a value of 66.73% for named entity 
classification. The results are very promising, and 
they are being comparable with the existing 
systems for Romanian, and even better for Person 
recognition. 
4 Identify users opinions on Romanian 
4.1 Resources 
In such a task as sentiment identification, linguistic 
resources play a very important role. The core 
resource is a manually built list of words and 
groups of words that semantically signal a positive 
or a negative sentiment. From now on, we will 
refer to such a word or group of words as 
?sentiment trigger?. Certain weights have been 
assigned to these words after multiple revisions. 
The weights vary from -3, meaning strong negative 
to +3, which translates to a strong positive. There 
are a total of 3,741 sentiment triggers distributed to 
weight groups as can be observed in Figure 2. The 
triggers are lemmas, so the real number of words 
that can be identified as having a sentiment value 
is much higher. 
191
This list is not closed and it suffers modifications, 
especially by adding new triggers, but in certain 
cases, if a bad behavior is observed, the weights 
may also be altered.  
 
 
Figure 2. Number of sentiment words by weight groups 
 
We define a modifier as a word or a group of 
words that can increase or diminish the intensity of 
a sentiment trigger. We have a manually built list 
of modifiers. We consider negation words a special 
case of modifiers that usually have a greater impact 
on sentiment triggers. So, we also built a small list 
of negation words.   
4.2 Formalism 
General definitions: We define a sentiment 
segment as follows: 
  = (	
, , 		) 
 
sSG is a tuple in which the first two elements are 
optional.  
Let NL be the set of negation words that we use, 
ML the set of modifiers and TL the set of sentiment 
triggers.  We define two partially ordered sets: 
  = (, ?), 