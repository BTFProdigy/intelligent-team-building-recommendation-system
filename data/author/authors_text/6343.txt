Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of the ACL Student Research Workshop, pages 13?18,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
An Extensive Empirical Study of Collocation Extraction Methods
Pavel Pecina
Institute of Formal and Applied Linguistics
Charles University, Prague, Czech Republic
pecina@ufal.mff.cuni.cz
Abstract
This paper presents a status quo of an
ongoing research study of collocations ?
an essential linguistic phenomenon hav-
ing a wide spectrum of applications in
the field of natural language processing.
The core of the work is an empirical eval-
uation of a comprehensive list of auto-
matic collocation extraction methods us-
ing precision-recall measures and a pro-
posal of a new approach integrating mul-
tiple basic methods and statistical classi-
fication. We demonstrate that combining
multiple independent techniques leads to
a significant performance improvement in
comparisonwith individualbasic methods.
1 Introduction and motivation
Natural language cannot be simply reduced to lex-
icon and syntax. The fact that individual words
cannot be combined freely or randomly is common
for most natural languages. The ability of a word
to combine with other words can be expressed ei-
ther intensionally or extensionally. The former case
refers to valency. Instances of the latter case are
called collocations ( ?Cerm?k and Holub, 1982). The
term collocation has several other definitions but
none of them is widely accepted. Most attempts
are based on a characteristic property of colloca-
tions: non-compositionality. Choueka (1988) de-
fines a collocational expression as ?a syntactic and
semantic unit whose exact and unambiguous mean-
ing or connotation cannot be derived directly from
the meaning or connotation of its components?.
The term collocation has both linguistic and lexi-
cographic character. It covers a wide range of lexical
phenomena, such as phrasal verbs, light verb com-
pounds, idioms, stock phrases, technological ex-
pressions, and proper names. Collocations are of
high importance for many applications in the field
of NLP. The most desirable ones are machine trans-
lation, word sense disambiguation, language genera-
tion, and information retrieval. The recent availabil-
ity of large amounts of textual data has attracted in-
terest in automatic collocation extraction from text.
In the last thirty years a number of different methods
employing various association measures have been
proposed. Overview of the most widely used tech-
niques is given e.g. in (Manning and Sch?tze, 1999)
or (Pearce, 2002). Several researches also attempted
to compare existing methods and suggested different
evaluation schemes, e.g Kita (1994) or Evert (2001).
A comprehensive study of statistical aspects of word
cooccurrences can be found in (Evert, 2004).
In this paper we present a compendium of 84
methods for automatic collocation extraction. They
came from different research areas and some of them
have not been used for this purpose yet. A brief
overview of these methods is followed by their com-
parative evaluation against manually annotated data
by the means of precision and recall measures. In
the end we propose a statistical classification method
for combining multiple methods and demonstrate a
substantial performance improvement.
In our research we focus on two-word (bigram)
collocations, mainly for the reason that experiments
with longer expressions would require processing of
much larger amounts of data and limited scalability
of some methods to high order n-grams. The exper-
iments are performed on Czech data.
13
2 Collocation extraction
Most methods for collocation extraction are based
on verification of typical collocation properties.
These properties are formally described by mathe-
matical formulas that determine the degree of as-
sociation between components of collocation. Such
formulas are called association measures and com-
pute an association score for each collocation candi-
date extracted from a corpus. The scores indicate a
chance of a candidate to be a collocation. They can
be used for ranking or for classification ? by setting
a threshold. Finding such a threshold depends on the
intended application.
The most widely tested property of collocations is
non-compositionality: If words occur together more
often than by a chance, then this is the evidence that
they have a special function that is not simply ex-
plained as a result of their combination (Manning
and Sch?tze, 1999). We think of a corpus as a ran-
domly generated sequence of words that is viewed as
a sequence of word pairs. Occurrence frequencies
of these bigrams are extracted and kept in contin-
gency tables (Table 1a). Values from these tables are
used in several association measures that reflect how
much the word coocurrence is accidental. A list of
such measures is given in Table 2 and includes: es-
timation of bigram and unigram probabilities (rows
3?5), mutual information and derived measures (6?
11), statistical tests of independence (12?16), likeli-
hood measures (17?18), and various other heuristic
association measures and coefficients (19?57).
Another frequently tested property is taken di-
rectly from the definition that a collocation is a syn-
tactic and semantic unit. For each bigram occurring
in the corpus, information of its empirical context
(frequencies of open-class words occurring within
a specified context window) and left and right im-
mediate contexts (frequencies of words immediately
preceding or following the bigram) is extracted (Ta-
ble 1b). By determining the entropy of the im-
mediate contexts of a word sequence, the associa-
tion measures rank collocations according to the as-
sumption that they occur as units in a (information-
theoretically) noisy environment (Shimohata et al,
1997) (58?62). By comparing empirical contexts of
a word sequence and its components, the associa-
tion measures rank collocations according to the as-
a) a=f(xy) b=f(xy?) f(x?)
c=f(x?y) d=f(x?y?) f(x??)
f(?y) f(?y?) N
b) Cw empirical context of w
Cxy empirical context of xy
Clxy left immediate context of xy
Crxy right immediate context of xy
Table 1: a) A contingency table with observed frequencies and
marginal frequencies for a bigram xy; w? stands for any word
except w; ? stands for any word; N is a total number of bi-
grams. The table cells are sometimes referred as fij . Statistical
tests of independence work with contingency tables of expected
frequencies f?(xy)=f(x?)f(?y)/N . b) Different notions of em-
pirical contexts.
sumption that semantically non-compositional ex-
pressions typically occur in different contexts than
their components (Zhai, 1997). Measures (63?76)
have information theory background and measures
(77?84) are adopted from the field of information
retrieval. Context association measures are mainly
used for extracting idioms.
Besides all the association measures described
above, we also take into account other recommended
measures (1?2) (Manning and Sch?tze, 1999) and
some basic linguistic characteristics used for filter-
ing non-collocations (85?87). This information can
be obtained automatically from morphological tag-
gers and syntactic parsers available with reasonably
high accuracy for many languages.
3 Empirical evaluation
Evaluation of collocation extraction methods is a
complicated task. On one hand, different applica-
tions require different setting of association score
thresholds. On the other hand, methods give differ-
ent results within different ranges of their associa-
tion scores. We need a complex evaluation scheme
covering all demands. In such a case, Evert (2001)
and other authors suggest using precision and recall
measures on a full reference data or on n-best lists.
Data. All the presented experiments were per-
formed on morphologically and syntactically anno-
tated Czech text from the Prague Dependency Tree-
bank (PDT) (Hajic? et al, 2001). Dependency trees
were broken down into dependency bigrams consist-
ing of: lemmas and part-of-speech of the compo-
nents, and type of dependence between the compo-
nents.
For each bigram type we counted frequencies in
its contingency table, extracted empirical and imme-
diate contexts, and computed all the 84 association
measures from Table 2. We processed 81 614 sen-
14
# Name Formula
1. Mean component offset 1n
Pn
i=1 di
2. Variance component offset 1n?1
Pn
i=1
`
di?d?
?2
3. Joint probability P (xy)
4. Conditional probability P (y|x)
5. Reverse conditional prob. P (x|y)
?6. Pointwise mutual inform. log P (xy)P (x?)P (?y)
7. Mutual dependency (MD) log P (xy)2P (x?)P (?y)
8. Log frequency biased MD log P (xy)
2
P (x?)P (?y)+logP (xy)
9. Normalized expectation 2f(xy)f(x?)+f(?y)
?10. Mutual expectation 2f(xy)f(x?)+f(?y) ?P (xy)
11. Salience log P (xy)
2
P (x?)P (?y) ? logf(xy)
12. Pearson?s ?2 test Pi,j
(fij?f?ij)
2
f?ij
13. Fisher?s exact test f(x?)!f(x??)!f(?y)!f(?y?)!N!f(xy)!f(xy?)!f(x?y)!f(x?y?)!
14. t test f(xy)?f?(xy)?
f(xy)(1?(f(xy)/N))
15. z score f(xy)?f?(xy)?
f?(xy)(1?(f?(xy)/N))
16. Poison significance measure f?(xy)?f(xy) logf?(xy)+logf(xy)!logN
17. Log likelihood ratio ?2Pi,jfij log
fij
f?ij
18. Squared log likelihood ratio ?2Pi,j
logfij
2
f?ij
Association coefficients:
19. Russel-Rao aa+b+c+d
20. Sokal-Michiner a+da+b+c+d
?21. Rogers-Tanimoto a+da+2b+2c+d
22. Hamann (a+d)?(b+c)a+b+c+d
23. Third Sokal-Sneath b+ca+d
24. Jaccard aa+b+c
?25. First Kulczynsky ab+c
26. Second Sokal-Sneath aa+2(b+c)
27. Second Kulczynski 12 (
a
a+b +
a
a+c )
28. Fourth Sokal-Sneath 14 (
a
a+b +
a
a+c +
d
d+b +
d
d+c )
29. Odds ratio adbc
30. Yulle?s ?
?
ad?
?
bc?
ad+
?
bc
?31. Yulle?s Q ad?bcad+bc
32. Driver-Kroeber a?
(a+b)(a+c)
33. Fifth Sokal-Sneath ad?
(a+b)(a+c)(d+b)(d+c)
34. Pearson ad?bc?
(a+b)(a+c)(d+b)(d+c)
35. Baroni-Urbani a+
?
ad
a+b+c+
?
ad
36. Braun-Blanquet amax(a+b,a+c)
37. Simpson amin(a+b,a+c)
38. Michael 4(ad?bc)
(a+d)2+(b+c)2
39. Mountford 2a2bc+ab+ac
40. Fager a?
(a+b)(a+c)
? 12max(b, c)
41. Unigram subtuples log adbc ?3.29
q
1
a +
1
b +
1
c +
1
d
42. U cost log(1+ min(b,c)+amax(b,c)+a )
43. S cost log(1+min(b,c)a+1 )
?12
44. R cost log(1+ aa+b )?log(1+
a
a+c )
45. T combined cost ?U?S?R
46. Phi P (xy)?P (x?)P (?y)?
P (x?)P (?y)(1?P (x?))(1?P (?y))
47. Kappa P (xy)+P (x?y?)?P (x?)P (?y)?P (x??)P (?y?)1?P (x?)P (?y)?P (x??)P (?y?)
48. J measure max[P (xy)logP (y|x)P (?y) +P (xy?)log
P (y?|x)
P (?y?) ,
P (xy)log
P (x|y)
P (x?) +P (x?y)log
P (x?|y)
P (x??) ]
# Name Formula
49. Gini index max[P (x?)(P (y|x)2+P (y?|x)2)?P (?y)2
+P (x??)(P (y|x?)2+P (y?|x?)2)?P (?y?)2,
P (?y)(P (x|y)2+P (x?|y)2)?P (x?)2
+P (?y?)(P (x|y?)2+P (x?|y?)2)?P (x??)2]
50. Confidence max[P (y|x), P (x|y)]
51. Laplace max[NP (xy)+1NP (x?)+2 ,
NP (xy)+1
NP (?y)+2 ]
52. Conviction max[P (x?)P (?y)P (xy?) ,
P (x??)P (?y)
P (x?y) ]
53. Piatersky-Shapiro P (xy)?P (x?)P (?y)
54. Certainity factor max[P (y|x)?P (?y)1?P (?y) ,
P (x|y)?P (x?)
1?P (x?) ]
55. Added value (AV) max[P (y|x)?P (?y), P (x|y)?P (x?)]
?56. Collective strength P (xy)+P (x?y?)P (x?)P (y)+P (x??)P (?y) ?
1?P (x?)P (?y)?P (x??)P (?y)
1?P (xy)?P (x?y?)
57. Klosgen
p
P (xy) ?AV
Context measures:
?58. Context entropy ?Pw P (w|Cxy) logP (w|Cxy)
59. Left context entropy ?Pw P (w|Clxy) logP (w|Clxy)
60. Right context entropy ?Pw P (w|Crxy) logP (w|Crxy)
?61. Left context divergence P (x?) logP (x?)
?
P
wP (w|C
l
xy) logP (w|C
l
xy)
62. Right context divergence P (?y) logP (?y)
?
P
wP (w|C
r
xy) logP (w|C
r
xy)
63. Cross entropy ?PwP (w|Cx) logP (w|Cy)
64. Reverse cross entropy ?PwP (w|Cy) logP (w|Cx)
65. Intersection measure 2|Cx?Cy||Cx|+|Cy|
66. Euclidean norm
qP
w(P (w|Cx)?P (w|Cy))
2
67. Cosine norm
P
w P (w|Cx)P (w|Cy)
P
w P (w|Cx)
2?
P
w P (w|Cy)
2
68. L1 norm Pw |P (w|Cx)?P (w|Cy)|
69. Confusion probability Pw
P (x|Cw)P (y|Cw)P (w)
P (x?)
70. Reverse confusion prob. Pw
P (y|Cw)P (x|Cw)P (w)
P (?y)
?71. Jensen-Shannon diverg. 12 [D(p(w|Cx)|| 12 (p(w|Cx)+p(w|Cy)))
+D(p(w|Cy)||
1
2 (p(w|Cx)+p(w|Cy)))]
72. Cosine of pointwise MI
P
w MI(w,x)MI(w,y)?P
w MI(w,x)
2?
?P
w MI(w,y)
2
?73. KL divergence Pw P (w|Cx) log
P (w|Cx)
P (w|Cy)
?74. Reverse KL divergence Pw P (w|Cy) log
P (w|Cy)
P (w|Cx)
75. Skew divergence D(p(w|Cx)||?(w|Cy)+(1??)p(w|Cx))
76. Reverse skew divergence D(p(w|Cy)||?p(w|Cx)+(1??)p(w|Cy))
77. Phrase word coocurrence 12 (
f(x|Cxy)
f(xy) +
f(y|Cxy)
f(xy) )
78. Word association 12 (
f(x|Cy)?f(xy)
f(xy) +
f(y|Cx)?f(xy)
f(xy) )
Cosine context similarity: 12 (cos(cx,cxy)+cos(cy,cxy))
cz=(zi); cos(cx,cy)=
P
xiyi?P
xi
2?
?P
yi
2
?79. in boolean vector space zi=?(f(wi|Cz))
80. in tf vector space zi=f(wi|Cz)
81. in tf?idf vector space zi=f(wi|Cz)? Ndf(wi); df(wi)= |{x :wi?Cx}|
Dice context similarity: 12 (dice(cx,cxy)+dice(cy,cxy))
cz=(zi); dice(cx,cy)=
2
P
xiyiP
xi
2+
P
yi
2
?82. in boolean vector space zi=?(f(wi|Cz))
?83. in tf vector space zi=f(wi|Cz)
?84. in tf?idf vector space zi=f(wi|Cz)? Ndf(wi); df(wi)= |{x :wi?Cx}|
Linguistic features:
?85. Part of speech {Adjective:Noun, Noun:Noun, Noun:Verb, . . . }
?86. Dependency type {Attribute, Object, Subject, . . . }
87. Dependency structure {?,?}
Table 2: Association measures and linguistic features used in bigram collocation extraction methods. ? denotes those selected by
the attribute selection method discussed in Section 4. References can be found at the end of the paper.
15
tences with 1 255 590 words and obtained a total of
202 171 different dependency bigrams.
Krenn (2000) argues that collocation extraction
methods should be evaluated against a reference set
of collocations manually extracted from the full can-
didate data from a corpus. However, we reduced the
full candidate data from PDT to 21 597 bigram by
filtering out any bigrams which occurred 5 or less
times in the data and thus we obtained a reference
data set which fulfills requirements of a sufficient
size and a minimal frequency of observations which
is needed for the assumption of normal distribution
required by some methods.
We manually processed the entire reference data
set and extracted bigrams that were considered to be
collocations. At this point we applied part-of-speech
filtering: First, we identified POS patterns that never
form a collocation. Second, all dependency bigrams
having such a POS pattern were removed from the
reference data and a final reference set of 8 904 bi-
grams was created. We no longer consider bigrams
with such patterns to be collocation candidates.
This data set contained 2 649 items considered to
be collocations. The a priori probability of a bi-
gram to be a collocation was 29.75 %. A strati-
fied one-third subsample of this data was selected
as test data and used for evaluation and testing pur-
poses in this work. The rest was taken apart and used
as training data in later experiments.
Evaluation metrics. Since we manually anno-
tated the entire reference data set we could use the
suggested precision and recall measures (and their
harmonic mean F-measure). A collocation extrac-
tion method using any association measure with a
given threshold can be considered a classifier and
the measures can be computed in the following way:
Precision =
# correctly classified collocations
# total predicted as collocations
Recall =
# correctly classified collocations
# total collocations
The higher these scores, the better the classifier is.
By changing the threshold we can tune the clas-
sifier performance and ?trade? recall for precision.
Therefore, collocation extraction methods can be
thoroughly compared by comparing their precision-
-recall curves: The closer the curve to the top right
corner, the better the method is.
 100
 90
 80
 60
 30
100806040200
P
re
ci
si
on
 (
%
)
Recall (%)
baseline = 29.75 %
Pointwise mutual information
Pearson?s test
Mountford
Kappa
Left context divergence
Context intersection measure
Cosine context similarity in boolean VS
Figure 1: Precision-recall curves for selected assoc. measures.
Results. Presenting individual results for all of
the 84 association measures is not possible in a paper
of this length. Therefore, we present precision-recall
graphs only for the best methods from each group
mentioned in Section 2; see Figure 1. The baseline
system that classifies bigrams randomly, operates
with a precision of 29.75 %. The overall best re-
sult was achieved by Pointwise mutual information:
30 % recall with 85.5 % precision (F-measure 44.4),
60 % recall with 78.4 % precision (F-measure 68.0),
and 90 % recall with 62.5 % precision (F-measure
73.8).
4 Statistical classification
In the previous section we mentioned that collo-
cation extraction is a classification problem. Each
method classifies instances of the candidate data set
according to the values of an association score. Now
we have several association scores for each candi-
date bigram and want to combine them together to
achieve better performance. A motivating example
is depicted in Figure 3: Association scores of Point-
wise mutual information and Cosine context simi-
larity are independent enough to be linearly com-
bined to provide better results. Considering all as-
sociation measures, we deal with a problem of high-
dimensional classification into two classes.
In our case, each bigram x is described by the
attribute vector x=(x1, . . . , x87) consisting of lin-
guistic features and association scores from Table 2.
Now we look for a function assigning each bigram
one class : f(x)?{collocation, non-collocation}.
The result of this approach is similar to setting a
threshold of the association score in methods us-
16
0.9
0.5
0.1
16.98.80.7
C
os
in
e 
co
nte
xt
 s
im
ila
ri
ty
 in
 b
oo
le
an
 ve
cto
r
 s
pac
e
Pointwise mutual information
collocations
non-collocations
linear discriminant
Figure 2: Data visualization in two dimensions. The dashed line
denotes a linear discriminant obtained by logistic linear regres-
sion. By moving this boundary we can tune the classifier output
(a 5 % stratified sample of the test data is displayed).
ing one association measure, which is not very use-
full for our purpose. Some classification meth-
ods, however, output also the predicted probability
P (x is collocation) that can be considered a regular
association measure as described above. Thus, the
classification method can be also tuned by changing
a threshold of this probability and can be compared
with other methods by the same means of precision
and recall.
One of the basic classification methods that gives
a predicted probability is Logistic linear regression.
The model defines the predicted probability as:
P (x is collocation) =
exp?0+?1x1...+?nxn
1 + exp?0+?1x1...+?nxn
where the coefficients ?i are obtained by the iter-
atively reweighted least squares (IRLS) algorithm
which solves the weighted least squares problem
at each iteration. Categorial attributes need to be
transformed to numeric dummy variables. It is also
recommended to normalize all numeric attributes to
have zero mean and unit variance.
We employed the datamining software Weka by
Witten and Frank (2000) in our experiments. As
training data we used a two-third subsample of the
reference data described above. The test data was
the same as in the evaluation of the basic methods.
By combining all the 87 attributes, we achieved
the results displayed in Table 3 and illustrated in Fig-
ure 3. At a recall level of 90 % the relative increase
in precision was 35.2 % and at a precision level of
90 % the relative increase in recall was impressive
242.3 %.
 100
 90
 80
 60
 30
100806040200
P
re
ci
si
on
 (
%
)
Recall (%)
baseline = 29.75 %
Logistic regression on all attributes
Logistic regression on 17 selected attributes
Figure 3: Precision-recall curves of two classifiers based on
i) logistic linear regression on the full set of 87 attributes and
ii) on the selected subset with 17 attributes. The thin unlabeled
curves refer to the methods from the 17 selected attributes
Attribute selection. In the final step of our exper-
iments, we attempted to reduce the attribute space of
our data and thus obtain an attribute subset with the
same prediction ability. We employed a greedy step-
wise search method with attribute subset evaluation
via logistic regression implemented in Weka. It per-
forms a greedy search through the space of attribute
subsets and iteratively merges subsets that give the
best results until the performance is no longer im-
proved.
We ended up with a subset consisting of the fol-
lowing 17 attributes: (6, 10, 21, 25, 31, 56, 58, 61, 71,
73, 74, 79, 82, 83, 84, 85, 86) which are also marked in
Table 2. The overview of achieved results is shown
in Table 3 and precision-recall graphs of the selected
attributes and their combinations are in Figure 3.
5 Conclusions and future work
We implemented 84 automatic collocation extrac-
tion methods and performed series of experiments
on morphologically and syntactically annotated
data. The methods were evaluated against a refer-
ence set of collocations manually extracted from the
Recall Precision
30 60 90 70 80 90
P. mutual information 85.5 78.4 62.5 78.0 56.0 16.3
Logistic regression-17 92.6 89.5 84.5 96.7 86.7 55.8
Absolute improvement 7.1 11.1 22.0 17.7 30.7 39.2
Relative improvement 8.3 14.2 35.2 23.9 54.8 242.3
Table 3: Precision (the 3 left columns) and recall (the 3 right
columns) scores (in %) for the best individual method and linear
combination of the 17 selected ones.
17
same source. The best method (Pointwise mutual in-
formation) achieved 68.3 % recall with 73.0 % pre-
cision (F-measure 70.6) on this data. We proposed
to combine the association scores of each candidate
bigram and employed Logistic linear regression to
find a linear combination of the association scores
of all the basic methods. Thus we constructed a col-
location extraction method which achieved 80.8 %
recall with 84.8 % precision (F-measure 82.8). Fur-
thermore, we applied an attribute selection tech-
nique in order to lower the high dimensionality of
the classification problem and reduced the number
of regressors from 87 to 17 with comparable perfor-
mance. This result can be viewed as a kind of evalu-
ation of basic collocation extraction techniques. We
can obtain the smallest subset that still gives the best
result. The other measures therefore become unin-
teresting and need not be further processed and eval-
uated.
The reseach presented in this paper is in progress.
The list of collocation extraction methods and as-
sociation measures is far from complete. Our long
term goal is to collect, implement, and evaluate all
available methods suitable for this task, and release
the toolkit for public use.
In the future, we will focus especially on im-
proving quality of the training and testing data, em-
ploying other classification and attribute-selection
techniques, and performing experiments on English
data. A necessary part of the work will be a rigorous
theoretical study of all applied methods and appro-
priateness of their usage. Finally, we will attempt to
demonstrate contribution of collocations in selected
application areas, such as machine translation or in-
formation retrieval.
Acknowledgments
This research has been supported by the Ministry
of Education of the Czech Republic, project MSM
0021620838. I would also like to thank my advisor,
Dr. Jan Hajic?, for his continued support.
References
Y. Choueka. 1988. Looking for needles in a haystack or lo-
cating interesting collocational expressions in large textual
databases. In Proceedings of the RIAO, pages 43?38.
I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-based models
of word cooccurrence probabilities. Machine Learning, 34.
T. E. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguistics,
19(1):61?74.
S. Evert and B. Krenn. 2001. Methods for the qualitative eval-
uation of lexical association measures. In Proceedings 39th
Annual Meeting of the Association for Computational Lin-
guistics, pages 188?195.
S. Evert. 2004. The Statistics of Word Cooccurrences: Word
Pairs and Collocations. Ph.D. thesis, University of Stuttgart.
J. Hajic?, E. Hajic?ov?, P. Pajas, J. Panevov?, P. Sgall, and
B. Vidov?-Hladk?. 2001. Prague dependency treebank 1.0.
Published by LDC, University of Pennsylvania.
K. Kita, Y. Kato, T. Omoto, and Y. Yano. 1994. A comparative
study of automatic extraction of collocations from corpora:
Mutual information vs. cost criteria. Journal of Natural Lan-
guage Processing, 1(1):21?33.
B. Krenn. 2000. Collocation Mining: Exploiting Corpora for
Collocation Idenfication and Representation. In Proceedings
of KONVENS 2000.
L. Lee. 2001. On the effectiveness of the skew divergence
for statistical language analysis. Artificial Inteligence and
Statistics, pages 65?72.
C. D. Manning and H. Sch?tze. 1999. Foundations of Statis-
tical Natural Language Processing. The MIT Press, Cam-
bridge, Massachusetts.
D. Pearce. 2002. A comparative evaluation of collocation ex-
traction techniques. In Third International Conference on
language Resources and Evaluation, Las Palmas, Spain.
T. Pedersen. 1996. Fishing for exactness. In Proceedings of
the South Central SAS User?s Group Conference, pages 188?
200, Austin, TX.
S. Shimohata, T. Sugio, and J. Nagata. 1997. Retrieving col-
locations by co-occurrences and word order constraints. In
Proc. of the 35th Annual Meeting of the ACL and 8th Con-
ference of the EACL, pages 476?81, Madrid. Spain.
P. Tan, V. Kumar, and J. Srivastava. 2002. Selecting the right
interestingness measure for association patterns. In Proceed-
ings of the Eight A CM SIGKDD International Conference
on Knowledge Discovery and Data Mining.
A. Thanopoulos, N. Fakotakis, and G. Kokkinakis. 2002. Com-
parative evaluation of collocation extraction metrics. In 3rd
International Conference on Language Resources and Eval-
uation, volume 2, pages 620?625, Las Palmas, Spain.
F. ?Cerm?k and J. Holub. 1982. Syntagmatika a paradigmatika
c?esk eho slova: Valence a kolokabilita. St?tn? pedagogick?
nakladatelstv?, Praha.
I. H. Witten and E. Frank. 2000. Data Mining: Practical
machine learning tools with Java implementations. Morgan
Kaufmann, San Francisco.
C. Zhai. 1997. Exploiting context to identify lexical atoms
? A statistical view of linguistic context. In International
and Interdisciplinary Conference on Modelling and Using
Context (CONTEXT-97).
18
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 945?952,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Leveraging Reusability: Cost-effective Lexical Acquisition 
for Large-scale Ontology Translation 
 
G. Craig Murray 
Bonnie J. Dorr 
Jimmy Lin 
Institute for Advanced Computer Studies 
University of Maryland 
{gcraigm,bdorr,jimmylin}@umd.edu 
Jan Haji? 
Pavel Pecina 
 
Institute for Formal and Applied Linguistics 
Charles University 
{hajic,pecina}@ufal.mff.cuni.cz 
 
  
 
Abstract 
Thesauri and ontologies provide impor-
tant value in facilitating access to digital 
archives by representing underlying prin-
ciples of organization.  Translation of 
such resources into multiple languages is 
an important component for providing 
multilingual access.  However, the speci-
ficity of vocabulary terms in most on-
tologies precludes fully-automated ma-
chine translation using general-domain 
lexical resources.  In this paper, we pre-
sent an efficient process for leveraging 
human translations when constructing 
domain-specific lexical resources.  We 
evaluate the effectiveness of this process 
by producing a probabilistic phrase dic-
tionary and translating a thesaurus of 
56,000 concepts used to catalogue a large 
archive of oral histories.  Our experi-
ments demonstrate a cost-effective tech-
nique for accurate machine translation of 
large ontologies. 
1 Introduction 
Multilingual access to digital collections is an 
important problem in today?s increasingly inter-
connected world.  Although technologies such as 
cross-language information retrieval and ma-
chine translation help humans access information 
they could not otherwise find or understand, they 
are often inadequate for highly specific domains. 
Most digital collections of any significant size 
use a system of organization that facilitates easy 
access to collection contents. Generally, the or-
ganizing principles are captured in the form of a 
controlled vocabulary of keyword phrases (de-
scriptors) representing specific concepts.  These 
descriptors are usually arranged in a hierarchic 
thesaurus or ontology, and are assigned to collec-
tion items as a means of providing access (either 
via searching for keyword phases, browsing the 
hierarchy, or a combination both).  MeSH (Medi-
cal Subject Headings) serves as a good example 
of such an ontology; it is a hierarchically-
arranged collection of controlled vocabulary 
terms manually assigned to medical abstracts in a 
number of databases.  It provides multilingual 
access to the contents of these databases, but 
maintaining translations of such a complex struc-
ture is challenging (Nelson, et al 2004). 
For the most part, research in multilingual in-
formation access focuses on the content of digital 
repositories themselves, often neglecting signifi-
cant knowledge that is explicitly encoded in the 
associated ontologies.  However, information 
systems cannot utilize such ontologies by simply 
applying off-the-shelf machine translation. Gen-
eral-purpose translation resources provide insuf-
ficient coverage of the vocabulary contained 
within these domain-specific ontologies. 
This paper tackles the question of how one 
might efficiently translate a large-scale ontology 
to facilitate multilingual information access.  If 
we need humans to assist in the translation proc-
ess, how can we maximize access while mini-
mizing cost?  Because human translation is asso-
ciated with a certain cost, it is preferable not to 
incur costs of retranslation whenever compo-
nents of translated text are reused. Moreover, 
when exhaustive human translation is not practi-
cal, the most ?useful? components should be 
translated first.  Identifying reusable elements 
and prioritizing their translation based on utility 
is essential to maximizing effectiveness and re-
ducing cost. 
945
We present a process of prioritized translation 
that balances the issues discussed above.  Our 
work is situated in the context of the MALACH 
project, an NSF-funded effort to improve multi-
lingual information access to large archives of 
spoken language (Gustman, et al, 2002).  Our 
process leverages a small set of manually-
acquired English-Czech translations to translate a 
large ontology of keyword phrases, thereby pro-
viding Czech speakers access to 116,000 hours 
of video testimonies in 32 languages. Starting 
from an initial out-of-vocabulary (OOV) rate of 
85%, we show that a small set of prioritized 
translations can be elicited from human infor-
mants, aligned, decomposed and then recom-
bined to cover 90% of the access value in a com-
plex ontology.  Moreover, we demonstrate that 
prioritization based on hierarchical position and 
frequency of use facilitates extremely efficient 
reuse of human input.  Evaluations show that our 
technique is able to boost performance of a sim-
ple translation system by 65%. 
2 The Problem 
The USC Shoah Foundation Institute for Vis-
ual History and Education manages what is pres-
ently the world's largest archive of videotaped 
oral histories (USC, 2006). The archive contains 
116,000 hours of video from the testimonies of 
over 52,000 survivors, liberators, rescuers and 
witnesses of the Holocaust.  If viewed end to 
end, the collection amounts to 13 years of con-
tinuous video.  The Shoah Foundation uses a hi-
erarchically arranged thesaurus of 56,000 key-
word phrases representing domain-specific con-
cepts.  These are assigned to time-points in the 
video testimonies as a means of indexing the 
video content.  Although the testimonies in the 
collection represent 32 different languages, the 
thesaurus used to catalog them is currently avail-
able only in English.  Our task was to translate 
this resource to facilitate multilingual access, 
with Czech as the first target language. 
Our first pass at automating thesaurus transla-
tion revealed that only 15% of the words in the 
vocabulary could be found in an available 
aligned corpus (?mejrek, et al, 2004).  The rest 
of the vocabulary was not available from general 
resources.  Lexical information for translating 
these terms had to be acquired from human in-
put.  Reliable access to digital archives requires 
accuracy. Highly accurate human translations 
incur a cost that is generally proportional to the 
number of words being translated.  However, the 
keyword phrases in the Shoah Foundation?s ar-
chive occur in a Zipfian distribution?a rela-
tively small number of terms provide access to a 
large portion of the video content.  Similarly, a 
great number of highly specific terms describe 
only a small fraction of content.  Therefore, not 
every keyword phrase in the thesaurus carries the 
same value for access to the archive.  The hierar-
chical arrangement of keyword phrases presents 
another issue: some concepts, while not of great 
value for access to segments of video, may be 
important for organizing other concepts and for 
browsing the hierarchy.  These factors must be 
balanced in developing a cost-effective process 
that maximizes utility. 
3 Our Solution 
This paper presents a prioritized human-in-the-
loop approach to translating large-scale ontolo-
gies that is fast, efficient, and cost effective.  Us-
ing this approach, we collected 3,000 manual 
translations of keyword phrases and reused the 
translated terms to generate a lexicon for auto-
mated translation of the rest of the thesaurus.  
The process begins by prioritizing keyword 
phrases for manual translation in terms of their 
value in accessing the collection and the reus-
ability of their component terms.  Translations 
collected from one human informant are then 
checked and aligned to the original English terms 
by a second informant.  From these alignments 
we induce a probabilistic English-Czech phrase 
dictionary.   
To test the effectiveness of this process we 
implemented a simple translation system that 
utilizes the newly generated lexical resources.  
Section 4 reports on two evaluations of the trans-
lation output that quantify the effectiveness of 
our human-in-the-loop approach. 
3.1 Maximizing Value and Reusability 
To quantify their utility, we defined two values 
for each keyword phrase in the thesaurus: a the-
saurus value, representing the importance of the 
keyword phrase for providing access to the col-
lection, and a translation value, representing the 
usefulness of having the keyword phrase trans-
lated.  These values are not identical, but the 
second is related to the first. 
Thesaurus value: Keyword phrases in the 
Shoah Foundation?s thesaurus are arranged into a 
poly-hierarchy in which child nodes may have 
multiple parents.  Internal (non-leaf) nodes of the 
hierarchy are used to organize concepts and sup-
port concept browsing.  Some internal nodes are 
also used to index video content.  Leaf nodes are 
946
very specific and are only used to index video 
content.  Thus, the usefulness of any keyword 
phrase for providing access to the digital collec-
tion is directly related to the concept?s position in 
the thesaurus hierarchy. 
A fragment of the hierarchy is shown in Fig-
ure 1. The keyword phrase ?Auschwitz II-
Birkenau (Poland: Death Camp)?, which de-
scribes a Nazi death camp, is assigned to 17,555 
video segments in the collection.  It has broader 
(parent) terms and narrower (child) terms.  Some 
of the broader and narrower terms are also as-
signed to segments, but not all.  Notably, ?Ger-
man death camps? is not assigned to any video 
segments.  However, ?German death camps? has 
very important narrower terms including 
?Auschwitz II-Birkenau? and others. 
From this example, we can see that an internal 
node is valuable in providing access to its chil-
dren, even if the keyword phrase itself is not as-
signed to any segments.  The value we assign to 
any term must reflect this fact.  If we were to 
reduce cost by translating only the nodes as-
signed to video segments, we would neglect 
nodes that are crucial for browsing.  However, if 
we value a node by the sum value of all its chil-
dren, grandchildren, etc., the resulting calcula-
tion would bias the top of the hierarchy.  Any 
prioritization based on this method would lead to 
translation of the top of the hierarchy first.  
Given limited resources, leaf nodes might never 
be translated.  Support for searching and brows-
ing calls for different approaches to prioritization. 
To strike a balance between these factors, we 
calculate a thesaurus value, which represents the 
importance of each keyword phrase to the the-
saurus as a whole.  This value is computed as: 
( ) ( )kchildren
h
scounth kchildreni ikk
? ?+= )(  
For leaf nodes in our thesaurus, this value is sim-
ply the number of video segments to which the 
concept has been assigned.  For parent nodes, the 
thesaurus value is the number of segments (if 
any) to which the node has been assigned, plus 
the average of the thesaurus value of any child 
nodes. 
This recursive calculation yields a micro-
averaged value that represents the reachability of  
segments via downward edge traversals from a 
given node in the hierarchy.  That is, it gives a 
kind of weighted value for the number of seg-
ments described by a given keyword phrase or its 
narrower-term keyword phrases. 
 
For example, in Figure 2 each of the leaf 
nodes n3, n4, and n5 have values based solely on 
the number of segments to which they are as-
signed. Node n1 has value both as an access point 
to the segments at s2 and as an access point to the 
keyword phrases at nodes n3 and n4.  Other inter-
nal nodes, such as n2 have value only in provid-
ing access to other nodes/keyword phrases. 
Working from the bottom of the hierarchy up to 
the primary node (n0) we can compute the the-
saurus value for each node in the hierarchy.  In 
our example, we start with nodes n3 through n5, 
counting the number of the segments that have 
been assigned each keyword phrase.  Then we 
move up to nodes n1 and n2.  At n1 we count the 
number of segments s2 to which n1 was assigned 
and add that count to the average of the thesau-
rus values for n3, and n4.  At n2 we simply aver-
age the thesaurus values for n4 and n5.  The final 
values quantify how valuable the translation of 
any given keyword phrase would be in providing 
access to video segments. 
Translation value: After obtaining the the-
saurus value for each node, we can compute the 
translation value for each word in the vocabulary 
Figure 2. Bottom-up micro-averaging 
Figure 1. Sample keyword phrase  
with broader and narrower terms 
Auschwitz II-Birkenau (Poland : Death Camp) 
 Assigned to 17555 video segments 
 Has as broader term phrases: 
Cracow (Poland : Voivodship) 
  [ 534 narrower terms] [ 204 segments] 
German death camps 
  [  6 narrower terms] [ 0 segments] 
 Has seven narrower term phrases including: 
Block 25 (Auschwitz II-Birkenau) 
  [leaf node] [ 35 segments]  
Kanada (Auschwitz II-Birkenau) 
  [leaf node] [ 378 segments] 
  ...  
disinfection chamber (Auschwitz II-Birkenau) 
  [leaf node] [ 9 segments]  
primary 
keyword 
segments 
n2 
n4 n3 
n0 
n5 
keyword 
phrases 
s2 
n1 
s1 s3 s4 
947
as the sum of the thesaurus value for every key-
word phrase that contains that word: 
tw= ?
?? wk
kh   where Kw={x | phrase x contains w} 
For example, the word ?Auschwitz? occurs in 35 
concepts.  As a candidate for translation, it car-
ries a large impact, both in terms of the number 
of keyword phrases that contains this word, and 
the potential value of those keyword phrases 
(once they are translated) in providing access to 
segments in the archive.  The end result is a list 
of vocabulary words and the impact that correct 
translation of each word would have on the over-
all value of the translated thesaurus. 
We elicited human translations of entire key-
word phrases rather than individual vocabulary 
terms.  Having humans translate individual 
words without their surrounding context would 
have been less efficient.  Also, the value any 
keyword phrase holds for translation is only indi-
rectly related to its own value as a point of access 
to the collection (i.e., its thesaurus value).  Some 
keyword phrases contain words with high trans-
lation value, but the keyword phrase itself has 
low thesaurus value.  Thus, the value gained by 
translating any given phrase is more accurately 
estimated by the total value of any untranslated 
words it contains. Therefore, we prioritized the 
order of keyword phrase translations based on 
the translation value of the untranslated words in 
each keyword phrase. 
Our next step was to iterate through the the-
saurus keyword phrases, prioritizing their trans-
lation based on the assumption that any words 
contained in a keyword phrase of higher priority 
would already have been translated.  Starting 
from the assumption that the entire thesaurus is 
untranslated, we select the one keyword phrase 
that contains the most valuable un-translated 
words?we simply add up the translation value 
of all the untranslated words in each keyword 
phrase, and select the keyword phrase with the 
highest value.  We add this keyword phrase to a 
prioritized list of items to be manually translated 
and we remove it from the list of untranslated 
phrases.  We update our vocabulary list and, as-
suming translations of all the words in the prior 
keyword phrase to now be translated (neglecting 
issues such as morphology), we again select the 
keyword phrase that contains the most valuable 
untranslated words.  We iterate the process until 
all vocabulary terms have been included at least 
one keyword phrases on the prioritized list.  Ul-
timately we end up with an ordered list of the 
keyword phrases that should be translated to 
cover the entire vocabulary, with the most impor-
tant words being covered first. 
A few words about additional characteristics 
of this approach: note that it is greedy and biased 
toward longer keyword phrases.  As a result, 
some words may be translated more than once 
because they appear in more than one keyword 
phrase with high translation value.  This side 
effect is actually desirable.  To build an accurate 
translation dictionary, it is helpful to have more 
than one translation of frequently occuring words, 
especially for morphologically rich languages 
such as Czech.  Our technique makes the opera-
tional assumption that translations of a word 
gathered in one context can be reused in another 
context.  Obviously this is not always true, but 
contexts of use are relatively stable in controlled 
vocabularies.  Our evaluations address the ac-
ceptability of this operational assumption and 
demonstrate that the technique yields acceptable 
translations. 
Following this process model, the most impor-
tant elements of the thesaurus will be translated 
first, and the most important vocabulary terms 
will quickly become available for automated 
translation of keyword phrases with high thesau-
rus value that do not make it onto the prioritized 
list for manual translation (i.e., low translation 
value).  The overall access value of the thesaurus 
rises very quickly after initial translations.  With 
each subsequent human translation of keyword 
phrases on the prioritized list, we gain tremen-
dous value in terms of providing non-English 
access to the collection of video testimonies.  
Figure 3 shows this rate of gain.  It can be seen 
that prioritization based on translation value 
gives a much higher yield of total access than 
prioritization based on thesaurus value. 
Figure 3. Gain rate of access value based on  
number of human translations 
Gain rate of prioritized translation schemes
0%
20%
40%
60%
80%
100%
0 500 1000 1500 2000
number of translations
pe
rc
en
t o
f t
o
ta
l a
cc
es
s 
v
al
u
e
priority by thesaurus value priority by translation value
948
3.2 Alignment and Decomposition 
Following the prioritization scheme above, we 
obtained professional translations for the top 
3000 English keyword phrases.  We tokenized 
these translations and presented them to another 
bilingual Czech speaker for verification and 
alignment.  This second informant marked each 
Czech word in a translated keyword phrase with 
a link to the equivalent English word(s).  Multi-
ple links were used to convey the relationship 
between a single word in one language and a 
string of words in another.  The output of the 
alignment process was then used to build a prob-
abilistic dictionary of words and phrases. 
 
Figure 4. Sample alignment 
Figure 4 shows an example of an aligned 
tranlsation.  The word ?stills? is recorded as a 
translation for ?statick? sn?mky? and ?kl??tery? 
is recorded as a translation for ?convents and 
monasteries.?  We count the number of occur-
rences of each alignment in all of the translations 
and calculate probabilities for each Czech word 
or phrase given an English word or phrase.  For 
example, in the top 3000 keyword phrases 
?stills? appears 29 times.  It was aligned with 
?statick? sn?mky? 28 times and only once with 
?statick? z?b?ry?, giving us a translation prob-
ability of 28/29=0.9655 for ?statick? sn?mky?. 
Human translation of the 3000 English key-
word phrases into Czech took approximately 70 
hours, and the alignments took 55 hours.  The 
overall cost of human input (translation and 
alignment) was less than 1000 ?.  The projected 
cost of full translation for the entire thesaurus 
would have been close to 20000 ? and would not 
have produced any reusable resources.  Naturally, 
costs for building resources in this manner will 
vary, but in our case the cost savings is approxi-
mately twenty fold. 
3.3 Machine Translation 
To demonstrate the effectiveness of our approach, 
we show that a probabilistic dictionary, induced 
through the process we just described, facilitates 
high quality machine translation of the rest of the 
thesaurus.  We evaluated translation quality us-
ing a relatively simple translation system.  How-
ever, more sophisticated systems can draw equal 
benefit from the same lexical resources. 
Our translation system implemented a greedy 
coverage algorithm with a simple back-off strat-
egy.  It first scans the English input to find the 
longest matching substring in our dictionary, and 
replaces it with the most likely Czech translation.  
Building on the example above, the system looks 
up ?monasteries and convents stills? in the dic-
tionary, finds no translation, and backs off to 
?monasteries and convents?, which is translated 
to ?kl??tery?.  Had this phrase translation not 
been found, the system would have attempted to 
find a match for the individual tokens.  Failing a 
match in our dictionary, the system then backs 
off to the Prague Czech-English Dependency 
Treebank dictionary, a much larger dictionary 
with broader scope.  If no match is found in ei-
ther dictionary for the full token, we stem the 
token and look for matches based on the stem.  
Finally, tokens whose translations can not be 
found are simply passed through untranslated. 
A minimal set of heuristic rules was applied to 
reordering the Czech tokens but the output is 
primarily phrase by phrase/word by word transla-
tion.  Our evaluation scores below will partially 
reflect the simplicity of our system.  Our system 
is simple by design.  Any improvement or degra-
dation to the input of our system has direct influ-
ence on the output.  Thus, measures of transla-
tion accuracy for our system can be directly in-
terpreted as quality measures for the lexical re-
sources used and the process by which they were 
developed. 
4 Evaluation 
We performed two different types of evaluation 
to validate our process.  First, we compared our 
system output to human reference translations 
using Bleu (Papineni, et al, 2002), a widely-
accepted objective metric for evaluation of ma-
chine translations.  Second, we showed corrected 
and uncorrected machine translations to Czech 
speakers and collected subjective judgments of 
fluency and accuracy. 
For evaluation purposes, we selected 418 
keyword phrases to be used as target translations.  
These phrases were selected using a stratified 
sampling technique so that different levels of 
thesaurus value would be represented.  There 
was no overlap between these keyword phrases 
and the 3000 prioritized keyword phrases used to 
build our lexicon.  Prior to machine translation 
we obtained at least two independent human-
generated reference translations for each of the 
418 keyword phrases. 
monasteries convents and stills ( ) 
statick? kl??tery sn?mky ( ) 
949
After collecting the first 2500 prioritized 
translations, we induced a probabilistic diction-
ary and generated machine translations of the 
418 target keyword phrases. These were then 
corrected by native Czech speakers, who ad-
justed word order, word choice, and morphology. 
We use this set of human-corrected machine 
translations as a second reference for evaluation. 
Measuring the difference between our uncor-
rected machine translations (MT) and the human-
generated reference establishes how accurate our 
translations are compared to an independently 
established target.  Measuring the difference be-
tween our MT and the human-corrected machine 
translations (corrected MT) establishes how ac-
ceptable our translations are.  We also measured 
the difference between corrected MT and the 
human-generated translations.  We take this to be 
an upper bound on realistic system performance. 
The results from our objective evaluation are 
shown in Figure 5.  Each set of bars in the graph 
shows performance after adding a different num-
ber of aligned translations into the lexicon (i.e., 
performance after adding 500, 1000, ..., 3000 
aligned translations.)  The zero condition is our 
baseline: translations generated using only the 
dictionary available in the Prague Czech-English 
Dependency Treebank.  Three different reference 
sets are shown: human-generated, corrected MT, 
and a combination of the two. 
There is a notable jump in Bleu score after the 
very first translations are added into our prob-
abilistic dictionary.  Without any elicitation and 
alignment we got a baseline score of 0.46 
(against the human-generated reference transla-
tions).  After the aligned terms from only 500 
translations were added to our dictionary, our 
Bleu score rose to 0.66.  After aligned terms 
from 3000 translations were added, we achieved 
0.69.  Using corrected MT as the reference our 
Bleu scores improve from 0.48 to 0.79.  If hu-
man-generated and human-corrected references 
are both considered to be correct translations, the 
improvement goes from .49 to .80.  Regardless 
of the reference set, there is a consistent per-
formance improvement as more and more trans-
lations are added.  We found the same trend us-
ing the TER metric on a smaller data set 
(Murray, et al, 2006).  The fact that the Bleu 
scores continue to rise indicates that our ap-
proach is successful in quickly expanding the 
lexicon with accurate translations.  It is important 
to point out that Bleu scores are not meaningful 
in an absolute sense; the scores here should be 
interpreted with respect to each other.  The trend 
in scores strongly indicates that our prioritization 
scheme is effective for generating a high-quality 
translation lexicon at relatively low cost.   
To determine an upper bound on machine per-
formance, we compared our corrected MT output 
to the initial human-generated reference transla-
tions, which were collected prior to machine 
translation.  Corrected MT achieved a Bleu score 
of 0.82 when compared to the human-generated 
reference translations.  This upper bound is the 
?limit? indicated in Figure 5. 
To determine the impact of external resources, 
we removed the Prague Czech-English Depend-
ency Treebank dictionary as a back-off resource 
and retranslated keyword phrases using only the 
lexicons induced from our aligned translations.  
The results of this experiment showed only mar-
ginal degradation of the output.  Even when as 
few as 500 aligned translations were used for our 
dictionary, we still achieved a Bleu score of 0.65 
against the human reference translations.  This 
means that even for languages where prior re-
sources are not available our prioritization 
scheme successfully addresses the OOV problem. 
In our subjective evaluation, we presented a 
random sample of our system output to seven 
Distribution of Subjective Judgment Scores
0%
20%
40%
60%
80%
100%
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
fluency accuracy fluency accuracy
MT Corrected MT
Judgment scores
Pe
rc
en
t o
f s
co
re
s
Bleu Scores After Increasing Translations
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 500 1000 1500 2000 2500 3000
Number of Translations
B
le
u
-
4
corrected human reference both limit
Figure 5. Objective evaluation results 
Figure 6. Subjective evaluation results 
950
native Czech speakers and collected judgments 
of accuracy and fluency using a 5-point Likert 
scale (1=good, 3=neutral, 5=bad).  An overview 
of the results is presented in Figure 6.  Scores are 
shown for corrected and uncorrected MT.  In all 
cases, the mode is 1 (i.e., good fluency and good 
accuracy).  59% of the machine translated 
phrases were rated 2 or better for fluency.  66% 
were rated 2 or better for accuracy.  Only a small 
percentage of the translations had meanings that 
were far from the intended meaning.  Disfluen-
cies were primarily due to errors in morphology 
and word order.  
5 Related Work  
Several studies have taken a knowledge-
acquisition approach to collecting multilingual 
word pairs.  For example, Sadat et al (2003) 
automatically extracted bilingual word pairs 
from comparable corpora.  This approach is 
based on the simple assumption that if two words 
are mutual translations, then their most frequent 
collocates are likely to be mutual translations as 
well.  However, the approach requires large com-
parable corpora, the collection of which presents 
non-trivial challenges.  Others have made similar 
mutual-translation assumptions for lexical acqui-
sition (Echizen-ya, et al, 2005; Kaji & Aizono, 
1996; Rapp, 1999; Tanaka & Iwasaki, 1996).  
Most make use of either parallel corpora or a 
bilingual dictionary for the task of bilingual term 
extraction.  Echizen-ya, et al (2005) avoided 
using a bilingual dictionary, but required a paral-
lel corpus to achieve their goal; whereas Fung 
(2000) and others have relied on pre-existing 
bilingual dictionaries.  In either case, large bilin-
gual resources of some kind are required.  In ad-
dition, these approaches focused on the extrac-
tion of single-word pairs, not phrasal units. 
Many recent approaches to dictionary and the-
saurus translation are geared toward providing 
domain-specific thesauri to specialists in a par-
ticular field, e.g., medical terminology (D?jean, 
et al, 2005) and agricultural terminology (Chun 
& Wenlin, 2002).  Researchers on these projects 
are faced with either finding human translators 
who are specialized enough to manage the do-
main-particular translations?or applying auto-
matic techniques to large-scale parallel corpora 
where data sparsity poses a problem for low-
frequency terms.  Data sparsity is also an issue 
for more general state-of-the-art bilingual align-
ment approaches (Brown, et al, 2000; Och & 
Ney, 2003; Wantanabe & Sumita, 2003). 
6 Conclusion 
The task of translating large ontologies can be 
recast as a problem of implementing fast and ef-
ficient processes for acquiring task-specific lexi-
cal resources.  We developed a method for pri-
oritizing keyword phrases from an English the-
saurus of concepts and elicited Czech transla-
tions for a subset of the keyword phrases.  From 
these, we decomposed phrase elements for reuse 
in an English-Czech probabilistic dictionary.  We 
then applied the dictionary in machine translation 
of the rest of the thesaurus.   
Our results show an overall improvement in 
machine translation quality after collecting only 
a few hundred human translations.  Translation 
quality continued to rise as more and more hu-
man translations were added.  The test data used 
in our evaluations are small relative to the overall 
task.  However, we fully expect these results to 
hold across larger samples and for more sophisti-
cated translation systems.   
We leveraged the reusability of translated 
words to translate a thesaurus of 56,000 keyword 
phrases using information gathered from only 
3000 manual translations.  Our probabilistic dic-
tionary was acquired at a fraction of the cost of 
manually translating the entire thesaurus.  By 
prioritizing human translations based on the 
translation value of the words and the thesaurus 
value of the keyword phrases in which they ap-
pear, we optimized the rate of return on invest-
ment. This allowed us to choose a trade-off point 
between cost and utility.  For this project we 
chose to stop human translation at a point where 
less than 0.01% of the value of the thesaurus 
would be gained from each additional human 
translation.  This choice produced a high-quality 
lexicon with significant positive impact on ma-
chine translation systems.  For other applications, 
a different trade-off point will be appropriate, 
depending on the initial OOV rate and the impor-
tance of detailed coverage. 
The value of our work lies in the process 
model we developed for cost-effective elicitation 
of lexical resources.  The metrics we established 
for assessing the impact of each translation item 
are key to our approach.  We use these to opti-
mize the value gained from each human transla-
tion.  In our case the items were keyword phrases 
arranged in a hierarchical thesaurus that de-
scribes an ontology of concepts.  The operational 
value of these keyword phrases was determined 
by the access they provide to video segments in a 
large archive of oral histories.  However, our 
technique is not limited to this application. 
951
We have shown that careful prioritization of 
elicited human translations facilitates cost-
effective thesaurus translation with minimal hu-
man input.  Our use of a prioritization scheme 
addresses the most important deficiencies in the 
vocabulary first.  We induced a framework 
where the utility of lexical resources gained from 
each additional human translation becomes 
smaller and smaller.  Under such a framework, 
choosing the number of human translation to 
elicit becomes merely a function of the financial 
resources available for the task. 
Acknowledgments 
Our thanks to Doug Oard for his contribution to 
this work.  Thanks also to our Czech informants: 
Robert Fischmann, Eliska Kozakova, Alena 
Prunerova and Martin Smok; and to Soumya 
Bhat for her programming efforts. 
This work was supported in part by NSF IIS 
Award 0122466 and NSF CISE RI Award 
EIA0130422.  Additional support also came 
from grants of the MSMT CR #1P05ME786 and 
#MSM0021620838, and the Grant Agency of the 
CR #GA405/06/0589.   
References 
Brown, P. F., Della-Pietra, V. J., Della-Pietra, S. A., 
& Mercer, R. L. (1993). The mathematics of statis-
tical machine translation: Parameter estimation. 
Computational Linguistics, 19(2), 263-311. 
Chun, C., & Wenlin, L. (2002). The translation of 
agricultural multilingual thesaurus. In Proceedings 
of the Third Asian Conference for Information 
Technology in Agriculture. Beijing, China: Chinese 
Academy of Agricultural Sciences (CAAS) and 
Asian Federation for Information Technology in 
Agriculture (AFITA). 
?mejrek, M., Cu??n, J., Havelka, J., Haji?, J., & Ku-
bon, V. (2004). Prague Czech-English dependecy 
treebank: Syntactically annotated resources for ma-
chine translation. In 4th International Conference 
on Language Resources and Evaluation Lisbon, 
Portugal. 
D?jean, H., Gaussier, E., Renders, J.-M., & Sadat, F. 
(2005). Automatic processing of multilingual 
medical terminology: Applications to thesaurus en-
richment and cross-language information retrieval. 
Artificial Intelligence in Medicine, 33(2 ), 111-124. 
Echizen-ya, H., Araki, K., & Momouchi, Y. (2005). 
Automatic acquisition of bilingual rules for extrac-
tion of bilingual word pairs from parallel corpora. 
In Proceedings of the ACL-SIGLEX Workshop on 
Deep Lexical Acquisition (pp. 87-96).  
Fung, P. (2000). A statistical view of bilingual lexicon 
extraction: From parallel corpora to non-parallel 
corpora. In Jean Veronis (ed.), Parallel Text Proc-
essing. Dordrecht: Kluwer Academic Publishers. 
Gustman, Soergel, Oard, Byrne, Picheny, Ramabhad-
ran, & Greenberg. (2002). Supporting access to 
large digital oral history archives.  In Proceedings 
of the Joint Conference on Digital Libraries. Port-
land, Oregon. (pp. 18-27). 
Kaji, H., & Aizono, T. (1996). Extracting word corre-
spondences from bilingual corpora based on word 
co-occurrence information. In Proceedings of 
COLING '96 (pp. 23-28).  
Murray, G. C., Dorr, B., Lin, J., Haji?, J., & Pecina, P. 
(2006).  Leveraging recurrent phrase structure in 
large-scale ontology translation.  In Proceedings of 
the 11th Annual Conference of the European Asso-
ciation for Machine Translation.  Oslo, Norway. 
Nelson, S. J., Schopen, M., Savage, A. G., Schulman, 
J.-L., & Arluk, N. (2004). The MeSH translation 
maintenance system: Structure, interface design, 
and implementation. In Proceedings of the 11th 
World Congress on Medical Informatics. (pp. 67-
69). Amsterdam: IOS Press. 
Och, F. J., & Ney, H. (2003). A systematic compari-
son of various statistical alignment models. Com-
putational Linguistics, 29(1), 19-51. 
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. 
(2002). BLEU: A method for automatic evaluation 
of machine translation. In Proceedings of the 40th 
Annual Meeting of the Association for Computa-
tional Linguistics (pp. 331-318). 
Rapp, R. (1999). Automatic identification of word 
translations from unrelated English and German 
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics. (pp. 519-526). 
Sadat, F., Yoshikawa, M., & Uemura, S. (2003). En-
hancing cross-language information retrieval by an 
automatic acquisition of bilingual terminology 
from comparable corpora . In Proceedings of the 
26th Annual International ACM SIGIR Conference 
on Research and Development in Information Re-
trieval (pp. 397-398).  
Tanaka, K., & Iwasaki, H. (1996). Extraction of lexi-
cal translations from non-aligned corpora. In Pro-
ceedings of COLING '96. (pp. 580-585). 
USC. (2006) USC Shoah Foundation Institute for 
Visual History and Education, [online] 
http://www.usc.edu/schools/college/vhi 
Wantanabe, T., & Sumita, E. (2003). Example-based 
decoding for statistical machine translation. In Pro-
ceedings of MT Summit IX (pp. 410-417).  
952
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 651?658,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Combining Association Measures for Collocation Extraction
Pavel Pecina and Pavel Schlesinger
Institute of Formal and Applied Linguistics
Charles University, Prague, Czech Republic
{pecina,schlesinger}@ufal.mff.cuni.cz
Abstract
We introduce the possibility of combining
lexical association measures and present
empirical results of several methods em-
ployed in automatic collocation extrac-
tion. First, we present a comprehensive
summary overview of association mea-
sures and their performance on manu-
ally annotated data evaluated by precision-
-recall graphs and mean average precision.
Second, we describe several classification
methods for combining association mea-
sures, followed by their evaluation and
comparison with individual measures. Fi-
nally, we propose a feature selection algo-
rithm significantly reducing the number of
combined measures with only a small per-
formance degradation.
1 Introduction
Lexical association measures are mathematical
formulas determining the strength of association
between two or more words based on their occur-
rences and cooccurrences in a text corpus. They
have a wide spectrum of applications in the field
of natural language processing and computational
linguistics such as automatic collocation extrac-
tion (Manning and Sch?tze, 1999), bilingual word
alignment (Mihalcea and Pedersen, 2003) or de-
pendency parsing. A number of various associa-
tion measures were introduced in the last decades.
An overview of the most widely used techniques
is given e.g. in Manning and Sch?tze (1999) or
Pearce (2002). Several researchers also attempted
to compare existing methods and suggest differ-
ent evaluation schemes, e.g Kita (1994) and Evert
(2001). A comprehensive study of statistical as-
pects of word cooccurrences can be found in Evert
(2004) or Krenn (2000).
In this paper we present a novel approach to au-
tomatic collocation extraction based on combin-
ing multiple lexical association measures. We also
address the issue of the evaluation of association
measures by precision-recall graphs and mean av-
erage precision scores. Finally, we propose a step-
wise feature selection algorithm that reduces the
number of combined measures needed with re-
spect to performance on held-out data.
The term collocation has both linguistic and
lexicographic character. It has various definitions
but none of them is widely accepted. We adopt
the definition from Choueka (1988) who defines
a collocational expression as ?a syntactic and se-
mantic unit whose exact and unambiguous mean-
ing or connotation cannot be derived directly from
the meaning or connotation of its components?.
This notion of collocation is relatively wide and
covers a broad range of lexical phenomena such as
idioms, phrasal verbs, light verb compounds, tech-
nological expressions, proper names, and stock
phrases. Our motivation originates from machine
translation: we want to capture all phenomena that
may require special treatment in translation.
Experiments presented in this paper were per-
formed on Czech data and our attention was re-
stricted to two-word (bigram) collocations ? pri-
marily for the limited scalability of some meth-
ods to higher-order n-grams and also for the rea-
son that experiments with longer word expressions
would require processing of much larger corpus to
obtain enough evidence of the observed events.
2 Reference data
The first step in our work was to create a refer-
ence data set. Krenn (2000) suggests that col-
location extraction methods should be evaluated
against a reference set of collocations manually
extracted from the full candidate data from a cor-
pus. To avoid the experiments to be biased by
underlying data preprocessing (part-of-speech tag-
ging, lemmatization, and parsing), we extracted
the reference data from morphologically and syn-
tactically annotated Prague Dependency Treebank
2.0 containing about 1.5 million words annotated
on analytical layer (PDT 2.0, 2006). A corpus of
this size is certainly not sufficient for real-world
applications but we found it adequate for our eval-
uation purposes ? a larger corpus would have made
the manual collocation extraction task infeasible.
651
Dependency trees from the corpus were broken
down into dependency bigrams consisting of lem-
mas of the head word and its modifier, their part-
-of-speech pattern, and dependency type. From
87 980 sentences containing 1 504 847 words, we
obtained a total of 635 952 different dependency
bigrams types. Only 26 450 of them occur in the
data more than five times. The less frequent bi-
grams do not meet the requirement of sufficient
evidence of observations needed by some meth-
ods used in this work (they assume normal dis-
tribution of observations and become unreliable
when dealing with rare events) and were not in-
cluded in the evaluation. We, however, must
agree with Moore (2004) arguing that these cases
comprise majority of all the data (the Zipfian
phenomenon) and thus should not be excluded
from real-world applications. Finally, we filtered
out all bigrams having such part-of-speech pat-
terns that never form a collocation (conjunction?
preposition, preposition?pronoun, etc.) and ob-
tained a list consisting of 12 232 dependency bi-
grams, further called collocation candidates.
2.1 Manual annotation
The list of collocation candidates was manually
processed by three trained linguists in parallel and
independently with the aim of identifying colloca-
tions as defined by Choueka. To simplify and clar-
ify the work they were instructed to select those
bigrams that can be assigned to these categories:
? idiomatic expressions
- studen? v?lka (cold war)
- vis? otazn?k (question mark is hanging? open question)
? technical terms
- pr?edseda vl?dy (prime minister)
- oc?it? sve?dek (eye witness)
? support verb constructions
- m?t pravdu (to be right)
- uc?init rozhodnut? (make decision)
? names of persons, locations, and other entities
- Pra?sk? hrad (Prague Castle)
- C?erven? kr??? (Red Cross)
? stock phrases
- z?sadn? probl?m (major problem)
- konec roku (end of the year)
The first (expected) observation was that the in-
terannotator agreement among all the categories
was rather poor: the Cohen?s ? between annota-
tors ranged from 0.29 to 0.49, which demonstrates
that the notion of collocation is very subjective,
domain-specific, and somewhat vague. The reason
that three annotators were used was to get a more
precise and objective idea about what can be con-
sidered a collocation by combining outcomes from
multiple annotators. Only those bigrams that all
three annotators independently recognized as col-
locations (of any type) were considered true collo-
cations. The reference data set contains 2 557 such
bigrams, which is 20.9% of all. ? between these
two categories reanged from 0.52 to 0.58.
The data was split into six stratified samples.
Five folds were used for five-fold cross validation
and average performance estimation. The remain-
ing one fold was put aside and used as held-out
data in experiments described in Section 5.
3 Association measures
In the context of collocation extraction, lexical as-
sociation measures are formulas determining the
degree of association between collocation com-
ponents. They compute an association score for
each collocation candidate extracted from a cor-
pus. The scores indicate the potential for a can-
didate to be a collocation. They can be used for
ranking (candidates with high scores at the top),
or for classification (by setting a threshold and dis-
carding all bigrams below this threshold).
If some words occur together more often than
by chance, then this may be evidence that they
have a special function that is not simply explained
as a result of their combination (Manning and
Sch?tze, 1999). This property is known in linguis-
tics as non-compositionality. We think of a cor-
pus as a randomly generated sequence of words
that is viewed as a sequence of word pairs (de-
pendency bigrams in our case). Occurrence fre-
quencies and marginal frequencies are used in sev-
eral association measures that reflect how much
the word cooccurrence is accidental. Such mea-
sures include: estimation of joint and conditional
bigram probabilities (Table 1, 1?3), mutual infor-
mation and derived measures (4?9), statistical tests
of independence (10?14), likelihood measures (15?
16), and various other heuristic association mea-
sures and coefficients (17?55) originating in differ-
ent research fields.
By determining the entropy of the immediate
context of a word sequence (words immediately
preceding or following the bigram), the associa-
tion measures (56?60) rank collocations according
to the assumption that they occur as (syntactic)
units in a (information-theoretically) noisy envi-
ronment (Shimohata et al, 1997). By comparing
empirical contexts of a word sequence and of its
components (open-class words occurring within
652
# Name Formula
1. Joint probability P (xy)
?2. Conditional probability P (y|x)
3. Reverse conditional prob. P (x|y)
4. Pointwise mutual inform. log P (xy)P (x?)P (?y)
5. Mutual dependency (MD) log P (xy)2P (x?)P (?y)
6. Log frequency biased MD log P (xy)2P (x?)P (?y)+logP (xy)
7. Normalized expectation 2f(xy)f(x?)+f(?y)
8. Mutual expectation 2f(xy)f(x?)+f(?y) ?P (xy)
?9. Salience log P (xy)2P (x?)P (?y) ? logf(xy)
10. Pearson?s ?2 test Pi,j
(fij?f?ij)2
f?ij
11. Fisher?s exact test f(x?)!f(x??)!f(?y)!f(?y?)!N!f(xy)!f(xy?)!f(x?y)!f(x?y?)!
12.t test f(xy)?f?(xy)?f(xy)(1?(f(xy)/N))
13.z score f(xy)?f?(xy)?
f?(xy)(1?(f?(xy)/N))
14. Poison significance measure f?(xy)?f(xy) logf?(xy)+logf(xy)!logN
15. Log likelihood ratio ?2Pi,jfij log
fij
f?ij
16. Squared log likelihood ratio ?2Pi,j
logfij2
f?ij
Association coefficients:
17. Russel-Rao aa+b+c+d
18. Sokal-Michiner a+da+b+c+d
19. Rogers-Tanimoto a+da+2b+2c+d
20. Hamann (a+d)?(b+c)a+b+c+d
21. Third Sokal-Sneath b+ca+d
22. Jaccard aa+b+c
?23. First Kulczynsky ab+c
24. Second Sokal-Sneath aa+2(b+c)
25. Second Kulczynski 12 ( aa+b+ aa+c )
?26. Fourth Sokal-Sneath 14 ( aa+b+ aa+c+ dd+b+ dd+c )
?27. Odds ratio adbc
28. Yulle?s ?
?ad??bc?ad+?bc
29. Yulle?s Q ad?bcad+bc
30. Driver-Kroeber a?(a+b)(a+c)
31. Fifth Sokal-Sneath ad?(a+b)(a+c)(d+b)(d+c)
32. Pearson ad?bc?(a+b)(a+c)(d+b)(d+c)
33. Baroni-Urbani a+
?ad
a+b+c+?ad
?34. Braun-Blanquet amax(a+b,a+c)
?35. Simpson amin(a+b,a+c)
36. Michael 4(ad?bc)(a+d)2+(b+c)2
37. Mountford 2a2bc+ab+ac
38. Fager a?(a+b)(a+c)?
1
2max(b, c)
39. Unigram subtuples log adbc?3.29
q
1
a+ 1b + 1c + 1d
40. U cost log(1+ min(b,c)+amax(b,c)+a )
41. S cost log(1+min(b,c)a+1 )?
1
2
42. R cost log(1+ aa+b )?log(1+ aa+c )
43. T combined cost ?U?S?R
44. Phi P (xy)?P (x?)P (?y)?P (x?)P (?y)(1?P (x?))(1?P (?y))
45. Kappa P (xy)+P (x?y?)?P (x?)P (?y)?P (x??)P (?y?)1?P (x?)P (?y)?P (x??)P (?y?)
46. J measure max[P (xy)logP (y|x)P (?y) +P (xy?)log
P (y?|x)
P (?y?) ,
P (xy)logP (x|y)P (x?) +P (x?y)log
P (x?|y)
P (x??) ]
# Name Formula
47. Gini index max[P (x?)(P (y|x)2+P (y?|x)2)?P (?y)2
+P (x??)(P (y|x?)2+P (y?|x?)2)?P (?y?)2,
P (?y)(P (x|y)2+P (x?|y)2)?P (x?)2
+P (?y?)(P (x|y?)2+P (x?|y?)2)?P (x??)2]
48. Confidence max[P (y|x), P (x|y)]
49. Laplace max[NP (xy)+1NP (x?)+2 ,
NP (xy)+1
NP (?y)+2 ]
50. Conviction max[P (x?)P (?y)P (xy?) ,
P (x??)P (?y)
P (x?y) ]
51. Piatersky-Shapiro P (xy)?P (x?)P (?y)
52. Certainity factor max[P (y|x)?P (?y)1?P (?y) ,
P (x|y)?P (x?)
1?P (x?) ]
53. Added value (AV) max[P (y|x)?P (?y), P (x|y)?P (x?)]
54. Collective strength P (xy)+P (x?y?)P (x?)P (y)+P (x??)P (?y) ?
1?P (x?)P (?y)?P (x??)P (?y)
1?P (xy)?P (x?y?)
?55. Klosgen pP (xy) ?AV
Context measures:
?56. Context entropy ?Pw P (w|Cxy) logP (w|Cxy)
?57. Left context entropy ?Pw P (w|Clxy) logP (w|Clxy)
58. Right context entropy ?Pw P (w|Crxy) logP (w|Crxy)
59. Left context divergence P (x?) logP (x?)
?PwP (w|Clxy) logP (w|Clxy)
60. Right context divergence P (?y) logP (?y)
?PwP (w|Crxy) logP (w|Crxy)
61. Cross entropy ?PwP (w|Cx) logP (w|Cy)
62. Reverse cross entropy ?PwP (w|Cy) logP (w|Cx)
63. Intersection measure 2|Cx?Cy||Cx|+|Cy|
?64. Euclidean norm
qP
w(P (w|Cx)?P (w|Cy))2
65. Cosine norm
P
w P (w|Cx)P (w|Cy)P
w P (w|Cx)2?
P
w P (w|Cy)2
?66. L1 norm Pw |P (w|Cx)?P (w|Cy)|
67. Confusion probability Pw P (x|Cw)P (y|Cw)P (w)P (x?)
?68. Reverse confusion prob. Pw P (y|Cw)P (x|Cw)P (w)P (?y)
?69. Jensen-Shannon diverg. 12 [D(p(w|Cx)|| 12 (p(w|Cx)+p(w|Cy)))
+D(p(w|Cy)|| 12 (p(w|Cx)+p(w|Cy)))]
?70. Cosine of pointwise MI
P
w MI(w,x)MI(w,y)?P
w MI(w,x)2?
?P
w MI(w,y)2
71. KL divergence Pw P (w|Cx) logP (w|Cx)P (w|Cy)
72. Reverse KL divergence Pw P (w|Cy) log
P (w|Cy)
P (w|Cx)
?73. Skew divergence D(p(w|Cx)||?(w|Cy)+(1??)p(w|Cx))
74. Reverse skew divergence D(p(w|Cy)||?p(w|Cx)+(1??)p(w|Cy))
75. Phrase word coocurrence 12 (
f(x|Cxy)
f(xy) +
f(y|Cxy)
f(xy) )
76. Word association 12 (
f(x|Cy)?f(xy)
f(xy) +
f(y|Cx)?f(xy)
f(xy) )
Cosine context similarity: 12 (cos(cx,cxy)+cos(cy,cxy))
cz=(zi); cos(cx,cy)=
P xiyi?P xi2?
?P yi2
?77. in boolean vector space zi=?(f(wi|Cz))
78. in tf vector space zi=f(wi|Cz)
79. in tf?idf vector space zi=f(wi|Cz)? Ndf(wi); df(wi)= |{x :wi?Cx}|
Dice context similarity: 12 (dice(cx,cxy)+dice(cy ,cxy))
cz=(zi); dice(cx,cy)=
2P xiyiP xi2+
P yi2
80. in boolean vector space zi=?(f(wi|Cz))
81. in tf vector space zi=f(wi|Cz)
82. in tf?idf vector space zi=f(wi|Cz)? Ndf(wi); df(wi)= |{x :wi?Cx}|
a=f(xy) b=f(xy?) f(x?)
c=f(x?y) d=f(x?y?) f(x??)
f(?y) f(?y?) N
A contingency table contains observed frequencies and marginal frequencies for a bigram
xy; w? stands for any word except w; ? stands for any word; N is a total number of bi-
grams. The table cells are sometimes referred to as fij . Statistical tests of independence
work with contingency tables of expected frequenciesf?(xy)=f(x?)f(?y)/N .
Cw empirical context of w
Cxy empirical context of xy
Clxy left immediate context of xy
Crxy right immediate context of xy
Table 1: Lexical association measures used for bigram collocation extraction.
?denotes those selected by the model reduction algorithm discussed in Section 5.
653
Recall
Pr
ec
isi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.
2
0.
4
0.
6
0.
8
1.
0
Unaveraged precision curve
Averaged precison curve
Figure 1: Vertical averaging of precision-recall curves. Thin
curves represent individual non-averaged curves obtained by
Pointwise mutual information (4) on five data folds.
a specified context window), the association mea-
sures rank collocations according to the assump-
tion that semantically non-compositional expres-
sions typically occur as (semantic) units in differ-
ent contexts than their components (Zhai, 1997).
Measures (61?74) have information theory back-
ground and measures (75?82) are adopted from the
field of information retrieval.
3.1 Evaluation
Collocation extraction can be viewed as classifi-
cation into two categories. By setting a threshold,
any association measure becomes a binary clas-
sifier: bigrams with higher association scores fall
into one class (collocations), the rest into the other
class (non-collocations). Performance of such
classifiers can be measured for example by accu-
racy ? fraction of correct predictions. However,
the proportion of the two classes in our case is far
from equal and we want to distinguish classifier
performance between them. In this case, several
authors, e.g. Evert (2001), suggest using precision
? fraction of positive predictions correct and re-
call ? fraction of positives correctly predicted. The
higher the scores the better the classification is.
3.2 Precision-recall curves
Since choosing a classification threshold depends
primarily on the intended application and there is
no principled way of finding it (Inkpen and Hirst,
2002), we can measure performance of associa-
tion measures by precision?recall scores within
the entire interval of possible threshold values. In
this manner, individual association measures can
be thoroughly compared by their two-dimensional
precision-recall curves visualizing the quality of
ranking without committing to a classification
threshold. The closer the curve stays to the top
and right, the better the ranking procedure is.
Recall
Av
er
ag
e 
pr
ec
isi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.
2
0.
4
0.
6
0.
8
1.
0
Pointwise mutual information (4)
Pearson?s test (10)
z score (13)
Cosine context similarity in boolean vector space (77)
Unigram subtuple measure (39)
Figure 2: Crossvalidated and averaged precision-recall
curves of selected association measures (numbers in brack-
ets refer to Table 1).
Precision-recall curves are very sensitive to data
(see Figure 1). In order to obtain a good esti-
mate of their shapes cross validation and averag-
ing are necessary: all cross-validation folds with
scores for each instance are combined and a single
curve is drawn. Averaging can be done in three
ways: vertical ? fixing recall, averaging precision,
horizontal ? fixing precision, averaging recall, and
combined ? fixing threshold, averaging both preci-
sion and recall (Fawcett, 2003). Vertical averag-
ing, as illustrated in Figure 1, worked reasonably
well in our case and was used in all experiments.
3.3 Mean average precision
Visual comparison of precision-recall curves is
a powerfull evaluation tool in many research fields
(e.g. information retrieval). However, it has a seri-
ous weakness. One can easily compare two curves
that never cross one another. The curve that pre-
dominates another one within the entire interval
of recall seems obviously better. When this is not
the case, the judgment is not so obvious. Also
significance tests on the curves are problematic.
Only well-defined one-dimensional quality mea-
sures can rank evaluated methods by their per-
formance. We adopt such a measure from in-
formation retrieval (Hull, 1993). For each cross-
-validation data fold we define average precision
(AP) as the expected value of precision for all pos-
sible values of recall (assuming uniform distribu-
tion) and mean average precision (MAP) as a mean
of this measure computed for each data fold. Sig-
nificance testing in this case can be realized by
paired t-test or by more appropriate nonparametric
paired Wilcoxon test.
Due to the unreliable precision scores for low
recall and their fast changes for high recall, esti-
mation of AP should be limited only to some nar-
rower recall interval, e.g. ?0.1,0.9?
654
M
ea
n 
av
er
ag
e 
pr
ec
isi
on
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
77 80 32 30 10 42 4 28 63 22 23 7 20 19 43 6 9 50 48 8 59 73 61 25 11 74 68 53 52 35 41 55 47 81 46 2 51 78 58 57 1739 38 31 13 5 37 27 29 16 24 45 33 21 18 34 54 76 3 82 44 66 71 26 15 14 72 70 64 49 65 69 40 75 56 12 60 36 79 62 1 67 77 38 30 5 4 29 22 45 20 18 6 76 48 44 73 26 11 72 53 49 41 40 81 12 51 79 57 67
67
57
79
51
12
81
40
41
49
53
72
11
26
73
44
48
76
6
18
20
45
22
29
4
5
30
38
77
Figure 3: a) Mean average precision of all association measures in descending order. Methods are referred by numbers
from Table 1. The solid points correspond to measures selected by the model reduction algorithm from Section 5. b) Visu-
alization of p-values from the significance tests of difference between each method pair (order is the same for both graphs). The
darker points correspond to p-values greater than ?=0.1 and indicate methods with statistically indistinguishable performance
(measured by paired Wilcoxon test on values of average precision obtained from five independent data folds).
3.4 Experiments and results
In the initial experiments, we implemented all 82
association measures from Table 1, processed all
morphologically and syntactically annotated sen-
tences from PDT 2.0, and computed scores of all
the association measures for each dependency bi-
gram in the reference data. For each associa-
tion measure and each of the five evaluation data
folds, we computed precision-recall scores and
drew an averaged precision-recall curve. Curves
of some well-performing methods are depicted in
Figure 2. Next, for each association measure and
each data fold, we estimated scores of average pre-
cision on narrower recall interval ?0.1,0.9?, com-
puted mean average precision, ranked the asso-
ciation measures according to MAP in descend-
ing order, and result depicted in Figure 3 a). Fi-
nally, we applied a paired Wilcoxon test, detected
measures with statistically indistinguishable per-
formance, and visualized this information in Fig-
ure 3 b).
A baseline system ranking bigrams randomly
operates with average precision of 20.9%. The
best performing method for collocation extrac-
tion measured by mean average precision is co-
sine context similarity in boolean vector space (77)
(MAP 66.49%) followed by other 16 associa-
tion measures with nearly identical performance
(Figure 3 a). They include some popular meth-
ods well-known to perform reliably in this task,
such as pointwise mutual information (4), Pear-
son?s ?2 test (10), z score (13), odds ratio (27), or
squared log likelihood ratio (16).
The interesting point to note is that, in terms
of MAP, context similarity measures, e.g. (77),
slightly outperform measures based on simple oc-
curence frequencies, e.g. (39). In a more thorough
comparison by percision-recall curves, we observe
that the former very significantly predominates the
latter in the first half of the recall interval and vice
versa in the second half (Figure 2). This is a case
where the MAP is not a sufficient metric for com-
parison of association measure performance. It is
also worth pointing out that even if two methods
have the same precision-recall curves the actual bi-
gram rank order can be very different. Existence
of such non-correlated (in terms of ranking) mea-
sures will be essential in the following sections.
4 Combining association measures
Each collocation candidate xi can be described by
the feature vector xi = (xi1, . . . , xi82)T consisting
of 82 association scores from Table 1 and assigned
a label yi ? {0, 1} which indicates whether the
bigram is considered to be a collocation (y = 1)
or not (y = 0). We look for a ranker function
f(x)?R that determines the strength of lexical
association between components of bigram x and
hence has the character of an association measure.
This allows us to compare it with other association
measures by the same means of precision-recall
curves and mean average precision. Further, we
present several classification methods and demon-
strate how they can be employed for ranking, i.e.
what function can be used as a ranker. For refer-
ences see Venables and Ripley (2002).
4.1 Linear logistic regression
An additive model for binary response is repre-
sented by a generalized linear model (GLM) in
a form of logistic regression:
logit(pi) = ?0 + ?1x1 + . . .+ ?pxp
655
method AP MAP
R=20 R=50 R=80 R=?0.1,0.9? +
NNet (5 units) 89.56 82.74 70.11 80.81 21.53
NNet (3 units) 89.41 81.99 69.64 79.71 19.88
NNet (2 units) 86.92 81.68 68.33 78.77 18.47
SVM (linear) 85.72 79.49 63.86 75.66 13.79
LDA 84.72 77.18 62.90 75.11 12.96
SVM (quadratic) 84.29 79.54 64.24 74.53 12.09
NNet (1 unit) 77.98 76.83 66.75 73.25 10.17
GLM 82.45 76.26 58.61 71.88 8.11
Cosine similarity (77) 80.94 68.90 50.54 66.49 0.00
Unigram subtuples (39) 74.55 67.49 55.16 65.74 -
Table 2: Performance of methods combining all association
measures: average precision (AP) for fixed recall values and
mean average precision (MAP) on the narrower recall interval
with relative improvement in the last column (values in %).
where logit(pi)= log(pi/(1?pi)) is a canonical link
function for odds-ratio and pi ? (0, 1) is a con-
ditional probability for positive response given
a vector x. The estimation of ?0 and ? is done
by maximum likelihood method which is solved
by the iteratively reweighted least squares algo-
rithm. The ranker function in this case is defined
as the predicted value ?, or equivalently (due to
the monotonicity of logit link function) as the lin-
ear combination ??0 + ??Tx.
4.2 Linear discriminant analysis
The basic idea of Fisher?s linear discriminant anal-
ysis (LDA) is to find a one-dimensional projection
defined by a vector c so that for the projected com-
bination cTx the ratio of the between variance B
to the within variance W is maximized:
maxc
cTBc
cTW c
After projection, cTx can be directly used as ranker.
4.3 Support vector machines
For technical reason, let us now change the labels
yi?{-1,+1}. The goal in support vector machines
(SVM) is to estimate a function f(x)=?0+?Tx and
find a classifier y(x) = sign(f(x)) which can be
solved through the following convex optimization:
min
?0,?
n?
i=1
[1?yi(?0 + ?T xi)
]++ ?2 ||?||
2
with ? as a regularization parameter. The hinge
loss function L(y,f(x)) = [1?yf(x)]+ is active
only for positive values (i.e. bad predictions) and
therefore is very suitable for ranking models with
??0+ ??Tx as a ranker function. Setting the regu-
larization parameter ? is crucial for both the es-
timators ??0, ?? and further classification (or rank-
ing). As an alternative to a often inappropriate grid
Recall
Av
er
ag
e 
pr
ec
isi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.
2
0.
4
0.
6
0.
8
1.
0
Neural network (5 units)
Support vector machine (linear)
Linear discriminant analysis
Neural network (1 unit)
Linear logistic regression
Cosine context similarity in boolean vector space (77)
Unigram subtuple measure (39)
Figure 4: Precision-recall curves of selected methods com-
bining all association measures compared with curves of two
best measures employed individually on the same data sets.
search, Hastie (2004) proposed an effective algo-
rithm which fits the entire SVM regularization path
[?0(?),?(?)] and gave us the option to choose the
optimal value of ?. As an objective function we
used total amount of loss on training data.
4.4 Neural networks
Assuming the most common model of neural net-
works (NNet) with one hidden layer, the aim is to
find inner weights wjh and outer weights whi for
yi=?0
(?0 +
?
whi?h(?h +
?
wjhxj)
)
where h ranges over units in the hidden layer. Ac-
tivation functions ?h and function ?0 are fixed.
Typically, ?h is taken to be the logistic function
?h(z) = exp(z)/(1 + exp(z)) and ?0 to be the
indicator function ?0(z) = I(z > ?) with ? as
a classification threshold. For ranking we simply
set ?0(z) = z. Parameters of neural networks are
estimated by the backpropagation algorithm. The
loss function can be based either on least squares
or maximum likehood. To avoid problems with
convergence of the algorithm we used the former
one. The tuning parameter of a classifier is then
the number of units in the hidden layer.
4.5 Experiments and results
To avoid incommensurability of association mea-
sures in our experiments, we used a common pre-
processing technique for multivariate standardiza-
tion: we centered values of each association mea-
sure towards zero and scaled them to unit variance.
Precision-recall curves of all methods were ob-
tained by vertical averaging in five-fold cross val-
idation on the same reference data as in the ear-
lier experiments. Mean average precision was
computed from average precision values estimated
656
on the recall interval ?0.1,0.9?. In each cross-
-validation step, four folds were used for training
and one fold for testing.
All methods performed very well in compari-
son with individual measures. The best result was
achieved by a neural network with five units in the
hidden layer with 80.81% MAP, which is 21.53%
relative improvement compared to the best indi-
vidual associaton measure. More complex mod-
els, such as neural networks with more than five
units in the hidden layer and support vector ma-
chines with higher order polynomial kernels, were
highly overfitted on the training data folds and bet-
ter results were achieved by simpler models. De-
tailed results of all experiment are given in Ta-
ble 2 and precision-recall curves of selected meth-
ods depicted in Figure 4.
5 Model reduction
Combining association measures by any of the
presented methods is reasonable and helps in the
collocation extraction task. However, the combi-
nation models are too complex in number of pre-
dictors used. Some association measures are very
similar (analytically or empirically) and as predic-
tors perhaps even redundant. Such measures have
no use in the models, make their training harder,
and should be excluded. Principal component
analysis applied to the evaluation data showed that
95% of its total variance is explained by only 17
principal components and 99.9% is explained by
42 of them. This gives us the idea that we should
be able to significantly reduce the number of vari-
ables in our models with no (or relativelly small)
degradation in their performance.
5.1 The algorithm
A straightforward, but in our case hardly feasible,
approach is an exhaustive search through the space
of all possible subsets of all association measures.
Another option is a heuristic step-wise algorithm
iteratively removing one variable at a time until
some stopping criterion is met. Such algorithms
are not very robust, they are sensitive to data and
generally not very recommended. However, we
tried to avoid these problems by initializing our
step-wise algorithm by clustering similar variables
and choosing one predictor from each cluster as
a representative of variables with the same contri-
bution to the model. Thus we remove the highly
corelated predictors and continue with the step-
-wise procedure.
Recall
Av
er
ag
e 
pr
ec
isi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.
2
0.
4
0.
6
0.
8
1.
0
NNet (5 units) with 82 predictors
NNet (5 units) with 42 predictors
NNet (5 units) with 17 predictors
NNet (5 units) with 7 predictors
Cosine context similarity in boolean vector space (77)
Unigram subtuple measure (39)
Figure 5: Precision-recall curves of four NNet models from
the model reduction process with different number of predic-
tors compared with curves of two best individual methods.
The algorithm starts with the hierarchical clus-
tering of variables in order to group those with
a similar contribution to the model, measured by
the absolute value of Pearson?s correlation coeffi-
cient. After 82?d iterations, variables are grouped
into d non-empty clusters and one representative
from each cluster is selected as a predictor into the
initial model. This selection is based on individual
predictor performance on held-out data.
Then, the algorithm continues with d predictors
in the initial model and in each iteration removes
a predictor causing minimal degradation of perfor-
mance measured by MAP on held-out data. The
algorithm stops when the difference becomes sig-
nificant ? either statistically (by paired Wilcoxon
test) or practically (set by a human).
5.2 Experiments and results
We performed the model reduction experiment on
the neural network with five units in the hidden
layer (the best performing combination method).
The similarity matrix for hierarchical clustering
was computed on the held-out data and parame-
ter d (number of initial predictors) was experimen-
tally set to 60. In each iteration of the algorithm,
we used four data folds (out of the five used in pre-
vious experiments) for fitting the models and the
held-out fold to measure the performance of these
models and to select the variable to be removed.
The new model was cross-validated on the same
five data-folds as in the previous experiments.
Precision-recall curves for some intermediate
models are shown in Figure 5. We can conclude
that we were able to reduce the NNet model to
about 17 predictors without statistically signifi-
cant difference in performance. The correspond-
ing association measures are marked in Table 1
and highlighted in Figure 3a). They include mea-
sures from the entire range of individual mean av-
erage precision values.
657
6 Conclusions and discussion
We created and manually annotated a reference
data set consisting of 12 232 Czech dependency
bigrams. 20.9% of them were agreed to be a col-
location by three annotators. We implemented 82
association measures, employed them for collo-
cation extraction and evaluated them against the
reference data set by averaged precision-recall
curves and mean average precision in five-fold
cross validation. The best result was achieved by
a method measuring cosine context similarity in
boolean vector space with mean average precision
of 66.49%.
We exploit the fact that different subgroups of
collocations have different sensitivity to certain
association measures and showed that combining
these measures aids in collocation extraction. All
investigated methods significantly outperformed
individual association measures. The best results
were achieved by a simple neural network with
five units in the hidden layer. Its mean average
precision was 80.81% which is 21.53% relative
improvement with respect to the best individual
measure. Using more complex neural networks or
a quadratic separator in support vector machines
led to overtraining and did not improve the perfor-
mace on test data.
We proposed a stepwise feature selection algo-
rithm reducing the number of predictors in com-
bination models and tested it with the neural net-
work. We were able to reduce the number of its
variables from 82 to 17 without significant degra-
dation of its performance.
No attempt in our work has been made to select
the ?best universal method? for combining associ-
ation measures nor to elicit the ?best association
measures? for collocation extraction. These tasks
depend heavily on data, language, and notion of
collocation itself. We demonstrated that combin-
ing association measures is meaningful and im-
proves precission and recall of the extraction pro-
cedure and full performance improvement can be
achieved by a relatively small number of measures
combined.
Preliminary results of our research were already
published in Pecina (2005). In the current work,
we used a new version of the Prague Dependecy
Treebank (PDT 2.0, 2006) and the reference data
was improved by additional manual anotation by
two linguists.
Acknowledgments
This work has been supported by the Ministry of
Education of the Czech Republic, projects MSM
0021620838 and LC 536. We would like to thank
our advisor Jan Hajic?, our colleagues, and anony-
mous reviewers for their valuable comments.
References
Y. Choueka. 1988. Looking for needles in a haystack or lo-
cating interesting collocational expressions in large textual
databases. In Proceedings of the RIAO.
S. Evert and B. Krenn. 2001. Methods for the qualitative
evaluation of lexical association measures. In Proceedings
of the 39th Annual Meeting of the ACL, Toulouse, France.
S. Evert. 2004. The Statistics of Word Cooccurrences: Word
Pairs and Collocations. Ph.D. thesis, Univ. of Stuttgart.
T. Fawcett. 2003. ROC graphs: Notes and practical con-
siderations for data mining researchers. Technical report,
HPL-2003-4. HP Laboratories, Palo Alto, CA.
T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. 2004. The
entire regularization path for the support vector machine.
Journal of Machine Learning Research, 5.
D. Hull. 1993. Using statistical testing in the evaluation of
retrieval experiments. In Proceedings of the 16th annual
international ACM SIGIR conference on Research and de-
velopment in information retrieval, New York, NY.
D. Inkpen and G. Hirst. 2002. Acquiring collocations for
lexical choice between near synonyms. In SIGLEX Work-
shop on Unsupervised Lexical Acquisition, 40th meeting
of the ACL, Philadelphia.
K. Kita, Y. Kato, T. Omoto, and Y. Yano. 1994. A compar-
ative study of automatic extraction of collocations from
corpora: Mutual information vs. cost criteria. Journal of
Natural Language Processing.
B. Krenn. 2000. The Usual Suspects: Data-Oriented Models
for Identification and Representation of Lexical Colloca-
tions. Ph.D. thesis, Saarland University.
C. D. Manning and H. Sch?tze. 1999. Foundations of Statis-
tical Natural Language Processing. The MIT Press, Cam-
bridge, Massachusetts.
R. Mihalcea and T. Pedersen. 2003. An evaluation exercise
for word alignment. In Proceedings of HLT-NAACL Work-
shop, Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, Edmonton, Alberta.
R. C. Moore. 2004. On log-likelihood-ratios and the signif-
icance of rare events. In Proceedings of the 2004 Confer-
ence on EMNLP, Barcelona, Spain.
D. Pearce. 2002. A comparative evaluation of collocation ex-
traction techniques. In Third International Conference on
language Resources and Evaluation, Las Palmas, Spain.
P. Pecina. 2005. An extensive empirical study of colloca-
tion extraction methods. In Proceedings of the ACL 2005
Student Research Workshop, Ann Arbor, USA.
S. Shimohata, T. Sugio, and J. Nagata. 1997. Retrieving col-
locations by co-occurrences and word order constraints.
In Proc. of the 35th Meeting of ACL/EACL, Madrid, Spain.
W. N. Venables and B. D. Ripley. 2002. Modern Applied
Statistics with S. 4th ed. Springer Verlag, New York.
C. Zhai. 1997. Exploiting context to identify lexical atoms:
A statistical view of linguistic context. In International
and Interdisciplinary Conf. on Modeling and Using Context.
PDT 2.0. 2006. http://ufal.mff.cuni.cz/pdt2.0/.
658
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 33?36,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
A Simple Automatic MT Evaluation Metric
Petr Homola
Charles University
Prague, Czech Republic
Vladislav Kubon?
Charles University
Prague, Czech Republic
{homola|vk|pecina}@ufal.mff.cuni.cz
Pavel Pecina
Charles University
Prague, Czech Republic
Abstract
This paper describes a simple evaluation
metric for MT which attempts to overcome
the well-known deficits of the standard
BLEU metric from a slightly different an-
gle. It employes Levenshtein?s edit dis-
tance for establishing alignment between
the MT output and the reference transla-
tion in order to reflect the morphological
properties of highly inflected languages. It
also incorporates a very simple measure
expressing the differences in the word or-
der. The paper also includes evaluation on
the data from the previous SMT workshop
for several language pairs.
1 Introduction
The problem of finding a reliable machine trans-
lation metrics corresponding with a human judg-
ment has recently returned to the centre of atten-
tion. After a brief period following the introduc-
tion of generally accepted and widely used met-
rics, BLEU (Papineni et al, 2002) and NIST (Dod-
dington, 2002), when it seemed that this persistent
problem has finally been solved, the researchers
active in the field of machine translation (MT)
started to express their worries that although these
metrics are simple, fast and able to provide con-
sistent results for a particular system during its de-
velopment, they are not sufficiently reliable for the
comparison of different systems or different lan-
guage pairs.
The results of the NIST evaluation in 2005
(Le and Przybocki, 2005) have also strengthened
the suspicion that the correlation between human
judgment and the BLEU and NIST measures is not
as strong as it was widely believed. Both mea-
sures seem to favor the MT output created by sys-
tems based on n-gram architecture, they are un-
able to take into account certain factors which are
very important for the human judges of translation
quality.
The article (Callison-Burch et al, 2006) thor-
oughly discusses the deficits of the BLEU and
similar metrics. The authors claim that the existing
automatic metrics, including some of the new and
seemingly more reliable ones as e.g. Meteor (cf.
(Banerjee and Lavie, 2005)) ?. . . they are all quite
rough measures of translation similarity, and have
inexact models of allowable variation in transla-
tion.? This claim is supported by a construction of
translation variations which have identical BLEU
score, but which are very different for a human
judge. The authors identify three prominent fac-
tors which contribute to the inadequacy of BLEU ?
the failure to deal with synonyms and paraphrases,
no penalties for missing content, and the crudeness
of the brevity penalty.
Let us add some more factors based on our ex-
periments with languages typologically different
than English, Arabic or Chinese, which are prob-
ably the languages most frequently used in recent
shared-task MT evaluations. The highly inflected
languages and languages with a higher degree of
word-order freedom may provide additional ex-
amples of sentences in which relatively small al-
terations of correct word forms may have a dire
effect on the BLEU score while the sentence still
remains understandable and acceptable for human
evaluators.
The effect of rich inflection has been observed
for example in (Ty?novsky?, 2007), where the au-
thor mentions the fact that the BLEU score used
for measuring the improvements in his experimen-
tal Czech-German EBMT system penalized heav-
ily all subtle errors in Czech morphology arising
from an out-of-context combined partial transla-
tions taken from different examples.
The problem of the insensitivity of BLEU to the
variations of the order of n-grams identified in ref-
erence translations has already been mentioned in
33
the paper (Callison-Burch et al, 2006). The au-
thors showed examples where changing a good
word order into an unacceptable one did not af-
fect the BLEU score. We may add a different ex-
ample documenting the phenomenon that a pair
of syntactically correct Czech sentences with the
same word forms, differing only in the word order
whose n-gram score for n = 2, 3, and 4 differs
greatly. Let us take one of the sentences from the
2008 SMT workshop and its reference translation:
When Caligula appointed his horse to the Sen-
ate, the horse at least did not have blood on its
hoofs. ? Kdyz? Caligula zvolil do sena?tu sve?ho
kone?, neme?l jeho ku?n? aspon? na kopytech krev.
If we modify the Czech reference sentence into
Kdyz? sve?ho kone? do sena?tu zvolil Caligula, jeho
ku?n? aspon? neme?l na kopytech krev., we destroy 8
out of 15 bigrams, 11 out of 14 trigrams and 12
out of 13 quadrigrams while we still have sentence
with almost identical meaning and probably very
similar human evaluation. The BLEU score of the
modified sentence is, however, lower than it would
be for the identical copy of the reference transla-
tion.
2 The description of the proposed metric
There is one aspect of the problem of a MT
quality metric which tends to be overlooked but
which is very important from the practical point
of view. This aspect concerns the expected diffi-
culties when post-editing the MT output. It is very
important for everybody who really wants to use
the MT output and who faces the decision whether
it is better to post-edit the MT output or whether a
new translation made by human translators would
be faster and more efficient way towards the de-
sired quality. It is no wonder that such a met-
ric is mentioned only in connection with systems
which really aim at practical exploitation, not with
a majority of experimental MT system which will
hardly ever reach the stage of industrial exploita-
tion.
We have described one example of such practi-
cally oriented metric in (Hajic? et al, 2003). The
metric exploits the matching algorithm of Trados
Translator?s Workbench for obtaining the percent-
age of differences between the MT output and the
reference translation (created by post-editing the
MT output). The advantage of this measure is its
close connection to the real world of human trans-
lating by means of translation memory, the disad-
vantage concerns the use of a proprietary match-
ing algorithm which has not been made public and
which requires the actual use of the Trados soft-
ware.
Nevertheless, the matching algorithm of Trados
gives results which to a great extent correspond
to a much simpler traditional metric, to the Lev-
enshtein?s edit distance. The use of this metric
may help to refine a very strict treatment of word-
form differences by BLEU. A similar approach at
the level of unigram matching has been used by
the well-known METEOR metric (Agarwal and
Lavie, 2008), which proved its qualities during the
previous MT evaluation task in 2008 (Callison-
Burch et al, 2008). Meteor uses Porter stemmer
as one step in the word alignment algorithm. It
also relies on synonymy relations in WordNet.
When designing our metric, we have decided to
follow two general strategies ? to use as simple
means as possible and to avoid using any language
dependent tools or resources. Levenshtein metric
(or its modification for word-level edit distance)
therefore seemed to be the best candidate for sev-
eral aspects of the proposed measure.
The first aspect we have decided to include was
the inflection. The edit distance has one advan-
tage over the language independent stemmer ? it
can uniformly handle the differences regardless of
their position in the string. The stemmer will prob-
ably face certain problems with changes inside the
stem as e.g. in the Czech equivalent of the word
house in different cases du?m (nom.sg) ? domu
(gen., dat. or loc. sg.) or German Mann in differ-
ent numbers der Mann (sg.) ? die Ma?nner (pl.),
while the edit distance will treat them uniformly
with the variation of prefixes, suffixes and infixes.
As mentioned above, we have also intended to
aim at the treatment of the free word order in our
metric. However this seems to be one of the ma-
jor flaws of the BLEU score, it turned out that the
word order is extremely difficult if we stick to the
use of simple and language independent means. If
we take Czech as an example of a language with
relatively high degree of word-order freedom, we
can still find certain restrictions (e.g. the sentence-
second position of clitics, their mutual order, the
adjectives typically, but not always preceding the
nouns they depend upon etc.) which will defi-
nitely influence the human judgment of the accept-
ability of a particular sentence. These restrictions
are language dependent (for example Polish, the
34
language very closely related to Czech, has dif-
ferent rules for congruent attributes, the adjectives
stand much more often to the right of the govern-
ing noun) and they are also very difficult to capture
algorithmically. If the MT output is compared to
a single reference translation only, there is, in fact,
no way how the metric could account for the pos-
sible correct variations of the word order without
exploiting very deep language dependent informa-
tion. If there are more reference translations, it is
possible that they will provide the natural varia-
tions of the word order, but it, in fact, means that
if we want to stick to the above mentioned require-
ments, we have to give up the hope that our metric
will capture this important phenomenon.
2.1 Word alignment algorithm
In order to capture the word form variations
caused by the inflection, we have decided to em-
ploy the following alignment algorithm at the level
of individual word forms. Let us use the follow-
ing notation: Let the reference translation R be a
sequence of words ri, where i ?< 1, . . . , n >.
Let the MT output T be a sequence of words tj,
where j ?< 1, . . . ,m >. Let us also set a thresh-
old of similarity s ?< 0, 1 >. (s roughly ex-
presses how different the forms of a lemma may
be. The idea behind this criterion is that a mistake
in one morphological category (reflected mostly
by a different ending of the corresponding word
form) is not as serious as a completely different
lexeme. This holds especially for morphologically
rich languages that can have tens or even hun-
dreds of distinct word forms for a single lemma.)
Starting from t1, let us find for each tj the best
ri for i ?< 1, . . . , n > such that the edit dis-
tance dj from tj to ri normalized by the length
of tj is minimal and at the same time dj < s.
If the ri is already aligned to some tk, k < j
and the edit distance dk > dj , then align tj to
ri and re-calculate the alignment for tk to its sec-
ond best candidate, otherwise take the second best
candidate rl conforming with the above mentioned
conditions and align it to tj . As a result of this
process, we get the alignment score ATR from T
to R. ATR =
?
(1?di)
m (for i ?< 1, . . . , n >)
where di = 1 for those word forms ti which are
not aligned to any of the word forms rj from R.
Then we calculate the alignment score ART using
the same algorithm and aligning the words from R
to T. The similarity score S equals the minimum
from ATR and ART . The way how the similar-
ity score S is constructed ensures that the score
takes into account a difference in length between
T and R, therefore it is not necessary to include
any brevity penalty into the metric.
2.2 A structural metric
In order to express word-order difference between
the MT output and the reference translation we
have designed a structural part of the metric. It
is based on an algorithm similar to one of the stan-
dard sorting methods, an insert sort. The refer-
ence translation R represents the desired word or-
der and the algorithm counts the number of op-
erations necessary for obtaining the correct word
order from the word order of the MT output T by
inserting the words ti to their desired positions rj
(ti is aligned to rj). If a particular word ti is not
aligned to any rj , a penalty of 1 is added to the
number of operations.
2.3 A combination of both metrics
The overall score is computed as a weighted aver-
age of both metrics mentioned above. Let L be the
lexical similarity score and M the structural score
based on a word mapping. Then then overall score
S can be obtained as follows:
S = aL+ bM
The coefficients a and b must sum up to one.
They allow to capture the difference in the degree
of word-order freedom among target languages.
The coefficient b should be set lower for the tar-
get languages with more free word-order. Because
both then partial measures L andM have values in
the interval < 0, 1 >, the value of S will also fall
into this interval.
3 The experiment
We have performed a test of the proposed met-
ric using the data from the last year?s SMT work-
shop.1 The parameters a, b, and s have been set to
the same value for all evaluated language pairs, no
language dependent alterations were tested in this
experiment:
Parameter Value
s 0.15
a 0.9
b 0.1
1The data are available at http://www.statmt.org/wmt08.
35
The values for the parameters have been set up
empirically with special attention being paid to
Czech, the only language with really rich inflec-
tion among the languages being tested.
We have performed sentence-level and system-
level evaluation using the Spearman?s rank corre-
lation coefficient which is defined as follows:
? = 1?
6
?
d2i
n(n2 ? 1)
where di = xi?yi is the difference between the
ranks of corresponding values Xi and Yi and n is
the number of values in each data set.
The following scores express the correlation of
our automatic metric and the human judgements
for the language pairs English-Czech and English-
German. The sentence-level correlation ?sent is
the average of Spearman?s ? across all sentences.
Language pair Metric ?sent ?sys
English-Czech proposed 0.20 0.50
English-Czech BLEU 0.21 0.50
English-German proposed 0.91 0.37
English-German BLEU 0.90 0.20
3.1 Conclusions
The metric presented in this paper attempts to
combine some of the important factors which
seem to be neglected by some generally accepted
MT evaluation metrics. Inspired by the fact that
human judges tend to accept incorrect word-forms
of corectly translated lemmas, it employs a simi-
larity measure relaxing the requirements on iden-
tity (or similarity) of matching word forms in the
MT output and the reference translation. At the
same time, it also incorporates a penalty for dif-
ferent length of the MT output and the reference
translation. The second component of the metric
tackles the problem of incorrect word-order. The
constants used in the metric allow to set the weight
of its two components with regard to the target lan-
guage properties.
The experiments performed on the data from
the previous shared evaluation task are promising.
They indicate that the first component of the met-
ric succesfully replaces the strict unigram mea-
sure used in BLEU while the second component
may require certain alteration in order to achieve a
higher correlation with human judgement.
Acknowledgments
The presented research has been supported by the
grant No. 1ET100300517 of the GAAV C?R and
by Ministry of Education of the Czech Republic,
project MSM 0021620838.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor,
M-BLEU and M-TER: Evaluation metrics for high
correlation with human rankings of machine trans-
lation output. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 115-
118. Columbus, Ohio, Association for Computa-
tional Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments.. In Workshop
on Intrinsic and Extrinsic Evaluation Measures for
MT and/or Summarization, Ann Arbor, Michigan.
Chris Callison-Burch, Miles Osborne, Philipp Koehn.
2006. Re-evaluating the Role of BLEU in Ma-
chine Translation Research.. In Proceedings of the
EACL?06, Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, Josh Schroeder. 2008.
Further Meta-Evaluation of Machine Translation..
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70-106, Columbus,
Ohio. Association for Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, San Diego, California,USA
Jan Hajic?, Petr Homola, Vladislav Kubon?. 2003. A
Simple Multilingual Machine Translation System..
In Proceedings of the MT Summit IX, New Orleans,
USA.
Kishore Papineni, Salim Roukos, ToddWard, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation.. In Proceedings of
ACL 2002.
Audrey Le and Mark Przybocki. 2005. NIST 2005
machine translation evaluation official results.. Of-
ficial release of automatic evaluation scores for all
submissions.
Miroslav Ty?novsky?. 2007. Exploitation of Linguis-
tic Information in EBMT.. Master thesis at Charles
University in Prague, Faculty of Mathematics and
Physics.
36
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 634?639,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Simpler unsupervised POS tagging with bilingual projections
Long Duong, 12 Paul Cook, 1 Steven Bird, 1 and Pavel Pecina2
1 Department of Computing and Information Systems, The University of Melbourne
2 Charles University in Prague, Czech Republic
lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au,
sbird@unimelb.edu.au, pecina@ufal.mff.cuni.cz
Abstract
We present an unsupervised approach to
part-of-speech tagging based on projec-
tions of tags in a word-aligned bilingual
parallel corpus. In contrast to the exist-
ing state-of-the-art approach of Das and
Petrov, we have developed a substantially
simpler method by automatically identi-
fying ?good? training sentences from the
parallel corpus and applying self-training.
In experimental results on eight languages,
our method achieves state-of-the-art re-
sults.
1 Unsupervised part-of-speech tagging
Currently, part-of-speech (POS) taggers are avail-
able for many highly spoken and well-resourced
languages such as English, French, German, Ital-
ian, and Arabic. For example, Petrov et al (2012)
build supervised POS taggers for 22 languages us-
ing the TNT tagger (Brants, 2000), with an aver-
age accuracy of 95.2%. However, many widely-
spoken languages ? including Bengali, Javanese,
and Lahnda ? have little data manually labelled
for POS, limiting supervised approaches to POS
tagging for these languages.
However, with the growing quantity of text
available online, and in particular, multilingual
parallel texts from sources such as multilin-
gual websites, government documents and large
archives of human translations of books, news, and
so forth, unannotated parallel data is becoming
more widely available. This parallel data can be
exploited to bridge languages, and in particular,
transfer information from a highly-resourced lan-
guage to a lesser-resourced language, to build un-
supervised POS taggers.
In this paper, we propose an unsupervised ap-
proach to POS tagging in a similar vein to the
work of Das and Petrov (2011). In this approach,
a parallel corpus for a more-resourced language
having a POS tagger, and a lesser-resourced lan-
guage, is word-aligned. These alignments are ex-
ploited to infer an unsupervised tagger for the tar-
get language (i.e., a tagger not requiring manually-
labelled data in the target language). Our ap-
proach is substantially simpler than that of Das
and Petrov, the current state-of-the art, yet per-
forms comparably well.
2 Related work
There is a wealth of prior research on building un-
supervised POS taggers. Some approaches have
exploited similarities between typologically simi-
lar languages (e.g., Czech and Russian, or Telugu
and Kannada) to estimate the transition probabil-
ities for an HMM tagger for one language based
on a corpus for another language (e.g., Hana et al,
2004; Feldman et al, 2006; Reddy and Sharoff,
2011). Other approaches have simultaneously
tagged two languages based on alignments in a
parallel corpus (e.g., Snyder et al, 2008).
A number of studies have used tag projection
to copy tag information from a resource-rich to
a resource-poor language, based on word align-
ments in a parallel corpus. After alignment, the
resource-rich language is tagged, and tags are pro-
jected from the source language to the target lan-
guage based on the alignment (e.g., Yarowsky and
Ngai, 2001; Das and Petrov, 2011). Das and
Petrov (2011) achieved the current state-of-the-art
for unsupervised tagging by exploiting high con-
fidence alignments to copy tags from the source
language to the target language. Graph-based la-
bel propagation was used to automatically produce
more labelled training data. First, a graph was
constructed in which each vertex corresponds to
a unique trigram, and edge weights represent the
syntactic similarity between vertices. Labels were
then propagated by optimizing a convex function
to favor the same tags for closely related nodes
634
Model Coverage Accuracy
Many-to-1 alignments 88% 68%
1-to-1 alignments 68% 78%
1-to-1 alignments: Top 60k sents 91% 80%
Table 1: Token coverage and accuracy of many-
to-one and 1-to-1 alignments, as well as the top
60k sentences based on alignment score for 1-to-1
alignments, using directly-projected labels only.
while keeping a uniform tag distribution for un-
related nodes. A tag dictionary was then extracted
from the automatically labelled data, and this was
used to constrain a feature-based HMM tagger.
The method we propose here is simpler to that
of Das and Petrov in that it does not require con-
vex optimization for label propagation or a feature
based HMM, yet it achieves comparable results.
3 Tagset
Our tagger exploits the idea of projecting tag infor-
mation from a resource-rich to resource-poor lan-
guage. To facilitate this mapping, we adopt Petrov
et al?s (2012) twelve universal tags: NOUN,
VERB, ADJ, ADV, PRON (pronouns), DET (de-
terminers and articles), ADP (prepositions and
postpositions), NUM (numerals), CONJ (conjunc-
tions), PRT (particles), ?.? (punctuation), and X
(all other categories, e.g., foreign words, abbrevia-
tions). These twelve basic tags are common across
taggers for most languages.
Adopting a universal tagset avoids the need
to map between a variety of different, language-
specific tagsets. Furthermore, it makes it possi-
ble to apply unsupervised tagging methods to lan-
guages for which no tagset is available, such as
Telugu and Vietnamese.
4 A Simpler Unsupervised POS Tagger
Here we describe our proposed tagger. The key
idea is to maximize the amount of information
gleaned from the source language, while limit-
ing the amount of noise. We describe the seed
model and then explain how it is successively re-
fined through self-training and revision.
4.1 Seed Model
The first step is to construct a seed tagger from
directly-projected labels. Given a parallel corpus
for a source and target language, Algorithm 1 pro-
vides a method for building an unsupervised tag-
ger for the target language. In typical applications,
the source language would be a better-resourced
language having a tagger, while the target lan-
guage would be lesser-resourced, lacking a tagger
and large amounts of manually POS-labelled data.
Algorithm 1 Build seed model
1: Tag source side.
2: Word align the corpus with Giza++ and re-
move the many-to-one mappings.
3: Project tags from source to target using the re-
maining 1-to-1 alignments.
4: Select the top n sentences based on sentence
alignment score.
5: Estimate emission and transition probabilities.
6: Build seed tagger T.
We eliminate many-to-one alignments (Step 2).
Keeping these would give more POS-tagged to-
kens for the target side, but also introduce noise.
For example, suppose English and French were
the source and target language, respectively. In
this case alignments such as English laws (NNS)
to French les (DT) lois (NNS) would be expected
(Yarowsky and Ngai, 2001). However, in Step 3,
where tags are projected from the source to target
language, this would incorrectly tag French les as
NN. We build a French tagger based on English?
French data from the Europarl Corpus (Koehn,
2005). We also compare the accuracy and cov-
erage of the tags obtained through direct projec-
tion using the French Melt POS tagger (Denis and
Sagot, 2009). Table 1 confirms that the one-to-one
alignments indeed give higher accuracy but lower
coverage than the many-to-one alignments. At
this stage of the model we hypothesize that high-
confidence tags are important, and hence eliminate
the many-to-one alignments.
In Step 4, in an effort to again obtain higher
quality target language tags from direct projection,
we eliminate all but the top n sentences based on
their alignment scores, as provided by the aligner
via IBM model 3. We heuristically set this cutoff
to 60k to balance the accuracy and size of the seed
model.1 Returning to our preliminary English?
French experiments in Table 1, this process gives
improvements in both accuracy and coverage.2
1We considered values in the range 60?90k, but this
choice had little impact on the accuracy of the model.
2We also considered using all projected labels for the top
60k sentences, not just 1-to-1 alignments, but in preliminary
experiments this did not perform as well, possibly due to the
previously-observed problems with many-to-one alignments.
635
The number of parameters for the emission prob-
ability is |V | ? |T | where V is the vocabulary and
T is the tag set. The transition probability, on the
other hand, has only |T |3 parameters for the tri-
gram model we use. Because of this difference
in number of parameters, in step 5, we use dif-
ferent strategies to estimate the emission and tran-
sition probabilities. The emission probability is
estimated from all 60k selected sentences. How-
ever, for the transition probability, which has less
parameters, we again focus on ?better? sentences,
by estimating this probability from only those sen-
tences that have (1) token coverage > 90% (based
on direct projection of tags from the source lan-
guage), and (2) length > 4 tokens. These cri-
teria aim to identify longer, mostly-tagged sen-
tences, which we hypothesize are particularly use-
ful as training data. In the case of our preliminary
English?French experiments, roughly 62% of the
60k selected sentences meet these criteria and are
used to estimate the transition probability. For un-
aligned words, we simply assign a random POS
and very low probability, which does not substan-
tially affect transition probability estimates.
In Step 6 we build a tagger by feeding the es-
timated emission and transition probabilities into
the TNT tagger (Brants, 2000), an implementation
of a trigram HMM tagger.
4.2 Self training and revision
For self training and revision, we use the seed
model, along with the large number of target lan-
guage sentences available that have been partially
tagged through direct projection, in order to build
a more accurate tagger. Algorithm 2 describes
this process of self training and revision, and as-
sumes that the parallel source?target corpus has
been word aligned, with many-to-one alignments
removed, and that the sentences are sorted by
alignment score. In contrast to Algorithm 1, all
sentences are used, not just the 60k sentences with
the highest alignment scores.
We believe that sentence alignment score might
correspond to difficulty to tag. By sorting the sen-
tences by alignment score, sentences which are
more difficult to tag are tagged using a more ma-
ture model. Following Algorithm 1, we divide
sentences into blocks of 60k.
In step 3 the tagged block is revised by com-
paring the tags from the tagger with those ob-
tained through direct projection. Suppose source
Algorithm 2 Self training and revision
1: Divide target language sentences into blocks
of n sentences.
2: Tag the first block with the seed tagger.
3: Revise the tagged block.
4: Train a new tagger on the tagged block.
5: Add the previous tagger?s lexicon to the new
tagger.
6: Use the new tagger to tag the next block.
7: Goto 3 and repeat until all blocks are tagged.
language word wsi is aligned with target language
word wtj with probability p(wtj |wsi ), T si is the tag
for wsi using the tagger available for the source
language, and T tj is the tag for wtj using the tagger
learned for the target language. If p(wtj |wsi ) > S,
where S is a threshold which we heuristically set
to 0.7, we replace T tj by T si .
Self-training can suffer from over-fitting, in
which errors in the original model are repeated
and amplified in the new model (McClosky et al,
2006). To avoid this, we remove the tag of
any token that the model is uncertain of, i.e., if
p(wtj |wsi ) < S and T tj ?= T si then T tj = Null. So,
on the target side, aligned words have a tag from
direct projection or no tag, and unaligned words
have a tag assigned by our model.
Step 4 estimates the emission and transition
probabilities as in Algorithm 1. In Step 5, emis-
sion probabilities for lexical items in the previous
model, but missing from the current model, are
added to the current model. Later models therefore
take advantage of information from earlier mod-
els, and have wider coverage.
5 Experimental Results
Using parallel data from Europarl (Koehn, 2005)
we apply our method to build taggers for the same
eight target languages as Das and Petrov (2011)
? Danish, Dutch, German, Greek, Italian, Por-
tuguese, Spanish and Swedish ? with English as
the source language. Our training data (Europarl)
is a subset of the training data of Das and Petrov
(who also used the ODS United Nations dataset
which we were unable to obtain). The evaluation
metric and test data are the same as that used by
Das and Petrov. Our results are comparable to
theirs, although our system is penalized by having
less training data. We tag the source language with
the Stanford POS tagger (Toutanova et al, 2003).
636
Danish Dutch German Greek Italian Portuguese Spanish Swedish Average
Seed model 83.7 81.1 83.6 77.8 78.6 84.9 81.4 78.9 81.3
Self training + revision 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4
Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Table 2: Token-level POS tagging accuracy for our seed model, self training and revision, and the method
of Das and Petrov (2011). The best results on each language, and on average, are shown in bold.
0 5 10 15 20 25 30
Iteration
50
60
70
80
Pe
rce
nta
ge
s Overall Acc
Know Acc
OOV Acc
Know tkn
0 5 10 15 20 25 30
Iteration
70
75
80
85
90
Pe
rce
nta
ge
s Overall Acc
Know Acc
OOV Acc
Know tkn
Figure 1: Overall accuracy, accuracy on known tokens, accuracy on unknown tokens, and proportion of
known tokens for Italian (left) and Dutch (right).
Table 2 shows results for our seed model, self
training and revision, and the results reported by
Das and Petrov. Self training and revision im-
prove the accuracy for every language over the
seed model, and gives an average improvement
of roughly two percentage points. The average
accuracy of self training and revision is on par
with that reported by Das and Petrov. On individ-
ual languages, self training and revision and the
method of Das and Petrov are split ? each per-
forms better on half of the cases. Interestingly, our
method achieves higher accuracies on Germanic
languages ? the family of our source language,
English?while Das and Petrov perform better on
Romance languages. This might be because our
model relies on alignments, which might be more
accurate for more-related languages, whereas Das
and Petrov additionally rely on label propagation.
Compared to Das and Petrov, our model per-
forms poorest on Italian, in terms of percentage
point difference in accuracy. Figure 1 (left panel)
shows accuracy, accuracy on known words, accu-
racy on unknown words, and proportion of known
tokens for each iteration of our model for Italian;
iteration 0 is the seed model, and iteration 31 is
the final model. Our model performs poorly on
unknown words as indicated by the low accuracy
on unknown words, and high accuracy on known
words compared to the overall accuracy. The poor
performance on unknown words is expected be-
cause we do not use any language-specific rules
to handle this case. Moreover, on average for the
final model, approximately 10% of the test data
tokens are unknown. One way to improve the per-
formance of our tagger might be to reduce the pro-
portion of unknown words by using a larger train-
ing corpus, as Das and Petrov did.
We examine the impact of self-training and re-
vision over training iterations. We find that for
all languages, accuracy rises quickly in the first
5?6 iterations, and then subsequently improves
only slightly. We exemplify this in Figure 1 (right
panel) for Dutch. (Findings are similar for other
languages.) Although accuracy does not increase
much in later iterations, they may still have some
benefit as the vocabulary size continues to grow.
6 Conclusion
We have proposed a method for unsupervised POS
tagging that performs on par with the current state-
of-the-art (Das and Petrov, 2011), but is substan-
tially less-sophisticated (specifically not requiring
convex optimization or a feature-based HMM).
The complexity of our algorithm is O(nlogn)
compared to O(n2) for that of Das and Petrov
637
(2011) where n is the size of training data.3 We
made our code are available for download.4
In future work we intend to consider using a
larger training corpus to reduce the proportion of
unknown tokens and improve accuracy. Given
the improvements of our model over that of Das
and Petrov on languages from the same family
as our source language, and the observation of
Snyder et al (2008) that a better tagger can be
learned from a more-closely related language, we
also plan to consider strategies for selecting an ap-
propriate source language for a given target lan-
guage. Using our final model with unsupervised
HMM methods might improve the final perfor-
mance too, i.e. use our final model as the ini-
tial state for HMM, then experiment with differ-
ent inference algorithms such as ExpectationMax-
imization (EM), Variational Bayers (VB) or Gibbs
sampling (GS).5 Gao and Johnson (2008) compare
EM, VB and GS for unsupervised English POS
tagging. In many cases, GS outperformed other
methods, thus we would like to try GS first for our
model.
7 Acknowledgements
This work is funded by Erasmus Mundus
European Masters Program in Language and
Communication Technologies (EM-LCT) and
by the Czech Science Foundation (grant no.
P103/12/G084). We would like to thank Prokopis
Prokopidis for providing us the Greek Treebank
and Antonia Marti for the Spanish CoNLL 06
dataset. Finally, we thank Siva Reddy and Span-
dana Gella for many discussions and suggestions.
References
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the sixth con-
ference on Applied natural language processing
(ANLP ?00), pages 224?231. Seattle, Washing-
ton, USA.
Dipanjan Das and Slav Petrov. 2011. Unsu-
pervised part-of-speech tagging with bilingual
graph-based projections. In Proceedings of
3We re-implemented label propagation from Das and
Petrov (2011). It took over a day to complete this step on
an eight core Intel Xeon 3.16GHz CPU with 32 Gb Ram, but
only 15 minutes for our model.
4https://code.google.com/p/universal-tagger/
5We in fact have tried EM, but it did not help. The overall
performance dropped slightly. This might be because self-
training with revision already found the local maximal point.
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies - Volume 1 (ACL 2011), pages
600?609. Portland, Oregon, USA.
Pascal Denis and Beno??t Sagot. 2009. Coupling
an annotated corpus and a morphosyntactic lex-
icon for state-of-the-art POS tagging with less
human effort. In Proceedings of the 23rd Pa-
cific Asia Conference on Language, Information
and Computation, pages 721?736. Hong Kong,
China.
Anna Feldman, Jirka Hana, and Chris Brew. 2006.
A cross-language approach to rapid creation of
new morpho-syntactically annotated resources.
In Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC?06), pages 549?554. Genoa, Italy.
Jianfeng Gao and Mark Johnson. 2008. A com-
parison of bayesian estimators for unsupervised
hidden markov model pos taggers. In Proceed-
ings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?08,
pages 344?352. Association for Computational
Linguistics, Stroudsburg, PA, USA.
Jiri Hana, Anna Feldman, and Chris Brew. 2004.
A resource-light approach to Russian morphol-
ogy: Tagging Russian using Czech resources.
In Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Process-
ing (EMNLP ?04), pages 222?229. Barcelona,
Spain.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Proceed-
ings of the Tenth Machine Translation Summit
(MT Summit X), pages 79?86. AAMT, Phuket,
Thailand.
David McClosky, Eugene Charniak, and Mark
Johnson. 2006. Effective self-training for pars-
ing. In Proceedings of the main conference on
Human Language Technology Conference of the
North American Chapter of the Association of
Computational Linguistics (HLT-NAACL ?06),
pages 152?159. New York, USA.
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012. A universal part-of-speech tagset. In
Proceedings of the Eight International Confer-
ence on Language Resources and Evaluation
(LREC?12), pages 2089?2096. Istanbul, Turkey.
Siva Reddy and Serge Sharoff. 2011. Cross lan-
guage POS Taggers (and other tools) for Indian
638
languages: An experiment with Kannada using
Telugu resources. In Proceedings of the IJC-
NLP 2011 workshop on Cross Lingual Infor-
mation Access: Computational Linguistics and
the Information Need of Multilingual Societies
(CLIA 2011). Chiang Mai, Thailand.
Benjamin Snyder, Tahira Naseem, Jacob Eisen-
stein, and Regina Barzilay. 2008. Unsupervised
multilingual learning for POS tagging. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP
?08), pages 1041?1050. Honolulu, Hawaii.
Kristina Toutanova, Dan Klein, Christopher D.
Manning, and Yoram Singer. 2003. Feature-
rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the
2003 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics on Human Language Technology - Vol-
ume 1 (NAACL ?03), pages 173?180. Edmon-
ton, Canada.
David Yarowsky and Grace Ngai. 2001. Induc-
ing multilingual POS taggers and NP bracketers
via robust projection across aligned corpora. In
Proceedings of the Second Meeting of the North
American Chapter of the Association for Com-
putational Linguistics on Language technolo-
gies (NAACL ?01), pages 1?8. Pittsburgh, Penn-
sylvania, USA.
639
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143?148,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MATREX: The DCU MT System for WMT 2010
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat, Pratyush Banerjee, Ankit K. Srivastava,
Jinhua Du, Pavel Pecina, Sudip Kumar Naskar, Mikel L. Forcada, Andy Way
CNGL, School of Computing
Dublin City University, Dublin 9, Ireland
{ spenkale, rhaque, sdandapat, pbanerjee, asrivastava, jdu, ppecina, snaskar, mforcada, away }@computing.dcu.ie
Abstract
This paper describes the DCU machine
translation system in the evaluation cam-
paign of the Joint Fifth Workshop on Sta-
tistical Machine Translation and Metrics
in ACL-2010. We describe the modular
design of our multi-engine machine trans-
lation (MT) system with particular focus
on the components used in this partici-
pation. We participated in the English?
Spanish and English?Czech translation
tasks, in which we employed our multi-
engine architecture to translate. We also
participated in the system combination
task which was carried out by the MBR
decoder and confusion network decoder.
1 Introduction
In this paper, we present the DCU multi-engine
MT system MATREX (Machine Translation using
Examples). This system exploits example-based
MT, statistical MT (SMT), and system combina-
tion techniques.
We participated in the English?Spanish (en?
es) and English?Czech (en?cs) translation
tasks. For these two tasks, we employ several
individual MT systems: 1) Baseline: phrase-
based SMT (Koehn et al, 2007); 2) EBMT:
Monolingually chunking both source and target
sides of the dataset using a marker-based chunker
(Gough and Way, 2004); 3) Factored translation
model (Koehn and Hoang, 2007); 4) Source-side
context-informed (SSCI) systems (Stroppa et al,
2007); 5) the moses-chart (a Moses imple-
mentation of the hierarchical phrase-based (HPB)
approach of Chiang (2007)) and 6) Apertium (For-
cada et al, 2009) rule-based machine translation
(RBMT). Finally, we use a word-level combina-
tion framework (Rosti et al, 2007) to combine the
multiple translation hypotheses and employ a new
rescoring model to generate the final translation.
For the system combination task, we first use
the minimum Bayes-risk (MBR) (Kumar and
Byrne, 2004) decoder to select the best hypoth-
esis as the alignment reference for the confusion
network (CN) (Mangu et al, 2000). We then build
the CN using the TER metric (Snover et al, 2006),
and finally search for the best translation.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the multi-engine strate-
gies used for the shared task. In Section 3, we
outline the complete system setup for the shared
task and provide evaluation results on the test set.
Section 4 concludes the paper.
2 The MATREX System
2.1 System Architecture
The MATREX system is a combination-based
multi-engine architecture, which exploits as-
pects of both the EBMT and SMT paradigms.
The architecture includes various individual sys-
tems: phrase-based, example-based, hierarchical
phrase-based and tree-based MT.
The combination structure uses the MBR and
CN decoders, and is based on a word-level com-
bination strategy (Du et al, 2009). In the final
stage, we use a new rescoring module to process
the N -best list generated by the combination mod-
ule. Figure 1 illustrates the architecture.
2.2 Example-Based Machine Translation
The EBMT system uses a language-specific, re-
duced set of closed-class marker morphemes or
lexemes (Gough and Way, 2004) to define a way
to segment sentences into chunks, which are then
aligned using an edit-distance-style algorithm, in
which edit costs depend on word-to-word transla-143
Figure 1: System Framework.
tion probabilities and the amount of word-to-word
cognates (Stroppa and Way, 2006).
Once these phrase pairs were obtained they
were merged with the phrase pairs extracted by
the baseline system adding word alignment infor-
mation.
2.3 Apertium RBMT
Apertium1 is a free/open-source platform for
RBMT. The current version of the en?es system
in Apertium was used for the system combination
task (section 2.7), and its morphological analysers
and part-of-speech taggers were used to build a
factored Moses model.
2.4 Factored Translation Model
We also used a factored model for the en?es
translation task. Factored models (Koehn and
Hoang, 2007) facilitate the translation by break-
ing it down into several factors which are further
combined using a log-linear model (Och and Ney,
2002).
We used three factors in our factored translation
model, which are used in two different decoding
paths: a surface form (SF) to SF translation factor,
a lemma to lemma translation factor, and a part-of-
speech (PoS) to PoS translation factor.
Finally, we used two decoding paths based on
1http://www.apertium.org
the above three translation factors: an SF to SF
decoding path and a path which maps lemma to
lemma, PoS to PoS, and an SF generated using
the TL lemma and PoS. The lemmas and PoS for
en and es were obtained using Apertium (sec-
tion 2.3).
2.5 Source-Side Context-informed PB-SMT
One natural way to express a context-informed
feature (h?MBL) is to view it as the conditional
probability of the target phrases (e?k) given the
source phrase (f?k) and its source-side context in-
formation (CI):
h?MBL = logP (e?k|f?k,CI(f?k)) (1)
We use a memory-based machine learning
(MBL) classifier (TRIBL:2 Daelemans and
van den Bosch (2005)) that is able to estimate
P (e?k|f?k,CI(f?k)) by similarity-based reasoning
over memorized nearest-neighbour examples of
source?target phrase translations. In equation (1),
SSCI may include any feature (lexical, syntactic,
etc.), which can provide useful information to
disambiguate a given source phrase. In addition
to using local words and PoS-tags as features,
as in (Stroppa et al, 2007), we incorporate
grammatical dependency relations (Haque et al,
2009a) and supertags (Haque et al, 2009b) as
syntactic source context features in the log-linear
PB-SMT model.
In addition to the above feature, we derived a
simple binary feature h?best, defined in (2):
h?best =
{
1 if e?k maximizes P (e?k|f?k,CI(f?k))
0 otherwise
(2)
We performed experiments by integrating these
two features, h?MBL and h?best, directly into the
log-linear framework of Moses.
2.6 Hierarchical PB-SMT model
For the en?cs translation task, we built
a weighted synchronous context-free grammar
model (Chiang, 2007) of translation that uses
the bilingual phrase pairs of PB-SMT as a start-
ing point to learn hierarchical rules. We used
the open-source Tree-Based translation system
moses-chart3 to perform this experiment.
2An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl
3http://www.statmt.org/moses/?n=Moses.SyntaxTutorial144
2.7 System Combination
For multiple system combination, we used an
MBR-CN framework (Du et al, 2009, 2010) as
shown in Figure 1. Due to the varying word or-
der in the MT hypotheses, it is essential to define
the backbone which determines the general word
order of the CN. Instead of using a single system
output as the skeleton, we employ an MBR de-
coder to select the best single system output Er
from the merged N -best list by minimizing the
BLEU (Papineni et al, 2002) loss, as in (3):
r = argmin
i
Ns?
j=1
(1? BLEU(Ej , Ei)) (3)
where Ns indicates the number of translations in
the merged N -best list, and {Ei}Nsi=1 are the trans-
lations themselves. In our task, we only merge the
1-best output of each individual system.
The CN is built by aligning other hypotheses
against the backbone, based on the TER metric.
Null words are allowed in the alignment. Ei-
ther votes or different confidence measures are as-
signed to each word in the network. Each arc in
the CN represents an alternative word at that po-
sition in the sentence and the number of votes for
each word is counted when constructing the net-
work. The features we used are as follows:
? word posterior probability (Fiscus, 1997);
? 3, 4-gram target language model;
? word length penalty;
? Null word length penalty;
We use MERT (Och, 2003) to tune the weights
of the CN.
2.8 Rescoring
Rescoring is a very important part in post-
processing which can select a better hypothesis
from the N -best list. We augmented our previ-
ous rescoring model (Du et al, 2009) with more
large-scale data. The features we used include:
? Direct and inverse IBM model;
? 3, 4-gram target language model;
? 3, 4, 5-gram PoS language model (Schmid,
1994; Ratnaparkhi, 1996);
? Sentence length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
Best list (Zens and Ney, 2006);
? Minimum Bayes Risk probability;
? Length ratio between source and target sen-
tence;
The weights are optimized via MERT.
3 Experimental Setup
This section describes our experimental setup for
the en?cs and en?es translation tasks.
3.1 Data
Bilingual data: In the experiments we used data
sets provided by the workshop organizers. For the
en?cs translation table extraction we employed
both parallel corpora (News-Commentary10 and
CzEng 0.9), and for the en?es experiments, we
used the Europarl(Koehn, 2005), News Commen-
tary and United Nations parallel data. We used a
maximum sentence length of 80 for en?es and
40 for en?cs. Detailed statistics are shown in Ta-
ble 1.
Corpus Langs. Sent. Source
tokens
Target
tokens
Europarl en?es 1.6M 43M 45M
News-comm en?es 97k 2.4M 2.7M
UN en?es 5.9M 160M 190M
News-Comm en?cs 85k 1.8M 1.6M
CzEng en?cs 7.8M 80M 69M
Table 1: Statistics of en?cs and en?es parallel data.
Monolingual data: For language modeling pur-
poses, in addition to the target parts of the bilin-
gual data, we used the monolingual News corpus
for cs; and the Gigaword corpus for es. For both
languages, we used the SRILM toolkit (Stolcke,
2002) to train a 5-gram language model using all
monolingual data provided. However, for en?es
we used the IRSTLM toolkit (Federico and Cet-
tolo, 2007) to train a 5-gram language model using
the es Gigaword corpus. Both language models
use modified Kneser-Ney smoothing (Chen and
Goodman, 1996). Statistics for the monolingual
corpora are given in Table 2.
Corpus Language Sentences Tokens
E/N/NC/UN es 9,6M 290M
Gigaword es 40M 1,2G
News cs 13M 210M
Table 2: Statistics of Monolingual Data. E/N/NC/UN
refers to Europarl/News/News Commentary/United Nations
corpora.
For all the systems except Apertium, we first
lowercase and tokenize all the monolingual and
bilingual data using the tools provided by the
WMT10 organizers. After translation, system
combination output is detokenised and true-cased.145
3.2 English?Czech (en?cs) Experiments
The CzEng corpus (Bojar and Z?abokrtsky?, 2009)
is a collection of parallel texts from sources of dif-
ferent quality and as such it contains some noise.
As the first step, we discarded those sentence pairs
having more than 10% of non-Latin characters.
The CzEng corpus is quite large (8M sen-
tence pairs). Although we were able to build
a vanilla SMT system on all parallel data avail-
able (News-Commentary + CzEng), we also at-
tempted to build additional systems using News-
Commentary data (which we considered in-
domain) and various in-domain subsets of CzEng
hoping to achieve better results on domain-
specific data.
For our first system, we selected 128,218 sen-
tence pairs from CzEng labeled as news. For the
other two systems, we selected subsets of 2M and
4M sentence pairs identified as most similar to
the development sets (as a sample of in-domain
data) based on cosine similarity of their represen-
tation in a TF-IDF weighted vector space model
(cf. Byrne et al (2003)). We also applied the
pseudo-relevavance-feedback technique for query
expansion (Manning et al, 2008) to select another
subset with 2M sentence pairs.
We used the output of 15 systems for sys-
tem combination for the en?cs translation task.
Among these, 5 systems were built using Moses
and varying the size of the training data (DCU-
All, DCU-Ex2M, DCU-4M, DCU-2M and DCU-
News); 9 context-informed PB-SMT systems
(DCU-SSCI-*) using (combinations of) various
context features (word, PoS, supertags and depen-
dency relations) trained only on the News Com-
mentary data (marked with ? in Table 4); and one
system using the moses-chart decoder, also
trained on the news commentary data.
3.3 English?Spanish (en?es) Experiments
Three baseline systems using Moses were built,
where we varied the amount of training data used:
? epn: This system uses all of the Europarl and
News-Commentary parallel data.
? UN-half: This system uses the data suplied
to ?epn?, plus an additional 2.1M sentences
pairs randomly selected from the United Na-
tions corpus.
? all: This system uses all of the available par-
allel data.
For en?es we also obtained output from the
factored model (trained only on the news com-
mentary corpus) and the Apertium RBMT sys-
tem. We also derived phrase alignments using the
MaTrEx EBMT system (Stroppa and Way, 2006),
and added those phrase translations in the Moses
phrase table. The systems marked with ? use a
language model built using the Spanish Gigaword
corpus, in addition to the one built using the pro-
vided monolingual data. These 6 sets of system
outputs are then used for system combination.
3.4 Experimental Results
The evaluation results for en?es and en?cs ex-
periments are shown in Table 3 and Table 4 re-
spectively. The output of the systems marked ?
were submitted in the shared tasks.
System BLEU NIST METEOR TER
DCU-half ?? 29.77% 7.68 59.86% 59.55%
DCU-all ?? 29.63% 7.66 59.82% 59.74%
DCU-epn ?? 29.45% 7.66 59.71% 59.64%
DCU-ebmt ?? 29.38% 7.62 59.59% 60.11%
DCU-factor 22.58% 6.56 54.94% 67.65%
DCU-apertium 19.22% 6.37 49.68% 67.68%
DCU-system-
combination ? 30.42% 7.78 60.56% 58.71%
Table 3: en?es experimental results.
System BLEU NIST METEOR TER
DCU-All 10.91% 4.60 39.18% 81.76%
DCU-Ex2M 10.63% 4.56 39.12% 81.96%
DCU-4M 10.61% 4.56 39.26% 82.04%
DCU-2M 10.48% 4.58 39.35% 81.56%
DCU-Chart 9.34% 4.25 37.04% 83.87%
DCU-News 8.64% 4.16 36.27% 84.96%
DCU-SSCI-ccg? 8.26% 4.02 34.76% 85.58%
DCU-SSCI-
supertag-pair? 8.11% 3.95 34.93% 86.63%
DCU-SSCI-
ccg-ltag? 8.09% 3.96 34.90% 86.62%
DCU-SSCI-PR? 8.06% 4.00 34.89% 85.99%
DCU-SSCI-base? 8.05% 3.97 34.61% 86.02%
DCU-SSCI-PRIR? 8.03% 3.99 34.81% 85.98%
DCU-SSCI-ltag? 8.00% 3.95 34.57% 86.41%
DCU-SSCI-PoS? 7.91% 3.94 34.57% 86.51%
DCU-SSCI-word? 7.57% 3.88 34.16% 87.14%
DCU-system-
combination ? 13.22% 4.98 40.39% 78.59%
Table 4: en?cs experimental results.
4 Conclusion
This paper presents the Dublin City University
MT system in WMT2010 shared task campaign.
This was DCU?s first attempt to translate from en
to es and cs in any shared task. We developed a
multi-engine framework which combined the out-
puts of several individual MT systems and gener-
ated a new N -best list after CN decoding. Then by146
using some global features, the rescoring model
generated the final translation output. The experi-
mental results demonstrated that the combination
module and rescoring module are effective in our
framework for both language pairs, and produce
statistically significant improvements as measured
by bootstrap resampling methods (Koehn, 2004)
on BLEU over the single best system.
Acknowledgements: This work is supported
by Science Foundation Ireland (Grant No.
07/CE/I1142) and by PANACEA, a 7th Frame-
work Research Programme of the European
Union, contract number 7FP-ITC-248064. M.L.
Forcada?s sabbatical stay at Dublin City Univer-
sity is supported by Science Foundation Ireland
through ETS Walton Award 07/W.1/I1802 and by
the Universitat d?Alacant (Spain).
References
Bojar, O. and Z?abokrtsky?, Z. (2009). CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics,
92:63?83.
Byrne, W., Khudanpur, S., Kim, W., Kumar, S.,
Pecina, P., Virga, P., Xu, P., and Yarowsky, D.
(2003). The Johns Hopkins University 2003
Chinese?English machine translation system.
In Proceedings of MT Summit IX, pages 447?
450, New Orleans, LA.
Chen, S. F. and Goodman, J. (1996). An Empir-
ical Study of Smoothing Techniques for Lan-
guage Modeling. In Proc. 34th Ann. Meeting of
the Association for Computational Linguistics,
pages 310?318, San Francisco, CA.
Chiang, D. (2007). Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?228.
Daelemans, W. and van den Bosch, A. (2005).
Memory-Based Language Processing (Studies
in Natural Language Processing). Cambridge
University Press, New York, NY.
Du, J., He, Y., Penkale, S., and Way, A. (2009).
MaTrEx: The DCU MT System for WMT2009.
In Proc. 3rd Workshop on Statistical Machine
Translation, EACL 2009, pages 95?99, Athens,
Greece.
Du, J., Pecina, P., and Way, A. (2010). An
Augmented Three-Pass System Combination
Framework: DCU Combination System for
WMT 2010. In Proc. ACL 2010 Joint Workshop
in Statistical Machine Translation and Metrics
Matr, Uppsala, Greece.
Federico, M. and Cettolo, M. (2007). Efficient
Handling of N-gram Language Models for Sta-
tistical Machine Translation. In Proceedings
of the Second Workshop on Statistical Machine
Translation, pages 88?95, Prague, Czech Re-
public.
Fiscus, J. G. (1997). A post-processing sys-
tem to yield reduced word error rates: Recog-
nizer output voting error reduction (ROVER).
In Proceedings 1997 IEEE Workshop on Auto-
matic Speech Recognition and Understanding
(ASRU), pages 347?352, Santa Barbara, CA.
Forcada, M. L., Tyers, F. M., and Ram??rez-
Sa?nchez, G. (2009). The free/open-source ma-
chine translation platform Apertium: Five years
on. In Proceedings of the First International
Workshop on Free/Open-Source Rule-Based
Machine Translation FreeRBMT?09, pages 3?
10.
Gough, N. and Way, A. (2004). Robust Large-
Scale EBMT with Marker-Based Segmenta-
tion. In Proceedings of the 10th International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-04), pages
95?104, Baltimore, MD.
Haque, R., Naskar, S. K., Bosch, A. v. d., and
Way, A. (2009a). Dependency relations as
source context in phrase-based smt. In Proc.
23rd Pacific Asia Conference on Language, In-
formation and Computation, pages 170?179,
Hong Kong, China.
Haque, R., Naskar, S. K., Ma, Y., and Way, A.
(2009b). Using supertags as source language
context in SMT. In EAMT-2009: Proceed-
ings of the 13th Annual Conference of the Eu-
ropean Association for Machine Translation,
pages 234?241, Barcelona, Spain.
Koehn, P. (2004). Statistical significance tests for
machine translation evaluation. In Proceedings
of EMNLP, volume 4, pages 388?395.
Koehn, P. (2005). Europarl: A Parallel Corpus
for Statistical Machine Translation. In Machine
Translation Summit X, pages 79?86, Phuket,
Thailand.
Koehn, P. and Hoang, H. (2007). Factored Trans-
lation Models. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural147
Language Learning (EMNLP-CoNLL), pages
868?876, Prague, Czech Republic.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
demonstration session, pages 177?180, Prague,
Czech Republic.
Kumar, S. and Byrne, W. (2004). Minimum
Bayes-Risk Decoding for Statistical Machine
Translation. In Proceedings of the Joint Meet-
ing of the Human Language Technology Con-
ference and the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL 2004), pages 169?176, Boston,
MA.
Mangu, L., Brill, E., and Stolcke, A. (2000). Find-
ing consensus in speech recognition: Word er-
ror minimization and other applications of con-
fusion networks. Computer Speech and Lan-
guage, 14(4):373?400.
Manning, C. D., Raghavan, P., and Schu?tze, H.
(2008). Introduction to Information Retrieval.
Cambridge University Press.
Och, F. (2003). Minimum error rate training
in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan.
Och, F. and Ney, H. (2002). Discriminative train-
ing and maximum entropy models for statistical
machine translation. In Proceedings of ACL,
volume 2, pages 295?302.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages
311?318, Philadelphia, PA.
Ratnaparkhi, A. (1996). A Maximum Entropy
Model for Part-Of-Speech Tagging. In Pro-
ceedings of the Empirical Methods in Natural
Language Processing Conference (EMNLP),
pages 133?142, Philadelphia, PA.
Rosti, A.-V. I., Xiang, B., Matsoukas, S.,
Schwartz, R., Ayan, N. F., and Dorr, B. J.
(2007). Combining outputs from multiple ma-
chine translation systems. In Proceedings of the
Joint Meeting of the Human Language Technol-
ogy Conference and the North American Chap-
ter of the Association for Computational Lin-
guistics (HLT-NAACL 2007), pages 228?235,
Rochester, NY.
Schmid, H. (1994). Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of International Conference on New Methods
in Language Processing, pages 44?49, Manch-
ester, UK.
Snover, M., Dorr, B., Schwartz, R., Micciula, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Amer-
icas (AMTA 2006), pages 223?231, Cambridge,
MA.
Stolcke, A. (2002). SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of
the International Conference Spoken Language
Processing, pages 901?904, Denver, CO.
Stroppa, N., van den Bosch, A., and Way, A.
(2007). Exploiting Source Similarity for SMT
using Context-Informed Features. In Proceed-
ings of the 11th International Conference on
Theoretical and Methodological Issues in Ma-
chine Translation (TMI-07), pages 231?240,
Sko?vde, Sweden.
Stroppa, N. and Way, A. (2006). MaTrEx: the
DCU machine translation system for IWSLT
2006. In Proceedings of the International Work-
shop on Spoken Language Translation, pages
31?36, Kyoto, Japan.
Zens, R. and Ney, H. (2006). N-gram Poste-
rior Probabilities for Statistical Machine Trans-
lation. In Proceedings of the Joint Meeting of
the Human Language Technology Conference
and the North American Chapter of the As-
sociation for Computational Linguistics (HLT-
NAACL 2006), pages 72?77, New York, NY.
148
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 290?295,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
An Augmented Three-Pass System Combination Framework:
DCU Combination System for WMT 2010
Jinhua Du, Pavel Pecina, Andy Way
CNGL, School of Computing
Dublin City University
Dublin 9, Ireland
{jdu,ppecina,away}@computing.dcu.ie
Abstract
This paper describes the augmented three-
pass system combination framework of
the Dublin City University (DCU) MT
group for the WMT 2010 system combi-
nation task. The basic three-pass frame-
work includes building individual confu-
sion networks (CNs), a super network, and
a modified Minimum Bayes-risk (mCon-
MBR) decoder. The augmented parts for
WMT2010 tasks include 1) a rescoring
component which is used to re-rank the
N -best lists generated from the individual
CNs and the super network, 2) a new hy-
pothesis alignment metric ? TERp ? that
is used to carry out English-targeted hy-
pothesis alignment, and 3) more differ-
ent backbone-based CNs which are em-
ployed to increase the diversity of the
mConMBR decoding phase. We took
part in the combination tasks of English-
to-Czech and French-to-English. Exper-
imental results show that our proposed
combination framework achieved 2.17 ab-
solute points (13.36 relative points) and
1.52 absolute points (5.37 relative points)
in terms of BLEU score on English-to-
Czech and French-to-English tasks re-
spectively than the best single system. We
also achieved better performance on hu-
man evaluation.
1 Introduction
In several recent years, system combination has
become not only a research focus, but also a pop-
ular evaluation task due to its help in improving
machine translation quality. Generally, most com-
bination approaches are based on a confusion net-
work (CN) which can effectively re-shuffle the
translation hypotheses and generate a new target
sentence. A CN is essentially a directed acyclic
graph built from a set of translation hypotheses
against a reference or ?backbone?. Each arc be-
tween two nodes in the CN denotes a word or to-
ken, possibly a null item, with an associated pos-
terior probability.
Typically, the dominant CN is constructed at the
word level by a state-of-the-art framework: firstly,
a minimum Bayes-risk (MBR) decoder (Kumar
and Byrne, 2004) is utilised to choose the back-
bone from a merged set of hypotheses, and then
the remaining hypotheses are aligned against the
backbone by a specific alignment approach. Cur-
rently, most research in system combination has
focused on hypothesis alignment due to its signif-
icant influence on combination quality.
A multiple CN or ?super-network? framework
was firstly proposed in Rosti et al (2007) who
used each of all individual system results as the
backbone to build CNs based on the same align-
ment metric, TER (Snover et al, 2006). A consen-
sus network MBR (ConMBR) approach was pre-
sented in (Sim et al, 2007), where MBR decod-
ing is employed to select the best hypothesis with
the minimum cost from the original single system
outputs compared to the consensus output.
Du and Way (2009) proposed a combination
strategy that employs MBR, super network, and
a modified ConMBR (mConMBR) approach to
construct a three-pass system combination frame-
work which can effectively combine different hy-
pothesis alignment results and easily be extended
to more alignment metrics. Firstly, a number of
individual CNs are built based on different back-
bones and different kinds of alignment metrics.
Each network generates a 1-best output. Secondly,
a super network is constructed combining all the
individual networks, and a consensus is generated
based on a weighted search model. In the third290
pass, all the 1-best hypotheses coming from sin-
gle MT systems, individual networks, and the su-
per network are combined to select the final result
using the mConMBR decoder.
In the system combination task of WMT 2010,
we adopted an augmented framework by extend-
ing the strategy in (Du and Way, 2009). In addi-
tion to the basic three-pass architecture, we aug-
ment our combination system as follows:
? We add a rescoring component in Pass 1 and
Pass 2.
? We introduce the TERp (Snover et al, 2009)
alignment metric for the English-targeted
combination.
? We employ different backbones and hypothe-
sis alignment metrics to increase the diversity
of candidates for our mConMBR decoding.
The remainder of this paper is organised as fol-
lows. In Section 2, we introduce the three hy-
pothesis alignment methods used in our frame-
work. Section 3 details the steps for building our
augmented three-pass combination framework. In
Section 4, a rescoring model with rich features
is described. Then, Sections 5 and 6 respec-
tively report the experimental settings and exper-
imental results on English-to-Czech and French-
to-English combination tasks. Section 7 gives our
conclusions.
2 Hypothesis Alignment Methods
Hypothesis alignment plays a vital role in the CN,
as the backbone sentence determines the skeleton
and the word order of the consensus output.
In the combination evaluation task, we inte-
grated TER (Snover et al, 2006), HMM (Ma-
tusov et al, 2006) and TERp (Snover et al,
2009) into our augmented three-pass combination
framework. In this section, we briefly describe
these three methods.
2.1 TER
The TER (Translation Edit Rate) metric measures
the ratio of the number of edit operations between
the hypothesis E? and the reference Eb to the total
number of words in Eb. Here the backbone Eb is
assumed to be the reference. The allowable edits
include insertions (Ins), deletions (Del), substitu-
tions (Sub), and phrase shifts (Shft). The TER of
E? compared to Eb is computed as in (1):
TER(E?, Eb) = Ins + Del + Sub + ShftNb ? 100% (1)
where Nb is the total number of words in Eb. The
difference between TER and Levenshtein edit dis-
tance (or WER) is the sequence shift operation al-
lowing phrasal shifts in the output to be captured.
The phrase shift edit is carried out by a greedy
algorithm and restricted by three constraints: 1)
The shifted words must exactly match the refer-
ence words in the destination position. 2) The
word sequence of the hypothesis in the original
position and the corresponding reference words
must not exactly match. 3) The word sequence
of the reference that corresponds to the desti-
nation position must be misaligned before the
shift (Snover et al, 2006).
2.2 HMM
The hypothesis alignment model based on HMM
(Hidden Markov Model) considers the align-
ment between the backbone and the hypoth-
esis as a hidden variable in the conditional
probability Pr(E?|Eb). Given the backbone
Eb = {e1, . . . , eI} and the hypothesis E? =
{e?1, . . . , e?J}, which are both in the same lan-
guage, the probability Pr(E?|Eb) is defined as in
(2):
Pr(E?|Eb) =
?
A
Pr(E?, A|Eb) (2)
where the alignemnt A ? {(j, i) : 1 ? j ?
J ; 1 ? i ? I}, i and j represent the word po-
sition in Eb and E? respectively. Hence, the align-
ment issue is to seek the optimum alignment A?
such that:
A? = argmax
A
P (A|eI1, e?J1 ) (3)
For the HMM-based model, equation (2) can be
represented as in (4):
Pr(E?|Eb) =
?
aJj
J?
j=1
[p(aj |aj?1, I) ? p(e?j |eaj )] (4)
where p(aj |aj?1, I) is the alignment probability
and p(e?j |ei) is the translation probability.
2.3 TER-Plus
TER-Plus (TERp) is an extension of TER that
aligns words in the hypothesis and reference not
only when they are exact matches but also when
the words share a stem or are synonyms (Snover
et al, 2009). In addition, it uses probabilistic
phrasal substitutions to align phrases in the hy-
pothesis and reference. In contrast to the use of291
the constant edit cost for all operations such as
shifts, insertion, deleting or substituting in TER,
all edit costs in TERp are optimized to maximize
correlation with human judgments.
TERp uses all the edit operations of TER ?
matches, insertions, deletions, substitutions, and
shifts ? as well as three new edit operations:
stem matches, synonym matches, and phrase sub-
stitutions (Snover et al, 2009). TERp employs
the Porter stemming algorithm (Porter, 1980) and
WordNet (Fellbaum, 1998) to perform the ?stem
match? and ?synonym match? respectively. Se-
quences of words in the reference are considered
to be paraphrases of a sequence of words in the
hypothesis if that phrase pair occurs in the TERp
phrase table (Snover et al, 2009).
In our experiments, TERp was used for the
French-English system combination task, and we
used the default configuration of optimised edit
costs.
3 Augmented Three-Pass Combination
Framework
The construction of the augmented three-pass
combination framework is shown in Figure 1.
Hypotheses Set
BLEU TER TERp
MBR
BLEU TER TERp
Top M Single
HMM TER TERp
Alignment
Individual CNs
Nbest 
Re-ranking Super CN Networks
mConMBR
Pass 1
Pass 2
Pass 3
N Single MT 
Systems
Figure 1: Three-Pass Combination Framework
In Figure 1, the dashed boxes labeled ?TERp?
indicate that the TERp alignment is only appli-
cable for English-targeted hypothesis alignment.
The lines with arrows pointing to ?mConMBR?
represent adding outputs into the mConMBR de-
coding component. ?Top M Single? indicates that
the 1-best results from the best M individual MT
systems are also used as backbones to build in-
dividual CNs under different alignment metrics.
The three dashed boxes represent Pass 1, Pass 2
and Pass 3 respectively. The steps can be sum-
marised as follows:
Pass 1: Specific Metric-based Single Networks
1. Merge all the 1-best hypotheses from single
MT systems into a new N -best set Ns.
2. Utilise the standard MBR decoder to se-
lect one from the Ns as the backbone given
some specific loss function such as TER,
BLEU (Papineni et al, 2002) and TERp; Ad-
ditionally, in order to increase the diversity
of candidates used for Pass 2 and Pass 3, we
also use the 1-best hypotheses from the top
M single MT systems as the backbone. Add
the backbones generated by MBR into Ns.
3. Perform the word alignment between the dif-
ferent backbones and the other hypotheses
via the TER, HMM, TERp (only for English)
metrics.
4. Carry out word reordering based on word
alignment (TER and TERp have completed
the reordering in the process of scoring) and
build individual CNs (Rosti et al, 2007);
5. Decode the single networks and export the 1-
best outputs and the N -best lists separately.
Add these 1-best outputs into Ns.
Pass 2: Super-Network
1. Connect the single networks using a start
node and an end node to form a super-
network based on multiple hypothesis align-
ment and different backbones. In this evalu-
ation, we set uniform weights for these dif-
ferent individual networks when building the
super network(Du and Way, 2009).
2. Decode the super network and generate a
consensus output as well as the N -best list.
Add the 1-best result into Ns.
3. Rescore the N -best lists from all individual
networks and super network and add the new
1-best results into Ns.
Pass 3: mConMBR
1. Rename the set Ns as a new set Ncon;
2. Use mConMBR decoding to search for the
best final result from Ncon. In this step, we
set a uniform distribution between the candi-
dates in Ncon.292
4 Rescoring Model
We adapted our previous rescoring model (Du
et al, 2009) to larger-scale data. The features we
used are as follows:
? Direct and inverse IBM model;
? 4-gram and 5-gram target language model;
? 3, 4, and 5-gram Part-of-Speech (POS) lan-
guage model (Schmid, 1994; Ratnaparkhi,
1996);
? Sentence-length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
best list (Zens and Ney, 2006);
? Minimum Bayes Risk cost. This process is
similar to the calculation of the MBR decod-
ing in which we take the current hypothesis
in the N -best list as the ?backbone?, and then
calculate and sum up all the Bayes risk cost
between the backbone and each of the rest of
the N -best list using BLEU metric as the loss
function;
? Length ratio between source and target sen-
tence.
The weights are optimized via the MERT algo-
rithm (Och, 2003).
5 Experimental Settings
We participated in the English?Czech and
French?English system combination tasks.
In our system combination framework, we use
a large-scale monolingual data to train language
models and carry out POS-tagging.
5.1 English-Czech
Training Data
The statistics of the data used for language models
training are shown in Table 1.
Monolingual Number of
Corpus tokens (Cz) sentences
News-Comm 2,214,757 84,706
CzEng 81,161,278 8,027,391
News 205,600,053 13,042,040
Total 288,976,088 21,154,137
Table 1: Statistics of data in the En?Cz task
All the data are provided by the workshop
organisers. 1 In Table 1, ?News-Comm? indi-
cates the data set of News-Commentary v1.0 and
1http://www.statmt.org/wmt10/translation-task.html
?CzEng? is the Czech?English corpus v0.9 (Bo-
jar and Z?abokrtsky?, 2009). ?News? is the Czech
monolingual News corpus.
As to our CN and rescoring components,
we use ?News-Comm+CzEng? to train a
4-gram language model and use ?News-
Comm+CzEng+News? to train a 5-gram
language model. Additionally, we per-
form POS tagging (Hajic?, 2004) for ?News-
Comm+CzEng+News? data, and train 3-gram,
4-gram, and 5-gram POS-tag language models.
Devset and Testset
The devset includes 455 sentences and the testset
contains 2,034 sentences. Both data sets are pro-
vided by the workshop organizers. Each source
sentence has only one reference. There are 11 MT
systems in the En-Cz track and we use all of them
in our combination experiments.
5.2 French-English
Training Data
The statistics of the data used for language models
training and POS tagging are shown in Table 2.
Monolingual Number of
Corpus tokens (En) sentences
News-Comm 2,973,711 125,879
Europarl 50,738,215 1,843,035
News 1,131,527,255 48,648,160
Total 1,184,234,384 50,617,074
Table 2: Statistics of data in the Fr?En task
?News? is the English monolingual News
corpus. We use ?News-Comm+Europarl? to
train a 4-gram language model and use ?News-
Comm+Europarl+News? to train a 5-gram lan-
guage model. We also perform POS tagging (Rat-
naparkhi, 1996) for all available data, and train
3-gram, 4-gram and, 5-gram POS-tag language
models.
Devset and Testset
We also use all the 1-best results to carry out sys-
tem combination. There are 14 MT systems in the
Fr-En track and we use all of them in our combi-
nation experiments.
6 Experimental Results
In this section, all the results are reported on de-
vsets in terms of BLEU and NIST scores.
6.1 English?Czech
In this task, we only used one hypothesis align-
ment method ? TER ? to carry out hypothesis293
alignment. However, in order to increase diversity
for our 3-pass framework, in addition to using the
output from MBR decoding as the backbone, we
also separately selected the top 4 individual sys-
tems (SYS1, SYS4, SYS6, and SYS11 in our sys-
tem set) in terms of BLEU scores on the devset as
the backbones so that we can build multiple indi-
vidual CNs for the super network. All the results
are shown in Table 3.
SYS BLEU4 NIST
Worst 9.09 3.83
Best 17.28 4.99
SYS1 15.11 4.76
SYS4 12.67 4.40
SYS6 17.28 4.99
SYS11 15.75 4.81
CN-SYS1 17.36 5.12
CN-SYS4 16.94 5.10
CN-SYS6 17.91 5.13
CN-SYS11 17.45 5.09
CN-MBR 18.29 5.15
SuperCN 18.44 5.17
mConMBR-BAS 18.60 5.18
mConMBR-New 18.84 5.11
Table 3: Automatic evaluation of the combination
results on the En-Cz devset.
?Worst? indicates the 1-best hypothesis from
the worst single system, the ?Best? is the 1-best
hypothesis from the best single system (SYS11)).
?CN-SYSX? denotes that we use SYSX (X =
1, 4, 6, 11 and MBR) as the backbone to build an
individual CN. ?mConMBR-BAS? stands for the
original three-pass combination framework with-
out rescoring component, while ?mConMBR-
New? indicates the proposed augmented combina-
tion framework. It can be seen from Table 3 that 1)
in all individual CNs, the CN-MBR achieved the
best performance; 2) SuperCN and mConMBR-
New improved by 1.16 (6.71% relative) and 1.56
(9.03% relative) absolute BLEU points compared
to the best single MT system. 3) our new
three-pass combination framework achieved the
improvement of 0.24 absolute (1.29% relative)
BLEU points than the original framework.
The final results on the test set are shown in Ta-
ble 4.
SYS BLEU4 human eval.(%win)
Best 16.24 70.38
mConMBR-BAS 17.91 -
mConMBR-New 18.41 2 75.17
Table 4: Evaluation of the combination results on
the En-Cz testset.
It can be seen that our ?mConMBR-New?
framework performs better than the best single
system and our original framework ?mConMBR-
BAS? in terms of automatic BLEU scores and hu-
man evaluation for the English-to-Czech task. In
this task campaign, we achieved top 1 in terms of
the human evaluation.
6.2 French?English
We used three hypothesis alignment methods ?
TER, TERp and HMM ? to carry out word align-
ment between the backbone and the rest of the
hypotheses. Apart from the backbone generated
from MBR, we separately select the top 5 individ-
ual systems (SYS1, SYS10, SYS11, SYS12, and
SYS13 in our system set) respectively as the back-
bones using HMM, TER and TERp to carry out
hypothesis alignment so that we can build more
individual CNs for the super network to increase
the diversity of candidates for mConMBR. The re-
sults are shown in Table 5.3
SYS BLEU4(%) NIST
Worst 15.04 4.97
Best 28.88 6.71
CN-SYS1-TER 29.56 6.78
CN-SYS1-HMM 29.60 6.84
CN-SYS1-TERp 29.77 6.83
CN-MBR-TER 30.16 6.91
CN-MBR-HMM 30.19 6.92
CN-MBR-TERp 30.27 6.92
SuperCN 30.58 6.90
mConMBR-BAS 30.74 7.01
mConMBR-New 31.02 6.96
Table 5: Automatic evaluation of the combination
results on the Fr-En devset.
?CN-MBR-X? represents the different possi-
ble hypothesis alignment methods (X = {TER,
HMM, TERp}) which are used to build indi-
vidual CNs using the output from MBR de-
coding as the backbone. We can see that the
SuperCN and mConMBR-New respectively im-
proved by 1.7 absolute (5.89% relative) and 2.88
absolute (9.97% relative) BLEU points compared
to the best single system. Furthermore, our aug-
mented framework ?mConMBR-New? achieved
the improvement of 0.28 absolute (0.91% relative)
BLEU points than the original three-pass frame-
work as well.
2This score was measured in-house on the refer-
ence provided by the organizer using metric mteval-v13
(ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl).
3In this Table, we take SYS1 as an example to show the
results using a single MT system as the backbone under the
three alignment metrics.294
The final results on the test set are shown in Ta-
ble 6.
SYS BLEU4 human eval.(%win)
Best 28.30 66.84
mConMBR-BAS 29.21 -
mConMBR-New 29.82 2 72.15
Table 6: Evaluation of the combination results on
Fr-En test set.
It can be seen that our ?mConMBR-New?
framework performs the best than the best single
system and our original framework ?mConMBR-
BAS? in terms of automatic BLEU scores and hu-
man evaluation for the French?English task.
7 Conclusions and Future Work
We proposed an augmented three-pass mul-
tiple system combination framework for the
WMT2010 system combination shared task. The
augmented parts include 1) a rescoring model to
select the potential 1-best result from the indi-
vidual CNs and super network to increase the di-
versity for ?mConMBR? decoding; 2) a new hy-
pothesis alignment metric ?TERp? for English-
targeted alignment; 3) 1-best results from the top
M individual systems employed to build CNs
to augment the ?mConMBR? decoding. We
took part in the English-to-Czech and French-to-
English tasks. Experimental results reported on
test set of these two tasks showed that our aug-
mented framework performed better than the best
single system in terms of BLEU scores and hu-
man evaluation. Furthermore, the proposed aug-
mented framework achieved better results than our
basic three-pass combination framework (Du and
Way, 2009) as well in terms of automatic evalua-
tion scores. In the released preliminary results, we
achieved top 1 and top 3 for the English-to-Czech
and French-to-English tasks respectively in terms
of human evaluation.
As for future work, firstly we plan to do further
experiments using automatic weight-tuning algo-
rithm to tune our framework. Secondly, we plan
to examine how the differences between the hy-
pothesis alignment metrics impact on the accuracy
of the super network. We also intend to integrate
more alignment metrics to the networks and verify
on the other language pairs.
Acknowledgments
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University
and has been partially funded by PANACEA, a 7th Frame-
work Research Programme of the European Union (contract
number: 7FP-ITC-248064) as well as partially supported by
the project GA405/09/0278 of the Grant Agency of the Czech
Republic. Thanks also to the reviewers for their insightful
comments.
References
Bojar, O. and Z?abokrtsky?, Z. (2009). CzEng0.9: Large Par-
allel Treebank with Rich Annotation. Prague Bulletin of
Mathematical Linguistics, 92.
Du, J., He, Y., Penkale, S., and Way, A. (2009). MaTrEx:
The DCU MT System for WMT2009. In Proceedings of
the EACL-WMT 2009, pages 95?99, Athens, Greece.
Du, J. and Way, A. (2009). A Three-pass System Com-
bination Framework by Combining Multiple Hypothesis
Alignment Methods. In Proceedings of the International
Conference on Asian Language Processing (IALP), pages
172?176, Singapore.
Fellbaum, C., editor (1998). WordNet: an electronic lexical
database. MIT Press.
Hajic?, J. (2004). Disambiguation of Rich Inflection (Compu-
tational Morphology of Czech), volume 1. Charles Uni-
versity Press, Prague.
Kumar, S. and Byrne, W. (2004). Minimum Bayes-Risk De-
coding for Statistical Machine Translation. In Proceed-
ings of the HLT-NAACL 2004, pages 169?176, Boston,
MA.
Matusov, E., Ueffing, N., and Ney, H. (2006). Computing
consensus translation from multiple machine translation
systems using enhanced hypotheses alignment. In Pro-
ceedings of EACL?06, pages 33?40.
Och, F. (2003). Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
BLEU: a Method for Automatic Evaluation of Machine
Translation. In Proceedings of the ACL-02, pages 311?
318, Philadelphia, PA.
Porter, M. F. (1980). An algorithm for suffix stripping, pro-
gram.
Ratnaparkhi, A. (1996). A Maximum Entropy Model
for Part-of-Speech Tagging. In Proceedings of the
EMNLP?96, pages 133?142, Philadelphia, PA.
Rosti, A., Matsoukas, S., and Schwartz, R. (2007). Improved
Word-Level System Combination for Machine Transla-
tion. In Proceedings of ACL?07, pages 312?319.
Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In Proceedings of International Con-
ference on New Methods in Language Processing, pages
44?49, Manchester, UK.
Sim, K., Byrne, W., Gales, M., Sahbi, H., and Woodland, P.
(2007). Consensus network decoding for statistical ma-
chine translation system combination. In Proceedings of
the ICASSP?07, pages 105?108.
Snover, M., Dorr, B., Schwartz, R., Micciula, L., and
Makhoul, J. (2006). A study of translation edit rate
with targeted human annotation. In Proceedings of the
AMTA?06), pages 223?231, Cambridge, MA.
Snover, M., Madnani, N., J.Dorr, B., and Schwartz, R.
(2009). Fluency, adequacy, or HTER? Exploring different
human judgments with a tunable MT metric. In Proceed-
ings of the WMT?09, pages 259?268, Athens, Greece.
Zens, R. and Ney, H. (2006). N-gram Posterior Probabilities
for Statistical Machine Translation. In Proceedings of the
HLT-NAACL?06), pages 72?77, New York, USA.
295
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 19?27,
Beijing, August 2010
Automatic Extraction of Arabic Multiword Expressions
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel Pecina and Josef van Genabith
School of Computing, Dublin City University
{mattia,atoral,ltounsi,ppecina,josef}@computing.dcu.ie
Abstract
In this paper we investigate the automatic
acquisition of Arabic Multiword Expres-
sions (MWE). We propose three com-
plementary approaches to extract MWEs
from available data resources. The first
approach relies on the correspondence
asymmetries between Arabic Wikipedia
titles and titles in 21 different languages.
The second approach collects English
MWEs from Princeton WordNet 3.0,
translates the collection into Arabic us-
ing Google Translate, and utilizes differ-
ent search engines to validate the output.
The third uses lexical association mea-
sures to extract MWEs from a large unan-
notated corpus. We experimentally ex-
plore the feasibility of each approach and
measure the quality and coverage of the
output against gold standards.
1 Introduction
A lexicon of multiword expressions (MWEs) has
a significant importance as a linguistic resource
because MWEs cannot usually be analyzed lit-
erally, or word-for-word. In this paper we ap-
ply three approaches to the extraction of Arabic
MWEs from multilingual, bilingual, and monolin-
gual data sources. We rely on linguistic informa-
tion, frequency counts, and statistical measures to
create a refined list of candidates. We validate the
results with manual and automatic testing.
The paper is organized as follows: in this intro-
duction we describe MWEs and provide a sum-
mary of previous related research. Section 2 gives
a brief description of the data sources used. Sec-
tion 3 presents the three approaches used in our
experiments, and each approach is tested and eval-
uated in its relevant sub-section. In Section 4 we
discuss the results of the experiments. Finally, we
conclude in Section 5.
1.1 What Are Multiword Expressions?
Multiword expressions (MWEs) are defined
as idiosyncratic interpretations that cross word
boundaries or spaces (Sag et al, 2002). The exact
meaning of an MWE is not directly obtained from
its component parts. Accommodating MWEs in
NLP applications has been reported to improve
tasks, such as text mining (SanJuan and Ibekwe-
SanJuan, 2006), syntactic parsing (Nivre and Nils-
son, 2004; Attia, 2006), and Machine Translation
(Deksne, 2008).
There are two basic criteria for identifying
MWEs: first, component words exhibit statisti-
cally significant co-occurrence, and second, they
show a certain level of semantic opaqueness or
non-compositionality. Statistically significant co-
occurrence can give a good indication of how
likely a sequence of words is to form an MWE.
This is particularly interesting for statistical tech-
niques which utilize the fact that a large number
of MWEs are composed of words that co-occur to-
gether more often than can be expected by chance.
The compositionality, or decomposabil-
ity (Villavicencio et al 2004), of MWEs is also
a core issue that presents a challenge for NLP ap-
plications because the meaning of the expression
is not directly predicted from the meaning of the
component words. In this respect, composition-
alily varies between phrases that are highly com-
19
positional, such as,       	  
 	   qfla-
?idatun ?askariyyatun, ?military base?, and those
that show a degree of idiomaticity, such as,       

   Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46?54,
Beijing, August 2010
Handling Named Entities and Compound Verbs in             
Phrase-Based Statistical Machine Translation 
Santanu Pal*, Sudip Kumar Naskar?, Pavel Pecina?,  
Sivaji Bandyopadhyay* and Andy Way? 
*Dept. of Comp. Sc. & Engg. 
Jadavpur University 
santanupersonal1@gmail.com, sivaji_cse_ju@yahoo.com 
?CNGL, School of Computing 
Dublin City University 
{snaskar, ppecina, away}@computing.dcu.ie 
 
Abstract 
Data preprocessing plays a crucial role in 
phrase-based statistical machine transla-
tion (PB-SMT). In this paper, we show 
how single-tokenization of two types of 
multi-word expressions (MWE), namely 
named entities (NE) and compound 
verbs, as well as their prior alignment 
can boost the performance of PB-SMT. 
Single-tokenization of compound verbs 
and named entities (NE) provides sig-
nificant gains over the baseline PB-SMT 
system. Automatic alignment of NEs 
substantially improves the overall MT 
performance, and thereby the word 
alignment quality indirectly. For estab-
lishing NE alignments, we transliterate 
source NEs into the target language and 
then compare them with the target NEs. 
Target language NEs are first converted 
into a canonical form before the com-
parison takes place. Our best system 
achieves statistically significant im-
provements (4.59 BLEU points absolute, 
52.5% relative improvement) on an Eng-
lish?Bangla translation task. 
1 Introduction 
Statistical machine translation (SMT) heavily 
relies on good quality word alignment and 
phrase alignment tables comprising translation 
knowledge acquired from a bilingual corpus. 
Multi-word expressions (MWE) are defined 
as ?idiosyncratic interpretations that cross word 
boundaries (or spaces)? (Sag et al, 2002). Tradi-
tional approaches to word alignment following 
IBM Models (Brown et al, 1993) do not work 
well with multi-word expressions, especially 
with NEs, due to their inability to handle many-
to-many alignments. Firstly, they only carry out 
alignment between words and do not consider 
the case of complex expressions, such as multi-
word NEs. Secondly, the IBM Models only al-
low at most one word in the source language to 
correspond to a word in the target language 
(Marcu, 2001, Koehn et al, 2003). 
In another well-known word alignment ap-
proach, Hidden Markov Model (HMM: Vogel et 
al., 1996), the alignment probabilities depend on 
the alignment position of the previous word. It 
does not explicitly consider many-to-many 
alignment either. 
We address this many-to-many alignment 
problem indirectly. Our objective is to see how 
to best handle the MWEs in SMT. In this work, 
two types of MWEs, namely NEs and compound 
verbs, are automatically identified on both sides 
of the parallel corpus. Then, source and target 
language NEs are aligned using a statistical 
transliteration method. We rely on these auto-
matically aligned NEs and treat them as transla-
tion examples. Adding bilingual dictionaries, 
which in effect are instances of atomic transla-
tion pairs, to the parallel corpus is a well-known 
practice in domain adaptation in SMT (Eck et 
al., 2004; Wu et al, 2008). We modify the paral-
lel corpus by converting the MWEs into single 
tokens and adding the aligned NEs in the parallel 
corpus in a bid to improve the word alignment, 
and hence the phrase alignment quality. This 
46
preprocessing results in improved MT quality in 
terms of automatic MT evaluation metrics. 
The remainder of the paper is organized as 
follows. In section 2 we discuss related work. 
The System is described in Section 3.  Section 4 
includes the results obtained, together with some 
analysis. Section 5 concludes, and provides ave-
nues for further work. 
2 Related Work 
Moore (2003) presented an approach for si-
multaneous NE identification and translation. He 
uses capitalization cues for identifying NEs on 
the English side, and then he applies statistical 
techniques to decide which portion of the target 
language corresponds to the specified English 
NE. Feng et al (2004) proposed a Maximum 
Entropy model based approach for English?
Chinese NE alignment which significantly out-
performs IBM Model4 and HMM. They consid-
ered 4 features: translation score, transliteration 
score, source NE and target NE's co-occurrence 
score, and the distortion score for distinguishing 
identical NEs in the same sentence. Huang et al 
(2003) proposed a method for automatically ex-
tracting NE translingual equivalences between 
Chinese and English based on multi-feature cost 
minimization. The costs considered are translit-
eration cost, word-based translation cost, and NE 
tagging cost. 
Venkatapathy and Joshi (2006) reported a dis-
criminative approach of using the compositional-
ity information about verb-based multi-word 
expressions to improve word alignment quality. 
(Ren et al, 2009) presented log likelihood ratio-
based hierarchical reducing algorithm to auto-
matically extract bilingual MWEs, and investi-
gated the usefulness of these bilingual MWEs in 
SMT by integrating bilingual MWEs into Moses 
(Koehn et al, 2007) in three ways. They ob-
served the highest improvement when they used 
an additional feature to represent whether or not 
a bilingual phrase contains bilingual MWEs. 
This approach was generalized in Carpuat and 
Diab (2010). In their work, the binary feature 
was replaced by a count feature representing the 
number of MWEs in the source language phrase. 
Intuitively, MWEs should be both aligned in 
the parallel corpus and translated as a whole. 
However, in the state-of-the-art PB-SMT, it 
could well be the case that constituents of an 
MWE are marked and aligned as parts of con-
secutive phrases, since PB-SMT (or any other 
approaches to SMT) does not generally treat 
MWEs as special tokens. Another problem SMT 
suffers from is that verb phrases are often 
wrongly translated, or even sometimes deleted in 
the output in order to produce a target sentence 
considered good by the language model. More-
over, the words inside verb phrases seldom show 
the tendency of being aligned one-to-one; the 
alignments of the words inside source and target 
verb phrases are mostly many-to-many, particu-
larly so for the English?Bangla language pair. 
These are the motivations behind considering 
NEs and compound verbs for special treatment 
in this work. 
By converting the MWEs into single tokens, 
we make sure that PB-SMT also treats them as a 
whole. The objective of the present work is two-
fold; firstly to see how treatment of NEs and 
compound verbs as a single unit affects the 
overall MT quality, and secondly whether prior 
automatic alignment of these single-tokenized 
MWEs can bring about any further improvement 
on top of that. 
We carried out our experiments on an Eng-
lish?Bangla translation task, a relatively hard 
task with Bangla being a morphologically richer 
language. 
3 System Description 
3.1 PB-SMT 
Translation is modeled in SMT as a decision 
process, in which the translation Ie1 = e1 . . . ei . . 
. eI of a source sentence
Jf1 = f1 . . . fj . . . fJ is 
chosen to maximize (1): 
)().|(maxarg)|(maxarg 111
,
11
, 11
IIJ
eI
JI
eI
ePefPfeP
II
=      (1)  
where )|( 11
IJ efP  and )( 1
IeP  denote respec-
tively the translation model and the target lan-
guage model (Brown et al, 1993). In log-linear 
phrase-based SMT, the posterior probability 
)|( 11
JI feP  is directly modeled as a log-linear 
combination of features (Och and Ney, 2002), 
that usually comprise M translational features, 
and the language model, as in (2): 
 
47
?
=
=
M
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?  
)(log 1
I
LM eP?+        (2)     
where k
k sss ...11 =  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik, bk, jk), 
          
kk iik
eee ...? 11 +?= , 
         
kk jbk
fff ...? = .          (3) 
and each feature mh?  in (2) can be rewritten as in 
(4): 
?
=
=
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(                  (4) 
where mh? is a feature that applies to a single 
phrase-pair. It thus follows (5): 
? ??
= ==
=
K
k
K
k
kkkkkkm
M
m
m sefhsefh
1 11
),?,?(?),?,?(??      (5) 
where m
M
m
mhh ??
1
?
=
= ? .            
3.2 Preprocessing of the Parallel Corpus 
The initial English?Bangla parallel corpus is 
cleaned and filtered using a semi-automatic 
process. We employed two kinds of multi-word 
information: compound verbs and NEs. Com-
pound verbs are first identified on both sides of 
the parallel corpus. Chakrabarty et al (2008) 
analyzed and identified a category of V+V com-
plex predicates called lexical compound verbs 
for Hindi. We adapted their strategy for identifi-
cation of compound verbs in Bangla. In addition 
to V+V construction, we also consider N+V and 
ADJ+V structures. 
NEs are also identified on both sides of trans-
lation pairs. NEs in Bangla are much harder to 
identify than in English (Ekbal and Bandyop-
adhyay, 2009). This can be attributed to the fact 
that (i) there is no concept of capitalization in 
Bangla; and (ii) Bangla common nouns are often 
used as proper names. In Bangla, the problem is 
compounded by the fact that suffixes (case 
markers, plural markers, emphasizers, specifiers) 
are also added to proper names, just like to any 
other common nouns. As a consequence, the ac-
curacy of Bangla NE recognizers (NER) is much 
poorer compared to that for English. Once the 
compound verbs and the NEs are identified on 
both sides of the parallel corpus, they are con-
verted into and replaced by single tokens. When 
converting these MWEs into single tokens, we 
replace the spaces with underscores (?_?). Since 
there are already some hyphenated words in the 
corpus, we do not use hyphenation for this pur-
pose; besides, the use of a special word separator 
(underscore in our case) facilitates the job of 
deciding which single-token (target language) 
MWEs to detokenize into words comprising 
them, before evaluation. 
3.3 Transliteration  Using Modified Joint 
Source-Channel Model 
Li et al (2004) proposed a generative framework 
allowing direct orthographical mapping of trans-
literation units through a joint source-channel 
model, which is also called n-gram translitera-
tion model. They modeled the segmentation of 
names into transliteration units (TU) and their 
alignment preferences using maximum likeli-
hood via EM algorithm (Dempster et al, 1977). 
Unlike the noisy-channel model, the joint 
source-channel model tries to capture how 
source and target names can be generated simul-
taneously by means of contextual n-grams of the 
transliteration units. For K aligned TUs, they 
define the bigram model as in (6): 
 )...,,...,(),( 2121 KK bbbeeePBEP =  
  ),...,,,( 21 KbebebeP ><><><=  
   ? ><><= K
=k
k bebeP
1
1-k
1 ),|,(         (6) 
where E refers to the English name and B the 
transliteration in Bengali, while ei and bi refer to 
the ith English and Bangla segment (TU) respec-
tively. 
Ekbal et al (2006) presented a modification to 
the joint source-channel model to incorporate 
different contextual information into the model 
for Indian languages. They used regular expres-
sions and language-specific heuristics based on 
consonant and vowel patterns to segment names 
into TUs. Their modified joint source-channel 
model, for which they obtained improvement 
48
over the original joint source-channel model, 
essentially considers a trigram model for the 
source language and a bigram model for the tar-
get, as in (7). 
 ? +><><= K
=k
kk ebebePBEP
1
11-k ),,|,(),(   (7) 
Ekbal et al (2006) reported a word agreement 
ratio of 67.9% on an English?Bangla translit-
eration task. In the present work, we use the 
modified joint source-channel model of (Ekbal 
et al, 2006) to translate names for establishing 
NE alignments in the parallel corpus. 
3.4 Automatic Alignment of NEs through 
Transliteration 
We first create an NE parallel corpus by extract-
ing the source and target (single token) NEs 
from the NE-tagged parallel translations in 
which both sides contain at least one NE. For 
example, we extract the NE translation pairs 
given in (9) from the sentence pair shown in (8), 
where the NEs are shown as italicized. 
(8a) Kirti_Mandir , where Mahatma_Gandhi 
was born , today houses a photo exhibition on 
the life and times of the Mahatma , a library, a 
prayer hall and other memorabilia . 
(8b) ??????_??n? , ?????? ???t?_??n? ??n????? , 
???????? ?????? ???t?? ???? o ??i ????? 
?????????? e??? ??tp????????? , e??? ??i?b?? o 
e??? p?????? ?? e?? a????? s ????????? ??????t 
??? ? 
(9a) Kirti_Mandir Mahatma_Gandhi Mahatma 
(9b) ??????_??n? ???t?_??n? ???t?? 
Then we try to align the source and target NEs 
extracted from a parallel sentence, as illustrated 
in (9). If both sides contain only one NE then the 
alignment is trivial, and we add such NE pairs to 
seed another parallel NE corpus that contains 
examples having only one token in both side. 
Otherwise, we establish alignments between the 
source and target NEs using transliteration. We 
use the joint source-channel model of translitera-
tion (Ekbal et al, 2006) for this purpose.  
If both the source and target side contains n 
number of NEs, and the alignments of n-1 NEs 
can be established through transliteration or by 
means of already existing alignments, then the 
nth alignment is trivial. However, due to the rela-
tive performance difference of the NERs for the 
source and target language, the number of NEs 
identified on the source and target sides is al-
most always unequal (see Section 4). Accord-
ingly, we always use transliteration to establish 
alignments even when it is assumed to be trivial. 
Similarly, for multi-word NEs, intra-NE word 
alignments are established through translitera-
tion or by means of already existing alignments. 
For a multi-word source NE, if we can align all 
the words inside the NE with words inside a tar-
get NE, then we assume they are translations of 
each other. Due to the relatively poor perform-
ance of the Bangla NER, we also store the im-
mediate left and right neighbouring words for 
every NE in Bangla, just in case the left or the 
right word is a valid part of the NE but is not 
properly tagged by the NER. 
As mentioned earlier, since the source side 
NER is much more reliable than the target side 
NER, we transliterate the English NEs, and try 
to align them with the Bangla NEs. For aligning 
(capitalized) English words to Bangla words, we 
take the 5 best transliterations produced by the 
transliteration system for an English word, and 
compare them against the Bangla words. Bangla 
NEs often differ in their choice of matras (vowel 
modifiers). Thus we first normalize the Bangla 
words, both in the target NEs and the transliter-
ated ones, to a canonical form by dropping the 
matras, and then compare the results. In effect, 
therefore, we just compare the consonant se-
quences of every transliteration candidate with 
that of a target side Bangla word; if they match, 
then we align the English word with the Bangla 
word. 
???? (? + ??+ ? + ?) -- ????? (? + ?? + ? + ?? + ?) 
      (10) 
The example in (10) illustrates the procedure. 
Assume, we are trying to align ?Niraj? with 
???????. The transliteration system produces 
?????? from the English word ?Niraj? and we 
compare ?????? with ???????. Since the conso-
nant sequences match in both words, ?????? is 
considered a spelling variation of ???????, and 
the English word ?Niraj? is aligned to the 
Bangla word ???????. 
In this way, we achieve word-level align-
ments, as well as NE-level alignments. (11) 
shows the alignments established from (8). The 
word-level alignments help to establish new 
49
word / NE alignments. Word and NE alignments 
obtained in this way are added to the parallel 
corpus as additional training data. 
(11a) Kirti-Mandir  ? ??????-??n?  
(11b) Kirti ? ?????? 
(11c) Mandir  ? ??n? 
(11d) Mahatma-Gandhi ? ???t?-??n?  
(11e) Mahatma ? ???t? 
(11f) Gandhi ? ??n? 
(11g) Mahatma ? ???t?? 
3.5 Tools and Resources Used 
A sentence-aligned English?Bangla parallel 
corpus containing 14,187 parallel sentences from 
a travel and tourism domain was used in the pre-
sent work. The corpus was obtained from the 
consortium-mode project ?Development of Eng-
lish to Indian Languages Machine Translation 
(EILMT) System? 1. 
The Stanford Parser2 and the CRF chunker3 
were used for identifying compound verbs in the 
source side of the parallel corpus. The Stanford 
NER4 was used to identify NEs on the source 
side (English) of the parallel corpus. 
The sentences on the target side (Bangla) 
were POS-tagged by using the tools obtained 
from the consortium mode project ?Develop-
ment of Indian Languages to Indian Languages 
Machine Translation (ILILMT) System?. NEs in 
Bangla are identified using the NER system of 
Ekbal and Bandyopadhyay (2008). We use the 
Stanford Parser, Stanford NER and the NER for 
Bangla along with the default model files pro-
vided, i.e., with no additional training. 
The effectiveness of the MWE-aligned paral-
lel corpus developed in the work is demonstrated 
by using the standard log-linear PB-SMT model 
as our baseline system: GIZA++ implementation 
of IBM word alignment model 4, phrase-
extraction heuristics described in (Koehn et al, 
2003), minimum-error-rate training (Och, 2003) 
on a held-out development set, target language 
model with Kneser-Ney smoothing (Kneser and 
                                                 
1 The EILMT and ILILMT projects are funded by the De-
partment of Information Technology (DIT), Ministry of 
Communications and Information Technology (MCIT), 
Government of India. 
2 http://nlp.stanford.edu/software/lex-parser.shtml 
3 http://crfchunker.sourceforge.net/ 
4 http://nlp.stanford.edu/software/CRF-NER.shtml 
Ney, 1995) trained with SRILM (Stolcke, 2002), 
and Moses decoder (Koehn et al, 2007). 
4 Experiments and Results 
We randomly extracted 500 sentences each for 
the development set and testset from the initial 
parallel corpus, and treated the rest as the train-
ing corpus. After filtering on maximum allow-
able sentence length of 100 and sentence length 
ratio of 1:2 (either way), the training corpus con-
tained 13,176 sentences. In addition to the target 
side of the parallel corpus, a monolingual Bangla 
corpus containing 293,207 words from the tour-
ism domain was used for the target language 
model. We experimented with different n-gram 
settings for the language model and the maxi-
mum phrase length, and found that a 4-gram 
language model and a maximum phrase length 
of 4 produced the optimum baseline result. We 
therefore carried out the rest of the experiments 
using these settings. 
English Bangla In training set 
T U T U 
Compound verbs 4,874 2,289 14,174 7,154
Single-word NEs 4,720 1,101 5,068 1,175
2-word NEs 4,330 2,961 4,147 3,417
>2 word NEs 1,555 1,271 1,390 1,278
Total NEs 10,605 5,333 10,605 5,870
Total NE words 22,931 8,273 17,107 9,106
Table 1.  MWE statistics (T - Total occur-
rence, U ? Unique). 
Of the 13,676 sentences in the training and 
development set, 13,675 sentences had at least 
one NE on both sides, only 22 sentences had 
equal number of NEs on both sides, and 13,654 
sentences had an unequal number of NEs. Simi-
larly, for the testset, all the sentences had at least 
one NE on both sides, and none had an equal 
number of NEs on both sides. It gives an indica-
tion of the relative performance differences of 
the NERs. 6.6% and 6.58% of the source tokens 
belong to NEs in the training and testset respec-
tively. These statistics reveal the high degree of 
NEs in the tourism domain data that demands 
special treatment. Of the 225 unique NEs ap-
pearing on the source side of the testset, only 65 
NEs are found in the training set.  
50
Experiments Exp BLEU METEOR NIST WER PER TER 
Baseline 1 8.74 20.39 3.98 77.89 62.95 74.60
NEs of any length as Single 
Token (New-MWNEaST) 
2 9.15 18.19 3.88 77.81 63.85 74.61
NEs of length >2 as  
Single Tokens (MWNE-
aST) 
3 8.76 18.78 3.86 78.31 63.78 75.15
 
 
NEs as Single  
Tokens  
(NEaST) 
2-Word NEs as Single To-
kens (2WNEaST) 
4 9.13 17.28 3.92 78.12 63.15 74.85
Compound Verbs as  Single Tokens 
(CVaST) ? 
5 9.56 15.35 3.96 77.60 63.06 74.46
Alignment of NEs of any 
length (New-MWNEA) ? 
6 13.33 24.06 4.44 74.79 60.10 71.25
Alignment of NEs of length 
upto 2 (New-2WNEA) ? 
7 10.35 20.93 4.11 76.49 62.20 73.05
Alignment of NEs of length 
>2 (MWNEA) ? 
8 12.39 23.13 4.36 75.51 60.58 72.06
 
 
 
 
NE Alignment 
(NEA) 
Alignment of NEs of length 
2 (2WNEA) ? 
9 11.2 23.14 4.26 76.13 60.72 72.57
New-MWNEaST 10 8.62 16.64 3.73 78.41 65.21 75.47
MWNEaST 11 8.74 14.68 3.84 78.40 64.05 75.40
 
CVaST 
+NEaST 2WNEaST 12 8.85 16.60 3.86 78.17 63.90 75.33
New-MWNEA? 13 11.22 21.02 4.16 75.99 61.96 73.06
New-2WNEA? 14 10.07 17.67 3.98 77.08 63.35 74.18
MWNEA? 15 10.34 16.34 4.07 77.12 62.38 73.88
 
CVaST +NEA 
2WNEA? 16 10.51 18.92 4.08 76.77 62.28 73.56
Table 2.  Evaluation results for different experimental setups (The ??? marked systems produce 
statistically significant improvements on BLEU over the baseline system).
Table 1 shows the MWE statistics of the 
parallel corpus as identified by the NERs. The 
average NE length in the training corpus is 
2.16 for English and 1.61 for Bangla. As can 
be seen from Table 1, 44.5% and 47.8% of the 
NEs are single-word NEs in English and 
Bangla respectively, which suggests that prior 
alignment of the single-word NEs, in addition 
to multi-word NE alignment, should also be 
beneficial to word and phrase alignment. 
Of all the NEs in the training and develop-
ment sets, the transliteration-based alignment 
process was able to establish alignments of 
4,711 single-word NEs, 4,669 two-word NEs 
and 1,745 NEs having length more than two. 
It is to be noted that, some of the single-word 
NE alignments, as well as two-word NE 
alignments, result from multi-word NE align-
ment. 
We analyzed the output of the NE align-
ment module and observed that longer NEs 
were aligned better than the shorter ones, 
which is quite intuitive, as longer NEs have 
more tokens to be considered for intra-NE 
alignment. Since the NE alignment process is 
based on transliteration, the alignment method 
does not work where NEs involve translation 
or acronyms. We also observed that English 
multi-word NEs are sometimes fused together 
into single-word NEs. 
We performed three sets of experiments: 
treating compound verbs as single tokens, 
treating NEs as single tokens, and the combi-
nation thereof. Again for NEs, we carried out 
three types of preprocessing: single-
tokenization of (i) two-word NEs, (ii) more 
than two-word NEs, and (iii) NEs of any 
length. We make distinctions among these 
three to see their relative effects. The devel-
opment and test sets, as well as the target lan-
guage monolingual corpus (for language mod-
eling), are also subjected to the same preproc-
essing of single-tokenizing the MWEs. For 
NE alignment, we performed experiments us-
ing 4 different settings: alignment of (i) NEs 
of length up to two, (ii) NEs of length two, 
51
(iii) NEs of length greater than two, and (iv) 
NEs of any length. Before evaluation, the sin-
gle-token (target language) underscored 
MWEs are expanded back to words compris-
ing the MWEs. 
Since we did not have the gold-standard 
word alignment, we could not perform intrin-
sic evaluation of the word alignment. Instead 
we carry out extrinsic evaluation on the MT 
quality using the well known automatic MT 
evaluation metrics: BLEU (Papineni et al, 
2002), METEOR (Banerjee and Lavie, 2005), 
NIST (Doddington, 2002), WER, PER and 
TER (Snover et al, 2006). As can be seen 
from the evaluation results reported in Table 
2, baseline Moses without any preprocessing 
of the dataset produces a BLEU score of 8.74. 
The low score can be attributed to the fact that 
Bangla, a morphologically rich language, is 
hard to translate into. Moreover, Bangla being 
a relatively free phrase order language (Ekbal 
and Bandyopadhyay, 2009) ideally requires 
multiple set of references for proper evalua-
tion. Hence using a single reference set does 
not justify evaluating translations in Bangla. 
Also the training set was not sufficiently large 
enough for SMT. Treating only longer than 2-
word NEs as single tokens does not help im-
prove the overall performance much, while 
single tokenization  of two-word NEs as single 
tokens produces some improvements (.39 
BLEU points absolute, 4.5% relative). Con-
sidering compound verbs as single tokens 
(CVaST) produces a .82 BLEU point im-
provement (9.4% relative) over the baseline. 
Strangely, when both compound verbs and 
NEs together are counted as single tokens, 
there is hardly any improvement. By contrast, 
automatic NE alignment  (NEA) gives a huge 
impetus to system performance, the best of 
them (4.59 BLEU points absolute, 52.5% rela-
tive improvement) being the alignment of NEs 
of any length that produces the best scores 
across all metrics. When NEA is combined 
with CVaST, the improvements are substan-
tial, but it can not beat the individual im-
provement on NEA. The (?) marked systems 
produce statistically significant improvements 
as measured by bootstrap resampling method 
(Koehn, 2004) on BLEU over the baseline 
system. Metric-wise individual best scores are 
shown in bold in Table 2. 
5 Conclusions and Future Work 
In this paper, we have successfully shown 
how the simple yet effective preprocessing of 
treating two types of MWEs, namely NEs and 
compound verbs, as single-tokens, in conjunc-
tion with prior NE alignment can boost the 
performance of PB-SMT system on an Eng-
lish?Bangla translation task. Treating com-
pound verbs as single-tokens provides signifi-
cant gains over the baseline PB-SMT system. 
Amongst the MWEs, NEs perhaps play the 
most important role in MT, as we have clearly 
demonstrated through experiments that auto-
matic alignment of NEs by means of translit-
eration improves the overall MT performance 
substantially across all automatic MT evalua-
tion metrics. Our best system yields 4.59 
BLEU points improvement over the baseline, 
a 52.5% relative increase. We compared a 
subset of the output of our best system with 
that of the baseline system, and the output of 
our best system almost always looks better in 
terms of either lexical choice or word order-
ing. The fact that only 28.5% of the testset 
NEs appear in the training set, yet prior auto-
matic alignment of the NEs brings about so 
much improvement in terms of MT quality, 
suggests that it not only improves the NE 
alignment quality in the phrase table, but word 
alignment and phrase alignment quality must 
have also been improved significantly. At the 
same time, single-tokenization of MWEs 
makes the dataset sparser, but yet improves 
the quality of MT output to some extent. Data-
driven approaches to MT, specifically for 
scarce-resource language pairs for which very 
little parallel texts are available, should benefit 
from these preprocessing methods. Data 
sparseness is perhaps the reason why single-
tokenization of NEs and compound verbs, 
both individually and in collaboration, did not 
add significantly to the scores. However, a 
significantly large parallel corpus can take 
care of the data sparseness problem introduced 
by the single-tokenization of MWEs. 
The present work offers several avenues for 
further work. In future, we will investigate 
how these automatically aligned NEs can be 
52
used as anchor words to directly influence the 
word alignment process. We will look into 
whether similar kinds of improvements can be 
achieved for larger datasets, corpora from dif-
ferent domains and for other language pairs. 
We will also investigate how NE alignment 
quality can be improved, especially where 
NEs involve translation and acronyms. We 
will also try to perform morphological analy-
sis or stemming on the Bangla side before NE 
alignment. We will also explore whether dis-
criminative approaches to word alignment can 
be employed to improve the precision of the 
NE alignment. 
Acknowledgements 
This research is partially supported by the Sci-
ence Foundation Ireland (Grant 07/CE/I1142) 
as part of the Centre for Next Generation Lo-
calisation (www.cngl.ie) at Dublin City Uni-
versity, and EU projects PANACEA (Grant 
7FP-ITC-248064) and META-NET (Grant 
FP7-ICT-249119). 
References 
Banerjee, Satanjeev, and Alon Lavie. 2005. An 
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 
proceedings of the ACL-2005 Workshop on In-
trinsic and Extrinsic Evaluation Measures for 
MT and/or Summarization, pp. 65-72. Ann Ar-
bor, Michigan., pp. 65-72. 
Brown, Peter F., Stephen A. Della Pietra, Vincent 
J. Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: 
parameter estimation. Computational Linguis-
tics, 19(2):263-311. 
Carpuat, Marine, and Mona Diab. 2010. Task-
based Evaluation of Multiword Expressions: a 
Pilot Study in Statistical Machine Translation. 
In Proceedings of Human Language Technology 
conference and the North American Chapter of 
the Association for Computational Linguistics 
conference (HLT-NAACL 2010), Los Angeles, 
CA, pp. 242-245. 
Chakrabarti, Debasri, Hemang Mandalia, Ritwik 
Priya, Vaijayanthi Sarma, and Pushpak Bhat-
tacharyya. 2008. Hindi compound verbs and 
their automatic extraction. In Proceedings 
of  the 22nd International Conference on Com-
putational Linguistics (Coling 2008), Posters 
and demonstrations, Manchester, UK, pp. 27-
30. 
Dempster, A.P., N.M. Laird, and D.B. Rubin. 
1977). Maximum Likelihood from Incomplete 
Data via the EM Algorithm. Journal of the 
Royal Statistical Society, Series B (Methodo-
logical) 39 (1): 1?38. 
Doddington, George. 2002. Automatic evaluation 
of machine translation quality using n-gram 
cooccurrence statistics. In Proceedings of the 
Second International Conference on Human 
Language Technology Research (HLT-2002), 
San Diego, CA, pp. 128-132. 
Eck, Matthias, Stephan Vogel, and Alex Waibel. 
2004. Improving statistical machine translation 
in the medical domain using the Unified Medi-
cal Language System. In Proceedings of  the 
20th International Conference on Computational 
Linguistics (COLING 2004), Ge-
neva, Switzerland, pp. 792-798. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2009. 
Voted NER system using appropriate unlabeled 
data. In proceedings of the ACL-IJCNLP-2009 
Named Entities Workshop (NEWS 2009), 
Suntec, Singapore, pp. 202-210. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2008. 
Maximum Entropy Approach for Named Entity 
Recognition in Indian Languages. International 
Journal for Computer Processing of Lan-
guages (IJCPOL), Vol. 21(3):205-237. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. 
A new approach for English-Chinese named en-
tity alignment. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2004), Barcelona, 
Spain, pp. 372-379. 
Huang, Fei, Stephan Vogel, and Alex Waibel. 
2003. Automatic extraction of named entity 
translingual equivalence based on multi-feature 
cost minimization. In Proceedings of the ACL-
2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition, 2003, 
Sapporo, Japan, pp. 9-16. 
Kneser, Reinhard, and Hermann Ney. 1995. Im-
proved backing-off for m-gram language model-
ing. In Proceedings of the IEEE Internation 
Conference on Acoustics, Speech, and Signal 
Processing (ICASSP), vol. 1, pp. 181-184. De-
troit, MI. 
Koehn, Philipp, Franz Josef Och, and Daniel 
Marcu. 2003. Statistical phrase-based transla-
tion. In Proceedings of HLT-NAACL 2003: 
53
conference combining Human Language Tech-
nology conference series and the North Ameri-
can Chapter of the Association for Computa-
tional Linguistics conference series,  Edmonton, 
Canada, pp. 48-54. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, 
Chris Callison-Burch, Marcello Federico, Ni-
cola Bertoldi, Brooke Cowan, Wade Shen, 
Christine Moran, Richard Zens, Chris Dyer, 
Ond?ej Bojar, Alexandra Constantin, and Evan 
Herbst. 2007. Moses: open source toolkit for 
statistical machine translation. In Proceedings of 
the 45th Annual meeting of the Association for 
Computational Linguistics (ACL 2007): Pro-
ceedings of demo and poster sessions, Prague, 
Czech Republic, pp. 177-180. 
Koehn, Philipp. 2004. Statistical significance tests 
for machine translation evaluation. In  EMNLP-
2004: Proceedings of the 2004 Conference on 
Empirical Methods in Natural Language Proc-
essing, 25-26 July 2004, Barcelona, Spain, pp. 
388-395. 
Marcu, Daniel. 2001. Towards a Unified Approach 
to Memory- and Statistical-Based Machine 
Translation. In Proceedings of the 39th Annual 
Meeting of the Association for Computational 
Linguistics (ACL 2001), Toulouse, France, pp. 
386-393. 
Moore, Robert C. 2003. Learning translations of 
named-entity phrases from parallel corpora. In 
Proceedings of 10th Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics (EACL 2003), Budapest, 
Hungary; pp. 259-266. 
Och, Franz J. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), Sap-
poro, Japan, pp. 160-167. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proceedings of the 40th Annual Meeting of the 
Association for Computational Linguistics 
(ACL-2002), Philadelphia, PA, pp. 311-318. 
Ren, Zhixiang, Yajuan L?, Jie Cao, Qun Liu, and 
Yun Huang. 2009. Improving statistical ma-
chine translation using domain bilingual multi-
word expressions. In Proceedings of the 2009 
Workshop on Multiword Expressions, ACL-
IJCNLP 2009, Suntec, Singapore, pp. 47-54. 
Sag, Ivan A., Timothy Baldwin, Francis Bond, 
Ann Copestake and Dan Flickinger. 2002. Mul-
tiword expressions: A pain in the neck for NLP. 
In Proceedings of the 3rd International Confer-
ence on Intelligent Text Processing and Compu-
tational Linguistics (CICLing-2002), Mexico 
City, Mexico, pp. 1-15. 
Snover, Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted hu-
man annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Transla-
tion in the Americas (AMTA 2006), Cambridge, 
MA, pp. 223-231. 
Vogel, Stephan, Hermann Ney, and Christoph 
Tillmann. 1996. HMM-based word alignment in 
statistical translation. In Proceedings of the 16th 
International Conference on Computational 
Linguistics (COLING 1996), Copenhagen, pp. 
836-841. 
Venkatapathy, Sriram, and Aravind K. Joshi. 2006. 
Using information about multi-word expres-
sions for the word-alignment task. In Proceed-
ings of Coling-ACL 2006: Workshop on Multi-
word Expressions: Identifying and Exploiting 
Underlying Properties, Sydney, pp. 20-27. 
Wu, Hua Haifeng Wang, and Chengqing Zong. 
2008. Domain adaptation for statistical machine 
translation with domain dictionary and mono-
lingual corpora. In Proceedings of the 22nd In-
ternational Conference on Computational Lin-
guistics (COLING 2008),  Manchester, UK, pp. 
993-1000. 
54
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 42?50,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Determining Compositionality of Word Expressions
Using Word Space Models
Lubom??r Krc?ma?r?, Karel Jez?ek
University of West Bohemia
Faculty of Applied Sciences
Department of Computer Science and Engineering
Pilsen, Czech Republic
{lkrcmar,jezek ka}@kiv.zcu.cz
Pavel Pecina
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Prague, Czech Republic
pecina@ufal.mff.cuni.cz
Abstract
This research focuses on determining seman-
tic compositionality of word expressions us-
ing word space models (WSMs). We discuss
previous works employing WSMs and present
differences in the proposed approaches which
include types of WSMs, corpora, preprocess-
ing techniques, methods for determining com-
positionality, and evaluation testbeds.
We also present results of our own approach
for determining the semantic compositionality
based on comparing distributional vectors of
expressions and their components. The vec-
tors were obtained by Latent Semantic Analy-
sis (LSA) applied to the ukWaC corpus. Our
results outperform those of all the participants
in the Distributional Semantics and Composi-
tionality (DISCO) 2011 shared task.
1 Introduction
A word expression is semantically compositional
if its meaning can be understood from the literal
meaning of its components. Therefore, semanti-
cally compositional expressions involve e.g. ?small
island? or ?hot water?; on the other hand, seman-
tically non-compositional expressions are e.g. ?red
tape? or ?kick the bucket?.
The notion of compositionality is closely related
to idiomacy ? the higher the compositionality the
lower the idiomacy and vice versa (Sag et al, 2002;
Baldwin and Kim, 2010).
Non-compositional expressions are often referred
to as Multiword Expressions (MWEs). Baldwin and
Kim (2010) differentiate the following sub-types of
compositionality: lexical, syntactic, semantic, prag-
matic, and statistical. This paper is concerned with
semantic compositionality.
Compositionality as a feature of word expressions
is not discrete. Instead, expressions populate a con-
tinuum between two extremes: idioms and free word
combinations (McCarthy et al, 2003; Bannard et al,
2003; Katz, 2006; Fazly, 2007; Baldwin and Kim,
2010; Biemann and Giesbrecht, 2011). Typical ex-
amples of expressions between the two extremes are
?zebra crossing? or ?blind alley?.
Our research in compositionality is motivated
by the hypothesis that a special treatment of se-
mantically non-compositional expressions can im-
prove results in various Natural Language Process-
ing (NPL) tasks, as shown for example by Acosta et
al. (2011), who utilized MWEs in Information Re-
trieval (IR). Besides that, there are other NLP ap-
plications that can benefit from knowing the degree
of compositionality of expressions such as machine
translation (Carpuat and Diab, 2010), lexicography
(Church and Hanks, 1990), word sense disambigua-
tion (Finlayson and Kulkarni, 2011), part-of-speech
(POS) tagging and parsing (Seretan, 2008) as listed
in Ramisch (2012).
The main goal of this paper is to present an anal-
ysis of previous approaches using WSMs for de-
termining the semantic compositionality of expres-
sions. The analysis can be found in Section 2. A
special attention is paid to the evaluation of the pro-
posed models that is described in Section 3. Section
4 presents our first intuitive experimental setup and
results of LSA applied to the DISCO 2011 task. Sec-
tion 5 concludes the paper.
42
2 Semantic Compositionality of Word
Expressions Determined by WSMs
Several recent works, including Lin (1999), Schone
and Jurafsky (2001), Baldwin et al (2003), Mc-
Carthy et al (2003), Katz (2006), Johannsen et al
(2011), Reddy et al (2011a), and Krc?ma?r? et al
(2012), show the ability of methods based on WSMs
to capture the degree of semantic compositionality
of word expressions. We analyse the proposed meth-
ods and discuss their differences. As further de-
scribed in detail and summarized in Table 1, the ap-
proaches differ in the type of WSMs, corpora, pre-
processing techniques, methods for determining the
compositionality, datasets for evaluation, and meth-
ods of evaluation itself.
Our understanding of WSM is in agreement with
Sahlgren (2006): ?The word space model is a com-
putational model of word meaning that utilizes the
distributional patterns of words collected over large
text data to represent semantic similarity between
words in terms of spatial proximity?. For more
information on WSMs, see e.g. Turney and Pan-
tel (2010), Jurgens and Stevens (2010), or Sahlgren
(2006).
WSMs and their parameters WSMs can be built
by different algorithms including LSA (Landauer
and Dumais, 1997), Hyperspace Analogue to Lan-
guage (HAL) (Lund and Burgess, 1996), Random
Indexing (RI) (Sahlgren, 2005), and Correlated Oc-
currence Analogue to Lexical Semantics (COALS)
(Rohde et al, 2005). Every algorithm has its own
specifics and can be configured in different ways.
The configuration usually involves e.g. the choice
of context size, weighting functions, or normaliz-
ing functions. While Schone and Jurafsky (2001),
Baldwin et al (2003), and Katz (2006) addopted
LSA-based approaches, Johannsen et al (2011) and
Krc?ma?r? et al (2012) employ COALS; the others use
their own specific WSMs.
Corpora and text preprocessing Using differ-
ent corpora and their preprocessing naturally leads
to different WSMs. The preprocessing can differ
e.g. in the choice of used word forms or in re-
moval/retaining of low-frequency words. For exam-
ple, while Lin (1999) employs a 125-million-word
newspaper corpus, Schone and Jurafsky (2001) use
a 6.7-million-word subset of the TREC databases,
Baldwin et al (2003) base their experiments on
90 million words from the British National Corpus
(Burnard, 2000). Krc?ma?r? et al (2012), Johannsen et
al. (2011), and Reddy et al (2011a) use the ukWaC
corpus, consisting of 1.9 billion words from web
texts (Baroni et al, 2009). As for preprocessing,
Lin (1999) extracts triples with dependency relation-
ships, Baldwin et al (2003), Reddy et al (2011a),
and Krc?ma?r? et al (2012) concatenate word lemmas
with their POS categories. Johannsen et al (2011)
use word lemmas and remove low-frequency words
while Reddy et al (2011a), for example, keep only
frequent content words.
Methods We have identified three basic methods
for determining semantic compositionality:
1) The substitutability-based methods exploit
the fact that replacing components of non-
compositional expressions by words which are
similar leads to anti-collocations (Pearce, 2002).
Then, frequency or mutual information of such
expressions (anti-collocations) is compared with
the frequency or mutual information of the original
expressions. For example, consider expected occur-
rence counts of ?hot dog? and its anti-collocations
such as ?warm dog? or ?hot terrier?.
2) The component-based methods, utilized for ex-
ample by Baldwin et al (2003) or Johannsen et al
(2011), compare the distributional characteristics of
expressions and their components. The context vec-
tors expected to be different from each other are
e.g. the vector representing the expression ?hot dog?
and the vector representing the word ?dog?.
3) The compositionality-based methods compare
two vectors of each analysed expression: the true
co-occurrence vector of an expression and the vec-
tor obtained from vectors corresponding to the com-
ponents of the expression using a compositional-
ity function (Reddy et al, 2011a). The most com-
mon compositionality functions are vector addition
or pointwise vector multiplication (Mitchell and La-
pata, 2008). For example, the vectors for ?hot dog?
and ?hot???dog? are supposed to be different.
Evaluation datasets There is still no consensus
on how to evaluate models determining semantic
compositionality. However, by examining the dis-
cussed papers, we have observed an increasing ten-
43
Paper Corpora WSMs Methods Data (types) Evaluation
Lin (1999) 125m, triples own SY NVAA c. dicts., P/R
Schone+Jurafsky(2001) 6.7m TREC LSA SY, CY all types WN, P/Rc
Baldwin et al (2003) BNC+POS LSA CT NN, VP WN, PC
McCarthy et al (2003) BNC+GR own CTn PV MA, WN, dicts., S
Katz (2006) GNC LSA CY PNV MA, P/R, Fm
Krc?ma?r? et al (2012) ukWaC+POS COALS SY AN, VO, SV MA, CR, APD, CL
Johannsen et al (2011) ukWaC COALS SY, CT AN, VO, SV MA, CR, APD, CL
Reddy et al (2011a) ukWaC+POS own CT, CY NN MA, S, R2
Table 1: Overview of experiments applying WSMs to determine semantic compositionality of word expressions. BNC
- British National Corpus, GR - grammatical relations, GNC - German newspaper corpus, TREC - TREC corpus;
SY - substitutability-based methods, CT - component-based methods, CTn - component-based methods comparing
WSM neighbors of expressions and their components, CY - compositionality-based methods; NVAP c. - noun, verb,
adjective, adverb combinations, NN - noun-noun, VP - verb-particles, AN - adjective-noun, VO - verb-object, SV -
subject-verb, PV - phrasal-verb, PNV - preposition-noun-verb; dicts. - dictionaries of idioms, WN - Wordnet, MA
- use of manually annotated data, S - Spearman correlation, PC - Pearson correlation, CR - Spearman and Kendall
correlations, APD - average point difference, CL - classification, P/R - Precision/Recall, P/Rc - Precision/Recall
curves, Fm - F measure, R2 - goodness.
dency to exploit manually annotated data from a
specific corpus, ranging from semantically composi-
tional to non-compositional expressions (McCarthy
et al, 2003; Katz, 2006; Johannsen et al, 2011;
Reddy et al, 2011a; Krc?ma?r? et al, 2012).
This approach, as opposed to the methods
based on dictionaries of MWEs (idioms) or Word-
net (Miller, 1995), has the following advantages:
Firstly, the classification of a manually annotated
data is not binary but finer-grained, enabling the
evaluation to be more detailed. Secondly, the low-
coverage problem of dictionaries, which originates
for example due to the facts that new MWEs still
arise or are domain specific, is avoided.1 For exam-
ple, Lin (1999), Schone and Jurafsky (2001), Bald-
win et al (2003) used Wordnet or other dictionary-
type resources.
3 Evaluation Methods
This section discusses evaluation methods includ-
ing average point difference (APD), Spearman and
Kendall correlations, and precision of classifica-
tion (PoC) suggested by Biemann and Giesbrecht
(2011); Precision/nBest, Recall/nBest and Preci-
sion/Recall curves proposed by Evert (2005); and
1The consequence of using a low-coverage dictionary can
cause underestimation of the used method since the dictionary
does not have to contain MWEs correctly found by that method.
Average Precision used by Pecina (2009). Our eval-
uation is based on the English part of the manu-
ally annotated datasets DISCO 2011 (Biemann and
Giesbrecht, 2011), further referred to as DISCO-En-
Gold.
Disco-En-Gold consists of 349 expressions di-
vided into training (TrainD), validation (ValD), and
test data (TestD) manually assigned scores from 0
to 100, indicating the level of compositionality (the
lower the score the lower the compositionality and
vice versa). The expressions are of the following
types: adjective-noun (AN), verb-object (VO), and
subject-verb (SV). Based on the numerical scores,
the expressions are also classified into three disjoint
classes (coarse scores): low, medium, and high com-
positional.2 A sample of the Disco-En-Gold data is
presented in Table 2.
Comparison of evaluation methods The purpose
of the DISCO workshop was to find the best meth-
ods for determining semantic compositionality. The
participants were asked to create systems capable of
assigning the numerical values closest to the ones
assigned by the annotators (Gold values). The pro-
posed APD evaluation measure is calculated as the
mean difference between the particular systems? val-
2Several expressions with the numerical scores close to the
specified thresholds were not classified into any class.
44
Type Expression Ns Cs
EN ADJ NN blue chip 11 low
EN V OBJ buck trend 14 low
EN ADJ NN open source 49 medium
EN V OBJ take advantage 57 medium
EN ADJ NN red squirrel 90 high
EN V SUBJ student learn 98 high
Table 2: A sample of manually annotated expressions
from Disco-En-Gold with their numerical scores (Ns) and
coarse scores (Cs).
ues and the Gold values assigned to the same expres-
sions. PoC is defined as the ratio of correct coarse
predictions to the number of all the predictions.
Following Krc?ma?r? et al (2012), we argue that
for the purpose of comparison of the methods, the
values assigned to a set of expressions by a certain
model are not as important as is the ranking of the
expressions (which is not sensitive to the original
distribution of compositionality values). Similarly
as Evert (2005), Pecina (2009), and Krc?ma?r? et al
(2012) we adopt evaluation based on ranking (al-
though the measures such as PoC or APD might pro-
vide useful information too).
Evaluation based on ranking can be realized
by measuring ranked correlations (Spearman and
Kendall) or Precision/Recall scores and curves com-
monly used e.g. in IR (Manning et al, 2008). In
IR, Precision is defined as the ratio of found rele-
vant documents to all the retrieved documents with
regards to a user?s query. Recall is defined as the ra-
tio of found relevant documents to all the relevant
documents in a test set to the user?s query. The
Precision/Recall curve is a curve depicting the de-
pendency of Precision upon Recall. Analogously,
the scheme can be used for evaluation of the meth-
ods finding semantically non-compositional expres-
sions. However, estimation of Recall is not possible
without knowledge of the correct class3 for every ex-
pression in a corpus. To bypass this, Evert (2005)
calculates Recall with respect to the set of annotated
data divided into non-compositional and composi-
tional classes. The Precision/nBest, Recall/nBest,
and Precision/Recall curves for the LSA experiment
3A semantically non-compositional expression or a seman-
tically compositional expressions
described in the following section are depicted in
Figures 1 and 2.
Evert?s (2005) curves allow us to visually com-
pare the results of the methods in more detail. To
facilitate comparison of several methods, we also
suggest using average precision (AP) adopted from
Pecina (2009), which reduces information provided
by a single Precision/Recall curve to one value. AP
is defined as a mean Precision at all the values of
Recall different from zero.
4 LSA experiment
LSA is WSM based on the Singular Value De-
composition (SVD) factorization (Deerwester et al,
1990) applied to the co-occurrence matrix. In the
matrix, the numbers of word occurrences in speci-
fied contexts4 are stored. The row vectors of the ma-
trix capture the word meanings.5 The idea of using
SVD is to project vectors corresponding to the words
into a lower-dimensional space and thus bring the
vectors of words with similar meaning near to each
other.
We built LSA WSM and applied the component-
based method to Disco-En-Gold. We used our
own modification of the LSA algorithm originally
implemented in the S-Space package (Jurgens and
Stevens, 2010). The modification lies in treating ex-
pressions and handling stopwords. Specifically, we
added vectors for the examined expressions to WSM
in such a way that the original vectors for words
were preserved. This differentiates our approach
e.g. from Baldwin et al (2003) or Johannsen et al
(2011) who label the expressions ahead of time and
build WSMs treating them as single words. Treat-
ing the expressions as the single words affects the
WSM vectors of their constituents. As an example,
consider the replacement of occurrences of ?short
distance? by e.g. the EXP#123 label. This affects
the WSM vectors of ?short? and ?distance? since
the numbers of their occurrences and the numbers
of contexts they occur in drops. Consequently, this
also affects the methods for determining the compo-
sitionality which are based upon using the vectors of
4The commonly used contexts for words are documents or
the preceding and following words in a specified window.
5WSMs exploit Harris? distributional hypothesis (Harris,
1954), which states that semantically similar words tend to ap-
pear in similar contexts.
45
expressions? constituents.
As for treating stopwords, we mapped the trigram
expressions containing the determiners ?the?, ?a?,
or ?an? as the middle word to the corresponding bi-
gram expressions without the determiners. The intu-
ition is to extract more precise co-occurrence vectors
for the VO expressions often containing some inter-
vening determiner. As an example, compare the oc-
currences of ?reinvent wheel? and ?reinvent (deter-
miner) wheel? in the ukWaC corpus which are 27
and 623, respectively, or the occurrences of ?cross
bridge? and ?cross (determiner) bridge? being 50
and 1050, respectively.6
We built LSA WSM from the whole ukWaC
POS-tagged corpus for all the word lemmas con-
catenated with their POS tags excluding stopwords.
We treated the following strings as stopwords: the
lemmas with frequency below 50 (omitting low-
frequency words), the strings containing two adja-
cent non-letter characters (omitting strings such as
web addresses and sequences of e.g. star symbols),
and lemmas with a different POS tag from noun,
proper noun, adjective, verb, and adverb (omitting
closed-class words). As contexts, the entire docu-
ments were used.
The co-occurrence matrix for words was normal-
ized by applying the log-entropy transformation and
reduced to 300 dimensions. Using these settings,
Landauer and Dumais (1997) obtained the best re-
sults. Finally, the co-occurrence vectors of expres-
sions were expressed in the lower-dimensional space
of words in a manner analogous to how a user?s
query is being expressed in lower-dimensional space
of documents in IR (Berry et al, 1995). The Disco-
En-Gold expressions were sorted in ascending order
by the average cosine similarity between the vec-
tors corresponding to the expressions and the vectors
corresponding to their components.
Evaluation We have not tried to find the optimal
parameter settings for the LSA-based model yet.
Therefore, we present the results on the concate-
nation of TrainD with ValD giving us TrainValD
and on TestD. The expressions ?leading edge? and
?broken link? were removed from TestD because
they occur in the ukWaC corpus assigned with the
6More precisely, the occurrences were calculated from the
POS-tagged parallels of the expressions.
required POS tags less than 50 times. APs with
the Spearman and Kendall correlations between the
compositionality values assigned by the LSA-based
model and the Gold values are depicted in Table 3.
The Spearman correlations of the LSA model ap-
plied to the whole TrainValD and TestD are highly
significant with p-values < 0.001. For the AP evalu-
ation, the expressions with numerical values less or
equal to 50 were classified as non-compositional7,
giving us the ratio of non-compositional expressions
in TrainValD and TestD equal to 0.26 and 0.20, re-
spectively. The Precision/nBest and Recall/nBest
graphs corresponding to the LSA-based model ap-
plied to TestD are depicted in Figure 1. The Preci-
sion/Recall graphs corresponding to the LSA-based
model applied to TrainD and TestD are depicted in
Figure 2.
For comparison, the graphs in Figures 1 and 2
also show the curves corresponding to the evaluation
of Pointwise Mutual Information (PMI).8 The co-
occurrence statistics of the expressions in Disco-En-
Gold was extracted from the window of size three,
sliding through the whole lemmatized ukWaC cor-
pus.
Discussion As suggested in Section 3, we com-
pare the results of the methods using Spearman and
Kendall correlations, AP, and Everts? curves. We
present the results of the LSA and PMI models
alongside the results of the best performing models
participating in the DISCO task. Namely, Table 3
presents the correlation values of our models, the
best performing WSM-based model (Reddy et al,
2011b), the best performing model based upon as-
sociation measures (Chakraborty et al, 2011), and
random baseline models.
The poor results achieved by employing PMI are
similar to the results of random baselines and in ac-
cordance with those of participants of the DISCO
workshop (Chakraborty et al, 2011). We hypoth-
esize that the PMI-based model incorrectly assigns
low values of semantic compositionality (high val-
7Choice of this value can affect the results. The value of 50
was chosen since it is the middle value between the manually
assigned scores ranging from 0 to 100.
8PMI is an association measure used to determine the
strength of association between two or more words based
on their occurrences and co-occurrences in a corpus (Pecina,
2009).
46
Model Dataset ?-All ?-AN ?-VO ?-SV ? -All ? -AN ? -VO ? -SV AP-All
LSA TrainValD 0.47 0.54 0.36 0.57 0.32 0.38 0.24 0.44 0.61
PMI TrainValD 0.02 -0.25 0.29 0.14 0.01 -0.18 0.20 0.10 0.28
baseline TrainValD 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.26
LSA TestD 0.50 0.50 0.56 0.41 0.35 0.36 0.39 0.30 0.53
Reddy-WSM TestD 0.35 - - - 0.24 - - - -
StatMix TestD 0.33 - - - 0.23 - - - -
PMI TestD -0.08 -0.07 0.13 -0.08 -0.06 -0.04 0.08 -0.07 0.21
baseline TestD 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.20
Table 3: The values of AP, Spearman (?) and Kendall (? ) correlations between the LSA-based and PMI-based model
respectively and the Gold data with regards to the expression type. Every zero value in the table corresponds to the
theoretically achieved mean value of correlation calculated from the infinite number of correlation values between the
ranking of scores assigned by the annotators and the rankings of scores being obtained by a random number genarator.
Reddy-WSM stands for the best performing WSM in the DISCO task (Reddy et al, 2011b). StatMix stands for the best
performing system based upon association measures (Chakraborty et al, 2011). Only ?-All and ? -All are available for
the models explored by Reddy et al (2011b) and Chakraborty et al (2011).
ues of PMI) to frequently occurring fixed expres-
sions. For example, we observed that the calculated
values of PMI for ?international airport? and ?reli-
gious belief? were high.
To the contrary, our results achieved by employ-
ing the LSA model are statistically significant and
better than those of all the participants of the DISCO
workshop. However, the data set is probably not
large enough to provide statistically reliable com-
parison of the methods and it is not clear how re-
liable the dataset itself is (the interannotator agree-
ment was not analyzed) and therefore we can not
make any hard conclusions.
5 Conclusion
We analysed the previous works applying WSMs
for determining the semantic compositionality of ex-
pressions. We discussed and summarized the major-
ity of techniques presented in the papers. Our anal-
ysis reveals a large diversity of approaches which
leads to incomparable results (Table 1). Since it has
been shown that WSMs can serve as good predic-
tors of semantic compositionality, we aim to create
a comparative study of the approaches.
Our analysis implies to evaluate the proposed ap-
proaches using human annotated data and evalua-
tion techniques based on ranking. Namely, we sug-
gest using Spearman and Kendall correlations, Pre-
cision/nBest, Recall/nBest, Precision/Recall curves,
and AP.
Using the suggested evaluation techniques, we
present the results of our first experiments exploit-
ing LSA (Figures 1, 2 and Table 3). The results of
the LSA-based model, compared with random base-
lines, PMI-based model, and all the WSM-based and
statistical-based models proposed by the participants
of the DISCO task, are very promising.
Acknowledgments
We thank to V??t Suchomel for providing the
ukWaC corpus and the anonymous reviewers for
their helpful comments and suggestions. The re-
search is supported by Advanced Computing and
Information Systems (grant no. SGS-2013-029)
and by the Czech Science Foundation (grant no.
P103/12/G084). Also, the access to the CERIT-SC
computing facilities provided under the programme
Center CERIT Scientific Cloud, part of the Opera-
tional Program Research and Development for Inno-
vations, reg. no. CZ. 1.05/3.2.00/08.0144 is highly
appreciated.
References
Otavio Costa Acosta, Aline Villavicencio, and Viviane P.
Moreira. 2011. Identification and treatment of multi-
word expressions applied to information retrieval. In
Proceedings of the Workshop on Multiword Expres-
sions: from Parsing and Generation to the Real World,
MWE ?11, pages 101?109, Stroudsburg, PA, USA.
47
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing,
Second Edition. CRC Press, Taylor and Francis Group,
Boca Raton, FL. ISBN 978-1420085921.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. Proceedings
of the ACL 2003 workshop on Multiword expressions
analysis acquisition and treatment, pages 89?96.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 work-
shop on Multiword expressions: analysis, acquisition
and treatment, volume 18 of MWE ?03, pages 65?72,
Stroudsburg, PA, USA.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Journal of Language Resources And
Evaluation, 43(3):209?226.
Michael W. Berry, Susan T. Dumais, and Gavin W.
O?Brien. 1995. Using linear algebra for intelligent
information retrieval. SIAM Rev., 37(4):573?595.
Chris Biemann and Eugenie Giesbrecht. 2011. Distri-
butional semantics and compositionality 2011: shared
task description and results. In Proceedings of the
Workshop on Distributional Semantics and Composi-
tionality, DiSCo ?11, pages 21?28.
Lou Burnard. 2000. User reference guide for the British
National Corpus. Technical report, Oxford University
Computing Services.
Marine Carpuat and Mona Diab. 2010. Task-based eval-
uation of multiword expressions: a pilot study in statis-
tical machine translation. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 242?245, Strouds-
burg, PA, USA.
Tanmoy Chakraborty, Santanu Pal, Tapabrata Mondal,
Tanik Saikh, and Sivaju Bandyopadhyay. 2011.
Shared task system description: Measuring the com-
positionality of bigrams using statistical methodolo-
gies. In Proceedings of the Workshop on Distribu-
tional Semantics and Compositionality, pages 38?42,
Portland, Oregon, USA.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Comput. Linguist., 16(1):22?29.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Stefan Evert. 2005. The statistics of word cooccur-
rences: word pairs and collocations. Ph.D. the-
sis, Universita?t Stuttgart, Holzgartenstr. 16, 70174
Stuttgart.
Afsaneh Fazly. 2007. Automatic Acquisition of Lexical
Knowledge about Multiword Predicates. Ph.D. thesis,
University of Toronto.
Mark Alan Finlayson and Nidhi Kulkarni. 2011. De-
tecting multi-word expressions improves word sense
disambiguation. In Proceedings of the Workshop on
Multiword Expressions: from Parsing and Generation
to the Real World, MWE ?11, pages 20?24, Strouds-
burg, PA, USA.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Anders Johannsen, Hector Martinez Alonso, Christian
Rish?j, and Anders S?gaard. 2011. Shared task sys-
tem description: frustratingly hard compositionality
prediction. In Proceedings of the Workshop on Distri-
butional Semantics and Compositionality, DiSCo ?11,
pages 29?32, Stroudsburg, PA, USA.
David Jurgens and Keith Stevens. 2010. The s-space
package: an open source package for word space mod-
els. In Proceedings of the ACL 2010 System Demon-
strations, ACLDemos ?10, pages 30?35, Stroudsburg,
PA, USA.
Graham Katz. 2006. Automatic identification of
non-compositional multi-word expressions using la-
tent semantic analysis. In In Proceedings of the
ACL/COLING-06 Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Proper-
ties, pages 12?19.
Lubom??r Krc?ma?r?, Karel Jez?ek, and Massimo Poesio.
2012. Detection of semantic compositionality using
semantic spaces. Lecture Notes in Computer Science,
7499 LNAI:353?361.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, ACL ?99,
pages 317?324, Stroudsburg, PA, USA.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28(2):203?
208.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
48
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL 2003 workshop on
Multiword expressions analysis acquisition and treat-
ment, volume 18 of MWE ?03, pages 73?80.
George A. Miller. 1995. WordNet: A lexical database
for English. Communications of the ACM, 38:39?41.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio.
Darren Pearce. 2002. A Comparative Evaluation of
Collocation Extraction Techniques. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation, LREC.
Pavel Pecina. 2009. Lexical Association Measures: Col-
location Extraction, volume 4 of Studies in Compu-
tational and Theoretical Linguistics. U?FAL, Praha,
Czechia.
Carlos Ramisch. 2012. A generic framework for multi-
word expressions treatment: from acquisition to appli-
cations. In Proceedings of ACL 2012 Student Research
Workshop, ACL ?12, pages 61?66, Stroudsburg, PA,
USA.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011a. An empirical study on compositionality in
compound nouns. In Proceedings of 5th International
Joint Conference on Natural Language Processing,
pages 210?218, Chiang Mai, Thailand.
Siva Reddy, Diana McCarthy, Suresh Manandhar, and
Spandana Gella. 2011b. Exemplar-based word-space
model for compositionality detection: Shared task sys-
tem description. In Proceedings of the Workshop on
Distributional Semantics and Compositionality, pages
54?60, Portland, Oregon, USA.
Douglas L. Rohde, Laura M. Gonnerman, and David C.
Plaut. 2005. An improved model of semantic sim-
ilarity based on lexical co-occurrence. Unpublished
manuscript.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for nlp. In Proceedings
of the Third International Conference on Computa-
tional Linguistics and Intelligent Text Processing, CI-
CLing ?02, pages 1?15, London, UK. Springer-Verlag.
Magnus Sahlgren. 2005. An introduction to random in-
dexing. In Methods and Applications of Semantic In-
dexing Workshop at the 7th International Conference
on Terminology and Knowledge Engineering, Leipzig,
Germany.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of the
2001 Conference on Empirical Methods in Natural
Language Processing, pages 100?108.
Violeta Seretan. 2008. Collocation extraction based on
syntactic parsing. Ph.D. thesis, University of Geneva.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
J. Artif. Int. Res., 37(1):141?188.
49
baseline PMI LSA
0 2 5 5 0 7 5 100 125 150 175
nBest
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
1.05
Pre
cisio
n
baseline PMI LSA
0 2 5 5 0 7 5 100 125 150 175
nBest
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
1.05
Rec
all
Figure 1: Smoothed graphs depicting the dependency of Precision (left) and Recall (right) upon the nBest selected
non-compositional candidates from the ordered list of expressions in TestD created by the LSA and PMI-based models.
baseline PMI LSA
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Recall
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
1.05
Pre
cisio
n
baseline PMI LSA
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Recall
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
1.05
Pre
cisio
n
Figure 2: Smoothed graphs depicting the dependency of Precision upon Recall using the LSA and PMI-based models
ordering the expressions in TrainValD (left) and TestD (right) according to their non-compositionality.
50
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 106?115,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Syntactic Identification of Occurrences of Multiword Expressions in Text
using a Lexicon with Dependency Structures
Eduard Bejc?ek, Pavel Stran?a?k, Pavel Pecina
Charles University in Prague, Faculty of Mathematics and Physics,
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, 118 00 Praha 1, Czechia
{bejcek,stranak,pecina}@ufal.mff.cuni.cz
Abstract
We deal with syntactic identification of oc-
currences of multiword expression (MWE)
from an existing dictionary in a text corpus.
The MWEs we identify can be of arbitrary
length and can be interrupted in the surface
sentence. We analyse and compare three ap-
proaches based on linguistic analysis at a vary-
ing level, ranging from surface word order to
deep syntax. The evaluation is conducted us-
ing two corpora: the Prague Dependency Tree-
bank and Czech National Corpus. We use the
dictionary of multiword expressions SemLex,
that was compiled by annotating the Prague
Dependency Treebank and includes deep syn-
tactic dependency trees of all MWEs.
1 Introduction
Multiword expressions (MWEs) exist on the inter-
face of syntax, semantics, and lexicon, yet they are
almost completely absent from major syntactic the-
ories and semantic formalisms. They also have inter-
esting morphological properties and for all these rea-
sons, they are important, but challenging for Natural
Language Processing (NLP). Recent advances show
that taking MWEs into account can improve NLP
tasks such as dependency parsing (Nivre and Nils-
son, 2004; Eryig?it et al, 2011), constituency parsing
(Arun and Keller, 2005), text generation (Hogan et
al., 2007), or machine translation (Carpuat and Diab,
2010).
The Prague Dependency Treebank (PDT) of
Czech and the associated lexicon of MWEs Sem-
Lex1 offer a unique opportunity for experimentation
1http://ufal.mff.cuni.cz/lexemann/mwe/semlex.zip
with MWEs. In this paper, we focus on identifica-
tion of their syntactic structures in the treebank us-
ing various levels of linguistic analysis and match-
ing algorithms.2 We compare approaches operating
on manually and automatically annotated data with
various depth of annotation from two sources: the
Prague Dependency Treebank and Czech National
Corpus (CNC).
The remainder of the paper is organised as fol-
lows. Section 2 describes the state of the art of in
acquisition and identification of MWEs. Section 3
explains what we consider a MWE. In Section 4
we describe the data used for our experiments. Sec-
tion 5 gives the details of our experiments, and in
Section 6 we analyse and discuss the results. Con-
clusions from the analysis are drawn in Section 7.
2 Processing of Multiword Expressions
and Related Work
Automatic processing of multiword expressions in-
cludes two distinct (but interlinked) tasks. Most of
the effort has been put into acquisition of MWEs
appearing in a particular text corpus into a lexi-
con of MWEs (types) not necessarily linked with
their occurrences (instances) in the text. The best-
performing methods are usually based on lexical as-
sociation measures that exploit statistical evidence
of word occurrences and co-occurrences acquired
from a corpus to determine degree of lexical asso-
ciation between words (Pecina, 2005). Expressions
that consist of words with high association are then
2We do not aim at disambiguating the occurrences as figura-
tive or literal. We have not observed enough literal uses to sub-
stantiate working on this step. There are bigger improvements
to be gained from better identification of syntactic occurrences.
106
denoted as MWEs. Most of the current approaches
are limited to bigrams despite the fact that higher-
order MWEs are quite common.
The task of identification of MWE occurrences
expects a list of MWEs as the input and identifies
their occurrences (instances) in a corpus. This may
seem to be a trivial problem. However, the complex
nature of this phenomenon gives rise to problems on
all linguistic levels of analysis: morphology, syntax,
and semantics.
In morphologically complex languages, a single
MWE can appear in a number of morphological
variants, which differ in forms of their individual
components; and at the same time, a sequence of
words whose base forms match with base forms
of components of a given MWE do not neces-
sarily represent an instance of this MWE (Praco-
val dnem i noc?? / He?s been working day and night
vs. Ti dva byli jako den a noc / Those two were as
day and night).
MWEs differ in the level of syntactic fixedness.
On the one hand, certain MWEs can be modified
by inserting words in between their components
or by changing word order. Such expressions can
only be identified by matching their syntactic struc-
tures, but only if a reliable syntactic information is
available in both the lexion and text (Po pr?evratu
padaly hlavy / After the coup, heads were rolling
vs. Hlavy zkorumpovany?ch na?me?stku? budou padat
jedna za druhou / One head of a corrupt deputy
will be rolling after the other). On the other hand,
some MWEs can appear only as fixed expressions
with no modifications allowed. In that case, the syn-
tactic matching approach can miss-indicate their in-
stances because of an inserted word or altered word
order (Vys?s??? spolec?nost / High society vs. *Vys?s??? bo-
hats??? spolec?nost / High rich society).
From the semantic point of view, MWEs are of-
ten characterized by more or less non-compositional
(figurative) meaning. Their components, however,
can also occur with the same syntax but composi-
tional (literal) semantics, and therefore not acting
as MWEs (Jedinou branku dal az? v posledn?? minute?
za?pasu / He scored his only goal in the last minute of
the match. vs. Rozhodc??? dal branku zpe?t na sve? m??sto
/ The referee put a goal back to its place).
Automatic discrimination between figurative and
literal meaning is a challenging task similar to
word sense disambiguation which has been stud-
ied extensively: Katz and Giesbrecht (2006), Cook
et al (2007), Hashimoto and Kawahara (2008), Li
and Sporleder (2009), and Fothergill and Baldwin
(2011). Seretan (2010) includes MWE identification
(based on a lexicon) in a syntactic parser and reports
an improvement of parsing quality. As a by-product,
the parser identified occurrences of MWEs from a
lexicon. Similarly, Green et al (2013) embed identi-
fication of some MWEs in a Tree Substitution Gram-
mar and achieve improvement both in parsing qual-
ity and MWE identification effectiveness. None of
these works, however, attempt to identify all MWEs,
regardless their length or complexity, which is the
main goal of this paper.
3 Definition of Multiword Expressions
We can use the rough definition of MWEs put for-
ward by Sag et al (2002): ?idiosyncratic interpreta-
tions that cross word boundaries (or spaces)?. We
can also start from their ? or Bauer?s (1983) ? ba-
sic classification of MWEs as lexicalised or insti-
tutionalised phrases, where lexicalised phrases in-
clude some syntactic, semantic or lexical (i.e. word
form) element, that is idiosyncratic. Institutionalised
phrases are syntactically and semantically compo-
sitional, but still require a particular lexical choice,
e.g. disallowing synonyms (mobile phone, but not
*movable phone).
We need to make just one small adjustment to the
above: ?phrase? above must be understood as a sub-
tree, i.e. it can have holes in the surface sentence, but
not in terms of a dependency tree.
In reality there is no clear boundary, in particu-
lar between the institutional phrases and other collo-
cations. Like many other traditional linguistic cate-
gories, cf. Manning (2003), this phenomenon seems
to be more continuous than categorial.
For the purpose of this paper, however, it is not
important at all. We simply try to find all instances
of the expressions (subtrees) from a lexicon in a text,
whatever form the expression may take in a sen-
tence.
4 Data
In this work we use two datasets: Czech National
Corpus (CNC), version SYN2006-PUB, and the
107
Prague Dependency Treebank (PDT), version 2.5.
We run and compare results of our experiments on
both manual annotation of PDT, and automatic anal-
ysis of both PDT and CNC (see Section 5.3). We
also make use of SemLex, a lexicon of MWEs in
the PDT featuring their dependency structures that
is described in Section 4.3.
4.1 Corpora ? Czech National Corpus and
Prague Dependency Treebank
CNC is a large3 corpus of Czech. Its released ver-
sions are automatically segmented and they contain
automatic morphological tagging (Hajic?, 2004).
PDT (Bejc?ek et al, 2011) is a smaller news-
domain corpus based on a subset of the news section
of CNC. It contains approx. 0.8 million words that
have three layers of annotation: morphological, ana-
lytical (surface syntax), and tectogrammatical (deep
syntax).
Annotation of a sentence on the morphological
layer consists of attaching morphological lemma
and tag to the tokens. A sentence at the analytical
layer is represented as a rooted ordered tree with la-
belled nodes. The dependency relation between two
nodes is captured by an edge with a functional label.
On the tectogrammatical layer only content words
form nodes in a tree (t-nodes).4 Auxiliary words are
represented by various attributes of t-nodes, as they
do not have their own lexical meaning, but rather
modify the meaning of the content words. Each t-
node has a t-lemma: an attribute whose value is the
node?s basic lexical form, and a dependency func-
tion that relates it to its parent. Figure 1 shows the
relations between the neighbouring layers of PDT.
4.2 MWE in Prague Dependency Treebank 2.5
In the Functional Generative Description (Sgall et
al., 1986, FGD)5 the tectogrammatical layer is con-
strued as a layer of the linguistic meaning of text.
This meaning is composed by means of ?deep?
(tecto-grammatical) syntax from single-meaning-
carrying units: monosemic lexemes.
3It contains 200 mil. words in SYN2000, 600 mil. in
SYN2006-PUB; http://www.korpus.cz.
4with a few exceptions (personal pronouns or coord. heads)
5FGD is a framework for systematic description of a lan-
guage, that the PDT project is based upon.
Byl         by        ?el       do     lesa       .
Byl          by                   do                 lesa
?el
AuxV          AuxV                     AuxP                   
  b?t             b?t            j?t            do        les             .  VpYS---XR-AA---      Vc-------------       VpYS---XR-AA---    RR?2----------    NNIS2-----A----        Z:------------- 
Adv
Pred
AuxS
j?tPRED
#PersPronACT  lesDIR3a
tree.r
f
a/aux.
rf
a/aux
.rf
a/lex.
rf
a/aux
.rf
a/lex.
rf
t - lay
er
a - lay
e r
m - la
yer
   .AuxK
Figure 1: A visualisation of the annotation schema of
PDT. Lit.: ?[He] would have gone into forest.?
In order to better facilitate this concept of t-layer,
all multiword expressions in the release of PDT 2.5
(Bejc?ek et al, 2011) have been annotated and they
are by default displayed as single units, although
their inner structure is still retained.
A lexicon of the MWEs has been compiled. A
simple view of the result of this annotation is given
in the Figure 2. A detailed description can be found
in Bejc?ek and Stran?a?k (2010), and Stran?a?k (2010).
The MWEs in PDT 2.5 include both multiword lex-
emes (phrasemes, idioms) and named entities (NEs).
In the present work we ignore the named entities,
concentrating on the lexemes. Some NEs (names of
persons, geographical entities) share characteristics
of multiword lexemes, other NEs do not (addresses,
bibliographic information).
We build on the PDT 2.5 data and MWE lexicon
SemLex (Section 4.3) to evaluate the approach with
various automatic methods for detection of MWEs.
4.3 Lexicon of MWEs ? SemLex
SemLex is the lexicon of all the MWEs annotators
identified during the preparation of PDT 2.5 t-layer.
In the PDT 2.5 these instances of MWEs can then be
displayed as single nodes and all the MWEs them-
selves are compiled in the SemLex lexicon. The lex-
icon itself is freely available. See http://ufal.
mff.cuni.cz/lexemann/mwe/. Length (size)
108
Can word sense disambiguation help statistical machine translation?
help
disambiguation
sense
word
translation
machine
statistical
#root
help
WSD MT
#root
statistical
Word sense disambiguation
Machine translation ?
BASIC_FORM: Word sense disambiguation
TREE_STRUCT: disambiguation?sense?word
LEMMATIZED: ?
?
SemLex
Figure 2: An illustration of changes in t-trees in PDT 2.5;
every MWE forms a single node and has its lexicon entry
distribution of MWEs in PDT 2.5 is given in Table 1.
There are three attributes of SemLex entries cru-
cial for our task:
BASIC FORM ? The basic form of a MWE. In
many languages including Czech it often contains
word forms in other than the basic form for the given
word on its own. E.g. ?vysoke? uc?en??? contains a
neuter suffix of the adjective ?vysoky?? (high) be-
cause of the required agreement in gender with the
noun, whereas the traditional lemma of adjectives in
Czech is in the masculine form.
LEMMATIZED ? ?Lemmatised BASIC FORM?,
i.e. take the basic form of an entry and substitute
each form with its morphological lemma. This at-
tribute is used for the identification of MWEs on the
morphological layer. For more details see Section 5.
TREE STRUCT (TS) ? A simplified tectogram-
matical dependency tree structure of an entry. Each
node in this tree structure has only two attributes: its
tectogrammatical lemma, and a reference to its ef-
fective parent.
4.4 Enhancing SemLex for the Experiments
SemLex contains all the information we use for the
identification of MWEs on t-layer.6 It also contains
basic information we use for MWE identification on
m-layer: the basic form and the lemmatized form of
each entry. For the experiments with MWE iden-
tification on analytical (surface syntactic) layer we
6Automatic identification of MWES was, after all, one of
the reasons for its construction.
a) len types instances
2 7063 18914
3 1260 2449
4 305 448
5 100 141
6 42 42
7 16 15
8 4 5
9 4 3
11 1 0
12 2 2
b) len types instances
18 148 534
2 7444 19490
3 843 1407
4 162 244
5 34 32
6 13 8
7 3 1
8 4 1
9 1 1
10 0 0
Table 1: Distribution of MWE length in terms of words (a)
and t-nodes (b) in SemLex (types) and PDT (instances).
need to add some information about the surface syn-
tactic structures of MWEs. Given the annotated oc-
currences of MWEs in the t-layer and links from
t-layer to a-layer, the extraction is straightforward.
Since one tectogrammatical TS can correspond to
several analytical TSs that contain auxiliaries and
use morphological lemmas, we add a list of a-layer
TSs with their frequency in data to each SemLex en-
try (MWE). In reality the difference between t-layer
and a-layer is unfortunately not as big as one could
expect. Lemmas of t-nodes still often include even
minute morphological variants, which goes against
the vision of tectogrammatics, as described in Sgall
et al (1986).7 Our methods would benefit from more
unified t-lemmas, see also Section 6.2.
5 Methodology of Experiments
SemLex ? with its almost 8,000 types of MWEs and
their 22,000 instances identified in PDT ? allows us
to measure accuracy of MWE identification on vari-
ous layers, since it is linked with the different layers
of PDT 2.5. In this section, we present the method
for identification of MWEs on t-layer in compari-
son with identification on a-layer and m-layer. The
7These variants are unified in FGD theory, but time consum-
ing to annotate in practice. Therefore, this aspect was left out
from the current version of PDT.
8Indeed, there are expressions that are multiword, but
?single-node?. E.g.: the preposition in bez va?ha?n?? (without hes-
itation) does not have its own node on t-layer; the phrase na
spra?vnou m??ru (lit.: into correct scale) is already annotated as
one phrasal node in PDT with the lemma ?na spra?vnou m??ru?;
the verbal expression ume?t si pr?edstavit (can imagine) has again
only one node for reflexive verb ?pr?edstavit si? plus an attribute
for the ability (representing ?ume?t? as explained in Section 4.1).
109
idea of using tectogrammatical TS for identification
is that with a proper tectogrammatical layer (as it
is proposed in FGD, i.e. with correct lemmatisation,
added nodes in place of ellipses, etc.), this approach
should have the highest Precision.
Our approach to identification of MWEs in this
work is purely syntactic. We simply try to find
MWEs from a lexicon in any form they may take
(including partial ellipses in coordination, etc.). We
do not try to exploit semantics, instead we want to
put a solid baseline for future work which may do
so, as mentioned in Section 2.
5.1 MWE Identification on t-layer
We assume that each occurrence of a given MWE
has the same t-lemmas and the same t-layer struc-
ture anywhere in the text. During the manual con-
struction of SemLex, these tectogrammatical ?tree
structures? (TSs) were extracted from PDT 2.5 and
inserted into the lexicon. In general this approach
works fine and for majority of MWEs only one TS
was obtained. For the MWEs with more than one TS
in data we used the most frequent one. These cases
are due to some problems of t-layer, not deficiencies
of the theoretical approach. See section 6.2 for the
discussion of the problems.
These TSs are taken one by one and we try to find
them in the tectogrammatical structures of the input
sentences. Input files are processed in parallel. The
criteria for matching are so far only t-lemmas and
topology of the subtree.9 Comparison of tree struc-
tures is done from the deepest node and we consider
only perfect matches of structure and t-lemmata.
5.2 MWE Identification on a-layer and m-layer
We use identification of MWE occurrences on a-
layer and m-layer mainly for comparison with our
approach based on the t-layer.
9It is not sufficient, though. Auxiliary words that are ig-
nored on t-layer are occasionally necessary for distinguishing
MWE from similar group of nodes. (E.g. ?v tomto sme?ru? (?in
this regard?) is an MWE whereas ?o tomto sme?ru? (?about
this direction?) is not.) There are also attributes in t-layer that
are?although rarely?important for distinguishing the mean-
ing. (E.g. words typeset in bold in ?Leonardo dal svy?m go?lem
signa?l.? (?Leonardo signalled by his goal.?) compose exactly
the same structure as in ?Leonardo dal go?l.? (?Leonardo scored
a goal.?). I.e., the dependency relation is ?dal governs go?l? in
both cases. The difference is in the dependency function of go?l:
it is either MEANS or DIRECT OBJECT (CPHR).)
We enhance SemLex with a-tree structures as ex-
plained in Section 4.4, and then a-layer is processed
in the same manner as t-layer: analytical TS is taken
from the SemLex and the algorithm tries to match it
to all a-trees. Again, if more than one TS is offered
in lexicon, only the most frequent one is used for
searching.
MWE identification on the m-layer is based on
matching lemmas (which is the only morphological
information we use). The process is parametrised
by a width of a window which restricts the maxi-
mum distance (in a sentence) of MWE components
to span (irrespective of their order) measured in the
surface word order. However, in the setting which
does not miss any MWE in a sentence (100% Re-
call), this parameter is set to the whole sentence and
the maximum distance is not restricted at all.
The algorithm processes each sentence at a time,
and tries to find all lemmas the MWE consists of,
running in a cycle over all MWEs in SemLex. This
method naturally over-generates ? it correctly finds
all MWEs that have all their words present in the sur-
face sentence with correct lemmatisation (high Re-
call), but it also marks words as parts of some MWE
even if they appear at the opposite ends of the sen-
tence by complete coincidence (false positives, low
Precision).
In other experiments, the window width varies
from two to ten and MWE is searched for within a
limited context.
5.3 Automatic Analysis of Data Sets
The three MWE identification methods are applied
on three corpora:
? manually annotated PDT: This is the same
data, from which the lexicon was created. Results
evaluated on the same data can be seen only as num-
bers representing the maximum that can be obtained.
? automatically annotated PDT: These are the
same texts (PDT), but their analysis (morphological,
analytical as well as tectogrammatical) started from
scratch. Results can be still biased ? first, there are
no new lexemes that did not appear during annota-
tion (that is as if we had a complete lexicon); second,
it should be evaluated only on eval part of the data ?
see discussion in Section 6.1.
? automatically annotated CNC: Automatic
analysis from scratch on different sentences. The
110
layer/span PDT/man PDT/auto CNC/auto
tecto 61.99 / 95.95 / 75.32 63.40 / 86.32 / 73.11 44.44 / 58.00 / 50.33
analytical 66.11 / 88.67 / 75.75 66.09 / 81.96 / 73.18 45.22 / 60.00 / 51.58
morpho / 2 67.76 / 79.96 / 73.36 67.77 / 79.26 / 73.07 51.85 / 56.00 / 53.85
3 62.65 / 90.50 / 74.05 62.73 / 89.80 / 73.86 46.99 / 60.00 / 52.70
4 58.84 / 92.03 / 71.78 58.97 / 91.29 / 71.65 42.83 / 61.33 / 50.48
5 56.46 / 92.94 / 70.25 56.59 / 92.16 / 70.12 40.09 / 61.33 / 48.49
6 54.40 / 93.29 / 68.81 54.64 / 92.51 / 68.70 38.27 / 61.33 / 47.13
7 52.85 / 93.42 / 67.51 53.01 / 92.64 / 67.43 36.99 / 61.33 / 46.15
8 51.39 / 93.46 / 66.32 51.57 / 92.68 / 66.27 35.59 / 61.33 / 45.04
9 50.00 / 93.46 / 65.15 50.18 / 92.68 / 65.11 34.67 / 61.33 / 44.30
10 48.57 / 93.46 / 63.92 48.71 / 92.68 / 63.86 33.84 / 61.33 / 43.64
? 35.12 / 93.51 / 51.06 35.16 / 92.72 / 50.99 22.70 / 62.00 / 33.24
P / R / F P / R / F P / R / F
Table 2: Evaluation of all our experiments in terms of Precision (P), Recall (R) and F1 score (F) in percent. Experiments
on the m-layer are shown for different widths of window (see Section 5.2).
disadvantage here is the absence of gold data. Man-
ual evaluation of results has to be accomplished.
For the automatic analysis we use the modular
NLP workflow system Treex (Popel and Z?abokrtsky?,
2010). Both datasets were analysed by the standard
Treex scenario ?Analysis of Czech? that includes the
following major blocks:
1) standard rule-based Treex segmentation and to-
kenisation
2) morphology (Hajic?, 2004) and Featurama tag-
ger (Spousta, 2011) trained on the train part of
the PDT
3) MST Parser with an improved set of features by
Nova?k and Z?abokrtsky? (2007)
4) and t-trees structure provided by standard rule-
based Treex block.
6 Results
Effectiveness of our methods of identification of
MWE occurrences is presented in Table 2. Numbers
are given as percentages of Precision and Recall The
first two columns show the results of the evaluation
against gold data in PDT 2.5, the third column re-
flects the manual evaluation on 546 sentences. The
results obtained for PDT (the first two columns) are
also visualised in Figure 3.
The important issue to be decided when evaluat-
ing MWE identification is whether partial match be-
tween automatic identification and gold data MWE
is to be counted. Because of cases containing el-
lipses (see Section 6.2), it can happen that longer
MWE is used for annotation of its subset in text.10
We do not want to penalise automatic identification
(either performing this behaviour or confronted with
it in the gold data), so we treated subset as a match.
Another decision is that although the MWEs can-
not be nested in gold data, we accept it for automatic
identification. Since one word can belong to several
MWEs, the Recall rises, while Precision declines.11
6.1 Discussion of Results
The automatically parsed part of the CNC consists
of 546 sentences. Thus the third column in Table 2
represents evaluation on a much smaller data set.
During manual annotation of this data carried out
by one annotator (different from those who anno-
tated PDT data, but using the same methodology and
a tool), 163 occurences of MWEs were found. Out
10Let us say, only elliptic term Ministry of Industry is seen
in the data (instead of the full name Ministry of Industry and
Trade) annotated by the full-term lexicon entry. Whenever Min-
istry of Industry and Trade is spotted in the test data, its first
part is identified. Should that be qualified as a mistake when
confronted with the gold annotation of the whole term? The as-
signed lexicon entry is the same ? only the extent is different.
11For example, annotator had to choose only one MWE to an-
notate in vla?dn?? na?vrh za?kona o dani z pr???jmu (lit.: government
proposal of the Law on Income Tax), while it is allowed to auto-
matically identify vla?dn?? na?vrh za?kona, za?kon o dani and dan? z
pr???jmu together with the whole phrase. Recall for this example
is 1, whereas Precision is 0.25.
111
 Str?nka 1
78 80 82 84 86 88 90 92 94 96 98
30
35
40
45
50
55
60
65
70
m-layer a-layer t-layer
Recall
Pre
cis
ion
 
Str?nka 1
78 80 82 84 86 88 90 92 94 96 98
30
35
40
45
50
55
60
65
70
m-layer a-layer t-layer
Recall
Pre
cis
ion
Figure 3: Precision?Recall scores of identification of MWE structures on manually/automatically annotated PDT.
of them, 46 MWEs were out-of-vocabulary expres-
sions: they could not be found by automatic prece-
dure using the original SemLex lexicon.
Note that results obtained using automatically
parsed PDT are very close to those for manual data
on all layers (see Table 2). The reasons need to be
analysed in more detail. Our hypotheses are:
? M-layer identification reaches the same results
on both data. It is caused by the fact that the ac-
curacy of morphological tagging is comparable to
manual morphological annotation: 95.68% (Spous-
tova?, 2008).
? Both a- and t-parsers have problems mostly in
complex constructions such as coordinations, that
very rarely appear inside MWEs.
There are generally two issues that hurt our accu-
racy and that we want to improve to get better re-
sults. First, better data can help. Second, the method
can always be improved. In our case, all data are
annotated?we do nothing on plain text?and it can
be expected that with a better parser, but also possi-
bly a better manual annotation we can do better, too.
The room for improvement is bigger as we go deeper
into the syntax: data are not perfect on the a-layer
(both automatically parsed and gold data) and on
the significantly more complex t-layer it gets even
worse. By contrast, the complexity of methods and
therefore possible improvements go in the opposite
direction. The complexity of tectogrammatic anno-
tation results in a tree with rich, complex attributes
of t-nodes, but simple topology and generalised lem-
mas. Since we only use tree topology and lemmas,
the t-layer method can be really simple. It is slightly
more complex on the a-layer (with auxiliary nodes,
for example); and finally on the m-layer there is vir-
tually unlimited space for experiments and a lot of
literature on that problem. As we can see, these two
issues (improving data and improving the method)
complement each other with changing ratio on indi-
vidual layers.
It is not quite clear from Table 2 that MWE iden-
tification should be done on the t-layer, because it is
currently far from our ideal. It is also not clear that it
should be done on the m-layer, because it seems that
the syntax is necessary for this task.
6.2 Error Analysis and Possible Improvements
There are several reasons, why the t-layer results are
not clearly better:
1. our representation of tree structures proved a
bit too simple,
2. there are some deficiencies in the current t-
layer parser, and
3. t-layer in PDT has some limitations relative to
the ideal tectogrammatical layer.
Ad 1. We thought the current SemLex implemen-
tation of simple tree structures would be sufficient
for our purpose, but it is clear now that it is too
simple and results in ambiguities. At least auxiliary
words and some further syntactico-semantic infor-
mation (such as tectogrammatical functions) should
be added to all nodes in these TSs.
Ad 2. Current tectogrammatical parser does not
do several things we would like to use. E.g. it cannot
112
properly generate t-nodes for elided parts of coordi-
nated MWEs that we need in order to have the same
TS of all MWE occurrences (see below).
Ad 3. The total of 771 out of 8,816 SemLex en-
tries, i.e. 8.75%, have been used with more than one
tectogrammatical tree structure in the PDT 2.5. That
argues against our hypothesis (stated in Section 5.1)
and cause false negatives in the output, since we cur-
rently search for only one TS. In this part we analyze
two of the most important sources of these inconsis-
tent t-trees and possible improvements:
? Gender opposites, diminutives and lemma vari-
ations. These are currently represented by variations
of t-lemma. We believe that they should rather be
represented by attributes of t-nodes that could be
roughly equivalent to some of the lexical functions
in the Meaning-text theory (see Mel?c?uk (1996)).
This should be tackled in some future version of
PDT. Once resolved it would allow us to identify
following (and many similar) cases automatically.
? obchodn?? r?editel vs. obchodn?? r?editelka
(lit.: managing director-man vs. managing
director-woman)
? rodinny? du?m vs. rodinny? domek
(lit.: family house vs. family little-house; but
the diminutive domek does not indicate that the
house is small)
? obc?ansky? za?kon vs. obc?ansky? za?kon??k
(lit.: citizen law vs. citizen law-codex, meaning
the same thing in modern Czech)
These cases were annotated as instances of the same
MWE, with a vision of future t-lemmas disregard-
ing this variation. Until that happens, however, we
cannot identify the MWEs with these variations au-
tomatically using the most frequent TS only.
? Elided parts of MWEs in coordinations. Al-
though t-layer contains many newly established t-
nodes in place of elided words, not all t-nodes
needed for easy MWE annotation were there. This
decision resulted in the situation, when some MWEs
in coordinations cannot be correctly annotated, esp.
in case of coordination of several multiword lexemes
like inz?eny?rska?, monta?z?n?? a stavebn?? spolec?nost (en-
gineering, assembling and building company), there
is only one t-node for company. Thus the MWE
inz?eny?rska? spolec?nost / engineering company is not
in PDT 2.5 data and cannot be found by the t-layer
identification method. It can, however, be found by
the m-layer surface method, provided the window is
large enough and MWEs can overlap.
7 Conclusions
Identification of occurrences of multiword expres-
sions in text has not been extensively studied yet
although it is very important for a lot of NLP ap-
plications. Our lexicon SemLex is a unique resource
with almost 9 thousand MWEs, each of them with
a tree-structure extracted from data. We use this re-
source to evaluate methods for automatic identifica-
tion of MWE occurrences in text based on matching
syntactic tree structures (tectogrammatical ? deep-
syntactic, and analytical ? surface-syntactic trees)
and sequences of lemmas in the surface sentence.
The theoretically ideal approach based on tec-
togrammatical layer turned out not to perform bet-
ter, mainly due to the imperfectness of the t-layer
implemented in PDT and also due to the low ac-
curacy of automatic tectogrammatical parser. It still
shows very high Recall, as expected ? due to sim-
ple topology of the trees ? however Precision is not
ideal. Morphology-based MWE identification guar-
antees high Recall (especially when no limits are put
on the MWE component distance) but Precision of
this approach is rather low. On the other hand, if the
maximum distance is set to 4?5 words we get a very
interesting trade-off between Precision and Recall.
Using analytical layer (and thus introducing surface
syntax to the solution) might be a good approach for
many applications, too. It provides high Precision as
well as reasonable Recall.
Acknowledgements
This research was supported by the Czech Sci-
ence Foundation (grant n. P103/12/G084 and
P406/2010/0875). This work has been using lan-
guage resources developed and/or stored and/or dis-
tributed by the LINDAT-Clarin project of the Min-
istry of Education of the Czech Republic (project
LM2010013). We want to thank to our colleagues
Michal Nova?k, Martin Popel and Ondr?ej Dus?ek for
providing the automatic annotation of the PDT and
CNC data.
113
References
Abhishek Arun and Frank Keller. 2005. Lexicaliza-
tion in crosslinguistic probabilistic parsing: The case
of French. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 306?313, Ann Arbor, Michigan.
Laurie Bauer. 1983. English Word-formation. Cam-
bridge Textbooks in Linguistics. Cambridge Univer-
sity Press.
Eduard Bejc?ek and Pavel Stran?a?k. 2010. Annotation of
multiword expressions in the Prague dependency tree-
bank. Language Resources and Evaluation, (44):7?
21.
Eduard Bejc?ek, Jarmila Panevova?, Jan Popelka, Lenka
Smejkalova?, Pavel Stran?a?k, Magda S?evc???kova?, Jan
S?te?pa?nek, Josef Toman, Zdene?k Z?abokrtsky?, and
Jan Hajic?. 2011. Prague dependency tree-
bank 2.5. http://hdl.handle.net/11858/
00-097C-0000-0006-DB11-8. Data.
Marine Carpuat and Mona Diab. 2010. Task-based eval-
uation of multiword expressions: a pilot study in statis-
tical machine translation. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 242?245, Strouds-
burg, PA, USA.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: exploiting syntactic forms
for the automatic identification of idiomatic expres-
sions in context. In Proceedings of the Workshop on a
Broader Perspective on Multiword Expressions, MWE
?07, pages 41?48.
Gu?ls?en Eryig?it, Tugay I?lbay, and Ozan Arkan Can. 2011.
Multiword expressions in statistical dependency pars-
ing. In Proceedings of the Second Workshop on Sta-
tistical Parsing of Morphologically Rich Languages,
SPMRL ?11, pages 45?55, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Fothergill and Timothy Baldwin. 2011. Flesh-
ing it out: A supervised approach to MWE-token and
MWE-type classification. In Proceedings of 5th Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 911?919, Chiang Mai, Thailand.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2013. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
Jan Hajic?. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stv?? Karolinum.
Chikara Hashimoto and Daisuke Kawahara. 2008. Con-
struction of an idiom corpus and its application to id-
iom identification based on WSD incorporating idiom-
specific features. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 992?1001.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units in
history-based probabilistic generation. In EMNLP-
CoNLL, pages 267?276. ACL.
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
?06, pages 12?19.
Linlin Li and Caroline Sporleder. 2009. Classifier com-
bination for contextual idiom detection without la-
belled data. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 315?323.
Christopher D. Manning, 2003. Probabilistic Linguistics,
chapter Probabilistic Syntax, pages 289?341. MIT
Press, Cambridge, MA.
Igor Mel?c?uk. 1996. Lexical functions: A tool for the
description of lexical relations in a lexicon. In Leo
Wanner, editor, Lexical Functions in Lexicography and
Natural Language Processing, volume 31 of Studies
in Language Companion Series, pages 37?102. John
Benjamins.
Joachim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Dias, G., Lopes, J. G. P. and
Vintar, S. (eds.) MEMURA 2004 - Methodologies and
Evaluation of Multiword Units in Real-World Applica-
tions, Workshop at LREC 2004, pages 39?46, Lisbon,
Portugal.
Va?clav Nova?k and Zdene?k Z?abokrtsky?. 2007. Fea-
ture engineering in maximum spanning tree depen-
dency parser. In Va?clav Matous?ek and Pavel Mautner,
editors, Lecture Notes in Artificial Intelligence, Pro-
ceedings of the 10th International Conference on Text,
Speech and Dialogue, volume 4629 of Lecture Notes
in Computer Science, pages 92?98, Berlin / Heidel-
berg. Springer.
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings of the
ACL Student Research Workshop, pages 13?18, Ann
Arbor, Michigan.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
Modular NLP framework. In Hrafn Loftsson, Eirikur
Ro?gnvaldsson, and Sigrun Helgadottir, editors, Lec-
ture Notes in Artificial Intelligence, Proceedings of the
7th International Conference on Advances in Natural
Language Processing (IceTAL 2010), volume 6233 of
LNCS, pages 293?304, Berlin / Heidelberg. Iceland
Centre for Language Technology (ICLT), Springer.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
114
expressions: A pain in the neck for NLP. In Com-
putational Linguistics and Intelligent Text Process-
ing: Third International Conference, CICLing, vol-
ume 2276/2002 of Lecture Notes in Computer Science.
Springer Berlin / Heidelberg.
Violeta Seretan. 2010. Syntax-Based Collocation Ex-
traction, volume 44 of Text, Speech and Language
Technology. Springer.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. Academia/Reidel Publ. Comp.,
Praha/Dordrecht.
Miroslav Spousta. 2011. Featurama. http://
sourceforge.net/projects/featurama/.
Software.
Drahom??ra ?johanka? Spoustova?. 2008. Combining sta-
tistical and rule-based approaches to morphological
tagging of Czech texts. The Prague Bulletin of Math-
ematical Linguistics, 89:23?40.
Pavel Stran?a?k. 2010. Annotation of Multiword Expres-
sions in The Prague Dependency Treebank. Ph.D. the-
sis, Charles University in Prague.
115
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 64?73,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Determining Compositionality of Word Expressions Using
Various Word Space Models and Measures
Lubom??r Krc?ma?r?1,2
1University of West Bohemia,
Faculty of Applied Sciences,
NTIS ? New Technologies
for the Information Society,
Pilsen, Czech Republic
lkrcmar@kiv.zcu.cz
Karel Jez?ek2
2University of West Bohemia,
Faculty of Applied Sciences,
Department of Computer
Science and Engineering,
Pilsen, Czech Republic
jezek ka@kiv.zcu.cz
Pavel Pecina3
3Charles University in Prague,
Faculty of Mathematics and
Physics, Institute of Formal
and Applied Linguistics,
Prague, Czech Republic
pecina@ufal.mff.cuni.cz
Abstract
This paper presents a comparative study
of 5 different types of Word Space Mod-
els (WSMs) combined with 4 different
compositionality measures applied to the
task of automatically determining seman-
tic compositionality of word expressions.
Many combinations of WSMs and mea-
sures have never been applied to the task
before.
The study follows Biemann and Gies-
brecht (2011) who attempted to find a list
of expressions for which the composition-
ality assumption ? the meaning of an ex-
pression is determined by the meaning of
its constituents and their combination ?
does not hold. Our results are very promis-
ing and can be appreciated by those inter-
ested in WSMs, compositionality, and/or
relevant evaluation methods.
1 Introduction
Our understanding of WSM is in agreement with
Sahlgren (2006): ?The word space model is a
computational model of word meaning that uti-
lizes the distributional patterns of words collected
over large text data to represent semantic similar-
ity between words in terms of spatial proximity?.
There are many types of WSMs built by different
algorithms. WSMs are based on the Harris distri-
butional hypothesis (Harris, 1954), which assumes
that words are similar to the extent to which they
share similar linguistic contexts. WSM can be
viewed as a set of words associated with vectors
representing contexts in which the words occur.
Then, similar vectors imply (semantic) similarity
of the words and vice versa. Consequently, WSMs
provide a means to find words semantically simi-
lar to a given word. This capability of WSMs is
exploited by many Natural Language Processing
(NLP) applications as listed e.g. by Turney and
Pantel (2010).
This study follows Biemann and Giesbrecht
(2011), who attempted to find a list of non-
compositional expressions whose meaning is not
fully determined by the meaning of its con-
stituents and their combination. The task turned
out to be frustratingly hard (Johannsen et al,
2011). Biemann?s idea and motivation is that non-
compositional expressions could be treated as sin-
gle units in many NLP applications such as In-
formation Retrieval (Acosta et al, 2011) or Ma-
chine Translation (Carpuat and Diab, 2010). We
extend this motivation by stating that WSMs could
also benefit from a set of non-compositional ex-
pressions. Specifically, WSMs could treat se-
mantically non-compositional expressions as sin-
gle units. As an example, consider ?kick the
bucket?, ?hot dog?, or ?zebra crossing?. Treat-
ing such expressions as single units might improve
the quality of WSMs since the neighboring words
of these expressions should not be related to their
constituents (?kick?, ?bucket?, ?dog? or ?zebra?),
but instead to the whole expressions.
Recent works, including that of Lin (1999),
Baldwin et al (2003), Biemann and Giesbrecht
(2011), Johannsen et al (2011), Reddy et al
(2011a), Krc?ma?r? et al (2012), and Krc?ma?r? et al
(2013), show the applicability of WSMs in deter-
mining the compositionality of word expressions.
The proposed methods exploit various types of
WSMs combined with various measures for de-
termining the compositionality applied to various
datasets. First, this leads to non-directly compa-
rable results and second, many combinations of
64
WSMs and measures have never before been ap-
plied to the task. The main contribution and nov-
elty of our study lies in systematic research of
several basic and also advanced WSMs combined
with all the so far, to the best of our knowledge,
proposed WSM-based measures for determining
the semantic compositionality.
The explored WSMs, described in more detail
in Section 2, include the Vector Space Model,
Latent Semantic Analysis, Hyperspace Analogue
to Language, Correlated Occurrence Analogue to
Lexical Semantics, and Random Indexing. The
measures, including substitutability, endocentric-
ity, compositionality, and neighbors-in-common-
based, are described in detail in Section 3. Sec-
tion 4 describes our experiments performed on
the manually annotated datasets ? Distributional
Semantics and Compositionality dataset (DISCO)
and the dataset built by Reddy et al (2011a). Sec-
tion 5 summarizes the results and Section 6 con-
cludes the paper.
2 Word Space Models
The simplest and oldest types of WSMs1 are the
Vector Space Model (VSM) and Hyperspace Ana-
logue to Language (HAL). More recent and ad-
vanced models include Latent Semantic Analy-
sis (LSA), which is based on VSM, and Corre-
lated Occurrence Analogue to Lexical Semantics
(COALS), which originates from HAL. Random
Indexing (RI) is WSM joining the principles of
LSA and HAL. Many other WSMs have been pro-
posed too. Their description is outside the scope
of this paper and can be found e.g. in Turney and
Pantel (2010) or Jurgens and Stevens (2010).
VSM is based on the assumption that similar (re-
lated) words tend to occur in the same documents.2
VSM stores occurrence counts of all word types
in documents a given corpus in a co-occurrence
matrix C. The row vectors of the matrix corre-
spond to the word types and the columns to the
documents in the corpus. The numbers of occur-
rences cij in C are usually weighted by the prod-
uct of the local and global weighting functions
(Nakov et al, 2001). The local function weights
cij by the same mathematical function; typically
none (further denoted as no), log(cij + 1) (de-
1WSMs are also referred to as distributional models of
semantics, vector space models, or semantic spaces.
2VSM was originally developed for the SMART informa-
tion retrieval system (Salton, 1971).
noted as log) or
?
cij (denoted as sqrt). The
purpose of local weighting is to lower the im-
portance of highly occurring words in the docu-
ment. The global function weights every value
in row i of C by the same value calculated for
row i. Typically: none (denoted as No), In-
verse Document Frequency (denoted as Idf ) or
a function referred to as Entropy (Ent). Idf
is calculated as 1 + log(ndocs/df(i)) and Ent
as 1 + {
?
j p(i, j) log p(i, j)}/ log ndocs, where
ndocs is the number of documents in the corpora,
df(i) is the number of documents containing word
type i, and p(i, j) is the probability of occurrence
of word type i in document j.
LSA builds on VSM and was introduced by
Landauer and Dumais (1997). The LSA algo-
rithm works with the same co-occurrence matrix
C which can be weighted in the same manner as
in VSM. The matrix is than transformed by Sin-
gular Value Decomposition (SVD) (Deerwester et
al., 1990) into C. The purpose of SVD is to
project the row vectors and column vectors of C
into a lower-dimensional space and thus bring the
vectors of word types and vectors of documents,
respectively, with similar meanings near to each
other.3 The output number of dimensions is a pa-
rameter of SVD and typically ranges from 200 to
1000 (Landauer and Dumais, 1997; Rohde et al,
2005).
HAL was first explored by Lund and Burgess
(1996). It differs from VSM and LSA in that it
only exploits neighboring words as contexts for
word types. HAL processes the corpus by moving
a sliding double-sided window with a size rang-
ing from 1 to 5 around the word type in focus
and accumulating the weighted co-occurrences of
the preceding and following words into a matrix.
Typically, the linear weighting function is used
to ensure that the occurrences of words which
are closer to the word type in focus are more
significant. The dimensions of the resulting co-
occurrence matrix are of size |V | and 2|V |, where
V denotes the vocabulary consisting of all the
word types occurring in the processed corpora. Fi-
nally, the HAL co-occurrence matrix can be re-
duced by retaining the most informative columns
only. The columns with the highest values of en-
tropy (?
?
j pj log pj , where pj denotes the prob-
3In this way, LSA is able to capture higher-order co-
occurrences.
65
ability of a word in the investigated column j) can
be considered as the most informative. The alter-
natives and their description can be found e.g. in
Song et al (2004).
COALS was introduced by Rohde et al (2005).
Compared to HAL, COALS also processes a cor-
pus by using a sliding window and linear weight-
ing, but differs in several aspects: the window size
of COALS is 4 and this value is fixed; COALS
does not distinguish between the preceding and
following words and treats them equally; applying
COALS supposes that all but the most frequent m
columns reflecting the most common open-class
words are discarded; COALS transforms weighted
counts in the co-occurrence matrix in a special
way (all the word pair correlations are calculated,
negative values are set to 0, and non-negative ones
are square rooted ? corr); and optionally, Singu-
lar Value Decomposition (Deerwester et al, 1990)
can be applied to the COALS co-occurrence ma-
trix.
RI is described in Sahlgren (2005) and can be
viewed as a mixture of HAL and LSA. First, RI
assigns random vectors to each word type in the
corpus. The random vectors, referred to as index
vectors, are very sparse, typically with a length
of thousands, and contain only several (e.g. 7)
non-zero values from the {-1,1} set. Second, RI
processes the corpus by exploiting a sliding win-
dow like HAL and COALS. However, RI does not
accumulate the weighted co-occurrence counts of
neighboring words to the vector of the word type
in focus. Instead, RI accumulates the index vec-
tors of the co-occurring words. For accounting the
word order, the permutation variant of RI was also
developed (Sahlgren et al, 2008). This variant
permutes the index vectors of neighboring words
of the word type in focus according to the word
order.
3 Compositionality Measures
We experimented with four basically different
compositionality measures (further referred to as
Measures) (Krc?ma?r? et al, 2013). Each Measure
employs a function to measure similarity of WSM
vectors. We experimented with the following
ones: cosine (cos), Euclidian (inverse to Euclid-
ian distance) (euc), and Pearson correlation (cor).
The mathematical formulas are presented below.
cos(a,b) =
?n
i=1 aibi??n
i=1(ai)
2
?n
i=1(bi)
2
euc(a,b) =
1
1 +
??n
i=1 (ai ? bi)
2
cor(a,b) =
?n
i=1 (ai ? a?)(bi ? b?)??n
i=1(ai ? a?)
2
?n
i=1(bi ? b?)
2
where a? =
?n
i=1 ai
n
, b? =
?n
i=1 bi
n
SU The substitutability-based Measure is based
on the fact that the replacement of non-
compositional expressions? constituents by the
words similar to them leads to anti-collocations
(Pearce, 2002). The compositionality of expres-
sions is calculated as the ratio between the num-
ber of occurrences of the expression in a corpora
and the sum of occurrences of its alternatives ?
possibly anti-collocations. In a similar way, we
can compare pointwise mutual information scores
(Lin, 1999). As an example, consider the possible
occurrences of ?hot dog? and ?warm dog? in the
corpora.
Formally, adopted from Krc?ma?r? et al (2012),
we calculate the compositionality score csu for an
examined expression as follows:
csu =
?H
i=1W ?a
h
i ,m? ?
?M
j=1W ?h, a
m
j ?
W ?h,m?
,
where ?h,m? denotes the number of corpora oc-
currences of the examined expression consisting
of a head and a modifying word, ahi and a
m
j denote
i-th and j-th most similar word4 in a certain WSM
to the head and modifying word of the expression,
respectively. W stands for a weighting function;
following Krc?ma?r? et al (2012), we experimented
with no (no) and logarithm (log) weighting. The
? symbol stands for one of the two operators: ad-
dition (plus) and multiplication (mult).
EN The endocentricity-based Measure, also re-
ferred to as component or constituent-based, com-
pares the WSM vectors of the examined expres-
sions and their constituents. The vectors expected
to be different from each other are e.g. the vector
representing the expression ?hot dog? and the vec-
tor representing the word ?dog?. Formally, the
4When exploiting POS tags, we constrained the similar
words to be of the same POS category in our experiments.
66
compositionality score cen can be calculated as
follows:
cen = f(xh, xm) ,
where xh and xm denote the similarity (sim) or
inverse rank distance (?dist) between the exam-
ined expression and its head and modifying con-
stituent, respectively, with regards to a certain
WSM. Function f stands for a combination of its
parameters: 0.5xh + 0.5xm (avg), 0xh + 1xm
(mOnly), 1xh + 0xm (hOnly), min(xh, xm) (min),
and max(xh, xm) (max).
CO The compositionality-based Measure com-
pares the true co-occurrence vector of the exam-
ined expression and the vector obtained from the
vectors corresponding to the constituents of the
expression using some compositionality function
(Reddy et al, 2011a). Commonly used compo-
sitionality functions are vector addition (?) and
pointwise vector multiplication (?) (Mitchell and
Lapata, 2008). The vectors expected to be dif-
ferent from each other are e.g. ?hot dog? and
?hot???dog?. Formally,
cco = s(ve, vh ? vm) ,
where ve, vh, and vm stand for vectors of an ex-
amined expression, its head and modifying con-
stituents, respectively. ? stands for a vector opera-
tion.
NE The neighbors-in-common-based Measure
is based on overlap of the most similar words to
the examined expression and to its constituents
(McCarthy et al, 2003). As an example, consider
that ?hot dog? is similar to ?food? or ?chips? and
?dog? is similar to ?cat? or ?bark?. On the other
hand, the list of neighbors of a semantically com-
positional expression such as ?black dog? is sup-
posed to overlap with at least one of the lists of
neighbors of both the expression constituents. For-
mally,
cne = o
h
N + o
m
N ,
where ohN and o
m
N stand for the number of same
words occurring in the list of the most similar
words to the examined expression and to its head
and modifying constituent, respectively.
4 Experiments
We evaluated the ability of various combinations
of WSMs and Measures to rank expressions as the
human annotators had done ahead of time.
Datasets We experimented with the DISCO
(Biemann and Giesbrecht, 2011) and Reddy
(Reddy et al, 2011a) human annotated datasets,
built for the task of automatic determining of se-
mantic compositionality. The DISCO and Reddy
datasets consist of manually scored expressions
of adjective-noun (AN), verb-object (VO), and
subject-verb (SV) types and the noun-noun (NN)
type, respectively. The DISCO dataset consists
of 349 expressions divided into training, valida-
tion, and test data (TestD); the Reddy dataset con-
sists of one set containing 90 expressions. Since
the DISCO validation data are of low size (35),
we concatenated them with the training data (Tr-
ValD). To TrValD and TestD we added the Reddy
dataset, which we had divided stratifically ahead
of time. Numbers of expressions of all the differ-
ent types are summarized in Table 1.
dataset AN-VO-SV AN VO SV NN
TrValD 175 68 68 39 45
TestD 174 77 62 35 45
Table 1: Numbers of expressions of all the differ-
ent types from the DISCO and Reddy datasets.
WSM construction Since the DISCO and
Reddy data were extracted from the ukWaC cor-
pus (Baroni et al, 2009), we also build our WSMs
from the same corpus. We use our own modifica-
tion of the S-Space package (Jurgens and Stevens,
2010). The modification lies in treating multiword
expressions and handling stopwords. Specifically,
we extended the package with the capability of
building WSM vectors for the examined expres-
sions in such a way that the WSM vectors previ-
ously built for words are preserved. This differen-
tiates our approach e.g. from Baldwin et al (2003),
who label the expressions in the corpus ahead of
time and treat them as single words.5 As for treat-
ing stopwords, we map trigrams containing deter-
miners as the middle word into bigrams without
the determiners. The intuition is to extract better
co-occurrence statistics for VO expressions often
containing an intervening determiner. As an ex-
ample, compare the occurrences of ?reinvent (de-
5Since many single word occurrences disappear, the
WSM vectors for words change. The more expressions are
treated as single words, the more WSM changes. Conse-
quently, we believe that this approach cannot be used for
building a list of all expressions occurring in an examined
corpus ordered by their compositionality score.
67
terminer) wheel? and ?reinvent wheel? in ukWaC
being 623 and 27, respectively.
We experimented with lemmas (noT) or with
lemmas concatenated with their part of speech
(POS) tags (yesT). We labeled the following
strings in ukWaC as stopwords: low-frequency
words (lemmas with frequency< 50), strings con-
taining two adjacent non-letter characters (thus
omitting sequences of various symbols), and
closed-class words.
For our experiments, we built WSMs using var-
ious parameters examined in previous works (see
Section 2) and parameters which are implied from
our own experience with WSMs. Figure 1 sum-
marizes all the parameters we used for building
WSMs.
Measure settings We examined various Mea-
sure settings (see Section 3), summarized in Ta-
ble 2. For all the vector comparisons, we used the
cos similarity. Only for HAL we also examined
euc and for COALS cor, since these are the rec-
ommended similarity functions for these particu-
lar WSMs (Lund and Burgess, 1996; Rohde et al,
2005).
Met. par. possible values
all sim. cos, euc if HAL, cor if COALS
SU H 0,1,...,20,30,...,100
SU M 0,1,...,20,30,...,100
SU W no, log
SU ? plus, mult
EN x sim, ?dist
EN f avg, mOnly, hOnly, min, max
CO ? ?, ?
NE N 10,20,...,50,100,200,...,500,1000
Table 2: All the parameters of Measures for de-
termining semantic compositionality described in
Section 3 used in our experiments.
Experimental setup Following Biemann and
Giesbrecht (2011), Reddy et al (2011a), Krc?ma?r?
et al (2012), and Krc?ma?r? et al (2013), we use
the Spearman correlation (?) for the evaluation of
all the combinations of WSMs and Measures (Se-
tups). Since the distribution of scores assigned to
Reddy?s NN dataset might not have corresponded
to the distribution of DISCO scores, we decided
not to map them to the same scale. Thus, we do not
create a single list consisting of all the examined
expressions. Instead, we order our Setups accord-
ing to the weighted average of Spearman corre-
lations calculated across all the expression types.
The weights are directly proportional to the fre-
quencies of the particular expression types. Thus,
the Setup score (wAvg) is calculated as follows:
wAvg =
|AN |?AN + |V O|?V O + |SV |?SV + |NN |?NN
|AN | + |V O| + |SV | + |NN |
.
Having the evaluation testbed, we tried to find
the optimal parameter settings for all WSMs com-
bined with all Measures with the help of TrValD.
Then, we applied the found Setups to TestD.
Notes Because several expressions or their con-
stituents concatenated with their POS tags did not
occur sufficiently often (for expressions: ? 0,
for constituents: ? 50) in ukWaC, we removed
them from the experiments; we removed ?number
crunching?, ?pecking order?, and ?sacred cow?
from TrValD and ?leading edge?, ?broken link?,
?spinning jenny?, and ?sitting duck? from TestD.
5 Results
The Setups achieving the highest wAvg when ap-
plied to TrValD are depicted in Table 3. The same
Setups and their results when applied to TestD are
depicted in Table 4. The values of Spearman cor-
relations in TestD confirm many of the observa-
tions from TrValD6:
Almost all the combinations of WSMs and
Measures achieve correlation values which are sta-
tistically significant. This is best illustrated by the
?(AN ?V O?SV ) column in Table 4, where a
lot of correlation values are statistically (p<0.05)
or highly statistically (p<0.001) significant, with
regards to the number of expressions (172).
The results suggest that for every expression
type, the task of determining compositionality is
of varying difficulty. While determining the com-
positionality of the NN expression type seems to
be the simplest (the highest correlations observed),
determining the compositionality of the SV ex-
pression type seems to be hard since the majority
of values in the ?SV column are not statistically
significant; taking into account the number of SV
expressions in TestD ? 35, the statistically signifi-
cant value of ? at the p<0.05 level is 0.34.
The correlation values differ with regards to the
expression type. Certain WSMs combined with
6A test of statistical difference between two values of the
Spearman correlation is adopted from Papoulis (1990).
68
Figure 1: All the parameters of WSMs described in Section 2 used in all our experiments. Semicolon
denotes OR. All the examined combinations of parameters are implied from reading the diagram from
left to right.
certain Measures, although achieving high corre-
lations upon certain expression types, fail to cor-
relate with the rest of the expression types. Com-
pare e.g. the correlation values of VSM and LSA
combined with the SU Measure upon the AN and
SV types with the correlation values upon the VO
and NN types.
The results, as expected, illustrate that employ-
ing more advanced alternatives of basic WSMs is
more appropriate. Specifically, LSA outperforms
VSM and COALS outperforms HAL in 21 and 23
correlation values out of 24, respectively. Con-
cerning RI, the values of correlations seem to be
close to the values of VSM and HAL.
An interesting observation showing the appro-
priateness of using wAvg(of?) as a good evalu-
ation score is supported by a comparison of the
wAvg(of?) and ?(AN?V O?SV ) columns. The
columns suggest that some Setups might only be
able to order the expressions of the same type and
might not be able to order the expressions of dif-
ferent types among each other. As an example,
compare the value of ? = 0.42 in wAvg(of?)
with ? = 0.28 in ?(AN?V O?SV ) in the row cor-
responding to COALS combined with SU. Con-
sider also that all the values of correlations are
higher or equal to the value in ?(AN?V O?SV ).
As for the parameters learned from applying
all the combinations of differently set WSM algo-
rithms and Measures to TrValD, their diversity is
well illustrated in Tables 5 and 6. Due to this diver-
sity, we cannot recommend any particular settings
except for one. All our SU Measures benefit from
weighting numbers of expression occurrences by
logarithm.
The correlation values in TestD are slightly
lower ? probably due to overfitting ? than the
ones observed in TrValD. HAL combined with the
Measures using euc similarity was not as success-
ful as when combined with cos.7
For comparison, the results of Reddy et al
(2011b) and Chakraborty et al (2011) as the
results of the best performing Setups based on
WSMs and association measures, respectively, ap-
plied to the DISCO data, are presented (Biemann
and Giesbrecht, 2011). The correlation values of
our Setups based on LSA and COALS, respec-
tively, are mostly higher. However, the improve-
ments are not statistically significant. Also, the re-
cent results achieved by Krc?ma?r? et al (2012) em-
ploying COALS and Krc?ma?r? et al (2013) employ-
7However, using HAL combined with euc, we observed
significant negative correlations which deserve further explo-
ration.
69
ing LSA are depicted.
Discussion As described above, we observed
different values of correlations for different ex-
pression types. This motivates us to think about
other classes of expressions different from types;
Measures could be e.g. varyingly successful with
regards to different occurrence frequency classes
of expressions (Evert, 2005). However, with such
small datasets, as shown e.g. by the fact that the
majority of our results are statistically indistin-
guishable, we cannot carry out any deeper in-
vestigations. A large dataset would provide a
more reliable comparison. Ideally, this would
consist of all the candidate expressions occurring
in some smaller corpus. Also, we would pre-
fer the annotated dataset not to be biased towards
non-compositional expressions and to be provided
with an inner-annotator agreement (Pecina, 2008);
which is unfortunately not the case of the DISCO
dataset.
6 Conclusion
Our study suggests that different WSMs combined
with different Measures perform reasonably well
in the task of determining the semantic composi-
tionality of word expressions of different types.
Especially, LSA and COALS perform well in
our experiments since their results are better than
those of their basic variants (VSM and HAL, re-
spectively) and, although not statistically signifi-
cantly, they outperform the best results of the pre-
viously proposed approaches (Table 4).
Importantly, our results demonstrate (Section 5)
that the datasets used for the experiments are small
for: first, a statistical learning of optimal parame-
ters of both WSM algorithms and Measures; sec-
ond, a thorough (different types) and reliable (sta-
tistically significant) comparison of our and the
previously proposed approaches.
Therefore, we plan to build a larger manually-
annotated dataset. Finally, we plan to extract
a list of semantically non-compositional expres-
sions from a given corpus and experiment with us-
ing it in NLP applications.
Acknowledgments
We thank to V??t Suchomel for providing the
ukWaC corpus and the anonymous reviewers
for their helpful comments and suggestions.
This work was supported by the European
Regional Development Fund (ERDF), project
NTIS ? New Technologies for the Informa-
tion Society, European Centre of Excellence,
CZ.1.05/1.1.00/02.0090; by Advanced Comput-
ing and Information Systems (grant no. SGS-
2013-029); and by the Czech Science Foun-
dation (grant no. P103/12/G084). Also, the
access to the CERIT-SC computing facilities
provided under the programme Center CERIT
Scientific Cloud, part of the Operational Pro-
gram Research and Development for Innovations,
reg. no. CZ.1.05/3.2.00/08.0144 is highly appreci-
ated.
References
Otavio Costa Acosta, Aline Villavicencio, and Vi-
viane P. Moreira. 2011. Identification and treatment
of multiword expressions applied to information re-
trieval. In Proceedings of the Workshop on Multi-
word Expressions: from Parsing and Generation to
the Real World, MWE ?11, pages 101?109, Strouds-
burg, PA, USA.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. Proceed-
ings of the ACL 2003 workshop on Multiword ex-
pressions analysis acquisition and treatment, pages
89?96.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide web:
a collection of very large linguistically processed
web-crawled corpora. Journal of Language Re-
sources And Evaluation, 43(3):209?226.
Chris Biemann and Eugenie Giesbrecht. 2011. Dis-
tributional semantics and compositionality 2011:
shared task description and results. In Proceedings
of the Workshop on Distributional Semantics and
Compositionality, DiSCo ?11, pages 21?28.
Marine Carpuat and Mona Diab. 2010. Task-based
evaluation of multiword expressions: a pilot study
in statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 242?
245, Stroudsburg, PA, USA.
Tanmoy Chakraborty, Santanu Pal, Tapabrata Mondal,
Tanik Saikh, and Sivaju Bandyopadhyay. 2011.
Shared task system description: Measuring the com-
positionality of bigrams using statistical methodolo-
gies. In Proceedings of the Workshop on Distribu-
tional Semantics and Compositionality, pages 38?
42, Portland, Oregon, USA.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
70
WSM Measure wAvg(of ?) ?AN-VO-SV ?AN ?VO ?SV ?NN
VSM1 SU1 0.31 0.11 -0.03 0.36 0.31 0.75
VSM2 EN1 0.36 0.32 0.41 0.30 0.10 0.61
VSM3 CO1 0.40 0.34 0.40 0.26 0.39 0.64
VSM1 NE1 0.34 0.26 0.20 0.48 0.07 0.60
LSA1 SU2 0.34 0.19 -0.05 0.46 0.42 0.71
LSA2 EN2 0.56 0.53 0.54 0.51 0.59 0.65
LSA3 CO1 0.55 0.53 0.49 0.56 0.63 0.58
LSA2 NE2 0.50 0.45 0.46 0.37 0.64 0.62
HAL1 SU3 0.45 0.36 0.28 0.50 0.40 0.67
HAL2 EN3 0.36 0.35 0.47 0.28 0.27 0.38
HAL3 CO1 0.23 0.15 0.28 0.12 -0.01 0.54
HAL4 NE3 0.27 0.25 0.31 0.21 0.17 0.39
COALS1 SU4 0.48 0.41 0.28 0.56 0.49 0.68
COALS2 EN2 0.58 0.54 0.6 0.63 0.37 0.68
COALS2 CO1 0.59 0.54 0.6 0.64 0.37 0.70
COALS2 NE4 0.58 0.56 0.61 0.58 0.46 0.67
RI1 SU5 0.52 0.44 0.45 0.51 0.52 0.68
RI2 EN3 0.45 0.44 0.41 0.57 0.33 0.45
RI3 CO1 0.21 0.13 0.13 0.16 0.11 0.54
RI2 NE5 0.43 0.43 0.43 0.53 0.21 0.49
Table 3: The Spearman correlations ? of the best performing (wAvg) combinations of particular WSMs
and Measures from all the tested Setups applied to TrValD. The highest correlation values in the particular
columns and the correlation values which are not statistically different from them (p < 0.05) are in bold
(yet we do not know how to calculate the stat. significance for the wAvg(of ?) column). The parameters
of WSMs and Measures corresponding to the indexes are depicted in Tables 5 and 6, respectively.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Stefan Evert. 2005. The statistics of word cooccur-
rences : word pairs and collocations. Ph.D. the-
sis, Universita?t Stuttgart, Holzgartenstr. 16, 70174
Stuttgart.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Anders Johannsen, Hector Martinez Alonso, Christian
Rish?j, and Anders S?gaard. 2011. Shared task sys-
tem description: frustratingly hard compositionality
prediction. In Proceedings of the Workshop on Dis-
tributional Semantics and Compositionality, DiSCo
?11, pages 29?32, Stroudsburg, PA, USA.
David Jurgens and Keith Stevens. 2010. The s-
space package: an open source package for word
space models. In Proceedings of the ACL 2010 Sys-
tem Demonstrations, ACLDemos ?10, pages 30?35,
Stroudsburg, PA, USA.
Lubom??r Krc?ma?r?, Karel Jez?ek, and Massimo Poesio.
2012. Detection of semantic compositionality using
semantic spaces. Lecture Notes in Computer Sci-
ence, 7499 LNAI:353?361.
Lubom??r Krc?ma?r?, Karel Jez?ek, and Pavel Pecina. 2013.
Determining compositionality of word expressions
using word space models. In Proceedings of the 9th
Workshop on Multiword Expressions, pages 42?50,
Atlanta, Georgia, USA.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Dekang Lin. 1999. Automatic identification of
non-compositional phrases. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
ACL ?99, pages 317?324, Stroudsburg, PA, USA.
Kevin Lund and Curt Burgess. 1996. Produc-
ing high-dimensional semantic spaces from lexi-
cal co-occurrence. Behavior Research Methods,
28(2):203?208.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Proceedings of the ACL 2003
workshop on Multiword expressions analysis acqui-
sition and treatment, volume 18 of MWE ?03, pages
73?80.
71
WSM Measure wAvg(of ?) ?AN-VO-SV ?AN ?VO ?SV ?NN
VSM1 SU1 0.28 0.03 0.01 0.51 0.04 0.62
VSM2 EN1 0.26 0.19 0.08 0.29 0.04 0.69
VSM3 CO1 0.32 0.26 0.24 0.23 0.25 0.65
VSM1 NE1 0.32 0.19 0.36 0.25 -0.13 0.73
LSA1 SU2 0.31 0.06 0.05 0.50 0.20 0.59
LSA2 EN2 0.50 0.40 0.39 0.55 0.32 0.78
LSA3 CO1 0.48 0.36 0.29 0.60 0.42 0.69
LSA2 NE2 0.44 0.33 0.34 0.40 0.44 0.67
HAL1 SU3 0.29 0.16 0.09 0.32 0.34 0.56
HAL2 EN3 0.36 0.28 0.33 0.35 0.26 0.53
HAL3 CO1 0.24 0.22 0.25 0.16 0.15 0.42
HAL4 NE3 0.21 0.14 0.02 0.33 0.06 0.47
COALS1 SU4 0.42 0.28 0.28 0.54 0.30 0.59
COALS2 EN2 0.49 0.44 0.52 0.51 0.07 0.72
COALS2 CO1 0.47 0.40 0.47 0.51 0.07 0.74
COALS2 NE4 0.52 0.48 0.55 0.50 0.21 0.74
RI1 SU5 0.30 0.14 0.14 0.29 0.12 0.72
RI2 EN3 0.44 0.34 0.37 0.54 0.20 0.63
RI3 CO1 0.23 0.23 0.29 0.17 0.17 0.26
RI2 NE5 0.31 0.26 0.26 0.42 0.04 0.44
Reddy-WSM - 0.35 - - - -
StatMix - 0.33 - - - -
Krcmar-COALS - 0.42 0.42 0.69 0.24 -
Krcmar-LSA - 0.50 0.50 0.56 0.41 -
Table 4: The Spearman correlations ? of the best performing (wAvg) combinations of particular WSMs
and Measures trained in TranValD applied to TestD. The highest correlation values in the particular
columns and the correlation values which are not statistically different from them (p < 0.05) are in bold
(yet we do not know how to calculate the stat. significance for the wAvg(of ?) column). Reddy-WSM and
StatMix stand for the best performing system based on WSMs and association measures, respectively,
applied to the DISCO task (Biemann and Giesbrecht, 2011). Krcmar-COALS and Krcmar-LSA stand for
the best published results achieved upon the dataset presented in Krc?ma?r? et al (2012) and Krc?ma?r? et al
(2013), respectively. The parameters of WSMs and Measures corresponding to the indexes are depicted
in Tables 5 and 6, respectively.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio.
Preslav Nakov, Antonia Popova, and Plamen Ma-
teev. 2001. Weight functions impact on lsa per-
formance. In Proceedings of the EuroConference
Recent Advances in Natural Language Processing
(RANLP?01), pages 187?193.
Athanasios Papoulis. 1990. Probability & statistics.
Prentice Hall.
Darren Pearce. 2002. A Comparative Evaluation of
Collocation Extraction Techniques. In Proceedings
of the Third International Conference on Language
Resources and Evaluation, LREC.
Pavel Pecina. 2008. Reference data for Czech collo-
cation extraction. In Proceedings of the LREC 2008
Workshop Towards a Shared Task for Multiword Ex-
pressions, pages 11?14, Marrakech, Morocco. Euro-
pean Language Resources Association.
Siva Reddy, Diana McCarthy, and Suresh Manand-
har. 2011a. An empirical study on composition-
ality in compound nouns. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 210?218, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Siva Reddy, Diana McCarthy, Suresh Manandhar, and
Spandana Gella. 2011b. Exemplar-based word-
space model for compositionality detection: Shared
task system description. In Proceedings of the Work-
shop on Distributional Semantics and Composition-
ality, pages 54?60, Portland, Oregon, USA.
72
WSM parameters
VSM tags trans.
VSM1 noT noNo
VSM2 yesT noNo
VSM3 yesT noIdf
LSA tags trans. dim.
LSA1 noT logEnt 900
LSA2 yesT noNo 300
LSA3 noT noIdf 300
HAL tags win s. ret. c.
HAL1 noT 5 20000
HAL2 yesT 5 20000
HAL3 noT 2 10000
HAL4 yesT 5 all
COALS tags ret. c.
COALS1 noT 7000
COALS2 yesT 7000
RI tags win. s. vec. s. perm.
RI1 noT 2 4000 no
RI2 noT 4 4000 no
RI3 noT 2 4000 yes
Table 5: Parameters of WSMs (Section 2) which,
combined with particular Measures, achieved the
highest average correlation in TrValD.
Douglas L. Rohde, Laura M. Gonnerman, and David C.
Plaut. 2005. An improved model of semantic sim-
ilarity based on lexical co-occurrence. Unpublished
manuscript.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order in
word space. In V. Sloutsky, B. Love, and K. Mcrae,
editors, Proceedings of the 30th Annual Conference
of the Cognitive Science Society, pages 1300?1305.
Cognitive Science Society, Austin, TX.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Semantic
Indexing Workshop at the 7th International Confer-
ence on Terminology and Knowledge Engineering,
Leipzig, Germany.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Gerard Salton. 1971. The SMART Retrieval Sys-
tem; Experiments in Automatic Document Process-
ing. Prentice-Hall, Inc., Upper Saddle River, NJ,
USA.
Dawei Song, Peter Bruza, and Richard Cole. 2004.
Concept learning and information inferencing on a
Measure parameters
SU sim. ? W H M
SU1 cos plus log 30 3
SU2 cos plus log 100 5
SU3 cos mult log 12 2
SU4 cos mult log 80 4
SU5 cos mult log 4 3
EN sim. func. x
EN1 cos min sim
EN2 cos avg sim
EN3 cos min ?dist
CO sim. ?
CO1 cos ?
NE sim. O
NE1 cos 1000
NE2 cos 500
NE3 cos 50
NE4 cor 500
NE5 cos 20
Table 6: Parameters of Measures (Section 3)
which, combined with particular WSMs, achieved
the highest average correlation in TrValD.
highdimensional semantic space. In ACM SIGIR
2004 Workshop on Mathematical/Formal Methods
in Information Retrieval.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of seman-
tics. J. Artif. Int. Res., 37(1):141?188.
73
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12?58,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Findings of the 2014 Workshop on Statistical Machine Translation
Ond
?
rej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Christian Federmann
Microsoft Research
Barry Haddow
University of Edinburgh
Philipp Koehn
JHU / Edinburgh
Johannes Leveling
Dublin City University
Christof Monz
University of Amsterdam
Pavel Pecina
Charles University in Prague
Matt Post
Johns Hopkins University
Herve Saint-Amand
University of Edinburgh
Radu Soricut
Google
Lucia Specia
University of Sheffield
Ale
?
s Tamchyna
Charles University in Prague
Abstract
This paper presents the results of the
WMT14 shared tasks, which included a
standard news translation task, a sepa-
rate medical translation task, a task for
run-time estimation of machine translation
quality, and a metrics task. This year, 143
machine translation systems from 23 insti-
tutions were submitted to the ten transla-
tion directions in the standard translation
task. An additional 6 anonymized sys-
tems were included, and were then evalu-
ated both automatically and manually. The
quality estimation task had four subtasks,
with a total of 10 teams, submitting 57 en-
tries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2014. This workshop builds
on eight previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al., 2007, 2008,
2009, 2010, 2011, 2012; Bojar et al., 2013).
This year we conducted four official tasks: a
translation task, a quality estimation task, a met-
rics task
1
and a medical translation task. In the
translation task (?2), participants were asked to
translate a shared test set, optionally restricting
themselves to the provided training data. We held
ten translation tasks this year, between English and
each of Czech, French, German, Hindi, and Rus-
sian. The Hindi translation tasks were new this
year, providing a lesser resourced data condition
on a challenging language pair. The system out-
puts for each task were evaluated both automati-
cally and manually.
1
The metrics task is reported in a separate paper
(Mach?a?cek and Bojar, 2014).
The human evaluation (?3) involves asking
human judges to rank sentences output by
anonymized systems. We obtained large num-
bers of rankings from researchers who contributed
evaluations proportional to the number of tasks
they entered. Last year, we dramatically increased
the number of judgments, achieving much more
meaningful rankings. This year, we developed a
new ranking method that allows us to achieve the
same with fewer judgments.
The quality estimation task (?4) this year
included sentence- and word-level subtasks:
sentence-level prediction of 1-3 likert scores,
sentence-level prediction of percentage of word
edits necessary to fix a sentence, sentence-level
prediction of post-editing time, and word-level
prediction of scores at different levels of granular-
ity (correct/incorrect, accuracy/fluency errors, and
specific types of errors). Datasets were released
with English-Spanish, English-German, Spanish-
English and German-English news translations
produced by 2-3 machine translation systems and,
for some subtasks, a human translation.
The medical translation task (?5) was intro-
duced this year. Unlike the ?standard? translation
task, the test sets come from the very specialized
domain of medical texts. The aim of this task was
not only domain adaptation but also the utilization
of translation systems in a larger scenario, namely
cross-lingual information retrieval (IR). Extrinsic
evaluation in an IR setting was a part of this task
(on the other hand, manual evaluation of transla-
tion quality was not carried out).
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation and estimation methodologies for
machine translation. As before, all of the data,
12
translations, and collected human judgments are
publicly available.
2
We hope these datasets serve
as a valuable resource for research into statistical
machine translation and automatic evaluation or
prediction of translation quality.
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and other languages.
As in the previous years, the other languages in-
clude German, French, Czech and Russian.
We dropped Spanish and added Hindi this year.
From a linguistic point of view, Spanish poses
similar problems as French, making its prior in-
clusion less valuable. Hindi is not only interest-
ing since it is a more distant language than the
European languages we include, but also because
we have much less training data, thus forcing re-
searchers to deal with low resource conditions, but
also providing them with a language pair that does
not suffer from the computational complexities of
having to deal with massive amounts of training
data.
We created a test set for each language pair by
translating newspaper articles and provided train-
ing data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources, as before. How-
ever, we changed our method to create the test sets.
In previous years, we took equal amounts of
source sentences from all six languages involved
(around 500 sentences each), and translated them
into all other languages. While this produced a
multi-parallel test corpus that could be also used
for language pairs (such as Czech-Russian) that
we did not include in the evaluation, it did suf-
fer from artifacts from the larger distance between
source and target sentences. Most test sentences
involved the translation a source sentence that
was translated from a their language into a tar-
get sentence (which was compared against a trans-
lation from that third language as well). Ques-
tions have been raised, if the evaluation of, say,
French-English translation is best served when
testing on sentences that have been originally writ-
ten in, say, Czech. For discussions about trans-
lationese please for instance refer to Koppel and
Ordan (2011).
2
http://statmt.org/wmt14/results.html
This year, we took about 1500 English sen-
tences and translated them into the other 5 lan-
guages, and then additional 1500 sentences from
each of the other languages and translated them
into English. This gave us test sets of about 3000
sentences for our English-X language pairs, which
have been either written originally written in En-
glish and translated into X, or vice versa.
The composition of the test documents is shown
in Table 1. The stories were translated by the pro-
fessional translation agency Capita, funded by the
EU Framework Programme 7 project MosesCore,
and by Yandex, a Russian search engine com-
pany.
3
All of the translations were done directly,
and not via an intermediate language.
2.2 Training data
As in past years we provided parallel corpora
to train translation models, monolingual cor-
pora to train language models, and development
sets to tune system parameters. Some train-
ing corpora were identical from last year (Eu-
roparl
4
, United Nations, French-English 10
9
cor-
pus, CzEng, Common Crawl, Russian-English
Wikipedia Headlines provided by CMU), some
were updated (Russian-English parallel data pro-
vided by Yandex, News Commentary, monolin-
gual data), and a new corpus was added (Hindi-
English corpus, Bojar et al. (2014)), Hindi-English
Wikipedia Headline corpus).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their entry
names are listed in Table 2; each system did not
necessarily appear in all translation tasks. We also
included four commercial off-the-shelf MT sys-
tems and four online statistical MT systems, which
we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3
http://www.yandex.com/
4
As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
13
Europarl Parallel Corpus
French? English German? English Czech? English
Sentences 2,007,723 1,920,209 646,605
Words 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 183,251 201,288 146,549 165,602
Words 5,688,656 4,659,619 5,105,101 5,046,157 3,288,645 3,590,287 4,153,847 4,339,974
Distinct words 72,863 62,673 150,760 65,520 139,477 55,547 151,101 60,801
Common Crawl Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 3,244,152 2,399,123 161,838 878,386
Words 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
French? English
Sentences 12,886,831
Words 411,916,781 360,341,450
Distinct words 565,553 666,077
10
9
Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Hindi-English Parallel Corpus
Hindi? English
Sentences 287,202
Words 6,002,418 3,953,851
Distinct words 121,236 105,330
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English Hindi? English
Sentences 514,859 32,863
Words 1,191,474 1,230,644 141,042 70,075
Distinct words 282,989 251,328 25,678 26,989
Europarl Language Model Data
English French German Czech
Sentence 2,218,201 2,190,579 2,176,537 668,595
Words 59,848,044 63,439,791 53,534,167 14,946,399
Distinct words 123,059 145,496 394,781 172,461
News Language Model Data
English French German Czech Russian Hindi
Sentence 90,209,983 30,451,749 89,634,193 36,426,900 32,245,651 1,275,921
Words 2,109,603,244 748,852,739 1,606,506,785 602,950,410 575,423,682 36,297,394
Distinct words 4,089,792 1,906,470 10,248,707 3,101,846 2,860,837 258,759
News Test Set
French? English German? English Czech? English Russian? English Hindi? English
Sentences 3003 3003 3003 3003 2507
Words 81,194 71,147 63,078 67,624 60,240 68,866 62,107 69,329 86,974 55,822
Distinct words 11,715 10,610 13,930 10,458 16,774 9,893 17,009 9,938 8,292 9,217
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
14
Language Sources (Number of Documents)
Czech aktu?aln?e.cz (2), blesk.cz (3), blisty.cz (1), den??k.cz (9), e15.cz (1), iDNES.cz (17), ihned.cz (14), lidovky.cz (8), medi-
afax.cz (2), metro.cz (1), Novinky.cz (5), pravo.novinky.cz (6), reflex.cz (2), tyden.cz (1), zdn.cz (1).
French BBC French Africa (1), Canoe (9), Croix (4), Cyber Presse (12), Dernieres Nouvelles (1), dhnet.be (5), Equipe (1),
Euronews (6), Journal Metro.com (1), La Libre.be (2), La Meuse.be (2), Le Devoir (3), Le Figaro (8), Le Monde (3),
Les Echos (15), Lexpress.fr (3), Liberation (1), L?independant (2), Metro France (1), Nice-Matin (6), Le Nouvel Ob-
servateur (3), Radio Canada (6), Reuters (7).
English ABC News (5), BBC (5), CBS News (5), CNN (5), Daily Mail (5), Financial Times (5), Fox News (2), Globe and
Mail (1), Independent (1), Los Angeles Times (1), New Yorker (1), News.com Australia (16), Reuters (3), Scotsman (2),
smh.com.au (2), stv.tv (1), Telegraph (6), UPI (2).
German Abendzeitung N?urnberg (1), all-in.de (2), Augsburger Allgemeine (1), AZ Online (1), B?orsenzeitung (1), come-
on.de (1), Der Westen (2), DZ Online (1), Reutlinger General-Anzeiger (1), Generalanzeiger Bonn (1), Giessener
Anzeiger (1), Goslarsche Zeitung (1), Hersfelder Zeitung (1), J?udische Allgemeine (1), Kreisanzeiger (2),
Kreiszeitung (2), Krone (1), Lampertheimer Zeitung (2), Lausitzer Rundschau (1), Mittelbayerische (1), Morgen-
post (1), nachrichten.at (1), Neue Presse (1), OP Online (1), Potsdamer Neueste Nachrichten (1), Passauer Neue
Presse (1), Recklingh?auser Zeitung (1), Rhein Zeitung (1), salzburg.com (1), Schwarzw?alder Bote (29), Segeberger
Zeitung (1), Soester Anzeiger (1), S?udkurier (17), svz.de (1), Tagesspiegel (1), Usinger Anzeiger (3), Volksblatt.li (1),
Westf?alischen Anzeiger (3), Wiener Zeitung (1), Wiesbadener Kurier (1), Westdeutsche Zeitung (1), Wilhelmshavener
Zeitung (1), Yahoo Deutschland (1).
Hindi Bhaskar (24), Jagran (61), Navbharat Times / India Times (4), ndtv (2).
Russian 168.ru (1), aif (3), altapress.ru (2), argumenti.ru (2), BBC Russian (3), belta.by (2), communa.ru (1), dp.ru (1), eg-
online.ru (1), Euronews (2), fakty.ua (2), gazeta.ru (1), inotv.rt.com (1), interfax (1), Izvestiya (1), Kommersant (7),
kp (2), lenta.ru (4), lgng (1), litrossia.ru (1), mirnov.ru (5), mk (8), mn.ru (2), newizv (2), nov-pravda.ru (1), no-
vayagazeta (1), nr2.ru (8), pnp.ru (1), rbc.ru (3), ria.ru (4), rosbalt.ru (1), sovsport.ru (6), Sport Express (10), trud.ru (4),
tumentoday.ru (1), vesti.ru (10), zr.ru (1).
Table 1: Composition of the test set. For more details see the XML test files. The docid tag gives the source and the date for
each document in the test set, and the origlang tag indicates the original source language.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
This year?s evaluation was conducted a bit dif-
ferently. The main differences are:
? In contrast to the past two years, we collected
judgments entirely from researchers partici-
pating in the shared tasks and trusted friends
of the community. Last year, about two thirds
of the data were solicited from random volun-
teers on the Amazon Mechanical Turk. For
some language pairs, the Turkers data had
much lower inter-annotator agreement com-
pared to the researchers.
? As a result, we collected about seventy-five
percent less data, but were able to obtain
good confidence intervals on the clusters with
the use of new approaches to ranking.
? We compared three different ranking method-
ologies, selecting the one with the highest ac-
curacy on held-out data.
We also maintain many of our customs from
prior years, including the presentation of the re-
sults in terms of a partial ordering (clustering) of
the systems. Systems in the same cluster could not
be meaningfully distinguished and should be con-
sidered ties.
3.1 Data collection
The system ranking is produced from a large set of
pairwise annotations between system pairs. These
pairwise annotations are collected in an evaluation
campaign that enlists participants in the shared
task to contribute one hundred ?Human Intelli-
gence Tasks? (HITs) per system submitted. Each
HIT consists of three ranking tasks. In a rank-
ing task, an annotator is presented with a source
segment, a human reference translation, and the
outputs of five anonymized systems, randomly se-
lected from the set of participating systems, and
randomly ordered.
To run the evaluation, we use Appraise
5
(Fe-
dermann, 2012), an open-source tool built on
Python?s Django framework. At the top of each
HIT, the following instructions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
5
https://github.com/cfedermann/Appraise
15
ID Institution
AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014)
CIMS University of Stuttgart / University of Munich (Cap et al., 2014)
CMU Carnegie Mellon University (Matthews et al., 2014)
CU-* Charles University, Prague (Tamchyna et al., 2014)
DCU-FDA Dublin City University (Bicici et al., 2014)
DCU-ICTCAS Dublin City University (Li et al., 2014b)
DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014)
EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014)
KIT Karlsruhe Institute of Technology (Herrmann et al., 2014)
IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014)
IIIT-HYDERABAD IIIT Hyderabad
IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014)
IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014)
KAZNU Amandyk Kartbayev, FBK
LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014)
MANAWI-* Universit?at des Saarlandes (Tan and Pal, 2014)
MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014)
PROMT-RULE,
PROMT-HYBRID
PROMT
RWTH RWTH Aachen (Peitz et al., 2014)
STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014)
UA-* University of Alicante (S?anchez-Cartagena et al., 2014)
UEDIN-PHRASE,
UEDIN-UNCNSTR
University of Edinburgh (Durrani et al., 2014b)
UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014)
UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014)
YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C,G] Four online statistical machine translation systems
RBMT-[1,4] Two rule-based statistical machine translation systems
Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
16
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly ordered), and is asked to
rank these according to their translation quality, with ties allowed.
A screenshot of the ranking interface is shown in
Figure 2. Annotators are asked to rank the sys-
tems from 1 (best) to 5 (worst), with ties permit-
ted. Note that a lower rank is better. The rankings
provided by a ranking task are then reduced to a
set of ten pairwise rankings produced by consider-
ing all
(
5
2
)
combinations of systems in the ranking
task. For example, consider the following annota-
tion provided among systems A,B, F,H , and J :
1 2 3 4 5
F ?
A ?
B ?
J ?
H ?
This is reduced to the following set of pairwise
judgments:
A > B,A = F,A > H,A < J
B < F,B < H,B < J
F > H,F < J
H < J
Here,A > B should be read is ?A is ranked higher
than (worse than) B?. Note that by this procedure,
the absolute value of ranks and the magnitude of
their differences are discarded.
For WMT13, nearly a million pairwise anno-
tations were collected from both researchers and
paid workers on Amazon?s Mechanical Turk, in
a roughly 1:2 ratio. This year, we collected data
from researchers only, an ability that was enabled
by the use of a new technique for producing the
partial ranking for each task (?3.3.3). Table 3 con-
tains more detail.
3.2 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960). If P (A) be
the proportion of times that the annotators agree,
and P (E) is the proportion of time that they would
17
LANGUAGE PAIR Systems Rankings Average
Czech?English 5 21,130 4,226.0
English?Czech 10 55,900 5,590.0
German?English 13 25,260 1,943.0
English?German 18 54,660 3,036.6
French?English 8 26,090 3,261.2
English?French 13 33,350 2,565.3
Russian?English 13 34,460 2,650.7
English?Russian 9 28,960 3,217.7
Hindi?English 9 20,900 2,322.2
English?Hindi 12 28,120 2,343.3
TOTAL WMT 14 110 328,830 2,989.3
WMT13 148 942,840 6,370.5
WMT12 103 101,969 999.6
WMT11 133 63,045 474.0
Table 3: Amount of data collected in the WMT14 manual evaluation. The final three rows report summary information from
the previous two workshops.
agree by chance, then Cohen?s kappa is:
? =
P (A)? P (E)
1? P (E)
Note that ? is basically a normalized version of
P (A), one which takes into account how mean-
ingful it is for annotators to agree with each other
by incorporating P (E). The values for ? range
from 0 to 1, with zero indicating no agreement and
1 perfect agreement.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A < B, A = B, or A > B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it captures the probability that two
annotators would agree randomly. Therefore:
P (E) = P (A<B)
2
+ P (A=B)
2
+ P (A>B)
2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 4 gives ? values for inter-annotator agree-
ment for WMT11?WMT14 while Table 5 de-
tails intra-annotator agreement scores, including
the division of researchers (WMT13
r
) and MTurk
(WMT13
m
) data. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0?0.2 is slight, 0.2?0.4 is
fair, 0.4?0.6 is moderate, 0.6?0.8 is substantial,
and 0.8?1.0 is almost perfect. The agreement rates
are more or less in line with prior years: worse for
some tasks, better for others, and on average, the
best since WMT11 (where agreement scores were
likely inflated due to inclusion of reference trans-
lations in the comparisons).
3.3 Models of System Rankings
The collected pairwise rankings are used to pro-
duce a ranking of the systems. Machine transla-
tion evaluation has always been a subject of con-
tention, and no exception to this rule exists for the
WMT manual evaluation. While the precise met-
ric has varied over the years, it has always shared
a common idea of computing the average num-
ber of times each system was judged better than
other systems, and ranking from highest to low-
est. For example, in WMT11 Callison-Burch et al.
(2011), the metric computed the percentage of the
time each system was ranked better than or equal
to other systems, and included comparisons to hu-
man references. In WMT12 Callison-Burch et al.
(2012), comparisons to references were dropped.
In WMT13, rankings were produced over 1,000
bootstrap-resampled sets of the training data. A
rank range was collected for each system across
these folds; the average value was used to order
the systems, and a 95% confidence interval across
these ranks was used to organize the systems into
equivalence classes containing systems with over-
18
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.400 0.311 0.244 0.342 0.279 0.305
English?Czech 0.460 0.359 0.168 0.408 0.075 0.360
German?English 0.324 0.385 0.299 0.443 0.324 0.368
English?German 0.378 0.356 0.267 0.457 0.239 0.427
French?English 0.402 0.272 0.275 0.405 0.321 0.357
English?French 0.406 0.296 0.231 0.434 0.237 0.302
Hindi?English ? ? ? ? ? 0.400
English?Hindi ? ? ? ? ? 0.413
Russian?English ? ? 0.278 0.315 0.324 0.324
English?Russian ? ? 0.243 0.416 0.207 0.418
MEAN 0.395 0.330 0.260 0.367
Table 4: ? scores measuring inter-annotator agreement. See Table 5 for corresponding intra-annotator agreement scores.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.597 0.454 0.479 0.483 0.478 0.382
English?Czech 0.601 0.390 0.290 0.547 0.242 0.448
German?English 0.576 0.392 0.535 0.643 0.515 0.344
English?German 0.528 0.433 0.498 0.649 0.452 0.576
French?English 0.673 0.360 0.578 0.585 0.565 0.629
English?French 0.524 0.414 0.495 0.630 0.486 0.507
Hindi?English ? ? ? ? ? 0.605
English?Hindi ? ? ? ? ? 0.535
Russian?English ? ? 0.450 0.363 0.477 0.629
English?Russian ? ? 0.513 0.582 0.500 0.570
MEAN 0.583 0.407 0.479 0.522
Table 5: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation.
lapping ranges.
This year, we introduce two new changes. First,
we pit the WMT13 method against two new ap-
proaches: that of Hopkins and May (2013, ?3.3.2),
and another based on TrueSkill (Sakaguchi et al.,
2014, ?3.3.3). Second, we compare these two
methods against WMT13?s ?Expected Wins? ap-
proach, and then select among them by determin-
ing which of them has the highest accuracy in
terms of predicting annotations on a held-out set
of pairwise judgments.
3.3.1 Method 1: Expected Wins (EW)
Introduced for WMT13, the EXPECTED WINS has
an intuitive score demonstrated to be accurate in
ranking systems according to an underlying model
of ?relative ability? (Koehn, 2012a). The idea is
to gauge the probability that a system S
i
will be
ranked better than another system randomly cho-
sen from a pool of opponents {S
j
: j 6= i}. If
we define the function win(A,B) as the number
of times system A is ranked better than system B,
then we can define this as follows:
score
EW
(S
i
) =
1
|{S
j
}|
?
j,j 6=i
win(S
i
, S
j
)
win(S
i
, S
j
) + win(S
j
, S
i
)
Note that this score ignores ties.
3.3.2 Method 2: Hopkins and May (HM)
Hopkins and May (2013) introduced a graphical
model formulation of the task, which makes the
notion of underlying system ability even more ex-
plicit. Each system S
J
in the pool {S
j
} is repre-
sented by an associated relative ability ?
j
and a
variance ?
2
a
(fixed across all systems) which serve
as the parameters of a Gaussian distribution. Sam-
ples from this distribution represent the quality
of sentence translations, with higher quality sam-
ples having higher values. Pairwise annotations
(S
1
, S
2
, pi) are generated according to the follow-
ing process:
19
1. Select two systems S
1
and S
2
from the pool
of systems {S
j
}
2. Draw two ?translations?, adding random
Gaussian noise with variance ?
2
obs
to simulate
the subjectivity of the task and the differences
among annotators:
q
1
? N (?
S
1
, ?
2
a
) +N (0, ?
2
obs
)
q
2
? N (?
S
2
, ?
2
a
) +N (0, ?
2
obs
)
3. Let d be a nonzero real number that defines
a fixed decision radius. Produce a rating pi
according to:
pi =
?
?
?
< q
1
? q
2
> d
> q
2
? q
1
> d
= otherwise
Hopkins and May use Gibbs sampling to infer
the set of system means from an annotated dataset.
Details of this inference procedure can be found in
Sakaguchi et al. (2014). The score used to produce
the rankings is simply the system mean associated
with each system:
score
HM
(S
i
) = ?
S
i
3.3.3 Method 3: TrueSkill (TS)
TrueSkill is an adaptive, online system that em-
ploys a similar model of relative ability Herbrich
et al. (2006). It was initially developed for Xbox
Live?s online player community, where it is used
to model player ability, assign levels, and select
competitive matches. Each player S
j
is modeled
by two parameters: TrueSkill?s current estimate
of each system?s relative ability, ?
S
j
, and a per-
system measure of TrueSkill?s uncertainty of those
estimates, ?
2
S
j
. When the outcome of a match is
observed, TrueSkill uses the relative status of the
two systems to update these estimates. If a trans-
lation from a system with a high mean is judged
better than a system with a greatly lower mean, the
result is not surprising, and the update size for the
corresponding system means will be small. On the
other hand, when an upset occurs in a competition,
the means will receive larger updates. Sakaguchi
et al. (2014) provide an adaptation of this approach
to the WMT manual evaluation, and showed that
it performed well on WMT13 data.
Similar to the Hopkins and May model,
TrueSkill scores systems by their inferred means:
score
TS
(S
i
) = ?
S
i
This score is then used to sort the systems and pro-
duce the ranking.
3.4 Method Selection
We have three methods which, provided with the
collected data, produce different rankings of the
systems. Which of them is correct? More imme-
diately, which one of them should we publish as
the official ranking for the WMT14 manual eval-
uation? As discussed, the method used to com-
pute the ranking has been tweaked a bit each year
over the past few years in response to criticisms
(e.g., Lopez (2012); Bojar et al. (2011)). While the
changes were reasonable (and later corroborated),
Hopkins and May (2013) pointed out that this task
of model selection should be driven by empirical
evaluation on held-out data, and suggested per-
plexity as the metric of choice.
We choose instead a more direct gold-standard
evaluation metric: the accuracy of the rankings
produced by each method in predicting pairwise
judgments. We use each method to produce a par-
tial ordering of the systems, grouping them into
equivalence classes. This partial ordering unam-
biguously assigns a prediction pi
P
between any
pair of systems (S
i
, S
j
). By comparing the pre-
dicted relationship pi
P
to the actual annotation for
each pairwise judgment in the test data (by token),
we can compute an accuracy score for each model.
We predict accuracy in this manner using 100-
fold cross-validation. For each task, we split the
data into a fixed set of 100 randomly-selected
folds. Each fold serves as a test set, with the
remaining ninety-nine folds available as training
data for each method. Note that the total order-
ing over systems provided by the score
?
functions
defined do not predict ties. In order to do enable
the models to predict ties, we produce equivalence
classes using the following procedure:
? Assign S
1
to a cluster
? For each system S
i
, assign it to the current
cluster if score(S
i?1
) ? score(S
i
) ? r; oth-
erwise, assign it to a new cluster
The value of r (the decision radius for ties)
is tuned using accuracy on the entire training
data using grid search over the values r ?
0, 0.01, 0.02, . . . , .25 (26 values in total). This
value is tuned separately for each method on each
fold. Table 6 contains an example partial ordering.
20
System Score Rank
B 0.60 1
D 0.44 2
E 0.39 2
A 0.25 2
F -0.09 3
C -0.22 3
Table 6: The partial ordering computed with the provided
scores when r = 0.15.
Task EW HM TS Oracle
Czech?English 40.4 41.1 41.1 41.2
English?Czech 45.3 45.6 45.9 46.8
French?English 49.0 49.4 49.3 50.3
English?French 44.6 44.4 44.7 46.0
German?English 43.5 43.7 43.7 45.2
English?German 47.3 47.4 47.2 48.2
Hindi?English 62.5 62.2 62.5 62.6
English?Hindi 53.3 53.7 53.5 55.7
Russian?English 47.6 47.7 47.7 50.6
English?Russian 46.5 46.1 46.4 48.2
MEAN 48.0 48.1 48.2 49.2
Table 7: Accuracies for each method across 100 folds, for
each translation task. The oracle uses the most frequent out-
come between each pair of systems, and therefore might not
constitute a feasible ranking.
After training, each model has defined a partial
ordering over systems.
6
This is then used to com-
pute accuracy on all the pairwise judgments in the
test fold. This process yields 100 accuracies for
each method; the average accuracy across all the
folds can then be used to compute the best method.
Table 7 contains accuracy results for the three
methods on the WMT14 tasks. On average, there
is a small improvement in accuracy moving from
Expected Wins to the H&M model, and then again
to the TrueSkill model; however, there is no pat-
tern to the best model for each class. The Oracle
column is computed by selecting the most prob-
able outcome (pi ? {<,=, >}) for each system
pair, and provides an upper bound on accuracy
when predicting outcomes using only system-level
information. Furthermore, this method of oracle
computation might not represent a feasible rank-
ing or clustering,
7
.
The TrueSkill approach was best overall, so we
used it to produce the official rankings for all lan-
6
It is a total ordering when r = 0, or when all the system
scores are outside the decision radius.
7
For example, if there were a cycle of ?better than? judg-
ments among a set of systems.
guage pairs.
3.5 Rank Ranges and Clusters
Above we saw how to produce system scores for
each method, which provides a total ordering of
the systems. But we would also like to know if the
obtained system ranking is statistically significant.
Given the large number of systems that participate,
and the similarity of the underlying systems result-
ing from the common training data condition and
(often) toolsets, there will be some systems that
will be very close in quality. These systems should
be grouped together in equivalence classes.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute a TrueSkill model score for each system
based on this sample, and then rank the systems
from 1..|{S
j
}|. By repeating this procedure 1,000
times, we can determine a range of ranks, into
which system falls at least 95% of the time (i.e.,
at least 950 times) ? corresponding to a p-level
of p ? 0.05. Furthermore, given the rank ranges
for each system, we can cluster systems with over-
lapping rank ranges.
8
Table 8 reports all system scores, rank ranges,
and clusters for all language pairs and all systems.
The official interpretation of these results is that
systems in the same cluster are considered tied.
Given the large number of judgments that we col-
lected, it was possible to group on average about
two systems in a cluster, even though the systems
in the middle are typically in larger clusters.
3.6 Cluster analysis
The official ranking results for English-German
produced clusters compute at the 90% confidence
level due to the presence of a very large cluster
(of nine systems). While there is always the pos-
sibility that this cluster reflects a true ambiguity, it
is more likely due to the fact that we didn?t have
enough data: English?German had the most sys-
8
Formally, given ranges defined by start(S
i
) and end(S
i
),
we seek the largest set of clusters {C
c
} that satisfies:
?S ?C : S ? C
S ? C
a
, S ? C
b
? C
a
= C
b
C
a
6= C
b
? ?S
i
? C
a
, S
j
? C
b
:
start(S
i
) > end(S
j
) or start(S
j
) > end(S
i
)
21
Czech?English
# score range system
1 0.591 1 ONLINE-B
2 0.290 2 UEDIN-PHRASE
3 -0.171 3-4 UEDIN-SYNTAX
-0.243 3-4 ONLINE-A
4 -0.468 5 CU-MOSES
English?Czech
# score range system
1 0.371 1-3 CU-DEPFIX
0.356 1-3 UEDIN-UNCNSTR
0.333 1-4 CU-BOJAR
0.287 3-4 CU-FUNKY
2 0.169 5-6 ONLINE-B
0.113 5-6 UEDIN-PHRASE
3 0.030 7 ONLINE-A
4 -0.175 8 CU-TECTO
5 -0.534 9 COMMERCIAL1
6 -0.950 10 COMMERCIAL2
Russian?English
# score range system
1 0.583 1 AFRL-PE
2 0.299 2 ONLINE-B
3 0.190 3-5 ONLINE-A
0.178 3-5 PROMT-HYBRID
0.123 4-7 PROMT-RULE
0.104 5-8 UEDIN-PHRASE
0.069 5-8 YANDEX
0.066 5-8 ONLINE-G
4 -0.017 9 AFRL
5 -0.159 10 UEDIN-SYNTAX
6 -0.306 11 KAZNU
7 -0.487 12 RBMT1
8 -0.642 13 RBMT4
English?Russian
# score range system
1 0.575 1-2 PROMT-RULE
0.547 1-2 ONLINE-B
2 0.426 3 PROMT-HYBRID
3 0.305 4-5 UEDIN-UNCNSTR
0.231 4-5 ONLINE-G
4 0.089 6-7 ONLINE-A
0.031 6-7 UEDIN-PHRASE
5 -0.920 8 RBMT4
6 -1.284 9 RBMT1
German?English
# score range system
1 0.451 1 ONLINE-B
2 0.267 2-3 UEDIN-SYNTAX
0.258 2-3 ONLINE-A
3 0.147 4-6 LIMSI-KIT
0.146 4-6 UEDIN-PHRASE
0.138 4-6 EU-BRIDGE
4 0.026 7-8 KIT
-0.049 7-8 RWTH
5 -0.125 9-11 DCU-ICTCAS
-0.157 9-11 CMU
-0.192 9-11 RBMT4
6 -0.306 12 RBMT1
7 -0.604 13 ONLINE-C
French?English
# score range system
1 0.608 1 UEDIN-PHRASE
2 0.479 2-4 KIT
0.475 2-4 ONLINE-B
0.428 2-4 STANFORD
3 0.331 5 ONLINE-A
4 -0.389 6 RBMT1
5 -0.648 7 RBMT4
6 -1.284 8 ONLINE-C
English?French
# score range system
1 0.327 1 ONLINE-B
2 0.232 2-4 UEDIN-PHRASE
0.194 2-5 KIT
0.185 2-5 MATRAN
0.142 4-6 MATRAN-RULES
0.120 4-6 ONLINE-A
3 0.003 7-9 UU-DOCENT
-0.019 7-10 PROMT-HYBRID
-0.033 7-10 UA
-0.069 8-10 PROMT-RULE
4 -0.215 11 RBMT1
5 -0.328 12 RBMT4
6 -0.540 13 ONLINE-C
English?German
# score range system
1 0.264 1-2 UEDIN-SYNTAX
0.242 1-2 ONLINE-B
2 0.167 3-6 ONLINE-A
0.156 3-6 PROMT-HYBRID
0.155 3-6 PROMT-RULE
0.155 3-6 UEDIN-STANFORD
3 0.094 7 EU-BRIDGE
4 0.033 8-10 RBMT4
0.031 8-10 UEDIN-PHRASE
0.012 8-10 RBMT1
5 -0.032 11-12 KIT
-0.069 11-13 STANFORD-UNC
-0.100 12-14 CIMS
-0.126 13-15 STANFORD
-0.158 14-16 UU
-0.191 15-16 ONLINE-C
6 -0.307 17-18 IMS-TTT
-0.325 17-18 UU-DOCENT
Hindi?English
# score range system
1 1.326 1 ONLINE-B
2 0.559 2-3 ONLINE-A
0.476 2-4 UEDIN-SYNTAX
0.434 3-4 CMU
3 0.323 5 UEDIN-PHRASE
4 -0.198 6-7 AFRL
-0.280 6-7 IIT-BOMBAY
5 -0.549 8 DCU-LINGO24
6 -2.092 9 IIIT-HYDERABAD
English?Hindi
# score range system
1 1.008 1 ONLINE-B
2 0.915 2 ONLINE-A
3 0.214 3 UEDIN-UNCNSTR
4 0.120 4-5 UEDIN-PHRASE
0.054 4-5 CU-MOSES
5 -0.111 6-7 IIT-BOMBAY
-0.142 6-7 IPN-UPV-CNTXT
6 -0.233 8-9 DCU-LINGO24
-0.261 8-9 IPN-UPV-NODEV
7 -0.449 10-11 MANAWI-H1
-0.494 10-11 MANAWI
8 -0.622 12 MANAWI-RMOOV
Table 8: Official results for the WMT14 translation task. Systems are ordered by their inferred system means. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05, except for English?German, where p ? 0.1.
This method is also used to determine the range of ranks into which system falls. Systems with grey background indicate use
of resources that fall outside the constraints provided for the shared task.
22
tems (18, compared to 13 for the next languages),
yet only an average amount of per-system data.
Here, we look at this language pair in more detail,
in order to justify this decision, and to shed light
on the differences between the ranking methods.
Table 9 presents the 95% confidence-level clus-
terings for English?German computed with each
of the three methods, along with lines that show
the reorderings of the systems between them. Re-
orderings of this type have been used to argue
against the reliability of the official WMT rank-
ing (Lopez, 2012; Hopkins and May, 2013). This
table shows that these reorderings are captured en-
tirely by the clustering approach we used. This rel-
ative consensus of these independently-computed
and somewhat different models suggests that the
published ranking is approaching the true ambigu-
ity underlying systems within the same cluster.
Looking across all language pairs, we find that
the total ordering predicted by EW and TS is ex-
actly the same for eight of the ten language pair
tasks, and is constrained to reorderings within
the official cluster for the other two (German?
English ? just one adjacent swap ? and English?
German, depicted in Table 9).
3.7 Conclusions
The official ranking method employed by WMT
over the past few years has changed a few times as
a result of error analysis and introspection. Until
this year, these results were largely based on the
intuitions of the community and organizers about
deficiencies in the models. In addition to their in-
tuitive appeal, many of these changes (such as the
decision to throw out comparisons against refer-
ences) have been empirically validated Hopkins
and May (2013). The actual effect of the refine-
ments in the ranking metric has been minor pertur-
bations in the permutation of systems. The cluster-
ing method of Koehn (2012b), in which the official
rankings are presented as a partial (instead of to-
tal) ordering, alleviated many of the problems ob-
served by Lopez (2012), and also capture all the
variance across the new systems introduced this
year. In addition, presenting systems as clusters
appeals to intuition. As such, we disagree with
claims that there is a problem with irreproducibil-
ity of the results of the workshop evaluation task,
and especially disagree that there is anything ap-
proaching a ?crisis of confidence? (Hopkins and
May, 2013). These claims seem to us to be over-
stated.
Conducting proper model selection by compar-
ison on held-out data, however, is a welcome sug-
gestion, and our inclusion of this process supports
improved confidence in the ranking results. That
said, it is notable that the different methods com-
pute very similar orderings. This avoids hallu-
cinating distinctions among systems that are not
really there, and captures the intuition that some
systems are basically equivalent. The chief ben-
efit of the TrueSkill model is not in outputting a
better complete ranking of the systems, but lies in
its reduced variance, which allow us to cluster the
systems with less data. There is also the unex-
plored avenue of using TrueSkill to drive the data
collection, steering the annotations of judges to-
wards evenly matched systems during the collec-
tion phase, potentially allowing confident results
to be presented while collecting even less data.
There is, of course, more work to be done.
We have produced this year statistically significant
clusters with a third of the data required last year,
which is an improvement. Models of relative abil-
ity are a natural fit for the manual evaluation, and
the introduction of an online Bayesian approach
to data collection present further opportunities to
reduce the amount of data needed. These methods
also provide a framework for extending the models
in a variety of potentially useful ways, including
modeling annotator bias, incorporating sentence
metadata (such as length, difficulty, or subtopic),
and adding features of the sentence pairs.
4 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
third edition of the WMT shared task on qual-
ity estimation builds on the previous editions of
the task (Callison-Burch et al., 2012; Bojar et al.,
2013), with tasks including both sentence-level
and word-level estimation, with new training and
test datasets.
The goals of this year?s shared task were:
? To investigate the effectiveness of different
quality labels.
? To explore word-level quality prediction at
23
Expected Wins Hopkins & May TrueSkill
UEDIN-SYNTAX UEDIN-SYNTAX UEDIN-SYNTAX
ONLINE-B ONLINE-B ONLINE-B
ONLINE-A UEDIN-STANFORD ONLINE-A
UEDIN-STANFORD PROMT-HYBRID PROMT-HYBRID
PROMT-RULE ONLINE-A PROMT-RULE
PROMT-HYBRID PROMT-RULE UEDIN-STANFORD
EU-BRIDGE EU-BRIDGE EU-BRIDGE
RBMT4 UEDIN-PHRASE RBMT4
UEDIN-PHRASE RBMT4 UEDIN-PHRASE
RBMT1 RBMT1 RBMT1
KIT KIT KIT
STANFORD-UNC STANFORD-UNC STANFORD-UNC
CIMS CIMS CIMS
STANFORD STANFORD STANFORD
UU UU UU
ONLINE-C ONLINE-C ONLINE-C
IMS-TTT UU-DOCENT IMS-TTT
UU-DOCENT IMS-TTT UU-DOCENT
Table 9: A comparison of the rankings produced by Expected Wins, Hopkins & May, and TrueSkill for English?German (the
task with the most systems and the largest cluster). The lines extending all the way across mark the official English?German
clustering (computed from TrueSkill with 90% confidence intervals), while bold entries mark the start of new clusters within
each method or column (computed at the 95% confidence level). The TrueSkill clusterings contain all the system reorderings
across the other two ranking methods.
different levels of granularity.
? To study the effects of training and test
datasets with mixed domains, language pairs
and MT systems.
? To examine the effectiveness of quality pre-
diction methods on human translations.
Four tasks were proposed: Tasks 1.1, 1.2, 1.3
are defined at the sentence-level (Sections 4.1),
while Task 2, at the word-level (Section 4.2). Each
task provides one or more datasets with up to four
language pairs each: English-Spanish, English-
German, German-English, Spanish-English, and
up to four alternative translations generated by:
a statistical MT system (SMT), a rule-based MT
system (RBMT), a hybrid MT system, and a hu-
man. These datasets were annotated with differ-
ent labels for quality by professional translators as
part of the QTLaunchPad
9
project. External re-
sources (e.g. parallel corpora) were provided to
participants. Any additional resources, including
additional quality estimation training data, could
9
http://www.qt21.eu/launchpad/
be used by participants (no distinction between
open and close tracks is made). Participants were
also provided with a software package to extract
quality estimation features and perform model
learning, with a suggested list of baseline features
and learning method for sentence-level prediction.
Participants, described in Section 4.3, could sub-
mit up to two systems for each task.
Data used for building specific MT systems or
internal system information (such as n-best lists)
were not made available this year as multiple MT
systems were used to produced the datasets, in-
cluding rule-based systems. In addition, part of
the translations were produced by humans. Infor-
mation on the sources of translations was not pro-
vided either. Therefore, as a general rule, partici-
pants were only allowed to use black-box features.
4.1 Sentence-level Quality Estimation
For the sentence-level tasks, two variants of the
results could be submitted for each task and lan-
guage pair:
? Scoring: An absolute quality score for each
sentence translation according to the type of
24
prediction, to be interpreted as an error met-
ric: lower scores mean better translations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning).
Evaluation was performed against the true label
and/or HTER ranking using the same metrics as in
previous years:
? Scoring: Mean Average Error (MAE) (pri-
mary metric), Root Mean Squared Error
(RMSE).
? Ranking: DeltaAvg (primary metric) (Bojar
et al., 2013) and Spearman?s rank correlation.
For all sentence-level these tasks, the same 17
features as in WMT12-13 were used to build base-
line systems. The SVM regression algorithm
within QUEST (Specia et al., 2013)
10
was applied
for that with RBF kernel and grid search for pa-
rameter optimisation.
Task 1.1 Predicting post-editing effort
Data in this task is labelled with discrete and
absolute scores for perceived post-editing effort,
where:
? 1 = Perfect translation, no post-editing
needed at all.
? 2 = Near miss translation: translation con-
tains maximum of 2-3 errors, and possibly
additional errors that can be easily fixed (cap-
italisation, punctuation, etc.).
? 3 = Very low quality translation, cannot be
easily fixed.
The datasets were annotated in a ?triage? phase
aimed at selecting translations of type ?2? (near
miss) that could be annotated for errors at the
word-level using the MQM metric (see Task 2, be-
low) for a more fine-grained and systematic trans-
lation quality analysis. Word-level errors in trans-
lations of type ?3? are too difficult if not impos-
sible to annotate and classify, particularly as they
often contain inter-related errors in contiguous or
overlapping word spans.
10
http://www.quest.dcs.shef.ac.uk/
For the training of prediction models, we pro-
vide a new dataset consisting of source sen-
tences and their human translations, as well as
two-three versions of machine translations (by an
SMT system, an RBMT system and, for English-
Spanish/German only, a hybrid system), all in the
news domain, extracted from tests sets of various
WMT years and MT systems that participated in
the translation shared task:
# Source sentences # Target sentences
954 English 3,816 Spanish
350 English 1,400 German
350 German 1,050 English
350 Spanish 1,050 English
As test data, for each language pair and MT sys-
tem (or human translation) we provide a new set
of translations produced by the same MT systems
(and humans) as those used for the training data:
# Source sentences # Target sentences
150 English 600 Spanish
150 English 600 German
150 German 450 English
150 Spanish 450 English
The distribution of true scores in both training
and test sets for each language pair is given in Fig-
ures 3.
0%#
10%#
20%#
30%#
40%#
50%#
60%#
{en-
de-1
}#
{en-
de-2
}#
{en-
de-3
}#
{de-
en-1
}#
{de-
en-2
}#
{de-
en-3
}#
{en-
es-1
}#
{en-
es-2
}##
{en-
es-3
}##
{es-
en-1
}#
{es-
en-2
}#
{es-
en-3
}#
#Training##### #Test####
Figure 3: Distribution of true 1-3 scores by langauge pair.
Additionally, we provide some out of domain
test data. These translations were annotated in
the same way as above, each dataset by one Lan-
guage Service Provider (LSP), i.e, one profes-
sional translator, with two LPSs producing data in-
dependently for English-Spanish. They were gen-
erated using the LSPs? own source data (a different
domain from news), and own MT system (differ-
ent from the three used for the official datasets).
The results on these datasets were not considered
25
for the official ranking of the participating sys-
tems:
# Source sentences # Target sentences
971 English 971 Spanish
297 English 297 German
388 Spanish 388 English
Task 1.2 Predicting percentage of edits
In this task we use HTER (Snover et al., 2006) as
quality score. This score is to be interpreted as
the minimum edit distance between the machine
translation and its manually post-edited version,
and its range is [0, 1] (0 when no edit needs to
be made, and 1 when all words need to be edited).
We used TERp (default settings: tokenised, case
insensitive, etc., but capped to 1)
11
to compute the
HTER scores.
For practical reasons, the data is a subset of
Task 1.1?s dataset: only translations produced
by the SMT system English-Spanish. As train-
ing data, we provide 896 English-Spanish trans-
lation suggestions and their post-editions. As
test data, we provide a new set of 208 English-
Spanish translations produced by the same SMT
system. Each of the training and test translations
was post-edited by a professional translator using
the CASMACAT
12
web-based tool, which also col-
lects post-editing time on a sentence-basis.
Task 1.3 Predicting post-editing time
For this task systems are required to produce, for
each translation, a real valued estimate of the time
(in milliseconds) it takes a translator to post-edit
the translation. The training and test sets are a sub-
set of that uses in Task 1.2 (subject to filtering of
outliers). The difference is that the labels are now
the number of milliseconds that were necessary to
post-edit each translation.
As training data, we provide 650 English-
Spanish translation suggestions and their post-
editions. As test data, we provide a new set of 208
English-Spanish translations (same test data as for
Task 1.2).
4.2 Word-level Quality Estimation
The data for this task is based on a subset of the
datasets used for Task 1.1, for all language pairs,
11
http://www.umiacs.umd.edu/
?
snover/terp/
12
http://casmacat.eu/
human and machine translations: those transla-
tions labelled ?2? (near misses), plus additional
data provided by industry (either on the news do-
main or on other domains, such as technical doc-
umentation, produced using their own MT sys-
tems, and also pre-labelled as ?2?). All seg-
ments were annotated with word-level labels by
professional translators using the core categories
in MQM (Multidimensional Quality Metrics)
13
as
error typology (see Figure 4). Each word or se-
quence of words was annotated with a single error.
For (supposedly rare) cases where a decision be-
tween multiple fine-grained error types could not
be made, annotators were requested to choose a
coarser error category in the hierarchy.
Participants are asked to produce a label for
each token that indicates quality at different lev-
els of granularity:
? Binary classification: an OK / bad label,
where bad indicates the need for editing the
token.
? Level 1 classification: an OK / accuracy /
fluency label, specifying coarser level cate-
gories of errors for each token, or ?OK? for
tokens with no error.
? Multi-class classification: one of the labels
specifying the error type for the token (termi-
nology, mistranslation, missing word, etc.) in
Figure 4, or ?OK? for tokens with no error.
As training data, we provide tokenised transla-
tion output for all language pairs, human and ma-
chine translations, with tokens annotated with all
issue types listed above, or ?OK?. The annotation
was performed manually by professional transla-
tors as part of the QTLaunchPad project. For
the coarser variants, fine-grained errors are gen-
eralised to Accuracy or Fluency, or ?bad? for the
binary variant. The amount of available training
data varies by language pair:
# Source sentences # Target sentences
1,957 English 1,957 Spanish
715 English 715 German
350 German 350 English
900 Spanish 900 English
13
http://www.qt21.eu/launchpad/content/
training
26
Figure 4: MQM metric as error typology.
As test data, we provide additional data points
for all language pairs, human and machine trans-
lations:
# Source sentences # Target sentences
382 English 382 Spanish
150 English 150 German
100 German 100 English
150 Spanish 150 English
In contrast to Tasks 1.1?1.3, no baseline feature
set is provided to the participants.
Similar to last year (Bojar et al., 2013), the
word-level task is primarily evaluated by macro-
averaged F-measure (in %). Because the class dis-
tribution is skewed ? in the test data about 78% of
the tokens are marked as ?OK? ? we compute pre-
cision, recall, and F
1
for each class individually,
weighting F
1
scores by the frequency of the class
in the test data. This avoids giving undue impor-
tance to less frequent classes. Consider the follow-
ing confusion matrix for Level 1 annotation, i.e.
the three classes (O)K, (F)luency, and (A)ccuracy:
reference
O F A
predicted
O 4172 1482 193
F 1819 1333 214
A 198 133 69
For each of the three classes we assume a binary
setting (one-vs-all) and derive true-positive (tp),
false-positive (fp), and false-negative (fn) counts
from the rows and columns of the confusion ma-
trix as follows:
tp
O
= 4172
fp
O
= 1482 + 193 = 1675
fn
O
= 1819 + 198 = 2017
tp
F
= 1333
fp
F
= 1819 + 214 = 2033
fn
F
= 1482 + 133 = 1615
tp
A
= 69
fp
A
= 198 + 133 = 331
fn
A
= 193 + 214 = 407
We continue to compute F
1
scores for each
class c ? {O,F,A}:
precision
c
= tp
c
/(tp
c
+ fp
c
)
recall
c
= tp
c
/(tp
c
+ fn
c
)
F
1,c
=
2 ? precision
c
? recall
c
precision
c
+recall
c
yielding:
precision
O
= 4172/(4172 + 1675) = 0.7135
recall
O
= 4172/(4172 + 2017) = 0.6741
F
1,O
=
2 ? 0.7135 ? 0.6741
0.7135 + 0.6741
= 0.6932
? ? ?
F
1,F
= 0.4222
F
1,A
= 0.1575
Finally, we compute the average of F
1,c
scores
weighted by the occurrence count N(c) of c:
weightedF
1,ALL
=
1
?
c
N(c)
?
c
N
c
? F
1,c
weightedF
1,ERR
=
1
?
c:c 6=O
N(c)
?
c:c 6=O
N
c
? F
1,c
27
which for the above example gives:
weightedF
1,ALL
=
1
6189 + 2948 + 476
?
(6189 ? 0.6932 + 2948 ? 0.4222
+476 ? 0.1575) = 0.5836
weightedF
1,ERR
=
1
2948 + 476
?
(2948 ? 0.4222 + 476 ? 0.1575)
= 0.3854
We choose F
1,ERR
as our primary evaluation mea-
sure because it most closely mimics the common
application of F
1
scores in binary classification:
one is interested in the performance in detecting a
positive class, which in this case would be erro-
neous words. This does, however, ignore the num-
ber of correctly classified words of the OK class,
which is why we also report F
1,ALL
. In addition,
we follow Powers (2011) and report Matthews
Correlation Coefficient (MCC), averaged in the
same way as F
1
, as our secondary metric. Finally,
for contrast we also report Accuracy (ACC).
4.3 Participants
Table 10 lists all participating teams. Each team
was allowed up to two submissions for each task
and language pair. In the descriptions below, par-
ticipation in specific tasks is denoted by a task
identifier: T1.1, T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.2,
T1.3): QUEST is used to extract 17 system-
independent features from source and trans-
lation sentences and parallel corpora (same
features as in the WMT12 shared task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? language model (LM) probability of
source and target sentences based on
models for the WMT News Commen-
tary corpus.
? average number of translations per
source word in the sentence as given by
IBM Model 1 extracted from the WMT
News Commentary parallel corpus, and
thresholded so that P (t|s) > 0.2, or
so that P (t|s) > 0.01 weighted by the
inverse frequency of each word in the
source side of the parallel corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source language
extracted from the WMT News Com-
mentary corpus.
? percentage of unigrams in the source
sentence seen in the source side of the
WMT News Commentary corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within
the SCIKIT-LEARN toolkit. The ?,  and C
parameters were optimised via grid search
with 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as ?baseline?, it is in fact a strong
system. It has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting various forms of post-
editing effort (Callison-Burch et al., 2012;
Bojar et al., 2013).
DCU (T1.1): DCU-MIXED and DCU-SVR use
a selection of features available in QUEST,
such as punctuation statistics, LM perplex-
ity, n-gram frequency quartile statistics and
coarse-grained POS frequency ratios, and
four additional feature types: combined POS
and stop word LM features, source-side
pseudo-reference features, inverse glass-box
features for translating the translation and er-
ror grammar parsing features. For machine
learning, the QUEST framework is expanded
to combine logistic regression and support
vector regression and to handle cross- valida-
tion and randomisation in a way that training
items with the same source side are kept to-
gether. External resources are monolingual
corpora taken from the WMT 2014 transla-
tion task for LMs, the MT system used for the
inverse glass-box features (Li et al., 2014b)
and, for error grammar parsing, the Penn-
Treebank and an error grammar derived from
it (Foster, 2007).
28
ID Participating team
DCU Dublin City University Team 1, Ireland (Hokamp et al., 2014)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis,
2014)
FBK-UPV-UEDIN Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia,
Spain & University of Edinburgh, UK (Camargo de Souza et al., 2014)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al., 2014)
LIMSI Laboratoire d?Informatique pour la M?ecanique et les Sciences de l?Ing?enieur,
France (Wisniewski et al., 2014)
MULTILIZER Multilizer, Finland
RTM-DCU Dublin City University Team 2, Ireland (Bicici and Way, 2014)
SHEF-lite University of Sheffield Team 1, UK (Beck et al., 2014)
USHEFF University of Sheffield Team 2, UK (Scarton and Specia, 2014)
YANDEX Yandex, Russia
Table 10: Participants in the WMT14 Quality Estimation shared task.
DFKI (T1.2): DFKI/SVR builds upon the base-
line system (above) by adding non-redundant
data from the WMT13 task for predicting
the same label (HTER) and additional fea-
tures such as (a) rule-based language cor-
rections (language tool) (b), PCFG parsing
statistics and counts of tree labels, (c) po-
sition statistics of parsing labels, (d) posi-
tion statistics of trigrams with low probabil-
ity. DFKI/SVRxdata uses a similar setting,
with the addition of more training data from
non-minimally post-edited translation out-
puts (references), filtered based on a thresh-
old on the edit distance between the MT out-
put and the freely-translated reference.
FBK-UPV-UEDIN (T1.2, T1.3, T2): The sub-
missions for the word-level task (T2) use fea-
tures extracted from word posterior probabil-
ities and confusion network descriptors com-
puted over the 100k-best hypothesis transla-
tions generated by a phrase-based SMT sys-
tem. They also use features from word lexi-
cons, and POS tags of each word for source
and translation sentences. The predictions of
the Binary model are used as a feature for the
Level 1 and Multi-class settings. Both condi-
tional random fields (CRF) and bidirectional
long short-term memory recurrent neural net-
works (BLSTM-RNNs) are used for the Bi-
nary setting, and BLSTM-RNNs only for the
Level 1 and Multi-class settings.
The sentence-level QE submissions (T1.2
and T1.3) are trained on black-box features
extracted using QUEST in addition to fea-
tures based on word alignments, word poste-
rior probabilities and diversity scores (Souza
et al., 2013). These features are computed
over 100k-best hypothesis translations also
used for task 2. In addition, a set of ratios
computed from the word-level predictions of
the model trained on the binary setting of
task 2 is used. A total of 221 features and
the extremely randomised trees (Geurts et al.,
2006) learning algorithm are used to train re-
gression models.
LIG (T2): Conditional Random Fields classi-
fiers are trained with features used in LIG?s
WMT13 systems (Luong et al., 2013): tar-
get and source words, alignment informa-
tion, source and target alignment context,
LM scores, target and source POS tags,
lexical categorisations (stopword, punctua-
tion, proper name, numerical), constituent
label, depth in the constituent tree, target
polysemy count, pseudo reference. These
are combined with novel features: word
occurrence in multiple translation systems
and POS tag-based LM scores (longest tar-
get/source n-gram length and backoff score
for POS tag). These features require external
NLP tools and resources such as: TreeTag-
ger, GIZA++, Bekerley parser, Link Gram-
mar parser, WordNet and BabelNet, Google
Translate (pseudo-reference). For the binary
task, the optimal classification threshold is
tuned based on a development set split from
the original training set. Feature selection is
employed over the all features (for the binary
29
task only), with the Sequential Backward Se-
lection algorithm. The best performing fea-
ture set is then also used for the Level 1 and
Multi-class variants.
LIMSI (T2): The submission relies on a ran-
dom forest classifier and considers only 16
dense and continuous features. To prevent
sparsity issues, lexicalised information such
as the word or the previous word identities
is not included. The features considered are
mostly classic MT features and can be cat-
egorised into two classes: association fea-
tures, which describe the quality of the as-
sociation between the source sentence and
each target word, and fluency features, which
describe the ?quality? of the translation hy-
potheses. The latter rely on different lan-
guage models (either on POS or on words)
and the former on IBM Model 1 translation
probabilities and on pseudo- references, i.e.
translation produced by an independent MT
system. Random forests are known to per-
form well in tasks like this one, in which
only a few dense and continuous features are
available, possibly because of their ability to
take into account complex interactions be-
tween features and to automatically partition
the continuous feature values into a discrete
set of intervals that achieves the best classifi-
cation performance. Since they predict the
class probabilities, it is possible to directly
optimize the F
1
score during training by find-
ing, with a grid search method, the decision
threshold that achieved the best F
1
score on
the training set.
MULTILIZER (T1.2, T1.3): The 80 black-box
features from QUEST are used in addition to
new features based on using other MT en-
gines for forward and backward translations.
In forward translations, the idea is that dif-
ferent MT engines make different mistakes.
Therefore, when several forward translations
are similar to each other, these translations
are more likely to be correct. This is con-
firmed by the Pearson correlation of similar-
ities between the forward translations against
the true scores (above 0.5). A backward
translation is very error-prone and therefore
it has to be used in combination with for-
ward translations. A single back-translation
similar to original source segment does not
bring much information. Instead, when sev-
eral MT engines give back-translations simi-
lar to this source segment, one can conclude
that the translation is reliable. Those transla-
tions where similarities both in forward trans-
lation and backward translation are high are
intuitively more likely to be good. A simple
feature selection method that omits all fea-
tures with Pearson correlation against the true
scores below 0.2 is used. The systems sub-
mitted are obtained using linear regression
models.
RTM-DCU (T1.1, T1.2, T1.3, T2): RTM-DCU
systems are based on referential translation
machines (RTM) (Bic?ici, 2013) and parallel
feature decay algorithms (ParFDA5) (Bic?ici
et al., 2014), which allow language and MT
system-independent predictions. For each
task, individual RTM models are developed
using the parallel corpora and the language
model corpora distributed by the WMT14
translation task and the language model cor-
pora provided by LDC for English and Span-
ish. RTMs use 337 to 437 sentence-level fea-
tures for coverage and diversity, IBM1 and
sentence translation performance, retrieval
closeness and minimum Bayes retrieval risk,
distributional similarity and entropy, IBM2
alignment, character n-grams, sentence read-
ability, and parse output tree structures. The
features use ngrams defined over text or com-
mon cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over
which similarity calculations are performed.
Learning models include ridge regression
(RR), support vector machines (SVR), and
regression trees (TREE), which are applied
after partial least squares (PLS) or feature
selection (FS). For word-level prediction,
generalised linear models (GLM) (Collins,
2002) and GLM with dynamic learning
(GLMd) (Bic?ici, 2013) are used with word-
level features including CCL links, word
length, location, prefix, suffix, form, context,
and alignment, totalling up to a couple of mil-
lion features.
SHEF-lite (T1.1, T1.2, T1.3): These submis-
sions use the framework of Multi-task Gaus-
sian Processes, where multiple datasets are
30
combined in a multi-task setting similar to
the one used by Cohn and Specia (2013).
For T1.1, data for all language pairs is put
together, and each language is considered a
task. For T1.2 and T1.3, additional datasets
from previous shared task years are used,
each encoded as a different task. For all tasks,
the QUEST framework is used to extract a set
of 80 black-box features (a superset of the 17
baseline features). To cope with the large size
of the datasets, the SHEF-lite-sparse submis-
sion uses Sparse Gaussian Processes, which
provide sensible sparse approximations using
only a subset of instances (inducing inputs)
to speed up training and prediction. For this
?sparse? submission, feature selection is per-
formed following the approach of Shah et al.
(2013) by ranking features according to their
learned length-scales and selecting the top 40
features.
USHEFF (T1.1, T1.2, T1.3): USHEFF submis-
sions exploit the use of consensus among
MT systems by comparing the MT sys-
tem output to several alternative translations
generated by other MT systems (pseudo-
references). The comparison is done using
standard evaluation metrics (BLEU, TER,
METEOR, ROUGE for all tasks, and two
metrics based on syntactic similarities from
shallow and dependency parser information
for T1.2 and T1.3). Figures extracted from
such metrics are used as features to com-
plement prediction models trained on the 17
baseline features. Different from the standard
use of pseudo-reference features, these fea-
tures do not assume that the alternative MT
systems are better than the system of inter-
est. A more realistic scenario is considered
where the quality of the pseudo-references is
not known. For T1, no external systems in
addition to those provided for the shared task
are used: for a given translation, all alter-
native translations for the same source seg-
ment (two or three, depending on the lan-
guage pair) are used as pseudo-references.
For T1.2 and T1.3, for each source sentence,
all alternative translations produced by MT
systems on the same data (WMT12/13) are
used as pseudo-references. The hypothesis
is that by using translations from several MT
systems one can find consensual information
and this can smooth out the effect of ?coinci-
dences? in the similarities between systems?
translations. SVM regression with radial ba-
sis function kernel and hyper-parameters op-
timised via grid search is used to build the
models.
YANDEX (T1.1): Both submissions are based
on the the 80 black-box features, plus an
LM score from a larger language model,
a pseudo-reference, and several additional
features based on POS tags and syntactic
parsers. The first attempt uses an extract
of the top 5 features selected with a greedy
search from the set of all features. SVM re-
gression is used as machine learning algo-
rithm. The second attempt uses the same
features processed with Yandex? implemen-
tation of the gradient tree boosting (Ma-
trixNet).
4.4 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing effort
Table 11 summarises the results for the ranking
variant of Task 1.1. They are sorted from best to
worst using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are as follows: for English-Spanish
it is RTM-DCU/RTM-TREE, with a DeltaAvg
score of 0.26; for Spanish-English it is USH-
EFF, with a DeltaAvg score of 0.23; for English-
German it is again RTM-DCU/RTM-TREE, with a
DeltaAvg score of 0.39; and for German-English it
is RTM-DCU/RTM-RR, with a DeltaAvg score of
0.38. These winning submissions are better than
the baseline system by a large margin, which indi-
cates that current best performance in MT quality
estimation has reached levels that are clearly be-
yond what the baseline system can produce. As for
the other systems, according to DeltaAvg, com-
pared to the previous year results a smaller per-
centage of systems is able to beat the baseline.
This might be a consequence of the use of the met-
ric for the prediction of only three discrete labels.
The results for the scoring task are presented in
Table 12, sorted from best to worst using the MAE
31
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.26 0.38
? RTM-DCU/RTM-TREE 0.26 0.41
? YANDEX/SHAD BOOSTEDTREES2 0.23 0.35
USHEFF 0.21 0.33
SHEFF-lite 0.21 0.33
YANDEX/SHAD SVR1 0.18 0.29
SHEFF-lite-sparse 0.17 0.27
Baseline SVM 0.14 0.22
Spanish-English
? USHEFF 0.23 0.30
? RTM-DCU/RTM-PLS-RR 0.20 0.35
? RTM-DCU/RTM-FS-RR 0.19 0.36
Baseline SVM 0.12 0.21
SHEFF-lite-sparse 0.12 0.17
SHEFF-lite 0.11 0.15
English-German
? RTM-DCU/RTM-TREE 0.39 0.54
RTM-DCU/RTM-PLS-TREE 0.33 0.42
USHEFF 0.26 0.41
SHEFF-lite 0.26 0.36
Baseline SVM 0.23 0.34
SHEFF-lite-sparse 0.23 0.33
German-English
? RTM-DCU/RTM-RR 0.38 0.51
? RTM-DCU/RTM-PLS-RR 0.35 0.45
USHEFF 0.28 0.30
SHEFF-lite 0.24 0.27
Baseline SVM 0.21 0.25
SHEFF-lite-sparse 0.14 0.17
Table 11: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
32
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.49 0.61
? SHEFF-lite 0.49 0.63
? USHEFF 0.49 0.63
? SHEFF-lite/sparse 0.49 0.69
? RTM-DCU/RTM-TREE 0.49 0.61
Baseline SVM 0.52 0.66
YANDEX/SHAD BOOSTEDTREES2 0.56 0.68
YANDEX/SHAD SVR1 0.64 0.81
DCU-Chris/SVR 0.66 0.88
DCU-Chris/MIXED 0.94 1.14
Spanish-English
? RTM-DCU/RTM-FS-RR 0.53 0.64
? SHEFF-lite/sparse 0.54 0.69
? RTM-DCU/RTM-PLS-RR 0.55 0.71
USHEFF 0.57 0.67
Baseline SVM 0.57 0.68
SHEFF-lite 0.62 0.77
DCU-Chris/MIXED 0.65 0.91
English-German
? RTM-DCU/RTM-TREE 0.58 0.68
RTM-DCU/RTM-PLS-TREE 0.60 0.71
SHEFF-lite 0.63 0.74
USHEFF 0.64 0.75
SHEFF-lite/sparse 0.64 0.75
Baseline SVM 0.64 0.76
DCU-Chris/MIXED 0.69 0.98
German-English
? RTM-DCU/RTM-RR 0.55 0.67
? RTM-DCU/RTM-PLS-RR 0.57 0.74
USHEFF 0.63 0.76
SHEFF-lite 0.65 0.77
Baseline SVM 0.65 0.78
Table 12: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
33
metric scores as primary key and the RMSE metric
scores as secondary key.
The winning submissions for the scoring variant
of Task 1.1 are as follows: for English-Spanish it
is RTM-DCU/RTM-TREE with a MAE of 0.49;
for Spanish-English it is RTM-DCU/RTM-FS-
RR with a MAE of 0.53; for English-German
it is again RTM-DCU/RTM-TREE, with a MAE
of 0.58; and for German-English it is RTM-
DCU/RTM-RR with a MAE of 0.55. These sub-
missions are again much better than the baseline
system, which under the scoring variant seems
to perform at a middle-of-the-pack level or lower
compared to the overall pool of submissions.
Overall, more systems are able to outperform the
baseline according to the scoring metric.
The top system for most language pairs are
essentially based on the same core techniques
(RTM-DCU) according to both the DeltaAvg and
MAE metrics. The ranking of other systems, how-
ever, can be substantially different according to the
two metrics.
Task 1.2 Predicting percentage of edits
Table 13 summarises the results for the ranking
variant of Task 1.2. For readability purposes we
have used a multiplication-factor of 100 in the
scoring script, which makes the HTER numbers
(both predicted and gold) to be in the [0, 100]
range. They are sorted from best to worst using
the DeltaAvg metric scores as primary key and the
Spearman?s rank correlation scores as secondary
key.
The winning submission for the ranking vari-
ant of Task 1.2 is RTM-DCU/RTM-SVR, with a
DeltaAvg score of 9.31. There is a large mar-
gin between this score and the baseline score of
DeltaAvg 5.08, which indicates again that current
best performance has reached levels that are much
beyond what this baseline system can produce.
The vast majority of the submissions perform bet-
ter than the baseline (the only exception is the sub-
mission from SHEFF-lite, for which the authors
report a major issue with the learning algorithm).
The results for the scoring variant are presented
in Table 14, sorted from best to worst by using the
MAE metric scores as primary key and the RMSE
metric scores as secondary key.
The winning submission for the scoring variant
of Task 1.2 is FBK-UPV-UEDIN/WP with a MAE
of 12.89, while the baseline system has a MAE
of 15.23. Most of the submissions perform better
than the baseline.
Task 1.3 Predicting post-editing time
Table 15 summarises the results for the ranking
variant of Task 1.3. For readability purposes, we
have used a multiplication-factor of 0.001 in the
scoring script, which makes the time (both pre-
dicted and gold) to be measured in seconds. They
are sorted from best to worst using the DeltaAvg
metric scores as primary key and the Spearman?s
rank correlation scores as secondary key.
The winning submission for the ranking vari-
ant of Task 1.3 is RTM-DCU/RTM-RR, with a
DeltaAvg score of 17.02 (when predicting sec-
onds). The interesting aspect of these results is
that the DeltaAvg numbers have a direct real-
world interpretation, in terms of time spent (or
saved, depending on one?s view-point) for post-
editing machine-produced translations. A more
elaborate discussion on this point can be found in
Section 4.5.
The winning submission for the scoring variant
of Task 1.3 is RTM-DCU/RTM-SVR, with a MAE
of 16.77. Note that all of the submissions perform
significantly better than the baseline, which has a
MAE of 21.49, and that the majority is not signif-
icantly worse than the top scoring submission.
Task 2 Predicting word-level edits
The results for Task 2 are summarised in Tables
17?19. The results are ordered by F
1
score for
the Error (BAD) class. For comparison, two triv-
ial baselines are included, one that marks every
word as correct and that marks every word with
the most common error class found in the training
data. Both baselines are clearly useless for any ap-
plication, but help put the results in perspective.
Most teams submitted systems for a single lan-
guage pair: English-Spanish; only a single team
produced predictions for all four pairs.
Table 17 gives the results of the binary (OK vs.
BAD) classification variant of Task 2. The win-
ning submissions for this variant are as follows:
for English-Spanish it is FBK-UPV-UEDIN/RNN
with a weighted F
1
of 48.73; for Spanish-
English it is RTM-DCU/RTM-GLMd with a
weighted F
1
of 29.14; for English-German it is
RTM-DCU/RTM-GLM with a weighted F
1
of
45.30; and for German-English it is again RTM-
DCU/RTM-GLM with a weighted F
1
of 26.13.
Remarkably, for three out of four language
pairs, the systems fail to beat our trivial baseline of
34
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-SVR 9.31 0.53
? RTM-DCU/RTM-TREE 8.57 0.48
? USHEFF 7.93 0.45
SHEFF-lite/sparse 7.69 0.43
Baseline 5.08 0.31
SHEFF-lite 0.72 0.09
Table 13: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (100k times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
System ID MAE RMSE
English-Spanish
? FBK-UPV-UEDIN/WP 12.89 16.74
? RTM-DCU/RTM-SVR 13.40 16.69
? USHEFF 13.61 17.84
RTM-DCU/RTM-TREE 14.03 17.48
DFKI/SVR 14.32 17.74
FBK-UPV-UEDIN/NOWP 14.38 18.10
SHEFF-lite/sparse 15.04 18.38
MULTILIZER 15.04 20.86
Baseline 15.23 19.48
DFKI/SVRxdata 16.01 19.52
SHEFF-lite 18.15 23.41
Table 14: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-RR 17.02 0.68
? RTM-DCU/RTM-SVR 16.60 0.67
SHEFF-lite/sparse 16.33 0.63
SHEFF-lite 16.08 0.64
USHEFF 14.98 0.59
Baseline 14.71 0.57
Table 15: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
35
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-SVR 16.77 26.17
?MULTILIZER/MLZ2 17.07 25.83
? SHEFF-lite 17.13 27.33
?MULTILIZER/MLZ1 17.31 25.51
? SHEFF-lite/sparse 17.42 27.35
? FBK-UPV-UEDIN/WP 17.48 25.31
RTM-DCU/RTM-RR 17.50 25.97
FBK-UPV-UEDIN/NOWP 18.69 26.58
USHEFF 21.48 34.28
Baseline 21.49 34.28
Table 16: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
weighted F
1
F
1
System ID All Bad ? MCC ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 64.38
Baseline (always Bad) 18.71 52.53 0.00 35.62
? FBK-UPV-UEDIN/RNN 62.00 48.73 18.23 61.62
LIMSI/RF 60.55 47.32 15.44 60.09
LIG/FS 63.55 44.47 19.41 64.67
LIG/BL ALL 63.77 44.11 19.91 65.12
FBK-UPV-UEDIN/RNN+tandem+crf 62.17 42.63 16.32 63.26
RTM-DCU/RTM-GLM 60.68 35.08 13.45 63.74
RTM-DCU/RTM-GLMd 60.24 32.89 12.98 63.97
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 82.37
Baseline (always Bad) 5.28 29.98 0.00 17.63
? RTM-DCU/RTM-GLMd 79.54 29.14 25.47 82.98
RTM-DCU/RTM-GLM 79.42 26.91 25.93 83.43
English-German
Baseline (always OK) 59.39 0.00 0.00 71.33
Baseline (always Bad) 12.78 44.57 0.00 28.67
? RTM-DCU/RTM-GLM 71.51 45.30 28.61 72.97
RTM-DCU/RTM-GLMd 68.73 36.91 21.32 71.41
German-English
Baseline (always OK) 67.82 0.00 0.00 77.60
Baseline (always Bad) 8.20 36.60 0.00 22.40
? RTM-DCU/RTM-GLM 72.41 26.13 16.08 76.14
RTM-DCU/RTM-GLMd 71.42 22.97 12.63 75.46
Table 17: Official results for the binary part of the WMT14 Quality Evaluation Task 2. The winning submissions are indicated
by a ?. All values are given as percentages.
36
marking all the words as wrong. This may either
indicate that the predictions themselves are of low
quality or the chosen evaluation approach is mis-
leading. On the other hand F
1
scores are a com-
mon measure of binary classification performance
and no averaging is performed here.
Table 18 gives the results of the Level 1
classification (OK, Fluency, Accuracy) variant
of Task 2. Here the second baseline is to
always predict Fluency errors, as this is the
most common error category in the training
data. The winning submissions of this vari-
ant are as follows: for English-Spanish it
is FBK-UPV-UEDIN/RNN+tandem+crf with a
weighted F
1
of 23.94 and for Spanish-English,
English-German, and German-English it is RTM-
DCU/RTM-GLMd with weighted F
1
scores of
23.94, 21.94, and 8.57 respectively.
As before, all systems fail to outperform the
single-class baseline for the Spanish-English lan-
guage pair according to our primary metric. How-
ever, for Spanish-English and English-German
both submissions are able to beat the baseline by
large margin. We also observe that the absolute
numbers vary greatly between language pairs.
Table 19 gives the results of the Multi-class
classification variant of Task 2. Again, the sec-
ond baseline is to always predict the most common
error category in the training data, which varies
depending on language pair and produces and in-
creasingly weak baseline as the number of classes
rises.
The winning submissions of this variant are
as follows: for English-Spanish, Spanish-English,
and English-German it is RTM-DCU/RTM-GLM
with weighted F
1
scores of 26.84, 8.75, and 15.02
respectively and and for German-English it is
RTM-DCU/RTM-GLMd with a weighted F
1
of
3.08. Not only do these systems perform above
our baselines for all but the German-English lan-
guage pair, they also outperform all other sub-
missions for English-Spanish. Remarkably, RTM-
DCU/RTM-GLM wins English-Spanish for all of
the proposed metrics by a sizeable margin.
4.5 Discussion
In what follows, we discuss the main accomplish-
ments of this year?s shared task starting from the
goals we had previously identified for it.
Investigating the effectiveness of different
quality labels
For the sentence-level tasks, the results of this
year?s shared task allow us to investigate the ef-
fectiveness of predicting translation quality using
three very different quality labels: perceived post-
editing effort on a scale of [1-3] (Task 1.1); HTER
scores (Task 1.2); and the time that a translator
takes to post-edit the translation (Task 1.3). One of
the ways one can compare the effectiveness across
all these different labels is to look at how well
the models can produce predictions that correlate
with the gold label that we have at our disposal.
A measure of correlation that does not depend
on the value of the labels is Spearman?s ranking
correlation. From this perspective, the label that
seems the most effective appears to be post-editing
time (Task 1.3), with the best system (RTM-
DCU/RTM-RR) producing a Spearman?s ? of 0.68
(English-Spanish translations, see Table 15). In
comparison, when perceived post-editing effort la-
bels are used (Task 1.1), the best systems achieve
a Spearman?s ? of 0.38 and 0.30 for English-
Spanish and Spanish-English translations, respec-
tively, and ? of 0.54 and 0.51 for English-German
and German-English, respectively (Table 11); for
HTER scores (Task 1.2) the best systems achieve
a Spearman?s ? of 0.53 for English-Spanish trans-
lations (Table 13).
This comparison across tasks seems to indicate
that, among the three labels we have proposed,
post-editing time seems to be the most learnable,
in the sense that automatic predictions can vest
match the gold labels (in this case, with respect
to the rankings they induce). A possible reason
for this is that post-editing time correlates with the
length of the source sentence whereas HTER is a
normalised measure.
Compared to the results regarding time predic-
tion in the Quality Evaluation shared task from
2013 (Bojar et al., 2013), we note that this time
all submissions were able to beat the baseline sys-
tem (compared to only 1/3 of the submissions in
2013). In addition, better handling of the data
acquisition reduced the number of outliers in this
year?s dataset allowing for numbers that are more
reliably interpretable. As an example of its in-
terpretability, consider the following: the winning
submission for the ranking variant of Task 1.3 is
RTM-DCU/RTM-RR, with a a Spearman?s ? of
0.68 and a DeltaAvg score of 17.02 (when predict-
37
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always fluency) 14.39 40.41 0.00 0.00 30.67
? FBK-UPV-UEDIN/RNN+tandem+crf 58.36 38.54 16.63 13.89 57.98
FBK-UPV-UEDIN/RNN 60.32 37.25 18.22 15.51 61.75
LIG/BL ALL 58.97 31.79 14.95 11.48 61.13
LIG/FS 58.95 31.78 14.92 11.46 61.10
RTM-DCU/RTM-GLMd 58.23 26.62 12.60 12.76 62.94
RTM-DCU/RTM-GLM 56.47 29.91 8.11 7.96 58.56
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always fluency) 2.67 15.13 0.00 0.00 12.24
? RTM-DCU/RTM-GLMd 78.89 23.94 25.41 25.45 83.17
RTM-DCU/RTM-GLM 78.78 21.96 26.31 26.99 83.69
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always fluency) 3.83 13.35 0.00 0.00 14.82
? RTM-DCU/RTM-GLMd 64.58 21.94 17.69 15.92 69.26
RTM-DCU/RTM-GLM 64.43 21.10 16.99 14.93 69.34
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always fluency) 3.34 14.92 0.00 0.00 13.79
? RTM-DCU/RTM-GLMd 69.17 8.57 10.61 5.76 75.91
RTM-DCU/RTM-GLM 69.09 8.26 9.95 5.76 75.97
Table 18: Official results for the Level 1 classification part of the WMT14 Quality Evaluation Task 2. The winning submissions
are indicated by a ?. All values are given as percentages.
38
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always unintelligible) 7.93 22.26 0.00 0.00 21.99
? RTM-DCU/RTM-GLM 60.52 26.84 23.77 21.45 66.83
FBK-UPV-UEDIN/RNN+tandem+crf 52.96 23.07 15.17 10.74 52.13
LIG/BL ALL 56.66 20.50 18.56 13.39 60.39
LIG/FS 56.66 20.50 18.56 13.39 60.39
FBK-UPV-UEDIN/RNN 52.84 17.09 7.66 4.24 57.18
RTM-DCU/RTM-GLMd 51.87 3.22 10.16 4.04 64.42
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always word order) 0.34 1.96 0.00 0.00 4.24
? RTM-DCU/RTM-GLM 76.34 8.75 19.82 13.43 83.27
RTM-DCU/RTM-GLMd 76.21 8.19 19.35 15.32 83.17
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always mistranslation) 2.48 8.66 0.00 0.00 11.78
? RTM-DCU/RTM-GLM 63.57 15.02 17.57 15.08 70.82
RTM-DCU/RTM-GLMd 63.33 12.48 18.70 13.20 71.45
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always word order) 1.56 6.96 0.00 0.00 9.23
? RTM-DCU/RTM-GLMd 67.62 3.08 7.19 1.48 74.73
RTM-DCU/RTM-GLM 67.86 2.36 7.55 1.79 75.75
Table 19: Official results for the Multi-class classification part of the WMT14 Quality Evaluation Task 2. The winning
submissions are indicated by a ?. All values are given as percentages.
39
ing seconds). This number has a direct real-world
interpretation: using the order proposed by this
system, a human translator would spend, on av-
erage, about 17 seconds less on a sentence taken
from the top of the ranking compared to a sen-
tence picked randomly from the set.
14
To put this
number into perspective, for this dataset the av-
erage time to complete a sentence post-editing is
39 seconds. As such, one has an immediate inter-
pretation for the usefulness of using such a rank-
ing: translating around 100 sentences taken from
the top of the rankings would take around 36min
(at about 22 seconds/sentence), while translating
the same number of sentences extracted randomly
from the same dataset would take around 1h5min
(at about 39 seconds/sentence). It is in this sense
that we consider post-editing time an interpretable
label.
Another desirable property of label predictions
is usefulness; this property, however, it highly
task-dependent and therefore cannot be judged in
the absence of a specific task. For instance, an in-
terpretable label like post-editing time may not be
that useful in a task the requires one to place the
machine translations into ?ready to publish? and
?not ready to publish? bins. For such an appli-
cation, labels such as the ones used by Task 1.1
are clearly more useful, and also very much inter-
pretable within the scope of the task. Our attempt
at presenting the Quality Prediction task with a va-
riety of prediction labels illustrates a good range
of properties for the proposed labels and enables
one to draw certain conclusions depending on the
needs of the specific task at hand.
For the word-level tasks, different quality labels
equate with using different levels of granularity for
the predictions, which we discuss next.
Exploring word-level quality prediction at
different levels of granularity
Previous work on word-level predictions, e.g. (Bo-
jar et al., 2013) has focused on prediction of auto-
matically derived labels, generally due to practical
considerations as the manual annotation is labour
intensive. While easily applicable, automatic an-
notations, using for example TER alignment be-
tween the machine translation and reference (or
post-edition), face the same problems as automatic
14
Note that the 17.02 seconds figure is a difference in real-
time, not predicted time; what is considered in this variant of
Task 1.3 is only the predicted ranking of data points, not the
absolute values of the predictions.
MT evaluation metrics as they fail to account for
different word choices and lack the ability to re-
liably distinguish meaning preserving reorderings
from those that change the semantics of the out-
put. Furthermore, previous automatic annotation
for word-level quality estimation has focused on
binary labels: correct / incorrect, or at most, the
main edit operations that can be captured by align-
ment metrics like TER: correct, insertion, dele-
tion, substitution.
In this year?s task we were able to provide
manual fine-grained annotations at the word-level
produced by humans irrespective of references or
post-editions. Error categories range from fre-
quent ones, such as unintelligible, mistranslation,
and terminology, to rare ones such as additions or
omissions. For example, only 10 out of more than
3,400 errors in the English-Spanish test set fall
into the latter categories, while over 2,000 words
are marked as unintelligible. By hierarchically
grouping errors into coarser categories we aimed
to find a compromise between data sparsity and
the expressiveness of the labels. What marks a
good compromise depends on the use case, which
we do not specify here, and the quality of the finer
grained predictions: if a system is able to predict
even rare errors these may be grouped later if nec-
essary.
Overall, word-level error prediction seems to re-
main a challenging task as evidenced by the fact
that many submissions were unable to beat a triv-
ial baseline. We hypothesise that this is at least
partially due to a mismatch in loss-functions used
in training and testing. We know from the sys-
tem descriptions that some systems were tuned to
optimise squared error or accuracy, while evalua-
tion was performed using weighted F
1
scores. On
the other hand, even a comparison of just accuracy
shows that systems struggle to obtain a lower error
rates than the ?all-OK? baseline.
Such performance problems are consistent over
the three levels of granularity, contrary to the in-
tuition that binary classification would be easier.
A notable exception is the RTM-DCU/RTM-GLM
system, which is able to beat both the baseline and
all other systems on the Multi-Class variant of the
English-Spanish task ? cf. Table 19 ? with regard
to all metrics. For this and most other submis-
sions we observe that labels are not consistent for
different granularities, i.e. at token marked with a
specific error in the multi-class variant may still
40
carry an ?OK? label in binary annotation. Thus,
additional coarse grained annotations may be de-
rived by automatic means. For example, mapping
the multi-class predictions of the above system to
coarser categories improves the F
1,ERR
score in
Table 17 from 35.08 to 37.02 but does not change
the rank with respect to the other entries.
The fact that coarse grained predictions seem
not to be derived from the fine-grained ones leads
us to believe that most participants treated the
different granularities as independent classifica-
tion tasks. The FBK-UPV-UEDIN team trans-
fers information in the opposite direction by using
their binary predictions as features for Level-1 and
multi-class.
Given the current quality of word-level predic-
tion it remains unclear if these systems can already
be employed in a practical setting, e.g. to focus the
attention of post-editors.
Studying the effects of training and test
datasets with mixed domains, language pairs
and MT systems
This year?s shared task made available datasets for
more than one language pair with the same or dif-
ferent types of annotation, 2-3 multiple MT sys-
tems (plus a human translation) per language pair,
and out-of-domain test data (Tasks 1.1 and 2). In-
stances for each language pair were kept in sep-
arate datasets and thus the ?language pair? vari-
able can be analysed independently. However, for
a given language pair, datasets mix translation sys-
tems (and humans) in Task 1.1, and also text do-
mains in Task 2.
Directly comparing the performance across lan-
guage pairs is not possible, given that their
datasets have different numbers of instances (pro-
duced by 3 or 4 systems) and/or different true
score distributions (see Figure 3). For a relative
comparison (although not all systems submitted
results for all language pairs, which is especially
true in Task 2), we observe in Task 1.1 that for all
language pairs generally at least half of the sys-
tems did better than the baseline. To our surprise,
only one submission combined data for multiple
languages together for Task 1.1: SHEF-lite, treat-
ing each language pair data as a different task in
a multi-task learning setting. However, only for
the ?sparse? variant of the submission significant
gains were reported over modelling each task in-
dependently (with the tasks still sharing the same
data kernel and the same hyperparameters).
The interpretation of the results for Task 2 is
very dependent on the evaluation metric used,
but generally speaking a large variation in per-
formance was found between different languages,
with English-Spanish performing the best, possi-
bly given the much larger number of training in-
stances. Data for Task 2 also presented varied true
score distributions (as shown by the performance
of the baseline (e.g. always ?OK?) in Tables 17-
19.
One of the main goals with Task 1.1 (and Task 2
to some extent) was to test the robustness of mod-
els in a blind setting where multiple MT systems
(and human translations) are put together and their
identifiers are now known. All submissions for
these tasks were therefore translation system ag-
nostic, with no submission attempting to perform
meta-identification of the origins of the transla-
tions. For Task 1.1, data from multiple MT sys-
tems was explicitly used by USHEFF though the
idea of consensus translations. Translations from
all but the system of interest for the same source
segment were used as pseudo-references. The
submission significantly outperformed the base-
line for all language pairs and did particularly well
for Spanish-English and English-Spanish.
An in depth analysis of Task 1.1?s datasets on
the difference in prediction performance between
models built and applied for individual transla-
tion systems and models built and tested for all
translations pooled together is presented in (Shah
and Specia, 2014). Not surprisingly, the former
models perform significantly better, with MAE
scores ranging between 0.35 and 0.5 for differ-
ent language pairs and MT systems, and signifi-
cantly lower scores for models trained and tested
on human translations only (MAE scores between
0.2 and 0.35 for different language pairs), against
MAE scores ranging between 0.5 and 0.65 for
models with pooled data.
For Tasks 1.2 and 1.3, two submissions included
English-Spanish data which had been produced by
yet different MT systems (SHEF-lite and DFKI).
While using these additional instances seemed at-
tractive given the small number of instances avail-
able for these tasks, it is not clear what their contri-
bution was. For example, with a reduced set of in-
stances (only 400) from the combined sets, SHEF-
lite/sparse performed significantly better than its
variant SHEF-lite.
Finally, with respect to out-of-domain (different
41
text domain and MT system) test data, for Task
1.1, none of the papers submitted included experi-
ments. (Shah and Specia, 2014) applied the mod-
els trained on pooled datasets (as explained above)
for each language pair to the out-of-domain test
sets. The results were surprisingly positive, with
average MAE score of 0.5, compared to the 0.5-
0.65 range for in-domain data (see above). Further
analysis is necessary to understand the reasons for
that.
In Task 2, the official training and test sets al-
ready include out-of-domain data because of the
very small amount of in-domain data available,
and thus is is hard to isolate the effect of this data
on the results.
Examining the effectiveness of quality
prediction methods on human translations
Datasets for Tasks 1.1 and 2 contain human trans-
lations, in addition to the automatic translations
from various MT systems. Predicting human
translation quality is an area that has been largely
unexplored. Previous work has looked into dis-
tinguishing human from machine translations (e.g.
(Gamon et al., 2005)), but this problem setting is
somehow artificial, and moreover arguably harder
to solve nowadays given the higher general qual-
ity of current MT systems (Shah and Specia,
2014). Although human translations are obviously
of higher quality in general, many segments are
translated by MT systems with the same or similar
levels of quality as human translation. This is par-
ticularly true for Task 2, since data had been pre-
viously categorised and only ?near misses? were
selected for the word-level annotation, i.e., human
and machine translations that were both nearly
perfect in this case.
While no distinction was made between human
and machine translations in our tasks, we believe
the mix of these two types of translations has had
a negative impact in prediction performance. Intu-
itively, one can expect errors in human translation
to be more subtle, and hence more difficult to cap-
ture via standard quality estimation features. For
example, an incorrect lexical choice (due to, e.g.,
ambiguity) which still fits the context and does not
make the translation ungrammatical is unlikely to
be captured. We hoped that participants would de-
sign features for this particular type of translation,
but although linguistically motivated features have
been exploited, they did not seem appropriate for
human translations.
It is interesting to mention the indirect use of
human translations by USHEFF for Tasks 1.1-1.3:
given a translation for a source segment, all other
translations for the same segment were used as
pseudo-references. Apart from when this transla-
tion was actually the human translation, the hu-
man translation was effectively used as a refer-
ence. While this reference was mixed with 2-
3 other pseudo-references (other machine transla-
tions) for the feature computations, these features
led to significant gains in performance over the
baseline features Scarton and Specia (2014).
We believe that more investigation is needed for
human translation quality prediction. Tasks ded-
icated to this type of data at both sentence- and
word-level in the next editions of this shared task
would be a possible starting point. The acquisi-
tion of such data is however much more costly, as
it is arguably hard to find examples of low quality
human translation, unless specific settings, such as
translation learner corpora, are considered.
5 Medical Translation Task
The Medical Translation Task addresses the prob-
lem of domain-specific and genre-specific ma-
chine translation. The task is split into two sub-
tasks: summary translation, focused on transla-
tion of sentences from summaries of medical ar-
ticles, and query translation, focused on transla-
tion of queries entered by users into medical infor-
mation search engines.
In general, texts of specific domains and gen-
res are characterized by the occurrence of special
vocabulary and syntactic constructions which are
rare or even absent in traditional (general-domain)
training data and therefore difficult for MT. Spe-
cific training data (containing such vocabulary and
syntactic constructions) is usually scarce or not
available at all. Medicine, however, is an exam-
ple of a domain for which in-domain training data
(both parallel and monolingual) is publicly avail-
able in amounts which allow to train a complete
SMT system or to adapt an existing one.
5.1 Task Description
In the Medical Translation Task, we provided links
to various medical-domain training resources and
asked participants to use the data to train or adapt
their systems to translate unseen test sets for both
subtasks between English and Czech (CS), Ger-
man (DE), and French (FR), in both directions.
42
The summary translation test data is domain-
specific, but otherwise can be considered as ordi-
nary sentences. On the other hand, the query trans-
lation test data is also specific for its genre (gen-
eral style) ? it contains short sequences of (more
or less) of independent terms rather than complete
and grammatical sentences, the usual target of cur-
rent MT systems.
Similarly to the standard Translation Task, the
participants of the Medical Translation Task were
allowed to use only the provided resources in the
constrained task (in addition to data allowed in
the constrained standard Translation Task), but
could exploit any additional resources in the un-
constrained task. The submissions were expected
with true letter casing and detokenized. The trans-
lation quality was measured using automatic eval-
uation metrics, manual evaluation was not per-
formed.
5.2 Test and Development Data
The test and development data sets for this task
were provided by the EU FP7 project Khres-
moi.
15
This projects develops a multi-lingual
multi-modal search and access system for biomed-
ical information and documents and its MT com-
ponent allows users to use non-English queries to
search in English documents and see summaries
of retrieved documents in their preferred language
(Czech, German, or French). The statistics of the
data sets are presented in Tables 20 and 21.
For the summary translation subtask, 1,000
and 500 sentences were provided for test devel-
opment purposes, respectively. The sentences
were randomly sampled from automatically gen-
erated summaries (extracts) of English documents
(web pages) containing medical information rel-
evant to 50 topics provided for the CLEF 2013
eHealth Task 3.
16
Out-of-domain and ungram-
matical sentences were manually removed. The
sentences were then translated by medical experts
into Czech, German and French, and the transla-
tions were reviewed. Each sentence was provided
with the corresponding document ID and topic ID.
The set also included a description for each of the
50 topics. The data package (Khresmoi Summary
Translation Test Data 1.1) is now available from
the LINDAT/CLARIN repository
17
and more de-
15
http://khresmoi.eu/
16
https://sites.google.com/site/
shareclefehealth/
17
http://hdl.handle.net/11858/
tails can be found in Zde?nka Ure?sov?a and Pecina
(2014).
For the query translation subtask, the main
test set contains 1,000 queries for test and 508
queries for development purposes. The original
English queries were extracted at random from
real user query logs provided by the Health on the
Net foundation
18
(queries by general public) and
the Trip database
19
(queries by medical experts).
Each query was translated into Czech, German,
and French by medical experts and the transla-
tions were reviewed. The data package (Khresmoi
Query Translation Test Data 1.0) is available from
the LINDAT/CLARIN repository.
20
An additional test set for the query translation
subtask was adopted from the CLEF 2013 eHealth
Task 3 (Pecina et al., 2014). It contains 50 queries
constructed from titles of the test topics (originally
in English) translated into Czech, German, and
French by medical experts. The participants were
asked to translate the queries back to English and
the resulting translations were used in an informa-
tion retrieval (IR) experiment for extrinsic evalua-
tion.
5.3 Training Data
This section reviews the in-domain resources
which were allowed for the constrained Medical
Translation Task in addition to resources for the
constrained standard Translation Task (see Section
2). Most of the corpora are available for direct
download, others can be obtained upon registra-
tion. The corpora usually employ their own, more
or less complex data format. To lower the entry
barrier, we provided a set of easy-to-use scripts to
convert the data to a plain text format suitable for
MT training.
5.3.1 Parallel Training Data
The medical-domain parallel data includes the fol-
lowing corpora (see Table 22 for statistics): The
EMEA corpus (Tiedemann, 2009) contains doc-
uments from the European Medicines Agency,
automatically processed and aligned on sentence
level. It is available for many language pairs, in-
cluding those relevant to this task. UMLS is a
multilingual metathesaurus of health and biomed-
00-097C-0000-0023-866E-1
18
http://www.hon.ch/
19
http://www.tripdatabase.com/
20
http://hdl.handle.net/11858/
00-097C-0000-0022-D9BF-5
43
sents tokens
total Czech German French English
dev 500 9,209 9,924 12,369 10,350
test 1,000 19,191 20,831 26,183 21,423
Table 20: Statistics of summary test data.
queries tokens
total general expert Czech German French English
dev 508 249 259 1,128 1,041 1,335 1,084
test 1,000 500 500 2,121 1,951 2,490 2,067
Table 21: Statistics of query test data.
L1?L2 Czech?English DE?EN FR?EN
data set sents L1 tokens L2 tokens sents L1 tokens L2 tokens sents L1 tokens L2 tokens
EMEA 1,053 13,872 14,378 1,108 13,946 14,953 1,092 17,605 14,786
UMLS 1,441 4,248 5,579 2,001 6,613 8,153 2,171 8,505 8,524
Wiki 3 5 6 10 19 22 8 19 17
MuchMore 29 688 740
PatTr 1,848 102,418 106,727 2,201 127,098 108,665
COPPA 664 49,016 39,933
Table 22: Statistics of the in-domain parallel training data allowed for the constrained task (in thousands).
data set English Czech German French
PatTR 121,592 53,242 54,608
UMLS 7,991 63 24 37
Wiki 26,945 1,784 10,232 8,376
AACT 13,341
DrugBank 953
FMA 884
GENIA 557
GREC 62
PIL 662
Table 23: Sizes of monolingual training data allowed for the
constrained tasks (in thousands of tokens).
ical vocabularies and standards (U.S. National Li-
brary of Medicine, 2009). The UMLS dataset
was constructed by selecting the concepts which
have translations in the respective languages. The
Wiki dataset contains bilingual pairs of titles of
Wikipedia articles belonging to the categories
identified to be medical-domain within the Khres-
moi project. It is available for all three lan-
guage pairs. The MuchMore Springer Corpus
is a German?English parallel corpus of medical
journals abstracts published by Springer (Buitelaar
et al., 2003). PatTR is a parallel corpus extracted
from the MAREC patent collection (W?aschle and
Riezler, 2012). It is available for German?English
and French?English. For the medical domain,
we only consider text from patents indicated to
be from the medicine-related categories (A61,
C12N, C12P). COPPA (Corpus of Parallel Patent
Applications (Pouliquen and Mazenc, 2011) is a
French?English parallel corpus extracted from the
MAREC patent collection (W?aschle and Riezler,
2012). The medical-domain subset is identified by
the same categories as in PatTR.
5.3.2 Monolingual Training Data
The medical-domain monolingual data consists of
the following corpora (statistics are presented in
Table 23): The monolingual UMLS dataset con-
tains concept descriptions in CS, DE, and FR ex-
tracted from the UMLS Metathesaurus (see Sec-
tion 5.3.1). The monolingual Wiki dataset con-
sists of articles belonging to the categories iden-
tified to be medical-domain within the Khresmoi
project. The PatTR dataset contains non-parallel
data extracted from the medical patents included
in the PatTR corpus (see Section 5.3.1). AACT is a
collection of restructured and reformatted English
texts publicly available and downloadable from
ClinicalTrials.gov, containing clinical studies con-
ducted around the world. DrugBank is a bioin-
formatics and cheminformatics resource contain-
ing drug descriptions (Knox et al., 2011). GENIA
is a corpus of biomedical literature compiled and
annotated within the GENIA project (Kim et al.,
2003). FMA stands for the Foundational Model
of Anatomy Ontology, a knowledge source for
biomedical informatics concerned with symbolic
representation of the phenotypic structure of the
human body (Rosse and Mejino Jr., 2008). GREC
(Gene Regulation Event Corpus) is a semantically
annotated English corpus of abstracts of biomedi-
cal papers (Thompson et al., 2009). The PIL cor-
pus is a collection of documents giving instruc-
tions to patients about their medication (Bouayad-
Agha et al., 2000).
5.4 Participants
A total of eight teams participated in the Medical
Translation Task by submitting their systems to at
least one subtask for one or more translation direc-
tions. A list of the participants is given in Table 24;
we provide short descriptions of their systems in
the following.
CUNI was involved in the organization of the task,
and their primary goal was to set up a baseline for
both the subtasks and for all translation directions.
44
ID Participating team
CUNI Charles University in Prague (Du?sek et al., 2014)
DCU-Q Dublin City University (Okita et al., 2014)
DCU-S Dublin City University (Zhang et al., 2014)
LIMSI Laboratoire dInformatique pour la Mecanique et les Sciences de lIng?enieur (P?echeux et al., 2014)
POSTECH Pohang University of Science and Technology (Li et al., 2014a)
UEDIN University of Edinburgh (Durrani et al., 2014a)
UM-DA University of Macau (Wang et al., 2014)
UM-WDA University of Macau (Lu et al., 2014)
Table 24: Participants in the WMT14 Medical Translation Task.
Their systems are based on the Moses phrase-
based toolkit and linear interpolation of in-domain
and out-of-domain language models and phrase ta-
bles. The constrained/unconstrained systems dif-
fer in the training data only. The constrained
ones are built using all allowed training data; the
unconstrained ones take advantage of additional
web-crawled monolingual data used for training of
the language models, and additional parallel non-
medical data from the PatTr and COPPA patent
collections.
DCU-Q submitted a system designed specifically
for terminology translation in the query translation
task for EN?FR and FR?EN. This system supports
six terminology extraction methods and is able to
detect rare word pairs including zero-appearance
word pairs. It uses monotonic decoding with lat-
tice inputs, avoiding unnecessary hypothesis ex-
pansions by the reordering model.
DCU-S submitted a system to the FR?EN sum-
mary translation subtask only. The system is
similar to DCU?s system for patent translation
(phrased-based using Moses) but adapted to trans-
late medical summaries and reports.
LIMSI took part in the summary translation sub-
task for English to French.Their primary submis-
sion uses a combination of two translation sys-
tems: NCODE, based on bilingual n-gram trans-
lation models; and an on-the-fly estimation of
the parameters of Moses along with a vector
space model to perform domain adaptation. A
continuous-space language model is also used in
a post-processing step for each system.
POSTECH submitted a phrase-based SMT sys-
tem and query translation system for the DE?EN
language pair in both subtasks. They analysed
three types of query formation, generated query
translation candidates using term-to-term dictio-
naries and a phrase-based system, and then scored
them using a co-occurrence word frequency mea-
sure to select the best candidate.
UEDIN applied the Moses phrase-based system to
all language pairs and both subtasks. They used
the hierarchical reordering model and the OSM
feature, same as in UEDIN?s news translation sys-
tem, and applied compound splitting to German
input. They used separate language models built
on in-domain and out-of-domain data with linear
interpolation. For all language pairs except CS-
EN and DE-EN, they selected data for the transla-
tion model using modified Moore-Lewis filtering.
For DE-EN and CS-EN, they concatenated all the
supplied parallel training data.
UM-DA submitted systems for all language pairs
in the summary translation subtask based on a
combination of different adaptation steps, namely
domain-specific pre-processing, language model
adaptation, translation model adaptation, numeric
adaptation, and hyphenated word adaptation. Data
for the domain-adapted language and translation
models were selected using various data selection
techniques.
UM-WDA submitted systems for all language
pairs in the summary translation subtask. Their
systems are domain-adapted using web-crawled
in-domain resources: bilingual dictionaries and
monolingual data. The translation model and lan-
guage model trained on the crawled data were in-
terpolated with the best-performing language and
translation model employed in the UM-DA sys-
tems.
5.5 Results
MT quality in the Medical Translation Task
is evaluated using automatic evaluation metrics:
BLEU (Papineni et al., 2002), TER (Snover et al.,
2006), PER (Tillmann et al., 1997), and CDER
(Leusch et al., 2006). BLEU scores are reported as
percentage and all error rates are reported as one
minus the original value, also as percentage, so
that all metrics are in the 0-100 range, and higher
scores indicate better translations.
The main reason for not conducting human
evaluation, as it happens in the standard Trans-
45
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 29.64 29.79
?
1.07 47.45
?
1.15 61.64
?
1.06 52.18
?
0.98 31.68
?
1.14 49.84
?
1.10 64.38
?
1.06 54.10
?
0.96
CUNI 22.44 22.57
?
0.95 41.43
?
1.16 55.46
?
1.09 46.42
?
0.96 32.34
?
1.12 50.24
?
1.20 65.07
?
1.10 54.42
?
0.96
UEDIN 36.65 36.87
?
1.23 54.35
?
1.19 67.16
?
1.00 57.61
?
1.01 38.02
?
1.24 56.14
?
1.17 69.24
?
1.01 58.96
?
0.96
UM-DA 37.62 37.79
?
1.26 54.55
?
1.20 68.29
?
0.88 57.28
?
1.03 38.81
?
1.28 56.04
?
1.20 70.06
?
0.82 58.45
?
1.05
CUNI 22.92 23.06
?
0.97 42.49
?
1.10 56.10
?
1.12 47.13
?
0.95 33.18
?
1.15 51.48
?
1.15 66.00
?
1.03 55.30
?
0.96
CUNI 22.69 22.84
?
0.98 42.21
?
1.14 56.01
?
1.11 46.79
?
0.94 32.84
?
1.13 51.10
?
1.11 65.79
?
1.07 54.81
?
0.96
UM-WDA 37.35 37.53
?
1.26 54.39
?
1.19 68.21
?
0.83 57.16
?
1.07 38.61
?
1.27 55.92
?
1.17 70.02
?
0.81 58.36
?
1.07
ONLINE 39.57
?
1.21 58.24
?
1.14 70.16
?
0.78 60.04
?
1.02 40.62
?
1.23 59.72
?
1.11 71.94
?
0.74 61.26
?
1.01
German?English
CUNI 28.20 28.34
?
1.12 46.66
?
1.13 61.53
?
1.03 50.57
?
0.93 30.69
?
1.19 48.91
?
1.16 64.12
?
1.04 52.52
?
0.95
CUNI 28.85 28.99
?
1.15 47.12
?
1.15 61.98
?
1.07 50.72
?
0.98 31.37
?
1.21 49.29
?
1.13 64.53
?
1.05 52.64
?
0.98
POSTECH 25.92 25.99
?
1.06 43.66
?
1.14 59.62
?
0.92 47.13
?
0.90 26.97
?
1.06 45.13
?
1.12 61.53
?
0.89 48.37
?
0.88
UEDIN 37.31 37.53
?
1.19 55.72
?
1.14 68.82
?
0.99 58.35
?
0.95 38.60
?
1.25 57.18
?
1.12 70.46
?
0.98 59.53
?
0.94
UM-DA 35.71 35.81
?
1.23 53.08
?
1.16 66.82
?
0.98 55.91
?
0.96 36.55
?
1.27 54.01
?
1.13 68.05
?
0.97 56.78
?
0.95
CUNI 30.58 30.71
?
1.10 48.68
?
1.09 63.19
?
1.08 52.72
?
0.94 33.14
?
1.19 50.98
?
1.06 65.88
?
1.04 54.74
?
0.94
CUNI 30.22 30.32
?
1.12 47.71
?
1.18 62.20
?
1.10 52.17
?
0.91 32.75
?
1.20 50.00
?
1.14 64.87
?
1.06 54.19
?
0.92
UM-WDA 32.70 32.88
?
1.19 49.60
?
1.18 63.74
?
1.01 53.50
?
0.96 33.95
?
1.23 51.05
?
1.19 65.54
?
0.98 54.73
?
0.96
ONLINE 41.18
?
1.24 59.33
?
1.09 70.95
?
0.92 61.92
?
1.01 42.29
?
1.23 60.76
?
1.08 72.51
?
0.88 63.06
?
0.96
French?English
CUNI 34.42 34.55
?
1.20 52.24
?
1.17 64.52
?
1.03 56.48
?
0.91 36.52
?
1.23 54.35
?
1.12 67.07
?
1.00 58.34
?
0.91
CUNI 33.67 33.59
?
1.16 50.39
?
1.23 61.75
?
1.16 56.74
?
0.97 35.55
?
1.21 52.55
?
1.26 64.45
?
1.13 58.63
?
0.91
DCU-B 44.85 45.01
?
1.24 62.57
?
1.12 74.11
?
0.78 64.33
?
0.99 46.12
?
1.26 64.04
?
1.06 75.84
?
0.74 65.55
?
0.94
UEDIN 46.44 46.68
?
1.26 64.12
?
1.16 74.47
?
0.87 66.40
?
0.96 48.01
?
1.29 65.70
?
1.15 76.30
?
0.86 67.76
?
0.91
UM-DA 47.08 47.22
?
1.33 64.08
?
1.16 75.41
?
0.88 66.15
?
0.96 48.23
?
1.31 65.36
?
1.10 76.95
?
0.89 67.18
?
0.93
CUNI 34.74 34.89
?
1.12 52.39
?
1.16 63.76
?
1.09 57.29
?
0.94 36.84
?
1.17 54.56
?
1.13 66.43
?
1.07 59.14
?
0.90
CUNI 35.04 34.99
?
1.18 52.11
?
1.24 63.24
?
1.09 57.51
?
0.97 37.04
?
1.18 54.38
?
1.17 66.02
?
1.05 59.55
?
0.93
UM-WDA 43.84 44.06
?
1.32 61.14
?
1.18 73.13
?
0.87 63.09
?
1.00 45.17
?
1.36 62.63
?
1.15 74.94
?
0.84 64.37
?
0.99
ONLINE 46.99
?
1.35 64.31
?
1.12 76.07
?
0.78 66.09
?
1.00 47.99
?
1.33 65.65
?
1.07 77.65
?
0.75 67.20
?
0.96
English?Czech
CUNI 17.36 17.65
?
0.96 37.17
?
1.02 49.13
?
0.98 40.31
?
0.95 18.75
?
0.96 38.32
?
1.02 50.82
?
0.91 41.39
?
0.94
CUNI 16.64 16.89
?
0.93 36.57
?
1.05 48.79
?
0.98 39.46
?
0.90 17.94
?
0.96 37.74
?
1.03 50.50
?
0.97 40.59
?
0.91
UEDIN 23.45 23.74
?
1.00 44.20
?
1.10 55.38
?
0.88 46.23
?
0.99 24.20
?
1.00 44.92
?
1.08 56.38
?
0.90 46.78
?
1.00
UM-DA 22.61 22.72
?
0.98 42.73
?
1.16 54.12
?
0.93 44.73
?
1.01 23.12
?
1.01 43.41
?
1.14 55.11
?
0.93 45.32
?
1.02
CUNI 20.56 20.84
?
1.01 39.98
?
1.09 51.98
?
0.99 42.86
?
1.00 22.03
?
1.05 41.19
?
1.08 53.66
?
0.97 43.93
?
1.01
CUNI 19.50 19.72
?
0.97 38.09
?
1.10 50.12
?
1.06 41.50
?
0.96 20.91
?
1.02 39.26
?
1.12 51.79
?
1.04 42.59
?
0.96
UM-WDA 22.14 22.33
?
0.96 42.30
?
1.11 53.89
?
0.92 44.48
?
1.01 22.72
?
0.97 43.02
?
1.09 54.89
?
0.95 45.08
?
0.99
ONLINE 33.45
?
1.28 51.64
?
1.28 61.82
?
1.10 53.97
?
1.18 34.02
?
1.31 52.35
?
1.22 62.84
?
1.08 54.52
?
1.18
English?German
CUNI 12.52 12.64
?
0.77 29.84
?
0.99 45.38
?
1.14 34.69
?
0.81 16.63
?
0.91 33.63
?
1.07 50.03
?
1.24 38.43
?
0.87
CUNI 12.42 12.53
?
0.77 29.02
?
1.05 44.27
?
1.16 34.62
?
0.78 16.41
?
0.91 32.87
?
1.08 48.99
?
1.21 38.37
?
0.86
POSTECH 15.46 15.59
?
0.91 34.41
?
1.01 49.00
?
0.83 37.11
?
0.90 15.98
?
0.92 34.98
?
1.00 49.94
?
0.81 37.60
?
0.87
UEDIN 20.88 21.01
?
1.03 40.03
?
1.08 55.54
?
0.91 42.95
?
0.90 21.40
?
1.03 40.55
?
1.08 56.33
?
0.92 43.41
?
0.90
UM-DA 20.89 21.09
?
1.07 40.76
?
1.03 55.45
?
0.89 43.02
?
0.93 21.52
?
1.08 41.31
?
1.01 56.38
?
0.90 43.58
?
0.91
CUNI 14.29 14.42
?
0.81 31.82
?
1.03 47.01
?
1.13 36.81
?
0.79 18.87
?
0.90 35.76
?
1.11 51.76
?
1.17 40.65
?
0.87
CUNI 13.44 13.58
?
0.75 30.37
?
1.03 45.80
?
1.14 35.80
?
0.76 17.84
?
0.89 34.41
?
1.13 50.75
?
1.18 39.85
?
0.78
UM-WDA 18.77 18.91
?
1.00 37.92
?
1.02 53.59
?
0.85 40.90
?
0.86 19.30
?
1.02 38.42
?
1.01 54.40
?
0.85 41.34
?
0.86
ONLINE 23.92
?
1.06 44.33
?
0.97 57.47
?
0.80 46.35
?
0.91 24.29
?
1.07 44.83
?
0.98 58.20
?
0.80 46.71
?
0.92
English?French
CUNI 30.30 30.67
?
1.11 46.59
?
1.09 59.83
?
1.04 50.51
?
0.93 32.06
?
1.12 48.01
?
1.09 61.66
?
1.00 51.83
?
0.94
CUNI 29.35 29.71
?
1.10 45.84
?
1.07 58.81
?
1.04 50.00
?
0.96 31.02
?
1.10 47.24
?
1.09 60.57
?
1.02 51.31
?
0.94
LIMSI 40.14 43.54
?
1.22 59.70
?
1.04 69.45
?
0.86 61.35
?
0.96 44.04
?
1.22 60.32
?
1.03 70.20
?
0.85 61.90
?
0.94
LIMSI 38.83 42.21
?
1.13 58.88
?
1.01 68.70
?
0.81 60.59
?
0.93 42.69
?
1.12 59.53
?
0.98 69.50
?
0.80 61.17
?
0.91
UEDIN 40.74 44.24
?
1.16 60.66
?
1.07 70.35
?
0.82 62.28
?
0.95 44.85
?
1.17 61.43
?
1.05 71.27
?
0.81 62.94
?
0.91
UM-DA 41.24 41.68
?
1.12 58.72
?
1.06 69.37
?
0.78 60.12
?
0.95 42.16
?
1.11 59.39
?
1.05 70.21
?
0.77 60.71
?
0.92
CUNI 32.23 32.61
?
1.09 48.48
?
1.08 61.13
?
1.01 52.24
?
0.93 34.08
?
1.10 49.93
?
1.11 62.92
?
0.99 53.65
?
0.92
CUNI 32.45 32.84
?
1.06 48.68
?
1.06 61.32
?
0.98 52.35
?
0.94 34.22
?
1.07 50.09
?
1.04 63.04
?
0.96 53.67
?
0.91
UM-WDA 40.78 41.16
?
1.13 58.20
?
0.99 68.93
?
0.84 59.64
?
0.94 41.79
?
1.12 59.10
?
0.96 70.01
?
0.84 60.39
?
0.91
ONLINE 58.63
?
1.26 70.70
?
1.12 78.22
?
0.81 71.89
?
0.96 59.27
?
1.26 71.50
?
1.10 79.16
?
0.81 72.63
?
0.94
Table 25: Official results of translation quality evaluation in the medical summary translation subtask.
46
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 10.71 10.57
?
3.42 15.72
?
2.77 23.37
?
3.03 18.68
?
2.42 30.13
?
4.85 53.38
?
3.01 62.53
?
2.84 55.44
?
2.87
CUNI 9.92 9.78
?
3.04 16.84
?
2.84 23.80
?
3.08 19.85
?
2.40 28.21
?
4.56 54.15
?
3.04 62.56
?
2.99 55.91
?
2.79
UEDIN 24.66 24.68
?
4.52 39.88
?
3.05 49.97
?
3.29 41.81
?
2.80 28.25
?
4.94 45.31
?
3.14 55.66
?
3.06 46.67
?
2.77
CUNI 12.00 11.86
?
3.42 18.49
?
2.74 24.67
?
2.85 21.08
?
2.29 31.91
?
4.81 57.61
?
3.13 65.02
?
2.99 59.24
?
2.69
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
ONLINE 28.88
?
4.96 47.31
?
3.35 55.19
?
3.21 49.88
?
2.89 35.33
?
5.20 55.80
?
3.20 64.05
?
2.97 57.94
?
2.85
German?English
CUNI 10.90 10.74
?
3.41 18.89
?
2.39 26.09
?
2.00 20.29
?
2.07 32.15
?
5.23 55.56
?
2.90 63.68
?
2.34 56.45
?
2.62
CUNI 10.71 10.55
?
3.47 18.40
?
2.35 25.45
?
2.04 19.84
?
2.07 32.06
?
5.19 54.85
?
2.91 62.87
?
2.39 55.52
?
2.61
POSTECH 18.06 17.97
?
4.38 28.57
?
3.30 40.38
?
2.77 31.79
?
2.80 21.99
?
4.65 35.76
?
3.35 47.84
?
2.82 38.84
?
2.92
POSTECH 17.99 17.88
?
4.72 29.79
?
3.04 41.15
?
2.48 32.49
?
2.63 24.41
?
4.83 41.72
?
3.19 53.33
?
2.55 44.06
?
2.88
UEDIN 23.33 23.39
?
4.37 38.55
?
3.65 48.21
?
3.43 40.75
?
3.05 27.17
?
4.63 43.87
?
3.52 53.76
?
3.48 45.72
?
3.03
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
CUNI 8.75 8.49
?
3.60 19.10
?
2.27 24.98
?
1.95 19.95
?
2.02 30.00
?
5.59 56.07
?
2.92 62.92
?
2.32 56.27
?
2.56
ONLINE 19.97
?
4.46 37.03
?
3.26 43.91
?
3.22 40.95
?
2.93 33.86
?
4.87 53.28
?
3.28 60.86
?
3.22 56.33
?
2.98
French?English
CUNI 13.90 13.79
?
3.61 18.49
?
2.55 28.35
?
2.81 20.36
?
2.20 34.97
?
5.34 59.54
?
2.94 72.30
?
2.63 58.86
?
2.76
CUNI 12.10 11.95
?
3.41 17.23
?
2.57 27.12
?
2.88 19.15
?
2.28 33.74
?
5.01 58.95
?
2.96 71.25
?
2.76 58.20
?
2.81
DCU-Q 30.85 31.24
?
5.08 58.88
?
2.97 67.94
?
2.62 59.19
?
2.62 36.88
?
5.07 66.38
?
2.85 75.86
?
2.37 66.29
?
2.55
DCU-Q 26.51 26.16
?
4.40 48.02
?
3.72 57.34
?
3.24 53.56
?
2.79 28.61
?
4.52 53.65
?
3.73 63.51
?
3.21 59.07
?
2.79
UEDIN 27.20 27.60
?
3.98 38.54
?
3.22 48.81
?
3.26 39.77
?
2.95 32.23
?
4.27 43.66
?
3.20 54.31
?
3.17 44.53
?
2.79
CUNI 14.03 14.00
?
3.30 20.11
?
2.38 29.00
?
2.71 21.62
?
2.22 38.98
?
5.08 62.90
?
2.87 74.49
?
2.45 62.12
?
2.64
CUNI 13.38 13.16
?
3.52 17.79
?
2.56 28.84
?
2.81 19.17
?
2.23 35.00
?
5.20 59.52
?
2.98 73.08
?
2.57 58.41
?
2.68
ONLINE 32.96
?
5.04 53.68
?
3.21 64.27
?
2.80 54.40
?
2.66 38.09
?
5.52 61.44
?
3.08 72.59
?
2.61 61.60
?
2.78
English?Czech
CUNI 8.37 8.00
?
3.65 17.74
?
2.23 26.46
?
1.96 19.48
?
2.10 19.49
?
4.60 41.53
?
2.94 51.34
?
2.51 42.54
?
2.74
CUNI 9.04 8.75
?
3.64 18.25
?
2.27 26.97
?
1.92 19.69
?
2.11 21.46
?
5.05 42.36
?
3.09 51.99
?
2.40 43.18
?
2.68
UEDIN 12.57 12.40
?
3.61 21.15
?
2.96 33.56
?
2.80 22.30
?
2.67 14.06
?
3.80 24.92
?
2.90 37.85
?
2.72 25.58
?
2.70
UEDIN 6.64 6.21
?
4.73 -2.35
?
3.06 5.95
?
3.48 -0.97
?
3.12 14.35
?
3.52 14.51
?
3.19 24.96
?
3.50 15.11
?
3.10
CUNI 9.06 8.64
?
3.82 19.92
?
2.24 26.97
?
1.94 20.82
?
2.06 22.42
?
5.24 44.89
?
2.94 52.89
?
2.40 45.36
?
2.78
CUNI 8.49 8.01
?
6.05 18.13
?
2.28 25.19
?
1.86 19.19
?
2.01 21.04
?
4.80 42.66
?
2.87 50.34
?
2.47 43.30
?
2.74
ONLINE 21.09
?
4.60 48.56
?
2.82 54.72
?
2.51 48.30
?
2.83 24.37
?
4.80 51.93
?
2.74 58.10
?
2.50 51.62
?
2.80
English?German
CUNI 10.17 10.01
?
3.92 26.48
?
3.24 36.71
?
3.37 29.26
?
2.96 13.02
?
4.17 31.96
?
3.41 42.39
?
3.21 34.61
?
2.95
CUNI 9.98 9.69
?
3.94 26.16
?
3.19 35.50
?
3.23 28.86
?
2.94 12.90
?
4.28 31.75
?
3.33 41.24
?
3.21 34.38
?
3.05
POSTECH 13.43 13.01
?
5.91 26.38
?
3.09 35.75
?
3.16 27.86
?
2.82 15.05
?
5.71 30.45
?
3.10 39.89
?
3.14 31.79
?
3.00
POSTECH 13.41 13.15
?
5.21 22.18
?
3.09 30.89
?
3.31 24.17
?
3.06 14.96
?
5.15 26.13
?
3.19 34.92
?
3.40 27.98
?
3.12
UEDIN 10.45 10.14
?
3.86 23.44
?
3.43 34.55
?
3.34 25.46
?
3.17 11.91
?
4.42 27.91
?
3.45 39.08
?
3.42 29.63
?
3.31
CUNI 8.91 7.72
?
6.48 30.05
?
3.22 40.65
?
2.71 31.91
?
2.88 13.66
?
5.37 35.51
?
3.28 46.12
?
2.74 37.27
?
3.01
CUNI 9.14 8.69
?
6.44 27.66
?
3.31 37.95
?
3.45 31.00
?
2.82 14.03
?
5.92 33.53
?
3.45 44.03
?
3.53 36.73
?
3.00
ONLINE 20.07
?
6.06 41.07
?
3.23 47.41
?
2.86 41.61
?
3.02 21.67
?
6.23 43.78
?
3.23 50.18
?
2.95 44.26
?
3.06
English?French
CUNI 13.12 12.92
?
2.84 21.95
?
2.41 33.19
?
2.09 23.70
?
2.24 28.42
?
3.98 51.43
?
2.90 63.74
?
2.35 52.64
?
2.58
CUNI 12.80 12.65
?
2.81 19.16
?
2.61 31.61
?
2.21 21.91
?
2.32 27.52
?
4.05 47.47
?
3.08 61.43
?
2.37 49.82
?
2.72
DCU-Q 27.69 27.84
?
4.11 48.97
?
3.06 60.90
?
2.55 51.84
?
2.83 28.98
?
4.16 51.73
?
3.10 63.84
?
2.47 54.43
?
2.76
UEDIN 20.16 21.76
?
3.42 31.66
?
4.23 44.37
?
4.13 44.29
?
2.73 23.25
?
3.49 35.38
?
4.19 48.52
?
4.07 47.94
?
2.75
CUNI 13.78 13.57
?
3.00 21.92
?
2.51 33.47
?
2.03 24.16
?
2.32 30.07
?
4.10 51.12
?
3.08 63.61
?
2.45 52.96
?
2.67
CUNI 15.27 15.24
?
3.12 23.58
?
2.54 34.39
?
2.54 25.79
?
2.32 31.40
?
4.15 53.60
?
2.96 65.39
?
2.57 55.47
?
2.69
ONLINE 28.93
?
3.66 49.20
?
3.08 60.85
?
2.69 51.68
?
2.78 30.88
?
3.66 52.25
?
3.08 64.06
?
2.62 54.59
?
2.68
Table 26: Official results of translation quality evaluation in the medical query translation subtask.
source lang. ID P@5 P@10 NDCG@5 NDCG@10 MAP Rprec bpref rel
Czech?English CUNI 0.3280 0.3340 0.2873 0.2936 0.2217 0.2362 0.3473 1461
German?English CUNI 0.2800 0.3000 0.2467 0.2630 0.2057 0.2077 0.3310 1426
French?English CUNI 0.3280 0.3380 0.2811 0.2882 0.2206 0.2284 0.3504 1481
DCU-Q 0.3480 0.3460 0.3060 0.3072 0.2252 0.2358 0.3659 1524
UEDIN 0.4440 0.4300 0.3793 0.3826 0.2843 0.2935 0.3936 1544
English (monolingual) 0.4600 0.4700 0.4091 0.4205 0.3035 0.3198 0.3858 1638
Table 27: Official results of retrieval evaluation in the query translation subtask.
47
lation Task, was the lack of domain expertise of
prospective raters. While in the standard task, the
only requirement for the raters was to be a na-
tive speaker of the target language, in the Med-
ical Translation Task, a very good knowledge of
the domain would be necessary to provide reli-
able judgements and the raters with such an ex-
pertise (medical doctors and native speakers) were
not available.
The complete results of the task are presented
in Table 25 (for summary translation) and Ta-
bles 26 and 27 (for query translation). Partici-
pant IDs given in bold indicate primary submis-
sions, IDs in normal font refer to contrastive sub-
missions. The first section for each translation di-
rection (white background) refers to constrained
submissions and the second one (light-gray back-
ground) to unconstrained submissions. The col-
umn denoted as ?original? contains BLEU scores
as reported by the Matrix submission system ob-
tained on the original submitted translations. Due
to punctuation inconsistency in the original refer-
ence translations, we decided to perform punctu-
ation normalization before calculating the official
scores. The columns denoted as ?normalized true-
cased? contain scores obtained on the submitted
translations after punctuation normalization and
the columns denoted as ?normalized lowercased?
contain scores obtained after punctuation normal-
ization and lowercasing. The normalization script
is available in the package with summary transla-
tion test data. The confidence intervals were ob-
tained by bootstrap resampling with a confidence
level of 95%. Figures in bold denote the best con-
strained system and, if its score is higher, the best
unconstrained system for each translation direc-
tion and each metric. For comparison, we also
present results of a major on-line translation sys-
tem (denoted as ONLINE).
The results of the extrinsic evaluation of query
translation submissions are given in 27. We used
the CLEF 2013 eHealth Task 3 test collection con-
taining about 1 million web pages (in English),
50 test queries (originally in English and trans-
lated to Czech, German, and French), and their
relevance assessments. Some of the participants
of the WMT Medical Task (three teams with five
submissions in total) submitted translations of the
queries (from Czech, German, and French) into
English and these translations were used to query
the CLEF 2013 eHealth Task 3 test collection us-
ing a state-of-the-art system based on a BM25
model, described in Pecina et al. (2014). Origi-
nally, we asked for 10 best translations for each
query, but only the best one were used for the
evaluation. The results are provided in terms of
standard IR evaluation measures: precision at a
cut-off of 5 and 10 documents (P@5, P@10),
normalized discounted cumulative gain (J?arvelin
and Kek?al?ainen, 2002) at 5 and 10 documents
(NDCG@5, NDCG@10), mean average precision
(MAP) (Voorhees and Harman, 2005), precision
reached after R documents retrieved, where R in-
dicates the number of the relevant documents for
each query in the entire collection (Rprec), binary
preference (bpref) (Buckley and Voorhees, 2004),
and number or relevant documents retrieved (rel).
The cross-lingual results are also compared with
the monolingual one (obtained by using the refer-
ence (English) translations of the test topics) to see
how the system would perform if the queries were
translated perfectly.
5.6 Discussion and Conclusion
Both the subtasks turned out to be quite challeng-
ing not only because of the specific domain ? in
summary sentences, we can observe much higher
density of terminology than in ordinary sentences;
the queries, which are also rich in terminology, do
not form sentences at all.
Most submissions were based on systems par-
ticipating in the standard Translation Task and
trained on the provided data or its subsets CUNI
provided baseline systems for all language pairs in
both subtasks, which turned to be relatively strong
for the query translation task, especially in trans-
lation to English, but only in terms of scores ob-
tained on normalized and lowercased translations
since their truecasing component did not perform
well.
In the summary translation subtask, the best
overall results were achieved by the UEDIN team
which won for DE?EN, EN?CS, and EN?FR, fol-
lowed by the UM-DA team, which performed on
par with UEDIN in all other translation.
The unconstrained submissions in almost all
cases did not outperform the results of the con-
strained submissions. Some improvements were
observed in the query translations subtasks by the
CUNI?s unconstrained system with language mod-
els trained on larger in-domain data.
The ONLINE system outperforms all other sub-
48
missions with only two exceptions ? the UM-DA?s
and UEDIN?s systems for the summary translation
in the FR?EN direction, though the score differ-
ences are within the 95% confidence interval.
In the query translation subtask, DCU-Q built
a system designed specifically for terminology
translation between French and English and out-
performed all other participants in translation into
English; however, the confidence intervals in the
query translation task are much wider and most of
the differences in scores of the automatic metrics
are not statistically significant.
The extrinsic evaluation in the cross-lingual in-
formation retrieval was conducted for translations
into English only. CUNI provided the baselines
for all directions, but other submissions were done
for FR?EN only. Here, the winner is UEDIN, who
outperformed both CUNI and DCU-Q, and their
scores are very close to those obtained using the
reference English translations.
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Yandex.
We would also like to thank our colleagues Ma-
tou?s Mach?a?cek and Martin Popel for detailed dis-
cussions.
References
Avramidis, E. (2014). Efforts on machine learning
over human-mediated translation edit rate. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Beck, D., Shah, K., and Specia, L. (2014). Shef-
lite 2.0: Sparse multi-task gaussian processes
for translation quality estimation. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Bic?ici, E. (2013). Referential translation machines
for quality estimation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, Sofia, Bulgaria.
Bic?ici, E., Liu, Q., and Way, A. (2014). Parallel
FDA5 for fast deployment of accurate statisti-
cal machine translation systems. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, USA. Association
for Computational Linguistics.
Bicici, E., Liu, Q., and Way, A. (2014). Parallel
fda5 for fast deployment of accurate statistical
machine translation systems. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bicici, E. and Way, A. (2014). Referential transla-
tion machines for predicting translation quality.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Bojar, O., Buck, C., Callison-Burch, C., Feder-
mann, C., Haddow, B., Koehn, P., Monz, C.,
Post, M., Soricut, R., and Specia, L. (2013).
Findings of the 2013 Workshop on Statistical
Machine Translation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 1?42, Sofia, Bulgaria. Association
for Computational Linguistics.
Bojar, O., Diatka, V., Rychl?y, P., Stra?n?ak, P.,
Tamchyna, A., and Zeman, D. (2014). Hindi-
English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth Inter-
national Language Resources and Evaluation
Conference, Reykjavik, Iceland. ELRA.
Bojar, O., Ercegov?cevi?c, M., Popel, M., and
Zaidan, O. (2011). A grain of salt for the WMT
manual evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation,
pages 1?11, Edinburgh, Scotland. Association
for Computational Linguistics.
Borisov, A. and Galinskaya, I. (2014). Yandex
school of data analysis russian-english machine
translation system for wmt14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bouayad-Agha, N., Scott, D. R., and Power, R.
(2000). Integrating content and style in doc-
uments: A case study of patient information
leaflets. Information Design Journal, 9(2?
3):161?176.
Buckley, C. and Voorhees, E. M. (2004). Re-
trieval evaluation with incomplete information.
49
In Proceedings of the 27th Annual International
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 25?
32, Sheffield, United Kingdom.
Buitelaar, P., Sacaleanu, B.,
?
Spela Vintar, Stef-
fen, D., Volk, M., Dejean, H., Gaussier, E.,
Widdows, D., Weiser, O., and Frederking, R.
(2003). Multilingual concept hierarchies for
medical information organization and retrieval.
Public deliverable, MuchMore project.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Gonz?alez-Rubio, J.,
Buck, C., Turchi, M., and Negri, M. (2014).
Fbk-upv-uedin participation in the wmt14 qual-
ity estimation shared-task. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Cap, F., Weller, M., Ramm, A., and Fraser, A.
(2014). Cims ? the cis and ims joint submis-
sion to wmt 2014 translating from english into
german. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling an-
notator bias with multi-task gaussian processes:
An application to machine translation quality
estimation. In Proceedings of the 51st An-
nual Meeting of the Association for Compu-
tational Linguistics, ACL-2013, pages 32?42,
Sofia, Bulgaria.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Costa-juss`a, M. R., Gupta, P., Rosso, P., and
Banchs, R. E. (2014). English-to-hindi sys-
tem description for wmt 2014: Deep source-
context features for moses. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Do, Q. K., Herrmann, T., Niehues, J., Allauzen,
A., Yvon, F., and Waibel, A. (2014). The
kit-limsi translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Dungarwal, P., Chatterjee, R., Mishra, A.,
Kunchukuttan, A., Shah, R., and Bhattacharyya,
P. (2014). The iit bombay hindi-english transla-
tion system at wmt 2014. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
50
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014a). Edinburgh?s phrase-based machine
translation systems for wmt-14. In Proceedings
of the ACL 2014 Ninth Workshop of Statistical
Machine Translation, Baltimore, USA.
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014b). Edinburghs phrase-based machine
translation systems for wmt-14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Du?sek, O., Haji?c, J., Hlav?a?cov?a, J., Nov?ak, M.,
Pecina, P., Rosa, R., Tamchyna, A., Ure?sov?a,
Z., and Zeman, D. (2014). Machine transla-
tion of medical texts in the khresmoi project. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Foster, J. (2007). Treebanks gone bad: Parser eval-
uation and retraining using a treebank of un-
grammatical sentences. International Journal
on Document Analysis and Recognition, 10(3-
4):129?145.
Freitag, M., Peitz, S., Wuebker, J., Ney, H., Huck,
M., Sennrich, R., Durrani, N., Nadejde, M.,
Williams, P., Koehn, P., Herrmann, T., Cho,
E., and Waibel, A. (2014). Eu-bridge mt:
Combined machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Gamon, M., Aue, A., and Smets, M. (2005).
Sentence-level MT evaluation without reference
translations: beyond language modeling. In
Proceedings of the Annual Conference of the
European Association for Machine Translation,
Budapest.
Geurts, P., Ernst, D., and Wehenkel, L. (2006). Ex-
tremely randomized trees. Machine Learning,
63(1):3?42.
Green, S., Cer, D., and Manning, C. (2014).
Phrasal: A toolkit for new directions in statis-
tical machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hardmeier, C., Stymne, S., Tiedemann, J., Smith,
A., and Nivre, J. (2014). Anaphora models and
reordering for phrase-based smt. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Herbrich, R., Minka, T., and Graepel, T. (2006).
TrueSkill
TM
: A Bayesian Skill Rating Sys-
tem. In Proceedings of the Twentieth Annual
Conference on Neural Information Processing
Systems, pages 569?576, Vancouver, British
Columbia, Canada. MIT Press.
Herrmann, T., Mediani, M., Cho, E., Ha, T.-L.,
Niehues, J., Slawik, I., Zhang, Y., and Waibel,
A. (2014). The karlsruhe institute of technol-
ogy translation systems for the wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Hokamp, C., Calixto, I., Wagner, J., and Zhang,
J. (2014). Target-centric features for transla-
tion quality estimation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hopkins, M. and May, J. (2013). Models of trans-
lation competitions. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1416?1424, Sofia, Bulgaria.
J?arvelin, K. and Kek?al?ainen, J. (2002). Cumu-
lated gain-based evaluation of ir techniques.
ACM Transactions on Information Systems,
20(4):422?446.
Kim, J.-D., Ohta, T., Tateisi, Y., and Tsujii, J.
(2003). GENIA corpus ? a semantically anno-
tated corpus for bio-textmining. Bioinformatics,
19(suppl 1):i180?i182.
Knox, C., Law, V., Jewison, T., Liu, P., Ly,
S., Frolkis, A., Pon, A., Banco, K., Mak, C.,
Neveu, V., Djoumbou, Y., Eisner, R., Guo,
A. C., and Wishart, D. S. (2011). DrugBank 3.0:
a comprehensive resource for Omics research
on drugs. Nucleic acids research, 39(suppl
1):D1035?D1041.
Koehn, P. (2012a). Simulating human judgment in
51
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. (2012b). Simulating Human Judgment
in Machine Translation Evaluation Campaigns.
In Proceedings of the Ninth International Work-
shop on Spoken Language Translation, pages
179?184, Hong Kong, China.
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Koppel, M. and Ordan, N. (2011). Translationese
and its dialects. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Techolo-
gies, pages 1318?1326, Portland, Oregon.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Leusch, G., Ueffing, N., and Ney, H. (2006). Cder:
Efficient mt evaluation using block movements.
In Proceedings of the 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 241?248, Trento,
Italy.
Li, J., Kim, S.-J., Na, H., and Lee, J.-H. (2014a).
Postech?s system description for medical text
translation task. In Proceedings of the ACL
2014 Ninth Workshop of Statistical Machine
Translation, Baltimore, USA.
Li, L., Wu, X., Vaillo, S. C., Xie, J., Way, A., and
Liu, Q. (2014b). The dcu-ictcas mt system at
wmt 2014 on german-english translation task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Lopez, A. (2012). Putting Human Assessments of
Machine Translation Systems in Order. In Pro-
ceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 1?9, Montr?eal,
Canada. Association for Computational Lin-
guistics.
Lu, Y., Wang, L., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira, F. (2014). Domain adapta-
tion for medical text translation using web re-
sources. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
Luong, N. Q., Besacier, L., and Lecouteux, B.
(2014). Lig system for word level qe task at
wmt14. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Mach?a?cek, M. and Bojar, O. (2014). Results of
the wmt14 metrics shared task. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Matthews, A., Ammar, W., Bhatia, A., Feely, W.,
Hanneman, G., Schlinger, E., Swayamdipta, S.,
Tsvetkov, Y., Lavie, A., and Dyer, C. (2014).
The cmu machine translation systems at wmt
2014. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Neidert, J., Schuster, S., Green, S., Heafield, K.,
and Manning, C. (2014). Stanford universitys
submissions to the wmt 2014 translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Okita, T., Vahid, A. H., Way, A., and Liu, Q.
(2014). Dcu terminology translation system for
medical query subtask at wmt14. In Proceed-
ings of the ACL 2014 Ninth Workshop of Statis-
tical Machine Translation, Baltimore, USA.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a method for automatic eval-
uation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 311?318,
Philadelphia, PA, USA. Association for Com-
putational Linguistics.
P?echeux, N., Gong, L., Do, Q. K., Marie, B.,
Ivanishcheva, Y., Allauzen, A., Lavergne, T.,
52
Niehues, J., Max, A., and Yvon, Y. (2014).
LIMSI @ WMT?14 Medical Translation Task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, USA.
Pecina, P., Du?sek, O., Goeuriot, L., Haji?c, J.,
Hlav?a?cov?a, J., Jones, G., Kelly, L., Leveling, J.,
Mare?cek, D., Nov?ak, M., Popel, M., Rosa, R.,
Tamchyna, A., and Ure?sov?a, Z. (2014). Adapta-
tion of machine translation for multilingual in-
formation retrieval in the medical domain. Arti-
ficial Intelligence in Medicine, (0):?.
Peitz, S., Wuebker, J., Freitag, M., and Ney, H.
(2014). The rwth aachen german-english ma-
chine translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Pouliquen, B. and Mazenc, C. (2011). COPPA,
CLIR and TAPTA: three tools to assist in over-
coming the patent barrier at WIPO. In Pro-
ceedings of the Thirteenth Machine Translation
Summit, pages 24?30, Xiamen, China. Asia-
Pacific Association for Machine Translation.
Powers, D. M. W. (2011). Evaluation: from preci-
sion, recall and f-measure to roc, informedness,
markedness & correlation. Journal of Machine
Learning Technologies.
Quernheim, D. and Cap, F. (2014). Large-scale ex-
act decoding: The ims-ttt submission to wmt14.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Rosse, C. and Mejino Jr., J. L. V. (2008). The
foundational model of anatomy ontology. In
Burger, A., Davidson, D., and Baldock, R., ed-
itors, Anatomy Ontologies for Bioinformatics,
volume 6 of Computational Biology, pages 59?
117. Springer London.
Rubino, R., Toral, A., S?anchez-Cartagena, V. M.,
Ferr?andez-Tordera, J., Ortiz Rojas, S., Ram??rez-
S?anchez, G., S?anchez-Mart??nez, F., and Way,
A. (2014). Abu-matran at wmt 2014 transla-
tion task: Two-step data selection and rbmt-
style synthetic rules. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Sakaguchi, K., Post, M., and Van Durme, B.
(2014). Efficient elicitation of annotations for
human evaluation of machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland.
S?anchez-Cartagena, V. M., P?erez-Ortiz, J. A., and
S?anchez-Mart??nez, F. (2014). The ua-prompsit
hybrid machine translation system for the 2014
workshop on statistical machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Scarton, C. and Specia, L. (2014). Exploring con-
sensus in machine translation for quality esti-
mation. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Schwartz, L., Anderson, T., Gwinnup, J., and
Young, K. (2014). Machine translation and
monolingual postediting: The afrl wmt-14 sys-
tem. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An
investigation on the effectiveness of features for
translation quality estimation. In Proceedings
of the Machine Translation Summit XIV, pages
167?174, Nice, France.
Shah, K. and Specia, L. (2014). Quality estimation
for translation selection. In Proceedings of the
17th Annual Conference of the European As-
sociation for Machine Translation, Dubrovnik,
Croatia.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
53
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria.
Tamchyna, A., Popel, M., Rosa, R., and Bojar, O.
(2014). Cuni in wmt14: Chimera still awaits
bellerophon. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, Balti-
more, Maryland, USA. Association for Compu-
tational Linguistics.
Tan, L. and Pal, S. (2014). Manawi: Using
multi-word expressions and named entities to
improve machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Thompson, P., Iqbal, S., McNaught, J., and Ana-
niadou, S. (2009). Construction of an annotated
corpus to support biomedical information ex-
traction. BMC bioinformatics, 10(1):349.
Tiedemann, J. (2009). News from OPUS ? a
collection of multilingual parallel corpora with
tools and interfaces. In Recent Advances in
Natural Language Processing, volume 5, pages
237?248, Borovets, Bulgaria. John Benjamins.
Tillmann, C., Vogel, S., Ney, H., Zubiaga, A.,
and Sawaf, H. (1997). Accelerated DP based
search for statistical translation. In Kokki-
nakis, G., Fakotakis, N., and Dermatas, E., edi-
tors, Proceedings of the Fifth European Confer-
ence on Speech Communication and Technol-
ogy, pages 2667?2670, Rhodes, Greece. Inter-
national Speech Communication Association.
U.S. National Library of Medicine (2009). UMLS
reference manual. Metathesaurus. Bethesda,
MD, USA.
Voorhees, E. M. and Harman, D. K., editors
(2005). TREC: Experiment and evaluation in
information retrieval, volume 63 of Digital li-
braries and electronic publishing series. MIT
press Cambridge, Cambridge, MA, USA.
Wang, L., Lu, Y., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira., F. (2014). Combining domain
adaptation approaches for medical text transla-
tion. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
W?aschle, K. and Riezler, S. (2012). Analyz-
ing parallelism and domain similarities in the
MAREC patent corpus. In Salampasis, M. and
Larsen, B., editors, Multidisciplinary Informa-
tion Retrieval, volume 7356 of Lecture Notes
in Computer Science, pages 12?27. Springer
Berlin Heidelberg.
Williams, P., Sennrich, R., Nadejde, M., Huck, M.,
Hasler, E., and Koehn, P. (2014). Edinburghs
syntax-based systems at wmt 2014. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Wisniewski, G., P?echeux, N., Allauzen, A., and
Yvon, F. (2014). Limsi submission for wmt?14
qe task. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
wu, x., Haque, R., Okita, T., Arora, P., Way, A.,
and Liu, Q. (2014). Dcu-lingo24 participation
in wmt 2014 hindi-english translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Zde?nka Ure?sov?a, Ond?rej Du?sek, J. H. and Pecina,
P. (2014). Multilingual test sets for machine
translation of search queries for cross-lingual
information retrieval in the medical domain. In
To appear in Proceedings of the Ninth Interna-
tional Conference on Language Resources and
Evaluation, Reykjavik, Iceland.
Zhang, J., Wu, X., Calixto, I., Vahid, A. H., Zhang,
X., Way, A., and Liu, Q. (2014). Experiments in
medical translation shared task at wmt 2014. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
54
A Pairwise System Comparisons by Human Judges
Tables 28?37 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according the official method used in
Table 8. Gray lines separate clusters based on non-overlapping rank ranges.
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
C
U
-
M
O
S
E
S
ONLINE-B ? .47? .43? .42? .39?
UEDIN-PHRASE .53? ? .44? .44? .41?
UEDIN-SYNTAX .57? .56? ? .49 .48?
ONLINE-A .58? .56? .51 ? .48?
CU-MOSES .61? .59? .52? .52? ?
score .57 .54 .47 .46 .44
rank 1 2 3-4 3-4 5
Table 28: Head to head comparison, ignoring ties, for Czech-English systems
C
U
-
D
E
P
F
I
X
U
E
D
I
N
-
U
N
C
N
S
T
R
C
U
-
B
O
J
A
R
C
U
-
F
U
N
K
Y
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
O
N
L
I
N
E
-
A
C
U
-
T
E
C
T
O
C
O
M
M
E
R
C
I
A
L
1
C
O
M
M
E
R
C
I
A
L
2
CU-DEPFIX ? .50 .42? .48 .44? .43? .41? .35? .30? .24?
UEDIN-UNCNSTR .50 ? .51 .48 .42? .37? .42? .39? .31? .26?
CU-BOJAR .58? .49 ? .49 .45? .44? .40? .36? .32? .24?
CU-FUNKY .52 .52 .51 ? .48 .47? .44? .34? .33? .26?
ONLINE-B .56? .58? .55? .52 ? .48 .47? .41? .31? .26?
UEDIN-PHRASE .57? .63? .56? .53? .52 ? .48 .44? .32? .27?
ONLINE-A .59? .58? .60? .56? .53? .52 ? .45? .37? .30?
CU-TECTO .65? .61? .64? .66? .59? .56? .55? ? .42? .30?
COMMERCIAL1 .70? .69? .68? .67? .69? .68? .63? .58? ? .40?
COMMERCIAL2 .76? .74? .76? .74? .74? .73? .70? .70? .60? ?
score .60 .59 .58 .57 .54 .52 .50 .44 .36 .28
rank 1-3 1-3 1-4 3-4 5-6 5-6 7 8 9 10
Table 29: Head to head comparison, ignoring ties, for English-Czech systems
55
O
N
L
I
N
E
-
B
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
L
I
M
S
I
-
K
I
T
E
U
-
B
R
I
D
G
E
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
R
W
T
H
D
C
U
-
I
C
T
C
A
S
C
M
U
R
B
M
T
4
R
B
M
T
1
O
N
L
I
N
E
-
C
ONLINE-B ? .46 .40? .41? .35? .42? .38? .35? .40? .31? .33? .32? .22?
UEDIN-SYNTAX .54 ? .51 .47 .47 .45 .45? .39? .36? .38? .35? .34? .27?
ONLINE-A .60? .49 ? .42? .44? .51 .41? .38? .44? .42? .38? .31? .20?
LIMSI-KIT .59? .53 .58? ? .55 .53 .31? .45? .39? .41? .37? .35? .29?
EU-BRIDGE .65? .53 .56? .45 ? .45 .44? .48 .40? .37? .39? .37? .30?
UEDIN-PHRASE .58? .55 .49 .47 .55 ? .48 .39? .34? .45? .40? .40? .34?
KIT .62? .55? .59? .69? .56? .52 ? .45? .41? .45? .47 .40? .31?
RWTH .65? .61? .62? .55? .52 .61? .55? ? .54 .44? .44? .38? .37?
DCU-ICTCAS .60? .64? .56? .61? .60? .66? .59? .46 ? .51 .49 .46? .40?
CMU .69? .62? .58? .59? .63? .55? .55? .56? .49 ? .53 .42? .43?
RBMT4 .67? .65? .62? .63? .61? .60? .53 .56? .51 .47 ? .51 .37?
RBMT1 .68? .66? .69? .65? .63? .60? .60? .62? .54? .58? .49 ? .38?
ONLINE-C .78? .73? .80? .71? .70? .66? .69? .63? .60? .57? .63? .62? ?
score .63 .58 .58 .55 .55 .54 .49 .47 .45 .44 .44 .40 .32
rank 1 2-3 2-3 4-6 4-6 4-6 7-8 7-8 9-11 9-11 9-11 12 13
Table 30: Head to head comparison, ignoring ties, for German-English systems
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
S
T
A
N
F
O
R
D
E
U
-
B
R
I
D
G
E
R
B
M
T
4
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
1
K
I
T
S
T
A
N
F
O
R
D
-
U
N
C
C
I
M
S
S
T
A
N
F
O
R
D
U
U
O
N
L
I
N
E
-
C
I
M
S
-
T
T
T
U
U
-
D
O
C
E
N
T
UEDIN-SYNTAX ? .55? .46? .45? .46? .44? .41? .45? .43? .41? .38? .38? .36? .33? .38? .30? .30? .25?
ONLINE-B .45? ? .50 .48 .50 .47 .43? .46? .41? .45? .39? .39? .37? .32? .35? .34? .30? .29?
ONLINE-A .54? .50 ? .44? .52 .50 .45? .43? .43? .42? .39? .41? .42? .42? .37? .44? .38? .33?
PROMT-HYBRID .55? .52 .56? ? .45? .47 .47 .46? .50 .44? .42? .40? .41? .38? .39? .39? .33? .34?
PROMT-RULE .54? .50 .48 .55? ? .51 .47 .47 .45? .38? .42? .40? .43? .41? .43? .38? .35? .29?
UEDIN-STANFORD .56? .53 .50 .53 .49 ? .48 .50 .47 .44? .46 .36? .36? .36? .36? .35? .30? .32?
EU-BRIDGE .59? .57? .55? .53 .53 .52 ? .46? .43? .52 .42? .42? .45? .35? .36? .41? .38? .30?
RBMT4 .55? .54? .57? .54? .53 .50 .54? ? .53 .49 .44? .49 .50 .47 .40? .42? .38? .40?
UEDIN-PHRASE .57? .59? .57? .50 .55? .53 .57? .47 ? .50 .55? .47 .45? .44? .43? .42? .37? .34?
RBMT1 .59? .55? .58? .56? .62? .56? .48 .51 .50 ? .47 .47 .45? .47 .43? .42? .38? .41?
KIT .62? .61? .61? .58? .58? .54 .58? .56? .45? .53 ? .47 .49 .46 .43? .48 .34? .37?
STANFORD-UNC .62? .61? .59? .60? .60? .64? .58? .51 .53 .53 .53 ? .48 .47 .45? .45? .39? .41?
CIMS .64? .63? .58? .59? .57? .64? .55? .50 .55? .55? .51 .52 ? .53 .42? .52 .47 .42?
STANFORD .67? .68? .58? .62? .59? .64? .65? .53 .56? .53 .54 .53 .47 ? .53 .42? .39? .48
UU .62? .65? .62? .61? .57? .64? .64? .60? .57? .57? .57? .55? .58? .47 ? .46? .45? .38?
ONLINE-C .70? .66? .56? .61? .62? .65? .59? .58? .58? .58? .52 .55? .48 .58? .54? ? .48 .47
IMS-TTT .70? .70? .62? .67? .65? .70? .62? .62? .63? .62? .66? .61? .53 .61? .55? .52 ? .49
UU-DOCENT .75? .71? .67? .66? .71? .68? .70? .60? .66? .59? .63? .59? .58? .52 .62? .53 .51 ?
score .60 .59 .56 .56 .56 .56 .54 .51 .51 .50 .48 .47 .46 .44 .43 .42 .38 .37
rank 1-2 1-2 3-6 3-6 3-6 3-6 7 8-10 8-10 8-10 11-12 11-13 12-14 13-15 14-16 15-16 17-18 17-18
Table 31: Head to head comparison, ignoring ties, for English-German systems
56
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
O
N
L
I
N
E
-
B
S
T
A
N
F
O
R
D
O
N
L
I
N
E
-
A
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
UEDIN-PHRASE ? .48 .48 .45? .43? .28? .28? .19?
KIT .52 ? .54? .48 .44? .31? .29? .21?
ONLINE-B .52 .46? ? .51 .47 .31? .30? .24?
STANFORD .55? .52 .49 ? .46? .34? .30? .23?
ONLINE-A .57? .56? .53 .54? ? .32? .29? .21?
RBMT1 .72? .69? .69? .66? .68? ? .42? .33?
RBMT4 .72? .71? .70? .70? .71? .58? ? .39?
ONLINE-C .81? .79? .76? .77? .79? .67? .61? ?
score .63 .60 .59 .58 .57 .40 .35 .25
rank 1 2-4 2-4 2-4 5 6 7 8
Table 32: Head to head comparison, ignoring ties, for French-English systems
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
M
A
T
R
A
N
M
A
T
R
A
N
-
R
U
L
E
S
O
N
L
I
N
E
-
A
U
U
-
D
O
C
E
N
T
P
R
O
M
T
-
H
Y
B
R
I
D
U
A
P
R
O
M
T
-
R
U
L
E
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
ONLINE-B ? .46? .48 .46? .50 .41? .39? .39? .37? .38? .37? .35? .27?
UEDIN-PHRASE .54? ? .50 .47 .46 .46? .42? .41? .46? .42? .35? .34? .33?
KIT .52 .50 ? .53 .51 .50 .43? .49 .41? .42? .35? .37? .29?
MATRAN .54? .53 .47 ? .49 .50 .43? .43? .38? .48 .40? .34? .32?
MATRAN-RULES .50 .54 .49 .51 ? .53 .40? .45? .46? .42? .44? .40? .34?
ONLINE-A .59? .54? .50 .50 .47 ? .44? .49 .47 .45? .42? .37? .34?
UU-DOCENT .61? .58? .57? .57? .60? .56? ? .43? .52 .46? .39? .44? .33?
PROMT-HYBRID .61? .59? .51 .57? .55? .51 .57? ? .50 .41? .46? .44? .35?
UA .63? .54? .59? .62? .54? .53 .48 .50 ? .49 .46? .43? .34?
PROMT-RULE .62? .58? .58? .52 .58? .55? .54? .59? .51 ? .47 .39? .37?
RBMT1 .63? .65? .65? .60? .56? .58? .61? .54? .54? .53 ? .46? .45?
RBMT4 .65? .66? .63? .66? .60? .63? .56? .56? .57? .61? .54? ? .45?
ONLINE-C .73? .67? .71? .67? .66? .66? .67? .65? .66? .63? .55? .55? ?
score .59 .57 .55 .55 .54 .53 .49 .49 .48 .47 .43 .40 .34
rank 1 2-4 2-5 2-5 4-6 4-6 7-9 7-10 7-10 8-10 11 12 13
Table 33: Head to head comparison, ignoring ties, for English-French systems
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
S
Y
N
T
A
X
C
M
U
U
E
D
I
N
-
P
H
R
A
S
E
A
F
R
L
I
I
T
-
B
O
M
B
A
Y
D
C
U
-
L
I
N
G
O
2
4
I
I
I
T
-
H
Y
D
E
R
A
B
A
D
ONLINE-B ? .36? .33? .37? .31? .21? .20? .14? .00
ONLINE-A .64? ? .48 .47? .44? .31? .30? .24? .12?
UEDIN-SYNTAX .67? .52 ? .47 .46? .33? .29? .24? .12?
CMU .63? .53? .53 ? .47 .37? .31? .26? .11?
UEDIN-PHRASE .69? .56? .54? .53 ? .40? .33? .25? .11?
AFRL .79? .69? .67? .63? .60? ? .53 .40? .16?
IIT-BOMBAY .80? .70? .71? .69? .67? .47 ? .44? .19?
DCU-LINGO24 .86? .76? .76? .74? .75? .60? .56? ? .19?
IIIT-HYDERABAD .94? .88? .88? .89? .89? .84? .81? .81? ?
score .75 .62 .61 .60 .57 .44 .41 .34 .13
rank 1 2-3 2-4 3-4 5 6-7 6-7 8 9
Table 34: Head to head comparison, ignoring ties, for Hindi-English systems
57
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
U
N
C
N
S
T
R
U
E
D
I
N
-
P
H
R
A
S
E
C
U
-
M
O
S
E
S
I
I
T
-
B
O
M
B
A
Y
I
P
N
-
U
P
V
-
C
N
T
X
T
D
C
U
-
L
I
N
G
O
2
4
I
P
N
-
U
P
V
-
N
O
D
E
V
M
A
N
A
W
I
-
H
1
M
A
N
A
W
I
M
A
N
A
W
I
-
R
M
O
O
V
ONLINE-B ? .49 .28? .29? .27? .23? .22? .20? .17? .12? .13? .13?
ONLINE-A .51 ? .31? .29? .27? .25? .20? .20? .21? .19? .16? .15?
UEDIN-UNCNSTR .72? .69? ? .44? .49 .39? .40? .34? .39? .29? .30? .27?
UEDIN-PHRASE .71? .71? .56? ? .48 .45? .44? .39? .37? .31? .31? .32?
CU-MOSES .73? .73? .51 .52 ? .47 .42? .40? .45? .36? .35? .33?
IIT-BOMBAY .77? .75? .61? .55? .53 ? .50 .47 .45? .41? .40? .36?
IPN-UPV-CNTXT .78? .80? .60? .56? .58? .50 ? .51 .41? .40? .40? .37?
DCU-LINGO24 .80? .80? .66? .61? .60? .53 .49 ? .52 .41? .41? .39?
IPN-UPV-NODEV .83? .79? .61? .63? .55? .55? .59? .48 ? .46? .44? .38?
MANAWI-H1 .88? .81? .71? .69? .64? .59? .60? .59? .54? ? .35? .34?
MANAWI .87? .84? .70? .69? .65? .60? .60? .59? .56? .65? ? .39?
MANAWI-RMOOV .87? .85? .73? .68? .67? .64? .63? .61? .62? .66? .61? ?
score .77 .75 .57 .54 .52 .47 .46 .43 .42 .38 .35 .31
rank 1 2 3 4-5 4-5 6-7 6-7 8-9 8-9 10-11 10-11 12
Table 35: Head to head comparison, ignoring ties, for English-Hindi systems
A
F
R
L
-
P
E
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
P
H
R
A
S
E
Y
A
N
D
E
X
O
N
L
I
N
E
-
G
A
F
R
L
U
E
D
I
N
-
S
Y
N
T
A
X
K
A
Z
N
U
R
B
M
T
1
R
B
M
T
4
AFRL-PE ? .42? .40? .39? .39? .41? .35? .39? .28? .26? .26? .29? .21?
ONLINE-B .58? ? .42? .43? .45? .45? .42? .43? .46? .37? .33? .29? .31?
ONLINE-A .60? .58? ? .50 .45? .51 .47 .45? .42? .40? .33? .32? .30?
PROMT-HYBRID .61? .57? .50 ? .47 .45? .49 .44? .43? .44? .39? .31? .27?
PROMT-RULE .61? .55? .55? .53 ? .46? .47 .49 .48 .42? .36? .34? .30?
UEDIN-PHRASE .59? .55? .49 .55? .54? ? .49 .50 .47 .44? .32? .37? .29?
YANDEX .65? .58? .53 .51 .53 .51 ? .48 .50 .43? .34? .36? .34?
ONLINE-G .61? .57? .55? .56? .51 .50 .52 ? .48 .43? .39? .35? .30?
AFRL .72? .54? .58? .57? .52 .53 .50 .52 ? .44? .41? .41? .37?
UEDIN-SYNTAX .74? .63? .60? .56? .58? .56? .57? .57? .56? ? .51 .36? .37?
KAZNU .74? .67? .67? .61? .64? .68? .66? .61? .59? .49 ? .44? .38?
RBMT1 .71? .71? .68? .69? .66? .63? .64? .65? .59? .64? .56? ? .47
RBMT4 .79? .69? .70? .73? .70? .71? .66? .70? .63? .63? .62? .53 ?
score .66 .58 .55 .55 .53 .53 .52 .51 .49 .45 .40 .36 .32
rank 1 2 3-5 3-5 4-7 5-8 5-8 5-8 9 10 11 12 13
Table 36: Head to head comparison, ignoring ties, for Russian-English systems
P
R
O
M
T
-
R
U
L
E
O
N
L
I
N
E
-
B
P
R
O
M
T
-
H
Y
B
R
I
D
U
E
D
I
N
-
U
N
C
N
S
T
R
O
N
L
I
N
E
-
G
O
N
L
I
N
E
-
A
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
4
R
B
M
T
1
PROMT-RULE ? .51 .45? .43? .43? .39? .38? .15? .00
ONLINE-B .49 ? .50 .47? .38? .36? .38? .16? .13?
PROMT-HYBRID .55? .50 ? .49 .47 .39? .40? .18? .15?
UEDIN-UNCNSTR .57? .53? .51 ? .50 .44? .36? .25? .18?
ONLINE-G .57? .62? .53 .50 ? .46? .44? .23? .18?
ONLINE-A .61? .64? .61? .56? .54? ? .49 .24? .18?
UEDIN-PHRASE .62? .62? .60? .64? .56? .51 ? .30? .21?
RBMT4 .85? .84? .82? .75? .77? .76? .70? ? .42?
RBMT1 .91? .87? .85? .82? .82? .82? .79? .58? ?
score .64 .64 .61 .58 .55 .51 .49 .26 .19
rank 1-2 1-2 3 4-5 4-5 6-7 6-7 8 9
Table 37: Head to head comparison, ignoring ties, for English-Russian systems
58
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 221?228,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Machine Translation of Medical Texts in the Khresmoi Project
Ond
?
rej Du
?
sek, Jan Haji
?
c, Jaroslava Hlav
?
a
?
cov
?
a, Michal Nov
?
ak,
Pavel Pecina, Rudolf Rosa, Ale
?
s Tamchyna, Zde
?
nka Ure
?
sov
?
a, Daniel Zeman
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk?e n?am?est?? 25, 11800 Prague, Czech Republic
{odusek,hajic,hlavacova,mnovak,pecina,rosa,tamchyna,uresova,zeman}@ufal.mff.cuni.cz
Abstract
This paper presents the participation of
the Charles University team in the WMT
2014 Medical Translation Task. Our sys-
tems are developed within the Khresmoi
project, a large integrated project aim-
ing to deliver a multi-lingual multi-modal
search and access system for biomedical
information and documents. Being in-
volved in the organization of the Medi-
cal Translation Task, our primary goal is
to set up a baseline for both its subtasks
(summary translation and query transla-
tion) and for all translation directions.
Our systems are based on the phrase-
based Moses system and standard meth-
ods for domain adaptation. The con-
strained/unconstrained systems differ in
the training data only.
1 Introduction
The WMT 2014 Medical Translation Task poses
an interesting challenge for Machine Translation
(MT). In the ?standard? translation task, the end
application is the translation itself. In the Medi-
cal Translation Task, the MT system is considered
a part of a larger system for Cross-Lingual Infor-
mation Retrieval (CLIR) and is used to solve two
different problems: (i) translation of user search
queries, and (ii) translation of summaries of re-
trieved documents.
In query translation, the end user does not even
necessarily see the MT output as their queries are
translated and search is performed on documents
in the target language. In summary translation, the
sentences to be translated come from document
summaries (snippets) displayed to provide infor-
mation on each of the documents retrieved by the
search. Therefore, translation quality may not be
the most important measure in this task ? the per-
formance of the CLIR system as a whole is the
final criterion. Another fundamental difference
from the standard task is the nature of the trans-
lated texts. While we can consider document sum-
maries to be ordinary texts (despite their higher in-
formation density in terms of terminology from a
narrow domain), search queries in the medical do-
main are an extremely specific type of data, and
traditional techniques for system development and
domain adaptation are truly put to a test here.
This work is a part of the of the large integrated
EU-funded Khresmoi project.
1
Among other
goals, such as joint text and image retrieval of ra-
diodiagnostic records, Khresmoi aims to develop
technology for transparent cross-lingual search of
medical sources for both professionals and laypeo-
ple, with the emphasis primarily on publicly avail-
able web sources.
In this paper, we describe the Khresmoi sys-
tems submitted to the WMT 2014 Medical Trans-
lation Task. We participate in both subtasks (sum-
mary translation and query translation) for all
language pairs (Czech?English, German?English,
and French?English) in both directions (to English
and from English). Our systems are based on the
Moses phrase-based translation toolkit and stan-
dard methods for domain adaptation. We submit
one constrained and one unconstrained system for
each subtask and translation direction. The con-
strained and unconstrained systems differ in train-
ing data only: The former use all allowed training
data, the latter take advantage of additional web-
crawled data.
We first summarize previous works in MT do-
main adaptation in Section 2, then describe the
data we used for our systems in Section 3. Sec-
1
http://www.khresmoi.eu/
221
tion 4 contains an account of the submitted sys-
tems and their performance in translation of search
queries and document summaries. Section 5 con-
cludes the paper.
2 Related work
To put our work in the context of other approaches,
we first describe previous work on domain adap-
tation in Statistical Machine Translation (SMT),
then focus specifically on SMT in the medical do-
main.
2.1 Domain adaptation of Statistical machine
translation
Many works on domain adaptation examine the
usage of available in-domain data to directly im-
prove in-domain performance of SMT. Some au-
thors attempt to combine the predictions of two
separate (in-domain and general-domain) transla-
tion models (Langlais, 2002; Sanchis-Trilles and
Casacuberta, 2010; Bisazza et al., 2011; Nakov,
2008) or language models (Koehn and Schroeder,
2007). Wu and Wang (2004) use in-domain data
to improve word alignment in the training phase.
Carpuat et al. (2012) explore the possibility of us-
ing word sense disambiguation to discriminate be-
tween domains.
Other approaches concentrate on the acquisition
of larger in-domain corpora. Some of them ex-
ploit existing general-domain corpora by select-
ing data that resemble the properties of in-domain
data (e.g., using cross-entropy), thus building a
larger pseudo-in-domain training corpus. This
technique is used to adapt language models (Eck
et al., 2004b; Moore and Lewis, 2010) as well as
translation models (Hildebrand et al., 2005; Axel-
rod et al., 2011) or their combination (Mansour et
al., 2011). Similar approaches to domain adapta-
tion are also applied in other tasks, e.g., automatic
speech recognition (Byrne et al., 2004).
2.2 Statistical machine translation in the
medical domain
Eck et al. (2004a) employ an SMT system for the
translation of dialogues between doctors and pa-
tients and show that according to automatic met-
rics, a dictionary extracted from the Unified Medi-
cal Language System (UMLS) Metathesaurus and
its semantic type classification (U.S. National Li-
brary of Medicine, 2009) significantly improves
translation quality from Spanish to English when
applied to generalize the training data.
Wu et al. (2011) analyze the quality of MT on
PubMed
2
titles and whether it is sufficient for pa-
tients. The conclusions are very positive espe-
cially for languages with large training resources
(English, Spanish, German) ? the average fluency
and content scores (based on human evaluation)
are above four on a five-point scale. In automatic
evaluation, their systems substantially outperform
Google Translate. However, the SMT systems are
specifically trained, tuned, and tested on the do-
main of PubMed titles, and it is not evident how
they would perform on other medical texts.
Costa-juss`a et al. (2012) are less optimistic re-
garding SMT quality in the medical domain. They
analyze and evaluate the quality of public web-
based MT systems (such as Google Translate) and
conclude that in both automatic and manual eval-
uation (on 7 language pairs), the performance of
these systems is still not good enough to be used
in daily routines of medical doctors in hospitals.
Jimeno Yepes et al. (2013) propose a method
for obtaining in-domain parallel corpora from ti-
tles and abstracts of publications in the MED-
LINE
3
database. The acquired corpora contain
from 30,000 to 130,000 sentence pairs (depending
on the language pair) and are reported to improve
translation quality when used for SMT training,
compared to a baseline trained on out-of-domain
data. However, the authors use only one source
of in-domain parallel data to adapt the translation
model, and do not use any in-domain monolingual
data to adapt the language model.
In this work, we investigate methods combining
the different kinds of data ? general-domain, in-
domain, and pseudo-in-domain ? to find the opti-
mal approach to this problem.
3 Data description
This section includes an overview of the parallel
and monolingual data sources used to train our
systems. Following the task specification, they
are split into constrained and unconstrained sec-
tions. The constrained section includes medical-
domain data provided for this task (extracted by
the provided scripts), and general-domain texts
provided as constrained data for the standard task
(?general domain? here is used to denote data
2
http://www.ncbi.nlm.nih.gov/pubmed/
3
http://www.nlm.nih.gov/pubs/
factsheets/medline.html
222
Czech?English German?English French?English
dom set pairs source target pairs source target pairs source target
med con 2,498 18,126 19,964 4,998 123,686 130,598 6,139 202,245 171,928
gen con 15,788 226,711 260,505 4,520 112,818 119,404 40,842 1,470,016 1,211,516
gen unc ? ? ? 9,320 525,782 574,373 13,809 961,991 808,222
Table 1: Number of sentence pairs and tokens (source/target) in parallel training data (in thousands).
dom set English Czech German French
med con 172,991 1,848 63,499 63,022
gen con 6,132,107 627,493 1,728,065 1,837,457
med unc 3,275,272 36,348 361,881 908,911
gen unc 618,084 ? 339,595 204,025
Table 2: Number of tokens in monolingual training data (in thousands).
which comes from a mixture of various different
domains, mostly news, parliament proceedings,
web-crawls, etc.). The unconstrained section con-
tains automatically crawled data from medical and
health websites and non-medical data from patent
collections.
3.1 Parallel data
The parallel data summary is presented in Table 1.
The main sources of the medical-domain data
for all the language pairs include the EMEA cor-
pus (Tiedemann, 2009), the UMLS metathesaurus
of health and biomedical vocabularies and stan-
dards (U.S. National Library of Medicine, 2009),
and bilingual titles of Wikipedia articles belonging
to the categories identified to be medical domain.
Additional medical-domain data comes from the
MAREC patent collection: PatTR (W?aschle and
Riezler, 2012) available for DE?EN and FR?EN,
and COPPA (Pouliquen and Mazenc, 2011) for
FR?EN (only patents from the medical categories
A61, C12N, and C12P are allowed in the con-
strained systems).
The constrained general-domain data include
three parallel corpora for all the language pairs:
CommonCrawl (Smith et al., 2013), Europarl ver-
sion 6 (Koehn, 2005), the News Commentary cor-
pus (Callison-Burch et al., 2012). Further, the con-
strained data include CzEng (Bojar et al., 2012)
for CS?EN and the UN corpus for FR?EN.
For our unconstrained experiments, we also em-
ploy parallel data from the non-medical patents
from the PatTR and COPPA collections (other cat-
egories than A61, C12N, and C12P).
3.2 Monolingual data
The monolingual data is summarized in Table 2.
The main sources of the medical-domain mono-
lingual data for all languages involve Wikipedia
pages, UMLS concept descriptions, and non-
parallel texts extracted from the medical patents
of the PatTR collections. For English, the main
source is the AACT collection of texts from Clin-
icalTrials.gov. Smaller resources include: Drug-
Bank (Knox et al., 2011), GENIA (Kim et al.,
2003), FMA (Rosse and Mejino Jr., 2008), GREC
(Thompson et al., 2009), and PIL (Bouayad-Agha
et al., 2000).
In the unconstrained systems, we use additional
monolingual data from web pages crawled within
the Khresmoi project: a collection of about one
million HON-certified
4
webpages in English re-
leased as the test collection for the CLEF 2013
eHealth Task 3 evaluation campaign,
5
additional
web-crawled HON-certified pages (not publicly
available), and other webcrawled medical-domain
related webpages.
The constrained general-domain resources in-
clude: the News corpus for CS, DE, EN, and FR
collected for the purpose of the WMT 2014 Stan-
dard Task, monolingual parts of the Europarl and
News-Commentary corpora, and the Gigaword for
EN and FR.
For the FR?EN and DE?EN unconstrained sys-
tems, the additional general domain monolingual
data is taken from monolingual texts of non-
medical patents in the PatTR collection.
4
https://www.hon.ch/
5
https://sites.google.com/site/
shareclefehealth/
223
medical general
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
u
n
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
Figure 1: Distribution of the domain-specificity
scores in the English?French parallel data sets.
3.3 Data preprocessing
The data consisting of crawled web pages, namely
CLEF, HON, and non-HON, needed to be cleaned
and transformed into a set of sentences. The
Boilerpipe (Kohlsch?utter et al., 2010) and Justext
(Pomik?alek, 2011) tools were used to remove boil-
erplate texts and extract just the main content from
the web pages. The YALI language detection tool
(Majli?s, 2012) trained on both in-domain and gen-
eral domain data then filtered out those cleaned
pages which were not identified as written in one
of the concerned languages.
The rest of the preprocessing procedure was ap-
plied to all the datasets mentioned above, both
parallel and monolingual. The data were tok-
enized and normalized by converting or omit-
ting some (mostly punctuation) characters. A
set of language-dependent heuristics was applied
in an attempt to restore and normalize the open-
ing/closing quotation marks, i.e. convert "quoted"
to ?quoted? (Zeman, 2012). The motivation here
is twofold: First, we hope that paired quota-
tion marks could occasionally work as brackets
and better denote parallel phrases for Moses; sec-
ond, if Moses learns to output directed quotation
marks, the subsequent detokenization will be eas-
ier. For all systems which translate from German,
decompounding is employed to reduce source-side
data sparsity. We used BananaSplit for this task
(M?uller and Gurevych, 2006).
We perform all training and internal evaluation
on lowercased data; we trained recasers to post-
process the final submissions.
medical general
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
u
n
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
Figure 2: Distribution of the domain-specificity
scores in the French monolingual data sets.
4 Submitted systems
We first describe our technique of psedo-in-
domain data selection in Section 4.1, then com-
pare two methods of combining the selected data
in Section 4.2. This, along with using constrained
and unconstrained data sets to train the systems
(see Section 3), amounts to a total of four system
variants submitted for each task. A description of
the system settings used is given in Section 4.3.
4.1 Data selection
We follow an approach originally proposed for
selection of monolingual sentences for language
modeling (Moore and Lewis, 2010) and its modi-
fication applied to selection of parallel sentences
(Axelrod et al., 2011). This technique assumes
two language models for sentence scoring, one
trained on (true) in-domain text and one trained
on (any) general-domain text in the same lan-
guage (e.g., English). For both data domains
(general and medical), we score each sentence
by the difference of its cross-perplexity given the
in-domain language model and cross-perplexity
given the general-domain language model (in this
order). We only keep sentences with a negative
score in our data, assuming that these are the
most ?medical-like?. Visualisation of the domain-
specificity scores (cross-perplexity difference) in
the FR?EN parallel data and FR monolingual data
is illustrated in Figures 1 and 2, respectively.
6
The
scores (Y axis) are presented for each sentence in
increasing order from left to right (X axis).
6
For the medical domain, constrained and unconstrained
parallel data are identical.
224
cs?en de?en en?cs en?de en?fr fr?en
con concat 33.64?1.14 32.84?1.24 18.10?0.94 18.29?0.92 33.39?1.11 36.71?1.17
con interpol 32.94?1.11 32.31?1.20 18.96?0.93 18.41?0.93 34.06?1.11 37.42?1.21
unc concat 34.10?1.11 34.52?1.20 21.12?1.03 19.76?0.92 36.23?1.03 38.15?1.16
unc interpol 34.48?1.16 34.92?1.17 22.15?1.06 20.81?0.95 36.26?1.13 37.91?1.13
Table 3: BLEU scores of summary translations.
cs?en de?en en?cs en?de en?fr fr?en
con concat 30.87?4.70 33.21?5.03 23.25?4.85 17.72?4.75 28.64?3.77 35.56?4.94
con interpol 32.46?5.05 33.74?4.97 21.56?4.80 16.90?4.39 29.34?3.73 35.28?5.26
unc concat 34.88?5.04 31.24?5.59 22.61?4.91 19.13?5.66 33.08?3.80 36.73?4.88
unc interpol 33.82?5.16 34.19?5.27 23.93?5.16 15.87?11.31 31.19?3.73 40.25?5.14
Table 4: BLEU scores of query translations.
The two language models for sentence scoring
are trained with a restricted vocabulary extracted
from the in-domain training data as words occur-
ring at least twice (singletons and other words are
treated as out-of-vocabulary). In our experiments,
we apply this technique to select both monolin-
gual data for language models and parallel data
for translation models. Selection of parallel data
is based on the English side only. The in-domain
models are trained on the monolingual data in the
target language (constrained or unconstrained, de-
pending on the setting). The general-domain mod-
els are trained on the WMT News data.
Compared to the approach of Moore and Lewis
(2010) and Axelrod et al. (2011), we prune the
model vocabulary more aggressively ? we discard
not only the singletons, but also all words with
non-Latin characters, which helps clean the mod-
els from noise introduced by the automatic process
of data acquisition by web crawling.
4.2 Data combination
For both parallel and monolingual data, we obtain
two data sets after applying the data selection:
? ?medical-like? data from the medical domain
? ?medical-like? data from the general domain.
For each language pair and for each system
type (constrained/unconstrained), we submitted
two system variants which differ in how the se-
lected data are combined. The first variant uses
a simple concatenation of the two datasets both
for parallel data and for language model data. In
the second variant, we train separate models for
each section and use linear interpolation to com-
bine them into a single model. For language mod-
els, we use the SRILM linear interpolation feature
(Stolcke, 2002). We interpolate phrase tables us-
ing Tmcombine (Sennrich, 2012). In both cases,
the held-out set for minimizing the perplexity is
the system development set.
4.3 System details
We compute word alignment on lowercase 4-cha-
racter stems using fast align (Dyer et al., 2013).
We create phrase tables using the Moses toolkit
(Koehn et al., 2007) with standard settings. We
train 5-gram language models on the target-side
lowercase forms using SRILM. We use MERT
(Och, 2003) to tune model weights in our systems
on the development data provided for the task.
The only difference between the system variants
for query and summary translation is the tuning
set. In both cases, we use the respective sets pro-
vided offcially for the shared task.
4.4 Results
Tables 3 and 4 show case-insensitive BLEU scores
of our systems.
7
As expected, the unconstrained
systems outperform the constrained ones. Linear
interpolation outperforms data concatenation quite
reliably across language pairs for summary trans-
lation. While the picture for query translation is
similar, there is more variance in the results, so
we cannot state that interpolation definitely works
7
As we use the same recasers for both summary and query
translation, our systems are heavily penalized for wrong let-
ter case in query translation. However, letter case is not taken
into account in most CLIR systems. All BLEU scores re-
ported in this paper will be case-insensitive for this reason.
225
better in this case. This is due to the sizes of the
development and test sets and most importantly
due to sentence lengths ? queries are very short,
making BLEU unreliable, MERT unstable, and
bootstrap resampling intervals wide.
If we compare our score to the other competi-
tors, we are clearly worse than the best systems for
summary translation. From this perspective, our
data filtering seems overly eager (i.e., discarding
all sentence pairs with a positive perplexity differ-
ence). An experiment which we leave for future
work is doing one more round of interpolation to
combine a model trained on the data with negative
perplexity with models trained on the remainder.
5 Conclusions
We described the Charles University MT system
used in the Shared Medical Translation Task of
WMT 2014. Our primary goal was to set up a
baseline for both the subtasks and all translation
directions. The systems are based on the Moses
toolkit, pseudo-in-domain data selection based on
perplexity difference and two different methods of
in-domain and out-of-domain data combination:
simple data concatenation and linear model inter-
polation.
We report results of constrained and uncon-
strained systems which differ in the training data
only. In most experiments, using additional data
improved the results compared to the constrained
systems and using linear model interpolation out-
performed data concatenation. While our systems
are on par with best results for case-insensitive
BLEU score in query translation, our overly ea-
ger data selection techniques caused lower scores
in summary translation. In future work, we plan
to include a special out-of-domain model in our
setup to compensate for this problem.
Acknowledgments
This work was supported by the EU FP7 project
Khresmoi (contract no. 257528), the Czech Sci-
ence Foundation (grant no. P103/12/G084), and
SVV project number 260 104. This work has
been using language resources developed, stored,
and distributed by the LINDAT/CLARIN project
of the Ministry of Education, Youth and Sports of
the Czech Republic (project LM2010013).
References
A. Axelrod, X. He, and J. Gao. 2011. Domain adap-
tation via pseudo in-domain data selection. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 355?
362, Edinburgh, United Kingdom. ACL.
A. Bisazza, N. Ruiz, and M. Federico. 2011. Fill-
up versus interpolation methods for phrase-based
SMT adaptation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 136?143, San Francisco, CA, USA. Interna-
tional Speech Communication Association.
O. Bojar, Z.
?
Zabokrtsk?y, O. Du?sek, P. Galu?s?c?akov?a,
M. Majli?s, D. Mare?cek, J. Mar?s??k, M. Nov?ak,
M. Popel, and A. Tamchyna. 2012. The joy of
parallelism with CzEng 1.0. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation, pages 3921?3928, Istanbul,
Turkey. European Language Resources Association.
N. Bouayad-Agha, D. R. Scott, and R. Power. 2000.
Integrating content and style in documents: A case
study of patient information leaflets. Information
Design Journal, 9(2?3):161?176.
W. Byrne, D. S. Doermann, M. Franz, S. Gustman,
J. Haji?c, D. W. Oard, et al. 2004. Automatic recog-
nition of spontaneous speech for access to multilin-
gual oral history archives. Speech and Audio Pro-
cessing, IEEE Transactions on, 12(4):420?435.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montr?eal,
Canada. ACL.
M. Carpuat, H. Daum?e III, A. Fraser, C. Quirk,
F. Braune, A. Clifton, et al. 2012. Domain adap-
tation in machine translation: Final report. In
2012 Johns Hopkins Summer Workshop Final Re-
port, pages 61?72. Johns Hopkins University.
M. R. Costa-juss`a, M. Farr?us, and J. Serrano Pons.
2012. Machine translation in medicine. A qual-
ity analysis of statistical machine translation in the
medical domain. In Proceedings of the 1st Virtual
International Conference on Advanced Research in
Scientific Areas, pages 1995?1998,
?
Zilina, Slovakia.
?
Zilinsk?a univerzita.
C. Dyer, V. Chahuneau, and N. A. Smith. 2013. A sim-
ple, fast, and effective reparameterization of IBM
model 2. In Proceedings of NAACL-HLT, pages
644?648.
M. Eck, S. Vogel, and A. Waibel. 2004a. Improv-
ing statistical machine translation in the medical do-
main using the Unified Medical Language System.
In COLING 2004: Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 792?798, Geneva, Switzerland. ACL.
226
M. Eck, S. Vogel, and A. Waibel. 2004b. Language
model adaptation for statistical machine translation
based on information retrieval. In Maria Teresa
Lino, Maria Francisca Xavier, F?atima Ferreira, Rute
Costa, and Raquel Silva, editors, Proceedings of the
International Conference on Language Resources
and Evaluation, pages 327?330, Lisbon, Portugal.
European Language Resources Association.
A. S. Hildebrand, M. Eck, S. Vogel, and A. Waibel.
2005. Adaptation of the translation model for statis-
tical machine translation based on information re-
trieval. In Proceedings of the 10th Annual Con-
ference of the European Association for Machine
Translation, pages 133?142, Budapest, Hungary.
European Association for Machine Translation.
A. Jimeno Yepes,
?
E. Prieur-Gaston, and A. N?ev?eol.
2013. Combining MEDLINE and publisher data to
create parallel corpora for the automatic translation
of biomedical text. BMC Bioinformatics, 14(1):1?
10.
J.-D Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus ? a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(suppl 1):i180?
i182.
C. Knox, V. Law, T. Jewison, P. Liu, Son Ly, A. Frolkis,
A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoum-
bou, R. Eisner, A. C. Guo, and D. S. Wishart.
2011. DrugBank 3.0: a comprehensive resource for
?Omics? research on drugs. Nucleic acids research,
39(suppl 1):D1035?D1041.
P. Koehn and J. Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 224?227, Prague,
Czech Republic. ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages
177?180, Praha, Czechia, June. ACL.
P. Koehn. 2005. Europarl: a parallel corpus for sta-
tistical machine translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages
79?86, Phuket, Thailand. Asia-Pacific Association
for Machine Translation.
C. Kohlsch?utter, P. Fankhauser, and W. Nejdl. 2010.
Boilerplate detection using shallow text features. In
Proceedings of the Third ACM International Confer-
ence on Web Search and Data Mining, WSDM ?10,
pages 441?450, New York, NY, USA. ACM.
P. Langlais. 2002. Improving a general-purpose statis-
tical translation engine by terminological lexicons.
In COLING-02 on COMPUTERM 2002: second
international workshop on computational terminol-
ogy, volume 14, pages 1?7, Taipei, Taiwan. ACL.
M. Majli?s. 2012. Yet another language identifier. In
Proceedings of the Student Research Workshop at
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, pages
46?54, Avignon, France. ACL.
S. Mansour, J. Wuebker, and H. Ney. 2011. Com-
bining translation and language model scoring for
domain-specific data filtering. In International
Workshop on Spoken Language Translation, pages
222?229, San Francisco, CA, USA. ISCA.
R. C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the ACL 2010 Conference Short Papers, pages 220?
224, Uppsala, Sweden. ACL.
C. M?uller and I. Gurevych. 2006. Exploring the po-
tential of semantic relatedness in information re-
trieval. In LWA 2006 Lernen ? Wissensentdeck-
ung ? Adaptivit?at, 9.-11.10.2006, Hildesheimer In-
formatikberichte, pages 126?131, Hildesheim, Ger-
many. Universit?at Hildesheim.
P. Nakov. 2008. Improving English?Spanish statistical
machine translation: Experiments in domain adapta-
tion, sentence paraphrasing, tokenization, and recas-
ing. In Proceedings of the Third Workshop on Statis-
tical Machine Translation, pages 147?150, Colum-
bus, OH, USA. ACL.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167, Morristown,
NJ, USA. ACL.
J. Pomik?alek. 2011. Removing Boilerplate and Du-
plicate Content from Web Corpora. PhD thesis,
Masaryk University, Faculty of Informatics, Brno.
B. Pouliquen and C. Mazenc. 2011. COPPA, CLIR
and TAPTA: three tools to assist in overcoming the
patent barrier at WIPO. In Proceedings of the Thir-
teenth Machine Translation Summit, pages 24?30,
Xiamen, China. Asia-Pacific Association for Ma-
chine Translation.
C. Rosse and Jos?e L. V. Mejino Jr. 2008. The foun-
dational model of anatomy ontology. In A. Burger,
D. Davidson, and R. Baldock, editors, Anatomy On-
tologies for Bioinformatics, volume 6 of Computa-
tional Biology, pages 59?117. Springer London.
G. Sanchis-Trilles and F. Casacuberta. 2010. Log-
linear weight optimisation via Bayesian adaptation
in statistical machine translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1077?1085, Bei-
jing, China. ACL.
227
R. Sennrich. 2012. Perplexity minimization for trans-
lation model domain adaptation in statistical ma-
chine translation. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 539?549. ACL.
J. R. Smith, H. Saint-Amand, M. Plamada, P. Koehn,
C. Callison-Burch, and A. Lopez. 2013. Dirt cheap
web-scale parallel text from the common crawl. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1374?1383, Sofia, Bulgaria.
ACL.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, Den-
ver, Colorado, USA.
P. Thompson, S. Iqbal, J. McNaught, and Sophia Ana-
niadou. 2009. Construction of an annotated corpus
to support biomedical information extraction. BMC
bioinformatics, 10(1):349.
J. Tiedemann. 2009. News from OPUS ? a collection
of multilingual parallel corpora with tools and in-
terfaces. In Recent Advances in Natural Language
Processing, volume 5, pages 237?248, Borovets,
Bulgaria. John Benjamins.
U.S. National Library of Medicine. 2009. UMLS
reference manual. Metathesaurus. Bethesda, MD,
USA.
K. W?aschle and S. Riezler. 2012. Analyzing paral-
lelism and domain similarities in the MAREC patent
corpus. In M. Salampasis and B. Larsen, edi-
tors, Multidisciplinary Information Retrieval, vol-
ume 7356 of Lecture Notes in Computer Science,
pages 12?27. Springer Berlin Heidelberg.
H. Wu and H. Wang. 2004. Improving domain-specific
word alignment with a general bilingual corpus. In
Robert E. Frederking and Kathryn B. Taylor, editors,
Machine Translation: From Real Users to Research,
volume 3265 of Lecture Notes in Computer Science,
pages 262?271. Springer Berlin Heidelberg.
C. Wu, F. Xia, L. Deleger, and I. Solti. 2011. Statistical
machine translation for biomedical text: are we there
yet? AMIA Annual Symposium proceedings, pages
1290?1299.
D. Zeman. 2012. Data issues of the multilingual trans-
lation matrix. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 395?
400, Montr?eal, Canada. ACL.
228
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 409?413,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Tolerant BLEU: a Submission to the WMT14 Metrics Task
Jind
?
rich Libovick?y and Pavel Pecina
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal ad Applied Linguistics
{libovicky, pecina}@ufal.mff.cuni.cz
Abstract
This paper describes a machine translation
metric submitted to the WMT14 Metrics
Task. It is a simple modification of the
standard BLEU metric using a monolin-
gual alignment of reference and test sen-
tences. The alignment is computed as
a minimum weighted maximum bipartite
matching of the translated and the refer-
ence sentence words with respect to the
relative edit distance of the word prefixes
and suffixes. The aligned words are in-
cluded in the n-gram precision compu-
tation with a penalty proportional to the
matching distance. The proposed tBLEU
metric is designed to be more tolerant to
errors in inflection, which usually does not
effect the understandability of a sentence,
and therefore be more suitable for measur-
ing quality of translation into morphologi-
cally richer languages.
1 Introduction
Automatic evaluation of machine translation (MT)
quality is an important part of the machine trans-
lation pipeline. The possibility to run an evalua-
tion algorithm many times while training a system
enables the system to be optimized with respect to
such a metric (e.g., by Minimum Error Rate Train-
ing (Och, 2003)). By achieving a high correlation
of the metric with human judgment, we expect the
system performance to be optimized also with re-
spect to the human perception of translation qual-
ity.
In this paper, we propose an MT metric called
tBLEU (tolerant BLEU) that is based on the stan-
dard BLEU (Papineni et al., 2002) and designed to
suit better when translation into morphologically
richer languages. We aim to have a simple lan-
guage independent metric that correlates with hu-
man judgment better than the standard BLEU.
Several metrics try to address this problem
as well and usually succeed to gain a higher
correlation with human judgment (e.g. ME-
TEOR (Denkowski and Lavie, 2011), TerrorCat
(Fishel et al., 2012)). However, they usually
use some language-dependent tools and resources
(METEOR uses stemmer and parahprasing tables,
TerrorCat uses lemmatization and needs training
data for each language pair) which prevent them
from being widely adopted.
In the next section, the previous work is briefly
summarized. Section 3 describes the metric in de-
tail. The experiments with the metric are described
in Section 4 and their results are summarized in
Section 5.
2 Previous Work
BLEU (Papineni et al., 2002) is an established and
the most widely used automatic metric for evalua-
tion of MT quality. It is computed as a harmonic
mean of the n-gram precisions multiplied by the
brevity penalty coefficient which ensures also high
recall. Formally:
BLEU = BP ? exp
(
4
?
n=1
1
4
log p
n
)
,
where BP is the brevity penaly defined as follows:
BP =
{
1 if c > r
e
1?
r
c
otherwise
,
c is the length of the test sentence (number of to-
kens), r is the length of the reference sentence, and
p
n
is the proportion of n-grams from the test sen-
tence found in the reference translations.
The original experiments with the English to
Chinese translation (Papineni et al., 2002) re-
ported very high correlation of BLEU with human
judgments. However, these scores were computed
using multiple reference translations (to capture
translation variability) but in practice, only one
409
Reference:
Source:
Translation:
I(am(driving(a(new(red(car
Jedu(nov?m(?erven?m(autem
Jedu(s(nov?m(?erven?m(autob
7 3
Corrected(and(
wighted(translation: UJeduE(1p(UsE(1p(Unov?mE(243p(U?erven?mE(546p(UautemE(143p
Unigram(precision
Jedu
s
nov?m
?erven?m
autem
tBLEU(unigram(precision(=
1
1
243
546
143
Bigram(precision
Jedu(s
s(nov?m
nov?m(?erven?m
?erven?m(autem
tBLEU(bigram(precision(=
1
546
344
7412
11
6 5(?(7b367
avgU1E1p(=
avgU1E(243p(=
avgU243E(546p(=
avgU546E143p(=
BLEU(unigram(precision(=(1(4(5(=(7b2
Jedu
s
nov?m
?erven?m
auto
Jedu(s
s(nov?m
nov?m(?erven?m
?erven?m(auto
16
12 4(?(7b333
BLEU(bigram(precision(=(7(4(4(=(7
26131
Figure 1: An example of the unigram and bigram precision computation for translation from English to
Czech with the test sentence having minor inflection errors and an additional preposition. The first two
lines contain the source sentence in English and a correct reference translation in Czech. On the third
line, there is an incorrectly translated sentence with errors in inflection. Between the second and the
third line, the matching with respect to the affix distance is shown. The fourth line contains the corrected
test sentence with the words weights. The bottom part of the figure shows computation of the unigram
and bigram precisions. The first column contains the original translation n-grams, the second one the
corrected n-grams, the third one the n-gram weights and the last one indicates whether a matching n-
gram is contained in the reference sentence.
reference translation is usually available and there-
fore the BLEU scores are often underestimated.
The main disadvantage of BLEU is the fact that
it treats words as atomic units and does not allow
any partial matches. Therefore, words which are
inflectional variants of each other are treated as
completely different words although their mean-
ing is similar (e.g. work, works, worked, working).
Further, the n-gram precision for n> 1 penalizes
difference in word order between the reference and
the test sentences even though in languages with
free word order both sentences can be correct (Bo-
jar et al., 2010; Condon et al., 2009).
There are also other widely recognized MT
evaluation metrics: The NIST score (Dodding-
ton, 2002) is also an n-gram based metric, but
in addition it reflects how informative particular
n-grams are. A metric that achieves a very high
correlation with human judgment is METEOR
(Denkowski and Lavie, 2011). It creates a mono-
lingual alignment using language dependent tools
as stemmers and synonyms dictionaries and com-
putes weighted harmonic mean of precision and
recall based on the matching.
Some metrics are based on measuring the
edit distance between the reference and test sen-
tences. The Position-Independent Error Rate
(PER) (Leusch et al., 2003) is computed as
a length-normalized edit distance of sentences
treated as bags of words. The Translation Edit
Rate (TER) (Snover et al., 2006) is a number of
edit operation needed to change the test sentence
to the most similar reference sentence. In this
case, the allowed editing operations are insertions,
deletions and substitutions and also shifting words
within a sentence.
A different approach is used in TerrorCat
(Fishel et al., 2012). It uses frequencies of auto-
matically obtained translation error categories as
base for machine-learned pairwise comparison of
translation hypotheses.
In the Workshop of Machine Translation
(WMT) Metrics Task, several new MT metrics
compete annually (Mach?a?cek and Bojar, 2013). In
the comptetition, METEOR and TerrorCat scored
better that the other mentioned metrics.
410
3 Metric Description
tBLEU is computed in in two steps. Similarly to
the METEOR score, we first make a monolingual
alignment between the reference and the test sen-
tences and then apply an algorithm similar to the
standard BLEU but with modified n-gram preci-
sions.
The monolingual alignment is computed as a
minimum weighted maximum bipartite matching
between words in a reference sentence and a trans-
lation sentence
1
using the Munkres assignment al-
gorithm (Munkres, 1957).
We define a weight of an alignment link as the
affix distance of the test sentence word w
t
i
and the
reference sentence word w
r
j
: Let S be the longest
common substring of w
t
i
and w
r
i
. We can rewrite
the strings as a concatenation of a prefix, the com-
mon substring and a suffix:
w
t
= w
t
i,p
Sw
t
i,s
w
r
= w
r
j,p
Sw
r
j,s
Further, we define the affix distance as:
AD(w
r
, w
t
)= max
{
1,
L(w
r
j,p
,w
t
i,p
)+L(w
r
s,j
,w
t
s,i
)
|S|
}
if |S| > 0 and AD(w
r
, w
t
) = 1 otherwise. L is the
Levensthein distance between two strings.
For example the affix distance of two Czech
words vzpomenou and zapomenout (different
forms of verbs remember and forget) is computed
in the following way: The longest common sub-
string is pomenou which has a length of 7. The
prefixes are vz and za and their edit distance is 2.
The suffixes are an empty string and t which with
the edit distance 1. The total edit distance of pre-
fixes and suffixes is 3. By dividing the total edit
distance by the length of the longest common sub-
string, we get the affix distance
3
7
? 0.43.
We denote the resulting set of matching pairs
of words as M = {(w
r
i
, w
t
i
)}
m
i=1
and for each test
sentence S
t
= (w
t
1
, ..., w
t
m
) we create a corrected
sentence
?
S
t
= (w?
t
1
, ..., w?
t
m
) such that
w?
t
i
=
{
w
r
if ?w
t
: (w
r
, w
t
)?M & AD(w
r
, w
t
) ? 
w
t
i
otherwise.
This means that the words from the test sen-
tence which were matched with the affix distance
1
The matching is always one-to-one which means that
some words remain unmatched if the sentences have differ-
ent number of words.
0 0.2 0.4 0.6 0.8 10.7
0.75
0.8
0.85
0.9
0.95
en-csen-deen-esen-fr
Affix distance threshold
Pea
rson
's co
rrela
tion 
coef
fitien
t
0 0.2 0.4 0.6 0.8 10.91
0.92
0.93
0.94
0.95
0.96
0.97
cs-ende-enes-enfr-en
Affix distance threshold
Pea
rson
's co
rrela
tion 
coef
ficie
nt
Figure 2: Dependence of the Pearson?s correlation
of tBLEU with the WMT13 human judgments on
the affix distance threshold for translations from
English and to English.
smaller than  are ?corrected? by substituting them
by the matching words from the reference sen-
tence. The threshold  is a free parameter of the
metric. When the threshold is set to zero, no
corrections are made and therefore the metric is
equivalent to the standard BLEU.
The words in the corrected sentence are as-
signed the weights as follows:
v(w?
t
i
) =
{
1?AD(w?
t
i
, w
t
i
) if w?
t
i
6= w
t
i
1 otherwise.
In other words, the weights penalize the corrected
words proportionally to the affix distance from the
original words.
While computing the n-gram precision, two
matching n-grams (w?
t
1
, . . . w?
t
n
) and (w
r
1
, . . . w
r
n
)
contribute to the n-gram precision with a score of
s(w
t
1
, . . . , w
t
n
) =
n
?
i=1
v(w?
t
i
) / n
instead of one as it is in the standard BLEU. The
rest of the BLEU score computation remains un-
changed. While using multiple reference transla-
tion, the matching is done for each of the refer-
ence sentence, and while computing the n-gram
precision, the reference sentences with the highest
weight is chosen. The computation of the n-gram
precision is illustrated in Figure 1.
411
direction BLEU METEOR tBLEU
en-cs .781 .860 .787
en-de .835 .868 .850
en-es .875 .878 .884
en-fr .887 .906 .906
from English .844 .878 .857
Table 1: System level Pearson?s correlation with
the human judgment for systems translating from
English computed on the WMT13 dataset.
4 Evaluation
We evaluated the proposed metric on the dataset
used for the WMT13 Metrics Task (Mach?a?cek and
Bojar, 2013). The dataset consists of 135 systems?
outputs in 10 directions (5 into English 5 out of
English). Each system?s output and the reference
translation contain 3000 sentences. According to
the WMT14 guidelines, we report the the Pear-
son?s correlation coefficient instead of the Spear-
man?s coefficient that was used in the last years.
Twenty values of the affix distance threshold
were tested in order to estimate what is the most
suitable threshold setting. We report only the sys-
tem level correlation because the metric is de-
signed to compare only the whole system outputs.
5 Results
The tBLEU metric generally improves the cor-
relation with human judgment over the standard
BLEU metric for directions from English to lan-
guages with richer inflection.
Examining the various threshold values showed
that dependence between the affix distance thresh-
old and the correlation with the human judgment
varies for different language pairs (Figure 2). For
translation from English to morphologically richer
languages than English ? Czech, German, Spanish
and French ? using the tBLEU metric increased
the correlation over the standard BLEU. For Czech
the correlation quickly decreases for threshold val-
ues bigger than 0.1, whereas for the other lan-
guages it still grows. We hypothesize this because
the big morphological changes in Czech can en-
tirely change the meaning.
For translation to English, the correlation
slightly increases with the increasing threshold
value for translation from French and Spanish, but
decreases for Czech and German.
There are different optimal affix distance
direction BLEU METEOR tBLEU
cs-en .925 .985 .927
de-en .916 .962 .917
es-en .957 .968 .953
fr-en .940 .983 .933
to English .923 .974 .935
Table 2: System level Pearson?s correlation with
the human judgment for systems translating to En-
glish computed on the WMT13 dataset.
thresholds for different language pairs. However,
the threshold of 0.05 was used for our WMT14
submission because it had the best average cor-
relation on the WMT13 data set. Tables 1 and
2 show the results of the tBLEU for the particu-
lar language pairs for threshold 0.05. While com-
pared to the BLEU score, the correlation is slightly
higher for translation from English and approxi-
mately the same for translation to English.
The results on the WMT14 dataset did not show
any improvement over the BLEU metric. The rea-
son of the results will be further examined.
6 Conclusion and Future Work
We presented tBLEU, a language-independent MT
metric based on the standard BLEU metric. It in-
troduced the affix distance ? relative edit distances
of prefixes and suffixes of two string after remov-
ing their longest common substring. Finding a
matching between translation and reference sen-
tences with respect to this matching allows a pe-
nalized substitution of words which has been most
likely wrongly inflected and therefore less penal-
izes errors in inflection.
This metric achieves a higher correlation with
the human judgment than the standard BLEU
score for translation to morphological richer lan-
guages without the necessity to employ any lan-
guage specific tools.
In future work, we would like to improve word
alignment between test and reference translations
by introducing word position and potentially other
features, and implement tBLEU in MERT to ex-
amine its impact on system tuning.
7 Acknowledgements
This research has been funded by the Czech Sci-
ence Foundation (grant n. P103/12/G084) and the
EU FP7 project Khresmoi (contract no. 257528).
412
References
Ond?rej Bojar, Kamil Kos, and David Mare?cek. 2010.
Tackling sparse data issue in machine translation
evaluation. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 86?91. Association for
Computational Linguistics.
Sherri Condon, Gregory A Sanders, Dan Parvaz, Alan
Rubenstein, Christy Doran, John Aberdeen, and
Beatrice Oshika. 2009. Normalization for auto-
mated metrics: English and arabic speech transla-
tion. Proceedings of MT Summit XII. Association
for Machine Translation in the Americas, Ottawa,
ON, Canada.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 85?91, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Mark Fishel, Rico Sennrich, Maja Popovi?c, and Ond?rej
Bojar. 2012. Terrorcat: a translation error
categorization-based mt quality metric. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 64?70. Association for Compu-
tational Linguistics.
Gregor Leusch, Nicola Ueffing, Hermann Ney, et al.
2003. A novel string-to-string distance measure
with applications to machine translation evaluation.
In Proceedings of MT Summit IX, pages 240?247.
Citeseer.
Matou?s Mach?a?cek and Ond?rej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45?51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
James Munkres. 1957. Algorithms for the assignment
and transportation problems. Journal of the Society
for Industrial & Applied Mathematics, 5(1):32?38.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223?231.
413
