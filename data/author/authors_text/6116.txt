Ecient Dialogue Strategy to Find Users Intended Items
from Information Query Results
Kazunori Komatani Tatsuya Kawahara Ryosuke Ito Hiroshi G Okuno
Graduate School of Informatics Kyoto University
Kyoto  Japan
fkomatani kawahara rito okunogkuis	kyotou	ac	jp
Abstract
We address a dialogue framework that narrows
down the users query results obtained by an in
formation retrieval system The followup dia
logue to constrain query results is signicant es
pecially with the speech interfaces such as tele
phones because a lot of query results cannot be
presented to the user The proposed dialogue
framework generates guiding questions based on
an information theoretic criterion to eliminate
retrieved candidates by a spontaneous query
without assuming a semantic slot structure We
rst describe its concept on general information
query tasks and then deal with a query task
on the appliance manual where structured task
knowledge is available A hierarchical conrma
tion strategy is proposed by making use of a tree
structure of the manual and then three cost
functions for selecting optimal question nodes
are compared Experimental evaluation demon
strates that the proposed system helps users
nd their intended items more eciently
 Introduction
In the past years a great number of spoken
dialogue systems have been developed Their
typical task domains include airline information
Levin et al 			
 Potamianos et al 			

SanSegundo et al 			 and train information
Allen et al 
 Bennacef et al 
 Sturm
et al 
 Lamel et al  Most of them
model speech understanding process as convert
ing recognition results into semantic representa
tions equivalent to database query SQL com
mands and dialogue process as disambiguating
their unxed slots Usually the semantic slots
are dened a priori and manually The approach
is workable only when data structure of the ap
plication is wellorganized typically as a rela
tional database RDB
Dierent and more exible approach is
needed for spoken dialogue interfaces to ac
cess information described in less rigid format
in particular normal text database For the
purpose information retrieval IR technique is
useful to nd a list of matching documents from
the input query Typically keywords are ex
tracted from the query and statistical matching
is performed Call routing task ChuCarroll
and Carpenter  can be regarded as the
special case
In IR systems many candidates are usually
obtained as a query result thus there is a sig
nicant problem of how to nd the users in
tended item among them Especially either on
the telephone or electrical appliances there is
not a large screen displaying the candidates and
all the query results cannot be presented to a
user So it is desirable for the system to narrow
down the query results interactively Moreover
interactive query is more friendly to novice users
rather than requiring them to input a detailed
query from the beginning
In this paper we address a dialogue strat
egy to nd the users intended item from the
retrieved result which is initiated by a spon
taneous query utterance In section  we de
scribe a method to generate a guiding question
that narrows down the query results eciently
using an example of a restaurant query task
The question is selected based on an informa
tion theoretic criterion In section  we present
a dialogue management method for a query task
on the appliance manual where structured task
knowledge is available We propose a conr
mation strategy by making use of a tree struc
ture of the manual and dene three cost func
tions for selecting question nodes The method
is evaluated by the number of average dialogue
turns
Although there are previous studies on
optimizing dialogue strategies Niimi and
Kobayashi 
 Levin et al 
 Litman
et al 			 most of them assume the tasks
of lling semantic slots that are denitely and
manually dened and few focus on followup
dialogue of information retrieval For example
Denecke  proposed a method to generate
guiding questions by making use of a tree struc
ture constructed by unifying retrieved items
based on semantic slots In this paper we do
not assume any structure of semantic slots In
stead we make use of distribution of document
statistics or a structure of task knowledge We
also investigate cost functions for optimal dia
logue control by taking into account of speech
recognition errors
 Dialogue Strategy in General
Information Query Task
Interaction in an information query task can be
regarded as a process seeking a common part
between the users request and system knowl
edge In order to help users to nd their in
tended items from the system knowledge the
system has to carry out not only interpreting
what users say but also showing the relevant
portion of the system knowledge to them
We assume that users freely set and retract
query keys based on their preference for infor
mation query systems If many candidates still
remain even after specifying all possible hisher
preference to the system users may have di
culty in narrowing down further the query re
sult Thus the system should generate ecient
guiding questions to help users nd their in
tended items
In this section we presume the system knowl
edge as a pair of an item and a set of keywords
Figure  We dene keywords as a set of
words representing contents of the items and
their categories such as place food and so on
are given This is similar to indexing words in
a conventional information retrieval task Note
that it is not needed that the system knowledge
is structured like an RDB
Keywords are extracted from a users utter
ance and are matched with the system knowl
edge Here we adopt the following matching
 
Restaurant A
Chinese noodles meat dumpling
Shinjuku Kabukicho Ekoda
Restaurant B
Chinese noodles meat dumpling
Shinjuku Kabukicho
Restaurant C
Chinese noodles meat dumpling
noodles with boiledporkribs
Takadanobaba
Restaurant D
Chinese noodles fried garlic Yebisu

 
Figure  An example of system knowledge
function for each item j
L
j

X
iK
j

CM
i
 log
N
df
i

Here K
j
is a set of keywords for item j CM
i
is
a condence measure of speech recognition for
keyword i Komatani and Kawahara 			 N
is the total number of items and df
i
is the num
ber of items including keyword i Intuitively
keyword that is recognized with high condence
and does not appear in many items gets higher
likelihood L
j
by CM
i
and df
i
 respectively
Then we dene amount of information that
is obtained when the system generates yesno
question and the user answers it Here C is a
current query condition A is a condition that
is added by the systems question and countx
is the number of items that satisfy the condi
tion x The condition consists of the conjunc
tion of the keywords the user specied Suppose
each item occurs by equal likelihood the fol
lowing equation denotes the likelihood p

A
yes

that the yesno question corresponding to the
adding condition A will be answered as yes
p

A
yes
 
countC A
countC
We weight on each item j with the likelihood
L
j

pA
yes
 
P
jfCAg
L
j
P
jfCg
L
j
The amount of information that is obtained
when the users answer is yes is represented
as follows
IA
yes
  log


pA
yes

The following equation gives HA the ex
pected value of amount of information that is
obtained by generating a question about con
dition A and getting users answer yes or
no
HA 
X
xfyesnog
pA
x
 log


pA
x

By calculating HA for all conditions A that
can be added to the current query condition
the system generates the question that has the
maximum value of HA The question is gen
erated using the category information of each
keyword
Because the obtained condition A is selected
by a viewpoint of narrowing down the current
set of items eciently the selected condition
may be unimportant for the user In such a case
it is not cooperative to force the user an ar
mative or negative reply Our system does not
force the reluctant decision by allowing the user
to say It does not matter anyhow Instead
the system presents the second best proposal
We explain the method with the following ex
ample in our restaurant query system in the
Tokyo area When a user says Please tell me a
restaurant where I can eat Chinese noodle and
meat dumpling in Shinjuku area three key
words are extracted Shinjuku Chinese noo
dle and meat dumpling As a result of the
matching using these three keywords  query
results are obtained It is not cooperative to
read out all of the  query results with a TTS
texttospeech system Here the expected val
ues of amount of information HA are calcu
lated for each condition that corresponds to key
words included in the matched items except for
the three keywords Shinjuku Chinese noo
dle and meat dumpling Then we select the
keyword noodles with boiledporkribs that
has the maximum value HA By generating a
question like Would you like one which serves
noodles with boiledporkribs and obtaining
a reply from the user the system adds the new
condition and narrows down the candidates ef
ciently If the user thinks that the condition
noodles with boiledporkribs is not impor
tant and tells the system so for example Ei
ther will do the system can show the second
best proposal Would you like one located in
Kabukicho area Thus the query result can
be narrowed down without forcing the user un
natural yesno answers
 Dialogue Strategy for Query on
Appliance Manuals
In this section we present another ecient solu
tion in the case that the structure or hierarchy
of task knowledge is available The task here
is to nd the appropriate item in the manual
of electric appliances with a spoken dialogue in
terface Such an interface will be useful as the
recent appliances become complex with many
features and so are their manuals In the ap
pliances such as VTR Video Tape Recorder
and FAX machines there is not a large screen
to display the list of matched candidates to be
selected by the user Therefore we address a
spoken dialogue strategy to determine the most
appropriate one from the list of candidates
An alternative system design is the use of di
rectory search as adopted in voice portal sys
tems where the documents are hierarchically
structured and the system prompts users to se
lect one of the menu from the top to the leaf
The method is rigid and not userfriendly since
users often have trouble in selection and want
to specify by their own expression The pro
posed system allows users to make queries spon
taneously and makes use of the directory struc
ture in the followup dialogue to determine the
most appropriate one
 System Overview
An overview of the system is illustrated in Fig
ure  It consists of following processes
 Keyword spotting from user utterances us
ing an ASR automatic speech recognition
system Kawahara et al 
A natural spoken language query is ac
cepted and keywords are extracted A con
dence measure CM
i
is assigned to each
keyword i based on the Nbest recognition
result Komatani and Kawahara 			
yes/no
system user
manual
tree
structure
entries
keyword spotting
matching
follow-up
dialogue
keywords with
confidence
entries with
likelihood
result
spoken query
Figure  System overview
 Matching with manual items documents
The extracted keywords are matched with
a set of manual items The matching is
performed on the initial portion index and
rst summary paragraph of each manual
section We adopt the following matching
score function for an item j K
j
is a set of
keywords for item j
L
j


n
j
X
iK
j
CM
i
 log
N
df
i

Here df
i
is the number of items that con
tain keyword i referred as a document fre
quency and N is the total number of items
The inverse document frequency idf is
weighted with a condence measure CM
i
and summed over keywords then normal
ized by n
j
 the number of keywords in the
item j
 Generating dialogue to determine the most
appropriate one from the list of candidates
As a result of the matching many candi
dates are usually found They may include
irrelevant ones because of speech recogni
tion errors But it is not practical to read
out all of them in order with a TTS text
tospeech system Therefore dialogue is
invoked to narrow down to the intended
one This dialogue is restricted to system
initiated yesno questions in order to
play record search setting
normal
play
slow
play
.............
Figure  Example of tree structure of manual
avoid further recognition errors and back
up dialogue The dialogue strategy is ex
plained in the next subsection
 Dialogue Strategy using Structure
of Manual
If one of the candidates is more plausible than
others with a signicant margin we should
make conrmation on it When there are many
candidates with similar condence and they
can be hierarchically grouped into several cate
gories we had better rst identify which cate
gory the intended one belongs to In this work
we make use of the section structure of the man
ual ie section is the rst layer subsection is
the secondlayer and so on The tree structure
is automatically derived from its table of con
tents An example for VTR manual is shown in
Figure 
For each node of the tree likelihood L

j
is
assigned as follows
 For a leaf node the matching score L
j
is
assigned after normalizing so that the sum
over all leaves manual items is 	
 For a nonleaf node the sum of the likeli
hood of its children nodes is assigned
Then a dialogue is generated as follows
 Among ancestor nodes of the leaf of the
largest likelihood L

j
 pick up the one whose
heuristic cost function described below is
smallest
 Make a yesno question on the node for
example Do you want to know about 
The content of the question is associated
with the section title
 If the users answer is yes eliminate the
nodes other than descendants of the con
0.4 0.1 0 0.1 0.2 0.2 0 0
0.5 0.1 0.4 0
0.6 0.4
0.4 0.1 0 0 0 0 0 0
0.5 0 0 0
0.5 0
0 0 0 0.1 0.2 0.2 0 0
0 0.1 0.4 0
0.1 0.4
"Yes" "No"
leaf with
best score
selected by
cost function
generate a "yes-no" question
on selected node
Figure Using a Mixture of N-Best Lists from Multiple MT Systems
in Rank-Sum-Based Confidence Measure for MT Outputs ?
Yasuhiro Akiba?,?, Eiichiro Sumita?, Hiromi Nakaiwa?,
Seiichi Yamamoto?, and Hiroshi G. Okuno?
? ATR Spoken Language Translation Research Laboratories
2-2-2 Hikaridai, Keihana Science City, Kyoto 619-0288, Japan
? Graduate School of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku, Kyoto 606-8501, Japan
{yasuhiro.akiba, eiichiro.sumita, hiromi.nakaiwa seiichi.yamamoto}@atr.jp, and okuno@i.kyoto-u.ac.jp
Abstract
This paper addressees the problem of eliminat-
ing unsatisfactory outputs from machine trans-
lation (MT) systems. The authors intend to
eliminate unsatisfactory MT outputs by using
confidence measures. Confidence measures for
MT outputs include the rank-sum-based confi-
dence measure (RSCM) for statistical machine
translation (SMT) systems. RSCM can be ap-
plied to non-SMT systems but does not always
work well on them. This paper proposes an
alternative RSCM that adopts a mixture of the
N-best lists from multiple MT systems instead
of a single-system?s N-best list in the exist-
ing RSCM. In most cases, the proposed RSCM
proved to work better than the existing RSCM
on two non-SMT systems and to work as well
as the existing RSCM on an SMT system.
1 Introduction
This paper addresses the challenging problem of
eliminating unsatisfactory outputs from machine
translation (MT) systems, which are subsystems of
a speech-to-speech machine translation (S2SMT)
system. The permissible range of translation quality
by MT/S2SMT systems depends on the user. Some
users permit only perfect translations, while other
users permit even translations with flawed grammar.
Unsatisfactory MT outputs are those whose transla-
tion quality is worse than the level the user can per-
mit.
In this paper, the authors intend to eliminate un-
satisfactory outputs by using confidence measures
for MT outputs. The confidence measures1 indicate
how perfect/satisfactory the MT outputs are. In the
? This research was supported in part by the Ministry of Public
Management, Home Affairs, Posts and Telecommunications,
Japan.
1These confidence measures are a kind of automatic evalu-
ator such as mWER (Niessen et al, 2000) and BLEU (Papineni
et al, 2001). While mWER and BLEU cannot be used online,
these confidence measures can. This is because the former are
based on reference translations, while the latter is not.
discipline of MT, confidence measures for MT out-
puts have rarely been investigated.
The few existing confidence measures include
the rank-sum-based confidence measure (RSCM)
for statistical machine translation (SMT) systems,
Crank in (Ueffing et al, 2003). The basic idea
of this confidence measure is to roughly calculate
the word posterior probability by using ranks of
MT outputs in an N-best list from an SMT system.
In the discipline of non-parametric statistical test,
ranks of numerical values are commonly used in-
stead of the numerical values themselves for statis-
tical tests. In the case of the existing RSCM, the
ranks of probabilities of MT outputs in the N-best
list were used instead of the probabilities of the out-
puts themselves. The existing RSCM scores each
word in an MT output by summing the comple-
mented ranks of candidates in the N-best list that
contain the same word in a Levenshtein-aligned po-
sition (Levenshtein, 1966). When the confidence
values of all words in the MT output are larger than
a fixed threshold, the MT output is judged as cor-
rect/perfect. Otherwise, the output is judged as in-
correct/imperfect.
The existing RSCM does not always work well
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
Performance of existing method (A|BCD)
J2E SAT + Existing method
J2E HPAT + Existing method
J2E D3 + Existing method
Figure 1: Performance of the existing RSCM on three
different types of Japanese-to-English (J2E) MT sys-
tems: D3, HPAT, and SAT. The existing RSCM tried to
accept perfect MT outputs (grade A in Section 4) and to
reject imperfect MT outputs (grades B, C, and D in Sec-
tion 4).
on types of MT systems other than SMT systems.
Figure 1 shows the differences among the perfor-
mances, indicated by the Receiver Operating Char-
acteristics (ROC) curve (Section 4.1), of the exist-
ing RSCM on each of three MT systems (Section
4.2.1): D3, HPAT, and SAT (Doi and Sumita, 2003;
Imamura et al, 2003; Watanabe et al, 2003). Only
SAT is an SMT system; the others are not. The ideal
ROC curve is a square (0,1), (1,1), (1,0); thus, the
closer the curve is to a square, the better the perfor-
mance of the RSCM is. The performances of the
existing RSCM on the non-SMT systems, D3 and
HPAT, are much worse than that on the SMT sys-
tem, SAT.
The performance of the existing RSCM depends
on the goodness/density of MT outputs in the N-
best list from the system. However, the system?s
N-best list does not always give a good approxi-
mation of the total summation of the probability
of all candidate translations given the source sen-
tence/utterance. The N-best list is expected to ap-
proximate the total summation as closely as possi-
ble.
This paper proposes a method that eliminates
unsatisfactory top output by using an alternative
RSCM based on a mixture of N-best lists from mul-
tiple MT systems (Figure 2). The elimination sys-
tem is intended to be used in the selector architec-
ture, as in (Akiba et al, 2002). The total transla-
tion quality of the selector architecture proved to be
better than the translation quality of each element
MT system. The final output from the selection sys-
tem is the best among the satisfactory top2 outputs
from the elimination system. In the case of Fig-
ure 2, the selection system can receive zero to three
top MT outputs. When the selection system receive
fewer than two top MT outputs, the selection sys-
tem merely passes a null output or the one top MT
output.
The proposed RSCM differs from the existing
RSCM in its N-best list. The proposed RSCM re-
2To distinguish the best output from the selection system,
the MT output in the first place in each N-best list (e.g., N-best
lista in Figure 2 ) refers to the top MT output.
The best output
Elimination System
Selection System
Satisfactory top outputs
MTa MTb MTc
The top outputa
?
The M-th best outputa
Input
M-best lista
The top outputb
?
The M-th best outputb
M-best listb
The top outputc
?
The M-th best outputc
M-best listc
Figure 2: Image of our eliminator
ceives an M-best list from each element MT sys-
tem. Next, it sorts the mixture of the MT outputs in
all M-best lists in the order of the average product
(Section 3.2) of the scores of a language model and
a translation model (Akiba et al, 2002). This sorted
mixture is used instead of the system?s N-best list in
the existing RSCM.
To experimentally evaluate the proposed RSCM,
the authors applied the proposed RSCM and the ex-
isting RSCM to a test set of the Basic Travel Ex-
pression Corpus (Takezawa et al, 2002). The pro-
posed RSCM proved to work better than the exist-
ing RSCM on the non-SMT systems and to work as
well as the existing RSCM on the SMT system.
The next section outlines the existing RSCM.
Section 3 proposes our RSCM. Experimental results
are shown and discussed in Section 4. Finally, our
conclusions are presented in Section 5.
2 The Existing RSCM
The existing confidence measures include the rank-
sum-based confidence measure (RSCM) for SMT
systems (Ueffing et al, 2003). The basic idea of
this RSCM is to roughly calculate the word poste-
rior probability by using ranks of MT outputs in the
N-best list of an SMT system. That is, the ranks of
probabilities of MT outputs in the N-best list were
used instead of the probabilities of the outputs them-
selves, as in the non-parametric statistical test.
Hereafter, e?I1 and wIn1 denote the top output2
and the n-th best output in the N-best list, respec-
tively. e?i denotes the i-th word in the top MT output
e?I1. Li(e?I1, wIn1 ) denote the Levenshtein alignment3
(Levenshtein, 1966) of e?i on the n-th best output
wIn1 according to the top output e?I1. The existing
RSCM of the word e?i is the sum of the ranks of MT
outputs in an N-best list containing the word e?i in a
position that is aligned to i in the Levenshtein align-
ment, which is normalized by the total rank sum:
Crank(e?i) =
?N
n=1(N ? n) ? ?(e?i, Li(e?I1, wIn1 ))
N(N + 1)/2 ,
where ?(?, ?) is the Kronecker function, that is, if
words/morphemes x and y are the same, ?(x, y) =
1; otherwise, ?(x, y) = 0. Thus, only in the case
where e?i and Li(e?I1, wIn1 ) are the same, the rank of
the MT output wIn1 , N ? n, is summed. In the
calculation of Crank, N ? n is summed instead of
the rank n because ranks near the top of the N-best
list contribute more to the score Crank.
3This is the word on the n-th best output wIn1 , aligned with
the i-th word e?i, in the calculation of edit distance from the top
MT output e?I1 to the n-th best output wIn1 .
In this paper, the calculation of Crank is slightly
modified to sum N ? n + 1 so that the total sum-
mation is equal to N(N + 1)/2. Moreover, when
there are MT outputs that have the same score, such
MT outputs are assigned the average rank as in the
discipline of non-parametric statistical test.
As shown in Section 1, the existing RSCM does
not always work well on types of MT systems other
than SMT systems. This is because the system?s
N-best list does not always give a good approxi-
mation of the total summation of the probability
of all candidate translations given the source sen-
tence/utterance. The N-best list is expected to ap-
proximate the total summation as closely as possi-
ble.
3 Proposed Method
In this section, the authors propose a method that
eliminates unsatisfactory top output by using an al-
ternative RSCM based on a mixture of N-best lists
from multiple MT systems. The judgment that
the top output is satisfactory is based on the same
threshold comparison as the judgment that the top
output is perfect, as mentioned in Section 1. The
elimination system and the alternative RSCM are
explained in Sections 3.1 and 3.2, respectively.
3.1 Elimination system
This section proposes a method that eliminates
unsatisfactory top outputs by using an alternative
RSCM based on a mixture of N-best lists from mul-
tiple MT systems (Figure 3). This elimination sys-
tem is intended to be used in the selector architec-
ture (Figure 2). The elimination system receives
an M-best list from each element MT system and
outputs only top2 outputs whose translation quality
is better than or as good as that which the user can
permit. In the case of Figure 3, the number of MT
systems is three; thus, the elimination system can
output zero to three top MT outputs, which depends
on the number of the eliminated top outputs.
MTa MTb MTc
The top outputa
?
The M-th best outputa
Input
Satisfactory top outputs
M-best lista
The top outputb
?
The M-th best outputb
M-best listb
The top outputc
?
The M-th best outputc
M-best listc
3M outputs sorted in the higher order
Sorter based on SMT?s scoring system
Checker based on rank sum
Elimination System
Figure 3: Proposed RSCM
The proposed elimination system judges whether
a top output is satisfactory by using a threshold
comparison, as in (Ueffing et al, 2003). When
the confidence values of all words in the top out-
put, which are calculated by using the alternative
RSCM explained in Section 3.2, are larger than a
fixed threshold, the top output is judged as satisfac-
tory. Otherwise, the top output is judged as unsatis-
factory. The threshold was optimized on a develop-
ment corpus.
3.2 The proposed RSCM
The proposed RSCM is an extension of the existing
RSCM outlined in Section 2. The proposed RSCM
differs from the existing RSCM in the adopted N-
best list (Figure 3). The proposed RSCM receives
an M-best list from each element MT system. Next
the proposed RSCM sorts the mixture of all the MT
outputs in the order of the average product of the
scores of a language model and a translation model
(Akiba et al, 2002). This sorted mixture is alter-
natively used instead of the system?s N-best list in
the existing RSCM. That is, the proposed RSCM
checks whether it accepts/rejects each top MT out-
put in the original M-best lists by using the sorted
mixture; on the other hand, the existing RSCM
checks whether it accepts/rejects the top MT out-
put in the system?s N-best list by using the system?s
N-best.
For scoring MT outputs, the proposed RSCM
uses a score based on a translation model called
IBM4 (Brown et al, 1993) (TM-score) and a score
based on a language model for the translation tar-
get language (LM-score). As Akiba et al (2002)
reported, the products of TM-scores and LM-scores
are statistical variables. Even in the case where the
translation model (TM) and the language model for
the translation target language (LM) are trained on
a sub-corpus of the same size, changing the training
corpus also changes the TM-score, the LM-score,
and their product. Each pair of TM-score and LM-
score differently order the MT outputs.
For robust scoring, the authors adopt the multi-
ple scoring technique presented in (Akiba et al,
2002). The multiple scoring technique prepares
C1 Ck
C
k-fold Cross Validation
?..
TM1LM1 TMkLMk?..
C0
TM0LM0
Parallel corpus
Figure 4: Method for training multiple pairs of Lan-
guage Models (LMs) and Translation Models (TMs)
(Akiba et al, 2002).
multiple subsets of the full parallel corpus accord-
ing to k-fold cross validation (Mitchell, 1997) and
trains both TM and LM on each subset. Each
MT output is scored in k ways. For example, the
full parallel corpus C is divided into three subsets
Vi (i = 0, 1, 2). For each i, the proposed method
trains a translation model TMi on Ci (= C ? Vi)
and a language model LMi on the target-language
part of Ci (Figure 4). MT outputs in the mixture are
sorted by using the average of the product scores
by TMi and LMi for each i. In (Akiba et al, 2002),
this multiple scoring technique was shown to select
the best translation better than a single scoring tech-
nique that uses TM and LM trained from a full cor-
pus.
4 Experimental Comparison
The authors conducted an experimental compari-
son between the proposed RSCM and the existing
RSCM in the framework of the elimination system.
The task of both RSCMs was to judge whether each
top2 MT output from an MT system is satisfactory,
that is, whether the translation quality of the top MT
output is better than or as good as that which the
user can permit.
In this experiment, the translation quality of MT
outputs was assigned one of four grades: A, B,
C, or D as follows: (A) Perfect: no problems in
either information or grammar; (B) Fair: easy-to-
understand, with either some unimportant informa-
tion missing or flawed grammar; (C) Acceptable:
broken, but understandable with effort; (D) Non-
sense: important information has been translated in-
correctly. This evaluation standard was introduced
by Sumita et al (1999) to evaluate S2SMT systems.
In advance, each top MT output was evaluated by
nine native speakers of the target language, who
were also familiar with the source language, and
then assigned the median grade of the nine grades.
To conduct a fair comparison, the number of MT
outputs in the system?s N-best list and the number
of MT outputs in the mixture are expected to be
the same. Thus, the authors used either a three-
best list from each of three MT systems or a five-
best list from each of two non-SMT MT systems
for the proposed RSCM and a ten-best list for the
existing RSCM. Naturally, this setting4 is not disad-
vantageous for the existing RSCM.
4In the future, we will conduct a large-scale experiment to
investigate how both RSCMs work while increasing the size of
the system?s N-best list and the mixture of M-best lists.
Table 1: Confusion matrix
Accept Reject Subtotal
Satisfactory Vs,a Vs,r Vs (= Vs,a + Vs,r)
Unsatisfactory Vu,a Vu,r Vu (= Vu,a + Vu,r)
4.1 Evaluation metrics
The performances of both RSCMs were evaluated
by using three different metrics: ROC Curve, H-
mean, and Accuracy. For each MT system, these
metrics were separately calculated by using a con-
fusion matrix (Table 1). For example, for J2E
D3 (Section 4.2.1), the proposed RSCM checked
each top MT output from J2E D3 by using the input
mixture of three-best lists from the three J2E MT
systems (Section 4.2.1); on the other hand, the ex-
isting RSCM checked each top MT output from J2E
D3 by using the input ten-best list from J2E D3. For
J2E D3, the results were counted up into the con-
fusion matrix of each RSCM, and the metrics were
calculated as follows:
ROC Curve plots the correct acceptance rate ver-
sus the correct rejection rate for different values of
the threshold. Correct acceptance rate (CAR) is
defined as the number of satisfactory outputs that
have been accepted, divided by the total number of
satisfactory outputs, that is, Vs,a/Vs (Table 1). Cor-
rect rejection rate (CRR) is defined as the number
of unsatisfactory outputs that have been rejected, di-
vided by the total number of unsatisfactory outputs,
that is, Vu,r/Vu (Table 1).
H-mean is defined as a harmonic mean5 of
the CAR and the CRR (Table 1), 2 ? CAR ?
CRR/(CAR + CRR).
Accuracy is defined as a weighted mean6 of the
CAR and the CRR (Table 1), (Vs ? CAR + Vu ?
CRR)/(Vs + Vu) = (Vs,a + Vu,r)/(Vs + Vu).
For each performance of H-mean and Accuracy,
10-fold cross validation was conducted. The thresh-
old was fixed such that the performance was maxi-
mized on each non-held-out subset, and the perfor-
mance was calculated on the corresponding held-out
subset. To statistically test the differences in per-
formance (H-mean or Accuracy) between the confi-
dence measures, the authors conducted a pairwise t-
test (Mitchell, 1997), which was based on the results
of 10-fold cross validation. When the difference in
performance meets the following condition, the dif-
ference is statistically different at a confidence level
5This harmonic mean is used for summarizing two mea-
sures, each of which has a trade-off relationship with each
other. For example, F-measure is the harmonic mean of pre-
cision and recall, which is well used in the discipline of Infor-
mation Retrieval.
6This weighted mean is used for evaluating classification
tasks in the discipline of Machine Learning.
00.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
J2E-D3 (A|BCD)
0.80.7
y=x
Existing method
Proposed method (D3+HPAT+SAT)
Proposed method (D3+HPAT)
Existing method + reordering
Contours by H-mean
Figure 5: ROC Curves of both
RSCMs for J2E-D3
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
J2E-HPAT (A|BCD)
0.80.7
y=x
Existing method
Proposed method (D3+HPAT+SAT)
Proposed method (D3+HPAT)
Existing method + reordering
Contours by H-mean
Figure 6: ROC Curves of both
RSCMs for J2E-HPAT
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
J2E-SAT (A|BCD)
0.80.7
y=x
Existing method
Proposed method (D3+HPAT+SAT)
Contours by H-mean
Figure 7: ROC Curves of both
RSCMs for J2E-SAT
Table 2: Performance of MT systems: Each number
in the AB row indicates the ratio of A-or-B-graded
translation by each MT system. Each number in the
other rows similarly indicates corresponding ratios.
J2E MT systems E2J MT systems
D3 HPAT SAT D3 HPAT SAT
A 63.7 42.5 67.2 58.4 59.6 69.8
AB 72.1 63.7 74.7 72.9 75.4 81.1
ABC 78.8 79.0 82.5 83.3 86.8 88.0
of 1-?%.
|ppro ? pext| > t(?,10?1) ? S/
?
10,
where ppro and pext, respectively, denote the aver-
age performance of the proposed RSCM and the ex-
isting RSCM, t(?,10?1) denotes the upper ? point of
the Student?s t-distribution with (10 ? 1) degrees of
freedom, and S denotes the estimated standard de-
viation of the average difference in performance.
4.2 Experimental conditions
4.2.1 MT systems
Three English-to-Japanese (E2J) MT systems and
three Japanese-to-English (J2E) MT systems of the
three types described below were used. Table 2
shows the performances of these MT systems.
D3 (DP-match Driven transDucer) is an
example-based MT system using online-
generated translation patterns (Doi and Sumita,
2003).
HPAT (Hierarchical Phrase Alignment based
Translation) is a pattern-based system using au-
tomatically generated syntactic transfer (Imamura
et al, 2003).
SAT (Statistical ATR Translator) is an SMT
system using a retrieved seed translation as the
start point for decoding/translation (Watanabe et al,
2003).
4.2.2 Test set
The test set used consists of five hundred and ten
pairs of English and Japanese sentences, which
Table 3: Corpora for training TMs and LMs: Basic
Travel Expression Corpus Nos. 1-3 (Takezawa et al,
2002), Travel Reservation Corpus (Takezawa, 1999), and
MT-Aided Dialogue Corpus No. 1 (Kikui et al, 2003)
.
Japanese English
# of sentences 449,357
# of words 3,471,996 2,978,517
Vocabulary size 43,812 28,217
Ave. sent. length 7.7 6.6
were randomly selected from the Basic Travel Ex-
pression Corpus (BTEC) (Takezawa et al, 2002).
BTEC contains a variety of expressions used in a
number of situations related to overseas travel.
4.2.3 Training TMs and LMs
The corpora used for training TMs and LMs de-
scribed in Section 3.2 were merged corpora (Table
3). The number of trained TMs/LMs was three.
The translation models and language models were
learned by using GIZA++ (Och and Ney, 2000) and
the CMU-Cambridge Toolkit (Clarkson and Rosen-
feld, 1997), respectively.
4.3 Experimental results and discussion
4.3.1 ROC Curve
In order to plot the ROC Curve, the authors con-
ducted the same experiment as shown in Figure 1.
That is, in the case where the grade of satisfactory
translations is only grade A, each of the proposed
and existing RSCMs tried to accept grade A MT
outputs and to reject grade B, C, or D MT outputs.
Figures 5 to 7 show the ROC Curves for each of the
three J2E MT systems (D3, HPAT, and SAT).
The curves with diamond marks, cross marks,
triangle marks, and circle marks show the ROC
Curves for the existing RSCM, the proposed RSCM
by using the mixture of three-best lists from D3,
HPAT and SAT, the proposed RSCM by using the
mixture of five-best lists from D3 and HPAT, and
the existing RSCM with reordering, respectively. In
the existing RSCM with reordering, the system?s
Table 4: Ten-fold cross-validated pairwise t-test of H-mean: Each set of three columns corresponds to the experimen-
tal results of each of the three MT systems: D3, HPAT, and SAT. Each floating number in the first to third column of
each MT system indicates the average performance of the proposed RSCM, the average difference of the performance
of the proposed RSCM from that of the existing RSCM, and the t-value of the left-next difference, respectively. The
bold floating numbers indicate that the left-next difference is significant at a confidence level of 95%. The floating
numbers on the three rows for each MT system, whose row heads are ?A | BCD?, ?AB | CD?, or ?ABC | D?, corre-
spond to the three types of experiments in which each RSCM tried to accept/reject the MT output assigned one of the
grades left/right of ?|?, respectively.
E2J-D3 E2J-HPAT E2J-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 76.2 15.7 4.424 73.2 14.1 5.099 65.5 0.3 0.108
AB | CD 77.3 16.5 5.154 72.6 14.3 3.865 66.9 2.8e-5 0.002
ABC | D 74.9 11.4 5.963 74.7 16.6 4.906 73.2 5.5 2.281
J2E-D3 J2E-HPAT J2E-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 76.8 16.1 4.928 75.5 25.8 9.218 70.2 -3.3 1.618
AB | CD 79.6 15.9 4.985 70.8 28.9 6.885 66.0 -5.9 2.545
ABC | D 77.7 14.4 4.177 71.0 22.6 4.598 72.1 1.7 0.588
Table 5: Ten-fold cross-validated pairwise t-test of Accuracy: The description of this figure is the same as that of
Table 4 except that Accuracy is used instead of H-mean.
E2J-D3 E2J-HPAT E2J-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 77.4 10.5 4.354 71.1 15.4 5.667 76.4 1.1 1.000
AB | CD 78.2 4.9 2.953 78.2 2.5 2.176 81.1 0.0 0.000
ABC | D 85.0 1.3 1.172 84.1 -2.9 2.182 88.0 0.0 0.000
J2E-D3 J2E-HPAT J2E-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 78.8 15.8 8.243 76.2 18.2 8.118 76.4 3.1 1.041
AB | CD 77.8 4.1 3.279 72.7 8.8 3.288 77.6 -1.5 0.537
ABC | D 83.3 2.9 1.771 77.4 -1.7 1.646 82.7 0.1 0.428
original N-best list was sorted by using the aver-
age of the product scores from the multiple scor-
ing technique described in Section 3.2, and the ex-
isting RSCM with reordering used this sorted sys-
tem?s N-best instead of the system?s original N-best.
The dotted lines indicate the contours by H-mean
from 0.7 to 0.8. The ideal ROC curve is a square
(0, 1), (1, 1), (1, 0); thus, the closer the curve is to a
square, the better the performance of the RSCM is.
In Figures 5 and 6, the curves of the proposed
RSCM by using the mixture of three-best lists from
the three MT systems are much closer to a square
than that of the existing RSCM; moreover, the
curves of the proposed RSCM by using the mixture
of five-best lists from the two MT systems are much
closer to a square than that of the existing RSCM.
Note that the superiority of the proposed RSCM to
the existing RSCM is maintained even in the case
where an M-best list from the SMT system was not
used. The curves of the existing RSCM with re-
ordering are closer to a square than those of the ex-
isting RSCM. Thus the performance of the proposed
RSCM on the non-SMT systems, D3 and HPAT, are
much better than that of the existing RSCM. The
difference between the performance of the proposed
and existing RSCMs is due to both resorting the MT
outputs and using a mixture of N-best lists.
In Figure 7, the curve of the proposed RSCM is a
little closer when CRR is larger than CAR; and the
curve of the existing RSCM is a little closer when
CAR is larger than CRR. Thus, the performance
of the proposed RSCM on the SMT system, SAT,
is a little better than that of the existing RSCM in
the case where CRR is regarded as important; sim-
ilarly, the performance of the proposed RSCM on
the SMT system is a little worse than that of the ex-
isting RSCM in the case where CAR is regarded as
important.
4.3.2 H-mean and Accuracy
Tables 4 and 5 show the experimental results of ten-
fold cross-validated pairwise t-tests of the perfor-
mance of H-mean and Accuracy, respectively.
On the non-SMT systems, Table 4 shows that at
every level of translation quality that the user would
permit, the H-mean of the proposed RSCM is sig-
nificantly better than that of the existing RSCM. On
the SMT MT system, Table 4 shows that at every
permitted level of translation quality, there is no sig-
nificant difference between the H-mean of the pro-
posed RSCM and that of the existing RSCM except
for two cases: ?ABC | D? for E2J- SAT and ?AB |
CD? for J2E- SAT.
Table 5 shows almost the same tendency as Table
4. As for difference, in the case where the transla-
tion quality that the user would permit is better than
D, there is no significant difference between the Ac-
curacy of the proposed RSCM and that of the exist-
ing RSCM except in the one case of ?ABC | D? for
E2J-HPAT.
As defined in Section 4.1, Accuracy is an eval-
uation metric whose value is sensitive/inclined to
the ratio of the number of satisfactory translations
and unsatisfactory translations. H-mean is an eval-
uation metric whose value is independent/natural to
this ratio. We need to use these different evaluation
metrics according to the situations encountered. For
general purposes, the natural evaluation metric, H-
mean, is better. In the case where the test set reflects
special situations encountered, Accuracy is useful.
Regardless of whether we encounter any special
situation, in most cases on a non-SMT system, the
proposed RSCM proved to be significantly better
than the existing RSCM. In most cases on an SMT
system, the proposed RSCM proved to be as good
in performance as the existing RSCM.
This paper reports a case study in which a mixture
of N-best lists from multiple MT systems boosted
the performance of the RSCM for MT outputs. The
authors believe the proposed RSCM will work well
only when each of the element MT systems comple-
ments the others, but the authors leave the question
of the best combination of complementary MT sys-
tems open for future study.
5 Conclusions
This paper addressed the problem of eliminating un-
satisfactory outputs from MT systems. It proposed
a method that eliminates unsatisfactory outputs by
using an alternative RSCM based on a mixture of
N-best lists from multiple MT systems. The au-
thors compared the proposed and existing RSCMs
in the framework of an elimination system. When
the number of MT outputs both in the N-best list for
the existing RSCM and in the mixture of N-best lists
for the proposed RSCM is almost the same number,
i.e. ten, in most cases, the proposed RSCM proved
to work better than the existing RSCM on two non-
SMT systems and to work as well as the existing
RSCM on an SMT system.
In the future, the authors will conduct the follow-
ing experiments: (1) investigating how the proposed
RSCM works when the size of the M-best lists is
increased, and (2) seeing how the proposed RSCM
influences the performance of the selection system.
References
Yasuhiro Akiba, Taro Watanabe, and Eiichiro Sumita. 2002.
Using language and translation models to select the best
among outputs from multiple MT systems. In Proc.
COLING-2002, pages 8?14.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical lan-
guage modeling using the CMU-Cambridge toolkit. In
Proc. EUROSPEECH-1997, pages 2707?2710.
Takao Doi and Eiichiro Sumita. 2003. Input sentence splitting
and translating. In Proc. the HLT-NAACL 2003 Workshop
on DDMT, pages 104?110.
Kenji Imamura, Eiichiro Sumita, and Yuji Matsumoto. 2003.
Feedback cleaning of machine translation rules using auto-
matic evaluation. In Proc. ACL-2003, pages 447?454.
Genichiro Kikui, Eiichiro Sumita, Toshiyuki Takezawa, and
Seiichi Yamamoto. 2003. Creating corpora for speech-
to-speech translation. In Proc. EUROSPEECH-2003, vol-
ume 1, pages 381?384.
Vladimir I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet Physics
Doklady, 10(8):707?710.
Tom M. Mitchell. 1997. Machine Learning. The McGraw-Hill
Companies Inc., New York, USA.
Sonja Niessen, Franz J. Och, G. Leusch, and Hermann Ney.
2000. An evaluation tool for machine translation: Fast eval-
uation for machine translation research. In Proc. LREC-
2000, pages 39?45.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proc. ACL-2000, pages 440?447.
Kishore A. Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a method for automatic evaluation of ma-
chine translation. In Technical Report RC22176 (W0109-
022), IBM Research Division, Thomas J. Watson Research
Center, Yorktown Heights, NY, pages 257?258.
Eiichiro Sumita, Setsuo Yamada, Kazuhiro Yamamoto,
Michael Paul, Hideki Kashioka, Kai Ishikawa, and Satoshi
Shirai. 1999. Solutions to problems inherent in spoken-
language translation: The ATR-MATRIX approach. In
Proc. MT Summit VII, pages 229?235.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, Hiro-
fumi Yamamoto, and Seiichi Yamamoto. 2002. Toward a
broad-coverage bilingual corpus for speech translation of
travel conversations in the real world. In Proc. LREC-2002,
pages 147?152.
Toshiyuki Takezawa. 1999. Building a bilingual travel conver-
sation database for speech translation research. In Proc. the
Oriental COCOSDA Workshop-1999, pages 17?20.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003.
Confidence measures for statistical machine translation. In
Proc. MT Summit IX, pages 394?401.
Taro Watanabe, Eiichiro Sumita, and Hiroshi G. Okuno. 2003.
Chunk-based statistical translation. In Proc. MT Summit IX,
pages 410?417.
Efficient Confirmation Strategy for Large-scale Text Retrieval
Systems with Spoken Dialogue Interface
Kazunori Komatani Teruhisa Misu Tatsuya Kawahara Hiroshi G. Okuno
Graduate School of Informatics
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,kawahara,okuno}@i.kyoto-u.ac.jp
Abstract
Adequate confirmation for keywords is in-
dispensable in spoken dialogue systems
to eliminate misunderstandings caused by
speech recognition errors. Spoken lan-
guage also inherently includes out-of-
domain phrases and redundant expressions
such as disfluency, which do not contribute
to task achievement. It is necessary to
appropriately make confirmation for im-
portant portions. However, a set of key-
words necessary to achieve the tasks can-
not be predefined in retrieval for a large-
scale knowledge base unlike conventional
database query tasks. In this paper, we
describe two statistical measures for iden-
tifying portions to be confirmed. A rele-
vance score represents the matching degree
with the target knowledge base. A sig-
nificance score detects portions that conse-
quently affect the retrieval results. These
measures are defined based on information
that is automatically derived from the tar-
get knowledge base. An experimental eval-
uation shows that our method improved the
success rate of retrieval by generating con-
firmation more efficiently than using a con-
ventional confidence measure.
1 Introduction
Information retrieval systems with spoken lan-
guage have been studied (Harabagiu et al, 2002;
Hori et al, 2003). They require both automatic
speech recognition (ASR) and information re-
trieval (IR) technologies. As a straight mani-
festation to create these systems, we can think
of using ASR results as an input for IR systems
that retrieve a text knowledge base (KB). How-
ever, two problems occur in the characteristics
of speech.
1. Speech recognition errors
2. Redundancy included in spoken language
expressions
One is an ASR error, which is basically in-
evitable in speech communications. Therefore,
an adequate confirmation is indispensable in
spoken dialogue systems to eliminate the mis-
understandings caused by ASR errors.
If keywords to be confirmed are defined, the
system can confirm them using confidence mea-
sures (Komatani and Kawahara, 2000; Hazen
et al, 2000) to manage the errors. In con-
ventional tasks for spoken dialogue systems in
which their target of retrieval was well-defined,
such as the relational database, keywords that
are important to achieve the tasks correspond
to items in the relational database. Most spo-
ken dialogue systems that have been developed,
such as airline information systems (Levin et al,
2000; Potamianos et al, 2000; San-Segundo et
al., 2000) and train information systems (Allen
et al, 1996; Sturm et al, 1999; Lamel et al,
1999), are categorized here. However, it is not
feasible to define such keywords in retrieval for
operation manuals (Komatani et al, 2002) or
WWW pages, where the target of retrieval is
not organized and is written as natural language
text.
Another problem is that a user?s utterances
may include redundant expressions or out-of-
domain phrases. A speech interface has been
said to have the advantage of ease of input. This
means that redundant expressions, such as dis-
fluency and irrelevant phrases, are easily input.
These do not directly contribute to task achieve-
ment and might even be harmful. ASR results
that may include such redundant portions are
not adequate for an input of IR systems.
A novel method is described in this paper
that automatically detects necessary portions
for task achievement from the ASR results of
a user?s utterances; that is, the system deter-
mines if each part of the ASR results is neces-
sary for the retrieval. We introduce two mea-
sures for each portion of the results. One is a
relevance score (RS) with the target document
 
[HOWTO] Use Speech Recognition in
Windows XP
The information in this article applies to:
? Microsoft Windows XP Professional
? Microsoft Windows XP Home Edition
Summary: This article describes how to use
speech recognition in Windows XP. If you
installed speech recognition with Microsoft
Office XP, or if you purchased a new com-
puter that has Office XP installed, you can
use speech recognition in all Office pro-
grams as well as other programs for which
it is enabled.
Detail information: Speech recognition en-
ables the operating system to convert spo-
ken words to written text. An internal
driver, called a speech recognition engine,
recognizes words and converts them to
text. The speech recognition engine ...
 
Figure 1: Example of one article in database
set. The score is computed with a document
language model and is used for making confir-
mation prior to the retrieval. The other is a sig-
nificance score (SS) in the document matching.
It is computed after the retrieval using N-best
results and is used for prompting the user for
post-selection if necessary. Information neces-
sary to define these two measures, such as a doc-
ument language model and retrieval results for
N-best candidates of the ASR, can be automat-
ically derived from the target knowledge base.
Therefore, the system can detect the portions
necessary for the retrieval and make the confir-
mation efficiently without defining the keywords
manually.
2 Text Retrieval System for
Large-scale Knowledge Base
Our task involves text retrieval for a large-
scale knowledge base. As the target domain,
we adopted a software support knowledge base
provided by the Microsoft Corporation. The
knowledge base consists of the following three
components: glossary, frequently asked ques-
tions (FAQ), and a database of support articles.
Figure 1 is an example of the database. The
knowledge base is very large-scale, as shown in
Table 1.
The Dialog Navigator (Kiyota et al, 2002)
was developed in the University of Tokyo as a
Table 1: Document set (Knowledge base)
Text collection # of texts # of characters
Glossary 4,707 700,000
FAQ 11,306 6,000,000
Support articles 23,323 22,000,000
text retrieval system for this knowledge base.
The system accepts a typed-text input as ques-
tions from users and outputs a result of the re-
trieval. The system interprets input sentences
taking a syntactic dependency and synonymous
expression into consideration for matching it
with the knowledge base. The system can also
navigate for the user when he/she makes vague
questions based on scenarios (dialog card) that
were described manually beforehand. Hundreds
of the dialog cards have been prepared to ask
questions back to the users. If a user question
matches its input part, the system generates a
question based on its description.
We adopted the Dialog Navigator as a back-
end system and constructed a text retrieval sys-
tem with a spoken dialogue interface. We then
investigated a confirmation strategy to interpret
the user?s utterances robustly by taking into ac-
count the problems that are characteristic of
spoken language, as previously described.
3 Confirmation Strategy using
Relevance Score and Significance
Score
Making confirmations for every portion that has
the possibility to be an ASR error is tedious.
This is because every erroneous portion does
not necessarily affect the retrieval results. We
therefore take the influence of recognition er-
rors for retrieval into consideration, and control
generation of confirmation.
We make use of N-best results of the ASR
for the query and test if a significant difference
is caused among N-best sets of retrieved can-
didates. If there actually is, we then make a
confirmation on the portion that makes the dif-
ference. This is regarded as a posterior confir-
mation. On the other hand, if a critical error
occurs in the ASR result, such as those in the
product name in software support, the follow-
ing retrieval would make no sense. Therefore,
we also introduce a confirmation prior to the
retrieval for critical words.
The system flow including the confirmation is
summarized below.
1. Recognize a user?s utterance.
Speech input
System User
ASR
(N-best candidates)
Calculation of
relevance score
Language model
for ASR
Language model
trained with KB
Confirmation using
relevance score
Critical
words
Confirmation for
influential words
Reply or rephrase
Matching with KB with
weighting by relevance score
retrieval
result
retrieval
result
retrieval
result
Confirmation using significance score
Final result
Display the result
Confirmation for difference
between candidates
Reply or rephrase
Dialog
Navigator
(text retrieval)
Target text
KB TFIDF
Figure 2: System flow
2. Calculate a relevance score for each phrase
of ASR results.
3. Make a confirmation for critical words with
a low relevance score.
4. Retrieve the knowledge base using the Dia-
log Navigator for N-best candidates of the
ASR.
5. Calculate significance scores and generate
a confirmation based on them.
6. Show the retrieval results to the user.
This flow is also shown in Figure 2 and ex-
plained in the following subsections in detail.
3.1 Definition of Relevance Score
We use test-set perplexity for each portion of
the ASR results as one of the criteria in deter-
mining whether the portion is influential or not
for the retrieval. The language model to cal-
culate the perplexity was trained only with the
target knowledge base. It is different from that
used in the ASR.
The perplexity is defined as an exponential of
entropy per word, and it represents the average
number of the next words when we observe a
word sequence. The perplexity can be denoted
as the following equation because we assume an
ergodicity on language and use a trigram as a
language model.
log PP = ? 1
n
?
k
log P (wk|wk?1, wk?2)
This perplexity PP represents the degree that
a given word sequence, w1w2 ? ? ?wn, matches
the knowledge base with which the language
model P (wn|wn?1, wn?2) was trained. If the
perplexity is small, it indicates the sequence ap-
pears frequently in the knowledge base. On the
contrary, the perplexity for a portion including
the ASR errors increases because it is contex-
tually less frequent. The perplexity for out-of-
domain phrases similarly increases because they
scarcely appear in the knowledge base. It en-
ables us to detect a portion that is not influen-
tial for retrieval or those portions that include
ASR errors. Here, a phrase, called bunsetsu1
in Japanese, is adopted as a portion for which
the perplexity is calculated. We use a syntac-
tic parser KNP (Kurohashi and Nagao, 1994)
to divide the ASR results into the phrases2.
1Bunsetsu is a commonly used linguistic unit in
Japanese, which consists of one or more content words
and zero or more functional words that follow.
2As the parser was designed for written language, the
division often fails for portions including ASR errors.
The division error, however, does not affect the whole
system?s performance because the perplexity for the er-
roneous portions increases, indicating they are irrelevant.
 
User utterance:
?Atarashiku katta XP no pasokon de fax kinou
wo tsukau niha doushitara iidesu ka??
(Please tell me how to use the facsimile func-
tion in the personal computer with Windows
XP that I recently bought.)
Speech recognition result:
?Atarashiku katta XP no pasokon de fax kinou
wo tsukau ni sono e ikou??
[The underlined part was incorrectly recognized
here.]
Division into phrases (bunsetsu):
?Atarashiku / katta / XP no / pasokon de / fax
kinou wo / tsukau ni / sono / e / ikou??
Calculation of perplexity:
phrases (their context) PP RS
(<S>) Atarashiku (katta) 499.57 0.86
(atarashiku) katta (XP) 2079.83 0.47
(katta) XP no (pasokon) 105.64 0.99
(no) pasokon de (FAX) 185.92 0.95
(de) FAX kinou wo (tsukau) 236.23 0.89
(wo) tsukau ni (sono) 98.40 0.99
(ni) sono (e) 1378.72 0.62
(sono) e (ikou) 144.58 0.96
(e) ikou (</S>) 27150.00 0.00
<S>, </S> denote the beginning and end of a sen-
tence.
 
Figure 3: Example of calculating perplexity
(PP ) and relevance score (RS)
We then calculate the perplexity for the
phrases (bunsetsu) to which the preceding and
following words are attached. We then define
the relevance score by applying a sigmoid-like
transform to the perplexity using the following
equation. Thus, the score ranges between 0 and
1 by the transform and can be used as a weight
for each bunsetsu.
RS =
1
1 + exp(? ? (log PP ? ?))
Here, ? and ? are constants and are empiri-
cally set to 2.0 and 11.0. An example of calcu-
lating the relevance score is shown in Figure 3.
In this sample, a portion, ?Atarashiku katta (=
that I bought recently)?, that appeared in the
beginning of the utterance does not contribute
to any retrieval. A portion at the end of the sen-
tence was incorrectly recognized because it may
have been pronounced weakly. The perplexity
for these portions increases as a result, and the
relevance score correspondingly decreases.
3.2 Confirmation for Critical Words
using Relevance Score
Critical words should be confirmed before the
retrieval. This is because a retrieval result will
be severely damaged if the portions are not cor-
rectly recognized. We define a set of words that
are potentially critical using tf?idf values, which
are often used in information retrieval. They
can be derived from the target knowledge base
automatically. We regard a word with the max-
imum tf?idf values in each document as being
its representative, and the words that are rep-
resentative in more documents are regarded as
being more important. When the amount of
documents represented by the more important
words exceeds 10% out of the whole number of
documents, we define a set of the words as being
critical. As a result, 35 words were selected as
potentially critical ones in the knowledge base,
such as ?set up?, ?printer?, and ?(Microsoft) Of-
fice?.
We use the relevance score to determine
whether we should make a confirmation for the
critical words. If a critical word is contained
in a phrase whose relevance score is lower than
threshold ?, the system makes a confirmation.
We set threshold ? through the preliminary ex-
periment. The confirmation is done by present-
ing the recognition results to the user. Users can
either confirm or discard or correct the phrase
before passing it to the following matching mod-
ule.
3.3 Weighted Matching using
Relevance Score
A phrase that has a low relevance score is likely
to be an ASR error or a portion that does not
contribute to retrieval. We therefore use the rel-
evance score RS as a weight for phrases during
the matching with the knowledge base. This re-
lieves damage to the retrieval by ASR errors or
redundant expressions.
3.4 Significance Score using Retrieval
Results
The significance score is defined by using plural
retrieval results corresponding to N-best candi-
dates of the ASR. Ambiguous portions during
the ASR appear as the differences between the
N-best candidates. The score represents the de-
gree to which the portions are influential.
The significance score is calculated for por-
tions that are different among N-best candi-
dates. We define the significance score SS(n,m)
as the difference between the retrieval results of
n-th and m-th candidates. The value is defined
by the equation,
SS(n,m) = 1 ? |res(n) ? res(m)|
2
|res(n)||res(m)| .
Here, res(n) denotes a set of retrieval results
for the n-th candidate, and |res(n)| denotes the
number of elements in the set. That is, the sig-
nificance score decreases if the retrieval results
have a large common part.
Figure 4 has an example of calculating the
significance score. In this sample, the portions
of ?suuzi (numerals)? and ?suushiki (numeral
expressions)? differ between the first and sec-
ond candidates of the ASR. As the retrieval re-
sults for each candidate, 14 and 15 items are
obtained, respectively. The number of com-
mon items between the two retrieval results is
8. Then, the significance score for the portion
is 0.70 by the above equation.
3.5 Confirmation using Significance
Score
The confirmation is also made for the portions
detected by the significance score. If the score
is higher than a threshold, the system makes
the confirmation by presenting the difference to
users3. Here, we set the number of N-best can-
didates of the ASR to 3, and the threshold for
the score is set to 0.5.
In the confirmation phrase, if a user selects
from the list, the system displays the corre-
sponding retrieval results. If the score is lower
than the threshold, the system does not make
the confirmation and presents retrieval results
of the first candidate of the ASR. If a user
judges all candidates as inappropriate, the sys-
tem rejects the current candidates and prompts
him/her to utter the query again.
4 Experimental Evaluation
We implemented and evaluated our method as
a front-end of Dialog Navigator. The front-end
works on a Web browser, Internet Explorer 6.0.
Julius (Lee et al, 2001) for SAPI4 was used as a
speech recognizer on PCs. The system presents
a confirmation to users on the display. He or she
replies to the confirmation by selecting choices
with the mouse.
3Confirmation will be generated practically if one of
the significance scores between the first candidate and
others exceeds the threshold.
4http://julius.sourceforge.jp/sapi/
 
[#1 candidate of ASR]
?WORD2002 de suuzi wo nyuryoku suru
houhou wo oshiete kudasai.? (Please tell me
the way to input numerals in WORD 2002.)
Retrieval results (# of the results was 14.)
1. Input the present date and time in Word
2. WORD: Add a space between Japanese and
alphanumeric characters
3. WORD: Check the form of inputted char-
acters
4. WORD: Input a handwritten signature
5. WORD: Put watermark characters into the
background of a character
6. ...
[#2 candidate of ASR]
?WORD2002 de suushiki wo nyuryoku suru
houhou wo oshiete kudasai.? (Please tell
me the way to input numerical expressions in
WORD 2002.)
Retrieval results (# of the results was 15.)
1. Insert numerical expressions in Word
2. Input the present date and time in Word
3. Input numerical expressions in Spreadsheet
4. Input numerical expressions in PowerPoint
5. Input numerical expressions in Excel
6. ...
Significance score
SS(1, 2) = 1 ? 8214?15 = 0.70
(# of common items in the retrieval results
was 8.)
 
Figure 4: Example of calculating significance
score
We collected the test data by 30 subjects who
had not used our system. Each subject was re-
quested to retrieve support information for 14
tasks, which consisted of 11 prepared scenarios
(query sentences are not given) and 3 sponta-
neous queries. Subjects were allowed to utter
the sentence again up to 3 times per task if a rel-
evant retrieval result was not obtained. We ob-
tained 651 utterances for 420 tasks in total. The
average word accuracy of the ASR was 76.8%.
4.1 Evaluation of Success Rate of
Retrieval
We calculated the success rates of retrieval for
the collected speech data. We regarded the re-
trieval as having succeeded when the retrieval
results contained an answer for the user?s initial
question. We set three experimental conditions:
Table 2: Comparison of success rate of retrieval with method using only ASR results
# utterances Transcription ASR results Our method
651 520 421 457
(79.9%) (64.7%) (70.2%)
1. Transcription: A correct transcription of
user utterances, which was made manually,
was used as an input to the Dialog Naviga-
tor. This condition corresponds to a case of
100% ASR accuracy, indicating an utmost
performance obtained by improvements in
the ASR and our dialogue strategy.
2. ASR results: The first candidate of the
ASR was used as an input (baseline).
3. Our method: The N-best candidates of the
ASR were used as an input, and confirma-
tion was generated based on our method
using both the relevance and significance
scores. It was assumed that the users
responded appropriately to the generated
confirmation.
Table 2 lists the success rate. The rate when
the transcription was used as the input was
79.9%. The remaining errors included those
caused by irrelevant user utterances and those
in the text retrieval system. Our method at-
tained a better success rate than the condition
where the first candidate of the ASR was used.
Improvement of 36 cases (5.5%) was obtained by
our method, including 30 by the confirmations
and 14 by weighting during the matching using
the relevance score, though the retrieval failed
eight times as side effects of the weighting.
We further investigated the results shown in
Table 2. Table 3 lists the relations between the
success rate of the retrieval and the accuracy
of the ASR per utterance. The improvement
rate out of the number of utterances was rather
high between 40% and 60%. This means that
our method was effective not only for utterances
with high ASR accuracy but also for those with
around 50% accuracy. That is, an appropriate
confirmation was generated even for utterances
whose ASR accuracy was not very high.
4.2 Evaluation of Confirmation
Efficiency
We also evaluated our method from the number
of generated confirmations. Our method gener-
ated 221 confirmations. This means that con-
firmations were generated once every three ut-
terances on the average. The 221 confirmations
consisted of 66 prior to the retrieval using the
relevance score and 155 posterior to the retrieval
using the significance score.
We compared our method with a conventional
one, which used a confidence measure (CM)
based on N-best candidates of the ASR (Ko-
matani and Kawahara, 2000)5. In this method,
the system generated confirmation only for con-
tent words with a confidence measure lower
than ?1. The thresholds to generate confirma-
tion (?1) were set to 0.4, 0.6, and 0.8. If a con-
tent word that was confirmed was rejected by
the user, the retrieval was executed after remov-
ing a phrase that included it.
The number of confirmations and retrieval
successes are shown in Table 4. Our method
achieved a higher success rate with a less num-
ber of confirmations (less than half) compared
with the case of ?1 = 0.8 in the conventional
method. Thus, the generated confirmations
based on the two scores were more efficient.
The confidence measure used in the conven-
tional method only reflects the acoustic and
linguistic likelihood of the ASR results. Our
method, however, reflects the domain knowl-
edge because the two scores are derived by ei-
ther a language model trained with the target
knowledge base or by retrieval results for the
N-best candidates. The domain knowledge can
be introduced without any manual deliberation.
The experimental results show that the scores
are appropriate to determine whether a confir-
mation should be generated or not.
5 Conclusion
We described an appropriate confirmation strat-
egy for large-scale text retrieval systems with a
spoken dialogue interface. We introduced two
measures, relevance score and significance score,
for ASR results. The measures are useful to con-
trol confirmation efficiently for portions includ-
ing either ASR errors or redundant expressions.
The portions to be confirmed are determined
5We used a word-level CM only because defining se-
mantic categories for content words is required to cal-
culate the concept-level CM. Because the semantic cate-
gory corresponded to items in a relational database, we
cannot use the concept-level CM in this task.
Table 3: Success rate of retrieval per ASR accuracy
ASR accuracy (%) # utterances ASR results Our method # improvement
?40 37 9 11 2 ( 5.4%)
?60 73 33 42 9 (12.3%)
?80 194 116 129 13 ( 6.7%)
?100 347 263 275 12 ( 3.5%)
Total 651 421 457 36 ( 5.5%)
Table 4: Comparison with method using confidence measure (CM)
Our method CM (?1 = 0.4) CM (?1 = 0.6) CM (?1 = 0.8)
# confirmation 221 77 254 484
# success (success rate) 457 (70.2%) 427 (65.6%) 435 (66.8%) 445 (68.4%)
using information that is automatically derived
from the target knowledge base, such as a statis-
tical language model, tf?idf values, and retrieval
results. An experimental evaluation shows that
our method can efficiently generate confirma-
tions for better task achievement compared with
that using a conventional confidence measure of
the ASR. Our method is not dependent on the
software support task, and expected to be ap-
plicable to general text retrieval tasks.
6 Acknowledgments
The authors are grateful to Prof. Kurohashi and
Mr. Kiyota at the University of Tokyo and Ms.
Kido at Microsoft Corporation for their helpful
advice.
References
J. F. Allen, B. W. Miller, E. K. Ringger, and
T. Sikorski. 1996. A robust system for natu-
ral spoken dialogue. In Proc. of the 34th An-
nual Meeting of the ACL, pages 62?70.
S. Harabagiu, D. Moldovan, and J. Picone.
2002. Open-domain voice-activated question
answering. In Proc. COLING, pages 502?508.
T. J. Hazen, T. Burianek, J. Polifroni, and
S. Seneff. 2000. Integrating recognition con-
fidence scoring with language understanding
and dialogue modeling. In Proc. ICSLP.
C. Hori, T. Hori, H. Isozaki, E. Maeda, S. Kata-
giri, and S. Furui. 2003. Deriving disambigu-
ous queries in a spoken interactive ODQA
system. In Proc. IEEE-ICASSP.
Y. Kiyota, S. Kurohashi, and F. Kido. 2002.
?Dialog Navigator?: A question answering
system based on large text knowledge base.
In Proc. COLING, pages 460?466.
K. Komatani and T. Kawahara. 2000. Flexible
mixed-initiative dialogue management using
concept-level confidence measures of speech
recognizer output. In Proc. COLING, pages
467?473.
K. Komatani, T. Kawahara, R. Ito, and H. G.
Okuno. 2002. Efficient dialogue strategy to
find users? intended items from information
query results. In Proc. COLING, pages 481?
487.
S. Kurohashi and M. Nagao. 1994. A syntactic
analysis method of long Japanese sentences
based on the detection of conjunctive struc-
tures. Computational Linguistics, 20(4):507?
534.
L. F. Lamel, S. Rosset, J-L. S. Gauvain, and
S. K. Bennacef. 1999. The LIMSI ARISE
system for train travel information. In Proc.
IEEE-ICASSP.
A. Lee, T. Kawahara, and K. Shikano. 2001.
Julius ? an open source real-time large vo-
cabulary recognition engine. In Proc. EU-
ROSPEECH, pages 1691?1694.
E. Levin, S. Narayanan, R. Pieraccini, K. Bia-
tov, E. Bocchieri, G. Di Fabbrizio, W. Eck-
ert, S. Lee, A. Pokrovsky, M. Rahim, P. Rus-
citti, and M. Walker. 2000. The AT&T-
DARPA communicator mixed-initiative spo-
ken dialogue system. In Proc. ICSLP.
A. Potamianos, E. Ammicht, and H.-K. J. Kuo.
2000. Dialogue management in the Bell labs
communicator system. In Proc. ICSLP.
R. San-Segundo, B. Pellom, W. Ward, and
J. Pardo. 2000. Confidence measures for dia-
logue management in the CU communicator
system. In Proc. IEEE-ICASSP.
J. Sturm, E. Os, and L. Boves. 1999. Issues in
spoken dialogue systems: Experiences with
the Dutch ARISE system. In Proc. ESCA
workshop on Interactive Dialogue in Multi-
Modal Systems.
Rapid Prototyping of Robust Language Understanding Modules
for Spoken Dialogue Systems
?Yuichiro Fukubayashi, ?Kazunori Komatani, ?Mikio Nakano,
?Kotaro Funakoshi, ?Hiroshi Tsujino, ?Tetsuya Ogata, ?Hiroshi G. Okuno
?Graduate School of Informatics, Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto
606-8501, Japan
{fukubaya,komatani}@kuis.kyoto-u.ac.jp
{ogata,okuno}@kuis.kyoto-u.ac.jp
?Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama
351-0188, Japan
nakano@jp.honda-ri.com
{funakoshi,tsujino}@jp.honda-ri.com
Abstract
Language understanding (LU) modules for
spoken dialogue systems in the early phases
of their development need to be (i) easy
to construct and (ii) robust against vari-
ous expressions. Conventional methods of
LU are not suitable for new domains, be-
cause they take a great deal of effort to
make rules or transcribe and annotate a suf-
ficient corpus for training. In our method,
the weightings of the Weighted Finite State
Transducer (WFST) are designed on two
levels and simpler than those for conven-
tional WFST-based methods. Therefore,
our method needs much fewer training data,
which enables rapid prototyping of LU mod-
ules. We evaluated our method in two dif-
ferent domains. The results revealed that our
method outperformed baseline methods with
less than one hundred utterances as training
data, which can be reasonably prepared for
new domains. This shows that our method
is appropriate for rapid prototyping of LU
modules.
1 Introduction
The language understanding (LU) of spoken dia-
logue systems in the early phases of their devel-
opment should be trained with a small amount of
data in their construction. This is because large
amounts of annotated data are not available in the
early phases. It takes a great deal of effort and time
to transcribe and provide correct LU results to a
Figure 1: Relationship between our method and con-
ventional methods
large amount of data. The LU should also be robust,
i.e., it should be accurate even if some automatic
speech recognition (ASR) errors are contained in its
input. A robust LU module is also helpful when col-
lecting dialogue data for the system because it sup-
presses incorrect LU and unwanted behaviors. We
developed a method of rapidly prototyping LU mod-
ules that is easy to construct and robust against var-
ious expressions. It makes LU modules in the early
phases easier to develop.
Several methods of implementing an LU mod-
ule in spoken dialogue systems have been proposed.
Using grammar-based ASR is one of the simplest.
Although its ASR output can easily be transformed
into concepts based on grammar rules, complicated
grammars are required to understand the user?s ut-
terances in various expressions. It takes a great deal
of effort to the system developer. Extracting con-
210
Figure 2: Example of WFST for LU
cepts from user utterances by keyword spotting or
heuristic rules has also been proposed (Seneff, 1992)
where utterances can be transformed into concepts
without major modifications to the rules. However,
numerous complicated rules similarly need to be
manually prepared. Unfortunately, neither method
is robust against ASR errors.
To cope with these problems, corpus-based (Su-
doh and Tsukada, 2005; He and Young, 2005) and
Weighted Finite State Transducer (WFST)-based
methods (Potamianos and Kuo, 2000; Wutiwi-
watchai and Furui, 2004) have been proposed as LU
modules for spoken dialogue systems. Since these
methods extract concepts using stochastic analy-
sis, they do not need numerous complicated rules.
These, however, require a great deal of training data
to implement the module and are not suitable for
constructing new domains.
Here, we present a new WFST-based LU module
that has two main features.
1. A statistical language model (SLM) for ASR
and a WFST for parsing that are automatically
generated from the domain grammar descrip-
tion.
2. Since the weighting for the WFST is simpler
than that in conventional methods, it requires
fewer training data than conventional weight-
ing schemes.
Our method accomplishes robust LU with less ef-
fort using SLM-based ASR and WFST parsing. Fig-
ure 1 outlines the relationships between our method
and conventional schemes. Since rule- or grammar-
based approaches do not require a large amount of
data, they take less effort than stochastic techniques.
However, they are not robust against ASR errors.
Stochastic approaches, on the contrary, take a great
deal of effort to collect data but are robust against
ASR errors. Our method is an intermediate approach
that lies between these. That is, it is more robust than
rule- or grammar-based approaches and takes less
effort than stochastic techniques. This characteristic
makes it easier to rapidly prototype LU modules for
a new domain and helps development in the early
phases.
2 Related Work and WFST-based
Approach
A Finite State Transducer (FST)-based LU is ex-
plained here, which accepts ASR output as its in-
put. Figure 2 shows an example of the FST for a
video recording reservation domain. The input, ?,
means that a transition with no input is permitted at
the state transition. In this example, the LU mod-
ule returns the concept [month=2, day=22] for the
utterance ?It is February twenty second please?.
Here, a FILLER transition in which any word is ac-
cepted is appropriately allowed between phrases. In
Figure 2, ?F? represents 0 or more FILLER tran-
sitions. A FILLER transition from the start to the
end is inserted to reject unreliable utterances. This
FILLER transition enables us to ignore unnecessary
words listed in the example utterances in Table 1.
The FILLER transition helps to suppress the inser-
tion of incorrect concepts into LU results.
However, many output sequences are obtained for
one utterance due to the FILLER transitions, be-
cause the utterance can be parsed with several paths.
We used a WFST to select the most appropriate
path from several output sequences. The path with
the highest cumulative weight, w, is selected in a
211
Table 2: Many LU results for input ?It is February twenty second please?
LU output LU result w
It is February twenty second please month=2, day=22 2.0
It is FILLER twenty second please day=22 1.0
It is FILLER twenty second FILLER day=22 1.0
FILLER FILLER FILLER FILLER FILLER FILLER n/a 0
Table 1: Examples of utterances with FILLERs
ASR output
Well, it is February twenty second please
It is uhm, February twenty second please
It is February, twe-, twenty second please
It is February twenty second please, OK?
(LU result = [month=2, day=22])
WFST-based LU. In the example in Table 2, the
concept [month=2, day=22] has been selected, be-
cause its cumulative weight, w, is 2.0, which is the
highest.
The weightings of conventional WFST-based ap-
proaches used an n-gram of concepts (Potamianos
and Kuo, 2000) and that of word-concept pairs (Wu-
tiwiwatchai and Furui, 2004). They obtained the
n-grams from several thousands of annotated ut-
terances. However, it takes a great deal of ef-
fort to transcribe and annotate a large corpus. Our
method enables prototype LU modules to be rapidly
constructed that are robust against various expres-
sions with SLM-based ASR and WFST-based pars-
ing. The SLM and WFST are generated automat-
ically from a domain grammar description in our
toolkit. We need fewer data to train WFST, because
its weightings are simpler than those in conventional
methods. Therefore, it is easy to develop an LU
module for a new domain with our method.
3 Domain Grammar Description
A developer defines grammars, slots, and concepts
in a domain in an XML file. This description en-
ables an SLM for ASR and parsing WFST to be au-
tomatically generated. Therefore, a developer can
construct an LU module rapidly with our method.
Figure 3 shows an example of a descrip-
tion. A definition of a slot is described in
keyphrase-class tags and its keyphrases and
...
<keyphrase-class name="month">
...
<keyphrase>
<orth>February</orth>
<sem>2</sem>
</keyphrase>
...
</keyphrase-class>
...
<action type="specify-attribute">
<sentence> {It is} [*month] *day [please]
</sentence>
</action>
Figure 3: Example of a grammar description
the values are in keyphrase tags. The month is
defined as a slot in this figure. February and 2 are
defined as one of the phrases and values for the slot
month. A grammar is described in a sequence of
terminal and non-terminal symbols. A non-terminal
symbol represents a class of keyphrases, which is
defined in keyphrase-class. It begins with an
asterisk ?*? in a grammar description in sentence
tags. Symbols that can be skipped are enclosed
by brackets []. The FILLER transition described
in Section 2 is inserted between the symbols un-
less they are enclosed in brackets [] or braces {}.
Braces are used to avoid FILLER transitions from
being inserted. For example, the grammar in Figure
3 accepts ?It is February twenty second please.? and
?It is twenty second, OK??, but rejects ?It is Febru-
ary.? and ?It, uhm, is February twenty second.?.
A WFST for parsing can be automatically gener-
ated from this XML file. The WFST in Figure 2 is
generated from the definition in Figure 3. Moreover,
we can generate example sentences from the gram-
mar description. The SLM for the speech recognizer
is generated with our method by using many exam-
ple sentences generated from the defined grammar.
212
4 Weighting for ASR Outputs on Two
Levels
We define weights on two levels for a WFST. The
first is a weighting for ASR outputs, which is set to
select paths that are reliable at a surface word level.
The second is a weighting for concepts, which is
used to select paths that are reliable on a concept
level. The weighting for concepts reflects correct-
ness at a more abstract level than the surface word
level. The weighting for ASR outputs consists of
two categories: a weighting for ASR N-best outputs
and one for accepted words. We will describe the
definitions of these weightings in the following sub-
sections.
4.1 Weighting for ASR N-Best Outputs
The N-best outputs of ASR are used for an input of
a WFST. Weights are assigned to each sentence in
ASR N-best outputs. Larger weights are given to
more reliable sentences, whose ranks in ASR N-best
are higher. We define this preference as
wis =
e??scorei
?N
j e??scorej
,
where wis is a weight for the i-th sentence in ASRN-best outputs, ? is a coefficient for smoothing, and
scorei is the log-scaled score of the i-th ASR out-put. This weighting reflects the reliability of the
ASR output. We set ? to 0.025 in this study after
a preliminary experiment.
4.2 Weighting for Accepted Words
Weights are assigned to word sequences that have
been accepted by the WFST. Larger weights are
given to more reliable sequences of ASR outputs at
the surface word level. Generally, longer sequences
having more words that are not fillers and more re-
liable ASR outputs are preferred. We define these
preferences as the weights:
1. word(const.): ww = 1.0,
2. word(#phone): ww = l(W ), and
3. word(CM): ww = CM(W ) ? ?w.
The word(const.) gives a constant weight to
all accepted words. This means that sequences
with more words are simply preferred. The
word(#phone) takes the length of each accepted
word into consideration. This length is measured by
its number of phonemes, which are normalized by
that of the longest word in the vocabulary. The nor-
malized values are denoted as l(W ) (0 < l(W ) ?
1). By adopting word(#phone), the length of se-
quences is represented more accurately. We also
take the reliability of the accepted words into ac-
count as word(CM). This uses confidence measures
(Lee et al, 2004) for a word, W , in ASR outputs,
which are denoted as CM(W ). The ?w is the thresh-old for determining whether word W is accepted or
not. The ww obtains a negative value for an unreli-able word W when CM(W ) is lower than ?w. Thisrepresents a preference for longer and more reliable
sequences.
4.3 Weighting for Concepts
In addition to the ASR level, weights on a concept
level are also assigned. The concepts are obtained
from the parsing results by the WFST, and contain
several words. Weights for concepts are defined by
using the measures of all words contained in a con-
cept.
We prepared three kinds of weights for the con-
cepts:
1. cpt(const.): wc = 1.0,
2. cpt(avg):
wc =
?
W (CM(W ) ? ?c)
#W , and
3. cpt(#pCM(avg)):
wc =
?
W (CM(W ) ? l(W ) ? ?c)
#W ,
where W is a set of accepted words, W , in the corre-
sponding concept, and #W is the number of words
in W .
The cpt(const.) represents a preference for
sequences with more concepts. The cpt(avg)
is defined as the weight by using the CM(W )
of each word contained in the concept. The
cpt(#pCM(avg)) represents a preference for longer
and reliable sequences with more concepts. The ?cis the threshold for the acceptance of a concept.
213
Table 3: Examples of weightings when parameter set is: word(CM) and cpt(#pCM(avg))
ASR onput No, it is February twenty second
LU output FILLER it is February twenty second
CM(W ) 0.3 0.7 0.6 0.9 1.0 0.9
l(W ) 0.3 0.2 0.2 0.9 0.6 0.5
Concept - - - month=2 day=22
word - 0.7 ? ?w 0.6 ? ?w 0.9 ? ?w 1.0 ? ?w 0.9 ? ?wcpt - - - (0.9 ? 0.9 ? ?c)/1 (1.0 ? 0.6 ? ?c + 0.9 ? 0.5 ? ?c)/2
'
&
$
%
Reference From June third please
ASR output From June third uhm FIT please LU result
CM(W ) 0.771 0.978 0.757 0.152 0.525 0.741
LU reference From June third FILLER FILLER FILLER month:6, day:3
Our method From June third FILLER FILLER FILLER month:6, day:3
Keyword spotting From June third FILLER FIT please month:6, day:3, car:FIT
(?FIT? is the name of a car.)
Figure 4: Example of LU with WFST
4.4 Calculating Cumulative Weight and
Training
The LU results are selected based on the weighted
sum of the three weights in Subsection 4.3 as
wi = wis + ?w
?
ww + ?c
?
wc
The LU module selects an output sequence with
the highest cumulative weight, wi, for 1 ? i ? N .
Let us explain how to calculate cumulative weight
wi by using the example specified in Table 3. Here,
word(CM) and cpt(#pCM(avg)) are selected as pa-
rameters. The sum of weights in this table for ac-
cepted words is ?w(4.1 ? 5?w), when the input se-quence is ?No, it is February twenty second.?.
The sum of weights for concepts is ?c(1.335 ? 2?c)because the weight for ?month=2? is ?c(0.81 ? ?c)and the weight for ?day=22? is ?c(0.525 ? ?c).Therefore, cumulative weight wi for this input se-
quence is wis + ?w(4.1 ? 5?w) + ?c(1.335 ? 2?c).In the training phase, various combinations of pa-
rameters are tested, i.e., which weightings are used
for each of ASR output and concept level, such as
N = 1 or 10, coefficient ?w,c = 1.0 or 0, and thresh-old ?w,c = 0 to 0.9 at intervals of 0.1, on the train-ing data. The coefficient ?w,c = 0 means that acorresponding weight is not added. The optimal pa-
rameter settings are obtained after testing the various
combinations of parameters. They make the concept
error rate (CER) minimum for a training data set.
We calculated the CER in the following equation:
CER = (S +D + I)/N , where N is the number of
concepts in a reference, and S, D, and I correspond
to the number of substitution, deletion, and insertion
errors.
Figure 4 shows an example of LU with our
method, where it rejects misrecognized concept
[car:FIT], which cannot be rejected by keyword
spotting.
5 Experiments and Evaluation
5.1 Experimental Conditions
We discussed our experimental investigation into the
effects of weightings in Section 4. The user utter-
ance in our experiment was first recognized by ASR.
Then, the i-th sentence of ASR output was input to
WFST for 1 ? i ? N , and the LU result for the
highest cumulative weight, wi, was obtained.
We used 4186 utterances in the video recording
reservation domain (video domain), which consisted
of eight different dialogues with a total of 25 differ-
ent speakers. We also used 3364 utterances in the
rent-a-car reservation domain (rent-a-car domain) of
214
eight different dialogues with 23 different speakers.
We used Julius 1 as a speech recognizer with an
SLM. The language model was prepared by using
example sentences generated from the grammars of
both domains. We used 10000 example sentences in
the video and 40000 in the rent-a-car domain. The
number of the generated sentences was determined
empirically. The vocabulary size was 209 in the
video and 891 in the rent-a-car domain. The average
ASR accuracy was 83.9% in the video and 65.7%
in the rent-a-car domain. The grammar in the video
domain included phrases for dates, times, channels,
commands. That of the rent-a-car domain included
phrases for dates, times, locations, car classes, op-
tions, and commands. The WFST parsing mod-
ule was implemented by using the MIT FST toolkit
(Hetherington, 2004).
5.2 Performance of WFST-based LU
We evaluated our method in the two domains: video
and rent-a-car. We compared the CER on test data,
which was calculated by using the optimal settings
for both domains. We evaluated the results with 4-
fold cross validation. The number of utterances for
training was 3139 (=4186*(3/4)) in the video and
2523 (=3364*(3/4)) in the rent-a-car domain.
The baseline method was simple keyword spot-
ting because we assumed a condition where a large
amount of training data was not available. This
method extracts as many keyphrases as possible
from ASR output without taking speech recogni-
tion errors and grammatical rules into consideration.
Both grammar-based and SLM-based ASR outputs
are used for input in keyword spotting (denoted as
?Grammar & spotting? and ?SLM & spotting? in
Table 4). The grammar for grammar-based ASR
was automatically generated by the domain descrip-
tion file. The accuracy of grammar-based ASR was
66.3% in the video and 43.2% in the rent-a-car do-
main.
Table 4 lists the CERs for both methods. In key-
word spotting with SLM-based ASR, the CERs were
improved by 5.2 points in the video and by 22.2
points in the rent-a-car domain compared with those
with grammar-based ASR. This is because SLM-
based ASR is more robust against fillers and un-
1http://julius.sourceforge.jp/
Table 4: Concept error rates (CERs) in each domain
Domain Grammar &spotting SLM &spotting Ourmethod
Video 22.1 16.9 13.5
Rent-a-car 51.1 28.9 22.0
known words than grammar-based ASR. The CER
was improved by 3.4 and 6.9 points by optimal
weightings for WFST. Table 5 lists the optimal pa-
rameters in both domains. The ?c = 0 in the videodomain means that weights for concepts were not
used. This result shows that optimal parameters de-
pend on the domain for the system, and these need
to be adapted for each domain.
5.3 Performance According to Training Data
We also investigated the relationship between the
size of the training data for our method and the CER.
In this experiment, we calculated the CER in the
test data by increasing the number of utterances for
training. We also evaluated the results by 4-fold
cross validation.
Figures 5 and 6 show that our method outper-
formed the baseline methods by about 80 utterances
in the video domain and about 30 utterances in
the rent-a-car domain. These results mean that our
method can effectively be used to rapidly prototype
LU modules. This is because it can achieve robust
LU with fewer training data compared with conven-
tional WFST-based methods, which need over sev-
eral thousand sentences for training.
6 Conclusion
We developed a method of rapidly prototyping ro-
bust LU modules for spoken language understand-
ing. An SLM for a speech recognizer and a WFST
for parsing were automatically generated from a do-
main grammar description. We defined two kinds
of weightings for the WFST at the word and con-
cept levels. These two kinds of weightings were
calculated by ASR outputs. This made it possi-
ble to create an LU module for a new domain with
less effort because the weighting scheme was rel-
atively simpler than those of conventional methods.
The optimal parameters could be selected with fewer
training data in both domains. Our experiment re-
215
Table 5: Optimal parameters in each domain
Domain N ?w ww ?c wc
Video 1 1.0 word(const.) 0 -
Rent-a-car 10 1.0 word(CM)-0.0 1.0 cpt(#pCM(avg))-0.8
 0
 5
 10
 15
 20
 25
 30
 35
 40
 3000 1000 500 250 100 50 10
C
E
R
#utt. for training
Grammar-based ASR & keyword spotting
SLM-based ASR & keyword spotting
Our method
Figure 5: CER in video domain
 0
 5
 10
 15
 20
 25
 30
 50
 55
 3000 1000 500 250 100 50 10
C
E
R
#utt. for training
Grammar-based ASR & keyword spotting
SLM-based ASR & keyword spotting
Our method
Figure 6: CER in rent-a-car domain
vealed that the CER could be improved compared to
the baseline by training optimal parameters with a
small amount of training data, which could be rea-
sonably prepared for new domains. This means that
our method is appropriate for rapidly prototyping
LU modules. Our method should help developers
of spoken dialogue systems in the early phases of
development. We intend to evaluate our method on
other domains, such as database searches and ques-
tion answering in future work.
Acknowledgments
We are grateful to Dr. Toshihiko Ito and Ms. Yuka
Nagano of Hokkaido University for constructing the
rent-a-car domain system.
References
Yulan He and Steve Young. 2005. Spoken Language
Understanding using the Hidden Vector State Model.Speech Communication, 48(3-4):262?275.
Lee Hetherington. 2004. The MIT finite-state trans-ducer toolkit for speech and language processing. InProc. 6th International Conference on Spoken Lan-guage Processing (INTERSPEECH-2004 ICSLP).
Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawahara.
2004. Real-time word confidence scoring using lo-cal posterior probabilities on tree trellis search. InProc. 2004 IEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP 2004),volume 1, pages 793?796.
Alexandors Potamianos and Hong-Kwang J. Kuo. 2000.
Statistical recursive finite state machine parsingfor speech understanding. In Proc. 6th Interna-tional Conference on Spoken Language Processing(INTERSPEECH-2000 ICSLP), pages 510?513.
Stephanie Seneff. 1992. TINA: A natural language sys-tem for spoken language applications. ComputationalLinguistics, 18(1):61?86.
Katsuhito Sudoh and Hajime Tsukada. 2005. Tightly in-
tegrated spoken language understanding using word-to-concept translation. In Proc. 9th European Con-ference on Speech Communication and Technology(INTERSPEECH-2005 Eurospeech), pages 429?432.
Chai Wutiwiwatchai and Sadaoki Furui. 2004. Hybridstatistical and structural semantic modeling for Thaimulti-stage spoken language understanding. In Proc.HLT-NAACL Workshop on Spoken Language Under-standing for Conversational Systems and Higher LevelLinguistic Information for Speech Processing, pages
2?9.
216
Proceedings of NAACL HLT 2009: Short Papers, pages 133?136,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Speech Understanding Framework
that Uses Multiple Language Models and Multiple Understanding Models
?Masaki Katsumaru, ?Mikio Nakano, ?Kazunori Komatani,
?Kotaro Funakoshi, ?Tetsuya Ogata, ?Hiroshi G. Okuno
?Graduate School of Informatics, Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto
606-8501, Japan
{katumaru, komatani}@kuis.kyoto-u.ac.jp
{ogata, okuno}@kuis.kyoto-u.ac.jp
?Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama
351-0188, Japan
{nakano, funakoshi}@jp.honda-ri.com
Abstract
The optimal combination of language model
(LM) and language understanding model
(LUM) varies depending on available training
data and utterances to be handled. Usually, a
lot of effort and time are needed to find the op-
timal combination. Instead, we have designed
and developed a new framework that uses
multiple LMs and LUMs to improve speech
understanding accuracy under various situa-
tions. As one implementation of the frame-
work, we have developed a method for select-
ing the most appropriate speech understand-
ing result from several candidates. We use
two LMs and three LUMs, and thus obtain six
combinations of them. We empirically show
that our method improves speech understand-
ing accuracy. The performance of the oracle
selection suggests further potential improve-
ments in our system.
1 Introduction
The speech understanding component in a spoken
dialogue system consists of an automatic speech
recognition (ASR) component and a language un-
derstanding (LU) component. To develop a speech
understanding component, we need to prepare an
ASR language model (LM) and a language under-
standing model (LUM) for the dialogue domain
of the system. There are many types of LMs
such as finite-state grammars and N-grams, and
many types of LUMs such as finite-state transduc-
ers (FST), weighted finite-state transducers (WFST),
and keyphrase-extractors (extractor). Selecting a
suitable combination of LM and LUM is necessary
for robust speech understanding against various user
utterances.
Conventional studies of speech understanding
have investigated which LM and LUM give the best
performance by using fixed training and test data
such as the Air Travel Information System (ATIS)
corpus. However, in real system development, re-
sources such as training data for statistical models
and efforts to write finite-state grammars vary ac-
cording to the available human resources or budgets.
Domain-dependent training data are particularly dif-
ficult to obtain. Therefore, in conventional system
development, system developers determine the types
of LM and LUM by trial and error. Every LM and
LUM has some advantages and disadvantages, so it
is difficult for a single combination of LM and LUM
to gain high accuracy except in a situation involv-
ing a lot of training data and effort. Therefore, using
multiple speech understanding methods is a more ef-
fective approach.
In this paper, we propose a speech understand-
ing framework called ?Multiple Language models
and Multiple Understanding models (MLMU)?, in
which multiple LMs and LUMs are used, to achieve
better performance under the various development
situations. It selects the best speech understanding
result from the multiple results generated by arbi-
trary combinations of LMs and LUMs.
So far there have been several attempts to im-
prove ASR and speech understanding using mul-
tiple speech recognizers and speech understanding
modules. ROVER (Fiscus, 1997) tried to improve
ASR accuracy by integrating the outputs of multi-
ple ASRs with different acoustic and language mod-
133
LM 1utterance LM 2
LM n 
resultconfidenceintegrationcomponent
LUcomponent speechunderstandingresultsASRcomponent
LUM n LUM 2
LUM 1
LM: Language Model
LUM: Language Understanding Model
Figure 1: Flow of speech understanding in MLMU
els. The work is different from our study in the fol-
lowing two points: it does not deal with speech un-
derstanding, and it assumes that each ASR is well-
developed and achieves high accuracy for a variety
of speech inputs. Eckert et al (1996) used multiple
LMs to deal with both in-grammar utterances and
out-of-grammar utterances, but did not mention lan-
guage understanding. Hahn et al (2008) used mul-
tiple LUMs, but just a single language model.
2 Speech Understanding Framework
MLMU
MLMU is a framework by which system developers
can use multiple speech understanding methods by
preparing multiple LMs and multiple LUMs. Fig-
ure 1 illustrates the flow of speech understanding in
MLMU. System developers list available LMs and
LUMs for each system?s domain, and the system
understands utterances by using these models. The
framework selects one understanding result from
multiple results or calculates a confidence score of
the result by using the generated multiple under-
standing results.
MLMU can improve speech understanding for the
following reason. The performance of each speech
understanding (a combination of LM and LUM)
might not be very high when either training data for
the statistical model or available expertise and ef-
fort for writing grammar are insufficient. In such
cases, some utterances might not be covered by the
system?s finite-state grammar LM, and probability
estimation in the statistical models may not be very
good. Using multiple speech understanding mod-
els is expected to solve this problem because each
model has different specialities. For example, finite-
state grammar LMs and FST-based LUMs achieve
high accuracy in recognizing and understanding in-
grammar utterances, whereas out-of-grammar utter-
ances are covered by N-gram models and LUMs
based on WFST and keyphrase-extractors. There-
fore it is more possible that the understanding results
of MLMU will include the correct result than a case
when a single understanding model is used.
The understanding results of MLMU will be help-
ful in many ways. We used them to achieve better
understanding accuracy by selecting the most reli-
able one. This selection is based on features con-
cerning ASR results and language understanding re-
sults. It is also possible to delay the selection, hold-
ing multiple understanding result candidates that
will be disambiguated as the dialogue proceeds (Bo-
hus, 2004). Furthermore, confidence scores, which
enable an efficient dialogue management (Komatani
and Kawahara, 2000), can be calculated by ranking
these results or by voting on them, by using multi-
ple speech understanding results. The understanding
results can be used in the discourse understanding
module and the dialogue management module. They
can choose one of the understanding results depend-
ing on the dialogue situation.
3 Implementation
3.1 Available Language Models and Language
Understanding Models
We implemented MLMU as a library of RIME-
TK, which is a toolkit for building multi-domain
spoken dialogue systems (Nakano et al, 2008).
With the current implementation, developers can use
the following LMs:
1. A LM based on finite-state grammar (FSG)
2. A domain-dependent statistical N-gram model
(N-gram)
and the following LUMs:
1. Finite-state transducer (FST)
2. Weighted FST (WFST)
3. Keyphrase-extractor (extractor).
System developers can use multiple finite-state-
grammar-based LMs or N-gram-based LMs, and
134
also multiple FSTs and WFSTs. They can specify
the combination for each domain by preparing LMs
and LUMs. They can specify grammar models when
sufficient human labor is available for writing gram-
mar, and specify statistical models when a corpus for
training models is available.
3.2 Selecting Understanding Result based on
ASR and LU Features
We also implemented a mechanism for selecting one
of the understanding results as the best hypothesis.
The mechanism chooses the result with the highest
estimated probability of correctness. Probabilities
are estimated for each understanding result by using
logistic regression, which uses several ASR and LU
features.
We define Pi as the probability that speech under-
standing result i is correct, and we select one result
based on argmax
i
Pi. We denote each speech un-
derstanding result as i (i = 1,. . . ,6). We constructed
a logistic regression model for Pi. The regression
function can be written as:
Pi = 11 + exp(?(ai1Fi1 + . . . + aimFim + bi)) .(1)
The coefficients ai1, . . . , aim, bi were fitted us-
ing training data. The independent variables
Fi1, Fi2, ..., Fim are listed in Table 1. In the table,
n indicates the number of understanding results, that
is, n = 6 in this paper?s experiment. Here, we denote
the features as Fi1, Fi2, ..., Fim.
Features from Fi1 to Fi3 represent characteristics
of ASR results. The acoustic scores were normal-
ized by utterance durations in seconds. These fea-
tures are used for verifying its ASR result. Features
from Fi4 to Fi9 represent characteristics of LU re-
sults. Features from Fi4 to Fi6 are defined on the
basis of the concept-based confidence scores (Ko-
matani and Kawahara, 2000).
4 Preliminary Experiment
We conducted a preliminary experiment to show the
potential of the framework by using the two LMs
and three LUMs noted in Section 3.1.
Table 1: Features from speech understanding result i
Fi1: acoustic score of ASR
Fi2: difference between Fi1 and acoustic score
of ASR for utterance verification
Fi3: utterance duration [sec.]
Fi4: average confidence scores for concepts in i
Fi5: average of Fi4 ( 1n
?n
i Fi4)
Fi6: proportion of Fi4 (Fi4 /?ni Fi5)
Fi7: average # concepts ( 1n
?n
i #concepti)
Fi8: max. # concepts (max (#concepti) )
Fi9: min. # concepts (min (#concepti) )
4.1 Preparing LMs and LUMs
The finite-state grammar rules were written in sen-
tence units manually. A domain-dependent statisti-
cal N-gram model was trained on 10,000 sentences
randomly generated from the grammar. The vocab-
ulary sizes of the grammar LM and the domain-
dependent statistical LM were both 278. We
also used a domain-independent statistical N-gram
model for obtaining acoustic scores for utterance
verification, which was trained onWeb texts (Kawa-
hara et al, 2004). Its vocabulary size was 60,250.
The grammar used in the FST was the same as the
FSG used as one of the LMs, which was manually
written by a system developer. The WFST-based LU
was based on a method to estimate WFST parame-
ters with a small amount of data (Fukubayashi et al,
2008). Its parameters were estimated by using 105
utterances of just one user. The keyphrase extrac-
tor extracts as many concepts as possible from an
ASR result on the basis of a grammar while ignor-
ing words that do not match the grammar.
4.2 Target Data for Evaluation
We used 3,055 utterances in the rent-a-car reserva-
tion domain (Nakano et al, 2007). We used Julius
(ver. 4.0.2) as the speech recognizer and a 3000-
state phonetic tied-mixture (PTM) triphone model
as the acoustic model1. ASR accuracy in mora ac-
curacy when using the FSG and the N-gram model
were 71.9% and 75.5% respectively. We used con-
cept error rates (CERs) to represent the speech un-
derstanding accuracy, which is calculated as fol-
1http://julius.sourceforge.jp/
135
Table 2: CERs [%] for each speech understanding
method
speech understanding method
(LM + LUM) CER
(1) FSG + FST 26.9
(2) FSG + WFST 29.9
(3) FSG + extractor 27.1
(4) N-gram + FST 35.2
(5) N-gram + WFST 25.3
(6) N-gram + extractor 26.0
selection from (1) through (6) (our method) 22.7
oracle selection 13.5
lows:
CER = # error concepts#concepts in utterances . (2)
We manually annotated whether an understanding
result of each utterance was correct or not, and
used them as training data to fit the coefficients
ai1, . . . , aim, bi.
4.3 Evaluation in Concept Error Rates
We fitted the coefficients of regression functions and
selected understanding results with a 10-fold cross
validation. Table 2 lists the CERs based on combi-
nations of single LM and LUM and by our method.
Of all combinations of single LM and LUM, the best
accuracy was obtained with (5) (N-gram + WFST).
Our method improved by 2.6 points over (5). Al-
though we achieved a lower CER, we used a lot
of data to estimate logistic regression coefficients.
Such a large amount of data may not be available in a
real situation. We will conduct more experiments by
changing the amount of training data. Table 2 also
shows the accuracy of the oracle selection, which
selected the best speech understanding result man-
ually. The CER of the oracle selection was 13.5%,
a significant improvement compared to all combina-
tions of a LM and LUM. There is no combination of
a LM and LUM whose understanding results were
not selected at all in the oracle selection and our
method?s selection. These results show that using
multiple LMs and multiple LUMs can potentially
improve speech understanding accuracy.
5 Ongoing work
We will conduct more experiments in other domains
or with other resources to evaluate the effectiveness
of our framework. We plan to investigate the case
in which a smaller amount of the training data is
used to estimate the coefficients of the logistic re-
gressions. Furthermore, finding a way to calculate
confidence scores of speech understanding results is
on our agenda.
References
Dan Bohus. 2004. Error awareness and recovery in
task-oriented spoken dialogue systems. Ph.D. thesis,
Carnegie Mellon University.
Wieland Eckert, Florian Gallwitz, and Heinrich Nie-
mann. 1996. Combining stochastic and linguistic lan-
guage models for recognition of spontaneous speech.
In Proc. ICASSP, pages 423?426.
Jonathan G. Fiscus. 1997. A post-processing system
to yield reduced word error rates: Recognizer Out-
put Voting Error Reduction (ROVER). In Proc. ASRU,
pages 347?354.
Yuichiro Fukubayashi, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tetsuya
Ogata, and Hiroshi G. Okuno. 2008. Rapid prototyp-
ing of robust language understanding modules for spo-
ken dialogue systems. In Proc. IJCNLP, pages 210?
216.
Stefan Hahn, Patrick Lehnen, and Hermann Ney. 2008.
System combination for spoken language understand-
ing. In Proc. Interspeech, pages 236?239.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Recent
progress of open-source LVCSR Engine Julius and
Japanese model repository. In Proc. ICSLP, pages
3069?3072.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management using
concept-level confidence measures of speech recog-
nizer output. In Proc. COLING, volume 1, pages 467?
473.
Mikio Nakano, Yuka Nagano, Kotaro Funakoshi, Toshi-
hiko Ito, Kenji Araki, Yuji Hasegawa, and Hiroshi Tsu-
jino. 2007. Analysis of user reactions to turn-taking
failures in spoken dialogue systems. In Proc. SIGdial,
pages 120?123.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and
Hiroshi Tsujino. 2008. A framework for building con-
versational agents based on a multi-expert model. In
Proc. SIGdial, pages 88?91.
136
Flexible Guidance Generation using
User Model in Spoken Dialogue Systems
Kazunori Komatani Shinichi Ueno Tatsuya Kawahara Hiroshi G. Okuno
Graduate School of Informatics
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
fkomatani,ueno,kawahara,okunog@kuis.kyoto-u.ac.jp
Abstract
We address appropriate user modeling in
order to generate cooperative responses to
each user in spoken dialogue systems. Un-
like previous studies that focus on user?s
knowledge or typical kinds of users, the
user model we propose is more compre-
hensive. Specifically, we set up three di-
mensions of user models: skill level to
the system, knowledge level on the tar-
get domain and the degree of hastiness.
Moreover, the models are automatically
derived by decision tree learning using
real dialogue data collected by the sys-
tem. We obtained reasonable classifica-
tion accuracy for all dimensions. Dia-
logue strategies based on the user model-
ing are implemented in Kyoto city bus in-
formation system that has been developed
at our laboratory. Experimental evalua-
tion shows that the cooperative responses
adaptive to individual users serve as good
guidance for novice users without increas-
ing the dialogue duration for skilled users.
1 Introduction
A spoken dialogue system is one of the promising
applications of the speech recognition and natural
language understanding technologies. A typical task
of spoken dialogue systems is database retrieval.
Some IVR (interactive voice response) systems us-
ing the speech recognition technology are being put
into practical use as its simplest form. According to
the spread of cellular phones, spoken dialogue sys-
tems via telephone enable us to obtain information
from various places without any other special appa-
ratuses.
However, the speech interface involves two in-
evitable problems: one is speech recognition er-
rors, and the other is that much information can-
not be conveyed at once in speech communications.
Therefore, the dialogue strategies, which determine
when to make guidance and what the system should
tell to the user, are the essential factors. To cope
with speech recognition errors, several confirma-
tion strategies have been proposed: confirmation
management methods based on confidence measures
of speech recognition results (Komatani and Kawa-
hara, 2000; Hazen et al, 2000) and implicit con-
firmation that includes previous recognition results
into system?s prompts (Sturm et al, 1999). In terms
of determining what to say to the user, several stud-
ies have been done not only to output answers cor-
responding to user?s questions but also to generate
cooperative responses (Sadek, 1999). Furthermore,
methods have also been proposed to change the di-
alogue initiative based on various cues (Litman and
Pan, 2000; Chu-Carroll, 2000; Lamel et al, 1999).
Nevertheless, whether a particular response is co-
operative or not depends on individual user?s char-
acteristics. For example, when a user says nothing,
the appropriate response should be different whether
he/she is not accustomed to using the spoken dia-
logue systems or he/she does not know much about
the target domain. Unless we detect the cause of the
silence, the system may fall into the same situation
repeatedly.
In order to adapt the system?s behavior to individ-
ual users, it is necessary to model the user?s patterns
(Kass and Finin, 1988). Most of conventional stud-
ies on user models have focused on the knowledge
of users. Others tried to infer and utilize user?s goals
to generate responses adapted to the user (van Beek,
1987; Paris, 1988). Elzer et al (2000) proposed a
method to generate adaptive suggestions according
to users? preferences.
However, these studies depend on knowledge of
the target domain greatly, and therefore the user
models need to be deliberated manually to be ap-
plied to new domains. Moreover, they assumed that
the input is text only, which does not contain errors.
On the other hand, spoken utterances include various
information such as the interval between utterances,
the presence of barge-in and so on, which can be
utilized to judge the user?s character. These features
also possess generality in spoken dialogue systems
because they are not dependent on domain-specific
knowledge.
We propose more comprehensive user models to
generate user-adapted responses in spoken dialogue
systems taking account of all available information
specific to spoken dialogue. The models change
both the dialogue initiative and the generated re-
sponse. In (Eckert et al, 1997), typical users? be-
haviors are defined to evaluate spoken dialogue sys-
tems by simulation, and stereotypes of users are as-
sumed such as patient, submissive and experienced.
We introduce user models not for defining users? be-
haviors beforehand, but for detecting users? patterns
in real-time interaction.
We define three dimensions in the user models:
?skill level to the system?, ?knowledge level on the
target domain? and ?degree of hastiness?. The for-
mer two are related to the strategies in manage-
ment of the initiative and the response generation.
These two enable the system to adaptively gener-
ate dialogue management information and domain-
specific information, respectively. The last one is
used to manage the situation when users are in hurry.
Namely, it controls generation of the additive con-
tents based on the former two user models. Handling
such a situation becomes more crucial in speech
communications using cellular phones.
The user models are trained by decision tree
Sys: Please tell me your current bus stop, your destination
or the specific bus route.
User: Shijo-Kawaramachi.
Sys: Do you take a bus from Shijo-Kawaramachi?
User: Yes.
Sys: Where will you get off the bus?
User: Arashiyama.
Sys: Do you go from Shijo-Kawaramachi to Arashiyama?
User: Yes.
Sys: Bus number 11 bound for Arashiyama has departed
Sanjo-Keihanmae, two bus stops away.
Figure 1: Example dialogue of the bus system
learning algorithm using real data collected from the
Kyoto city bus information system. Then, we imple-
ment the user models and adaptive dialogue strate-
gies on the system and evaluate them using data col-
lected with 20 novice users.
2 Kyoto City Bus Information System
We have developed the Kyoto City Bus Information
System, which locates the bus a user wants to take,
and tells him/her how long it will take before its
arrival. The system can be accessed via telephone
including cellular phones1. From any places, users
can easily get the bus information that changes ev-
ery minute. Users are requested to input the bus stop
to get on, the destination, or the bus route number
by speech, and get the corresponding bus informa-
tion. The bus stops can be specified by the name of
famous places or public facilities nearby. Figure 1
shows a simple example of the dialogue.
Figure 2 shows an overview of the system.
The system operates by generating VoiceXML
scripts dynamically. The real-time bus information
database is provided on the Web, and can be ac-
cessed via Internet. Then, we explain the modules
in the following.
VWS (Voice Web Server)
The Voice Web Server drives the speech recog-
nition engine and the TTS (Text-To-Speech)
module according to the specifications by the
generated VoiceXML.
Speech Recognizer
The speech recognizer decodes user utterances
1+81-75-326-3116
           VWS
(Voice Web Server)
response
sentences
recognition results
(only language info.)
recognition results
(including features other
 than language info.) Voice
   XML
user
TTS speech
recognizer
VoiceXML
generator
dialogue
manageruser
profiles
real bus
information
user model
identifier
CGI
the system except for
proposed user models
Figure 2: Overview of the bus system with user
models
based on specified grammar rules and vocabu-
lary, which are defined by VoiceXML at each
dialogue state.
Dialogue Manager
The dialogue manager generates response sen-
tences based on speech recognition results (bus
stop names or a route number) received from
the VWS. If sufficient information to locate a
bus is obtained, it retrieves the corresponding
information from the real-time bus information
database.
VoiceXML Generator
This module dynamically generates VoiceXML
files that contain response sentences and spec-
ifications of speech recognition grammars,
which are given by the dialogue manager.
User Model Identifier
This module classifies user?s characters based
on the user models using features specific to
spoken dialogue as well as semantic attributes.
The obtained user profiles are sent to the dia-
logue manager, and are utilized in the dialogue
management and response generation.
3 Response Generation using User Models
3.1 Classification of User Models
We define three dimensions as user models listed be-
low.
 Skill level to the system
 Knowledge level on the target domain
 Degree of hastiness
Skill Level to the System
Since spoken dialogue systems are not
widespread yet, there arises a difference in the
skill level of users in operating the systems. It
is desirable that the system changes its behavior
including response generation and initiative man-
agement in accordance with the skill level of the
user. In conventional systems, a system-initiated
guidance has been invoked on the spur of the
moment either when the user says nothing or
when speech recognition is not successful. In our
framework, by modeling the skill level as the user?s
property, we address a radical solution for the
unskilled users.
Knowledge Level on the Target Domain
There also exists a difference in the knowledge
level on the target domain among users. Thus, it is
necessary for the system to change information to
present to users. For example, it is not cooperative
to tell too detailed information to strangers. On the
other hand, for inhabitants, it is useful to omit too
obvious information and to output additive informa-
tion. Therefore, we introduce a dimension that rep-
resents the knowledge level on the target domain.
Degree of Hastiness
In speech communications, it is more important
to present information promptly and concisely com-
pared with the other communication modes such as
browsing. Especially in the bus system, the concise-
ness is preferred because the bus information is ur-
gent to most users. Therefore, we also take account
of degree of hastiness of the user, and accordingly
change the system?s responses.
3.2 Response Generation Strategy using User
Models
Next, we describe the response generation strategies
adapted to individual users based on the proposed
user models: skill level, knowledge level and hasti-
ness. Basic design of dialogue management is based
on mixed-initiative dialogue, in which the system
makes follow-up questions and guidance if neces-
sary while allowing a user to utter freely. It is in-
vestigated to add various contents to the system re-
sponses as cooperative responses in (Sadek, 1999).
Such additive information is usually cooperative, but
some people may feel such a response redundant.
Thus, we introduce the user models and control
the generation of additive information. By introduc-
ing the proposed user models, the system changes
generated responses by the following two aspects:
dialogue procedure and contents of responses.
Dialogue Procedure
The dialogue procedure is changed based on the
skill level and the hastiness. If a user is identified as
having the high skill level, the dialogue management
is carried out in a user-initiated manner; namely, the
system generates only open-ended prompts. On the
other hand, when user?s skill level is detected as low,
the system takes an initiative and prompts necessary
items in order.
When the degree of hastiness is low, the system
makes confirmation on the input contents. Con-
versely, when the hastiness is detected as high, such
a confirmation procedure is omitted.
Contents of Responses
Information that should be included in the sys-
tem response can be classified into the following two
items.
1. Dialogue management information
2. Domain-specific information
The dialogue management information specifies
how to carry out the dialogue including the instruc-
tion on user?s expression like ?Please reply with ei-
ther yes or no.? and the explanation about the fol-
lowing dialogue procedure like ?Now I will ask in
order.? This dialogue management information is
determined by the user?s skill level to the system,
 58.8>=
the maximum number of filled slots
dialogue state
initial state otherwise
presense of barge-in
rate of no input
0.07>
30 1 2
average of
recognition score
58.8<
skill level
high
skill level
high
skill level
low
skill level
low
Figure 3: Decision tree for the skill level
and is added to system responses when the skill level
is considered as low.
The domain-specific information is generated ac-
cording to the user?s knowledge level on the target
domain. Namely, for users unacquainted with the
local information, the system adds the explanation
about the nearest bus stop, and omits complicated
contents such as a proposal of another route.
The contents described above are also controlled
by the hastiness. For users who are not in hurry, the
system generates the additional contents as cooper-
ative responses. On the other hand, for hasty users,
the contents are omitted in order to prevent the dia-
logue from being redundant.
3.3 Classification of User based on Decision
Tree
In order to implement the proposed user models as a
classifier, we adopt a decision tree. It is constructed
by decision tree learning algorithm C5.0 (Quinlan,
1993) with data collected by our dialogue system.
Figure 3 shows the derived decision tree for the skill
level.
We use the features listed in Figure 4. They in-
clude not only semantic information contained in the
utterances but also information specific to spoken
dialogue systems such as the silence duration prior
to the utterance and the presence of barge-in. Ex-
cept for the last category of Figure 4 including ?at-
tribute of specified bus stops?, most of the features
are domain-independent.
The classification of each dimension is done for
every user utterance except for knowledge level. The
model of a user can change during a dialogue. Fea-
tures extracted from utterances are accumulated as
history information during the session.
Figure 5 shows an example of the system behav-
 features obtained from a single utterance
? dialogue state (defined by already filled slots)
? presence of barge-in
? lapsed time of the current utterance
? recognition result (something recognized / un-
certain / no input)
? score of speech recognizer
? the number of filled slots by the current utter-
ance
 features obtained from the session
? the number of utterances
? dialogue state of the previous utterance
? lapsed time from the beginning of the session
? the number of repetitions of a same question
? the average number of repetitions of a same
question
? ratio of the total time of user utterances in
whole elapsed time
? ratio of the occurrence of barge-in out of the
whole number of utterances
? recognition result of the previous utterance
? ratio of something recognized
? ratio of getting uncertain results
? ratio of no input
? the number of barge-in
? the number of something recognized
? the number of getting uncertain results
? the number of no input
? average of recognition scores
? the maximum number of slots filled by a single
utterance
 features obtained from the session (used only in clas-
sification of knowledge level)
? attribute of specified bus stops
? the way to specify bus stops (whether a bus stop
is specified by its correct name or not)
? ratio of each attribute of specified bus stops
? ratio of each way of specifying bus stops
Figure 4: Features to classify the user models
ior with the proposed user models. The skill level is
classified as being low by the decision tree, because
the first user?s utterance includes only one content
word. Then, dialogue procedure is changed to the
system-initiated one. Similarly, the hastiness is clas-
sified as being low by the decision tree, and the sys-
tem includes the explanation on the dialogue pro-
cedure and instruction on the expression in the re-
sponses. They are omitted if the hastiness is identi-
fied as high.
3.4 Decision Tree Learning for User Models
We train and evaluate the decision tree for the user
models using dialogue data collected by our system.
The data was collected from December 10th 2001 to
case: [skill level: low][knowledge level: low]
[hastiness: low]
Sys: Please tell me your current bus stop, your destination
or the specific bus route number.
User: Maruyama Park.
Sys: Let me confirm one by one.
( The explanation is added by [skill level: low].)
Will you get on at Maruyama Park?
( The content is confirmed by [hastiness: low].)
Please reply with yes or no.
( The instruction is added by [skill level: low].)
User: Yes.
Sys: Where will you get off the bus? Please specify by a
bus stop name or a well-known landmark.
( The explanation is added by [skill level: low].)
User: Tofukuji temple.
Sys: Will you get off the bus at Tofukuji temple?
Please reply with yes or no.
User: Yes.
Sys: The following buses are located.
( The explanation is added by [skill level: low].)
The nearest bus stop to Maruyama Park is Gion.
( The domain information is added by [knowl-
edge level: low].)
Bus number 202 bound for Nishioji-Kujo has de-
parted Higashiyama-Sanjo, which is two stops away.
...
Figure 5: An example dialogue with the proposed
user models
low indeterminable high total
skill level 743 253 496 1492
knowledge level 275 808 409 1492
hastiness 421 932 139 1492
Table 1: Number of manually labeled items for de-
cision tree learning
May 10th 2002. The number of the sessions (tele-
phone calls) is 215, and the total number of utter-
ances included in the sessions is 1492. We anno-
tated the subjective labels by hand. The annotator
judges the user models for every utterances based
on recorded speech data and logs. The labels were
given to the three dimensions described in section
3.3 among ?high?, ?indeterminable? or ?low?. It is
possible that annotated models of a user change dur-
ing a dialogue, especially from ?indeterminable? to
?low? or ?high?. The number of labeled utterances is
shown in Table 1.
Using the labeled data, we evaluated the classi-
fication accuracy of the proposed user models. All
the experiments were carried out by the method of
10-fold cross validation. The process, in which one
tenth of all data is used as the test data and the re-
mainder is used as the training data, is repeated ten
times, and the average of the accuracy is computed.
The result is shown in Table 2. The conditions #1,
#2 and #3 in Table 2 are described as follows.
#1: The 10-fold cross validation is carried out per
utterance.
#2: The 10-fold cross validation is carried out per
session (call).
#3: We calculate the accuracy under more realis-
tic condition. The accuracy is calculated not
in three classes (high / indeterminable / low)
but in two classes that actually affect the dia-
logue strategies. For example, the accuracy for
the skill level is calculated for the two classes:
low and the others. As to the classification of
knowledge level, the accuracy is calculated for
dialogue sessions because the features such as
the attribute of a specified bus stop are not ob-
tained in every utterance. Moreover, in order
to smooth unbalanced distribution of the train-
ing data, a cost corresponding to the reciprocal
ratio of the number of samples in each class is
introduced. By the cost, the chance rate of two
classes becomes 50%.
The difference between condition #1 and #2 is that
the training was carried out in a speaker-closed or
speaker-open manner. The former shows better per-
formance.
The result in condition #3 shows useful accuracy
in the skill level. The following features play im-
portant part in the decision tree for the skill level:
the number of filled slots by the current utterance,
presence of barge-in and ratio of no input. For the
knowledge level, recognition result (something rec-
ognized / uncertain / no input), ratio of no input and
the way to specify bus stops (whether a bus stop is
specified by its exact name or not) are effective. The
hastiness is classified mainly by the three features:
presence of barge-in, ratio of no input and lapsed
time of the current utterance.
condition #1 #2 #3
skill level 80.8% 75.3% 85.6%
knowledge level 73.9% 63.7% 78.2%
hastiness 74.9% 73.7% 78.6%
Table 2: Classification accuracy of the proposed user
models
4 Experimental Evaluation of the System
with User Models
We evaluated the system with the proposed user
models using 20 novice subjects who had not used
the system. The experiment was performed in the
laboratory under adequate control. For the speech
input, the headset microphone was used.
4.1 Experiment Procedure
First, we explained the outline of the system to sub-
jects and gave the document in which experiment
conditions and the scenarios were described. We
prepared two sets of eight scenarios. Subjects were
requested to acquire the bus information using the
system with/without the user models. In the sce-
narios, neither the concrete names of bus stops nor
the bus number were given. For example, one of
the scenarios was as follows: ?You are in Kyoto
for sightseeing. After visiting the Ginkakuji temple,
you go to Maruyama Park. Supposing such a situa-
tion, please get information on the bus.? We also set
the constraint in order to vary the subjects? hastiness
such as ?Please hurry as much as possible in order
to save the charge of your cellular phone.?
The subjects were also told to look over question-
naire items before the experiment, and filled in them
after using each system. This aims to reduce the sub-
ject?s cognitive load and possible confusion due to
switching the systems (Over, 1999). The question-
naire consisted of eight items, for example, ?When
the dialogue did not go well, did the system guide in-
telligibly?? We set seven steps for evaluation about
each item, and the subject selected one of them.
Furthermore, subjects were asked to write down
the obtained information: the name of the bus stop
to get on, the bus number and how much time it
takes before the bus arrives. With this procedure,
we planned to make the experiment condition close
to the realistic one.
duration (sec.) # turn
group 1 with UM 51.9 4.03
(with UM w/o UM) w/o UM 47.1 4.18
group 2 w/o UM 85.4 8.23
(w/o UM with UM) with UM 46.7 4.08
UM: User Model
Table 3: Duration and the number of turns in dia-
logue
The subjects were divided into two groups; a half
(group 1) used the system in the order of ?with
user models  without user models?, the other half
(group 2) used in the reverse order.
The dialogue management in the system without
user models is also based on the mixed-initiative di-
alogue. The system generates follow-up questions
and guidance if necessary, but behaves in a fixed
manner. Namely, additive cooperative contents cor-
responding to skill level described in section 3.2 are
not generated and the dialogue procedure is changed
only after recognition errors occur. The system with-
out user models behaves equivalently to the initial
state of the user models: the hastiness is low, the
knowledge level is low and the skill level is high.
4.2 Results
All of the subjects successfully completed the given
task, although they had been allowed to give up if the
system did not work well. Namely, the task success
rate is 100%.
Average dialogue duration and the number of
turns in respective cases are shown in Table 3.
Though the users had not experienced the system at
all, they got accustomed to the system very rapidly.
Therefore, as shown in Table 3, both the duration
and the number of turns were decreased obviously
in the latter half of the experiment in either group.
However, in the initial half of the experiment, the
group 1 completed with significantly shorter dia-
logue than group 2. This means that the incorpora-
tion of the user models is effective for novice users.
Table 4 shows a ratio of utterances for which the
skill level was identified as high. The ratio is calcu-
lated by dividing the number of utterances that were
judged as high skill level by the number of all utter-
ances in the eight sessions. The ratio is much larger
for group 1 who initially used the system with user
group 1 with UM 0.72
(with UM  w/o UM) w/o UM 0.70
group 2 w/o UM 0.41
(w/o UM  with UM) with UM 0.63
Table 4: Ratio of utterances for which the skill level
was judged as high
models. This fact means that novice users got ac-
customed to the system more rapidly with the user
models, because they were instructed on the usage
by cooperative responses generated when the skill
level is low. The results demonstrate that coopera-
tive responses generated according to the proposed
user models can serve as good guidance for novice
users.
In the latter half of the experiment, the dialogue
duration and the number of turns were almost same
between the two groups. This result shows that the
proposed models prevent the dialogue from becom-
ing redundant for skilled users, although generating
cooperative responses for all users made the dia-
logue verbose in general. It suggests that the pro-
posed user models appropriately control the genera-
tion of cooperative responses by detecting characters
of individual users.
5 Conclusions
We have proposed and evaluated user models for
generating cooperative responses adaptively to in-
dividual users. The proposed user models consist
of the three dimensions: skill level to the system,
knowledge level on the target domain and the de-
gree of hastiness. The user models are identified us-
ing features specific to spoken dialogue systems as
well as semantic attributes. They are automatically
derived by decision tree learning, and all features
used for skill level and hastiness are independent of
domain-specific knowledge. So, it is expected that
the derived user models can be used in other do-
mains generally.
The experimental evaluation with 20 novice users
shows that the skill level of novice users was im-
proved more rapidly by incorporating the user mod-
els, and accordingly the dialogue duration becomes
shorter more immediately. The result is achieved
by the generated cooperative responses based on the
proposed user models. The proposed user models
also suppress the redundancy by changing the dia-
logue procedure and selecting contents of responses.
Thus, they realize user-adaptive dialogue strategies,
in which the generated cooperative responses serve
as good guidance for novice users without increas-
ing the dialogue duration for skilled users.
References
Jennifer Chu-Carroll. 2000. MIMIC: An adaptive
mixed initiative spoken dialogue system for informa-
tion queries. In Proc. of the 6th Conf. on applied Nat-
ural Language Processing, pages 97?104.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system eval-
uation. In Proc. IEEE Workshop on Automatic Speech
Recognition and Understanding, pages 80?87.
Stephanie Elzer, Jennifer Chu-Carroll, and Sandra Car-
berry. 2000. Recognizing and utilizing user prefer-
ences in collaborative consultation dialogues. In Proc.
of the 4th Int?l Conf. on User Modeling, pages 19?24.
Timothy J. Hazen, Theresa Burianek, Joseph Polifroni,
and Stephanie Seneff. 2000. Integrating recognition
confidence scoring with language understanding and
dialogue modeling. In Proc. ICSLP.
Robert Kass and Tim Finin. 1988. Modeling the user in
natural language systems. Computational Linguistics,
14(3):5?22.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management using
concept-level confidence measures of speech recog-
nizer output. In Proc. Int?l Conf. Computational Lin-
guistics (COLING), pages 467?473.
Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, and Samir
Bennacef. 1999. The LIMSI ARISE system for
train travel information. In IEEE Int?l Conf. Acoust.,
Speech & Signal Process.
Diane J. Litman and Shimei Pan. 2000. Predicting and
adapting to poor speech recognition in a spoken dia-
logue system. In Proc. of the 17th National Confer-
ence on Artificial Intelligence (AAAI2000).
Paul Over. 1999. Trec-7 interactive track report. In Proc.
of the 7th Text REtrieval Conference (TREC7).
Cecile L. Paris. 1988. Tailoring object descriptions to
a user?s level of expertise. Computational Linguistics,
14(3):64?78.
J. Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo, CA.
http://www.rulequest.com/see5-info.html.
David Sadek. 1999. Design considerations on dia-
logue systems: From theory to technology -the case
of artimis-. In Proc. ESCA workshop on Interactive
Dialogue in Multi-Modal Systems.
Janienke Sturm, Els den Os, and Lou Boves. 1999. Is-
sues in spoken dialogue systems: Experiences with the
Dutch ARISE system. In Proc. ESCA workshop on In-
teractive Dialogue in Multi-Modal Systems.
Peter van Beek. 1987. A model for generating better
explanations. In Proc. of the 25th Annual Meeting of
the Association for Computational Linguistics (ACL-
87), pages 215?220.
Chunk-based Statistical Translation
Taro Watanabe?, Eiichiro Sumita? and Hiroshi G. Okuno?
{taro.watanabe, eiichiro.sumita}@atr.co.jp
? ATR Spoken Language Translation ?Department of Intelligence Science
Research Laboratories and Technology
2-2-2 Hikaridai, Keihanna Science City Graduate School of Informatics, Kyoto Uniersity
Kyoto 619-0288 JAPAN Kyoto 606-8501 JAPAN
Abstract
This paper describes an alternative trans-
lation model based on a text chunk un-
der the framework of statistical machine
translation. The translation model sug-
gested here first performs chunking. Then,
each word in a chunk is translated. Fi-
nally, translated chunks are reordered.
Under this scenario of translation model-
ing, we have experimented on a broad-
coverage Japanese-English traveling cor-
pus and achieved improved performance.
1 Introduction
The framework of statistical machine translation for-
mulates the problem of translating a source sentence
in a language J into a target language E as the
maximization problem of the conditional probability
?E = argmaxE P(E|J). The application of the Bayes
Rule resulted in ?E = argmaxE P(E)P(J|E). The for-
mer term P(E) is called a language model, repre-
senting the likelihood of E. The latter term P(J|E)
is called a translation model, representing the gener-
ation probability from E into J.
As an implementation of P(J|E), the word align-
ment based statistical translation (Brown et al,
1993) has been successfully applied to similar lan-
guage pairs, such as French?English and German?
English, but not to drastically different ones, such
as Japanese?English. This failure has been due to
the limited representation by word alignment and
the weak model structure for handling complicated
word correspondence.
This paper provides a chunk-based statistical
translation as an alternative to the word alignment
based statistical translation. The translation process
inside the translation model is structured as follows.
A source sentence is first chunked, and then each
chunk is translated into target language with local
word alignments. Next, translated chunks are re-
ordered to match the target language constraints.
Based on this scenario, the chunk-based statis-
tical translation model is structured with several
components and trained by a variation of the EM-
algorithm. A translation experiment was carried
out with a decoder based on the left-to-right beam
search. It was observed that the translation quality
improved from 46.5% to 52.1% in BLEU score and
from 59.2% to 65.1% in subjective evaluation.
The next section briefly reviews the word align-
ment based statistical machine translation (Brown et
al., 1993). Section 3 discusses an alternative ap-
proach, a chunk-based translation model, ranging
from its structure to training procedure and decod-
ing algorithm. Then, Section 4 provides experimen-
tal results on Japanese-to-English translation in the
traveling domain, followed by discussion.
2 Word Alignment Based Statistical
Translation
Word alignment based statistical translation rep-
resents bilingual correspondence by the notion of
word alignment A, allowing one-to-many generation
from each source word. Figure 1 illustrates an exam-
ple of English and Japanese sentences, E and J, with
sample word alignments. In this example, ?show1?
has generated two words, ?mise5? and ?tekudasai6?.
E = NULL0 show1 me2 the3 one4 in5 the6 window7
J = uindo1 no2 shinamono3 o4 mise5 tekudasai6
A = ( 7 0 4 0 1 1 )
Figure 1: Example of word alignment
Under this word alignment assumption, the transla-
tion model P(J|E) can be further decomposed with-
out approximation.
P(J|E) =
?
A
P(J, A|E)
2.1 IBM Model
During the generation process from E to J, P(J, A|E)
is assumed to be structured with a couple of pro-
cesses, such as insertion, deletion and reorder. A
scenario for the word alignment based translation
model defined by Brown et al (1993), for instance
IBM Model 4, goes as follows (refer to Figure 2).
1. Choose the number of words to generate for
each source word according to the Fertility
Model. For example, ?show? was increased to
2 words, while ?me? was deleted.
2. Insert NULLs at appropriate positions by the
NULL Generation Model. Two NULLs were
inserted after each ?show? in Figure 2.
3. Translate word-by-word for each generated
word by looking up the Lexicon Model. One of
the two ?show? words was translated to ?mise.?
4. Reorder the translated words by referring to the
Distortion Model. The word ?mise? was re-
ordered to the 5th position, and ?uindo? was
reordered to the 1st position. Positioning is de-
termined by the previous word?s alignment to
capture phrasal constraints.
For the meanings of each symbol in each model, re-
fer to Brown et al (1993).
2.2 Problems of Word Alignment Based
Translation Model
The strategy for the word alignment based transla-
tion model is to translate each word by generating
multiple single words (a bag of words) and to deter-
mine the position of each translated word. Although
show1 show show mise uindo1
me2 show NULL no no2
the3 one show tekudasai shinamono3
one4 window NULL o o4
in5 one shinamono mise5
the6 window uindo tekudasai6
window7
n(2|E1)
n(0|E2)
n(0|E3)
...
Fertility
(4
2
)
p4?20 p
2
1
NULL t(J5 |E1)
t(J6 |E1)
t(J3 |E4)
...
Lexicon
d1(1 ?  31 |E4, J1)
d1(3 ?  5+62 |E1, J3)
d1(5 ?  2+42 |NULL, J5)
d
>1(6 ? 5|J6)
Distortion
Figure 2: Word alignment based translation model
P(J, A|E) (IBM Model 4)
this procedure is sufficient to capture the bilingual
correspondence for similar language pairs, some is-
sues remain for drastically different pairs:
Insertion/Deletion Modeling Although deletion
was modeled in the Fertility Model, it merely as-
signs zero to each deleted word without considering
context. Similarly, inserted words are selected by
the Lexical Model parameter and inserted at the po-
sitions determined by a binomial distribution.
This insertion/deletion scheme contributed to the
simplicity of this representation of the translation
processes, allowing a sophisticated application to
run on an enormous bilingual sentence collection.
However, it is apparent that the weak modeling of
those phenomena will lead to inferior performance
for language pairs such as Japanese and English.
Local Alignment Modeling The IBM Model 4
(and 5) simulates phrasal constraints, although there
were implicitly implemented as its Distortion Model
parameters. In addition, the entire reordering is
determined by a collection of local reorderings in-
sufficient to capture the long-distance phrasal con-
straints.
The next section introduces an alternative model-
ing, chunk-based statistical translation, which was
intended to resolve the above two issues.
3 Chunk-based Statistical Translation
Chunk-based statistical translation models the pro-
cess of chunking for both the source and target sen-
tences, E and J,
P(J|E) =
?
J
?
E
P(J,J ,E|E)
where J and E are the chunked sentences for J
and E, respectively, defined as two-dimentional ar-
E = show1 me2 1 the3 one4 2 in5 the6 window7 3
mise5 tekudasai6 shinamono3 o4 uindo1 no2
J = uindo1 no2
1
shinamono3 o4
2
mise5 tekudasai6
3
A = ( 3 2 1 )
A = ( [ 7, 0 ] [ 4, 0 ] [ 1, 1 ] )
Figure 3: Example of chunk-based alignment
rays. For instance, Ji, j represents the jth word of
the ith chunk. The number of chunks for source
and target is assumed to be equal, |J| = |E|,
so that each chunk can convey a unit of meaning
without added/subtracted information. The term
P(J,J ,E|E) is further decomposed by chunk align-
ment A and word alignment for each chunk transla-
tion A.
P(J,J ,E|E) =
?
A
?
A
P(J,J , A,A,E|E)
The notion of alignment A is the same as those found
in the word alignment based translation model,
which assigns a source chunk index for each target
chunk. A is a two-dimensional array which assigns
a source word index for each target word per chunk.
For example, Figure 3 shows two-level alignments
taken from the example in Figure 1. The target
chunk at position 3, J3, ?mise tekudasai? is aligned
to the first position (A3 = 1), and both the words
?mise? and ?tekudasai? are aligned to the first posi-
tion of the source sentence (A3,1 = 1,A3,2 = 1).
3.1 Translation Model Structure
The term P(J,J , A,A,E|E) is further decomposed
with approximation according to the scenario de-
scribed below (refer to Figure 4).
1. Perform chunking for source sentence E by
P(E|E). For instance, chunks of ?show me? and
?the one? were derived. The process is mod-
eled by two steps:
(a) Selection of chunk size (Head Model).
For each word Ei, assign the chunk size
?i using the head model (?i |Ei). A word
with chunk size more than 0 (?i > 0) is
treated as a head word, otherwise a non-
head (refer to the words in bold in Figure
4).
(b) Associate each non-head word to a head
word (Chunk Model). Each non-head
word Ei is associated to a head word Eh by
the probability ?(c(Eh)|h? i, c(Ei)), where
h is the position of a head word and c(E)
is a function to map a word E to its word
class (i.e. POS). For instance, ?the3? is
associated with the head word ?one4? lo-
cated at 4 ? 3 = +1.
2. Select words to be translated with Deletion and
Fertility Model.
(a) Select the number of head words. For each
head word Eh (?h > 0), choose fertility ?h
according to the Fertility Model ?(?h|Eh).
We assume that the head word must be
translated, therefore ?h > 0. In addition,
one of them is selected as a head word at
target position using a uniform distribu-
tion 1/?h.
(b) Delete some non-head words. For
each non-head word Ei (?i = 0),
delete it according to the Deletion Model
?(di|c(Ei), c(Eh)), where Eh is the head
word in the same chunk and di is 1 if Ei
is deleted, otherwise 0.
3. Insert some words. In Figure 4, NULLs were
inserted for two chunks. For each chunk Ei,
select the number of spurious words ??i by In-
sertion Model ?(??i |c(Eh)), where Eh is the head
word of Ei.
4. Translate word-by-word. Each source word Ei,
including spurious words, is translated to Jj ac-
cording to the Lexicon Model, ?(Jj|Ei).
5. Reorder words. Each word in a chunk is
reordered according to the Reorder Model
P(A j|EA j ,J j). The chunk reordering is taken
after the Distortion Model of IBM Model 4,
where the position is determined by the relative
position from the head word,
P(A j|EA j ,J j) =
|A j |
?
k=1
?(k ? h|c(E
AA j ,k ), c(J j,k))
where h is the position of a head word for the
chunk J j. For example, ?no? is positioned ?1
of ?uindo?.
show1 show show show mise mise uindo1
me2 me show show tekudasai tekudasai no2
the3 the NULL o shinamono shinamono3
one4 one one one shinamono o o4
in5 in mise5
the6 the NULL no uindo tekudasai6
window7 window window window uindo no
Chunking Deletion& Fertility Insertion Lexicon Reorder
Chunk
Reorder
Figure 4: Chunk-based translation model. The words in bold are head words.
6. Reorder chunks. All of the chunks are
reordered according to the Chunk Reorder
Model, P(A|E,J). The chunk reordering is
also similar to the Distortion Model, where the
positioning is determined by the relative posi-
tion from the previous alignment
P(A|E,J) =
|J|
?
j=1

( j ? j?|c(EA j?1,h?), c(J j,h))
where j? is the chunk alignment of the the pre-
vious chunk aEA j?1. h and h? are the head word
indices for J j and EA j?1, respectively. Note
that the reordering is dependent on head words.
To summarize, the chunk-based translation model
can be formulated as
P(J|E) =
?
E,J ,A,A
?
i
(?i|Ei)
?
?
i:?i=0
?(c(Ehi)|hi ? i, c(Ei))
?
?
i:?i>0
?(?i|Ei)/?i
?
?
i:?i=0
?(di |c(Ei), c(Ehi))
?
?
i:?i>0
?(??i |c(Ei)) ?
?
j
?
k
?(J j,k|EA j,k )
?
?
j
P(A j|EA j ,J j) ? P(A|E,J)
.
3.2 Characteristics of chunk-based Translation
Model
The main difference to the word alignment based
translation model is the treatment of the bag of
word translations. The word alignment based trans-
lation model generates a bag of words for each
source word, while the chunk-based model con-
structs a set of target words from a set of source
words. The behavior is modeled as a chunking pro-
cedure by first associating words to the head word
of its chunk and then performing chunk-wise trans-
lation/insertion/deletion.
The complicated word alignment is handled by
the determination of word positions in two stages:
translation of chunk and chunk reordering. The for-
mer structures local orderings while the latter con-
stitutes global orderings. In addition, the concept of
head associated with each chunk plays the central
role in constraining different levels of the reordering
by the relative positions from heads.
3.3 Parameter Estimation
The parameter estimation for the chunk-based trans-
lation model relies on the EM-algorithm (Dempster
et al, 1977). Given a large bilingual corpus the
conditional probability of P(J , A,A,E|J, E) =
P(J,J , A,A,E|E)/?
J ,A,A,E P(J,J , A,A,E|E) is
first estimated for each pair of J and E (E-step),
then each model parameters is computed based
on the estimated conditional probability (M-step).
The above procedure is iterated until the set of
parameters converge.
However, this naive algorithm will suffer from se-
vere computational problems. The enumeration of
all possible chunkings J and E together with word
alignment A and chunk alignment A requires a sig-
nificant amount of computation. Therefore, we have
introduced a variation of the Inside-Outside algo-
rithm as seen in (Yamada and Knight, 2001) for E-
step computation. The details of the procedure are
described in Appendix A.
In addition to the computational problem, there
exists a local-maximum problem, where the EM-
Algorithm converges to a maximum solution but
does not guarantee finding the global maximum. In
order to solve this problem and to make the pa-
rameters converge quickly, IBM Model 4 parame-
ters were used as the initial parameters for training.
We directly applied the Lexicon Model and Fertility
Model to the chunk-based translation model but set
other parameters as uniform.
3.4 Decoding
The decoding algorithm employed for this chunk-
based statistical translation is based on the beam
search algorithm for word alignment statistical
translation presented in (Tillmann and Ney, 2000),
which generates outputs in left-to-right order by
consuming input in an arbitrary order.
The decoder consists of two stages:
1. Generate possible output chunks for all possi-
ble input chunks.
2. Generate hypothesized output by consuming
input chunks in arbitrary order and combining
possible output chunks in left-to-right order.
The generation of possible output chunks is es-
timated through an inverted lexicon model and
sequences of inserted strings (Tillmann and Ney,
2000). In addition, an example-based method is
also introduced, which generates candidate chunks
by looking up the viterbi chunking and alignment
from a training corpus.
Since the combination of all possible chunks is
computationally very expensive, we have introduced
the following pruning and scoring strategies.
beam pruning: Since the search space is enor-
mous, we have set up a size threshold to main-
tain partial hypotheses for both of the above
two stages. We also incorporated a threshold
for scoring, which allows partial hypotheses
with a certain score to be processed.
example-based scoring: Input/output chunk pairs
that appeared in a training corpus are ?re-
warded? so that they are more likely kept in
the beam. During the decoding process, when
a pair of chunks appeared in the first stage, the
score is boosted by using this formula in the log
domain,
log Ptm(J|E) + log Plm(E)
Table 1: Basic Travel Expression Corpus
Japanese English
# of sentences 171,894
# of words 1,181,188 1,009,065
vocabulary size 20472 16232
# of singletons 82,06 5,854
3-gram perplexity 23.7 35.8
+ weight ?
?
j
f req(EA j ,J j)
in which Ptm(J|E) and Plm(E) are translation
model and language model probability, respec-
tively1, f req(EA j ,J j) is the frequency for the
pair EA j and J j appearing in the training cor-
pus, and weight is a tuning parameter.
4 Experiments
The corpus for this experiment was extracted from
the Basic Travel Expression Corpus (BTEC), a col-
lection of conversational travel phrases for Japanese
and English (Takezawa et al, 2002) as seen in Ta-
ble 1. The entire corpus was split into three parts:
152,169 sentences for training, 4,846 sentences for
testing, and the remaining 10,148 sentences for pa-
rameter tuning, such as the termination criteria for
the training iteration and the parameter tuning for
decoders.
Three translation systems were tested for compar-
ison:
model4: Word alignment based translation model,
IBM Model 4 with a beam search decoder.
chunk3: Chunk-based translation model, limiting
the maximum allowed chunk size to 3.
model3+: chunk3 with example-based chunk can-
didate generation.
Figure 5 shows some examples of viterbi chunking
and chunk alignment for chunk3.
Translations were carried out on 510 sentences se-
lected randomly from the test set and evaluated ac-
cording to the following criteria with 16 reference
sets.
WER: Word-error-rate, which penalizes the edit distance
against reference translations.
1For simplicity of notation, dependence on other variables
are omitted, such as J .
[ i * have ] [ the * number ] [ of my * passport ]
[ *?????? e] [ *????? ] [? *???? ]
[ i * have ] [ a * stomach ache ] [ please * give me ] [ some * medicine ]
[??? *?? ] [ *?? ] [ *?? ] [ *??? ]
[ * i ] [ * ?d ] [ * like ] [ a * table ] [ * for ] [ * two ] [ by the * window ] [ * if possible ]
[ *???? ] [?? ] [? *?? ] [ *??? ] [? *????? ] [??? *?? ] [ *??? ] [ *???? ]
[ i ? have ] [ a ? reservation ] [ ? for ] [ two ? nights ] [ my ? name is ] [ ? risa kobayashi ]
[? ?? ] [ ?? ] [??? ?? ] [??? ???? ] [? ???? ] [?? ????? ]
Figure 5: Examples of viterbi chunking and chunk alignment for English-to-Japanese translation model.
Chunks are bracketed and the words with ? to the left are head words.
Table 2: Experimental results for Japanese?English
translation
Model WER PER BLEU SE [%]
[%] [%] [%] A A+B A+B+C
model4 43.3 37.2 46.5 59.2 74.1 80.2
chunk3 40.9 36.1 48.4 59.8 73.5 78.8
chunk3+ 38.5 33.7 52.1 65.1 76.3 80.6
PER: Position independent WER, which penalizes without
considering positional disfluencies.
BLEU: BLEU score, which computes the ratio of n-gram for
the translation results found in reference translations (Pa-
pineni et al, 2002).
SE: Subjective evaluation ranks ranging from A to D
(A:Perfect, B:Fair, C:Acceptable and D:Nonsense),
judged by native speakers.
Table 2 summarizes the evaluation of Japanese-to-
English translations, and Figure 6 presents some of
the results by model4 and chunk3+.
As Table 2 indicates, chunk3 performs better than
model4 in terms of the non-subjective evaluations,
although it scores almost equally in subjective eval-
uations. With the help of example-based decoding,
chunk3+ was evaluated as the best among the three
systems.
5 Discussion
The chunk-based translation model was originally
inspired by transfer-based machine translation but
modeled by chunks in order to capture syntax-based
correspondence. However, the structures evolved
into complicated modeling: The translation model
involves many stages, notably chunking and two
kinds of reordering, word-based and chunk-based
alignments. This is directly reflected in parameter
input: ????????????????
reference: is this all the baggage from flight one five two
model4: is this all you baggage for flight one five two
chunk3: is this all the baggage from flight one five two
input: ?????????????????
reference: may i have room service for breakfast please
model4: please give me some room service please
chunk3: i ?d like room service for breakfast
input: ??????????????????????
reference: hello i ?d like to change my reservation for march nineteenth
model4: i ?d like to change my reservation for ninety days be march hello
chunk3: hello i ?d like to change my reservation on march nineteenth
input: ?????????????????
reference: wait a couple of minutes i ?m telephoning now
model4: is this the line is busy now a few minutes
chunk3: i ?m on another phone now please wait a couple of minutes
Figure 6: Translation examples by word alignment
based model and chunk-based model
estimation, where chunk3 took 20 days for 40 iter-
ations, which is roughly the same amount of time
required for training IBM Model 5 with pegging.
The unit of chunk in the statistical machine
translation framework has been extensively dis-
cussed in the literature. Och et al (1999) pro-
posed a translation template approach that com-
putes phrasal mappings from the viterbi align-
ments of a training corpus. Watanabe et al (2002)
used syntax-based phrase alignment to obtain
chunks. Marcu and Wong (2002) argued for a dif-
ferent phrase-based translation modeling that di-
rectly induces a phrase-by-phrase lexicon model
from word-wise data. All of these methods bias
the training and/or decoding with phrase-level ex-
amples obtained by preprocessing a corpus (Och et
al., 1999; Watanabe et al, 2002) or by allowing a
lexicon model to hold phrases (Marcu and Wong,
2002). On the other hand, the chunk-based transla-
tion model holds the knowledge of how to construct
a sequence of chunks from a sequence of words. The
former approach is suitable for inputs with less de-
viation from a training corpus, while the latter ap-
proach will be able to perform well on unseen word
sequences, although chunk-based examples are also
useful for decoding to overcome the limited context
of a n-gram based language model.
Wang (1998) presented a different chunk-based
method by treating the translation model as a phrase-
to-string process. Yamada and Knight (2001) fur-
ther extended the model to a syntax-to-string trans-
lation modeling. Both assume that the source part
of a translation model is structured either with a se-
quence of chunks or with a parse tree, while our
method directly models a string-to-string procedure.
It is clear that the string-to-string modeling with hi-
den chunk-layers is computationally more expensive
than those structure-to-string models. However, the
structure-to-string approaches are already biased by
a monolingual chunking or parsing, which, in turn,
might not be able to uncover the bilingual phrasal or
syntactical constraints often observed in a corpus.
Alshawi et al (2000) also presented a two-level
arranged word ordering and chunk ordering by a hi-
erarchically organized collection of finite state trans-
ducers. The main difference from our work is that
their approach is basically deterministic, while the
chunk-based translation model is non-deterministic.
The former method, of course, performs more ef-
ficient decoding but requires stronger heuristics to
generate a set of transducers. Although the latter
approach demands a large amount of decoding time
and hypothesis space, it can operate on a very broad-
coverage corpus with appropriate translation model-
ing.
Acknowledgments
The research reported here was supported in part by
a contract with the Telecommunications Advance-
ment Organization of Japan entitled ?A study of
speech dialogue translation technology based on a
large corpus?.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
A. P. Dempster, N.M. Laird, and D.B.Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society,
B(39):1?38.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. of EMNLP-2002, Philadelphia, PA, July.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proc. of EMNLP/WVLC, Univer-
sity of Maryland, College Park, MD, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002,
pages 311?318.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world. In
Proc. of LREC 2002, pages 147?152, Las Palmas, Ca-
nary Islands, Spain, May.
Christoph Tillmann and Hermann Ney. 2000. Word
re-ordering and dp-based search in statistical machine
translation. In Proc. of the COLING 2000, July-
August.
Ye-Yi Wang. 1998. Grammar Inference and Statis-
tical Machine Translation. Ph.D. thesis, School of
Computer Science, Language Technologies Institute,
Carnegie Mellon University.
Taro Watanabe, Kenji Imamura, and Eiichiro Sumita.
2002. Statistical machine translation based on hierar-
chical phrase alignment. In Proc. of TMI 2002, pages
188?198, Keihanna, Japan, March.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL 2001,
Toulouse, France.
Appendix A Inside-Outside Algorithm for
Chunk-based Translation
Model
The basic idea of inside-outside computation is to
separate the whole process into two parts, chunk
translation and chunk reordering. Chunk transla-
tion handles translation of each chunk, while chunk
reordering performs chunking and chunk reoprder-
ing. The inside (backward or beta) probabilities
can be derived, which represent the probability of
source/target paring of chunks and sentences. The
outside (forward or alpha) probabilities can be de-
fined as the probability of a particular source and
target pair appearing at a particular chunking and re-
ordering.
Inside Probability First, given E and J, compute
chunk translation inside probabilities for all the pos-
sible source and target chunks pairing Ei?i and J
j?
j in
which Ei?i is the chunk ranging from index i to i
?
,
?(Ei?i , J j
?
j ) =
?
A?
P(A, J j?j |Ei
?
i )
=
?
A?
?
?
P
?
(A?, J j?j , Ei
?
i )
where P
?
is the probability of a model with asso-
ciated values for corresponding random variables,
such as (?i|Ei) or ?(Jj|Ei), except for the chunk re-
order model 
. A? is a word alignment for the chunks
Ei?i and J
j?
j .
Second, compute the inside probability for sen-
tence pairs E and J by considering all possible
chunkings and chunk alignments.
?(E, J) =
?
E,J:|E|=|J|
?
A
P(A,E,J , J|E)
=
?
E,J:|E|=|J|
?
A
P(A|E,J)
?
j
?(EA j ,J j)
Outside Probability The outside probability for
sentence pairing is always 1.
?(E, J) = 1.0
The outside probabilities for each chunk pair is
?(Ei?i , J j
?
j ) = ?(E, J)
?
E,J:|E|=|J|
?
A
P(A|E,J)
?
?
EAkE
i?
i ,JkJ
j?
j
?(EAk ,Jk)
.
Inside-Outside Computation The combination
of the above inside-outside probabilities yields the
following formulas for the accumulated counts of
pair occurrences.
First, the counts for each model parameter ? with
associated random variables count
?
(?) is
count
?
(?) =
?
<E,J>
?
?(A?,Ei?i ,J
j?
j )
?(Ei?i , J j
?
j )/?(E, J)
?
?
?
?
P
?
?(A?, J j?j , Ei
?
i )
.
Second, the count for chunk reordering with asso-
ciated random variables count


(?) is
count


(?) =
?
<E,J>
?(E, J)/?(E, J)
?
?(A,E,J)
P


(A|E,J)
?
k
?(EAk ,Jk)
.
Approximation Even with the introduction of the
inside-outside parameter estimation paradigm, the
enumeration of all possible chunk pairing and word
alignment requires O(lmk4(k + 1)k) computations,
where l and m are sentence length for E and J, re-
spectively, and k is the maximum allowed number
of words per chunk. In addition, the enumeration
of all possible alignments for all possible chunked
sentences is O(2l2mn!), where n = |J| = |E|.
In order to handle the massive amount of compu-
tational demand, we have applied an approximation
to the inside-outside estimation procedure. First,
the enumeration of word alignment computation for
chunk translations was approximated by a set of
alignments, the viterbi alignment and neighboring
alignment through move/swap operations of partic-
ular word alignments.
Second, the chunk alignment enumeration was
also approximated by a set of chunking and chunk
alignments as follows.
1. Determines the number of chunks per sentence
2. Determine initial chunking and alignment
3. Compute viterbi chunking-alignment via hill-climbing
using the following operators
? Move boundary of chunk
? Swap chunk alignment
? Move head position
4. Compute neighboring chunking-alignment using the
above operators
Flexible Spoken Dialogue System based on User Models
and Dynamic Generation of VoiceXML Scripts
Kazunori Komatani Fumihiro Adachi Shinichi Ueno
Tatsuya Kawahara Hiroshi G. Okuno
Graduate School of Informatics
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,adachi,ueno,kawahara,okuno}@kuis.kyoto-u.ac.jp
Abstract
We realize a telephone-based collab-
orative natural language dialogue sys-
tem. Since natural language involves
very various expressions, a large num-
ber of VoiceXML scripts need to be pre-
pared to handle all possible input patterns.
We realize flexible dialogue management
for various user utterances by generating
VoiceXML scripts dynamically. More-
over, we address appropriate user mod-
eling in order to generate cooperative re-
sponses to each user. Specifically, we set
up three dimensions of user models: skill
level to the system, knowledge level on
the target domain and the degree of hasti-
ness. The models are automatically de-
rived by decision tree learning using real
dialogue data collected by the system. Ex-
perimental evaluation shows that the co-
operative responses adapted to individual
users serve as good guidance for novice
users without increasing the dialogue du-
ration for skilled users.
Keywords: spoken dialogue system, user model,
VoiceXML, cooperative responses, dialogue
strategy
1 Introduction
A Spoken dialogue system is one of the promising
applications of the speech recognition and natural
language understanding technologies. A typical task
of spoken dialogue systems is database retrieval.
Some IVR (interactive voice response) systems us-
ing the speech recognition technology are being put
into practical use as its simplest form. According to
the spread of cellular phones, spoken dialogue sys-
tems via telephone enable us to obtain information
from various places without any other special appa-
ratuses.
In order to realize user-friendly interaction, spo-
ken dialogue systems should be able to (1) accept
various user utterances to enable mixed-initiative di-
alogue and (2) generate cooperative responses. Cur-
rently, a lot of IVR systems via telephone operate
by using VoiceXML, which is a script language to
prescribe procedures of spoken dialogues. How-
ever, only the next behaviors corresponding to ev-
ery input are prescribed in the VoiceXML scripts,
so the dialogue procedure is basically designed as
system-initiated one, in which the system asked re-
quired items one by one. In order to realize mixed-
initiative dialogue, the system should be able to ac-
cept various user-initiated utterances. By allowing
to accept various user utterances, the combination
of words included in the utterances accordingly gets
enormous, and then it is practically impossible to
prepare VoiceXML scripts that correspond to the
enormous combinations of input words in advance.
It is also difficult to generate cooperative responses
adaptively in the above framework.
We propose a framework to generate VoiceXML
scripts dynamically in order to realize the mixed-
initiative dialogue, in which the system is needed to
accept various user utterances. This framework real-
izes flexible dialogue management without requiring
preparation for a large number of VoiceXML scripts
in advance. Furthermore, it enables various behav-
iors adaptive to the dialogue situations such as ob-
tained query results.
Another problem to realize user-friendly interac-
tion is how to generate cooperative responses. When
we consider the responses generated from the sys-
tem side, the dialogue strategies, which determine
when to make guidance and what the system should
tell to the user, are the essential factors in spoken di-
alogue systems. There are many studies in respect of
the dialogue strategy such as confirmation manage-
ment using confidence measures of speech recogni-
tion results (Komatani and Kawahara, 2000; Hazen
et al, 2000), dynamic change of dialogue initiative
(Litman and Pan, 2000; Chu-Carroll, 2000; Lamel
et al, 1999), and addition of cooperative contents
to system responses (Sadek, 1999). Nevertheless,
whether a particular response is cooperative or not
depends on individual user?s characteristic.
In order to adapt the system?s behavior to individ-
ual users, it is necessary to model the user?s patterns
(Kass and Finin, 1988). Most of conventional stud-
ies on user models have focused on the knowledge
of users. Others tried to infer and utilize user?s goals
to generate responses adapted to the user (van Beek,
1987; Paris, 1988). Elzer et al (2000) proposed
a method to generate adaptive suggestions accord-
ing to users? preferences. However, these studies
depend on knowledge of the target domain greatly,
and therefore the user models need to be deliberated
manually to be applied to new domains. Moreover,
they assumed that the input is text only, which does
not contain errors.
We propose more comprehensive user models to
generate user-adapted responses in spoken dialogue
systems taking account of information specific to
spoken dialogue. Spoken utterances include vari-
ous information such as the interval between the ut-
terances, the presence of barge-in and so on, which
can be utilized to judge the user?s character. These
features also possess generality in spoken dialogue
systems because they are not dependent on domain-
specific knowledge. As user models in spoken di-
alogue systems, Eckert et al (1997) defined stereo-
types of users such as patient, submissive and ex-
perienced, in order to evaluate spoken dialogue sys-
tems by simulation. We introduce user models not
for defining users? behaviors beforehand, but for de-
tecting users? patterns in real-time interaction.
We define three dimensions in the user models:
?skill level to the system?, ?knowledge level on the
target domain? and ?degree of hastiness?. The user
models are trained by decision tree learning algo-
rithm, using real data collected from the Kyoto city
bus information system. Then, we implement the
user models on the system and evaluate them using
data collected with 20 novice users.
2 Flexible Spoken Dialogue System based
on Dynamic Generation of VoiceXML
Scripts
VoiceXML1 is a script language to prescribe pro-
cedures in spoken dialogues mainly on telephone,
and is becoming to a standard language of IVR sys-
tems. The VoiceXML scripts consist of three parts:
(1) specifications of system?s prompts, (2) specifica-
tions of grammars to accept a user?s utterance, and
(3) description of the next behaviors.
However, most of existing services using the
VoiceXML imposes rigid interaction, in which user
utterances are restricted by system-initiated prompts
and a user is accordingly allowed to specify only re-
quested items one by one. It is more user-friendly
that users can freely convey their requests by natural
language expressions.
We present a framework to realize flexible inter-
action by generating VoiceXML scripts dynamically
(Pargellis et al, 1999; Nyberg et al, 2002). The
framework enables users to express their requests
by natural language even in VoiceXML-based sys-
tems. Furthermore, cooperative responses in the Ky-
oto city bus information system that has been devel-
oped in our laboratory are also presented in this sec-
tion.
2.1 Dynamic Generation of VoiceXML Scripts
In VoiceXML scripts, acceptable keywords and cor-
responding next states must be explicitly specified.
However, since there exists enormous combinations
of keywords in natural language expressions, it is
practically impossible to describe all VoiceXML
scripts that correspond to the combinations. Then,
we introduce the framework in which VoiceXML
1VoiceXML Forum. http://www.voicexml.org/
speech synthesizedspeech
VoiceXML
dialogue
manager
TTS enginespeech
recognizer
grammar
ruleskeywords
response
sentences
Front end
CGI script
user
VWS (Voice Web Server)
VoiceXML
generator
database
Figure 1: Overview of spoken dialogue system
based on dynamic generation of VoiceXML scripts
scripts are generated dynamically to enable the sys-
tem to accept natural language expressions.
Figure 1 shows the overview of the framework.
The front end that operates based on VoiceXML
scripts is separated from the dialogue management
portion, which accepts speech recognition results
and generates corresponding VoiceXML scripts.
The user utterance is recognized based on a gram-
mar rule specified in VoiceXML scripts, and key-
words extracted from a speech recognition result are
passed to a CGI script. The CGI script retrieves cor-
responding information from the database on Web,
and generates a VoiceXML script for the succeed-
ing interaction. If sufficient information is not ob-
tained from a user utterance, a script that prompts to
fill remaining contents is generated, and if the user
utterance contains ambiguity, a script that makes a
disambiguating question is generated.
Consequently, the generation of VoiceXML
scripts enables to accept natural language expres-
sions without preparing a large number of the scripts
corresponding to various inputs beforehand. The
framework also enables to generate cooperative re-
sponses adapted to the situation such as retrieval re-
sults without spoiling portability.
Sys: Please tell me your current bus stop, your destination
or the specific bus route.
User: Shijo-Kawaramachi.
Sys: Do you take a bus from Shijo-Kawaramachi?
User: Yes.
Sys: Where will you get off the bus?
User: Arashiyama.
Sys: Do you go from Shijo-Kawaramachi to Arashiyama?
User: Yes.
Sys: Bus number 11 bound for Arashiyama has departed
Sanjo-Keihanmae, two bus stops away.
Figure 2: Example dialogue of the bus system
2.2 Kyoto City Bus Information System
We have developed the Kyoto city bus information
system, which locates the bus a user wants to take,
and tell him/her how long it will take before arriving.
The system can be accessed via telephone including
cellular phones2. From any places, users can easily
get the information on the bus that changes every
minute. Users are requested to input the bus stop to
get on, the destination, or the bus route number by
speech, and consequently get the bus information.
There are 652 bus stops and 85 bus routes in Kyoto
city. The bus stops can also be specified by the name
of famous places or public facilities nearby. Figure
2 shows a simple example of the dialogue.
2.3 Generation of Cooperative Responses in
Kyoto City Bus Information System
In spoken dialogue systems, it is important to gener-
ate cooperative responses by guessing the user?s in-
tention. The cooperative responses consist of adding
information more than the user has requested, telling
the reason when there is no query result, present-
ing the alternatives, and so on (Sadek, 1999). In
order to generate cooperative responses, it is neces-
sary to change responses according to the situations.
Since we separate language-understanding modules
from individual VoiceXML scripts that are gener-
ated dynamically, our system can generate appropri-
ate responses cooperatively according to query re-
sults. The examples of the cooperative responses
implemented in our Kyoto city bus information sys-
tem are listed below.
2+81-75-326-3116
(1) Suggestive answers
The information related to a query result is offered
when the query result is negative.
(Example)
User: From Hyakumanben to Kyoto Station by bus
number 17.
Sys: Bus number 17 does not come for a while, but
bus number 206 has departed Takano, four bus
stops away.
(2) Corrective answers
The reason is presented when there is no query result
or when a user utterance contains an error.
(Example)
User: From Kyoto Station to Arashiyama.
Sys: There is no bus that directly goes from Kyoto
station to Arashiyama.
(3) Intensional answers
If several query results are obtained, generating in-
tensional answer is more cooperative than listing all
the results.
(Example)
User: From Shijo-Kawaramachi to Shijo-Karasuma.
Sys: All buses to Shijo-Karasuma do not come for a
while.
(4) Completion answers
It is cooperative to generate responses including in-
formation more than the user?s request. However,
whether the information should be presented or not
differs in individual users.
(Example)
User: From Kyoto station by bus number 5.
Sys: Bus number 5 will depart at 17:34 from plat-
form ?A1?.
(There are a lot of platforms in Kyoto station.)
3 Response Generation Using User Models
We propose comprehensive user models to generate
user-adaptive responses taking account of informa-
tion specific to spoken dialogue. Whether a partic-
ular response is regarded as cooperative depends on
individual user?s characteristics. So, we address ap-
propriate user modeling in order to generate cooper-
ative responses to the users.
3.1 Classification of User Models
We define three dimensions as user models listed be-
low.
? Skill level to the system
? Knowledge level on the target domain
? Degree of hastiness
Skill Level to the System
Since spoken dialogue systems are not
widespread yet, there arises a difference in the
skill level of users in operating the systems. It
is desirable that the system changes its behavior
including response generation and initiative man-
agement in accordance with the skill level of the
user. In conventional systems, a system-initiated
guidance has been invoked on the spur of the
moment either when the user says nothing or
when speech recognition is not successful. In our
framework, we address a radical solution for the
unskilled users by modeling the skill level as the
user?s property before such a problem arises.
Knowledge Level on the Target Domain
There also exists a difference in the knowledge
level on the target domain among users. Thus, it is
necessary for the system to change information to
present to users. For example, it is not cooperative
to tell too detailed information to strangers. On the
other hand, for inhabitants, it is useful to omit too
obvious information and to output more detailed in-
formation. Therefore, we introduce a dimension that
represents the knowledge level on the target domain.
Degree of Hastiness
In speech communications, it is more important
to present information promptly and concisely com-
pared with the other communication modes such as
browsing. Especially in the bus system, the concise-
ness is preferred because the bus information is ur-
gent to most users. Therefore, we also take account
of degree of hastiness of the user, and accordingly
change the system?s responses.
3.2 Response Generation Strategy Using User
Models
Next, we describe the response generation strategies
adapted to individual users based on the proposed
user models: skill level, knowledge level and hasti-
ness. Basic design of dialogue management is based
on mixed-initiative dialogue, in which the system
makes follow-up questions and guidance if neces-
sary while allowing a user to utter freely. It is in-
vestigated to add various contents to the system re-
sponses as cooperative responses in (Sadek, 1999).
Such additional information is usually cooperative,
but some people may feel such a response redun-
dant.
Thus, we introduce the user models and con-
trol the generation of additional information. By
introducing the proposed user models, the system
changes generated responses by the following two
aspects: dialogue procedure and contents of re-
sponses.
Dialogue Procedure
The dialogue procedure is changed based on the
skill level and the hastiness. If a user is identified as
having the high skill level, the dialogue management
is carried out in a user-initiated manner; namely, the
system generates only open-ended prompts. On the
other hand, when user?s skill level is detected as low,
the system takes an initiative and prompts necessary
items in order.
When the degree of hastiness is low, the system
makes confirmation on the input contents. Con-
versely, when the hastiness is detected as high, such
a confirmation procedure is omitted; namely, the
system immediately makes a query and outputs the
result without making such a confirmation.
Contents of Responses
Information that should be included in the sys-
tem response can be classified into the following two
items.
1. Dialogue management information
2. Domain-specific information
The dialogue management information specifies
how to carry out the dialogue including the instruc-
tion on user?s expression for yes/no questions like
?Please reply with either yes or no.? and the expla-
nation about the following dialogue procedure like
?Now I will ask in order.? This dialogue manage-
ment information is determined by the user?s skill
 58.8>=
the maximum number of filled slots
dialogue state
initial state otherwise
presense of barge-in
rate of no input
0.07>
30 1 2
average of
recognition score
58.8<
skill level
high
skill level
high
skill level
low
skill level
low
Figure 3: Decision tree for the skill level
level to the system, and is added to system responses
when the skill level is considered as low.
The domain-specific information is generated ac-
cording to the user?s knowledge level on the target
domain. Namely, for users unacquainted with the
local information, the system adds the explanation
about the nearest bus stop, and omits complicated
contents such as a proposal of another route.
The contents described above are also controlled
by the hastiness. For users who are not in hurry, the
system generates the additional contents that corre-
spond to their skill level and knowledge level as co-
operative responses. On the other hand, for hasty
users, the contents are omitted to prevent the dia-
logue from being redundant.
3.3 Classification of User based on Decision
Tree
In order to implement the proposed user models as
classifiers, we adopt a decision tree. It is constructed
by decision tree learning algorithm C5.0 (Quinlan,
1993) with data collected by our dialogue system.
Figure 3 shows an example of the derived decision
tree for the skill level.
We use the features listed in Figure 4. They in-
clude not only semantic information contained in the
utterances but also information specific to spoken di-
alogue systems such as the silence duration prior to
the utterance, the presence of barge-in and so on.
Except for the last category of Figure 4 including
?attribute of specified bus stops?, most of the fea-
tures are domain-independent.
The classification of each dimension is done for
every user utterance except for knowledge level. The
model of a user can change during a dialogue. Fea-
tures extracted from utterances are accumulated as
history information during the session.
? features obtained from a single utterance
? dialogue state (defined by already filled slots)
? presence of barge-in
? lapsed time of the current utterance
? recognition result (something recognized / un-
certain / no input)
? score of speech recognizer
? the number of filled slots by the current utter-
ance
? features obtained from the session
? the number of utterances
? dialogue state of the previous utterance
? lapsed time from the beginning of the session
? the number of the repetition of a same question
? the average number of the repetition of a same
question
? ratio of the time of user utterances in whole
elapsed time
? ratio of the occurrence of barge-in out of the
whole number of utterances
? recognition result of the previous utterance
? ratio of something recognized
? ratio of getting uncertain results
? ratio of no input
? the number of barge-in
? the number of something recognized
? the number of getting uncertain results
? the number of no input
? average of recognition scores
? the maximum number of slots filled by a single
utterance
? features obtained from the session (used only in clas-
sification of knowledge level)
? attribute of specified bus stops
? the way of specifying bus stops (whether a bus
stop is specified by its correct name or not)
? ratio of each attribute of specified bus stops
? ratio of each way of specifying bus stops
Figure 4: Features to classify the user models
Figure 5 shows an example of the system behav-
ior with the proposed user models. The skill level is
classified as being low by the decision tree, because
the first user?s utterance includes only one content
word. Then, dialogue procedure is changed to the
system-initiated one. Similarly, the hastiness is clas-
sified as being low by the decision tree, and the sys-
tem includes the explanation on the dialogue pro-
cedure and the instruction on the expression in the
responses. They are omitted if the hastiness is iden-
tified as high.
3.4 Decision Tree Learning for User Models
We train and evaluate the decision tree for the user
models using dialogue data collected by our sys-
case: [skill level: low][knowledge level: low]
[hastiness: low]
Sys: Please tell me your current bus stop, your destination
or the specific bus route number.
User: Maruyama Park.
Sys: Let me confirm one by one.
(? The explanation is added by [skill level: low].)
Will you get on at Maruyama Park?
(? The content is confirmed by [hastiness: low].)
Please reply with yes or no.
(? The instruction is added by [skill level: low].)
User: Yes.
Sys: Where will you get off the bus? Please specify by a
bus stop name or a well-known landmark.
(? The explanation is added by [skill level: low].)
User: Tofukuji temple.
Sys: Will you get off the bus at Tofukuji temple?
Please reply with yes or no.
User: Yes.
Sys: The following buses are located.
(? The explanation is added by [skill level: low].)
The nearest bus stop to Maruyama Park is Gion.
(? The domain information is added by [knowl-
edge level: low].)
Bus number 202 bound for Nishioji-Kujo has de-
parted Higashiyama-Sanjo, which is two stops away.
...
Figure 5: An example dialogue with the proposed
user models
low indeterminable high total
skill level 743 253 496 1492
knowledge level 275 808 409 1492
hastiness 421 932 139 1492
Table 1: Number of manually annotated labels for
decision tree learning
tem. The data was collected from December 10th
2001 to May 10th 2002. The number of the ses-
sions (telephone calls) is 215, and the total number
of utterances included in the sessions is 1492. We
annotated the subjective labels of the user models
by hand. The annotator judges the user models for
every utterance based on the recorded speech data
and logs. The labels were given to the three dimen-
sions described in section 3.1 among ?high?, ?inde-
terminable? or ?low?. It is possible that the annotated
model of a user changes during a dialogue, espe-
cially from ?indeterminable? to ?low? or ?high?. The
number of the labeled utterances is shown in Table
1.
condition #1 #2 #3
skill level 80.8% 75.3% 85.6%
knowledge level 73.9% 63.7% 78.2%
hastiness 74.9% 73.7% 78.6%
Table 2: Classification accuracy of the proposed user
models
Using the labeled data, we trained the decision
tree and evaluated the classification accuracy of the
proposed user models. All the experiments were car-
ried out by the method of 10-fold cross validation.
The process, in which one tenth of all data is used as
the test data, and the remainder is used as the train-
ing data, is repeated ten times, and the average of
the classification accuracy is computed. The result
is shown in Table 2. The conditions #1, #2 and #3 in
Table 2 are described as follows.
#1: The 10-fold cross validation is carried out per
utterance.
#2: The 10-fold cross validation is carried out per
session (call).
#3: We calculate the accuracy under more realis-
tic condition. The accuracy is calculated not
in three classes (high / indeterminable / low)
but in two classes that actually affect the dia-
logue strategies. For example, the accuracy for
the skill level is calculated for the two classes:
low and the others. As to the classification of
knowledge level, the accuracy is calculated for
dialogue sessions, because the features such as
the attribute of a specified bus stop are not ob-
tained in every utterance. Moreover, in order
to smooth unbalanced distribution of the train-
ing data, a cost corresponding to the reciprocal
ratio of the number of samples in each class is
introduced. By the cost, the chance rate of two
classes becomes 50%.
The difference between condition #1 and #2 is
that the training was carried out in a speaker-closed
or speaker-open manner. The former shows better
performance.
The result in condition #3 shows useful accuracy
in the skill level. The following features play im-
portant part in the decision tree for the skill level:
user
profiles
database
on Web
CGI
the system except for
proposed user models
user
VWS
(Voice Web Server)
VoiceXML
generator
dialogue
manager
user model
identifier
VoiceXMLrecognition results(keywords)
recognition results
(including features other
than language info.)
Figure 6: Overview of the Kyoto city bus informa-
tion system with user models
the number of filled slots by the current utterance,
presence of barge-in and ratio of no input. For the
knowledge level, recognition result (something rec-
ognized / uncertain / no input), ratio of no input and
the way to specify bus stops (whether a bus stop is
specified by its exact name or not) are effective. The
hastiness is classified mainly by the three features:
presence of barge-in, ratio of no input and lapsed
time of the current utterance.
3.5 System Overview
Figure 6 shows an overview of the Kyoto city bus in-
formation system with the user models. The system
operates by generating VoiceXML scripts dynami-
cally as described in section 2.1. The real-time bus
information database is provided on the Web, which
can be accessed via Internet. Then, we explain the
modules in the following.
VWS (Voice Web Server)
The Voice Web Server drives the speech recog-
nition engine and the TTS (Text-To-Speech)
module accordingly to the specifications by the
generated VoiceXML script.
Speech Recognizer
The speech recognizer decodes user utterances
based on specified grammar rules and vocabu-
lary, which are defined by VoiceXML at each
dialogue state.
Dialogue Manager
The dialogue manager generates response sen-
tences based on recognition results (bus stop
names or a route number) received from the
VWS. If sufficient information to locate a bus
is obtained, it retrieves the corresponding bus
information on the Web.
VoiceXML Generator
This module dynamically generates VoiceXML
scripts that contain response sentences and
specifications of speech recognition grammars,
which are given by the dialogue manager.
User model identifier
This module classifies user?s characters based
on the user models using features specific to
spoken dialogue as well as semantic attributes.
The obtained user profiles are sent to the dia-
logue manager, and are utilized in the dialogue
management and response generation.
4 Experimental Evaluation of the System
with User Models
We evaluated the system with the proposed user
models using 20 novice subjects who had not used
the system. The experiment was performed in the
laboratory under adequate control. For the speech
input, the headset microphone was used.
4.1 Experiment Procedure
First, we explained the outline of the system to sub-
jects and gave a document in which experiment con-
ditions and scenarios were described. We prepared
two sets of eight scenarios. Subjects were requested
to acquire the bus information according to the sce-
narios using the system with/without the user mod-
els. In the scenarios, neither the concrete names of
bus stops nor the bus number were given. For exam-
ple, one of the scenarios was as follows: ?You are in
Kyoto for sightseeing. After visiting the Ginkakuji
temple, you go to Maruyama Park. Supposing such
a situation, please get information on the bus.? We
also set the constraint in order to vary the subjects?
hastiness such as ?Please hurry as much as possible
in order to save the charge of your cellular phone.?
The subjects were also told to look over question-
naire items before the experiment, and filled in them
duration (sec.) # turn
group 1 with UM 51.9 4.03
(with UM? w/o UM) w/o UM 47.1 4.18
group 2 w/o UM 85.4 8.23
(w/o UM? with UM) with UM 46.7 4.08
UM: User Model
Table 3: Duration and the number of turns in dia-
logue
after using each system. This aims to reduce the sub-
ject?s cognitive load and possible confusion due to
switching the systems (Over, 1999). The question-
naire consisted of eight items, for example, ?When
the dialogue did not go well, did the system guide in-
telligibly?? We set seven steps for evaluation about
each item, and the subject selected one of them.
Furthermore, subjects were asked to write down
the obtained information: the name of the bus stop
to get on, the bus number and how much time it
takes before the bus arrives. With this procedure,
we planned to make the experiment condition close
to the realistic one.
The subjects were divided into two groups; a half
(group 1) used the system in the order of ?with
user models ? without user models?, the other half
(group 2) used in the reverse order.
The dialogue management in the system without
user models is also based on the mixed-initiative
dialogue. The system generates follow-up ques-
tions and guidance if necessary, but behaves in a
fixed manner. Namely, additional cooperative con-
tents corresponding to skill level described in section
3.2 are not generated and the dialogue procedure is
changed only after recognition errors occur. The
system without user models behaves equivalently to
the initial state of the user models: the hastiness is
low, the knowledge level is low and the skill level is
high.
4.2 Results
All of the subjects successfully completed the given
task, although they had been allowed to give up if the
system did not work well. Namely, the task success
rate is 100%.
Average dialogue duration and the number of
turns in respective cases are shown in Table 3.
Though the users had not experienced the system at
group 1 with UM 0.72
(with UM ? w/o UM) w/o UM 0.70
group 2 w/o UM 0.41
(w/o UM ? with UM) with UM 0.63
Table 4: Ratio of utterances for which the skill level
was judged as high
all, they got accustomed to the system very rapidly.
Therefore, as shown in Table 3, the duration and
the number of turns were decreased obviously in the
latter half of the experiment in both groups. How-
ever, in the initial half of the experiment, the group
1 completed with significantly shorter dialogue than
group 2. This means that the incorporation of the
user models is effective for novice users. Table 4
shows a ratio of utterances for which the skill level
was identified as high. The ratio is calculated by di-
viding the number of utterances that were judged as
high skill level by the number of all utterances. The
ratio is much larger for group 1 who initially used
the system with user models. This fact means that
the novice users got accustomed to the system more
rapidly with the user models, because they were in-
structed on the usage by cooperative responses gen-
erated when the skill level is low. The results demon-
strate that cooperative responses generated accord-
ing to the proposed user models can serve as good
guidance for novice users.
In the latter half of the experiment, the dialogue
duration and the number of turns were almost same
between the two groups. This result shows that the
proposed models prevent the dialogue from becom-
ing redundant for skilled users, although generating
cooperative responses for all users made the dia-
logue verbose in general. It suggests that the pro-
posed user models appropriately control the genera-
tion of cooperative responses by detecting characters
of individual users.
5 Conclusions
We have presented a framework to realize flexible
interaction by dynamically generating VoiceXML
scripts. This framework realizes mixed-initiative di-
alogues and the generation of cooperative responses
in VoiceXML-based systems.
We have also proposed and evaluated user mod-
els for generating cooperative responses adaptively
to individual users. The proposed user models con-
sist of the three dimensions: skill level to the sys-
tem, knowledge level on the target domain and the
degree of hastiness. The user models are identified
by decision tree using features specific to spoken di-
alogue systems as well as semantic attributes. They
are automatically derived by decision tree learning,
and all features used for skill level and hastiness are
independent of domain-specific knowledge. So, it is
expected that the derived user models can be gener-
ally used in other domains.
The experimental evaluation with 20 novice users
shows that the skill level of novice users was im-
proved more rapidly by incorporating user mod-
els, and accordingly the dialogue duration becomes
shorter more immediately. The result is achieved
by the generated cooperative responses based on the
proposed user models. The proposed user models
also suppress the redundancy by changing the dia-
logue procedure and selecting contents of responses.
Thus, the framework generating VoiceXML
scripts dynamically and the proposed user models
realize a user-adaptive dialogue strategies, in which
the generated cooperative responses serve as good
guidance for novice users without increasing the di-
alogue duration for skilled users.
References
Jennifer Chu-Carroll. 2000. MIMIC: An adaptive
mixed initiative spoken dialogue system for informa-
tion queries. In Proc. of the 6th Conf. on applied Nat-
ural Language Processing, pages 97?104.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system eval-
uation. In Proc. IEEE Workshop on Automatic Speech
Recognition and Understanding, pages 80?87.
Stephanie Elzer, Jennifer Chu-Carroll, and Sandra Car-
berry. 2000. Recognizing and utilizing user prefer-
ences in collaborative consultation dialogues. In Proc.
of the 4th Int?l Conf. on User Modeling, pages 19?24.
Timothy J. Hazen, Theresa Burianek, Joseph Polifroni,
and Stephanie Seneff. 2000. Integrating recognition
confidence scoring with language understanding and
dialogue modeling. In Proc. Int?l Conf. Spoken Lan-
guage Processing (ICSLP).
Robert Kass and Tim Finin. 1988. Modeling the user in
natural language systems. Computational Linguistics,
14(3):5?22.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management using
concept-level confidence measures of speech recog-
nizer output. In Proc. Int?l Conf. Computational Lin-
guistics (COLING), pages 467?473.
Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, and Samir
Bennacef. 1999. The LIMSI ARISE system for
train travel information. In IEEE Int?l Conf. Acoust.,
Speech & Signal Processing (ICASSP).
Diane J. Litman and Shimei Pan. 2000. Predicting and
adapting to poor speech recognition in a spoken dia-
logue system. In Proc. of the 17th National Confer-
ence on Artificial Intelligence (AAAI2000).
Eric Nyberg, Teruko Mitamura, Paul Placeway, Michael
Duggan, and Nobuo Hataoka. 2002. Dialogxml:
Extending voicexml for dynamic dialog manage-
ment. In Proc. of Human Language Technology 2002
(HLT2002), pages 286?291.
Paul Over. 1999. Trec-7 interactive track report. In Proc.
of the 7th Text REtrieval Conference (TREC7).
Andrew Pargellis, Jeff Kuo, and Chin-Hui Lee. 1999.
Automatic dialogue generator creates user defined ap-
plications. In Proc. European Conf. Speech Commun.
& Tech. (EUROSPEECH).
Cecile L. Paris. 1988. Tailoring object descriptions to
a user?s level of expertise. Computational Linguistics,
14(3):64?78.
J. Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo, CA.
http://www.rulequest.com/see5-info.html.
David Sadek. 1999. Design considerations on dia-
logue systems: From theory to technology -the case
of artimis-. In Proc. ESCA workshop on Interactive
Dialogue in Multi-Modal Systems.
Peter van Beek. 1987. A model for generating better
explanations. In Proc. of the 25th Annual Meeting of
the Association for Computational Linguistics (ACL-
87), pages 215?220.
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 9?17,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multi-Domain Spoken Dialogue System
with Extensibility and Robustness against Speech Recognition Errors
Kazunori Komatani Naoyuki Kanda Mikio Nakano?
Kazuhiro Nakadai? Hiroshi Tsujino? Tetsuya Ogata Hiroshi G. Okuno
Kyoto University, Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,ogata,okuno}@i.kyoto-u.ac.jp
? Honda Research Institute Japan Co., Ltd., 8-1 Honcho, Wako, Saitama 351-0188, Japan
{nakano,nakadai,tsujino}@jp.honda-ri.com
Abstract
We developed a multi-domain spoken dia-
logue system that can handle user requests
across multiple domains. Such systems
need to satisfy two requirements: extensi-
bility and robustness against speech recog-
nition errors. Extensibility is required to
allow for the modification and addition
of domains independent of other domains.
Robustness against speech recognition er-
rors is required because such errors are
inevitable in speech recognition. How-
ever, the systems should still behave ap-
propriately, even when their inputs are er-
roneous. Our system was constructed on
an extensible architecture and is equipped
with a robust and extensible domain selec-
tion method. Domain selection was based
on three choices: (I) the previous domain,
(II) the domain in which the speech recog-
nition result can be accepted with the high-
est recognition score, and (III) other do-
mains. With the third choice we newly
introduced, our system can prevent dia-
logues from continuously being stuck in
an erroneous domain. Our experimental
results, obtained with 10 subjects, showed
that our method reduced the domain selec-
tion errors by 18.3%, compared to a con-
ventional method.
1 Introduction
Many spoken dialogue systems have been devel-
oped for various domains, including: flight reser-
vations (Levin et al, 2000; Potamianos and Kuo,
2000; San-Segundo et al, 2000), train travel in-
formation (Lamel et al, 1999), and bus informa-
tion (Komatani et al, 2005b; Raux and Eskenazi,
2004). Since these systems only handle a sin-
gle domain, users must be aware of the limita-
tions of these domains, which were defined by
the system developer. To handle various domains
through a single interface, we have developed a
multi-domain spoken dialogue system, which is
composed of several single-domain systems. The
system can handle complicated tasks that contain
requests across several domains.
Multi-domain spoken dialogue systems need to
satisfy the following two requirements: (1) exten-
sibility and (2) robustness against speech recog-
nition errors. Many such systems have been de-
veloped on the basis of a master-slave architec-
ture, which is composed of a single master module
and several domain experts handling each domain.
This architecture has the advantage that each do-
main expert can be independently developed, by
modifying existing experts or adding new experts
into the system. In this architecture, the master
module needs to select a domain expert to which
response generation and dialogue management for
the user?s utterance are committed. Hereafter, we
will refer to this selecting process domain selec-
tion.
The second requirement is robustness against
speech recognition errors, which are inevitable in
systems that use speech recognition. Therefore,
these systems must robustly select domains even
when the input may be incorrect due to speech
recognition errors.
We present an architecture for a multi-domain
spoken dialogue system that incorporates a new
domain selection method that is both extensi-
ble and robust against speech recognition errors.
Since our system is based on extensible architec-
ture similar to that developed by O?Neill (O?Neill
et al, 2004), we can add and modify the domain
9
Speech recognition
Domain selection
Utterance generation
Speech synthesis
User
utterance
System
response
User System
Central module Expert for domain A
method
Language understanding
Dialogue state update
Dialogue management
Dialogue statesvariable
Expert for domain B
method
variable
Expert for domain B
method
variable
Expert for domain B
method
Language understanding
Dialogue state update
Dialogue management
Dialogue statesvariable
Figure 1: Distributed-type architecture for multi-domain spoken dialogue systems
experts easily. In order to maintain robustness,
domain selection takes into consideration vari-
ous features concerning context and situations of
the dialogues. We also designed a new selection
framework that satisfies the extensibility issue by
abstracting the transitions between the current and
next domains. Specifically, our system selects the
next domain based on: (I) the previous domain,
(II) the domain in which the speech recognition
result can be accepted with the highest recognition
score, and (III) other domains. Conventional meth-
ods cannot select the correct domain when neither
the previous domain nor the speech recognition re-
sults for a current utterance are correct. To over-
come this drawback, we defined another choice as
(III) that enables the system to detect an erroneous
situation and thus prevent the dialogue from con-
tinuing to be incorrect. We modeled this frame-
work as a classification problem using machine
learning, and showed it is effective by perform-
ing an experimental evaluation of 2,205 utterances
collected from 10 subjects.
2 Architecture used for Multi-Domain
Spoken Dialogue Systems
In multi-domain spoken dialogue systems, the sys-
tem design is more complicated than in single do-
main systems. When the designed systems are
closely related to each other, a modification in a
certain domain may affect the whole system. This
type of a design makes it difficult to modify ex-
isting domains or to add new domains. Therefore,
a distributed-type architecture has been previously
proposed (Lin et al, 2001), which enables system
developers to design each domain independently.
In this architecture, the system is composed of
two kinds of components: a part that can be de-
signed independently of all other domains, and a
part in which relations among domains should be
considered. By minimizing the latter component,
a system developer can design each domain semi-
independently, which enables domains to be eas-
ily added or modified. Many existing systems are
based on this architecture (Lin et al, 2001; O?Neill
et al, 2004; Pakucs, 2003; Nakano et al, 2005).
Thus, we adopted the distributed-type architec-
ture (Nakano et al, 2005). Our system is roughly
composed of two parts, as shown in Figure 1: sev-
eral experts that control dialogues in each domain,
and a central module that controls each expert.
When a user speaks to the system, the central mod-
ule drives a speech recognizer, and then passes
the result to each domain expert. Each expert,
which controls its own domains, executes a lan-
guage understanding module, updates its dialogue
states based on the speech recognition result, and
returns the information required for domain selec-
tion1. Based on the information obtained from
the experts, the central module selects an appro-
priate domain for giving the response. An expert
then takes charge of the selected domain and deter-
mines the next dialogue act based on its dialogue
state. The central module generates a response
based on the dialogue act obtained from the expert,
and outputs the synthesized speech to the user.
Communications between the central module and
each expert are realized using method-calls in the
central module. Each expert is required to have
several methods, such as utterance understanding
or response selection, to be considered an expert
1Dialogue states in a domain that are not selected during
domain selection are returned to their previous states.
10
in this architecture.
As was previously described, the central mod-
ule is not concerned with processing the speech
recognition results; instead, the central module
leaves this task to each expert. Therefore, it is
important that the central module selects an ex-
pert that is committed to the process of the speech
recognition result. Furthermore, information used
during domain selection should also be domain
independent, because this allows easier domain
modification and addition, which is, after all, the
main advantage of distributed-type architecture.
3 Extensible and Robust Domain
Selection
Domain selection in the central module should
also be performedwithin an extensible framework,
and also should be robust against speech recogni-
tion errors.
In many conventional methods, domain selec-
tion is based on estimating the most likely do-
mains based on the speech recognition results.
Since these methods are heavily dependent on
the performance of the speech recognizers, they
are not robust because the systems will fail when
a speech recognizer fails. To behave robustly
against speech recognition errors, the success of
speech recognition and of domain selection should
be treated separately. Furthermore, in some con-
ventional methods, accurate language models are
required to construct the domain selection parts
before new domains are added to a multi-domain
system. This means that they are not extensible.
When selecting a domain, other studies have
used the information on the domain in which a pre-
vious response was made. Lin et al (2001) gave
preference to the domain selected in the previous
turn by adding a certain score as an award when
comparing the N-best candidates of the speech
recognition for each domain. Lane and Kawa-
hara (2005) also assigned a similar preference in
the classification with Support Vector Machine
(SVM). A system described in (O?Neill et al,
2004) does not change its domain until its sub-task
is completed, which is a constraint similar to keep-
ing dialogue in one domain. Since these methods
assume that the previous domain is most likely the
correct domain, it is expected that these methods
keep a system in the domain despite errors due
to speech recognition problems. Thus, should do-
main selection be erroneous, the damage due to the
Same domain as
previous response
Domain having 
the highest score in
speech recognizer
User utterancePrevious turn Current turn
(I)
(II)
(III)
?????
????
?
???
Other domains
except (I), (II)
Selected
domain
Figure 2: Overview of domain selection
error is compounded, as the system assumes that
the previous domain is always correct. Therefore,
we solve this problem by considering features that
represent the confidence of the previously selected
domain.
We define domain selection as being based on
the following 3-class categorization: (I) the previ-
ous domain, (II) the domain in which the speech
recognition results can be accepted with the high-
est recognition score, which is different from the
previous domain, and (III) other domains. Figure
2 depicts the three choices. This framework in-
cludes the conventional methods as choices (I) and
(II). Furthermore, it considers the possibility that
the current interpretations may be wrong, which
is represented as choice (III). This framework also
has extensibility for adding new domains, since it
treats domain selection not by detecting each do-
main directly, but by defining only a relative re-
lationship between the previous and current do-
mains.
Since our framework separates speech recogni-
tion results and domain selection, it can keep di-
alogues in the correct domain even when speech
recognition results are wrong. This situation is
represented as choice (I). An example is shown
in Figure 3. Here, the user?s first utterance (U1)
is about the restaurant domain. Although the sec-
ond utterance (U2) is also about the restaurant do-
main, an incorrect interpretation for the restaurant
domain is obtained because the utterance contains
an out-of-vocabulary word and is incorrectly rec-
ognized. Although a response for utterance U2
should ideally be in the restaurant domain, the sys-
tem control shifts to the temple sightseeing infor-
mation domain, in which an interpretation is ob-
tained based on the speech recognition result. This
11
? ?
U1: Tell me bars in Kawaramachi area.
(domain: restaurant)
S1: Searching for bars in Kawaramachi area.
30 items found.
U2: I want Tamanohikari (name of liquor).
(domain: restaurant)
Tamanohikari is out-of-vocabulary word, and
misrecognized as Tamba-bashi (name of place).
(domain: temple)
S2 (bad): Searching spots near Tamba-bashi. 10 items
found. (domain: temple)
S2 (good): I do not understand what you said. Do you
have any other preferences? (domain: restaurant)
? ?
Figure 3: Example in which choice (I) is appropri-
ate in spite of speech recognition error
is shown as utterance S2 (bad). In such cases, our
framework is capable of behaving appropriately.
This is shown as S2 (good), which is made by
selecting choice (I). Accepting erroneous recogni-
tion results is more harmful than rejecting correct
ones for the following reasons: 1) a user needs to
solve the misunderstanding as a result of the false
acceptance, and 2) an erroneous utterance affects
the interpretation of the utterances following it.
Furthermore, we define choice (III), which de-
tects the cases where normal dialogue manage-
ment is not suitable, in which case the central
module selects an expert based on either the pre-
vious domain or the domain based on the speech
recognition results. The situation corresponds to
a succession of recognition errors. However, this
problem is more difficult to solve than merely de-
tecting a simple succession of the errors because
the system needs to distinguish between speech
recognition errors and domain selection errors in
order to generate appropriate next utterances. Fig-
ure 4 shows an example of such a situation. Here,
the user?s utterances U1 and U2 are about the tem-
ple domain, but a speech recognition error oc-
curred in U2, and system control shifts to the hotel
domain. The user again says (U3), but this results
in the same recognition error. In this case, a do-
main that should ideally be selected is neither the
domain in the previous turn nor the domain deter-
mined based on the speech recognition results. If
this situation can be detected, the system should be
able to generate an appropriate response, like S3
(good), and prevent inappropriate responses based
? ?
U1: Tell me the address of Horin-ji (temple name).
(domain: temple)
S1: The address of Horin-ji is ...
U2: Then, what is the fee for Horin-ji?
(domain: temple)
misrecognized as ?the fee of Holiday Inn?.
(domain: hotel)
S2: The fee of Holiday Inn is ...
U3: The fee of Horin-ji. (domain: temple)
again misrecognized as ?the fee of Holiday Inn?.
(domain: hotel)
S3 (bad): The fee of Holiday Inn is ...
S3 (good): Are you asking about hotel information?
U4: No.
S4: Would you like to return to the temple information
service?
? ?
Figure 4: Example in which choice (III) should be
selected
on an incorrect domain determination. It is pos-
sible for the system to restart from two utterances
before (U1), after asking a confirmatory question
(S4) about whether to return to it or not. After that,
repetition of similar errors can also be avoided if
the system prohibits transition to the hotel domain.
4 Domain Selection using Dialogue
History
We constructed a classifier that selects the appro-
priate domains using various features, including
dialogue histories. The selected domain candi-
dates are based on: (I) the previous domain, (II)
the domain in which the speech recognition results
can be accepted with the highest recognition score,
or (III) other domains. Here, we describe the fea-
tures present in our domain selection method.
In order to not spoil the system?s extensibility,
an advantage of the distributed-type architecture,
the features used in the domain selection should
not depend on the specific domains. We categorize
the features used into three categories listed below:
? Features representing the confidence with
which the previous domain can be considered
correct (Table 1)
? Features about a user?s speech recognition re-
sult (Table 2)
12
Table 1: Features representing confidence in pre-
vious domain
P1: number of affirmatives after entering the domain
P2: number of negations after entering the domain
P3: whether tasks have been completed in the domain
(whether to enter ?requesting detailed information?
in database search task)
P4: whether the domain appeared before
P5: number of changed slots after entering the domain
P6: number of turns after entering the domain
P7: ratio of changed slots (= P5/P6)
P8: ratio of user?s negative answers (= P2/(P1 + P2))
P9: ratio of user?s negative answers in the domain (=
P2/P6)
P10: states in tasks
Table 2: Features of speech recognition results
R1: best posteriori probability of the N-best candidates
interpreted in the previous domain
R2: best posteriori probability for the speech recogni-
tion result interpreted in the domain, that is the do-
main with the highest score
R3: average of word?s confidence scores for the best
candidate of speech recognition results in the do-
main, that is, the domain with the highest score
R4: difference of acoustic scores between candidates
selected as (I) and (II)
R5: ratio of averages of words? confidence scores be-
tween candidates selected as (I) and (II)
? Features representing the situation after do-
main selection (Table 3)
We can take into account the possibility that a
current estimated domain might be erroneous, by
using features representing the confidence in the
previous domain. Each feature from P1 to P9 is
defined to represent the determination of whether
an estimated domain is reliable or not. Specifi-
cally, if there are many affirmative responses from
a user or many changes of slot values during in-
teractions in the domain, we regard the current do-
main as reliable. Conversely, the domain is not
reliable if there are many negative answers from a
user after entering the domain.
We also adopted the feature P10 to represent
the state of the task, because the likelihood that
a domain is changed depends on the state of the
task. We classified the tasks that we treat into two
categories using the following classifications first
made by Araki et al (1999). For a task catego-
rized as a ?slot-filling type?, we defined the di-
alogue states as one of the following two types:
?not completed?, if not all of the requisite slots
have been filled; and ?completed?, if all of the
Table 3: Features representing situations after do-
main selection
C1: dialogue state after the domain selection after se-
lecting previous domain
C2: whether the interpretation of the user?s utterance is
negative in previous domain
C3: number of changed slots after selecting previous
domain
C4: dialogue state after selecting the domain with the
highest speech recognition score
C5: whether the interpretation of the user?s utterance
is negative in the domain with the highest speech
recognition score
C6: number of changed slots after selecting the domain
with the highest speech recognition score
C7: number of common slots (name of place, here)
changed after selecting the domain with the high-
est speech recognition score
C8: whether the domain with the highest speech recog-
nition score has appeared before
requisite slots have been filled. For a task catego-
rized as a ?database search type?, we defined the
dialogue states as one of the following two types:
?specifying query conditions? and ?requesting de-
tailed information?, which were defined in (Ko-
matani et al, 2005a).
The features which represent the user?s speech
recognition result are listed in Table 2 and corre-
spond to those used in conventional studies. R1
considers the N-best candidates of speech recogni-
tion results that can be interpreted in the previous
domain. R2 and R3 represent information about a
domain with the highest speech recognition score.
R4 and R5 represent the comparisons between the
above-mentioned two groups.
The features that characterize the situations af-
ter domain selection correspond to the information
each expert returns to the central module after un-
derstanding the speech recognition results. These
are listed in Table 3. Features listed from C1 to
C3 represent a situation in which the previous do-
main (choice (I)) is selected. Those listed from
C4 to C8 represent a situation in which a domain
with the highest recognition score (choice (II)) is
selected.
Note that these features listed here have sur-
vived after feature selection. A feature survives
if the performance in the domain classification is
degradedwhen it is removed from a feature set one
by one. We had prepared 32 features for the initial
set.
13
Table 4: Specifications of each domain
Name of Class of # of vocab. # of
domain task in ASR slots
restaurant database search 1,562 10
hotel database search 741 9
temple database search 1,573 4
weather slot filling 87 3
bus slot filling 1,621 3
total - 7,373 -
5 Experimental Evaluation
5.1 Implementation
We implemented a Japanese multi-domain spoken
dialogue system with five domain experts: restau-
rant, hotel, temple, weather, and bus. Specifica-
tions of each expert are listed in Table 4. If there
is any overlapping slot between the vocabularies
of the domains, our architecture can treat it as a
common slot, whose value is shared among the
domains when interacting with the user. In our
system, place names are treated as a common slot.
We adopted Julian as the grammar-based
speech recognizer (Kawahara et al, 2004). The
grammar rules for the speech recognizer can be
automatically generated from those used in the
language understanding modules in each domain.
As a phonetic model, we adopted a 3000-states
PTM triphone model (Kawahara et al, 2004).
5.2 Collecting Dialogue Data
We collected dialogue data using a baseline sys-
tem from 10 subjects. First, the subjects used the
system by following a sample scenario, to get ac-
customed to the timing to speak. They, then, used
the system by following three scenarios, where at
least three domains were mentioned, but neither
an actual temple name nor domain was explicitly
mentioned. One of the scenarios is shown in Fig-
ure 5. Domain selection in the baseline system
was performed on the basis of the baseline method
that will be mentioned in Section 5.4, in which ?
was set to 40 after preliminary experiments.
In the experiments, we obtained 2,205 utter-
ances (221 per subject, 74 per dialogue). The
accuracy of the speech recognition was 63.3%,
which was rather low. This was because the sub-
jects tended to repeat similar utterances even after
misrecognition occurred due to out-of-grammar or
out-of-vocabulary utterances. Another reason was
that the dialogues for subjects with worse speech
recognition results got longer, which resulted in an
increase in the total number of misrecognition.
? ?
Tomorrow or the day after, you are planning a sightsee-
ing tour of Kyoto. Please find a shrine you want to visit
in the Arashiyama area, and determine, after consider-
ing the weather, on which day you will visit the shrine.
Please, ask for a temperature on the day of travel. Also
find out how to go to the shrine, whether you can take a
bus from the Kyoto station to there, when the shrine is
closing, and what the entrance fee is.
? ?
Figure 5: Example of scenarios
5.3 Construction of the Domain Classifier
We used the data containing 2,205 utterances col-
lected using the baseline system, to construct a do-
main classifier. We used C5.0 (Quinlan, 1993) as
a classifier. The features used were described in
Section 4. Reference labels were given by hand
for each utterance based on the domains the sys-
tem had selected and transcriptions of the user?s
utterances, as follows2.
Label (I): When the correct domain for a user?s
utterance is the same as the domain in which
the previous system?s response was made.
Label (II): Except for case (I), when the correct
domain for a user?s utterance is the domain
in which a speech recognition result in the N-
best candidates with the highest score can be
interpreted.
Label (III): Domains other than (I) and (II).
5.4 Evaluation of Domain Selection
We compared the performance of our domain se-
lection with that of the baseline method described
below.
Baseline method: A domain having an interpre-
tation with the highest score in the N-best
candidates of the speech recognition was se-
lected, after adding ? for the acoustic likeli-
hood of the speech recognizer if the domain
was the same as the previous one. We calcu-
lated the accuracies of domain selections for
various ?.
2Although only one of the authors assigned the labels,
they could be easily assigned without ambiguity, since the
labels were automatically defined as previously described.
Thus, the annotator only needs to judge whether a user?s re-
quest was about the same domain as the previous system?s re-
sponse or whether it was about a domain in the speech recog-
nition result.
14
0100
200
300
400
500
600
700
800
900
0 10 20 30 40 50 60
?
#
 
o
f
 
e
r
r
o
r
s
 
i
n
 
d
o
m
a
i
n
 
s
e
l
e
c
t
i
o
n
 total
 domain in previous utt.
 domain with highest score
 other domain
Figure 6: Accuracy of domain selection in the
baseline method
Our method: A domain was selected based on
our method. The performance was calculated
with a 10-fold cross validation, that is, one
tenth of the 2,205 utterances were used as test
data, and the remainder was used as training
data. The process was repeated 10 times, and
the average of the accuracies was computed.
Accuracies for domain selection were calculated
per utterance. When there were several domains
that had the same score after domain selection, one
domain was randomly selected among them as an
output.
Figure 6 shows the number of errors for do-
main selection in the baseline method, categorized
by their reference labels as ? changed. As ? in-
creases, so does the system desire to keep the pre-
vious domain. A condition where ? = 0 cor-
responds to a method in which domains are se-
lected based only on the speech recognition re-
sults, which implies that there are no constraints
on keeping the current domain. As we can see
in Figure 6, the number of errors whose refer-
ence labels are ?a domain in the previous response
(choice (I))? decreases as ? gets larger. This is be-
cause incorrect domain transitions due to speech
recognition errors were suppressed by the con-
straint to keep the domains. Conversely, we can
see an increase in errors whose labels are ?a do-
main with the highest speech recognition score
(choice (II))?. This is because there is too much
incentive for keeping the previous domain. The
smallest number of errors was 634 when ? = 35,
and the error rate of domain selection was 28.8%
(= 634/2205). There were 371 errors whose refer-
ence labels were neither ?a domain in the previous
response? nor ?a domain with the highest speech
recognition score?, which cannot be detected even
when ? is changed based on conventional frame-
works.
We also calculated the classification accuracy of
our method. Table 5 shows the results as a con-
fusion matrix. The left hand figure denotes the
number of outputs in the baseline method, while
the right hand figure denotes the number of out-
puts in our method. Correct outputs are in the
diagonal cells, while the domain selection errors
are in the off diagonal cells. Total accuracy in-
creased by 5.3%, from 71.2% to 76.5%, and the
number of errors in domain selection was reduced
from 634 to 518, so the error reduction rate was
18.3% (= 116/634). There was no output in the
baseline method for ?other domains (III)?, which is
in the third column, because conventional frame-
works have not taken this choice into considera-
tion. Our method was able to detect this kind of
error in 157 of 371 utterances, which allows us
to prevent further errors from continuing. More-
over, accuracies for (I) and (II) did not get worse.
Precision for (I) improved from 0.77 to 0.83, and
the F-measure for (I) also improved from 0.83 to
0.86. Although recall for (II) got worse, its preci-
sion improved from 0.52 to 0.62, and consequently
the F-measure for (II) improved slightly from 0.61
to 0.62. These results show that our method can
detect choice (III), which was newly introduced,
without degrading the existing classification accu-
racies.
The features that follow played an important
role in the decision tree. The features that repre-
sent confidence in the previous domain appeared
in the upper part of the tree, including ?the num-
ber of affirmatives after entering the domain (P1)?,
?the ratio of user?s negative answers in the do-
main (P9)?, ?the number of turns after entering the
domain (P6)?, and ?the number of changed slots
based on the user?s utterances after entering the
domain (P5)?. These were also ?whether a domain
with the highest score has appeared before (C8)?
and ?whether an interpretation of a current user?s
utterance is negative (C2)?.
6 Conclusion
We constructed a multi-domain spoken dialogue
system using an extensible framework. Domain
selection in conventional studies is based on ei-
ther the domain based on the speech recognition
15
Table 5: Confusion matrix in domain selection (baseline / our method)
reference label \ output in previous response (I) with highest score (II) others (III) # total label (recall)
in previous response (I) 1289 / 1291 162 / 85 0 / 75 1451 (0.89 / 0.89)
with highest score (II) 84 / 99 299? / 256? 0 / 28 383 (0.74 / 0.62)
others (III) 293 / 172 78 / 42 0 / 157 371 ( 0 / 0.42)
total 1666 / 1562 539 / 383 0 / 260 2205
(precision) (0.77) / (0.83) (0.52) / (0.62) ( - ) / (0.60) (0.712 / 0.765)
?: These include 17 errors because of random selection when there were several domains having the same highest scores.
results or the previous domain. However, we no-
ticed that these conventional frameworks cannot
cope with situations where neither of these do-
mains is correct. Detection of such situations
can prevent dialogues from staying in the incor-
rect domain, which allows our domain selection
method to be robust against speech recognition er-
rors. Furthermore, our domain selection method
is also extensible. Our method does not select the
domains directly, but, by categorizing them into
three classes, it can cope with an increase or de-
crease in the number of domains. Based on the re-
sults of an experimental evaluation using 10 sub-
jects, our method was able to reduce domain se-
lection errors by 18.3% compared to a baseline
method. This means our system is robust against
speech recognition errors.
There are still some issues that could make
our system more robust, and this is included in
future work. For example, in this study, we
adopted a grammar-based speech recognizer to
construct each domain expert easily. However,
other speech recognition methods could be used,
such as a statistical language model. As well,
multiple speech recognizers employing different
domain-dependent grammars could be run in par-
allel. Thus, we need to investigate how to integrate
these approaches into our framework, without de-
stroying the extensibility.
References
Masahiro Araki, Kazunori Komatani, Taishi Hirata,
and Shuji Doshita. 1999. A dialogue library for
task-oriented spoken dialogue systems. In Proc.
IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, pages 1?7.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Re-
cent progress of open-source LVCSR engine Julius
and japanese model repository. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), pages 3069?
3072.
Kazunori Komatani, Naoyuki Kanda, Tetsuya Ogata,
and Hiroshi G. Okuno. 2005a. Contextual
constraints based on dialogue models in database
search task for spoken dialogue systems. In Proc.
European Conf. Speech Commun. & Tech. (EU-
ROSPEECH), pages 877?880, Sep.
Kazunori Komatani, Shinichi Ueno, Tatsuya Kawa-
hara, and Hiroshi G. Okuno. 2005b. User model-
ing in spoken dialogue systems to generate flexible
guidance. User Modeling and User-Adapted Inter-
action, 15(1):169?183.
Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, and
Samir Bennacef. 1999. The LIMSI ARISE sys-
tem for train travel information. In IEEE Int?l Conf.
Acoust., Speech & Signal Processing (ICASSP),
pages 501?504, Phoenix, AZ.
Ian R. Lane and Tatsuya Kawahara. 2005. Utterance
verification incorporating in-domain confidence and
discourse coherence measures. In Proc. European
Conf. Speech Commun. & Tech. (EUROSPEECH),
pages 421?424.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky,M. Rahim, P. Ruscitti, andM.Walker.
2000. The AT&T-DARPA communicator mixed-
initiative spoken dialogue system. In Proc. Int?l
Conf. Spoken Language Processing (ICSLP).
Bor-shen Lin, Hsin-min Wang, and Lin-shan Lee.
2001. A distributed agent architecture for intelli-
gent multi-domain spoken dialogue systems. IEICE
Trans. on Information and Systems, E84-D(9):1217?
1230, Sept.
Mikio Nakano, Yuji Hasegawa, Kazuhiro Nakadai,
Takahiro Nakamura, Johane Takeuchi, Toyotaka
Torii, Hiroshi Tsujino, Naoyuki Kanda, and Hi-
roshi G. Okuno. 2005. A two-layer model for be-
havior and dialogue planning in conversational ser-
vice robots. In 2005 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems (IROS),
pages 1542?1548.
Ian O?Neill, Philip Hanna, Xingkun Liu, and Michael
McTear. 2004. Cross domain dialogue modelling:
An object-based approach. In Proc. Int?l Conf. Spo-
ken Language Processing (ICSLP).
Botond Pakucs. 2003. Towards dynamic multi-
domain dialogue processing. In Proc. European
16
Conf. Speech Commun. & Tech. (EUROSPEECH),
pages 741?744.
Alexandros Potamianos and Hong-Kwang J. Kuo.
2000. Statistical recursive finite state machine pars-
ing for speech understanding. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), volume 3,
pages 510?513.
J. Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo,
CA. http://www.rulequest.com/see5-info.html.
Antoine Raux and Maxine Eskenazi. 2004. Non-
native users in the let?s go!! spoken dialogue sys-
tem: Dealing with linguistic mismatch. In Proc. of
HLT/NAACL.
Ruben San-Segundo, Bryan Pellom, Wayne Ward, and
Jose M. Pardo. 2000. Confidence measures for di-
alogue management in the CU communicator sys-
tem. In IEEE Int?l Conf. Acoust., Speech & Signal
Processing (ICASSP).
17
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 314?321,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Ranking Help Message Candidates Based on Robust Grammar
Verification Results and Utterance History in Spoken Dialogue Systems
Kazunori Komatani Satoshi Ikeda Yuichiro Fukubayashi
Tetsuya Ogata Hiroshi G. Okuno
Graduate School of Informatics
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,sikeda,fukubaya,ogata,okuno}@kuis.kyoto-u.ac.jp
Abstract
We address an issue of out-of-grammar
(OOG) utterances in spoken dialogue sys-
tems by generating help messages for
novice users. Help generation for OOG
utterances is a challenging problem be-
cause language understanding (LU) re-
sults based on automatic speech recogni-
tion (ASR) results for such utterances are
always erroneous as important words are
often misrecognized or missed from such
utterances. We first develop grammar ver-
ification for OOG utterances on the ba-
sis of a Weighted Finite-State Transducer
(WFST). It robustly identifies a grammar
rule that a user intends to utter, even when
some important words are missed from the
ASR result. We then adopt a ranking algo-
rithm, RankBoost, whose features include
the grammar verification results and the
utterance history representing the user?s
experience.
1 Introduction
Studies on spoken dialogue systems have recently
proceeded from in-laboratory systems to ones de-
ployed to the open public (Raux et al, 2006; Ko-
matani et al, 2007; Nisimura et al, 2005). Ac-
cordingly, opportunities are increasing as general
citizens use the systems. This situation means
that novice users directly access the systems with
no instruction, which is quite different from in-
laboratory experiments where some instructions
can be given. In such cases, users often experi-
ence situations where their utterances are not cor-
rectly recognized. This is because of a gap be-
tween the actual system and a user?s mental model,
that is, a user?s expectation of the system. Ac-
tually, a user?s utterance often cannot be inter-
preted by the system because of the system?s lim-
ited grammar for language understanding (LU).
We call such an unacceptable utterance an ?out-
of-grammar (OOG) utterance.? When users? ut-
terances are OOG, they cannot change their ut-
terances into acceptable ones unless they are in-
formed what expressions are acceptable by the
system.
We aim to manage the problem of OOG utter-
ances by providing help messages showing an ex-
ample of acceptable language expressions when a
user utterance is not acceptable. We prepare help
messages corresponding to each grammar rule the
system has. We therefore assume that appropri-
ate help messages can be provided if a user?s in-
tention, i.e., a grammar rule the user originally
intends to use by his utterance, is correctly esti-
mated.
Issues for generating such help messages in-
clude:
1. Estimating a grammar rule corresponding to
user intention even from OOG utterances,
and
2. Complementing missing information in a sin-
gle utterance.
The first issue focuses on the fact that automatic
speech recognition (ASR) results, used as main in-
put data, are erroneous for OOG utterances. Es-
timating a grammar rule that the user intends to
use becomes accordingly difficult especially when
content words, which correspond to database en-
tries such as place names and their attributes, are
not correctly recognized. That is, any type of ASR
error in any position should be taken into consid-
eration in ASR results of OOG utterances. On the
314
other hand, the second issue focuses on the fact
that an ASR result for an OOG utterance does not
necessarily contain sufficient information to esti-
mate the user intention. This is because of ASR
errors or that users may omit some elements from
their utterances because they are in context.
We develop a grammar verification method
based on Weighted Finite-State Transducer
(WFST) as a solution to the first issue. The
grammar verification method robustly estimates
which a grammar rule is intended to use by a
user?s utterance. The WFST is automatically
generated to represent an ASR result in which any
possibility of error is taken into consideration. We
furthermore adopt a boosting algorithm, Rank-
Boost (Freund et al, 2003), to put help messages
in order of probability to address the second issue.
Because it is difficult even for human annotators
to uniquely determine which help message should
be provided for each case, we adopt an algorithm
that can be used for training on several data
examples that have a certain order of priority.
We also incorporate features representing the
user?s utterance history for preventing message
repetition.
2 Related Work
Various studies have been done on generating help
messages in spoken dialogue systems. Gorrell et
al. (2002) trained a decision tree to classify causes
of errors for OOG utterances. Hockey et al (2003)
also classified OOG utterances into the three cate-
gories of endpointing errors, unknown vocabulary,
and subcategorization mistakes, by comparing two
kinds of ASR results. This was called Targeted
Help and provided a user with immediate feedback
tailored to what the user said. Lee et al (2007) also
addressed error recovery by generating help mes-
sages in an example-based dialog modeling frame-
work. These studies, however, determined what
help messages should be provided mainly on the
basis of literal ASR results. Therefore, help mes-
sages would be degraded by ASR results in which
a lot of information was missing, especially for
OOG utterances. The same help messages would
be repeated when the same ASR results were ob-
tained.
An example dialogue enabled by our method,
especially the part of the method described in Sec-
tion 4, is shown in Figure 1. Here, user utter-
ances are transcriptions, and utterance numbers
U1: Tell me your recommending sites.
Underlined parts are not in-vocabulary and no
valid LU result is obtained. The estimated gram-
mar is [Obtaining info on a site] although the most
appropriate help message is that corresponding to
[Searching tourist sites].
S1: I did not understand. You can say ?Tell me
the address of Kiyomizu Temple? for example,
if getting information on a site.
The help message corresponding to [Obtaining info
on a site] is provided.
U2: Tell me your recommending sites.
The user repeats the same utterance probably be-
cause the help message (S1) was not helpful. The
estimated grammar is [Obtaining info on a site]
again.
S2: I did not understand. You can say ?Search
shrines or museums? for example, if searching
tourist sites.
Another help message corresponding to [Searching
tourist sites] is provided after ranking candidates
by also using the user?s utterance history.
[] denotes grammar rules.
Figure 1: Example dialogue enabled by our
method
start with ?S? and ?U? denote system and user
utterances, respectively. In this example, ASR
results for the user utterances (U1 and U2) do
not contain sufficient information because the ut-
terances are short and contain out-of-vocabulary
words. These two results are similar, and ac-
cordingly, the help message after U2 provided by
methods like Targeted Help (Gorrell et al, 2002;
Hockey et al, 2003) is the same as Utterance S1
because they are only based on ASR results. Our
method can provide different help messages as Ut-
terance S2 after ranking candidates by consider-
ing the utterance history and grammar verification
results. Because the candidates are arranged in
the order of probability, the most appropriate help
message can be provided in fewer attempts.
This ranking method for help message candi-
dates is also useful in multimodal interfaces with
speech input. Help messages are necessary when
ASR is used as its input modality, and such mes-
sages were actually implemented in City Browser
(Gruenstein and Seneff, 2007), for example. This
system lists template-based help messages on the
screen by using ASR results and internal states of
the system. The order of help messages is impor-
tant, especially in portable devices with a small
screen, on which the number of help messages dis-
315
played at one time is limited, as Hartmann and
Schreiber (2008) pointed out. Even in cases where
sufficiently large screens are available, too many
help messages without any order will distract the
user?s attention and thus spoil its usability.
3 Grammar Verification based on WFST
We estimate a user?s intention even from OOG ut-
terances as a grammar rule that the user intends
to use by his utterance. We call this estimation
grammar verification. This process is applied to
ASR outputs based on a statistical language model
(LM) in this paper. We use two transducers: a
finite-state transducer (FST) representing the task
grammar, and weighted FST (WFST) representing
an ASR result and its confidence score. Hereafter,
we denote these two as ?grammar FST? and ?input
WFST? and depict examples in Figure 2.
A strong point of our method is that it takes
all three types of ASR error into consideration.
The input WFST is designed to represent all cases
where any word in an ASR result is an inserted or
substituted error, or any word is deleted. Its weight
is designed to reflect confidence scores of ASR re-
sults. By composing this WFST and the gram-
mar FST, we can obtain all possible sequences
and their accumulated weights when arbitrary se-
quences represented by the input WFST are input
into the grammar FST. The optimal results having
the maximum accumulated weight consist of the
LU result and the grammar rule that is the nearest
to the ASR result. The result can be obtained even
when any element in it is misrecognized or absent
from the ASR result.
An LU result is a set of concepts that consist
of slots and their values corresponding to database
entries the system handles. For example, an LU
result ?month=2, day=22? consists of two con-
cepts, such as the value of slotmonth is 2, and the
value of slot day is 22.
3.1 Design of input WFST and grammar FST
In input WFSTs and grammar FSTs, each arc rep-
resenting state transitions has a label in the form of
?a:b/c? denoting its input symbol, output symbol,
and weight, in this order. Input symbol ? means a
state transition without any input symbol, that is,
an epsilon transition. Output symbol ? means no
output in the state transition. For example, a state
transition ?please:?/1.0? is executed when an in-
put symbol is ?please,? no output symbol is gen-
erated, and 1.0 is added to the accumulated weight.
Weights are omitted in the grammar FST because
no weight is given in it.
An input WFST is automatically constructed
from an ASR result. Sequential state transitions
are assigned to each word in the ASR result, and
each of them is paralleled by filler transitions, as
shown in Figure 2 where the ASR result was ?Ev-
ery Monday please? for example. Filler transitions
such as INS, DEL, and SUB are assigned to each
state for representing every kind of error such as
insertion, deletion, and substitution errors. All in-
put symbols in the input WFST are ?, by which the
WFST represents all possible sequences contain-
ing arbitrary errors. For example, the input WFST
in Figure 2 represents all possible sequences such
as ?Every Monday please,? ?Every Monday F,? ?F
Monday F,? and so on. Here, every word can be
replaced by the symbol F that represents an inser-
tion or substitution error. Moreover, the error sym-
bol DEL can be inserted into its output symbol se-
quence at any position, which corresponds to dele-
tion errors in ASR results. Each weight per state
transition is summed up and then the optimal re-
sult is determined. The weights will be explained
in Section 3.2.
A grammar FST is generated from a task gram-
mar, which is written by a system developer for
each task. It determines whether an input se-
quence conforms to the task grammar. We also
assign filler transitions to each state for handling
each type of error of ASR results considered in
the input WFST. A filler transition, either of INS,
DEL, or SUB, is added to each state in the FST
except for states within keyphrases, which are ex-
plicitly indicated by a system developer. In the
example shown in Figure 2, ?SUB $ Monday
date-repeat=Mon please? is output for an input
sequence ?SUB Monday please?. Here, date-
repeat=Mon denotes an LU result, and $ is a sym-
bol for marking words corresponding to a concept.
3.2 Weights assigned to input WFST
We defined two kinds of weights:
1. Rewards for accepted words (wacc), and
2. Penalties for each kind of error (wsub, wdel,
wins).
An accumulated weight for a single utterance is
defined as the sum of these weights as shown be-
316
Input WFST
Every:
Every
Monday:
Monday
please:
please
Grammar FST
input:output/weight
ASR result: ?Every Monday please?
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !!
  !"#$%& :
 !!
  !"#$"%& :
 !!
  !"#$%&' :
 !"# !"
 !"# !"
 !"# !"
$:?
repeat-date:?
Mon=
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"
  !"#$ :
 !"# !"  !"# !"
 !"# !"
 !"# !"
 !"# !"
Figure 2: Example of input WFST and grammar FST
low.
w =
?
Eaccepted
wacc +
?
Eerror
(wsub + wdel + wins)
Here, Eaccepted denotes a set of accepted words
corresponding to elements of each grammar rule,
and Eerror denotes a set of words that are not ac-
cepted and that have either error symbol. Note that
the weights are not given beforehand but are cal-
culated and given to the input WFST in runtime
according to each ASR result.
A weight for an accepted word easr is defined
by using its confidence score CM(easr) (Lee et
al., 2004) and its word length. A word length in
mora is denoted as l(?), which is normalized by
that of the longest word in the vocabulary.
wacc = CM(easr)l(easr)
This weight wacc gives preference to sequences
containing longer words with higher confidence
scores.
Weights for each type of error have negative val-
ues because they are penalties:
wsub = ?{CM(easr)l(easr) + l(egram)}/2
wdel = ?{l(e) + l(egram)}/2
wins = ?{CM(easr)l(easr) + l(e)}/2
where l(e) is the average word length in the vocab-
ulary and egram is a grammar element i.e., either a
word or a class. A deletion error is a case when a
grammar element does not correspond to any word
in the ASR result. A substitution error is a case
when an element is replaced by another word in
the ASR result. An insertion error is a case when
no grammar element corresponds to the ASR re-
sult. Every weight is defined as an average of a
word length of a grammar element and the corre-
sponding one in the ASR result multiplied by its
confidence score. When correspondences cannot
be defined in insertion and deletion errors, l(e) is
used instead. In the case when egram is a class in
the grammar, the average word length in that class
is used as l(egram).
3.3 Example of calculating the weights
We show how a weight is calculated by using the
example in Figure 3. In this example, the user ut-
terance was ?Tell me a liaison of Koetsu-ji (a tem-
ple name).? The word ?liaison? was not in the sys-
tem vocabulary. The ASR result accordingly con-
tained errors for that part; the result was ?Tell me
all Sakyo-ward Koetsu-ji.?
Weights are calculated for each grammar rule
the system has. This example shows calcula-
tions for two grammar rules: [get info] accept-
ing ?Tell me ?item name? of ?temple name?,? and
[search ward] accepting ?Tell me ?facility name?
of ?ward name?.? Here, [] and ?? denote a gram-
mar rule and a class in grammars. Two alignment
results are also shown for grammar [get info] in
this example. Weights are calculated for any align-
ment as shown here, and the alignment result with
the largest weight is selected. In this example,
weight +0.16 for the grammar [get info] was the
largest.
We consequently obtained the result that gram-
mar rule [get info] had the highest score for this
OOG utterance and its accumulated weight was
317
User utterance: ?Tell me a liaison of Koetsu-ji?. (Underlined parts denote OOG.)
ASR result tell me all Sakyo-ward Koetsu-ji
(ward) (temple)
grammar [get info] tell me ?item name? of ?temple name?
WFST output tell me INS SUB DEL Koetsu-ji
weights +0.09 +0.06 ?0.04 ?0.11 ?0.02 +0.18 +0.16
grammar [get info] tell me ?item name ? of ?temple name?
WFST output tell me SUB SUB Koetsu-ji
weights +0.09 +0.06 ?0.21 ?0.10 +0.18 +0.02
grammar [search ward] tell me ?facility type? in ?ward name?
WFST output tell me INS SUB DEL SUB
weights +0.09 +0.06 ?0.04 ?0.12 ?0.02 ?0.21 ?0.24
Figure 3: Example of calculating weights in our grammar verification
+0.16. The result also indicated each type of er-
ror as a result of the alignment: ?item name? was
substituted by ?Sakyo-ward?, ?of? in the grammar
[get info] was deleted, and ?all? in the ASR result
was inserted.
4 Ranking Help Message Candidates by
Integrating Dialogue Context
We furthermore develop a method to rank help
message candidates per grammar rule by integrat-
ing the grammar verification result and the user?s
utterance history. This complements information
that is often absent from utterances or misrecog-
nized in ASR and prevents that the same help mes-
sages are repeated. An outline of the method is
depicted in Figure 4.
4.1 Features used in Ranking
Features used in our methods are listed in Table
1. These features are calculated for each help
message candidate corresponding to each gram-
mar rule. Features H1 to H5 represent how reli-
able a grammar verification result is. Feature H1 is
a grammar verification score, that is, the resulting
accumulated weight described in Section 3. Fea-
ture H2 is calculated by normalizing H1 by the
total score of all grammar rules. This represents
how reliable the grammar verification result is rel-
atively compared to others. Features H3 to H5
represent how partially the user utterance matches
with the grammar rule.
Features H6 and H7 correspond to a dialogue
context. Feature H6 reflects the case in which
users tend to repeat similar utterances when their
utterances were not understood by the system.
Feature H7 represents whether and how the user
knows about the language expression of the gram-
mar rule. This feature corresponds to the known
degree we previously proposed (Fukubayashi et
Table 1: Features of each instance (help message
candidate)
H1: accumulated weight of GV (GV score)
H2: GV score normalized by the total GV score of other
instances
H3: ratio of # of accepted words in GV result to # of all
words
H4: maximum number of successively accepted words
in GV result
H5: number of accepted slots in GV result
H6: how before the grammar rule was selected as GV
result (in # of utterances)
H7: maximum GV score for the grammar rule until then
H8: whether it belongs to the ?command? class
H9: whether it belongs to the ?query? class
H10: whether it belongs to the ?request-info? class
H11-H17: products of H8 and each of H1 to H7
H18-H24: products of H9 and each of H1 to H7
H25-H31: products of H10 and each of H1 to H7
GV: grammar verification
al., 2006), and prevents a help message the user
already knows from being provided repeatedly.
Features H8 to H10 represent properties of
utterances corresponding to the grammar rules,
which are categorized into three classes such as
?command,? ?query,? and ?request-info.? In the
sightseeing task, the numbers of grammar rules for
the three classes were 8, 4, and 11, respectively.
More specifically, utterances in either ?query? or
?request-info? class tend to appear successively
because they are used when users try and com-
pare several query conditions; on the other hand,
utterances in ?command? class tend to appear in-
dependently of the context. Features H11 to H31
are the products of features H8, H9, and H10 and
each feature from H1 to H7. These were defined to
consider combinations of properties of utterances
represented by H8, H9, and H10 and their reliabil-
ity represented by H1 to H7, because RankBoost
318
Help candidate 
Help candidate
Ranking
(RankBoost)
?
=
T
t
tt
xhxH )()( ?
1
x
LL )()(
111
xfxf
i
LL )()(
1 nin
xfxf
n
x
User
utterance
Context
deft
qi,,,??
Parameters
Training 
data
p
x
q
x
Grammar
verification
Calculating features
Sorted by H(x)
Statistical LM-based
ASR outputs
Figure 4: Outline of our ranking method for help message candidates
does not consider them.
4.2 Ranking Algorithm
We adopt RankBoost (Freund et al, 2003), a
boosting algorithm based on machine learning, to
rank help message candidates. This algorithm can
be used for training on several data examples hav-
ing a certain order of priority. This attribute fits
for the problem in this paper; it is difficult even
for human annotators to determine the unique ap-
propriate help message to be provided. Target in-
stances x of the algorithm are help message can-
didates corresponding to grammar rules in this pa-
per.
RankBoost trains a score function H(x) and ar-
ranges instances x in the order. Here, H(x?) <
H(x??) means x?? is ranked higher than x?. This
score function is defined as a linear combination
of weak rankers giving partial information regard-
ing the order:
H(x) =
T
?
t
?tht(x)
where T , ht(), and ?t denote the number of boost-
ing iterations, a weak ranker, and its associated
weight, respectively. The weak ranker ht is de-
fined by comparing the value of a feature fi of an
instance x with a threshold ?. That is,
ht(x) =
?
?
?
?
?
1 if fi(x) > ?
0 if fi(x) ? ?
qdef if fi(x) = ?
(1)
where qdef ? {0, 1}. Here, fi(x) denotes the
value of the i-th feature of instance x, and ? de-
notes that no value is given in fi(x).
5 Experimental Evaluation
5.1 Target Data
Data were collected by 30 subjects in total by us-
ing a multi-domain spoken dialogue system that
handles five domains such as restaurant, hotel,
sightseeing, bus, and weather (Komatani et al,
2008). The data consisted of 180 dialogues and
11,733 utterances. Data from five subjects were
used to determine the number of boosting iter-
ations and to improve LMs for ASR. We used
utterances in the restaurant, hotel, and sightsee-
ing domains because the remaining two, bus and
weather, did not have many grammar rules. We
then extracted OOG utterances on the basis of the
grammar verification results to evaluate the per-
formance of our method for such utterances. We
regarded an utterance whose accumulated weight
was negative as OOG. As a result, 1,349 OOG ut-
terances by 25 subjects were used for evaluation,
hereafter. These consisted of 363 utterances in the
restaurant domain, 563 in the hotel domain, and
423 in the sightseeing domain. These data were
collected under the following conditions: subjects
were given no instructions on concrete language
expressions the system accepts. System responses
were made only by speech, and no screen for dis-
playing outputs was used. Subjects were given six
scenarios describing tasks to be completed.
We used Julius1 that is a statistical-LM-based
ASR engine. We constructed class 3-gram LMs
for ASR by using 10,000 sentences generated
from the task grammars and the 600 utterances
collected by the five subjects. The vocabulary
sizes for the restaurant, hotel, and sightseeing do-
mains were 3,456, 2,625, and 3,593, and ASR ac-
curacies for them were 45.8%, 57.1%, and 43.5%,
respectively. These ASR accuracies were not very
high because the target utterances were all OOG.
A set of possible thresholds in the weak rankers
described in Section 4.2 consisted of all feature
values that appeared in the training data. The num-
bers of boosting iterations were determined on the
basis of accuracies for the data by the five sub-
1http://julius.sourceforge.jp/
319
!"#
$"#
%"#
&"#
'"#
("#
)"#
*""#
*+,-./ 0+,-./ !+,-./ $+,-./ %+,-./
1+,-./
!
"
"
#
$
%
"
&
!"#$%&'$ ()*+,$-.(/
Figure 5: Accuracy when N candidates were pro-
vided in sightseeing domain (1 ? N ? 5)
jects. The numbers were 400, 100, and 500 for the
restaurant, hotel, and sightseeing domains.
5.2 Evaluation Criterion
We manually gave five help messages correspond-
ing to grammar rules as reference labels per ut-
terance in the order of having a strong relation to
the utterance. The numbers of candidate help mes-
sages were 28, 27, and 23 for the restaurant, hotel
and sightseeing domains, respectively.
We evaluated our ranking method as the accu-
racy where at least one of the reference labels was
contained in its top N candidates. This corre-
sponds to a probability where at least one appro-
priate help message was contained in a list of N
candidates. The accuracy was calculated by 5-fold
cross validation. In the baseline method we set,
help messages were provided only by using the
grammar verification scores.
5.3 Results
Results in the sightseeing domain are plotted in
Figure 5. We can see that our method outper-
formed the baseline in the accuracies for all N
values. All these differences were statistically sig-
nificant (p < 0.05) by the McNemar test. The ac-
curacies were also better in the other two domains
for all N values, and the average differences for
the three domains were 11.7 points for N=1, 9.7
points for N=2, and 6.7 points for N=3. The dif-
ferences were large especially for small N values.
This result indicates that we can successfully re-
duce the number of help messages when providing
several ones for users. The improvements were
derived from the features we incorporated such as
the estimated user knowledge in addition to gram-
mar verification results. The baseline method was
only based on grammar verification results for sin-
gle utterances, which contained insufficient infor-
mation because OOG utterances were often mis-
recognized or misunderstood.
Table 2: Sum of absolute values of weight ? for
each feature
H7 H17 H19 H2 H6
(H7*H8) (H2*H9)
9.58 6.91 6.61 6.02 6.01
We also investigated dominant features by cal-
culating the sum of absolute values of final weight
? for each feature in RankBoost. Five dominant
features based on the sums are shown in Table
2. These five features include a feature obtained
from grammar verification result (H2), a feature
about the user?s utterance history (H6), a feature
representing estimated user knowledge (H7), and
features representing properties of the utterances.
The most dominant feature was H7, which ap-
peared twice in this table. This was because user
utterances were not likely to be OOG utterances
again after the user had already known an expres-
sion corresponding to the grammar rule, which can
be detected when user utterances for it were cor-
rectly accepted, that is, its grammar verification
score was high. The second dominant feature was
H2, which showed that grammar verification re-
sults worked effectively.
6 Conclusion
We addressed an issue of OOG utterances in spo-
ken dialogue systems by generating help mes-
sages. To manage situations when a user utter-
ance could not be accepted, we robustly estimated
a user?s intention as a grammar rule that the user
intends to use. We furthermore integrated various
information as well as the grammar verification
results for complementing missing information in
single utterances, and then ranked help message
candidates corresponding to the grammar rules for
efficiently providing them.
Our future work includes the following. The
evaluation in this paper was taken place only on
the basis of utterances collected beforehand. Pro-
viding help messages itself should be evaluated by
another experiment through dialogues. Further-
more, we assumed that language expressions of
help messages to show an example language ex-
pression were fixed. We also need to investigate
what kind of expression is more helpful to novice
users.
320
References
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and
Yoram Singer. 2003. An efficient boosting algo-
rithm for combining preferences. Journal of Ma-
chine Learning Research, 4:933?969.
Yuichiro Fukubayashi, Kazunori Komatani, Tetsuya
Ogata, and Hiroshi G. Okuno. 2006. Dynamic
help generation by estimating user?s mental model in
spoken dialogue systems. In Proc. Int?l Conf. Spo-
ken Language Processing (INTERSPEECH), pages
1946?1949.
Genevieve Gorrell, Ian Lewin, and Manny Rayner.
2002. Adding intelligent help to mixed-initiative
spoken dialogue systems. In Proc. Int?l Conf. Spo-
ken Language Processing (ICSLP), pages 2065?
2068.
Alexander Gruenstein and Stephanie Seneff. 2007.
Releasing a multimodal dialogue system into the
wild: User support mechanisms. In Proc. 8th SIG-
dial Workshop on Discourse and Dialogue, pages
111?119.
Melanie Hartmann and Daniel Schreiber. 2008. Proac-
tively adapting interfaces to individual users for mo-
bile devices. In Adaptive Hypermedia and Adap-
tive Web-Based Systems, 5th International Confer-
ence (AH 2008), volume 5149 of Lecture Notes in
Computer Science, pages 300?303. Springer.
Beth A. Hockey, Oliver Lemon, Ellen Campana, Laura
Hiatt, Gregory Aist, James Hieronymus, Alexander
Gruenstein, and John Dowding. 2003. Targeted
help for spoken dialogue systems: intelligent feed-
back improves naive users? performance. In Proc.
10th Conf. of the European Chapter of the ACL
(EACL2003), pages 147?154.
Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.
Okuno. 2007. Analyzing temporal transition of real
user?s behaviors in a spoken dialogue system. In
Proc. INTERSPEECH, pages 142?145.
Kazunori Komatani, Satoshi Ikeda, Tetsuya Ogata,
and Hiroshi G. Okuno. 2008. Managing out-of-
grammar utterances by topic estimation with domain
extensibility in multi-domain spoken dialogue sys-
tems. Speech Communication, 50(10):863?870.
Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawa-
hara. 2004. Real-time word confidence scoring us-
ing local posterior probabilities on tree trellis search.
In IEEE Int?l Conf. Acoust., Speech & Signal Pro-
cessing (ICASSP), volume 1, pages 793?796.
Cheongjae Lee, Sangkeun Jung, Donghyeon Lee, and
Gary Guenbae Lee. 2007. Example-based error re-
covery strategy for spoken dialog system. In Proc.
of IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU), pages 538?543.
Ryuichi Nisimura, Akinobu Lee, Masashi Yamada, and
Kiyohiro Shikano. 2005. Operating a public spo-
ken guidance system in real environment. In Proc.
European Conf. Speech Commun. & Tech. (EU-
ROSPEECH), pages 845?848.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year of
Let?s Go! experience. In Proc. INTERSPEECH.
321
Coling 2010: Poster Volume, pages 579?587,
Beijing, August 2010
Automatic Allocation of Training Data for Rapid Prototyping
of Speech Understanding based on Multiple Model Combination
Kazunori Komatani? Masaki Katsumaru? Mikio Nakano?
Kotaro Funakoshi? Tetsuya Ogata? Hiroshi G. Okuno?
? Graduate School of Informatics, Kyoto University
{komatani,katumaru,ogata,okuno}@kuis.kyoto-u.ac.jp
? Honda Research Institute Japan Co., Ltd.
{nakano,funakoshi}@jp.honda-ri.com
Abstract
The optimal choice of speech understand-
ing method depends on the amount of
training data available in rapid prototyp-
ing. A statistical method is ultimately
chosen, but it is not clear at which point
in the increase in training data a statisti-
cal method become effective. Our frame-
work combines multiple automatic speech
recognition (ASR) and language under-
standing (LU) modules to provide a set
of speech understanding results and se-
lects the best result among them. The
issue is how to allocate training data to
statistical modules and the selection mod-
ule in order to avoid overfitting in training
and obtain better performance. This paper
presents an automatic training data alloca-
tion method that is based on the change
in the coefficients of the logistic regres-
sion functions used in the selection mod-
ule. Experimental evaluation showed that
our allocation method outperformed base-
line methods that use a single ASR mod-
ule and a single LU module at every point
while training data increase.
1 Introduction
Speech understanding in spoken dialogue systems
is the process of extracting a semantic represen-
tation from a user?s speech. That is, it consists
of automatic speech recognition (ASR) and lan-
guage understanding (LU). Because vocabularies
and language expressions depend on individual
systems, it needs to be constructed for each sys-
tem, and accordingly, training data are required
for each. To collect more real training data, which
will lead to higher performance, it is more desir-
able to use a prototype system than that based on
the Wizard-of-Oz (WoZ) method where real ASR
errors cannot be observed, and to use a more ac-
curate speech understanding module. That is, in
the bootstrapping phase, spoken dialogue systems
need to operate before sufficient real data have
been collected.
We have been addressing the issue of rapid pro-
totyping on the basis of the ?Multiple Language
model for ASR and Multiple language Under-
standing (MLMU)? framework (Katsumaru et al,
2009). In MLMU, the most reliable speech un-
derstanding result is selected from candidates pro-
duced by various combinations of multiple ASR
and LU modules using hand-crafted grammar and
statistical models. A grammar-based method is
still effective at an early stage of system devel-
opment because it does not require training data;
Schapire et al (2005) also incorporated human-
crafted prior knowledge into their boosting al-
gorithm. By combining multiple understanding
modules, complementary results can be obtained
by different kinds of ASR and LU modules.
We propose a novel method to allocate avail-
able training data to statistical modules when the
amount of training data increases. The training
data need to be allocated adaptively because there
are several modules to be trained, and they would
cause overfitting without data allocation. There
are speech understanding modules that have lan-
guage models (LMs) for ASR and LU models
579
(LUMs), and a selection module that selects the
most reliable speech understanding result from
multiple candidates in the MLMU framework.
When the amount of available training data is
small, and an LUM and the selection module are
trained on the same data set, they are trained un-
der a closed-set condition, and thus the training
data for the selection module include too many
correct understanding results. In such cases, the
data need to be divided into subdata sets to avoid
overfitting. On the other hand, when the amount
of available training data is large, so that overfit-
ting does not occur, all available data should be
used to train each statistical module to prepare as
much training data as possible.
We therefore develop a method for switching
data allocation policies. More specifically, two
points are automatically determined at which sta-
tistical modules with more parameters start to be
trained. As a result, better overall performance
is achieved at every point while the amount of
training data increases, compared with all combi-
nations of a single ASR module and a single LU
module.
2 Related Work
It is important to consider the amount of available
training data when designing a speech understand-
ing module. Many statistical LU methods have
been studied, e.g., (Wang and Acero, 2006; Jeong
and Lee, 2006; Raymond and Riccardi, 2007;
Hahn et al, 2008; Dinarelli et al, 2009). They
generally outperform grammar-based LU meth-
ods when a sufficient amount of training data is
available; but sufficient training data are not nec-
essarily available during rapid prototyping. Sev-
eral LU methods were constructed using a small
amount of training data (Fukubayashi et al, 2008;
Dinarelli et al, 2009). Fukubayashi et al (2008)
constructed an LU method based on the weighted
finite state transducer (WFST), in which filler
transitions accepting arbitrary inputs and transi-
tion weights were added to a hand-crafted FST.
This method is placed between a grammar-based
method and a statistical method because a sta-
tistically selected weighting scheme is applied
to a hand-crafted grammar model. Therefore,
the amount of training data can be smaller com-
pared with general statistical LU methods, but this
method does not outperform them when plenty of
training data are available. Dinarelli et al (2009)
used a generative model for which overfitting is
less prone to occur than discriminative models
when the amount of training data is small, but
they did not use a grammar-based model, which is
expected to achieve reasonable performance even
when the amount of training data is very small.
Raymond et al (2007) compared the perfor-
mances of statistical LU methods for various
amounts of training data. They used a statis-
tical finite-state transducer (SFST) as a genera-
tive model and a support vector machine (SVM)
and conditional random fields (CRF) as discrim-
inative models. The generative model was more
effective when the amount of data was small,
and the discriminative models were more effec-
tive when it was large. This shows that the perfor-
mance of an LUmethod depends on the amount of
training data available, and therefore, LU meth-
ods need to be switched automatically. Wang et
al. (2002) developed a two-stage speech under-
standing method by applying statistical methods
first and then grammatical rules. They also ex-
amined the performance of the statistical methods
at their first stage for various amounts of train-
ing data and confirmed that the performance is not
very high when a small amount of data is used.
Schapire et al (2005) showed that accuracy
of call classification in spoken dialogue systems
improved by incorporating hand-crafted prior
knowledge into their boosting algorithm. Their
idea is the same as ours in that they improve the
system?s performance by using hand-crafted hu-
man knowledge while only a small amount of
training data is available. We furthermore solve
the data allocation problem because there are mul-
tiple statistical models to be trained in speech
understanding, while their call classification has
only one statistical model.
3 MLMU Framework
MLMU is the framework for selecting the most
reliable speech understanding result from multi-
ple speech understanding modules (Katsumaru et
al., 2009). In this paper, we furthermore adapt the
selection module to the amount of available train-
580
LU model
#1
Language
model #1
LU
modules
ASR
modules
Result:
1
CM
N
CM
MN
CM
?
i
i
CMmaxarg
N
1
Selection module
LU
results
ASR
results
Utterance
ASR: automatic speech recognition
LU: language understanding
CM: confidence measure
M ?
M ?
Speech understanding
Language
model #2
Language
model #N
LU model
#2
LU model
#M
Logistic
regression #1
Logistic
regression #N
Logistic
regression #
Figure 1: Overview of speech understanding framework MLMU
ing data. More specifically, the allocation policy
of training data is changed and thus appropriate
LMs and LUMs are selected as its result.
An overview of MLMU is shown in Figure 1.
MLMU uses multiple LMs for ASR and multi-
ple LUMs and selects the most reliable speech un-
derstanding result from all combinations of them.
We denote a speech understanding module as SUi
(i = 1, . . . , n). Its result is a semantic representa-
tion consisting of a set of concepts. The concept is
either a semantic slot and its value or an utterance
type. Note that n = N ? M , when N LMs and
M LUMs are used. The confidence measure per
utterance for a result of i-th speech understanding
module SUi is denoted as CMi. The speech un-
derstanding result having the highest confidence
measure is selected as the final result for the ut-
terance. That is, the result is the output of SUm
where m = argmaxi CMi.
The confidence measure is calculated by logis-
tic regression based on the features of each speech
understanding result. A logistic regression func-
tion is constructed for each speech understanding
module SUi:
CMi =
1
1 + e?(ai1Fi1+...+ai7Fi7+bi) . (1)
Parameters ai1, . . . , ai7 and bi are determined by
using training data. In the training phase, teacher
signal 1 is given when a speech understanding re-
sult is completely correct; that is, when no error is
contained in the result. Otherwise, 0 is given. We
use seven features, Fi1, Fi2, . . . , Fi7, as indepen-
dent variables. Each feature value is normalized
Table 1: Features of speech understanding result
obtained from SUi
Fi1: Acoustic score normalized by utterance length
Fi2: Difference between Fi1 and normalized acoustic
scores of verification ASR
Fi3: Average concept CM in understanding result
Fi4: Minimum concept CM in understanding result
Fi5: Number of concepts in understanding result
Fi6: Whether any understanding result is obtained
Fi7: Whether understanding result is yes/no
CM: confidence measure
so as to make its mean zero and its variance one.
The features used are listed in Table 1. Com-
pared with those used in our previous paper (Kat-
sumaru et al, 2009), we deleted ones that were
highly correlated with other features and added
ones regarding content of the speech understand-
ing results. Features Fi1 and Fi2 are obtained
from an ASR result. Another ASR with a gen-
eral large vocabulary LM is executed for verifying
the i-th ASR result. Fi2 is the difference between
its score and Fi1 (Komatani et al, 2007). These
two features represent the reliability of the ASR
result. Fi3 and Fi4 are calculated for each concept
in the LU result on the basis of the posterior prob-
ability of the 10-best ASR candidates (Komatani
and Kawahara, 2000). Fi5 is the number of con-
cepts in the LU result. This feature is effective be-
cause the LU results of lengthy utterances tend to
be erroneous in a grammar-based LU. Fi6 repre-
sents the case when an ASR result is not accepted
by the subsequent LU module. In such cases, no
speech understanding result is obtained, which is
581
U1: It is June ninth.
ASR result:
- grammar ?It is June ninth.?
- N-gram ?It is June noon and?
LU result:
- grammar + FST ?month:6 day:9 type:refer-time?
- N-gram + WFST ?month:6 type:refer-time?
U2: I will borrow it on twentieth.
(Underlined part is out-of-grammar.)
ASR result:
- grammar ?Around two pm on twentieth.?
- N-gram ?Around two at ten on twentieth.?
LU result:
- grammar + FST ?day:20 hour:14 type:refer-time?
- N-gram + WFST ?day:20 type:refer-time?
Combination of LM and LUM is denoted as ?LM+LUM?.
Figure 2: Example of speech understanding re-
sults in MLMU framework
regarded as an error. Fi7 is added because affirma-
tive and negative responses, typically ?Yes? and
?No?, tend to be correctly recognized and under-
stood.
Figure 2 depicts an example when multiple
ASRs based on LMs and multiple LUs are used.
In short, the correct speech understanding result is
obtained from a different combination of LMs and
LUMs.
4 Automatic Allocation of Training Data
Using Change in Coefficients
The training data need to be allocated to the
speech understanding modules (i.e., statistical LM
and statistical LUM) and the selection module. If
more data are allocated to the ASR and LU mod-
ules, the performances of these modules are im-
proved, but the overall performance is degraded
because of the low performance of the selection
module. On the other hand, even if more training
data are allocated to the selection module, the per-
formance of each ASR and LU module remains
low.
4.1 Allocation Policy
We focus on the convergence of the logistic re-
gression functions when the amount of training
data increases. The convergence is defined as
the change in their coefficients, which will appear
later as Equation 2, and determines two points
1. All data are used to
train selection modules
2. Data are allocated to SU
and selection modules
3. Data are
not divided
No No
Yes
Yes
Selection module 
first converges?
No over-fitting
occurs?
Amount of training data increases
SU: speech understanding
Figure 3: Flowchart of data allocation
during the increase in training data, and thus three
phases are defined. The flowchart of data alloca-
tion is depicted in Figure 3. The three phases are
explained below.
In the first phase, the first priority is given to
the selection module. This is because the lo-
gistic regression functions used in the selection
module converge with relatively less training data
than those in the statistical ASR and LU mod-
ules for speech understanding; there are eight pa-
rameters for each logistic regression function as
shown in Equation 1, far fewer than for other sta-
tistical models such as N-gram and CRF. The out-
put from a speech understanding module that em-
ploys grammar-based LM and LUM would be the
most reliable in many cases because its perfor-
mance is better than that of other statistical mod-
ules when a very small amount of training data is
available. As a result, equivalent or better perfor-
mance would be achieved than methods using a
single ASR module and a single LU module.
In the second phase, the training data are also
allocated to the speech understanding modules af-
ter the selection module converges. This aims
to improve the performance of the speech under-
standing modules by allocating as much training
data to them as possible. The amount of train-
ing data is fixed in this phase to the amount al-
located to the selection module determined in the
first phase. The remaining data are used to train
the speech understanding modules.
When the performances of all the speech under-
standing modules stabilize, the allocation phase
proceeds to the third one. After this point, we
hypothesize that overfitting does not occur in this
phase because plenty of training data are avail-
able. All available data are used to train all mod-
582
ules without dividing the data in this phase.
4.2 Determining When to Switch Allocation
Policies
Automatic switching from one phase to the next
requires the determination of two points in the
number of training utterances: when the selec-
tion module first converges (konlysel) and when
the speech understanding modules all become sta-
ble (knodiv). These points are determined by fo-
cusing on the changes in the coefficients of the
logistic regression functions when the number of
utterances used as training data increases. We ob-
serve the sum of the changes in the coefficients of
the functions and then identify the points at which
the changes converge. The points are determined
individually by the following algorithm.
Step 1 Construct two logistic regression func-
tions for speech understanding module SUi
by using k and (k + ?k) utterances out of
kmax utterances, where kmax is the amount
of training data available.
Step 2 Calculate the change in coefficients from
the two logistic regression functions by
?i(k) =
?
j
|aij(k + ?k) ? aij(k)|
+|bi(k + ?k) ? bi(k)|, (2)
where aij(k) and bi(k) denote the param-
eters of the logistic regression functions,
shown in Equation 1, for speech understand-
ing module SUi, when k utterances are used
to train the functions.
Step 3 If ?i(k) becomes smaller than threshold
?, consider that the training of the functions
has converged, and record this k as the point
of convergence. If not, return to Step 1 after
k ? k + ?k.
The ?k is the minimum unit of training data con-
taining various utterances. We set it as the number
of utterances in one dialogue session, whose aver-
age was 17. Threshold ? was set to 8, which corre-
sponds to the number of parameters in the logistic
regression functions. No experiments were con-
ducted to determine if better performance could
be achieved with other choices of ?1.
The first point, konlysel, is determined using the
speech understanding module that uses no training
data. Specifically, we used ?grammar+FST? as
method SUi. Here, ?LM+LUM? denotes a com-
bination of LM for ASR and LUM. If the func-
tion converges at k utterances, we set konlysel to
k and fix the k utterances as training data used by
the selection module. The remaining (kmax ? k)
utterances are allocated to the speech understand-
ing modules, that is, the LMs and LUMs. Note
that if k becomes equal to kmax before ?i con-
verges, all training data are allocated to the selec-
tion module; that is, no data are allocated to the
LMs and LUMs. In this case, no output is ob-
tained from statistical speech understanding mod-
ules, and only outputs from the grammar-based
modules are used.
The second point, knodiv , is determined on the
basis of the speech understanding module that
needs the largest amount of data for training. The
amount of data needed depends on the number of
parameters. Specifically, we used ?N-gram+CRF?
as SUi in Equation 2. If the function converges,
we hypothesize that the performance of all the
speech understanding modules stabilize and thus
overfitting does not occur. We then stop the divi-
sion of training data, and use all available data to
train the statistical modules.
5 Experimental Evaluation
5.1 Target Data and Implementation
We used a data set previously collected through
actual dialogues with a rent-a-car reservation sys-
tem (Nakano et al, 2007) with 39 participants.
Each participant performed 8 dialogue sessions,
and 5900 utterances were collected in total. Out
of these utterances, we used 5240 for which the
automatic voice activity detection (VAD) results
agreed with manual annotation. We divided the
utterances into two sets: 2121 with 16 participants
as training data and 3119 with 23 participants as
the test data.
1We do not think the value is very critical after seeing the
results shown in Figure 4.
583
We constructed another rent-a-car reservation
system to evaluate our allocation method. The
system included two language models (LMs)
and four language understanding models (LUMs).
That is, eight speech understanding results in total
were obtained. The two LMs were a grammar-
based LM (?grammar?, hereafter) and a domain-
specific statistical LM (?N-gram?). The grammar
model was described by hand to be equivalent to
the FST model used in LU. The N-gram model
was a class 3-gram and was trained on a tran-
scription of the available training data. The vo-
cabulary size was 281 for the grammar model and
420 for the N-gram model when all the training
data were used. The ASR accuracies of the gram-
mar and N-gram models were 67.8% and 90.5%
for the training data and 66.3% and 85.0% for the
test data when all the training data were used. We
used Julius (ver. 4.1.2) as the speech recognizer
and a gender-independent phonetic-tied mixture
model as the acoustic model (Kawahara et al,
2004). We also used a domain-independent statis-
tical LM with a vocabulary size of 60250, which
was trained on Web documents (Kawahara et al,
2004), as the verification model.
The four LUMs were a finite-state transducer
(FST) model, a weighted FST (WFST) model,
a keyphrase-extractor (Extractor) model, and a
conditional random fields (CRF) model. In the
FST-based LUM, the FST was constructed by
hand. The WFST-based LUM is based on the
method developed by Fukubayashi et al (2008).
The WFSTs were constructed by using the MIT
FST Toolkit (Hetherington, 2004). The weight-
ing scheme used for the test data was selected by
using training data (Fukubayashi et al, 2008). In
the extractor-based LUM, as many parts as pos-
sible in the ASR result were simply transformed
into concepts. As the CRF-based LUM, we used
open-source software, CRF++2, to construct the
LUM. As its features, we use a word in the ASR
result, its first character, its last character, and the
ASR confidence of the word. Its parameters were
estimated by using training data.
The metric used for speech understanding per-
formance was concept understanding accuracy,
2http://crfpp.sourceforge.net/
Table 2: Absolute degradation in oracle accuracy
when each module was removed
Case (A) (B)
With all modules (%) 86.6 90.1
w/o grammar ASR -12.0 -1.1
w/o N-gram ASR -6.1 -7.7
w/o FST LUM -0.4 0.0
w/o WFST LUM -1.2 -0.5
w/o Extractor LUM -0.1 0.0
w/o CRF LUM -0.6 -3.7
(w/o FST & Extractor LUMs) -1.0 -0.1
(A): 141 utterances with 1 participant
(B): 2121 utterances with 16 participants
defined as
1 ? SUB + INS + DEL
no. of concepts in correct results,
where SUB, INS, and DEL denote the numbers of
substitution, insertion, and deletion errors.
5.2 Effectiveness of Using Multiple LMs and
LUMs
We investigated how much the performance of our
framework degraded when one ASR or LU mod-
ule was removed. We used the oracle accuracies,
i.e., when the most appropriate result was selected
by hand. The result reveals the contribution of
each ASR and LU module to the performance of
the framework. A module is regarded as more im-
portant when the accuracy is degraded more when
it is removed than when another one is removed.
Two cases (A) and (B) were defined: when the
amount of available training data was (A) small
and (B) large. We used 141 utterances with 1 par-
ticipant for case (A) and 2121 utterances with 16
participants for case (B). The results are shown in
Table 2.
When a small amount of training data was
available (case (A)), the accuracy was degraded by
12.0 points when the grammar-based ASRmodule
was removed and 6.1 points when the N-gram-
based ASR module was removed. The accuracy
was thus degraded substantially when either ASR
module was removed. This indicates that the two
ASR modules work complementarily.
584
020
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(a) grammar+FST
0
20
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(b) N-gram+CRF
Figure 4: Change in the sum of coefficients ?i when amount of training data increases (?LM+LUM?
denotes combination of LM and LUM)
On the other hand, when a large amount of
training data was available (case (B)), the ac-
curacy was degraded by 1.1 points when the
grammar-based ASR was removed. This means
that it became less important when there are
plenty of training data because the coverage of the
N-gram-based ASR became wider. In short, espe-
cially when the amount of training data is smaller,
speech understanding modules based on a hand-
crafted grammar are more important because of
the low performance of statistical modules.
Concerning the LUMs, the accuracy was de-
graded when any of the LUM modules was re-
moved when a small amount of training data was
available. When a large amount of training data
was available, the module based on CRF in par-
ticular became more important.
5.3 Results and Evaluation of Automatic
Allocation
Figure 4 shows the change in the sum of the co-
efficients, ?i, with the increase in the amount of
training data. In Figure 4(a), the change was very
large while the amount of training data was small,
and decreased dramatically and converged around
one hundred utterances. By applying ? (=8) to ?i,
we set 111 utterances as the first point, konlysel,
up to which all the training data are allocated to
the selection module, as described in Section 4.1.
Similarly, from the results shown in Figure 4(b),
we set 207 utterances as the second point, knodiv,
from which the training data are not divided.
To evaluate our method for allocating training
55
60
65
70
75
80
85
90
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
Our method
Na?ve allocation
No division
Figure 5: Results of allocation methods
data, we compared it with two baseline methods:
? No-division method: All data available at
each point were used to train both the speech
understanding modules and the selection
module. That is, the same data set was used
to train them.
? Naive-allocation method: Training data
available at each point were allocated equally
to the speech understanding modules and the
selection module.
As shown in Figure 5, our method had the best
concept understanding accuracy when the amount
of training data was small, that is, up to about
278 utterances. This indicates that our method for
allocating the available training data is effective
when the amount of training data is small.
This result is explained more specifically by us-
585
Table 3: Concept understanding accuracy for 141
utterances
Accuracy (%)
Our method 77.9
Naive allocation 73.5
No division 74.1
ing the case in which 141 utterances were used as
the training data. 111 (= konlysel) were secured to
train the selection module and 30 utterances were
allocated to train the speech understanding mod-
ules. As shown in Table 3, the accuracy with our
method was 3.8 points higher than that with the
no-division baseline method. This was achieved
by avoiding the overfitting of the logistic regres-
sion functions; i.e., the data input to the functions
became similar to the test data due to allocation,
so the concept understanding accuracy for the test
set was improved. The accuracy with our method
was 4.4 points higher than that with the naive al-
location baseline method. This was because the
amount of training data allocated to the selection
module was less than our method, and accordingly
the selection module was not trained sufficiently.
5.4 Comparison with methods using a single
ASR and a single LU
Figure 6 plots concept understanding accuracy
with our method against baseline methods using
a single ASR module and a single LU module for
various amounts of training data. Each module for
comparison was constructed by using all available
training data at each point while training data in-
creased; i.e., the same condition as our method.
The accuracies of only three speech understand-
ing modules are shown in the figure, out of the
eight obtained by combining two LMs for ASR
and four LUMs. These three are the ones with the
highest accuracies while the amount of training
data increased. Our method switched the alloca-
tion phase at 111 and 207 utterances, as described
in Section 5.3.
Our method performed equivalently or better
than all baseline methods even when only a small
amount of training data was available. As a result,
our method outperformed all the baseline methods
55
60
65
70
75
80
85
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
our method
grammar+FST
N-gram+WFST
N-gram+CRF
Figure 6: Comparison with baseline methods us-
ing single speech understanding
at every point while training data increase.
6 Conclusion
We developed a method to automatically allo-
cate training data to statistical modules so as to
avoid performance degradation caused by overfit-
ting. Experimental evaluation showed that speech
understanding accuracies achieved by our method
were equivalent or better than the baseline meth-
ods based on all combinations of a single ASR
module and a single LU module at every point
while training data increase. This includes a case
when a very small amount of training data is avail-
able. We also showed empirically that the training
data should be allocated while an amount of train-
ing data is not sufficient. Our method allocated
available training data on the basis of our alloca-
tion policy described in Section 4.1, and outper-
formed the two baselines where the training data
were equivalently allocated and not allocated.
When plenty of training data were available,
there was no difference between our method and
the speech understanding method that requires the
most training data, i.e., N-gram+CRF, as shown in
Figure 6. It is possible that our method combin-
ing multiple speech understanding modules would
outperform it as Schapire et al (2005) reported.
In their data, there were some examples that only
a hand-crafted rules can parse. Including such a
task as more complicated language understanding
grammar is required, verification of our method in
other tasks is one of the future works.
586
References
Dinarelli, Marco, Alessandro Moschitti, and Giuseppe
Riccardi. 2009. Re-Ranking Models for Spoken
Language Understanding. In Proc. European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 202?210.
Fukubayashi, Yuichiro, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tet-
suya Ogata, and Hiroshi G. Okuno. 2008. Rapid
prototyping of robust language understanding mod-
ules for spoken dialogue systems. In Proc. Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP), pages 210?216.
Hahn, Stefan, Patrick Lehnen, and Hermann Ney.
2008. System Combination for Spoken Language
Understanding. In Proc. Annual Conference of the
International Speech Communication Association
(INTERSPEECH), pages 236?239.
Hetherington, Lee. 2004. The MIT Finite-State Trans-
ducer Toolkit for Speech and Language Processing.
In Proc. Int?l Conf. Spoken Language Processing
(ICSLP), pages 2609?2612.
Jeong, Minwoo and Gary Geunbae Lee. 2006. Ex-
ploiting non-local features for spoken language un-
derstanding. In Proc. COLING/ACL 2006 Main
Conference Poster Sessions, pages 412?419.
Katsumaru, Masaki, Mikio Nakano, Kazunori Ko-
matani, Kotaro Funakoshi, Tetsuya Ogata, and Hi-
roshi G. Okuno. 2009. Improving speech un-
derstanding accuracy with limited training data us-
ing multiple language models and multiple under-
standing models. In Proc. Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pages 2735?2738.
Kawahara, Tatsuya, Akinobu Lee, Kazuya Takeda,
Katsunobu Itou, and Kiyohiro Shikano. 2004. Re-
cent progress of open-source LVCSR engine Julius
and Japanese model repository. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), pages 3069?
3072.
Komatani, Kazunori and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management us-
ing concept-level confidence measures of speech
recognizer output. In Proc. Int?l Conf. Computa-
tional Linguistics (COLING), pages 467?473.
Komatani, Kazunori, Yuichiro Fukubayashi, Tetsuya
Ogata, and Hiroshi G. Okuno. 2007. Introducing
utterance verification in spoken dialogue system to
improve dynamic help generation for novice users.
In Proc. 8th SIGdial Workshop on Discourse and
Dialogue, pages 202?205.
Nakano, Mikio, Yuka Nagano, Kotaro Funakoshi,
Toshihiko Ito, Kenji Araki, Yuji Hasegawa, and Hi-
roshi Tsujino. 2007. Analysis of user reactions to
turn-taking failures in spoken dialogue systems. In
Proc. 8th SIGdial Workshop on Discourse and Dia-
logue, pages 120?123.
Raymond, Christian and Giuseppe Riccardi. 2007.
Generative and Discriminative Algorithms for Spo-
ken Language Understanding. In Proc. Annual
Conference of the International Speech Communi-
cation Association (INTERSPEECH), pages 1605?
1608.
Shapire, Robert E., Marie Rochery, Mazin Rahim, and
Narendra Gupta. 2005. Boosting with prior knowl-
edge for call classification. IEEE Trans. on Speech
and Audio Processing, 13(2):174?181.
Wang, Ye-Yi and Alex Acero. 2006. Discrimina-
tive models for spoken language understanding. In
Proc. Int?l Conf. Spoken Language Processing (IN-
TERSPEECH), pages 2426?2429.
Wang, Ye-Yi, Alex Acero, Ciprian Chelba, Brendan
Frey, and Leon Wong. 2002. Combination of Sta-
tistical and Rule-based Approaches for Spoken Lan-
guage Understanding. In Proc. Int?l Conf. Spoken
Language Processing (ICSLP), pages 609?612.
587
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 289?296,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Online Error Detection of Barge-In Utterances by Using
Individual Users? Utterance Histories in Spoken Dialogue System
Kazunori Komatani? Hiroshi G. Okuno
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,okuno}@kuis.kyoto-u.ac.jp
Abstract
We develop a method to detect erroneous
interpretation results of user utterances
by exploiting utterance histories of indi-
vidual users in spoken dialogue systems
that were deployed for the general pub-
lic and repeatedly utilized. More specifi-
cally, we classify barge-in utterances into
correctly and erroneously interpreted ones
by using features of individual users? utter-
ance histories such as their barge-in rates
and estimated automatic speech recogni-
tion (ASR) accuracies. Online detection
is enabled by making these features ob-
tainable without any manual annotation
or labeling. We experimentally compare
classification accuracies for several cases
when an ASR confidence measure is used
alone or in combination with the features
based on the user?s utterance history. The
error reduction rate was 15% when the ut-
terance history was used.
1 Introduction
Many researchers have tackled the problem of
automatic speech recognition (ASR) errors by
developing ASR confidence measures based on
utterance-level (Komatani and Kawahara, 2000)
or dialogue-level information (Litman et al, 1999;
Walker et al, 2000; Hazen et al, 2000). Especially
in systems deployed for the general public such as
those of (Komatani et al, 2005; Raux et al, 2006),
the systems need to correctly detect interpretation
errors caused by various utterances made by var-
ious users, including novices. Error detection us-
ing individual user models would be a promising
way of improving performance in such systems
?Currently with Graduate School of Engineering, Nagoya
University, Furo-cho, Chikusa-ku, Nagoya 464-8603, Japan.
komatani@nuee.nagoya-u.ac.jp
because users often access them repeatedly (Ko-
matani et al, 2007).
We choose to detect interpretation errors of
barge-in utterances, mostly caused by ASR er-
rors, as a task for showing the effectiveness of
the user?s utterance histories. We try to improve
the accuracy of classifying barge-in utterances into
correctly and erroneously interpreted ones with-
out any manual labeling. By classifying utter-
ances accurately, the system can reduce erroneous
responses caused by the errors and unnecessary
confirmations. Here, a ?barge-in utterance? is a
user utterance that interrupts the system?s prompt.
In this situation, the system stops its prompt and
starts recognizing the user utterance.
In this study, we combine the ASR confidence
measure with features obtained from the user?s ut-
terance history, i.e., the estimated ASR accuracy
and the barge-in rate, to detect interpretation er-
rors of barge-in utterances. We show that the fea-
tures are still effective when they are used together
with the ASR confidence measure, which is usu-
ally used to detect erroneous ASR results. The
characteristics of our method are summarized as
follows:
1. The user?s utterance history used as his/her
profile: The user?s current barge-in rate and
ASR accuracy are used for error detection.
2. Online user modeling: We try to obtain the
user profiles listed above without any man-
ual labeling after the dialogue has been com-
pleted. This means that the system can im-
prove its performance while it is deployed.
In our earlier report (Komatani and Rudnicky,
2009), we defined the estimated ASR accuracy
and showed that it is helpful in improving the ac-
curacy of classifying barge-in utterances into cor-
rectly and erroneously interpreted ones, by using it
in conjunction with the user?s barge-in rate. In this
289
Table 1: ASR accuracy per barge-in
Correct Incorrect Total Accuracy
w/o barge-in 16,694 3,612 20,306 (82.2%)
w/ barge-in 3,281 3,912 7,193 (45.6%)
Total 19,975 7,524 27,499 (72.6%)
report, we verify our approach when the ASR con-
fidence measure is also incorporated into it. Thus,
we show the individual user?s utterance history is
helpful as a user profile and works as prior infor-
mation for the ASR confidence.
2 Barge-in Utterance and its Errors
Barge-in utterances were often incorrectly inter-
preted mainly because of ASR errors in our data
as shown in Table 1. The table lists the ASR
accuracy per utterance for two cases: when the
system prompts were played to the end (denoted
as ?w/o barge-in?) and when the system prompts
were barged in (?w/ barge-in?). Here, an utter-
ance is assumed to be correct only when all con-
tent words in the utterance are correctly recog-
nized; one is counted as an error if any word in it
is misrecognized. Table 1 shows that barge-in ut-
terances amounted to 26.2% (7,193/27,499) of all
utterances, and half of those utterances contained
ASR errors in their content words.
This result implies that many false barge-ins oc-
curred despite the user?s intention. Specifically,
the false barge-ins included instances when back-
ground noises were incorrectly regarded as barge-
ins and the system?s prompt stopped. Such in-
stances often occur when the user accesses the
system using mobile phones in crowded places.
Breathing and whispering were also prone to be
incorrectly regarded as barge-ins. Moreover, dis-
fluency in one utterance may be unintentionally di-
vided into two portions, which causes further mis-
recognitions and unexpected system actions. The
abovementioned phenomena, except background
noises, are caused by the user?s unfamiliarity with
the system. That is, some novice users are not
unaware of the timing at which to utter, and this
causes the system to misrecognize the utterance.
On the other hand, users who have already become
accustomed to the system often use the barge-
in functions intentionally and, accordingly, make
their dialogues more efficient.
The results in Table 2 show the relationship be-
tween barge-in rate per user and the correspond-
ing ASR accuracies of barge-in utterances. We
Table 2: ASR accuracy of barge-in utterances for
different barge-in rates
Barge-in rate Correct Incorrect Acc. (%)
0.0 - 0.2 407 1,750 18.9
0.2 - 0.4 205 842 19.6
0.4 - 0.6 1,602 880 64.5
0.6 - 0.8 1,065 388 73.3
0.8 - 1.0 2 36 5.3
1.0 0 16 0.0
Total 3,281 3,912 45.6
here ignore a small number of users whose barge-
in rates were greater than 0.8, which means al-
most all utterances were barge-ins, because most
of their utterances were misrecognized because
of severe background noises and accordingly they
gave up using the system. We thus focus on users
whose barge-in rates were less than 0.8. The ASR
accuracy of barge-in utterances was high for users
who frequently barged-in. This suggests that the
barge-ins were intentional. On the other hand, the
ASR accuracies of barge-in utterances were less
than 20% for users whose barge-in rates were less
than 0.4. This suggests that the barge-ins of these
users were unintentional.
A user study conducted by Rose and
Kim (2003) revealed that there are many more
disfluencies when users barge in compared with
when users wait until the system prompt ends.
Because such disfluencies and resulting utterance
fragments are parts of human speech, it is difficult
to select erroneous utterances to be rejected by
using a classifier that distinguishes speech from
noise on the basis of the Gaussian Mixture Model
(Lee et al, 2004). These errors cannot be detected
by using only bottom-up information obtained
from single utterances such as acoustic features
and ASR results.
To cope with the problem, we use individual
users? utterance histories as their profiles. More
specifically, we use each user?s average barge-in
rate and ASR accuracy from the time the user
started using the system until the current utterance.
The barge-in rate intuitively corresponds to the de-
gree to which the user is accustomed to using the
system, especially to using its barge-in function.
That is, this reflects the tendency shown in Table
2; that is, the ASR accuracy of barge-in utterances
is higher for users whose barge-in rates are higher.
Each user?s ASR accuracy also indicates the user?s
habituation. This corresponds to an empirical ten-
dency that ASR accuracies of more accustomed
290
time
A user?s
utterances
current
Utterance history
Classification:
Current utterance is 
?Correctly interpreted?
?Erroneous?
2. the user?s ASR accuracy
1. the user?s barge-in rate
3. ASR confidence
Figure 1: Overview of detecting interpretation errors
users are higher (Komatani et al, 2007; Levow,
2003). To account for another fact that some ex-
pert users have low barge-in rates, and, accord-
ingly, not all expert users barge in frequently (Ko-
matani et al, 2007), we use both the user?s barge-
in rate and ASR accuracy to represent degree of
habituation, and verify their effectiveness as prior
information for detecting erroneous interpretation
results when they are used together with an ASR
confidence measure.
To obtain the user?s ASR accuracy without any
manual labeling, we exploit certain dialogue pat-
terns indicating that ASR results at certain po-
sitions are reliable. For example, Sudoh and
Nakano (2005) proposed a ?post-dialogue con-
fidence scoring? in which ASR results corre-
sponding to the user?s intention upon dialogue
completion are assumed to be correct and are
used for confidence scoring. Bohus and Rud-
nicky (2007) proposed ?implicitly supervised
learning? in which user responses following the
system?s explicit confirmations are used for confi-
dence scoring. If the ASR results can be regarded
as reliable after the dialogue, machine learning al-
gorithms can use them as teacher signals. This ap-
proach does not need any manual labeling or tran-
scription, a task which requires much time and la-
bor when spoken dialogue systems are being de-
veloped. We focus on users? affirmative and neg-
ative responses to the system?s explicit confirma-
tions, and estimated the user?s ASR accuracy on
the basis of his or her history of responses (Ko-
matani and Rudnicky, 2009). This estimated ASR
accuracy can be also used as an online feature rep-
resenting a user?s utterance history.
3 Detecting Errors by using the User?s
Utterance History
We detect interpretation errors of barge-in utter-
ances by using the following three information
sources:
1. the current user?s barge-in rate,
2. the current user?s ASR accuracy, and
3. ASR confidence of the current utterance.
The error detection method is depicted in Figure
1. Barge-in rate and ASR accuracy are accumu-
lated and averaged from the beginning until the
current utterance and are used as each user?s ut-
terance history. Then, at every point a user makes
an utterance, the barge-in utterances are classified
into correctly or erroneously interpreted ones by
using a logistic regression function:
P =
1
1 + exp(?(a
1
x
1
+ a
2
x
2
+ a
3
x
3
+ b))
,
(1)
where x
1
, x
2
and x
3
denote the barge-in rate, the
ASR accuracy until the current utterance, and the
ASR confidence measure of the current utterance,
respectively. Coefficients a
i
and b are determined
by 10-fold cross validation on evaluation data. In
the following subsections, we describe how to ob-
tain these features.
3.1 Barge-In Rate
The barge-in rate is defined as the ratio of the num-
ber of barge-in utterances to all the user?s utter-
ances until the current utterance. Note that the
current utterance itself is included in this calcula-
tion. We confirmed that the barge-in rate changes
as the user becomes accustomed to the system
291
U1: 205. (Number 100)
S1: Will you use bus number 100?
U2: No. (No)
S2: Please tell me your bus stop or bus route number.
U3: Nishioji Matsu... [disfluency] (Rejected)
S3: Please tell me your bus stop or bus route number.
U4: From Nishioji Matsubara. (From Nishioji Matsubara)
S4: Do you get on a bus at Nishioji Matsubara?
U5: Yes. (Yes)
Initial characters ?U? and ?S? denote the user and system utterance.
A string in parentheses denotes the ASR result of the utterance.
Figure 2: Example dialogue
(Komatani et al, 2007). To take these tempo-
ral changes into consideration, we set a window
when calculating the rate (Komatani et al, 2008).
That is, when the window width is N , the rate is
calculated on the basis of only the last N utter-
ances, and utterances before those ones are dis-
carded. When the window width exceeds the total
number of utterances by the user, the barge-in rate
is calculated on the basis of all the user?s utter-
ances. Thus, when the width exceeds 2,838, the
maximum number of utterances made by one user
in our data, the barge-in rates equal the average
rates of all utterances by the user.
3.2 ASR Accuracy
ASR accuracy is calculated per utterance. It is de-
fined as the ratio of the number of correctly rec-
ognized utterances to all the user?s utterances until
the previous utterance. Note that the current utter-
ance is not included in this calculation. The ?cor-
rectly recognized? utterance denotes a case when
every content word in the ASR result of the ut-
terance was correctly recognized and no content
word was incorrectly inserted. The ASR accuracy
of the user?s initial utterance is regarded as 0, be-
cause there is no utterance before it. We do not set
any window when calculating the ASR accuracies,
because classification accuracy did not improve as
a result of setting one (Komatani and Rudnicky,
2009). This is because each users? ASR accura-
cies tend to converge faster than the barge-in rates
do (Komatani et al, 2007), and the changes in the
ASR accuracies are relatively small in comparison
with those of the barge-in rates.
We use two kinds of ASR accuracies:
1. actual ASR accuracy and
2. estimated ASR accuracy (Komatani and Rud-
nicky, 2009).
The actual ASR accuracy is calculated from man-
ual transcriptions for investigating the upper limit
of improvement of the classification accuracy
when ASR accuracy is used. Thus, it cannot be
obtained online because manual transcriptions are
required.
The estimated ASR accuracy is calculated on
the basis of the user?s utterance history. This is
obtainable online, that is, without the need for
manual transcriptions after collecting the utter-
ances. We focus on users? affirmative or negative
responses following the system?s explicit confir-
mations, such as ?Leaving from Kyoto Station. Is
that correct?? To estimate the accuracy, we make
three assumptions as follows:
1. The ASR results of the users? affirmative or
negative responses are correctly recognized.
This assumption will be verified in Section
4.2.
2. A user utterance corresponding to the content
of the affirmative responses is also correctly
recognized, because the user affirms the sys-
tem?s explicit confirmation for it.
3. The remaining utterances are not correctly
recognized. This corresponds to when users
do not just say ?no? in response to explicit
confirmations with incorrect content and in-
stead use other expressions.
To summarize the above, we assume that the
ASR results of the following utterances are cor-
rect: an affirmative response, its corresponding ut-
terance which is immediately preceded by it, and
292
Table 3: Distribution of ASR confidence measures
for barge-in utterances
Confidence measure Correct Incorrect (%)
0.0 - 0.1 0 1491 0.0
0.1 - 0.2 0 69 0.0
0.2 - 0.3 0 265 0.0
0.3 - 0.4 0 708 0.0
0.4 - 0.5 241 958 20.1
0.5 - 0.6 639 333 65.7
0.6 - 0.7 1038 68 93.9
0.7 - 0.8 1079 20 98.2
0.8 - 0.9 284 0 100.0
0.9 - 1.0 0 0 ?
Total 3281 3912 45.6
a negative response. All other utterances are as-
sumed to be incorrect. We thus calculate the user?s
estimated ASR accuracy as follows:
(Estimated ASR accuracy)
=
2 ? (#affirmatives) + (#negatives)
(#all utterances) (2)
Here is an example of the calculation for the ex-
ample dialogue shown in Figure 2. U2 is a neg-
ative response, and U5 is an affirmative response.
When the dialogue reaches the point of U5, U2
and U5 are regarded as correctly recognized on the
basis of the first assumption. Next, U4 is regarded
as correct on the basis of the second assumption,
because the explicit confirmation for it (S4) was
affirmed by the user as U5. Then, the remaining
U1 and U3 are regarded as misrecognized on the
basis of the third assumption. As a result, the esti-
mated ASR accuracy at U5 is 60%.
The estimated ASR accuracy is updated for ev-
ery affirmative or negative response by the user.
For a neither affirmative nor negative response, the
latest estimated accuracy before it was used in-
stead.
3.3 ASR Confidence Measure
We use an ASR confidence measure calculated per
utterance. Specifically, we use the one derived
from the ASR engine in the Voice Web Server, a
product of Nuance Communications, Inc.1
Table 3 shows the distribution of ASR confi-
dence measures for barge-in utterances. By us-
ing this ASR confidence, even a naive method can
have high classification accuracy (90.8%) in which
just one threshold (? = 0.516) is set and utter-
ances whose confidence measure is greater than
1http://www.nuance.com/
Table 4: ASR accuracy by user response type
Correct Incorrect Total (Acc.)
Affirmative 9,055 243 9,298 (97.4%)
Negative 2,006 286 2,292 (87.5%)
Other 8,914 6,995 15,909 (56.0%)
Total 19,975 7,524 27,499 (72.6%)
the threshold are accepted. This accuracy is re-
garded as the baseline.
4 Experimental Evaluation
4.1 Data
We used data collected by the Kyoto City Bus In-
formation System (Komatani et al, 2005). This
system locates a bus that a user wants to ride and
tells the user how long it will be before the bus
arrives. The system was accessible to the public
by telephone. It adopted the safest strategy to pre-
vent erroneous responses; that is, it makes explicit
confirmations for every user utterance except for
affirmative or negative responses such as ?Yes? or
?No?.
We used 27,499 utterances that did not involve
calls whose phone numbers were not recorded or
those the system developer used for debugging.
The data contained 7,988 valid calls from 671
users. Out of these, there were 7,193 barge-in ut-
terances (Table 1). All the utterances were manu-
ally transcribed for evaluation; human annotators
decided whether every content word in the ASR
results was correctly recognized or not.
The phone numbers of most of the calls were
recorded, and we assumed that each number cor-
responded to one individual. Most of the numbers
were those of mobile phones, which are usually
not shared; thus, the assumption seems reasonable.
4.2 Verifying Assumption in Calculating
Estimated ASR Accuracy
We confirmed our assumption that the ASR re-
sults of affirmative or negative responses follow-
ing explicit confirmations are correct. We clas-
sified the user utterances into affirmatives, nega-
tives, and other, and calculated the ASR accura-
cies (precision rates) per utterance as shown in Ta-
ble 4. Affirmatives include hai (?yes?), soudesu
(?that?s right?), OK, etc; and negatives include iie
(?no?), chigaimasu (?I don?t agree?), dame (?No
good?), etc. The table indicates that the ASR ac-
curacies of affirmatives and negatives were high.
One of the reasons for the high accuracy was that
293
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
Es
tim
at
ed
 A
SR
 A
cc
ur
ac
y
Actual ASR Accuracy
Figure 3: Correlation between actual and esti-
mated ASR accuracy
these utterances are much shorter than other con-
tent words, so they were less confused with other
content words. Another reason was that the system
often gave help messages such as ?Please answer
yes or no.?
We then analyzed the correlation between the
actual ASR accuracy and the estimated ASR accu-
racy based on Equation 2. We plotted the two ASR
accuracies (Figure 3) for 26,231 utterances made
after at least one affirmative/negative response by
the user. The correlation coefficient between them
was 0.806. Although the assumption that all ASR
results of affirmative/negative responses are cor-
rect might be rather strong, the estimated ASR ac-
curacy had a high correlation with the actual ASR
accuracy.
4.3 Comparing Classification Accuracies
When the Used Features Vary
We investigated the classification accuracy of the
7,193 barge-in utterances. The classification accu-
racies are shown in Table 5 in descending order for
various sets of features x
i
used as input into Equa-
tion 1. The conditions for when barge-in rates are
used also show the window width w for the high-
est classification accuracy. The mean average er-
ror (MAE) is also listed, which is the average of
the differences between an output of the logistic
regression function X
j
and a reference label man-
ually given X?
j
(0 or 1):
MAE =
1
m
m
?
j
|X?
j
? X
j
|, (3)
where m denotes the total number of barge-in ut-
terances. This indicates how well the output of
90
90.5
91
91.5
92
92.5
1 10 100 1000
Cl
as
sif
ica
tio
n 
Ac
cu
ra
cy
 [%
]
Window width (# utterance)
(1) actual ASR + barge-in + CM
(2) estimated ASR + barge-in + CM
(4) barge-in + CM
(6) only CM
Figure 4: Classification accuracy when window
width varies used to calculate barge-in rate
the logistic regression function (Equation 1) dis-
tributes. Regarding Condition (12) in Table 5 (ma-
jority baseline), the MAE was calculated by as-
suming X
j
= 0.456, which is the average ASR
accuracy, for all j. Its classification accuracy is
the majority baseline; that is, all interpretation re-
sults are regarded as incorrect.
4.4 Experimental Results
The results are shown in Table 5. First, we can see
that the classification accuracies for Conditions (1)
to (6) are high because the ASR confidence mea-
sure (CM) works well (Table 3). The MAEs are
also small, which means the outputs of the logis-
tic regression functions are good indicators of the
reliability of the interpretation result.
Upon comparing Condition (6) with Conditions
(1) to (5), we can see that the classification accura-
cies improve as a result of incorporating the user?s
utterance histories such as barge-in rates and ASR
accuracies. Table 6 lists p-values of the differences
when the barge-in rate and the estimated ASR ac-
curacy were used in addition to the CM. The sig-
nificance test was based on the McNemar test. As
shown in the table, all the differences were statisti-
cally significant (p < 0.01). That is, it was exper-
imentally shown that these utterance histories of
users are different information sources from those
of single utterances and that they contribute to
improving the classification accuracy even when
used together with ASR confidence measures. The
relative improvement in the error reduction rate
was 15.2% between Conditions (2) and (6), that is,
by adding the barge-in rate and the estimated ASR
accuracy, both of which can be obtained without
manual labeling.
294
Table 5: Best classification accuracy for each condition and optimal window width
Conditions Window Classification MAE
(features used) width accuracy (%)
(1) CM + barge-in rate + actual ASR acc. w=40 92.6 0.112
(2) CM + barge-in rate + estimated ASR acc w=30 92.2 0.119
(3) CM + actual ASR acc. - 91.7 0.121
(4) CM + barge-in rate w=30 91.6 0.126
(5) CM + estimated ASR acc. - 91.2 0.128
(6) CM - 90.8 0.134
(7) barge-in rate + actual ASR acc. w=50 80.0 0.312
(8) barge-in rate + estimated ASR acc. w=50 77.7 0.338
(9) actual ASR acc. - 72.8 0.402
(10) barge-in rate w=30 71.8 0.404
(11) estimated ASR acc. - 57.6 0.431
(12) majority baseline - 54.4 0.496
CM: confidence measure
MAE: mean absolute error
Table 6: Results of significance test
Condition pair p-value
(2) vs (4) 0.00066
(2) vs (5) 0.00003
(4) vs (6) 0.00017
(5) vs (6) 0.00876
Figure 4 shows the results in more detail; the
classification accuracies for Conditions (1), (2),
(4), and (6) are shown for various window widths.
Under Condition (6), the classification accuracy
does not depend on the window width because the
barge-in rate is not used. Under Conditions (1),
(2), and (4), the accuracies depend on the window
width for the barge-in rate and are highest when
the width is 30 or 40. These results show the effec-
tiveness of the window, which indicates that tem-
poral changes in user behaviors should be taken
into consideration, and match those of our earlier
reports (Komatani et al, 2008; Komatani and Rud-
nicky, 2009): the user?s utterance history becomes
effective after he/she uses the system about ten
times because the average number of utterances
per dialogue is around five.
By comparing Conditions (2) and (4), we can
see that the classification accuracy improves af-
ter adding the estimated ASR accuracy to Condi-
tion (4). This shows that the estimated ASR accu-
racy also contributes to improving the classifica-
tion accuracy. By comparing Conditions (1) and
(2), we can see that Condition (1), in which the ac-
tual ASR accuracy is used, outperforms Condition
(2), in which the estimated one is used. This sug-
gests that the classification accuracy, whose upper
limit is Condition (1), can be improved by making
the ASR accuracy estimation shown in Section 3.2
more accurate.
5 Conclusion
We described a method of detecting interpretation
errors of barge-in utterances by exploiting the ut-
terance histories of individual users, such as their
barge-in rate and ASR accuracy. The estimated
ASR accuracy as well as the barge-in rate and
the ASR confidence measure is obtainable online.
Thus, the detection method does not require man-
ual labeling. We showed through experiments that
the utterance history of each user is helpful for de-
tecting interpretation errors even when the ASR
confidence measure is used.
The proposed method is effective in systems
that are repeatedly used by the same user over 10
times, as indicated by the results of Figure 4. It is
also assumed that the user?s ID is known (we used
their telephone number). The part of our method
that estimates the user?s ASR accuracy assumes
that the system?s dialogue strategy is to make ex-
plicit confirmations about every utterance by the
user and that all affirmative and negative responses
followed by explicit confirmations are correctly
recognized. Our future work will attempt to re-
duce or remove these assumptions and to enhance
the generality of our method. The experimental
295
result was shown only in the Kyoto City Bus do-
main, in which dialogues were rather well struc-
tured. Experimental evaluations in other domains
will assure the generality.
Acknowledgments
We are grateful to Prof. Tatsuya Kawahara of Ky-
oto University who led the Kyoto City Bus Infor-
mation System project. The evaluation data used
in this study was collected during the project. This
research was partly supported by Grants-in-Aid
for Scientific Research (KAKENHI).
References
DanBohus andAlexander Rudnicky. 2007. Implicitly-
supervised learning in spoken language interfaces:
an application to the confidence annotation problem.
In Proc. SIGdial Workshop on Discourse and Dia-
logue, pages 256?264.
Timothy J. Hazen, Theresa Burianek, Joseph Polifroni,
and Stephanie Seneff. 2000. Integrating recognition
confidence scoring with language understanding and
dialogue modeling. In Proc. Int?l Conf. Spoken Lan-
guage Processing (ICSLP), pages 1042?1045, Bei-
jing, China.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management us-
ing concept-level confidence measures of speech
recognizer output. In Proc. Int?l Conf. Computa-
tional Linguistics (COLING), pages 467?473.
Kazunori Komatani and Alexander I. Rudnicky.
2009. Predicting barge-in utterance errors by using
impricitly-supervised asr accuracy and barge-in rate
per user. In Proc. ACL-IJCNLP, pages 89?92.
Kazunori Komatani, Shinichi Ueno, Tatsuya Kawa-
hara, and Hiroshi G. Okuno. 2005. User modeling
in spoken dialogue systems to generate flexible guid-
ance. User Modeling andUser-Adapted Interaction,
15(1):169?183.
Kazunori Komatani, Tatuya Kawahara, and Hiroshi G.
Okuno. 2007. Analyzing temporal transition of
real user?s behaviors in a spoken dialogue sys-
tem. In Proc. Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 142?145.
Kazunori Komatani, Tatuya Kawahara, and Hiroshi G.
Okuno. 2008. Predicting asr errors by exploiting
barge-in rate of individual users for spoken dialogue
systems. In Proc. Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 183?186.
Akinobu Lee, Keisuke Nakamura, Ryuichi Nisimura,
Hiroshi Saruwatari, and Kiyohiro Shikano. 2004.
Noice robust real world spoken dialogue system us-
ing GMM based rejection of unintended inputs. In
Proc. Int?l Conf. Spoken Language Processing (IC-
SLP), pages 173?176.
Gina-Anne Levow. 2003. Learning to speak to a spo-
ken language system: Vocabulary convergence in
novice users. In Proc. 4th SIGdial Workshop on Dis-
course and Dialogue, pages 149?153.
Diane J. Litman, Marilyn A. Walker, and Michael S.
Kearns. 1999. Automatic detection of poor speech
recognition at the dialogue level. In Proc. Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 309?316.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year of
Let?s Go! experience. In Proc. Int?l Conf. Spoken
Language Processing (INTERSPEECH).
Richard C. Rose and Hong Kook Kim. 2003. A hy-
brid barge-in procedure for more reliable turn-taking
in human-machine dialog systems. In Proc. IEEE
Automatic Speech Recognition and Understanding
Workshop (ASRU), pages 198?203.
Katsuhito Sudoh and Mikio Nakano. 2005. Post-
dialogue confidence scoring for unsupervised statis-
tical language model training. Speech Communica-
tion, 45:387?400.
Marilyn Walker, Irene Langkilde, Jerry Wright, Allen
Gorin, and Diane Litman. 2000. Learning to predict
problematic situations in a spoken dialogue system:
Experiments with How May I Help You? In Proc.
North American Chapter of Association for Compu-
tational Linguistics (NAACL), pages 210?217.
296
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 18?29,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
A Two-Stage Domain Selection Framework for Extensible Multi-Domain
Spoken Dialogue Systems
Mikio Nakano
Honda Research Institute Japan
Wako, Saitama, Japan
nakano@jp.honda-ri.com
Shun Sato
Tokyo Denki University
Hatoyama, Saitama, Japan
rela.relakuma@gmail.com
Kazunori Komatani
Nagoya University
Nagoya, Aichi, Japan
komatani@nuee.nagoya-u.ac.jp
Kyoko Matsuyama?
Kyoto University
Kyoto, Kyoto, Japan
matuyama@kuis.kyoto-u.ac.jp
Kotaro Funakoshi
Honda Research Institute Japan
Wako, Saitama, Japan
funakoshi@jp.honda-ri.com
Hiroshi G. Okuno
Kyoto University
Kyoto, Kyoto, Japan
okuno@i.kyoto-u.ac.jp
Abstract
This paper describes a general and effective
domain selection framework for multi-domain
spoken dialogue systems that employ dis-
tributed domain experts. The framework con-
sists of two processes: deciding if the current
domain continues and estimating the probabil-
ities for selecting other domains. If the current
domain does not continue, the domain with
the highest activation probability is selected.
Since those processes for each domain expert
can be designed independently from other ex-
perts and can use a large variety of informa-
tion, the framework achieves both extensibil-
ity and robustness against speech recognition
errors. The results of an experiment using
a corpus of dialogues between humans and
a multi-domain dialogue system demonstrate
the viability of the proposed framework.
1 Introduction
As spoken dialogue interfaces are becoming more
widely utilized, they will be expected to be able to
engage in dialogues in a wide variety of topics. Par-
ticularly, spoken dialogue interfaces for office robots
(Asoh et al, 1999) and multimodal kiosk systems
(Gustafson and Bell, 2000) are expected to deal with
people?s various requests, unlike automated call cen-
ter systems that are dedicated to specific tasks.
One effective methodology to build such a sys-
tem is to integrate systems in small domains by
employing distributed multi-domain system archi-
tecture. This architecture has distributed modules
?Currently with Panasonic Corporation.
that independently manage their own dialogue state
and knowledge for speech understanding and ut-
terance generation (e.g., Lin et al (1999)). From
an engineering viewpoint, such architecture has an
advantage in that each domain expert can be de-
signed independently and that it is easy to add new
domains. It enables each domain expert to em-
ploy a dialogue strategy very different from those
for other domains. For example, the strategy may
be frame-based mixed-initiative, finite-state-based
system-initiative, or plan-based dialogue manage-
ment (McTear, 2004).
One of the crucial issues with distributed multi-
domain spoken dialogue systems is how to select an
appropriate domain for each user utterance so that
the system can appropriately understand it and an-
swer it. So far several methods have been proposed
but none of them satisfy two basic requirements at
the same time: the ability to be used with a variety
of domain experts (extensibility) and being robust
against ASR (Automatic Speech Recognition) errors
(robustness). We suspect that this is one of the
main reasons why not many multi-domain spoken
dialogue systems have been developed even though
their utility is widely recognized.
This paper presents a new general framework for
domain selection that satisfies the above two require-
ments. In our framework, each expert needs to have
two additional submodules: one for estimating the
probability that it is newly activated, and one for de-
ciding domain continuation when it is already acti-
vated. Since these submodules can be designed in-
dependently from those of other experts, there is no
restriction on designing experts in our framework,
18
and thus extensibility is achieved. Robustness is also
achieved because those submodules can be designed
so that they can utilize domain-dependent informa-
tion, including information on speech understanding
and dialogue history, without detracting from ex-
tensibility. Especially the submodule for deciding
domain continuation has the ability to utilize dia-
logue history to avoid erroneous domain shifts that
often occur in previous approaches. Note that we do
not focus on classifying each utterance without con-
textual information (e.g., Chu-Carroll and Carpenter
(1999)). Rather, we try to estimate the user inten-
tion with regard to continuing and shifting domains
in the course of dialogues.
In what follows, Section 2 explains the distributed
multi-domain spoken dialogue system architecture
and requirements for domain selection. Section 3
discusses previous work, and Section 4 presents our
proposed framework. Section 5 describes an exam-
ple implementation and its evaluation results, and
Section 6 concludes the paper.
2 Domain Selection in Multi-Domain
Spoken Dialogue Systems
2.1 Distributed Architecture
In distributed multi-domain spoken dialogue archi-
tecture (Figure 1), distributed modules indepen-
dently manage their own dialogue state and knowl-
edge for speech understanding and utterance gener-
ation (Lin et al, 1999; Salonen et al, 2004; Pakucs,
2003; Nakano et al, 2008). Although those modules
are referred to with various names in that literature,
we call them domain experts in this paper. In this
architecture, when an input utterance is received, its
ASR results are sent to domain experts. They try to
understand the ASR results using their own knowl-
edge for understanding. The domain selector gathers
information from those experts and decides which
expert should deal with the utterance and then de-
cide on the system utterances. In this paper, the do-
main expert engaging in understanding user utter-
ances and deciding system utterances is called acti-
vated.
2.2 Example Systems
So far many multi-domain spoken dialogue sys-
tems based on distributed architecture have been
user utterance
information for domain selection
domain selector
activate/deactivate
system utterance(from the activatedexpert)
speech understanding utterance generation
domain expert 1
domain expert 2
domain expert 3
dialoguehistory
Figure 1: Distributed multi-domain spoken dialogue sys-
tem architecture.
built and have demonstrated their ability to engage
in dialogues in a variety of domains. For exam-
ple, several systems integrated information provid-
ing and database searches in multiple domains (Lin
et al, 1999; Komatani et al, 2006; O?Neill et al,
2004; Gustafson and Bell, 2000). Some other sys-
tems integrated domain experts that employ very
different dialogue strategies. Lee et al (2009) and
Nakano et al (2006) integrated task-oriented and
non-task-oriented dialogue managements. Nakano
et al (2008) integrated domain experts for not only
dialogues but also tasks requiring physical actions.
Below we explain an example system that we
used to collect dialogue data for the domain se-
lection experiment described in Section 5. It is a
Japanese system that can provide information on
UNESCOWorld Heritage Sites using speech, slides,
and Microsoft Agent1 gestures. It employs the fol-
lowing ten domain experts:
A question answering (QA) expert: It has a
database consisting of question-answer pairs on
World Heritage Sites (Narimatsu et al, 2010). Each
question-answer pair consists of a set of example
question sentences and answers to them. The an-
swers consist of one or more utterances, plus slides.
Keyphrase-based matching is used to select an ex-
ample question that matches the input utterance.
Eight interactive presentation (IP) experts: Each
of them explains in detail a famous World Her-
itage Site and adds a detailed explanation or skips
some explanation according to the user?s interrup-
tions (Nishimura et al, 2007). Patterns of interrup-
tion utterances are described in the expert?s knowl-
edge base.
1http://www.microsoft.com/MSAgent/
19
utterance expert
S1: Hello. I can answer questions about fa-
mous World Heritage Sites in Italy, Ger-
many, Greece, and other areas. I can also
explain some World Heritage Sites in de-
tail. Please ask me anything.
QA
U1: Show me the list of the World Heritage
Sites in Italy.
QA
S2: (show a slide of the list) I can tell you about
the World Heritage Sites in these areas in
Italy.
QA
U2: Can you show me the list of World Her-
itage Sites you can explain in detail?
QA
S3: (show a slide of the list) I can explain these
in detail.
QA
U3: Can you explain Maritime Greenwich? RU
S4: Maritime Greenwich, right? RU
U4: Yes. RU
S5: (show a slide on Greenwich) Okay. I?ll ex-
plain Maritime Greenwich, a historic port
town known for Greenwich Mean Time.
IP1
U5: What is Greenwich Mean Time? IP1
S6: Greenwich Mean Time is the mean solar
time at the Royal Greenwich Observatory.
IP1
S means system utterance and U means user utterance.
IP1 is one of the IP experts.
Figure 2: An example dialogue with the system for data
collection (translation from Japanese).
A request understanding (RU) expert: It under-
stands the user?s request to start one of the interac-
tive presentations explained above, and engages in
a dialogue to confirm the request. When the under-
standing finishes, the understood request is sent to
a module called task planner (Nakano et al, 2008;
Nakano et al, 2011). The task planner then activates
another expert to perform the requested presentation
(S5 in Figure 2).
Figure 2 shows an example dialogue between a
human and this system. Note that user utterances
are relatively short and include words related to spe-
cific World Heritage Sites or area names. If those
words are misrecognized, domain selection is diffi-
cult unless dialogue context information is used.
This figure also indicates the domain experts that
understood each user utterance and selected each
system utterance. The domain expert that should
deal with a user utterance is decided based on the set
of user utterances that the expert is designed to deal
with. The domains of utterances U1 and U3 are dif-
ferent because the QA expert has knowledge for un-
derstanding U1 and the RU expert has knowledge for
understanding U3. Thus, in this study, the domain of
each utterance is determined based on the design of
the experts employed in the system. If none of the
experts can deal with an utterance, it is considered
as an out-of-domain utterance. Sometimes the cor-
rect domain needs to be determined using contextual
information. For example, utterance U4 ?Yes? can
appear in all domains, but, since this is a reply to S4,
its domain is RU.
This definition of domain is different from that of
domain (or topic) recognition and adaptation stud-
ies in text, monologue, and human-human conver-
sation processing, in which reference domains are
annotated based on human perspectives rather than
system perspectives. From a human perspective, all
user utterances in Figure 2 may be in ?World Her-
itage Site? domain. However, it is not always easy
to build domain experts according to such domain
definitions, because different dialogue tasks in one
such domain may require different dialogue strate-
gies (such as question answering and request under-
standing).
2.3 Requirements for Domain Selection
We pursue a method for domain selection that can
be used in distributed architecture. Such a method
must satisfy the following two requirements.
Extensibility It must not detract from the extensi-
bility of distributed architecture, that is, any kind of
expert must be able to be incorporated, and each ex-
pert must be able to be designed independently from
other experts. This requires the interface between
each domain expert and the domain selector to be as
simple as possible.
Robustness It needs to be robust against ASR er-
rors; that is, the system needs to be able to avoid
erroneous domain transition caused by ASR errors.
3 Previous Work
So far various methods for domain selection have
been proposed, but, as far as we know, no method
satisfies both extensibility and robustness. Isobe et
al. (2003) estimate a score for each domain from the
20
ASR result and select the domain with the highest
score (hereafter referred to as RECSCORE). Since
each domain expert has only to output a numeric
score, it satisfies extensibility. However, because
this method does not take into account dialogue con-
text, it tends to erroneously shift domains when the
score of some experts becomes high by chance. For
example, if U4:?Yes? in Figure 2 is recognized as
?Italy? with a high recognition score in the QA ex-
pert, the domain erroneously shifts to QA and the
system explains about World Heritage Sites in Italy.
Thus this method is not robust.
To avoid erroneous domain shifts, Lin et al
(1999) give preference to the preceding domain
(the domain in which the previous system utterance
was made) by adding a certain value to the score
of the preceding domain (hereafter called REC-
SCORE+BIAS ). However, to what extent the do-
main tends to continue varies depending on the dia-
logue context. For example, if a dialogue task in one
domain finishes (e.g., when an IP expert finishes its
presentation and says ?This is the end of the presen-
tation. Do you have any questions??), the domain is
likely to shift. So, adding a fixed score does not al-
ways work. O?Neill et al?s (2004) system does not
change the dialogue domain until it finishes a task
in the domain, but it cannot recover from erroneous
domain shifts.
To achieve robustness against ASR errors, several
domain selection methods based on a classifier that
uses features concerning dialogue history as well as
ones concerning speech understanding results have
been developed (Komatani et al, 2006; Ikeda et al,
2008; Lee et al, 2009). These studies, however, use
some features available only in some specific type
of domain experts, such as features concerning slot-
filling, so they cannot be used with other kinds of
domain experts. That is, these methods do not sat-
isfy extensibility.
Methods that use classifiers based on word (and
n-gram) frequencies have been developed for utter-
ance classification (e.g., Chu-Carroll and Carpenter
(1999)), topic estimation for ASR of speech cor-
pora (e.g., Hsu and Glass (2006) and Heidel and
Lee (2007)) and human-human dialogues (Lane and
Kawahara, 2005). These methods can be applied to
domain selection in multi-domain spoken dialogue
systems. However, since they require training data
in the same set of domains as the target system, it
detracts from extensibility. In addition, they are not
robust because they cannot utilize a variety of di-
alogue and understanding related features. Word
frequencies are not always effective when two do-
mains share words as in our system described in Sec-
tion 2.2.
4 Proposed Framework
4.1 Basic Idea
To achieve extensibility, we need to restrict the infor-
mation that each expert sends to the domain selector
to a simple one such as numeric scores. Although
RECSCORE and RECSCORE+BIAS satisfy this, they
would not achieve high accuracy as explained above.
One possible extension to those methods to im-
prove accuracy is to use not only recognition scores
but also various expert-dependent features such as
ones concerning dialogue history and speech under-
standing. Each expert first estimates the probability
that the input utterance is in its domain using such
features, and then the expert with the highest proba-
bility is selected (hereafter called MAXPROB). This
method retains extensibility because the domain se-
lector does not directly use those expert-dependent
features. However, it suffers from the same prob-
lem as RECSCORE and RECSCORE+BIAS; if one of
the experts other than the preceding domain?s expert
outputs a high probability by mistake, the domain
shifts regardless of the dialogue state in the preced-
ing domain?s expert.
We focus attention on the fact that the domain
does not often shift. Our idea is to decide if the do-
main continues or not by using information available
in the preceding domain?s expert. This prevents er-
roneous domain shifts when the utterance is consid-
ered not to change the domain. When it is decided
that the currently active domain does not continue,
each remaining expert estimates the probability of
being newly activated using information available in
the expert, and the expert whose probability is the
highest is selected as the new domain expert.
We further refine this idea in two ways. One is by
taking into account how likely the input utterance is
to activate one of the other domain experts. We pro-
pose to use the maximum value of probabilities for
other experts? activation (maximum activation prob-
21
user utterance
expert for the preceding domain
experts for other domains
maximumprobability
select thepreceding domain
select the domain with maximumactivation probability
features
Stage 1
Stage 2
...
domain continuationdecision maker
select expert with maximumactivationprobability
speech understanding
features activation probability estimator
speech understanding
features activation probability estimator
speech understanding
decision is tocontinue? yes
no
out-of-domain
handle as an out-of-domain utterance
dialoguehistory
dialoguehistory
dialoguehistory
Figure 3: Two-stage domain selection framework.
ability) in the decision regarding domain continua-
tion. Since the maximum activation probability is
just a numeric score, this does not spoil extensibil-
ity. Unlike RECSCORE and RECSCORE+BIAS, in
our method, even if the maximum activation prob-
ability is very high, the preceding domain?s expert
can decide to continue or not to continue based on
its internal state. This makes it possible to retain ro-
bustness.
The other refinement is to explicitly deal with ut-
terances that are not in any domains (out-of-domain
(OOD) utterances). They include fillers and mur-
murs. They should be treated separately, because
they appear context-independently. So we make the
expert detect OOD utterances when deciding do-
main continuation. That is, it performs three-fold
classification, continue, not-continue, and OOD.
4.2 Two-Stage Domain Selection Framework
This idea can be summarized as a domain selection
framework which consists of two stages (Figure 3).
It assumes that each domain expert has two submod-
ules: activation probability estimator and a domain
continuation decision maker, which use information
available in the expert itself.
When a new input utterance is received, at Stage
1, the activation probability estimators of all non-
activated experts estimate probabilities and send
them to the domain selector. Then at Stage 2, the
domain selector sends their maximum value to the
expert of the preceding domain and asks it to decide
whether it continues to deal with the new input utter-
ances or does not continue, or it deals with the utter-
ance as out-of-domain. If it decides not to continue,
the domain selector selects the expert that outputs
the highest probability at Stage 1.
The reason we use the term ?framework? is that
it does not specify the details of the algorithm and
features used in each domain expert?s submodules
for domain selection. It rather specifies the inter-
faces of those submodules. Note that RECSCORE,
RECSCORE+BIAS, and MAXPROB can be consid-
ered as one of the implementations of this frame-
work. This framework, however, allows developers
to use a wider variety of features and gives flexibility
in designing those submodules.
5 Example Implementation and
Evaluation
Since the proposed framework is an extension of
the previous methods, if the activation probability
estimator and domain continuation decision maker
for each expert are designed well and trained using
enough data, it should outperform previous methods
that satisfy extensibility. We believe that this theo-
retical consideration and an experimental result us-
ing a human-system dialogue corpus show the via-
bility of the framework. Below we explain our im-
plementation and an experiment.
5.1 Data
For the implementation and evaluation, we used a
corpus of dialogues between human users and the
World Heritage Site information system described
in Section 2.2. Domain selection of this system was
performed using hand-crafted rules.
35 participants (17 males and 18 females) whose
ages range from 19 to 57 were asked to engage in
22
domain preceding training training test
domain data A data B data
RU RU 134 169 145
QA 51 102 59
IP 21 16 23
subtotal 206 287 227
QA RU 46 55 51
QA 783 870 888
IP 59 87 66
subtotal 888 1,012 1,005
IP RU 2 1 3
QA 7 11 18
IP 311 305 277
subtotal 320 317 298
OOD RU 24 19 39
QA 168 155 183
IP 66 68 113
subtotal 258 242 335
total 1,672 1,858 1,865
Table 1: Number of utterances in each domain in the
training and test data.
conversation with the system four times. Each ses-
sion lasted eight minutes. For each utterance, the
correct domain or an OOD label was manually an-
notated. We also annotated its preceding domain,
i.e., the domain in which the previous system utter-
ance was made. It can be different from the previous
user utterance?s domain because of the system?s er-
roneous domain selection. Utterances including re-
quests in two domains at the same time should be
given an OOD label but there are no such utterances.
We used data from 23 participants (3,530 utterances)
for training and those from the remaining 12 par-
ticipants (1,865 utterances) for testing. We further
split the training data into training data A (1,672
utterances) and B (1,858 utterances) to train each
of the two submodules. Each training data set in-
cludes data from two sessions for each participant.
Table 1 shows detailed numbers of utterances in the
data sets.
5.2 Implementation
5.2.1 Expert Classes
Among the ten experts, eight IP (Interactive Pre-
sentation) experts have the same dialogue strategy
and most of the predicted user utterance patterns. In
addition, the number of training utterances for each
expert class QA IP RU
LM for ASR trigram trigram finite-state
grammar
language keyphrase keyphrase finite-state
understanding -based -based transducer
vocabulary 1,140 407 79
size (word)
phone error 10.95 19.47 23.60
rate (%)
Table 2: Speech understanding in each expert.
IP expert?s domain is small. We therefore used all
training utterances in the IP domains to build a com-
mon ASR language model (LM), a common acti-
vation probability estimator, and a common domain
continuation decision maker for all IP experts. Here-
after we call the set of IP experts the IP expert class.
The RU (Request Understanding) expert and the QA
(Question Answer) expert are themselves also expert
classes.
5.2.2 Speech Understanding
For all experts, we used the Julius speech recog-
nizer and the acoustic model in the Japanese model
repository (Kawahara et al, 2004).2 Features of
speech understanding in each expert class are shown
in Table 2. Compared to the system used for data
collection, LMs are enhanced based on the training
data. We obtained the ASR performance on the ut-
terances in each domain in the test data in terms of
phone error rates. This is because Japanese has no
standard word boundaries so it is not easy to cor-
rectly compute word error rates. The poor perfor-
mance of ASR for IP is mainly due to the small
amount of training utterances for LM and that for
RU is mainly due to out-of-grammar utterances.
5.2.3 Stage 1
For Stage 1, we used logistic regression to es-
timate the probability that a non-activated expert
would be activated by a user utterance. Features for
logistic regression include those concerning speech
recognition and understanding results as well as dia-
logue history (see Table 5 for the full list of features).
These features are expert-dependent. This makes it
possible to estimate how the input utterance is suit-
2Multiple LMs can be used at the same time with Julius.
23
able to the dialogue context more precisely than us-
ing just features available in any kind of expert.
To train the activation probability estimators, we
fitted logistic regression coefficients using Weka
data mining toolkit ver.3.6.2 (Witten and Frank,
2005)3 and training data A. In the training for each
expert class, we used utterances whose preceding
domain was not that of the class because activation
probabilities are estimated only for such utterances
during domain selection. If the utterance is in a do-
main of the expert class, it is assigned an activate la-
bel and otherwise not-activate. Next, we performed
feature selection to avoid overfitting. We used back-
ward stepwise selection so that the weighted (by the
sizes of activate and not-activate labels) average of
the F1 scores for training set B could be maximized.
Table 6 lists the remaining features and their sig-
nificances in terms of the F1 score obtained when
each feature is removed. Then, we duplicated the
activate-labeled utterances in the training data A so
that the ratio of activate-labeled utterances to not-
activate-labeled utterances became 1 to 3. This is
because the training data include a larger number of
not-activate-labeled utterances and thus the results
would be biased. The ratio was decided by trial and
error so that the weighted average of the F1 scores
for training data B becomes high.
5.2.4 Stage 2
For Stage 2, we used multi-class support vector
machines (SVMs)4 to decide if the activated expert
should continue to be activated, should not continue,
or should regard the input utterance as OOD. We
used the same set of features as Stage 1 as well
as the maximum activation probability obtained at
Stage 1. The training data for the SVM of each ex-
pert class is the set of utterances in training data B
whose preceding domain is in that expert class, be-
cause domain continuation is decided only for such
utterances during domain selection. They are la-
beled continue, not-continue, or OOD. Next, we
performed backward stepwise feature selection so
that the weighted average of F1 scores for continue,
not-continue, and OOD utterance detection on train-
ing data A could be maximized. Remaining fea-
3Multinominal logistic regression model with a ridge esti-
mator with Weka?s default values.
4Weka?s SMO with the linear kernel and its default values.
tures are listed in Table 7. The maximum activa-
tion probability was found to be significant in all ex-
pert classes. This suggests our two-stage framework
that uses maximum activation probability is viable.
Then, we duplicated utterances with not-continue la-
bel and OOD label in the training data so that the
ratio of continue, not-continue, and OOD utterances
became 3:1:1. This is because the number of utter-
ances with the continue label is far greater than oth-
ers. The ratio was experimentally decided by trial
and error so that the weighted average of F1 scores
on training data A becomes high.
5.3 Evaluation
5.3.1 Compared Methods
We compared the full implementation described in
Section 5.2 (FULLIMPL hereafter) with the follow-
ing four methods which satisfy extensibility. Note
that the first three methods were mentioned in Sec-
tion 4.
RECSCORE: This chooses the expert class whose
recognition score is the maximum (Isobe et al,
2003). We used the ASR acoustic score normalized
by the duration of the utterance. If the IP expert class
was chosen, the IP expert that had been most re-
cently activated was chosen, because, in this system,
domain shifts to other IP experts never occur due to
the system constraints and the user did not try to do
it. If none of the experts had a higher score than a
fixed threshold, it recognized the utterance as OOD.
The threshold was experimentally determined using
the training data so that the weighted (by the sizes
of OOD and non-OOD utterances) average of the
F1 scores of OOD/non-OOD classification is max-
imized.
RECSCORE+BIAS: This is the same as REC-
SCORE except that a fixed value (bias) is added to
the score used in RECSCORE for the expert of the
preceding domain. This is basically the same as Lin
et al?s (1999) method but we use a different recog-
nition score since the recognition score they used
cannot be used in our system due to the difference
of speech understanding methods. The most appro-
priate bias for each expert class was decided using
the training data so that the weighted average of the
F1 scores could be maximized. OOD detection was
done in the same way as RECSCORE.
24
method class recall prec- F1 weighted
ision ave. F1
RECSCORE cont. 0.763 0.867 0.812
shift 0.559 0.239 0.335
OOD 0.501 0.848 0.630 0.789
RECSCORE cont. 0.917 0.824 0.868
+BIAS shift 0.400 0.421 0.410
OOD 0.501 0.848 0.630 0.838
MAXPROB cont. 0.925 0.843 0.882
shift 0.282 0.264 0.273
OOD 0.275 0.477 0.348 0.832
NOACTIV cont. 0.875 0.890 0.882
PROB shift 0.464 0.385 0.421
OOD 0.785 0.843 0.813 0.849
FULLIMPL cont. 0.902 0.907 0.904
shift 0.591 0.565 0.578
OOD 0.824 0.829 0.826 0.883
CLASSIFIER cont. 0.956 0.881 0.917
(reference) shift 0.545 0.759 0.635
OOD 0.755 0.885 0.815 0.899
Table 3: Evaluation results (?cont.? means ?continue.?).
MAXPROB: The activation probabilities for all ex-
perts were obtained using logistic regression and the
expert whose probability was the maximum was se-
lected. IP experts that had never been activated were
excluded because they cannot be activated due to
system constraint. For logistic regression, in addi-
tion to the features used in FULLIMPL, the previous
domain was used as a feature so that domain conti-
nuity was taken into account. Feature selection was
also performed. The probability that the utterance is
OOD was estimated in the same way using the fea-
tures concerning speech understanding. If the maxi-
mum probability of OOD detection was greater than
the maximum activation probability, then the utter-
ance was considered to be OOD.
NOACTIVPROB: This is the same as FULLIMPL
except that Stage 2 does not use the result of Stage
1, i.e., maximum activation probability.
5.3.2 Evaluation Results
To evaluate the domain selection, we focused on
domain shifts rather than the selected domain. We
classified the domain selection results into domain
continuations, domain shifts, and OOD utterance
detection. As the evaluation metric, we used the
weighted average of F1 scores for those classes.
Here the weight is the ratio of those classes of cor-
rect labels. Note that shifting to an incorrect do-
main is counted as a false positive when calculat-
ing precision for domain shifts. Table 3 shows the
results. In addition, the confusion matrices for the
three best methods are shown in Table 4. We found
FULLIMPL outperforms the other four methods. We
also found that the differences between the results of
the compared methods are all statistically significant
(p < .01) by two-tailed binomial tests.
For reference, we also evaluated a classifier-based
method that uses features from all the experts. Note
that this method does not satisfy extensibility be-
cause it requires training data in the same set of do-
mains as the target system. We evaluated this just
for estimating how well our proposed method works
while satisfying extensibility. It classifies each ut-
terance into one of four categories: the QA expert?s
domain, the RU expert?s domain, the most recently
activated IP expert?s domain, and OOD. If no IP ex-
pert has been activated before the utterance, three-
fold classification was performed. The training and
test data were split depending on whether one of the
IP experts has been activated before, and training
and testing were separately conducted. The training
data A was used for training SVM classifiers. Then
feature selection was performed using the training
data B. The performance of this method is shown
as CLASSIFIER in Tables 3 and 4. Although this
method outperforms FULLIMPL, FULLIMPL?s per-
formance is close to this method. This shows that
our method does not degrade its performance very
much even though it satisfies extensibility.
5.3.3 Discussion
One of the reasons why FULLIMPL outperforms
other methods is that its precision for domain shifts
is relatively higher than the other methods. This
suggests it can avoid erroneous domain shifts, thus
the proposed two-stage framework is more robust.
RECSCORE+BIAS performed relatively well despite
it used only limited features. We guess this is be-
cause adding preferences to the preceding domain
was effective since domain shifts are rare in these
data. Its low F1 score for OOD utterances suggests
using just recognition scores is insufficient to detect
them. The comparison of FULLIMPL with NOAC-
TIVPROB shows the effectiveness of using maxi-
mum activation probability in the second stage.
The F1 score for domain shifts is low even with
25
RECSCORE+BIAS:
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,201 - 82 27 1,310
shift 115 88 14 3 220
OOD 142 - 25 168 335
total 1,458 88 121 198 1,865
NOACTIVPROB:
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,146 - 123 41 1,310
shift 92 102 18 8 220
OOD 50 - 22 263 335
total 1,288 102 163 312 1,865
FULLIMPL:
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,181 - 77 52 1,310
shift 70 130 15 5 220
OOD 51 - 8 276 335
total 1,302 130 100 333 1,865
CLASSIFIER (reference):
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,252 - 30 28 1310
shift 92 120 3 5 220
OOD 77 - 5 253 335
total 1,421 120 38 286 1,865
Table 4: Confusion matrices for the domain shifts.
FULLIMPL, although it is higher than those with
other methods. One typical reason for this is that
when one keyword in the ASR result of an utter-
ance to shift the domain is also in the vocabulary of
the preceding domain?s expert, the selection tends to
continue the previous domain by mistake. For ex-
ample, an utterance ?tell me about other World Her-
itage Sites? to shift from an IP domain to the QA
domain is sometimes misclassified as an IP domain
utterance, because ?World Heritage Sites? is also in
IP domains? vocabulary. We think this is because
the training data do not include a sufficient amount
of utterances that shift domains, and that a larger
amount of training data would solve this problem.
6 Concluding Remarks
This paper presented a novel general framework for
domain selection in extensible multi-domain spoken
dialogue systems. This framework makes it possi-
ble to build a robust domain selector because of its
flexibility in exploiting features and taking into ac-
count domain continuity. An experiment with data
collected with an example multi-domain system sup-
ported the viability of the proposed framework. We
believe that this framework will promote the devel-
opment of multi-domain spoken dialogue systems
and conversational robots/agents.
Among future work is to investigate how accurate
the activation probability estimator and the domain
continuation decision maker in each domain expert
should be for achieving a reasonable accuracy in do-
main selection. We also plan to conduct experiments
with systems that have a larger number of domain
experts to verify the scalability of this framework.
In addition, we will explore a way to estimate the
confidence of the domain selection to reduce erro-
neous domain selections.
Acknowledgments
The authors would like to thank Hiroshi Tsujino,
Yuji Hasegawa, and Hiromi Narimatsu for their sup-
port for this research.
References
Hideki Asoh, Toshihiro Matsui, John Fry, Futoshi Asano,
and Satoru Hayamizu. 1999. A spoken dialog system
for a mobile office robot. In Proc. 6th Eurospeech,
pages 1139?1142.
Jennifer Chu-Carroll and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational
Linguistics, 25(3):361?388.
Joakim Gustafson and Linda Bell. 2000. Speech tech-
nology on trial: Experiences from the August system.
Natural Language Engineering, 6(3&4):273?286.
Aaron Heidel and Lin-shan Lee. 2007. Robust topic in-
ference for latent semantic language model adaptation.
In Proc. ASRU-07, pages 177?182.
Bo-June (Paul) Hsu and James Glass. 2006. Style and
topic language model adaptation using HMM-LDA.
In Proc. EMNLP ?06, pages 373?381,.
Satoshi Ikeda, Kazunori Komatani, Tetsuya Ogata, and
Hiroshi G. Okuno. 2008. Extensibility verification
26
of robust domain selection against out-of-grammar ut-
terances in multi-domain spoken dialogue system. In
Proc. Interspeech-2008 (ICSLP), pages 487?490.
T. Isobe, S. Hayakawa, H. Murao, T. Mizutani,
K. Takeda, and F. Itakura. 2003. A study on do-
main recognition of spoken dialogue systems. In Proc.
Eurospeech-2003, pages 1889?1892.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Recent
progress of open-source LVCSR engine Julius and
Japanese model repository. In Proc. Interspeech-2004
(ICSLP), pages 3069?3072.
Kazunori Komatani, Naoyuki Kanda, Mikio Nakano,
Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata,
and Hiroshi G. Okuno. 2006. Multi-domain spo-
ken dialogue system with extensibility and robustness
against speech recognition errors. In Proc. 7th SIGdial
Workshop, pages 9?17.
Ian R. Lane and Tatsuya Kawahara. 2005. Incorporating
dialogue context and topic clustering in out-of-domain
detection. In Proc. ICASSP-2005, pages 1045?1048.
Cheongjae Lee, Sangkeun Jung, Seokhwan Kim, and
Gary Geunbae Lee. 2009. Example-based dialog
modeling for practical multi-domain dialog system.
Speech Communication, 51(5):466?484.
Bor-shen Lin, Hsin-ming Wang, and Lin-shan Lee. 1999.
A distributed architecture for cooperative spoken dia-
logue agents with coherent dialogue state and history.
In Proc. ASRU-99.
Michael F. McTear. 2004. Spoken Dialogue Technology.
Springer.
Mikio Nakano, Atsushi Hoshino, Johane Takeuchi,
Yuji Hasegawa, Toyotaka Torii, Kazuhiro Nakadai,
Kazuhiko Kato, and Hiroshi Tsujino. 2006. A robot
that can engage in both task-oriented and non-task-
oriented dialogues. In Proc. Humanoids-2006, pages
404?411.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and
Hiroshi Tsujino. 2008. A framework for building con-
versational agents based on a multi-expert model. In
Proc. 9th SIGdial Workshop, pages 88?91.
Mikio Nakano, Yuji Hasegawa, Kotaro Funakoshi, Jo-
hane Takeuchi, Toyotaka Torii, Kazuhiro Nakadai,
Naoyuki Kanda, Kazunori Komatani, Hiroshi G.
Okuno, and Hiroshi Tsujino. 2011. A multi-expert
model for dialogue and behavior control of conversa-
tional robots and agents. Knowledge-Based Systems,
24(2):248?256.
Hiromi Narimatsu, Mikio Nakano, and Kotaro Fu-
nakoshi. 2010. A classifier-based approach to
supporting the augmentation of the question-answer
database for spoken dialogue systems. In Proc. 2nd
IWSDS, pages 182?187.
Yoshitaka Nishimura, Shinichiro Minotsu, Hiroshi Dohi,
Mitsuru Ishizuka, Mikio Nakano, Kotaro Funakoshi,
Johane Takeuchi, Yuji Hasegawa, and Hiroshi Tsujino.
2007. A markup language for describing interactive
humanoid robot presentations. In Proc. IUI?07, pages
333?336.
Ian O?Neill, Philip Hanna, Xingkun Liu, and Michael
McTear. 2004. Cross domain dialogue modelling:
an object-based approach. In Proc. Interspeech-2004
(ICSLP), pages 205?208.
Botond Pakucs. 2003. Towards dynamic multi-domain
dialogue processing. In Proc. Eurospeech-2003, pages
741?744.
Esa-Pekka Salonen, Mikko Hartikainen, Markku Tu-
runen, Jaakko Hakulinen, and J. Adam Funk. 2004.
Flexible dialogue management using distributed and
dynamic dialogue control. In Proc. Interspeech-2004
(ICSLP), pages 197?200.
Ian H.Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques, 2nd Edi-
tion. Morgan Kaufmann, San Francisco.
27
expert Features
class
all Fi,r1 If SRRi,1 is obtained or not
classes Fi,r2 If SRRi,1 contains a filler or not
i = ru, Fi,r3 min (CMs of words in SRRi,1)
ip, qa Fi,r4 avg (CMs of words in SRRi,1)
Fi,r5 (acoustic score of SRRi,1) / duration
Fi,r6 LM score of SRRi,1
Fi,r7 # of words in SRRi,1
Fi,r8 # of words in SRRi,all
Fi,r9 (Fi,r5 - (acoustic score of SRRlv,1))
/ duration
RU Fru,r10 If SRRru,1 is an affirmative response
Fru,r11 If SRRru,1 is a denial response
Fru,r12 # of ASR results with LMru
Fru,r13 If SRRru,1 contains the name of a
World Heritage Site
Fru,r14 max (CMs of words comprising the
name of a World Heritage Site)
Fru,r15 ave (CMs of words comprising the
name of a World Heritage Site)
Fru,h1 If SRRru,1 is an affirmative response
(Stage 2 only)
Fru,h2 # of turns since this expert is acti-
vated
Fru,h3 # of denial responses recognized
since this expert is activated
Fru,h4 Fru,h4/Fru,h3
Fru,h5 If the previous system utterance is a
confirmation request to a user request
for starting a presentation
Fru,h6 If the previous system utterance is
an utterance to react to a non-
understandable user utterance
Fru,h7 If the system has made a confirma-
tion request to a user request for start-
ing a presentation since this expert
was activated
Fru,h8 If the system has made an utterance
to react to a non-understandable user
utterance since this expert was acti-
vated
Fru,h9 If the system has made a confirma-
tion request to a user request for start-
ing a presentation before
Fru,h10 If the system has made an utterance
to react to a non-understandable user
utterance before
expert Features
class
IP Fip,r10 If the SRRip,1 is out of database
Fip,r11
P
j((# of keyphrases in SRRip,j) / (# of words in
SRRip,j) ) / (# of ASR results)
Fip,r12 mini( # of keyphrasei in SRRip,all / (# of ASR re-
sults))
Fip,r13 maxi( # of keyphrasei in SRRip,all / (# of ASR re-
sults))
Fip,r14 avg( CM of keyphrasei in SRRip,1)
Fip,r15 mini ( CM of keyphrasei in SRRip,1)
Fip,r16 maxi ( CM of keyphrasei in SRRip,1)
Fip,h1 If this expert has been activated before
Fip,h2 Same as Fru,h2
Fip,h3 If the previous system utterance is the final utter-
ance of the presentation
Fip,h4 If the previous system utterance is an utterance to
react to a user interruption
Fip,h5 Same as Fru,h6
Fip,h6 If the system has made the final utterance of the pre-
sentation since this expert was activated
Fip,h7 If the system has made an utterance to react to a user
interruption since this expert was activated
Fip,h8 Same as Fru,h8
Fip,h9 If the system has made the final utterance of the pre-
sentation before
Fip,h10 If the system has made an utterance to react to a user
interruption before
Fip,h11 Same as Fru,h10
QA Fqa,r10 Same as Fip,r12
Fqa,r11 Same as Fip,r13
Fqa,r12 Same as Fip,r14
Fqa,r13 Same as Fip,r15
Fqa,r14 Same as Fip,r16
Fqa,r15 Same as Fip,r17
Fqa,r16 If SRRqa,1 is an acknowledgment
Fqa,h1 Same as Fru,h1
Fqa,h2 Same as Fru,h2
Fqa,h3 Same as Fru,h3
Fqa,h4 Fqa,h4/Fqa,h3
Fqa,h5 If the previous system utterance is the final utter-
ance of an answer
Fqa,h6 Same as Fru,h6
Fqa,h7 If the system has made the final utterance of an an-
swer since this expert was activated
Fqa,h8 Same as Fru,h8
Fqa,h9 If the system has made the final utterance of an an-
swer before
Fqa,h10 Same as Fru,h10
SRRi,j means j-th speech recognition result with the language model (LM) for expert class i. SRRi,all means all the recognition
results in the n-best list. Fi,rx are speech understanding related features and Fi,hx are dialogue history related features. SRRlv,j
is an ASR result with a large-vocabulary (60,250 words) statistical model (Kawahara et al, 2004), which we used for utterance
verification. CM means confidence measure.
Table 5: Features used in the experiment.
28
expert class
(F1 score
obtained
after feature
selection)
remaining features (F1 score obtained
when each feature is removed)
RU(0.948) Fru,r9 (0.922), Fru,h8 (0.939), Fru,r5
(0.940), Fru,r14 (0.941), Fru,r2
(0.944), Fru,h9 (0.944), Fru,h5
(0.944), Fru,r13 (0.945), Fru,h10
(0.945), Fru,r10 (0.946), Fru,r8
(0.946), Fru,r7 (0.946)
IP(0.837) Fip,r7 (0.771), Fip,r6 (0.772), Fip,h9
(0.781), Fip,h7 (0.781), Fip,h11
(0.786), Fip,r4 (0.79), Fip,r2 (0.799),
Fip,r16 (0.809), Fip,r5 (0.809), Fip,r3
(0.809), Fip,h4 (0.809), Fip,r9 (0.814),
Fip,r15 (0.833), Fip,r12 (0.834), Fip,r13
(0.835), Fip,h10 (0.836)
QA(0.836) Fqa,r14 (0.813), Fqa,r7 (0.817),
Fqa,r16 (0.817), Fqa,r10 (0.818),
Fqa,h6 (0.820), Fqa,r6 (0.822), Fqa,r3
(0.831), Fqa,r5 (0.832)
Table 6: Features that remained after feature selection at
Stage 1 and their significances in terms of the F1 score
obtained when each feature is removed.
expert class
(F1 score
obtained
after feature
selection)
remaining features (F1 score obtained
when each feature is removed)
RU(0.773) Fru,r3 (0.728), Fru,a (0.737), Fru,h5
(0.743), Fru,h1 (0.751), Fru,r9 (0.754),
Fru,h10 (0.757), Fru,h8 (0.757),
Fru,r5 (0.758), Fru,r2 (0.759), Fru,r13
(0.762), Fru,r14 (0.763), Fru,h9
(0.767), Fru,r15 (0.768), Fru,r10
(0.768), Fru,h3 (0.772)
IP(0.827) Fip,h5 (0.808), Fip,r5 (0.809), Fip,r4
(0.810), Fip,r6 (0.811), Fip,a (0.812),
Fip,h4 (0.812), Fip,r13 (0.813), Fip,h3
(0.817), Fip,r15 (0.818), Fip,r3 (0.818),
Fip,h10 (0.819), Fip,r12 (0.820),
Fip,h7 (0.821), Fip,r11 (0.822), Fip,r10
(0.822), Fip,h8 (0.822), Fip,h6 (0.822),
Fip,r2 (0.824), Fip,r8 (0.824), Fip,h9
(0.824), Fip,h2 (0.825)
QA(0.873) Fqa,a (0.838), Fqa,r5 (0.857), Fqa,h1
(0.859), Fqa,r3 (0.862), Fqa,r6 (0.865),
Fqa,h8 (0.867), Fqa,r7 (0.868), Fqa,r15
(0.870), Fqa,r8 (0.870), Fqa,h7 (0.870),
Fqa,r12 (0.871), Fqa,r2 (0.871), Fqa,r16
(0.871), Fqa,h4 (0.871), Fqa,h3 (0.871),
Fqa,r11 (0.872), Fqa,h6 (0.872), Fqa,h5
(0.872)
Table 7: Features that remained after feature selection at
Stage 2 and their significances in terms of the F1 score
obtained when each feature is removed. Fru,a, Fip,a, and
Fqa,a are the maximum activation probabilities obtained
at Stage 1.
29
