Back Transliteration from Japanese to English 
Using Target English Context  
Isao Goto?, Naoto Kato??, Terumasa Ehara???, and Hideki Tanaka? 
?NHK Science and Technical 
Research Laboratories  
1-11-10 Kinuta, Setagaya,  
Tokyo, 157-8510, Japan 
goto.i-es@nhk.or.jp 
tanaka.h-ja@nhk.or.jp 
??ATR Spoken Language Trans-
lation Research Laboratories 
2-2-2 Hikaridai, Keihanna  
Science City, Kyoto, 619-0288, 
Japan 
naoto.kato@atr.jp 
???Tokyo University of  
Science, Suwa  
5000-1, Toyohira, Chino,  
Nagano, 391-0292, Japan 
eharate@rs.suwa.tus.
ac.jp 
 
Abstract 
This paper proposes a method of automatic 
back transliteration of proper nouns, in which 
a Japanese transliterated-word is restored to 
the original English word. The English words 
are created from a sequence of letters; thus 
our method can create new English words that 
are not registered in dictionaries or English 
word lists. When a katakana character is con-
verted into English letters, there are various 
candidates of alphabetic characters. To ensure 
adequate conversion, the proposed method 
uses a target English context to calculate the 
probability of an English character or string 
corresponding to a Japanese katakana charac-
ter or string. We confirmed the effectiveness 
of using the target English context by an ex-
periment of personal-name back translitera-
tion.  
1 Introduction 
In transliteration, a word in one language is con-
verted into a character string of another language 
expressing how it is pronounced. In the case of 
transliteration into Japanese, special characters 
called katakana are used to show how a word is 
pronounced. For example, a personal name and 
its transliterated word are shown below.  
Cunningham         ?????
(ka ni n ga mu)
[Transliteration]
 
Here, the italic alphabets are romanized Japanese 
katakana characters.  
New transliterated words such as personal 
names or technical terms in katakana are not al-
ways listed in dictionaries. It would be useful for 
cross-language information retrieval if these 
words could be automatically restored to the 
original English words.  
Back transliteration is the process of restoring 
transliterated words to the original English words. 
Here is a problem of back transliteration.  
?                ?????????
(English word) (ku ra cchi fi ? ru do)
[Back transliteration]
 
There are many ambiguities to restoring a 
transliterated katakana word to its original Eng-
lish word. For example, should "a" in "ku ra cchi 
fi ? ru do" be converted into the English letter of 
"a" or "u" or some other letter or string? Trying 
to resolve the ambiguity is a difficult problem, 
which means that back transliteration to the cor-
rect English word is also difficult.  
Using the pronunciation of a dictionary or lim-
iting output English words to a particular English 
word list prepared in advance can simplify the 
problem of back transliteration. However, these 
methods cannot produce a new English word that 
is not registered in a dictionary or an English 
word list. Transliterated words are mainly proper 
nouns and technical terms, and such words are 
often not registered. Thus, a back transliteration 
framework for creating new words would be very 
useful.  
A number of back transliteration methods for 
selecting English words from an English pronun-
ciation dictionary have been proposed. They in-
clude Japanese-to-English (Knight and Graehl, 
1998) 1 , Arabic-to-English (Stalls and Knight, 
                                                          
1 Their English letter-to-sound WFST does not convert Eng-
lish words that are not registered in a pronunciation diction-
ary.  
1998), and Korean-to-English (Lin and Chen, 
2002).  
There are also methods that select English 
words from an English word list, e.g., Japanese-
to-English (Fujii and Ishikawa, 2001) and Chi-
nese-to-English (Chen et al, 1998).  
Moreover, there are back transliteration meth-
ods capable of generating new words, there are 
some methods for back transliteration from Ko-
rean to English (Jeong et al, 1999; Kang and 
Choi, 2000).  
These previous works did not take the target 
English context into account for calculating the 
plausibility of matching target characters with the 
source characters.  
This paper presents a method of taking the tar-
get English context into account to generate an 
English word from a Japanese katakana word. 
Our character-based method can produce new 
English words that are not listed in the learning 
corpus.  
This paper is organized as follows. Section 2 
describes our method. Section 3 describes the 
experimental set-up and results. Section 4 dis-
cusses the performance of our method based on 
the experimental results. Section 5 concludes our 
research. 
2 Proposed Method 
2.1 Advantage of using English context 
First we explain the difficulty of back translitera-
tion without a pronunciation dictionary. Next, we 
clarify the reason for the difficulty. Finally, we 
clarify the effect using English context in back 
transliteration.  
In back transliteration, an English letter or 
string is chosen to correspond to a katakana char-
acter or string. However, this decision is difficult. 
For example, there are cases that an English letter 
"u" corresponds to "a" of katakana, and there are 
cases that the same English letter "u" does not 
correspond to the same "a" of katakana. "u" in 
Cunningham corresponds to "a" in katakana and 
"u" in Bush does not correspond to "a" in kata-
kana. It is difficult to resolve this ambiguity 
without the pronunciation registered in a diction-
ary.  
The difference in correspondence mainly 
comes from the difference of the letters around 
the English letter "u." The correspondence of an 
English letter or string to a katakana character or 
string varies depending on the surrounding char-
acters, i.e., on its English context.  
Thus, our back transliteration method uses the 
target English context to calculate the probability 
of English letters corresponding to a katakana 
character or string.  
2.2 Notation and conversion-candidate 
lattice  
We formulate the word conversion process as a 
unit conversion process for treating new words. 
Here, the unit is one or more characters that form 
a part of characters of the word.  
A katakana word, K, is expressed by equation 
2.1 with "^" and "$" added to its start and end, 
respectively.  
1
0 0 1 1...
m
mk k k k
+
+= =K  (2.1) 
0 ^k = , 1 $mk + =  (2.2) 
where jk  is the j-th character in the katakana 
word, and m is the number of characters except 
for "^" and "$" and 10
mk +  is a character string 
from 0k  to 1mk + .  
We use katakana units constructed of one or 
more katakana characters. We denote a katakana 
unit as ku. For any ku, many English units, eu, 
could be corresponded as conversion-candidates. 
The ku's and eu's are generated using a learning 
corpus in which bilingual words are separated 
into units and every ku unit is related an eu unit.  
{ }EL  denotes the lattice of all eu's correspond-
ing to ku's covering a Japanese word. Every eu is 
a node of the lattice and each node is connected 
with next nodes. { }EL  has a lattice structure start-
ing from "^" and ending at "$." Figure 1 shows an 
example of { }EL  corresponding to a katakana 
word "?????????(ki ru shu shu ta i 
n)." In the figure, each circle represents one eu. 
A character string linking individual character 
units in the paths 1 2( , ,.., )d qp p p p?  between "^" 
and "$" in { }EL  becomes a conversion candidate, 
where q is the number of paths between "^" and 
"$" in { }EL . 
We get English word candidates by joining eu's 
from "^" to "$" in { }EL . We select a certain path, 
pd, in { }EL . The number of character units 
cchi
?(ki)
chi
ci
cki
cy
k
ke
khi
ki
kie
kii
ky
qui
ch
che
chou
chu
s
sc
sch
schu
sh
?(ru) ??(shu) ??(shu) ?(ta) ?(i) ?(n)
ta
tad
tag
te
ter
tha
ti
to
tta
tu
e
hi
i
ji
y
yeh
yi
m
mon
mp
n
ne
ng
ngh
nin
nn
nne
nt
nw
t
??(tai)
l
ld
le
les
lew
ll
lle
llu
lou
lu
r
rc
rd
re
rg
roo
rou
rr
rre
rt
lu
she
shu
su
sy
sz
ch
che
chou
chu
s
sc
sch
schu
sh
she
shu
su
sy
sz
taj
tay
tey
ti
tie
ty
tye? ? ?
? ? ?
? ? ? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
^ $
 
Figure 1: Example of lattice { }EL  of conversion candidates units. 
 
except for "^" and "$" in pd is expressed as ( )dn p . 
The character units in pd are numbered from start 
to end.  
The English word, E, resulting from the con-
version of a katakana word, K, for pd is expressed 
as follows: 
1
0 0 1 1..
m
mk k k k
+
+= =K  
( ) 1
0 0 1 ( ) 1..
d
d
n p
n p
+
+= ku = ku ku ku , (2.3) 
( ) 1
0 0 1 ( ) 1..
d
d
l p
l pe e e e
+
+= =E  
( ) 1
0 0 1 ( ) 1..
d
d
n p
n p
+
+= eu = eu eu eu , (2.4) 
0 0 0 0 ^k e= = = =ku eu ,  
1 ( ) 1 ( ) 1 ( ) 1 $d d dm l p n p n pk e+ + + += = = =ku eu ,  (2.5) 
where ej is the j-th character in the English word. 
( )dl p  is the number of characters except for "^" 
and "$" in the English word. ( ) 10 d
n p +eu  for each pd 
in { }EL  in equation 2.4 becomes the candidate 
English word. ( ) 10 d
n p +ku  in equation 2.3 shows the 
sequence of katakana units. 
2.3 Probability models using target Eng-
lish context 
To determine the corresponding English word for 
a katakana word, the following equation 2.6 must 
be calculated: 
? arg max ( | )P
E
E = E K . (2.6) 
Here, E?  represents an output result. 
To use the English context for calculating the 
matching of an English unit with a katakana unit, 
the above equation is transformed into Equation 
2.7 by using Bayes? theorem. 
? arg max ( ) ( | )P P=
E
E E K E  (2.7) 
Equation 2.7 contains a translation model in 
which an English word is a condition and kata-
kana is a result.  
The word in the translation model ( | )P K E  in 
Equation 2.7 is broken down into character units 
by using equations 2.3 and 2.4. 
{
}
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1 ( ) 1
0 0 0
? arg max ( )
( , , | )
arg max ( )
( | , , )
( | , ) ( | )
d d
n p n pd d
d d
n p n pd d
d d d
n p n p
n p n p
n p n p n p
P
P
P
P
P P
+ +
+ +
+ +
+ +
+ + +
=
?
=
?
?
? ?
? ?
E
eu ku
E
eu ku
E E
K ku eu E
E
K ku eu E
ku eu E eu E
 (2.8) 
( ) 1
0
dn p +eu  includes information of E. K is only 
affected by ( ) 1
0
dn p +ku . Thus equation 2.8 can be 
rewritten as follows:  
( ) 1 ( ) 1
0 0
( ) 1
0
( ) 1 ( ) 1 ( ) 1
0 0 0 .
? argmax ( )
( | )
( | ) ( | )
d
n p n pd d
d d d
n p
n p n p n p
P
P
P P
+ +
+
+ + +
???
???
=
?
?
? ?
E
eu ku
E E
K ku
ku eu eu E
 (2.9) 
( ) 1
0( | )
dn pP +K ku  is 1 when the string of K and 
( ) 1
0
dn p +ku  is the same, and the strings of the 
( ) 1
0
dn p +ku  of all paths in the lattice and the string 
of the K is the same. Thus, ( ) 1
0( | )
dn pP +K ku  is al-
ways 1.  
We approximate the sum of paths by selecting 
the maximum path.  
( ) 1 ( ) 1
0 0
( ) 1
0
? arg max ( ) ( | )
( | )
d d
d
n p n p
n p
P P
P
+ +
+
?
?
E
E E ku eu
eu E
 
 (2.10) 
We show an instance of each probability 
model with a concrete value as follows: 
 
( )
(^Crutchfield$)
P
P
E , 
 
( ) 1
0( | )
(^ | ^ / )
( ) ( / / / / / )
dn pP
P
ku ra cchi fi ru do ku ra cchi fi ru do
+
? ?
K ku
?????????$ ?/?/??/???/?/?/$ , 
 
( ) 1 ( ) 1
0 0( | )
(^ / | ^ / C/ru/tch/fie/l/d / $)
( / / / / / )
d dn p n pP
P
ku ra cchi fi ru do
+ +
?
ku eu
?/?/??/???/?/?/$ , 
 
( ) 1
0( | )
(^ / C/ru/tch/fie/ld / $ | ^Crutchfield$)
dn pP
P
+eu E . 
 
We broke down the language model ( )P E  in 
equation 2.10 into letters.  
( ) 1
1
1
( | )( )
dl p
j
j j a
j
P e eP
+
?
?
=
? ?E  (2.11) 
Here, a is a constant. Equation 2.11 is an (a+1)-
gram model of English letters.  
Next, we approximate the translation model 
( ) 1 ( ) 1
0 0( | )
d dn p n pP + +ku eu  and the chunking model 
( ) 1
0( | )
dn pP +eu E . For this, we use our previously 
proposed approximation technique (Goto et al, 
2003). The outline of the technique is shown as 
follows.  
( ) 1 ( ) 1
0 0( | )
d dn p n pP + +ku eu  is approximated by reduc-
ing the condition.  
( ) 1 ( ) 1
0 0
( ) 1
( ) 11
0 0
1
( | )
( | , )
d d
d
d
n p n p
n p
n pi
i
i
P
P
+ +
+
+?
=
= ?
ku eu
ku ku eu
 
( ) 1
( ) 1
( ) ( ) 1
1
( | , , )
dn p
start i
i start i b i end i
i
P e e
+
?
? +
=
? ? ku eu
 (2.12)
 
where start(i) is the first position of the i-th char-
acter unit eui, while end(i) is the last position of 
the i-th character unit eui; and b is a constant. 
Equation 2.12 takes English context ( ) 1( )
start i
start i be
?
?  and 
( ) 1end ie +  into account.  
Next, the chunking model ( ) 10( | )d
n pP +eu E  is 
transformed. All chunking patterns of ( ) 10 d
l pe +=E  
into ( ) 10 d
n p +eu  are denoted by each l(pd)+1 point 
between l(pd)+2 characters that serve or do not 
serve as delimiters. eu0 and ( ) 1dn p +eu  are deter-
mined in advance. l(pd)-1 points remain ambigu-
ous. We represent the value that is delimiter or is 
non-delimiter between ej and ej+1 by zj. We call 
the zj delimiter distinction.  { delimiternon-delimiterjz =  (2.13) 
Here, we show an example of English units us-
ing zj.  
(e1 e2 e3 e4  e5  e6 e7 e8 e9  e10 e11)
C r u t c h f i e l d
(z1  z2  z3  z4  z5  z6 z7  z8  z9  z10)
/ / / /
1  0  1  0  0  1  0  0  1  1
English:
Values of zj:
/
 
 
In this example, a delimiter of zj is represented by 
1 and a non-delimiter is represented by 0.  
The chunking model is transformed into a 
processing per character by using zj. And we re-
duce the condition.  
( ) 1
0
( ) 1 ( ) 1
0 0
( ) 1
( ) 11
0 0
1
( | )
( | )
( | , )
d
d d
d
d
n p
l p l p
l p
l pj
j
j
P
P z e
P z z e
+
? +
?
+?
=
=
= ?
eu E
 
( ) 1
1 1
1
1
( | , )
dl p
j j
j j c j c
j
P z z e
?
? +
? ? ?
=
? ?  (2.14) 
The conditional information of the English 
1j
j ce
+
?  is as many as c characters and 1 character 
before and after zj, respectively. The conditional 
information of 1 1
j
j cz
?
? ?  is as many as c+1 delimiter 
distinctions before zj.  
By using equation 2.11, 2.12, and 2.14, equa-
tion 2.10 becomes as follows:  
( ) 1
1
1
( )
( ) 1
( ) ( ) 1
1
( ) 1
1 1
1
1
? arg max ( | )
( | , , )
( | , ).
d
d
d
l p
j
j j a
j
n p
start i
i start i b i end i
i
l p
j j
j j c j c
j
P e e
P e e
P z z e
+
?
?
=
?
? +
=
?
? +
? ? ?
=
?
?
?
?
?
?
E
E
ku eu
 (2.15) 
Equation 2.15 is the equation of our back 
transliteration method.  
2.4 Beam search solution for context 
sensitive grammar 
Equation 2.15 includes context-sensitive gram-
mar. As such, it can not be carried out efficiently. 
In decoding from the head of a word to the tail, 
eend(i)+1 in equation 2.15 becomes context-
sensitive. Thus we try to get approximate results 
by using a beam search solution. To get the re-
sults, we use dynamic programming. Every node 
of eu in the lattice keeps the N-best results evalu-
ated by using a letter of eend(i)+1 that gives the 
maximum probability in the next letters. When 
the results of next node are evaluated for select-
ing the N-best, the accurate probabilities from the 
previous nodes are used.  
2.5 Learning probability models based 
on the maximum entropy method 
The probability models are learned based on the 
maximum entropy method. This makes it possi-
ble to prevent data sparseness relating to the 
model as well as to efficiently utilize many con-
ditions, such as context, simultaneously. We use 
the Gaussian Prior (Chen and Rosenfeld, 1999) 
smoothing method for the language model. We 
use one Gaussian variance. We use the value of 
the Gaussian variance that minimizes the test 
set's perplexity.  
The feature functions of the models based on 
the maximum entropy method are defined as 
combinations of letters. In addition, we use 
vowel, consonant, and semi-vowel classes for the 
translation model. We manually define the com-
binations of the letter positions such as ej and ej-1. 
The feature functions consist of the letter combi-
nations that meet the combinations of the letter 
positions and are observed at least once in the 
learning data.  
2.6 Corpus for learning 
A Japanese-English word list aligned by unit was 
used for learning the translation model and the 
chunking model and for generating the lattice of 
conversion candidates. The alignment was done 
by semi-automatically. A romanized katakana 
character usually corresponds to one or several 
English letters or strings. For example, a roman-
ized katakana character "k" usually corresponds 
to an English letter "c," "k," "ch," or "q." With 
such heuristic rules, the Japanese-English word 
corpus could be aligned by unit and the align-
ment errors were corrected manually.  
3 Experiment 
3.1 Learning data and test data 
We conducted an experiment on back translitera-
tion using English personal names. The learning 
data used in the experiment are described below. 
The Dictionary of Western Names of 80,000 
People2 was used as the source of the Japanese-
English word corpus. We chose the names in al-
phabet from A to Z and their corresponding kata-
kana. The number of distinct words was 39,830 
for English words and 39,562 for katakana words. 
The number of English-katakana pairs was 
83,0573. We related the alphabet and katakana 
character units in those words by using the 
method described in section 2.6. We then used 
the corpus to make the translation and the chunk-
ing models and to generate a lattice of conversion 
candidates. 
The learning of the language model was car-
ried out using a word list that was created by 
merging two word lists: an American personal-
                                                          
2 Published by Nichigai Associates in Japan in 1994.  
3  This corpus includes many identical English-katakana 
word pairs.  
name list4, and English head words of the Dic-
tionary of Western Names of 80,000 people. The 
American name list contains frequency informa-
tion for each name; we also used the frequency 
data for the learning of the language model. A 
test set for evaluating the value of the Gaussian 
variance was created using the American name 
list. The list was split 9:1, and we used the larger 
data for learning and the smaller data for evaluat-
ing the parameter value.  
The test data is as follows. The test data con-
tained 333 katakana name words of American 
Cabinet officials, and other high-ranking officials, 
as well as high-ranking governmental officials of 
Canada, the United Kingdom, Australia, and 
New Zealand (listed in the World Yearbook 2002 
published by Kyodo News in Japan). The English 
name words that were listed along with the corre-
sponding katakana names were used as answer 
words. Words that included characters other than 
the letters A to Z were excluded from the test 
data. Family names and First names were not 
distinguished.  
3.2 Experimental models 
We used the following methods to test the indi-
vidual effects of each factor of our method.  
? Method A 
Used a model that did not take English context 
into account. The plausibility is expressed as fol-
lows: 
( )
1
? arg max ( | )
dn p
i i
i
P
=
= ?
E
E eu ku . (3.1) 
? Method B 
Used our language model and a translation model 
that did not consider English context. The con-
stant a = 3 in the language model. The plausibil-
ity is expressed as follows: 
( ) 1 ( )
1
3
1 1
? arg max ( | ) ( | )
d dl p n p
j
j j i i
j i
P e e P
+
?
?
= =
= ? ?
E
E ku eu . 
 (3.2) 
? Method C 
Applied our chunking model to method B, with c 
= 3 in the chunking model. The plausibility is 
expressed as follows: 
                                                          
4  Prepared from the 1990 Census conducted by the U.S. 
Department of Commerce. Available at 
http://www.census.gov/genealogy/names/ . The list includes 
91,910 distinct words.  
( ) 1 ( )
1
3
1 1
? arg max ( | ) ( | )
d dl p n p
j
j j i i
j i
P e e P
+
?
?
= =
= ? ?
E
E ku eu  
( ) 1
1 4
4 3
1
( | , ).
dl p
j j
j j j
j
P z z e
?
? +
? ?
=
? ?  (3.3) 
? Method D 
Used our translation model that considered Eng-
lish context, but not the chunking model. b = 3 in 
the translation model. The plausibility is ex-
pressed as follows: 
( )
( ) 1
1
3
1
( )
( ) 1
( ) 3 ( ) 1
1
? arg max ( | )
| , , .
d
d
l p
j
j j
j
n p
start i
i start i i end i
i
P e e
P e e
+
?
?
=
?
? +
=
=
?
?
?
E
E
ku eu
 (3.4) 
? Method E 
Used our language model, translation model, and 
chunking model. The plausibility is expressed as 
follows: 
( )
( ) 1
1
3
1
( )
( ) 1
( ) 3 ( ) 1
1
( ) 1
1 4
4 3
1
? arg max ( | )
| , ,
( | , ).
d
d
d
l p
j
j j
j
n p
start i
i start i i end i
i
l p
j j
j j j
j
P e e
P e e
P z z e
+
?
?
=
?
? +
=
?
? +
? ?
=
=
?
?
?
?
?
E
E
ku eu  
 (3.5) 
3.3 Results 
Table 1 shows the results of the experiment5 on 
back transliteration from Japanese katakana to 
English. The conversion was determined to be 
successful if the generated English word agreed 
perfectly with the English word in the test data. 
Table 2 shows examples of back transliterated 
words.  
Method A B C D E 
Top 1 23.7 57.4 61.6 63.1 66.4
Top 2 34.8 69.1 72.4 71.8 74.2
Top 3 42.9 73.6 76.6 75.4 79.3
Top 5 54.1 77.5 79.9 80.8 83.5
Top 10 63.4 82.0 85.3 86.5 87.7
Table 1: Ratio (%) of including the answer word 
in high-ranking words.  
                                                          
5 For model D and E, we used N=50 for the beam search 
solution. In addition, we kept paths that represented parts of 
words existing in the learning data.  
Japanese katakana 
(romanized katakana) 
Created English 
??????? 
(a shu ku ro fu to) 
Ashcroft 
????????? 
(ki ru shu shu ta i n) 
Kirschstein 
????? 
(su pe n sa -) 
Spencer 
 
???? 
(pa u e ru) 
Powell 
 
????? 
(pu ri n shi pi) 
Principi 
 
Table 2: Example of English words produced.  
4 Discussion 
The correct-match ratio of the method E for the 
first-ranked words was 66%. Its correct-match 
ratio for words up to the 10th rank was 87%.  
Regarding the top 1 ranked words, method B 
that used a language model increase the ratio 33-
points from method A that did not use a language 
model. This demonstrates the effectiveness of the 
language model. 
Also for the top 1 ranked words, method C 
which adopted the chunking model increase the 
ratio 4-points from method B that did not adopt 
the chunking model in the top 1 ranked words. 
This indicates the effectiveness of the chunking 
model. 
Method D that used a translation model taking 
English context into account had a ratio 5-points 
higher in top 1 ranked words than that of method 
B that used a translation model not taking Eng-
lish context into account. This demonstrates the 
effectiveness of the language model.  
Method E gave the best ratio. Its ratio for the 
top 1 ranked word was 42-points higher than 
method A's.  
These results demonstrate the effectiveness of 
using English context for back transliteration.  
5 Conclusion 
This paper described a method for Japanese to 
English back transliteration. Unlike conventional 
methods, our method uses a target English con-
text to calculate the plausibility of matching be-
tween English and katakana. Our method can 
treat English words that do not exist in learning 
data. We confirmed the effectiveness of our 
method in an experiment using personal names. 
We will apply this technique to cross-language 
information retrieval.  
References  
Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, 
and Shih-Chung Tsai. 1998. Proper Name Transla-
tion in Cross-Language Information Retrieval. 36th 
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference 
on Computational Linguistics, pp.232-236. 
Stanley F. Chen, Ronald Rosenfeld. 1999. A Gaussian 
Prior for Smoothing Maximum Entropy Models. 
Technical Report CMU-CS-99-108, Carnegie Mel-
lon University.  
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating Names and Technical Terms in Arabic Text. 
COLING/ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Teru-
masa Ehara. 2003. Transliteration Considering 
Context Information based on the Maximum En-
tropy Method. Machine Translation Summit IX, 
pp.125-132. 
Kil Soon Jeong, Sung Hyun Myaeng, Jae Sung Lee, 
and Key-Sun Choi. 1999. Automatic Identification 
and Back-Transliteration of Foreign Words for In-
formation Retrieval. Information Processing and 
Management, Vol.35, No.4, pp.523-540. 
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic 
Transliteration and Back-Transliteration by Deci-
sion Tree Learning. International Conference on 
Language Resources and Evaluation. pp.1135-1411. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics, Vol.24, 
No.4, pp.599-612. 
Wei-Hao Lin and Hsin-Hsi Chen. 2002. Backward 
Machine Transliteration by Learning Phonetic 
Similarity. 6th Conference on Natural Language 
Learning, pp.139-145.  
Atsushi Fujii and Tetsuya Ishikawa. 2001. Japa-
nese/English Cross-Language Information Re-
trieval: Exploration of Query Translation and 
Transliteration. Computers and the Humanities, 
Vol.35, No.4, pp.389-420. 
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 845?850,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Converting Continuous-Space Language Models into
N-gram Language Models for Statistical Machine Translation
Rui Wang1,2,3, Masao Utiyama2, Isao Goto2, Eiichro Sumita2, Hai Zhao1,3 and Bao-Liang Lu1,3
1 Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering,
Shanghai Jiao Tong Unviersity, Shanghai, 200240, China
2 Multilingual Translation Laboratory, MASTAR Project,
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
3 MOE-Microsoft Key Lab. for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong Unviersity, Shanghai 200240 China
wangrui.nlp@gmail.com, mutiyama/igoto/eiichiro.sumita@nict.go.jp, zhaohai@cs.sjtu.edu.cn, bllu@sjtu.edu.cn
Abstract
Neural network language models, or
continuous-space language models (CSLMs),
have been shown to improve the performance
of statistical machine translation (SMT)
when they are used for reranking n-best
translations. However, CSLMs have not
been used in the first pass decoding of SMT,
because using CSLMs in decoding takes a lot
of time. In contrast, we propose a method
for converting CSLMs into back-off n-gram
language models (BNLMs) so that we can
use converted CSLMs in decoding. We show
that they outperform the original BNLMs and
are comparable with the traditional use of
CSLMs in reranking.
1 Introduction
Language models are important in natural language
processing tasks such as speech recognition and
statistical machine translation. Traditionally, back-
off n-gram language models (BNLMs) (Chen and
Goodman, 1996; Chen and Goodman, 1998;
Stolcke, 2002) are being widely used for these tasks.
Recently, neural network language models,
or continuous-space language models (CSLMs)
(Bengio et al, 2003; Schwenk, 2007; Le et al, 2011)
are being used in statistical machine translation
(SMT) (Schwenk et al, 2006; Son et al, 2010;
Schwenk et al, 2012; Son et al, 2012; Niehues
and Waibel, 2012). These works have shown that
CSLMs can improve the BLEU (Papineni et al,
2002) scores of SMT when compared with BNLMs,
on the condition that the training data for language
modeling are the same size. However, in practice,
CSLMs have not been widely used in SMT.
One reason is that the computational costs of
training and using CSLMs are very high. Various
methods have been proposed to tackle the training
cost issues (Son et al, 2010; Schwenk et al, 2012;
Mikolov et al, 2011). However, there has been little
work on reducing using costs. Since the using costs
of CSLMs are very high, it is difficult to use CSLMs
in decoding directly.
A common approach in SMT using CSLMs is
the two pass approach, or n-best reranking. In this
approach, the first pass uses a BNLM in decoding
to produce an n-best list. Then, a CSLM is used to
rerank those n-best translations in the second pass.
(Schwenk et al, 2006; Son et al, 2010; Schwenk et
al., 2012; Son et al, 2012)
Another approach is using restricted Boltzmann
machines (RBMs) (Niehues and Waibel, 2012)
instead of using multi-layer neural networks
(Bengio et al, 2003; Schwenk, 2007; Le et al,
2011). Since probability in a RBM can be calculated
very efficiently (Niehues and Waibel, 2012), they
can use the RBM language model in SMT decoding.
However, the RBM was just used in an adaptation of
SMT, not in a large SMT task, because the training
costs of RBMs are very high.
The last approach is using a BNLM to simulate
a CSLM (Deoras et al, 2011; Arsoy et al, 2013).
(Deoras et al, 2011) used a recurrent neural network
language model (RNNLM) to generate a large
amount of text, which was generated by sampling
words from the probability distributions calculated
by the RNNLM. Then, they trained the BNLM
845
from the text using the interpolated Kneser-Ney
smoothing method. (Arsoy et al, 2013) converted
neural network language models of increasing order
to pruned back-off language models, using lower-
order models to constrain the n-grams allowed in
higher-order models.
Both of these methods were used in decoding for
speech recognition. These methods were applied
to not-so-large scale experiments (55 million (M)
words for training their BNLMs) (Arsoy et al,
2013). In contrast, our method is applied to SMT
and can be used to improve a BNLM created from
746 M words by using a CSLM trained from 42 M
words.
Because BNLMs can be trained from much larger
corpora than those that can be used for training
CSLMs, improving a BNLM by using a CSLM
trained from a smaller corpus is very important.
Actually, a CSLM trained from a smaller corpus
can improve the BLEU scores of SMT if it is used
in the n-best reranking (Schwenk, 2010; Huang et
al., 2013). In contrast, we will demonstrate that a
BNLM simulating a CSLM can improve the BLEU
scores of SMT in the first pass decoding.
Our approach is as follows: (1) First, we train a
CSLM (Schwenk, 2007) from a corpus. (2) Second,
we also train a BNLM from the same corpus or
larger corpus. (3) Finally, we rewrite the probability
of each n-gram of the BNLM with that probability
calculated from the CSLM.We also re-normalize the
probabilities of the BNLM, then use the re-written
BNLM in SMT decoding.
In Section 2, we describe the BNLM and CSLM
(Schwenk, 2010) used for re-writing BNLMs. In
Section 3, we describe the method of converting
a CSLM into a BNLM. In Sections 4 and 5, we
evaluate our method and conclude.
2 Language Models
In this section, we will introduce the standard
BNLM and CSLM structure and probability
calculation.
2.1 Standard back-off ngram language model
A BNLM predicts the probability of a wordwi given
its preceding n ? 1 words hi = wi?1i?n+1. But
it will suffer from data sparseness if the context,
hi, does not appear in the training data. So an
estimation by ?backing-off? to models with smaller
histories is necessary. In the case of the modified
Kneser-Ney smoothing (Chen and Goodman, 1998),
the probability of wi given hi under a BNLM,
Pb(wi|hi), is:
Pb(wi|hi) = P?b(wi|hi) + ?(hi)Pb(wi|wi?1i?n+2) (1)
where P?b(wi|hi) is a discounted probability and
?(hi) is the back-off weight. A BNLM is used with
a CSLM as shown below.
2.2 CSLM structure and probability
calculation
The main structure of a CSLM using a multi-
layer neural network contains four layers: the input
layer projects all words in the context hi onto
the projection layer (the first hidden layer); the
second hidden layer and the output layer achieve the
non-liner probability estimation and calculate the
language model probability P (wi|hi) for the given
context. (Schwenk, 2007).
The CSLM calculates the probabilities of all
words in the vocabulary of the corpus given
the context at once. However, because the
computational complexity of calculating the
probabilities of all words is quite high, the CSLM is
only used to calculate the probabilities of a subset
of the whole vocabulary. This subset is called
a short-list, which consists of the most frequent
words in the vocabulary. The CSLM also calculates
the sum of the probabilities of all words not in the
short-list by assigning a neuron for that purpose.
The probabilities of other words not in the short-list
are obtained from a BNLM (Schwenk, 2007;
Schwenk, 2010).
Let wi, hi be the current word and history. The
CSLM with a BNLM calculates the probability of
wi given hi, P (wi|hi), as follows:
P (wi|hi) =
{
Pc(wi|hi)
1?Pc(o|hi)
Ps(hi) if wi ? short-list
Pb(wi|hi) otherwise
(2)
where Pc(?) is the probability calculated by the
CSLM, Pc(o|hi) is the probability of the neuron
for the words not in the short-list, Pb(?) is the
probability calculated by the BNLM as in Eq. 1,
and
Ps(hi) =
?
v?short-list
Pb(v|hi). (3)
846
It can be considered that the CSLM redistributes
the probability mass of all words in the short-list.
This probability mass is calculated by using the
BNLM.
3 Conversion of CSLM into BNLM
As described in the introduction, we first train a
CSLM from a corpus. We also train a BNLM from
the same corpus or a larger corpus. Then, we rewrite
the probability of each ngram in the BNLM with the
probability calculated from the CSLM.
First, we use the probabilities of 1-grams in
the BNLM as they are. Next, we rewrite the
probabilities of n-grams (n=2,3,4,5) in the BNLM
with the probabilities calculated by using the n-gram
CSLM, respectively. Note that the n-gram CSLM
means that the length of its history is n ? 1. Note
also that we only need to rewrite the probabilities
of n-grams ending with a word in the short-list.
Finally, we re-normalize the probabilities of the
BNLM using the SRILM?s ?-renorm? option.
When we rewrite a BNLM trained from a larger
corpus, the ngrams in the BNLM often contain
unknown words for the CSLM. In that case, we use
the probabilities in the BNLM as they are.
4 Experiments
4.1 Common settings
We used the patent data for the Chinese to English
patent translation subtask from the NTCIR-9 patent
translation task (Goto et al, 2011). The parallel
training, development, and test data consisted of 1
M, 2,000, and 2,000 sentences, respectively.
We followed the settings of the NTCIR-9 Chinese
to English translation baseline system (Goto et al,
2011) except that we used various language models
to compare them. We used the MOSES phrase-
based SMT system (Koehn et al, 2003), together
with Giza++ (Och and Ney, 2003) for alignment and
MERT (Och, 2003) for tuning on the development
data. The translation performance was measured by
the case-insensitive BLEU scores on the tokenized
test data. We used mteval-v13a.pl for
calculating BLEU scores.1
1It is available at http://www.itl.nist.gov/iad/
mig/tests/mt/2009/
We used the 14 standard SMT features: five
translation model scores, one word penalty score,
seven distortion scores and one language model
score. Each of the different language models was
used to calculate the language model score.
As the baseline BNLM, we trained a 5-gram
BNLM with modified Kneser-Ney smoothing using
the English side of the 1 M sentences training data,
which consisted of 42 M words. We did not discard
any n-grams in training this model. That is, we
did not use count cutoffs. We call this BNLM as
BNLM42.
A 5-gram CSLM was trained on the same
1 M training sentences using the CSLM toolkit
(Schwenk, 2010). The settings for the CSLM
were: projection layer of dimension 256 for each
word, hidden layer of dimension 384 and output
layer (short-list) of dimension 8192, which were
recommended in the CSLM toolkit. We call this
CSLM CSLM42. CSLM42 used BNLM42 as the
background BNLM.
We also trained a larger 5-gram BNLM with
modified Kneser-Ney smoothing by adding
sentences from the 2005 US patent data distributed
in the NTCIR-8 patent translation task (Fujii et al,
2010) to the 42 M words. The data consisted of
746 M words. We call this BNLM BNLM746. We
discarded 3,4,5-grams that occurred only once when
we created BNLM746.
Next, we re-wrote BNLM42 with CSLM42 by
using the method described in Section 3. This
re-written BNLM was interpolated with BNLM42.
The interpolation weight was determined by the grid
search. That is, we changed the interpolation weight
to 0.1, 0.3, 0.5, 0.7, 0.9 to create an interpolated
BNLM. Then we used that BNLM in the SMT
system to tune the weight parameters on the first
half of the development data. Next, we selected
the interpolation weight that obtained the highest
BLEU score on the second half of the development
data. After we selected the interpolation weight,
we applied MERT again to the 2,000 sentence
development data to tune the weight parameters.2
We call this BNLM CONV42. We also obtained
CONV746 by re-writing BNLM746 with CSLM42
2We aware that the interpolation weight might be
determined by minimizing the perplexity on the development
data. However, we opted to directly maximize the BLEU score.
847
in the same way.
The vocabulary of these language models was the
same, which was extracted from the 1 M training
sentences.
4.2 Experimental results
Table 1 shows the percent BLEU scores on the test
data. The figures in the ?1st pass? column show
the BLEU scores in the first pass decoding when
we changed the language model. The figures in the
?reranking? column show the BLEU scores when
we applied CSLM42 to rerank the 100-best lists for
the different language models. When we applied
CSLM42 for reranking, we added the CSLM42
score as the additional 15th feature. The weight
parameters were tuned by using Z-MERT (Zaidan,
2009).
LMs 1st pass rerank
BNLM42 31.60 32.44
CONV42 32.58 32.98
BNLM746 32.83 33.36
CONV746 33.22 33.54
Table 1: Comparison of BLEU scores
We also performed the paired bootstrap re-
sampling test (Koehn, 2004).3 We sampled 2000
samples for each significance test.
Table 2 shows the results of a statistical
significance test, in which the ?1st? is short for
the ?1st pass?. The marks indicate whether the
LM to the left of a mark is significantly better
than that above the mark at a certain level. (???:
significantly better at ? = 0.01, ?>?: ? = 0.05,
???: not significantly better at ? = 0.05)
First, as shown in the tables, the reranking
by applying CSLM42 increased the BLEU scores
for all language models. This observation is in
accordance with those of previous work (Schwenk,
2010; Huang et al, 2013).
Second, the reranking results of BNLM42 (32.44)
were not better than those of the first pass of
BNLM746 (32.83). This indicates that if the
underlying BNLM is made from a small corpus, the
reranking using CSLM can not compensate for it.
3We used the code available at http://www.ark.cs.
cmu.edu/MT/.
BN
LM
74
6
(re
ra
nk
)
CO
N
V
74
6
(1
st)
CO
N
V
42
(re
ra
nk
)
BN
LM
74
6
(1
st)
CO
N
V
42
(1
st)
BN
LM
42
(re
ra
nk
)
BN
LM
42
(1
st)
CONV746 (rerank) ? ? ? ? ? ? ?
BNLM746 (rerank) ? ? > ? ? ?
CONV746 (1st) ? ? ? ? ?
CONV42 (rerank) ? ? ? ?
BNLM746 (1st) ? ? ?
CONV42 (1st) ? ?
BNLM42 (rerank) ?
Table 2: Significance tests for systems with different LMs
Third, CONV42 was better than BNLM42 for
both first-pass and reranking. This also holds in the
case of CONV746 and BNLM746. This indicated
that our conversion method improved the BNLMs,
even if the underlying BNLMwas trained on a larger
corpus than that used for training the CSLM. As
described in the introduction, this is very important
because BNLMs can be trained from much larger
corpora than those that can be used for training
CSLMs. This observation has not been found in the
previous work.
In addition, the first-pass of CONV42 and
CONV746 (32.58 and 33.22) were comparable with
those of the reranking results of BNLM42 and
BNLM746 (32.44 and 33.36), respectively. That is,
there were no significant differences between these
results. This indicates that our conversion method
preserves the performance of the reranking using
CSLM.
5 Conclusion
We have proposed a method for converting CSLMs
into BNLMs. The method can be used to improve
a BNLM by using a CSLM trained from a smaller
corpus than that used for training the BNLM. We
have also shown that BNLMs created by our method
performs as good as the reranking using CSLMs.
Our future work is to compare our conversion
method with that of (Arsoy et al, 2013).4
4We aware that (Arsoy et al, 2013) compared their method
with the one that is identical with our method. However, the
experiments were conducted on a speech recognition task and
the scale of the experiment was not so large. Since we noticed
their work just before the submission of our paper, we did not
have time to compare their method with our method in SMT.
848
Acknowledgments
We appreciate the helpful discussion with Andrew
Finch and Paul Dixon, and three anonymous
reviewers for many invaluable comments and
suggestions to improve our paper. This work
is supported by the National Natural Science
Foundation of China (Grant No. 60903119, No.
61170114 and No. 61272248), the National
Basic Research Program of China (Grant No.
2013CB329401) and the Science and Technology
Commission of Shanghai Municipality (Grant No.
13511500200).
References
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2013. Converting neural network
language models into back-off language models for
efficient decoding in automatic speech recognition.
In Proc. of IEEE Int. Conf. on Acoustics, Speech
and Signal Processing (ICASSP 2013), Vancouver,
Canada, May. IEEE.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic
language model. Journal of Machine Learning
Research (JMLR), 3:1137?1155, March.
Stanley F. Chen and Joshua Goodman. 1996. An
empirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meeting
on Association for Computational Linguistics, ACL
?96, pages 310?318, Santa Cruz, California, June.
Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical report, Computer Science Group,
Harvard Univ.
A. Deoras, T. Mikolov, S. Kombrink, M. Karafiat,
and Sanjeev Khudanpur. 2011. Variational
approximation of long-span language models for lvcsr.
In Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages 5532?
5535, Prague, Czech Republic, May. IEEE.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2010. Overview of the patent
translation task at the ntcir-8 workshop. In In
Proceedings of the 8th NTCIR Workshop Meeting
on Evaluation of Information Access Technologies:
Information Retrieval, Question Answering and Cross-
lingual Information Access, pages 293?302, Tokyo,
Japan, June.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR-9 Workshop Meeting, pages
559?578, Tokyo, Japan, December.
Zhongqiang Huang, Jacob Devlin, and Spyros
Matsoukas. 2013. Bbn?s systems for the chinese-
english sub-task of the ntcir-10 patentmt evaluation.
In NTCIR-10, Tokyo, Japan, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the
North American Chapter of the Association for
Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54,
Edmonton, Canada. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and
F. Yvon. 2011. Structured output layer neural
network language model. In Acoustics, Speech and
Signal Processing (ICASSP), 2011 IEEE International
Conference on, pages 5524?5527, Prague, Czech
Republic, May. IEEE.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernock. 2011. Strategies for
training large scale neural network language models.
In Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages 196?
201, Prague, Czech Republic, May. IEEE.
Jan Niehues and Alex Waibel. 2012. Continuous space
language models using restricted boltzmann machines.
In Proceedings of the International Workshop for
Spoken Language Translation, IWSLT 2012, pages
311?318, Hong Kong.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for
Computational Linguistics, ACL ?02, pages 311?
849
318, Philadelphia, Pennsylvania, June. Association for
Computational Linguistics.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proceedings
of the COLING/ACL on Main conference poster
sessions, COLING-ACL ?06, pages 723?730, Sydney,
Australia, July. Association for Computational
Linguistics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of LanguageModeling for HLT,
WLM ?12, pages 11?19, Montreal, Canada, June.
Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492?
518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. The Prague
Bulletin of Mathematical Linguistics, pages 137?146.
Le Hai Son, Alexandre Allauzen, Guillaume Wisniewski,
and Franc?ois Yvon. 2010. Training continuous
space language models: some practical issues. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 778?788, Cambridge, Massachusetts,
October. Association for Computational Linguistics.
Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL HLT ?12, pages
39?48, Montreal, Canada, June. Association for
Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings International
Conference on Spoken Language Processing, pages
257?286, November.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
850
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311?316,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Post-ordering by Parsing for Japanese-English Statistical
Machine Translation
Isao Goto Masao Utiyama
Multilingual Translation Laboratory, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
{igoto, mutiyama, eiichiro.sumita}@nict.go.jp
Eiichiro Sumita
Abstract
Reordering is a difficult task in translating
between widely different languages such as
Japanese and English. We employ the post-
ordering framework proposed by (Sudoh et
al., 2011b) for Japanese to English transla-
tion and improve upon the reordering method.
The existing post-ordering method reorders
a sequence of target language words in a
source language word order via SMT, while
our method reorders the sequence by: 1) pars-
ing the sequence to obtain syntax structures
similar to a source language structure, and 2)
transferring the obtained syntax structures into
the syntax structures of the target language.
1 Introduction
The word reordering problem is a challenging one
when translating between languages with widely
different word orders such as Japanese and En-
glish. Many reordering methods have been proposed
in statistical machine translation (SMT) research.
Those methods can be classified into the following
three types:
Type-1: Conducting the target word selection and
reordering jointly. These include phrase-based SMT
(Koehn et al, 2003), hierarchical phrase-based SMT
(Chiang, 2007), and syntax-based SMT (Galley et
al., 2004; Ding and Palmer, 2005; Liu et al, 2006;
Liu et al, 2009).
Type-2: Pre-ordering (Xia and McCord, 2004;
Collins et al, 2005; Tromble and Eisner, 2009; Ge,
2010; Isozaki et al, 2010b; DeNero and Uszkoreit,
2011; Wu et al, 2011). First, these methods re-
order the source language sentence into the target
language word order. Then, they translate the re-
ordered source word sequence using SMT methods.
Type-3: Post-ordering (Sudoh et al, 2011b; Ma-
tusov et al, 2005). First, these methods translate
the source sentence almost monotonously into a se-
quence of the target language words. Then, they
reorder the translated word sequence into the target
language word order.
This paper employs the post-ordering framework
for Japanese-English translation based on the dis-
cussions given in Section 2, and improves upon the
reordering method. Our method uses syntactic struc-
tures, which are essential for improving the target
word order in translating long sentences between
Japanese (a Subject-Object-Verb (SOV) language)
and English (an SVO language).
Before explaining our method, we explain the pre-
ordering method for English to Japanese used in the
post-ordering framework.
In English-Japanese translation, Isozaki et al
(2010b) proposed a simple pre-ordering method that
achieved the best quality in human evaluations,
which were conducted for the NTCIR-9 patent ma-
chine translation task (Sudoh et al, 2011a; Goto et
al., 2011). The method, which is called head final-
ization, simply moves syntactic heads to the end of
corresponding syntactic constituents (e.g., phrases
and clauses). This method first changes the English
word order into a word order similar to Japanese
word order using the head finalization rule. Then,
it translates (almost monotonously) the pre-ordered
311
Japanese
HFE
monotone translation
English
post-ordering
Figure 1: Post-ordering framework.
English words into Japanese.
There are two key reasons why this pre-ordering
method works for estimating Japanese word order.
The first reason is that Japanese is a typical head-
final language. That is, a syntactic head word comes
after nonhead (dependent) words. Second, input En-
glish sentences are parsed by a high-quality parser,
Enju (Miyao and Tsujii, 2008), which outputs syn-
tactic heads. Consequently, the parsed English in-
put sentences can be pre-ordered into a Japanese-
like word order using the head finalization rule.
Pre-ordering using the head finalization rule nat-
urally cannot be applied to Japanese-English trans-
lation, because English is not a head-final language.
If we want to pre-order Japanese sentences into an
English-like word order, we therefore have to build
complex rules (Sudoh et al, 2011b).
2 Post-ordering for Japanese to English
Sudoh et al (2011b) proposed a post-ordering
method for Japanese-English translation. The trans-
lation flow for the post-ordering method is shown in
Figure 1, where ?HFE? is an abbreviation of ?Head
Final English?. An HFE sentence consists of En-
glish words in a Japanese-like structure. It can be
constructed by applying the head-finalization rule
(Isozaki et al, 2010b) to an English sentence parsed
by Enju. Therefore, if good rules are applied to this
HFE sentence, the underlying English sentence can
be recovered. This is the key observation of the post-
ordering method.
The process of post-ordering translation consists
of two steps. First, the Japanese input sentence is
translated into HFE almost monotonously. Then, the
word order of HFE is changed into an English word
order.
Training for the post-ordering method is con-
ducted by first converting the English sentences in
a Japanese-English parallel corpus into HFE sen-
tences using the head-finalization rule. Next, a
monotone phrase-based Japanese-HFE SMT model
is built using the Japanese-HFE parallel corpus
Japanese: kare    wa        kinou        hon        wo       katta
HFE:
he    
_va0
    yesterday    books    
_va2
    bought
HFE:
he    
_va0
    yesterday    books    
_va2
    bought
NP_ST NP_ST
VP_SW
VP_SW
S_ST
English:
he   (
_va0
)   bought    books   (
_va2
)   yesterday
NP NP
VP
VP
S
Parsing
Reordering
Figure 2: Example of post-ordering by parsing.
whose HFE was converted from English. Finally,
an HFE-to-English word reordering model is built
using the HFE-English parallel corpus.
3 Post-ordering Models
3.1 SMT Model
Sudoh et al (2011b) have proposed using phrase-
based SMT for converting HFE sentences into En-
glish sentences. The advantage of their method is
that they can use off-the-shelf SMT techniques for
post-ordering.
3.2 Parsing Model
Our proposed model is called the parsing model.
The translation process for the parsing model is
shown in Figure 2. In this method, we first parse the
HFE sentence into a binary tree. We then swap the
nodes annotated with ? SW? suffixes in this binary
tree in order to produce an English sentence.
The structures of the HFE sentences, which are
used for training our parsing model, can be obtained
from the corresponding English sentences as fol-
lows.1 First, each English sentence in the training
Japanese-English parallel corpus is parsed into a bi-
nary tree by applying Enju. Then, for each node in
this English binary tree, the two children of each
node are swapped if its first child is the head node
(See (Isozaki et al, 2010b) for details of the head
1The explanations of pseudo-particles ( va0 and va2) and
other details of the HFE is given in Section 4.2.
312
final rules). At the same time, these swapped nodes
are annotated with ? SW?. When the two nodes are
not swapped, they are annotated with ? ST? (indi-
cating ?Straight?). A node with only one child is
not annotated with either ? ST? or ? SW?. The re-
sult is an HFE sentence in a binary tree annotated
with ? SW? and ? ST? suffixes.
Observe that the HFE sentences can be regarded
as binary trees annotated with syntax tags aug-
mented with swap/straight suffixes. Therefore, the
structures of these binary trees can be learnable by
using an off-the-shelf grammar learning algorithm.
The learned parsing model can be regarded as an
ITG model (Wu, 1997) between the HFE and En-
glish sentences. 2
In this paper, we used the Berkeley Parser (Petrov
and Klein, 2007) for learning these structures. The
HFE sentences can be parsed by using the learned
parsing model. Then the parsed structures can be
converted into their corresponding English struc-
tures by swapping the ? SW? nodes. Note that this
parsing model jointly learns how to parse and swap
the HFE sentences.
4 Detailed Explanation of Our Method
This section explains the proposed method, which
is based on the post-ordering framework using the
parsing model.
4.1 Translation Method
First, we produce N-best HFE sentences us-
ing Japanese-to-HFE monotone phrase-based SMT.
Next, we produce K-best parse trees for each HFE
sentence by parsing, and produce English sentences
by swapping any nodes annotated with ? SW?. Then
we score the English sentences and select the En-
glish sentence with the highest score.
For the score of an English sentence, we use
the sum of the log-linear SMT model score for
Japanese-to-HFE and the logarithm of the language
model probability of the English sentence.
2There are works using the ITG model in SMT: ITG was
used for training pre-ordering models (DeNero and Uszkoreit,
2011); hierarchical phrase-based SMT (Chiang, 2007), which is
an extension of ITG; and reordering models using ITG (Chen et
al., 2009; He et al, 2010). These methods are not post-ordering
methods.
4.2 HFE and Articles
This section describes the details of HFE sentences.
In HFE sentences: 1) Heads are final except for
coordination. 2) Pseudo-particles are inserted after
verb arguments: va0 (subject of sentence head),
va1 (subject of verb), and va2 (object of verb).
3) Articles (a, an, the) are dropped.
In our method of HFE construction, unlike that
used by (Sudoh et al, 2011b), plural nouns are left
as-is instead of converted to the singular.
Applying our parsing model to an HFE sentence
produces an English sentence that does not have
articles, but does have pseudo-particles. We re-
moved the pseudo-particles from the reordered sen-
tences before calculating the probabilities used for
the scores of the reordered sentences. A reordered
sentence without pseudo-particles is represented by
E. A language model P (E) was trained from En-
glish sentences whose articles were dropped.
In order to output a genuine English sentence E?
fromE, articles must be inserted intoE. A language
model trained using genuine English sentences is
used for this purpose. We try to insert one of the
articles {a, an, the} or no article for each word in E.
Then we calculate the maximum probability word
sequence through dynamic programming for obtain-
ing E?.
5 Experiment
5.1 Setup
We used patent sentence data for the Japanese to
English translation subtask from the NTCIR-9 and
8 (Goto et al, 2011; Fujii et al, 2010). There
were 2,000 test sentences for NTCIR-9 and 1,251
for NTCIR-8. XML entities included in the data
were decoded to UTF-8 characters before use.
We used Enju (Miyao and Tsujii, 2008) v2.4.2 for
parsing the English side of the training data. Mecab
3 v0.98 was used for the Japanese morphological
analysis. The translation model was trained using
sentences of 64 words or less from the training cor-
pus as (Sudoh et al, 2011b). We used 5-gram lan-
guage models using SRILM (Stolcke et al, 2011).
We used the Berkeley parser (Petrov and Klein,
2007) to train the parsing model for HFE and to
3http://mecab.sourceforge.net/
313
parse HFE. The parsing model was trained using 0.5
million sentences randomly selected from training
sentences of 40 words or less. We used the phrase-
based SMT system Moses (Koehn et al, 2007) to
calculate the SMT score and to produce HFE sen-
tences. The distortion limit was set to 0. We used
10-best Moses outputs and 10-best parsing results
of Berkeley parser.
5.2 Compared Methods
We used the following 5 comparison methods:
Phrase-based SMT (PBMT), Hierarchical phrase-
based SMT (HPBMT), String-to-tree syntax-based
SMT (SBMT), Post-ordering based on phrase-based
SMT (PO-PBMT) (Sudoh et al, 2011b), and Post-
ordering based on hierarchical phrase-based SMT
(PO-HPBMT).
We used Moses for these 5 systems. For
PO-PBMT, a distortion limit 0 was used for the
Japanese-to-HFE translation and a distortion limit
20 was used for the HFE-to-English translation.
The PO-HPBMT method changes the post-ordering
method of PO-PBMT from a phrase-based SMT
to a hierarchical phrase-based SMT. We used a
max-chart-span 15 for the hierarchical phrase-based
SMT. We used distortion limits of 12 or 20 for
PBMT and a max-chart-span 15 for HPBMT.
The parameters for SMT were tuned by MERT
using the first half of the development data with HFE
converted from English.
5.3 Results and Discussion
We evaluated translation quality based on the case-
insensitive automatic evaluation scores of RIBES
v1.1 (Isozaki et al, 2010a) and BLEU-4. The results
are shown in Table 1.
Ja-to-En NTCIR-9 NTCIR-8
RIBES BLEU RIBES BLEU
Proposed 72.57 31.75 73.48 32.80
PBMT (limit 12) 68.44 29.64 69.18 30.72
PBMT (limit 20) 68.86 30.13 69.63 31.22
HPBMT 69.92 30.15 70.18 30.94
SBMT 69.22 29.53 69.87 30.37
PO-PBMT 68.81 30.39 69.80 31.71
PO-HPBMT 70.47 27.49 71.34 28.78
Table 1: Evaluation results (case insensitive).
From the results, the proposed method achieved
the best scores for both RIBES and BLEU for
NTCIR-9 and NTCIR-8 test data. Since RIBES is
sensitive to global word order and BLEU is sensitive
to local word order, the effectiveness of the proposed
method for both global and local reordering can be
demonstrated through these comparisons.
In order to investigate the effects of our post-
ordering method in detail, we conducted an ?HFE-
to-English reordering? experiment, which shows the
main contribution of our post-ordering method in
the framework of post-ordering SMT as compared
with (Sudoh et al, 2011b). In this experiment, we
changed the word order of the oracle-HFE sentences
made from reference sentences into English, this is
the same way as Table 4 in (Sudoh et al, 2011b).
The results are shown in Table 2.
This results show that our post-ordering method
is more effective than PO-PBMT and PO-HPBMT.
Since RIBES is based on the rank order correla-
tion coefficient, these results show that the proposed
method correctly recovered the word order of the
English sentences. These high scores also indicate
that the parsing results for high quality HFE are
fairly trustworthy.
oracle-HFE-to-En NTCIR-9 NTCIR-8
RIBES BLEU RIBES BLEU
Proposed 94.66 80.02 94.93 79.99
PO-PBMT 77.34 62.24 78.14 63.14
PO-HPBMT 77.99 53.62 80.85 58.34
Table 2: Evaluation resutls focusing on post-ordering.
In these experiments, we did not compare our
method to pre-ordering methods. However, some
groups used pre-ordering methods in the NTCIR-9
Japanese to English translation subtask. The NTT-
UT (Sudoh et al, 2011a) and NAIST (Kondo et al,
2011) groups used pre-ordering methods, but could
not produce RIBES and BLEU scores that both were
better than those of the baseline results. In contrast,
our method was able to do so.
6 Conclusion
This paper has described a new post-ordering
method. The proposed method parses sentences that
consist of target language words in a source lan-
guage word order, and does reordering by transfer-
ring the syntactic structures similar to the source lan-
guage syntactic structures into the target language
syntactic structures.
314
References
Han-Bin Chen, Jian-Cheng Wu, and Jason S. Chang.
2009. Learning Bilingual Linguistic Reordering
Model for Statistical Machine Translation. In Pro-
ceedings of Human Language Technologies: The 2009
NAACL, pages 254?262, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the 43rd ACL, pages
531?540, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
John DeNero and Jakob Uszkoreit. 2011. Inducing Sen-
tence Structure from Parallel Corpora for Reordering.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 193?
203, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Yuan Ding and Martha Palmer. 2005. Machine Transla-
tion Using Probabilistic Synchronous Dependency In-
sertion Grammars. In Proceedings of the 43rd ACL,
pages 541?548, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-
hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and
Sayori Shimohata. 2010. Overview of the Patent
Translation Task at the NTCIR-8 Workshop. In Pro-
ceedings of NTCIR-8, pages 371?376.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
273?280, Boston, Massachusetts, USA, May 2 - May
7. Association for Computational Linguistics.
Niyu Ge. 2010. A Direct Syntax-Driven Reordering
Model for Phrase-Based Machine Translation. In Pro-
ceedings of NAACL-HLT, pages 849?857, Los Ange-
les, California, June. Association for Computational
Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent Ma-
chine Translation Task at the NTCIR-9 Workshop. In
Proceedings of NTCIR-9, pages 559?578.
Yanqing He, Yu Zhou, Chengqing Zong, and Huilin
Wang. 2010. A Novel Reordering Model Based on
Multi-layer Phrase for Statistical Machine Translation.
In Proceedings of the 23rd Coling, pages 447?455,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic Eval-
uation of Translation Quality for Distant Language
Pairs. In Proceedings of the 2010 EMNLP, pages 944?
952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head Finalization: A Simple Re-
ordering Rule for SOV Languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 244?251, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of the 2003 HLT-NAACL, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th ACL, pages 177?180, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Shuhei Kondo, Mamoru Komachi, Yuji Matsumoto, Kat-
suhito Sudoh, Kevin Duh, and Hajime Tsukada. 2011.
Learning of Linear Ordering Problems and its Applica-
tion to J-E Patent Translation in NTCIR-9 PatentMT.
In Proceedings of NTCIR-9, pages 641?645.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proceedings of the 21st ACL, pages
609?616, Sydney, Australia, July. Association for
Computational Linguistics.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
Tree-to-Tree Translation with Packed Forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558?566, Suntec, Singapore, August.
Association for Computational Linguistics.
E. Matusov, S. Kanthak, and Hermann Ney. 2005. On
the Integration of Speech Recognition and Statistical
Machine Translation. In Proceedings of Interspeech,
pages 3177?3180.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics, Volume 34, Number 1, pages 81?88.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. InNAACL-HLT, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at Sixteen: Update and
Outlook. In Proceedings of IEEE Automatic Speech
Recognition and Understanding Workshop.
315
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Masaaki
Nagata, Xianchao Wu, Takuya Matsuzaki, and
Jun?ichi Tsujii. 2011a. NTT-UT Statistical Machine
Translation in NTCIR-9 PatentMT. In Proceedings of
NTCIR-9, pages 585?592.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011b. Post-ordering
in Statistical Machine Translation. In Proceedings of
the 13th Machine Translation Summit, pages 316?323.
Roy Tromble and Jason Eisner. 2009. Learning Linear
Ordering Problems for Better Translation. In Proceed-
ings of the 2009 EMNLP, pages 1007?1016, Singa-
pore, August. Association for Computational Linguis-
tics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting Pre-
ordering Rules from Chunk-based Dependency Trees
for Japanese-to-English Translation. In Proceedings
of the 13th Machine Translation Summit, pages 300?
307.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
Fei Xia and Michael McCord. 2004. Improving a Statis-
tical MT System with Automatically Learned Rewrite
Patterns. In Proceedings of Coling, pages 508?514,
Geneva, Switzerland, Aug 23?Aug 27. COLING.
316
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 155?165,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Distortion Model Considering Rich Context
for Statistical Machine Translation
Isao Goto?,? Masao Utiyama? Eiichiro Sumita?
Akihiro Tamura? Sadao Kurohashi?
?National Institute of Information and Communications Technology
?Kyoto University
goto.i-es@nhk.or.jp
{mutiyama, eiichiro.sumita, akihiro.tamura}@nict.go.jp
kuro@i.kyoto-u.ac.jp
Abstract
This paper proposes new distortion mod-
els for phrase-based SMT. In decoding, a
distortion model estimates the source word
position to be translated next (NP) given
the last translated source word position
(CP). We propose a distortion model that
can consider the word at the CP, a word
at an NP candidate, and the context of the
CP and the NP candidate simultaneously.
Moreover, we propose a further improved
model that considers richer context by dis-
criminating label sequences that specify
spans from the CP to NP candidates. It
enables our model to learn the effect of
relative word order among NP candidates
as well as to learn the effect of distances
from the training data. In our experiments,
our model improved 2.9 BLEU points for
Japanese-English and 2.6 BLEU points for
Chinese-English translation compared to
the lexical reordering models.
1 Introduction
Estimating appropriate word order in a target lan-
guage is one of the most difficult problems for
statistical machine translation (SMT). This is par-
ticularly true when translating between languages
with widely different word orders.
To address this problem, there has been a lot
of research done into word reordering: lexical
reordering model (Tillman, 2004), which is one
of the distortion models, reordering constraint
(Zens et al, 2004), pre-ordering (Xia and Mc-
Cord, 2004), hierarchical phrase-based SMT (Chi-
ang, 2007), and syntax-based SMT (Yamada and
Knight, 2001).
In general, source language syntax is useful for
handling long distance word reordering. However,
obtaining syntax requires a syntactic parser, which
is not available for many languages. Phrase-based
SMT (Koehn et al, 2007) is a widely used SMT
method that does not use a parser.
Phrase-based SMT mainly1 estimates word re-
ordering using distortion models2. Therefore, dis-
tortion models are one of the most important com-
ponents for phrase-based SMT. On the other hand,
there are methods other than distortion models for
improving word reordering for phrase-based SMT,
such as pre-ordering or reordering constraints.
However, these methods also use distortion mod-
els when translating by phrase-based SMT. There-
fore, distortion models do not compete against
these methods and are commonly used with them.
If there is a good distortion model, it will improve
the translation quality of phrase-based SMT and
benefit to the methods using distortion models.
In this paper, we propose two distortion mod-
els for phrase-based SMT. In decoding, a distor-
tion model estimates the source word position to
be translated next (NP) given the last translated
source word position (CP). The proposed models
are the pair model and the sequence model. The
pair model utilizes the word at the CP, a word at
an NP candidate site, and the words surrounding
the CP and the NP candidates (context) simultane-
ously. In addition, the sequence model, which is
the further improved model, considers richer con-
text by identifying the label sequence that spec-
ify the span from the CP to the NP. It enables
our model to learn the effect of relative word or-
der among NP candidates as well as to learn the
effect of distances from the training data. Our
model learns the preference relations among NP
1A language model also supports the estimation.
2In this paper, reordering models for phrase-based SMT,
which are intended to estimate the source word position to
be translated next in decoding, are called distortion models.
This estimation is used to produce a hypothesis in the target
language word order sequentially from left to right.
155
kinou  kare  wa  pari  de  hon  wo  katta
he   bought   books   in   Paris   yesterday
Source:
Target:
Figure 1: An example of left-to-right translation
for Japanese-English. Boxes represent phrases
and arrows indicate the translation order of the
phrases.
candidates. Our model consists of one probabilis-
tic model and does not require a parser. Exper-
iments confirmed the effectiveness of our method
for Japanese-English and Chinese-English transla-
tion, using NTCIR-9 Patent Machine Translation
Task data sets (Goto et al, 2011).
2 Distortion Model for Phrase-Based
SMT
A Moses-style phrase-based SMT generates target
hypotheses sequentially from left to right. There-
fore, the role of the distortion model is to esti-
mate the source phrase position to be translated
next whose target side phrase will be located im-
mediately to the right of the already generated hy-
potheses. An example is shown in Figure 1. In
Figure 1, we assume that only the kare wa (En-
glish side: ?he?) has been translated. The target
word to be generated next will be ?bought? and the
source word to be selected next will be its corre-
sponding Japanese word katta. Thus, a distortion
model should estimate phrases including katta as
a source phrase position to be translated next.
To explain the distortion model task in more de-
tail, we need to redefine more precisely two terms,
the current position (CP) and next position (NP) in
the source sentence. CP is the source sentence po-
sition corresponding to the rightmost aligned tar-
get word in the generated target word sequence.
NP is the source sentence position corresponding
to the leftmost aligned target word in the target
phrase to be generated next. The task of the distor-
tion model is to estimate the NP3 from NP candi-
dates (NPCs) for each CP in the source sentence.4
3NP is not always one position, because there may be mul-
tiple correct hypotheses.
4This definition is slightly different from that of existing
methods such as Moses and (Green et al, 2010). In existing
methods, CP is the rightmost position of the last translated
source phrase and NP is the leftmost position of the source
phrase to be translated next. Note that existing methods do
kinou
1
 kare
2
 wa
3
 pari
4
 de
5
 hon
6
 wo
7
 katta
8
he   bought   books   in   Paris   yesterday
(a)
kinou
1
 kare
2
 wa
3
 pari
4
 de
5
 ni
6
 satsu
7
 hon
8
 wo
9
 katta
10
he   bought   two   books   in   Paris   yesterday
(b)
kinou
1
 kare
2
 wa
3
 hon
4
 wo
5
 karita
6
 ga
7
 kanojo
8
 wa
9
 katta
10
he   borrowed   books  yesterday  but  she  bought
(c)
kinou
1
 kare
2
 wa
3
 kanojo
4
 ga
5
 katta
6
 hon
7
 wo
8
 karita
9
yesterday  he  borrowed  the  books  that  she  bought
(e)
kinou
1
 kare
2
 wa
3
 hon
4
 wo
5
 katta
6
 ga
7
 kanojo
8
 wa
9
 karita
10
he   bought   books   yesterday   but   she   borrowed
(d)
??
?~
??
??
?~
??
?~
???~
CP NP
Figure 2: Examples of CP and NP for Japanese-
English translation. The upper sentence is the
source sentence and the sentence underneath is a
target hypothesis for each example. The NP is in
bold, and the CP is in bold italics. The point of an
arrow with a ? mark indicates a wrong NP candi-
date.
Estimating NP is a difficult task. Figure 2 shows
some examples. The superscript numbers indicate
the word position in the source sentence.
In Figure 2 (a), the NP is 8. However, in Fig-
ure 2 (b), the word (kare) at the CP is the same as
(a), but the NP is different (the NP is 10). From
these examples, we see that distance is not the es-
sential factor in deciding an NP. And it also turns
out that the word at the CP alone is not enough to
estimate the NP. Thus, not only the word at the CP
but also the word at a NP candidate (NPC) should
be considered simultaneously.
In (c) and (d) in Figure 2, the word (kare) at the
CP is the same and karita (borrowed) and katta
(bought) are at the NPCs. Karita is the word at
the NP and katta is not the word at the NP for
(c), while katta is the word at the NP and karita
is not the word at the NP for (d). From these ex-
amples, considering what the word is at the NP
not consider word-level correspondences.
156
is not enough to estimate the NP. One of the rea-
sons for this difference is the relative word order
between words. Thus, considering relative word
order is important.
In (d) and (e) in Figure 2, the word (kare) at the
CP and the word order between katta and karita
are the same. However, the word at the NP for
(d) and the word at the NP for (e) are different.
From these examples, we can see that selecting
a nearby word is not always correct. The differ-
ence is caused by the words surrounding the NPCs
(context), the CP context, and the words between
the CP and the NPC. Thus, these should be con-
sidered when estimating the NP.
In summary, in order to estimate the NP, the fol-
lowing should be considered simultaneously: the
word at the NP, the word at the CP, the relative
word order among the NPCs, the words surround-
ing NP and CP (context), and the words between
the CP and the NPC.
There are distortion models that do not require
a parser for phrase-based SMT. The linear dis-
tortion cost model used in Moses (Koehn et al,
2007), whose costs are linearly proportional to
the reordering distance, always gives a high cost
to long distance reordering, even if the reorder-
ing is correct. The MSD lexical reordering model
(Tillman, 2004; Koehn et al, 2005; Galley and
Manning, 2008) only calculates probabilities for
the three kinds of phrase reorderings (monotone,
swap, and discontinuous), and does not consider
relative word order or words between the CP and
the NPC. Thus, these models are not sufficient for
long distance word reordering.
Al-Onaizan and Papineni (2006) proposed a
distortion model that used the word at the CP and
the word at an NPC. However, their model did not
use context, relative word order, or words between
the CP and the NPC.
Ni et al (2009) proposed a method that adjusts
the linear distortion cost using the word at the CP
and its context. Their model does not simultane-
ously consider both the word specified at the CP
and the word specified at the NPCs.
Green et al (2010) proposed distortion mod-
els that used context. Their model (the outbound
model) estimates how far the NP should be from
the CP using the word at the CP and its con-
text.5 Their model does not simultaneously con-
5They also proposed another model (the inbound model)
sider both the word specified at the CP and the
word specified at an NPC. For example, the out-
bound model considers the word specified at the
CP, but does not consider the word specified at an
NPC. Their models also do not consider relative
word order.
In contrast, our distortion model solves the
aforementioned problems. Our distortion models
utilize the word specified at the CP, the word spec-
ified at an NPC, and also the context of the CP
and the NPC simultaneously. Furthermore, our se-
quence model considers richer context including
the relative word order among NPCs and also in-
cluding all the words between the CP and the NPC.
In addition, unlike previous methods, our models
learn the preference relations among NPCs.
3 Proposed Method
In this section, we first define our distortion model
and explain our learning strategy. Then, we de-
scribe two proposed models: the pair model and
the sequence model that is the further improved
model.
3.1 Distortion Model and Learning Strategy
First, we define our distortion model. Let i be a
CP, j be an NPC, S be a source sentence, andX be
the random variable of the NP. In this paper, dis-
tortion probability is defined as P (X = j|i, S),
which is the probability of an NPC j being the NP.
Our distortion model is defined as the model cal-
culating the distortion probability.
Next, we explain the learning strategy for our
distortion model. We train this model as a dis-
criminative model that discriminates the NP from
NPCs. Let J be a set of word positions in S other
than i. We train the distortion model subject to
?
j?J
P (X = j|i, S) = 1.
The model parameters are learned to maximize the
distortion probability of the NP among all of the
NPCs J in each source sentence. This learning
strategy is a kind of preference relation learning
(Evgniou and Pontil, 2002). In this learning, the
that estimates reverse direction distance. Each NPC is re-
garded as an NP, and the inbound model estimates how far
the corresponding CP should be from the NP using the word
at the NP and its context.
157
distortion probability of the actual NP will be rel-
atively higher than those of all the other NPCs J .
This learning strategy is different from that of
(Al-Onaizan and Papineni, 2006; Green et al,
2010). For example, Green et al (2010) trained
their outbound model subject to ?c?C P (Y =
c|i, S) = 1, where C is the set of the nine distor-
tion classes6 and Y is the random variable of the
correct distortion class that the correct distortion is
classified into. Distortion is defined as j ? i ? 1.
Namely, the model probabilities that they learned
were the probabilities of distortion classes in all
of the training data, not the relative preferences
among the NPCs in each source sentence.
3.2 Pair Model
The pair model utilizes the word at the CP, the
word at an NPC, and the context of the CP and the
NPC simultaneously to estimate the NP. This can
be done by our distortion model definition and the
learning strategy described in the previous section.
In this work, we use the maximum entropy
method (Berger et al, 1996) as a discriminative
machine learning method. The reason for this
is that a model based on the maximum entropy
method can calculate probabilities. However, if
we use scores as an approximation of the distor-
tion probabilities, various discriminative machine
learning methods can be applied to build the dis-
tortion model.
Let s be a source word and sn1 = s1s2...sn be
a source sentence. We add a beginning of sen-
tence (BOS) marker to the head of the source sen-
tence and an end of sentence (EOS) marker to the
end, so the source sentence S is expressed as sn+10
(s0 = BOS, sn+1 = EOS). Our distortion model
calculates the distortion probability for an NPC
j ? {j|1 ? j ? n + 1 ? j ?= i} for each CP
i ? {i|0 ? i ? n}
P (X = j|i, S) = 1Zi
exp
(
wTf (i, j, S, o, d)
)
(1)
where
o =
{
0 (i < j)
1 (i > j) , d =
?
??
??
0 (|j ? i| = 1)
1 (2 ? |j ? i| ? 5)
2 (6 ? |j ? i|)
,
6(??,?8], [?7,?5], [?4,?3], ?2, 0, 1, [2, 3], [4, 6],
and [7,?). In (Green et al, 2010), ?1 was used as one of
distortion classes. However, ?1 represents the CP in our def-
inition, and CP is not an NPC. Thus, we shifted all of the
distortion classes for negative distortions by ?1.
Template
?o?, ?o, sp?1, ?o, ti?, ?o, tj?, ?o, d?, ?o, sp, sq?2,
?o, ti, tj?, ?o, ti?1, ti, tj?, ?o, ti, ti+1, tj?,
?o, ti, tj?1, tj?, ?o, ti, tj , tj+1?, ?o, si, ti, tj?,
?o, sj , ti, tj?
1 p ? {p|i? 2 ? p ? i + 2 ? j ? 2 ? p ? j + 2}
2 (p, q) ? {(p, q)|i ? 2 ? p ? i + 2 ? j ? 2 ? q ?
j + 2 ? (|p? i| ? 1 ? |q ? j| ? 1)}
Table 1: Feature templates. t is the part of speech
of s.
w is a weight parameter vector, each element
of f(?) is a binary feature function, and Zi =?
j?{j|1?j?n+1 ? j ?=i}(numerator of Equation 1)
is a normalization factor. o is an orientation of i to
j and d is a distance class.
The binary feature function that constitutes an
element of f(?) returns 1 when its feature is
matched and if else, returns 0. Table 1 shows the
feature templates used to produce the features. A
feature is an instance of a feature template.
In Equation 1, i, j, and S are used by the feature
functions. Thus, Equation 1 can utilize features
consisting of both si, which is the word specified
at i, and sj , which is the word specified at j, or
both the context of i and the context of j simulta-
neously. Distance is considered using the distance
class d. Distortion is represented by distance and
orientation. The pair model considers distortion
using six joint classes of d and o.
3.3 Sequence Model
The pair model does not consider relative word or-
der among NPCs or all the words between the CP
and an NPC. In this section, we propose a further
improved model, the sequence model, which con-
siders richer context including relative word order
among NPCs and also including all the words be-
tween the CP and an NPC.
In (c) and (d) in Figure 2, karita (borrowed) and
katta (bought) occur in the source sentences. The
pair model considers the effect of distances using
only the distance class d. If these positions are
in the same distance class, the pair model cannot
consider the differences in distances. In this case,
these are conflict instances during training and it
is difficult to distinguish the NP for translation.
Now to explain how to consider the relative
word order by the sequence model. The sequence
model considers the relative word order by dis-
criminating the label sequence corresponding to
the NP from the label sequences corresponding to
158
Label Description
C A position is the CP.
I A position is a position between the CP
and the NPC.
N A position is the NPC.
Table 2: The ?C, I, and N? label set.
La
be
ls
eq
ue
nc
e
ID
1 N C
3 C N
4 C I N
5 C I I N
6 C I I I N
7 C I I I I N
8 C I I I I I N
9 C I I I I I I N
10 C I I I I I I I N
11 C I I I I I I I I N
B
O
S0
ki
no
u1
ka
re
2
w
a3
ho
n4
w
o5
ka
ri
ta
6
ga
7
ka
no
jo
8
w
a9
ka
tta
10
EO
S1
1
(y
es
te
rd
ay
)
(h
e)
(b
oo
k)
(b
or
ro
w
ed
)
(s
he
)
(b
ou
gh
t)
Source sentence
Figure 3: Example of label sequences that specify
spans from the CP to each NPC for the case of
Figure 2 (c). The labels (C, I, and N) in the boxes
are the label sequences.
each NPC in each sentence. Each label sequence
corresponds to one NPC. Therefore, if we identify
the label sequence that corresponds to the NP, we
can obtain the NP. The label sequences specify the
spans from the CP to each NPC using three kinds
of labels indicating the type of word positions in
the spans. The three kinds of labels, ?C, I, and N,?
are shown in Table 2. Figure 3 shows examples
of the label sequences for the case of Figure 2 (c).
In Figure 3, the label sequences are represented by
boxes and the elements of the sequences are labels.
The NPC is used as the label sequence ID for each
label sequence.
The label sequence can treat relative word or-
der. For example, the label sequence ID of 10 in
Figure 3 knows that karita exists to the left of the
NPC of 10. This is because karita6 carries a la-
bel I while katta10 carries a label N, and a position
with label I is defined as relatively closer to the CP
than a position with label N. By utilizing the label
sequence and corresponding words, the model can
reflect the effect of karita existing between the CP
and the NPC of 10 on the probability.
For the sequence model, karita (borrowed) and
katta (bought) in (c) and (d) in Figure 2 are not
conflict instances in training, whereas they are
conflict instances in training for the pair model.
The reason is as follows. In order to make the
probability of the NPC of 10 smaller than the NPC
of 6, instead of making the weight parameters for
the features with respect to the word at the position
of 10 with label N smaller than the weight param-
eters for the features with respect to the word at
the position of 6 with label N, the sequence model
can give negative weight parameters for the fea-
tures with respect to the word at the position of 6
with label I.
We use a sequence discrimination technique
based on CRF (Lafferty et al, 2001) to identify the
label sequence that corresponds to the NP. There
are two differences between our task and the CRF
task. One difference is that CRF discriminates la-
bel sequences that consist of labels from all of the
label candidates, whereas we constrain the label
sequences to sequences where the label at the CP
is C, the label at an NPC is N, and the labels be-
tween the CP and the NPC are I. The other dif-
ference is that CRF is designed for discriminat-
ing label sequences corresponding to the same ob-
ject sequence, whereas we do not assign labels to
words outside the spans from the CP to each NPC.
However, when we assume that another label such
as E has been assigned to the words outside the
spans and there are no features involving label E,
CRF with our label constraints can be applied to
our task. In this paper, the method designed to
discriminate label sequences corresponding to the
different word sequence lengths is called partial
CRF.
The sequence model based on partial CRF is de-
rived by extending the pair model. We introduce
the label l and extend the pair model to discrimi-
nating the label sequences. There are two exten-
sions to the pair model. One extension uses la-
bels. We suppose that label sequences specify the
spans from the CP to each NPC. We conjoined all
the feature templates in Table 1 with an additional
feature template ?li, lj? to include the labels into
features where li is the label corresponding to the
position of i. The other extension uses sequence.
In the pair model, the position pair of (i, j) is used
to derive features. In contrast, to descriminate la-
bel sequences in the sequence model, the position
pairs of (i, k), k ? {k|i < k ? j ? j ? k < i}
159
and (k, j), k ? {k|i ? k < j ? j < k ? i}
are used to derive features. Note that in the feature
templates in Table 1, i and j are used to specify
two positions. When features are used for the se-
quence model, one of the positions is regarded as
k.
The distortion probability for an NPC j being
the NP given a CP i and a source sentence S is
calculated as:
P (X = j|i, S) =
1
Zi
exp
( ?
k?M?{j}
wTf (i, k, S, o, d, li, lk)
+
?
k?M?{i}
wTf (k, j, S, o, d, lk, lj)
)
(2)
where
M =
{
{m|i < m < j} (i < j)
{m|j < m < i} (i > j)
and Zi = ?j?{j|1?j?n+1 ? j ?=i}(numerator of
Equation 2) is a normalization factor. Since j is
used as the label sequence ID, discriminating j
also means discriminating label sequence IDs.
The first term in exp(?) in Equation 2 considers
all of the word pairs located at i and other posi-
tions in the sequence, and also their context. The
second term in exp(?) in Equation 2 considers all
of the word pairs located at j and other positions
in the sequence, and also their context.
By designing our model to discriminate among
different length label sequences, our model can
naturally handle the effect of distances. Many fea-
tures are derived from a long label sequence be-
cause it will contain many labels between the CP
and the NPC. On the other hand, fewer features
are derived from a short label sequence because a
short label sequence will contain fewer labels be-
tween the CP and the NPC. The bias from these
differences provides important clues for learning
the effect of distances.7
7Note that the sequence model does not only consider
larger context than the pair model, but that it also considers
labels. The pair model does not discriminate labels, whereas
the sequence model uses label N and label I for the positions
except for the CP, depending on each situation. For example,
in Figure 3, at position 6, label N is used in the label sequence
ID of 6, but label I is used in the label sequence IDs of 7 to
11. Namely, even if they are at the same position, the labels
in the label sequences are different. The sequence model dis-
criminates the label differences.
BOS  kare  wa  pari  de  hon  wo  katta  EOS
BOS  he  bought  books  in  Paris  EOS
Source:
Target:
training data
Figure 4: Examples of supervised training data.
The lines represent word alignments. The English
side arrows point to the nearest word aligned on
the right.
3.4 Training Data for Discriminative
Distortion Model
To train our discriminative distortion model, su-
pervised training data is needed. The training data
is built from a parallel corpus and word alignments
between corresponding source words and target
words. Figure 4 shows examples of training data.
We select the target words aligned to the source
words sequentially from left to right (target side
arrows). Then, the order of the source words in
the target word order is decided (source side ar-
rows). The source sentence and the source side
arrows are the training data.
4 Experiment
In order to confirm the effects of our distortion
model, we conducted a series of Japanese to En-
glish (JE) and Chinese to English (CE) translation
experiments.8
4.1 Common Settings
We used the patent data for the Japanese to En-
glish and Chinese to English translation subtasks
from the NTCIR-9 Patent Machine Translation
Task (Goto et al, 2011). There were 2,000 sen-
tences for the test data and 2,000 sentences for the
development data.
Mecab9 was used for the Japanese morpholog-
ical analysis. The Stanford segmenter10 and tag-
ger11 were used for Chinese segmentation and
POS tagging. The translation model was trained
using sentences of 40 words or less from the train-
ing data. So approximately 2.05 million sen-
tence pairs consisting of approximately 54 million
8We conducted JE and CE translation as examples of
language pairs with different word orders and of languages
where there is a great need for translation into English.
9http://mecab.sourceforge.net/
10http://nlp.stanford.edu/software/segmenter.shtml
11http://nlp.stanford.edu/software/tagger.shtml
160
Japanese tokens whose lexicon size was 134k and
50 million English tokens whose lexicon size was
213k were used for JE. And approximately 0.49
million sentence pairs consisting of 14.9 million
Chinese tokens whose lexicon size was 169k and
16.3 million English tokens whose lexicon size
was 240k were used for CE. GIZA++ and grow-
diag-final-and heuristics were used to obtain word
alignments. In order to reduce word alignment er-
rors, we removed articles {a, an, the} in English
and particles {ga, wo, wa} in Japanese before per-
forming word alignments because these function
words do not correspond to any words in the other
languages. After word alignment, we restored the
removed words and shifted the word alignment po-
sitions to the original word positions. We used 5-
gram language models that were trained using the
English side of each set of bilingual training data.
We used an in-house standard phrase-based
SMT system compatible with the Moses decoder
(Koehn et al, 2007). The SMT weighting param-
eters were tuned by MERT (Och, 2003) using the
development data. To stabilize the MERT results,
we tuned three times by MERT using the first half
of the development data and we selected the SMT
weighting parameter set that performed the best on
the second half of the development data based on
the BLEU scores from the three SMT weighting
parameter sets.
We compared systems that used a common
SMT feature set from standard SMT features and
different distortion model features. The com-
mon SMT feature set consists of: four translation
model features, phrase penalty, word penalty, and
a language model feature. The compared different
distortion model features are: the linear distortion
cost model feature (LINEAR), the linear distortion
cost model feature and the six MSD bidirectional
lexical distortion model (Koehn et al, 2005) fea-
tures (LINEAR+LEX), the outbound and inbound
distortion model features discriminating nine dis-
tortion classes (Green et al, 2010) (9-CLASS), the
proposed pair model feature (PAIR), and the pro-
posed sequence model feature (SEQUENCE).
4.2 Training for the Proposed Models
Our distortion model was trained as follows: We
used 0.2 million sentence pairs and their word
alignments from the data used to build the trans-
lation model as the training data for our distortion
models. The features that were selected and used
were the ones that had been counted12, using the
feature templates in Table 1, at least four times
for all of the (i, j) position pairs in the training
sentences. We conjoined the features with three
types of label pairs ?C, I?, ?I,N?, or ?C,N? as in-
stances of the feature template ?li, lj? to produce
features for SEQUENCE. The L-BFGS method
(Liu and Nocedal, 1989) was used to estimate the
weight parameters of maximum entropy models.
The Gaussian prior (Chen and Rosenfeld, 1999)
was used for smoothing.
4.3 Training for the Compared Models
For 9-CLASS, we used the same training data as
for our distortion models. Let ti be the part of
speech of si. We used the following feature tem-
plates to produce features for the outbound model:
?si?2?, ?si?1?, ?si?, ?si+1?, ?si+2?, ?ti?, ?ti?1, ti?,
?ti, ti+1?, and ?si, ti?. These feature templates corre-
spond to the components of the feature templates
of our distortion models. In addition to these fea-
tures, we used a feature consisting of the relative
source sentence position as the feature used by
(Green et al, 2010). The relative source sentence
position is discretized into five bins, one for each
quintile of the sentence. For the inbound model13,
i of the feature templates was changed to j. Fea-
tures occurring four or more times in the train-
ing sentences were used. The maximum entropy
method with Gaussian prior smoothing was used
to estimate the model parameters.
The MSD bidirectional lexical distortion model
was built using all of the data used to build the
translation model.
4.4 Results and Discussion
We evaluated translation quality based on the case-
insensitive automatic evaluation score BLEU-4
(Papineni et al, 2002). We used distortion lim-
its of 10, 20, 30, and unlimited (?), which limited
the number of words for word reordering to a max-
imum number. Table 3 presents our main results.
The proposed SEQUENCE outperformed the base-
lines for both Japanese to English and Chinese to
English translation. This demonstrates the effec-
tiveness of the proposed SEQUENCE. The scores
of the proposed SEQUENCE were higher than those
12When we counted features for selection, we only counted
features that were from the feature templates of ?si, sj?,
?ti, tj?, ?si, ti, tj?, and ?sj , ti, tj? in Table 1 when j was not
the NP, in order to avoid increasing the number of features.
13The inbound model is explained in footnote 5.
161
Japanese-English Chinese-English
Distortion limit 10 20 30 ? 10 20 30 ?
LINEAR 27.98 27.74 27.75 27.30 29.18 28.74 28.31 28.33
LINEAR+LEX 30.25 30.37 30.17 29.98 30.81 30.24 30.16 30.13
9-CLASS 30.74 30.98 30.92 30.75 31.80 31.56 31.31 30.84
PAIR 31.62 32.36 31.96 32.03 32.51 32.30 32.25 32.32
SEQUENCE 32.02 32.96 33.29 32.81 33.41 33.44 33.35 33.41
Table 3: Evaluation results for each method. The values are case-insensitive BLEU scores. Bold numbers
indicate no significant difference from the best result in each language pair using the bootstrap resampling
test at a significance level ? = 0.01 (Koehn, 2004).
Japanese-English Chinese-English
HIER 30.47 32.66
Table 4: Evaluation results for hierarchical phrase-
based SMT.
of the proposed PAIR. This confirms the effective-
ness for considering relative word order and words
between the CP and an NPC. The proposed PAIR
outperformed 9-CLASS, confirming that consider-
ing both the word specified at the CP and the word
specified at the NPC simultaneously was more ef-
fective than that of 9-CLASS.
For translating between languages with widely
different word orders such as Japanese and En-
glish, a small distortion limit is undesirable be-
cause there are cases where correct translations
cannot be produced with a small distortion limit,
since the distortion limit prunes the search space
that does not meet the constraint. Therefore,
a large distortion limit is required to translate
correctly. For JE translation, our SEQUENCE
achieved significantly better results at distortion
limits of 20 and 30 than that at a distortion limit
of 10, while the baseline systems of LINEAR,
LINEAR+LEX, and 9-CLASS did not achieve this.
This indicate that SEQUENCE could treat long
distance reordering candidates more appropriately
than the compared methods.
We also tested hierarchical phrase-based SMT
(Chiang, 2007) (HIER) using the Moses imple-
mentation. The common data was used to train
HIER. We used unlimited max-chart-span for the
system setting. Results are given in Table 4. Our
SEQUENCE outperformed HIER. The gain for JE
was large but the gain for CE was modest. Since
phrase-based SMT is generally faster in decod-
ing speed than hierarchical phrase-based SMT,
achieving better or comparable scores is worth-
Distortion
P
r
o
b
a
b
i
l
i
t
y
Figure 5: Average probabilities for large distortion
for Japanese-English translation.
while.
To investigate the tolerance for sparsity of the
training data, we reduced the training data for
the sequence model to 20,000 sentences for JE
translation.14 SEQUENCE using this model with
a distortion limit of 30 achieved a BLEU score
of 32.22.15 Although the score is lower than the
score of SEQUENCE with a distortion limit of 30
in Table 3, the score was still higher than those
of LINEAR, LINEAR+LEX, and 9-CLASS for JE
in Table 3. This indicates that the sequence model
also works even when the training data is not large.
This is because the sequence model considers not
only the word at the CP and the word at an NPC
but also rich context, and rich context would be ef-
fective even for a smaller set of training data.
14We did not conduct experiments using larger training
data because there would have been a very high computa-
tional cost to build models using the L-BFGS method.
15To avoid effects from differences in the SMT weighting
parameters, we used the same SMT weighting parameters for
SEQUENCE, with a distortion limit of 30, in Table 3.
162
To investigate how well SEQUENCE learns the
effect of distance, we checked the average distor-
tion probabilities for large distortions of j ? i? 1.
Figure 5 shows three kinds of probabilities for dis-
tortions from 3 to 20 for Japanese-English transla-
tion. One is the average distortion probabilities
in the Japanese test sentences for each distortion
for SEQUENCE, and another is this for PAIR. The
third (CORPUS) is the probabilities for the actual
distortions in the training data that were obtained
from the word alignments used to build the trans-
lation model. The probability for a distortion for
CORPUS was calculated by the number of the dis-
tortion divided by the total number of distortions
in the training data.
Figure 5 shows that when a distance class fea-
ture used in the model was the same (e.g., distor-
tions from 5 to 20 were the same distance class
feature), PAIR produced average distortion prob-
abilities that were almost the same. In contrast,
the average distortion probabilities for SEQUENCE
decreased when the lengths of the distortions in-
creased, even if the distance class feature was
the same, and this behavior was the same as that
of CORPUS. This confirms that the proposed
SEQUENCE could learn the effect of distances ap-
propriately from the training data.16
5 Related Works
We discuss related works other than discussed in
Section 2. Xiong et al (2012) proposed a model
predicting the orientation of an argument with re-
spect to its verb using a parser. Syntactic struc-
tures and predicate-argument structures are useful
for reordering. However, orientations do not han-
dle distances. Thus, our distortion model does not
compete against the methods predicting orienta-
tions using a parser and would assist them if used
16We also checked the average distortion probabilities for
the 9-CLASS outbound model in the Japanese test sentences
for Japanese-English translation. We averaged the average
probabilities for distortions in a distortion span of [4, 6] and
also averaged those in a distortion span of [7, 20], where the
distortions in each span are in the same distortion class. The
average probability for [4, 6] was 0.058 and that for [7, 20]
was 0.165. From CORPUS, the average probabilities in the
training data for each distortion in [4, 6] were higher than
those for each distortion in [7, 20]. However, the converse
was true for the comparison between the two average prob-
abilities for the outbound model. This is because the sum
of probabilities for distortions from 7 and above was larger
than the sum of probabilities for distortions from 4 to 6 in the
training data. This comparison indicates that the 9-CLASS
outbound model could not appropriately learn the effects of
large distances for JE translation.
together.
There are word reordering constraint methods
using ITG (Wu, 1997) for phrase-based SMT
(Zens et al, 2004; Yamamoto et al, 2008; Feng et
al., 2010). These methods consider sentence level
consistency with respect to ITG. The ITG con-
straint does not consider distances of reordering
and was used with other distortion models. Our
distortion model does not consider sentence level
consistency, so our distortion model and ITG con-
straint methods are thought to be complementary.
There are tree-based SMT methods (Chiang,
2007; Galley et al, 2004; Liu et al, 2006). In
many cases, tree-based SMT methods do not use
the distortion models that consider reordering dis-
tance apart from translation rules because it is not
trivial to use distortion scores considering the dis-
tances for decoders that do not generate hypothe-
ses from left to right. If it could be applied to these
methods, our distortion model might contribute to
tree-based SMT methods. Investigating the effects
will be for future work.
6 Conclusion
This paper described our distortion models for
phrase-based SMT. Our sequence model simply
consists of only one probabilistic model, but it can
consider rich context. Experiments indicate that
our models achieved better performance and the
sequence model could learn the effect of distances
appropriately. Since our models do not require a
parser, they can be applied to many languages. Fu-
ture work includes application to other language
pairs, incorporation into ITG constraint methods
and other reordering methods, and application to
tree-based SMT methods.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 529?536, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):39?71, March.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical report.
163
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Theodoros Evgniou and Massimiliano Pontil. 2002.
Learning preference relations from data. Neural
Nets Lecture Notes in Computer Science, 2486:23?
32.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010.
An efficient shift-reduce decoding algorithm for
phrased-based machine translation. In Coling 2010:
Posters, pages 285?293, Beijing, China, August.
Coling 2010 Organizing Committee.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 273?280, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR-9, pages 559?578.
Spence Green, Michel Galley, and Christopher D.Man-
ning. 2010. Improved models of distortion cost
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 867?875, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of 18th International Conference on Machine Learn-
ing, pages 282?289.
D.C. Liu and J. Nocedal. 1989. On the limited memory
method for large scale optimization. Mathematical
Programming B, 45(3):503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609?616, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-
hesan Niranjan. 2009. Handling phrase reorder-
ings for machine translation. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
241?244, Suntec, Singapore, August. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 101?
104, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of Coling 2004, pages 508?
514, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 902?911, Jeju
Island, Korea, July. Association for Computational
Linguistics.
164
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523?530, Toulouse,
France, July. Association for Computational Lin-
guistics.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro
Sumita. 2008. Imposing constraints from the source
tree on ITG constraints for SMT. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
1?9, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In Pro-
ceedings of Coling 2004, pages 205?211, Geneva,
Switzerland, Aug 23?Aug 27. COLING.
165
