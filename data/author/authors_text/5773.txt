Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 78?83,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Domain-Independent Shallow Sentence Ordering
Thade Nahnsen
School of Informatics
University of Edinburgh
T.Nahnsen@sms.ed.ac.uk
Abstract
We present a shallow approach to the sentence
ordering problem. The employed features are
based on discourse entities, shallow syntac-
tic analysis, and temporal precedence relations
retrieved from VerbOcean. We show that these
relatively simple features perform well in a
machine learning algorithm on datasets con-
taining sequences of events, and that the re-
sulting models achieve optimal performance
with small amounts of training data. The
model does not yet perform well on datasets
describing the consequences of events, such as
the destructions after an earthquake.
1 Introduction
Sentence ordering is a problem in many natural lan-
guage processing tasks. While it has, historically,
mainly been considered a challenging problem in
(concept-to-text) language generation tasks, more
recently, the issue has also generated interest within
summarization research (Barzilay, 2003; Ji and Pul-
man, 2006). In the spirit of the latter, this paper
investigates the following questions: (1) Does the
topic of the text influence the factors that are im-
portant to sentence ordering? (2) Which factors are
most important for determining coherent sentence
orderings? (3) How much performance is gained
when using deeper knowledge resources?
Past research has investigated a wide range of as-
pects pertaining to the ordering of sentences in text.
The most prominent approaches include: (1) tem-
poral ordering in terms of publication date (Barzi-
lay, 2003), (2) temporal ordering in terms of textual
cues in sentences (Bollegala et al, 2006), (3) the
topic of the sentences (Barzilay, 2003), (4) coher-
ence theories (Barzilay and Lapata, 2008), e.g., Cen-
tering Theory, (5) content models (Barzilay and Lee,
2004), and (6) ordering(s) in the underlying docu-
ments in the case of summarisation (Bollegala et al,
2006; Barzilay, 2003).
2 The Model
We view coherence assessment, which we recast as
a sentence ordering problem, as a machine learning
problem using the feature representation discussed
in Section 2.1. It can be viewed as a ranking task be-
cause a text can only be more or less coherent than
some other text. The sentence ordering task used
in this paper can easily be transformed into a rank-
ing problem. Hence, paralleling Barzilay and Lapata
(2008), our model has the following structure.
The data consists of alternative orderings
(xij , xik) of the sentences of the same document di.
In the training data, the preference ranking of the
alternative orderings is known. As a result, training
consists of determining a parameter vector w that
minimizes the number of violations of pairwise
rankings in the training set, a problem which
can be solved using SVM constraint optimization
(Joachims, 2002). The following section explores
the features available for this optimization.
2.1 Features
Approaches to sentence ordering can generally be
categorized as knowledge-rich or knowledge-lean.
Knowledge-rich approaches rely on manually cre-
ated representations of sentence orderings using do-
78
main communication knowledge.
Barzilay and Lee (2004)?s knowledge-lean ap-
proach attempts to automate the inference of
knowledge-rich information using a distributional
view of content. In essence, they infer a number of
topics using clustering. The clusters are represented
by corresponding states in a hidden Markov model,
which is used to model the transitions between top-
ics.
Lapata (2003), in contrast, does not attempt to
model topics explicitly. Instead, she reduces sen-
tence ordering to the task of predicting the next sen-
tence given the previous sentence, which represents
a coarse attempt at capturing local coherence con-
straints. The features she uses are derived from three
categories - verbs, nouns, and dependencies - all of
which are lexicalised. Her system thereby, to some
extent, learns a precedence between the words in the
sentences, which in turn represent topics.
Ji and Pulman (2006) base their ordering strategy
not only on the directly preceding sentence, but on
all preceding sentences. In this way, they are able to
avoid a possible topic bias when summarizing mul-
tiple documents. This is specific to their approach as
both Lapata (2003)?s and Barzilay and Lee (2004)?s
approaches are not tailored to summarization and
therefore do not experience the topic bias problem.
The present paper deviates from Lapata (2003)
insofar as we do not attempt to learn the ordering
preferences between pairs of sentences. Instead, we
learn the ranking of documents. The advantage of
this approach is that it allows us to straightforwardly
discern the individual value of various features (cf.
Barzilay and Lapata (2008)).
The methods used in this paper are mostly shallow
with the exception of two aspects. First, some of the
measures make use of WordNet relations (Fellbaum,
1998), and second, some use the temporal ordering
provided by the ?happens-before? relation in VerbO-
cean (Chklovski and Pantel, 2004). While the use of
WordNet is self-explanatory, its effect on sentence
ordering algorithms does not seem to have been ex-
plored in any depth. The use of VerbOcean is meant
to reveal the degree to which common sense order-
ings of events affect the ordering of sentences, or
whether the order is reversed.
With this background, the sentence ordering fea-
tures used in this paper can be grouped into three
categories:
2.1.1 Group Similarity
The features in this category are inspired by dis-
course entity-based accounts of local coherence.
Yet, in contrast to Barzilay and Lapata (2008), who
employ the syntactic properties of the respective oc-
currences, we reduce the accounts to whether or not
the entities occur in subsequent sentences (similar
to Karamanis (2004)?s NOCB metric). We also in-
vestigate whether using only the information from
the head of the noun group (cf. Barzilay and Lapata
(2008)) suffices, or whether performance is gained
when allowing the whole noun group in order to de-
termine similarity. Moreover, as indicated above,
some of the noun group measures make use of Word-
Net synonym, hypernym, hyponym, antonym rela-
tionships. For completeness, we also consider the
effects of using verb groups and whole sentences as
syntactic units of choice.
2.1.2 Temporal Ordering
This set of features uses information on the tem-
poral ordering of sentences, although it currently
only includes the ?happens-before? relations in Ver-
bOcean.
2.1.3 Longer Range Relations
The group similarity features only capture the re-
lation between a sentence and its immediate suc-
cessor. However, the coherence of a text is clearly
not only defined by direct relations, but also re-
quires longer range relations between sentences
(e.g., Barzilay and Lapata (2008)). The features in
this section explore the impact of such relations on
the coherence of the overall document as well as the
appropriate way of modeling them.
3 Experiments
This section introduces the datasets used for the ex-
periments, describes the experiments, and discusses
our main findings.
3.1 Evaluation Datasets
The three datasets used for the automatic evaluation
in this paper are based on human-generated texts
(Table 1). The first two are the earthquake and acci-
dent datasets used by Barzilay and Lapata (2008).
79
Each of these sets consists of 100 datasets in the
training and test sets, respectively, as well as 20 ran-
dom permutations for each text.
The third dataset is similar to the first two in that
it contains original texts and random permutations.
In contrast to the other two sources, however, this
dataset is based on the human summaries from DUC
2005 (Dang, 2005). It comprises 300 human sum-
maries on 50 document sets, resulting in a total of
6,000 pairwise rankings split into training and test
sets. The source furthermore differs from Barzilay
and Lapata (2008)?s datasets in that the content of
each text is not based on one individual event (an
earthquake or accident), but on more complex top-
ics followed over a period of time (e.g., the espi-
onage case between GM and VW along with the
various actions taken to resolve it). Since the differ-
ent document sets cover completely different topics
the third dataset will mainly be used to evaluate the
topic-independent properties of our model.
Dataset Training Testing
Earthquakes 1,896 2,056
Accidents 2,095 2,087
DUC2005 up to 3,300 2,700
Table 1: Number of pairwise rankings in the training and
test sets for the three datasets
3.2 Experiment 1
In the first part of this experiment, we consider the
problem of the granularity of the syntactic units to be
used. That is, does it make a difference whether we
use the words in the sentence, the words in the noun
groups, the words in the verb groups, or the words
in the respective heads of the groups to determine
coherence? (The units are obtained by processing
the documents using the LT-TTT2 tools (Grover and
Tobin, 2006); the lemmatizer used by LT-TTT2 is
morpha (Minnen and Pearce, 2000).) We also con-
sider whether lemmatization is beneficial in each of
the granularities.
The results - presented in Table 2 - indicate that
considering only the heads of the verb and noun
groups separately provides the best performance. In
particular, the heads outperform the whole groups,
and the heads separately also outperform noun and
verb group heads together. As for the question
of whether lemmatization provides better results,
one needs to distinguish the case of noun and verb
groups. For noun groups, lemmatization improves
performance, which can mostly be attributed to sin-
gular and plural forms. In the case of verb groups,
however, the lemmatized version yields worse re-
sults than the surface forms, a fact mainly explained
by the tense and modality properties of verbs.
Syntactic Unit Processing AccuracyAcc Earth
sentence surface form 52.27 14.21lemma 52.27 12.04
heads sentence surface form 77.35 60.30lemma 73.18 61.67
noun group surface form 80.14 59.84lemma 81.58 59.54
head NG surface form 80.49 59.75lemma 81.65 59.12
verb group surface form 71.57 68.14lemma 53.40 68.01
head VG surface form 71.15 68.39lemma 53.76 67.85
Table 2: Performance with respect to the syntactic unit
of processing of the training datasets. Accuracy is the
fraction of correctly ranked pairs of documents over the
total number of pairs. (?Heads sentence? is the heads of
NGs and VGs.)
Given the appropriate unit of granularity, we can
consider the impact of semantic relations between
surface realizations on coherence. For these exper-
iments we use the synonym, hypernym, hyponym,
and antonym relations in WordNet. The rationale
for the consideration of semantic relations lies in the
fact that the frequent use of the same words is usu-
ally deemed bad writing style. One therefore tends
to observe the use of semantically similar terms in
neighboring sentences. The results of using seman-
tic relations for coherence rating are provided in Ta-
ble 3. Synonym detection improves performance,
while the other units provide poorer performance.
This suggests that the hypernym and hyponym rela-
tions tend to over-generalize in the semantics.
The third category of features investigated is the
temporal ordering of sentences; we use VerbO-
cean to obtain the temporal precedence between two
events. One would expect events to be described ei-
80
Syntactic Unit Processing AccuracyAcc Earth
head NG
synonyms 82.37 59.40
hypernyms 76.98 61.02
hyponyms 81.59 59.14
antonyms 74.20 48.07
combines 70.84 56.51
head VG
synonyms 54.19 70.80
hypernyms 53.36 60.54
hyponyms 55.27 68.32
antonyms 47.45 63.91
combines 49.73 66.77
Table 3: The impact of WordNet on sentence ordering
accuracy
Temporal Ordering AccuracyAcc Earth
Precedence Ordering 60.41 47.09
Reverse Ordering 39.59 52.61
Precedence w/ matching NG 62.65 57.52
Reverse w/ matching NG 37.35 42.48
Table 4: The impact of the VerbOcean ?happens-before?
temporal precedence relation on accuracy on the training
datasets
ther in chronological order or in its reverse. While
the former ordering represents a factual account of
some sequence of events, the latter corresponds to
newswire-style texts, which present the most impor-
tant event(s) first, even though they may derive from
previous events.
Table 4 provides the results of the experiments
with temporal orderings. The first two rows vali-
date the ordering of the events, while the latter two
require the corresponding sentences to have a noun
group in common in order to increase the likeli-
hood that two events are related. The results clearly
show that there is potential in the direct ordering
of events. This suggests that sentence ordering can
to some degree be achieved using simple temporal
precedence orderings in a domain-independent way.
This holds despite the results indicating that the fea-
tures work better for sequences of events (as in the
accident dataset) as opposed to accounts of the re-
sults of some event(s) (as in the earthquake dataset).
Range AccuracyAcc Earth
2 occ. in 2 sent. 80.57 50.11
2 occ. in 3 sent. 73.17 45.43
3 occ. in 3 sent. 71.35 52.81
2 occ. in 4 sent. 66.95 50.41
3 occ. in 4 sent. 69.38 41.61
4 occ. in 4 sent. 71.93 58.97
2 occ. in 5 sent. 61.48 66.25
3 occ. in 5 sent. 68.59 42.33
4 occ. in 5 sent. 65.77 40.75
5 occ. in 5 sent. 81.39 62.40
sim. w/ sent. 1 sent. away 83.39 71.94
sim. w/ sent. 2 sent. away 60.44 67.52
sim. w/ sent. 3 sent. away 52.28 54.65
sim. w/ sent. 4 sent. away 49.65 44.50
sim. w/ sent. 5 sent. away 43.68 52.11
Table 5: Effect of longer range relations on coherence
accuracy
The final category of features investigates the de-
gree to which relations between sentences other than
directly subsequent sentences are relevant. To this
end, we explore two different approaches. The first
set of features considers the distribution of entities
within a fixed set of sentences, and captures in how
many different sentences the entities occur. The re-
sulting score is the number of times the entities oc-
cur in N out of M sentences. The second set only
considers the similarity score from the current sen-
tence and the other sentences within a certain range
from the current sentence. The score of this fea-
ture is the sum of the individual similarities. Table 5
clearly confirms that longer range relations are rele-
vant to the assessment of the coherence of text. An
interesting difference between the two approaches is
that sentence similarity only provides good results
for neighboring sentences or sentences only one sen-
tence apart, while the occurrence-counting method
also works well over longer ranges.
Having evaluated the potential contributions of
the individual features and their modeling, we now
use SVMs to combine the features into one com-
prehensive measure. Given the indications from the
foregoing experiments, the results in Table 6 are dis-
appointing. In particular, the performance on the
81
Combination AccuracyAcc Earth
Chunk+Temp+WN+LongRange+ 83.11 54.88
Chunk+Temp+WN+LongRange- 77.67 62.76
Chunk+Temp+WN-LongRange+ 74.17 59.28
Chunk+Temp+WN-LongRange- 68.15 63.55
Chunk+Temp-WN+LongRange+ 86.88 63.83
Chunk+Temp-WN+LongRange- 80.19 59.43
Chunk+Temp-WN-LongRange+ 76.63 60.86
Chunk+Temp-WN-LongRange- 64.43 60.94
NG Similarity w/ Synonyms 85.90 63.55
Coreference+Syntax+Salience+ 90.4 87.2
Coreference-Syntax+Salience+ 89.9 83.0
HMM-based Content Models 75.8 88.0
Latent Semantic Analysis 87.3 81.0
Table 6: Comparison of the developed model with other
state-of-the-art systems. Coreference+Syntax+Salience+
and Coreference?Syntax+Salience+ are the Barzilay and
Lapata (2008) model, HMM-based Content Models is the
Barzilay and Lee (2004) paper and Latent Semantic Anal-
ysis is the Barzilay and Lapata (2008) implementation of
Peter W. Foltz and Landauer (1998). The results of these
systems are reproduced from Barzilay and Lapata (2008).
(Temp = Temporal; WN = WordNet)
earthquake dataset is below standard. However, it
seems that sentence ordering in that set is primarily
defined by topics, as only content models perform
well. (Barzilay and Lapata (2008) only perform well
when using their coreference module, which de-
termines antecedents based on the identified coref-
erences in the original sentence ordering, thereby
biasing their orderings towards the correct order-
ing.) Longer range and WordNet relations together
(Chunk+Temp-WN+LongRange+) achieve the best
performance. The corresponding configuration is
also the only one that achieves reasonable perfor-
mance when compared with other systems.
4 Experiment 2
As stated, the ultimate goal of the models presented
in this paper is the application of sentence ordering
to automatically generated summaries. It is, in this
regard, important to distinguish coherence as studied
in Experiment 1 and coherence in the context of au-
tomatic summarization. Namely, for newswire sum-
marization systems, the topics of the documents are
Coreference+Syntax+Salience+
Test Earthquakes AccidentsTrain
Earthquakes 87.3 67.0
Accidents 69.7 90.4
HMM-based Content Models
Test Earthquakes AccidentsTrain
Earthquakes 88.0 31.7
Accidents 60.3 75.8
Chunk+Temporal-WordNet+LongRange+
Test Earthquakes AccidentsTrain
Earthquakes 63.83 86.63
Accidents 64.19 86.88
Table 7: Cross-Training between Accident and
Earthquake datasets. The results for Corefer-
ence+Syntax+Salience+ and HMM-Based Content
Models are reproduced from Barzilay and Lapata (2008).
unknown at the time of training. As a result, model
performance on out-of-domain texts is important for
summarization. Experiment 2 seeks to evaluate how
well our model performs in such cases. To this
end, we carry out two sets of tests. First, we cross-
train the models between the accident and earth-
quake datasets to determine system performance in
unseen domains. Second, we use the dataset based
on the DUC 2005 model summaries to investigate
whether our model?s performance on unseen topics
reaches a plateau after training on a particular num-
ber of different topics.
Surprisingly, the results are rather good, when
compared to the poor results in part of the previ-
ous experiment (Table 7). In fact, model perfor-
mance is nearly independent of the training topic.
Nevertheless, the results on the earthquake test set
indicate that our model is missing essential compo-
nents for the correct prediction of sentence order-
ings on this set. When compared to the results ob-
tained by Barzilay and Lapata (2008) and Barzilay
and Lee (2004), it would appear that direct sentence-
to-sentence similarity (as suggested by the Barzilay
and Lapata baseline score) or capturing topic se-
quences are essential for acquiring the correct se-
quence of sentences in the earthquake dataset.
The final experimental setup applies the best
82
Different Topics Training Pairs Accuracy
2 160 55.17
4 420 63.54
6 680 65.20
8 840 65.57
10 1,100 64.80
15 1,500 64.93
20 2,100 64.87
25 2,700 64.94
30 3,300 65.61
Table 8: Accuracy on 20 test topics (2,700 pairs) with
respect to the number of topics used for training using
the model Chunk+Temporal-WordNet+LongRange+
model (Chunk+Temporal-WordNet+LongRange+)
to the summarization dataset and evaluates how well
the model generalises as the number of topics in the
training dataset increases. The results - provided in
Table 8 - indicate that very little training data (both
regarding the number of pairs and the number of dif-
ferent topics) is needed. Unfortunately, they also
suggest that the DUC summaries are more similar
to the earthquake than to the accident dataset.
5 Conclusions
This paper investigated the effect of different fea-
tures on sentence ordering. While a set of features
has been identified that works well individually as
well as in combination on the accident dataset, the
results on the earthquake and DUC 2005 datasets are
disappointing. Taking into account the performance
of content models and the baseline of the Barzilay
and Lapata (2008) model, the most convincing ex-
planation is that the sentence ordering in the earth-
quake datasets is based on some sort of topic notion,
providing a variety of possible antecedents between
which our model is thus far unable to distinguish
without resorting to the original (correct) ordering.
Future work will have to concentrate on this aspect
of sentence ordering, as it appears to coincide with
the structure of the summaries for the DUC 2005
dataset.
References
Barzilay, R. (2003). Information fusion for multi-
document summarization: paraphrasing and gen-
eration. Ph. D. thesis, Columbia University.
Barzilay, R. and M. Lapata (2008). Modeling local
coherence: An entity-based approach. Comput.
Linguist. 34, 1?34.
Barzilay, R. and L. Lee (2004). Catching the drift:
probabilistic content models, with applications to
generation and summarization. In Proceedings of
HLT-NAACL 2004.
Bollegala, D., N. Okazaki, and M. Ishizuka (2006).
A bottom-up approach to sentence ordering for
multi-document summarization. In Proceedings
of ACL-44.
Chklovski, T. and P. Pantel (2004). Verbocean: Min-
ing the web for fine-grained semantic verb rela-
tions. In Proceedings of EMNLP 2004.
Dang, H. (2005). Overview of duc 2005.
Fellbaum, C. (Ed.) (1998). WordNet An Electronic
Lexical Database. The MIT Press.
Grover, C. and R. Tobin (2006). Rule-based chunk-
ing and reusability. In Proceedings of LREC 2006.
Ji, P. D. and S. Pulman (2006). Sentence order-
ing with manifold-based classification in multi-
document summarization. In Proceedings of
EMNLP 2006.
Joachims, T. (2002). Evaluating retrieval perfor-
mance using clickthrough data. In Proceedings
of the SIGIR Workshop on Mathematical/Formal
Methods in Information Retrieval.
Karamanis, N. (2004). Evaluating centering for sen-
tence ordering in two new domains. In Proceed-
ings of the NAACL 2004.
Lapata, M. (2003). Probabilistic text structuring:
Experiments with sentence ordering. In Proc. of
ACL 2003.
Minnen, G., C. J. and D. Pearce (2000). Robust, ap-
plied morphological generation. In Proceedings
of the 1st International Natural Language Gener-
ation Conference.
Peter W. Foltz, W. K. and T. K. Landauer (1998).
Textual coherence using latent semantic analysis.
Discourse Processes 25, 285?307.
83
Lexical Chains and Sliding Locality Windows in Content-based 
Text Similarity Detection 
Thade Nahnsen, ?zlem Uzuner, Boris Katz 
Computer Science and Artificial Intelligence Laboratory 
Massachusetts Institute of Technology 
Cambridge, MA 02139 
{tnahnsen,ozlem,boris}@csail.mit.edu 
 
 
Abstract 
We present a system to determine 
content similarity of documents. 
Our goal is to identify pairs of book 
chapters that are translations of the 
same original chapter.  Achieving 
this goal requires identification of 
not only the different topics in the 
documents but also of the particular 
flow of these topics.   
Our approach to content 
similarity evaluation employs n-
grams of lexical chains and 
measures similarity using the 
cosine of vectors of n-grams of 
lexical chains, vectors of tf*idf-
weighted keywords, and vectors of 
unweighted lexical chains 
(unigrams of lexical chains).  Our 
results show that n-grams of 
unordered lexical chains of length 
four or more are particularly useful 
for the recognition of content 
similarity. 
1   Introduction 
This paper addresses the problem of determining 
content similarity between chapters of literary 
novels.  We aim to determine content similarity 
even when book chapters contain more than one 
topic by resolving exact content matches rather 
than finding similarities in dominant topics.   
Our solution to this problem relies on lexical 
chains extracted from WordNet [6]. 
2   Related Work 
Lexical Chains (LC) represent lexical items 
which are conceptually related to each other, for 
example, through hyponymy or synonymy 
relations.  Such conceptual relations have 
previously been used in evaluating cohesion, 
e.g., by Halliday and Hasan [2, 3].    Barzilay 
and Elhadad [1] used lexical chains for text 
summarization; they identified important 
sentences in a document by retrieving strong 
chains.  Silber and McCoy [7] extended the 
work of Barzilay and Elhadad; they developed 
an algorithm that is linear in time and space for 
efficient identification of lexical chains in large 
documents.  In this algorithm, Silber and McCoy 
first created a text representation in the form of 
metachains, i.e., chains that capture all possible 
lexical chains in the document.  After creating 
the metachains, they used a scoring algorithm to 
identify the lexical chains that are most relevant 
to the document, eliminated unnecessary 
overhead information from the metachains, and 
selected the lexical chains representing the 
document.  Our method for building lexical 
chains follows this algorithm. 
N-gram based language models, i.e., models 
that divide text into n-word (or n-character) 
strings, are frequently used in natural language 
processing.  In plagiarism detection, the overlap 
of n-grams between two documents has been 
used to determine whether one document 
plagiarizes another [4].   In general, n-grams 
capture local relations.  In our case, they capture 
local relations between lexical chains and 
between concepts represented by these chains.   
Three main streams of research in content 
similarity detection are: 1) shallow, statistical 
analysis of documents, 2) analysis of rhetorical 
relations in texts [5], and 3) deep syntactic 
150
analysis [8]. Shallow methods do not include 
much linguistic information and provide a very 
rough model of content while approaches that 
use syntactic analysis generally require 
significant computation. Our approach strikes a 
compromise between these two extremes: it uses 
the linguistic knowledge provided in WordNet 
as a way of making use of low-cost linguistic 
information for building lexical chains that can 
help detect content similarity.   
3   Lexical Chains in Content Similarity 
Detection 
3.1   Corpus 
The experiments in this paper were performed 
on a corpus consisting of chapters from 
translations of four books (Table 1) that cover a 
variety of topics.  Many of the chapters from 
each book deal with similar topics; therefore, 
fine-grained content analysis is required to 
identify chapters that are derived from the same 
original chapter. 
 
# 
translati
ons 
Title # 
chapters 
2 20,000 Leagues under the Sea 47 
3 Madame Bovary 35 
2 The Kreutzer Sonata 28 
2 War and Peace 365 
Table 1: Corpus 
3.2   Computing Lexical Chains 
Our approach to calculating lexical chains uses 
nouns, verbs, and adjectives present in 
WordNetV2.0.  We first extract such words 
from each chapter in the corpus and represent 
each chapter as a set of these word instances {I1, 
?, In}.  Each instance of each of these words 
has a set of possible interpretations, IN, in 
WordNet.  These interpretations are either the 
synsets or the hypernyms of the instances.  
Given these interpretations, we apply a slightly 
modified version of the algorithm by Silber and 
McCoy [7] to automatically disambiguate 
nouns, verbs, and adjectives, i.e., to select the 
correct interpretation, for each instance.  Silber 
and McCoy?s algorithm computes all of the 
scored metachains for all senses of each word in 
the document and attributes the word to the 
metachain to which it contributes the most.   
During this process, the algorithm computes the 
contribution of a word to a given chain by 
considering 1) the semantic relations between 
the synsets of the words that are members of the 
same metachain, and 2) the distance between 
their respective instances in the discourse.  Our 
approach uses these two parameters, with minor 
modifications.  Silber and McCoy measure 
distance in terms of paragraphs on prose text; 
we measure distance in terms of sentences in 
order to handle both dialogue and prose text.  
 
 
Figure 1: Intermediate representation after 
eliminating words that are not nouns, verbs, or 
adjectives and after identifying lexical chains 
(represented by WordNet synset IDs).  Note that 
{kitchen, bathroom} are represented by the same 
synset ID which corresponds to the synset ID of 
their common hypernym ?room?.  {kitchen, 
bathroom} is a lexical chain.  Ties are broken in 
favor of hypernyms. 
 
Following Silber and McCoy, we allow 
different types of conceptual relations to 
contribute differently to each lexical chain, i.e., 
the contribution of each word to a lexical chain 
is dependent on its semantic relation to the chain 
(see Table 2).  After scoring, concepts that are 
dominant in the text segment are identified and 
each word is represented by only the WordNet 
ID of the synset (or the hypernym/hyponym set) 
that best fits its local context.  Figure 1 gives an 
example of the resulting intermediate 
representation, corresponding to the 
interpretation, S, found for each word instance, 
I, that can be used to represent each chapter, C, 
where C = {S1, ?, Sm}.   
 
Lexical 
semantic 
relation 
Distance <= 
6 sentences 
Distance > 
6 sentences 
Same word 1 0 
Hyponym 0.5 0 
Hypernym 0.5 0 
Sibling 0.2 0 
Table 2: Contribution to lexical chains 
Original document (underlined words are represented 
with lexical chains): 
The furniture in the kitchen seems beautiful, but the bathroom 
seems untidy. 
 
Intermediate representation (lexical chains): 
03281101   03951013   02071636   00218842   03951013  
02071636   02336718    
 
151
3.3 Determining the Locality Window 
After computing the lexical chains, we created a 
representation for text by substituting the correct 
lexical chain for each noun, verb, and adjective 
in each document. We omitted the remaining 
parts of speech from the documents (see Figure 
1 for sample intermediate representation). We 
obtained ordered and unordered n-grams of 
lexical chains from this representation. 
Ordered n-grams consist of n consecutive 
lexical chains extracted from text. These ordered 
n-grams preserve the original order of the lexical 
chains in the text. Corresponding unordered n-
grams disregard this order. The resulting text 
representation is T = {gram1, gram2, ?, gramn}, 
where grami = [lc1, ?,  lcn], where lci ? {I1, ?, 
Ik} (the chains that represent Chapter C). The 
elements in grami may be sorted or unsorted, 
depending on the selected method.  N-grams are 
extracted from text using sliding locality 
windows and provide what we call ?attribute 
vectors?. The attribute vector for ordered n-
grams has the form C = {(e1, ?, en), (e2, ?, 
en+1), ?, (em-n, ?, em)} where (e1, ?, en) is an 
ordered n-gram and em is the last lexical chain in 
the chapter.  For unordered n-grams, the 
attribute vector has the form C = {sort[(e1, ?, 
en)], sort[(e2, ?, en+1)], ?, sort[(em-n, ?, em)]} 
where sort[?] indicates alphabetical sorting of 
chains (rather than the actual order in which the 
chains appear in the text).  
We evaluated similarity between pairs of 
book chapters using the cosine of the attribute 
vectors of n-grams of lexical chains (sliding 
locality windows of width n).  We varied the 
width of the sliding locality windows from two 
to five elements.  
4   Evaluation 
We used cosine similarity as the distance metric, 
computed the cosine of the angle between the 
vectors of pairs of documents in the corpus, and 
ranked the pairs based on this score.  We 
identified the top n most similar pairs (also 
referred to as ?selection level of n?) and 
considered them to be similar in content. 
We calculated similarity between pairs of 
documents in several different ways, evaluated 
these approaches with the standard information 
retrieval measures, i.e., precision, recall, and f-
measure, and compared our results with two 
baselines. The first baseline measured the 
similarity of documents with tf*idf-weighted 
keywords; the second used the cosine of 
unweighted lexical chains (unigrams of lexical 
chains). 
The corpus of parallel translations provides 
data that can be used as ground truth for content 
similarity; corresponding chapters from different 
translations of the same original title are 
considered similar in content, i.e., chapter 1 of 
translation 1 of Madame Bovary is similar in 
content to chapter 1 of translation 2 of Madame 
Bovary. 
Figure 2 shows the f-measure of different 
methods for measuring similarity between pairs 
of chapters using ordered lexical chains, 
unordered lexical chains, and baselines.  These 
graphs present the results when the top 100?
1,600 most similar pairs in the corpus are 
considered similar in content and the rest are 
considered dissimilar (selection level of 100?
1,600).  The total number of chapter pairs is 
approximately 1,000,000. Of these, 1,080 (475 
unique chapters with 2 or 3 translations each) 
are considered similar for evaluation purposes. 
The results indicate that four similarity 
measures gave the best performance.  These 
were tri-grams, quadri-grams, penta-grams, and 
hexa-grams of unordered lexical chains.  The 
peak f-measure at the selection level of 1,100 
chapter pairs was 0.981. Chi squared tests 
performed on the f-measures (when the top 
1,100 pairs were considered similar) were 
significant at p = 0.001. 
Closer analysis of the graphs in Figure 2 
shows that, at the optimal selection level, n-
grams of ordered lexical chains of length greater 
than four significantly outperformed the baseline 
at p = 0.001 while n-grams of ordered lexical 
chains of length less than or equal to four are 
significantly outperformed by the baseline at the 
same p. A similar observation cannot be made 
for the n-grams of unordered lexical chains; for 
these n-grams, the performance degradation 
appears at n = 7, i.e., the corresponding curves 
have a steeper negative incline than the baseline.   
After the cut-off point of 1,100 chapter pairs, 
the performance of all algorithms declines. This 
is due to the evaluation method we have chosen: 
although the cut-off for similarity judgement can 
be increased, the number of chapters that are in 
fact similar does not change and at high cut-off 
values many dissimilar pairs are considered 
similar, leading to degradation in performance. 
152
Figures 2a and 2b show that some of the 
lexical chain representations do not outperform 
the tf*idf-weighted baseline.  A comparison of 
Figures 2a and 2b shows that, for n < 5, n-grams 
of ordered lexical chains perform worse than n-
grams of unordered lexical chains. This 
indicates that between different translations of 
the same book the order of chains changes 
significantly, but that the chains within 
contiguous regions (locality windows) of the 
texts remain similar.   
Interestingly, ordered n-grams of length 3 to 5 
perform significantly better than unordered n-
grams of the same length. This implies that, 
during translation, the order of the content 
words does not change enormously for three to 
five lexical chain elements.  Allowing flexible 
order for the lexical chains (i.e., unordered 
lexical chains) in these n-grams therefore hurts 
performance by allowing many false positives.  
However, for longer n-grams to be successful, 
the order of the lexical chains has to be flexible. 
Figure 2: F-Measure
. 
F-M e as u r e  vs . C h ap te r s  Se le cte d  (Un o r d e r e d  N-Gr am s )
0
0,1
0 ,2
0 ,3
0 ,4
0 ,5
0 ,6
0 ,7
0 ,8
0 ,9
1
100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600
C h ap te r s  Se le cte d
F
-M
ea
su
re
u2gram/LC u3gram/LC u4gram/LC u5gram/LC u6gram/LC
u7gram/LC tf *id f c os ine
 
(a) F-Measure: Unordered n-grams vs. the baselines 
F- M e a s u r e  v s . C h a p t e r s  S e le c t e d  ( O r d e r e d  N-G r a m s )
0
0 ,1
0 ,2
0 ,3
0 ,4
0 ,5
0 ,6
0 ,7
0 ,8
0 ,9
1
100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600
C h a p te r s  S e le c t e d
F
-M
ea
su
re
tf * id f c o s in e 4g ram/LC 5g ram/LC 6g ram/LC
7g ram/LC 2g ram/LC 3g ram/LC
 
(b) F-Measure: Ordered n-grams vs. the baselines 
   
  ngram/LC ? unordered n-grams of lexical chains are used in the attribute vector 
  ungram/LC ? ordered n-grams of lexical chains are used in the attribute vector  
  tf*idf ? tf*idf weighted words are used in the attribute vector 
  cosine ? the standard information retrieval measure; words are used in the attribute vector 
153
5   Future Work 
Currently, our similarity measures do not 
employ any weighting scheme for n-grams, i.e., 
every n-gram is given the same weight.  For 
example, the n-gram ?be it as it has been? in 
lexical chain form corresponds to synsets for the 
words be, have and be.  The trigram of these 
lexical chains does not convey significant 
meaning.  On the other hand, the n-gram ?the 
lawyer signed the heritage? is converted into the 
trigram of lexical chains of lawyer, sign, and 
heritage.  This trigram is more meaningful than 
the trigram be have be, but in our scheme both 
trigrams will get the same weight.  As a result, 
two documents that share the trigram be have be 
will look as similar as two documents that share 
lawyer sign heritage. This problem can be 
addressed in two possible ways: using a ?stop 
word? list to filter such expressions completely 
or giving different weights to n-grams based on 
the number of their occurrences in the corpus.   
6   Conclusion 
We have presented a system that extends 
previous work on lexical chains to content 
similarity detection.   This system employs 
lexical chains and sliding locality windows, and 
evaluates similarity using the cosine of n-grams 
of lexical chains and tf*idf weighted keywords.  
The results indicate that lexical chains are 
effective for detecting content similarity 
between pairs of chapters corresponding to the 
same original in a corpus of parallel translations.  
References 
1. Barzilay, R., Elhadad, M. 1999. Using lexical 
chains for text summarization. In: Inderjeet 
Mani and Mark T. Maybury, eds., Advances 
in AutomaticText Summarization, pp. 111?
121. Cambridge/MA, London/England: MIT 
Press. 
2. Halliday, M. and Hasan, R. 1976. Cohesion in 
English. Longman, London. 
3. Halliday, M. and Hasan, R. 1989. Language, 
context, and text. Oxford University Press, 
Oxford, UK. 
4. Lyon, C., Malcolm, J. and Dickerson, B. 
2001. Detecting Short Passages of Similar 
Text in Large Document Collections, In 
Proceedings of the 2001 Conference on 
Empirical Methods in Natural Language 
Processing, pp.118-125. 
5. Marcu, D. 1997. The Rhetorical Parsing, 
Summarization, and Generation of Natural 
Language Texts (Ph.D. dissertation). Univ. of 
Toronto. 
6. Miller, G., Beckwith, R., Felbaum, C., Gross, 
D., and Miller, K. 1990. Introduction to 
WordNet: An online lexical database. J. 
Lexicography, 3(4), pp. 235-244. 
7. Silber, G. and McCoy, K. 2002. Efficiently 
computed lexical chains as an intermediate 
representation for automatic text 
summarization. Computational Linguistics, 
28(4). 
8. Uzuner, O., Davis, R., Katz, B. 2004. Using 
Empirical Methods for Evaluating Expression 
and Content Similarity. In: Proceedings of the 
37th Hawaiian International Conference on 
System Sciences (HICSS-37).  IEEE 
Computer Society. 
 
154
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 37?44, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Using Syntactic Information to Identify Plagiarism
O?zlem Uzuner, Boris Katz, and Thade Nahnsen
Massachusetts Institute of Technology
Computer Science and Articial Intelligence Laboratory
Cambridge, MA 02139
ozlem,boris,tnahnsen@csail.mit.edu
Abstract
Using keyword overlaps to identify pla-
giarism can result in many false negatives
and positives: substitution of synonyms
for each other reduces the similarity be-
tween works, making it difficult to rec-
ognize plagiarism; overlap in ambiguous
keywords can falsely inflate the similar-
ity of works that are in fact different in
content. Plagiarism detection based on
verbatim similarity of works can be ren-
dered ineffective when works are para-
phrased even in superficial and immate-
rial ways. Considering linguistic informa-
tion related to creative aspects of writing
can improve identification of plagiarism
by adding a crucial dimension to evalu-
ation of similarity: documents that share
linguistic elements in addition to content
are more likely to be copied from each
other. In this paper, we present a set of
low-level syntactic structures that capture
creative aspects of writing and show that
information about linguistic similarities
of works improves recognition of plagia-
rism (over tfidf-weighted keywords alone)
when combined with similarity measure-
ments based on tfidf-weighted keywords.
1 Introduction
To plagiarize is ?to steal and pass off (the ideas
or words of another) as one?s own; [to] use (an-
other?s production) without crediting the source; [or]
to commit literary theft [by] presenting as new and
original an idea or product derived from an exist-
ing source?.1 Plagiarism is frequently encountered
in academic settings. According to turnitin.com, a
2001 survey of 4500 high school students revealed
that ?15% [of students] had submitted a paper ob-
tained in large part from a term paper mill or web-
site?. Increased rate of plagiarism hurts quality of
education received by students; facilitating recog-
nition of plagiarism can help teachers control this
damage.
To facilitate recognition of plagiarism, in the re-
cent years many commercial and academic prod-
ucts have been developed. Most of these approaches
identify verbatim plagiarism2 and can fail when
works are paraphrased. To recognize plagiarism
in paraphrased works, we need to capture similar-
ities that go beyond keywords and verbatim over-
laps. Two works that exhibit similarity both in their
conceptual content (as indicated by keywords) and
in their expression of this content should be consid-
ered more similar than two works that are similar
only in content. In this context, content refers to
the story or the information; expression refers to the
linguistic choices of authors used in presenting the
content, i.e., creative elements of writing, such as
whether authors tend toward passive or active voice,
whether they prefer complex sentences with embed-
ded clauses or simple sentences with independent
clauses, as well as combinations of such choices.
Linguistic information can be a source of power
for measuring similarity between works based on
1www.webster.com
2www.turnitin.com
37
their expression of content. In this paper, we use lin-
guistic information related to the creative aspects of
writing to improve recognition of paraphrased doc-
uments as a first step towards plagiarism detection.
To identify a set of features that relate to the linguis-
tic choices of authors, we rely on different syntactic
expressions of the same content. After identifying
the relevant features (which we call syntactic ele-
ments of expression), we rely on patterns in the use
of these features to recognize paraphrases of works.
In the absence of real-life plagiarism data, in this
paper, we use a corpus of parallel translations of
novels as surrogate for plagiarism data. Transla-
tions of titles, i.e., original works, into English by
different people provide us with books that are para-
phrases of the same content. We use these para-
phrases to automatically identify:
1. Titles even when they are paraphrased, and
2. Pairs of book chapters that are paraphrases of
each other.
Our first experiment shows that syntactic elements
of expression outperform all baselines in recogniz-
ing titles even when they are paraphrased, provid-
ing a way of recognizing copies of works based on
the similarities in their expression of content. Our
second experiment shows that similarity measure-
ments based on the combination of tfidf-weighted
keywords and syntactic elements of expression out-
perform the weighted keywords in recognizing pairs
of book chapters that are paraphrases of each other.
2 Related Work
We define expression as ?the linguistic choices of
authors in presenting a particular content? (Uzuner,
2005; Uzuner and Katz, 2005). Linguistic similarity
between works has been studied in the text classifi-
cation literature for identifying the style of an author.
However, it is important to differentiate expression
from style. Style refers to the linguistic elements
that, independently of content, persist over the works
of an author and has been widely studied in author-
ship attribution. Expression involves the linguistic
elements that relate to how an author phrases par-
ticular content and can be used to identify potential
copyright infringement or plagiarism. Similarities
in the expression of similar content in two differ-
ent works signal potential copying. We hypothesize
that syntax plays a role in capturing expression of
content. Our approach to recognizing paraphrased
works is based on phrase structure of sentences in
general, and structure of verb phrases in particular.
Most approaches to similarity detection use com-
putationally cheap but linguistically less informed
features (Peng and Hengartner, 2002; Sichel, 1974;
Williams, 1975) such as keywords, function words,
word lengths, and sentence lengths; approaches that
include deeper linguistic information, such as syn-
tactic information, usually incur significant compu-
tational costs (Uzuner et al, 2004). Our approach
identifies useful linguistic information without in-
curring the computational cost of full text pars-
ing; it uses context-free grammars to perform high-
level syntactic analysis of part-of-speech tagged
text (Brill, 1992). It turns out that such a level of
analysis is sufficient to capture syntactic informa-
tion related to creative aspects of writing; this in
turn helps improve recognition of paraphrased doc-
uments. The results presented here show that ex-
traction of useful linguistic information for text clas-
sification purposes does not have to be computa-
tionally prohibitively expensive, and that despite the
tradeoff between the accuracy of features and com-
putational efficiency, we can extract linguistically-
informed features without full parsing.
3 Identifying Creative Aspects of Writing
In this paper, we first identify linguistic elements
of expression and then study patterns in the use of
these elements to recognize a work even when it is
paraphrased. Translated literary works provide ex-
amples of linguistic elements that differ in expres-
sion but convey similar content. These works pro-
vide insight into the linguistic elements that capture
expression. For example, consider the following se-
mantically equivalent excerpts from three different
translations of Madame Bovary by Gustave Flaubert.
Excerpt 1: ?Now Emma would often take it into
her head to write him during the day. Through her
window she would signal to Justin, and he would
whip off his apron and fly to la huchette. And when
Rodolphe arrived in response to her summons, it
was to hear that she was miserable, that her husband
was odious, that her life was a torment.? (Trans-
lated by Unknown1.)
38
Excerpt 2: ?Often, even in the middle of the day,
Emma suddenly wrote to him, then from the win-
dow made a sign to Justin, who, taking his apron
off, quickly ran to la huchette. Rodolphe would
come; she had sent for him to tell him that she was
bored, that her husband was odious, her life fright-
ful.? (Translated by Aveling.)
Excerpt 3: ?Often, in the middle of the day, Emma
would take up a pen and write to him. Then she
would beckon across to Justin, who would off with
his apron in an instant and fly away with the letter
to la huchette. And Rodolphe would come. She
wanted to tell him that life was a burden to her, that
she could not endure her husband and that things
were unbearable.? (Translated by Unknown2.)
Inspired by syntactic differences displayed in
such parallel translations, we identified a novel set
of syntactic features that relate to how people con-
vey content.
3.1 Syntactic Elements of Expression
We hypothesize that given particular content, au-
thors choose from a set of semantically equivalent
syntactic constructs to express this content. To para-
phrase a work without changing content, people
try to interchange semantically equivalent syntactic
constructs; patterns in the use of various syntactic
constructs can be sufficient to indicate copying.
Our observations of the particular expressive
choices of authors in a corpus of parallel translations
led us to define syntactic elements of expression in
terms of sentence-initial and -final phrase structures,
semantic classes and argument structures of verb
phrases, and syntactic classes of verb phrases.
3.1.1 Sentence-initial and -final phrase
structures
The order of phrases in a sentence can shift the
emphasis of a sentence, can attract attention to par-
ticular pieces of information and can be used as an
expressive tool.
1 (a) Martha can finally put some money in the bank.
(b) Martha can put some money in the bank, finally.
(c) Finally, Martha can put some money in the bank.
2 (a) Martha put some money in the bank on Friday.
(b) On Friday, Martha put some money in the bank.
(c) Some money is what Martha put in the bank on Fri-
day.
(d) In the bank is where Martha put some money on
Friday.
The result of such expressive changes affect the
distributions of various phrase types in sentence-
initial and -final positions; studying these distribu-
tions can help us capture some elements of expres-
sion. Despite its inability to detect the structural
changes that do not affect the sentence-initial and
-final phrase types, this approach captures some of
the phrase-level expressive differences between se-
mantically equivalent content; it also captures dif-
ferent sentential structures, including question con-
structs, imperatives, and coordinating and subordi-
nating conjuncts.
3.1.2 Semantic Classes of Verbs
Levin (1993) observed that verbs that exhibit sim-
ilar syntactic behavior are also related semantically.
Based on this observation, she sorted 3024 verbs
into 49 high-level semantic classes. Verbs of ?send-
ing and carrying?, such as convey, deliver,
move, roll, bring, carry, shuttle, and
wire, for example, are collected under this seman-
tic class and can be further broken down into five
semantically coherent lower-level classes which in-
clude ?drive verbs?, ?carry verbs?, ?bring and take
verbs?, ?slide verbs?, and ?send verbs?. Each of
these lower-level classes represents a group of verbs
that have similarities both in semantics and in syn-
tactic behavior, i.e., they can grammatically un-
dergo similar syntactic alternations. For example,
?send verbs? can be seen in the following alterna-
tions (Levin, 1993):
1. Base Form
? Nora sent the book to Peter.
? NP + V + NP + PP.
2. Dative Alternation
? Nora sent Peter the book.
? NP + V + NP + NP.
Semantics of verbs in general, and Levin?s verb
classes in particular, have previously been used for
evaluating content and genre similarity (Hatzivas-
siloglou et al, 1999). In addition, similar seman-
tic classes of verbs were used in natural language
processing applications: START was the first nat-
ural language question answering system to use
such verb classes (Katz and Levin, 1988). We use
39
Levin?s semantic verb classes to describe the ex-
pression of an author in a particular work. We as-
sume that semantically similar verbs are often used
in semantically similar syntactic alternations; we
describe part of an author?s expression in a par-
ticular work in terms of the semantic classes of
verbs she uses and the particular argument struc-
tures, e.g., NP + V + NP + PP, she prefers for them.
As many verbs belong to multiple semantic classes,
to capture the dominant semantic verb classes in
each document we credit all semantic classes of all
observed verbs. We extract the argument structures
from part of speech tagged text, using context-free
grammars (Uzuner, 2005).
3.1.3 Syntactic Classes of Verbs
Levin?s verb classes include exclusively ?non-
embedding verbs?, i.e., verbs that do not take
clausal arguments, and need to be supplemented by
classes of ?embedding verbs? that do take such argu-
ments. Alexander and Kunz (1964) identified syn-
tactic classes of embedding verbs, collected a com-
prehensive set of verbs for each class, and described
the identified verb classes with formulae written in
terms of phrasal and clausal elements, such as verb
phrase heads (Vh), participial phrases (Partcp.), in-
finitive phrases (Inf.), indicative clauses (IS), and
subjunctives (Subjunct.). We used 29 of the more
frequent embedding verb classes and identified their
distributions in different works. Examples of these
verb classes are shown in Table 1. Further examples
can be found in (Uzuner, 2005; Uzuner and Katz,
2005).
Syntactic Formula Example
NP + Vh + NP + from The belt kept him from dying.
+ Partcp.
NP + Vh + that + IS He admitted that he was guilty.
NP + Vh + that I request that she go alone.
+ Subjunct.
NP + Vh + to + Inf. My father wanted to travel.
NP + Vh + wh + IS He asked if they were alone.
NP + pass. + Partcp. He was seen stealing.
Table 1: Sample syntactic formulae and examples of
embedding verb classes.
We study the syntax of embedding verbs by iden-
tifying their syntactic class and the structure of
their observed embedded arguments. After identi-
fying syntactic and semantic characteristics of verb
phrases, we combine these features to create fur-
ther elements of expression, e.g., syntactic classes
of embedding verbs and the classes of semantic non-
embedding verbs they co-occur with.
4 Evaluation
We tested sentence-initial and -final phrase struc-
tures, semantic and syntactic classes of verbs, and
structure of verb arguments, i.e., syntactic elements
of expression, in paraphrase recognition and in pla-
giarism detection in two ways:
? Recognizing titles even when they are para-
phrased, and
? Recognizing pairs of book chapters that are
paraphrases of each other.
For our experiments, we split books into chapters,
extracted all relevant features from each chapter, and
normalized them by the length of the chapter.
4.1 Recognizing Titles
Frequently, people paraphrase parts of rather than
complete works. For example, they may paraphrase
chapters or paragraphs from a work rather than the
whole work. We tested the effectiveness of our
features on recognizing paraphrased components of
works by focusing on chapter-level excerpts (smaller
components than chapters have very sparse vectors
given our sentence-level features and will be the
foci of future research) and using boosted decision
trees (Witten and Frank, 2000).
Our goal was to recognize chapters from the ti-
tles in our corpus even when some titles were para-
phrased into multiple books; in this context, titles
are original works and paraphrased books are trans-
lations of these titles. For this, we assumed the ex-
istence of one legitimate book from each title. We
used this book to train a model that captured the syn-
tactic elements of expression used in this title. We
used the remaining paraphrases of the title (i.e., the
remaining books paraphrasing the title) as the test
set?these paraphrases are considered to be plagia-
rized copies and should be identified as such given
the model for the title.
40
4.1.1 Data
Real life plagiarism data is difficult to obtain.
However, English translations of foreign titles ex-
ist and can be obtained relatively easily. Titles that
have been translated on different occasions by dif-
ferent translators and that have multiple translations
provide us with examples of books that paraphrase
the same content and serve as our surrogate for pla-
giarism data.
To evaluate syntactic elements of expression on
recognizing paraphrased chapters from titles, we
compared the performance of these features with
tfidf-weighted keywords on a 45-way classifica-
tion task. The corpus used for this experiment
included 49 books from 45 titles. Of the 45 ti-
tles, 3 were paraphrased into a total of 7 books
(3 books paraphrased the title Madame Bovary, 2
books paraphrased 20000 Leagues, and 2 books
paraphrased The Kreutzer Sonata). The remaining
titles included works from J. Austen (1775-1817),
C. Dickens (1812-1870), F. Dostoyevski (1821-
1881), A. Doyle (1859-1887), G. Eliot (1819-
1880), G. Flaubert (1821-1880), T. Hardy (1840-
1928), V. Hugo (1802-1885), W. Irving (1789-
1859), J. London (1876-1916), W. M. Thack-
eray (1811-1863), L. Tolstoy (1828-1910), I. Tur-
genev (1818-1883), M. Twain (1835-1910), and
J. Verne (1828-1905).
4.1.2 Baseline Features
The task described in this section focuses on rec-
ognizing paraphrases of works based on the way
they are written. Given the focus of authorship attri-
bution literature on ?the way people write?, to eval-
uate the syntactic elements of expression on recog-
nizing paraphrased chapters of a work, we compared
these features against features frequently used in au-
thorship attribution as well as features used in con-
tent recognition.
Tfidf-weighted Keywords: Keywords, i.e., con-
tent words, are frequently used in content-based text
classification and constitute one of our baselines.
Function Words: In studies of authorship at-
tribution, many researchers have taken advantage
of the differences in the way authors use function
words (Mosteller and Wallace, 1963; Peng and Hen-
gartner, 2002). In our studies, we used a set of 506
function words (Uzuner, 2005).
Distributions of Word Lengths and Sentence
Lengths: Distributions of word lengths and sen-
tence lengths have been used in the literature for
authorship attribution (Mendenhall, 1887; Williams,
1975; Holmes, 1994). We include these features
in our sets of baselines along with information
about means and standard deviations of sentence
lengths (Holmes, 1994).
Baseline Linguistic Features: Sets of surface,
syntactic, and semantic features have been found to
be useful for authorship attribution and have been
adopted here as baseline features. These features
included: the number of words and the number of
sentences in the document; type?token ratio; aver-
age and standard deviation of the lengths of words
(in characters) and of the lengths of sentences (in
words) in the document; frequencies of declara-
tive sentences, interrogatives, imperatives, and frag-
mental sentences; frequencies of active voice sen-
tences, be-passives and get-passives; frequencies of
?s-genitives, of-genitives and of phrases that lack
genitives; frequency of overt negations, e.g., ?not?,
?no?, etc.; and frequency of uncertainty markers,
e.g., ?could?, ?possibly?, etc.
4.1.3 Experiment
To recognize chapters from the titles in our corpus
even when some titles were paraphrased into mul-
tiple books, we randomly selected 40?50 chapters
from each title. We used 60% of the selected chap-
ters from each title for training and the remaining
40% for testing. For paraphrased titles, we selected
training chapters from one of the paraphrases and
testing chapters from the remaining paraphrases. We
repeated this experiment three times; at each round,
a different paraphrase was chosen for training and
the rest were used for testing.
Our results show that, on average, syntactic ele-
ments of expression accurately recognized compo-
nents of titles 73% of the time and significantly out-
performed all baselines3 (see middle column in Ta-
ble 2).4
3The tfidf-weighted keywords used in this experiment do not
include proper nouns. These words are unique to each title and
can be easily replaced without changing content or expression
in order to trick a plagiarism detection system that would rely
on proper nouns.
4For the corpora used in this paper, a difference of 4% or
more is statistically significant with ? = 0.05.
41
Feature Set Avg. Avg.
accuracy accuracy
(complete (para-
corpus) phrases)
only
Syntactic elements of expression 73% 95%
Function words 53% 34%
Tfidf-weighted keywords 47% 38%
Baseline linguistic 40% 67%
Dist. of word length 18% 54%
Dist. of sentence length 12% 17%
Table 2: Classification results (on the test set) for
recognizing titles in the corpus even when some ti-
tles are paraphrased (middle column) and classifi-
cation results only on the paraphrased titles (right
column). In either case, random chance would rec-
ognize a paraphrased title 2% of the time.
The right column in Table 2 shows that the syntac-
tic elements of expression accurately recognized on
average 95% of the chapters taken from paraphrased
titles. This finding implies that some of our elements
of expression are common to books that are derived
from the same title. This commonality could be due
to the similarity of their content or due to the under-
lying expression of the original author.
4.2 Recognizing Pairs of Paraphrased
Chapters
Experiments in Section 4.1 show that we can use
syntactic elements of expression to recognize titles
and their components based on the way they are
written even when some works are paraphrased. In
this section, our goal is to identify pairs of chapters
that paraphrase the same content, i.e., chapter 1 of
translation 1 of Madame Bovary and chapter 1 of
translation 2 of Madame Bovary. For this evalua-
tion, we used a similar approach to that presented by
Nahnsen et al (2005).
4.2.1 Data
Our data for this experiment included 47 chap-
ters from each of two translations of 20000 Leagues
under the Sea (Verne), 35 chapters from each of 3
translations of Madame Bovary (Flaubert), 28 chap-
ters from each of two translations of The Kreutzer
Sonata (Tolstoy), and 365 chapters from each of 2
translations of War and Peace (Tolstoy). Pairing
up the chapters from these titles provided us with
more than 1,000,000 chapter pairs, of which approx-
imately 1080 were paraphrases of each other.5
4.2.2 Experiment
For experiments on finding pairwise matches, we
used similarity of vectors of tfidf-weighted key-
words;6 and the multiplicative combination of the
similarity of vectors of tfidf-weighed keywords of
works with the similarity of vectors of syntactic ele-
ments of expression of these works. We used cosine
to evaluate the similarity of the vectors of works. We
omitted the remaining baseline features from this
experiment?they are features that are common to
majority of the chapters from each book, they do
not relate to the task of finding pairs of chapters that
could be paraphrases of each other.
We ranked all chapter pairs in the corpus based
on their similarity. From this ranked list, we iden-
tified the top n most similar pairs and predicted that
they are paraphrases of each other. We evaluated our
methods with precision, recall, and f-measure.7
Figure 1: Precision.
Figures 1, 2, and 3 show that syntactic elements
of expression improve the performance of tfidf-
weighted keywords in recognizing pairs of para-
phrased chapters significantly in terms of precision,
recall, and f-measure for all n; in all of these figures,
the blue line marked syn tdf represents the per-
formance of tfidf-weighted keywords enhanced with
5Note that this number double-counts the paraphrased pairs;
however, this fact is immaterial for our discussion.
6In this experiment, proper nouns are included in the
weighted keywords.
7The ground truth marks only the same chapter from two
different translations of the same title as similar, i.e., chapter x
of translation 1 of Madame Bovary and chapter y of translation
2 of Madame Bovary are similar only when x = y.
42
Figure 2: Recall.
syntactic elements of expression. More specifically,
the peak f-measure for tfidf-weighted keywords is
approximately 0.77 without contribution from syn-
tactic elements of expression. Adding information
about similarity of syntactic features to cosine sim-
ilarity of tfidf-weighted keywords boosts peak f-
measure value to approximately 0.82.8 Although
the f-measure of both representations degrade when
n > 1100, this degradation is an artifact of the eval-
uation metric: the corpus includes only 1080 similar
pairs, at n > 1100, recall is very close to 1, and
therefore increasing n hurts overall performance.
Figure 3: F-measure.
5 Conclusion
Plagiarism is a problem at all levels of education.
Increased availability of digital versions of works
makes it easier to plagiarize others? work and the
large volumes of information available on the web
makes it difficult to identify cases of plagiarism.
8The difference is statistically significant at ? = 0.05.
To identify plagiarism even when works are para-
phrased, we propose studying the use of particular
syntactic constructs as well as keywords in docu-
ments.
This paper shows that syntactic information can
help recognize works based on the way they are
written. Syntactic elements of expression that fo-
cus on the changes in the phrase structure of works
help identify paraphrased components of a title. The
same features help improve identification of pairs
of chapters that are paraphrases of each other, de-
spite the content these chapters share with the rest
of the chapters taken from the same title. The re-
sults presented in this paper are based on experi-
ments that use translated novels as surrogate for pla-
giarism data. Our future work will extend our study
to real life plagiarism data.
6 Acknowledgements
The authors would like to thank Sue Felshin for her
insightful comments. This work is supported in part
by the Advanced Research and Development Activ-
ity as part of the AQUAINT research program.
References
D. Alexander and W. J. Kunz. 1964. Some classes of
verbs in English. In Linguistics Research Project. In-
diana University, June.
E. Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the 3rd Conference on Applied
Natural Language Processing.
V. Hatzivassiloglou, J. Klavans, and E. Eskin. 1999. De-
tecting similarity by applying learning over indicators.
In Proceedings of the 37th Annual Meeting of the ACL.
D. I. Holmes. 1994. Authorship attribution. Computers
and the Humanities, 28.
B. Katz and B. Levin. 1988. Exploiting lexical reg-
ularities in designing natural language systems. In
Proceedings of the 12th Int?l Conference on Compu-
tational Linguistics (COLING ?88).
B. Levin. 1993. English Verb Classes and Alternations.
A Preliminary Investigation. University of Chicago
Press.
T. C. Mendenhall. 1887. Characteristic curves of com-
position. Science, 11.
43
F. Mosteller and D. L. Wallace. 1963. Inference in an au-
thorship problem. Journal of the American Statistical
Association, 58(302).
T. Nahnsen, O?. Uzuner, and B. Katz. 2005. Lexical
chains and sliding locality windows in content-based
text similarity detection. CSAIL Memo, AIM-2005-
017.
R. D. Peng and H. Hengartner. 2002. Quantitative analy-
sis of literary styles. The American Statistician, 56(3).
H. S. Sichel. 1974. On a distribution representing
sentence-length in written prose. Journal of the Royal
Statistical Society (A), 137.
O?. Uzuner and B. Katz. 2005. Capturing expression us-
ing linguistic information. In Proceedings of the 20th
National Conference on Artificial Intelligence (AAAI-
05).
O?. Uzuner, R. Davis, and B. Katz. 2004. Using em-
pirical methods for evaluating expression and content
similarity. In Proceedings of the 37th Hawaiian Inter-
national Conference on System Sciences (HICSS-37).
IEEE Computer Society.
O?. Uzuner. 2005. Identifying Expression Fingerprints
Using Linguistic Information. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
C. B. Williams. 1975. Mendenhall?s studies of word-
length distribution in the works of Shakespeare and
Bacon. Biometrika, 62(1).
I. H. Witten and E. Frank. 2000. Data Mining: Practical
Machine Learning Tools with Java Implementations.
Morgan Kaufmann, San Francisco.
44
