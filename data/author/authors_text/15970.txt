Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 556?566,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Factored Soft Source Syntactic Constraints for Hierarchical Machine
Translation
Zhongqiang Huang
Raytheon BBN Technologies
50 Moulton St
Cambridge, MA, USA
zhuang@bbn.com
Jacob Devlin
Raytheon BBN Technologies
50 Moulton St
Cambridge, MA, USA
jdevlin@bbn.com
Rabih Zbib
Raytheon BBN Technologies
50 Moulton St
Cambridge, MA, USA
rzbib@bbn.com
Abstract
This paper describes a factored approach to
incorporating soft source syntactic constraints
into a hierarchical phrase-based translation
system. In contrast to traditional approaches
that directly introduce syntactic constraints to
translation rules by explicitly decorating them
with syntactic annotations, which often ex-
acerbate the data sparsity problem and cause
other problems, our approach keeps transla-
tion rules intact and factorizes the use of syn-
tactic constraints through two separate mod-
els: 1) a syntax mismatch model that asso-
ciates each nonterminal of a translation rule
with a distribution of tags that is used to
measure the degree of syntactic compatibil-
ity of the translation rule on source spans; 2)
a syntax-based reordering model that predicts
whether a pair of sibling constituents in the
constituent parse tree of the source sentence
should be reordered or not when translated to
the target language. The features produced
by both models are used as soft constraints
to guide the translation process. Experiments
on Chinese-English translation show that the
proposed approach significantly improves a
strong string-to-dependency translation sys-
tem on multiple evaluation sets.
1 Introduction
Hierarchical phrase-based translation models (Chi-
ang, 2007) are widely used in machine translation
systems due to their ability to achieve local flu-
ency through phrasal translation and handle non-
local phrase reordering using synchronous context-
free grammars. A large number of previous works
have tried to introduce grammaticality to the trans-
lation process by incorporating syntactic constraints
into hierarchical translation models. Despite some
differences in the granularity of syntax units (e.g.,
tree fragments (Galley et al, 2004; Liu et al, 2006),
treebank tags (Shen et al, 2008; Chiang, 2010), and
extended tags (Zollmann and Venugopal, 2006)),
most previous work incorporates syntax into hier-
archical translation models by explicitly decorating
translation rules with syntactic annotations. These
approaches inevitably exacerbate the data sparsity
problem and cause other problems such as increased
grammar size, worsened derivational ambiguity, and
unavoidable parsing errors (Hanneman and Lavie,
2013).
In this paper, we propose a factored approach
that incorporates soft source syntactic constraints
into a hierarchical string-to-dependency translation
model (Shen et al, 2008). The general ideas are ap-
plicable to other hierarchical models as well. Instead
of enriching translation rules with explicit syntactic
annotations, we keep the original translation rules
intact, and factorize the use of source syntactic con-
straints through two separate models.
The first is a syntax mismatch model that intro-
duces source syntax into the nonterminals of transla-
tion rules, and measures the degree of syntactic com-
patibility between a translation rule and the source
spans it is applied to during decoding. When a hi-
erarchical translation rule is extracted from a par-
allel training sentence pair, we determine a tag for
each nonterminal based on the dependency parse of
the source sentence. Instead of fragmenting rule
statistics by directly labeling nonterminals with tags,
556
we keep the original string-to-dependency transla-
tion rules intact and associate each nonterminal with
a distribution of tags. That distribution is then used
to measure the syntactic compatibility between the
syntactic context from which the translation rule is
extracted and the syntactic analysis of a test sen-
tence.
The second is a syntax-based reordering model
that takes advantage of phrasal cohesion in transla-
tion (Fox, 2002). The reordering model takes a pair
of sibling constituents in the source parse tree as in-
put, and uses source syntactic clues to predict the
ordering distribution (straight vs. inverted) of their
translations on the target side. The resulting order-
ing distribution is used in the decoder at the word
pair level to guide the translation process. This sep-
arate reordering model allows us to utilize source
syntax to improve reordering in hierarchical trans-
lation models without having to explicitly annotate
translation rules with source syntax.
Our results show that both the syntax mismatch
model and the syntax-based reordering model are
able to achieve significant gains over a strong
Chinese-English MT baseline. The rest of the pa-
per is organized as follows. Section 2 discusses
related work in the literature. Section 3 provides
an overview of our baseline string-to-dependency
translation system. Section 4 describes the details
of the syntax mismatch and syntax-based reordering
models. Experimental results are presented in Sec-
tion 5. The last section concludes the paper.
2 Related Work
Attempts to use rich syntactic annotations do not
always result in improved performance when com-
pared to purely hierarchical models that do not
use linguistic guidance. For example, as shown
in (Mi and Huang, 2008), tree-to-string translation
models (Huang et al, 2006) only start to outper-
form purely hierarchical models when significant ef-
forts were made to alleviate parsing errors by using
forest-based approaches in both rule extraction and
decoding. Using only syntactic phrases is too re-
strictive in phrasal translation as many useful phrase
pairs are not syntactic constituents (Koehn et al,
2003). The syntax-augmented translation model
of Zollmann and Venugopal (2006) annotates non-
terminals in hierarchical rules with thousands of ex-
tended syntactic categories in order to capture the
syntactic variations of phrase pairs. This results
in exacerbated data sparsity problems, partially due
to the requirement of exact matches in nonterminal
substitutions between translation rules in the deriva-
tion. Several solutions were proposed. Shen et
al. (2009) and Chiang (2010) used soft match fea-
tures to explicitly model the substitution of nonter-
minals with different labels; Venugopal et al (2009)
used a preference grammar to soften the syntactic
constraints through the use of a preference distribu-
tion of syntactic categories; and recently Hanneman
and Lavie (2013) proposed a clustering approach
to reduce the number of syntactic categories. Our
proposed syntax mismatch model associates non-
terminals with a distribution of tags. It is simi-
lar to the preference grammar in (Venugopal et al,
2009); however, we use treebank tags and focus on
the syntactic compatibility between translation rules
and the source sentence. The work of Huang et al
(2010) is most similar to ours, with the main differ-
ence being that their syntactic categories are latent
and learned automatically in a data driven fashion
while we simply use treebank tags based on depen-
dency parsing. Marton and Resnik (2008) also ex-
ploited soft source syntax constraints without mod-
ifying translation rules. However, they focused on
the quality of translation spans based on the syn-
tactic analysis of the source sentence, while our
method explicitly models the syntactic compatibil-
ity between translation rules and source spans.
Most research on reordering in machine transla-
tion focuses on phrase-based translation models as
they are inherently weak at non-local reordering.
Previous efforts to improve reordering for phrase-
based systems can be largely classified into two cat-
egories. Approaches in the first category try to re-
order words in the source sentence in a preprocess-
ing step to reduce reordering in both word alignment
and MT decoding. The reordering decisions are ei-
ther made using manual or automatically learned
rules (Collins et al, 2005; Xia and McCord, 2004;
Xia and McCord, 2004; Genzel, 2010) based on the
syntactic analysis of the source sentence, or con-
structed through an optimization procedure that uses
feature-based reordering models trained on a word-
aligned parallel corpus (Tromble and Eisner, 2009;
557
Khapra et al, 2013). Approaches in the second cate-
gory try to explicitly model phrase reordering in the
translation process. These approaches range from
simple distance based distortion models (Koehn et
al., 2003) that globally penalizes reordering based
on the distorted distance, to lexicalized reordering
models (Koehn et al, 2005; Al-Onaizan and Pap-
ineni, 2006) that assign reordering preferences of
adjacent phrases for individual phrases, and to hi-
erarchical reordering models (Galley and Manning,
2008; Cherry, 2013) that handle reordering prefer-
ences beyond adjacent phrases. Although hierarchi-
cal translation models are capable of handling non-
local reordering, their accuracy is far from perfect.
Xu et al (2009) showed that the syntax-augmented
hierarchical model (Zollmann and Venugopal, 2006)
also benefits from reordering source words in a pre-
processing step. Explicitly adding syntax to trans-
lation rules helps with reordering in general, but it
introduces additional complexities, and is still lim-
ited by the context-free nature of hierarchical rules.
Our work exploits an alternative direction that uses
an external reordering model to improve word re-
ordering of hierarchical models. Gao et al (2011),
Xiong et al (2012), and Li et al (2013) also studied
external reordering models for hierarchical models.
However, they focused on specific word pairs such
as a word and its dependents or a predicate and its
arguments, while our proposed general framework
considers all word pairs in a sentence. Our syntax-
based reordering model exploits phrasal cohesion in
translation (Fox, 2002) by modeling the reordering
of sibling constituents in the source parse tree, which
is similar to the recent work of Yang et al (2012).
However, the latter focuses on finding the optimal
reordering of sibling constituents before MT decod-
ing, while our proposed model generates reordering
features that are used together with other MT fea-
tures to determine the optimal reordering during MT
decoding.
3 String-to-Dependency Translation
Our baseline translation system is based on a string-
to-dependency translation model similar to the im-
plementation in (Shen et al, 2008). It is an extension
of the hierarchical translation model of Chiang et al
(2006) that requires the target side of a phrase pair
to have a well-formed dependency structure, defined
as either of the two types:
? fixed structure: a single rooted dependency
sub-tree with each child being a complete con-
stituent. In this case, the phrase has a unique
head word inside the phrase, i.e., the root of
the dependency sub-tree. Each dependent of
the head word, together with all of its descen-
dants, is either completely inside the phrase or
completely outside the phrase. For example,
the phrase give him in Figure 1 (a) has a fixed
dependency structure with head word give.
? floating structure: a sequence of siblings with
each being a complete constituent. In this case,
the phrase is composed of a sequence of sibling
constituents whose common parent is outside
the phrase. For example, the phrase him that
brown coat in Figure 1 is a floating structure
whose common parent give is not in the phrase.
Requiring the target side to have a well-formed
dependency structure is less restrictive than requir-
ing it to be a syntactic constituent, allowing more
translation rules to be extracted. However, it still
results in fewer rules than pure hierarchical transla-
tion models and might hurt MT performance. The
well-formed dependency structure on the target side
makes it possible to introduce syntax features dur-
ing decoding. Shen et al (2008) obtained signif-
icant improvements from including a dependency
language model score in decoding, outweighing the
negative effect of the dependency constraint. Shen et
al. (2009) proposed an approach to label each non-
terminal, which can be either on the left-hand-side
(LHS) or the right-hand-side (RHS) of the rule, with
the head POS tag of the underlying target phrase if
it has a fixed dependency structure1, and measure
the mismatches between nonterminal labels when a
RHS nonterminal of a rule is substantiated with the
LHS nonterminal of another rule during decoding.
This also resulted in further improvements in MT
performance. Figure 1 (c) shows an example string-
to-dependency translation rule in our baseline sys-
tem.
1Nonterminals corresponding to floating structures keep
their default label ?X? as experiments show that it is not bene-
ficial to label them differently.
558
X :  give  X
2 
 X
1
 
X  :  X
1
    X
2
(b) pure hierarchical rule
VV  :  give  PRP
2  
NN
1
 
X  :  X
1
    X
2
(c) string-to-dependency rule
??  ??  ?  ??    ?
 give   him
   
that  brown  coat 
(a) word alignments
Figure 1: An example of extracting a string-to-
dependency translation rule from word alignments. The
nonterminals on the target side of the hierarchical rule
(b) all correspond to fixed dependency structures and so
they are labeled by the respective head tag in the string-
to-dependency rule (c).
4 Factored Syntactic Constraints
Although the string-to-dependency formulation
helps to improve the grammaticality of translations,
it lacks the ability to incorporate source syntax into
the translation process. We next describe a factored
approach to address this problem by utilizing source
syntax through two models: one that introduces syn-
tactic awareness to translation rules themselves, and
another that focuses on reordering based on the syn-
tactic analysis of the source.
4.1 Syntax Mismatch Model
A straightforward method to introduce awareness
of source syntax to translation rules is to apply
the same well-formed dependency constraint and
head POS annotation on the target side of string-
to-dependency translation rules to the source side.
However, as discussed earlier, this would signifi-
cantly reduce the number of rules that can be ex-
tracted, exacerbate data sparsity, and cause other
problems, especially given that the target side is al-
ready constrained by the dependency requirement.
A relaxed method is to bypass the dependency
constraint and only annotate source nonterminals
whose underlying phrase is a fixed dependency
structure with the head POS tag of the phrase. This
method would still extract all of the rules that can
be extracted from the baseline string-to-dependency
VV  :  give  PRP
2  
NN
1
 
X  :  X
1
    X
2
2
4
VV : 0.7
NN : 0.1
X : 0.2
3
5
2
4
NN : 0.8
VV : 0.1
X : 0.1
3
5
2
4
PN : 0.5
NN : 0.4
X : 0.1
3
5
VV
(a) nonterminal tag distributions
source:
gross:
NN
PN
span tag:
(b) source span tags

?
his
? ?
pen
?
me


give
dependency:
Figure 2: Example distribution of tags for nonterminals
on the source side (a) and example tags for source spans
(b)
translation model, but the extra annotation on non-
terminals can split a rule into multiple rules, with the
only difference being the nonterminal labels on the
source side. Unfortunately, our experiments have
shown that even this moderate annotation results
in significantly lower translation quality due to the
fragmentation of translation rules, and the increased
derivational ambiguity. We have also tried to include
some source tag mismatch features (with details de-
scribed later) to measure the syntactic compatibility
between the nonterminal labels of a translation rule
and the corresponding tags of source spans. This im-
proves translation accuracy, but not enough to com-
pensate for the performance drop caused by annotat-
ing source nonterminals.
Our proposed method introduces syntax to trans-
lation rules without sacrificing performance. Instead
of imposing dependency constraints or explicitly an-
notating source nonterminals, we keep the original
string-to-dependency translation rules intact and as-
sociate each nonterminal on the source side with a
distribution of tags. The tags are determined based
on the dependency structure of training samples. If
the underlying source phrase of a nonterminal is a
fixed dependency structure in a training sample, we
use the head POS tag of the phrase as the tag. Oth-
erwise, we use the default tag ?X? to denote float-
559
Feature Condition Value
f1 ts = X P(tr = X)
f2 ts = X P(tr ? X)
f3 ts ? X P(tr = X)
f4 ts ? X P(tr = ts)
f5 ts ? X P(tr ? X, tr ? ts)
Table 1: Source tag mismatch features. The default value
of each feature is zero if the source span tag ts does not
match the condition
ing structures and dependency structures that are not
well formed. As a result, we still extract the same
set of rules as in the baseline string-to-dependency
translation model, and also obtain a distribution of
tags for each nonterminal. Figure 2 (a) illustrates the
example tag distributions of a string-to-dependency
translation rule. The tag distributions provide an ap-
proximation of the source syntax of the training data
from which the translation rules are extracted. They
are used to measure the syntactic compatibility be-
tween a translation rule and the source spans it is
applied to. At decoding time, we parse the source
sentence and assign each span a tag in the same way
as it is done during rule extraction, as shown in the
example in Figure 2 (b). When a translation rule is
used to expand a derivation, for each nonterminal
(which can be on the LHS or RHS) on the source
side of the rule, five source tag mismatch features
are computed based on the distribution of tags P(tr)
on the rule nonterminal, and the tag ts on the cor-
responding source span. The features are defined in
Table 1. We use soft features instead of hard syn-
tactic constraints, and allow the tuning process to
choose the appropriate weight for each feature. As
shown in Section 5, these source syntax mismatch
features help to improve the baseline system.
4.2 Syntax-based Reordering Model
Most previous research on reordering models has fo-
cused on improving word reordering for statistical
phrase-based translation systems (e.g., (Collins et
al., 2005; Al-Onaizan and Papineni, 2006; Tromble
and Eisner, 2009)). There has been less work on im-
proving the reordering of hierarchical phrase-based
translation systems (see (Xu et al, 2009; Gao et al,
2011; Xiong et al, 2012) for a few exceptions), ex-
cept through explicit syntactic annotation of transla-
tion rules. It is generally assumed that hierarchical
models are inherently capable of handling both lo-
cal and non-local reorderings. However, many hier-
archical translation rules are noisy and have limited
context, and so may not be able to produce transla-
tions in the right order.
We propose a general framework that incorpo-
rates external reordering information into the decod-
ing process of hierarchical translation models. To
simplify the presentation, we make the assumption
that every source word translates to one or more
target words, and that the translations for a pair
of source words is either straight or inverted. We
discuss the general case later. Given a sentence
w1,?,wn, suppose we have a separate reordering
model that predicts Porder(oij), the probability distri-
bution of ordering oij ? {straight, inverted} between
the translations of any source word pair (wi,wj).
We can measure the goodness of a given hypothe-
sis h with respect to the ordering predicted by the
reordering model as the sum of log probabilities2
for ordering each pair of source words, as defined
in Equation 1:
forder(h) = ?
1?i<j?n
log Porder(oij = ohij) (1)
where ohij is the ordering between the translations of
source word pair (wi,wj) in hypothesis h. The re-
ordering score forder(h) can be computed efficiently
through recursion during hierarchical decoding as
follows:
? Base case: for phrasal (i.e. non-hierarchical)
rules, the ordering of translations for any word
pair covered by the source phrase can be deter-
mined based on the word alignment of the rule.
The value of the reordering score can be simply
computed according to Equation 1.
? Recursive case: when a hierarchical rule is used
to expand a partial derivation, two types of
word pairs are encountered: a) word pairs that
are covered exclusively by one of the nonter-
minals on the RHS of the rule, and b) other
2In practice, the log probability is thresholded to avoid neg-
ative infinity, which would otherwise result in a hard constraint.
560
(a)
VV  :  give  PRP
2  
NN
1
 
X  :  X
1
    X
2
source:
gross:
 
his
 
pen

me

give
(b)
word pair translation order
(?, ) inverted
(?, ?) inverted
(?, ) inverted
(?, ?) inverted
(?, ) inverted
(?, ?) inverted
(, ?) straight
(?, ?) previously considered
(?, ?) previously considered
(?, ?) previously considered
Figure 3: An example rule application (a) with the trans-
lation order of new source word pairs covered by the rule
shown in (b). The translation order of word pairs covered
by X1 is previously considered and is thus not shown.
word pairs. The reordering scores of the for-
mer would be already computed in previous
rule applications, and can simply be retrieved
from the partial derivation. Word pairs of the
latter case are new word pairs introduced by the
hierarchical rule, and their ordering can be de-
termined based on the alignment of the hierar-
chical rule. The value of the reordering score
of the new derivation is the sum of the reorder-
ing scores retrieved from the partial derivations
for the nonterminals and the reordering scores
of the new word pairs.
Figure 3 shows an example of determining the
ordering of translations when applying a string-to-
dependency rule. The alignment in the translation
rule is able to fully determine the translation order
for all new word pairs introduced by the rule. For
example, ??/pen? is covered by X1 in the rule and
the translation order for X1 and ??/give? is inverted
on the target side. Since ??/pen? is translated to-
gether with other words covered by X1 as a group,
we can determine that the translation order between
the source word pair ??/pen? and ??/give? is also
inverted on the target side. The words ??/his?,
???, ?? /pen? are all covered by the same nonter-
Reordering features
The syntactic production rule
The syntactic labels of the nodes in the context
The head POS tags of the nodes in the context
The dep. labels of the nodes in the context
The seq. of dep. labels connecting the two nodes
The length of the nodes in the context
Table 2: Features in the reordering model
minal X1 and thus their pairwise reordering scores
have already been considered in previous rule appli-
cations.
In practice, not all source words in a translation
rule are translated to a target word; sometimes there
is no clear ordering between the translations of two
source words. In such cases we use a binary discount
feature instead of the reordering feature.
This reordering framework relies on an external
model to provide the ordering probability distribu-
tion of source word pairs. In this paper, we inves-
tigate a simple maximum-entropy reordering model
based on the syntactic parse tree of the source sen-
tence. This allows us to take advantage of the source
syntax to improve reordering without using syntactic
annotations in translation rules. The syntax-based
reordering model attempts to predict the reordering
probability of a pair of sibling constituents in the
source parse tree, building on the fact that syntac-
tic phrases tend to move in a group during transla-
tion (Fox, 2002). The reordering model is trained on
a word-aligned corpus. For each pair of sibling con-
stituents in the source parse tree, we determine the
translation order on the target side based on word
alignments. If there is a clear ordering3, i.e., either
straight or inverted, on the target side, we include
the context of the constituent pair and its translation
order as a sample for training or evaluating the max-
imum entropy reordering model. Table 2 lists the
features of the reordering model.
The ordering distributions of source word pairs
are determined based on the ordering distributions
of sibling constituent pairs. For each pair of sib-
3If the translations overlap with other, the non-overlapping
parts are used to determine the translation order.
561
ling constituents4 in the parse tree of a source sen-
tence, we compute its distribution of translation or-
der using the reordering model. The distribution is
shared among all word pairs covered by the respec-
tive constituents, which guarantees that the order-
ing distribution of any source word pair is computed
exactly once. The ordering distributions of source
word pairs are then used through the general reorder-
ing framework in the decoder to guide the decoding
process.
5 Experiments
5.1 Experimental Setup
Our main experiments use the Chinese-English par-
allel training data and development sets released by
the LDC, and made available to the DARPA GALE
and BOLT programs. We train the translation model
on 100 million words of parallel data. We use a 8 bil-
lion words of English monolingual data to train two
language models: a trigram language model used in
chart decoding, and a 5-gram language model used
in n-best rescoring. The systems are tuned and eval-
uated on a mixture of newswire and web forum text
from the development sets available for the DARPA
GALE and BOLT programs, with up to 4 indepen-
dent references for each source sentence. We also
evaluate our final systems on both newswire and
web text from the NIST MT06 and MT08 evalua-
tions using an experimental setup compatible with
the NIST MT12 Chinese-English constrained track.
In this setup, the translation and language models
are trained on 35 million words of parallel data and
3.8 billion words of English monolingual data, re-
spectively. The systems are tuned on the MT02-
05 development sets. All systems are tuned and
evaluated on IBM BLEU (Papineni et al, 2002).
The baseline string-to-dependency translation sys-
tem uses more than 10 core features and a large num-
ber of sparse binary features similar to the method
described in (Chiang et al, 2009). It achieves trans-
lation accuracies comparable to the top ranked sys-
tems in the NIST MT12 evaluation.
4Note that the constituent pairs used to train the reordering
model are filtered to only contain these with clear ordering on
the target side, while no such pre-filtering is applied to con-
stituent pairs when applying the reordering model in translation.
We leave it to future work to address this mismatch problem.
GIZA++ (Och and Ney, 2003) is used for auto-
matic word alignment in all of the experiments. We
use Charniak?s parser (Charniak and Johnson, 2005)
on the English side to obtain string-to-dependency
translation rules, and use a latent variable PCFG
parser (Huang and Harper, 2009) to parse the source
side of the parallel training data as well as the
test sentences for extracting syntax mismatch and
reordering features. For both languages, depen-
dency structures are read off constituency trees us-
ing manual head word percolation rules. We use
a lexicon-based longest-match-first word segmenter
to tokenize source Chinese sentences. Since the
source tokenization used in our MT system is dif-
ferent from the treebank tokenization used to train
the Chinese parser, the source sentences are first to-
kenized using the treebank-trained Stanford Chinese
segmenter (Tseng et al, 2005), then parsed with
the Chinese parser, and finally projected to MT tok-
enization based on the character alignment between
the tokens. The syntax-based reordering model is
trained on a set of Chinese-English manual word
alignment corpora released by the LDC5.
5.2 Syntax Mismatch Model
We first conduct experiments on the GALE/BOLT
data sets to evaluate different strategies of incor-
porating source syntax into string-to-dependency
translation rules. As mentioned in Section 4.1, con-
straining the source side of translation rules to only
well-formed dependency structures is too restrictive
given that our baseline system already has depen-
dency constraint on the target side. We evaluate
the relaxed method that only annotates source non-
terminals with the head POS tag of the underlying
phrase if the phrase is a fixed dependency structure.
As shown in Table 3, nonterminal annotation results
in a big drop in performance, decreasing the BLEU
score of the baseline from 27.82 to 25.54. This sug-
gests that it is undesirable to further fragment the
translation rules. Introducing the syntax mismatch
features described in Section 4.1 helps to improve
5The alignment corpora are LDC2012E24, LDC2012E72,
LDC2012E95, and LDC2013E02. The reordering model can
also be trained on automatically aligned data; however, our ex-
periments show that using manual alignments results in a bet-
ter accuracy for the reordering model itself and more improve-
ments for the MT system.
562
BLEU
baseline 27.82
+ tag annotation only 25.54
+ tag annotation, mismatch feat. 25.90
+ tag distribution, mismatch feat. 28.23
Table 3: Effects of tag annotation, tag distribution, and
syntax mismatch features on MT performance on the
GALE/BOLT data set.
BLEU from 25.54 to 25.90. This improvement is
not large enough to compensate for the performance
drop caused by annotating the nonterminals.
Our proposed approach, on the other hand, does
not modify the translation rules in the baseline sys-
tem, but only associates each nonterminal with a dis-
tribution of tags. For that reason, it does not suffer
from the aforementioned problem. It achieves ex-
actly the same performance as the baseline system
if no source syntactic constraints are imposed dur-
ing decoding. When the source syntax mismatch
features are used, the proposed approach is able to
achieve a gain of 0.41 in BLEU over the baseline
system. Table 4 lists the learned weights of the syn-
tax mismatch features after MT tuning. The nega-
tive weights of f1 and f2 mean that the MT system
penalizes source spans that do not have a fixed de-
pendency structure, and it assigns a higher penalty
to rules whose nonterminals have a high probability
of being extracted from source phrases that do not
have a fixed dependency structure. When the source
span has a fixed dependency structure, the MT sys-
tem prefers translation rules that have a high proba-
bility of matching the tag on the source span (feature
f4) over the ones that do not match (features f3 and
f5). This result is consistent with our expectations
of the syntax mismatch features.
Feature Description Weight
f1 ts = X, tr = X ?1.543
f2 ts = X, tr ? X ?0.676
f3 ts ? X, tr = X 0.380
f4 ts ? X, tr ? X, tr = ts 1.677
f5 ts ? X, tr ? X, tr ? ts 0.232
Table 4: Learned syntax mismatch feature weight
5.3 Syntax-based Reordering Model
Before evaluating the syntax-based reordering
model, we would like to establish the upper bound
improvement that could be achieved using the gen-
eral reordering framework for hierarchical transla-
tion models. Towards that goal, we conduct an ora-
cle experiment on the GALE/BOLT development set
that uses the oracle translation order from the ref-
erence as the external reordering model. For each
source sentence in the development set, we pair it
with the first reference translation (out of up to 4 in-
dependent translations). We then add the sentence
pairs from the development set to the parallel train-
ing data and run GIZA++ to obtain word alignments.
We consider the GIZA++ word alignments for the
development set to be all correct, and use it to de-
termine the oracle order in the reference translation.
For the ordering distribution, we set the log proba-
bility of the reference translation order to 0 and the
reverse order to -1 to avoid negative infinity. As
shown in Table 5, the system tuned and evaluated
with the oracle reordering model significantly out-
performs the baseline by a large margin of 2.32 in
BLEU on the GALE/BOLT test set. This suggests
that there is room for potential improvement by us-
ing a fairly trained reordering model.
BLEU
baseline 27.82
+ oracle reorder 30.14
+ syntax reorder 28.40
Table 5: Effects of external reordering features on MT
performance on the GALE/BOLT test set.
We next evaluate the syntax-based reordering
model. We train the model on manually aligned
Chinese-English corpora. Since the tokenization
used in the manual alignment corpora is different
from the tokenization used in our MT system, the
manual alignment is projected to the MT tokeniza-
tion based on the character alignment between the
tokens. Some extraneously tagged alignment links
in the manual alignment corpora are not useful for
machine translation and are thus removed before
projecting the alignment. As described in Sec-
tion 4.2, the syntax-based reordering method mod-
563
els the translation order of sibling constituent pairs
in the source parse tree. As a result of strong phrasal
cohesion (Fox, 2002), we find that 94% of con-
stituent pairs have a clear ordering on the target
side. We only retain these constituent pairs for train-
ing and evaluating the reordering model. In order
to evaluate the accuracy of the maximum entropy
reordering model, we divide the manual alignment
corpora into 2/3 for training and 1/3 for evaluation.
A baseline that only chooses the majority order (i.e.
straight) has an accuracy of 69%, while the syntax-
based reordering model improves the accuracy to
79%.
The final reordering model used in MT is trained
on all of the samples extracted from the manual
alignment corpora. As shown in Table 5, the syntax-
based reordering feature improves the baseline by
0.58 in BLEU, which is a good improvement given
our strong baseline. Table 6 lists the number of
shifting errors in TER measurement (Snover et al,
2006) of various systems on the GALE/BOLT test
set. The syntax-based reordering model achieves a
6.1% reduction in the number of shifting errors in
the baseline system, and its combination with the
syntax mismatch model achieves an additional re-
duction of 0.6%. This suggests that the proposed
method helps to improve word reordering in transla-
tion.
Shifting errors
baseline 3205
+ syntax mismatch 3089
+ syntax reorder 3010
+ syntax mismatch and reorder 2990
Table 6: Number of shifting errors in TER measurement
of multiple systems on the GALE/BOLT test set
5.4 Final Results
Table 7 shows the final results on the GALE/BOLT
test set, as well as the NIST MT06 and MT08 test
sets. Both the syntax mismatch and the syntax-based
reordering features improve the baseline system, re-
sulting in moderate to significant gains in all of the
five test sets. The two features are complementary
to each other and their combination results in better
improvement in four out of the five test sets com-
pared to adding them separately. In three out of the
five test sets, the improvement from the combina-
tion of the two features is statistically significant at
the 95% confidence level over the baseline, with the
largest absolute improvement of 1.43 in BLEU ob-
tained on MT08 web.
6 Conclusion
In this paper, We have discussed problems resulting
from explicitly decorating translation rules with syn-
tactic annotations. We presented a factored approach
to incorporate soft source syntax mismatch and re-
ordering constraints to hierarchical machine transla-
tion, and showed how our models avoid the pitfalls
of the explicit decoration approach. Experiments on
Chinese-English translation show that the proposed
approach significantly improves a strong string-to-
dependency translation baseline on multiple evalu-
ation sets. There are many directions in which this
work can be continued. The syntax mismatch model
can be extended to dynamically adjust the transla-
tion distribution based on the syntactic compatibil-
ity between a translation rule and a source sentence.
It also might be beneficial to look beyond syntactic
constituent pairs when modeling reordering, given
that phrasal cohesion does not always hold in trans-
lation. The general framework that uses an external
reordering model in hierarchical models via features
can also be naturally extended to use multiple re-
ordering models.
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program. The views expressed are those of the au-
thors and do not reflect the official policy or position
of the Department of Defense or the U.S. Govern-
ment. The authors would like to thank the anony-
mous reviewers for their helpful comments.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics.
564
MT06 news MT06 web MT08 news MT08 web GALE/BOLT
baseline 43.76 36.13 40.52 27.78 27.82
+ syntax mismatch 43.89 36.72 40.82+ 28.54- 28.23-
+ syntax reorder 44.01 36.40 41.23/ 28.95/ 28.40/
+ syntax mismatch and reorder 44.28+ 36.43+ 41.14- 29.21/ 28.62/
improvement over baseline +0.52 +0.30 +0.62 +1.43 +0.8
Table 7: Results on Chinese-English MT. The symbols +, -, and / indicate that the system is better than the baseline
at the 85%, 95%, and 99% confidence levels, respectively, as defined in (Koehn, 2004).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the Conference of the North American Chapter
of the Association for Computational Linguistics.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Conference of the European Chapter of the
Association for Computational Linguistics.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2004. What?s in a translation rule. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the International Conference
on Computational Linguistics.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsening
the label set. In Proceedings of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In Proceedings of the Workshop on Computa-
tionally Hard Problems and Joint Inference in Speech
and Language Processing.
Zhongqiang Huang, Martin C?mejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Mitesh M. Khapra, Ananthakrishnan Ramanathan, and
Karthik Visweswariah. 2013. Improving reordering
performance using higher order and structural features.
In Proceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Philipp Koehn, Amittai Axelrod, Alexandra Birch, Chris
Callison-Burch, Miles Osborne, and David Talbot.
2005. Edinburgh system description for the 2005 iwslt
565
speech translation evaluation. In International Work-
shop on Spoken Language Translation.
Philipp Koehn. 2004. Pharaoh: A bean search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Conference of Association
for Machine Translation in the Americas.
Junhui Li, Philip Resnik, and Hal Daume. 2013. Mod-
eling syntactic and semantic structures in hierarchi-
cal phrase-based translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computional Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the An-
nual Meeting on Association for Computational Lin-
guistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter. In Proceedings
of the SIGHAN Workshop on Chinese Language Pro-
cessing.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Proceedings of the Conference
of the North American Chapter of the Association for
Computational Linguistics.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the International Confer-
ence on Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Model-
ing the translation of predicate-argument structure for
smt. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve smt
for subject-object-verb languages. In Proceeding of
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation.
566
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49?59,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Machine Translation of Arabic Dialects
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas,
Richard Schwartz, John Makhoul, Omar F. Zaidan?, Chris Callison-Burch?
Raytheon BBN Technologies, Cambridge MA
?Microsoft Research, Redmond WA
?Johns Hopkins University, Baltimore MD
Abstract
Arabic Dialects present many challenges for
machine translation, not least of which is the
lack of data resources. We use crowdsourc-
ing to cheaply and quickly build Levantine-
English and Egyptian-English parallel cor-
pora, consisting of 1.1M words and 380k
words, respectively. The dialectal sentences
are selected from a large corpus of Arabic web
text, and translated using Amazon?s Mechan-
ical Turk. We use this data to build Dialec-
tal Arabic MT systems, and find that small
amounts of dialectal data have a dramatic im-
pact on translation quality. When translating
Egyptian and Levantine test sets, our Dialec-
tal Arabic MT system performs 6.3 and 7.0
BLEU points higher than a Modern Standard
Arabic MT system trained on a 150M-word
Arabic-English parallel corpus.
1 Introduction
The Arabic language is a well-known example of
diglossia (Ferguson, 1959), where the formal vari-
ety of the language, which is taught in schools and
used in written communication and formal speech
(religion, politics, etc.) differs significantly in its
grammatical properties from the informal varieties
that are acquired natively, which are used mostly for
verbal communication. The spoken varieties of the
Arabic language (which we refer to collectively as
Dialectal Arabic) differ widely among themselves,
depending on the geographic distribution and the
socio-economic conditions of the speakers, and they
diverge from the formal variety known as Mod-
ern Standard Arabic (MSA) (Embarki and Ennaji,
2011). Significant differences in the phonology,
morphology, lexicon and even syntax render some
of these varieties mutually incomprehensible.
The use of Dialectal Arabic has traditionally been
confined to informal personal speech, while writ-
ing has been done almost exclusively using MSA
(or its ancestor Classical Arabic). This situation is
quickly changing, however, with the rapid prolifer-
ation of social media in the Arabic-speaking part
of the world, where much of the communication
is composed in dialect. The focus of the Arabic
NLP research community, which has been mostly on
MSA, is turning towards dealing with informal com-
munication, with the introduction of the DARPA
BOLT program. This new focus presents new chal-
lenges, the most obvious of which is the lack of di-
alectal linguistic resources. Dialectal text, which is
usually user-generated, is also noisy, and the lack
of standardized orthography means that users often
improvise spelling. Dialectal data also includes a
wider range of topics than formal data genres, such
as newswire, due to its informal nature. These chal-
lenges require innovative solutions if NLP applica-
tions are to deal with Dialectal Arabic effectively.
In this paper:
? We describe a process for cheaply and quickly
developing parallel corpora for Levantine-
English and Egyptian-English using Amazon?s
Mechanical Turk crowdsourcing service (?3).
? We use the data to perform a variety of machine
translation experiments showing the impact of
morphological analysis, the limited value of
adding MSA parallel data, the usefulness of
cross-dialect training, and the effects of trans-
lating from dialect to MSA to English (?4).
We find that collecting dialect translations has a low
cost ($0.03/word) and that relatively small amounts
of data has a dramatic impact on translation quality.
When trained on 1.5M words of dialectal data, our
system performs 6.3 to 7.0 BLEU points higher than
when it is trained on 100 times more MSA data from
a mismatching domain.
49
2 Previous Work
Existing work on natural language processing of Di-
alectal Arabic text, including machine translation, is
somewhat limited. Previous research on Dialectal
Arabic MT has focused on normalizing dialectal in-
put words into MSA equivalents before translating
to English, and they deal with inputs that contain
a limited fraction of dialectal words. Sawaf (2010)
normalized the dialectal words in a hybrid (rule-
based and statistical) MT system, by performing a
combination of character- and morpheme-level map-
pings. They then translated the normalized source
to English using a hybrid MT or alternatively a
Statistical MT system. They tested their method
on proprietary test sets, observing about 1 BLEU
point (Papineni et al, 2002) increase on broadcast
news/conversation and about 2 points on web text.
Salloum and Habash (2011) reduced the proportion
of dialectal out-of-vocabulary (OOV) words also by
mapping their affixed morphemes to MSA equiva-
lents (but did not perform lexical mapping on the
word stems). They allowed for multiple morpho-
logical analyses, passing them on to the MT system
in the form of a lattice. They tested on a subset of
broadcast news and broadcast conversation data sets
consisting of sentences that contain at least one re-
gion marked as non-MSA, with an initial OOV rate
against an MSA training corpus of 1.51%. They
obtained a 0.62 BLEU point gain. Abo Bakr et
al. (2008) suggested another hybrid system to map
Egyptian Arabic to MSA, using morphological anal-
ysis on the input and an Egyptian-MSA lexicon.
Other work that has focused on tasks besides MT
includes that of Chiang et al (2006), who built a
parser for spoken Levantine Arabic (LA) transcripts
using an MSA treebank. They used an LA-MSA
lexicon in addition to morphological and syntac-
tic rules to map the LA sentences to MSA. Riesa
and Yarowsky (2006) built a statistical morphologi-
cal segmenter for Iraqi and Levantine speech tran-
scripts, and showed that they outperformed rule-
based segmentation with small amounts of training.
Some tools exist for preprocessing and tokenizing
Arabic text with a focus on Dialectal Arabic. For ex-
ample, MAGEAD (Habash and Rambow, 2006) is a
morphological analyzer and generator that can ana-
lyze the surface form of MSA and dialect words into
their root/pattern and affixed morphemes, or gener-
ate the surface form in the opposite direction.
Amazon?s Mechanical Turk (MTurk) is becom-
ing an essential tool for creating annotated resources
for computational linguistics. Callison-Burch and
Dredze (2010) provide an overview of various tasks
for which MTurk has been used, and offer a set of
best practices for ensuring high-quality data.
Zaidan and Callison-Burch (2011a) studied the
quality of crowdsourced translations, by quantifying
the quality of non-professional English translations
of 2,000 Urdu sentences that were originally trans-
lated by the LDC. They demonstrated a variety of
mechanisms that increase the translation quality of
crowdsourced translations to near professional lev-
els, with a total cost that is less than one tenth the
cost of professional translation.
Zaidan and Callison-Burch (2011b) created the
Arabic Online Commentary (AOC) dataset, a 52M-
word monolingual dataset rich in dialectal content.
Over 100k sentences from the AOC were annotated
by native Arabic speakers on MTurk to identify the
dialect level (and dialect itself) in each, and the col-
lected labels were used to train automatic dialect
identification systems. Although a large number
of dialectal sentences were identified (41% of sen-
tences), none were passed on to a translation phase.
3 Data Collection and Annotation
Following Zaidan and Callison-Burch (2011a,b), we
use MTurk to identify Dialectal Arabic data and to
create a parallel corpus by hiring non-professional
translators to translate the sentences that were la-
beled as being dialectal. We had Turkers perform
three steps for us: dialect classification, sentence
segmentation, and translation.
Since Dialectal Arabic is much less common in
written form than in spoken form, the first challenge
is to simply find instances of written Dialectal Ara-
bic. We draw from a large corpus of monolingual
Arabic text (approximately 350M words) that was
harvested from the web by the LDC, largely from
weblog and online user groups.1 Before present-
ing our data to annotators, we filter it to identify
1Corpora: LDC2006E32, LDC2006E77, LDC2006E90,
LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41,
LDC2008E54, LDC2009E14, LDC2009E93.
50
M
ag
hr
eb
i
E
gy
Ir
aq
i
G
ul
f
Ot
he
r
L
ev
Figure 1: One possible breakdown of spoken Arabic into
dialect groups: Maghrebi, Egyptian, Levantine, Gulf and
Iraqi. Habash (2010) gives a breakdown along mostly
the same lines. We used this map as an illustration for
annotators in our dialect classification task (Section 3.1),
with Arabic names for the dialects instead of English.
segments most likely to be dialectal (unlike Zaidan
and Callison-Burch (2011b), who did no such pre-
filtering). We eliminate documents with a large per-
centage of non-Arabic or MSA words. We then
retain documents that contain some number of di-
alectal words, using a set of manually selected di-
alectal words that was assembled by culling through
the transcripts of the Levantine Fisher and Egyp-
tian CallHome speech corpora. After filtering, the
dataset contained around 4M words, which we used
as a starting point for creating our Dialectal Arabic-
English parallel corpus.
3.1 Dialect Classification
To refine the document set beyond our keyword fil-
tering heuristic and to label which dialect each doc-
ument is written in, we hire Arabic annotators on
MTurk to perform classification similar to Zaidan
and Callison-Burch (2011b). Annotators were asked
to classify the filtered documents for being in MSA
or in one of four regional dialects: Egyptian, Lev-
antine, Gulf/Iraqi or Maghrebi, and were shown the
map in Figure 1 to explain what regions each of the
dialect labels corresponded to. We allowed an addi-
tional ?General? dialect option for ambiguous docu-
ments. Unlike Zaidan and Callison-Burch, our clas-
sification was applied to whole documents (corre-
sponding to a user online posting) instead of individ-
ual sentences. To perform quality control, we used
a set of documents for which correct labels were
known. We presented these 20% of the time, and
Dialect Classification HIT $10,064
Sentence Segmentation HIT $1,940
Translation HIT $32,061
Total cost $44,065
Num words translated 1,516,856
Cost per word 2.9 cents/word
Table 1: The total costs for the three MTurk subtasks in-
volved with the creation of our Dialectal Arabic-English
parallel corpus.
eliminated workers who did not correctly classify
them (2% of labels).
Identifying the dialect of a text snippet can be
challenging in the absence of phonetic cues. We
therefore required 3 classifications from different
workers for every document, and accepted a dialect
label if at least two of them agreed. The dialect dis-
tribution of the final output was: 43% Gulf/Iraqi,
28% Levantine, 11% Egyptian, and 16% could not
be classified. MSA and the other labels accounted
for 2%. We decided to translate only the Levantine
and Egyptian documents, since the pool of MTurk
workers contained virtually no workers from Iraq or
the Gulf region.
3.2 Sentence Segmentation
Since the data we annotated was mostly user-
generated informal web content, the existing punc-
tuation was often insufficient to determine sentence
boundaries. Since sentence boundaries are impor-
tant for correct translation, we segmented passages
into individual sentences using MTurk. We only re-
quired sentences longer than 15 words to be seg-
mented, and allowed Turkers to split and rejoin at
any point between the tokens. The instructions were
simply to ?divide the Arabic text into individual sen-
tences, where you believe it would be appropriate
to insert a period.? We also used a set of correctly
segmented passages for quality control, and scored
Turkers using a metric based on the precision and
recall of correct segmentation points. The rejection
rate was 1.2%.
3.3 Translation to English
Following Zaidan and Callison-Burch (2011a), we
hired non-professional translators on MTurk to
translate the Levantine and Egyptian sentences into
51
Sentence Arabic English
Data Set Pairs Tokens Tokens
MSA-150MW 8.0M 151.4M 204.4M
Dialect-1500KW 180k 1,545,053 2,257,041
MSA-1300KW 71k 1,292,384 1,752,724
MSA-Web-Tune 6,163 145,260 184,185
MSA-Web-Test 5,454 136,396 172,357
Lev-Web-Tune 2,600 20,940 27,399
Lev-Web-Test 2,600 21,092 27,793
Egy-Web-Test 2,600 23,671 33,565
E-Facebook-Tune 3,351 25,130 34,753
E-Facebook-Test 3,188 25,011 34,244
Table 2: Statistics about the training/tuning/test datasets
used in our experiments. The token counts are calculated
before MADA segmentation.
English. Among several quality control measures,
we rendered the Arabic sentences as images to pre-
vent Turkers from simply copying the Arabic text
into translation software. We still spot checked the
translations against the output of Google Translate
and Bing Translator. We also rejected gobbledygook
garbage translations that have a high percentage of
words not found in an English lexicon.
We quantified the quality of an individual Turker?s
translations in two ways: first by asking native Ara-
bic speaker judges to score a sample of the Turker?s
translations, and second by inserting control sen-
tences for which we have good reference translations
and measuring the Turker?s METEOR (Banerjee and
Lavie, 2005) and BLEU-1 scores (Papineni et al,
2002).2 The rejection rate of translation assignments
was 5%. We promoted good translators to a re-
stricted access ?preferred worker queue?. They were
paid at a higher rate, and were required to translate
control passages only 10% of the time as opposed
to 20% for general Turkers, thus providing us with a
higher translation yield for unseen data.
Worker turnout was initially slow, but increased
quickly as our reputation for being reliable payers
was established; workers started translating larger
volumes and referring their acquaintances. We had
121 workers who each completed 20 or more trans-
lation assignments. We eventually reached and sus-
tained a rate of 200k words of acceptable quality
2BLEU-1 provided a more reliable correlation with human
judgment in this case that the regular BLEU score (which uses
n-gram orders 1, . . . , 4), given the limited size of the sample
measured.
translated per week. Unlike Zaidan and Callison-
Burch (2011a), who only translated 2,000 Urdu sen-
tences, we translated sufficient volumes of Dialectal
Arabic to train machine translation systems. In total,
we had 1.1M words of Levantine and 380k words of
Egyptian translated into English, corresponding to
about 2.3M words on the English side.
Table 1 outlines the costs involved with creating
our parallel corpus. The total cost was $44k, or
$0.03/word ? an order of magnitude cheaper than
professional translation.
4 Experiments in Dialectal Arabic-English
Machine Translation
We performed a set of experiments to contrast sys-
tems trained using our dialectal parallel corpus with
systems trained on a (much larger) MSA-English
parallel corpus. All experiments use the same meth-
ods for training, decoding and parameter tuning, and
we only varied the corpora used for training, tun-
ing and testing. The MT system we used is based
on a phrase-based hierarchical model similar to that
of Shen et al (2008). We used GIZA++ (Och and
Ney, 2003) to align sentences and extract hierar-
chical rules. The decoder used a log-linear model
that combines the scores of multiple feature scores,
including translation probabilities, smoothed lexi-
cal probabilities, a dependency tree language model,
in addition to a trigram English language model.
Additionally, we used 50,000 sparse, binary-valued
source and target features based on Chiang et al
(2009). The English language model was trained on
7 billion words from the Gigaword and from a web
crawl. The feature weights were tuned to maximize
the BLEU score on a tuning set using the Expected-
BLEU optimization procedure (Devlin, 2009).
The Dialectal Arabic side of our corpus consisted
of 1.5M words (1.1M Levantine and 380k Egyp-
tian). Table 2 gives statistics about the various
train/tune/test splits we used in our experiments.
Since the Egyptian set was so small, we split it only
to training/test sets, opting not to have a tuning set.
The MSA training data we used consisted of Arabic-
English corpora totaling 150M tokens (Arabic side).
The MSA train/tune/test sets were constructed for
the DARPA GALE program.
We report translation quality in terms of BLEU
52
Simple Segment MADA Segment
Training Tuning BLEU OOV BLEU OOV ?BLEU ?OOV
MSA-Web-Test
MSA-150MW MSA-Web 26.21 1.69% 27.85 0.48% +1.64 -1.21%
MSA-1300KW 21.24 7.20% 25.23 1.95% +3.99 -5.25%
Egyptian-Web-Test
Dialect-1500KW Levantine-Web 18.55 6.31% 20.66 2.85% +2.11 -3.46%
Levantine-Web-Test
Dialect-1500KW Levantine-Web 17.00 6.22% 19.29 2.96% +2.29 -3.26%
Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal
Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are
more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora.
Training Tuning BLEU OOV BLEU OOV BLEU OOV
Egyptian-Web-Test Levantine-Web-Test MSA-Web-Test
MSA-150MW MSA-Web 14.76 4.42% 11.83 5.53% 27.85 0.48%
MSA-150MW Lev-Web 14.34 4.42% 12.29 5.53% 24.63 0.48%
MSA-150MW+Dial-1500KW 20.09 2.04% 19.11 2.27% 24.30 0.45%
Dialect-1500KW 20.66 2.85% 19.29 2.96% 15.53 3.70%
Egyptian-360KW 19.04 4.62% 11.21 9.00% - -
Levantine-360KW 14.05 7.11% 16.36 5.24% - -
Levantine-1100KW 17.79 4.83% 19.29 3.31% - -
Table 4: A comparison of translation quality of Egyptian, Levantine, andMSAweb text, using various training corpora.
The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian),
since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the
dialectal data results in marginally worse translations.
score.3 In addition, we also report the OOV rate of
the test set relative to the training corpus in each ex-
perimental setups.
4.1 Morphological Decomposition
Arabic has a complex morphology compared to En-
glish. Preprocessing the Arabic source by morpho-
logical segmentation has been shown to improve the
performance of Arabic MT (Lee, 2004; Habash and
Sadat, 2006) by decreasing the size of the source vo-
cabulary, and improving the quality of word align-
ments. The morphological analyzers that underlie
most segmenters were developed for MSA, but the
different dialects of Arabic share many of the mor-
phological affixes of MSA, and it is therefore not
unreasonable to expect MSA segmentation to also
improve Dialect Arabic to English MT. To test this,
3We also computed TER (Snover et al, 2006) andMETEOR
scores, but omit them because they demonstrated similar trends.
we ran experiments using the MADA morpholog-
ical analyzer (Habash and Rambow, 2005). Table
3 shows the effect of applying segmentation to the
text, for both MSA and Dialectal Arabic. The BLEU
score improves uniformly, although the improve-
ments are most dramatic for smaller datasets, which
is consistent with previous work (Habash and Sadat,
2006). Morphological segmentation gives a smaller
gain on dialectal input, which could be due to two
factors: the segmentation accuracy likely decreases
since we are using an unmodified MSA segmenter,
and there is higher variability in the written form of
dialect compared to MSA. Given the significant, al-
beit smaller gain on dialectal input, we use MADA
segmentation in all our experiments.
4.2 Effect of Dialectal Training Data Size
We next examine how the size of the dialectal train-
ing data affects MT performance, and whether it is
useful to combine it with MSA training data. We
53
oh
 ti
me
 (s
pa
ce
 om
itt
ed
). 
Ap
pe
are
d w
ith
in
 a 
po
em
.
11
yA
zm
n

?
lik
e y
ou
 (c
or
ru
pti
on
 of
 M
SA
 m
vl
k)
.
10
m
tlk
"#
$
by
 m
uc
h (
co
rru
pti
on
 of
 M
SA
 bk
vy
r).
11
bk
ty
r
&'$
()
I m
iss
 yo
u (
sp
ok
en
 to
 a 
fe
ma
le)
 ?
Eg
yp
tia
n.
14
w
H
$t
yn
y
/0
'$1
2?
Th
e l
as
t n
am
e (
Al
-N
a'o
om
) o
f a
 fo
ru
m 
ad
mi
n.
16
A
ln
E
w
m
?:;
0<?
a l
oo
ot 
(c
or
ru
pti
on
 of
 M
SA
 kv
yr
A
).
17
kt
yy
yr
&''
'$?
rea
lly
/fo
r r
ea
l ?
Le
va
nti
ne
.
31
E
nj
d
DE
0F
En
gli
sh
 E
qu
iva
len
t
Co
un
t
TL
Ar
ab
ic
Table 5: The most frequent OOV?s (with counts ? 10) of the dialectal test sets against the MSA training data.
Source (EGY):  ? ? ??	
?   ? ! !
Transliteration: Ant btEml lh AElAn wlA Ayh?!!
MSA-Sys. Output: You are working for a declarationand not?
Dial-Sys. Output: You are making the advertisementfor him or what?
Reference: Are you promoting it or what?!!
Source (EGY):  01?. ??78 6 35 34? ?
 9:;? <=>
Transliteration: nfsY Atm}n Elyh bEd mA $Af
AlSwrh dy
MSA-Sys. Output: Myself feel to see this image.
Dial-Sys. Output: I wish to check on him afterhe saw this picture.
Reference: I wish to be sure that he is fineafter he saw this images
Source (LEV):  ?0??? E7770 ?F? G7H
Transliteration: lhyk Aljw ktyyyr kwwwl
MSA-Sys. Output: God you the atmosphere.
Dial-Sys. Output: this is why the weather is so cool
Reference: This is why the weather is so cool
Source (LEV):  ?L M
 G3 0?;
Transliteration: Twl bAlk Em nmzH
MSA-Sys. Output: Do you think about a joke long.
Dial-Sys. Output: Calm down we are kidding
Reference: calm down, we are kidding
Figure 2: Examples of improvement in MT output when
training on our Dialectal Arabic-English parallel corpus
instead of an MSA-English parallel corpus.
Source (EGY):   	
 	  ? 
Transliteration: qAltlp Tb tEAlY nEd ,
MSA-Sys. Output: Medicine almighty promise.
Dial-Sys. Output: She said, OK, come and then
Reference: She told him, OK, lets count them ,
Source (LEV):  "#$%& 
#'01 ?-%. ! -,%+? ?? ?2 
Transliteration: fbqrA w>HyAnA bqDyhA Em
>tslY mE rfqAty
MSA-Sys. Output: I read and sometimes with gowith my uncle.
Dial-Sys. Output: So I read, and sometimes I spendtrying to make my self comfortwith my friends
Reference: So i study and sometimes I spendthe time having fun with my friends
Source (LEV):  ?@ ?< ??' => +? &#:9? B:C12D E?
?? %$?+G 
Transliteration: Allh ysAmHkn hlq kl wAHd TAlb
qrb bykwn bdw Erws
MSA-Sys. Output: God now each student near the
Bedouin bride.
Dial-Sys. Output: God forgive you, each one is aclose student would want the bride
Reference: God forgive you. Is every oneasking to be close, want a bride!
Figure 3: Examples of ambiguous words that are trans-
lated incorrectly by the MSA-English system, but cor-
rectly by the Dialectal Arabic-English system.
54
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
%
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Egyptian web test
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Levantine web test
Figure 4: Learning curves showing the effects of increas-
ing the size of dialectal training data, when combined
with the 150M-word MSA parallel corpus, and when
used alone. Adding the MSA training data is only use-
ful when the dialectal data is scarce (200k words).
started with a baseline system trained on the 150M-
word MSA parallel corpus, and added various sized
portions of the dialect parallel corpus to it. Figure 4
shows the resulting learning curve, and compares it
to the learning curve for a system trained solely on
the dialectal parallel corpus. When only 200k words
of dialectal data are available, combining it with the
150M-word MSA corpus results in improved BLEU
scores, adding 0.8?1.5 BLEU points. When 400k
words or more of dialectal data are available, the
MSA training data ceases to provide any gain, and
in fact starts to hurt the performance.
The performance of a system trained on the 1.5M-
word dialectal data is dramatically superior to a sys-
tem that uses only the 150M-word MSA data: +6.32
BLEU points on the Egyptian test set, or 44% rela-
tive gain, and +7.00 BLEU points on the Levantine
test set, or 57% relative gain (fourth line vs. second
line of Table 4). In Section 4.4, we show that those
gains are not an artifact of the similarity between test
and training datasets, or of using the same translator
pool to translate both sets.
Inspecting the difference in the outputs of the Di-
alectal vs. MSA systems, we see that the improve-
ment in score is a reflection of a significant improve-
ment in the quality of translations. Figure 2 shows
a few examples of sentences whose translations im-
prove significantly using the Dialectal system. Fig-
ure 3 shows a particularly interesting category of ex-
amples. Many words are homographs, with different
meanings (and usually different pronunciations) in
MSA vs. one or more dialects. The bolded tokens
in the sentences in Figure 3 are examples of such
words. They are translated incorrectly by the MSA
system, while the dialect system translates them cor-
rectly.4 If we examine the most frequent OOVwords
against the MSA training data (Table 5), we find a
number of corrupted MSA words and names, but
that a majority of OOVs are dialect words.
4.3 Cross-Dialect Training
Since MSA training data appeared to have little ef-
fect when translating dialectal input, we next inves-
tigated the effect of training data from one dialect on
translating the input of another dialect. We trained a
system with the 360k-word Egyptian training subset
of our dialectal parallel corpus, and another system
with a similar amount of Levantine training data. We
used each system to translate the test set of the other
dialect. As expected, a system performs better when
it translates a test set in the same dialect that it was
trained on (Table 4).
That said, since the Egyptian training set is so
small, adding the (full) Levantine training data im-
proves performance (on the Egyptian test set) by
1.62 BLEU points, compared to using only Egyp-
tian training data. In fact, using the Levantine
training data by itself outperforms the MSA-trained
system on the Egyptian test set by more than 3
BLEU points. (For the Levantine test set, adding
the Egyptian training data has no affect, possibly
due to the small amount of Egyptian data.) This
may suggest that the mismatch between dialects is
less severe than the mismatch between MSA and
dialects. Alternatively, the differences may be due
to the changes in genre from the MSA parallel cor-
pus (which is mainly formal newswire) to the news-
groups and weblogs that mainly comprise the dialec-
tal corpus.
4The word nfsY of Figure 2 (first word of second example)
is also a homograph, as it means myself in MSA and I wish in
Dialectal Arabic.
55
Training Tuning BLEU OOV
MSA-150MW Levantine-Web 13.80 4.16%
MSA-150MW+Dialect-1500KW 16.71 2.43%
Dialect-1500KW 15.75 3.79%
MSA-150MW Egyptian-Facebook 15.80 4.16%
MSA-150MW+Dialect-1500KW 18.50 2.43%
Dialect-1500KW 17.90 3.79%
Dialect-1000KW (random selection) Egyptian-Facebook 17.09 4.64%
Dialect-1000KW (no Turker overlap) 17.10 4.60%
Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are
entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:
+2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
4.4 Validation on Independent Test Data
To eliminate the possibility that the gains are solely
due to similarity between the test/training sets in the
dialectal data, we ran experiments using the same
dialectal training data, but using truly independent
test/tuning data sets selected at random from a larger
set of monolingual data that we collected from pub-
lic Egyptian Facebook pages. This data consists of
a set of original user postings and the subsequent
comments on each, giving the data a more conversa-
tional style than our other test sets. The postings
deal with current Egyptian political affairs, sports
and other topics. The test set we selected consisted
of 25,011 words (3,188 comments and 427 postings
from 86 pages), and the tuning set contained 25,130
words (3,351 comments and 415 conversations from
58 pages). We obtained reference translations for
those using MTurk as well.
Table 6 shows that using the 1.5M-word dialect
parallel corpus for training yields a 2 point BLEU
improvement over using the 150M-word MSA cor-
pus. Adding the MSA training data does yield an
improvement, though of less than a single BLEU
point. It remains true that training on 1.5M words
of dialectal data is better than training on 100 times
more MSA parallel data. The system performance
is sensitive to the tuning set choice, and improves
when it matches the test set in genre and origin.
To eliminate another potential source of artificial
bias, we also performed an experiment where we
removed any training translation contributed by a
Turker who translated any sentence in the Egyptian
Facebook set, to eliminate translator bias. For this,
we were left with 1M words of dialect training data.
This gave the same BLEU score as when training
with a randomly selected subset of the same size
(bottom part of Table 6).
4.5 Mapping from Dialectal Arabic to MSA
Before Translating to English
Given the large amount of linguistic resources that
have been developed for MSA over the past years,
and the extensive research that was conducted on
machine translation from MSA to English and other
languages, an obvious research question is whether
Dialectal Arabic is best translated to English by first
pivoting through MSA, rather than directly. The
proximity of Dialectal Arabic to MSA makes the
mapping in principle easier than general machine
translation, and a number of researchers have ex-
plored this direction (Salloum and Habash, 2011).
In this scenario, the dialectal source would first be
automatically transformed to MSA, using either a
rule-based or statistical mapping module.
The Dialectal Arabic-English parallel corpus we
created presents a unique opportunity to compare
the MSA-pivoting approach against direct transla-
tion. First, we collected equivalent MSA data for
the Levantine Web test and tuning sets, by asking
Turkers to transform dialectal passages to valid and
fluent MSA. Turkers were shown example transfor-
mations, and we encouraged fewer changes where
applicable (e.g. morphological rather than lexical
mapping), but allowed any editing operation in gen-
eral (deletion, substitution, reordering). Sample sub-
missions were independently shown to native Ara-
bic speaking judges, who confirmed they were valid
MSA. A lowOOV rate also indicated the correctness
of the mappings. By manually transforming the test
56
Training BLEU OOV BLEU OOV ?BLEU ?OOV
Direct dialect trans Map to MSA then trans
MSA-150MW 12.29 5.53% 14.59 1.53% +2.30 -4.00%
MSA-150MW+Dialect-200KW 15.37 3.59% 15.53 1.22% +0.16 -2.37%
MSA-150MW+Dialect-400KW 16.62 3.06% 16.25 1.13% -0.37 -1.93%
MSA-150MW+Dialect-800KW 17.83 2.63% 16.69 1.04% -1.14 -1.59%
MSA-150MW+Dialect-1500KW 19.11 2.27% 17.20 0.98% -1.91 -1.29%
Table 7: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English,
versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it
is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system,
the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.
dialectal sentence into MSA, we establish an opti-
mistic estimate of what could be done automatically.
Table 7 compares direct translation versus piv-
oting to MSA before translating, using the base-
line MSA-English MT system.5 The performance
of the system improves by 2.3 BLEU points with
dialect-to-MSA pivoting, compared to attempting to
translate the untransformed dialectal input directly.
As we add more dialectal training data, the BLEU
score when translating the untransformed dialect
test set improves rapidly (as seen previously in the
MSA+Dialect learning curve in Figure 4), while the
improvement is less rapid when the text is first trans-
formed to MSA. Direct translation becomes a better
option than mapping to MSA once 400k words of di-
alectal data are added, despite the significantly lower
OOV rate with MSA-mapping. This indicates that
simple vocabulary coverage is not sufficient, and
data domain mismatch, quantified by more complex
matching patterns, is more important.
5 Conclusion
We have described a process for building a Dialec-
tal Arabic-English parallel corpus, by selecting pas-
sages with a relatively high percentage of non-MSA
words from a monolingual Arabic web text corpus,
then using crowdsourcing to classify them by di-
alect, segment them into individual sentences and
translate them to English. The process was success-
fully scaled to the point of reaching and sustaining a
rate of 200k translated words per week, at 1/10 the
cost of professional translation. Our parallel corpus,
consisting of 1.5M words, was produced at a total
5The systems in each column of the table are tuned consis-
tently, using their corresponding tuning sets.
cost of $40k, or roughly $0.03/word.
We used the parallel corpus we constructed to
analyze the behavior of a Dialectal Arabic-English
MT system as a function of the size of the dialec-
tal training corpus. We showed that relatively small
amounts of training data render larger MSA corpora
from different data genres largely ineffective for this
test data. In practice, a system trained on the com-
bined Dialectal-MSA data is likely to give the best
performance, since informal Arabic data is usually
a mixture of Dialectal Arabic and MSA. An area of
future research is using the output of a dialect clas-
sifier, or other features to bias the translation model
towards the Dialectal or the MSA parts of the data.
We also validated the models built from the di-
alectal corpus by using them to translate an inde-
pendent data set collected from Egyptian Facebook
public pages. We finally investigated using MSA
as a ?pivot language? for Dialectal Arabic-English
translation, by simulating automatic dialect-to-MSA
mapping using MTurk. We obtained limited gains
from mapping the input to MSA, even when the
mapping is of good quality, and only at lower train-
ing set sizes. This suggests that the mismatch be-
tween training and test data is an important aspect of
the problem, beyond simple vocabulary coverage.
The aim of this paper is to contribute to setting
the direction of future research on Dialectal Arabic
MT. The gains we observed from using MSA mor-
phological segmentation can be further increased
with dialect-specific segmenters. Input preprocess-
ing can also be used to decrease the noise of the
user-generated data. Topic adaptation is another im-
portant problem to tackle if the large MSA linguistic
resources already developed are to be leveraged for
Dialectal Arabic-English MT.
57
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program, and in part by the EuroMatrixPlus project
funded by the European Commission (7th Frame-
work Programme). The views expressed are those
of the authors and do not reflect the official policy
or position of the Department of Defense or the U.S.
Government. Distribution Statement A (Approved
for Public Release, Distribution Unlimited).
References
Hitham M. Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for converting writ-
ten Egyptian colloquial dialect into diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008, Cairo, Egypt.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgments. In In Proc. of ACL
2005 Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, Ann Arbor,
Michigan.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Trento, Italy.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ?09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land, December.
Mohamed Embarki and Moha Ennaji, editors. 2011.
Modern Trends in Arabic Dialectology. The Red Sea
Press.
Charles A. Ferguson. 1959. Diglossia. Word, 15:325?
340.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Ann Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD: A
morphological analyzer and generator for the Arabic
dialects. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics (ACL),
Sydney, Australia.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the 2006 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics, New York,
New York.
Nizar Y. Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In HLT-NAACL ?04:
Proceedings of HLT-NAACL 2004, Boston, Mas-
sachusetts.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with applica-
tions to machine translation. In Proceedings of the 7th
Conf. of the Association for Machine Translation in the
Americas (AMTA 2006), Cambridge, MA.
Wael Salloum and Nizar Habash. 2011. Dialectal to stan-
dard Arabic paraphrasing to improve Arabic-English
statistical machine translation. In Proceedings of the
2011 Conference of Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Conf. of
the Association for Machine Translation in the Ameri-
cas (AMTA 2010), Denver, Colorado.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and Ralph Weischedel. 2006. A study of
translation error rate with targeted human annotation.
In Proceedings of the 7th Conf. of the Association for
Machine Translation in the Americas (AMTA 2006),
pages 223?231, Cambridge, MA.
58
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37?41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, June.
59
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 528?532,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Trait-Based Hypothesis Selection For Machine Translation
Jacob Devlin and Spyros Matsoukas
Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA
{jdevlin,smatsouk}@bbn.com
Abstract
In the area of machine translation (MT) sys-
tem combination, previous work on generat-
ing input hypotheses has focused on varying a
core aspect of the MT system, such as the de-
coding algorithm or alignment algorithm. In
this paper, we propose a new method for gen-
erating diverse hypotheses from a single MT
system using traits. These traits are simple
properties of the MT output such as ?aver-
age output length? and ?average rule length.?
Our method is designed to select hypotheses
which vary in trait value but do not signif-
icantly degrade in BLEU score. These hy-
potheses can be combined using standard sys-
tem combination techniques to produce a 1.2-
1.5 BLEU gain on the Arabic-English NIST
MT06/MT08 translation task.
1 Introduction
In Machine Translation (MT), the output from mul-
tiple decoding systems can be used to create a new
output which is better than any single input system,
using a procedure known as system combination.
Normally, the input systems are generated by
varying some important aspect of the MT system,
such as the alignment algorithm (Xu and Rosti,
This work was supported by DARPA/I2O Contract No.
HR0011-12-C-0014 under the BOLT program (Approved for
Public Release, Distribution Unlimited). The views, opinions,
and/or findings contained in this article are those of the au-
thor and should not be interpreted as representing the official
views or policies, either expressed or implied, of the Defense
Advanced Research Projects Agency or the Department of De-
fense.
2010) or tokenization algorithm (de Gispert et al,
2009). Unfortunately, creating novel algorithms to
perform some important aspect of MT decoding is
obviously quite challenging. Thus, it is difficult to
increase the number of input systems in a meaning-
ful way.
In this paper, we show it is possible to create
diverse input hypotheses for combination without
making any algorithmic changes. Instead, we use
traits, which are very simple attributes of the MT
output, such as ?output length? and ?average rule
length.? Our basic procedure is to intelligently se-
lect hypotheses from our decoding forest which vary
in trait value, but have minimal BLEU degradation
compared to our baseline. We then combine these
to produce a substantial gain. Note that all of the
hypotheses are generated from a single decode of a
single input system.
Additionally, our method is completely compati-
ble with multi-system combination, since our proce-
dure can be applied to each input system, and then
these systems can be combined as normal.
Methods for automatically creating diverse hy-
potheses from a single system have been explored
in speech recognition (Siohan et al, 2005), but we
know of no analogous work applied to machine
translation. Our procedure does share some surface
similarities with techniques such as variational de-
coding (VD) (Li et al, 2009), but the goal in those
techniques is to find output which is consistent with
the entire forest, rather than to select hypotheses
with particular attributes. In fact, VD can be applied
in conjunction by running VD on the rescored forest
528
for each trait condition.1
2 Description of MT System
Our machine translation system is a string-to-
dependency hierarchical decoder based on (Shen et
al., 2008) and (Chiang, 2007). Bottom-up chart
parsing is performed to produce a shared forest of
derivations. The decoder uses a log-linear transla-
tion model, so the score of derivation d is defined
as:
Sd(~w) =
m?
i=1
wi
?
r?R(d)
Fri (1)
where R(d) is the set of translation rules that make
up derivation d, m is the number of features, Fri
is the score of the ith feature in rule r, and wi is
the weight of feature i. This weight vector is opti-
mized discriminatively to maximize BLEU score on
a tuning set, using the Expected-BLEU optimization
procedure (Rosti et al, 2010).
Our decoder uses all of the standard statistical MT
features, such as the language model, rule probabil-
ities, and lexical probabilities. Additionally, we use
50,000 sparse, binary-valued features such as ?Is the
bi-gram ?united states? present in the output??, based
on (Chiang et al, 2009). We use a 3-gram LM for
decoding and a 5-gram LM for rescoring.
3 Trait Features
An MT trait represents a high-level property of the
MT output.
The traits used in this paper are:
? Null Source Words ? The percentage of source
content words which align to null, i.e., are not
translated.
? Source Reorder ? The percentage of source
terminals/non-terminals which cross alignment
links inside their decoding rule.
? Ngram Frequency ? The percentage of target 3-
grams which are seen more than 10 times in the
monolingual training.
? Rule Frequency ? The percentage of rules
which are seen more than 3 times in the par-
allel training.
1We do not use VD here because we have not found it to be
beneficial to our system.
? Rule Length ? The average number of target
words per rule.
? Output Length ? The ratio of the number of tar-
get words in the MT output divided by the num-
ber of source words in the input.
? High Lex Prob ? The percentage of source
words which have a lexical translation proba-
bility greater than 0.1.
Each trait can be represented as the ratio of two lin-
ear decoding features. For example, for the Output
Length trait, the ?numerator? feature is the number
of target words in the hypothesis, while the ?denom-
inator? feature is the number of source words in the
input sentence. We can sum these feature scores
over a test set, and the resulting quotient is the Out-
put Length for that set.
Intuitively, each trait is associated with a par-
ticular tradeoff, such as fluency/adequacy or preci-
sion/recall. For example, when MT performance is
maximized, shorter output tends to have higher pre-
cision but lower recall than longer output. For the
Ngram Frequency trait, a greater percentage of high-
frequency n-grams tends to result in more fluent but
less adequate output. Similar intuitive justifications
should be evident for the remaining traits.
4 Hypothesis Generation
The main goal of this work is to generate additional
hypotheses which vary in trait values, while mini-
mizing degradation to the BLEU score. So, imagine
that we have some baseline MT output. Then, we
want to generate a second set of hypotheses which
have maximal BLEU score, subject to the constraint
that the output must be 5% shorter.2
The question then becomes how to figure out
which 5% of words should be removed. Rather than
attempting to do this with a new algorithm, we sim-
ply let our existing MT models do it for us, using
our standard optimization procedure. This is the es-
sential purpose of the trait features ? using the Out-
put Length feature, the optimizer has a ?knob? with
which it can control the trait value independently
of everything else.3 Thus, the new hypotheses that
2Note that the trait value is always aggregated over the entire
set, and not computed sentence-by-sentence.
3A feature representing the number of words already exists
in our baseline system, but no such feature exists for the other 6
529
we select are ?optimal? in terms of our existing MT
model probabilities, but have trait values which vary
from the baseline in a precise way.
4.1 Optimization Function
Our normal optimization procedure uses n-
best-based Expected-BLEU tuning (Rosti et al,
2010), which is a differentiable approximation of
Maximum-BLEU tuning. To ?target? a particular
trait value, we add a second term equal to the
squared error between the current trait value and
the target trait value. Our modified optimization
function which we seek to maximize is then:
Obj(~w) = ExpBLEU(~w)? ?
(
N(~w)
D(~w)
? ??
)2
where ~w is the MT feature weight vector, ? is the
weight of the trait term, ? is the baseline value of
the trait, and ? is the ?target? trait multiplier, N(~w)
is the expected-value of the numerator feature, and
D(~w) is the expected value of the denominator fea-
ture.
To give an example, imagine that for our baseline
tune set the Output Length ratio is 1.2, and we want
to create a hypothesis set with 5% fewer words. In
that case, we would set ? = 1.2 and ? = 0.95, so the
target trait value is 1.14. We fix the free parameter
? to 10, which forces the optimized trait value to be
very close to the target.4
The trait-value functions N(~w) and D(~w) are
computed as standard expected value functions, e.g.:
N(~w) =
?
i
?
j
pij(~w)Nij
where pij(~w) is the posterior probability of the jth
hypothesis of sentence i, and Nij is the value of the
numerator feature for hypothesis ij.5
4.2 Meta-Optimization
It is somewhat problematic to use a fixed multiplier
? on all of the traits, since on some traits it may
cause a larger degradation than others. So, we take
the reverse approach ? for some targeted BLEU loss
traits.
4Note that theExpBLEU(~w) is raw BLEU not BLEU per-
centage, i.e., it?s 0.4528 not 45.28
5pij(~w) is computed the same way as in ExpBLEU(~w).
See (Rosti et al, 2010) for details.
?, we find the maximum (or minimum) value of ?
which causes a loss no greater than ?, as computed
on a held-out portion of the tune set.6 Here, we find
the maximum and minimum trait value for ? = 0.5
and ? = 2.0, resulting in 4 sets of weights per trait.
We can find the optimal ? for each ? by perform-
ing a binary search on ? , where we run our optimiza-
tion procedure and then compute the BLEU loss at
each iteration.
4.3 Forest-Based Optimization
Since we have 7 traits, and we generate 4 sets of
weights per trait, we have 28 ?systems? to combine.
Obviously, running 28 full decodes on each new test
sentence is highly undesirable.
We resolve this issue by using our baseline deriva-
tion forest for both optimization and hypothesis gen-
eration. We perform a single round of decoding to
generate a forest, and then perform iterative n-best
optimization by rescoring the forest rather than re-
decoding from scratch. 7 We constrain the 50,000
sparse feature weights to be fixed at their baseline
values, to prevent over-fitting.
Once the weight sets are generated, the hypothe-
ses for each trait condition can be generated by
rescoring the forest inside of the decoder. Therefore,
all 28 trait hypotheses can be generated for almost
no cost over a single decode.
It should be noted we have found it beneficial to
relax our MT pruning parameters in order to cre-
ate a larger forest. This results in decoding which
is roughly 2x-3x as slow as the baseline, and re-
quires storing the larger forest in memory. However,
we have found that the procedure still works well
even with the standard pruning parameters. Addi-
tionally, we are investigating methods for diversify-
ing the forest with less of a slowdown to decoding.
5 Combination
Once the different trait hypotheses have been gen-
erated, system combination can be performed using
any method.
Here, we use a confusion network decoder based
on (Rosti et al, 2010). The basic procedure is to
6For example if the held-out baseline BLEU is 40.0 and ? =
0.5, the BLEU after trait optimization can be no less than 39.5.
7Forest-based optimization such as (Pauls et al, 2009) could
be used instead.
530
select one hypothesis as the ?skeleton? and then in-
crementally align the remaining hypotheses to create
a confusion network. The confusion network is de-
coded using an arc-level confidence score for each
input system and a language model, the weights for
which are estimated discriminatively to maximize
BLEU.
6 Results
We present MT results in Table 1. Our experimen-
tal setup is compatible with the NIST MT08 con-
strained track. We trained our translation model on
35 million words of parallel data and our language
model on 3.8 billion words of monolingual data. We
use a portion of MT02-05 for tuning the MT baseline
and the trait systems, and another portion of MT02-
05 for tuning system combination.
We present results on Arabic-English MT06-
newswire and MT08-eval. The systems were tuned
and evaluated using IBM-BLEU. Our baseline sys-
tem is 1.5 BLEU better than the best result from the
NIST M08 evaluation.
For the Trait Feats condition, we simply added the
numerator and denominator features for all 7 traits to
the baseline system and re-optimized.8 Somewhat
surprisingly, this produces an 0.5-0.7 BLEU gain on
its own. In this condition, although we do not target
any particular trait values, the optimizer will natu-
rally fine-tune the trait values to whatever is optimal
for BLEU score. For example, the MT08 baseline
value of Source Reorder is 0.307, while for the Trait
Feats it is 0.330, so the system determined it is ?op-
timal? to have 7.5% (0.330/0.307) more re-ordering
than the baseline.
For the Trait Comb condition, we generated 28
trait hypothesis sets using the decoding forest from
the Trait Feats condition. We combined these with
the Trait Feats output using consensus network de-
coding. This produces an additional 0.8 BLEU gain,
resulting in a 1.2-1.5 BLEU gain over the baseline.
We also present another condition, n-best Comb,
where we perform confusion network combination
on the 28-best hypotheses from Trait Feats. This
represents the simplest and most trivial method of
hypothesis selection. We observe no gain in BLEU
on this condition. Other simple methods of hy-
8Including the 50k sparse features.
potheses selection, such as optimizing systems to be
?different? from one another (i.e., have high inter-
system TER), also produced no gain over the single
system. We include these results simply to demon-
strate that it is not trivial to select hypotheses from a
single system which produce a significant improve-
ment in from system combination.
MT06 nw MT08 eval
BLEU Len BLEU Len
Baseline 55.11 99.1% 46.75 96.1%
Trait Feats 55.79? 99.3% 47.23? 96.0%
+n-best Comb 55.65 99.3% 47.24 96.2%
+Trait Comb 56.65?? 99.3% 48.00?? 96.2%
Table 1: Results on Arabic-English MT. ? = Significant
improvement at 95% confidence, as defined by (Koehn,
2004). ?? = Significant improvement at 99.9% confi-
dence. BLEU = IBM-BLEU score. Len = Hypothesis-
to-reference length ratio.
7 Conclusions and Future Work
We demonstrated a method of intelligently select-
ing hypotheses from a decoding forest which can be
combined with the baseline hypotheses to produce
a significant gain in BLEU score. In the future, we
plan to explore more trait types and alternate meth-
ods of system combination.
One possible application of this work is in fielded
translation systems. Because our method produces
high-quality complementary hypotheses at a low
computational cost, the system could present these
to the user as alternate translations. Going further,
a user could prefer a particular output type, such as
the fluency-tuned condition, and set that to be their
default translation.
The major open question is how our trait-based
combination interacts with multi-system combina-
tion. Imagine there are three different types of de-
coders which can be combined to produce some gain
in the baseline condition. If you independently im-
prove all three using trait-based combination, will
the relative gain from multi-system combination be
reduced? Or can you jointly combine all of the trait
hypotheses and get an even greater relative gain? We
plan to thoroughly explore this in the future.
531
References
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In NAACL,
pages 218?226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum Bayes risk combination of translation
hypotheses from alternative morphological decompo-
sitions. In NAACL, pages 73?76.
P. Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA, pages 115?124.
Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational
decoding for statistical machine translation. In ACL,
pages 593?601.
A. Pauls, J. DeNero, and D. Klein. 2009. Consensus
training for consensus decoding in machine transla-
tion. In EMNLP, pages 1418?1427.
A. Rosti, B. Zhang, S. Matsoukas, and R. Schwartz.
2010. BBN system description for WMT10 system
combination task. In WMT/MetricsMATR, pages 321?
326.
L. Shen, J. Xu, and R. Weischedel. 2008. A new string-
to-dependency machine translation algorithm with a
target dependency language model. In ACL-HLT,
pages 577?585.
O. Siohan, B. Ramabhadran, and B. Kingsbury. 2005.
Constructing ensembles of ASR systems using ran-
domized decision trees. In ICASSP.
J. Xu and A. Rosti. 2010. Combining unsupervised and
supervised alignments for MT: An empirical study. In
EMNLP, pages 667?673.
532
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322?327,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Unsupervised Morphology Rivals Supervised Morphology for Arabic MT
David Stallard Jacob Devlin
Michael Kayser
BBN Technologies
{stallard,jdevlin,rzbib}@bbn.com
Yoong Keok Lee Regina Barzilay
CSAIL
Massachusetts Institute of Technology
{yklee,regina}@csail.mit.edu
Abstract
If unsupervised morphological analyzers
could approach the effectiveness of super-
vised ones, they would be a very attractive
choice for improving MT performance on
low-resource inflected languages. In this
paper, we compare performance gains for
state-of-the-art supervised vs. unsupervised
morphological analyzers, using a state-of-the-
art Arabic-to-English MT system. We apply
maximum marginal decoding to the unsu-
pervised analyzer, and show that this yields
the best published segmentation accuracy
for Arabic, while also making segmentation
output more stable. Our approach gives
an 18% relative BLEU gain for Levantine
dialectal Arabic. Furthermore, it gives higher
gains for Modern Standard Arabic (MSA), as
measured on NIST MT-08, than does MADA
(Habash and Rambow, 2005), a leading
supervised MSA segmenter.
1 Introduction
If unsupervised morphological segmenters could ap-
proach the effectiveness of supervised ones, they
would be a very attractive choice for improving ma-
chine translation (MT) performance in low-resource
inflected languages. An example of particular cur-
rent interest is Arabic, whose various colloquial di-
alects are sufficiently different from Modern Stan-
dard Arabic (MSA) in lexicon, orthography, and
morphology, as to be low-resource languages them-
selves. An additional advantage of Arabic for study
is the availability of high-quality supervised seg-
menters for MSA, such as MADA (Habash and
Rambow, 2005), for performance comparison. The
MT gain for supervised MSA segmenters on dialect
establishes a lower bound, which the unsupervised
segmenter must exceed if it is to be useful for dialect.
And comparing the gain for supervised and unsuper-
vised segmenters on MSA tells us how useful the
unsupervised segmenter is, relative to the ideal case
in which a supervised segmenter is available.
In this paper, we show that an unsupervised seg-
menter can in fact rival or surpass supervised MSA
segmenters on MSA itself, while at the same time
providing superior performance on dialect. Specifi-
cally, we compare the state-of-the-art morphological
analyzer of Lee et al (2011) with two leading super-
vised analyzers for MSA, MADA and Sakhr1, each
serving as an alternative preprocessor for a state-of-
the-art statistical MT system (Shen et al, 2008). We
measure MSA performance on NIST MT-08 (NIST,
2010), and dialect performance on a Levantine di-
alect web corpus (Zbib et al, 2012b).
To improve performance, we apply maximum
marginal decoding (Johnson and Goldwater, 2009)
(MM) to combine multiple runs of the Lee seg-
menter, and show that this dramatically reduces the
variance and noise in the segmenter output, while
yielding an improved segmentation accuracy that
exceeds the best published scores for unsupervised
segmentation on Arabic Treebank (Naradowsky and
Toutanova, 2011). We also show that it yields MT-
08 BLEU scores that are higher than those obtained
with MADA, a leading supervised MSA segmenter.
For Levantine, the segmenter increases BLEU score
by 18% over the unsegmented baseline.
1http://www.sakhr.com/Default.aspx
322
2 Related Work
Machine translation systems that process highly in-
flected languages often incorporate morphological
analysis. Some of these approaches rely on mor-
phological analysis for pre- and post-processing,
while others modify the core of a translation system
to incorporate morphological information (Habash,
2008; Luong et al, 2010; Nakov and Ng, 2011). For
instance, factored translation Models (Koehn and
Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis
and Koehn, 2008) parametrize translation probabili-
ties as factors encoding morphological features.
The approach we have taken in this paper is
an instance of a segmented MT model, which di-
vides the input into morphemes and uses the de-
rived morphemes as a unit of translation (Sadat and
Habash, 2006; Badr et al, 2008; Clifton and Sarkar,
2011). This is a mainstream architecture that has
been shown to be effective when translating from a
morphologically rich language.
A number of recent approaches have explored
the use of unsupervised morphological analyzers
for MT (Virpioja et al, 2007; Creutz and Lagus,
2007; Clifton and Sarkar, 2011; Mermer and Ak?n,
2010; Mermer and Saraclar, 2011). Virpioja et al
(2007) apply the unsupervised morphological seg-
menter Morfessor (Creutz and Lagus, 2007), and
apply an existing MT system at the level of mor-
phemes. The system does not outperform the word
baseline partially due to the insufficient accuracy of
the automatic morphological analyzer.
The work of Mermer and Ak?n (2010) and Mer-
mer and Saraclar (2011) attempts to integrate mor-
phology and MT more closely than we do, by in-
corporating bilingual alignment probabilities into a
Gibbs-sampled version of Morfessor for Turkish-to-
English MT. However, the bilingual strategy shows
no gain over the monolingual version, and nei-
ther version is competitive for MT with a super-
vised Turkish morphological segmenter (Oflazer,
1993). By contrast, the unsupervised analyzer we
report on here yields MSA-to-English MT perfor-
mance that equals or exceed the performance ob-
tained with a leading supervised MSA segmenter,
MADA (Habash and Rambow, 2005).
3 Review of Lee Unsupervised Segmenter
The segmenter of Lee et al (2011) is a probabilis-
tic model operating at word-type level. It is di-
vided into four sub-model levels. Model 1 prefers
small affix lexicons, and assumes that morphemes
are drawn independently. Model 2 generates a la-
tent POS tag for each word type, conditioning the
word?s affixes on the tag, thereby encouraging com-
patible affixes to be generated together. Model 3
incorporates token-level contextual information, by
generating word tokens with a type-level Hidden
Markov Model (HMM). Finally, Model 4 models
morphosyntactic agreement with a transition proba-
bility distribution, encouraging adjacent tokens with
the same endings to also have the same final suffix.
4 Applying Maximum Marginal Decoding
to Reduce Variance and Noise
Maximum marginal decoding (Johnson and Gold-
water, 2009) (MM) is a technique which assigns
to each latent variable the value with the high-
est marginal probability, thereby maximizing the
expected number of correct assignments (Rabiner,
1989). Johnson and Goldwater (2009) extend MM
to Gibbs sampling by drawing a set of N indepen-
dent Gibbs samples, and selecting for each word the
most frequent segmentation found in them. They
found that MM improved segmentation accuracy
over the mean, consistent with its maximization cri-
terion. However, for our setting, we find that MM
provides several other crucial advantages as well.
First, MM dramatically reduces the output vari-
ance of Gibbs sampling (GS). Table 1 documents the
severity of this variance for the MT-08 lexicon, as
measured by the average exact-match accuracy and
segmentation F-measure between different runs. It
shows that on average, 13% of the word tokens, and
25% of the word types, are segmented differently
from run to run, which obviously makes the input to
MT highly unstable. By contrast the ?MM? column
of Table 1 shows that two different runs of MM, each
derived by combining separate sets of 25 GS runs,
agree on the segmentations of over 95% of the word
token ? a dramatic improvement in stability.
Second, MM reduces noise from the spurious af-
fixes that the unsupervised segmenter induces for
large lexicons. As Table 2 shows, the segmenter
323
Decoding Level Rec Prec F1 Acc
Gibbs Type 82.9 83.2 83.1 74.5
Token 87.5 89.1 88.3 86.7
MM Type 95.9 95.8 95.9 93.9
Token 97.3 94.0 95.6 95.1
Table 1: Comparison of agreement in outputs between
25 runs of Gibbs sampling vs. 2 runs of MM on the
full MT-08 data set. We give the average segmentation
recall, precision, F1-measure, and exact-match accuracy
between outputs, at word-type and word-token levels.
ATB MT-08
GS GS MM Morf
Unique prefixes 17 130 93 287
Unique suffixes 41 261 216 241
Top-95 prefixes 7 7 6 6
Top-95 suffixes 14 26 19 19
Table 2: Affix statistics of unsupervised segmenters. For
the ATB lexicon, we show statistics for the Lee seg-
menter with regular Gibbs sampling (GS). For the MT-
08 lexicon, we also show the output of the Lee segmenter
with maximum marginal decoding (MM). In addition, we
show statistics for Morfessor.
induces 130 prefixes and 261 suffixes for MT-08
(statistics for Morfessor are similar). This phe-
nomenon is fundamental to Bayesian nonparamet-
ric models, which expand indefinitely to fit the data
they are given (Wasserman, 2006). But MM helps
to alleviate it, reducing unique prefixes and suffixes
for MT-08 by 28% and 21%, respectively. It also re-
duces the number of unique prefixes/suffixes which
account for 95% of the prefix/suffix tokens (Top-95).
Finally, we find that in our setting, MM increases
accuracy not just over the mean, but over even the
best-scoring of the runs. As shown in Table 3, MM
increases segmentation F-measure from 86.2% to
88.2%. This exceeds the best published results on
ATB (Naradowsky and Toutanova, 2011).
These results suggest that MM may be worth con-
sidering for other GS applications, not only for the
accuracy improvements pointed out by Johnson and
Goldwater (2009), but also for its potential to pro-
vide more stable and less noisy results.
Model Mean Min Max MM
M1 80.1 79.0 81.5 81.8
M2 81.4 80.2 83.0 82.0
M3 81.4 80.1 82.8 83.2
M4 86.2 85.4 87.2 88.2
Table 3: Segmentation F-scores on ATB dataset for Lee
segmenter, shown for each Model level M1?M4 on the
Arabic segmentation dataset used by (Poon et al, 2009):
We give the mean, minimum, and maximum F-scores for
25 independent runs of Gibbs sampling, together with the
F-score from running MM over that same set of runs.
5 MT Evaluation
5.1 Experimental Design
MT System. Our experiments were performed
using a state-of-the-art, hierarchical string-to-
dependency-tree MT system, described in Shen et
al. (2008).
Morphological Analyzers. We compare the Lee
segmenter with the supervised MSA segmenter
MADA, using its ?D3? scheme. We also compare
with Sakhr, an intensively-engineered, supervised
MSA segmenter which applies multiple NLP tech-
nologies to the segmentation problem, and which
has given the best results for our MT system in pre-
vious work (Zbib et al, 2012a). We also compare
with Morfessor.
MT experiments. We apply the appropriate seg-
menter to split words into morphemes, which we
then treat as words for alignment and decoding. Fol-
lowing Lee et al (2011), we segment the test and
training sets jointly, estimating separate translation
models for each segmenter/dataset combination.
Training and Test Corpora. Our ?Full MSA? cor-
pus is the NIST MT-08 Constrained Data Track Ara-
bic training corpus (35M total, 336K unique words);
our ?Small MSA? corpus is a 1.3M-word subset.
Both are tested on the MT-08 evaluation set. For
dialect, we use a Levantine dialectal Arabic cor-
pus collected from the web with 1.5M total, 160K
unique words and 18K words held-out for test (Zbib
et al, 2012b)
PerformanceMetrics. We evaluate MTwith BLEU
score. To calculate statistical significance, we use
the boot-strap resampling method of Koehn (2004).
324
5.2 Results and Discussion
Table 4 summarizes the BLEU scores obtained from
using various segmenters, for three training/test sets:
Full MSA, Small MSA, and Levantine dialect.
As expected, Sakhr gives the best results for
MSA. Morfessor underperforms the other seg-
menters, perhaps because of its lower accuracy on
Arabic, as reported by Poon et al (2009). The
Lee segmenter gives the best results for Levantine,
inducing valid Levantine affixes (e.g ?hAl+? for
MSA?s ?h*A-Al+?, English ?this-the?) and yielding
an 18% relative gain over the unsegmented baseline.
What is more surprising is that the Lee segmenter
compares favorably with the supervised MSA seg-
menters on MSA itself. In particular, the Lee seg-
menter with MM yields higher BLEU scores than
does MADA, a leading supervised segmenter, while
preserving almost the same performance as GS on
dialect. On Small MSA, it recoups 93% of even
Sakhr?s gain.
By contrast, the Lee segmenter recoups only 79%
of Sakhr?s gain on Full MSA. This might result from
the phenomenon alluded to in Section 4, where addi-
tional data sometimes degrades performance for un-
supervised analyzers. However, the Lee segmenter?s
gain on Levantine (18%) is higher than its gain on
Small MSA (13%), even though Levantine has more
data (1.5M vs. 1.3M words). This might be be-
cause dialect, being less standardized, has more or-
thographic and morphological variability, which un-
supervised segmentation helps to resolve.
These experiments also show that while Model 4
gives the best F-score, Model 3 gives the best MT
scores. Comparison of Model 3 and 4 segmentations
shows that Model 4 induces a much larger num-
ber of inflectional suffixes, especially the feminine
singular suffix ?-p?, which accounts for a plurality
(16%) of the differences by token. While such suf-
fixes improve F-measure on the segmentation refer-
ences, they do not correspond to any English lexical
unit, and thus do not help alignment.
An interesting question is how much performance
might be gained from a supervised segmenter that
was as intensively engineered for dialect as Sakhr
was for MSA. Assuming a gain ratio of 0.93, similar
to Small MSA, the estimated BLEU score would be
20.38, for a relative gain of just 5% over the unsuper-
System Small Full Lev
MSA MSA Dial
Unsegmented 38.69 43.45 17.10
Sakhr 43.99 46.51 19.60
MADA 43.23 45.64 19.29
Morfessor 42.07 44.71 18.38
Lee GS
M1 43.12 44.80 19.70
M2 43.16 45.45 20.15+
M3 43.07 44.82 19.97
M4 42.93 45.06 19.55
Lee MM
M1 43.53 45.14 19.75
M2 43.45 45.29 19.75
M3 43.64+ 45.84 20.09
M4 43.56 45.16 19.93
Table 4: BLEU scores for all experiments. Full MSA is
the the full MT-08 corpus, Small MSA is a 1.3M-word
subset, Lev Dial our Levantine dataset. For each of these,
the highest Lee segmenter score is in bold, with ?+? if
statistically significant vs. MADA at the 95% confidence
level or higher. The highest overall score is in bold italic.
vised segmenter. Given the large engineering effort
that would be required to achieve this gain, the un-
supervised segmenter may be a more cost-effective
choice for dialectal Arabic.
6 Conclusion
We compare unsupervised vs. supervised morpho-
logical segmentation for Arabic-to-English machine
translation. We add maximum marginal decoding
to the unsupervised segmenter, and show that it
surpasses the state-of-the-art segmentation perfor-
mance, purges the segmenter of noise and variabil-
ity, yields BLEU scores on MSA competitive with
those from supervised segmenters, and gives an 18%
relative BLEU gain on Levantine dialectal Arabic.
Acknowledgements
This material is based upon work supported by
DARPA under Contract Nos. HR0011-12-C00014
and HR0011-12-C00015, and by ONR MURI Con-
tract No. W911NF-10-1-0533. Any opinions, find-
ings and conclusions or recommendations expressed
in this material are those of the author(s) and do not
necessarily reflect the views of the US government.
We thank Rabih Zbib for his help with interpreting
Levantine Arabic segmentation output.
325
References
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proceedings of ACL-08: HLT.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Seg-
mentation for English-to-Arabic statistical machine
translation. In Proceedings of ACL-08: HLT, Short
Papers.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process., 4:3:1?
3:34, February.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
ACL.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in Arabic-English statisti-
cal machine translation. In Proceedings of ACL-08:
HLT, Short Papers.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparametric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of EMNLP-CoNLL, pages
868?876.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. 2011. Modeling syntactic context improves
morphological segmentation. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation
for machine translation of morphologically rich lan-
guages. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing.
Cos?kun Mermer and Ahmet Afs??n Ak?n. 2010. Unsuper-
vised search for the optimal segmentation for statisti-
cal machine translation. In Proceedings of the ACL
2010 Student Research Workshop, pages 31?36, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Cos?kun Mermer and Murat Saraclar. 2011. Unsuper-
vised Turkish morphological segmentation for statis-
tical machine translation. In Workshop on Machine
Translation and Morphologically-rich languages, Jan-
uary.
Preslav Nakov and Hwee Tou Ng. 2011. Trans-
lating from morphologically complex languages: A
paraphrase-based approach. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich hidden semi-Markov models.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
NIST. 2010. NIST 2008 Open Machine Translation
(Open MT) Evaluation. http://www.ldc.
upenn.edu/Catalog/catalogEntry.jsp?
catalogId=LDC2010T21/.
Kemal Oflazer. 1993. Two-level description of Turkish
morphology. In Proceedings of the Sixth Conference
of the European Chapter of the Association for Com-
putational Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. In Proceedings of the IEEE, pages 257?
286.
Fatiha Sadat and Nizar Habash. 2006. Combination
of Arabic preprocessing schemes for statistical ma-
chine translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology-aware statisti-
cal machine translation based on morphs induced in an
unsupervised manner. In Proceedings of the Machine
Translation Summit XI.
LarryWasserman. 2006. All of Nonparametric Statistics.
Springer.
326
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of EACL.
Rabih Zbib, Michael Kayser, Spyros Matsoukas, John
Makhoul, Hazem Nader, Hamdy Soliman, and Rami
Safadi. 2012a. Methods for integrating rule-based and
statistical systems for Arabic to English machine trans-
lation. Machine Translation, 26(1-2):67?83.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012b. Machine translation of Arabic dialects. In
NAACL 2012: Proceedings of the 2012 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Montreal, Quebec, Canada, June. Association for
Computational Linguistics.
327
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370?1380,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Fast and Robust Neural Network Joint Models for Statistical Machine
Translation
Jacob Devlin, Rabih Zbib, Zhongqiang Huang,
Thomas Lamar, Richard Schwartz, and John Makhoul
Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA
{jdevlin,rzbib,zhuang,tlamar,schwartz,makhoul}@bbn.com
Abstract
Recent work has shown success in us-
ing neural network language models
(NNLMs) as features in MT systems.
Here, we present a novel formulation for
a neural network joint model (NNJM),
which augments the NNLM with a source
context window. Our model is purely lexi-
calized and can be integrated into any MT
decoder. We also present several varia-
tions of the NNJM which provide signif-
icant additive improvements.
Although the model is quite simple, it
yields strong empirical results. On the
NIST OpenMT12 Arabic-English condi-
tion, the NNJM features produce a gain of
+3.0 BLEU on top of a powerful, feature-
rich baseline which already includes a
target-only NNLM. The NNJM features
also produce a gain of +6.3 BLEU on top
of a simpler baseline equivalent to Chi-
ang?s (2007) original Hiero implementa-
tion.
Additionally, we describe two novel tech-
niques for overcoming the historically
high cost of using NNLM-style models
in MT decoding. These techniques speed
up NNJM computation by a factor of
10,000x, making the model as fast as a
standard back-off LM.
This work was supported by DARPA/I2O Contract No.
HR0011-12-C-0014 under the BOLT program (Approved for
Public Release, Distribution Unlimited). The views, opin-
ions, and/or findings contained in this article are those of the
author and should not be interpreted as representing the of-
ficial views or policies, either expressed or implied, of the
Defense Advanced Research Projects Agency or the Depart-
ment of Defense.
1 Introduction
In recent years, neural network models have be-
come increasingly popular in NLP. Initially, these
models were primarily used to create n-gram neu-
ral network language models (NNLMs) for speech
recognition and machine translation (Bengio et al,
2003; Schwenk, 2010). They have since been ex-
tended to translation modeling, parsing, and many
other NLP tasks.
In this paper we use a basic neural network ar-
chitecture and a lexicalized probability model to
create a powerful MT decoding feature. Specifi-
cally, we introduce a novel formulation for a neu-
ral network joint model (NNJM), which augments
an n-gram target language model with an m-word
source window. Unlike previous approaches to
joint modeling (Le et al, 2012), our feature can be
easily integrated into any statistical machine trans-
lation (SMT) decoder, which leads to substantially
larger improvements than k-best rescoring only.
Additionally, we present several variations of this
model which provide significant additive BLEU
gains.
We also present a novel technique for training
the neural network to be self-normalized, which
avoids the costly step of posteriorizing over the
entire vocabulary in decoding. When used in con-
junction with a pre-computed hidden layer, these
techniques speed up NNJM computation by a fac-
tor of 10,000x, with only a small reduction on MT
accuracy.
Although our model is quite simple, we obtain
strong empirical results. We show primary results
on the NIST OpenMT12 Arabic-English condi-
tion. The NNJM features produce an improvement
of +3.0 BLEU on top of a baseline that is already
better than the 1st place MT12 result and includes
1370
a powerful NNLM. Additionally, on top of a sim-
pler decoder equivalent to Chiang?s (2007) origi-
nal Hiero implementation, our NNJM features are
able to produce an improvement of +6.3 BLEU ?
as much as all of the other features in our strong
baseline system combined.
We also show strong improvements on the
NIST OpenMT12 Chinese-English task, as well as
the DARPA BOLT (Broad Operational Language
Translation) Arabic-English and Chinese-English
conditions.
2 Neural Network Joint Model (NNJM)
Formally, our model approximates the probability
of target hypothesis T conditioned on source sen-
tence S. We follow the standard n-gram LM de-
composition of the target, where each target word
t
i
is conditioned on the previous n ? 1 target
words. To make this a joint model, we also condi-
tion on source context vector S
i
:
P (T |S) ? ?
|T |
i=1
P (t
i
|t
i?1
, ? ? ? , t
i?n+1
, S
i
)
Intuitively, we want to define S
i
as the window
that is most relevant to t
i
. To do this, we first say
that each target word t
i
is affiliated with exactly
one source word at index a
i
. S
i
is then them-word
source window centered at a
i
:
S
i
= s
a
i
?
m?1
2
, ? ? ? , s
a
i
, ? ? ? , s
a
i
+
m?1
2
This notion of affiliation is derived from the
word alignment, but unlike word alignment, each
target word must be affiliated with exactly one
non-NULL source word. The affiliation heuristic
is very simple:
(1) If t
i
aligns to exactly one source word, a
i
is
the index of the word it aligns to.
(2) If t
i
align to multiple source words, a
i
is the
index of the aligned word in the middle.
1
(3) If t
i
is unaligned, we inherit its affiliation
from the closest aligned word, with prefer-
ence given to the right.
2
An example of the NNJM context model for a
Chinese-English parallel sentence is given in Fig-
ure 1.
For all of our experiments we use n = 4 and
m = 11. It is clear that this model is effectively
an (n+m)-gram LM, and a 15-gram LM would be
1
We arbitrarily round down.
2
We have found that the affiliation heuristic is robust to
small differences, such as left vs. right preference.
far too sparse for standard probability models such
as Kneser-Ney back-off (Kneser and Ney, 1995)
or Maximum Entropy (Rosenfeld, 1996). Fortu-
nately, neural network language models are able
to elegantly scale up and take advantage of arbi-
trarily large context sizes.
2.1 Neural Network Architecture
Our neural network architecture is almost identi-
cal to the original feed-forward NNLM architec-
ture described in Bengio et al (2003).
The input vector is a 14-word context vector
(3 target words, 11 source words), where each
word is mapped to a 192-dimensional vector us-
ing a shared mapping layer. We use two 512-
dimensional hidden layers with tanh activation
functions. The output layer is a softmax over the
entire output vocabulary.
The input vocabulary contains 16,000 source
words and 16,000 target words, while the out-
put vocabulary contains 32,000 target words. The
vocabulary is selected by frequency-sorting the
words in the parallel training data. Out-of-
vocabulary words are mapped to their POS tag (or
OOV, if POS is not available), and in this case
P (POS
i
|t
i?1
, ? ? ? ) is used directly without fur-
ther normalization. Out-of-bounds words are rep-
resented with special tokens <src>, </src>,
<trg>, </trg>.
We chose these values for the hidden layer size,
vocabulary size, and source window size because
they seemed to work best on our data sets ? larger
sizes did not improve results, while smaller sizes
degraded results. Empirical comparisons are given
in Section 6.5.
2.2 Neural Network Training
The training procedure is identical to that of an
NNLM, except that the parallel corpus is used
instead of a monolingual corpus. Formally, we
seek to maximize the log-likelihood of the train-
ing data:
L =
?
i
log(P (x
i
))
where x
i
is the training sample, with one sample
for every target word in the parallel corpus.
Optimization is performed using standard back
propagation with stochastic gradient ascent (Le-
Cun et al, 1998). Weights are randomly initial-
ized in the range of [?0.05, 0.05]. We use an ini-
tial learning rate of 10
?3
and a minibatch size of
1371
Figure 1: Context vector for target word ?the?, using a 3-word target history and a 5-word source window
(i.e., n = 4 and m = 5). Here, ?the? inherits its affiliation from ?money? because this is the first aligned
word to its right. The number in each box denotes the index of the word in the context vector. This
indexing must be consistent across samples, but the absolute ordering does not affect results.
128.
3
At every epoch, which we define as 20,000
minibatches, the likelihood of a validation set is
computed. If this likelihood is worse than the pre-
vious epoch, the learning rate is multiplied by 0.5.
The training is run for 40 epochs. The training
data ranges from 10-30M words, depending on the
condition. We perform a basic weight update with
no L2 regularization or momentum. However, we
have found it beneficial to clip each weight update
to the range of [-0.1, 0.1], to prevent the training
from entering degenerate search spaces (Pascanu
et al, 2012).
Training is performed on a single Tesla K10
GPU, with each epoch (128*20k = 2.6M samples)
taking roughly 1100 seconds to run, resulting in
a total training time of ?12 hours. Decoding is
performed on a CPU.
2.3 Self-Normalized Neural Network
The computational cost of NNLMs is a significant
issue in decoding, and this cost is dominated by
the output softmax over the entire target vocabu-
lary. Even class-based approaches such as Le et
al. (2012) require a 2-20k shortlist vocabulary, and
are therefore still quite costly.
Here, our goal is to be able to use a fairly
large vocabulary without word classes, and to sim-
ply avoid computing the entire output layer at de-
code time.
4
To do this, we present the novel
technique of self-normalization, where the output
layer scores are close to being probabilities with-
out explicitly performing a softmax.
Formally, we define the standard softmax log
3
We do not divide the gradient by the minibatch size. For
those who do, this is equivalent to using an initial learning
rate of 10
?3
? 128 ? 10
?1
.
4
We are not concerned with speeding up training time, as
we already find GPU training time to be adequate.
likelihood as:
log(P (x)) = log
(
e
U
r
(x)
Z(x)
)
= U
r
(x)? log(Z(x))
Z(x) = ?
|V |
r
?
=1
e
U
r
?
(x)
where x is the sample, U is the raw output layer
scores, r is the output layer row corresponding to
the observed target word, and Z(x) is the softmax
normalizer.
If we could guarantee that log(Z(x)) were al-
ways equal to 0 (i.e., Z(x) = 1) then at decode
time we would only have to compute row r of the
output layer instead of the whole matrix. While
we cannot train a neural network with this guaran-
tee, we can explicitly encourage the log-softmax
normalizer to be as close to 0 as possible by aug-
menting our training objective function:
L =
?
i
[
log(P (x
i
))? ?(log(Z(x
i
))? 0)
2
]
=
?
i
[
log(P (x
i
))? ? log
2
(Z(x
i
))
]
In this case, the output layer bias weights are
initialized to log(1/|V |), so that the initial net-
work is self-normalized. At decode time, we sim-
ply use U
r
(x) as the feature score, rather than
log(P (x)). For our NNJM architecture, self-
normalization increases the lookup speed during
decoding by a factor of ?15x.
Table 1 shows the neural network training re-
sults with various values of the free parameter
?. In all subsequent MT experiments, we use
? = 10
?1
.
We should note that Vaswani et al (2013) im-
plements a method called Noise Contrastive Es-
timation (NCE) that is also used to train self-
normalized NNLMs. Although NCE results in
faster training time, it has the downside that there
1372
Arabic BOLT Val
? log(P (x)) | log(Z(x))|
0 ?1.82 5.02
10
?2
?1.81 1.35
10
?1
?1.83 0.68
1 ?1.91 0.28
Table 1: Comparison of neural network likelihood
for various ? values. log(P (x)) is the average
log-likelihood on a held-out set. | log(Z(x))| is
the mean error in log-likelihood when using U
r
(x)
directly instead of the true softmax probability
log(P (x)). Note that ? = 0 is equivalent to the
standard neural network objective function.
is no mechanism to control the degree of self-
normalization. By contrast, our ? parameter al-
lows us to carefully choose the optimal trade-off
between neural network accuracy and mean self-
normalization error. In future work, we will thor-
oughly compare self-normalization vs. NCE.
2.4 Pre-Computing the Hidden Layer
Although self-normalization significantly im-
proves the speed of NNJM lookups, the model
is still several orders of magnitude slower than a
back-off LM. Here, we present a ?trick? for pre-
computing the first hidden layer, which further in-
creases the speed of NNJM lookups by a factor of
1,000x.
Note that this technique only results in a signif-
icant speedup for self-normalized, feed-forward,
NNLM-style networks with one hidden layer. We
demonstrate in Section 6.6 that using one hidden
layer instead of two has minimal effect on BLEU.
For the neural network described in Section 2.1,
computing the first hidden layer requires mul-
tiplying a 2689-dimensional input vector
5
with
a 2689 ? 512 dimensional hidden layer matrix.
However, note that there are only 3 possible posi-
tions for each target word, and 11 for each source
word. Therefore, for every word in the vocabu-
lary, and for each position, we can pre-compute
the dot product between the word embedding and
the first hidden layer. These are computed offline
and stored in a lookup table, which is <500MB in
size.
Computing the first hidden layer now only re-
quires 15 scalar additions for each of the 512
hidden rows ? one for each word in the input
5
2689 = 14 words ? 192 dimensions + 1 bias
vector, plus the bias. This can be reduced to
just 5 scalar additions by pre-summing each 11-
word source window when starting a test sen-
tence. If our neural network has only one hid-
den layer and is self-normalized, the only remain-
ing computation is 512 calls to tanh() and a sin-
gle 513-dimensional dot product for the final out-
put score.
6
Thus, only ?3500 arithmetic opera-
tions are required per n-gram lookup, compared
to ?2.8M for self-normalized NNJM without pre-
computation, and ?35M for the standard NNJM.
7
Neural Network Speed
Condition lookups/sec sec/word
Standard 110 10.9
+ Self-Norm 1500 0.8
+ Pre-Computation 1,430,000 0.0008
Table 2: Speed of the neural network computa-
tion on a single CPU thread. ?lookups/sec? is the
number of unique n-gram probabilities that can be
computed per second. ?sec/word? is the amortized
cost of unique NNJM lookups in decoding, per
source word.
Table 2 shows the speed of self-normalization
and pre-computation for the NNJM. The decoding
cost is based on a measurement of ?1200 unique
NNJM lookups per source word for our Arabic-
English system.
8
By combining self-normalization and pre-
computation, we can achieve a speed of 1.4M
lookups/second, which is on par with fast back-
off LM implementations (Tanaka et al, 2013).
We demonstrate in Section 6.6 that using the self-
normalized/pre-computed NNJM results in only
a very small BLEU degradation compared to the
standard NNJM.
3 Decoding with the NNJM
Because our NNJM is fundamentally an n-gram
NNLM with additional source context, it can eas-
ily be integrated into any SMT decoder. In this
section, we describe the considerations that must
be taken when integrating the NNJM into a hierar-
chical decoder.
6
tanh() is implemented using a lookup table.
7
3500 ? 5? 512 + 2? 513; 2.8M ? 2? 2689? 512 +
2 ? 513; 35M ? 2 ? 2689 ? 512 + 2 ? 513 ? 32000. For
the sake of a fair comparison, these all use one hidden layer.
A second hidden layer adds 0.5M floating point operations.
8
This does not include the cost of duplicate lookups
within the same test sentence, which are cached.
1373
3.1 Hierarchical Parsing
When performing hierarchical decoding with an
n-gram LM, the leftmost and rightmost n ? 1
words from each constituent must be stored in the
state space. Here, we extend the state space to
also include the index of the affiliated source word
for these edge words. This does not noticeably in-
crease the search space. We also train a separate
lower-order n-gram model, which is necessary to
compute estimate scores during hierarchical de-
coding.
3.2 Affiliation Heuristic
For aligned target words, the normal affiliation
heuristic can be used, since the word alignment
is available within the rule. For unaligned words,
the normal heuristic can also be used, except when
the word is on the edge of a rule, because then the
target neighbor words are not necessarily known.
In this case, we infer the affiliation from the rule
structure. Specifically, if unaligned target word t
is on the right edge of an arc that covers source
span [s
i
, s
j
], we simply say that t is affiliated with
source word s
j
. If t is on the left edge of the arc,
we say it is affiliated with s
i
.
4 Model Variations
Recall that our NNJM feature can be described
with the following probability:
?
|T |
i=1
P (t
i
|t
i?1
, t
i?2
, ? ? ? , s
a
i
, s
a
i
?1
, s
a
i
+1
, ? ? ? )
This formulation lends itself to several natural
variations. In particular, we can reverse the trans-
lation direction of the languages, as well as the di-
rection of the language model.
We denote our original formulation as a source-
to-target, left-to-right model (S2T/L2R). We can
train three variations using target-to-source (T2S)
and right-to-left (R2L) models:
S2T/R2L
?
|T |
i=1
P (t
i
|t
i+1
, t
i+2
, ? ? ? , s
a
i
, s
a
i
?1
, s
a
i
+1
, ? ? ? )
T2S/L2R
?
|S|
i=1
P (s
i
|s
i?1
, s
i?2
, ? ? ? , t
a
?
i
, t
a
?
i
?1
, t
a
?
i
+1
, ? ? ? )
T2S/R2L
?
|S|
i=1
P (s
i
|s
i+1
, s
i+2
, ? ? ? , t
a
?
i
, t
a
?
i
?1
, t
a
?
i
+1
, ? ? ? )
where a
?
i
is the target-to-source affiliation, de-
fined analogously to a
i
.
The T2S variations cannot be used in decoding
due to the large target context required, and are
thus only used in k-best rescoring. The S2T/R2L
variant could be used in decoding, but we have not
found this beneficial, so we only use it in rescor-
ing.
4.1 Neural Network Lexical Translation
Model (NNLTM)
One issue with the S2T NNJM is that the prob-
ability is computed over every target word, so it
does not explicitly model NULL-aligned source
words. In order to assign a probability to every
source word during decoding, we also train a neu-
ral network lexical translation model (NNLMT).
Here, the input context is the 11-word source
window centered at s
i
, and the output is the tar-
get token t
s
i
which s
i
aligns to. The probabil-
ity is computed over every source word in the in-
put sentence. We treat NULL as a normal target
word, and if a source word aligns to multiple target
words, it is treated as a single concatenated token.
Formally, the probability model is:
?
|S|
i=1
P (t
s
i
|s
i
, s
i?1
, s
i+1
, ? ? ? )
This model is trained and evaluated like our
NNJM. It is easy and computationally inexpensive
to use this model in decoding, since only one neu-
ral network computation must be made for each
source word.
In rescoring, we also use a T2S NNLTM model
computed over every target word:
?
|T |
i=1
P (s
t
i
|t
i
, t
i?1
, t
i+1
, ? ? ? )
5 MT System
In this section, we describe the MT system used in
our experiments.
5.1 MT Decoder
We use a state-of-the-art string-to-dependency hi-
erarchical decoder (Shen et al, 2010). Our base-
line decoder contains a large and powerful set of
features, which include:
? Forward and backward rule probabilities
? 4-gram Kneser-Ney LM
? Dependency LM (Shen et al, 2010)
? Contextual lexical smoothing (Devlin, 2009)
? Length distribution (Shen et al, 2010)
? Trait features (Devlin and Matsoukas, 2012)
? Factored source syntax (Huang et al, 2013)
? 7 sparse feature types, totaling 50k features
(Chiang et al, 2009)
? LM adaptation (Snover et al, 2008)
1374
We also perform 1000-best rescoring with the
following features:
? 5-gram Kneser-Ney LM
? Recurrent neural network language model
(RNNLM) (Mikolov et al, 2010)
Although we consider the RNNLM to be part
of our baseline, we give it special treatment in the
results section because we would expect it to have
the highest overlap with our NNJM.
5.2 Training and Optimization
For Arabic word tokenization, we use the MADA-
ARZ tokenizer (Habash et al, 2013) for the BOLT
condition, and the Sakhr
9
tokenizer for the NIST
condition. For Chinese tokenization, we use a sim-
ple longest-match-first lexicon-based approach.
For word alignment, we align all of the train-
ing data with both GIZA++ (Och and Ney, 2003)
and NILE (Riesa et al, 2011), and concatenate the
corpora together for rule extraction.
For MT feature weight optimization, we use
iterative k-best optimization with an Expected-
BLEU objective function (Rosti et al, 2010).
6 Experimental Results
We present MT primary results on Arabic-English
and Chinese-English for the NIST OpenMT12 and
DARPA BOLT conditions. We also present a set
of auxiliary results in order to further analyze our
features.
6.1 NIST OpenMT12 Results
Our NIST system is fully compatible with the
OpenMT12 constrained track, which consists of
10M words of high-quality parallel training for
Arabic, and 25M words for Chinese.
10
The
Kneser-Ney LM is trained on 5B words of data
from English GigaWord. For test, we use
the ?Arabic-To-English Original Progress Test?
(1378 segments) and ?Chinese-to-English Orig-
inal Progress Test + OpenMT12 Current Test?
(2190 segments), which consists of a mix of
newswire and web data.
11
All test segments have
4 references. Our tuning set contains 5000 seg-
ments, and is a mix of the MT02-05 eval set as
well as held-out parallel training.
9
http://www.sakhr.com
10
We also make weak use of 30M-100M words of UN data
+ ISI comparable corpora, but this data provides almost no
benefit.
11
http://www.nist.gov/itl/iad/mig/openmt12results.cfm
NIST MT12 Test
Ar-En Ch-En
BLEU BLEU
OpenMT12 - 1st Place 49.5 32.6
OpenMT12 - 2nd Place 47.5 32.2
OpenMT12 - 3rd Place 47.4 30.8
? ? ? ? ? ? ? ? ?
OpenMT12 - 9th Place 44.0 27.0
OpenMT12 - 10th Place 41.2 25.7
Baseline (w/o RNNLM) 48.9 33.0
Baseline (w/ RNNLM) 49.8 33.4
+ S2T/L2R NNJM (Dec) 51.2 34.2
+ S2T NNLTM (Dec) 52.0 34.2
+ T2S NNLTM (Resc) 51.9 34.2
+ S2T/R2L NNJM (Resc) 52.2 34.3
+ T2S/L2R NNJM (Resc) 52.3 34.5
+ T2S/R2L NNJM (Resc) 52.8 34.7
?Simple Hier.? Baseline 43.4 30.1
+ S2T/L2R NNJM (Dec) 47.2 31.5
+ S2T NNLTM (Dec) 48.5 31.8
+ Other NNJMs (Resc) 49.7 32.2
Table 3: Primary results on Arabic-English and
Chinese-English NIST MT12 Test Set. The first
section corresponds to the top and bottom ranked
systems from the evaluation, and are taken from
the NIST website. The second section corresponds
to results on top of our strongest baseline. The
third section corresponds to results on top of a
simpler baseline. Within each section, each row
includes all of the features from previous rows.
BLEU scores are mixed-case.
Results are shown in the second section of Ta-
ble 3. On Arabic-English, the primary S2T/L2R
NNJM gains +1.4 BLEU on top of our baseline,
while the S2T NNLTM gains another +0.8, and
the directional variations gain +0.8 BLEU more.
This leads to a total improvement of +3.0 BLEU
from the NNJM and its variations. Considering
that our baseline is already +0.3 BLEU better than
the 1st place result of MT12 and contains a strong
RNNLM, we consider this to be quite an extraor-
dinary improvement.
12
For the Chinese-English condition, there is an
improvement of +0.8 BLEU from the primary
NNJM and +1.3 BLEU overall. Here, the base-
line system is already +0.8 BLEU better than the
12
Note that the official 1st place OpenMT12 result was our
own system, so we can assure that these comparisons are ac-
curate.
1375
best MT12 system. The smaller improvement on
Chinese-English compared to Arabic-English is
consistent with the behavior of our baseline fea-
tures, as we show in the next section.
6.2 ?Simple Hierarchical? NIST Results
The baseline used in the last section is a highly-
engineered research system, which uses a wide
array of features that were refined over a num-
ber of years, and some of which require linguis-
tic resources. Because of this, the baseline BLEU
scores are much higher than a typical MT system
? especially a real-time, production engine which
must support many language pairs.
Therefore, we also present results using a
simpler version of our decoder which emulates
Chiang?s original Hiero implementation (Chiang,
2007). Specifically, this means that we don?t
use dependency-based rule extraction, and our de-
coder only contains the following MT features: (1)
rule probabilities, (2) n-gram Kneser-Ney LM, (3)
lexical smoothing, (4) target word count, (5) con-
cat rule penalty.
Results are shown in the third section of Table 3.
The ?Simple Hierarchical? Arabic-English system
is -6.4 BLEU worse than our strong baseline, and
would have ranked 10th place out of 11 systems
in the evaluation. When the NNJM features are
added to this system, we see an improvement of
+6.3 BLEU, which would have ranked 1st place in
the evaluation.
Effectively, this means that for Arabic-English,
the NNJM features are equivalent to the combined
improvements from the string-to-dependency
model plus all of the features listed in Section 5.1.
For Chinese-English, the ?Simple Hierarchical?
system only degrades by -3.2 BLEU compared
to our strongest baseline, and the NNJM features
produce a gain of +2.1 BLEU on top of that.
6.3 BOLT Web Forum Results
DARPA BOLT is a major research project with the
goal of improving translation of informal, dialec-
tical Arabic and Chinese into English. The BOLT
domain presented here is ?web forum,? which was
crawled from various Chinese and Egyptian Inter-
net forums by LDC. The BOLT parallel training
consists of all of the high-quality NIST training,
plus an additional 3 million words of translated
forum data provided by LDC. The tuning and test
sets consist of roughly 5000 segments each, with
2 references for Arabic and 3 for Chinese.
Results are shown in Table 4. The baseline here
uses the same feature set as the strong NIST sys-
tem. On Arabic, the total gain is +2.6 BLEU,
while on Chinese, the gain is +1.3 BLEU.
BOLT Test
Ar-En Ch-En
BLEU BLEU
Baseline (w/o RNNLM) 40.2 30.6
Baseline (w/ RNNLM) 41.3 30.9
+ S2T/L2R NNJM (Dec) 42.9 31.9
+ S2T NNLTM (Dec) 43.2 31.9
+ Other NNJMs (Resc) 43.9 32.2
Table 4: Primary results on Arabic-English and
Chinese-English BOLT Web Forum. Each row
includes the aggregate features from all previous
rows.
6.4 Effect of k-best Rescoring Only
Table 5 shows performance when our S2T/L2R
NNJM is used only in 1000-best rescoring, com-
pared to decoding. The primary purpose of this is
as a comparison to Le et al (2012), whose model
can only be used in k-best rescoring.
BOLT Test
Ar-En
Without With
RNNLM RNNLM
BLEU BLEU
Baseline 40.2 41.3
S2T/L2R NNJM (Resc) 41.7 41.6
S2T/L2R NNJM (Dec) 42.8 42.9
Table 5: Comparison of our primary NNJM in de-
coding vs. 1000-best rescoring.
We can see that the rescoring-only NNJM per-
forms very well when used on top of a baseline
without an RNNLM (+1.5 BLEU), but the gain on
top of the RNNLM is very small (+0.3 BLEU).
The gain from the decoding NNJM is large in both
cases (+2.6 BLEU w/o RNNLM, +1.6 BLEU w/
RNNLM). This demonstrates that the full power of
the NNJM can only be harnessed when it is used
in decoding. It is also interesting to see that the
RNNLM is no longer beneficial when the NNJM
is used.
1376
6.5 Effect of Neural Network Configuration
Table 6 shows results using the S2T/L2R NNJM
with various configurations. We can see that re-
ducing the source window size, layer size, or vo-
cab size will all degrade results. Increasing the
sizes beyond the default NNJM has almost no ef-
fect (102%). Also note that the target-only NNLM
(i.e., Source Window=0) only obtains 33% of the
improvements of the NNJM.
BOLT Test
Ar-En
BLEU % Gain
?Simple Hier.? Baseline 33.8 -
S2T/L2R NNJM (Dec) 38.4 100%
Source Window=7 38.3 98%
Source Window=5 38.2 96%
Source Window=3 37.8 87%
Source Window=0 35.3 33%
Layers=384x768x768 38.5 102%
Layers=192x512 38.1 93%
Layers=128x128 37.1 72%
Vocab=64,000 38.5 102%
Vocab=16,000 38.1 93%
Vocab=8,000 37.3 83%
Activation=Rectified Lin. 38.5 102%
Activation=Linear 37.3 76%
Table 6: Results with different neural net-
work architectures. The ?default? NNJM in
the second row uses these parameters: SW=11,
L=192x512x512, V=32,000, A=tanh. All mod-
els use a 3-word target history (i.e., 4-gram LM).
?Layers? refers to the size of the word embedding
followed by the hidden layers. ?Vocab? refers to
the size of the input and output vocabularies. ?%
Gain? is the BLEU gain over the baseline relative
to the default NNJM.
6.6 Effect of Speedups
All previous results use a self-normalized neural
network with two hidden layers. In Table 7, we
compare this to using a standard network (with
two hidden layers), as well as a pre-computed neu-
ral network.
13
The ?Simple Hierarchical? base-
line is used here because it more closely approx-
imates a real-time MT engine. For the sake of
speed, these experiments only use the S2T/L2R
NNJM+S2T NNLTM.
13
The difference in score for self-normalized vs. pre-
computed is entirely due to two vs. one hidden layers.
Each result from Table 7 corresponds to a row
in Table 2 of Section 2.4. We can see that go-
ing from the standard model to the pre-computed
model only reduces the BLEU improvement from
+6.4 to +6.1, while increasing the NNJM lookup
speed by a factor of 10,000x.
BOLT Test
Ar-En
BLEU Gain
?Simple Hier.? Baseline 33.8 -
Standard NNJM 40.2 +6.4
Self-Norm NNJM 40.1 +6.3
Pre-Computed NNJM 39.9 +6.1
Table 7: Results for the standard NNs vs. self-
normalized NNs vs. pre-computed NNs.
In Table 2 we showed that the cost of unique
lookups for the pre-computed NNJM is only
?0.001 seconds per source word. This does not
include the cost of n-gram creation or cached
lookups, which amount to ?0.03 seconds per
source word in our current implementation.
14
However, the n-grams created for the NNJM can
be shared with the Kneser-Ney LM, which reduces
the cost of that feature. Thus, the total cost in-
crease of using the NNJM+NNLTM features in
decoding is only ?0.01 seconds per source word.
In future work we will provide more detailed
analysis regarding the usability of the NNJM in a
low-latency, high-throughput MT engine.
7 Related Work
Although there has been a substantial amount of
past work in lexicalized joint models (Marino et
al., 2006; Crego and Yvon, 2010), nearly all of
these papers have used older statistical techniques
such as Kneser-Ney or Maximum Entropy. How-
ever, not only are these techniques intractable to
train with high-order context vectors, they also
lack the neural network?s ability to semantically
generalize (Mikolov et al, 2013) and learn non-
linear relationships.
A number of recent papers have proposed meth-
ods for creating neural network translation/joint
models, but nearly all of these works have ob-
tained much smaller BLEU improvements than
ours. For each related paper, we will briefly con-
14
In our decoder, roughly 95% of NNJM n-gram lookups
within the same sentence are duplicates.
1377
trast their methodology with our own and summa-
rize their BLEU improvements using scores taken
directly from the cited paper.
Auli et al (2013) use a fixed continuous-space
source representation, obtained from LDA (Blei
et al, 2003) or a source-only NNLM. Also, their
model is recurrent, so it cannot be used in decod-
ing. They obtain +0.2 BLEU improvement on top
of a target-only NNLM (25.6 vs. 25.8).
Schwenk (2012) predicts an entire target phrase
at a time, rather than a word at a time. He obtains
+0.3 BLEU improvement (24.8 vs. 25.1).
Zou et al (2013) estimate context-free bilingual
lexical similarity scores, rather than using a large
context. They obtain an +0.5 BLEU improvement
on Chinese-English (30.0 vs. 30.5).
Kalchbrenner and Blunsom (2013) implement
a convolutional recurrent NNJM. They score a
1000-best list using only their model and are able
to achieve the same BLEU as using all 12 standard
MT features (21.8 vs 21.7). However, additive re-
sults are not presented.
The most similar work that we know of is Le et
al. (2012). Le?s basic procedure is to re-order the
source to match the linear order of the target, and
then segment the hypothesis into minimal bilin-
gual phrase pairs. Then, he predicts each target
word given the previous bilingual phrases. How-
ever, Le?s formulation could only be used in k-
best rescoring, since it requires long-distance re-
ordering and a large target context.
Le?s model does obtain an impressive +1.7
BLEU gain on top of a baseline without an NNLM
(25.8 vs. 27.5). However, when compared to
the strongest baseline which includes an NNLM,
Le?s best models (S2T + T2S) only obtain an +0.6
BLEU improvement (26.9 vs. 27.5). This is con-
sistent with our rescoring-only result, which indi-
cates that k-best rescoring is too shallow to take
advantage of the power of a joint model.
Le?s model also uses minimal phrases rather
than being purely lexicalized, which has two main
downsides: (a) a number of complex, hand-crafted
heuristics are required to define phrase boundaries,
which may not transfer well to new languages, (b)
the effective vocabulary size is much larger, which
substantially increases data sparsity issues.
We should note that our best results use six sep-
arate models, whereas all previous work only uses
one or two models. However, we have demon-
strated that we can obtain 50%-80% of the to-
tal improvement with only one model (S2T/L2R
NNJM), and 70%-90% with only two models
(S2T/L2R NNJM + S2T NNLTM). Thus, the one
and two-model conditions still significantly out-
perform any past work.
8 Discussion
We have described a novel formulation for a neural
network-based machine translation joint model,
along with several simple variations of this model.
When used as MT decoding features, these models
are able to produce a gain of +3.0 BLEU on top of
a very strong and feature-rich baseline, as well as
a +6.3 BLEU gain on top of a simpler system.
Our model is remarkably simple ? it requires no
linguistic resources, no feature engineering, and
only a handful of hyper-parameters. It also has no
reliance on potentially fragile outside algorithms,
such as unsupervised word clustering. We con-
sider the simplicity to be a major advantage. Not
only does this suggest that it will generalize well to
new language pairs and domains, but it also sug-
gests that it will be straightforward for others to
replicate these results.
Overall, we believe that the following factors set
us apart from past work and allowed us to obtain
such significant improvements:
1. The ability to use the NNJM in decoding
rather than rescoring.
2. The use of a large bilingual context vector,
which is provided to the neural network in
?raw? form, rather than as the output of some
other algorithm.
3. The fact that the model is purely lexicalized,
which avoids both data sparsity and imple-
mentation complexity.
4. The large size of the network architecture.
5. The directional variation models.
One of the biggest goals of this work is to quell
any remaining doubts about the utility of neural
networks in machine translation. We believe that
there are large areas of research yet to be explored.
For example, creating a new type of decoder cen-
tered around a purely lexicalized neural network
model. Our short term ideas include using more
interesting types of context in our input vector
(such as source syntax), or using the NNJM to
model syntactic/semantic structure of the target.
1378
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In HLT-NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Josep Maria Crego and Franc?ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, 24(2):159?
175.
Jacob Devlin and Spyros Matsoukas. 2012. Trait-
based hypothesis selection for machine translation.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 528?532, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jacob Devlin. 2009. Lexical features for statistical
machine translation. Master?s thesis, University of
Maryland.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
analysis and disambiguation for dialectal arabic. In
HLT-NAACL, pages 426?432.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored soft source syntactic constraints for
hierarchical machine translation. In EMNLP, pages
556?566.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, NAACL HLT ?12, pages 39?
48, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yann LeCun, L?eon Bottou, Genevieve B Orr, and
Klaus-Robert M?uller. 1998. Efficient backprop. In
Neural networks: Tricks of the trade, pages 9?50.
Springer.
Jos?e B Marino, Rafael E Banchs, Josep M Crego, Adri`a
De Gispert, Patrik Lambert, Jos?e AR Fonollosa, and
Marta R Costa-Juss`a. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746?
751.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2012. On the difficulty of training recurrent neural
networks. arXiv preprint arXiv:1211.5063.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 497?507, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ronald Rosenfeld. 1996. A maximum entropy ap-
proach to adaptive statistical language modeling.
Computer, Speech and Language, 10:187?228.
Antti Rosti, Bing Zhang, Spyros Matsoukas, and
Rich Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In
WMT/MetricsMATR, pages 321?326.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. Prague
Bull. Math. Linguistics, 93:137?146.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071?1080.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649?671,
December.
1379
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?08, pages 857?866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Makoto Tanaka, Yasuhara Toru, Jun-ya Yamamoto, and
Mikio Norimatsu. 2013. An efficient language
model using double-array structures.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387?1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1393?1398.
1380
