Proceedings of the EACL 2009 Student Research Workshop, pages 19?27,
Athens, Greece, 2 April 2009. c?2009 Association for Computational Linguistics
Combining a Statistical Language Model with Logistic Regression to
Predict the Lexical and Syntactic Difficulty of Texts for FFL
Thomas L. Franc?ois
Aspirant FNRS
CENTAL (Center for Natural Language Processing)
Universite? catholique de Louvain
1348 Louvain-la-Neuve, Belgium
thomas.francois@uclouvain.be
Abstract
Reading is known to be an essential task
in language learning, but finding the ap-
propriate text for every learner is far from
easy. In this context, automatic procedures
can support the teacher?s work. Some
tools exist for English, but at present there
are none for French as a foreign language
(FFL). In this paper, we present an origi-
nal approach to assessing the readability
of FFL texts using NLP techniques and
extracts from FFL textbooks as our cor-
pus. Two logistic regression models based
on lexical and grammatical features are
explored and give quite good predictions
on new texts. The results shows a slight
superiority for multinomial logistic re-
gression over the proportional odds model.
1 Introduction
The current massive mobility of people has put
increasing pressure on the language teaching sec-
tor, in terms of the availability of instructors and
suitable teaching materials. The development of
Intelligent Computer Aided Language Learning
(ICALL) has helped both these needs, while the
Internet has increasingly been used as a source of
exercises. Indeed, it allows immediate access to a
huge number of texts which can be used for edu-
cational purposes, either for classical reading com-
prehension tasks, or as a corpus for the creation of
various automatically generated exercises.
However, the strength of the Internet is also its
main flaw : there are so many texts available to the
teacher that he or she can get lost. Having gathered
some documents suitable in terms of subject mat-
ter, teachers still have to check if their readabil-
ity levels are suitable for their students : a highly
time-consuming task. This is where NLP applica-
tions able to classify documents according to their
reading difficulty level can be invaluable.
Related research will be discussed in Section 2.
In Section 3, the distinctive features of the cor-
pus used in this study and a difficulty scale suit-
able for FFL text classification are described. Sec-
tion 4 focuses on the independent linguistic vari-
ables considered in this research, while the statis-
tical techniques used for predictions are covered
in Section 5. Section 6 gives some details of the
implementations, and Section 7 presents the first
results of our models. Finally, Section 8 sums up
the contribution of this article before providing a
programme for future work and improvement of
the results.
2 Related research
The measurement of the reading difficulty of texts
has been a major concern in the English-speaking
literature since the 1920s and the first formula de-
veloped by Lively and Pressey (1923). The field
of readability has since produced many formulae
based on simple lexical and syntactic measures
such as the average number of syllables per word,
the average length of sentences in a piece of text
(Flesch, 1948; Kincaid et al, 1975), or the per-
centage of words not on a list combined with the
average sentence length (Chall and Dale, 1995).
French-speaking researchers discovered the
field of readability in 1956 through the work of
Andre? Conquet, La lisibilite? (1971), and the first
two formulae for French were adapted from Flesch
(1948) by Kandel and Moles (1958) and de Land-
sheere (1963). Both of these researchers stayed
quite close to the Flesch formula, and in so doing
they failed to take into account some specificities
of the French language.
Henry (1975) was the first to introduce spe-
cific formulae for French. He used a larger set
of variables to design three formulae : a com-
plete, an automatic and a short one, each of which
19
was adapted for three different educational lev-
els. His formulae are by far the best and most fre-
quently used in the French-speaking world. Later,
Richaudeau (1979) suggested a criteria of ?lin-
guistic efficiency? based on experiments on short-
term memory, while Mesnager (1989) coined what
is still, to the best of our knowledge, the most re-
cent specific formula for French, with children as
its target.
Compared to the mass of studies in English,
readability in French has never enthused the re-
search community. The cultural reasons for this
are analysed by Bosse?-Andrieu (1993) (who basi-
cally argues that the idea of measuring text diffi-
culty objectively seems far too pragmatic for the
French spirit). It follows that there is little cur-
rent research in this field: in Belgium, the Flesch
formula is still used to assess the readability of
articles in journalism studies. This example also
shows that the French-specific formulae are not
much used, probably because of their complexity
(Bosse?-Andrieu, 1993).
Of course, if there is little work on French read-
ability, there is even less on French as a foreign
language. We only know the study of Cornaire
(1988), which tested the adaptation of Henry?s
short formula to French as a foreign language,
and that of Uitdenbogerd (2005), which developed
a new measure for English-speaking learners of
French, stressing the importance of cognates when
developing a new formula for a related language.
Therefore, we had to draw our inspiration from
the English-speaking world, which has recently
experienced a revival of interest in research on
readability. Taking advantage of the increasing
power of computers and the development of NLP
techniques, researchers have been able to exper-
iment with more complex variables. Collins-
Thompson et al (2005) presented a variation of a
multinomial naive Bayesian classifier they called
the ?Smoothed Unigram? model. We retained
from their work the use of language models in-
stead of word lists to measure lexical complex-
ity. Schwarm and Ostendorf (2005) developed
a SVM categoriser combining a classifier based
on trigram language models (one for each level
of difficulty), some parsing features such as av-
erage tree height, and variables traditionally used
in readability. Heilman et al (2007) extended the
?Smoothed Unigram? model by the recognition of
syntactic structures, in order to assess L2 English
texts. Later, they improved the combination of
their various lexical and grammatical features us-
ing regression methods (Heilman et al, 2008). We
also found regression methods to be the most ef-
ficient of the statistical models with which we ex-
perimented. In this article, we consider some ways
to adapt these various ideas to the specific case of
FFL readability.
3 Corpus description
In the development of a new readability formula,
the first step is to collect a corpus labelled by
reading-difficulty level, a task that implies agree-
ment on the difficulty scale. In the US, a com-
mon choice is the 12 American grade levels corre-
sponding to primary and secondary school. How-
ever, this scale is less relevant for FFL education
in Europe. So, we looked for another scale.
Given that we are looking for an automatic way
of measuring text complexity for FFL learners par-
ticipating in an educational programme, an obvi-
ous choice was the difficulty scale used for assess-
ing students? levels in Europe, that is the Com-
mon European Framework of Reference for Lan-
guages (CEFR) (Council of Europe, 2001) . The
CEFR has six levels: A1 (Breakthrough); A2
(Waystage); B1 (Threshold); B2 (Vantage); C1
(Effective Operational Proficiency) and C2 (Mas-
tery). However differences in learners? skills can
be quite substantial at lower levels, so we divided
each of the A1, A2 and B1 grades in two, thus ob-
taining a total of nine levels.
We still needed to find a corpus labelled accord-
ing to these nine classes. Unlike traditional ap-
proaches, based on a limited set of texts usually
standardised by applying a closure test to a target
population, our NLP-oriented approach required a
large number of texts on which the statistical mod-
els could be trained. For that reason we opted for
FFL textbooks as a corpus. With the appearance of
the CEFR, FFL textbooks have undergone a kind
of standardisation and their levels have been clari-
fied. It is thus feasible to gather a large number of
documents already labelled in terms of the CEFR
scale by experts with an educational background.
However, not every textbook can be used as a
document source. Likewise, not all the material
from FFL textbooks is appropriate. We established
the following criteria for selecting textbooks and
texts:
? The CEFR was published in 2001, so only
20
textbooks published since then were con-
sidered. This restriction also ensures that
the language resembles present-day spoken
French.
? The target population for our formula is
young people and adults. Therefore, only
textbooks intended for this public were used.
? We retained only those texts made up of com-
plete sentences, linked to a reading compre-
hension task. So, all the transcriptions of
listening comprehension tasks were ignored.
Similarly, all instructions to the students were
excluded, because there is no guarantee the
language employed there is the same as the
rest of the textbook material (metalinguistic
terms and so on can be found there).
Up to now, using these criteria, we have gath-
ered more than 1,500 documents containing about
440,000 tokens. Texts cover a wide variety of sub-
jects ranging from French literature to newspaper
articles, as well as numerous dialogues, extracts
from plays, cooking recipes, etc. The goal is to
have as wide a coverage as possible, to achieve
maximum generalisability of the formula, and also
to check what sort of texts it does not fit (e.g. sta-
tistical descriptive analyses have considered songs
and poems as outliers).
4 Selection of lexical and syntactic
variables
Any text classification tasks require an object
(here a text) to be parameterised into variables,
whether qualitative or quantitative. These inde-
pendent variables must correlate as strongly as
possible with the dependent variable represent-
ing difficulty in order to explain the text?s com-
plexity, and they should also account for the var-
ious dimensions of the readability phenomenon.
Traditional approaches to readability have been
sharply criticised with respect to this second re-
quirement by Kintsch and Vipond (1979) and
Kemper (1983), who both insist on the impor-
tance of including the conceptual properties of
texts (such as the relations between propositions
and the ?inference load?). However, these new
approaches have not resulted in any easily repro-
ducible computational models, leading current re-
searchers to continue to use the classic semantic
and grammatical variables, enhancing them with
NLP techniques.
Because this research only spans the last year,
attempts to discover interesting variables are still
at an early stage. We explored the efficiency of
some traditional features such as the type-token
ratio, the number of letters per word, and the av-
erage sentence length, and found that, on our cor-
pus, only the word length and sentence length cor-
related significantly with difficulty. Then, we add
two NLP-oriented features, as described below: a
statistical language model and a measure of tense
difficulty.
4.1 The language model
The lexical difficulty of a text is quite an elaborate
phenomenon to parameterise. The logistic regres-
sion models we used in this study require us to re-
duce this complex reality to just one number, the
challenge being to achieve the most informative
number. Some psychological work (Howes and
Solomon, 1951; Gerhand and Barry, 1998; Brys-
baert et al, 2000) suggests that there is a strong re-
lationship between the frequency of words and the
speed with which they are recognised. We there-
fore opted to model the lexical difficulty for read-
ing as the global probability of a text T (with N
tokens) occurring:
P (T ) = P (t1)P (t2 | t1)
? ? ?P (tn | t1, t2, . . . , tn?1) (1)
This equation raises two issues :
1. Estimating the conditional probabilities. It
is well-known that it is impossible to train
such a model on a corpus, even the largest
one, because some sequences in this equa-
tion are unlikely to be encountered more than
once. However, following Collins-Thompson
and Callan (2005), we found that a simple
smoothed unigram model could give good re-
sults for readability. Thus, we assumed that
the global probability of a text T could be re-
duced to:
P (T ) =
n
?
i=1
p(ti) (2)
where p(ti) is the probability of meeting the
token ti in French; and n is the number of
tokens in a text.
2. Deciding what is the best linguistic unit to
consider. The equations introduced above use
21
tokens, as is traditional in readability formu-
lae, but the inflected nature of French sug-
gests that lemmas may be a better alternative.
Using tokens means that words taking numer-
ous inflected forms (such as verbs), have their
overall probability split between these differ-
ent forms. Consequently, compared to sel-
dom ? or never ? inflected words (such as ad-
verbs, prepositions, conjunctions), they seem
less frequent than they really are. Second, us-
ing tokens presupposes a theoretical position
according to which learners are not able to
link an inflected form with its lemma. Such
a view seems highly questionable for the ma-
jority of regular forms.
In order to settle this issue, we trained three
language models: one with lemmas (LM1),
another with inflected forms disambiguated
according to their tags (LM2), and a third
one with inflected forms (LM3). The ex-
periment was not very conclusive, since the
models all correlated with the dependent vari-
able to a similar extent, having Pearson?s r
coefficients of ?0.58, ?0.58, and ?0.59 re-
spectively. However, three factors militate in
favour of the lemma model: as well as the-
oretical likelihood, it is the model which is
most sensitive to outliers and most prone to
measurement error. This suggests that, if we
can reduce this error, the lemma model may
prove to be the best predictor of the three.
As a consequence of these considerations, we
decided to compute the difficulty of the text by us-
ing Equation 2 adapted for lemmas and, for com-
putational reasons, the logarithm of the probabili-
ties:
P (T ) = exp(
n
?
i=1
log[p(lemi)]) (3)
The resulting value is still correlated with the
length of the text, so it has to be normalised by
dividing it by N (the number of words in the text).
These operations give in a final value suitable for
the logistic regression model. More information
about the origin and smoothing of the probabilities
is given in Section 6.
4.2 Measuring the tense difficulty
Having considered the complexity of a text?s syn-
tactic structures through the traditional factor of
the ?mean number of words per sentence?, we de-
cided to also take into account the difficulty of
the conjugation of the verbs in the text. For this
purpose, we created 11 variables, each represent-
ing one tense or class of tenses: conditional, fu-
ture, imperative, imperfect, infinitive, past partici-
ple, present participle, present, simple past, sub-
junctive present and subjunctive imperfect.
The question then arose as to whether it would
be better to treat these variables as binary or con-
tinuous. Theoretical justifications for a binary pa-
rameterisation lie in the fact that a text becomes
more complex for a L2 language learner when
there is a large variety of tenses, especially dif-
ficult ones. The proportion of each tense seems
less significant. For this reason, we opted for bi-
nary variables. The other way of parameterising
the data should nevertheless be tested in further
research.
5 The regression models
By the end of the parameterisation stage, each text
of the corpus has been reduced to a vector com-
prising the 14 following predictive variables : the
result of the language model, the average number
of letters per word1, the average number of words
per sentence and the 11 binary variables for tense
complexity.
Each vector also has a label representing the
level of the text, which is the dependent variable
in our classification problem. From a statisti-
cal perspective, this variable may be considered
as a nominal, ordinal, or interval variable, each
level of measurement being linked to a particu-
lar regression technique: multiple linear regres-
sion for interval data; a popular cumulative logit
model called proportional odds for ordinal data;
and multinomial logistic regression for nominal
variables. Therefore, identifying the best scale of
measurement is an important issue for readability.
From a theoretical perspective, viewing the lev-
els of difficulty as an interval scale would imply
that they are ordered and evenly spaced. How-
ever, most FFL teachers would disagree with this
assumption: it is well known that the higher levels
take longer to complete than the earlier ones. So, a
more realistic position is to consider text difficulty
as an ordinal variable (since the CEFR levels are
1Pearson?s r coefficient between the language model and
the average number of letters in the words was ?0.68. This
suggests that there is some independent information in the
length of the words that can be used for prediction.
22
ordered). The third alternative, treating the levels
as a nominal scale, is not intuitively obvious to a
language teacher, because it suggests that there is
no particular order to the CEFR levels.
From a practical perspective, things are not so
clear. Traditional approaches have usually viewed
difficulty as an interval scale and applied mul-
tiple linear regression. Recent NLP perspective
have either considered difficulty as an ordinal vari-
able (Heilman et al, 2008), making use of logis-
tic regression, or as a nominal one, implementing
classifiers such as the naive Bayes, SVM or deci-
sion tree. Such a variety of practices convinced us
that we should experiment with all three scales of
measurement.
In an exploratory phase, we compared regres-
sion methods and decision tree classifiers on the
same corpus. We found that regression was more
precise and more robust, due to the current lim-
ited size of the corpus. Linear regression was
discarded because it gave poor results during the
test phase. So we retained two logistic regression
models, the PO model and the MLR model, which
are presented in the next section.
5.1 Proportional odds (PO) model
Logistic regression is a statistical technique first
developed for binary data. It generally de-
scribes the probability of a 0 or 1 outcome with
an S-shaped logistic function (see Hosmer and
Lemeshow (1989) for details). Adaptation of the
logistic regression for J ordinal classes involves
a model with J ? 1 response curves of the same
shape. For a fixed class j, each of these response
functions is comparable to a logistic regression
curve for a binary response with outcomes Y ? j
and Y > j (Agresti, 2002), where Y is the depen-
dent variable.
The PO model can be expressed as:
logit[P (Y ? j | x)] = ?j + ??x (4)
In Equation 4, x is the vector containing the inde-
pendent variables, ?j is the intercept parameter for
the jth level and ? is the vector of regression co-
efficients. From this formula, the particularity of
the PO model can be observed: it has the same set,
?, of parameters for each level. So, the response
functions only differ in their intercepts, ?j . This
simplification is only possible under the assump-
tion of ordinality.
Using this cumulative model, when 2 ? j ? J ,
the estimated probability of a text Y belonging to
the class j can be computed as:
P (Y = j | x) = logit[P (Y ? j | x)]
?logit[P (Y ? j ? 1 | x)] (5)
When j = 1, P (Y = 1 | x) is equal to P (Y ? j |
x).
We said above that this model involves a simpli-
fication, based on the proportional odds assump-
tion. This assumption needs to be tested with the
chi-squared form of the score test (Agresti, 2002).
The lower the chi-squared value, the better the PO
model fits the data.
5.2 Multinomial logistic regression
Multinomial logistic regression is also called
?baseline category?, because it compares each
class Y with a reference category, often the first
one (Y1), in order to regress to the binary case.
Each pair of classes (Yj , Y1) can then be described
by the ratio (Agresti, 2002, p. 268):
logP (Y = j | x)P (Y = 1 | x) = ?j + ?j
?x (6)
where the notation is as given above. On the ba-
sis of these J-1 regression equations, it is possible
to compute the probability of a text belonging to
difficulty level j using the values of its features
contained in the vector x. This may be calculated
using the equation (Agresti, 2002, p. 271):
P (Y = j | x) =
exp(?j + ?j ?x)
1 + ?Jh=2 exp(?h + ?j ?x)
(7)
Notice that for the baseline category (here, j = 1),
?1 and ?1 = 0. Thus, when looking for the proba-
bility of a text belonging to the baseline level, it is
easy to compute the numerator, since exp(0) = 1.
The value of the denominator is the same for each
j.
Heilman et al (2008) drew attention to the fact
that the MLR model multiplies the number of pa-
rameters by J ? 1 compared to the PO model.
Because of this, they recommend using the PO
model.
6 Implementation of the models
Having covered the theoretical aspects of our
model, we will now describe some of the partic-
ularities of our implementation.
23
6.1 The language model: probabilities and
smoothing
For our language model, we need a list of French
lemmas with their frequencies of occurrence. Get-
ting robust estimates for a large number of lem-
mas requires a very large corpus and is a time-
consuming process. We used Lexique3, a lexicon
provided by New et al (2001) and developed from
two corpora: the literary corpus Frantext contain-
ing about 15 million of words; and a corpus of film
subtitles (New et al, 2007), with about 50 million
words. The authors drew up a list of more than
50,000 tagged lemmas, each of which is associ-
ated with two frequency estimates, one from each
corpus.
We decided to use the frequencies from the sub-
title corpus, because we think it gives a more ac-
curate image of everyday language, which is the
language FFL teaching is mainly concerned with.
The frequencies were changed into probabilities,
and smoothed with the Simple Good-Turing al-
gorithm described by Gale and Sampson (1995).
This step is necessary to solve another well-known
problem in language models: the appearance in
a new text of previously unseen lemmas. In this
case, since the logarithm of probabilities is used,
an unseen lemma would result in a infinite value.
In order to prevent this, a smoothing process is
used to shift some of the model?s probability mass
from seen lemmas to unseen ones.
Once we had obtained a good estimate of the
probabilities, we could analyse the texts in the cor-
pus. Each of them was lemmatised and tagged us-
ing the TreeTagger (Schmid, 1994). This NLP tool
allows us to distinguish between homographs that
can represent different levels of difficulty. For in-
stance, the word actif is quite common as an ad-
jective, but the noun is infrequent and is only used
in the business lexicon. This distinction is possible
because Lexique3 provides tagged lemmas.
6.2 Variable selection
Having gathered the values for the 14 dependent
variables, it was possible to train the two statis-
tical models.2 However, an essential requirement
prior to training is feature selection. This proce-
dure, described by Hosmer and Lemeshow (1989),
consists of examining models with one, two, three,
2All statistical computations were performed with the
MASS package (Venables and Ripley, 2002) of the R soft-
ware.
etc., variables and comparing them to the full
model according to some specified criteria so as
to select one that is both efficient and parsimo-
nious. For logistic regression, the criterion se-
lected is the AIC (Akaike?s Information Criterion)
of the model. This can be obtained from:
AIC = ?2log-likelihood + 2k (8)
where k is the number of parameters in the model,
and the log-likelihood value is the result of a calcu-
lation detailed by Hosmer and Lemeshow (1989).
We applied the stepwise algorithm to our data,
trying both a backward and a forward procedure.
They converged to a simpler model containing
only 10 variables: the value obtained from our lan-
guage model, the number of letters per word, the
number of words per sentence, the past participle,
the present participle, and the imperfect, infinitive,
conditional, future and present subjunctive tenses.
Presumable the imperative and present tenses are
so common that they do not have much discrim-
inative power. On the other hand, the imperfect
subjunctive is so unusual that it is not useful for a
classification task. However, the non-appearance
of the simple past is surprising, since it is a nar-
rative tense which is not usually introduced until
an advanced stage in the learning of French. This
phenomenon deserves further investigation in the
future.
7 First results
To the best of our knowledge, no one has pre-
viously applied NLP technologies to the specific
issue of the readability of texts for FFL learn-
ers. So, any comparisons with previous studies are
somewhat flawed by the fact that neither the target
population nor the scale of difficulty is the same.
However, our results can be roughly compared to
some of the numerous studies on L1 English read-
ability presented in Section 2. Before making this
comparison, we will analyse the predictive ability
of the two models.
7.1 Models evaluation
The evaluation measures most commonly em-
ployed in the literature are Pearson?s product-
moment correlation coefficient, prediction accu-
racy as defined by Tan et al (2005), and adjacent
accuracy. Adjacent accuracy is defined by Heil-
man et al (2008) as ?the proportion of predictions
that were within one level of the human-assigned
24
Measure PO model MLR model
Results on training folds
Correl. 0.786 0.777
Exact Acc. 32.5% 38%
Adj. Acc. 70% 71.3%
Results on test folds
Correl. 0.783 0.772
Exact Acc. 32.4% 38%
Adj. Acc. 70% 71.2%
Table 1: Mean Pearson?s r coefficient, exact and
adjacent accuracies for both models with the ten-
fold cross-validation evaluation.
label for the given text?. They defended this mea-
sure by arguing that even human-assigned reading
levels are not always consistent. Nevertheless, it
should not be forgotten that it can give optimistic
values when the number of classes is small.
Exploratory analysis of the corpus highlighted
the importance of having a similar number of texts
per class. This requirement made it impossible
to use all the texts from the corpus. Some 465
texts were selected, distributed across the 9 levels
in such a way that each level contained about 50
texts. Within each class, an automatic procedure
discarded outliers located more than 3? from the
mean, leaving 440 texts. Both models were trained
on these texts.
The results on the training corpus were promis-
ing, but might be biased. So, we turned to a
ten-fold cross-validation process which guarantees
more reliable values for the three evaluation mea-
sures we had chosen, as well as a better insight
into the generalisability of the two models. The
resulting evaluation measures for training and test
folds are shown in Table 1. The similarity between
them clearly shows that, with 440 observations,
both the models were quite robust. On this corpus,
multinomial logistic regression was significantly
more accurate (with 38% of texts correctly classi-
fied against 32.4% for the PO model), while Pear-
son?s R was slightly higher for the PO model.
These results suggest that the exact accuracy
may be a better indicator of performance than the
correlation coefficient. However they conflict with
Heilman et al?s (2008) conclusion that the PO
model performed better than the MLR one. This
discrepancy might arise because the PO model
was less accurate for exact predictions, but better
when the adjacent accuracy by level was taken into
account. However, the data in Table 2 do not sup-
port this hypothesis; rather they confirm the supe-
riority of the MLR model when adjacent accuracy
is considered. In fact, PO model?s lower perfor-
mance seems to be due to a lack of fit to the data,
as revealed by the result of the score test for the
proportional-odds assumption. This yielded a p-
value below 0.0001, clearly showing that the PO
model was not a good fit to the corpus.
There remains one last issue to be discussed be-
fore comparing our results to those of other stud-
ies: the empirical evidence for tense being a good
predictor of reading difficulty. We selected tenses
because of our experience as FLE teacher rather
than on theoretical or empirical grounds. How-
ever we found that exact accuracy decreased by
10% when the tense variables were omitted from
the models. Further analysis showed that the tense
contributed significantly to the adjacent accuracy
of classifying the C1 and C2 texts.
7.2 Comparison with other studies
As stated above, it is not easy to compare our
results with those of previous studies, since the
scale, population of interest and often the lan-
guage are different. Furthermore, up till now, we
have not been able to run the classical formu-
lae for French (such as de Landsheere (1963) or
Henry (1975)) on our corpus. So we are limited to
comparing our evaluation measures with those in
the published literature.
With multinomial logistic regression, we ob-
tained a mean adjacent accuracy of 71% for 9
classes. This result seems quite good compared
to similar research on L1 English by Heilman et
al. (2008). Using more complex syntactic fea-
tures, they obtained an adjacent accuracy of 52%
with a PO model, and 45% with a MLR model.
However, they worked with 12 levels, which may
explain their lower percentage.
For French, Collins-Thompson and Callan
(2005) reported a Pearson?s R coefficient of 0.64
for a 5-classes naive Bayes classifier while we ob-
tained 0.77 for 9 levels with MLR. This differ-
ence might be explained by the tagging or the use
of better-estimated probabilities for the language
model. Further research on this point to determine
the specificities of an efficient approach to French
readability appears very promising.
25
Level A1 A1+ A2 A2+ B1 B1+ B2 C1 C2 Mean
PO model 91% 91% 67% 68% 53% 55% 56% 86% 68% 70%
MLR model 93% 90% 69% 51% 59% 56% 64% 88% 73% 71%
Table 2: Mean adjacent accuracy per level for PO model and MLR model (on the test folds).
8 Discussion and future research
This paper has proposed the first readability ?for-
mula? for French as a foreign language using NLP
and statistical models. It takes into account some
particularities of French such as its inflected na-
ture. A new scale to assess FFL texts within the
CECR framework, and a new criteria for the cor-
pus involving the use of textbooks, have also been
proposed. The two logistic models applied to a
440-text corpus gave results consistent with the lit-
erature. They also showed the superiority of the
MLR model over the PO model. Since Heilman
et al (2008) found the opposite, and the intuitive
view is that levels should be described by an ordi-
nal scale of measurement, this issue clearly needs
further investigation.
This research is still in progress, and further
analyses are planned. The predictive capacity of
some other lexical and grammatical features will
be explored. At the lexical level, statistical lan-
guage models seems to be best, and tagging the
texts to work with lemmas turned out to be effi-
cient for French, although it has not been shown
to be superior to disambiguated inflected forms.
Moreover, due to their higher sensibility to con-
text, smoothed n-grams might represent an alter-
native to lemmas.
Once the best unit has been selected, some
other issues remain: it is not clear whether a
model using the probabilities of this unit in the
whole language or probabilities per level (Collins-
Thompson and Callan, 2005) would be more ef-
ficient. We also wonder whether the L1 frequen-
cies of words are similar to those in L2 ? FFL
textbooks use a controlled vocabulary, linked to
specific situational tasks, which suggests that it is
highly possible that the frequencies of words in
FFL differ from those in mother-tongue French.
Grammatical features have been taken into ac-
count through simple parameterisation. More
complex measures (such as the presence of some
syntactic structures (Heilman et al, 2007) or the
characteristics of a syntactic-parsing tree) have
been explored in the literature. We hope that in-
cluding such factors may result in improved accu-
racy for our model. However, these techniques are
probably dependent on the quality of the parser?s
results. Parsers for French are less accurate than
those for English, which may generate some noise
in the analysis.
Finally, we intend to explore the performance
of other classification techniques. Logistic regres-
sion was the most efficient of the statistical mod-
els we tested, but as our corpus grows, more and
more data is becoming available, and data min-
ing approaches may become applicable to the text-
categorization problem for FFL readability. Sup-
port vector machines have already been shown to
be useful for readability purposes (Schwarm and
Ostendorf, 2005). We also want to try aggregating
approaches such as boosting, bagging, and random
forests (Breiman, 2001), since they claim to be ef-
fective when the sample is not perfectly represen-
tative of the population (which could be true for
our data). These analyses would aim to illuminate
some of the assets and flaws of each of the statis-
tical models considered.
Acknowledgments
Thomas L. Franc?ois is supported by the Bel-
gian Fund for Scientific Research (FNRS), as is
the research programme from which this material
comes.
I would like to thank my directors, Prof.
Ce?drick Fairon and Prof. Anne-Catherine Simon,
my colleagues, Laure Cuignet and the anonymous
reviewers for their valuable comments.
References
Alan Agresti. 2002. Categorical Data Analysis. 2nd
edition. Wiley-Interscience, New York.
J. Bosse?-Andrieu. 1993. La question de la lisi-
bilite? dans les pays anglophones et les pays fran-
cophones. Technostyle, Association canadienne des
professeurs de re?daction technique et scientifique,
11(2):73?85.
L. Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
26
M. Brysbaert, M. Lange, and I. Van Wijnendaele.
2000. The effects of age-of-acquisition and
frequency-of-occurrence in visual word recognition:
Further evidence from the Dutch language. Euro-
pean Journal of Cognitive Psychology, 12(1):65?85.
J.S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brook-
line Books, Cambridge.
K. Collins-Thompson and J. Callan. 2005. Predict-
ing reading difficulty with statistical language mod-
els. Journal of the American Society for Information
Science and Technology, 56(13):1448?1462.
A. Conquet. 1971. La lisibilite?. Assemble?e Perma-
nente des CCI de Paris, Paris.
C.M. Cornaire. 1988. La lisibilite? : essai d?application
de la formule courte d?Henry au franc?ais langue
e?trange`re. Canadian Modern Language Review,
44(2):261?273.
Council of Europe and Education Committee and
Council for Cultural Co-operation. 2001. Common
European Framework of Reference for Languages:
Learning, Teaching, Assessment. Press Syndicate of
the University of Cambridge.
G. De Landsheere. 1963. Pour une application des
tests de lisibilite? de Flesch a` la langue franc?aise. Le
Travail Humain, 26:141?154.
R. Flesch. 1948. A new readability yardstick. Journal
of Applied Psychology, 32(3):221?233.
W.A. Gale and G. Sampson. 1995. Good-Turing fre-
quency estimation without tears. Journal of Quanti-
tative Linguistics, 2(3):217?237.
S. Gerhand and C. Barry. 1998. Word frequency
effects in oral reading are not merely age-of-
acquisition effects in disguise. Journal of Experi-
mental Psychology. Learning, Memory, and Cogni-
tion, 24(2):267?283.
M. Heilman, K. Collins-Thompson, J. Callan, and
M. Eskenazi. 2007. Combining lexical and gram-
matical features to improve readability measures for
first and second language texts. In Proceedings of
NAACL HLT, pages 460?467.
M. Heilman, K. Collins-Thompson, and M. Eskenazi.
2008. An analysis of statistical models and fea-
tures for reading difficulty prediction. Association
for Computational Linguistics, The 3rd Workshop
on Innovative Use of NLP for Building Educational
Applications:1?8.
G. Henry. 1975. Comment mesurer la lisibilite?. Labor.
D.W. Hosmer and S. Lemeshow. 1989. Applied Logis-
tic Regression. Wiley, New York.
D.H. Howes and R.L. Solomon. 1951. Visual duration
threshold as a function of word probability. Journal
of Experimental Psychology, 41(40):1?4.
L. Kandel and A. Moles. 1958. Application de l?indice
de Flesch a` la langue franc?aise. Cahiers ?Etudes de
Radio-Te?le?vision, 19:253?274.
S. Kemper. 1983. Measuring the inference load
of a text. Journal of Educational Psychology,
75(3):391?401.
J. Kincaid, R.P. Fishburne, R. Rodgers, and
B. Chissom. 1975. Derivation of new read-
ability formulas for navy enlisted personnel.
Research Branch Report, 85.
W. Kintsch and D. Vipond. 1979. Reading compre-
hension and readability in educational practice and
psychological theory. Perspectives on Memory Re-
search, pages 329?366.
B.A. Lively and S.L. Pressey. 1923. A method for
measuring the vocabulary burden of textbooks. Ed-
ucational Administration and Supervision, 9:389?
398.
J. Mesnager. 1989. Lisibilite? des textes pour en-
fants: un nouvel outil? Communication et Lan-
gages, 79:18?38.
B. New, C. Pallier, L. Ferrand, and R. Matos. 2001.
Une base de donne?es lexicales du franc?ais con-
temporain sur internet: LEXIQUE. LAnne?e Psy-
chologique, 101:447?462.
B. New, M. Brysbaert, J. Veronis, and C. Pallier. 2007.
The use of film subtitles to estimate word frequen-
cies. Applied Psycholinguistics, 28(04):661?677.
F. Richaudeau. 1979. Une nouvelle formule de lisi-
bilite?. Communication et Langages, 44:5?26.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of International
Conference on New Methods in Language Process-
ing, volume 12. Manchester, UK.
S.E. Schwarm and M. Ostendorf. 2005. Reading level
assessment using support vector machines and sta-
tistical language models. Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 523?530.
P.-N. Tan, M. Steinbach, and V. Kumar. 2005. Intro-
duction to Data Mining. Addison-Wesley, Boston.
S. Uitdenbogerd. 2005. Readability of French as a
foreign language and its uses. In Proceedings of the
Australian Document Computing Symposium, pages
19?25.
W.N. Venables and B.D. Ripley. 2002. Modern Ap-
plied Statistics with S. Springer, New York.
27
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 466?477, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
An ?AI readability? formula for French as a foreign language
Thomas Franc?ois
IRCS, University of Pennsylvania
3401 Walnut Street Suite 400A Room 423
Philadelphia, PA 19104, USA
frthomas@sas.upenn.edu
Ce?drick Fairon
CENTAL, UCLouvain
Place Blaise Pascal, 1
1348 Louvain-la-Neuve, Belgium
Cedrick.Fairon@uclouvain.be
Abstract
This paper present a new readability formula
for French as a foreign language (FFL), which
relies on 46 textual features representative of
the lexical, syntactic, and semantic levels as
well as some of the specificities of the FFL
context. We report comparisons between sev-
eral techniques for feature selection and var-
ious learning algorithms. Our best model,
based on support vector machines (SVM), sig-
nificantly outperforms previous FFL formulas.
We also found that semantic features behave
poorly in our case, in contrast with some pre-
vious readability studies on English as a first
language.
1 Introduction
Whether in a first language (L1) or a second and for-
eign language (L2), learning to read has been and re-
mains one of the major concerns of education. When
a teacher wants to improve his/her students? reading
skills, he/she uses reading exercises, whether there
are guided or independent. For this practice to be
efficient, it is necessary that the texts suit the level
of students (O?Connor et al2002). This condition
is sometimes difficult to meet for teachers wishing
to get off the beaten tracks by not using texts from
levelled textbooks or readers.
In this context, readability formulas have long
been used to help teachers faster select texts for their
students. These formulas are reproducible meth-
ods that aim at matching readers and texts relative
to their reading difficulty level. The Flesch (1948)
and Dale and Chall (1948) formulas are probably
the best-known examples of those. They are typical
of classic formulas, the first major methodological
paradigm developed in the field during the 40?s and
50?s. They were kept as parsimonious as possible,
using linear regression to combined two, or some-
times, three surface features, such as word mean
length, sentence mean length, or proportion of out-
of-simple-vocabulary words.
Later, some scholars (Kintsch and Vipond, 1979;
Redish and Selzer, 1985) argued that the classic for-
mulas suffer from several shortcomings. These for-
mulas only take into account superficial features, ig-
noring other important aspects contributing to text
difficulty, such as coherence, content density, infer-
ence load, etc. They also omit the interactive as-
pect of the reading process. In the 80?s, a second
paradigm, inspired by structuro-cognitivist theories,
intended to overcome these issues. It focused on
higher textual dimensions, such as inference load
(Kintsch and Vipond, 1979; Kemper, 1983), den-
sity of concepts (Kintsch and Vipond, 1979), or
macrostructure (Meyer, 1982). However, these at-
tempts did not achieve better results than the clas-
sic approach, even though they used more principled
and more complex features.
Recently, a third paradigm, referred to as the ?AI
readability? by Franc?ois (2011a), has emerged in the
field. Studies that are part of this current share three
key features: the use of a large number of texts as-
sessed by experts (coming from textbooks, simpli-
fied newspapers or web resources) as training data ;
the use of NPL-enable features able to capture a
wider range of readability factors, and the combi-
nation of those features through a machine learning
466
algorithm. Since the work of Si and Callan (2001),
this paradigm have spawn several studies for English
(Collins-Thompson and Callan, 2005; Heilman et
al., 2008; Schwarm and Ostendorf, 2005; Feng et
al., 2010).
However, for French, the field is far from being so
thriving. To our knowledge, only two ?AI readabil-
ity? have been designed so far for French L1 and
only one for French as a foreign language (FFL)
(see Section 2). This paper reports some experi-
ments aimed at designing a more efficient readabil-
ity model for FFL. In Section 2, it is further argue
why a new formula was necessary for FFL. Section
3 covers the various methodological steps required
to devise the model, whose results are reported in
Section 4. Finally, Section 5 discusses some inter-
esting insights gained by this work.
2 Readability models for French
Readability of French never enjoyed a large suc-
cess: while readability studies on English dates back
to the 20?s, it is only in 1957 that the French-
speaking world discovered it through the work of
Conquet (1957). Since then, only a few studies fo-
cused on the topic.
The two first French L1 formulas were adap-
tations of the Flesch formula (Kandel and Moles,
1958; de Landsheere, 1963). It is only with
Henry (1975) that French got a model fitting the
particularities of the language. Henry used cloze
tests to assess the level of 60 texts from primary and
secondary school textbooks and trained three for-
mulas on this corpus. It is worth mentioning that
Henry?s formulas have been applied to FFL by Cor-
naire (1988). During the same time, Richaudeau
explored a different path, as a representative of the
structuro-cognitivist paradigm. He used the num-
ber of words recalled by a subject after he/she has
just read a sentence as a device to measure under-
standing and provided an ?efficiency formula? of
texts (Richaudeau, 1979). Although more modern
in its conception, Richaudeau?s hard-to-implement
formula did not achieve the same recognition in the
French speaking world as Henry?s.
After those two major efforts, few works fol-
lowed. It is worth mentioning two more authors:
Mesnager (1989), who designed a classic formula
for children that draw inspiration from the Dale and
Chall (1948) formula and Daoust et al1996), who
developed SATO-CALIBRAGE, a program assessing
text difficulty from the first to the eleventh grade.
It can be considered as the first ?AI formula? for
French L1, since it made use of NLP-enabled fea-
tures. It is also the last formula published for French
L1, if we except the adaptation of the model by
Collins-Thompson and Callan (2004) to French.
As regards to French L2, the literature is even
sparser. Tharp (1939) published a first formula tak-
ing into account one particularity of the L2 context:
the cognates. Those are words sharing a similar
form and meaning across two languages and hav-
ing a facilitating effect in reading. This idea was re-
cently replicated by Uitdenbogerd (2005), who com-
bined a syntactic feature, the mean number of words
per sentence, with the number of cognates per 100
words in her formula. Although taking into account
this effect of the L1 on L2 reading is very interest-
ing, these two studies are confined to a limited audi-
ence: English speakers learning French. As regards
a more generic approach, Franc?ois (2009) recently
published an ?AI formula? for FFL, based on lo-
gistic regression and ten features. Among those, he
stressed the use of verbal tense information as a way
to improve performance. However, the set of fea-
tures he experimented remains limited (about 20).
From all this, it seems clear that FFL readability
needs to be addressed more thoroughly, especially if
we are willing to get a generic model, able to make
predictions for L2 readers with any L1 background.
The rest of this paper describes one such attempt.
3 Design of the formula
The design of an ?AI readability? formula involves
the same three steps as a classification problem.
First, one need to gather a gold-standard corpus
large enough to reliably train the parameters of a
learning algorithm, as described in Section 3.1. The
next step, covered in Section 3.2, consists in defin-
ing a set of predictors, that is to say, linguistic char-
acteristics of the texts that will be used to predict the
difficulty level of new texts. Finally, the best sub-
set of these predictors is combined within a learning
algorithm to obtain the best model possible. Experi-
ments at this level are reported in Section 3.3.
467
3.1 The corpus
A gold-standard for readability consists in texts la-
belled according to their difficulty. For this, it is first
necessary to choose a difficulty scale used for the la-
bels (for English L1, it is usually the 12 grade levels
scale), that also constrains the output of the formula.
Then, each text have to be assessed with a method
able to measure the reading comprehension level of
the target population.
Regarding the scale, an obvious choice for
the foreign language context was the begin-
ner/intermediate/advanced continuum, recently re-
defined in the Common European Framework of
Reference for Languages (CEFR) (Council of Eu-
rope, 2001) as the six following levels: A1 (Break-
through); A2 (Waystage); B1 (Threshold); B2 (Van-
tage); C1 (Effective Operational Proficiency) and C2
(Mastery). This scale has now become the reference
for foreign language education, at least in Europe.
Assessing the reading difficulty of texts with re-
spect to a target population of readers was a more
challenging issue. Several techniques have been
used in the literature, the most important of which
are comprehension tests, cloze tests and expert
judgements. They all postulate a given population of
readers, although relying on expert judgements save
the need for a sample of subjects to take a test. In
this case, texts comes from textbooks whose content
difficulty have been assessed by the publishers.
This last criterion is now mainstream in ?AI read-
ability?, since it is very practical and facilitates the
creation of a large corpus, but it has its own short-
comings. Studies such as van Oosten et al2011)
found that expert agreement on a same corpus of
texts might be insufficient for a classification task.
For this study, we nevertheless relied on expert
judgements, since we needed a large amount of la-
belled texts to ensure a robust statistical learning.
We selected 28 FFL textbooks, published after 2001
and designed for adults or adolescents learning FFL
for general purposes. From those, we extracted
2,160 texts related to a reading comprehension task
and assigned to each of them the same level as the
textbook it came from.
As it was expected from van Oosten et al2011)?s
study, differences in the publishers? conception of
difficulty led to an heterogeneous labelling between
textbooks. This heterogeneity was detected in three
of the six levels (A1, A2, and B1) using ANOVA
based on two classic readability features as inde-
pendent variables: the mean number of words per
sentence and the mean number of letters per word.
A subsequent qualitative analysis revealed that most
of the heterogeneity was coming from textbooks fol-
lowing the new didactic approach recommended by
the CEFR: the task-oriented approach, which fo-
cuses more on the task than the text when labelling
the overall reading activity. Therefore, we decided
to remove those type of textbooks from our corpus,
which amounted to 5 books and 249 texts. The re-
maining 1,852 excerpts were kept for our experi-
ments. Their distribution is displayed in Table 1 as
regard to the number of texts and tokens.
3.2 The predictors
In a second step, every text of the corpus was rep-
resented as a numeric vector of 406 features, each
of them representing a linguistic dimension of the
text as a single number. Their implementation drew
on two different sources of inspiration: the existing
predictors in the English and French literature and
the psycholinguistic literature on the reading pro-
cess. The complete set was classified in four fam-
ilies, depending on the kind of information each one
is supposed to represent. These families were: ?lex-
ical?, ?syntactic?, ?semantic?, and ?specific to FFL
context?. Each of them was further divided in sub-
families, described in the rest of the section 1.
3.2.1 Lexical Features
Lexical features have been shown to be the most
important level of information in many readability
studies (Chall and Dale, 1995; Lorge, 1944). It is
then not surprising that a wide range of lexical pre-
dictors have been developed in the literature. Our
own set comprised the following subfamilies:
Statistics of lexical frequencies: frequencies of
words in a text are a good indicator of the text?s over-
all difficulty (Stenner, 1996). They are usually sum-
marized via the mean, but we also tested the median,
the interquartile range, as well as the 75th and 90th
percentiles.
1Space restrictions did not enable us to formally defined
each variable used in this study. The reader may consult
Franc?ois (2011b) for a more comprehensive description.
468
A1 A2 B1 B2 C1 C2 Total
430(58.561) 380(75.779) 552(176.973) 198(71.701) 184(92.327) 108(35.202) 1, 852(510; 543)
Table 1: Distribution of the number of texts and tokens per level in our corpus.
We used Lexique3 (New et al2007) as our fre-
quency database. It is a lexicon including about
50,000 lemmas and 125,000 inflected forms whose
frequencies were obtained from movie subtitles.
Since French has a rich morphology, we considered
the probabilities of both lemma and inflected forms.
Moreover, following an idea from Elley (1969), we
also computed the above mentioned statistics for
given POS words, such as content word, nouns,
verbs, etc.
Percentage of words not in a reference list: part
of the Dale and Chall (1948)?s formula, this feature
is one of the most famous in readability. For our
experiments, two word lists for FFL were used: the
well-known ? but already dated ? Gougenheim et
al. (1964)?s list and a second one that was found at
the end of one FFL textbook: Alter Ego (Berthet et
al., 2006). Different sizes were also experimented
for both lists.
Word length: mean word length is another classic
feature in readability (Flesch, 1948; Smith, 1961).
We used various statistics based on the number of
letters per word (mean, median, percentiles, etc.).
N-grams models: Si and Callan (2001) shown
that n-grams models can successfully be applied to
readability. We then used both a simple unigram ap-
proach based on the frequencies from Lexique3, and
a more complex bigram model trained on two dif-
ferent corpora: the Google n-grams (Michel et al
2011) and a corpus of newspaper articles from Le
Soir amounting to 5, 000, 000 words 2. Both were
normalized according the length n of the text as fol-
lows:
P (text) =
1
n
n?
i=1
logP (wi|h) (1)
where wi is the ith word and h a limited history of
length 0 (unigram) or 1 (bigram).
2Smoothing algorithms used were respectively the simple
Good-Turing algorithm (Gale and Sampson, 1995) for unigrams
and linear interpolation (Chen and Goodman, 1999) for the bi-
grams.
Lexical diversity: the repetition effect is another
factor known to affect the reading process (Bowers,
2000). It has been mainly implemented through the
classic type-token ratio (TTR) that suffers from be-
ing dependent on the text length. This is why we
defined a normalized TTR, which is the mean score
of several TTRs, computed on text?s fragments of
equal length. This way, long texts were made com-
parable with short ones.
Orthographic neighborhood: we finally sug-
gested a new lexical variable, based on the fact that
some characteristics of the orthographic neighbors 3
of a word are known to impact the reading of this
word (Andrews, 1997). Thirteen predictors were
implemented to account for the number or the fre-
quency of the orthographic neighbors of all words in
a text.
3.2.2 Syntactic features
The syntactic level of information is another tradi-
tional area of investigation in readability. Although
most of the scholars in the field agree that it does not
lead to such efficient predictors as the lexical level,
they have noticed it can be combined with the latter
to improve performance of readability formulas. We
therefore investigated the following subfamilies:
Sentence length: the traditional approach to syn-
tactic difficulty relied on the number of words per
sentence. We have approached it through various
statistics such as the mean, the median, or several
percentiles.
Part of speech ratios: Bormuth (1966) demon-
strated the good predictive power of some POS ra-
tios in a text. We computed 156 ratios based on
TreeTagger?s POS (Schmid, 1994). They operated
as proxies for the syntactic complexity of sentences,
since we did not use features based on a parser 4.
3The orthographic neighbors of a word X have been defined
by Coltheart (1978) as all the words of the same length asX and
varying from it only by one letter (eg. FIST and GIST).
4This choice was motivated as follows. Bormuth (1966),
who performed a manual annotation of the syntactic structures
469
Verbs: although the tense and moods found in a
text have been hardly considered in the field, Car-
reiras et al1997) suggested that verbal aspects are
important while building a mental representation of
a text and therefore impact its understanding. They
help the reader to distinguish between major and
minor elements associated with events described by
these verbs. We therefore replicated and enhanced
the feature set proposed by Franc?ois (2009), consid-
ering either binary indicators or proportions of the
use of tenses or moods in a text.
3.2.3 Semantic features
The importance of semantic and cognitive
factors have been particularly stressed by the
structuro-cognitivist paradigm, although Miller and
Kintsch (1980), as well as Kemper (1983), eventu-
ally admitted not being able to demonstrate the supe-
riority of those new predictors over traditional ones.
More recent work also reported limited evidence of
this alleged superiority (Pitler and Nenkova, 2008;
Feng et al2010). In order to clarify as much as
possible the situation for FFL, we implemented the
following features:
Personnalization level: Dale and Tyler (1934)
suggested that informal texts should be easier to read
and that informality might be assessed through the
type of personal pronouns found in a text. On this as-
sumption, 13 variables were defined to take into ac-
count various personal pronouns proportions in the
text.
Conceptual density: Kintsch et al1975) showed
that the number of propositions as well as the num-
ber of different arguments in a sentence influence
its reading time. Following Kintsch?s propositional
model, we used Denside?es (Lee et al2010) to cap-
ture conceptual complexity. It is a program able to
estimate the mean number of propositions per word
in a text using 35 rules relying on lexical and POS
clues.
in its corpus, noticed that features based on parse trees were
less efficient than classic ones, such as sentence length or part
of speech ratios. Therefore, it seemed unlikely that the infor-
mation collected by means of syntactic parsers, which are still
committing a significant number of errors, at least for French,
would belie these findings.
Lexical cohesion : the level of cohesion in a text
was measured as the average cosine of all pair of
adjacent sentences in the text. Each sentence was
represented by a numeric weighted vector (based on
words) and projected in a vector space. As sug-
gested by Foltz and al. (1998), two methods were
used to define the vector space and weight every
word: the tf-idf (term frequency-inverse document
frequency) and the latent semantic analysis (LSA).
The first approach, called ?word overlap?, corre-
sponds to the ?noun overlap? defined by Graesser et
al. (2004, 199), except that all type of POS are taken
into account. For LSA, we applied a singular value
decomposition (SVD), and after comparing various
sizes with a cross-validation procedure, we retained
a small 15-dimensional space.
3.2.4 Features specific to FFL
Apart from the effect of cognates (Uitdenbogerd,
2005; Tharp, 1939), few features specific to the L2
context were previously investigated. It is probably
because such an approach requires to train a model
for each pair of language of interest and gather suit-
able data for evaluation. Since our study intended to
design a generic model, we focused on specific pre-
dictors affecting L2 reading, whatever the learner?s
mother tongue is:
Multi-word expressions (MWE): MWEs are ac-
knowledged to cause problems to L2 learners for
production (Bahns and Eldaw, 1993). However, the
effect of MWE on the reception side remains un-
clear, especially for beginners. Ozasa et al2007)
tested the mean of the absolute frequency of all
MWEs in a text as an indication of its difficulty,
but it appeared non significant. In a latter experi-
ment involving a larger set of MWE-based predic-
tors, Franc?ois and Watrin (2011) detected a signifi-
cant, but limited effect. We therefore replicated this
set, which includes variables based on the frequen-
cies of MWE, their syntactic structure, their number
or their length. Frequencies were estimated on the
same corpora as the bigram model described above
(Google and Le Soir).
Type of text: Finally, we defined five simple vari-
ables aiming at identifying dialogues, such as pres-
ence of commas, ratio of punctuation, etc. as sug-
gested by Henry (1975). This focus on dialogue was
470
Level of information Tag Description of the variable ?
Lexical
PA-Alterego Proportion of absent words from a list of easy words 0.653
X90FFFC 90th percentile of inflected forms for content words only ?0.643
ML3 Unigram model based on lemmas ?0.553
NLM Mean number of letters per word 0.483
TTR Type-token ratio based on lemma 0.283
MedNeigh+Freq Median number of more frequent neighbor for words ?0.233
Syntactic
NMP Mean number of words per sentence 0.623
NWS90 Length (in words) of the 90th percentile sentence 0.613
LSDaoust Percentage of sentences longer than 30 words (Daoust et al1996) 0.563
PPres Presence of at least one present participle in the text 0.443
PRO.PRE Ratio of pronouns on prepositions ?0.353
PPres-C Proportion of present participle among verbs 0.413
PPasse Presence of at least one past participle 0.393
Impf Presence of at least one imperfect 0.273
Subp Presence of at least one subjunctive present 0.273
Cond Presence of at least one conditional 0.233
Imperatif Presence of at least one imperative 0.02
Subi Presence of at least one subjunctive imperfect 0.05
Semantic
avLocalLsa-Lem Average intersentential cohesion measured via LSA 0.633
PP1P2 Percentage of P1 and P2 personal pronouns ?0.333
Specific
NAColl Proportion of MWE having the structure NOUN ADJ 0.293
BINGUI Presence of commas 0.463
Table 2: Spearman correlation for some predictors in our set with difficulty. A positive correlation means that the
difficulty of texts increases with the value of the predictor. Signification levels are the following 1 :< 0.05; 2 :< 0.01;
and 3 : < 0.001.
explained by their extensive use in foreign language
teaching, especially in the first levels. Furthermore,
even for L1, various scholars stressed the fact that
dialogues are often written in a simpler style and
have a more mundane content (Dolch, 1948; Flesch,
1948).
3.3 The algorithms
The last step in the development of our formula was
to select the most informative subset of features and
combine them in a state-of-the-art machine learn-
ing algorithm. The algorithms originally consid-
ered were six: multinomial and ordinal logistic re-
gression (respectively MLR and OLR), classifica-
tion trees, bagging, boosting (both based on decision
trees) and support vector machine (SVM). However,
since the logistic models and the SVM clearly out-
performed the others three, we will reported only
about those in the next section.
4 Results
The experiments based on this methodology were
twofold. First, we assessed the predictive power
of each of the 406 features, considered in a bivari-
ate relationship with difficulty. Second, we selected
various subsets of features for training models and
compared their performance. The two next sections
summarize the main findings obtained during these
two steps.
4.1 The efficiency of predictors
Spearman correlation was used to assess the effi-
ciency of each predictor, to better account for non-
linear relationships with the criterion. Values for
some variables among the four families are reported
in Table 2. In accordance with the literature, it ap-
peared that the best family of predictors were the
lexical one, followed by the syntactic one. On the
contrary, semantic and specific to FFL features did
not perform so well, with the exception of the LSA-
based feature (avLocalLsa-Lem).
Of all predictors, the best was surprisingly PA-
Alterego, a list-based variable inspired by Dale and
Chall (1948), but adapted to the FFL context, since
the list of easy words used came from a FFL text-
book (Alter Ego 1). This suggests that, although the
predictive power of ?specific to FFL? features was
low, specialization to the FFL context was beneficial
at other levels.
471
4.2 The models
Once the best single predictors were identified, it
was possible to combine several of them in a read-
ability model for comparison. This required some
corpus preparation. Since preliminary experiments
showed that the equal prior probabilities are required
to ensure a unbiased training, the whole corpus was
resampled to get the same number of texts per level
(108), which amounted to a total of 648 texts. We
then split this smaller corpus into two sets. 240 texts
were kept for development purposes, mainly feature
selection and estimation of the meta-parameters ?
and C for the SVM. The remaining 408 texts were
used for evaluating performance of our readability
models.
4.2.1 Selection of the features
Several ways of selecting the smallest ?best? sub-
set of features were compared, given that some
variables are partly redundant when combined to-
gether. The first method was based on the
structuro-cognitivist assumption that readability for-
mulas should include other features than just lexico-
syntactical ones, in order to maximize variety of in-
formation. Therefore, we tried an ?expert? selec-
tion, keeping either the best feature among each of
the four families (set Exp1), or the two best features
(set Exp2) 5.
These ?expert? approaches were compared to an
automatic selection, using either a stepwise proce-
dure 6 for logistic regression (OLR and MLR) or
a built-in regularization (Bishop, 2006, 10) for the
SVM, based on the 46 best predictors inside each
subfamily.
For the sake of comparison, we also defined two
other sets: one that corresponds to a random clas-
sification (the empty subset), and a baseline, based
on two classics predictors (number of letters per
word and number of words per sentence), which
aimed to mimic classic formulas such as those of
5For the syntactic level, since the two best variables be-
longed to the same subfamily (see Section 3.2) and were too
highly intercorrelated, the 90th percentile of the sentence length
(NWS90) was replaced by the best feature from another subfam-
ily: the presence of at least one present participle (PPres).
6In order to suppress as much random effects as possible, the
selection process was repeated 100 times via a bootstrapping
.632 procedure (Tuffe?ry, 2007, 396-371) and only the features
selected at least 50 times out of 100 were kept.
Flesch (1948) or Dale and Chall (1948). A summary
of the features included in each subset is available in
Table 3.
4.2.2 Evaluation of the models
The next step then consisted in training logistic
and SVM models for each of the above subsets.
Their performances, reported in Table 4, were as-
sessed using five measures: the multiple correlation
ratio (R), the accuracy (acc), the adjacent accuracy 7
(adjacc), the root mean square error (rmse) and the
mean absolute error (mae). It should be noted that
each of these measures was estimated through a ten-
fold cross-validation procedure, which allowed us to
compare performances of different models with a T-
test.
The comparison between the models was per-
formed in two steps. First, we computed T-tests
based on adjacc to compare the models based on
a same set of features (either Exp1, Exp2, or Auto).
This allowed us to pick up the best classifier for each
set. In a second step, these three best models were
compared the same way, which resulted in the se-
lection of the very best classifier. The decision of
adopting the adjacent accuracy as a criterion instead
of the accuracy was motivated by our conviction that
our system should rather avoid serious errors (i.e.
larger than one level) than be more accurate, while
sometimes generating terrible mistakes. However, it
appeared that both metrics were mostly consistent.
The performance of the different models are dis-
played in Table 4. It is first interesting to note that
the baseline (based on SVM) already gives interest-
ing results. It reaches a classification accuracy of
34%, which is about twice the random. As regards
the first model (Exp1), based on RLM and including
four predictors, it outperforms the baseline by 5%, a
difference close to significance (t(9) = 1.77; p =
0.055). Therefore, combining variables from sev-
eral families seems to improve performance over the
?classic? baseline, limited to lexico-syntactic fea-
tures.
This finding is reinforced by the SVM model
from Exp2, which includes eight features. It per-
forms significantly better than the baseline (t(9) =
7Heilman et al2008) defined it as ?the proportion of pre-
dictions that were within one level of the human assigned label
for the given text?.
472
Model name Classifieur Set of features
Exp1 OLR, MLR and SVM PA-Alterego + NMP + avLocalLsa-Lem + BINGUI
Exp2 OLR, MLR and SVM PA-Alterego + X90FFFC + NMP + PPres + avLocalLsa-Lem + PP1P2 + BINGUI + NAColl
Auto-OLR OLR PA-Alterego + NMP + PPres + ML3
Auto-MLR MLR
PA-Alterego + Cond + Imperatif + Impf + PPasse + PPres + Subi + Subp
+ BINGUI + TTR + NWS90 + LSDaoust + MedNeigh+Freq
Auto-SVM SVM all the 46 variables
Table 3: Results from the two selection process: expert and automatic. Description of the features can be found in
Table 2.
Model Classifier Parameters R acc adjacc rmse mae
Random / / / 16.6 44.4 / /
Baseline SVM ? = 0.05;C = 25 0.62 34.0 68.2 1.51 1.06
Exp1 RLM / 0.70 39.4 74.2 1.34 0.97
Exp2 SVM ? = 0.002;C = 75 0.73 40.8 77.9 1.28 0.94
Auto-OLR OLR / 0.71 39.6 76.1 1.33 0.96
Auto SVM ? = 0.004;C = 5 0.73 49.1 79.6 1.27 0.90
Table 4: Evaluation measures for the best difficulty model from each feature set (Exp1, Exp2 and Auto), along with
values for a random classification, and the ?classic? baseline.
2.36; p = 0.02), with an accuracy gain of 7%. How-
ever, to that point, it was not clear whether this supe-
riority was indeed a consequence of maximizing the
kind of information brought to the model or merely
the result of the increased number of predictor.
We thus performed another experiment to address
this issue. The model Exp1 was compared with
Auto-OLR, the best ordinal logistic model obtained
through the stepwise selection (see Tables 4 and
3), and previously discarded as a result of the T-
test comparisons. Like Exp1, it also contains four
predictors, but they are all lexical or syntactic fea-
tures. Therefore, this model does not maximize the
type of information. Surprisingly, we observed that
Auto-OLR obtained similar and even slightly bet-
ter performance than Exp1 (+2% for both acc and
adjacc). Thus, the claim that maximizing the source
of information should yield better models did not
stand on our data.
Finally, our best performing model was based on
the Auto feature set and SVM. Its accuracy was in-
creased by 8% in comparison with the Exp2 model,
which is clearly a significant improvement (t(9) =
2.61; p = 0.01), and outperformed the baseline by
15%. As mentioned previously, this model includes
46 features coming from our four families. It is
worth mentioning that the quality of the predictions
is not the same across the levels, as shown in Ta-
ble 5. They are more accurate for classes situated
at both ends of the difficulty scale, namely A1, C1
and C2. For A1, this is explained because texts for
beginners are more typical, having very short sen-
tences and simple words. However, the case of C1
and C2 classes is more surprising and might be due
to some specificities of the learning algorithm.
A1 A2 B1 B2 C1 C2
Adj. acc. 100% 71% 67% 71% 86% 83%
Table 5: Adjacent accuracy per level, computed on one
of the 10 folds. Its adjacent accuracy was 79%, which is
very similar to the average value of the model.
We also assessed the specific contribution of each
family of features in two ways: on one hand, we
trained a model including only the features from this
family; on the other hand, we trained a model in-
cluding all features except those from this family.
Results for the four families are displayed at Table 6.
It appeared that the lexical family was the most
accurate set of predictors (40.5%) and yielded the
highest loss in performance when set aside, espe-
cially for adjacent accuracy. In fact, this was the
only set whose absence significantly impacted ad-
jacent accuracy, suggesting that the other type of
predictors can only improve the accuracy of predic-
tions, but are not able to reduce the amount of crit-
ical mistakes. The second best family was, expect-
edly, the syntactic one. Its accuracy closely match
that of the lexical set, although more severe mistakes
were made, as shown by the drop in adjacent accu-
473
racy. Finally, our two other families was clearly in-
ferior, but they still improved slightly the accuracy
of our model, although not the adjacent accuracy.
Family only All except family
Acc. Adj. acc. Acc. Adj. acc.
Lexical 40.5 75.6 41.1 73.5
Syntactic 39.3 69.5 43.2 78.4
Semantic 28.8 61.5 47.8 79.2
FFL 24.9 58.5 47.8 79.6
Table 6: Accuracy and adjacent accuracy (in percentage)
for models either using only one family of predictors, or
including all 46 features except those of one family.
4.2.3 Comparaison with previous work
Comparisons with other FFL models are difficult
to provide: not only there are few formulas available
for FFL, but some of these focus on a different au-
dience, making comparability low. This is why we
were able to compare our results with only two pre-
vious models.
The first of them is a classic readability formula
by Kandel and Moles (1958), which is an adaptation
of the Flesch (1948) formula for French:
Y = 207? 1.015lp? 0.736lm (2)
where Y is a readability score ranging from 100
(easiest) to 0 (harder); lp is the average number of
words per sentence and lm is the average number
of syllables per 100 words. Although it was not de-
signed for FFL, we considered it, since it is one of
the most well-known formula for French and the two
features combined are very general. Their predic-
tive power should not vary much in both contexts, as
shown by Greenfield (2004) for English. We evalu-
ated it on the same test corpus as our SVM model
and obtained really lower values : a R of 0.55 and
an accuracy of 33%.
The second model was that of Franc?ois (2009),
which is based on a multinomial logistic regression
including ten features: a unigram model similar to
ML3, the number of letters per word, the number of
words per sentence, and binary variables indicating
the presence of a past participle, present participle,
imperfect, infinitive, conditional, future and present
subjunctive tenses in the text. To our knowledge,
this model is the best current generic model avail-
able for FFL. On our data, it yielded an accuracy of
41% and an adjacent accuracy of 72.7%, both esti-
mated through a 10-fold cross-validation procedure.
Therefore, our new approach achieved an accuracy
gain of 8% over this state-of-the-art model, which
was considered as a significant difference (t(9) =
3.72; p = 0.002).
Apart of those two studies, Uitdenbogerd (2005)
also developed recently a FFL formula. However, as
explained previously, this work focused on a spe-
cific category of L2 readers, the English-speakers
learning FFL, which resulted in a different problem.
She reported a higher R than us (0.87 against 0.73).
However, this value might be the training one and
was estimated on a small amount of novel begin-
nings. It is therefore likely that our model generalize
better, especially across genres and L2 readers with
different L1 backgrounds.
5 Discussion and conclusion
In this paper, we introduced a new ?AI readability?
formula for FFL, able to predict the level of texts
according to the largely-spread CEFR scale. Our
model is based on a SVM classifier and combines 46
features corresponding to several levels of linguis-
tic information. Among those, we suggested some
new features: the normalized TTR and the set of
variables based on several characteristics of words?
neighbors. Comparing our approach with two pre-
viously published formulas, our model significantly
outperformed both these works. Therefore, it repre-
sent a robust generic solution for FFL readers will-
ing to find various kind of texts that suit their lin-
guistic abilities.
Besides the creation of a new FFL readability
formula, this study produced two valuable insights.
First, we showed that maximizing the type of lin-
guistic information might not be the best path to go,
since a model based on four lexico-syntactic fea-
tures yielded predictions as accurate as those of a
model relying on our Exp1 set of variables. How-
ever, this finding might be partly accounted by the
lower predictive power of the features from the se-
mantic and specific-to-FFL family, with the notable
exception of the LSA-based predictor (avLocalLsa-
Lem), which is the third best predictor when consid-
ered alone.
This leads us to our second finding, relative to the
474
set of semantic features. Yet their importance was
largely praised in the structuro-cognitivist paradigm
and in most of the recent works, our experiments
cast serious doubts about their efficiency, at least in
a L2 context. Not only the expert models, to which
we imposed the presence of one or two semantic pre-
dictors, did not perform the best, but none of the
features from our semantic set was retained during
the automatic selection of the variables for the lo-
gistic models. On the contrary, in some subsets,
the LSA-based feature was sometimes considered as
collinear with the other variables. Finally and fore-
most, we showed that dropping the semantic features
did not impact significantly the performance of our
best model.
With reservations one may have because of the
limited number of semantic predictors in our set,
these results however raise some concerns about
whether the information coming from semantic vari-
ables is really different from that carried by lexico-
syntactic features. Our results clearly show that
this may not be the case. This conclusion con-
tradicts the assumptions of the structuro-cognitivist
paradigm, but corroborates Chall and Dale (1995)?s
view that the information carried by semantic pre-
dictors is largely correlated with that of lexico-
syntactical ones.
Further investigation on this issue would defi-
nitely be worthwhile, since several facts could ex-
plain these contradictory findings. First, it might be
that semantic and lexical predictors are correlated
because the methods used for the parameterization
of the semantic factors heavily relie on lexical infor-
mation. This is the case for the LSA, as well as for
the propositional approach of the content density.
Alternatively, this difference with other work in
L1 could be due to the L2 context. Chall and
Dale (1995) explained that the lexicon and the syn-
tax are more important for children learning to read
than for more advanced readers, who then become
more sensitive to organisationnal aspects. From the
threshold hypothesis (Alderson, 1984), we know
that before reaching a sufficient level of proficiency,
L2 learners struggle mostly with the lexicon and
the syntactic structures. This might explain why
lexico-syntactic predictors were so predominant in
our experiments. Some further experiments are thus
needed to investigate which of these facts better ac-
count for our findings on the semantic features.
A last avenue of research worth mentioning would
be to develop the family of specific-to-FFL predic-
tors, to determine whether taking into account the
impact of a given L1 language on the readability of
L2 texts would increase performance over a generic
model enough so that tuning efforts are worthwhile.
Acknowledgments
Thomas Franc?ois was an Aspirant F.N.R.S. when
this work was performed. The writing of this paper
was done while being a recipient of a Fellowship of
the Belgian American Educational Foundation. We
thank both for their support. We would also like to
acknowledge the invaluable help of Bernadette De-
hottay for the collection of the corpus used in that
study.
References
J.C. Alderson. 1984. Reading in a foreign language :
a reading problem or a language problem ? In J.C.
Alderson and A.H Urquhart, editors, Reading in a For-
eign Language, pages 1?24. Longman, New York.
S. Andrews. 1997. The effect of orthographic similarity
on lexical retrieval: Resolving neighborhood conflicts.
Psychonomic Bulletin & Review, 4(4):439?461.
J. Bahns and M. Eldaw. 1993. Should We Teach EFL
Students Collocations? System, 21(1):101?14.
A. Berthet, C. Hugot, V. Kizirian, B. Sampsonis, and
M. Waendendries. 2006. Alter Ego 1. Hachette, Paris.
C.M. Bishop. 2006. Pattern recognition and machine
learning. Springer, New York.
J.R. Bormuth. 1966. Readability: A new approach.
Reading research quarterly, 1(3):79?132.
J.S. Bowers. 2000. In defense of abstractionist theories
of repetition priming and word identification. Psycho-
nomic bulletin and review, 7(1):83?99.
M. Carreiras, N. Carriedo, M.A. Alonso, and
A. Ferna?ndez. 1997. The role of verb tense and
verb aspect in the foregrounding of information
during reading. Memory & Cognition, 25(4):438?446.
J.S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brookline
Books, Cambridge.
S. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Com-
puter Speech and Language, 13(4):359?393.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
475
Proceedings of HLT/NAACL 2004, pages 193?200,
Boston, USA.
K. Collins-Thompson and J. Callan. 2005. Predict-
ing reading difficulty with statistical language models.
Journal of the American Society for Information Sci-
ence and Technology, 56(13):1448?1462.
M. Coltheart. 1978. Lexical access in simple reading
tasks. In G. Underwood, editor, Strategies of infor-
mation processing, pages 151?216. Academic Press,
London.
A. Conquet. 1957. La lisibilite?. Assemble?e Permanente
des CCI de Paris, Paris.
C.M. Cornaire. 1988. La lisibilite? : essai d?application
de la formule courte d?Henry au franc?ais langue
e?trange`re. Canadian Modern Language Review,
44(2):261?273.
Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teach-
ing, Assessment. Press Syndicate of the University of
Cambridge.
E. Dale and J.S. Chall. 1948. A formula for predicting
readability. Educational research bulletin, 27(1):11?
28.
E. Dale and R.W. Tyler. 1934. A study of the fac-
tors influencing the difficulty of reading materials for
adults of limited reading ability. The Library Quar-
terly, 4:384?412.
F. Daoust, L. Laroche, and L. Ouellet. 1996. SATO-
CALIBRAGE: Pre?sentation d?un outil d?assistance au
choix et a` la re?daction de textes pour l?enseignement.
Revue que?be?coise de linguistique, 25(1):205?234.
G. de Landsheere. 1963. Pour une application des tests
de lisibilite? de Flesch a` la langue franc?aise. Le Travail
Humain, 26:141?154.
E.W. Dolch. 1948. Problems in reading. The Garrard
Press, Champaign : Illinois.
W.B. Elley. 1969. The assessment of readability by
noun frequency counts. Reading Research Quarterly,
4(3):411?427.
L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.
2010. A Comparison of Features for Automatic Read-
ability Assessment. In COLING 2010: Poster Volume,
pages 276?284.
R. Flesch. 1948. A new readability yardstick. Journal of
Applied Psychology, 32(3):221?233.
P.W. Foltz, W. Kintsch, and T.K. Landauer. 1998. The
measurement of textual coherence with latent semantic
analysis. Discourse processes, 25(2):285?307.
T. Franc?ois and P. Watrin. 2011. On the contribution
of MWE-based features to a readability formula for
French as a foreign language. In Proceedings of the
International Conference RANLP 2011.
T. Franc?ois. 2009. Combining a statistical language
model with logistic regression to predict the lexical
and syntactic difficulty of texts for FFL. In Proceed-
ings of the 12th Conference of the EACL : Student Re-
search Workshop, pages 19?27.
T. Franc?ois. 2011a. La lisibilite? computationnelle
: un renouveau pour la lisibilite? du franc?ais langue
premie`re et seconde ? International Journal of Ap-
plied Linguistics (ITL), 160:75?99.
T. Franc?ois. 2011b. Les apports du traitement au-
tomatique du langage a` la lisibilite? du franc?ais langue
e?trange`re. Ph.D. thesis, Universite? Catholique de Lou-
vain. Thesis Supervisors : Ce?drick Fairon and Anne
Catherine Simon.
W.A. Gale and G. Sampson. 1995. Good-Turing fre-
quency estimation without tears. Journal of Quantita-
tive Linguistics, 2(3):217?237.
G. Gougenheim, R. Miche?a, P. Rivenc, and A. Sauvageot.
1964. L?e?laboration du franc?ais fondamental (1er
degre?). Didier, Paris.
A.C. Graesser, D.S. McNamara, M.M. Louwerse, and
Z. Cai. 2004. Coh-Metrix: Analysis of text on co-
hesion and language. Behavior Research Methods, In-
struments, & Computers, 36(2):193?202.
J. Greenfield. 2004. Readability formulas for EFL.
Japan Association for Language Teaching, 26(1):5?
24.
M. Heilman, K. Collins-Thompson, and M. Eskenazi.
2008. An analysis of statistical models and features
for reading difficulty prediction. In Proceedings of the
Third Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 1?8.
G. Henry. 1975. Comment mesurer la lisibilite?. Labor,
Bruxelles.
L. Kandel and A. Moles. 1958. Application de l?indice
de Flesch a` la langue franc?aise. Cahiers E?tudes de
Radio-Te?le?vision, 19:253?274.
S. Kemper. 1983. Measuring the inference load of a text.
Journal of Educational Psychology, 75(3):391?401.
W. Kintsch and D. Vipond. 1979. Reading comprehen-
sion and readability in educational practice and psy-
chological theory. In L.G. Nilsson, editor, Perspec-
tives on Memory Research, pages 329?365. Lawrence
Erlbaum, Hillsdale, NJ.
W. Kintsch, E. Kozminsky, W.J. Streby, G. McKoon, and
J.M. Keenan. 1975. Comprehension and recall of text
as a function of content variables1. Journal of Verbal
Learning and Verbal Behavior, 14(2):196?214.
H. Lee, P. Gambette, E. Maille?, and C. Thuillier. 2010.
Denside?es: calcul automatique de la densite? des ide?es
dans un corpus oral. In Actes de la douxime Rencon-
tre des tudiants Chercheurs en Informatique pour le
Traitement Automatique des langues (RECITAL).
476
I. Lorge. 1944. Predicting readability. the Teachers Col-
lege Record, 45(6):404?419.
J. Mesnager. 1989. Lisibilite? des textes pour enfants: un
nouvel outil? Communication et Langages, 79:18?38.
B.J.F. Meyer. 1982. Reading research and the composi-
tion teacher: The importance of plans. College com-
position and communication, 33(1):37?49.
J.B. Michel, Y.K. Shen, A.P. Aiden, A. Veres, M.K. Gray,
The Google Books Team, J.P. Pickett, D. Hoiberg,
D. Clancy, P. Norvig, J. Orwant, S. Pinker, M.A.
Nowak, and E.L. Aiden. 2011. Quantitative analysis
of culture using millions of digitized books. Science,
331(6014):176?182.
J.R. Miller and W. Kintsch. 1980. Readability and re-
call of short prose passages: A theoretical analysis.
Journal of Experimental Psychology: Human Learn-
ing and Memory, 6(4):335?354.
B. New, M. Brysbaert, J. Veronis, and C. Pallier. 2007.
The use of film subtitles to estimate word frequencies.
Applied Psycholinguistics, 28(04):661?677.
R.E. O?Connor, K.M. Bell, K.R. Harty, L.K. Larkin, S.M.
Sackor, and N. Zigmond. 2002. Teaching reading to
poor readers in the intermediate grades: A comparison
of text difficulty. Journal of Educational Psychology,
94(3):474?485.
T. Ozasa, G. Weir, and M. Fukui. 2007. Measuring read-
ability for Japanese learners of English. In Proceed-
ings of the 12th Conference of Pan-Pacific Association
of Applied Linguistics.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 186?195.
J.C. Redish and J. Selzer. 1985. The place of readability
formulas in technical communication. Technical com-
munication, 32(4):46?52.
F. Richaudeau. 1979. Une nouvelle formule de lisibilite?.
Communication et Langages, 44:5?26.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of International
Conference on New Methods in Language Processing,
volume 12. Manchester, UK.
S.E. Schwarm and M. Ostendorf. 2005. Reading level
assessment using support vector machines and statis-
tical language models. Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 523?530.
L. Si and J. Callan. 2001. A statistical model for sci-
entific readability. In Proceedings of the Tenth Inter-
national Conference on Information and Knowledge
Management, pages 574?576. ACM New York, NY,
USA.
E.A. Smith. 1961. Devereaux readability index. The
Journal of Educational Research, 54(8):289?303.
A.J. Stenner. 1996. Measuring reading comprehension
with the lexile framework. In Fourth North American
Conference on Adolescent/Adult Literacy.
J.B. Tharp. 1939. The Measurement of Vocabulary Dif-
ficulty. Modern Language Journal, pages 169?178.
S. Tuffe?ry. 2007. Data mining et statistique
de?cisionnelle l?intelligence des donne?es. E?d. Technip,
Paris.
S. Uitdenbogerd. 2005. Readability of French as a for-
eign language and its uses. In Proceedings of the Aus-
tralian Document Computing Symposium, pages 19?
25.
P. van Oosten, V. Hoste, and D. Tanghe. 2011. A pos-
teriori agreement as a quality measure for readabil-
ity prediction systems. In A. Gelbukh, editor, Com-
putational Linguistics and Intelligent Text Processing,
volume 6609 of Lecture Notes in Computer Science,
pages 424?435. Springer, Berlin / Heidelberg.
477
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 83?91,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
An N-gram frequency database reference to handle MWE extraction in NLP
applications
Patrick Watrin
Centre for Natural Language Processing
Institut Langage et Communication
UCLouvain
patrick.watrin@uclouvain.be
Thomas Fran?ois
Aspirant F.N.R.S.
Centre for Natural Language Processing
Institut Langage et Communication
UCLouvain
thomas.francois@uclouvain.be
Abstract
The identification and extraction of Multiword
Expressions (MWEs) currently deliver satis-
factory results. However, the integration of
these results into a wider application remains
an issue. This is mainly due to the fact that
the association measures (AMs) used to detect
MWEs require a critical amount of data and
that the MWE dictionaries cannot account for
all the lexical and syntactic variations inherent
in MWEs. In this study, we use an alterna-
tive technique to overcome these limitations. It
consists in defining an n-gram frequency data-
base that can be used to compute AMs on-the-
fly, allowing the extraction procedure to effi-
ciently process all the MWEs in a text, even if
they have not been previously observed.
1 Introduction
Multiword Expressions (MWEs) are commonly
defined as ?recurrent combinations of words that
co-occur more often than expected by chance and
that correspond to arbitrary word usages? (Smadja,
1993, 143). Their importance in the field of natu-
ral language processing (NLP) is undeniable. Al-
though composed of several words, these sequences
are nonetheless considered as simple units with re-
gard to part-of-speech at the lexical as well as syn-
tactic levels. Their identification is therefore essen-
tial to the efficiency of applications such as parsing
(Nivre and Nilsson, 2004), machine translation (Ren
et al, 2009), information extraction, or information
retrieval (Vechtomova, 2005). In these systems, the
principle of syntactic or semantic/informational unit
is particularly important.
Although the identification and extraction of
MWEs now deliver satisfactory results (Evert and
Krenn, 2001; Pearce, 2002), their integration into
a broader applicative context remains problematic
(Sag et al, 2001). The explanations for this situation
are twofold.
1. The most effective extraction methods resort
to statistical association measures based on the
frequency of lexical structures. They, therefore,
require a critical amount of data and cannot
function properly from a simple phrase or even
from a short text.
2. Since the syntactic and lexical variability of
MWEs may be high, lexical resources learned
from a corpus cannot take it into account. The
coverage of these resources is indeed too limi-
ted when applied to a new text.
To address these two limitations, this article des-
cribes how an n-gram frequency database can be
used to compute association measures (AMs) effi-
ciently, even for small texts. The specificity of this
new technique is that AMs are computed on-the-fly,
freeing it from the coverage limitation that afflicts
more simple techniques based on a dictionary.
We start off focussing on our extraction method,
and more particularly on the process via which a
candidate structure is statistically validated (Section
2). This presentation principally aims to identify
the precise needs of a frequency database reference,
both in terms of the interrogation process and in the
type of information to be kept in the database. Then,
we will address various issues of storage and query
performance raised by the design of the frequency
83
database (Section 3). Finally, Section 4 reports the
results of our experiments and Section 5 concludes
and open up future perspectives.
2 Extraction process
Our extraction procedure is comparable to those
developed by Smadja (1993) and Daille (1995).
They use a linguistic filter upstream of the statisti-
cal estimation. Unlike purely statistical techniques,
this solution provides less coverage but greater ac-
curacy. It also allows us to assign a unique morpho-
syntactic category to each extracted unit (as well as
a description of its internal structure), which facili-
tates its integration into a more complex procedure.
Concretely, we first tagged the texts to clear any
lexical ambiguities 1. We then identified all MWE
candidates in the tagged text with the help of a li-
brary of transducers 2 (or syntactic patterns). Finally,
the list of candidates was submitted to the statistical
validation module which assigns an AM to each of
these.
2.1 Linguistic filters
In this study, we consider four basic types of
nominal structures 3 : adjective-noun (AN), noun-
adjective (NA), noun-preposition-noun (N prepN),
and noun-noun (NN), which are likely to undergo
three types of variations : modification (mainly ad-
verbial insertion and / or adjectival), coordination,
and juxtaposition (e.g. N prepN prepN, N prepNN,
etc). This enables us to identify a wide variety of
sequences that are labelled by XML tags which spe-
cify :
? the lexical heads of the various components ;
? the adjectival and prepositional dependencies ;
? any possible coordination.
This information can be exploited later to carry out
the syntactic decomposition of the extracted struc-
tures and also to limit the statistical validation to the
content words of each structure.
1. The tagging is done with the TreeTagger (Schmid, 1994).
2. To apply our transducers to the tagged text, we use Unitex
(Paumier, 2003). The output of the process is a file containing
only the recognized sequences.
3. As we work in the field of indexation, we limit our ex-
traction to nominal terms.
2.2 Statistical validation
Association measures are conventionally used
to automatically determine whether an extracted
phrase is an MWE or not. They are mathematical
functions that aim to capture the degree of cohesion
or association between the constituents. The most
frequently used measures are the log-likelihood ratio
(Dunning, 1993), the mutual information (Church
and Hanks, 1990) or the ?2 (Church and Gale, 1991),
although up to 82 measures have been considered by
Pecina and Schlesinger (2006). In this paper, we did
not aim to compare AMs, but simply to select some
effective ones in order to evaluate the relevance of a
reference for MWE extraction.
However, association measures present two main
shortcomings that were troublesome for us : they are
designed for bigrams, although longer MWEs are
quite frequent in any corpus 4, and they require the
definition of a threshold above which an extracted
phrase is considered as an MWE. The first aspect is
very limiting when dealing with real data where lon-
ger units are common. The second may be dealt with
some experimental process to obtain the optimal va-
lue for a given dataset, but is prone to generalization
problems. In the next two sections, we present the
strategies we have used to overcome these two limi-
tations.
2.2.1 Beyond bigrams
A common way to go beyond the bigram limita-
tion is to compute the AMs at the bigram level and
then use the results as input for the computation of
higher order AMs (Seretan et al, 2003). However,
our preliminary experimentations have yielded un-
satisfactory results for this technique when it is ap-
plied to all words and not to heads only. This is pro-
bably a side effect of high frequency bigrams such
as preposition-determiner (prep det) in French.
Another strategy explored by Silva and
Lopes (1999) is the fair dispersion point normaliza-
tion. For a given n-gram, which has n?1 dispersion
points that define n ? 1 "pseudo-bigrams", they
compute the arithmetic mean of the probabilities of
the various combinations rather than attempting to
pick up the right point. This technique enables the
4. In our test corpus (see Section 4), 2044 MWEs out of
3714 are longer than the bigrams.
84
authors to generalize various conventional measures
beyond the bigram level. Among these, we selected
the fair log-likelihood ratio as the second AM for
our experiments (see Equation 1), given that the
classic log-likelihood ratio has been found to be
one of the best measures (Dunning, 1993; Evert and
Krenn, 2001).
LogLik f (w1 ? ? ?wn) = 2? logL(p f 1,k f 1,n f 1)
+ logL(p f 2,k f 2,n f 2)
? logL(p f ,k f 1,n f 1)
? logL(p f ,k f 2,n f 2) (1)
where
k f 1 = f (w1 ? ? ?wn) n f 1 = Avy
k f 2 = Avx? k f 1 n f 2 = N ?n f 1
Avx = 1
n?1
i=n?1
?
i=1
f (w1 ? ? ?wi)
Avy = 1
n?1
i=n
?
i=2
f (wi ? ? ?wn)
p f = k f 1+k f 2N p f 1 = k f 1n f 1 p f 2 = k f 2n f 2
and N is the number of n-grams in the corpus.
Silva and Lopes (1999) also suggested an AM of
their own : the Symmetrical Conditional Probabi-
lity, which corresponds to P(w1|w2)P(w2|w1) for a
bigram. They defined the fair dispersion point nor-
malization to extend it to larger n-grams, as shown
in Equation 2.
SCPf ([w1 ? ? ?wn]) =
p(w1 ? ? ?wn)2
Avp
(2)
where w1 ? ? ?wn is the n-gram considered and Avp is
defined as follows :
Avp = 1
n?1
i=n?1
?
i=1
p(w1 ? ? ?wi)? p(wi+1 ? ? ?wn) (3)
Finally, we considered a last AM : the Mutual Ex-
pectation (Dias et al, 1999) (see Equation 4). Its
specificity lies in its ability to take into account non-
contiguous MWEs such as ?to take __ decision? or
?a __ number of?, which can also be realized using
the heads (see above).
ME(w1 ? ? ?wn) =
f (w1 ? ? ?wn)? p(w1 ? ? ?wn)
FPE
(4)
where FPE is defined as follows :
FPE = 1
n
[p(w2 ? ? ?wn)+
n
?
i=2
p(w1 ? ? ? w?i ? ? ?wn)] (5)
It should be noted that the expression w1 ? ? ? w?i ? ? ?wn,
where the ? indicates an omitted term, represents all
the n (n-1)-grams the candidate MWE comprises.
FPE is then able to estimate the ?glue? between
all the constituents separated by a gap, but this ne-
vertheless requires a more complex string matching
process.
To summarize, we have selected the three follo-
wing association measures for n-grams : the fair log-
likelihood ratio, SCPf , and ME. Their efficiency is
further discussed in Section 4.
2.2.2 Selection of MWEs
The second problem that arises when one wants to
locate all the MWEs in a given text is the classifica-
tion criterion. For the log-likelihood ratio, which fol-
lows a chi-square distribution once it is transformed
as ?2? log?, a first solution is to base the decision
on the p-value. However, significance tests become
highly unreliable for large corpora, since the high
frequencies produce high scores for the chi-square
and all phenomena then appear significant (Kilgar-
riff, 2005).
A second technique commonly used in the MWE
literature is to select a threshold for the AM above
which an analyzed phrase is considered as an MWE.
Again, this threshold depends on the size of the cor-
pus used and cannot be fixed once and for all for
a specific AM. It must be obtained empirically for
each application of an MWE extractor to a new text
or to a new domain. In order not to resort to a thres-
hold, (Silva et al, 1999) suggested the LocalMax al-
gorithm that selects MWEs whose AMs are higher
than those of their neighborhood. In other words, a
given unit is classified as an MWE if g(w1 ? ? ?wn),
the associative function, is a local maximum.
In our case, since the notion of reference implies
a large corpus and high frequencies, we rejected
the first of these three approaches. We experimen-
ted with the second and third and show in Section 5
how the use of a reference could partially solve the
threshold issues.
85
3 Reference Building
The integration of MWEs in an NLP system is
usually done via a dictionary. MWEs are then re-
garded as a sequence of simple words separated by
spaces (Sag et al, 2001). As a result, their lexical
and syntactic structure is fixed and cannot be used
to take into account variation at this level.
Several methods have been proposed to overcome
this limitation. Nerima et al (2006) and Sag et
al. (2001) associate each MWE with a feature struc-
ture specifying the nature of units and the type of
fixedness. This approach requires a manual valida-
tion of the features when inserting them into the
dictionary. Watrin (2007) considers a simpler tech-
nique that consists in identifying, for each type of
structure, all the possible insertion points and spe-
cifying the lexical and syntactic nature of possible
modifiers. In this case, each MWE takes the form of
a regular expression formalizing all possible varia-
tions from the canonical form.
Both solutions enable to consider more MWEs
but fail to express all possible variations. For ins-
tance, phenomena such as coordination or juxta-
position do not seem to be taken into account by
the authors mentioned above including Nerima et
al. (2006). Moreover, they limit lexical variations to
a finite set of canonical structures that have been en-
countered and are therefore unable to recognize new
candidates.
The notion of reference which we define in this
article aims to overcome these two limitations. Ra-
ther than providing a list of MWEs that are pre-
computed on a corpus, we suggest storing the in-
formation needed to calculate various AMs within
a database. Hence, we no longer restrict MWEs to
a finite set of lexical entries but allow the on-the-fly
computation of AMs for any MWE candidate, wha-
tever the size of the input text.
3.1 Implementation details
From a computational point of view, this idea in-
volves the compression of a large number of lexi-
cal structures of order N as well as their absolute
frequency. Moreover, the calculation of the various
AMs considered in this study also requires the fre-
quencies of all structures of order n, strictly lower
than N (0 < n < N). The second type of informa-
tion can however be inferred from the frequency of
the structures of order N, provided the storage and
questioning system is efficient enough for real-time
applications. The need for efficiency also applies to
queries related to the ME measure or the LocalMax
algorithm that partly involve the use of wildcards.
This type of search tool can be efficiently im-
plemented with a PATRICIA tree (Morrison, 1968).
This data structure enables the compression of n-
grams that share a common prefix and of the nodes
that have only one child. The latter compression is
even more effective as most of the n-grams have a
unique suffix (Sekine, 2008). Beyond the compres-
sion that this structure allows, it also guarantees a
very fast access to data insofar as a query is a simple
tree traversal that can be done in constant time.
In order to further optimize the final data struc-
ture, we store the vocabulary in a table and associate
an integer as a unique identifier for every word. In
this way, we avoid the word repetition (whose size
in memory far exceeds that of an integer) in the tree.
Moreover, this technique also enables to speed up
the query mechanism, since the keys are smaller.
We derived two different implementations of this
structure. The first stores the data directly in me-
mory. While it enables easy access to data, the num-
ber of n-grams that can be stored is limited by the
capacity of the RAM. Therefore, in order to take a
huge number of n-grams into account, we also im-
plemented a ?disk? version of the tree.
Finally, in order to treat wildcard queries nee-
ded by the ME and the LocalMax, we enhanced our
structure with a set of indexes to improve access to
each word, whatever its depth within the tree. Ob-
viously, this mechanism might not be robust enough
for a system multiplying the number of wildcards,
but it is perfectly suited to the needs of an MWEs
extraction process.
3.2 References used
Once the computational aspects of reference buil-
ding have been dealt with, a corpus from which to
populate the database needs to be selected. This as-
pect raises two issues : the size and the nature of the
corpus used. Dunning (1993) has demonstrated that
the size of the corpus from which MWEs are extrac-
ted matters. On the other hand, common characteris-
tics of a corpus, such as its register, the contempora-
86
Reference # 5-Grams # Nodes
500 K 500,648 600,536
1000 K 1,001,080 1,183,346
5000 K 5,004,987 5,588,793
Google 1,117,140,444 62,159,203
TABLE 1: Number of 5-grams and nodes in the references
used
neity of its language or the nature of the topics co-
vered, may impact the performances of a reference
when used on a text with different characteristics.
Given these issues, four corpora were selected (cf.
Table 1). The first three are made up of articles pu-
blished in the Belgian daily newspaper Le Soir in
2009, with 500K, 1000K and 5000K words respec-
tively. They share many characteristics with our test
corpus. The last corpus is made up of the largest
amount of n-grams publicly available for French :
the Google 5-grams 5 (Michel et al, 2011). Its size
reaches 1T words 6, and its coverage in terms of to-
pic and register is supposedly wider than corpora of
newspaper articles only. In a sense, the Google re-
ference may be viewed as an attempt to a universal
reference.
4 Evaluation
Most evaluations of MWE extraction systems are
based on human judgments and restrict the valida-
tion process to the n-best candidates. Inevitably par-
tial, this method is unable to estimate performance
in terms of recall. To overcome these limitations,
we use the evaluation method described by Evert
and Krenn (2001). They propose an automatic me-
thod that consists in computing both recall and pre-
cision using various n-best samples. It involves the
formation of a golden standard (i.e. a list of MWEs
manually identified in a corpus) and a sorted list of
MWEs extracted automatically by applying AM on
the same corpus. The recall and precision rates are
therefore calculated by comparing the n-best (where
n increases from 0 till n in steps of x) to the golden
5. For the purposes of comparison, we also limited the size
of the n-grams indexed in Le Soir to 5 words.
6. In order to model a contemporary language, we only kept
the frequencies observed in texts written between 2000 and
2008.
standard list 7.
4.1 The test corpus
In this study, we use the corpus described in La-
porte et al (2006). It is a French corpus in which all
MWEs have been manually annotated. It consists of
two sub-corpora :
? the transcription, in a written style, of the Oc-
tober 3rd and 4th, 2006 meetings of the French
National Assembly (FNA), and
? the complete text of Jules Verne?s novel
"Around the World in 80 Days", published in
1873 (JV).
These two sub-corpora respectively contain 98,969
and 69,877 words for a total of 3,951 and 1,103
MWEs 8. We limit our evaluation to the FNA cor-
pus in order to keep data consistent both in terms
of register and time. We assume that these two va-
riables have a direct impact on the use of MWEs, a
hypothesis that seems to be confirmed by the rate of
MWEs in both sub-corpora.
4.2 Extractor Parameters
Before evaluating the performance of each of the
above mentioned references, we first assessed the in-
fluence of the various parameters involved in the ex-
traction process and which affect the performance
of the AMs. These parameters are the LocalMax,
the smoothing technique, the lemmatization of the
MWE constituents (LEMMA) 9 and the head-driven
validation (HDV) 10. To select the optimal parame-
ters for our extractor, we established an additional
reference (1000K words from Le Soir).
7. We build these lists from MWE types to avoid introdu-
cing a bias in the evaluation process. Well-recognised high fre-
quency MWEs might indeed gloss over poorly recognised low-
frequency MWEs.
8. These occurrences correspond to 1,384 MWE types for
the FNA corpus and 521 for the JV corpus.
9. The lemmatization of the MWE constituents is based on
the assumption that the inflexion of the lemmas implies a dis-
persal of the frequency mass (the overall frequency of a lemma
is split between its inflected forms) that may affect the behavior
of the AMs.
10. The HDV aims to focus on the lexical heads of the MWE
candidates. Therefore, function words (prepositions, conjunc-
tions, etc.) are ignored and replaced by wildcards in the queries
sent to the reference in order to keep the distance information.
For instance, from the sequence ministre de l?agriculture (Mi-
nister for Agriculture), we derive the form ministre * * agricul-
ture.
87
 10
 20
 30
 40
 50
 60
 70
 80
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n 
(%
)
MWE (%)
Measures -- Precision
Fair Log-Likelihood
Mutual Expectation
Symmetric Conditional Probability
 0
 10
 20
 30
 40
 50
 60
 0  10  20  30  40  50  60  70  80  90  100
R
ec
al
l (%
)
MWE (%)
Measures -- Recall
Fair Log-Likelihood
Mutual Expectation
Symmetric Conditional Probability
FIGURE 1: Evaluation of AMs
The first step of this selection procedure was to
define a baseline. For this purpose, we compared
the precision and recall rates of our three AMs (see
Figure 1) and kept only the best, namely the log-
likelihood ratio, for the rest of our experiments.
While the ME provides better precision for the top
five percent of the extracted units, the log-likelihood
ratio appears more reliable in that it maintains its
efficiency over time (for recall as well as precision).
The SCP, for its part, displays more stable results but
does not reach sufficient precision.
On the basis of this baseline, we then separately
compared the contribution of each of the four para-
meters. Results are reported in Figure 2 and detailed
in the following subsections.
4.2.1 The LocalMax
Figure 2 shows that the LocalMax significantly
improves the precision of the extraction. It emerges
as the most relevant parameter at this level. Howe-
 10
 20
 30
 40
 50
 60
 70
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n 
(%
)
MWE (%)
Parameters -- Precision
Lemmatization
Add Text Smoothing
LocalMax
Head
Baseline
 0
 10
 20
 30
 40
 50
 60
 0  10  20  30  40  50  60  70  80  90  100
R
ec
al
l (%
)
MWE (%)
Parameters -- Recall
Lemmatization
Add Text Smoothing
LocalMax
Head
Baseline
FIGURE 2: Evaluation of the parameters
ver, unlike other parameters, its application directly
affects the recall that falls below our baseline. This
may not be a problem for certain applications. In our
case, we aim to index and classify documents. The-
refore, while we can accommodate a lower preci-
sion, we cannot entirely neglect the recall. We thus
abandoned this parameter which, moreover, indubi-
tably increases the processing time in that it requires
the use of approximate matching (see Section 3.1).
4.2.2 The Add-text smoothing
Smoothing is another aspect worthy of considera-
tion. No matter how large the reference used is, it
will never constitute more than a subset of the lan-
guage. Therefore, it is necessary to find a solution
to estimate the frequency of unobserved n-grams.
For the baseline, we used a simple "add-one? (or
Laplace) smoothing (Manning and Sch?tze, 1999)
which presents a severe flaw when the size of the n-
grams to smooth increases : the normalization pro-
88
cess discounts too much probability mass from ob-
served events.
We therefore compare this simple method with
another one we consider more ?natural? : the ?add-
text? smoothing that adds the text to process to the
reference. We view this method as more natural to
the extent that it simulates a standard MWE extrac-
tion process. In this case, the reference complements
the frequency universe of the input corpus as if it for-
med a homogeneous whole. Figure 2 demonstrates a
clear superiority of the second smoothing procedure
over the first one which was therefore discarded.
4.2.3 Lemmatization and HDV
The lemmatization and HDV follow a similar
curve with regard to precision, although HDV is bet-
ter for recall. Nonetheless, this difference only ap-
pears when precision falls below 35%. This does
not seem sufficient to reject the lemmatization pro-
cess whose computation time is significantly lower
than for the HDV. We therefore limit the use of this
last parameter to the reference built from Google
whose n-grams cannot be lemmatized due to lack of
context. 11
4.3 Evaluation of the references
The estimation of the parameters allowed us to es-
tablish a specific evaluation framework. Two sets of
parameters were defined depending on whether they
apply to Google (ATS + HDV) or to the references
built from Le Soir (ATS + LEMMA). From a prac-
tical standpoint, we limited the MWE extraction to
nominal units of size inferior to five in order to meet
the characteristics of our test corpus (the annotations
of which are limited to nominal sequences), on the
one hand, and to allow comparability of results on
the other hand (the n-grams from Google do not ex-
ceed the order 5).
Initially, we considered the extraction of MWEs
in the whole evaluation corpus. Results displayed in
Figure 3 provide an advantage over the use of a refe-
rence with respect to the extraction carried out on the
test corpus only. In addition, we see a clear improve-
ment in performance with respect to that obtainable
with a dictionary of MWEs. 12
11. References constructed on the basis of the newspaper Le
Soir have been reindexed from a lemmatized text.
12. The MWE dictionary used in this experiment was ini-
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n 
(%
)
MWE (%)
References -- Precision
500K Words
1000K Words
5000K Words
Google (1T Words)
Text Only (3K)
External Dictionary
 0
 10
 20
 30
 40
 50
 60
 0  10  20  30  40  50  60  70  80  90  100
R
ec
al
l (%
)
MWE (%)
References -- Recall
500K Words
1000K Words
5000K Words
Google (1T Words)
Text Only (3K)
External Dictionary
FIGURE 3: Evaluation on the 100K Corpus
In a second step, we wanted to test the efficiency
of our references in the more adverse context of a
short text. We randomly selected 3K words of our
test corpus to simulate a short text while maintai-
ning a sufficient number of MWEs (i.e. 151 nominal
MWEs). Results shown in Figure 4 further confirm
our first experience and validate our concept of a re-
ference in a real application context.
Beyond validating the use of a frequency base,
these results also confirm the general idea that the
size of the corpus used for the reference matters. The
differences between the references of 500K, 1000K
and 5000K words showed a continuous improve-
ment both in precision and recall. The results obtai-
ned with the Google reference are more surprising,
since they do not meet that growing trend. Howe-
ver, given the number of errors that those n-grams
contain (mainly due to the OCR-ization and tokeni-
tially derived from the corpus of 5000K words used to build the
corresponding reference.
89
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n 
(%
)
MWE (%)
References (3K) -- Precision
500K Words
1000K Words
5000K Words
Google (1T Words)
Text Only (3K)
External Dictionary
 0
 10
 20
 30
 40
 50
 60
 0  10  20  30  40  50  60  70  80  90  100
R
ec
al
l (%
)
MWE (%)
References (3K) -- Recall
500K Words
1000K Words
5000K Words
Google (1T Words)
Text Only (3K)
External Dictionary
FIGURE 4: Evaluation on the 3K Corpus
zation processes), the result remains satisfactory. It
even confirms to some extent the importance of size
in the sense that preprocessing errors are being miti-
gated by the global mass of the frequencies.
5 Conclusion and perspectives
In this paper, we presented an MWE extraction
system based on the use of frequency references. We
have shown that its use enables MWE extraction on
short texts with performances that are at least com-
parable to those achieved by standard solutions and
far superior to solutions based on the use of MWE
dictionaries.
Moreover, as this system has been integrated wi-
thin an indexing engine, various issues were rai-
sed, some of which constitute avenues for future re-
search. First, since our indexer aims at the identifi-
cation of entities and terms specific to a given spe-
cialty area, the question of data representativeness
is of particular importance. It is not clear to what
MWE 500 K 1000 K 5000 K Google
m?me
groupe
0.73 1.44 3.85 1,746.03
nouveaux
instruments
3.81 3.3 49.83 2,793.65
lettres de
noblesse
33.99 52.43 232.51 27,202.17
TABLE 2: Examples of MWEs candidates whose log-
likelihood ratio is not significant on a small corpus and
becomes extremely significant on a large corpus. They
are compared to the score of an actual MWE.
extent a given reference can be applied to various
types of texts. We only noticed that the Google refe-
rence, whose features were less similar to the test
corpus, nevertheless yielded satisfactory results in
comparison with our other references that better fit-
ted the test corpus features.
In addition, our results show that the threshold is-
sue remains relevant. Although the LocalMax seems
to allow better discrimination of the MWE candi-
dates, it is not selective enough to keep only the ac-
tual MWEs. On the other hand, as the size of the
references increases, some results of the AMs based
on the log-likelihood ratio reach high values that can
no longer be interpreted by a chi-square significance
test (see Table 2).
We believe that our references offer an interes-
ting perspective to face this problem. The stability of
their frequencies makes it possible to define a thre-
shold corresponding to a specific percentage of pre-
cision and recall (set according to the needs of a gi-
ven application). Therefore, as long as the size of
the analyzed texts remains limited ? which can be
controlled ?, the efficiency of this threshold should
remain constant. Further experimentations on this
aspect are however required to determine to what
extent this assumption stands true as the size of the
analyzed texts grows.
References
K.W. Church and W.A. Gale. 1991. Concordances for
parallel text. In Proceedings of the Seventh Annual
Conference of the UW Centre for the New OED and
Text Research, pages 40?62.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1) :22?29.
90
J. da Silva and G.P. Lopes. 1999. A local maxima me-
thod and a fair dispersion normalization for extracting
multi-word units from corpora. In Sixth Meeting on
Mathematics of Language.
B. Daille. 1995. Combined approach for terminology
extraction : lexical statistics and linguistic filtering.
Technical report, Lancaster University.
G. Dias, S. Guillor?, and J.G.P. Lopes. 1999. Language
independent automatic acquisition of rigid multiword
units from unrestricted text corpora. Proceedings of
the 6th Conference on the Traitement Automatique des
Langues Naturelles (TALN1999), pages 333?339.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational linguistics,
19(1) :61?74.
S. Evert and B. Krenn. 2001. Methods for the qualitative
evaluation of lexical association measures. In Procee-
dings of the 39th Annual Meeting on Association for
Computational Linguistics, pages 188?195.
A. Kilgarriff. 2005. Language is never ever ever random.
Corpus linguistics and linguistic theory, 1(2) :263?
276.
E. Laporte, T. Nakamura, and S. Voyatzi. 2006. A french
corpus annotated for multiword expressions with ad-
verbial function. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC) : Linguis-
tic Annotation Workshop, pages 48?51.
C.D. Manning and H. Sch?tze, editors. 1999. Founda-
tions of Statistical Natural Language Processing. MIT
Press.
J.B. Michel, Y.K. Shen, A.P. Aiden, A. Veres, M.K. Gray,
The Google Books Team, J.P. Pickett, D. Hoiberg,
D. Clancy, P. Norvig, J. Orwant, S. Pinker, M.A. No-
wak, and E.L. Aiden. 2011. Quantitative analysis
of culture using millions of digitized books. Science,
331(6014) :176?182.
D.R. Morrison. 1968. PATRICIA?practical algorithm
to retrieve information coded in alphanumeric. Jour-
nal of the ACM, 15(4) :514?534.
L. Nerima, V. Seretan, and E. Wehrli. 2006. Le pro-
bl?me des collocations en TAL. Nouveaux cahiers de
linguistique fran?aise, 27 :95?115.
J. Nivre and J. Nilsson. 2004. Multiword units in syn-
tactic parsing. In Proceedings of LREC-04 Workshop
on Methodologies & Evaluation of Multiword Units in
Real-world Applications, pages 37?46.
S. Paumier. 2003. De la reconnaissance de formes lin-
guistiques ? l?analyse syntaxique. Ph.D. thesis, Uni-
versit? de Marne-la-Vall?e.
D. Pearce. 2002. A comparative evaluation of colloca-
tion extraction techniques. In Proc. of the 3rd Inter-
national Conference on Language Resources and Eva-
luation (LREC 2002), pages 1530?1536.
P. Pecina and P. Schlesinger. 2006. Combining associa-
tion measures for collocation extraction. In Procee-
dings of the 21th International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics (CO-
LING/ACL 2006), pages 651?658.
Z. Ren, Y. L, J. Cao, Q. Liu, and Y. Huang. 2009. Im-
proving statistical machine translation using domain
bilingual multiword expressions. In Proceedings of
the Workshop on Multiword Expressions : Identifica-
tion, Interpretation, Disambiguation and Applications,
pages 47?54.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Fli-
ckinger. 2001. Multiword expressions : A pain in the
neck for NLP. In In Proc. of the 3rd International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2002), pages 1?15.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of International
Conference on New Methods in Language Processing,
volume 12. Manchester, UK.
S. Sekine. 2008. A linguistic knowledge discovery tool :
Very large ngram database search with arbitrary wild-
cards. In COLING : Companion volume : Demonstra-
tions, pages 181?184.
V. Seretan, L. Nerima, and E. Wehrli. 2003. Extrac-
tion of Multi-Word Collocations Using Syntactic Bi-
gram Composition. In Proceedings of the 4th In-
ternational Conference on Recent Advances in NLP
(RANLP2003), pages 424?431.
J. da Silva, G. Dias, S. Guillor?, and J. Pereira Lopes.
1999. Using localmaxs algorithm for the extraction
of contiguous and non-contiguous multiword lexical
units. Progress in Artificial Intelligence, pages 849?
849.
F. Smadja. 1993. Retrieving collocations from text :
Xtract. Computational Linguistics, 19 :143?177.
O. Vechtomova. 2005. The role of multi-word units
in interactive information retrieval. In D.E. Losada
and J.M. Fern?ndez-Luna, editors, ECIR 2005, LNCS
3408, pages 403?420. Springer-Verlag, Berlin.
P. Watrin. 2007. Collocations et traitement automatique
des langues. In Actes du 26e Colloque international
sur le lexique et la grammaire, pages 1530?1536.
91
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 49?57,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Do NLP and machine learning improve traditional readability formulas?
Thomas Franc?ois
University of Pennsylvania
CENTAL, UCLouvain
3401 Walnut Street Suite 400A
Philadelphia, PA 19104, US
frthomas@sas.upenn.edu
Eleni Miltsakaki
University of Pennsylvania & Choosito!
3401 Walnut Street Suite 400A
Philadelphia, PA 19104, US
elenimi@seas.upenn.edu
Abstract
Readability formulas are methods used to
match texts with the readers? reading level.
Several methodological paradigms have pre-
viously been investigated in the field. The
most popular paradigm dates several decades
back and gave rise to well known readability
formulas such as the Flesch formula (among
several others). This paper compares this ap-
proach (henceforth ?classic?) with an emerg-
ing paradigm which uses sophisticated NLP-
enabled features and machine learning tech-
niques. Our experiments, carried on a corpus
of texts for French as a foreign language, yield
four main results: (1) the new readability for-
mula performed better than the ?classic? for-
mula; (2) ?non-classic? features were slightly
more informative than ?classic? features; (3)
modern machine learning algorithms did not
improve the explanatory power of our read-
ability model, but allowed to better classify
new observations; and (4) combining ?classic?
and ?non-classic? features resulted in a signif-
icant gain in performance.
1 Introduction
Readability studies date back to the 1920?s and have
already spawned probably more than a hundred pa-
pers with research on the development of efficient
methods to match readers and texts relative to their
reading difficulty. During this period of time, sev-
eral methodological trends have appeared in suc-
cession (reviewed in Klare (1963; 1984), DuBay
(2004)). We can group these trends in three ma-
jor approaches: the ?classic studies?, the ?structuro-
cognitivist paradigm? and the ?AI readability?, a
term suggested by Franc?ois (2011a).
The classic period started right after the seminal
work of Vogel and Washburne (1928) and Gray and
Leary (1935) and is characterized by an ideal of sim-
plicity. The models (readability formulas) proposed
to predict text difficulty for a given population are
kept simple, using multiple linear regression with
two, or sometimes, three predictors. The predictors
are simple surface features, such as the average num-
ber of syllables per word and the average number of
words per sentence. The Flesch (1948) and Dale and
Chall (1948) formulas are probably the best-known
examples of this period.
With the rise of cognitivism in psychological
sciences in the 70?s and 80?s, new dimensions of
texts are highlighted such as coherence, cohesion,
and other discourse aspects. This led some schol-
ars (Kintsch and Vipond, 1979; Redish and Selzer,
1985) to adopt a critical attitude to classic readabil-
ity formulas which could only take into account su-
perficial features, ignoring other important aspects
contributing to text difficulty. Kintsch and Vipond
(1979) and Kemper (1983), among others, suggested
new features for readability, based on those newly
discovered text dimensions. However, despite the
fact that the proposed models made use of more so-
phisticated features, they failed to outperform the
classic formulas. It is probably not coincidental that
after these attempts readability research efforts de-
clined in the 90s.
More recently, however, the development of ef-
ficient natural language processing (NLP) systems
and the success of machine learning methods led to
49
a resurgence of interest in readability as it became
clear that these developments could impact the de-
sign and performance of readability measures. Sev-
eral studies (Si and Callan, 2001; Collins-Thompson
and Callan, 2005; Schwarm and Ostendorf, 2005;
Feng et al, 2010) have used NLP-enabled feature
extraction and state-of-the-art machine learning al-
gorithms and have reported significant gains in per-
formance, suggesting that the AI approach might be
superior to previous attempts.
Going beyond reports of performance which are
often hard to compare due to a lack of a common
gold standard, we are interested in investigating AI
approaches more closely with the aim of understand-
ing the reasons behind the reported superiority over
classic formulas. AI readability systems use NLP
for richer feature extraction and a machine learning
algorithm. Given that the classic formulas are also
statistical, is performance boosted because of the ad-
dition of NLP-enabled feature extraction or by better
machine learning algorithms? In this paper, we re-
port initial findings of three experiments designed to
explore this question.
The paper is organized as follows. Section 2 re-
views previous findings in the field and the challenge
of providing a uniform explanation for these find-
ings. Section 3 gives a brief overview of prior work
on French readability, which is the context of our
experiments (evaluating the readability of French
texts). Because there is no prior work comparing
classic formulas with AI readablity measures for
French, we first report the results of this compari-
son in Section 3. Then, we proceed with the results
of three experiments (2-4), comparing the contribu-
tions of the AI enabled features with features used
in classic formulas, different machine learning al-
gorithms and the interactions of features with algo-
rithms. There results are reported in Sections 4, 5,
and 6, respectively. We conclude in Section 7 with a
summary of the main findings and future work.
2 Previous findings
Several readability studies in the past decade have
reported a performance gain when using NLP-
enabled features, language models, and machine
learning algorithms to evaluate the reading difficulty
of a variety of texts (Si and Callan, 2001; Collins-
Thompson and Callan, 2005; Schwarm and Osten-
dorf, 2005; Heilman et al, 2008; Feng et al, 2010).
A first explanation for this superiority would be
related to the new predictors used in recent mod-
els. Classic formulas relied mostly on surface lexical
and syntactic variables such as the average number
of words per sentence, the average number of letters
per word, the proportion of given POS tags in the
text or the proportion of out-of-simple-vocabulary
words. In the AI paradigm, several new features
have been added, including language models, parse
tree-based predictors, probability of discourse rela-
tions, estimates of text coherence, etc. It is rea-
sonable to assume that these new features capture a
wider range of readability factors thus bringing into
the models more and, possibly, better information.
However, the evidence from comparative studies
is not consistent on this question. In several cases,
AI models include features central to classic formu-
las which, when isolated, appear to be the stronger
predictors in the models. An exception to this trend
is the work of Pitler and Nenkova (2008) who re-
ported non-significant correlation for the mean num-
ber of words per sentence (r = 0.1637, p = 0.3874)
and the mean number of characters per word (r =
?0.0859, p = 0.6519). In their study, though, they
used text quality rather than text difficulty as the de-
pendent variable. The data consisted solely of text
from the Wall Street Journal which is ?intended for
an educated adult audience? text labelled for de-
grees of reading fluency. Feng et al (2010) com-
pared a set of similar variables and observed that
language models performed better than classic for-
mula features but classic formula features outper-
formed those based on parsing information. Collins-
Thompson and Callan (2005) found that the classic
type-token ratio or number of words not in the 3000-
words Dale list appeared to perform better than their
language model on a corpus from readers, but were
poorer predictors on web-extracted texts.
In languages other than English, Franc?ois (2011b)
surveyed a wide range of features for French and
reports that the feature that uses a limited vocabu-
lary list (just like in some classic formulas) has a
stronger correlation with reading difficulty that a un-
igram model and the best performing syntactic fea-
ture was the average number of words per sentences.
Aluisio et al (2010), also, found that the best corre-
50
late with difficulty was the average number of words
per sentence. All in all, while there is sufficient ev-
idence that the AI paradigm outperforms the classis
formulas, classic features have often been shown to
make the single strongest predictors.
An alternative explanation could be that, by com-
parison to the simpler statistical analyses that deter-
mined the coefficients of the classic formulas, ma-
chine learning algorithms, such as support machine
vector (SVM) or logistic regression are more sophis-
ticated and better able to learn the regularities in
training data, thus building more accurate models.
Work in this direction has been of smaller scale but
already reporting inconsistent results. Heilman et al
(2008) considered the performance of linear regres-
sion, ordinal and multinomial logistic regression,
and found the latter to be more efficient. However,
Kate et al (2010) obtained contradictory findings,
showing that regression-based algorithms perform
better, especially when regression trees are used for
bagging. For French, Franc?ois (2011b) found that
SVMs were more efficient than linear regression, or-
dinal and multinomial logistic regression, boosting,
and bagging.
Finally, it is quite possible that there are interac-
tions between types of features and types of statis-
tical algorithms and these interactions are primarily
responsible for the better performance.
In what follows, we present the results of three
studies (experiments 2-4), comparing the contribu-
tions of the AI enabled features with features used
in classic formulas, different machine learning al-
gorithms and the interactions of features with algo-
rithms. As mentioned earlier, all the studies have
been done on French data, consisting of text ex-
tracted from levelled FFL textbooks (French as For-
eign Language). Because there is no prior work
comparing classic formulas with AI readability mea-
sures for FFL, we first report the results of this com-
parison in the next section (experiment 1).
3 Experiment 1: Model comparison for
FFL
To compute a classic readability formula for FFL,
we used the formula proposed for French by Kandel
and Moles (1958). We compared the results of this
formula with the AI model trained on the FFL data
used by Franc?ois (2011b).
The Kandel and Moles (1958) formula is an adap-
tation of the Flesch formula for French, based on a
study of a bilingual corpus:
Y = 207? 1.015lp? 0.736lm (1)
where Y is a readability score ranging from 100
(easiest) to 0 (harder); lp is the average number of
words per sentence and lm is the average number of
syllables per 100 words. Although this formula is
not specifically designed for FFL, we chose to im-
plement it over formulas proposed for FFL (Tharp,
1939; Uitdenbogerd, 2005). FFL-specific formu-
las are optimized for English-speaking learners of
French while our dataset is agnostic to the native
language of the learners.
The computation of the Kandel and Moles (1958)
formula requires a syllabification system for French.
Due to unavailability of such a system for French,
we adopted a hybrid syllabification method. For
words included in Lexique (New et al, 2004), we
used the gold syllabification included in the dictio-
nary. For all other words, we generated API pho-
netic representations with espeak 1, and then applied
the syllabification tool used for Lexique3 (Pallier,
1999). The accuracy of this process exceeded 98%.
For the comparison with an AI model, we ex-
tracted the same 46 features (see Table 2 for the
complete list) used in Franc?ois? model 2 and trained
a SVM model.
For all the study, the gold-standard consisted of
data taken from textbooks and labeled according to
the classification made by the publishers. The cor-
pus includes a wide range of texts, including ex-
tracts from novels, newspapers articles, songs, mail,
dialogue, etc. The difficulty levels are defined by
the Common European Framework of Reference for
Languages (CEFR) (Council of Europe, 2001) as
follows: A1 (Breakthrough); A2 (Waystage); B1
(Threshold); B2 (Vantage); C1 (Effective Opera-
tional Proficiency) and C2 (Mastery). The test cor-
pus includes 68 texts per level, for a total of 408 doc-
uments (see Table 1).
We applied both readability models to this test
corpus. Assessing and comparing the performance
1Available at: http://espeak.sourceforge.net/.
2Details on how to implement these features can be found in
Franc?ois (2011b).
51
A1 A2 B1 B2 C1 C2 Total
68(10, 827) 68(12, 045) 68(17, 781) 68(25, 546) 68(92, 327) 68(39, 044) 408(127, 681)
Table 1: Distribution of the number of texts and tokens per level in our test corpus.
of the two models with accuracy scores (acc), as is
common in classification tasks, has proved challeng-
ing and, in the end, uninformative. This is because
the Kandel and Moles formula?s output scores are
not an ordinal variable, but intervals. To compute
accuracy we would have to define a set of rather
arbitrary cut off points in the intervals and corre-
spond them with level boundaries. We tried three
approaches to achieve this task. First, we used
correspondences between Flesch scores and seven
difficulty levels proposed for French by de Land-
sheere (1963): ?very easy? (70 to 80) to ?very dif-
ficult? (-20 to 10). Collapsing the ?difficult? and
?very difficult? categories into one, we were able
to roughly match this scale with the A1-C2 scale.
The second method was similar, except that those
levels were mapped on the values from the original
Flesch scale instead of the one adapted for French.
The third approach was to estimate normal distribu-
tion parameters ?j and ?j for each level j for the
Kandel and Moles? formula output scores obtained
on our corpus. The class membership of a given ob-
servation i was then computed as follows:
arg
6
max
j=1
P (i ? j | N(?j , ?j)) (2)
Since the parameters were trained on the same cor-
pus used for the evaluation, this computation should
yield optimal class membership thresholds for our
data.
Given the limitations of all three approaches, it is
not surprising that accuracy scores were very low:
9% for the first and 12% for the second, which is
worse than random (16.6%). The third approach
gave a much improved accuracy score, 33%, but still
quite low. The problem is that, in a continuous for-
mula, predictions that are very close to the actual
will be classified as errors if they fall on the wrong
side of the cut off threshold. These results are, in
any case, clearly inferior to the AI formula based on
SVM, which classified correctly 49% of the texts.
A more suitable evaluation measure for a contin-
uous formula would be to compute the multiple cor-
relation (R). The multiple correlation indicates the
extent to which predictions are close to the actual
classes, and, when R2 is used, it describes the per-
centage of the dependent variable variation which
is explained by the model. Kandel and Moles? for-
mula got a slightly better performance (R = 0.551),
which is still substantially lower that the score (R =
0.728) obtained for the SVM model. To check if
the difference between the two correlation scores
was significant, we applied the Hotelling?s T-test for
dependent correlation (Hotelling, 1940) (required
given that the two models were evaluated on the
same data). The result of the test is highly signif-
icant (t = ?19.5; p = 1.83e?60), confirming that
the SVM model performed better that the classic for-
mula.
Finally, we computed a partial Spearman corre-
lation for both models. We considered the output
of each model as a single variable and we could,
therefore, evaluate the relative predictive power of
each variable when the other variable is controlled.
The partial correlation for the Kandel and Moles for-
mula is very low (? = ?0.11; p = 0.04) while
the SVM model retains a good partial correlation
(? = ?0.53; p < 0.001).
4 Experiment 2: Comparison of features
In this section, we compared the contribution of the
features used in classic formulas with the more so-
phisticated NLP-enabled features used in the ma-
chine learning models of readability. Given that the
features used in classic formulas are very easy to
compute and require minimal processing by com-
parison to the NLP features that require heavy pre-
processing (e.g., parsing), we are, also, interested in
finding out how much gain we obtain from the NLP
features. A consideration that becomes important
for tasks requiring real time evaluation of reading
difficulty.
To evaluate the relative contribution of each set
of features, we experiment with two sets of fea-
tures (see Table 2. We labeled as ?classic?, not only
52
Family Tag Description of the variable ? Linear
Classic
PA-Alterego
Proportion of absent words from a list
0.652 No
of easy words from AlterEgo1
X90FFFC 90th percentile of inflected forms for content words only ?0.641 No
X75FFFC 75th percentile of inflected forms for content words only ?0.63 No
PA-Goug2000
Proportion of absent words from 2000 first
0.597 No
of Gougenheim et al (1964)?s list
MedianFFFC Median of the frequencies of inflected content words ?0.56 Yes
PM8 Pourcentage of words longer than 8 characters 0.525 No
NL90P
Length of the word corresponding to
0.521 No
the 90th percentile of word lengths
NLM Mean number of letters per word 0.483 Yes
IQFFFC Interquartile range of the frequencies of inflected content words 0.405 No
MeanFFFC Mean of the frequencies of inflected content words ?0.319 No
TTR Type-token ratio based on lemma 0.284 No
NMP Mean number of words per sentence 0.618 No
NWS90 Length (in words) of the 90th percentile sentence 0.61 No
PL30 Percentage of sentences longer than 30 words 0.56 Yes
PRE/PRO Ratio of prepositions and pronouns 0.345 Yes
GRAM/PRO Ratio of grammatical words and pronouns 0.34 Yes
ART/PRO Ratio of articles and pronouns 0.326 Yes
PRE/ALL Proportions of prepositions in the text 0.326 Yes
PRE/LEX Ratio of prepositions and lexical words 0.322 Yes
ART/LEX Ratio of articles and lexical words 0.31 Yes
PRE/GRAM Ratio of prepositions and grammatical words 0.304 Yes
NOM-NAM/ART Ratio of nouns (common and proper) and gramm. words ?0.29 Yes
PP1P2 Percentage of P1 and P2 personal pronouns ?0.333 No
PP2 Percentage of P2 personal pronouns ?0.325 Yes
PPD Percentage of personal pronouns of dialogue 0.318 No
BINGUI Presence of commas 0, 462 No
Non-classic
Unigram Probability of the text sequence based on unigrams 0.546 No
MeanNGProb-G Average probability of the text bigrams based on Google 0.407 Yes
FCNeigh75 75th percentile of the cumulated frequency of neighbors per word ?0.306 Yes
MedNeigh+Freq Median number of more frequent neighbor for words ?0.229 Yes
Neigh+Freq90 90th percentile of more frequent neighbor for words ?0.192 Yes
PPres Presence of at least one present participle in the text 0.44 No
PPres-C Proportion of present participle among verbs 0.41 Yes
PPasse Presence of at least one past participle 0.388 No
Infi Presence of at least one infinive 0.341 No
Impf Presence of at least one imperfect 0.272 No
Subp Presence of at least one subjunctive present 0.266 Yes
Futur Presence of at least one future 0.252 No
Cond Presence of at least one conditional 0.227 No
PasseSim Presence of at least one simple past 0.146 No
Imperatif Presence of at least one imperative 0.019 Yes
Subi Presence of at least one subjunctive imperfect 0.049 Yes
avLocalLsa-Lem Average intersentential cohesion measured via LSA 0, 63 No
ConcDens
Estimate of the conceptual density
0.253 Yes
with Denside?es (Lee et al, 2010)
NAColl Proportion of MWE having the structure NOUN ADJ 0.286 Yes
NCPW Average number of MWEs per word 0.135 Yes
Table 2: List of the 46 features used by Franc?ois (2011b) in his model. The Spearman correlation reported here also
comes from this study.
53
the features that are commonly used in traditional
formulas like Flesch (length of words and number
of words per sentence) but also other easy to com-
pute features that were identified in readability work.
Specifically, in the ?classic? set we include num-
ber of personal pronouns (given as a list) (Gray and
Leary, 1935), the Type Token Ratio (TTR) (Lively
and Pressey, 1923), or even simple ratios of POS
(Bormuth, 1966).
The ?non-classic? set includes more complex
NLP-enabled features (coherence measured through
LSA, MWE, n-grams, etc.) and features suggested
by the structuro-cognitivist research (e.g., informa-
tion about tense and variables based on orthograph-
ical neighbors).
For evaluation, we first computed and compared
the average bivariate correlations of both sets. This
test yielded a better correlation for the classic fea-
tures (r? = 0.48 over the non-classic features r? =
0.29)
As a second test, we trained a SVM model on each
set and evaluated performances in a ten-fold cross-
validation. For this test, we reduced the number of
classic features by six to equal the number of pre-
dictors of the non-classic set. Our hypothesis was
the SVM model using non-classic features would
outperform the classic set because the non-classic
features bring richer information. This assumption
was not strictly confirmed as the non-classic set per-
formed only slightly better than the classic set. The
difference in the correlation scores was small (0.01)
and non-significant (t(9) = 0.49; p = 0.32), but the
difference in accuracy was larger (3.8%) and close to
significance (t(9) = 1.50; p = 0.08). Then, in an ef-
fort to pin down the source of the SVM gain that did
not come out in the comparison above, we defined a
SVM baseline model (b) that included only two typ-
ical features of the classic set: the average number
of letter per word (NLM) and the average number of
word per sentence (NMP). Then, for each of the i
remaining variables (44), we trained a model mi in-
cluding three predictors: NLM, NMP, and i. The
difference between the correlation of the baseline
model and that of the model mi was interpreted as
the information gain carried by the feature i. There-
fore, for both sets, of cardinality Ns, we computed:
?Ns
i=1R(mi)?R(b)
Ns
(3)
where R(mi) is the multiple correlation of model
mi.
Our assumption was that, if the non-classic set
brings in more varied information, every predictor
should, on average, improve more theR of the base-
line model, while the classic variables, more redun-
dant with NLM and NP, would be less efficient. In
this test, the mean gain for R was 0.017 for the clas-
sic set and 0.022 for the non-classic set. Although
the difference was once more small, this test yielded
a similar trend than the previous test.
As a final test, we compared the performance of
the SVM model trained only on the ?classic? set
with the SVM trained on both sets. In this case,
the improvement was significant (t(9) = 3.82; p =
0.002) with accuracy rising from 37.5% to 49%. Al-
though this test does not help us decide on the nature
of the gain as it could be coming just from the in-
creased number of features, it shows that combining
?classic? and ?non-classic? variables is valuable.
5 Experiment 3: Comparison of statistical
models
In this section, we explore the hypothesis that AI
models outperform classic formulas because they
use better statistical algorithms. We compare the
performance of a?classic? algorithm, multiple linear
regression, with the performance of a machine learn-
ing algorithm, in this case SVM. Note that an SVMs
have an advantage over linear regression for features
non-linearly related with difficulty. Bormuth (1966,
98-102) showed that several classic features, espe-
cially those focusing on the word level, were indeed
non-linear. To control for linearity, we split the 46
features into a linear and a non-linear subset, using
the Guilford?s F test for linearity (Guilford, 1965)
and an ? = 0.05. This classification yielded two
equal sets of 23 variables (see Table 2). In Table
3, we report the performance of the four models in
terms of R, accuracy, and adjacent accuracy. Fol-
lowing, Heilman et al (2008), we define ?adjacent
accuracy? as the proportion of predictions that were
within one level of the assigned label in the corpus.
54
Model R Acc. Adj. acc.
Linear
LR 0.58 27% 72%
SVM 0.64 38% 73%
Non-Linear
LR 0.75 36% 81%
SVM 0.70 44% 76%
Table 3: Multiple correlation coefficient (R), accuracy
and adjacent accuracy for linear regression and SVM
models, using the set of features either linearly or non
linearly related to difficulty.
Adjacent accuracy is closer toR as it is less sensitive
to minor classification errors.
Our results showed a contradictory pattern, yield-
ing a different result depending on type of evalu-
tion: accuracy or R and adjacent accuracy. With
respect to accuracy scores, the SVM performed bet-
ter in the classification task, with a significant per-
formance gain for both linear (gain = 9%; t(9) =
2.42; p = 0.02) and non-linear features (gain = 8%;
t(9) = 3.01; p = 0.007). On the other hand, the dif-
ference in R was non-significant for linear (gain =
0.06; t(9) = 0.80; p = 0.22) and even negative and
close to significance for non-linear (gain = ?0.05;
t(9) = 1.61; p = 0.07). In the light of these re-
sults, linear regression (LR) appears to be as effi-
cient as SVM accounting for variation in the depen-
dant variable (their R2 are pretty similar), but pro-
duces poorer predictions.
This is an interesting finding, which suggests that
the contradictory results in prior literature with re-
gard to performance of different readability mod-
els (see Section 2) might be related to the evalua-
tion measure used. Heilman et al (2008, 7), who
compared linear and logistic regressions, found that
the R of the linear model was significantly higher
than the R of the logistic model (p < 0.01). In con-
trast, the logistic model behaved significantly better
(p < 0.01) in terms of adjacent accuracy. Similarly,
Kate and al. (2010, 548), which used R as evalua-
tion measure, reported that their preliminary results
?verified that regression performed better than clas-
sification?. Once they compared linear regression
and SVM regression, they noticed similar correla-
tions for both techniques (respectively 0.7984 and
0.7915).
To conclude this section, our findings suggest that
(1) linear regression and SVM are comparable in ac-
counting for the variance of text difficulty and (2)
SVM has significantly better accuracy scores than
linear regression.
6 Experiment 4: Combined evaluation
In Experiment 2, we saw that ?non-classic? features
are slightly, but non-significantly, better than the
?classic? features. In Experiment 3, we saw that
SVM performs better than linear regression when
the evaluation is done by accuracy but both demon-
strate similar explanatory power in accounting for
the variation. In this section, we report evaluation
results for four models, derived by combining two
sets of features, classic and non-classic, with two al-
gorithms, linear regression and SVM. The results are
shown in Table (4).
The results are consistent with the findings in
the previous sections. When evaluated with accu-
racy scores SVM performs better with both classic
(t(9) = 3.15; p = 0.006) and non-classic features
(t(9) = 3.32; p = 0.004). The larger effect obtained
for the non-classic features might be due to an in-
teraction, i.e., an SVM trained with non-classic fea-
tures might be better at discriminating reading lev-
els. However, with respect to R, both algorithms are
similar, with linear regression outperforming SVM
in adjacent accuracy (non-significant). Linear re-
gression and SVM, then, appear to have equal ex-
planatory power.
As regards the type of features, the explanatory
power of both models seems to increase with non-
classic features as shown in the increased R, al-
though significance is not reached (t(9) = 0.49; p =
0.32 for the regression and t(9) = 1.5; p = 0.08 for
the SVM).
7 General discussion and conclusions
Recent readability studies have provided prelimi-
nary evidence that the evaluation of readability us-
ing NLP-enabled features and sophisticated machine
learning algorithms outperform the classic readabil-
ity formulas, such as Flesch, which rely on surface
textual features. In this paper, we reported a number
of experiments the purpose of which was to identify
the source of this performance gain.
Specifically, we compared the performance of
classic and non-classic features and the performance
55
Model R Acc. Adj. acc.
Classic
LR 0.66 30.6% 78%
SVM 0.67 37.5% 76%
Non-classic
LR 0.68 32% 76%
SVM 0.68 41.8% 73%
Table 4: Multiple correlation coefficient (R), accuracy and adjacent accuracy for linear regression and SVM models
with either the classic or the non-classic set of predictors.
of two statistical algorithms: linear regression (used
in classic formulas) and SVM (in the context of FFL
readability). Our results indicate that classic features
are strong single predictors of readability. While
we were not able to show that the non-classic fea-
tures are better predictors by themselves, our find-
ings show that leaving out non-classic features has a
significant negative impact on the performance. The
best performance was obtained when both classic
and non-classic features were used.
Our experiments on the comparison of the two
statistical algorithms showed that the SVM outper-
forms linear regression by a measure of accuracy,
but the two algorithms are comparable in explana-
tory power accounting for the same amount of vari-
ability. This observation accounts for contradictory
conclusions reported in previous work. Our study
shows that different evaluation measures can lead to
quite different conclusions.
Finally, our comparison of four models derived
by combining linear regression and SVM with ?clas-
sic? and ?non-classic? features confirms the signif-
icant contribution of ?non-classic? features and the
SVM algorithm to classification accuracy. However,
by a measure of adjacent accuracy and explanatory
power, the two algorithms are comparable.
From a practical application point of view, it
would be interesting to try these algorithms in web
applications that process large amounts of text in
real time (e.g., READ-X (Miltsakaki, 2009)) to eval-
uate the trade-offs between accuracy and efficiency.
Acknowledgments
We would like to acknowledge the invaluable help of
Bernadette Dehottay for the collection of the corpus,
as well as the Belgian American Educational Foun-
dation that supported Dr. Thomas Franc?ois with a
Fellowship during this work.
References
S. Aluisio, L. Specia, C. Gasperin, and C. Scarton. 2010.
Readability assessment for text simplification. In Fifth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 1?9, Los Angeles.
J.R. Bormuth. 1966. Readability: A new approach.
Reading research quarterly, 1(3):79?132.
K. Collins-Thompson and J. Callan. 2005. Predict-
ing reading difficulty with statistical language models.
Journal of the American Society for Information Sci-
ence and Technology, 56(13):1448?1462.
Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teach-
ing, Assessment. Press Syndicate of the University of
Cambridge.
E. Dale and J.S. Chall. 1948. A formula for predicting
readability. Educational research bulletin, 27(1):11?
28.
G. de Landsheere. 1963. Pour une application des tests
de lisibilite? de Flesch a` la langue franc?aise. Le Travail
Humain, 26:141?154.
W.H. DuBay. 2004. The principles of read-
ability. Impact Information. Disponible sur
http://www.nald.ca/library/research/readab/readab.pdf.
L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.
2010. A Comparison of Features for Automatic Read-
ability Assessment. In COLING 2010: Poster Volume,
pages 276?284.
R. Flesch. 1948. A new readability yardstick. Journal of
Applied Psychology, 32(3):221?233.
T. Franc?ois. 2011a. La lisibilite? computationnelle
: un renouveau pour la lisibilite? du franc?ais langue
premie`re et seconde ? International Journal of Ap-
plied Linguistics (ITL), 160:75?99.
T. Franc?ois. 2011b. Les apports du traitement au-
tomatique du langage a` la lisibilite? du franais langue
e?trange`re. Ph.D. thesis, Universite? Catholique de Lou-
vain. Thesis Supervisors : Ce?drick Fairon and Anne
Catherine Simon.
G. Gougenheim, R. Miche?a, P. Rivenc, and A. Sauvageot.
1964. Le?laboration du franc?ais fondamental (1er
degre?). Didier, Paris.
56
W.S. Gray and B.E. Leary. 1935. What makes a book
readable. University of Chicago Press, Chicago: Illi-
nois.
J.P. Guilford. 1965. Fundamental statistics in psychol-
ogy and education. McGraw-Hill, New-York.
M. Heilman, K. Collins-Thompson, and M. Eskenazi.
2008. An analysis of statistical models and features
for reading difficulty prediction. In Proceedings of the
Third Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 1?8.
H. Hotelling. 1940. The selection of variates for use in
prediction with some comments on the general prob-
lem of nuisance parameters. The Annals of Mathemat-
ical Statistics, 11(3):271?283.
L. Kandel and A. Moles. 1958. Application de l?indice
de Flesch a` la langue franc?aise. Cahiers E?tudes de
Radio-Te?le?vision, 19:253?274.
R. Kate, X. Luo, S. Patwardhan, M. Franz, R. Florian,
R. Mooney, S. Roukos, and C. Welty. 2010. Learn-
ing to predict readability using diverse linguistic fea-
tures. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 546?554.
S. Kemper. 1983. Measuring the inference load of a text.
Journal of Educational Psychology, 75(3):391?401.
W. Kintsch and D. Vipond. 1979. Reading comprehen-
sion and readability in educational practice and psy-
chological theory. In L.G. Nilsson, editor, Perspec-
tives on Memory Research, pages 329?365. Lawrence
Erlbaum, Hillsdale, NJ.
G.R.. Klare. 1963. The Measurement of Readability.
Iowa State University Press, Ames, IA.
G.R. Klare. 1984. Readability. In P.D. Pearson, R. Barr,
M. L. Kamil, P. Mosenthal, and R. Dykstra, edi-
tors, Handbook of Reading Research, pages 681?744.
Longman, New York.
H. Lee, P. Gambette, E. Maille?, and C. Thuillier. 2010.
Denside?es: calcul automatique de la densite? des ide?es
dans un corpus oral. In Actes de la douxime Rencon-
tre des tudiants Chercheurs en Informatique pour le
Traitement Automatique des langues (RECITAL).
B.A. Lively and S.L. Pressey. 1923. A method for mea-
suring the vocabulary burden of textbooks. Educa-
tional Administration and Supervision, 9:389?398.
E. Miltsakaki. 2009. Matching readers? preferences and
reading skills with appropriate web texts. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Demonstrations Session, pages 49?52.
B. New, C. Pallier, M. Brysbaert, and L. Ferrand. 2004.
Lexique 2: A new French lexical database. Behav-
ior Research Methods, Instruments, & Computers,
36(3):516.
C. Pallier. 1999. Syllabation des repre?sentations
phone?tiques de brulex et de lexique. Technical report,
Technical Report, update 2004. Lien: http://www. pal-
lier. org/ressources/syllabif/syllabation. pdf.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 186?195.
J.C. Redish and J. Selzer. 1985. The place of readability
formulas in technical communication. Technical com-
munication, 32(4):46?52.
S.E. Schwarm and M. Ostendorf. 2005. Reading level
assessment using support vector machines and statis-
tical language models. Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 523?530.
L. Si and J. Callan. 2001. A statistical model for sci-
entific readability. In Proceedings of the Tenth Inter-
national Conference on Information and Knowledge
Management, pages 574?576. ACM New York, NY,
USA.
J.B. Tharp. 1939. The Measurement of Vocabulary Dif-
ficulty. Modern Language Journal, pages 169?178.
S. Uitdenbogerd. 2005. Readability of French as a for-
eign language and its uses. In Proceedings of the Aus-
tralian Document Computing Symposium, pages 19?
25.
M. Vogel and C. Washburne. 1928. An objective method
of determining grade placement of children?s reading
material. The Elementary School Journal, 28(5):373?
381.
57
