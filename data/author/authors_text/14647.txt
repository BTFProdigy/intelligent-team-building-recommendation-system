Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365?368,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
For the sake of simplicity:
Unsupervised extraction of lexical simplifications from Wikipedia
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil and Lillian Lee
my89@cornell.edu, bopang@yahoo-inc.com, cristian@cs.cornell.edu, llee@cs.cornell.edu
Abstract
We report on work in progress on extract-
ing lexical simplifications (e.g., ?collaborate?
? ?work together?), focusing on utilizing
edit histories in Simple English Wikipedia for
this task. We consider two main approaches:
(1) deriving simplification probabilities via an
edit model that accounts for a mixture of dif-
ferent operations, and (2) using metadata to
focus on edits that are more likely to be sim-
plification operations. We find our methods
to outperform a reasonable baseline and yield
many high-quality lexical simplifications not
included in an independently-created manu-
ally prepared list.
1 Introduction
Nothing is more simple than greatness; indeed, to be
simple is to be great. ?Emerson, Literary Ethics
Style is an important aspect of information pre-
sentation; indeed, different contexts call for differ-
ent styles. Here, we consider an important dimen-
sion of style, namely, simplicity. Systems that can
rewrite text into simpler versions promise to make
information available to a broader audience, such as
non-native speakers, children, laypeople, and so on.
One major effort to produce such text is the
Simple English Wikipedia (henceforth SimpleEW)1,
a sort of spin-off of the well-known English
Wikipedia (henceforth ComplexEW) where hu-
man editors enforce simplicity of language through
rewriting. The crux of our proposal is to learn lexical
simplifications from SimpleEW edit histories, thus
leveraging the efforts of the 18K pseudonymous in-
dividuals who work on SimpleEW. Importantly, not
all the changes on SimpleEW are simplifications; we
thus also make use of ComplexEW edits to filter out
non-simplifications.
Related work and related problems Previous
work usually involves general syntactic-level trans-
1http://simple.wikipedia.org
formation rules [1, 9, 10].2 In contrast, we explore
data-driven methods to learn lexical simplifications
(e.g., ?collaborate? ? ?work together?), which are
highly specific to the lexical items involved and thus
cannot be captured by a few general rules.
Simplification is strongly related to but distinct
from paraphrasing and machine translation (MT).
While it can be considered a directional form of
the former, it differs in spirit because simplification
must trade off meaning preservation (central to para-
phrasing) against complexity reduction (not a con-
sideration in paraphrasing). Simplification can also
be considered to be a form of MT in which the two
?languages? in question are highly related. How-
ever, note that ComplexEW and SimpleEW do not
together constitute a clean parallel corpus, but rather
an extremely noisy comparable corpus. For ex-
ample, Complex/Simple same-topic document pairs
are often written completely independently of each
other, and even when it is possible to get good
sentence alignments between them, the sentence
pairs may reflect operations other than simplifica-
tion, such as corrections, additions, or edit spam.
Our work joins others in using Wikipedia revi-
sions to learn interesting types of directional lexical
relations, e.g, ?eggcorns?3 [7] and entailments [8].
2 Method
As mentioned above, a key idea in our work is to
utilize SimpleEW edits. The primary difficulty in
working with these modifications is that they include
not only simplifications but also edits that serve
other functions, such as spam removal or correction
of grammar or factual content (?fixes?). We describe
two main approaches to this problem: a probabilis-
tic model that captures this mixture of different edit
operations (?2.1), and the use of metadata to filter
out undesirable revisions (?2.2).
2One exception [5] changes verb tense and replaces pro-
nouns. Other lexical-level work focuses on medical text [4, 2],
or uses frequency-filtered WordNet synonyms [3].
3A type of lexical corruption, e.g., ?acorn???eggcorn?.
365
2.1 Edit model
We say that the kth article in a Wikipedia corre-
sponds to (among other things) a title or topic (e.g.,
?Cat?) and a sequence ~dk of article versions caused
by successive edits. For a given lexical item or
phrase A, we write A ? ~dk if there is any version
in ~dk that contains A. From each ~dk we extract a
collection ek = (ek,1, ek,2, . . . , ek,nk) of lexical edit
instances, repeats allowed, where ek,i = A ? a
means that phrase A in one version was changed to
a in the next, A 6= a; e.g., ?stands for? ? ?is the
same as?. (We defer detailed description of how we
extract lexical edit instances from data to ?3.1.) We
denote the collection of ~dk in ComplexEW and Sim-
pleEW as C and S, respectively.
There are at least four possible edit operations: fix
(o1), simplify (o2), no-op (o3), or spam (o4). How-
ever, for this initial work we assume P (o4) = 0.4
Let P (oi | A) be the probability that oi is applied
to A, and P (a | A, oi) be the probability of A ? a
given that the operation is oi. The key quantities of
interest are P (o2 | A) in S, which is the probability
thatA should be simplified, and P (a | A, o2), which
yields proper simplifications of A. We start with an
equation that models the probability that a phrase A
is edited into a:
P (a | A) =
?
oi??
P (oi | A)P (a | A, oi), (1)
where ? is the set of edit operations. This involves
the desired parameters, which we solve for by esti-
mating the others from data, as described next.
Estimation Note that P (a | A, o3) = 0 if A 6= a.
Thus, if we have estimates for o1-related probabili-
ties, we can derive o2-related probabilities via Equa-
tion 1. To begin with, we make the working as-
sumption that occurrences of simplification in Com-
plexEW are negligible in comparison to fixes. Since
we are also currently ignoring edit spam, we thus
assume that only o1 edits occur in ComplexEW.5
Let fC(A) be the fraction of ~dk in C
containing A in which A is modified:
fC(A) =
|{~dk?C|?a,i such that ek,i=A?a}|
|{~dk?C|A?~dk}|
.
4Spam/vandalism detection is a direction for future work.
5This assumption also provides useful constraints to EM,
which we plan to apply in the future, by reducing the number of
parameter settings yielding the same likelihood.
We similarly define fS(A) on ~dk in S. Note that we
count topics (version sequences), not individual ver-
sions: if A appears at some point and is not edited
until 50 revisions later, we should not conclude
that A is unlikely to be rewritten; for example, the
intervening revisions could all be minor additions,
or part of an edit war.
If we assume that the probability of any particular
fix operation being applied in SimpleEW is propor-
tional to that in ComplexEW? e.g., the SimpleEW
fix rate might be dampened because already-edited
ComplexEW articles are copied over ? we have6
P? (o1 | A) = ?fC(A)
where 0 ? ? ? 1. Note that in SimpleEW,
P (o1 ? o2 | A) = P (o1 | A) + P (o2 | A),
where P (o1 ? o2 | A) is the probability that A is
changed to a different word in SimpleEW, which we
estimate as P? (o1 ? o2 | A) = fS(A). We then set
P?(o2 | A) = max (0, fS(A)? ?fC(A)).
Next, under our working assumption, we estimate
the probability of A being changed to a as a fix
by the proportion of ComplexEW edit instances that
rewrite A to a:
P? (a | A, o1) =
|{(k, i) pairs | ek,i = A? a ? ~dk ? C}|
?
a? |{(k, i) pairs | ek,i = A? a
? ? ~dk ? C}|
.
A natural estimate for the conditional probability
of A being rewritten to a under any operation type
is based on observations of A ? a in SimpleEW,
since that is the corpus wherein both operations are
assumed to occur:
P? (a | A) =
|{(k, i) pairs | ek,i = A? a ? ~dk ? S}|
?
a? |{(k, i) pairs | ek,i = A? a
? ? ~dk ? S}|
.
Thus, from (1) we get that for A 6= a:
P?(a | A,o2) =
P?(a | A)? P?(o1 | A)P?(a | A,o1)
P?(o2 | A)
.
2.2 Metadata-based methods
Wiki editors have the option of associating a com-
ment with each revision, and such comments some-
times indicate the intent of the revision. We there-
fore sought to use comments to identify ?trusted?
6Throughout, ?hats? denote estimates.
366
revisions wherein the extracted lexical edit instances
(see ?3.1) would be likely to be simplifications.
Let ~rk = (r1k, . . . , r
i
k, . . .) be the sequence of revi-
sions for the kth article in SimpleEW, where rik is the
set of lexical edit instances (A ? a) extracted from
the ith modification of the document. Let cik be the
comment that accompanies rik, and conversely, let
R(Set) = {rik|c
i
k ? Set}.
We start with a seed set of trusted comments,
Seed. To initialize it, we manually inspected a small
sample of the 700K+ SimpleEW revisions that bear
comments, and found that comments containing a
word matching the regular expression *simpl* (e.g,
?simplify?) seem promising. We thus set Seed :=
{ ? simpl?} (abusing notation).
The SIMPL method Given a set of trusted revi-
sions TRev (in our case TRev = R(Seed)), we
score each A ? a ? TRev by the point-wise mu-
tual information (PMI) between A and a.7 We write
RANK(TRev) to denote the PMI-based ranking of
A? a ? TRev, and use SIMPL to denote our most
basic ranking method, RANK(R(Seed)).
Two ideas for bootstrapping We also considered
bootstrapping as a way to be able to utilize revisions
whose comments are not in the initial Seed set.
Our first idea was to iteratively expand the set
of trusted comments to include those that most of-
ten accompany already highly ranked simplifica-
tions. Unfortunately, our initial implementations in-
volved many parameters (upper and lower comment-
frequency thresholds, number of highly ranked sim-
plifications to consider, number of comments to add
per iteration), making it relatively difficult to tune;
we thus omit its results.
Our second idea was to iteratively expand the
set of trusted revisions, adding those that contain
already highly ranked simplifications. While our
initial implementation had fewer parameters than
the method sketched above, it tended to terminate
quickly, so that not many new simplifications were
found; so, again, we do not report results here.
An important direction for future work is to differ-
entially weight the edit instances within a revision,
as opposed to placing equal trust in all of them; this
7PMI seemed to outperform raw frequency and conditional
probability.
could prevent our bootstrapping methods from giv-
ing common fixes (e.g., ?a?? ?the?) high scores.
3 Evaluation8
3.1 Data
We obtained the revision histories of both Sim-
pleEW (November 2009 snapshot) and ComplexEW
(January 2008 snapshot). In total, ?1.5M revisions
for 81733 SimpleEW articles were processed (only
30% involved textual changes). For ComplexEW,
we processed ?16M revisions for 19407 articles.
Extracting lexical edit instances. For each ar-
ticle, we aligned sentences in each pair of adja-
cent versions using tf-idf scores in a way simi-
lar to Nelken and Shieber [6] (this produced sat-
isfying results because revisions tended to repre-
sent small changes). From the aligned sentence
pairs, we obtained the aforementioned lexical edit
instances A ? a. Since the focus of our study
was not word alignment, we used a simple method
that identified the longest differing segments (based
on word boundaries) between each sentence, except
that to prevent the extraction of entire (highly non-
matching) sentences, we filtered out A ? a pairs if
either A or a contained more than five words.
3.2 Comparison points
Baselines RANDOM returns lexical edit instances
drawn uniformly at random from among those ex-
tracted from SimpleEW. FREQUENT returns the
most frequent lexical edit instances extracted from
SimpleEW.
Dictionary of simplifications The SimpleEW ed-
itor ?Spencerk? (Spencer Kelly) has assembled a list
of simple words and simplifications using a combi-
nation of dictionaries and manual effort9. He pro-
vides a list of 17,900 simple words ? words that do
not need further simplification ? and a list of 2000
transformation pairs. We did not use Spencerk?s set
as the gold standard because many transformations
we found to be reasonable were not on his list. In-
stead, we measured our agreement with the list of
transformations he assembled (SPLIST).
8Results at http://www.cs.cornell.edu/home/llee/data/simple
9http://www.spencerwaterbed.com/soft/simple/about.html
367
3.3 Preliminary results
The top 100 pairs from each system (edit model10
and SIMPL and the two baselines) plus 100 ran-
domly selected pairs from SPLIST were mixed and
all presented in random order to three native English
speakers and three non-native English speakers (all
non-authors). Each pair was presented in random
orientation (i.e., either as A ? a or as a ? A),
and the labels included ?simpler?, ?more complex?,
?equal?, ?unrelated?, and ??? (?hard to judge?). The
first two labels correspond to simplifications for the
orientations A ? a and a ? A, respectively. Col-
lapsing the 5 labels into ?simplification?, ?not a sim-
plification?, and ??? yields reasonable agreement
among the 3 native speakers (? = 0.69; 75.3% of the
time all three agreed on the same label). While we
postulated that non-native speakers11 might be more
sensitive to what was simpler, we note that they dis-
agreed more than the native speakers (? = 0.49) and
reported having to consult a dictionary. The native-
speaker majority label was used in our evaluations.
Here are the results; ?-x-y? means that x and y are
the number of instances discarded from the precision
calculation for having no majority label or majority
label ???, respectively:
Method Prec@100 # of pairs
SPLIST 86% (-0-0) 2000
Edit model 77% (-0-1) 1079
SIMPL 66% (-0-0) 2970
FREQUENT 17% (-1-7) -
RANDOM 17% (-1-4) -
Both baselines yielded very low precisions ?
clearly not all (frequent) edits in SimpleEW were
simplifications. Furthermore, the edit model yielded
higher precision than SIMPL for the top 100 pairs.
(Note that we only examined one simplification per
A for those A where P? (o2 | A) was well-defined;
thus ?# of pairs? does not directly reflect the full
potential recall that either method can achieve.)
Both, however, produced many high-quality pairs
(62% and 71% of the correct pairs) not included in
SPLIST. We also found the pairs produced by these
two systems to be complementary to each other. We
10We only considered those A such that freq(A ? ?) >
1 ? freq(A) > 100 on both SimpleEW and ComplexEW. The
final top 100 A ? a pairs were those with As with the highest
P (o2 | A). We set ? = 1.
11Native languages: Russian; Russian; Russian and Kazakh.
believe that these two approaches provide a good
starting point for further explorations.
Finally, some examples of simplifications found
by our methods: ?stands for? ? ?is the same
as?, ?indigenous? ? ?native?, ?permitted? ? ?al-
lowed?, ?concealed? ? ?hidden?, ?collapsed? ?
?fell down?, ?annually?? ?every year?.
3.4 Future work
Further evaluation could include comparison with
machine-translation and paraphrasing algorithms. It
would be interesting to use our proposed estimates
as initialization for EM-style iterative re-estimation.
Another idea would be to estimate simplification pri-
ors based on a model of inherent lexical complexity;
some possible starting points are number of sylla-
bles (which is used in various readability formulae)
or word length.
Acknowledgments We first wish to thank Ainur Yessenalina
for initial investigations and helpful comments. We are
also thankful to R. Barzilay, T. Bruce, C. Callison-Burch, J.
Cantwell, M. Dredze, C. Napoles, E. Gabrilovich, & the review-
ers for helpful comments; W. Arms and L. Walle for access to
the Cornell Hadoop cluster; J. Cantwell for access to computa-
tional resources; R. Hwa & A. Owens for annotation software;
M. Ulinski for preliminary explorations; J. Cantwell, M. Ott, J.
Silverstein, J. Yatskar, Y. Yatskar, & A. Yessenalina for annota-
tions. Supported by NSF grant IIS-0910664.
References
[1] R. Chandrasekar, B. Srinivas. Automatic induction of rules
for text simplification. Knowledge-Based Systems, 1997.
[2] L. Dele?ger, P. Zweigenbaum. Extracting lay paraphrases
of specialized expressions from monolingual comparable
medical corpora. Workshop on Building and Using Com-
parable Corpora, 2009.
[3] S. Devlin, J. Tait. The use of a psycholinguistic database in
the simplification of text for aphasic readers. In Linguistic
Databases, 1998.
[4] N. Elhadad, K. Sutaria. Mining a lexicon of technical terms
and lay equivalents. Workshop on BioNLP, 2007.
[5] B. Beigman Klebanov, K. Knight, D. Marcu. Text simplifi-
cation for information-seeking applications. OTM Confer-
ences, 2004.
[6] R. Nelken, S. M. Shieber. Towards robust context-sensitive
sentence alignment for monolingual corpora. EACL, 2006.
[7] R. Nelken, E. Yamangil. Mining Wikipedia?s article re-
vision history for training computational linguistics algo-
rithms. WikiAI, 2008.
[8] E. Shnarch, L. Barak, I. Dagan. Extracting lexical reference
rules from Wikipedia. ACL, 2009.
[9] A. Siddharthan, A. Nenkova, K. McKeown. Syntactic
simplification for improving content selection in multi-
document summarization. COLING, 2004.
[10] D. Vickrey, D. Koller. Sentence simplification for seman-
tic role labeling/ ACL, 2008.
368
Proceedings of NAACL-HLT 2013, pages 416?425,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Learning to Relate Literal and Sentimental Descriptions of Visual Properties
Mark Yatskar
Computer Science & Engineering
University of Washington
Seattle, WA
my89@cs.washington.edu
Svitlana Volkova
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Asli Celikyilmaz
Conversational Understanding Sciences
Microsoft
Mountain View, CA
asli@ieee.org
Bill Dolan
NLP Group
Microsoft Research
Redmond, WA
billdol@microsoft.edu
Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
Language can describe our visual world at
many levels, including not only what is lit-
erally there but also the sentiment that it in-
vokes. In this paper, we study visual language,
both literal and sentimental, that describes the
overall appearance and style of virtual char-
acters. Sentimental properties, including la-
bels such as ?youthful? or ?country western,?
must be inferred from descriptions of the more
literal properties, such as facial features and
clothing selection. We present a new dataset,
collected to describe Xbox avatars, as well as
models for learning the relationships between
these avatars and their literal and sentimen-
tal descriptions. In a series of experiments,
we demonstrate that such learned models can
be used for a range of tasks, including pre-
dicting sentimental words and using them to
rank and build avatars. Together, these re-
sults demonstrate that sentimental language
provides a concise (though noisy) means of
specifying low-level visual properties.
1 Introduction
Language can describe varied aspects of our visual
world, including not only what is literally there but
also the social, cultural, and emotional sentiment it
invokes. Recently, there has been a growing effort
to study literal language that describes directly ob-
servable properties, such as object color, shape, or
This is a light tan young man
with short and trim haircut. He
has straight eyebrows and large
brown eyes. He has a neat and
trim appearance.
State of mind: angry, upset,
determined. Likes: country
western, rodeo. Occupation:
cowboy, wrangler, horse trainer.
Overall: youthful, cowboy.
Figure 1: (A) Literal avatar descriptions and (B) sen-
timental descriptions of four avatar properties, in-
cluding possible occupations and interests.
category (Farhadi et al, 2009; Mitchell et al, 2010;
Matuszek et al, 2012). Here, we add a focus on
sentimental visual language, which compactly de-
scribes more subjective properties such as if a person
looks determined, if a resume looks professional, or
if a restaurant looks romantic. Such models enable
many new applications, such as text editors that au-
tomatically select properties including font, color, or
text alignment to best match high level descriptions
such as ?professional? or ?artistic.?
416
In this paper, we study visual language, both lit-
eral and sentimental, that describes the overall ap-
pearance and style of virtual characters, like those in
Figure 1. We use literal language as feature norms, a
tool used for studying semantic information in cog-
nitive science (Mcrae et al, 2005). Literal words,
such ?black? or ?hat,? are annotated for objects to in-
dicate how people perceive visual properties. Such
feature norms provide our gold-standard visual de-
tectors, and allow us to focus on learning to model
sentimental language, such as ?youthful? or ?goth.?
We introduce a new corpus of descriptions of
Xbox avatars created by actual gamers. Each avatar
is specified by 19 attributes, including clothing and
body type, allowing for more than 1020 possibil-
ities. Using Amazon Mechanical Turk,1 we col-
lected literal and sentimental descriptions of com-
plete avatars and many of their component parts,
such as the cowboy hat in Figure 1(B). In all, there
are over 100K descriptions. To demonstrate poten-
tial for learning, we also report an A/B test which
shows that native speakers can use sentimental de-
scriptions to distinguish the labeled avatars from
random distractors. This new data will enable study
of the relationships between the co-occurring literal
and sentimental text in a rich visual setting.2
We describe models for three tasks: (i) classify-
ing when words match avatars, (ii) ranking avatars
given a description, and (iii) constructing avatars to
match a description. Each model includes literal part
descriptions as feature norms, enabling us to learn
which literal and sentinel word pairs best predict
complete avatars.
Experiments demonstrate the potential for jointly
modeling literal and sentimental visual descriptions
on our new dataset. The approach outperforms sev-
eral baselines and learns varied relationships be-
tween the sentimental and literal descriptions. For
example, in one experiment ?nerdy student? is pre-
dictive of an avatar with features indicating its shirt
is ?plaid? and glasses are ?large? and faces that are
not ?bearded.? We also show that individual sen-
timental words can be predicted but that multiple
avatars can match a single sentimental description.
Finally, we use our model to build complete avatars
1www.mturk.com
2Data available at http://homes.cs.washington.
edu/?my89/avatar.
and show that we can accurately predict the senti-
mental terms annotators ascribe to them.
2 Related Work
To the best of our knowledge, our focus on learn-
ing to understand visual sentiment descriptions is
novel. However, visual sentiment has been stud-
ied from other perspectives. Jrgensen (1998) pro-
vides examples which show that visual descriptions
communicate social status and story information in
addition to literal object and properties. Tousch et
al. (2012) draw the distinction between ?of-ness?
(objective and concrete) and ?about-ness? (subjec-
tive and abstract) in image retrieval, and observe
that many image queries are abstract (for example,
images about freedom). Finally, in descriptions of
people undergoing emotional distress, Fussell and
Moss (1998) show that literal descriptions co-occur
frequently with sentimental ones.
There has been significant work on more lit-
eral aspects of grounded language understand-
ing, both visual and non-visual. The Words-
Eye project (Coyne and Sproat, 2001) generates
3D scenes from literal paragraph-length descrip-
tions. Generating literal textual descriptions of vi-
sual scenes has also been studied, including both
captions (Kulkarni et al, 2011; Yang et al, 2011;
Feng and Lapata, 2010) and descriptions (Farhadi
et al, 2010). Furthermore, Chen and Dolan (2011)
collected literal descriptions of videos with the
goal of learning paraphrases while Zitnick and
Parikh (2013) describe a corpus of descriptions for
clip art that supports the discovery of semantic ele-
ments of visual scenes.
There has also been significant recent work on au-
tomatically recovering visual attributes, both abso-
lute (Farhadi et al, 2009) and relative (Kovashka et
al., 2012), a challenge that we avoid having to solve
with our use of feature norms (Mcrae et al, 2005).
Grounded language understanding has also re-
ceived significant attention, where the goal is to
learn to understand situated non-visual language
use. For example, there has been work on learning
to execute instructions (Branavan et al, 2009; Chen
and Mooney, 2011; Artzi and Zettlemoyer, 2013),
provide sports commentary (Chen et al, 2010), un-
derstand high level strategy guides to improve game
417
Figure 2: The number of assets per category and ex-
ample images from the hair, shirt and hat categories.
play (Branavan et al, 2011; Eisenstein et al, 2009),
and understand referring expression (Matuszek et
al., 2012).
Finally, our work is similar in spirit to sentiment
analysis (Pang et al, 2002), emotion detection from
images and speech (Zeng et al, 2009), and metaphor
understanding (Shutova, 2010a; Shutova, 2010b).
However, we focus on more general visual context.
3 Data Collection
We gathered a large number of natural language de-
scriptions from Mechanical Turk (MTurk). They in-
clude: (1) literal descriptions of specific facial fea-
tures, clothing or accessories and (2) high level sub-
jective descriptions of human-generated avatars.3
Literal Descriptions We showed annotators a sin-
gle image of clothing, a facial feature or an acces-
sory and asked them to produce short descriptions.
Figure 2 shows the distribution over object types.
We restricted descriptions to be between 3 and 15
words. In all, we collected 33.2K descriptions and
had on average 7 words per descriptions. The ex-
ample annotations with highlighted overlapping pat-
terns are in Table 1.
Sentimental Descriptions We also collected 1913
gamer-created avatars from the web. The avatars
were filtered to contain only items from the set of
665 for which we gathered literal descriptions. The
gender distribution is 95% male.
3(2) also has phrases describing emotional reactions. We
also collected (3) multilingual literal, (4) relative literal and (5)
comprehensive full-body descriptions. We do not use this data,
but it will be included in the public release.
LITERAL DESCRIPTIONS
full-sleeved executive blue shirt
blue , long-sleeved button-up shirt
mens blue button dress shirt with dark blue stripes
multi-blue striped long-sleeve button-up dress
shirt with cuffs and breast pocket
Table 1: Literal descriptions of shirt in Figure 2.
To gather high level sentimental descriptions, an-
notators were presented with an image of an avatar
and asked to list phrases in response to the follow
different aspects:
- State of mind of the avatar.
- Things the avatar might care about.
- What the avatar might do for a living.
- Overall appearance of the avatar.
6144 unique vocabulary items occurred in these
descriptions, but only 1179 occurred more than 10
times. Figure 1 (B) shows an avatar and its corre-
sponding sentimental descriptions.
Quality Control All annotations in our dataset are
produced by non-expert annotators. We relied on
manual spot checks to limit poor annotations. Over
time, we developed a trusted crowd of annotators
who produced only high quality annotations during
the earliest stage of data collection.
4 Feasibility
Our hypothesis is that sentimental language does not
uniquely identify an avatar, but instead summarizes
or otherwise describes its overall look. In general,
there is a trade off between concise and precise de-
scriptions. For example, given a single word you
might be able to generally describe the overall look
of an avatar, but a long, detailed, literal description
would be required to completely specify their ap-
pearance.
To demonstrate that the sentimental descriptions
we collected are precise enough to be predictive
of appearance, we conducted an experiment that
prompts people to judge when avatars match de-
scriptions. We created an A/B test where we show
English speakers two avatars and one sentimental
description. They were asked to select which avatar
is better matched by the description and how dif-
ficult they felt, on a scale from 1 to 4, it was to
judge. For 100 randomly selected descriptions, we
418
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2  
1
 1.5
 2
 2.5
 3
 3.5
data 
diffic
ulty l
ess t
han X
Kapp
a vs 
Cum
ulativ
e Dif
ficult
y
game
r is m
ajorit
y lab
el kapp
a
portio
n of d
ata
Figure 3: Judged task difficulty versus agreement,
gamer avatar preference, and percentage of data cov-
ered. The difficulty axis is cumulative.
asked 5 raters to compare the gamer avatars to ran-
domly generated ones (where each asset is selected
independently according to a uniform distribution).
Figure 3 shows a plot of Kappa and the percent of
the time a majority of the raters selected the gamer
avatar. The easiest 20% of the data pairs had the
strongest agreement, with kappa=.92, and two thirds
of the data has kappa = .70. While agreement falls
off to .52 for the full data set, the gamer avatar re-
mains the majority judgment 81% of the time.
The fact that random avatars are sometimes pre-
ferred indicates that it can be difficult to judge sen-
timental descriptions. Consider the avatars in Fig-
ure 4. Neither conforms to a clear sentimental de-
scription based on the questions we asked. The
right one is described with conflicting words and
the words describing the left one are very general
(like ?dumb?). This corresponds to our intuition that
while many avatars can be succinctly summarized
with our questions, some would be more easily de-
scribed using literal language.
5 Tasks and Evaluation
We formulate three tasks to study the feasibility of
learning the relationship between sentimental and
literal descriptions. In this section, we first define
the space of possible avatars, followed by the tasks.
Avatars Figure 5 summarizes the notation we will
develop to describe the data. An avatar is defined by
a 19 dimensional vector ~a where each position is an
State of mind:
playful, happy;
Likes: sex
Occupation: hobo
Overall: dumb
State of mind: content, humble, satisfied,
peaceful, relaxed, calm. Likes: fashion,
friends, money, cars, music, education.
Occupation: teacher, singer, actor,
performer, dancer, computer engineer.
Overall: nerdy, cool, smart, comfy,
easygoing, reserved
Figure 4: Avatars rated as difficult.
index into a list of possible items~i. Each dimension
represents a position on the avatar, for example, hat
or nose. Each possible item is called an asset and
is associated with a set of positions it can fill. Most
assets take up exactly one position, while there are
a few cases where assets take multiple positions.4
An avatar ~a is valid if all of its mandatory positions
are filled, and no two assets conflict on a position.
Mandatory positions include hair, eyes, ears, eye-
brows, nose, mouth, chin, shirt, pants, and shoes.
All other positions are optional. We refer to this set
of valid ~a as A. Practically speaking, if an avatar is
not valid, it cannot be reliably rendered graphically.
Each item i is associated with the literal descrip-
tions ~di ? D where D is the set of literal descrip-
tions. Furthermore, every avatar~a is associated a list
of sentimental query words ~q, describing subjective
aspects of an avatar.5
Sentimental Word Prediction We first study in-
dividual words. The word prediction task is to de-
cide whether a given avatar can be described with a
4For example, long sleeve shirts cover up watches, so they
take up both shirt and wristwear positions. Costumes tend to
span many more positions, for example there a suit that takes
up shirt, pants, wristwear and shoes positions.
5We do not distinguish which prompt (e.g., ?state of mind?
or ?occupation?) a word in ~q came from, although the vocabu-
laries are relatively disjoint.
419
Figure 5: Avatars, queries, items, literal descriptions.
particular sentimental word q?. We evaluate perfor-
mance with F-score.
Avatar Ranking We also consider an avatar re-
trieval task, where the goal is to rank the set of
avatars in our data, ?j=1...n ~aj , according to which
one best matches a sentimental description, ~qi. As
an automated evaluation, we report the average per-
centile position assigned to the true ~ai for each ex-
ample. However, in general, many different avatars
can match each ~qi, an interesting phenomena we will
further study with human evaluation.
Avatar Generation Finally, we consider the prob-
lem of generating novel, previously unseen avatars,
by selecting a set of items that best embody some
sentimental description. As with ranking, we aim to
construct the avatar ~ai that matches each sentimen-
tal description ~qi. We evaluate by considering the
item overlap between ~ai and the output avatar ~a?,
discounting for empty positions:6
f =
?| ~a?|
j=1 I( ~a
?
j = ~aij)
max(numparts( ~a?), numparts(~ai))
, (1)
where numparts returns the number of non-empty
avatar positions. The score is a conservative measure
because some items are significantly more visually
salient than others. For instance, shirts and pants oc-
cupy a large portion of the physical realization of the
avatar, while rings are small and virtually unnotice-
able. We additionally perform a human evaluation
in Section 8 to better understand these challenges.
6Optional items are infrequently used. Therefore not pre-
dicting them at all offers a strong baseline. Yet doing this
demonstrates nothing about an algorithm?s ability to predict
items which contribute to the sentimental qualities of an avatar.
6 Methods
We present two different models: one that considers
words in isolation and another that jointly models
the query words. This section defines the models
and how we learn them.
6.1 Independent Sentimental Word Model
The independent word model (S-Independent) as-
sumes that each word independently describes the
avatar. We construct a separate linear model for each
word in the vocabulary.
To train these model, we transform the data to
form a binary classification problem for each word,
where the positive data includes all avatars the word
was seen with, (q, ~ai, 1) for all i and q ? ~qi, and the
rest are negative, (q, ~ai, 0) for all i and q /? ~qi.
We use the following features:
? an indicator feature for the cross product of a
sentiment query word q, a literal description
word w ? D, and the avatar position index j
(for example, q = ?angry? with w = ?pointy?
and j = eyebrows):
I(q ? ~qi, w ? ~daij , j)
? a bias feature for keeping a position empty:
I(q ? ~qi, aij = empty, j)
These features will allow the model to capture
correlations between our feature norms which pro-
vide descriptions of visual attributes, like black, and
sentimental words, like gothic.
420
S-Independent is used for both word prediction
and ranking. For prediction, we train a linear model
using averaged binary perceptron. For ranking, we
try to rank all positive instances above negative in-
stances. We use an averaged structured perceptron
to train the ranker (Collins, 2002). To rank with re-
spect to an entire query ~qi, we sum the scores of each
word q ? ~qi.
6.2 Joint Sentimental Model
The second approach (S-Joint) jointly models the
query words to learn the relationships between lit-
eral and sentimental words with score s:
s(~a|~q,D) =
|~a|?
i=1
|~q|?
j=1
?T f(~ai, ~qj , ~dai)
Where every word in the query has a separate factor
and every position is treated independently subject
to the constraint that ~a is valid. The feature function
f uses the same features as the word independent
model above.
This model is used for ranking and generation.
For ranking, we try to rank the avatar ai for query
qi above all other avatars in the candidate set. For
generation, we try to score ai above all other valid
avatars given the query qi. In both cases, we train
with averaged structured perceptron (Collins, 2002)
on the original data, containing query, avatar pairs
(~qi, ~ai).
7 Experimental Setup
Random Baseline For the ranking and avatar gen-
eration tasks, we report random baselines. For rank-
ing, we randomly order the avatars. In the genera-
tion case, we select an item randomly for every posi-
tion. This baseline does not generate optional assets
because they are rare in the real data.
Sentimental-Literal Overlap (SL-Overlap) We
also report a baseline that measures the overlap be-
tween words in the sentiment query ~qi and words in
the literal asset descriptions D. In generation, for
each position in the avatar, ~ai, SL-Overlap selects
the item whose literal description has the most words
in common with ~qi. If no item had overlap with the
query, we backoff to a random choice. In the case of
ranking, it orders avatars by the sum over every po-
sition of the number of words in common between
Word F-Score Precision Recall N
happi 0.84 0.89 0.78 149
student 0.78 0.82 0.74 129
friend 0.76 0.84 0.70 153
music 0.74 0.89 0.63 148
confid 0.74 0.82 0.76 157
sport 0.69 0.62 0.76 76
casual 0.63 0.6 0.67 84
youth 0.6 0.57 0.64 88
waitress 0.59 0.42 1 5
smart 0.57 0.54 0.6 88
fashion 0.54 0.54 0.54 70
monei 0.54 0.52 0.56 76
cool 0.54 0.52 0.56 84
relax 0.53 0.52 0.56 90
game 0.51 0.44 0.62 61
musician 0.51 0.44 0.61 66
parti 0.51 0.43 0.62 58
content 0.5 0.47 0.53 75
friendli 0.49 0.42 0.6 56
smooth 0.49 0.4 0.63 57
Table 2: Top 20 words (stemmed) for classification.
N is the number of occurances in the test set.
the literal description and the query, ~qi. This base-
line tests the degree to which literal and sentimental
descriptions overlap lexically.
Feature Generation For all models that use lexi-
cal features, we limited the number of words. 6144
unique vocabulary items occur in the query set, and
3524 in the literal description set. There are over
400 million entries in the full set of features that in-
clude the cross product of these sets with all possible
avatar positions, as described in Section 6. Since this
would present a challenge for learning, we prune in
two ways. We stem all words with a Porter stemmer.
We also filter out all features which do not occur at
least 10 times in our training set. The final model
has approximately 700k features.
8 Results
We present results for the tasks described in Sec-
tion 5 with the appropriate models from Section 6.
8.1 Word Prediction Results
The goal of our first experiment is to study when
individual sentiment words can be accurately pre-
dicted. We computed sentimental word classifica-
tion accuracy for 1179 word classes with 10 or more
421
Algorithm Percentile Rank
S-joint 77.3
S-independant 73.5
SL-overlap 60.4
Random 48.8
Table 3: Automatic evaluation of ranking. The aver-
age percentile that a test avatar was ranked given its
sentimental description.
mentions. Table 2 shows the top 20 words ordered
by F-score.7 Many common words can be predicted
with relatively high accuracy. Words with strong
individual cues like happy (a smiling mouth), and
confidence (wide eyes) and nerdi (particular glasses)
can be predicted well.
The average F-score among all words was .085.
33.2% of words have an F-score of zero. These zeros
include words like: unusual, bland, sarcastic, trust,
prepared, limber, healthy and poetry. Some of these
words indicate broad classes of avatars (e.g., unusual
avatars) and others indicate subtle modifications to
looks that without other words are not specific (e.g.,
a prepared surfer vs. a prepared business man). Fur-
thermore, evaluation was done assuming that when
a word is not mentioned, it is should be predicted as
negative. This fails to account for the fact that peo-
ple do not mention everything that?s true, but instead
make choices about what to mention based on the
most relevant qualities. Despite these difficulties,
the classification performance shows that we can ac-
curately capture usage patterns for many words.
8.2 Ranking Results
Ranking allows us to test the hypothesis that multi-
ple avatars are valid for a high level description. Fur-
thermore, we consider the differences between S-
Joint and S-Independent, showing that jointly mod-
elings all words improves ranking performance.
Automatic Evaluation The results are shown in
Table 3. Both S-Independent and S-Joint outperform
the SL-overlap baseline. SL-Overlap?s poor perfor-
mance can be attributed to low direct overlap be-
tween sentimental words and literal words. S-Joint
also outperforms the S-Independent.
7Accuracy numbers are inappropriate in this case because
the number of negative instances, in most cases, is far larger
than the number of positive ones.
Inspection of the parameters shows that S-Joint
does better than S-Independent in modeling words
that only relate to a subset of body positions. For
example, in one case we found that for the word
?puzzled? nearly 50% of the weights were on fea-
tures that related to eyebrows and eyes. This type
of specialization was far more pronounced for S-
Joint. The joint nature of the learning allows the fea-
tures for individual words to specialize for specific
positions. In contrast, S-Independent must indepen-
dently predict all parts for every word.
Human Evaluation We report human relevancy
judgments for the top-5 returned results from S-
Joint. On average, 56.2% were marked to be rele-
vant. This shows that S-Joint is performing better
than automatic numbers would indicate, confirming
our intuition that there is a one-to-many relationship
between a sentimental description and avatars. Sen-
timental descriptions, while having significant sig-
nal, are not exact. These results also indicate that
relying on automatic measures of accuracy that as-
sume a single reference avatar underestimates per-
formance. Figure 6 shows the top ranked results
returned by S-Joint for a sentimental description
where the model performs well.
8.3 Generation Results
Finally we evaluate three models for avatar genera-
tion: Random, SL-Overlap and S-Joint using auto-
matic measures and human evaluation.
Automatic Evaluation Table 4 presents results
for automatic evaluation. The Random baseline per-
forms badly, on average assigning items correctly to
less than 1 position in the generated avatar. The SL-
Overlap baseline improves, but still performs quite
poorly. The S-Joint model performs significantly
better, correctly guessing 2-3 items for each output
avatar. However, as we will see in the manual eval-
uation, many of the non-matching parts it produces
are still a good fit for the query.
Human Evaluation As before, there are many
reasonable avatars that could match as well as the
reference avatars. Therefore, we also evaluated gen-
eration with A/B tests, much like in Section 4. An-
notators were asked to judge which of two avatars
better matched a sentimental description. They
422
pensive,confrontational; music,socializing; musician,bar tending,club owner; smart,cool.
Figure 6: A sentimental description paired with the highest ranked avatars found by S-Joint.
Model Overlap
Random 0.041
SL-Overlap 0.049
S-Joint 0.126
Table 4: Automatic generation evaluation results.
The item overlap metric is defined in Section 5.
Kappa Majority Random Sys.
SL-Overlap 0.20 0.25 0.34 0.32
S-Joint 0.52 0.90 0.07 0.81
Gamer 0.52 0.81 0.08 0.77
Table 5: Human evaluation of automatically gener-
ated avatars. Majority represents the percentage of
time the system output is preferred by a majority of
raters. Random and System (Sys.) indicate the per-
centage of time each was preferred.
could rate System A or System B as better, or re-
port that they were equal or that neither matched
the description. We consider two comparisons: SL-
Overlap vs. Random and S-Joint vs Random. Five
annotators performed each condition, rating 100 ex-
amples with randomly ordered avatars.
We report the results for human evaluation includ-
ing kappa, majority judgments, and a distribution
over judgments in Table 5. The SL-Overlap baseline
is indistinguishable from a random avatar. This con-
trasts with the ranking case, where this simple base-
line showed improvement, indicating that generation
is a much harder problem. Furthermore, agreement
is low; people felt the need to make a choice but
were not consistent.
We also see in Table 5 that people prefer the S-
Joint model outputs to random avatars as often as
they prefer gamer to random. While this does not
necessarily imply that S-Joint creates gamer-quality
avatars, it indicates substantial progress by learning
a mapping between literal and sentimental words.
Qualitative Results Table 6 presents the highest
and lowest weighted features for different sentimen-
tal query words. Figure 7 shows four descriptions
that were assigned high quality avatars.
In general, many of the weaker avatars had as-
pects of the descriptions but lacked such distinctive
overall looks. This was especially true when the
descriptions contained seemingly contradictory in-
formation. For example, one avatar was described
as being both nerdy and popular. We generated a
look that had aspects of both of these descriptions,
including a head that contained both conservative el-
ements (like glasses) and less conservative elements
(like crazy hair and earrings). However, the combi-
nation would not be described as nerdy or popular,
because of difficult to predict global interactions be-
tween the co-occurring words and items. This is an
important area for future work.
9 Conclusions
We explored how visual language, both literal and
sentimental, maps to the overall physical appearance
and style of virtual characters. While this paper fo-
cused on avatar design, our approach has implica-
tions for a broad class of natural language-driven
423
Ambition; business,
fashion, success;
salesman; smooth,
professional.
Capable, confident, firm; heavy metal,
extreme sports, motorcycles; engineer,
mechanic, machinist; aggressive,
strong, protective.
Stressed, bored,
discontent; emo music;
works at a record store;
goth, dark, drab.
Happy, content, confident,
home, career, family,
secretary,student,
classy,clean,casual
Figure 7: Avatars automatically generated with the S-Joint model.
Sentiment positive features negative features
happi mouth:thick, mouth:smilei, mouth:make, mouth:open mouth:tight, mouth:emotionless, mouth:brownish, mouth:attract
gothic shoes:brown, shirt:black, pants:hot, shirt:band shirt:half, shirt:tight, pants:sexi, hair:brownish
retro eyebrows:men, eyebrows:large, hair:round, pants:light eyebrows:beauti, pants:side; eyebrows:trim, pants:cut
beach pants:yello, pants:half, nose:narrow, pants:white shirt:brown, shirt:side; shoes:long, pants:jean
Table 6: Most positive and negative features for a word stem. A feature is [position]:[literal word].
dialog scenarios. In many situations, a user may
be perfectly able to formulate a high-level descrip-
tion of their intent (?Make my resume look cleaner?
?Buy me clothes for a summer wedding,? or ?Play
something more danceable?) while having little or
no understanding of the complex parameter space
that the underlying software must manipulate in or-
der to achieve this result.
We demonstrated that these high-level sentimen-
tal specifications can have a strong relationship to
literal aspects of a problem space and showed that
sentimental language is a concise, yet noisy, way
of specifying high level characteristics. Sentimen-
tal language is an unexplored avenue for improving
natural language systems that operate in situated set-
tings. It has the potential to bridge the gap between
lay and expert understandings of a problem domain.
Acknowledgments
This work is partially supported by the DARPA
CSSG (N11AP20020) and the NSF (IIS-1115966).
The authors would like to thank Chris Brockett,
Noelle Sophy, Rico Malvar for helping with collect-
ing and processing the data. We would also like
to thank Tom Kwiatkowski and Nicholas FitzGer-
ald and the anonymous reviewers for their helpful
comments.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics, 1(1):49?62.
SRK Branavan, H. Chen, L.S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 82?90.
SRK Branavan, David Silver, and Regina Barzilay. 2011.
Learning to win by reading manuals in a monte-carlo
framework. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 268?
277.
David L. Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 190?200.
D.L. Chen and R.J. Mooney. 2011. Learning to interpret
natural language navigation instructions from observa-
424
tions. In Proceedings of the 25th AAAI Conference on
Artificial Intelligence (AAAI-2011), pages 859?865.
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal of Artificial
Intelligence Research, 37:397?435.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing, pages 1?8.
B. Coyne and R. Sproat. 2001. Wordseye: an automatic
text-to-scene conversion system. In Proceedings of the
28th annual conference on Computer graphics and in-
teractive techniques, pages 487?496.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 958?967.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
In Proceedings of the IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a story:
generating sentences from images. In Proceedings of
the 11th European conference on Computer Vision,
ECCV?10, pages 15?29.
Yansong Feng and Mirella Lapata. 2010. Topic models
for image annotation and text illustration. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 831?839.
Susan R Fussell and Mallie M Moss. 1998. Figura-
tive language in emotional communication. Social and
cognitive approaches to interpersonal communication,
page 113.
Corinne Jrgensen. 1998. Attributes of images in describ-
ing tasks. Information Processing & Management,
34(23):161 ? 174.
Adriana Kovashka, Devi Parikh, and Kristen Grauman.
2012. Whittlesearch: Image search with relative at-
tribute feedback. In Computer Vision and Pattern
Recognition (CVPR), pages 2973?2980.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.
Berg, and T.L. Berg. 2011. Baby talk: Understanding
and generating simple image descriptions. In Com-
puter Vision and Pattern Recognition (CVPR), pages
1601?1608.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A Joint
Model of Language and Perception for Grounded At-
tribute Learning. In Proc. of the 2012 International
Conference on Machine Learning.
Ken Mcrae, George S. Cree, Mark S. Seidenberg, and
Chris Mcnorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37(4):547?559.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2010. Natural reference to objects in a visual domain.
In Proceedings of the 6th International Natural Lan-
guage Generation Conference, INLG ?10, pages 95?
104.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language processing,
pages 79?86.
Ekaterina Shutova. 2010a. Automatic metaphor inter-
pretation as a paraphrasing task. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 1029?1037.
Ekaterina Shutova. 2010b. Models of metaphor in nlp.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL ?10, pages
688?697.
Anne-Marie Tousch, Stphane Herbin, and Jean-Yves Au-
dibert. 2012. Semantic hierarchies for image annota-
tion: A survey. Pattern Recognition, 45(1):333 ? 345.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing.
Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009.
A survey of affect recognition methods: Audio, vi-
sual, and spontaneous expressions. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
31(1):39?58.
C Lawrence Zitnick and Devi Parikh. 2013. Bringing
semantics into focus using visual abstraction. In Com-
puter Vision and Pattern Recognition (To Appear).
425
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 110?120,
Dublin, Ireland, August 23-24 2014.
See No Evil, Say No Evil:
Description Generation from Densely Labeled Images
Mark Yatskar
1
?
my89@cs.washington.edu
1
Computer Science & Engineering
University of Washington
Seattle, WA, 98195, USA
Michel Galley
2
mgalley@microsoft.com
Lucy Vanderwende
2
lucyv@microsoft.com
Luke Zettlemoyer
1
lsz@cs.washington.edu
2
Microsoft Research
One Microsoft Way
Redmond, WA, 98052, USA
Abstract
This paper studies generation of descrip-
tive sentences from densely annotated im-
ages. Previous work studied generation
from automatically detected visual infor-
mation but produced a limited class of sen-
tences, hindered by currently unreliable
recognition of activities and attributes. In-
stead, we collect human annotations of ob-
jects, parts, attributes and activities in im-
ages. These annotations allow us to build
a significantly more comprehensive model
of language generation and allow us to
study what visual information is required
to generate human-like descriptions. Ex-
periments demonstrate high quality output
and that activity annotations and relative
spatial location of objects contribute most
to producing high quality sentences.
1 Introduction
Image descriptions compactly summarize com-
plex visual scenes. For example, consider the de-
scriptions of the image in Figure 1, which vary in
content but focus on the women and what they are
doing. Automatically generating such descriptions
is challenging: a full system must understand the
image, select the relevant visual content to present,
and construct complete sentences. Existing sys-
tems aim to address all of these challenges but
use visual detectors for only a small vocabulary
of words, typically nouns, associated with objects
that can be reliably found.
1
Such systems are blind
?
This work was conducted at Microsoft Research.
1
While object recognition is improving (ImageNet accu-
racy is over 90% for 1000 classes) progress in activity recog-
nition has been slower; the state of the art is below 50% mean
average precision for 40 activity classes (Yao et al., 2011).
cars (Count:3) Isa: ride, vehicle,? Doing: parking,? Has: steering wheel,? Attrib: black, shiny,? 
children (Count:2) Isa: kids, children ? Doing: biking, riding ? Has: pants, bike ? Attrib: young, small ? 
bike (Count:1) Isa: bike, bicycle,? Doing: playing,? Has: chain, pedal,? Attrib: silver, white,? 
women(Count:3) Isa: girls, models,? Doing: smiling,...  Has: shorts, bags,? Attrib: young, tan,? 
purses(Count:3) Isa: accessory,? Doing: containing,? Has: body, straps,? Attrib: black, soft,? 
sidewalk(Count:1) Isa: sidewalk, street,? Doing: laying,? Has: stone, cracks,? Attrib: flat, wide,? 
woman(Count:1) Isa: person, female,? Doing: pointing,? Has: nose, legs,? Attrib: tall, skinny,? 
tree(Count:1) Isa: plant,? Doing: growing,? Has: branches,? Attrib: tall, green,? 
kids(Count:5) Isa: group, teens,? Doing: walking,? Has: shoes, bags,? Attrib: young,? 
Five young people on the street, two sharing a bicycle.
Several young people are walking near parked vehicles.
Three girls with large handbags walking down the sidewalk.
Three women walk down a city street, as seen from above.
Three young woman walking down a sidewalk looking up.
Figure 1: An annotated image with human generated sen-
tence descriptions. Each bounding polygon encompasses one
or more objects and is associated with a count and text la-
bels.This image has 9 high level objects annotated with over
250 textual labels.
to much of the visual content needed to generate
complete, human-like sentences.
In this paper, we instead study generation with
more complete visual support, as provided by hu-
man annotations, allowing us to develop more
comprehensive models than previously consid-
ered. Such models have the dual benefit of (1)
providing new insights into how to construct more
human-like sentences and (2) allowing us to per-
form experiments that systematically study the
contribution of different visual cues in generation,
suggesting which automatic detectors would be
most beneficial for generation.
In an effort to approximate relatively complete
visual recognition, we collected manually labeled
representations of objects, parts, attributes and ac-
tivities for a benchmark caption generation dataset
that includes images paired with human authored
110
descriptions (Rashtchian et al., 2010).
2
As seen
in Figure 1, the labels include object boundaries
and descriptive text, here including the facts that
the children are ?riding? and ?walking? and that
they are ?young.? Our goal is to be as exhaustive
as possible, giving equal treatment to all objects.
For example, the annotations in Figure 1 contain
enough information to generate the first three sen-
tences and most of the content in the remaining
two. Labels gathered in this way are a type of fea-
ture norms (McRae et al., 2005), which have been
used in the cognitive science literature to approxi-
mate human perception and were recently used as
a visual proxy in distributional semantics (Silberer
and Lapata, 2012). We present the first effort, that
we are aware of, for using feature norms to study
image description generation.
Such rich data allows us to develop significantly
more comprehensive generation models. We di-
vide generation into choices about which visual
content to select and how to realize a sentence that
describes that content. Our approach is grammar-
based, feature-rich, and jointly models both deci-
sions. The content selection model includes la-
tent variables that align phrases to visual objects
and features that, for example, measure how vi-
sual salience and spatial relationships influence
which objects are mentioned. The realization ap-
proach considers a number of cues, including lan-
guage model scores, word specificity, and relative
spatial information (e.g. to produce the best spa-
tial prepositions), when producing the final sen-
tence. When used with a reranking model, includ-
ing global cues such as sentence length, this ap-
proach provides a full generation system.
Our experiments demonstrate high quality vi-
sual content selection, within 90% of human per-
formance on unigram BLEU, and improved com-
plete sentence generation, nearly halving the dif-
ference from human performance to two base-
lines on 4-gram BLEU. In ablations, we measure
the importance of different annotations and visual
cues, showing that annotation of activities and rel-
ative bounding box information between objects
are crucial to generating human-like description.
2 Related Work
A number of approaches have been proposed
for constructing sentences from images, includ-
ing copying captions from other images (Farhadi
2
Available at : http://homes.cs.washington.edu/?my89/
et al., 2010; Ordonez et al., 2011), using text
surrounding an image in a news article (Feng
and Lapata, 2010), filling visual sentence tem-
plates (Kulkarni et al., 2011; Yang et al., 2011;
Elliott and Keller, 2013), and stitching together ex-
isting sentence descriptions (Gupta and Mannem,
2012; Kuznetsova et al., 2012). However, due to
the lack of reliable detectors, especially for activi-
ties, many previous systems have a small vocab-
ulary and must generate many words, including
verbs, with no direct visual support. These prob-
lems also extend to video caption systems (Yu and
Siskind, 2013; Krishnamoorthy et al., 2013).
The Midge algorithm (Mitchell et al., 2012)
is most closely related to our approach, and will
provide a baseline in our experiments. Midge is
syntax-driven but again uses a small vocabulary
without direct visual support for every word. It
outputs a large set of sentences to describe all
triplets of recognized objects in the scene, but does
not include a content selection model to select the
best sentence. We extend Midge with content and
sentence selection rules to use it as a baseline.
The visual facts we annotate are motivated by
research in machine vision. Attributes are a
good intermediate representation for categoriza-
tion (Farhadi et al., 2009). Activity recognition
is an emerging area in images (Li and Fei-Fei,
2007; Yao et al., 2011; Sharma et al., 2013) and
video (Weinland et al., 2011), although less stud-
ied than object recognition. Also, parts have been
widely used in object recognition (Felzenszwalb
et al., 2010). Yet, no work tests the contribution of
these labels for sentence generation.
There is also a significant amount of work
on other grounded language problems, where re-
lated models have been developed. Visual re-
ferring expression generation systems (Krahmer
and Van Deemter, 2012; Mitchell et al., 2013;
FitzGerald et al., 2013) aim to identify specific
objects, a sub-problem we deal with when de-
scribing images more generally. Other research
generates descriptions in simulated worlds and,
like this work, uses feature rich models (Angeli
et al., 2010), or syntactic structures like PCFGs
(Chen et al., 2010; Konstas and Lapata, 2012) but
does not combine the two. Finally, Zitnick and
Parikh (2013) study sentences describing clipart
scenes. They present a number of factors influenc-
ing overall descriptive quality, several of which we
use in sentence generation for the first time.
111
3 Dataset
We collected a dataset of richly annotated images
to approximate gold standard visual recognition.
In collecting the data, we sought a visual annota-
tion with sufficient coverage to support the gen-
eration of as many of the words in the original
image descriptions as possible. We also aimed to
make it as visually exhaustive as possible?giving
equal treatment to all visible objects. This ensures
less bias from annotators? perception about which
objects are important, since one of the problems
we would like to solve is content selection. This
dataset will be available for future experiments.
We built on the dataset from (Rashtchian et
al., 2010) which contained 8,000 Flickr images
and associated descriptions gathered using Ama-
zon Mechanical Turk (MTurk). Restricting our-
selves to Creative Commons images, we sampled
500 images for annotation.
We collected annotations of images in three
stages using MTurk, and assigned each annotation
task to 3-5 workers to improve quality through re-
dundancy (Callison-Burch, 2009). Below we de-
scribe the process for annotating a single image.
Stage 1: We prompted five turkers to list all ob-
jects in an image, ignoring objects that are parts of
larger objects (e.g., the arms of a person), which
we collected later in Stage 3. This list also in-
cluded groups, such as crowds of people.
Stage 2: For each unique object label from
Stage 1, we asked two turkers to draw a polygon
around the object identified.
3
In cases where the
object is a group, we also asked for the number of
objects present (1-6 or many). Finally, we created
a list of all references to the object from the first
stage, which we call the Object facet.
Stage 3: For each object or group, we prompted
three turkers to provide descriptive phrases of:
? Doing ? actions the object participates in, e.g.
?jumping.?
? Parts ? physical parts e.g. ?legs?, or other
items in the possession of the object e.g.
?shirt.?
? Attributes ? adjectives describing the object,
e.g. ?red.?
? Isa ? alternative names for a object e.g.
?boy?, ?rider.?
Figure 1 shows more examples for objects
3
We modified LabelMe (Torralba et al., 2010).
in a labeled image.
4
We refer to all of these
annotations, including the merged Object la-
bels, as facets. These labels provide feature
norms (McRae et al., 2005), which have recently
used as a visual proxy in distributional seman-
tics (Silberer and Lapata, 2012; Silberer et al.,
2013) but have not been previous studied for gen-
eration. This annotation of 500 images (2500
sentences) yielded over 4000 object instances and
100,000 textual labels.
4 Approach
Given such rich annotations, we can now de-
velop significantly more comprehensive genera-
tion models. In this section, we present an ap-
proach that first uses a generative model and then
a reranker. The generative model defines a dis-
tribution over content selection and content real-
ization choices, using diverse cues from the image
annotations. The reranker trades off our generative
model score, language model score (to encourage
fluency), and length to produce the final sentence.
Generative Model We want to generate a sen-
tence ~w = ?w
1
. . . w
n
? where each word w
i
? V
comes from a fixed vocabulary V . The vocabu-
lary V includes all 2700 words used in descriptive
sentences in the training set.
5
The model conditions on an annotated image I
that contains a set of objects O, where each ob-
ject o ? O has a bounding polygon and a number
of facets containing string labels. To model the
naming of specific objects, words w
i
can be asso-
ciated with alignment variables a
i
that range over
O. One such variable is introduced for each head
noun in the sentence. Figure 2 shows alignment
variable settings with colors that match objects in
the image. Finally, as a byproduct of the hierarchi-
cal generative process, we construct an undirected
dependency tree
~
d over the words in ~w.
The complete generative model defines the
probability p(~w,~a,
~
d | I) of a sentence ~w, word
alignments ~a, and undirected dependency tree
~
d,
given the annotated input image I . The overall
process unfolds recursively, as seen in Figure 3.
4
In the experiments, Parts and Isa facets do not improve
performance, so we do not use them in the final model. Isa
is redundant with the Object facet, as seen in Figure 1. Also
parts like clothing, were often annotated as separate objects.
5
We do not generate from image facets directly, because
only 20% of the sentences in our data can be produced like
this. Instead, we develop features which consider the similar-
ity between labels in the image and words in the vocabulary.
112
Three girls  with large  handbags  walking  down  the  sidewalk 
? = ?= ? = 
Figure 2: One path through the generative model and the
Bayesian network it induces. The first row of colored circles
are alignment variables to objects in the image. The second
row is words, generated conditioned on alignments.
The main clause is produced by first selecting the
subject alignment a
s
followed by the subject word
w
s
. It then chooses the verb and optionally the ob-
ject alignment a
o
and word w
o
. The process then
continues recursively, modifying the subject, verb,
and object of the sentence with noun and prepo-
sitional modifiers. The recursion begins at Step
2 in Figure 3. Given a parent word w and that
word?s relevant alignment variable a, the model
creates attachments where w is the grammatical
head of subsequently produced words. Choices
about whether to create noun modifiers or preposi-
tional modifiers are made in steps (a) and (b). The
process chooses values for the alignment variables
and then chooses content words, adding connec-
tive prepositions in the case of prepositional mod-
ifiers. It then chooses to end or submits new word-
alignment pairs to be recursively modified.
Each line defines a decision that must be made
according to a local probability distribution. For
example, Step 1.a defines the probability of align-
ing a subject word to various objects in the im-
age. The distributions are maximum entropy mod-
els, similar to previous work (Angeli et al., 2010),
using features described in the next section. The
induced undirected dependency tree
~
d has an edge
between each word and the previously generated
word (or the input word w in Steps 2.a.i and 2.a.ii,
when no previous word is available). Figure 2
shows a possible output from the process, along
with the Bayesian network that encodes what each
decision was conditioned on during generation.
Learning We learn the model from data
{(~w
i
,
~
d
i
, I
i
) | i = 1 . . .m} containing sentences
~w
i
, dependency trees
~
d
i
, computed with the Stan-
ford parser (de Marneffe et al., 2006), and images
1. for a main clause (d,e are optional), select:
(a) subject a
s
alignment from p
a
(a).
(b) subject word w
s
from p
n
(w | a
s
,
~
d
c
)
(c) verb word w
v
from p
v
(w | a
s
,
~
d
c
)
(d) object alignment a
o
from p
a
(a
?
| a
s
, w
v
,
~
d
c
)
(e) object word w
o
from p
n
(w | a
o
,
~
d
c
)
(f) end with p
stop
or go to (2) with (w
s
, a
s
)
(g) end with p
stop
or go to (2) with (w
v
, a
s
)
(h) end with p
stop
or go to (2) with (w
o
, a
o
)
2. for a (word, alignment) (w
?
, a) (a,b are optional):
(a) if w
?
not verb: modify w
?
with noun, select:
i. modifier word w
n
from p
n
(w | a,
~
d
c
).
ii. end with p
stop
or go to (2) with (a
m
, w
n
)
(b) modify w
?
with preposition, select:
i. preposition word w
p
if w
?
not a verb: from p
p
(w | a,
~
d
c
)
else: from p
p
(w | a,w
v
,
~
d
c
)
ii. object alignment a
p
from p
a
(a
?
| a,w
p
,
~
d
c
)
iii. object word w
n
from p
n
(w | a
p
,
~
d
c
).
iv. end with p
stop
or go to (2) with (a
p
, w
n
)
Figure 3: Generative process for producing words ~w, align-
ments ~a and dependencies
~
d. Each distribution is conditioned
on the partially complete path through generative process
~
d
c
to establish sentence context. The notation p
stop
is short hand
for p
stop
(STOP |~w,
~
d
c
) the stopping distribution.
I
i
. The dependency trees define the path that was
taken through the generative process in Figure 3
and are used to create a Bayesian network for ev-
ery sentence, like in Figure 2. However, object
alignments ~a
i
are latent during learning and we
must marginalize over them.
The model is trained to maximize the condi-
tional marginal log-likelihood of the data with reg-
ularization:
L(?) =
?
i
log
?
~a
p(~a, ~w
i
,
~
d
i
| I
i
; ?)? r|?|
2
where ? is the set of parameters and r is the regu-
larization coefficient. In essence, we maximize the
likelihood of every sentence?s observed Bayesian
network, while marginalizing over content selec-
tion variables we did not observe.
Because the model only includes pairwise de-
pendencies between the hidden alignment vari-
ables ~a, the inference problem is quadratic in the
number of objects and non-convex because ~a is
unobserved. We optimize this objective directly
with L-BFGS, using the junction-tree algorithm to
compute the sum and the gradient.
6
6
To compute the gradient, we differentiate the recurrence
in the junction-tree algorithm by applying the product rule.
113
Inference To describe an image, we need to
maximize over word, alignment, and the depen-
dency parse variables:
argmax
~w,~a,
~
d
p(~w,~a,
~
d | I)
This computation is intractable because we
need to consider all possible sentences, so we use
beam search for strings up to a fixed length.
Reranking Generating directly from the process
in Figure 3 results in sentences that may be short
and repetitive because the model score is a product
of locally normalized distributions. The reranker
takes as input a candidate list c, for an image I , as
decoded from the generative model. The candidate
list includes the top-k scoring hypotheses for each
sentence length up to a fixed maximum. A linear
scoring function is used for reranking optimized
with MERT (Och, 2003) to maximize BLEU-2.
5 Features
We construct indicator features to capture vari-
ation in usage in different parts of the sen-
tence, types of objects that are mentioned, visual
salience, and semantic and visual coordination be-
tween objects. The features are included in the
maximum entropy models used to parameterize
the distributions described in Figure 3. Whenever
possible, we use WordNet Synsets (Miller, 1995)
instead of lexical features to limit over-fitting.
Features in the generative model use tests for
local properties, such as the identity of a synset
of a word in WordNet, conjoined with an iden-
tifier that indicates context in the generative pro-
cess.
7
Generative model features indicate (1) vi-
sual and semantic information about objects in dis-
tributions over alignments (content selection) and
(2) preferences for referring to objects in distribu-
tions over words (content realization). Features in
the reranking model indicate global properties of
candidate sentences. Exact formulas for comput-
ing the features are in the appendix.
Visual features, such as an object?s position in
the image, are used for content selection. Pairwise
visual information between two objects, for exam-
ple the bounding box overlap between objects or
the relative position of the two objects, is included
in distributions where selection of an alignment
7
For example, in Figure 2 the context for the word ?side-
walk? would be ?word,syntactic-object,verb,preposition? in-
dicating it is a word, in the syntactic object of a preposition,
which was attached to a verb modifying prepositional phrase.
variable conditions on previously generated align-
ments. For verbs (Step 1.d in Figure 3) and prepo-
sitions (Step 2.b.ii), these features are conjoined
with the stem of the connective.
Semantic types of objects are also used in con-
tent selection. We define semantic types by finding
synsets of labels in objects that correspond to high
level types, a list motivated by the animacy hierar-
chy (Zaenen et al., 2004).
8
Type features indicate
the type of the object referred to by an alignment
variable as well as the cross product of types when
an alignment variable is on conditioning side of
a distribution (e.g. Step 1.d). Like above, in the
presence of a connective word, these features are
conjoined with the stem of the connective.
Content realization features help select words
when conditioning on chosen alignments (e.g.
Step 1.b). These features include the identity of
the WordNet synset corresponding to a word, the
word?s depth in the synset hierarchy, the language
model score for adding that word
9
and whether the
word matches labels in facets corresponding to the
object referenced by an alignment variable.
Reranking features are primarily used to over-
come issues of repetition and length in the genera-
tive distributions, more commonly used for align-
ment, than to create sentences. We use only four
features: length, the number of repetitions, gener-
ative model score, and language model score.
6 Experimental Setup
Data We used 70% of the data for training (1750
sentences, 350 images), 15% for development, and
15% for testing (375 sentences, 75 images).
Parameters The regularization parameter was
set on the held out data to r = 8. The reranker
candidate list included the top 500 sentences for
each sentence length up to 15 and weights were
optimized with Z-MERT (Zaidan, 2009).
Metrics Our evaluation is based on BLEU-n
(Papineni et al., 2001), which considers all n-
grams up to length n. To assess human perfor-
mance using BLEU, we score each of the five ref-
erences against the four other ones and finally av-
erage the five BLEU scores. In order to make these
results comparable to BLEU scores for our model
8
For example, human, animal, artifact (a human created
object), natural body (trees, water, ect.), or natural artifact
(stick, leaf, rock).
9
We use tri-grams with Kneser-Ney smoothing over the 1
million caption data set (Ordonez et al., 2011).
114
and baselines, we perform the same five-fold aver-
aging when computing BLEU for each system.
We also compute accuracy for different syn-
tactic positions in the sentence. We look at a
number of categories: the main clause?s compo-
nents (S,V,O), prepositional phrase components,
the preposition (Pp) and their objects (Po) and
noun modifying words (N), including determiners.
Phrases match if they have an exact string match
and share context identifiers as defined in the fea-
tures sections.
Human Evaluation Annotators rated sentences
output by our full model against either human or a
baseline system generated descriptions. Three cri-
teria were evaluated: grammaticality, which sen-
tence is more complete and well formed; truthful-
ness, which sentence is more accurately capturing
something true in the image; and salience, which
sentence is capturing important things in the image
while still being concise. Two annotators anno-
tated all test pairs for all criteria for a given pair of
systems. Six annotators were used (none authors)
and agreement was high (Cohen?s kappa = 0.963,
0.823 and 0.703 for grammar, truth and salience).
Machine Translation Baseline The first base-
line is designed to see if it is possible to generate
good sentences from the facet string labels alone,
with no visual information. We use an extension of
phrase-based machine translation techniques (Och
et al., 1999). We created a virtual bitext by pair-
ing each image description (the target sentence)
with a sequence
10
of visual identifiers (the source
?sentence?) listing strings from the facet labels.
Since phrases produced by turkers lack many of
the functions words needed to create fluent sen-
tences, we added one of 47 function words either
at the start or the end of each output phrase.
The translation model included standard fea-
tures such as language model score (using our cap-
tion language model described previously), word
count, phrase count, linear distortion, and the
count of deleted source words. We also define
three features that count the number of Object, Isa,
and Doing phrases, to learn a preference for types
of phrases. The feature weights are tuned with
MERT (Och, 2003) to maximize BLEU-4.
Midge Baseline As described in related work,
the Midge system creates a set of sentences to de-
scribe everything in an input image. These sen-
10
We defined a consistent ordering of visual identifiers and
set the distortion limit of the phrase-based decoder to infinity.
BL-1 BL-2 BL-3 BL-4
Human 61.0 42.0 27.8 18.3
Full Model 57.1 35.7 18.3 9.5
MT Baseline 39.8 23.6 13.2 6.1
Midge Baseline 43.5 20.2 9.4 0.0
Table 1: Results for the test set for the BLEU1-4 metrics.
Grammar Full Other Equal
Full vs Human 7.65 19.4 72.94
Full vs MT 6.47 5.29 88.23
Full vs Midge 40.59 15.88 43.53
Truth Full Other Equal
Full vs Human 0.59 67.65 31.76
Full vs MT 30.0 10.59 59.41
Full vs Midge 51.76 27.71 23.53
Salience Full Other Equal
Full vs Human 8.82 88.24 2.94
Full vs MT 51.76 16.47 31.77
Full vs Midge 71.18 14.71 14.12
Table 2: Human evaluation of our Full-Model in heads
up tests against Human authored sentences and baseline sys-
tems, the machine translation baseline (MT) and the Midge
inspired baseline. Bold indicates the better system. Other is
not the Full system. Equal indicates neither sentence is better.
tences must all be true, but do not have to select
the same content that a person would. It can be
adapted to our task by adding object selection and
sentence ranking rules. For object selection, we
choose the three most frequently named objects
in the scene according to a background corpus of
image descriptions. For sentence selection, we
take all sentences within one word of the average
length of a sentence in our corpus, 11, and select
the one with best Midge generation score.
7 Results
We report experiments for our generation pipeline
and ablations that remove data and features.
Overall Performance Table 1 shows the re-
sults on the test set. The full model consis-
tently achieves the highest BLEU scores. Overall,
these numbers suggest strong content selection by
getting high recall for individual words (BLEU-
1), but fall further behind human performance as
the length of the n-gram grows (BLEU-2 through
BLEU-4). These number match our perception
that the model is learning to produce high quality
sentences, but does not always describe all of the
important aspects of the scene or use exactly the
expected wording. Table 4 presents example out-
put, which we will discuss in more detail shortly.
115
Model BL-1 BL-2 BL-3 BL-4 S V O Pp Po N
Human 64.7 46.0 31.5 20.1 - - - - - -
Full-Model 59.0 36.9 19.3 10.5 64.9 40.4 36.8 50.0 20.7 69.1
? doing 51.1 32.6 16.9 9.2 63.2 15.8 10.5 45.5 21.6 69.7
? count 55.4 33.5 16.0 8.5 59.6 35.1 15.4 53.7 19.5 66.7
? properties 57.8 37.2 18.8 10.0 61.4 36.8 36.8 47.1 20.7 73.5
? visual 56.7 35.1 18.9 9.4 64.9 36.8 50.0 41.8 15.3 71.6
? pairwise 56.9 35.5 16.5 8.2 64.9 40.4 45.5 42.4 21.2 70.9
Table 3: Ablation results on development data using BLEU1-4 and reporting match accuracy for sentence structures.
S: A girl playing a
guitar in the grass
R: A woman with a nylon stringed
guitar is playing in a field
S: A man playing with two
dogs in the water
R: A man is throwing a log into
a waterway while two dogs watch
S: Two men playing with
a bench in the grass
R: Nine men are playing a game
in the park, shirts versus skins
S: Three kids sitting on a road
R: A boy runs in a race
while onlookers watch
Table 4: Two good examples of output (top), and two ex-
amples of poor performance (bottom). Each image has two
captions, the system output S and a human reference R.
Human Evaluation Table 2 presents the results
of a human evaluation. The full model outper-
forms all baselines on every measure, but is not
always competitive with human descriptions. It
performs the best on grammaticality, where it is
judged to be as grammatical as humans. How-
ever, surprisingly, in many cases it is also often
judged equal to the other baselines. Examination
of baseline output reveals that the MT baseline of-
ten generates short sentences, having little chance
of being judged ungrammatical. Furthermore, the
Midge baseline, like our system, is a syntax-based
system and therefore often produces grammatical
sentences. Although our system performs well
with respect to the baselines on truthfulness, of-
ten the system constructs sentences with incorrect
prepositions, an issue that could be improved with
better estimates of 3-d position in the image. On
truthfulness, the MT baseline is comparable to our
system, often being judged equal, because its out-
put is short. Our system?s strength is salience, a
factor the baselines do not model.
Data Ablation Table 3 shows annotation abla-
tion experiments on the development set, where
we remove different classes of data labels to mea-
sure the performance that can be achieved with
less visual information. In all cases, the overall
behavior of the system varies, as it tries to learn to
compensate for the missing information.
Ablating actions is by far the most detrimental.
Overall BLEU score suffers and prediction accu-
racy of the verb (V) degrades significantly causing
cascading errors that affect the object of the verb
(O). Removing count information affects noun at-
tachment (N) performance. Images where deter-
miner use is important or where groups of objects
are best identified by the number (for example,
three dogs) are difficult to describe naturally. Fi-
nally, we see a tradeoff when removing properties.
There is an increase in noun modifier accuracy (N)
but a decrease in content selection quality (BL-1),
showing recall has gone down. In essence, the ap-
proach learns to stop trying to generate adjectives
and other modifiers that would rely on the missing
properties. The difference in BLEU score with the
Full-Model is small, even without these modifiers,
because there often still exists a a short output with
high accuracy.
Feature Ablation The bottom two rows in Ta-
ble 3 show ablations of the visual and pairwise
features, measuring the contribution of the visual
information provided by the bounding box anno-
tations. The ablated visual information includes
bounding-box positions and relative pairwise vi-
sual information. The pairwise ablation removes
the ability to model any interactions between ob-
jects, for example, relative bounding box or pair-
wise object type information.
Overall, prepositional phrase accuracy is most
affected. Ablating visual features significantly im-
pacts accuracy of prepositional phrases (Pp and
Po), affecting the use of preposition words the
most, and lowering fluency (BL-4). Precision in
116
the object of the verb (O) rises; the model makes
? 50% fewer predictions in that position than the
Full-Model because it lacks features to coordinate
subject and object of the verb. Ablating pairwise
features has similar results. While the model cor-
rects errors in the object of the preposition (Po)
with the addition of visual features, fluency is still
worse than Full-Model, as reflected by BL-4.
Qualitative Results Table 4 has examples of
good and bad system output. The first two im-
ages are good examples, including both system
output (S) and a human reference (R). The sec-
ond two contain lower quality outputs. Overall,
the model captures common ways to refer to peo-
ple and scenes. However, it does better for images
with fewer sentient objects because content selec-
tion is less ambiguous.
Our system does well at finding important ob-
jects. For example, in the first good image, we
mention the guitar instead of the house, both of
which are prominent and have high overlap with
the woman. In the second case, we identify that
both dogs and humans tend to be important actors
in scenes but poorly identify their relationship.
The bad examples show difficult scenes. In the
first description the broad context is not identi-
fied, instead focusing on the bench (highlighted in
red). The second example identifies a weakness
in our annotation: it encodes contradictory group-
ings of the people. The groupings covers all of
the children, including the boy running, and many
subsets of the people near the grass. This causes
ambiguity and our methods cannot differentiate
them, incorrectly mentioning just the children and
picking an inappropriate verb (one participant in
the group is not sitting). Improved annotation of
groups would enable the study of generation for
more complex scenes, such as these.
8 Conclusion
In this work we used dense annotations of images
to study description generation. The annotations
allowed us to not only develop new models, better
capable of generating human-like sentences, but
also to explore what visual information is crucial
for description generation. Experiments showed
that activity and bounding-box information is im-
portant and demonstrated areas of future work. In
images that are more complex, for example multi-
ple sentient objects, object grouping and reference
will be important to generating good descriptions.
Issues of this type can be explored with annota-
tions of increasing complexity.
Appendix A
This appendix describes the feature templates for
the generative model in greater detail.
Features in the generative model conjoin indica-
tors for local tests, such as STEM(w) which in-
dicates the stem of a wordw, with a global contex-
tual identifier CONTEXT(v, d) that indicates
properties of the generation history, as described
in detail below. Table 5 provides a reference for
which feature templates are used in the generative
model distributions, as defined in Figure 3.
8.1 Feature Templates
CONTEXT(n, d) is an indicator for a contex-
tual identifier for a variable n in the model de-
pending on the dependency structure d. There is
an indicator for all combinations of the type of n
(alignment or word), the position of n (subject,
syntactic object, verb, noun-modifier, or preposi-
tion), the position of the earliest variable along
the path to generate n, and the type of attach-
ment to that variable (noun or prepositional mod-
ifier). For example, in Figure 2 the context for
the word ?sidewalk? would be ?word,syntactic-
object,verb,preposition? indicating it is a word, the
object of a preposition, whose path was along a
verb modifying prepositional phrase.
11
TYPE(a) indicates the high level type of an
object referred to by alignment variable a. We
use synsets to define high level types including
human, animal, artifact, natural artifact and var-
ious synsets that capture scene information,
12
a
list motivated by the animacy hierarchy (Zaenen
et al., 2004). Each object is assigned a type by
finding the synset for its name (object facet), and
tracing the hypernym structure in Wordnet to find
the appropriate class, if one exists. Additionally,
the type indicates whether the object is a group or
not. For example, in Figure 2, the blue polygon
has type ?person,group?, or the red bike polygon
has type ?artifact,single.?
11
Similarly ?large? is ?word,noun,subject,preposition?
while ?girls? is special cased to ?word,subject,root? be-
cause it has no initial attachment. The alignment vari-
able above the word handbags is ?alignment,syntactic-
object,subject,preposition? because it an alignment variable,
is in the syntactic object position of a preposition and can be
located by following a subject attached pp.
12
WordNet divides these into synsets expressing water,
weather, nature and a few more.
117
Feature Family Included In Steps
CONTEXT(a
?
,
~
d
c
)?
{TYPE(a
?
),MENTION(a
?
, do),MENTION(a
?
, obj ),VISUAL(a
?
)}
p
a
(a
?
|
~
d
c
)
p
a
(a
?
| a,w,
~
d
c
)
1.a, 1.d, 2.b.ii
CONTEXT(a
?
,
~
d
c
)? {TYPE(a)?TYPE(a
?
),VISUAL2(a, a
?
)} p
a
(a
?
| a,w,
~
d
c
) 1.d, 2.b.i
CONTEXT(a
?
,
~
d
c
)?
{TYPE(a)?TYPE(a
?
)? STEM(w),VISUAL2(a, a
?
)? STEM(w)}
p
a
(a
?
| a,w,
~
d
c
) 1.d, 2.b.i
CONTEXT(a,
~
d
c
)?
{WORDNET(w),MATCH(w, a),SPECIFICITY(w, a),
ADJECTIVE(w, a),DETERMINER(w, a)}
p
n
(w | a,
~
d
c
) 1.b, 1.e, 2.a.i
2.b.ii
CONTEXT(a,
~
d
c
)? {MATCH(w, a),TYPE(a)? STEM(w)} p
v
(w | a,
~
d
c
) 1.c
CONTEXT(a
?
,
~
d
c
)?TYPE(a)? STEM(w
p
) p
p
(w | a,
~
d
c
)
p
p
(w | a,w
v
,
~
d
c
)
2.b.i
CONTEXT(a
?
,
~
d
c
)? STEM(w
v
)? STEM(w) p
p
(w | a,w
v
,
~
d
c
) 2.b.i
Table 5: Feature families and distributions that include them. ? indicates the cross-product of the indi-
cator features. Distributions are listed more than once to indicate they use multiple feature families.
VISUAL(a) returns indicators for visual facts
about the object that a aligns to. There is an in-
dicator for two quantities: (1) overlap of object?s
polygon with every horizontal third of the image,
as a fraction of the object?s area, and (2) the ob-
ject?s distance to the center of the image as frac-
tion of the diagonal of the image. Each quantity,
v, is put into three overlapping buckets: if v > .1,
if v > .5, and if v > .9.
VISUAL2(a, a
?
) indicates pairwise visual
facts about two objects. There is an indicator for
the following quantities bucketed: the amount of
overlap between the polygons for a and a
?
as a
fraction of the size of a?s polygon, the distance
between the center of the polygon for a and a
?
as
a fraction of image?s diagonal, and the slope be-
tween the center of a and a
?
. Each quantity, v, is
put into three overlapping buckets: if v > .1, if
v > .5, and if v > .9. There is an indicator for the
relative position of extremities a and a
?
: whether
the rightmost point of a is further right than a
?
?s
rightmost or leftmost point, and the same for top,
left, and bottom.
WORDNET(w) returns indicators for all hy-
pernyms of a word w. The two most specific
synsets are not used when there at least 8 options.
MENTION(a, facet) returns the union of the
WORDNET(w) features for all words w in the
facet facet for the object referred to alignment a.
ADJECTIVE(w, a) indicates four types
of features specific to adjective usage. If
MENTION(w,Attributes) is not empty, indi-
cate : (1) the satellite adjective synset of w in
Wordnet, (2) the head adjective synset of w in
Wordnet, (3) the head adjective synset conjoined
withTYPE(a), and (4) the number of times there
exists a label in the Attributes facet of a that has
the same head adjective synset as w.
DETERMINER(w, a) indicates four deter-
miner specific features. If w is a determiner, then
indicate : (1) the identity of w conjoined with the
count (the label for numerosity) of a, (2) the iden-
tity of w conjoined with an indicator for if the
count of a is greater than one, (3) the identity of w
conjoined with TYPE(a) and (4) the frequency
with which w appears before its head word in the
Flikr corpus (Ordonez et al., 2011).
MATCH(w, a), indicates all facets of object
a that contain words with the same stem as w.
SPECIFICITY(w, a) is an indicator of the
specificity of the word w when referring to the ob-
ject aligned to a. Indicates the relative depth of
w in Wordnet, as compared to all words w
?
where
MATCH(w
?
, a) is not empty. The depth is buck-
eted into quintiles.
STEM(w) returns the Porter2 stem of w.
13
The distribution for stopping, p
stop
(STOP |
~
d
c
, ~w), contains two types of features. (1) Struc-
tural features indicating for the number of times
a contextual identifier has appeared so far in the
derivation and (2) mention features indicating the
types of objects mentioned.
14
To compute men-
tion features, we consider all possible types of ob-
jects, t, then there is an indicator for: (1) if ?o, ?w ?
~w : MATCH(w, o) 6= ? ?TYPE(o) = t, (2) whether
?o, 6 ?w ? ~w : MATCH(w, o) 6= ??TYPE(o) = t and
(3) if (1) does not hold but (2) does.
Acknowledgments This work is partially funded by DARPA
CSSG (D11AP00277) and ARO (W911NF-12-1-0197). We
thank L. Zitnick, B. Dolan, M. Mitchell, C. Quirk, A. Farhadi,
B. Russell for helpful conversations. Also, L. Zilles, Y. Atrzi,
N. FitzGerald, T. Kwiatkowski and reviewers for comments.
13
http://snowball.tartarus.org/algorithms/english/stemmer.html
14
Object mention features cannot contain ~a because that
creates large dependencies in inference for learning.
118
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP, pages 286?295, August.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. JAIR,
37:397?435.
Marie-Catherine de Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In LREC, volume 6, pages 449?454.
Desmond Elliott and Frank Keller. 2013. Image De-
scription using Visual Dependency Representations.
In EMNLP.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Computer Vision and Pattern Recogni-
tion, 2009. CVPR 2009. IEEE Conference on, pages
1778?1785. IEEE.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: Generating sentences from images.
In Proceedings of the 11th European conference on
Computer Vision, ECCV?10, pages 15?29.
Pedro F Felzenszwalb, Ross B Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part-based
models. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 32(9):1627?1645.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? Automatic caption gener-
ation for news images. In ACL, pages 1239?1249.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logi-
cal forms for referring expression generation. In
EMNLP.
Ankush Gupta and Prashanth Mannem. 2012. From
image annotation to image description. In NIPS,
volume 7667, pages 196?204.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
ACL, pages 369?378.
Emiel Krahmer and Kees Van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173?218.
Niveda Krishnamoorthy, Girish Malkarnenkar, Ray-
mond Mooney, Kate Saenko, and Sergio Guadar-
rama. 2013. Generating natural-language video de-
scriptions using text-mined knowledge. Procedings
of AAAI, 2013(2):3.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.
Berg, and T.L. Berg. 2011. Baby talk: Understand-
ing and generating simple image descriptions. In
Computer Vision and Pattern Recognition (CVPR),
pages 1601?1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C
Berg, Tamara L Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL, pages 359?368.
Li-Jia Li and Li Fei-Fei. 2007. What, where and who?
Classifying events by scene and object recognition.
In ICCV, pages 1?8. IEEE.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris Mcnorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior Research Methods, 37(4):547?
559.
George A Miller. 1995. WordNet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daum?e, III.
2012. Midge: Generating image descriptions from
computer vision detections. In EACL, pages 747?
756.
Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2013. Generating expressions that refer to vis-
ible objects. In Proceedings of NAACL-HLT, pages
1174?1184.
F. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint Conf. of Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 20?28.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing images using 1 million
captioned photographs. In NIPS, pages 1143?1151.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL.
C. Rashtchian, P. Young, M. Hodosh, and J. Hock-
enmaier. 2010. Collecting image annotations us-
ing Amazon?s Mechanical Turk. In Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, pages 139?147.
119
Gaurav Sharma, Fr?ed?eric Jurie, Cordelia Schmid, et al.
2013. Expanded parts model for human attribute
and action recognition in still images. In CVPR.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In EMNLP, July.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In ACL, pages 572?582.
Antonio Torralba, Bryan C Russell, and Jenny Yuen.
2010. LabelMe: Online image annotation and appli-
cations. Proceedings of the IEEE, 98(8):1467?1484.
Daniel Weinland, Remi Ronfard, and Edmond Boyer.
2011. A survey of vision-based methods for action
representation, segmentation and recognition. Com-
puter Vision and Image Understanding, 115(2):224?
241.
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing.
Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai
Lin, Leonidas J. Guibas, and Li Fei-Fei. 2011.
Action recognition by learning bases of action at-
tributes and parts. In ICCV, Barcelona, Spain,
November.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, volume 1, pages 53?63.
Annie Zaenen, Jean Carletta, Gregory Garretson,
Joan Bresnan, Andrew Koontz-Garboden, Tatiana
Nikitina, M Catherine O?Connor, and Tom Wasow.
2004. Animacy encoding in English: why and how.
In ACL Workshop on Discourse Annotation, pages
118?125.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
C. Lawrence Zitnick and Devi Parikh. 2013. Bring-
ing semantics into focus using visual abstraction. In
CVPR.
120
