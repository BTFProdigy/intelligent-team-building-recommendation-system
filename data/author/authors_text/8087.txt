Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 675?682,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Argumentative Feedback: A Linguistically-motivated Term
Expansion for Information Retrieval
Patrick Ruch, Imad Tbahriti, Julien Gobeill
Medical Informatics Service
University of Geneva
24 Micheli du Crest
1201 Geneva
Switzerland
{patrick.ruch,julien.gobeill,imad.tbahriti}@hcuge.ch
Alan R. Aronson
Lister Hill Center
National Library of Medicine
8600 Rockville Pike
Bethesda, MD 20894
USA
alan@nlm.nih.gov
Abstract
We report on the development of a new au-
tomatic feedback model to improve informa-
tion retrieval in digital libraries. Our hy-
pothesis is that some particular sentences,
selected based on argumentative criteria,
can be more useful than others to perform
well-known feedback information retrieval
tasks. The argumentative model we ex-
plore is based on four disjunct classes, which
has been very regularly observed in scien-
tific reports: PURPOSE, METHODS, RE-
SULTS, CONCLUSION. To test this hy-
pothesis, we use the Rocchio algorithm as
baseline. While Rocchio selects the fea-
tures to be added to the original query
based on statistical evidence, we propose
to base our feature selection also on argu-
mentative criteria. Thus, we restrict the ex-
pansion on features appearing only in sen-
tences classified into one of our argumen-
tative categories. Our results, obtained on
the OHSUMED collection, show a signifi-
cant improvement when expansion is based
on PURPOSE (mean average precision =
+23%) and CONCLUSION (mean average
precision = +41%) contents rather than on
other argumentative contents. These results
suggest that argumentation is an important
linguistic dimension that could benefit in-
formation retrieval.
1 Introduction
Information retrieval (IR) is a challenging en-
deavor due to problems caused by the underly-
ing expressiveness of all natural languages. One
of these problems, synonymy, is that authors
and users frequently employ different words or
expressions to refer to the same meaning (acci-
dent may be expressed as event, incident, prob-
lem, difficulty, unfortunate situation, the subject
of your last letter, what happened last week, etc.)
(Furnas et al, 1987). Another problem is ambi-
guity, where a specific term may have several
(and sometimes contradictory) meanings and
interpretations (e.g., the word horse as in Tro-
jan horse, light horse, to work like a horse, horse
about). In order to obtain better meaning-based
matches between queries and documents, vari-
ous propositions have been suggested, usually
without giving any consideration to the under-
lying domain.
During our participation in different interna-
tional evaluation campaigns such as the TREC
Genomics track (Hersh, 2005), the BioCreative
initiative (Hirschman et al, 2005), as well as
in our attempts to deliver advanced search
tools for biologists (Ruch, 2006) and health-
care providers (Ruch, 2002) (Ruch, 2004), we
were more concerned with domain-specific in-
formation retrieval in which systems must re-
turn a ranked list of MEDLINE records in re-
sponse to an expert?s information request. This
involved a set of available queries describing
typical search interests, in which gene, pro-
tein names, and diseases were often essential
for an effective retrieval. Biomedical publica-
tions however tend to generate new informa-
tion very rapidly and also use a wide varia-
tion in terminology, thus leading to the cur-
rent situation whereby a large number of names,
symbols and synonyms are used to denote the
same concepts. Current solutions to these issues
can be classified into domain-specific strate-
gies, such as thesaurus-based expansion, and
domain-independent strategies, such as blind-
feedback. By proposing to explore a third type
of approach, which attempts to take advan-
tage of argumentative specificities of scientific
reports, our study initiates a new research di-
rection for natural language processing applied
to information retrieval.
The rest of this paper is organized as follows.
Section 2 presents some related work in infor-
mation retrieval and in argumentative parsing,
while Section 3 depicts the main characteristics
of our test collection and the metrics used in
our experiments. Section 4 details the strategy
675
used to develop our improved feedback method.
Section 5 reports on results obtained by varying
our model and Section 6 contains conclusions on
our experiments.
2 Related works
Our basic experimental hypothesis is that some
particular sentences, selected based on argu-
mentative categories, can be more useful than
others to support well-known feedback informa-
tion retrieval tasks. It means that selecting sen-
tences based on argumentative categories can
help focusing on content-bearing sections of sci-
entific articles.
2.1 Argumentation
Originally inspired by corpus linguistics studies
(Orasan, 2001), which suggests that scientific
reports (in chemistry, linguistics, computer sci-
ences, medicine...) exhibit a very regular logi-
cal distribution -confirmed by studies conducted
on biomedical corpora (Swales, 1990) and by
ANSI/ISO professional standards - the argu-
mentative model we experiment is based on four
disjunct classes: PURPOSE, METHODS, RE-
SULTS, CONCLUSION.
Argumentation belongs to discourse analy-
sis1, with fairly complex computational mod-
els such as the implementation of the rhetori-
cal structure theory proposed by (Marcu, 1997),
which proposes dozens of rhetorical classes.
More recent advances were applied to docu-
ment summarization. Of particular interest for
our approach, Teufel and Moens (Teufel and
Moens, 1999) propose using a list of manually
crafted triggers (using both words and expres-
sions such as we argued, in this article, the
paper is an attempt to, we aim at, etc.) to
automatically structure scientific articles into
a lighter model, with only seven categories:
BACKGROUND, TOPIC, RELATED WORK,
PURPOSE, METHOD, RESULT, and CON-
CLUSION.
More recently and for knowledge discovery in
molecular biology, more elaborated models were
proposed by (Mizuta and Collier, 2004) (Mizuta
et al, 2005) and by (Lisacek et al, 2005) for
novelty-detection. (McKnight and Srinivasan,
2003) propose a model very similar to our four-
class model but is inspired by clinical trials.
Preliminary applications were proposed for bib-
1After Aristotle, discourses structured following an
appropriate argumentative distribution belong to logics,
while ill-defined ones belong to rhetorics.
liometrics and related-article search (Tbahriti
et al, 2004) (Tbahriti et al, 2005), informa-
tion extraction and passage retrieval (Ruch et
al., 2005b). In these studies, sentences were se-
lected as the basic classification unit in order
to avoid as far as possible co-reference issues
(Hirst, 1981), which hinder readibity of auto-
matically generated and extracted sentences.
2.2 Query expansion
Various query expansion techniques have been
suggested to provide a better match between
user information needs and documents, and to
increase retrieval effectiveness. The general
principle is to expand the query using words
or phrases having a similar or related meaning
to those appearing in the original request. Vari-
ous empirical studies based on different IR mod-
els or collections have shown that this type of
search strategy should usually be effective in en-
hancing retrieval performance. Scheme propo-
sitions such as this should consider the various
relationships between words as well as term se-
lection mechanisms and term weighting schemes
(Robertson, 1990). The specific answers found
to these questions may vary; thus a variety
of query expansion approaches were suggested
(Efthimiadis, 1996).
In a first attempt to find related search terms,
we might ask the user to select additional terms
to be included in a new query, e.g. (Velez et
al., 1997). This could be handled interactively
through displaying a ranked list of retrieved
items returned by the first query. Voorhees
(Voorhees, 1994) proposed basing a scheme
based on the WordNet thesaurus. The au-
thor demonstrated that terms having a lexical-
semantic relation with the original query words
(extracted from a synonym relationship) pro-
vided very little improvement (around 1% when
compared to the original unexpanded query).
As a second strategy for expanding the orig-
inal query, Rocchio (Rocchio, 1971) proposed
accounting for the relevance or irrelevance of
top-ranked documents, according to the user?s
manual input. In this case, a new query was
automatically built in the form of a linear com-
bination of the term included in the previous
query and terms automatically extracted from
both the relevant documents (with a positive
weight) and non-relevant items (with a nega-
tive weight). Empirical studies (e.g., (Salton
and Buckley, 1990)) demonstrated that such an
approach is usually quite effective, and could
676
be used more than once per query (Aalbers-
berg, 1992). Buckley et al (Singhal et al,
1996b) suggested that we could assume, with-
out even looking at them or asking the user, that
the top k ranked documents are relevant. De-
noted the pseudo-relevance feedback or blind-
query expansion approach, this approach is usu-
ally effective, at least when handling relatively
large text collections.
As a third source, we might use large text
corpora to derive various term-term relation-
ships, using statistically or information-based
measures (Jones, 1971), (Manning and Schu?tze,
2000). For example, (Qiu and Frei, 1993)
suggested that terms to be added to a new
query could be extracted from a similarity the-
saurus automatically built through calculating
co-occurrence frequencies in the search collec-
tion. The underlying effect was to add idiosyn-
cratic terms to the underlying document col-
lection, related to the query terms by language
use. When using such query expansion ap-
proaches, we can assume that the new terms are
more appropriate for the retrieval of pertinent
items than are lexically or semantically related
terms provided by a general thesaurus or dic-
tionary. To complement this global document
analysis, (Croft, 1998) suggested that text pas-
sages (with a text window size of between 100
to 300 words) be taken into account. This local
document analysis seemed to be more effective
than a global term relationship generation.
As a forth source of additional terms, we
might account for specific user information
needs and/or the underlying domain. In this
vein, (Liu and Chu, 2005) suggested that terms
related to the user?s intention or scenario might
be included. In the medical domain, it was ob-
served that users looking for information usu-
ally have an underlying scenario in mind (or
a typical medical task). Knowing that the
number of scenarios for a user is rather lim-
ited (e.g., diagnosis, treatment, etiology), the
authors suggested automatically building a se-
mantic network based on a domain-specific the-
saurus (using the Unified Medical Language
System (UMLS) in this case). The effective-
ness of this strategy would of course depend
on the quality and completeness of domain-
specific knowledge sources. Using the well-
known term frequency (tf)/inverse document
frequency (idf) retrieval model, the domain-
specific query-expansion scheme suggested by
Liu and Chu (2005) produces better retrieval
performance than a scheme based on statis-
tics (MAP: 0.408 without query expansion,
0.433 using statistical methods and 0.452 with
domain-specific approaches).
In these different query expansion ap-
proaches, various underlying parameters must
be specified, and generally there is no sin-
gle theory able to help us find the most ap-
propriate values. Recent empirical studies
conducted in the context of the TREC Ge-
nomics track, using the OHSUGEN collection
(Hersh, 2005), show that neither blind expan-
sion (Rocchio), nor domain-specific query ex-
pansion (thesaurus-based Gene and Protein ex-
pansion) seem appropriate to improve retrieval
effectiveness (Aronson et al, 2006) (Abdou et
al., 2006).
3 Data and metrics
To test our hypothesis, we used the OHSUMED
collection (Hersh et al, 1994), originally devel-
oped for the TREC topic detection track, which
is the most popular information retrieval collec-
tion for evaluating information search in library
corpora. Alternative collections (cf. (Savoy,
2005)), such as the French Amaryllis collection,
are usually smaller and/or not appropriate to
evaluate our argumentative classifier, which can
only process English documents. Other MED-
LINE collections, which can be regarded as sim-
ilar in size or larger, such as the TREC Ge-
nomics 2004 and 2005 collections are unfortu-
nately more domain-specific since information
requests in these collection are usually target-
ing a particular gene or gene product.
Among the 348,566 MEDLINE citations of
the OHSUMED collection, we use the 233,455
records provided with an abstract. An exam-
ple of a MEDLINE citation is given in Table 1:
only Title, Abstract, MeSH and Chemical (RN)
fields of MEDLINE records were used for index-
ing. Out of the 105 queries of the OHSUMED
collection, only 101 queries have at least one
positive relevance judgement, therefore we used
only this subset for our experiments. The sub-
set has been randomly split into a training set
(75 queries), which is used to select the different
parameters of our retrieval model, and a test set
(26 queries), used for our final evaluation.
As usual in information retrieval evaluations,
the mean average precision, which computes the
precision of the engine at different levels (0%,
10%, 20%... 100%) of recall, will be used in our
experiments. The precision of the top returned
677
Title: Computerized extraction of coded find-
ings from free-text radiologic reports. Work in
progress.
Abstract: A computerized data acquisition
tool, the special purpose radiology understand-
ing system (SPRUS), has been implemented as
a module in the Health Evaluation through Log-
ical Processing Hospital Information System.
This tool uses semantic information from a di-
agnostic expert system to parse free-text radi-
ology reports and to extract and encode both
the findings and the radiologists? interpreta-
tions. These coded findings and interpretations
are then stored in a clinical data base. The sys-
tem recognizes both radiologic findings and di-
agnostic interpretations. Initial tests showed a
true-positive rate of 87% for radiographic find-
ings and a bad data rate of 5%. Diagnostic in-
terpretations are recognized at a rate of 95%
with a bad data rate of 6%. Testing suggests
that these rates can be improved through en-
hancements to the system?s thesaurus and the
computerized medical knowledge that drives it.
This system holds promise as a tool to obtain
coded radiologic data for research, medical au-
dit, and patient care.
MeSH Terms: Artificial Intelligence*; Deci-
sion Support Techniques; Diagnosis, Computer-
Assisted; Documentation; Expert Systems; Hos-
pital Information Systems*; Human; Natural
Language Processing*; Online Systems; Radi-
ology Information Systems*.
Table 1: MEDLINE records with, title, abstract
and keyword fields as provided by MEDLINE
librarians: major concepts are marked with *;
Subheadings and checktags are removed.
document, which is obviously of major impor-
tance is also provided together with the total
number of relevant retrieved documents for each
evaluated run.
4 Methods
To test our experimental hypothesis, we use the
Rocchio algorithm as baseline. In addition, we
also provide the score obtained by the engine
before the feedback step. This measure is nec-
essary to verify that feedback is useful for query-
ing the OHSUMED collection and to establish a
strong baseline. While Rocchio selects the fea-
tures to be added to the original queries based
on pure statistical analysis, we propose to base
our feature expansion also on argumentative cri-
teria. That is, we overweight features appear-
ing in sentences classified in a particular argu-
mentative category by the argumentative cate-
gorizer.
4.1 Retrieval engine and indexing units
The easyIR system is a standard vector-space
engine (Ruch, 2004), which computes state-
of-the-art tf.idf and probabilistic weighting
schema. All experiments were conducted with
pivoted normalization (Singhal et al, 1996a),
which has recently shown some effectiveness
on MEDLINE corpora (Aronson et al, 2006).
Query and document weighings are provided in
Equation (1): the dtu formula is applied to the
documents, while the dtn formula is applied to
the query; t the number of indexing terms, dfj
the number of documents in which the term tj ;
pivot and slope are constants (fixed at pivot =
0.14, slope = 146).
dtu: wij = (Ln(Ln(tfij)+1)+1)?idfj(1?slope)?pivot+slope?nti
dtn: wij = idfj ? (Ln(Ln(tfif ) + 1) + 1)
(1)
As already observed in several linguistically-
motivated studies (Hull, 1996), we observe that
common stemming methods do not perform well
on MEDLINE collections (Abdou et al, 2006),
therefore indexing units are stored in the in-
verted file using a simple S-stemmer (Harman,
1991), which basically handles most frequent
plural forms and exceptions of the English lan-
guage such as -ies, -es and -s and exclude end-
ings such as -aies, -eies, -ss, etc. This simple
normalization procedure performs better than
others and better than no stemming. We also
use a slightly modified standard stopword list of
544 items, where strings such as a, which stands
for alpha in chemistry and is relevant in biomed-
ical expressions such as vitamin a.
4.2 Argumentative categorizer
The argumentative classifier ranks and catego-
rizes abstract sentences as to their argumenta-
tive classes. To implement our argumentative
categorizer, we rely on four binary Bayesian
classifiers, which use lexical features, and a
Markov model, which models the logical distri-
bution of the argumentative classes in MED-
LINE abstracts. A comprehensive description
of the classifier with feature selection and com-
parative evaluation can be found in (Ruch et
al., 2005a)
To train the classifier, we obtained 19,555 ex-
plicitly structured abstracts from MEDLINE. A
678
Abstract: PURPOSE: The overall prognosis
for patients with congestive heart failure is poor.
Defining specific populations that might demon-
strate improved survival has been difficult [...]
PATIENTS AND METHODS: We identified 11
patients with severe congestive heart failure (av-
erage ejection fraction 21.9 +/- 4.23% (+/- SD)
who developed spontaneous, marked improve-
ment over a period of follow-up lasting 4.25 +/-
1.49 years [...] RESULTS: During the follow-up
period, the average ejection fraction improved
in 11 patients from 21.9 +/- 4.23% to 56.64
+/- 10.22%. Late follow-up indicates an aver-
age ejection fraction of 52.6 +/- 8.55% for the
group [...] CONCLUSIONS: We conclude that
selected patients with severe congestive heart
failure can markedly improve their left ventric-
ular function in association with complete reso-
lution of heart failure [...]
Table 2: MEDLINE records with explicit ar-
gumentative markers: PURPOSE, (PATIENTS
and) METHODS, RESULTS and CONCLU-
SION.
Bayesian classifier
PURP. METH. RESU. CONC.
PURP. 80.65 % 0 % 3.23 % 16 %
METH. 8 % 78 % 8 % 6 %
RESU. 18.58 % 5.31 % 52.21 % 23.89 %
CONC. 18.18 % 0 % 2.27 % 79.55 %
Bayesian classifier with Markov model
PURP. METH. RESU. CONC.
PURP. 93.35 % 0 % 3.23 % 3 %
METH. 3 % 78 % 8 % 6 %
RESU. 12.73 % 2.07 % 57.15 % 10.01 %
CONC. 2.27 % 0 % 2.27 % 95.45 %
Table 3: Confusion matrix for argumentative
classification. The harmonic means between re-
call and precision score (or F-score) is in the
range of 85% for the combined system.
conjunctive query was used to combine the fol-
lowing four strings: PURPOSE:, METHODS:,
RESULTS:, CONCLUSION:. From the original
set, we retained 12,000 abstracts used for train-
ing our categorizer, and 1,200 were used for fine-
tuning and evaluating the categorizer, following
removal of explicit argumentative markers. An
example of an abstract, structured with explicit
argumentative labels, is given in Table 2. The
per-class performance of the categorizer is given
by a contingency matrix in Table 3.
4.3 Rocchio feedback
Various general query expansion approaches
have been suggested, and in this paper we com-
pared ours with that of Rocchio. In this latter
case, the system was allowed to add m terms ex-
tracted from the k best-ranked abstracts from
the original query. Each new query was derived
by applying the following formula (Equation 2):
Q? = ? ? Q + (?/k) ?? kj = 1wij (2), in which
Q? denotes the new query built from the previ-
ous query Q, and wij denotes the indexing term
weight attached to the term tj in the document
Di. By direct use of the training data, we de-
termine the optimal values of our model: m =
10, k = 15. In our experiments, we fixed ? =
2.0, ? = 0.75. Without feedback the mean av-
erage precision of the evaluation run is 0.3066,
the Rocchio feedback (mean average precision =
0.353) represents an improvement of about 15%
(cf. Table 5), which is statistically2 significant
(p < 0.05).
4.4 Argumentative selection for
feedback
To apply our argumentation-driven feedback
strategy, we first have to classify the top-ranked
abstracts into our four argumentative moves:
PURPOSE, METHODS, RESULTS, and CON-
CLUSION. For the argumentative feedback, dif-
ferent m and k values are recomputed on the
training queries, depending on the argumenta-
tive category we want to over-weight. The ba-
sic segment is the sentence; therefore the ab-
stract is split into a set of sentences before being
processed by the argumentative classifier. The
sentence splitter simply applies as set of regu-
lar expressions to locate sentence boundaries.
The precision of this simple sentence splitter
equals 97% on MEDLINE abstracts. In this
setting only one argumentative category is at-
tributed to each sentence, which makes the de-
cision model binary.
Table 4 shows the output of the argumenta-
tive classifier when applied to an abstract. To
determine the respective value of each argumen-
tative contents for feedback, the argumenta-
tive categorizer parses each top-ranked abstract.
These abstracts are then used to generate four
groups of sentences. Each group corresponds to
a unique argumentative class. Each argumenta-
tive index contains sentences classified in one of
four argumentative classes. Because argumen-
2Tests are computed using a non-parametric signed
test, cf. (Zobel, 1998) for more details.
679
CONCLUSION (00160116) The highly favorable pathologic stage
(RI-RII, 58%) and the fact that the majority of patients were
alive and disease-free suggested a more favorable prognosis
for this type of renal cell carcinoma.
METHODS (00160119) Tumors were classified according to
well-established histologic criteria to determine stage of
disease; the system proposed by Robson was used.
METHODS (00162303) Of 250 renal cell carcinomas analyzed,
36 were classified as chromophobe renal cell carcinoma,
representing 14% of the group studied.
PURPOSE (00156456) In this study, we analyzed 250 renal cell
carcinomas to a) determine frequency of CCRC at our Hospital
and b) analyze clinical and pathologic features of CCRCs.
PURPOSE (00167817) Chromophobe renal cell carcinoma (CCRC)
comprises 5% of neoplasms of renal tubular epithelium. CCRC
may have a slightly better prognosis than clear cell carcinoma,
but outcome data are limited.
RESULTS (00155338) Robson staging was possible in all cases,
and 10 patients were stage 1) 11 stage II; 10 stage III, and
five stage IV.
Table 4: Output of the argumentative catego-
rizer when applied to an argumentatively struc-
tured abstract after removal of explicit mark-
ers. For each row, the attributed class is fol-
lowed by the score for the class, followed by the
extracted text segment. The reader can com-
pare this categorization with argumentative la-
bels as provided in the original abstract (PMID
12404725).
tative classes are equally distributed in MED-
LINE abstracts, each index contains approxi-
mately a quarter of the top-ranked abstracts
collection.
5 Results and Discussion
All results are computed using the treceval pro-
gram, using the top 1000 retrieved documents
for each evaluation query. We mainly evaluate
the impact of varying the feedback category on
the retrieval effectiveness, so we separately ex-
pand our queries based a single category. Query
expansion based on RESULTS or METHODS
sentences does not result in any improvement.
On the contrary, expansion based on PURPOSE
sentences improve the Rocchio baseline by +
23%, which is again significant (p < 0.05). But
the main improvement is observed when CON-
CLUSION sentences are used to generate the
expansion, with a remarkable gain of 41% when
compared to Rocchio. We also observe in Table
5 that other measures (top precision) and num-
ber of relevant retrieved articles do confirm this
trend.
For the PURPOSE category, the optimal k
parameter, computed on the test queries was
11. For the CONCLUSION category, the opti-
mal k parameter, computed on the test queries
was 10. The difference between the m values be-
tween Rocchio feedback and the argumentative
feedback, respectively 15 vs. 11 and 10 for Roc-
chio, PURPOSE, CONCLUSION sentences can
No feeback
Relevant Top Mean average
retrieved precision precision
1020 0.3871 0.3066
Rocchio feedback
Relevant Top Mean average
retrieved precision precision
1112 0.4020 0.353
Argumentative feedback: PURPOSE
Relevant Top Mean average
retrieved precision precision
1136 0.485 0.4353
Argumentative feedback: CONCLUSION
Relevant Top Mean average
retrieved precision precision
1143 0.550 0.4999
Table 5: Results without feedback, with Roc-
chio and with argumentative feedback applied
on PURPOSE and CONCLUSION sentences.
The number of relevant document for all queries
is 1178.
be explained by the fact that less textual mate-
rial is available when a particular class of sen-
tences is selected; therefore the number of words
that should be added to the original query is
more targeted.
From a more general perspective, the impor-
tance of CONCLUSION and PURPOSE sen-
tences is consistent with other studies, which
aimed at selecting highly content bearing sen-
tences for information extraction (Ruch et al,
2005b). This result is also consistent with
the state-of-the-art in automatic summariza-
tion, which tends to prefer sentences appearing
at the beginning or at the end of documents to
generate summaries.
6 Conclusion
We have reported on the evaluation of a
new linguistically-motivated feedback strategy,
which selects highly-content bearing features for
expansion based on argumentative criteria. Our
simple model is based on four classes, which
have been reported very stable in scientific re-
ports of all kinds. Our results suggest that
argumentation-driven expansion can improve
retrieval effectiveness of search engines by more
than 40%. The proposed methods open new
research directions and are generally promis-
ing for natural language processing applied to
information retrieval, whose positive impact is
still to be confirmed (Strzalkowski et al, 1998).
Finally, the proposed methods are important
from a theoretical perspective, if we consider
680
that it initiates a genre-specific paradigm as
opposed to the usual information retrieval ty-
pology, which distinguishes between domain-
specific and domain-independent approaches.
Acknowledgements
The first author was supported by a visiting
faculty grant (ORAU) at the Lister Hill Cen-
ter of the National Library of Medicine in 2005.
We would like to thank Dina Demner-Fushman,
Susanne M. Humphrey, Jimmy Lin, Hongfang
Liu, Miguel E. Ruiz, Lawrence H. Smith, Lor-
raine K. Tanabe, W. John Wilbur for the fruit-
ful discussions we had during our weekly TREC
meetings at the NLM. The study has also been
partially supported by the Swiss National Foun-
dation (Grant 3200-065228).
References
I Aalbersberg. 1992. Incremental Relevance
Feedback. In SIGIR, pages 11?22.
S Abdou, P Ruch, and J Savoy. 2006. Gen-
eral vs. Specific Blind Query Expansion for
Biomedical Searches. In TREC 2005.
A Aronson, D Demner-Fushman, S Humphrey,
J Lin, H Liu, P Ruch, M Ruiz, L Smith,
L Tanabe, and J Wilbur. 2006. Fusion
of Knowledge-intensive and Statistical Ap-
proaches for Retrieving and Annotating Tex-
tual Genomics Documents. In TREC 2005.
J Xu B Croft. 1998. Corpus-based stem-
ming using cooccurrence of word variants.
ACM-Transactions on Information Systems,
16(1):61?81.
E Efthimiadis. 1996. Query expansion. Annual
Review of Information Science and Technol-
ogy, 31.
G Furnas, T Landauer, L Gomez, and S Du-
mais. 1987. The vocabulary problem in
human-system communication. Communica-
tions of the ACM, 30(11).
D Harman. 1991. How effective is suffixing ?
JASIS, 42 (1):7?15.
W Hersh, C Buckley, T Leone, and D Hickam.
1994. OHSUMED: An interactive retrieval
evaluation and new large test collection for
research. In SIGIR, pages 192?201.
W Hersh. 2005. Report on the trec 2004 ge-
nomics track. pages 21?24.
Lynette Hirschman, Alexander Yeh, Chris-
tian Blaschke, and Alfonso Valencia. 2005.
Overview of BioCreAtIvE: critical assessment
of information extraction for biology. BMC
Bioinformatics, 6 (suppl. 1).
G Hirst. 1981. Anaphora in Natural Language
Understanding: A Survey. Lecture Notes in
Computer Science 119 - Springer.
D Hull. 1996. Stemming algorithms: A case
study for detailed evaluation. Journal of
the American Society of Information Science,
47(1):70?84.
K Sparck Jones. 1971. Automatic Keyword
Classification for Information Retrieval. But-
terworths.
F Lisacek, C Chichester, A Kaplan, and San-
dor. 2005. Discovering Paradigm Shift Pat-
terns in Biomedical Abstracts: Application
to Neurodegenerative Diseases. In Proceed-
ings of the First International Symposium on
Semantic Mining in Biomedicine (SMBM),
pages 212?217. Morgan Kaufmann.
Z Liu and W Chu. 2005. Knowledge-based
query expansion to support scenario-specific
retrieval of medical free text. ACM-SAC In-
formation Access and Retrieval Track, pages
1076?1083.
C Manning and H Schu?tze. 2000. Foundations
of Statistical Natural Language Processing.
MIT Press.
D Marcu. 1997. The Rhetorical Parsing of Nat-
ural Language Texts. pages 96?103.
L McKnight and P Srinivasan. 2003. Cate-
gorization of sentence types in medical ab-
stracts. AMIA Annu Symp Proc., pages 440?
444.
Y Mizuta and N Collier. 2004. Zone iden-
tification in biology articles as a basis for
information extraction. Proceedings of the
joint NLPBA/BioNLP Workshop on Natural
Language for Biomedical Applications, pages
119?125.
Y Mizuta, A Korhonen, T Mullen, and N Col-
lier. 2005. Zone Analysis in Biology Articles
as a Basis for Information Extraction. Inter-
national Journal of Medical Informatics, to
appear.
C Orasan. 2001. Patterns in Scientific Ab-
stracts. In Proceedings of Corpus Linguistics,
pages 433?445.
Y Qiu and H Frei. 1993. Concept based query
expansion. ACM-SIGIR, pages 160?69.
S Robertson. 1990. On term selection for
query expansion. Journal of Documentation,
46(4):359?364.
J Rocchio. 1971. Relevance feedback in infor-
mation retrieval in The SMART Retrieval
System - Experiments in Automatic Docu-
ment Processing. Prentice-Hall.
681
P Ruch, R Baud, C Chichester, A Geissbu?hler,
F Lisacek, J Marty, D Rebholz-Schuhmann,
I Tbahriti, and AL Veuthey. 2005a. Extract-
ing Key Sentences with Latent Argumenta-
tive Structuring. In Medical Informatica Eu-
rope (MIE), pages 835?40.
P Ruch, L Perret, and J Savoy. 2005b. Features
Combination for Extracting Gene Functions
from MEDLINE. In European Colloquium
on Information Retrieval (ECIR), pages 112?
126.
P Ruch. 2002. Using contextual spelling correc-
tion to improve retrieval effectiveness in de-
graded text collections. COLING 2002.
P Ruch. 2004. Query translation by text cate-
gorization. COLING 2004.
P Ruch. 2006. Automatic Assignment of
Biomedical Categories: Toward a Generic
Approach. Bioinformatics, 6.
G Salton and C Buckley. 1990. Improving re-
trieval performance by relevance feedback.
Journal of the American Society for Informa-
tion Science, 41(4).
J Savoy. 2005. Bibliographic database access
using free-text and controlled vocabulary: An
evaluation. Information Processing and Man-
agement, 41(4):873?890.
A Singhal, C Buckley, and M Mitra. 1996a.
Pivoted document length normalization.
ACM-SIGIR, pages 21?29.
C Buckley A Singhal, M Mitra, and G Salton.
1996b. New retrieval approaches using smart.
In Proceedings of TREC-4.
T Strzalkowski, G Stein, G Bowden Wise,
J Perez Carballo, P Tapanainen, T Jarvinen,
A Voutilainen, and J Karlgren. 1998. Natu-
ral language information retrieval: TREC-7
report. In Text REtrieval Conference, pages
164?173.
J Swales. 1990. Genre Analysis: English in
Academic and Research Settings. Cambridge
University Press.
I Tbahriti, C Chichester, F Lisacek, and
P Ruch. 2004. Using Argumention to
Retrieve Articles with Similar Citations
from MEDLINE. Proceedings of the joint
NLPBA/BioNLP Workshop on Natural Lan-
guage for Biomedical Applications.
I Tbahriti, C Chichester, F Lisacek, and
P Ruch. 2005. Using Argumentation to Re-
trieve Articles with Similar Citations: an In-
quiry into Improving Related Articles Search
in the MEDLINE Digital Library. Interna-
tional Journal of Medical Informatics, to ap-
pear.
S Teufel and M Moens. 1999. Argumenta-
tive Classification of Extracted Sentences as
a First Step Towards Flexible Abstracting.
Advances in Automatic Text Summarization,
MIT Press, pages 155?171.
B Velez, R Weiss, M Sheldon, and D Gifford.
1997. Fast and effective query refinement. In
ACM SIGIR, pages 6?15.
E Voorhees. 1994. Query expansion using
lexical-semantic relations. In ACM SIGIR,
pages 61?69.
J Zobel. 1998. How reliable are large-scale
information retrieval experiments? ACM-
SIGIR, pages 307?314.
682
Using Argumentation to Retrieve Articles with Similar Citations from 
MEDLINE 
Imad Tbahriti1,2, Christine Chichester1, Fr?d?rique Lisacek1,3, Patrick Ruch4 
1Geneva Bioinformatics (GeneBio) SA, 25, avenue de Champel, Geneva 
2Computer Science Dept., University of Geneva 
3Swiss Institute of Bioinformatics, Geneva 
4SIM, University Hospital of Geneva 
{imad.tbahriti;christine.chichester;frederique.lisacek}@genebio.com - patrick.ruch@epfl.ch
 
Abstract 
The aim of this study is to investigate the 
relationships between citations and the 
scientific argumentation found in the abstract. 
We extracted citation lists from a set of 3200 
full-text papers originating from a narrow 
domain.  In parallel, we recovered the 
corresponding MEDLINE records for analysis 
of the argumentative moves. Our 
argumentative model is founded on four 
classes: PURPOSE, METHODS, RESULTS, 
and CONCLUSION. A Bayesian classifier 
trained on explicitly structured MEDLINE 
abstracts generates these argumentative 
categories. The categories are used to generate 
four different argumentative indexes. A fifth 
index contains the complete abstract, together 
with the title and the list of Medical Subject 
Headings (MeSH) terms. To appraise the 
relationship of the moves to the citations, the 
citation lists were used as the criteria for 
determining relatedness of articles, 
establishing a benchmark. Our results show 
that the average precision of queries with the 
PURPOSE and CONCLUSION features is the 
highest, while the precision of the RESULTS 
and METHODS features was relatively low. A 
linear weighting combination of the moves is 
proposed, which significantly improves 
retrieval of related articles. 
1 Introduction 
Numerous techniques help researchers locate 
relevant documents in an ever-growing mountain 
of scientific publications. Among these techniques 
is the analysis of bibliographic information, which 
can identify conceptual connections between large 
numbers of articles. Although helpful, most of 
these systems deliver masses of documents to the 
researcher for analysis, which contain various 
degrees of similarity. This paper introduces a 
method to determine the similarity of a 
bibliographic co-citation list, that is the list of 
citations that are shared between articles, and the 
argumentative moves of an abstract in an effort to 
define novel similarity searches. 
 
Authors of biological papers develop arguments 
and present the justification for their experiments 
based on previously documented results. These 
results are represented as citations to earlier 
scientific literature and establish the links between 
old and new findings.  The assumption is that the 
majority of scientific papers employing the same 
citations depict related viewpoints. The method 
described here is applied to improve retrieval of 
similar articles based on co-citations, but other 
applications are foreseen. Documents that should 
be conceptually correlated due to bibliographic 
relatedness but which propose different or novel 
arguments are often not easily located in the 
majority of bibliographically correlated articles. 
Our system can be tuned to identify these 
documents. Conversely, such a system could also 
be used as a platform to aid authors by means of 
automatic assembly or refinement of their 
bibliographies through the suggestion of citations 
coming from documents containing similar 
arguments. 
 
The rest of this paper is structured as follows: 
section 2 describes the background related to 
experiments using citations or argumentation that 
compare aspects connected to the logical content 
of publications. Section 3 details the method and 
the generation of the different indexes used in our 
analyses, e.g. the citation index, the four 
argumentative indexes and the abstract index 
(abstract, title and keywords). Section 4 presents 
the results of the evaluations we performed. 
Section 5 closes with a summary of the 
contribution of this work, limitations and future 
work.  
2 Background 
Digital libraries aim at structuring their records to 
facilitate user navigation. Interfaces visualizing 
8
overlapping relationships of the standard library 
fields such as author and title in document 
collections are usually the most accessible to the 
user. Beyond these well-known targets, researchers 
(see de Bruijn and Martin, 2002, or Hirschman and 
al. 2002, for a survey) interested in information 
extraction and retrieval for biomedical applications 
have mostly focused on studying specific 
biological interactions (Stapley and Benoit, 2000; 
N?dellec et al, 2002; Dobrokhotov et al, 2003) 
and related entities (Collier et al, 2000; 
Humphreys et al, 2000; Yu et al, 2002; 
Yamamoto et al, 2003; Albert et al, 2003) or 
using terms in biomedical vocabularies (Nazarenko 
et al, 2001; Ruch et al, 2004; Srinivasan and 
Hristovski, 2004). The use of bibliographical and 
argumentative information (McKnight and 
Srinivasan 2003) has been less well studied by 
researchers interested in applying natural language 
processing to biomedical texts. 
2.1 Citations 
Originating from bibliometrics, citation analysis 
(White, 2003) has been used to visualize a field via 
a representative slice of its literature.  Co-citation 
techniques make it possible to cluster documents 
by scientific paradigm or hypothesis (Noyons et 
al., 1999). Braam et al, (1991) have investigated 
co-citation as a tool to map subject-matter 
specialties. They found that the combination of 
keyword analysis and co-citation analysis was 
useful in revealing the cognitive content of 
publications.   Peters et al, (1995) further explored 
the citation relationships and the cognitive 
resemblance in scientific articles. Word profile 
similarities of publications that were 
bibliographically coupled by a single, highly cited 
article were compared with publications that were 
not bibliographically coupled to that specific 
article. A statistically significant relationship has 
been established between the content of articles 
and their shared citations. This result will serve as 
basis to establish our benchmark without relevance 
judgments (Wu and Crestani, 2003; Soborrof et al, 
2001). 
2.2 Argumentation in biomedical abstracts 
Scientific research is often described as a problem 
solving activity. In full text scientific articles this 
problem-solution structure has been crystallized in 
a fixed presentation known as Introduction, 
Methods, Results and Conclusion. This structure is 
often presented in a much-compacted version in 
the abstract and it has been clearly demonstrated 
by Schuemie et al, (2004) that abstracts contain a 
higher information density than full text.  
Correspondingly, the 4-move problem-solving 
structure (standardized according to ISO/ANSI 
guidelines) has been found quite stable in scientific 
reports (Orasan, 2001). Although the 
argumentative structure of an article is not always 
explicitly labeled, or can be labeled using slightly 
different markers (as seen in Figure 1), a similar 
implicit structure is common in most biomedical 
abstracts (Swales, 1990). Therefore, to find the 
most relevant argumentative status that describes 
the content of the article, we employed a 
classification method to separate the content dense 
sentences of the abstracts into the argumentative 
moves. 
INTRODUCTION: Chromophobe renal cell 
carcinoma (CCRC) comprises 5% of neoplasms of 
renal tubular epithelium. CCRC may have a slightly 
better prognosis than clear cell carcinoma, but 
outcome data are limited.  PURPOSE: In this study, 
we analyzed 250 renal cell carcinomas to a) 
determine frequency of CCRC at our Hospital and b) 
analyze clinical and pathologic features of CCRCs.  
METHODS: A total of 250 renal carcinomas were 
analyzed between March 1990 and March 1999. 
Tumors were classified according to well-established 
histologic criteria to determine stage of disease; the 
system proposed by Robson was used. RESULTS: Of 
250 renal cell carcinomas analyzed, 36 were 
classified as chromophobe renal cell carcinoma, 
representing 14% of the group studied. The tumors 
had an average diameter of 14 cm. Robson staging 
was possible in all cases, and 10 patients were stage 
1) 11 stage II; 10 stage III, and five stage IV. The 
average follow-up period was 4 years and 18 (53%) 
patients were alive without disease.  CONCLUSION: 
The highly favorable pathologic stage (RI-RII, 58%) 
and the fact that the majority of patients were alive 
and disease-free suggested a more favorable 
prognosis for this type of renal cell carcinoma. 
Figure 1: Example of an explicitly structured abstract in 
MEDLINE.  The 4-class argumentation model is 
sometimes split into classes that may carry slightly 
different names, as illustrated in this example by the 
INTRODUCTION marker. 
3 Methods 
We established a benchmark based on citation 
analysis to evaluate the impact of using 
argumentation to find related articles. In 
information retrieval, benchmarks are developed 
from three resources: a document collection, a 
query collection and a set of relevance rankings 
that relates each query to the set of documents. 
Existing information retrieval collections normally 
contain user queries composed of only a few 
words. These short queries are not suitable for 
evaluating a system tailored to retrieve articles 
with similar citations.  Therefore, we have created 
the collection and tuned the system to accept long 
queries such as abstracts (Figure 2). 
9
 
Figure 2:Flowchart for the chain of experimental procedures.  The benchmark was assembled from 
citations shared between documents and compared to the document similarity ranking of EasyIR. 
 
3.1 Data acquisition and citation indexing 
All the data used in these experiments were 
acquired from MEDLINE using the PubMed 
interface. 
 
Document Collection.. The document set was 
obtained from PubMed by executing a set of 
Boolean queries to recover articles related to small 
active peptides from many animal species 
excluding humans.  These peptides hold the 
promise of becoming novel therapeutics. The set 
consisted of 12500 documents, which were 
comprised of abstract, title and MeSH terms. For 
3200 of these documents we were able to recover 
the full text including the references for citation 
extraction and analysis.  
 
Queries.. Following statistical analysis confirmed 
by Buckley and Voorhees (2000), four sets of 25 
articles were selected from the 3200 full text 
articles. The title, abstract and MeSH terms fields 
were used to construct the queries.  For testing the 
influence the argumentative move, the specific 
sentences were extracted and tested either alone or 
in combination with the queries that contained the 
title, abstract and MeSH terms. 
 
Citation analysis.. Citation lists were 
automatically extracted from 3200 full-text articles 
that were correspondingly represented within the 
document set. This automatic parsing of citations 
was manually validated. Each citation was 
represented as a unique ID for comparison 
purposes. Citation analysis of the entire collection 
demonstrated that the full-text articles possessed a 
mean citation count of 28.30 + 24.15  (mean + 
S.D.) with a 95% CI  = 27.47 ? 29.13.  Within 
these records the mean co-citation count was 7.79 
+ 6.99 (mean + S.D.) with a 95% CI  = 7.55 ? 8.03.  
As would be expected in a document set which 
contains a variety of document types (reviews, 
journal articles, editorials), the standard deviations 
of these values are quite large.   
 
Citation benchmark.. For each set of queries, a 
benchmark was generated from the 10 cited 
articles that contained the greatest number of co-
citations in common with the query. For the 
benchmark, the average number of cited articles 
that have more than 9 co-citations was 15.70          
+ 6.58 (mean + S.D.). Query sets were checked to 
confirm that at least one sentence in each abstract 
was classified per argumentative class. 
3.2 Metrics  
The main measure for assessing information 
retrieval engines is mean average precision (MAP). 
MAP is the standard metric although it may tend to 
hide minor differences in ranking (Mittendorf and 
Sch?uble, 1996). 
3.3 Text indexing 
For indexing, we used the easyIR system1, which 
implements standard vector space IR schemes 
(Salton et al, 1983). The term-weighting schema 
                                                     
1 http://lithwww.epfl.ch/~ruch/softs/softs.html. 
10
composed of combinations of term frequency, 
inverse document frequency and length 
normalization was varied to determine the most 
relevant output ranking. Table 1 gives the most 
common term weighting factors (atc.atn, ltc.atn); 
the first letter triplet applies to the document, the 
second letter triplet applies to the query (Ruch, 
2002). 
 
Table 1. Weighting parameters, following SMART 
conventions. 
3.4 Argumentative classification 
The classifier segmented the abstracts into 4 
argumentative moves: PURPOSE, METHODS, 
RESULTS, and CONCLUSION. 
Figure 3: The classification results for the abstract shown 
in Figure 1.  In each box, the attributed class is first, 
followed by the score for the class, followed by the 
extracted text segment. In this example, one of RESULTS 
sentences is misclassified as METHODS 
 
The classification unit is the sentence which means 
that abstracts are preprocessed using an ad hoc 
sentence splitter. The confusion matrix for the four 
argumentative moves generated by the classifier is 
given in Table 2. This evaluation used explicitly 
structured abstracts; therefore, the argumentative 
markers were removed prior to the evaluation. 
Figure 3 shows the output from the classifier, when 
applied to the abstract shown in Figure 1. After 
extraction, each of the four types of argumentative 
moves was then used for indexing, retrieval and 
comparison tasks. 
Table 2. Confusion matrices for each argumentative class. 
 PURP METH RESU CONC 
PURP 93.55% 0% 3.23% 3%
METH 8% 81% 8% 3%
RESU 7.43% 5.31% 74.25% 13.01%
CONC 2.27% 0% 2.27% 95.45%
3.5 Argumentative combination 
We adjusted the weight of the four argumentative 
moves, based on their location and then combined 
them to improve retrieval effectiveness.  The query 
weights were recomputed as indicated in equation 
(1).  
 
Wnew = Wold * Sc * kc (1) 
 
c ? {PURPOSE; METHODS; RESULTS; 
CONCLUSION} 
Wold: the feature weight as given by the query 
weighting (ltc) 
S: the normalized score attributed by the 
argumentative classifier to each sentence in the 
abstract. This score is attributed to each feature 
appearing in the considered segment 
CONCLUSION |00160116| The highly favorable pathologic 
stage (RI-RII, 58%) and the fact that the majority of 
patients were alive and disease-free suggested a more 
favorable prognosis for this type of renal cell carcinoma. 
METHODS |00160119| Tumors were classified according to 
well-established histologic criteria to determine stage of 
disease; the system proposed by Robson was used. 
METHODS |00162303| Of 250 renal cell carcinomas 
analyzed, 36 were classified as chromophobe renal cell 
carcinoma, representing 14% of the group studied. 
PURPOSE |00156456| In this study, we analyzed 250 renal 
cell carcinomas to a) determine frequency of CCRC at our 
Hospital and b) analyze clinical and pathologic features of 
CCRCs. 
PURPOSE |00167817| Chromophobe renal cell carcinoma 
(CCRC) comprises 5% of neoplasms of renal tubular 
epithelium. CCRC may have a slightly better prognosis 
than clear cell carcinoma, but outcome data are limited. 
RESULTS |00155338| Robson staging was possible in all 
cases, and 10 patients were stage 1) 11 stage II; 10 stage 
III, and five stage IV. 
k: a constant for each value of c. The value is set 
empirically using the tuning set (TS). The initial 
value of k for each category is given by the 
distribution observed in Table 4 (i.e., 0.625, 0.164, 
0.176, 0.560 for the classes, PURPOSE, 
METHODS, RESULTS and CONCLUSION 
respectively), and then an increment step (positive 
and negative) is varied to get the most optimal 
combination. 
 
This equation combines the score (Sc) attributed by 
the original weighting (ltc) for each feature (Wold) 
found in the query with a boosting factor (kc). The 
boosting factor was derived from the score 
provided by the argumentative classifier for each 
classified sentence. For these experiments, the 
parameters were determined with a tuning set (TS), 
one of the four query sets, and the final evaluation 
was done using the remaining three sets, the 
validation sets (VS). The document feature factor 
(atn) remained unchanged. 
4 Results 
In this section, we described the generation of the 
baseline measure and the effects of different 
conditions on this baseline. 
11
4.1 Comparison of text index parameters 
The use of a domain specific thesaurus tends to 
improve the MAP when compared to the citation 
benchmark, 0.1528 vs. 0.1517 for ltc.atn and 
0.1452 vs. 0.1433 for atc.atn (Table 3).  The ltc.atn 
weighting schema in combination with the 
thesaurus produced the best results, therefore these 
parameters were more likely to retrieve abstracts 
found in the citation index and thus were used for  
 all subsequent experiments.  
Table 3. Mean average precision (MAP) for each query set 
(1,2,3, and 4) with different term weighting schemas.  The 
last column gives the average MAP. T represents the 
thesaurus 
4.2 Argumentation-based retrieval 
For demonstrating that argumentative features can 
improve document retrieval, we first determined 
which argumentative class was the most content 
bearing.  Subsequently, we combined the four 
argumentative classes to again improve document 
retrieval.  
 
Table 4. MAP results from querying the collection using 
only the argumentative move. 
 
To determine the value of each argumentative 
move in the retrieval, the argumentative 
categorizer first parses each query abstract, 
generating four groups each representing a unique 
argumentative class. The document collection was 
separately queried with each group. Table 4 gives 
the MAP measures for each type of argumentation. 
Table 4 shows the sentences classified as 
PURPOSE provide the most useful content to 
retrieve similar documents. Baseline precision of 
62.5% is achieved when using only this section of 
the abstract. The CONCLUSION move is the 
second most valuable at 56% of the baseline. The 
METHODS and RESULTS sections appear less 
content bearing for retrieving similar documents, 
16.4% and 17.6%, respectively, of the baseline. 
Each argumentative set represents roughly a 
quarter of the textual content of the original 
abstract.   Querying with the PURPOSE section, 
(25% of the available textual material) realizes 
almost 2/3 of the average precision and for the 
CONCLUSION section, it is more than 50% of the 
baseline precision. In information retrieval queries 
and documents are often seen as symmetrical 
elements. This fact may imply the possible use of 
the argumentative moves as a technique to reduce 
the size of the indexed document collection or to 
help indexing pruning in large repositories (Carmel 
and al. 2001).  
4.3 Argumentative overweighting 
As implied in Table 4, Table 5 confirms that 
overweighting the features of PURPOSE and 
CONCLUSION sentences results in a gain in 
average precision (respectively +3.39% and +3.98 
for CONCLUSION and PURPOSE) as measured 
by citation similarity.  More specifically, Table 5 
demonstrates the use of PURPOSE and 
CONCLUSION as follows: 
 Set 1 Set 2 Set 3 Set 4 Average 
atc.atn 0.1402 0.1417 0.1438 0.1476 0.1433
atc.atn + T 0.1440 0.1431 0.1477 0.1465 0.1452
ltc.atn 0.1505 0.1528 0.1506 0.1529 0.1517
ltc.atn + T 0.1524 0.1534 0.1530 0.1539 0.1532  ? PURPOSE applies a boosting coefficient to 
features classified as PURPOSE by the 
argumentative classifier; 
? CONCLUSION applies a boosting coefficient 
to features classified as CONCLUSION by the 
argumentative classifier; 
? COMBINATION applies two different 
boosting coefficients to features classified as 
CONCLUSION and PURPOSE by the 
argumentative classifier. 
 
The results, in Table 5, from boosting PURPOSE 
and CONCLUSION features are given alongside 
the MAP and show an improvement of precision at 
the 5 and 10 document level. At the 5-document 
level the advantage is with the PURPOSE features, 
but at the 10-document level boosting the 
CONCLUSION features is more effective.  While 
the improvement brought by boosting PURPOSE 
and CONCLUSION features, when measured by 
MAP is modest (3-4%), the improvement observed 
by their optimal combination reached a significant 
improvement: + 5.48%. The various combinations 
of RESULTS and METHODS sections did not 
lead to any improvement. 
 PURP METH RESU CONC ltc.atn + T
MAP 0.0958 (62.5%) 
0.0251 
(16.4%)
0.0270 
(17.6%)
0.0858 
(56.0%) 0.1532
 
Argumentation has typically been studied in 
relation to summarization (Teufel and Moens, 
2002). Its impact on information retrieval is more 
difficult to establish although recent experiments 
(Ruch et al, 2003) tend to confirm that 
argumentation is useful for information extraction, 
as demonstrated by the extraction of gene 
functions for LocusLink curation.  Similarly, using 
the argumentative structure of scientific articles 
has been proposed to reduce noise (Camon et al, 
2004) in the assignment of Gene Ontology codes 
as investigated in the BioCreative challenge. In 
particular, it was seen that the use of ?Material and 
Methods? sentences should be avoided.  A fact 
which is confirmed by our results with the 
METHOD argumentative move. 
12
 Table 5. Retrieval results for the argumentative classes 
PURPOSE and CONCLUSION, and the combination of 
both classes. 
5 Conclusion and Future work 
We have reported on the construction of an 
information retrieval engine tailored to search for 
documents with similar citations in MEDLINE 
collections. The tool retrieves similar documents 
by giving more weight to features located in 
PURPOSE and CONCLUSION segments. The 
RESULTS and METHODS argumentative moves 
are reported here as less useful for such a retrieval 
task. Evaluated on a citation benchmark, the 
system significantly improves retrieval 
effectiveness of a standard vector-space engine.  In 
this context, it would be interesting to investigate 
how argumentation can be beneficial to perform ad 
hoc retrieval tasks in MEDLINE (Kayaalp et al, 
2003). 
 
Evidently using citation information to build our 
benchmark raises some questions. Authors may 
refer to other work in many ways to benefit the 
tone of their argument.  Specifically, there are two 
major citation contexts, one where an article is 
cited negatively or contrastively and one where an 
article is cited positively, or the authors state that 
their own work originates from the cited work.  In 
this study we have not made a distinction between 
these contexts but we consider this as an avenue 
for building better representations of the cited 
articles in future work. Finally, we are now 
exploring the use of the tool to detect 
inconsistencies between articles.  We hope to use 
citation and content analysis to identify articles 
containing novel views so as to expose differences 
in the consensus of the research area?s intellectual 
focus. The idea is to retrieve documents having 
key citation similarity but show some dissimilarity 
regarding a given argumentative category.   
 
Finally, we have observed that citation networks in 
digital libraries are analogous to hyperlinks in web 
repositories.  Consequently using web-inspired 
similarity measures may be beneficial for our 
purposes. Of particular interest in relation to 
argumentation, is the fact that citations networks, 
like web pages, are hierarchically nested graph 
with argumentative moves introducing 
intermediate levels (Bharat et al, 2001). 
 MAP Precision at 5 
Precision 
at 10 
ltc.atn + T 0.1532 0.2080 0.1840 
PURPOSE 0.1593 (+3.98%) 0.2240 0.1760 
CONCLUSION 0.1584 (+3.39%) 0.2160 0.1920 
COMBINATION 0.1616 (+5.48%) 0.2320 0.1960 
Acknowledgements 
We would like to thank Patrick Brechbiehl for his 
assistance in organizing the computing 
environment and Ron Appel for his support. 
References  
S. Albert, S. Gaudan, H. Knigge, A. Raetsch, A. 
Delgado, B. Huhse, H. Kirsch, M. Albers, D. 
Rebholz-Schuhmann and M. Koegl. 2003. 
Computer-assisted generation of a protein-
interaction database for nuclear receptors. 
Journal of Molecular Endocrinology, 17(8): 
1555-1567. 
K. Bharat, B. Chang, M. Rauch Henzinger, M. 
Ruhl: Who Links to Whom: Mining Linkage 
between Web Sites. ICDM 2001: 51-58 
R. R. Braam, H.F. Moed, and A.F.J. van Raan. 
1991 Mapping of science by combined co-
citation and word analysis, I: Structural Aspects, 
Journal of the American Society for Information 
Science, 42 (4): 233-251. 
C. Buckley and E. M. Voorhees. 2000. Evaluating 
evaluation measure stability, ACM SIGIR, p. 33-
40. 
E. Camon et al Personnal communication on 
BioCreative Task 2 Evaluation. BMC 
Bioinformatics Special Issue on 
BioCreative.2004. To be submitted. 
D. Carmel, E. Amitay, M. Herscovici, Y. Maarek, 
Y. Petruschka and A. Soffer: Juru at TREC 10 - 
Experiments with Index Pruning. TREC 2001 
N. Collier, C. Nobata and  J.I. Tsujii. 2000. 
Extracting the Names of Genes and Gene 
Products with a Hidden Markov Model. 
COLING 2000. 201-207. 
B. de Bruijn and J. Martin. Getting to the (c)ore of 
knowledge: mining biomedical literature. 2002. 
In International Journal of Medical Informatics, 
P Ruch and R Baud, eds., pages 7-18, Volume 
67, Issues 1-3, 4 , p. 7-18  
P. B. Dobrokhotov, C. Goutte, A. L. Veuthey, and 
?. Gaussier: Combining NLP and probabilistic 
categorisation for document and term selection 
for Swiss-Prot medical annotation. 2003. ISMB 
2003, 91-94. 
W. Hersh, S. Moy, D. Kraemer, L. Sacherek and  
D. Olson. 2003. More Statistical Power Needed: 
The OHSU TREC 2002 Interactive Track 
Experiments, TREC 2002. 
13
L Hirschman, JC Park, JI Tsujii, L Wong, C Wu: 
Accomplishments and challenges in literature 
data mining for biology. Bioinformatics 18(12): 
1553-1561 (2002) 
K. Humphreys, G. Demetriou and R. Gaizauskas. 
2000. Two Applications of Information 
Extraction to Biological Science Journal 
Articles: Enzyme Interactions and Protein 
Structures In Proceedings of the Workshop on 
Natural Language Processing for Biology, held 
at the Pacific Symposium on Biocomputing 
(PSB2000). 
M. Kayaalp, A.R. Aronson, S.M.  Humphrey, N.C. 
Ide, L.K. Tanabe, L.H. Smith, D. Demner, R.R.  
Loane, J.G. Mork, and O. Bodenrieder. 2003. 
Methods for accurate retrieval of MEDLINE 
citations in functional genomics.  In Notebook of 
the TREC-2003, pages 175-184, Gaithersburg, 
MD. 
L. McKnight and P. Srinivasan. 2003. 
Categorization of Sentence Types in Medical 
Abstracts. Proceedings of the 2003 AMIA 
conference. 
H. Mima,  S. Ananiadou, G. Nenadic, and J. Tsujii. 
A methodology for terminology-based knowledge 
acquisition and integration, 2002. COLING. 
Morgan Kaufmann. 
E Mittendorf and P Sch?uble. 1996. Measuring the 
effects of data corruption on information 
retrieval. SDAIR Proceedings. 
A. Nazarenko, P. Zweigenbaum, B. Habert and J. 
Bouaud. 2001. Corpus-based Extension of a 
Terminological Semantic Lexicon, Recent 
Advances in Computational Terminology. John 
Benjamins,  2001. 
C. N?dellec, M. Vetah and P. Bessi?res.  2001. 
Sentence filtering for information extraction in 
genomics, a classification problem.  In 
Proceedings PKDD, pages 326-237, Springer-
Verlag, Berlin. 
E.C.M. Noyons, H.F. Moed, and M. Luwel. 1999. 
A Bibliometric Study Combining Mapping and 
Citation Analysis for Evaluative Bibliometric 
Purposes. Journal of the American Society for 
Information. Science, 50(2):115-131.  
C. Orasan. 2001.  Patterns in scientific abstracts. 
In Proceedings of Corpus Linguistics, 433-445. 
H. P. F. Peters, R.R. Braam, and  A.F.J. van Raan. 
1995. Cognitive Resemblance and Citation 
Relations in Chemical Engineering Publications. 
Journal of the American Society for Information 
Science, 46 (1): 9-21. 
J.C. Reynar and A. Ratnaparkhi.. 1997. A 
maximum entropy approach to identifying 
sentence boundaries.  In Proceedings of the Fifth 
Conference on Applied Natural Language 
Processing, 16-19. 
P. Ruch. 2002. Using Contextual Spelling 
Correction to Improve Retrieval Effectiveness in 
Degraded Text Collections. COLING 2002. 
Morgan Kaufmann. 
P. Ruch, R. Baud and A. Geissb?hler. 2003. 
Learning-free Text Categorization, AIME, M 
Dojat, E Keravnou and P Barahona (Eds.). 199-
208, LNAI 2780. Springer. 
P. Ruch, C Chichester, G Cohen, G Coray, F 
Ehrler, H Ghorbel, H M?ller, and V Pallotta. 
2004. Report on the TREC 2003 Experiment: 
Genomic Track, TREC. 
M.J. Schuemie, M. Weeber, B.J.A Schijvenaars, 
E.M. van Mulligen, C.C. van der Eijk, R. Jeliert 
B. Mons, and J. A. Kors. 2004. Distribution of 
information in biomedical abstracts and full text 
publications. Bioinformatics. Submitted. 
I Soborrof, C. Nicholas and P. Cahan. 2001. 
Ranking Retrieval Systems without Relevance 
Judgments. SIGIR 2001: 66-73 
P. Srinivasan and D. Hristovski. 2004. Distilling 
Conceptual Connections from MeSH Co-
Occurrences. MEDINFO 2004. Submitted. 
B Stapley and G Benoir. 2000. BioBibliometrics: 
information retrieval and visualisation from co-
occurrences of gene names in MEDLINE 
abstracts. Pac. Symp. Biocomp. 5:526-537. 
J. Swales. 1990 Genre analysis: English in 
academic and research settings. Cambridge 
University Press, UK. 
S. Teufel and M. Moens: Summarizing Scientific 
Articles: Experiments with Relevance and 
Rhetorical Status. Computational Linguistics 
28(4): 409-445 (2002) 
H. White. 2003. Pathfinder networks and author 
cocitation analysis: a remapping of paradigmatic 
information scientists. J. Am. Soc. Inf. Sci. 
Technol 54(5) 423-434. 
S. Wu and F. Crestani. 2003. Methods for Ranking 
Information Retrieval Systems Without 
Relevance Judgments. SAC 2003: 811-816. 
ACM. 
K. Yamamoto, T. Kudo, A. Konagaya and Y. 
Matsumoto. 2003. Protein name tagging for 
biomedical annotation in text. ACL Workshop 
on Natural Language Processing in Biomedicine, 
pp. 65-72, July 2003.  
H. Yu, V. Hatzivassiloglou, C. Friedman, I.H. 
Iossifov, A. Rzhetsky and W.J. Wilbur. 2002. A 
rule-based approach for automatically 
identifying gene and protein names in MEDLINE 
abstracts: A proposal. ISMB 2002. 
14
