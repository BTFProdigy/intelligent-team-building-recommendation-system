Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824?831,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Comparative Study of Parameter Estimation Methods for 
Statistical Natural Language Processing 
Jianfeng Gao*, Galen Andrew*, Mark Johnson*&, Kristina Toutanova* 
*Microsoft Research, Redmond WA 98052, {jfgao,galena,kristout}@microsoft.com 
&Brown University, Providence, RI 02912,  mj@cs.brown.edu 
 
Abstract 
This paper presents a comparative study of 
five parameter estimation algorithms on four 
NLP tasks. Three of the five algorithms are 
well-known in the computational linguistics 
community: Maximum Entropy (ME) estima-
tion with L2 regularization, the Averaged 
Perceptron (AP), and Boosting.  We also in-
vestigate ME estimation with L1 regularization 
using a novel optimization algorithm, and 
BLasso, which is a version of Boosting with 
Lasso (L1) regularization.  We first investigate 
all of our estimators on two re-ranking tasks: a 
parse selection task and a language model 
(LM) adaptation task.  Then we apply the best 
of these estimators to two additional tasks 
involving conditional sequence models: a 
Conditional Markov Model (CMM) for part of 
speech tagging and a Conditional Random 
Field (CRF) for Chinese word segmentation. 
Our experiments show that across tasks, three 
of the estimators ? ME estimation with L1 or 
L2 regularization, and AP ? are in a near sta-
tistical tie for first place. 
1 Introduction 
Parameter estimation is fundamental to many sta-
tistical approaches to NLP. Because of the 
high-dimensional nature of natural language, it is 
often easy to generate an extremely large number of 
features.  The challenge of parameter estimation is 
to find a combination of the typically noisy, re-
dundant features that accurately predicts the target 
output variable and avoids overfitting. Intuitively, 
this can be achieved either by selecting a small 
number of highly-effective features and ignoring 
the others, or by averaging over a large number of 
weakly informative features.  The first intuition 
motivates feature selection methods such as 
Boosting and BLasso (e.g., Collins 2000; Zhao and 
Yu, 2004), which usually work best when many 
features are completely irrelevant. L1 or Lasso 
regularization of linear models, introduced by 
Tibshirani (1996), embeds feature selection into 
regularization so that both an assessment of the 
reliability of a feature and the decision about 
whether to remove it are done in the same frame-
work, and has generated a large amount of interest 
in the NLP community recently (e.g., Goodman 
2003; Riezler and Vasserman 2004).  If on the other 
hand most features are noisy but at least weakly 
correlated with the target, it may be reasonable to 
attempt to reduce noise by averaging over all of the 
features.  ME estimators with L2 regularization, 
which have been widely used in NLP tasks (e.g., 
Chen and Rosenfeld 2000; Charniak and Johnson 
2005; Johnson et al 1999), tend to produce models 
that have this property.  In addition, the perceptron 
algorithm and its variants, e.g., the voted or aver-
aged perceptron, is becoming increasingly popular 
due to their competitive performance, simplicity in 
implementation and low computational cost in 
training (e.g., Collins 2002). 
While recent studies claim advantages for L1 
regularization, this study is the first of which we are 
aware to systematically compare it to a range of 
estimators on a diverse set of NLP tasks.  Gao et al 
(2006) showed that BLasso, due to its explicit use of 
L1 regularization, outperformed Boosting in the LM 
adaptation task.  Ng (2004) showed that for logistic 
regression, L1 regularization outperforms L2 regu-
larization on artificial datasets which contain many 
completely irrelevant features.  Goodman (2003) 
showed that in two out of three tasks, an ME esti-
mator with a one-sided Laplacian prior (i.e., L1 
regularization with the constraint that all feature 
weights are positive) outperformed a comparable 
estimator using a Gaussian prior (i.e., L2 regulari-
zation).  Riezler and Vasserman (2004) showed that 
an L1-regularized ME estimator outperformed an 
L2-regularized estimator for ranking the parses of a 
stochastic unification-based grammar. 
824
While these individual estimators are well de-
scribed in the literature, little is known about the 
relative performance of these methods because the 
published results are generally not directly compa-
rable.  For example, in the parse re-ranking task, 
one cannot tell whether the L2- regularized ME 
approach used by Charniak and Johnson (2005) 
significantly outperforms the Boosting method by 
Collins (2000) because different feature sets and 
n-best parses were used in the evaluations of these 
methods.  
This paper conducts a much-needed comparative 
study of these five parameter estimation algorithms 
on four NLP tasks: ME estimation with L1 and L2 
regularization, the Averaged Perceptron (AP), 
Boosting, and BLasso, a version of Boosting with 
Lasso (L1) regularization.  We first investigate all of 
our estimators on two re-ranking tasks: a parse 
selection task and a language model adaptation task. 
Then we apply the best of these estimators to two 
additional tasks involving conditional sequence 
models: a CMM for POS tagging and a CRF for 
Chinese word segmentation.  Our results show that 
ME estimation with L2 regularization achieves the 
best performing estimators in all of the tasks, and 
AP achieves almost as well and requires much less 
training time. L1 (Lasso) regularization also per-
forms well and leads to sparser models. 
2 Estimators 
All the four NLP tasks studied in this paper are 
based on linear models (Collins 2000) which re-
quire learning a mapping from inputs ? ? ? to 
outputs ? ? ?.  We are given: 
? Training samples (?? ,??) for ? = 1??, 
? A procedure ??? to generate a set of candi-
dates ???(?) for an input x,  
? A feature mapping ?:? ? ? ? ??  to map 
each (?,?) to a vector of feature values, and 
? A parameter vector ? ? ?? , which assigns a 
real-valued weight to each feature. 
For all models except the CMM sequence model for 
POS tagging, the components ???, ? and ? di-
rectly define a mapping from an input ? to an output 
?(?) as follows: 
? ? = arg max????? ? ? ?,? ? ?. (1) 
In the CMM sequence classifier, locally normalized 
linear models to predict the tag of each word token 
are chained together to arrive at a probability esti-
mate for the entire tag sequence, resulting in a 
slightly different decision rule. 
Linear models, though simple, can capture very 
complex dependencies because the features can be 
arbitrary functions of the input/output pair.  For 
example, we can define a feature to be the log con-
ditional probability of the output as estimated by 
some other model, which may in turn depend on 
arbitrarily complex interactions of ?basic? features.  
In practice, with an appropriate feature set, linear 
models achieve very good empirical results on 
various NLP tasks.  The focus of this paper however 
is not on feature definition (which requires domain 
knowledge and varies from task to task), but on 
parameter estimation (which is generic across 
tasks).  We assume we are given fixed feature 
templates from which a large number of features are 
generated.  The task of the estimator is to use the 
training samples to choose a parameter vector ?, 
such that the mapping ?(?) is capable of correctly 
classifying unseen examples. We will describe the 
five estimators in our study individually. 
2.1 ME estimation with L2 regularization 
Like many linear models, the ME estimator chooses 
? to minimize the sum of the empirical loss on the 
training set and a regularization term: 
? = arg min?  ? ? + ? ?   . (2) 
In this case, the loss term L(w) is the negative con-
ditional log-likelihood of the training data, 
 ? ? = ? log? ??  ??)
?
?=1 ,  where 
? ? ?) =
exp ? ?,? ? ? 
 exp(? ?,? ? ? ?)? ????? ? 
 
and the regularizer term ? ? = ? ??
2
?  is the 
weighted squared L2 norm of the parameters. Here, 
? is a parameter that controls the amount of regu-
larization, optimized on held-out data.  
This is one of the most popular estimators,  
largely due to its appealing computational proper-
ties: both ? ?  and ?(?) are convex and differen-
tiable, so gradient-based numerical algorithms can 
be used to find the global minimum efficiently.  
In our experiments, we used the limited memory 
quasi-Newton algorithm (or L-BFGS, Nocedal and 
Wright 1999) to find the optimal ? because this 
method has been shown to be substantially faster 
than other methods such as Generalized Iterative 
Scaling (Malouf 2002).  
825
Because for some sentences there are multiple 
best parses (i.e., parses with the same F-Score), we 
used the variant of ME estimator described in 
Riezler et al (2002), where ? ?  is defined as the 
likelihood of the best parses ? ? ?(?) relative to 
the n-best parser output ??? ? ,  (i.e., ? ? ?
???(?)): ? ? = ? log ?(?? |??)????(??)
?
?=1 . 
We applied this variant in our experiments of 
parse re-ranking and LM adaptation, and found that 
on both tasks it leads to a significant improvement 
in performance for the L2-regularied ME estimator 
but not for the L1-regularied ME estimator. 
2.2 ME estimation with L1 regularization 
This estimator also minimizes the negative condi-
tional log-likelihood, but uses an L1 (or Lasso) 
penalty. That is, ?(?) in Equation (2) is defined 
according to ? ? = ?  ??  ? . L1 regularization 
typically leads to sparse solutions in which many 
feature weights are exactly zero, so it is a natural 
candidate when feature selection is desirable. By 
contrast, L2 regularization produces solutions in 
which most weights are small but non-zero. 
Optimizing the L1-regularized objective function 
is challenging because its gradient is discontinuous 
whenever some parameter equals zero. Kazama and 
Tsujii (2003) described an estimation method that 
constructs an equivalent constrained optimization 
problem with twice the number of variables.  
However, we found that this method is impracti-
cally slow for large-scale NLP tasks. In this work 
we use the orthant-wise limited-memory qua-
si-Newton algorithm (OWL-QN), which is a mod-
ification of L-BFGS that allows it to effectively 
handle the discontinuity of the gradient (Andrew 
and Gao 2007). We provide here a high-level de-
scription of the algorithm. 
A quasi-Newton method such as L-BFGS uses 
first order information at each iterate to build an 
approximation to the Hessian matrix, ?, thus mod-
eling the local curvature of the function. At each 
step, a search direction is chosen by minimizing a 
quadratic approximation to the function: 
? ? =
1
2
 ? ? ?0 
?? ? ? ?0 + ?0
? (? ? ?0) 
where ?0 is the current iterate, and ?0 is the func-
tion gradient at ?0 .  If ? is positive definite, the 
minimizing value of ? can be computed analytically 
according to: ?? = ?0 ??
?1?0. 
L-BFGS maintains vectors of the change in gradient 
?? ? ???1 from the most recent iterations, and uses 
them to construct an estimate of the inverse Hessian 
???. Furthermore, it does so in such a way that 
??1?0 can be computed without expanding out the 
full matrix, which is typically unmanageably large. 
The computation requires a number of operations 
linear in the number of variables. 
OWL-QN is based on the observation that when 
restricted to a single orthant, the L1 regularizer is 
differentiable, and is in fact a linear function of ?.  
Thus, so long as each coordinate of any two con-
secutive search points does not pass through zero, 
?(?) does not contribute at all to the curvature of 
the function on the segment joining them.  There-
fore, we can use L-BFGS to approximate the Hes-
sian of ? ?  alone, and use it to build an approxi-
mation to the full regularized objective that is valid 
on a given orthant. To ensure that the next point is in 
the valid region, we project each point during the 
line search back onto the chosen orthant.1 At each 
iteration, we choose the orthant containing the 
current point and into which the direction giving the 
greatest local rate of function decrease points. 
This algorithm, although only a simple modifi-
cation of L-BFGS, works quite well in practice. It 
typically reaches convergence in even fewer itera-
tions than standard L-BFGS takes on the analogous 
L2-regularized objective (which translates to less 
training time, since the time per iteration is only 
negligibly higher, and total time is dominated by 
function evaluations). We describe OWL-QN more 
fully in (Andrew and Gao 2007). We also show that 
it is significantly faster than Kazama and Tsujii?s 
algorithm for L1 regularization and prove that it is 
guaranteed converge to a parameter vector that 
globally optimizes the L1-regularized objective. 
2.3 Boosting 
The Boosting algorithm we used is based on Collins 
(2000).  It optimizes the pairwise exponential loss 
(ExpLoss) function (rather than the logarithmic loss 
optimized by ME).  Given a training sample 
(?? ,??), for each possible output ?? ? ???(??), we 
                                                     
1 This projection just entails zeroing-out any coordinates 
that change sign. Note that it is possible for a variable to 
change sign in two iterations, by moving from a negative 
value to zero, and on a the next iteration moving from 
zero to a positive value. 
826
define the margin of the pair (?? ,?? ) with respect to 
? as ? ?? ,??  = ? ?? ,?? ? ? ?  ? ?? ,??  ? ?. 
Then ExpLoss is defined as 
ExpLoss ? =  exp  ?M yi , yj  
?????? ?? ?
 (3) 
Figure 1 summarizes the Boosting algorithm we 
used. It is an incremental feature selection proce-
dure. After initialization, Steps 2 and 3 are repeated 
T times; at each iteration, a feature is chosen and its 
weight is updated as follows.  
First, we define Upd(?,?, ?)  as an updated 
model, with the same parameter values as ? with 
the exception of ?? , which is incremented by ?: 
Upd ?, ?, ? = (?1 ,? ,?? + ?,? ,??)  
Then, Steps 2 and 3 in Figure 1 can be rewritten as 
Equations (4) and (5), respectively. 
 ??, ?? = arg min
? ,?
ExpLoss(Upd ?, ?, ? ) (4) 
?? = Upd(???1, ??, ??) (5) 
Because Boosting can overfit we update the weight 
of ??? by a small fixed step size ?, as in Equation (6), 
following the FSLR algorithm (Hastie et al 2001).  
?? = Upd(???1, ??, ? ? sign ?? ) (6) 
By taking such small steps, Boosting imposes a 
kind of implicit regularization, and can closely 
approximate the effect of L1 regularization in a local 
sense (Hastie et al 2001).  Empirically, smaller 
values of ? lead to smaller numbers of test errors. 
2.4 Boosted Lasso 
The Boosted Lasso (BLasso) algorithm was origi-
nally proposed in Zhao and Yu (2004), and was 
adapted for language modeling by Gao et al (2006). 
BLasso can be viewed as a version of Boosting with 
L1 regularization. It optimizes an L1-regularized 
ExpLoss function: 
LassoLoss ? = ExpLoss(?) + ?(?) (7) 
where ? ? = ?  ??  ?  . 
BLasso also uses an incremental feature selec-
tion procedure to learn parameter vector ?, just as 
Boosting does.  Due to the explicit use of the regu-
larization term ?(?), however, there are two major 
differences from Boosting.  
At each iteration, BLasso takes either a forward 
step or a backward step.  Similar to Boosting, at 
each forward step, a feature is selected and its 
weight is updated according to Eq. (8) and (9). 
 ??, ?? = ??? ???
? ,?=??
ExpLoss(Upd ?, ?, ? ) (8) 
?? = Upd(???1, ??, ? ? sign ?? ) (9) 
There is a small but important difference between 
Equations (8) and (4). In Boosting, as shown in 
Equation (4), a feature is selected by its impact on 
reducing the loss with its optimal update ?? . By 
contrast, in BLasso, as shown in Equation (8), 
rather than optimizing over ? for each feature, the 
loss is calculated with an update of either +? or ??, 
i.e., grid search is used for feature weight estima-
tion.  We found in our experiments that this mod-
ification brings a consistent improvement. 
The backward step is unique to BLasso.  At each 
iteration, a feature is selected and the absolute value 
of its weight is reduced by ? if and only if it leads to 
a decrease of the LassoLoss, as shown in Equations 
(10) and (11), where ?  is a tolerance parameter. 
?? = arg min
? :???0
ExpLoss(Upd(?, ?,??sign ?? ) (10) 
?? = Upd(???1 , ??,sign(???) ? ?)  (11) 
if LassoLoss ???1,???1 ? LassoLoss ?? ,?? > ? 
Figure 2 summarizes the BLasso algorithm we 
used. After initialization, Steps 4 and 5 are repeated 
T times; at each iteration, a feature is chosen and its 
weight is updated either backward or forward by a 
fixed amount ?.  Notice that the value of ? is adap-
tively chosen according to the reduction of ExpLoss 
during training.  The algorithm starts with a large 
initial ?, and then at each forward step the value of 
? decreases until ExpLoss stops decreasing.  This is 
intuitively desirable: it is expected that most highly 
effective features are selected in early stages of 
training, so the reduction of ExpLoss at each step in 
early stages are more substantial than in later stages.  
These early steps coincide with the Boosting steps 
most of the time.  In other words, the effect of 
backward steps is more visible at later stages.  It can 
be proved that for a finite number of features and 
? =0, the BLasso algorithm shown in Figure 2 
converges to the Lasso solution when ? ? 0. See 
Gao et al (2006) for implementation details, and 
Zhao and Yu (2004) for a theoretical justification 
for BLasso. 
1 Set w0 = argminw0ExpLoss(w); and wd = 0 for d=1?D 
2 Select a feature fk* which has largest estimated 
impact on reducing ExpLoss of Equation (3) 
3 Update ?k* ?  ?k* + ?*, and return to Step 2 
Figure 1: The boosting algorithm 
827
2.5 Averaged Perceptron 
The perceptron algorithm can be viewed as a form 
of incremental training procedure (e.g., using sto-
chastic approximation) that optimizes a minimum 
square error (MSE) loss function (Mitchell, 1997).  
As shown in Figure 3, it starts with an initial pa-
rameter setting and updates it for each training 
example. In our experiments, we used the Averaged 
Perceptron algorithm of Freund and Schapire 
(1999), a variation that has been shown to be more 
effective than the standard algorithm (Collins 
2002).  Let ??,?  be the parameter vector after the ?th 
training sample has been processed in pass ? over 
the training data. The average parameters are de-
fined as?  =
?
??
  ??,???  where T is the number of 
epochs, and N is the number of training samples. 
3 Evaluations 
From the four tasks we consider, parsing and lan-
guage model adaptation are both examples of 
re-ranking.  In these tasks, we assume that we have 
been given a list of candidates ???(?) for each 
training or test sample  ?,? , generated using a 
baseline model.  Then, a linear model of the form in 
Equation (1) is used to discriminatively re-rank the 
candidate list using additional features which may 
or may not be included in the baseline model.  Since 
the mapping from ? to ? by the linear model may 
make use of arbitrary global features of the output 
and is performed ?all at once?, we call such a linear 
model a global model.  
In the other two tasks (i.e., Chinese word seg-
mentation and POS tagging), there is no explicit 
enumeration of ???(?).  The mapping from ? to ? 
is determined by a sequence model which aggre-
gates the decisions of local linear models via a 
dynamic program.  In the CMM, the local linear 
models are trained independently, while in the CRF 
model, the local models are trained jointly.  We call 
these two linear models local models because they 
dynamically combine the output of models that use 
only local features. 
While it is straightforward to apply the five es-
timators to global models in the re-ranking 
framework, the application of some estimators to 
the local models is problematic. Boosting and 
BLasso are too computationally expensive to be 
applied to CRF training and we compared the other 
three better performing estimation methods for this 
model. The CMM is a probabilistic sequence model 
and the log-loss used by ME estimation is most 
natural for it; thus we limit the comparison to the 
two kinds of ME models for CMMs. Note that our 
goal is not to compare locally trained models to 
globally trained ones; for a study which focuses on 
this issue, see (Punyakanok et al 2005). 
In each task we compared the performance of 
different estimators using task-specific measures. 
We used the Wilcoxon signed rank test to test the 
statistical significance of the difference among the 
competing estimators. We also report other results 
such as number of non-zero features after estima-
tion, number of training iterations, and computation 
time (in minutes of elapsed time on an XEONTM MP 
3.6GHz machine). 
3.1 Parse re-ranking 
We follow the experimental paradigm of parse 
re-ranking outlined in Charniak and Johnson 
(2005), and fed the features extracted by their pro-
gram to the five rerankers we developed.  Each uses 
a linear model trained using one of the five esti-
mators. These rerankers attempt to select the best 
parse ?  for a sentence ?  from the 50-best list of 
possible parses ??? ?  for the sentence. The li-
near model combines the log probability calculated 
by the Charniak (2000) parser as a feature with 
1,219,272 additional features.  We trained the fea-
1 Initialize w0: set w0 = argminw0ExpLoss(w), and wd = 0 
for d=1?D. 
2 Take a forward step according to Eq. (8) and (9), and 
the updated model is denoted by w1 
3 Initialize ? = (ExpLoss(w0)-ExpLoss(w1))/? 
4 Take a backward step if and only if it leads to a de-
crease of LassoLoss according to Eq. (10) and (11), 
where ?  = 0; otherwise 
5 Take a forward step according to Eq. (8) and (9); 
update ? = min(?, (ExpLoss(wt-1)-ExpLoss(wt))/? ); 
and return to Step 4. 
Figure 2: The BLasso algorithm 
1 Set w0 = 1 and wd = 0 for d=1?D 
2 For t = 1?T (T = the total number of iterations) 
3    For each training sample (xi, yi), i = 1?N 
4 
?? = arg max
????? ?_? 
? ?? , ? ? ? 
Choose the best candidate zi from GEN(xi) using 
the current model w, 
5       w = w +  ?(?(xi, yi) ? ?(xi, zi)), where ? is the size of 
learning step, optimized on held-out data. 
Figure 3: The perceptron algorithm 
 
828
ture weights w on Sections 2-19 of the Penn Tree-
bank, adjusted the regularizer constant ? to max-
imize the F-Score on Sections 20-21 of the Tree-
bank, and evaluated the rerankers on Section 22.  
The results are presented in Tables 12 and 2, where 
Baseline results were obtained using the parser by 
Charniak (2000).  
The ME estimation with L2 regularization out-
performs all of the other estimators significantly 
except for the AP, which performs almost as well 
and requires an order of magnitude less time in 
training.  Boosting and BLasso are feature selection 
methods in nature, so they achieve the sparsest 
models, but at the cost of slightly lower perfor-
mance and much longer training time. The 
L1-regularized ME estimator also produces a rela-
tively sparse solution whereas the Averaged Per-
ceptron and the L2-regularized ME estimator assign 
almost all features a non-zero weight.  
3.2 Language model adaptation 
Our experiments with LM adaptation are based on 
the work described in Gao et al (2006). The va-
riously trained language models were evaluated 
according to their impact on Japanese text input 
accuracy, where input phonetic symbols ?  are 
mapped into a word string ?. Performance of the 
application is measured in terms of character error 
                                                     
2
 The result of ME/L2 is better than that reported in 
Andrew and Gao (2007) due to the use of the variant of 
L2-regularized ME estimator, as described in Section 2.1. 
 CER # features time (min) #train iter 
Baseline 10.24%    
MAP 7.98%    
ME/L2 6.99% 295,337 27 665 
ME/L1 7.01% 53,342 25 864 
AP 7.23% 167,591 6 56 
Boost 7.54% 32,994 175 71,000 
BLasso 7.20% 33,126 238 250,000 
Table 3. Performance summary of estimators 
(lower is better) on language model adaptation 
 ME/L2 ME/L1 AP Boost BLasso 
ME/L2  ~ >> >> >> 
ME/L1 ~  >> >> >> 
AP << <<  >> ~ 
Boost << << <<  << 
BLasso << << ~ >>  
Table 4. Statistical significance test results. 
rate (CER), which is the number of characters 
wrongly converted from ? divided by the number of 
characters in the correct transcript. 
Again we evaluated five linear rerankers, one for 
each estimator. These rerankers attempt to select the 
best conversions ? for an input phonetic string ? 
from a 100-best list ???(?)of possible conver-
sions proposed by a baseline system. The linear 
model combines the log probability under a trigram 
language model as base feature and additional 
865,190 word uni/bi-gram features.  These 
uni/bi-gram features were already included in the 
trigram model which was trained on a background 
domain corpus (Nikkei Newspaper). But in the 
linear model their feature weights were trained 
discriminatively on an adaptation domain corpus 
(Encarta Encyclopedia). Thus, this forms a cross 
domain adaptation paradigm.  This also implies that 
the portion of redundant features in this task could 
be much larger than that in the parse re-ranking 
task, especially because the background domain is 
reasonably similar to the adaptation domain.  
We divided the Encarta corpus into three sets 
that do not overlap.  A 72K-sentences set was used 
as training data, a 5K-sentence set as development 
data, and another 5K-sentence set as testing data. 
The results are presented in Tables 3 and 4, where 
Baseline is the word-based trigram model trained 
on background domain corpus, and MAP (maxi-
mum a posteriori) is a traditional model adaptation 
method, where the parameters of the background 
model are adjusted so as to maximize the likelihood 
of the adaptation data.  
 F-Score # features time (min) # train iter 
Baseline 0.8986     
ME/L2 0.9176 1,211,026 62     129  
ME/L1 0.9165 19,121 37 174  
AP 0.9164 939,248 2 8  
Boosting 0.9131 6,714 495 92,600  
BLasso 0.9133 8,085 239 56,500  
Table 1: Performance summary of estimators on 
parsing re-ranking (ME/L2: ME with L2 regulari-
zation; ME/L1:  ME with L1 regularization) 
 ME/L2 ME/L1 AP Boost BLasso 
ME/L2  >> ~ >> >> 
ME/L1 <<  ~ > ~ 
AP ~ ~  >> > 
Boost << < <<  ~ 
Blasso << ~ < ~  
Table 2: Statistical significance test results (?>>? 
or ?<<? means P-value < 0.01; > or < means 0.01 < 
P-value ? 0.05; ?~? means P-value > 0.05)  
829
The results are more or less similar to those in 
the parsing task with one visible difference: L1 
regularization achieved relatively better perfor-
mance in this task.  For example, while in the 
parsing task ME with L2 regularization significantly 
outperforms ME with L1 regularization, their per-
formance difference is not significant in this task. 
While in the parsing task the performance differ-
ence between BLasso and Boosting is not signifi-
cant, BLasso outperforms Boosting significantly in 
this task.  Considering that a much higher propor-
tion of the features are redundant in this task than 
the parsing task, the results seem to corroborate the 
observation that L1 regularization is robust to the 
presence of many redundant features. 
3.3 Chinese word segmentation 
Our third task is Chinese word segmentation 
(CWS). The goal of CWS is to determine the 
boundaries between words in a section of Chinese 
text.  The model we used is the hybrid Mar-
kov/semi- Markov CRF described by Andrew 
(2006), which was shown to have state-of-the-art 
accuracy. We tested models trained with the various 
estimation methods on the Microsoft Research Asia 
corpus from the Second International Chinese Word 
Segmentation, and we used the same train/test split 
used in the competition.  The model and experi-
mental setup is identical with that of Andrew (2006) 
except for two differences.  First, we extracted 
features from both positive and negative training 
examples, while Andrew (2006) uses only features 
that occur in some positive training example. 
Second, we used the last 4K sentences of the 
training data to select the weight of the regularizers 
and to determine when to stop perceptron training. 
We compared three of the best performing es-
timation procedures on this task: ME with L2 regu-
larization, ME with L1 regularization, and the Av-
eraged Perceptron.  In this case, ME refers to mi-
nimizing the negative log-probability of the correct 
segmentation, which is globally normalized, while 
the perceptron is trained using at each iteration the 
exact maximum-scoring segmentation with the 
current weights. We observed the same pattern as in 
the other tasks: the three algorithms have nearly 
identical performance, while L1 uses only 6% of the 
features, and the Averaged Perceptron requires 
significantly fewer training iterations.  In this case, 
L1 was also several times faster than L2. The results 
are summarized in Table 5.3 
We note that all three algorithms performed 
slightly better than the model used by Andrew 
(2006), which also used L2 regularization (96.84 
F1).  We believe the difference is due to the use of 
features derived from negative training examples. 
3.4 POS tagging 
Finally we studied the impact of the regularization 
methods on a Maximum Entropy conditional 
Markov Model (MEMM, McCallum et al 2000) for 
POS tagging. MEMMs decompose the conditional 
probability of a tag sequence given a word sequence 
as follows: 
? ?1 ? ??  ?1 ??? = ?(??|???1 ????? ,?1 ???)
?
?=1
 
where the probability distributions for each tag 
given its context are ME models.  Following pre-
vious work (Ratnaparkhi, 1996), we assume that the 
tag of a word is independent of the tags of all pre-
ceding words given the tags of the previous two 
words (i.e., ?=2 in the equation above). The local 
models at each position include features of the 
current word, the previous word, the next word, and 
features of the previous two tags.  In addition to 
lexical identity of the words, we used features of 
word suffixes, capitalization, and number/special 
character signatures of the words. 
We used the standard splits of the Penn Treebank 
from the tagging literature (Toutanova et al 2003) 
for training, development and test sets.  The training 
set comprises Sections 0-18, the development set ? 
Sections 19-21, and the test set ? Sections 22-24.  
We compared training the ME models using L1 and 
L2 regularization.  For each of the two types of 
regularization we selected the best value of the 
regularization constant using grid search to optim-
ize the accuracy on the development set.  We report 
final accuracy measures on the test set in Table 6.  
The results on this task confirm the trends we 
have seen so far.  There is almost no difference in 
                                                     
3 Only the L2 vs. AP comparison is significant at a 0.05 
level according to the Wilcoxon signed rank test. 
 Test F1 # features # train iter 
ME/L2 0.9719 8,084,086 713 
ME/L1 0.9713 317,146 201 
AP 0.9703 1,965,719 162 
Table 5. Performance summary of estimators on 
CWS 
 
830
accuracy of the two kinds of regularizations, and 
indeed the differences were not statistically signif-
icant.  Estimation with L1 regularization required 
considerably less time than estimation with L2, and 
resulted in a model which is more than ten times 
smaller.  
4 Conclusions 
We compared five of the most competitive para-
meter estimation methods on four NLP tasks em-
ploying a variety of models, and the results were 
remarkably consistent across tasks.  Three of the 
methods ? ME estimation with L2 regularization, 
ME estimation with L1 regularization, and the Av-
eraged Perceptron ? were nearly indistinguishable 
in terms of test set accuracy, with ME estimation 
with L2 regularization perhaps enjoying a slight 
lead.  Meanwhile, ME estimation with L1 regulari-
zation achieves the same level of performance while 
at the same time producing sparse models, and the 
Averaged Perceptron provides an excellent com-
promise of high performance and fast training. 
These results suggest that when deciding which 
type of parameter estimation to use on these or 
similar NLP tasks, one may choose any of these 
three popular methods and expect to achieve com-
parable performance.  The choice of which to im-
plement should come down to other considerations: 
if model sparsity is desired, choose ME estimation 
with L1 regularization (or feature selection methods 
such as BLasso); if quick implementation and 
training is necessary, use the Averaged Perceptron; 
and ME estimation with L2 regularization may be 
used if it is important to achieve the highest ob-
tainable level of performance. 
References 
Andrew, G. 2006. A hybrid Markov/semi-Markov condi-
tional random field for sequence segmentation. In EMNLP, 
465-472. 
Andrew, G. and Gao, J. 2007. Scalable training of 
L1-regularized log-linear models. In ICML. 
Charniak, E. 2000. A maximum-entropy-inspired parser. In 
NAACL, 132-139. 
Charniak, E. and Johnson, M. 2005. Coarse-to-fine n-best 
parsing and MaxEnt discriminative re-ranking. In ACL. 
173-180. 
Chen, S.F., and Rosenfeld, R. 2000. A survey of smoothing 
techniques for ME models. IEEE Trans. On Speech and Audio 
Processing, 8(2): 37-50. 
Collins, M. 2000. Discriminative re-ranking for natural 
language parsing. In ICML, 175-182. 
Collins, M. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with per-
ceptron algorithms. In EMNLP, 1-8. 
Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An 
efficient boosting algorithm for combining preferences. In 
ICML?98.  
Freund, Y. and Schapire, R. E. 1999. Large margin classifica-
tion using the perceptron algorithm. In Machine Learning, 
37(3): 277-296. 
Hastie, T., R. Tibshirani and J. Friedman. 2001. The elements of 
statistical learning. Springer-Verlag, New York. 
Gao, J., Suzuki, H., and Yu, B. 2006. Approximation lasso 
methods for language modeling. In ACL. 
Goodman, J. 2004. Exponential priors for maximum entropy 
models. In NAACL. 
Johnson, M., Geman, S., Canon, S., Chi, Z., and Riezler, S. 
1999. Estimators for stochastic ?Unification-based? 
grammars. In ACL. 
Kazama, J. and Tsujii, J. 2003. Evaluation and extension of 
maximum entropy models with inequality constraints. In 
EMNLP. 
Malouf, R. 2002. A comparison of algorithms for maximum 
entropy parameter estimation. In HLT. 
McCallum A, D. Freitag and F. Pereira. 2000. Maximum 
entropy markov models for information extraction and 
segmentation. In ICML. 
Mitchell, T. M. 1997. Machine learning. The McGraw-Hill 
Companies, Inc. 
Ng, A. Y. 2004. Feature selection, L1 vs. L2 regularization, 
and rotational invariance. In ICML. 
Nocedal, J., and Wright, S. J. 1999. Numerical Optimization. 
Springer, New York. 
Punyakanok, V., D. Roth, W. Yih, and D. Zimak. 2005. 
Learning and inference over constrained output. In IJCAI. 
Ratnaparkhi, A. 1996. A maximum entropy part-of-speech 
tagger. In EMNLP. 
Riezler, S., and Vasserman, A. 2004. Incremental feature 
selection and L1 regularization for relax maximum entro-
py modeling. In EMNLP.  
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, J., 
and Johnson, M. 2002. Parsing the wall street journal using 
a lexical-functional grammar and discriminative estima-
tion techniques. In ACL. 271-278.  
Tibshirani, R. 1996. Regression shrinkage and selection via 
the lasso. J. R. Statist. Soc. B, 58(1): 267-288. 
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. 
2003. Feature-rich Part-of-Speech tagging with a cyclic 
dependency network. In HLT-NAACL, 252-259. 
Zhao, P. and B. Yu. 2004. Boosted lasso. Tech Report, Statistics 
Department, U. C. Berkeley. 
 Accuracy (%) # features # train iter 
MEMM/L2 96.39 926,350 467 
MEMM/L1 96.41 84,070 85 
Table 6. Performance summary of estimators on 
POS tagging 
831
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 465?472,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Hybrid Markov/Semi-Markov Conditional Random Field
for Sequence Segmentation
Galen Andrew
Microsoft Research
One Microsoft Way
Redmond, WA 98052
galena@microsoft.com
Abstract
Markov order-1 conditional random fields
(CRFs) and semi-Markov CRFs are two
popular models for sequence segmenta-
tion and labeling. Both models have ad-
vantages in terms of the type of features
they most naturally represent. We pro-
pose a hybrid model that is capable of rep-
resenting both types of features, and de-
scribe efficient algorithms for its training
and inference. We demonstrate that our
hybrid model achieves error reductions of
18% and 25% over a standard order-1 CRF
and a semi-Markov CRF (resp.) on the
task of Chinese word segmentation. We
also propose the use of a powerful fea-
ture for the semi-Markov CRF: the log
conditional odds that a given token se-
quence constitutes a chunk according to
a generative model, which reduces error
by an additional 13%. Our best system
achieves 96.8% F-measure, the highest re-
ported score on this test set.
1 Introduction
The problem of segmenting sequence data into
chunks arises in many natural language applica-
tions, such as named-entity recognition, shallow
parsing, and word segmentation in East Asian lan-
guages. Two popular discriminative models that
have been proposed for these tasks are the condi-
tional random field (CRFs) (Lafferty et al, 2001)
and the semi-Markov conditional random field
(semi-CRF) (Sarawagi and Cohen, 2004).
A CRF in its basic form is a model for label-
ing tokens in a sequence; however it can easily
be adapted to perform segmentation via labeling
each token as BEGIN or CONTINUATION, or accord-
ing to some similar scheme. CRFs using this tech-
nique have been shown to be very successful at the
task of Chinese word segmentation (CWS), start-
ing with the model of Peng et al (2004). In the
Second International Chinese Word Segmentation
Bakeoff (Emerson, 2005), two of the highest scor-
ing systems in the closed track competition were
based on a CRF model. (Tseng et al, 2005; Asa-
hara et al, 2005)
While the CRF is quite effective compared with
other models designed for CWS, one wonders
whether it may be limited by its restrictive inde-
pendence assumptions on non-adjacent labels: an
order-M CRF satisfies the order-M Markov as-
sumption that, globally conditioned on the input
sequence, each label is independent of all other
labels given the M labels to its left and right.
Consequently, the model only ?sees? word bound-
aries within a moving window of M + 1 charac-
ters, which prohibits it from explicitly modeling
the tendency of strings longer than that window
to form words, or from modeling the lengths of
the words. Although the window can in principle
be widened by increasing M , this is not a practi-
cal solution as the complexity of training and de-
coding a linear sequence CRF grows exponentially
with the Markov order.
The semi-CRF is a sequence model that is de-
signed to address this difficulty via careful relax-
ation of the Markov assumption. Rather than re-
casting the segmentation problem as a labeling
problem, the semi-CRF directly models the dis-
tribution of chunk boundaries.1 In terms of inde-
1As it was originally described, the semi-CRF also as-
signs labels to each chunk, effectively performing joint seg-
mentation and labeling, but in a pure segmentation problem
such as CWS, the use of labels is unnecessary.
465
pendence, using an order-M semi-CRF entails the
assumption that, globally conditioned on the input
sequence, the position of each chunk boundary is
independent of all other boundaries given the po-
sitions of the M boundaries to its left and right
regardless of how far away they are. Even with an
order-1 model, this enables several classes of fea-
tures that one would expect to be of great utility
to the word segmentation task, in particular word
length and word identity.
Despite this, the only work of which we are
aware exploring the use of a semi-Markov CRF
for Chinese word segmentation did not find signif-
icant gains over the standard CRF (Liang, 2005).
This is surprising, not only because the additional
features a semi-CRF enables are intuitively very
useful, but because as we will show, an order-M
semi-CRF is strictly more powerful than an or-
der-M CRF, in the sense that any feature that can
be used in the latter can also be used in the for-
mer, or equivalently, the semi-CRF makes strictly
weaker independence assumptions. Given a judi-
cious choice of features (or simply enough training
data) the semi-CRF should be superior.
We propose that the reason for this discrepancy
may be that despite the greater representational
power of the semi-CRF, there are some valuable
features that are more naturally expressed in a
CRF segmentation model, and so they are not typ-
ically included in semi-CRFs (indeed, they have
not to date been used in any semi-CRF model for
any task, to our knowledge). In this paper, we
show that semi-CRFs are strictly more expressive,
and also demonstrate how CRF-type features can
be used in a semi-CRF model for Chinese word
segmentation. Our experiments show that a model
incorporating both types of features can outper-
form models using only one or the other type.
Orthogonally, we explore in this paper the use
of a very powerful feature for the semi-CRF de-
rived from a generative model.
It is common in statistical NLP to use as fea-
tures in a discriminative model the (logarithm of
the) estimated probability of some event accord-
ing to a generative model. For example, Collins
(2000) uses a discriminative classifier for choosing
among the top N parse trees output by a generative
baseline model, and uses the log-probability of a
parse according to the baseline model as a feature
in the reranker. Similarly, the machine translation
system of Och and Ney uses log-probabilities of
phrasal translations and other events as features in
a log-linear model (Och and Ney, 2002; Och and
Ney, 2004). There are many reasons for incorpo-
rating these types of features, including the desire
to combine the higher accuracy of a discriminative
model with the simple parameter estimation and
inference of a generative one, and also the fact that
generative models are more robust in data sparse
scenarios (Ng and Jordan, 2001).
For word segmentation, one might want to use
as a local feature the log-probability that a segment
is a word, given the character sequence it spans. A
curious property of this feature is that it induces
a counterintuitive asymmetry between the is-word
and is-not-word cases: the component generative
model can effectively dictate that a certain chunk
is not a word, by assigning it a very low probability
(driving the feature value to negative infinity), but
it cannot dictate that a chunk is a word, because
the log-probability is bounded above.2 If instead
the log conditional odds log Pi(y|x)Pi(?y|x) is used, the
asymmetry disappears. We show that such a log-
odds feature provides much greater benefit than
the log-probability, and that it is useful to include
such a feature even when the model also includes
indicator function features for every word in the
training corpus.
2 Hybrid Markov/Semi-Markov CRF
The model we describe is formally a type of semi-
Markov CRF, distinguished only in that it also in-
volves CRF-style features. So we first describe the
semi-Markov model in its general form.
2.1 Semi-Markov CRF
An (unlabeled) semi-Markov conditional random
field is a log-linear model defining the conditional
probability of a segmentation given an observation
sequence. The general form of a log-linear model
is as follows: given an input x ? X , an output
y ? Y , a feature mapping ? : X ? Y 7? Rn, and
a weight vector w, the conditional probability of
y given x is estimated as:
P (y | x) =
exp (w ? ?(x,y))
Z(x)
where Z : x 7? R is a normalizing factor. w
is typically chosen to maximize the conditional
likelihood of a labeled training set. In the word
2We assume the weight assigned to the log-probability
feature is positive.
466
segmentation task, x is an ordered sequence of
characters (x1, x2, . . . , xn), and y is a set of in-
dices corresponding to the start of each word:
{y1, y2, . . . , ym} such that y1 = 1, ym ? n, and
for all j, yj < yj+1. A log-linear model in this
space is an order-1 semi-CRF if its feature map ?
decomposes according to
?(x,y) =
m?
j=1
?S(yj , yj+1,x) (1)
where ?S is a local feature map that only considers
one chunk at a time (defining ym+1 = n+1). This
decomposition is responsible for the characteristic
independence assumptions of the semi-CRF.
Hand-in-hand with the feature decomposition
and independence assumptions comes the capac-
ity for exact decoding using the Viterbi algorithm,
and exact computation of the objective gradient
using the forward-backward algorithm, both in
time quadratic in the lengths of the sentences.
Furthermore, if the model is constrained to pro-
pose only chunkings with maximum word length
k, then the time for inference and training be-
comes linear in the sentence length (and in k). For
Chinese word segmentation, choosing a moderate
value of k does not pose any significant risk, since
the vast majority of Chinese words are only a few
characters long: in our training set, 91% of word
tokens were one or two characters, and 99% were
five characters or less.
Using a semi-CRF as opposed to a traditional
Markov CRF allows us to model some aspects
of word segmentation that one would expect to
be very informative. In particular, it makes pos-
sible the use of local indicator function features
of the type ?the chunk consists of character se-
quence ?1, . . . , ?`,? or ?the chunk is of length `.?
It also enables ?pseudo-bigram language model?
features, firing when a given word occurs in the
context of a given character unigram or bigram.3
And crucially, although it is slightly less natural
to do so, any feature used in an order-1 Markov
CRF can also be represented in a semi-CRF. As
Markov CRFs are used in the most competitive
Chinese word segmentation models to date, one
might expect that incorporating both types of fea-
tures could yield a superior model.
3We did not experiment with this type of feature.
2.2 CRF vs. Semi-CRF
In order to compare the two types of linear CRFs,
it is convenient to define a representation of the
segmentation problem in terms of character labels
as opposed to sets of whole words. Denote by
L(y) ? {B,C}n (for BEGIN vs. CONTINUATION)
the sequence {L1, L2, . . . Ln} of labels such that
Li = B if and only if yi ? y. It is clear that if we
constrain L1 = B, the two representations y and
L(y) are equivalent. An order-1 Markov CRF is a
log-linear model in which the global feature vector
? decomposes into a sum over local feature vec-
tors that consider bigrams of the label sequence:
?(x,y) =
n?
i=1
?M (Li, Li+1, i,x) (2)
(where Ln+1 is defined as B). The local features
that are most naturally expressed in this context
are indicators of some joint event of the label bi-
gram (Li, Li+1) and nearby characters in x. For
example, one might use the feature ?the current
character xi is ? and Li = C?, or ?the current and
next characters are identical and Li = Li+1 = B.?
Although we have heretofore disparaged the
CRF as being incapable of representing such pow-
erful features as word identity, the type of features
that it most naturally represents should be help-
ful in CWS for generalizing to unseen words. For
example, the first feature mentioned above could
be valuable to rule out certain word boundaries if
? were a character that typically occurs only as a
suffix but that combines freely with a variety of
root forms to create new words. This type of fea-
ture (specifically, a feature indicating the absence
as opposed to the presence of a chunk boundary)
is a bit less natural in a semi-CRF, since in that
case local features ?S(yj , yj+1,x) are defined on
pairs of adjacent boundaries. Information about
which tokens are not on boundaries is only im-
plicit, making it a bit more difficult to incorporate
that information into the features. Indeed, neither
Liang (2005) nor Sarawagi and Cohen (2004) nor
any other system using a semi-Markov CRF on
any task has included this type of feature to our
knowledge. We hypothesize (and our experiments
confirm) that the lack of this feature explains the
failure of the semi-CRF to outperform the CRF for
word segmentation in the past.
Before showing how CRF-type features can be
used in a semi-CRF, we first demonstrate that the
semi-CRF is indeed strictly more expressive than
467
the CRF, meaning that any global feature map ?
that decomposes according to (2) also decomposes
according to (1). It is sufficient to show that for
any feature map ?M of a Markov CRF, there exists
a semi-Markov-type feature map ?S such that for
any x,y,
?M (x,y) =
n?
i=1
?M (Li, Li+1, i,x) (3)
=
m?
j=1
?S(yj , yj+1,x) = ?S(x,y)
To this end, note that there are only four possible
label bigrams: BB, BC, CB, and CC. As a di-
rect result of the definition of L(y), we have that
(Li, Li+1) = (B,B) if and only if some word of
length one begins at i, or equivalently, there exists
a word j such that yj = i and yj+1?yj = 1. Sim-
ilarly, (Li, Li+1) = (B,C) if and only if some
word of length > 1 begins at i, etc. Using these
conditions, we can define ?S to satisfy equation 3
as follows:
?S(yj , yj+1,x) = ?M (B,B, yj ,x)
if yj+1 ? yj = 1, and
?S(yj , yj+1,x) = ?M (B,C, yj ,x)
+
yj+1?2?
k=yj+1
?M (C,C, k,x) (4)
+ ?M (C,B, yj+1 ? 1,x)
otherwise. Defined thus,
?m
j=1 ?
S will contain ex-
actly n ?M terms, corresponding to the n label bi-
grams.4
2.3 Order-1 Markov Features in a Semi-CRF
While it is fairly intuitive that any feature used in a
1-CRF can also be used in a semi-CRF, the above
argument reveals an algorithmic difficulty that is
likely another reason that such features are not typ-
ically used. The problem is essentially an effect of
the sum for CC label bigrams in (4): quadratic
time training and decoding assumes that the fea-
tures of each chunk ?S(yj , yj+1,x) can be multi-
plied with the weight vector w in a number of op-
erations that is roughly constant over all chunks,
4We have discussed the case of Markov order-1, but the
argument can be generalized to show that an order-M CRF
has an equivalent representation as an order-M semi-CRF,
for any M .
procedure ComputeScores(x,w)
for i = 2 . . . (n? 1) do
?CCi ? ?
M (C,C, i,x) ?w
end for
for a = 1 . . . n do
CCsum? 0
for b = (a+ 1) . . . (n + 1) do
if b? a = 1 then
?ab ? ?M (B,B, a,x) ?w
else
?ab ? ?M (B,C, a,x) ?w + CCsum
+?M (C,B, b? 1,x) ?w
CCsum? CCsum+ ?CCb?1
end if
end for
end for
Figure 1: Dynamic program for computing chunk
scores ?ab with 1-CRF-type features.
but if one na??vely distributes the product over the
sum, longer chunks will take proportionally longer
to score, resulting in cubic time algorithms.5
In fact, it is possible to use these features
without any asymptotic decrease in efficiency by
means of a dynamic program. Both Viterbi and
forward-backward involve the scores ?ab = w ?
?S(a, b,x). Suppose that before starting those al-
gorithms, we compute and cache the score ?ab of
each chunk, so that remainder the algorithm runs
in quadratic time, as usual. This pre-computation
can be done quickly if we first compute the values
?CCi = w ? ?
M (C,C, i,x), and use them to fill in
the values of ?ab as shown in Figure 1.
In addition, computing the gradient of the semi-
CRF objective requires that we compute the ex-
pected value of each feature. For CRF-type fea-
tures, this is tantamount to being able to compute
the probability that each label bigram (Li, Li+1)
takes any value. Assume that we have already run
standard forward-backward inference so that we
have for any (a, b) the probability that the subse-
quence (xa,xa+1, . . . ,xb?1) segments as a chunk,
P (chunk(a, b)). Computing the probability that
(Li, Li+1) takes the values BB, BC or CB is
simple to compute:
P (Li, Li+1 = BB) = P (chunk(i, i+ 1))
5Note that the problem would arise even if only zero-order
Markov (label unigram) features were used, only in that case
the troublesome features would be those that involved the la-
bel unigram C.
468
and, e.g.,
P (Li, Li+1 = BC) =
?
j>i+1
P (chunk(i, j)),
but the same method of summing over chunks can-
not be used for the value CC since for each label
bigram there are quadratically many chunks cor-
responding to that value. In this case, the solution
is deceptively simple: using the fact that for any
given label bigram, the sum of the probabilities of
the four labels must be one, we can deduce that
P (Li, Li+1 = CC) = 1.0? P (Li, Li+1 = BB)
? P (Li, Li+1 = BC)? P (Li, Li+1 = CB).
One might object that features of the C and CC
labels (the ones presenting algorithmic difficulty)
are unnecessary, since under certain conditions,
their removal would not in fact change the expres-
sivity of the model or the distribution that maxi-
mizes training likelihood. This will indeed be the
case when the following conditions are fulfilled:
1. All label bigram features are of the form
?M (Li,Li+1, i,x) =
1{(Li, Li+1) = ? & pred(i,x)}
for some label bigram ? and predicate pred,
and any such feature with a given predicate
has variants for all four label bigrams ?.
2. No regularization is used during training.
A proof of this claim would require too much
space for this paper, but the key is that, given a
model satisfying the above conditions, one can
obtain an equivalent model via adding, for each
feature type over pred, some constant to the four
weights corresponding to the four label bigrams,
such that the CC bigram features all have weight
zero.
In practice, however, one or both of these con-
ditions is always broken. It is common knowl-
edge that regularization of log-linear models with
a large number of features is necessary to achieve
high performance, and typically in NLP one de-
fines feature templates and chooses only those fea-
tures that occur in some positive example in the
training set. In fact, if both of these conditions are
fulfilled, it is very likely that the optimal model
will have some weights with infinite values. We
conclude that it is not a practical alternative to omit
the C and CC label features.
2.4 Generative Features in a Discriminative
Model
When using the output of a generative model as
a feature in a discriminative model, Raina et al
(2004) provide a justification for the use of log
conditional odds as opposed to log-probability:
they show that using log conditional odds as fea-
tures in a logistic regression model is equivalent
to discriminatively training weights for the fea-
tures of a Na??ve Bayes classifier to maximize
conditional likelihood.6 They demonstrate that
the resulting classifier, termed a ?hybrid genera-
tive/discriminative classifier?, achieves lower test
error than either pure Na??ve Bayes or pure logistic
regression on a text classification task, regardless
of training set size.
The hybrid generative/discriminative classifier
also uses a unique method for using the same data
used to estimate the parameters of the compo-
nent generative models for training the discrimina-
tive model parameters w without introducing bias.
A ?leave-one-out? strategy is used to choose w,
whereby the feature values of the i-th training ex-
ample are computed using probabilities estimated
with the i-th example held out. The beauty of this
approach is that since the probabilities are esti-
mated according to (smoothed) relative frequency,
it is only necessary during feature computation to
maintain sufficient statistics and adjust them as
necessary for each example.
In this paper, we experiment with the use of
a single ?hybrid? local semi-CRF feature, the
smoothed log conditional odds that a given sub-
sequence xab = (xa, . . . ,xb?1) forms a word:
log
wordcount(xab) + 1
nonwordcount(xab) + 1
,
where wordcount(xab) is the number of times
xab forms a word in the training set, and
nonwordcount(xab) is the number of times xab
occurs, not segmented into a single word. The
models we test are not strictly speaking hybrid
generative/discriminative models, since we also
use indicator features not derived from a genera-
tive model. We did however use the leave-one-out
approach for computing the log conditional odds
feature during training.
6In fact, one more step beyond what is shown in that paper
is required to reach the stated conclusion, since their features
are not actually log conditional odds, but log P (x|y)P (x|?y) . It is
simple to show that in the given context this feature is equiv-
alent to log conditional odds.
469
3 Experiments
To test the ideas discussed in this paper, we com-
pared the performance of semi-CRFs using vari-
ous feature sets on a Chinese word segmentation
task. The data used was the Microsoft Research
Beijing corpus from the Second International
Chinese Word Segmentation Bakeoff (Emerson,
2005), and we used the same train/test split used in
the competition. The training set consists of 87K
sentences of Beijing dialect Chinese, hand seg-
mented into 2.37M words. The test set contains
107K words comprising roughly 4K sentences.
We used a maximum word length k of 15 in our
experiments, which accounted for 99.99% of the
word tokens in our training set. The 249 train-
ing sentences that contained words longer than 15
characters were discarded. We did not discard any
test sentences.
In order to be directly comparable to the Bake-
off results, we also worked under the very strict
?closed test? conditions of the Bakeoff, which re-
quire that no information or data outside of the
training set be used, not even prior knowledge of
which characters represent Arabic numerals, Latin
characters or punctuation marks.
3.1 Features Used
We divide our main features into two types accord-
ing to whether they are most naturally used in a
CRF or a semi-CRF.
The CRF-type features are indicator functions
that fire when the character label (or label bigram)
takes some value and some predicate of the input
at a certain position relative to the label is satis-
fied. For each character label unigram L at posi-
tion i, we use the same set of predicate templates
checking:
? The identity of xi?1 and xi
? The identity of the character bigram starting
at positions i? 2, i? 1 and i
? Whether xj and xj+1 are identical, for j =
(i? 2) . . . i
? Whether xj and xj+2 are identical, for j =
(i? 3) . . . i
? Whether the sequence xj . . .xj+3 forms an
AABB sequence for j = (i? 4) . . . i
? Whether the sequence xj . . .xj+3 forms an
ABAB sequence for j = (i? 4) . . . i
The latter four feature templates are designed to
detect character or word reduplication, a morpho-
logical phenomenon that can influence word seg-
mentation in Chinese. The first two of these were
also used by Tseng et al (2005).
For label bigrams (Li, Li+1), we use the same
templates, but extending the range of positions
by one to the right.7 Each label uni- or bigram
also has a ?prior? feature that always fires for
that label configuration. All configurations con-
tain the above features for the label unigram B,
since these are easily used in either a CRF or semi-
CRF model. To determine the influence of CRF-
type features on performance, we also test config-
urations in which both B and C label features are
used, and configurations using all label uni- and
bigrams.
In the semi-Markov conditions, we also use as
feature templates indicators of the length of a word
`, for ` = 1 . . . k, and indicators of the identity of
the corresponding character sequence.
All feature templates were instantiated with val-
ues that occur in positive training examples. We
found that excluding CRF-type features that occur
only once in the training set consistently improved
performance on the development set, so we use a
count threshold of two for the experiments. We do
not do any thresholding of the semi-CRF features,
however.
Finally, we use the single generative feature,
log conditional odds that the given string forms
a word. We also present results using the more
typical log conditional probability instead of the
odds, for comparison. In fact, these are both semi-
Markov-type features, but we single them out to
determine what they contribute over and above the
other semi-Markov features.
3.2 Results
The results of test set runs are summarized in ta-
ble 3.2. The columns indicate which CRF-type
features were used: features of only the label B,
features of label unigrams B and C, or features
of all label unigrams and bigrams. The rows indi-
cate which semi-Markov-type features were used:
7For both label unigram and label bigram features, the in-
dices are chosen so that the feature set exhibits no asymmetry
with respect to direction: for each feature considering some
boundary and some property of the character(s) at a given
offset to the left, there is a corresponding feature considering
that boundary and the same property of the character(s) at the
same offset to the right, and vice-versa.
470
Features B only uni uni+bi
none 92.33 94.71 95.69
semi 95.28 96.05 96.46
prob 93.86 95.40 96.04
semi+prob 95.51 96.24 96.55
odds 95.10 96.06 96.40
semi+odds 96.27 96.77 96.84
Table 1: Test F-measure for different model con-
figurations.
?semi? means length and word identity features
were used, ?prob? means the log-probability fea-
ture was used, and ?odds? means the log-odds fea-
ture was used.
To establish the impact of each type of feature
(C label unigrams, label bigrams, semi-CRF-type
features, and the log-odds feature), we look at the
reduction in error brought about by adding each
type of feature. First consider the effect of the
CRF-type features. Adding the C label features
reduces error by 31% if no semi-CRF features are
used, by 16% when semi-CRF indicator features
are turned on, and by 13% when all semi-CRF fea-
tures (including log-odds) are used. Using all label
bigrams reduces error by 44%, 25%, and 15% in
these three conditions, respectively.
Contrary to previous conclusions, our results
show a significant impact due to the use of semi-
CRF-type features, when CRF-type features are
held constant. Adding semi-CRF indicator fea-
tures results in a 38% error reduction without
CRF-type features, and 18% with them. Adding
semi-CRF indicator features plus the log-odds fea-
ture gives 52% and 27% in these two conditions,
respectively.
Finally, across configurations, the log condi-
tional odds does much better than log condi-
tional probability. When the log-odds feature is
added to the complete CRF model (uni+bi) as
the only semi-CRF-type feature, errors are re-
duced by 24%, compared to only 7.6% for the log-
probability. Even when the other semi-CRF-type
features are present as well, log-odds reduces error
by 13% compared to 2.5% for log-probability.
Our best model, combining all features, resulted
in an error reduction of 12% over the highest score
on this dataset from the 2005 Sighan closed test
competition (96.4%), achieved by the pure CRF
system of Tseng et al (2005).
3.3 Discussion
Our results indicate that both Markov-type and
semi-Markov-type features are useful for generali-
zation to unseen data. This may be because the
two types of features are in a sense complemen-
tary: semi-Markov-type features such as word-
identity are valuable for modeling the tendency
of known strings to segment as words, while la-
bel based features are valuable for modeling prop-
erties of sub-lexical components such as affixes,
helping to generalize to words that have not previ-
ously been encountered. We did not explicitly test
the utility of CRF-type features for improving re-
call on out-of-vocabulary items, but we note that
in the Bakeoff, the model of Tseng et al (2005),
which was very similar to our CRF-only system
(only containing a few more feature templates),
was consistently among the best performing sys-
tems in terms of test OOV recall (Emerson, 2005).
We also found that for this sequence segmenta-
tion task, the use of log conditional odds as a fea-
ture results in much better performance than the
use of the more typical log conditional probabil-
ity. It would be interesting to see the log-odds
applied in more contexts where log-probabilities
are typically used as features. We have presented
the intuitive argument that the log-odds may be
advantageous because it does not exhibit the 0-1
asymmetry of the log-probability, but it would be
satisfying to justify the choice on more theoretical
grounds.
4 Relation to Previous Work
There is a significant volume of work explor-
ing the use of CRFs for a variety of chunking
tasks, including named-entity recognition, gene
prediction, shallow parsing and others (Finkel et
al., 2005; Culotta et al, 2005; Sha and Pereira,
2003). The current work indicates that these sys-
tems might be improved by moving to a semi-CRF
model.
There have not been a large number of studies
using the semi-CRF, but the few that have been
done found only marginal improvements over pure
CRF systems (Sarawagi and Cohen, 2004; Liang,
2005; Daume? III and Marcu, 2005). Notably,
none of those studies experimented with features
of chunk non-boundaries, as is achieved by the use
of CRF-type features involving the label C, and
we take this to be the reason for their not obtain-
ing higher results.
471
Although it has become fairly common in NLP
to use the log conditional probabilities of events
as features in a discriminative model, we are not
aware of any work using the log conditional odds.
5 Conclusion
We have shown that order-1 semi-Markov condi-
tional random fields are strictly more expressive
than order-1 Markov CRFs, and that the added
expressivity enables the use of features that lead
to improvements on a segmentation task. On the
other hand, Markov CRFs can more naturally in-
corporate certain features that may be useful for
modeling sub-chunk phenomena and generaliza-
tion to unseen chunks. To achieve the best per-
formance for segmentation, we propose that both
types of features be used, and we show how this
can be done efficiently.
Additionally, we have shown that a log condi-
tional odds feature estimated from a generative
model can be superior to the more common log
conditional probability.
6 Acknowledgements
Many thanks to Kristina Toutanova for her
thoughtful discussion and feedback, and also to
the anonymous reviewers for their suggestions.
References
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takahashi Tsuzuki. 2005. Combination of machine
learning methods for optimum chinese word seg-
mentation. In Proc. Fourth SIGHAN Workshop on
Chinese Language Processing, pages 134?137.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proc. 14th Interna-
tional Conf. on Machine Learning.
Aron Culotta, David Kulp, and Andrew McCallum.
2005. Gene prediction with conditional random
fields. Technical report, University of Massa-
chusetts Dept. of Computer Science, April.
Hal Daume? III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In Proc. 19th In-
ternational Conf. on Machine Learning.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proc. Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 123?133.
Jenny Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. Proc. 41th Annual Meeting of the Assi-
ciation of Computation Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282?289. Morgan Kauf-
mann, San Francisco, CA.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, Massachusetts Insti-
tute of Technology.
Andrew Y. Ng and Michael I. Jordan. 2001. On dis-
criminative vs. generative classifiers: A comparison
of logistic regression and Na??ve Bayes. In Proc. Ad-
vances in Neural Information Processing 14.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. Proc. 38th Annual
Meeting of the Assiciation of Computation Linguis-
tics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449,
December.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proc. 20th
International Conf. on Computational Linguistics.
Rajat Raina, Yirong Shen, Andrew Y. Ng, and Andrew
McCallum. 2004. Classification with hybrid gen-
erative/discriminative models. In Proc. Advances in
Neural Information Processing 17.
Brian Roark and Seeger Fisher. 2005. OGI/OHSU
baseline multilingual multi-document sumarization
system. In Proc. Multilingual Summarization Eval-
uation in ACL Workshop: Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Sunita Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Proc. 18th International Conf. on Ma-
chine Learning.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. Proc. HLT-NAACL.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proc. Fourth SIGHAN Workshop on
Chinese Language Processing, pages 168?171.
472
 A Conditional Random Field Word Segmenter  
for Sighan Bakeoff 2005 
Huihsin Tseng 
Dept. of Linguistics 
University of Colorado 
Boulder, CO 80302 
tseng@colorado.edu
Pichuan Chang, Galen Andrew,  
Daniel Jurafsky, Christopher Manning 
Stanford Natural Language Processing Group 
Stanford University 
Stanford, CA 94309 
{pichuan, pupochik, jurafsky, manning}@stanford.edu 
Abstract
We present a Chinese word seg-
mentation system submitted to the 
closed track of Sighan bakeoff 2005. 
Our segmenter was built using a condi-
tional random field sequence model 
that provides a framework to use a 
large number of linguistic features such 
as character identity, morphological 
and character reduplication features. 
Because our morphological features 
were extracted from the training cor-
pora automatically, our system was not 
biased toward any particular variety of 
Mandarin. Thus, our system does not 
overfit the variety of Mandarin most 
familiar to the system's designers. Our 
final system achieved a F-score of 
0.947 (AS), 0.943 (HK), 0.950 (PK) 
and 0.964 (MSR). 
1 Introduction 
The 2005 Sighan Bakeoff included four dif-
ferent corpora, Academia Sinica (AS), City 
University of Hong Kong (HK), Peking Univer-
sity (PK), and Microsoft Research Asia (MSR), 
each of which has its own definition of a word. 
In the 2003 Sighan Bakeoff (Sproat & Emer-
son 2003), no single model performed well on 
all corpora included in the task. Rather, systems 
tended to do well on corpora largely drawn from 
a set of similar Mandarin varieties to the one 
they were originally developed for. Across cor-
pora, variation is seen in both the lexicons and 
also in the word segmentation standards. We 
concluded that, for future systems, generaliza-
tion across such different Mandarin varieties is 
crucial. To this end, we proposed a new model 
using character identity, morphological and 
character reduplication features in a conditional 
random field modeling framework. 
2 Algorithm
Our system builds on research into condi-
tional random field (CRF), a statistical sequence 
modeling framework first introduced by Lafferty 
et al (2001). Work by Peng et al (2004) first 
used this framework for Chinese word segmen-
tation by treating it as a binary decision task, 
such that each character is labeled either as the 
beginning of a word or the continuation of one. 
Gaussian priors were used to prevent overfitting 
and a quasi-Newton method was used for pa-
rameter optimization.  
The probability assigned to a label sequence 
for a particular sequence of characters by a CRF 
is given by the equation below: 
( ) ( )??
?
??
?
= ??
?Cc k
c cXYkkXZ
XYP f ,,exp)(
1| ??
Y is the label sequence for the sentence, X is 
the sequence of unsegmented characters, Z(X) is 
a normalization term, fk is a feature function, and 
c indexes into characters in the sequence being 
labeled.
A CRF allows us to utilize a large number of 
n-gram features and different state sequence 
168
based features and also provides an intuitive 
framework for the use of morphological features.  
3 Feature engineering 
3.1 Features
The linguistic features used in our model fall 
into three categories: character identity n-grams,
morphological and character reduplication fea-
tures.
For each state, the character identity features 
(Ng & Low 2004, Xue & Shen 2003, Goh et al 
2003) are represented using feature functions 
that key off of the identity of the character in the 
current, proceeding and subsequent positions. 
Specifically, we used four types of unigram fea-
ture functions, designated as C0 (current charac-
ter), C1 (next character), C-1 (previous character), 
C-2 (the character two characters back). Fur-
thermore, four types of bi-gram features were 
used, and are notationally designated here as 
conjunctions of the previously specified unigram 
features, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0.
Given that unknown words are normally 
more than one character long, when representing 
the morphological features as feature functions, 
such feature functions keyed off the morpho-
logical information extracted from both the pro-
ceeding state and the current state. Our morpho-
logical features are based upon the intuition re-
garding unknown word features given in Gao et 
al. (2004). Specifically, their idea was to use 
productive affixes and characters that only oc-
curred independently to predict boundaries of 
unknown words. To construct a table containing 
affixes of unknown words, rather than using 
threshold-filtered affix tables in a separate un-
known word model as was done in Gao et al 
(2004), we first extracted rare words from a cor-
pus and then collected the first and last charac-
ters to construct the prefix and suffix tables. For 
the table of individual character words, we col-
lected an individual character word table for 
each corpus of the characters that always oc-
curred alone as a separate word in the given cor-
pus. We also collected a list of bi-grams from 
each training corpus to distinguish known 
strings from unknown. Adopting all the features 
together in a model and using the automatically 
generated morphological tables prevented our 
system from manually overfitting the Mandarin 
varieties we are most familiar with.  
The tables are used in the following ways: 
1) C-1+C0 unknown word feature functions 
were created for each specific pair of characters 
in the bi-gram tables. Such feature functions are 
active if the characters in the respective states 
match the corresponding feature function?s 
characters. These feature functions are designed 
to distinguish known strings from unknown.  
2) C-1, C0, and C1 individual character feature 
functions were created for each character in the 
individual character word table, and are likewise 
active if the respective character matches the 
feature function?s character. 
3) C-1 prefix feature functions are defined 
over characters in the prefix table, and fire if the 
character in the proceeding state matches the 
feature function?s character. 
4) C0 suffix feature functions are defined 
over suffix table characters, and fire if the char-
acter in the current state matches the feature 
function?s character. 
Additionally, we also use reduplication fea-
ture functions that are active based on the repeti-
tion of a given character. We used two such fea-
ture functions, one that fires if the previous and 
the current character, C-1 and C0, are identical 
and one that does so if the subsequent and the 
previous characters, C-1 and C1, are identical.  
Most features appeared in the first-order tem-
plates with a few of character identity features in 
the both zero-order and first-order templates. 
We also did normalization of punctuations due 
to the fact that Mandarin has a huge variety of 
punctuations.  
Table 1 shows the number of data features 
and lambda weights in each corpus.  
Table 1 The number of features in each corpus 
# of data features # of lambda weights 
AS 2,558,840 8,076,916
HK 2,308,067 7,481,164
PK 1,659,654 5,377,146
MSR 3,634,585 12,468,890
3.2 Experiments 
3.2.1 Results on Sighan bakeoff 2003 
Experiments done while developing this sys-
tem showed that its performance was signifi-
cantly better than that of Peng et al (2004).  
As seen in Table 2, our system?s F-score was 
0.863 on CTB (Chinese Treebank from Univer-
169
sity of Pennsylvania) versus 0.849 F on Peng et 
al. (2004). We do not at present have a good 
understanding of which aspects of our system 
give it superior performance. 
Table 2 Comparisons of Peng et al (2004) and our F-
score on the closed track in Sighan bakeoff 2003 
Sighan  
Bakeoff 2003 
Our F-score F-score 
Peng et al (2004) 
CTB 0.863 0.849 
AS 0.970 0.956 
HK 0.947 0.928 
PK 0.953 0.941 
3.2.2 Results on Sighan bakeoff 2005 
Our final system achieved a F-score of 0.947 
(AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR). 
This shows that our system successfully general-
ized and achieved state of the art performance 
on all four corpora. 
Table 3 Performance of the features cumulatively, 
starting with the n-gram.  
F-score AS HK PK MSR
n-gram 0.943 0.946 0.950 0.961
n-gram (PU fixed)  0.953   
+Unk&redupl 0.947 0.943 0.950 0.964
+Unk&redupl 
(PU fixed) 
 0.952   
Table 3 lists our results on the four corpora. 
We give our results using just character identity 
based features; character identity features plus 
unknown words and reduplication features. Our 
unknown word features only helped on AS and 
MSR. Both of these corpora have words that 
have more characters than HK and PK. This in-
dicates that our unknown word features were 
more useful for corpora with segmentation stan-
dards that tend to result in longer words. 
In the HK corpus, when we added in un-
known word features, our performance dropped. 
However, we found that the testing data uses 
different punctuation than the training set. Our 
system could not distinguish new word charac-
ters from new punctuation, since having a com-
plete punctuation list is considered external 
knowledge for closed track systems. If the new 
punctuation were not unknown to us, our per-
formance on HK data would have gone up to 
0.952 F and the unknown word features would 
have not hurt the system too much. 
Table 4 present recalls (R), precisions (P), f-
scores (F) and recalls on both unknown (Roov)
and known words (Riv).
Table 4 Detailed performances of each corpus 
R P F Roov Riv
AS 0.950 0.943 0.947? 0.718? 0.960
HK 0.941 0.946 0.943? 0.698? 0.961
HK
(PU-fix)
0.952 0.952 0.952 0.791 0.965
PK 0.946 0.954 0.950? 0.787? 0.956
MSR 0.962 0.966 0.964? 0.717? 0.968
3.3 Error analysis 
Our system performed reasonably well on 
morphologically complex new words, such as 
??? (CABLE in AS) and ??? (MUR-
DER CASE in PK), where ? (LINE) and ?
(CASE) are suffixes. However, it over-
generalized to words with frequent suffixes such 
as ?? (it should be ? ? ?to burn some-
one? in PK) and ?? (it should be? ? ?
?to look backward? in PK). For the corpora that 
considered 4 character idioms as a word, our 
system combined most of new idioms together. 
This differs greatly from the results that one 
would likely obtain with a more traditional 
MaxMatch based technique, as such an algo-
rithm would segment novel idioms. 
One short coming of our system is that it is 
not robust enough to distinguish the difference 
between ordinal numbers and numbers with 
measure nouns. For example, ?? (3rd year) 
and ?? (three years) are not distinguishable 
to our system. In order to avoid this problem, it 
might require having more syntactic knowledge 
than was implicitly given in the training data.  
Finally, some errors are due to inconsisten-
cies in the gold segmentation of non-hanzi char-
acter. For example, ?Pentium4? is a word, but 
?PC133? is two words. Sometimes, ?8? is a 
word, but sometimes it is segmented into two 
words.
170
4 Conclusion
Our system used a conditional random field 
sequence model in conjunction with character 
identity features, morphological features and 
character reduplication features. We extracted 
our morphological information automatically to 
prevent overfitting Mandarin from particular 
Mandarin-speaking area. Our final system 
achieved a F-score of 0.947 (AS), 0.943 (HK), 
0.950 (PK) and 0.964 (MSR).  
5 Acknowledgment 
Thanks to Kristina Toutanova for her gener-
ous help and to Jenny Rose Finkel who devel-
oped such a great conditional random field 
package. This work was funded by the Ad-
vanced Research and Development Activity's 
Advanced Question Answering for Intelligence 
Program, National Science Foundation award 
IIS-0325646 and a Stanford Graduate Fellow-
ship.
References
Lafferty, John, A. McCallum, and F. Pereira. 2001. 
Conditional Random Field: Probabilistic Models 
for Segmenting and Labeling Sequence Data. In 
ICML 18. 
Gao, Jianfeng Andi Wu, Mu Li, Chang-Ning Huang, 
Hongqiao Li, Xinsong Xia and Haowei Qin. 2004. 
Adaptive Chinese word segmentation. In ACL-
2004.
Goh, Chooi-Ling, Masayuki Asahara, Yuji Matsu-
moto. 2003. Chinese unknown word identification 
using character-based tagging and chunking. In 
ACL 2003 Interactive Poster/Demo Sessions. 
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once? 
Word-Based or Character-Based? In EMNLP 9.
Peng, Fuchun, Fangfang Feng and Andrew 
McCallum. 2004. Chinese segmentation and new 
word detection using conditional random fields. In 
COLING 2004.
Sproat, Richard and Tom Emerson. 2003. The first 
international Chinese word segmentation bakeoff. 
In SIGHAN 2. 
Xue, Nianwen and Libin Shen. 2003. Chinese Word 
Segmentation as LMR Tagging. In SIGHAN 2.
171
Verb Sense and Subcategorization:
Using Joint Inference to Improve Performance on Complementary Tasks
Galen Andrew, Trond Grenager, and Christopher Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040
{pupochik, grenager, manning}@cs.stanford.edu
Abstract
We propose a general model for joint inference in corre-
lated natural language processing tasks when fully anno-
tated training data is not available, and apply this model
to the dual tasks of word sense disambiguation and verb
subcategorization frame determination. The model uses
the EM algorithm to simultaneously complete partially
annotated training sets and learn a generative probabilis-
tic model over multiple annotations. When applied to the
word sense and verb subcategorization frame determina-
tion tasks, the model learns sharp joint probability dis-
tributions which correspond to linguistic intuitions about
the correlations of the variables. Use of the joint model
leads to error reductions over competitive independent
models on these tasks.
1 Introduction
Natural language processing research has tradition-
ally been divided into a number of separate tasks,
each of which is believed to be an important sub-
task of the larger language comprehension or gener-
ation problem. These tasks are usually addressed
separately, with systems designed to solve a sin-
gle problem. However, many of these tasks are not
truly independent; if solutions to one were known
they would facilitate finding solutions to the others.
For some sets of these problems, one would like to
be able to do joint inference, where information of
one kind can influence decisions about information
of another kind and vice versa. For instance, in-
formation about named entities can usefully inform
the decisions of a part-of-speech tagger, but equally,
part-of-speech information can help a named entity
recognizer. If one had a large corpus annotated with
all the information types of interest, one could es-
timate a joint distribution over all of the variables
simply by counting. However, it is more often the
case that one lacks any jointly annotated corpus,
or at least one that is sufficiently large, given that
the joint distribution is necessarily sparser than the
marginal distributions. It would therefore be useful
to be able to build a model for this joint inference
task using only partially supervised data. In this
System Name Accuracy
kunlp 57.6
jhu-english-JHU-final 56.6
SMUls 56.3
LIA-Sinequa-Lexsample 53.5
manning-cs224n 52.3
Table 1: Performance of the top 5 Senseval-2 word sense
disambiguation systems when considering accuracy only
on the 29 verbs. Systems not guessing on all instances
have been omitted.
paper we examine these problems in the context of
joint inference over verb senses and their subcate-
gorization frames (SCFs).
1.1 Verb Sense and Subcategorization
Of the syntactic categories tested in the Senseval
word sense disambiguation (WSD) competitions,
verbs have proven empirically to be the most dif-
ficult. In Senseval-1, Kilgarriff and Rosenzweig
(2000) found a 10-point difference between the
best systems? performance on verbs compared with
other parts-of-speech. In Senseval-2, Yarowsky and
Florian (2002) also found that while accuracies of
around 73% were possible for adjectives and nouns,
even the most competitive systems have accuracies
of around 57% when tested on verbs (see Table 1).
A likely explanation for this discrepancy is that dif-
ferent senses of common verbs can occur in sim-
ilar lexical contexts, thereby decreasing the effec-
tiveness of ?bag-of-words? models.
Verbs also pose serious challenges in a very dif-
ferent task: syntactic parsing. Verb phrases are syn-
tactically complex and frought with pitfalls for auto-
mated parsers, such as prepositional phrase attach-
ment ambiguities. These challenges may be par-
tially mitigated by the fact that particular verbs often
have strong preferences for particular SCFs. Unfor-
tunately, it is not the case that each verb consistently
takes the same SCF. More often, a verb has several
preferred SCFs, with rarer forms also occurring, for
example, in idioms. Jurafsky (1998) proposes us-
? NP PP NPPP VPto VPing
2:30:00 4 1 0 0 20 33
2:30:01 1 7 0 4 0 0
2:42:04 12 0 3 0 0 1
Table 2: The learned joint distribution over the senses
and subcategorizations of the verb begin (in percent
probability). Low probability senses and subcategoriza-
tions have been omitted.
ing a probabilistic framework to represent subcate-
gorization preferences, where each lexical item has
a corresponding distribution over the possible sets
of arguments. Modeling these distributions may be
useful: Collins (2003) has shown that verb subcate-
gorization information can be used to improve syn-
tactic parsing performance.
It has also been recognized that a much more ac-
curate prediction of verb subcategorization prefer-
ence can be made if conditioned on the sense of
the verb. Roland and Jurafsky (2002) conclude that
for a given lexical token in English, verb sense is
the best determiner of SCF, far outweighing either
genre or dialect. Demonstrating the utility of this,
Korhonen and Preiss (2003) achieve significant im-
provement at a verb subcategorization acquisition
task by conditioning on the verb sense as predicted
by a statistical word sense disambiguation system.
Conversely, if different senses have distinct subcat-
egorization preferences, it is reasonable to expect
that information about the way a verb subcatego-
rizes in a particular case may be of significant util-
ity in determining the verb?s sense. As an example,
Yarowsky (2000) makes use of rich syntactic fea-
tures to improve the performance of a supervised
WSD system.
As an illustration of this correlation, Table 2
shows a learned joint distribution over sense and
SCF for the common verb begin.1 Its common
senses, taken from WordNet, are as follows: sense
2:30:00, to initiate an action or activity, (?begin
working?), sense 2:30:01, to set in motion or cause
to start, (?to begin a war?), and sense 2:42:04, to
have a beginning, (?the day began?). The SCFs
shown here are a subset of the complete set of SCFs,
described in Table 3. Note that the sense and SCF
variables are highly correlated for this verb. Sense
2:30:00 occurs almost entirely with verb phrase ar-
guments, sense 2:30:01 occurs almost entirely as a
transitive verb, and sense 2:42:04 occurs as an in-
transitive verb (no arguments following the verb).
It should be evident that the strong correlation be-
1We cannot show an empirical joint distribution because of
the lack of a sufficiently large jointly annotated corpus, as dis-
cussed below.
tween these two variables can be exploited to in-
crease performance in the tasks of predicting their
values in either direction, even when the evidence is
weak or uncertain.
1.2 Learning a Joint Model
Performing joint inference requires learning a joint
distribution over sense and SCF for each verb. In
order to estimate the joint distribution directly from
data we would need a large corpus that is annotated
for both verb sense and SCF. Unfortunately, no such
corpus of adequate size exists.2 Instead, there are
some corpora such as SemCor and Senseval-2 la-
beled for sense, and others that are parsed and from
which it is possible to compute verb SCFs determin-
istically. In the current work we use two corpora to
learn a joint model: Senseval-2, labeled for sense
but not syntax, and the Penn Treebank, labeled for
syntax but not sense. We do so by treating the two
data sets as a single one with incompletely labeled
instances. This partially labeled data set then yields
a semi-supervised learning problem, suitable for the
Expectation-Maximization (EM) algorithm (Demp-
ster et al, 1977).
2 Tasks and Data Sets
We evaluate our system on both the WSD task and
the verb SCF determination task. We describe each
task in turn.
2.1 Word Sense Disambiguation
We used as our sense-annotated corpus the data
sets from the English lexical sample portion of the
Senseval-2 word sense disambiguation competition
(Kilgarriff and Rosenzweig, 2000). This data set
contains multiple instances of 73 different English
word types, divided into training and testing exam-
ples. Each word type is marked for part of speech,
so that the sense disambiguation task does not need
to distinguish between senses that have different
parts of speech. We selected from this data set al
29 words that were marked as verbs.
Each example consists of a marked occurrence of
the target word in approximately 100 words of sur-
rounding context. The correct sense of the word,
marked by human annotators, is also given. Each
instance is labeled with a sense corresponding to a
synset from WordNet (Miller, 1995). The number
of senses per word varies enormously: some words
have more than 30 senses, while others have five
2A portion of the Brown corpus has been used both in the
construction of the SemCor word sense database and in the con-
struction of the Penn Treebank, but coverage is very low, espe-
cially for sense markings, and the individual sentences have not
to our knowledge been explicitly aligned.
or fewer. These ?fine-grained? senses are also par-
titioned into a smaller number of ?coarse-grained?
senses, and systems are evaluated according to both
metrics. The number of training and testing exam-
ples per word varies from tens to nearly a thousand.
We used the same train/test division as in Senseval-
2, so that our reported accuracy numbers are directly
comparable with those of other Senseval-2 submis-
sions, as given in Table 1.
2.2 Verb Subcategorization
We use as our SCF-annotated corpus sentences
drawn from the Wall Street Journal section of the
Penn Treebank. For each target verb we select sen-
tences containing a form of the verb (tagged as a
verb) with length less than 40 words. We select
training examples from sections 2 through 21, and
test examples from all other sections.3
There are many conceivable ways to partition
the set of possible verb argument combinations into
SCFs. One possible approach would be to use as the
SCF representation the raw sequence of constituents
occurring in the verb phrase. This is certainly an
unbiased representation, but as there are many thou-
sands of rewrites for VP in the Penn Treebank, data
sparsity would present a significant problem. In ad-
dition, many of the variants do not contain useful
information for our task: for example, we wouldn?t
expect to get much value from knowing about the
presence or absence of an adverb in the phrase. In-
stead, we chose to use a small number of linguis-
tically motivated SCFs which form a partition over
the large space of possible verb arguments.
We chose as a starting point the SCF partition
specified in Roland (2001). These SCFs are defined
declaratively using a set of tgrep expressions that
match appropriate verb phrases.4 We made signifi-
cant modifications to the set of SCFs, and also sim-
plified the tgrep expressions used to match them.
One difference from Roland?s SCF set is that we
analyze verb particles as arguments, so that several
SCFs differ only in the existence of a particle. This
is motivated by the fact that the particle is a syntactic
feature that provides strong evidence about the verb
sense. One might argue that the presence of a par-
ticle should be considered a lexical feature modeled
independently from the SCF, but the distinction is
blurry, and we have instead combined the variables
in favor of model simplicity. A second difference is
3Sections 2 through 21 of the WSJ are typically used for
training PCFG parsers, and section 23 is typically used for test-
ing. Because of sparse data we drew our test examples from all
non-training sections.
4tgrep is a tree node matching program written by Richard
Pito, distributed with the Penn Treebank.
Subcat Description
? No arguments
NP Transitive
PP Prepositional phrase
NP PP Trans. with prep. phrase
VPing Gerundive verb phrase
NP VPing Perceptual complement
VPto Intrans. w/ infinitival VP
NP VPto Trans. w/ infinitival VP
S for to Intrans. w/ for PP and infin. VP
NP SBAR Trans. w/ finite clause
NP NP Ditransitive
PRT Particle and no args.
NP PRT Transitive w/ particle
PP PRT Intrans. w/ PP and particle
VP PRT Intrans. w/ VP and particle
SBAR PRT Intrans. w/ fin. clause and part.
Other None of the above
Table 3: The 17 subcategorization frames we use.
that unlike Roland, we do not put passive verb con-
structions in a separate ?passive? SCF, but instead
we undo the passivization and put them in the un-
derlying category. Although one might expect that
passivization itself is a weak indicator of sense, we
believe that the underlying SCF is more useful. Our
final set of SCFs is shown in Table 3.
Given a sentence annotated with a syntactic
parse, the SCF of the target verb can be computed by
attempting to match each of the SCF-specific tgrep
expressions with the verb phrase containing the tar-
get verb. Unlike those given by Roland, our tgrep
expressions are not designed to be mutually exclu-
sive; instead we determine verb SCF by attempting
matches in a prescribed sequence, using ?if-then-
else? logic.
3 Model Structure and Inference
Our generative probabilistic model can be thought
of as having three primary components: the sense
model, relating the verb sense to the surrounding
context, the subcategorization model, relating the
verb subcategorization to the sentence, and the joint
model, relating the sense and SCF of the verb to
each other. More formally, the model is a factored
representation of a joint distribution over these vari-
ables and the data: the verb sense (V ), the verb SCF
(C), the unordered context ?bag-of-words? (W ),
and the sentence as an ordered sequence of words
(S). The joint distribution P(V, C, W, S) is then
factored as
P(V )P(C|V )P(S|C)
?
i
P(Wi |V )
W S
V C
n
Figure 1: A graphical representation of the combined
sense and subcategorization probabilistic model. Note
that the box defines a plate, indicating that the model
contains n copies of this variable.
where Wi is the word type occurring in each po-
sition of the context (including the target sentence
itself). The first two terms together define a joint
distribution over verb sense (V ) and SCF (C), the
third term defines the subcategorization model, and
the last term defines the sense model. A graphical
model representation is shown in Figure 1.
The model assumes the following generative pro-
cess for a data instance of a particular verb. First we
generate the sense of the target verb. Conditioned
on the sense, we generate the SCF of the verb. (Note
that the decision to generate sense and then SCF
is arbitrary and forced by the desire to factor the
model; we discuss reversing the order below.) Then,
conditioned on the sense of the verb, we generate
an unordered collection of context words. (For the
Senseval-2 corpus, this collection includes not only
the words in the sentence in which the verb occurs,
but also the words in surrounding sentences.) Fi-
nally, conditioned on the SCF of the verb, we gen-
erate the immediate sentence containing the verb as
an ordered sequence of words.
An apparent weakness of this model is that it
double-generates the context words from the en-
closing sentence: they are generated once by the
sense model, as an unordered collection of words,
and once by the subcategorization model, as an or-
dered sequence of words. The model is thus defi-
cient in that it assigns a large portion of its probabil-
ity mass to impossible cases: those instances which
have words in the context which do not match those
in the sentence. However because the sentences are
always observed, we only consider instances in the
set of consistent cases, so the deficiency should be
irrelevant for the purpose of reasoning about sense
and SCF.
We discuss each of the model components in turn.
3.1 Verb Sense Model
The verb sense component of our model is an or-
dinary multinomial Naive Bayes ?bag-of-words?
model: P(V )
?
i P(Wi |V ). We learn the marginal
over verb sense with maximum likelihood estima-
tion (MLE) from the sense annotated data. We learn
the sense-conditional word model using smoothed
MLE from the sense annotated data, and to smooth
we use Bayesian smoothing with a Dirichlet prior.
The free smoothing parameter is determined empir-
ically, once for all words in the data set. In the inde-
pendent sense model, to infer the most likely sense
given a context of words P(S|W), we just find the V
that maximizes P(V )
?
i P(Wi |V ). Inference in thejoint model over sense and SCF is more complex,
and is described below.
In order to make our system competitive with
leading WSD systems we made an important modi-
fication to this basic model: we added relative posi-
tion feature weighting. It is known that words closer
to the target word are more predictive of sense, so it
is reasonable to weight them more highly. We de-
fine a set of ?buckets?, or partition over the position
of the context word relative to the target verb, and
we weight each context word feature with a weight
given by its bucket, both when estimating model pa-
rameters at train time and when performing infer-
ence at test time. We use the following 8 relative
position buckets: (??,?6], [?5,?3], ?2, ?1, 1,
2, [3, 5], and [6,?). The bucket weights are found
empirically using a simple optimization procedure
on k-fold training set accuracy. In ablation tests on
this system we found that the use of relative posi-
tion feature weighting, when combined with corre-
sponding evidence attenuation (see Section 3.3) in-
creased the accuracy of the standalone verb sense
disambiguation model from 46.2% to 54.0%.
3.2 Verb Subcategorization Model
The verb SCF component of our model P(S|C)
represents the probability of particular sentences
given each possible SCF. Because there are in-
finitely many possible sentences, a multinomial rep-
resentation is infeasible, and we instead chose to
encode the distribution using a set of probabilistic
context free grammars (PCFGs). A PCFG is created
for each possible SCF: each PCFG yields only parse
trees in which the distinguished verb subcategorizes
in the specified manner (but other verbs can parse
freely). Given a SCF-specific PCFG, we can deter-
mine the probability of the sentence using the inside
algorithm, which sums the probabilities of all pos-
sible trees in the grammar producing the sentence.
To do this, we modified the exact PCFG parser of
Klein and Manning (2003). In the independent SCF
model, to infer the most likely SCF given a sen-
tence P(C|S), we just find the C that maximizes
P(S|C)P(C). (For the independent model, the SCF
prior is estimated using MLE from the training ex-
amples.) Inference in the joint model over sense and
SCF is more complex, and is described below.
Learning this model, SCF-specific PCFGs, from
our SCF-annotated training data, requires some
care. Commonly PCFGs are learned using MLE
of rewrite rule probabilities from large sets of tree-
annotated sentences. Thus to learn SCF-specific
PCFGs, it seems that we should select a set of an-
notated sentences containing the target verb, deter-
mine the SCF of the target verb in each sentence,
create a separate corpus for each SCF of the target
verb, and then learn SCF-specific grammars from
the SCF-specific corpora. If we are careful to dis-
tinguish rules which dominate the target verb from
those which do not, then the grammar will be con-
strained to generate trees in which the target verb
subcategorizes in the specified manner, and other
verbs can occur in general tree structures. The prob-
lem with this approach is that in order to create a
broad-coverage grammar (which we will need in or-
der for it to generalize accurately to unseen test in-
stances) we will need a very large number of sen-
tences in which the target verb occurs, and we do
not have enough data for this approach.
Because we want to maximize the use of the
available data, we must instead make use of every
verb occurrence when learning SCF-specific rewrite
rules. We can accomplish this by making a copy
of each sentence for each verb occurrence (not just
the target verb), determining the SCF of the distin-
guished verb in each sentence, partitioning the sen-
tence copies by distinguished verb SCF, and learn-
ing SCF-specific grammars using MLE. Finally, we
change the lexicon by forcing the distinguished verb
tag to rewrite to only our target verb. The method
we actually use is functionally equivalent to this lat-
ter approach, but altered for efficiency. Instead of
making copies of sentences with multiple verbs, we
use a dense representation. We determine the SCF
of each verb in the sentence, and then annotate the
verb and all nonterminal categories occurring above
the verb in the tree, up to the root, with the SCF
of the verb. Note that some nonterminals will then
have multiple annotations. Then to learn a SCF-
specific PCFG, we count rules that have the speci-
fied SCF annotation as rules which can dominate the
distinguished verb, and then count all rules (includ-
ing the SCF-specific ones) as general rules which
cannot dominate the distinguished verb.
3.3 The Joint Model
Given a fully annotated dataset, it is trivial to learn
the parameters of the joint distribution over verb
sense and SCF P(V, C) using MLE. However, be-
cause we do not have access to such a dataset, we
instead use the EM algorithm to ?complete? the
missing annotations with expectations, or soft as-
signments, over the values of the missing variable
(we present the EM algorithm in detail in the next
section). Given this ?completed? data, it is again
trivial to learn the parameters of the joint proba-
bility model using smoothed MLE. We use simple
Laplace add-one smoothing to smooth the distribu-
tion.
However, a small complication arises from the
fact that the marginal distributions over senses and
SCFs for a particular verb may differ between the
two halves of our data set. They are, after all, wholly
different corpora, assembled by different people for
different purposes. For this reason, when testing
the system on the sense corpus we?d like to use a
sense marginal distribution trained from the sense
corpus, and when testing the system on the SCF
corpus we?d like to use a SCF marginal distribu-
tion trained from the SCF corpus. To address this,
recall from above that the factoring we choose for
the joint distribution is arbitrary. When performing
sense inference we use the model Pv(V )P j(C|V )
where P j(C|V ) was learned from the complete data,
and Pv(V ) was learned from the sense-marked ex-
amples only. When performing SCF inference we
use the equivalent factoring Pc(C)P j(V |C), where
P j(V |C) was learned from the complete data, and
Pc(C) was learned from the SCF-annotated exam-
ples only.
We made one additional modification to this joint
model to improve performance. When performing
inference in the model, we found it useful to dif-
ferentially weight different probability terms. The
most obvious need for this comes from the fact
that the sense-conditional word model employs rel-
ative position feature weighting, which can change
the relatively magnitude of the probabilities in this
term. In particular, by using feature weights greater
than 1.0 during inference we overestimate the ac-
tual amount of evidence. Even without the feature
weighting, however, the word model can still over-
estimate the actual evidence given that it encodes an
incorrect independence assumption between word
features (of course word occurrence in text is ac-
tually very highly correlated). The PCFG model
also suffers from a less severe instance of the same
problem: human languages are of course not con-
text free, and there is in fact correlation between
supposedly independent tree structures in different
parts of the tree. To remedy this evidence over-
confidence, it is helpful to attenuate or downweight
the evidence terms accordingly. More generally, we
place weights on each of the probability terms used
in inference calculations, yielding models of the fol-
lowing form:
P(V )?(v)P(C|V )?(c)P(S|C)?(s)[
?
i
P(Wi |V )]?(w)
These ?(?) weights are free parameters, and we
find them by simple optimization on k-fold accu-
racy. In ablation tests on this system, we found
that term weighting (particularly evidence attenua-
tion) increased the accuracy of the standalone sense
model from 51.9% to 54.0% at the fine-grained verb
sense disambiguation task.
We now describe the precise EM algorithm used.
Prior to running EM we first learn the independent
sense and SCF model parameters from their respec-
tive datasets. We also initialize the joint sense and
SCF distribution to the uniform distribution. Then
we iterate over the following steps:
? E-step: Using the current model parameters,
for each datum in the sense-annotated corpus,
compute expectations over the possible SCFs,
and for each datum in the SCF-annotated cor-
pus, compute expectations over the possible
senses.
? M-step: use the completed data to reestimate
the joint distribution over sense and SCF.
We run EM to convergence, which for our dataset
occurs within 6 iterations. Additional iterations do
not change the accuracy of our model. Early stop-
ping of EM after 3 iterations was found to hurt k-
fold sense accuracy by 0.1% and SCF accuracy by
0.2%. Early stopping of EM after only 1 iteration
was found to hurt k-fold sense accuracy by a total of
0.2% and SCF accuracy by 0.4%. These may seem
like small differences, but significant relative to the
advantages given by the joint model (see below).
In the E-step of EM, it is necessary to do infer-
ence over the joint model, computing posterior ex-
pectations of unknown variables conditioned on ev-
idence variables. During the testing phase, it is also
necessary to do inference, computing maximum a
posteriori (MAP) values of unknown variables con-
ditioned on evidence variables. In all cases we do
exact Bayesian network inference, which involves
conditioning on evidence variables, summing over
extraneous variables, and then either maximizing
over the resulting factors of query variables, or nor-
malizing them to obtain distributions of query vari-
ables. At test time, when querying about the MAP
sense (or SCF) of an instance, we chose to max-
imize over the marginal distribution, rather than
maximize over the joint sense and SCF distribution.
We found empirically that this gave us higher accu-
racy at the individual tasks. If instead we were do-
ing joint prediction, we would expect high accuracy
to result from maximizing over the joint.
4 Results and Discussion
In Figures 2, 3 and 4 we compare the perfor-
mance of the independent and joint models on the
verb sense disambiguation and verb SCF determina-
tion problems, evaluated using both 10-fold cross-
validation accuracy and test set accuracy. In Figure
2, we report the performance of a system resulting
from doing optimization of free parameters (such as
feature and term weights) on a per-verb basis. We
also provide a baseline computed by guessing the
most likely class.
Although the parameter optimization of Figure
2 was performed with respect to 10-fold cross-
validation on the training sets, its lower perfor-
mance on the test sets suggests that it suffers from
overfitting. To test this hypothesis we also trained
and tested on the test sets a version of the system
with corpus-wide free parameter optimization, and
the results of this test are shown in Figure 3. The
lower gap between the training set cross-validation
and test set performance on the WSD task confirms
our overfitting hypothesis. However, note that the
gap between training set cross-validation and test
set performance on the SCF determination task per-
sists (although it is diminished slightly). We believe
that this results from the fact that there is significant
data drift between the training sections of the WSJ
in the Penn Treebank (sections 2 through 21) and all
other sections.
Using corpus-wide optimization, the joint model
improves sense disambiguation accuracy by 1.9%
over the independent model, bringing our system
to 55.9% accuracy on the test set, performance that
is comparable with that of the state of the art sys-
tems on verbs given in Table 1. The joint model re-
duces sense disambiguation error by 4.1%. On the
verb SCF determination task, the joint model yields
a 2.1% improvement in accuracy over the indepen-
dent model, reducing total error by 5.1%.
We also report results of the independent and
joint systems on each verb individually in Table 4
Not surprisingly, making use of the joint distribution
was much more helpful for some verbs than others.
40.5
38.8
59.9
70.8
60.2
72.8
54.7
59.7
55.6
61.4
35
40
45
50
55
60
65
70
75
80
Sense Subcat
Te
st
 
Ac
cu
ra
cy
Baseline
Individual
10-fold
Joint 10-fold
Individual
test
Joint test
Figure 2: Chart comparing results of independent and
joint systems on the verb sense and SCF tasks, evaluated
with 10-fold cross-validation on the training sets and on
the test sets. The baseline shown is guessing most likely
class. These systems used per-verb optimization of free
parameters.
40.5
38.8
52.4
68.5
54.7
69.8
54.0
59.3
55.9
61.4
35
40
45
50
55
60
65
70
75
80
Sense Subcat
Te
st
 
Ac
cu
ra
cy
Baseline
Individual
10-fold
Joint 10-fold
Individual
test
Joint test
Figure 3: Chart comparing results of independent and
joint systems on the verb sense and SCF tasks. These
systems used corpus-wide optimization of free parame-
ters.
40.5
38.8
45.0
68.5
49.2
69.1
46.2
59.3
50.7
59.6
35
40
45
50
55
60
65
70
75
80
Sense Subcat
Te
st
 
Ac
cu
ra
cy
Baseline
Individual
10-fold
Joint 10-fold
Individual
test
Joint test
Figure 4: Chart comparing results of independent and
joint systems on the verb sense and SCF tasks. This sys-
tem has no relative position word feature weighting and
no term weighting.
Indep Joint Indep Joint
Verb Sense Sense Subcat Subcat
begin 76.8 84.3 57.0 63.3
call 39.4 42.4 44.9 49.0
carry 45.5 40.9 63.3 70.0
collaborate 90.0 90.0 100.0 100.0
develop 42.0 39.1 69.7 69.7
draw 29.3 26.8 72.7 63.6
dress 59.3 59.3 NA NA
drift 43.8 40.6 50.0 50.0
drive 45.2 52.4 54.5 54.5
face 81.7 80.6 82.4 82.4
ferret 100.0 100.0 NA NA
find 23.5 29.4 61.1 64.8
keep 46.3 58.2 52.1 53.5
leave 47.0 54.5 36.4 40.0
live 62.7 65.7 85.7 85.7
match 57.1 54.8 58.3 66.7
play 42.4 45.5 66.7 61.9
pull 28.3 26.7 44.4 55.6
replace 57.8 62.2 56.0 60.0
see 40.6 39.1 53.6 55.1
serve 60.8 52.9 72.0 72.0
strike 37.0 27.8 50.0 50.0
train 55.6 55.6 40.0 40.0
treat 52.3 54.5 69.2 76.9
turn 29.9 29.9 46.3 50.0
use 65.8 68.4 69.7 68.8
wander 78.0 80.0 NA NA
wash 50.0 41.7 0.0 0.0
work 41.7 43.3 67.9 66.1
Table 4: Comparison of the performance of the indepen-
dent and joint inference models on the verb sense and
SCF tasks,evaluated on the Senseval-2 test set, for each
of the 29 verbs in the study. These results were obtained
with no per-verb parameter optimization. Note the great
variation in problem difficulty and joint model perfor-
mance across verbs.
For example, on the verbs begin, drive, find, keep,
leave, and work, the joint model gives a greater than
5% accuracy boost on the WSD task. In contrast, for
some other verbs, the joint model showed a slight
decrease in accuracy on the test set relative to the
independent model.
We present a few representative examples where
the joint model makes better decisions than the in-
dividual model. In the sentence
. . . prices began weakening last month after
Campeau hit a cash crunch.
the sense model (based on bag-of-words evidence)
believes that the sense 2:42:04 is most likely (see
Table 2 for senses and joint distribution). How-
ever, the SCF model gives high weight to the frames
VPto and VPing, which when combined with the
joint distribution, give much more probability to
the sense 2:30:00. The joint model thus correctly
chooses sense 2:30:00. In the sentence
. . . before beginning a depressing eight-year
slide that continued through last year.
the sense model again believes that the sense
2:42:04 is most likely. However, the SCF model
correctly gives high weight to the NP frame, which
when combined with the joint distribution, gives
much more probability to the sense 2:30:01. The
joint model thus correctly chooses sense 2:30:01.
Given the amount of information contained in the
joint distribution it is surprising that the joint model
doesn?t yield a greater advantage over the indepen-
dent models. It seems to be the case that the word
sense model is able to capture much of the SCF in-
formation by itself, without using an explicit syn-
tactic model. This results from the relative posi-
tion weighting, since many of our SCFs correlate
highly with the presence of small sets of words in
particular positions (for instance, the infinitival ?to?,
prepositions, and pronouns). We tested this hypoth-
esis by examining how the addition of SCF informa-
tion affected performance of a weaker sense model,
obtained by removing feature and term weighting.
The results are shown in Figure 4. Indeed, when us-
ing this weaker word sense model, the joint model
yields a much larger 4.5% improvement in WSD ac-
curacy.
5 Future Work
We can imagine several modifications to the ba-
sic system that might improve performance. Most
importantly, more specific use could be made of
SCF information besides modeling its joint distribu-
tion with sense, for example conditioning on head-
words of (perceived) arguments, especially parti-
cles and prepositions. Second, although we made
some attempt at extracting the ?underlying? SCF of
verbs by analyzing passive constructions separately,
similar analysis of other types of movement such
as relative clauses may also be useful. Third, we
could hope to get some improvement from changing
our model structure to address the issue of double-
generation of words discussed in section 3. One way
this could be done would be to use a parser only
to estimate the probability of the sequence of word
tags (i.e., parts of speech) in the sentence, then to
use a sense-specific lexicon to estimate the proba-
bility of finding the words under the tags.
Although we chose WSD and SCF determination
as a test case, the approach of this paper is appli-
cable to other pairs of tasks. It may also be pos-
sible to improve parsing accuracy on verb phrases
or other phrases, by simultaneously resolving word
sense ambiguities, as attempted unsuccessfully by
Bikel (2000). This work is intended to introduce
a general methodology for combining disjoint NLP
tasks that is of use outside of these specific tasks.
6 Acknowledgements
This paper is based on work supported in part by
the Advanced Research and Development Activity
(ARDA)?s Advanced Question Answering for Intel-
ligence (AQUAINT) Program, and by the National
Science Foundation under Grant No. IIS-0085896,
as part of the Knowledge Discovery and Dissemina-
tion program. We additionally thank the reviewers
for their insightful comments.
References
Daniel M. Bikel. 2000. A statistical model for parsing
and word-sense disambiguation. Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. To appear in Computa-
tional Linguistics.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society, Se-
ries B (Methodological), 39(1):1?38.
Daniel Jurafsky. 1998. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science, 20(2):139?194.
Adam Kilgarriff and Joseph Rosenzweig. 2000. Frame-
work and results for english senseval. Computers and
the Humanities, 34(1-2):15?48.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics.
Anna Korhonen and Judita Preiss. 2003. Improving
subcategorization acquisition using word sense disam-
biguation. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics, Sap-
poro, Japan, pages 48?55.
G.A. Miller. 1995. Wordnet: A lexical database for en-
glish. Communications of the ACM, 38(11):39?41.
Douglas Roland and Daniel Jurafsky. 2002. Verb sense
and verb subcategorization probabilities. In Paola
Merlo and Suzanne Stevenson, editors, The Lexical
Basis of Sentence Processing, chapter 16. John Ben-
jamins, Amsterdam.
Douglas Roland. 2001. Verb Sense and Verb Subcate-
gorization Probabilities. Ph.D. thesis, University of
Colorado.
David Yarowsky and Radu Florian. 2002. Evaluating
sense disambiguation across diverse parameter spaces.
Natural Language Engineering, 8(4):293?310.
David Yarowsky. 2000. Hierarchical decision lists for
word sense disambiguation. Computers and the Hu-
manities, 34(1-2).
