Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 43?47,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Tight Integration of Speech Disfluency Removal into SMT
Eunah Cho Jan Niehues
Interactive Systems Lab
Institute of Anthropomatics
Karlsruhe Institute of Technology, Germany
{eunah.cho,jan.niehues,alex.waibel}@kit.edu
Alex Waibel
Abstract
Speech disfluencies are one of the main
challenges of spoken language processing.
Conventional disfluency detection systems
deploy a hard decision, which can have
a negative influence on subsequent appli-
cations such as machine translation. In
this paper we suggest a novel approach
in which disfluency detection is integrated
into the translation process.
We train a CRF model to obtain a disflu-
ency probability for each word. The SMT
decoder will then skip the potentially dis-
fluent word based on its disfluency prob-
ability. Using the suggested scheme, the
translation score of both the manual tran-
script and ASR output is improved by
around 0.35 BLEU points compared to the
CRF hard decision system.
1 Introduction
Disfluencies arise due to the spontaneous nature
of speech. There has been a great deal of effort to
detect disfluent words, remove them (Johnson and
Charniak, 2004; Fitzgerald et al., 2009) and use
the cleaned text for subsequent applications such
as machine translation (MT) (Wang et al., 2010;
Cho et al., 2013).
One potential drawback of conventional ap-
proaches is that the decision whether a token is
a disfluency or not is a hard decision. For an
MT system, this can pose a severe problem if the
removed token was not in fact a disfluency and
should have been kept for the correct translation.
Therefore, we pass the decision whether a word is
part of a disfluency or not on to the translation sys-
tem, so that we can use the additional knowledge
available in the translation system to make a more
reliable decision. In order to limit the complexity,
the search space is pruned prior to decoding and
represented in a word lattice.
2 Related Work
Disfluencies in spontaneous speech have been
studied from various points of view. In the noisy
channel model (Honal and Schultz, 2003), it is
assumed that clean text without any disfluencies
has passed through a noisy channel. The clean
string is retrieved based on language model (LM)
scores and five additional models. Another noisy
channel approach involves a phrase-level statisti-
cal MT system, where noisy tokens are translated
into clean tokens (Maskey et al., 2006). A tree ad-
joining grammar is combined with this noisy chan-
nel model in (Johnson and Charniak, 2004), using
a syntactic parser to build an LM.
Fitzgerald et al. (2009) present a method to de-
tect speech disfluencies using a conditional ran-
dom field (CRF) with lexical, LM, and parser
information features. While previous work has
been limited to the postprocessing step of the au-
tomatic speech recogition (ASR) system, further
approaches (Wang et al., 2010; Cho et al., 2013)
use extended CRF features or additional models
to clean manual speech transcripts and use them
as input for an MT system.
While ASR systems use lattices to encode hy-
potheses, lattices have been used for MT systems
with various purposes. Herrmann et al. (2013)
use lattices to encode different reordering variants.
Lattices have also been used as a segmentation tac-
tic for compound words (Dyer, 2009), where the
segmentation is encoded as input in the lattice.
One of the differences between our work and
previous work is that we integrate the disfluency
removal into an MT system. Our work is not lim-
ited to the preprocessing step of MT, instead we
use the translation model to detect and remove dis-
fluencies. Contrary to other systems where detec-
tion is limited on manual transcripts only, our sys-
43
tem shows translation performance improvements
on the ASR output as well.
3 Tight Integration using Lattices
In this chapter, we explain how the disfluency re-
moval is integrated into the MT process.
3.1 Model
The conventional translation of texts from sponta-
neous speech can be formulated as
e? = argmax
e
p(e| argmax
f
c
p(f
c
|f)) (1)
with
p(f
c
|f) =
I
?
i=1
p(c
i
|f
i
) (2)
where f
c
denotes the clean string
f
c
= {f
1
, . . . , f
I
| c
i
= clean} (3)
for the disfluency decision class c of each token.
c ?
{
clean
disfluent
(4)
Thus, using the conventional models, disfluency
removal is applied to the original, potentially noisy
string in order to obtain the cleaned string first.
This clean string is then translated.
The potential drawback of a conventional
speech translation system is caused by the rough
estimation in Equation 1, as disfluency removal
does not depend on maximizing the translation
quality itself. For example, we can consider the
sentence Use what you build, build what you use.
Due to its repetitive pattern in words and structure,
the first clause is often detected as a disfluency us-
ing automatic means. To avoid this, we can change
the scheme how the clean string is chosen as fol-
lows:
e? = argmax
e,f
c
(p(e|f
c
) ? p(f
c
|f)) (5)
This way a clean string which maximizes the
translation quality is chosen. Thus, no instant de-
cision is made whether a token is a disfluency or
not. Instead, the disfluency probability of the to-
ken will be passed on to the MT process, using
the log linear combination of the probabilities as
shown in Equation 5.
In this work, we use a CRF (Lafferty et al.,
2001) model to obtain the disfluency probability
of each token.
Since there are two possible classes for each to-
ken, the number of possible clean sentences is ex-
ponential with regard to the sentence length. Thus,
we restrict the search space by representing only
the most probable clean source sentences in a word
lattice.
3.2 CRF Model Training
In order to build the CRF model, we used the
open source toolkit CRF++ (Kudoh, 2007). As
unigram features, we use lexical and LM features
adopted from Fitzgerald et al. (2009), and addi-
tional semantics-based features discussed in (Cho
et al., 2013). In addition to the unigram features,
we also use a bigram feature to model first-order
dependencies between labels.
We train the CRF with four classes; FL for filler
words, RC for (rough) copy, NC for non-copy and
0 for clean tokens. The class FL includes obvious
filler words (e.g. uh, uhm) as well as other dis-
course markers (e.g. you know, well in English).
The RC class covers identical or roughly simi-
lar repetitions as well as lexically different words
with the same meaning. The NC class represents
the case where the speaker changes what to speak
about or reformulates the sentence and restarts the
speech fragments. The disfluency probability P
d
of each token is calculated as the sum of probabil-
ities of each class.
3.3 Lattice Implementation
We construct a word lattice which encodes long-
range reordering variants (Rottmann and Vogel,
2007; Niehues and Kolss, 2009). For translation
we extend this so that potentially disfluent words
can be skipped. A reordering lattice of the ex-
ample sentence Das sind die Vorteile, die sie uh
die sie haben. (En.gls: These are the advantages,
that you uh that you have.) is shown in Figure 1,
where words representing a disfluency are marked
in bold letters. In this sentence, the part die sie
uh was manually annotated as a disfluency, due to
repetition and usage of a filler word.
Table 1 shows the P
d
obtained from the CRF
model for each token. As expected, the words die
sie uh obtain a high P
d
from the CRF model.
In order to provide an option to avoid translating
a disfluent word, a new edge which skips the word
is introduced into the lattice when the word has a
higher P
d
than a threshold ?. During decoding the
importance of this newly introduced edge is opti-
mized by weights based on the disfluency proba-
44
0 1 das 2 sie 3 sie 4 sind 5 das 6 das 7 die 8 sind 9 sind 10 Vorteile 11 die 12 die 13 , 14 Vorteile 15 Vorteile 16 die 17 , 18 , 19 sie 20
 haben  die 21 die 22 uh 23
 sie 
24 sie 25 die 26
 uh 
27 uh 28 sie 29 haben 30
 die  die 31 haben  sie  sie 32 . 
Figure 1: Reordering lattice before adding alternative clean paths for an exemplary sentence
0 1 das 2 sie 3 sie 5 das 4
 sind  das 6 das 7
 die 
8 sind 9 sind 
10 Vorteile 
11 die 12 die 
13 , 
14 Vorteile 15 Vorteile 
16 die 19 haben 26 die 17 , 18 , 
 haben 20 sie  die  die  die 21 die 30 die 
22 sie 28 die 23 uh  die 
24 sie  die 
25 uh  die  die 
27 uh  die 
 die 
29 haben  sie  die 
31
 sie  sie  haben 32 . 
Figure 2: Extended lattice with alternative clean paths for an exemplary sentence
das 0.000732 sie 0.953126
sind 0.004445 uh 0.999579
die 0.013451 die 0.029010
Vorteile 0.008183 sie 0.001426
, 0.035408 haben 0.000108
die 0.651642 . 0.000033
Table 1: Disfluency probability of each word
bility and transition probability. The extended lat-
tice for the given sentence with ? = 0.5 is shown
in Figure 2, with alternative paths marked by a
dotted line. The optimal value of ? was manually
tuned on the development set.
4 System Description
The training data for our MT system consists of
1.76 million sentences of German-English paral-
lel data. Parallel TED talks
1
are used as in-domain
data and our translation models are adapted to the
domain. Before training, we apply preprocess-
ing such as text normalization, tokenization, and
smartcasing. Additionally, German compound
words are split.
To build the phrase table we use the Moses
package (Koehn et al., 2007). An LM is trained
on 462 million words in English using the SRILM
Toolkit (Stolcke, 2002). In order to extend source
word context, we use a bilingual LM (Niehues et
al., 2011). We use an in-house decoder (Vogel,
2003) with minimum error rate training (Venu-
gopal et al., 2005) for optimization.
For training and testing the CRF model, we use
61k annotated words of manual transcripts of uni-
1
http://www.ted.com
versity lectures in German. For tuning and testing
the MT system, the same data is used along with
its English reference translation. In order to make
the best use of the data, we split it into three parts
and perform three-fold cross validation. There-
fore, the train/development data consists of around
40k words, or 2k sentences, while the test data
consists of around 20k words, or 1k sentences.
5 Experiments
In order to compare the effect of the tight inte-
gration with other disfluency removal strategies,
we conduct different experiments on manual tran-
scripts as well as on the ASR output.
5.1 Manual Transcripts
As a baseline for manual transcripts, we use
the whole uncleaned data for development and
test. For ?No uh?, we remove the obvious filler
words uh and uhm manually. In the CRF-hard
experiment, the token is removed if the label
output of the CRF model is a disfluency class.
The fourth experiment uses the tight integration
scheme, where new source paths which jump over
the potentially noisy words are inserted based on
the disfluency probabilities assigned by the CRF
model. In the next experiments, this method is
combined with other aforementioned approaches.
First, we apply the tight integration scheme after
we remove all obvious filler words. In the next
experiment, we first remove all words whose P
d
is higher than 0.9 as early pruning and then apply
the tight integration scheme. In a final experiment,
we conduct an oracle experiment, where all words
annotated as a disfluency are removed.
45
5.2 ASR Output
The same experiments are applied to the ASR out-
put. Since the ASR output does not contain re-
liable punctuation marks, there is a mismatch be-
tween the training data of the CRF model, which is
manual transcripts with all punctuation marks, and
the test data. Thus, we insert punctuation marks
and augment sentence boundaries in the ASR out-
put using the monolingual translation system (Cho
et al., 2012). As the sentence boundaries differ
from the reference translation, we use the Leven-
shtein minimum edit distance algorithm (Matusov
et al., 2005) to align hypothesis for evaluation.
No optimization is conducted, but the scaling fac-
tors obtained when using the correponding setup
of manual transcripts are used for testing.
5.3 Results
Table 2 shows the results of our experiments. The
scores are reported in case-sensitive BLEU (Pap-
ineni et al., 2002).
System Dev Text ASR
Baseline 23.45 22.70 14.50
No uh 25.09 24.04 15.10
CRF-hard 25.32 24.50 15.15
Tight int. 25.30 24.59 15.19
No uh + Tight int. 25.41 24.68 15.33
Pruning + Tight int. 25.38 24.84 15.51
Oracle 25.57 24.87 -
Table 2: Translation results for the investigated
disfluency removal strategies
Compared to the baseline where all disfluen-
cies are kept, the translation quality is improved
by 1.34 BLEU points for manual transcripts by
simply removing all obvious filler words. When
we take the output of the CRF as a hard deci-
sion, the performance is further improved by 0.46
BLEU points. When using the tight integration
scheme, we improve the translation quality around
0.1 BLEU points compared to the CRF-hard deci-
sion. The performance is further improved by re-
moving uh and uhm before applying the tight inte-
gration scheme. Finally the best score is achieved
by using the early pruning coupled with the tight
integration scheme. The translation score is 0.34
BLEU points higher than the CRF-hard decision.
This score is only 0.03 BLEU points less than the
oracle case, without all disfluencies.
Experiments on the ASR output also showed a
considerable improvement despite word errors and
consequently decreased accuracy of the CRF de-
tection. Compared to using only the CRF-hard de-
cision, using the coupled approach improved the
performance by 0.36 BLEU points, which is 1.0
BLEU point higher than the baseline.
System Precision Recall
CRF-hard 0.898 0.544
Pruning + Tight int. 0.937 0.521
Table 3: Detection performance comparison
Table 3 shows a comparison of the disfluency
detection performance on word tokens. While re-
call is slightly worse for the coupled approach,
precision is improved by 4% over the hard deci-
sion, indicating that the tight integration scheme
decides more accurately. Since deletions made by
a hard decision can not be recovered and losing a
meaningful word on the source side can be very
critical, we believe that precision is more impor-
tant for this task. Consequently we retain more
words on the source side with the tight integration
scheme, but the numbers of word tokens on the
translated target side are similar. The translation
model is able to leave out unnecessary words dur-
ing translation.
6 Conclusion
We presented a novel scheme to integrate disflu-
ency removal into the MT process. Using this
scheme, it is possible to consider disfluency prob-
abilities during decoding and therefore to choose
words which can lead to better translation perfor-
mance. The disfluency probability of each token
is obtained from a CRF model, and is encoded in
the word lattice. Additional edges are added in the
word lattice, to bypass the words potentially rep-
resenting speech disfluencies.
We achieve the best performance using the tight
integration method coupled with early pruning.
This method yields an improvement of 2.1 BLEU
points for manual transcripts and 1.0 BLEU point
improvement over the baseline for ASR output.
Although the translation of ASR output is im-
proved using the suggested scheme, there is still
room to improve. In future work, we would like to
improve performance of disfluency detection for
ASR output by including acoustic features in the
model.
46
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
?
287658.
References
Eunah Cho, Jan Niehues, and Alex Waibel. 2012.
Segmentation and Punctuation Prediction in Speech
Language Translation using a Monolingual Trans-
lation System. In Proceedings of the Interna-
tional Workshop for Spoken Language Translation
(IWSLT), Hong Kong, China.
Eunah Cho, Thanh-Le Ha, and Alex Waibel. 2013.
CRF-based Disfluency Detection using Seman-
tic Features for German to English Spoken Lan-
guage Translation. In Proceedings of the Interna-
tional Workshop for Spoken Language Translation
(IWSLT), Heidelberg, Germany.
Chris Dyer. 2009. Using a Maximum Entropy Model
to Build Segmentation Lattices for MT. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, Boulder, Colorado, USA, June. Association for
Computational Linguistics.
Erin Fitzgerald, Kieth Hall, and Frederick Jelinek.
2009. Reconstructing False Start Errors in Sponta-
neous Speech Text. In Proceedings of the European
Association for Computational Linguistics (EACL),
Athens, Greece.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Matthias Honal and Tanja Schultz. 2003. Correction of
Disfluencies in Spontaneous Speech using a Noisy-
Channel Approach. In Eurospeech, Geneva.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based Noisy Channel Model of Speech Repairs. In
Proceedings of the Association for Computational
Linguistics (ACL).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the Association for Computational
Linguistics (ACL), Demonstration Session, Prague,
Czech Republic, June.
Taku Kudoh. 2007. CRF++: Yet Another CRF
Toolkit.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilitic Models for Segmenting and Labeling Se-
quence Data. In ICML, Massachusetts, USA.
Sameer Maskey, Bowen Zhou, and Yuqing Gao. 2006.
A Phrase-Level Machine Translation Approach for
Disfluency Detection using Weighted Finite State
Tranducers. In Interspeech, Pittsburgh, PA.
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Herrmann Ney. 2005. Evaluating Machine Trans-
lation Output with Automatic Sentence Segmenta-
tion. In Proceedings of the International Workshop
on Spoken Language Translation (IWSLT), Boulder,
Colorado, USA, October.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proceedings of the 4th Workshop on Statistical Ma-
chine Translation, Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Proceedings of the 6th Workshop on Statistical Ma-
chine Translation, Edinburgh, UK.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sk?ovde,
Sweden.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. Denver, Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In WPT-
05, Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
Wen Wang, Gokhan Tur, Jing Zheng, and Necip Fazil
Ayan. 2010. Automatic Disfluency Removal for Im-
proving Spoken Language Translation. In Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
47
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 349?355,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2012
Jan Niehues, Yuqi Zhang, Mohammed Mediani, Teresa Herrmann, Eunah Cho and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based SMT
systems developed for our participation
in the WMT12 Shared Translation Task.
Translations for English?German and
English?French were generated using a
phrase-based translation system which is
extended by additional models such as
bilingual, fine-grained part-of-speech (POS)
and automatic cluster language models and
discriminative word lexica. In addition, we
explicitly handle out-of-vocabulary (OOV)
words in German, if we have translations for
other morphological forms of the same stem.
Furthermore, we extended the POS-based
reordering approach to also use information
from syntactic trees.
1 Introduction
In this paper, we describe our systems for the
NAACL 2012 Seventh Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French. We use a
phrase-based decoder that can use lattices as input
and developed several models that extend the stan-
dard log-linear model combination of phrase-based
MT. In addition to the POS-based reordering model
used in past years, for German-English we extended
it to also use rules learned using syntax trees.
The translation model was extended by the bilin-
gual language model and a discriminative word lex-
icon using a maximum entropy classifier. For the
French-English and English-French translation sys-
tems, we also used phrase table adaptation to avoid
overestimation of the probabilities of the huge, but
noisy Giga corpus. In the German-English system,
we tried to learn translations for OOV words by ex-
ploring different morphological forms of the OOVs
with the same lemma.
Furthermore, we combined different language
models in the log-linear model. We used word-
based language models trained on different parts of
the training corpus as well as POS-based language
models using fine-grained POS information and lan-
guage models trained on automatic word clusters.
The paper is organized as follows: The next sec-
tion gives a detailed description of our systems in-
cluding all the models. The translation results for
all directions are presented afterwards and we close
with a conclusion.
2 System Description
For the French?English systems the phrase table
is based on a GIZA++ word alignment, while the
systems for German?English use a discriminative
word alignment as described in Niehues and Vogel
(2008). The language models are 4-gram SRI lan-
guage models using Kneser-Ney smoothing trained
by the SRILM Toolkit (Stolcke, 2002).
The problem of word reordering is addressed with
POS-based and tree-based reordering models as de-
scribed in Section 2.3. The POS tags used in the
reordering model are obtained using the TreeTagger
(Schmid, 1994). The syntactic parse trees are gen-
erated using the Stanford Parser (Rafferty and Man-
ning, 2008).
An in-house phrase-based decoder (Vogel, 2003)
is used to perform translation. Optimization with
349
regard to the BLEU score is done using Minimum
Error Rate Training as described in Venugopal et al
(2005). During decoding only the top 10 translation
options for every source phrase are considered.
2.1 Data
Our translation models were trained on the EPPS
and News Commentary (NC) corpora. Furthermore,
the additional available data for French and English
(i.e. UN and Giga corpora) were exploited in the
corresponding systems.
The systems were tuned with the news-test2011
data, while news-test2011 was used for testing in all
our systems. We trained language models for each
language on the monolingual part of the training cor-
pora as well as the News Shuffle and the Gigaword
(version 4) corpora. The discriminative word align-
ment model was trained on 500 hand-aligned sen-
tences selected from the EPPS corpus.
2.2 Preprocessing
The training data is preprocessed prior to training
the system. This includes normalizing special sym-
bols, smart-casing the first word of each sentence
and removing long sentences and sentences with
length mismatch.
For the German parts of the training corpus, in
order to obtain a homogenous spelling, we use the
hunspell1 lexicon to map words written according to
old German spelling rules to new German spelling
rules.
In order to reduce the OOV problem of German
compound words, Compound splitting as described
in Koehn and Knight (2003) is applied to the Ger-
man part of the corpus for the German-to-English
system.
The Giga corpus received a special preprocessing
by removing noisy pairs using an SVM classifier as
described in Mediani et al (2011). The SVM clas-
sifier training and test sets consist of randomly se-
lected sentence pairs from the corpora of EPPS, NC,
tuning, and test sets. Giving at the end around 16
million sentence pairs.
2.3 Word Reordering
In contrast to modeling the reordering by a distance-
based reordering model and/or a lexicalized distor-
1http://hunspell.sourceforge.net/
tion model, we use a different approach that relies on
POS sequences. By abstracting from surface words
to POS, we expect to model the reordering more ac-
curately. For German-to-English, we additionally
apply reordering rules learned from syntactic parse
trees.
2.3.1 POS-based Reordering Model
In order to build the POS-based reordering model,
we first learn probabilistic rules from the POS tags
of the training corpus and the alignment. Contin-
uous reordering rules are extracted as described in
Rottmann and Vogel (2007) to model short-range re-
orderings. When translating between German and
English, we apply a modified reordering model with
non-continuous rules to cover also long-range re-
orderings (Niehues and Kolss, 2009).
2.3.2 Tree-based Reordering Model
Word order is quite different between German and
English. And during translation especially verbs or
verb particles need to be shifted over a long dis-
tance in a sentence. Using discontinuous POS rules
already improves the translation tremendously. In
addition, we apply a tree-based reordering model
for the German-English translation. Syntactic parse
trees provide information about the words in a sen-
tence that form constituents and should therefore be
treated as inseparable units by the reordering model.
For the tree-based reordering model, syntactic parse
trees are generated for the whole training corpus.
Then the word alignment between the source and
target language part of the corpus is used to learn
rules on how to reorder the constituents in a Ger-
man source sentence to make it matches the English
target sentence word order better. In order to apply
the rules to the source text, POS tags and a parse
tree are generated for each sentence. Then the POS-
based and tree-based reordering rules are applied.
The original order of words as well as the reordered
sentence variants generated by the rules are encoded
in a word lattice. The lattice is then used as input to
the decoder.
For the test sentences, the reordering based on
POS and trees allows us to change the word order
in the source sentence so that the sentence can be
translated more easily. In addition, we build reorder-
ing lattices for all training sentences and then extract
350
phrase pairs from the monotone source path as well
as from the reordered paths.
2.4 Translation Models
In addition to the models used in the baseline system
described above, we conducted experiments includ-
ing additional models that enhance translation qual-
ity by introducing alternative or additional informa-
tion into the translation modeling process.
2.4.1 Phrase table adaptation
Since the Giga corpus is huge, but noisy, it is
advantageous to also use the translation probabil-
ities of the phrase pair extracted only from the
more reliable EPPS and News commentary cor-
pus. Therefore, we build two phrase tables for the
French?English system. One trained on all data
and the other only trained on the EPPS and News
commentary corpus. The two models are then com-
bined using a log-linear combination to achieve the
adaptation towards the cleaner corpora as described
in (Niehues et al, 2010). The newly created trans-
lation model uses the four scores from the general
model as well as the two smoothed relative frequen-
cies of both directions from the smaller, but cleaner
model. If a phrase pair does not occur in the in-
domain part, a default score is used instead of a rela-
tive frequency. In our case, we used the lowest prob-
ability.
2.4.2 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder according to the best com-
bination of phrases that maximize the translation
and language model scores. This segmentation into
phrases leads to the loss of context information at
the phrase boundaries. Although more target side
context is available to the language model, source
side context would also be valuable for the decoder
when searching for the best translation hypothesis.
To make also source language context available we
use a bilingual language model, in which each token
consists of a target word and all source words it is
aligned to. The bilingual tokens enter the translation
process as an additional target factor and the bilin-
gual language model is applied to the additional fac-
tor like a normal language model. For more details
see Niehues et al (2011).
2.4.3 Discriminative Word Lexica
Mauser et al (2009) have shown that the use
of discriminative word lexica (DWL) can improve
the translation quality. For every target word, they
trained a maximum entropy model to determine
whether this target word should be in the translated
sentence or not using one feature per one source
word.
When applying DWL in our experiments, we
would like to have the same conditions for the train-
ing and test case. For this we would need to change
the score of the feature only if a new word is added
to the hypothesis. If a word is added the second time,
we do not want to change the feature value. In order
to keep track of this, additional bookkeeping would
be required. Also the other models in our translation
system will prevent us from using a word too often.
Therefore, we ignore this problem and can calcu-
late the score for every phrase pair before starting
with the translation. This leads to the following def-
inition of the model:
p(e|f) =
J?
j=1
p(ej |f) (1)
In this definition, p(ej |f) is calculated using a max-
imum likelihood classifier.
Each classifier is trained independently on the
parallel training data. All sentences pairs where the
target word e occurs in the target sentence are used
as positive examples. We could now use all other
sentences as negative examples. But in many of
these sentences, we would anyway not generate the
target word, since there is no phrase pair that trans-
lates any of the source words into the target word.
Therefore, we build a target vocabulary for every
training sentence. This vocabulary consists of all
target side words of phrase pairs matching a source
phrase in the source part of the training sentence.
Then we use all sentence pairs where e is in the tar-
get vocabulary but not in the target sentences as neg-
ative examples. This has shown to have a postive
influence on the translation quality (Mediani et al,
2011) and also reduces training time.
2.4.4 Quasi-Morphological Operations for
OOV words
Since German is a highly inflected language, there
will be always some word forms of a given Ger-
351
Figure 1: Quasi-morphological operations
man lemma that did not occur in the training data.
In order to be able to also translate unseen word
forms, we try to learn quasi-morphological opera-
tions that change the lexical entry of a known word
form to the unknown word form. These have shown
to be beneficial in Niehues and Waibel (2011) using
Wikipedia2 titles. The idea is illustrated in Figure 1.
If we look at the data, our system is able to trans-
late a German word Kamin (engl. chimney), but not
the dative plural form Kaminen. To address this
problem, we try to automatically learn rules how
words can be modified. If we look at the example,
we would like the system to learn the following rule.
If an ?en? is appended to a German word, as it is
done when creating the dative plural form of Kami-
nen, we need to add an ?s? to the end of the English
word in order to perform the same morphological
word transformation. We use only rules where the
ending of the word has at most 3 letters.
Depending on the POS, number, gender or case of
the involved words, the same operation on the source
side does not necessarily correspond to the same op-
eration on the target side.
To account for this ambiguity, we rank the differ-
ent target operation using the following four features
and use the best ranked one. Firstly, we should not
generate target words that do not exist. Here, we
have an advantage that we can use monolingual data
to determine whether the word exists. In addition,
a target operation that often coincides with a given
source operation should be better than one that is
rarely used together with the source operation. We
therefore look at pairs of entries in the lexicon and
count in how many of them the source operation can
be applied to the source side and the target operation
can be applied to the target side. We then use only
operations that occur at least ten times. Furthermore,
2http://www.wikipedia.org/
we use the ending of the source and target word to
determine which pair of operations should be used.
Integration We only use the proposed method for
OOVs and do not try to improve translations of
words that the baseline system already covers. We
look for phrase pairs, for which a source operation
ops exists that changes one of the source words f1
into the OOV word f2. Since we need to apply a
target operation to one word on the target side of the
phrase pair, we only consider phrase pairs where f1
is aligned to one of the target words of the phrase
containing e1. If a target operation exists given f1
and ops, we select the one with the highest rank.
Then we generate a new phrase pair by applying
ops to f1 and opt to e1 keeping the original scores
from the phrase pairs, since the original and syn-
thesized phrase pair are not directly competing any-
way. We do not add several phrase pairs generated
by different operations, since we would then need to
add the features used for ranking the operations into
the MERT. This is problematic, since the operations
were only used for very few words and therefore a
good estimation of the weights is not possible.
2.5 Language Models
The 4-gram language models generated by the
SRILM toolkit are used as the main language mod-
els for all of our systems. For English-French and
French-English systems, we use a good quality cor-
pus as in-domain data to train in-domain language
models. Additionally, we apply the POS and clus-
ter language models in different systems. All lan-
guage models are integrated into the translation sys-
tem by a log-linear combination and received opti-
mal weights during tuning by the MERT.
2.5.1 POS Language Models
The POS language model is trained on the POS
sequences of the target language. In this evalua-
tion, the POS language model is applied for the
English-German system. We expect that having ad-
ditional information in form of probabilities of POS
sequences should help especially in case of the rich
morphology of German. The POS tags are gener-
ated with the RFTagger (Schmid and Laws, 2008)
for German, which produces fine-grained tags that
include person, gender and case information. We
352
use a 9-gram language model on the News Shuf-
fle corpus and the German side of all parallel cor-
pora. More details and discussions about the POS
language model can be found in Herrmann et al
(2011).
2.5.2 Cluster Language Models
The cluster language model follows a similar idea
as the POS language model. Since there is a data
sparsity problem when we substitute words with the
word classes, it is possible to make use of larger
context information. In the POS language model,
POS tags are the word classes. Here, we generated
word classes in a different way. First, we cluster
the words in the corpus using the MKCLS algorithm
(Och, 1999) given a number of classes. Second, we
replace the words in the corpus by their cluster IDs.
Finally, we train an n-gram language model on this
corpus consisting of cluster IDs. Generally, all clus-
ter language models used in our systems are 5-gram.
3 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to the
workshop. The following sections describe the ex-
periments for the individual language pairs and show
the translation results. The results are reported as
case-sensitive BLEU scores (Papineni et al, 2002)
on one reference translation.
3.1 German-English
The experiments for the German-English translation
system are summarized in Table 1. The Baseline
system uses POS-based reordering, discriminative
word alignment and a language model trained on the
News Shuffle corpus. By adding lattice phrase ex-
traction small improvements of the translation qual-
ity could be gained.
Further improvements could be gained by adding
a language model trained on the Gigaword corpus
and adding a bilingual and cluster-based language
model. We used 50 word classes and trained a 5-
gram language model. Afterwards, the translation
quality was improved by also using a discriminative
word lexicon. Finally, the best system was achieved
by using Tree-based reordering and using special
treatment for the OOVs. This system generates a
BLEU score of 22.31 on the test data. For the last
two systems, we did not perform new optimization
runs.
System Dev Test
Baseline 23.64 21.32
+ Lattice Phrase Extraction 23.76 21.36
+ Gigaward Language Model 24.01 21.73
+ Bilingual LM 24.19 21.91
+ Cluster LM 24.16 22.09
+ DWL 24.19 22.19
+ Tree-based Reordering - 22.26
+ OOV - 22.31
Table 1: Translation results for German-English
3.2 English-German
The English-German baseline system uses also
POS-based reordering, discriminative word align-
ment and a language model based on EPPS, NC and
News Shuffle. A small gain could be achieved by the
POS-based language model and the bilingual lan-
guage model. Further gain was achieved by using
also a cluster-based language model. For this lan-
guage model, we use 100 word classes and trained
a 5-gram language model. Finally, the best system
uses the discriminative word lexicon.
System Dev Test
Baseline 17.06 15.57
+ POSLM 17.27 15.63
+ Bilingual LM 17.40 15.78
+ Cluster LM 17.77 16.06
+ DWL 17.75 16.28
Table 2: Translation results for English-German
3.3 English-French
Table 3 summarizes how our English-French sys-
tem evolved. The baseline system here was trained
on the EPPS, NC, and UN corpora, while the lan-
guage model was trained on all the French part of
the parallel corpora (including the Giga corpus). It
also uses short-range reordering trained on EPPS
and NC. This system had a BLEU score of around
26.7. The Giga parallel data turned out to be quite
353
beneficial for this task. It improves the scores by
more than 1 BLEU point. More importantly, addi-
tional language models boosted the system quality:
around 1.8 points. In fact, three language models
were log-linearly combined: In addition to the afore-
mentioned, two additional language models were
trained on the monolingual sets (one for News and
one for Gigaword). We could get an improvement
of around 0.2 by retraining the reordering rules on
EPPS and NC only, but using Giza alignment from
the whole data. Adapting the translation model by
using EPPS and NC as in-domain data improves the
BLEU score by only 0.1. This small improvement
might be due to the fact that the news domain is
very broad and that the Giga corpus has already been
carefully cleaned and filtered. Furthermore, using a
bilingual language model enhances the BLEU score
by almost 0.3. Finally, incorporating a cluster lan-
guage model adds an additional 0.1 to the score.
This leads to a system with 30.58.
System Dev Test
Baseline 24.96 26.67
+ GigParData 26.12 28.16
+ Big LMs 29.22 29.92
+ All Reo 29.14 30.10
+ PT Adaptation 29.15 30.22
+ Bilingual LM 29.17 30.49
+ Cluster LM 29.08 30.58
Table 3: Translation results for English-French
3.4 French-English
The development of our system for the French-
English direction is summarized in Table 4. The
baseline system for this direction was trained on the
EPPS, NC, UN and Giga parallel corpora, while the
language model was trained on the French part of the
parallel training corpora. The baseline system in-
cludes the POS-based reordering model with short-
range rules. The largest improvement of 1.7 BLEU
score was achieved by the integration of the bigger
language models which are trained on the English
version of News Shuffle and the Gigaword corpus
(v4). We did not add the language models from the
monolingual English version of EPPS and NC data,
since the experiments have shown that they did not
provide improvement in our system. The second
largest improvement came from the domain adap-
tation that includes an in-domain language model
and adaptations to the phrase extraction. The BLEU
score has improved about 1 BLEU in total. The in-
domain data we used here are parallel EPPS and NC
corpus. Further gains were obtained by augmenting
the system with a bilingual language model adding
around 0.2 BLEU to the previous score. The sub-
mitted system was obtained by adding the cluster
5-gram language model trained on the News Shuf-
fle corpus with 100 clusters and thus giving 30.25 as
the final score.
System Dev Test
Baseline 25.81 27.15
+ Indomain LM 26.17 27.91
+ PT Adaptation 26.33 28.11
+ Big LMs 28.90 29.82
+ Bilingual LM 29.14 30.09
+ Cluster LM 29.31 30.25
Table 4: Translation results for French-English
4 Conclusions
We have presented the systems for our participation
in the WMT 2012 Evaluation for English?German
and English?French. In all systems we could im-
prove by using a class-based language model. Fur-
thermore, the translation quality could be improved
by using a discriminative word lexicon. Therefore,
we trained a maximum entropy classifier for ev-
ery target word. For English?French, adapting the
phrase table helps to avoid using wrong parts of the
noisy Giga corpus. For the German-to-English sys-
tem, we could improve the translation quality addi-
tionally by using a tree-based reordering model and
by special handling of OOV words. For the inverse
direction we could improve the translation quality
by using a 9-gram language model trained on the
fine-grained POS tags.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
354
References
Teresa Herrmann, Mohammed Mediani, Jan Niehues,
and Alex Waibel. 2011. The karlsruhe institute of
technology translation systems for the wmt 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 379?385, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In EACL, Budapest,
Hungary.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 - Vol-
ume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The kit english-
french translation systems for iwslt 2011. In Proceed-
ings of the eight International Workshop on Spoken
Language Translation (IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In
Proc. of Third ACL Workshop on Statistical Machine
Translation, Columbus, USA.
Jan Niehues and Alex Waibel. 2011. Using wikipedia to
translate domain-specific terms in smt. In Proceedings
of the eight International Workshop on Spoken Lan-
guage Translation (IWSLT).
Jan Niehues, Mohammed Mediani, Teresa Herrmann,
Michael Heck, Christian Herff, and Alex Waibel.
2010. The KIT Translation system for IWSLT 2010.
In Marcello Federico, Ian Lane, Michael Paul, and
Franc?ois Yvon, editors, Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 93?98.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Associa-
tion for Computational Linguistics, EACL ?99, pages
71?76, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division, T. J.
Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three german treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sko?vde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation of
Conditional Probabilities with Decision Trees and an
Application to Fine-Grained POS Tagging. In COL-
ING 2008, Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Beyond
(WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
355
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 104?108,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2013
Eunah Cho, Thanh-Le Ha, Mohammed Mediani, Jan Niehues,
Teresa Herrmann, Isabel Slawik and Alex Waibel
Karlsruhe Institute of Technology
Karlsruhe, Germany
firstname.lastname@kit.edu
Abstract
This paper describes the phrase-based
SMT systems developed for our partici-
pation in the WMT13 Shared Translation
Task. Translations for English?German
and English?French were generated us-
ing a phrase-based translation system
which is extended by additional models
such as bilingual, fine-grained part-of-
speech (POS) and automatic cluster lan-
guage models and discriminative word
lexica (DWL). In addition, we combined
reordering models on different sentence
abstraction levels.
1 Introduction
In this paper, we describe our systems for the
ACL 2013 Eighth Workshop on Statistical Ma-
chine Translation. We participated in the Shared
Translation Task and submitted translations for
English?German and English?French using a
phrase-based decoder with lattice input.
The paper is organized as follows: the next sec-
tion gives a detailed description of our systems
including all the models. The translation results
for all directions are presented afterwards and we
close with a conclusion.
2 System Description
The phrase table is based on a GIZA++ word
alignment for the French?English systems. For
the German?English systems we use a Discrim-
inative Word Alignment (DWA) as described in
Niehues and Vogel (2008). For every source
phrase only the top 10 translation options are con-
sidered during decoding. The SRILM Toolkit
(Stolcke, 2002) is used for training SRI language
models using Kneser-Ney smoothing.
For the word reordering between languages, we
used POS-based reordering models as described in
Section 4. In addition to it, tree-based reordering
model and lexicalized reordering were added for
German?English systems.
An in-house phrase-based decoder (Vogel,
2003) is used to perform translation. The trans-
lation was optimized using Minimum Error Rate
Training (MERT) as described in Venugopal et
al. (2005) towards better BLEU (Papineni et al,
2002) scores.
2.1 Data
The Europarl corpus (EPPS) and News Commen-
tary (NC) corpus were used for training our trans-
lation models. We trained language models for
each language on the monolingual part of the
training corpora as well as the News Shuffle and
the Gigaword corpora. The additional data such as
web-crawled corpus, UN and Giga corpora were
used after filtering. The filtering work for this data
is discussed in Section 3.
For the German?English systems we use the
news-test2010 set for tuning, while the news-
test2011 set is used for the French?English sys-
tems. For testing, news-test2012 set was used for
all systems.
2.2 Preprocessing
The training data is preprocessed prior to train-
ing the system. This includes normalizing special
symbols, smart-casing the first word of each sen-
tence and removing long sentences and sentence
pairs with length mismatch.
Compound splitting is applied to the German
part of the corpus of the German?English system
as described in Koehn and Knight (2003).
3 Filtering of Noisy Pairs
The filtering was applied on the corpora which
are found to be noisy. Namely, the Giga English-
French parallel corpus and the all the new web-
crawled data . The operation was performed using
104
an SVM classifier as in our past systems (Medi-
ani et al, 2011). For each pair, the required lexica
were extracted from Giza alignment of the corre-
sponding EPPS and NC corpora. Furthermore, for
the web-crawled data, higher precision classifiers
were trained by providing a larger number of neg-
ative examples to the classifier.
After filtering, we could still find English sen-
tences in the other part of the corpus. Therefore,
we performed a language identification (LID)-
based filtering afterwards (performed only on the
French-English corpora, in this participation).
4 Word Reordering
Word reordering was modeled based on POS se-
quences. For the German?English system, re-
ordering rules learned from syntactic parse trees
were used in addition.
4.1 POS-based Reordering Model
In order to train the POS-based reordering model,
probabilistic rules were learned based on the POS
tags from the TreeTagger (Schmid and Laws,
2008) of the training corpus and the alignment. As
described in Rottmann and Vogel (2007), continu-
ous reordering rules are extracted. This modeling
of short-range reorderings was extended so that it
can cover also long-range reorderings with non-
continuous rules (Niehues and Kolss, 2009), for
German?English systems.
4.2 Tree-based Reordering Model
In addition to the POS-based reordering, we
apply a tree-based reordering model for the
German?English translation to better address the
differences in word order between German and
English. We use the Stanford Parser (Rafferty and
Manning, 2008) to generate syntactic parse trees
for the source side of the training corpus. Then
we use the word alignment between source and
target language to learn rules on how to reorder
the constituents in a German source sentence to
make it match the English target sentence word or-
der better (Herrmann et al, 2013). The POS-based
and tree-based reordering rules are applied to each
input sentence. The resulting reordered sentence
variants as well as the original sentence order are
encoded in a word lattice. The lattice is then used
as input to the decoder.
4.3 Lexicalized Reordering
The lexicalized reordering model stores the re-
ordering probabilities for each phrase pair. Pos-
sible reordering orientations at the incoming and
outgoing phrase boundaries are monotone, swap
or discontinuous. With the POS- and tree-based
reordering word lattices encode different reorder-
ing variants. In order to apply the lexicalized re-
ordering model, we store the original position of
each word in the lattice. At each phrase boundary
at the end, the reordering orientation with respect
to the original position of the words is checked.
The probability for the respective orientation is in-
cluded as an additional score.
5 Translation Models
In addition to the models used in the baseline sys-
tem described above, we conducted experiments
including additional models that enhance trans-
lation quality by introducing alternative or addi-
tional information into the translation modeling
process.
5.1 Bilingual Language Model
During the decoding the source sentence is seg-
mented so that the best combination of phrases
which maximizes the scores is available. How-
ever, this causes some loss of context information
at the phrase boundaries. In order to make bilin-
gual context available, we use a bilingual language
model (Niehues et al, 2011). In the bilingual lan-
guage model, each token consists of a target word
and all source words it is aligned to.
5.2 Discriminative Word Lexicon
Mauser et al (2009) introduced the Discriminative
Word Lexicon (DWL) into phrase-based machine
translation. In this approach, a maximum entropy
model is used to determine the probability of using
a target word in the translation.
In this evaluation, we used two extensions to
this work as shown in (Niehues and Waibel, 2013).
First, we added additional features to model the
order of the source words better. Instead of rep-
resenting the source sentence as a bag-of-words,
we used a bag-of-n-grams. We used n-grams up to
the order of three and applied count filtering to the
features for higher order n-grams.
Furthermore, we created the training examples
differently in order to focus on addressing errors
of the other models of the phrase-based translation
105
system. We first translated the whole corpus with a
baseline system. Then we only used the words that
occur in the N-Best List and not in the reference as
negative examples instead of using all words that
do not occur in the reference.
5.3 Quasi-Morphological Operations
Because of the inflected characteristic of the
German language, we try to learn quasi-
morphological operations that change the lexi-
cal entry of a known word form to the out-of-
vocabulary (OOV) word form as described in
Niehues and Waibel (2012).
5.4 Phrase Table Adaptation
For the French?English systems, we built two
phrase tables; one trained with all data and the
other trained only with the EPPS and NC cor-
pora. This is due to the fact that Giga corpus is big
but noisy and EPPS and NC corpus are more reli-
able. The two models are combined log-linearly to
achieve the adaptation towards the cleaner corpora
as described in Niehues et al (2010).
6 Language Models
The 4-gram language models generated by the
SRILM toolkit are used as the main language
models for all of our systems. For the
English?French systems, we use a good quality
corpus as in-domain data to train in-domain lan-
guage models. Additionally, we apply the POS
and cluster language models in different systems.
For the German?English system, we build sepa-
rate language models using each corpus and com-
bine them linearly before the decoding by mini-
mizing the perplexity. Language models are inte-
grated into the translation system by a log-linear
combination and receive optimal weights during
tuning by the MERT.
6.1 POS Language Models
For the English?German system, we use the POS
language model, which is trained on the POS se-
quence of the target language. The POS tags are
generated using the RFTagger (Schmid and Laws,
2008) for German. The RFTagger generates fine-
grained tags which include person, gender, and
case information. The language model is trained
with up to 9-gram information, using the German
side of the parallel EPPS and NC corpus, as well
as the News Shuffle corpus.
6.2 Cluster Language Models
In order to use larger context information, we use
a cluster language model for all our systems. The
cluster language model is based on the idea shown
in Och (1999). Using the MKCLS algorithm, we
cluster the words in the corpus, given a number
of classes. Then words in the corpus are replaced
with their cluster IDs. Using these cluster IDs,
we train n-gram language models as well as a
phrase table with this additional factor of cluster
ID. Our submitted systems have diversed range of
the number of clusters as well as n-gram.
7 Results
Using the models described above we performed
several experiments leading finally to the systems
used for generating the translations submitted to
the workshop. The results are reported as case-
sensitive BLEU scores on one reference transla-
tion.
7.1 German?English
The experiments for the German to English trans-
lation system are summarized in Table 1. The
baseline system uses POS-based reordering, DWA
with lattice phrase extraction and language models
trained on the News Shuffle corpus and Giga cor-
pus separately. Then we added a 5-gram cluster
LM trained with 1,000 word classes. By adding a
language model using the filtered crawled data we
gained 0.3 BLEU on the test set. For this we com-
bined all language models linearly. The filtered
crawled data was also used to generate a phrase
table, which brought another improvement of 0.85
BLEU. Applying tree-based reordering improved
the BLEU score, and the performance had more
gain by adding the extended DWL, namely us-
ing both bag-of-ngrams and n-best lists. While
lexicalized reordering gave us a slight gain, we
added morphological operation and gained more
improvements.
7.2 English?German
The English to German baseline system uses POS-
based reordering and language models using par-
allel data (EPPS and NC) as shown in Table 2.
Gradual gains were achieved by changing align-
ment from GIZA++ to DWA, adding a bilingual
language model as well as a language model based
on the POS tokens. A 9-gram cluster-based lan-
guage model with 100 word classes gave us a
106
System Dev Test
Baseline 24.15 22.79
+ Cluster LM 24.18 22.84
+ Crawled Data LM (Comb.) 24.53 23.14
+ Crawled Data PT 25.38 23.99
+ Tree Rules 25.80 24.16
+ Extended DWL 25.59 24.54
+ Lexicalized Reordering 26.04 24.55
+ Morphological Operation - 24.62
Table 1: Translation results for German?English
small gain. Improving the reordering using lexi-
alized reordering gave us gain on the optimization
set. Using DWL let us have more improvements
on our test set. By using the filtered crawled data,
we gained a big improvement of 0.46 BLEU on
the test set. Then we extended the DWL with bag
of n-grams and n-best lists to achieve additional
improvements. Finally, the best system includes
lattices generated using tree rules.
System Dev Test
Baseline 17.00 16.24
+ DWA 17.27 16.53
+ Bilingual LM 17.27 16.59
+ POS LM 17.46 16.66
+ Cluster LM 17.49 16.68
+ Lexicalized Reordering 17.57 16.68
+ DWL 17.58 16.77
+ Crawled Data 18.43 17.23
+ Extended DWL 18.66 17.57
+ Tree Rules 18.63 17.70
Table 2: Translation results for English?German
7.3 French?English
Table 3 reports some remarkable improvements
as we combined several techniques on the
French?English direction. The baseline system
was trained on parallel corpora such as EPPS, NC
and Giga, while the language model was trained
on the English part of those corpora plus News
Shuffle. The newly presented web-crawled data
helps to achieve almost 0.6 BLEU points more
on test set. Adding bilingual language model and
cluster language model does not show a significant
impact. Further gains were achieved by the adap-
tation of in-domain data into general-theme phrase
table, bringing 0.15 BLEU better on the test set.
When we added the DWL feature, it notably im-
proves the system by 0.25 BLEU points, resulting
in our best system.
System Dev Test
Baseline 30.33 29.35
+ Crawled Data 30.59 29.93
+ Bilingual and Cluster LMs 30.67 30.01
+ In-Domain PT Adaptation 31.17 30.16
+ DWL 31.07 30.40
Table 3: Translation results for French?English
7.4 English?French
In the baseline system, EPPS, NC, Giga and News
Shuffle corpora are used for language modeling.
The big phrase tables tailored EPPC, NC and Giga
data. The system also uses short-range reordering
trained on EPPS and NC. Adding parallel and fil-
tered crawl data improves the system. It was fur-
ther enhanced by the integration of a 4-gram bilin-
gual language model. Moreover, the best config-
uration of 9-gram language model trained on 500
clusters of French texts gains 0.25 BLEU points
improvement. We also conducted phrase-table
adaptation from the general one into the domain
covered by EPPS and NC data and it helps as well.
The initial try-out with lexicalized reordering fea-
ture showed an improvement of 0.23 points on the
development set, but a surprising reduction on the
test set, thus we decided to take the system after
adaptation as our best English?French system.
System Dev Test
Baseline 30.50 27.77
+ Crawled Data 31.05 27.87
+ Bilingual LM 31.23 28.50
+ Cluster LM 31.58 28.75
+ In-Domain PT Adaptation 31.88 29.12
+ Lexicalized Reordering 32.11 28.98
Table 4: Translation results for English?French
8 Conclusions
We have presented the systems for our par-
ticipation in the WMT 2013 Evaluation for
English?German and English?French. All sys-
tems use a class-based language model as well
as a bilingual language model. Using a DWL
with source context improved the translation qual-
ity of English?German systems. Also for these
systems, we could improve even more with a
tree-based reordering model. Special handling
107
of OOV words improved German?English sys-
tem, while for the inverse direction the language
model with fine-grained POS tags was helpful. For
English?French, phrase table adaptation helps to
avoid using wrong parts of the noisy Giga corpus.
9 Acknowledgements
This work was partly achieved as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. The research lead-
ing to these results has received funding from
the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
n? 287658.
References
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The kit english-
french translation systems for iwslt 2011. In Pro-
ceedings of the eight International Workshop on
Spoken Language Translation (IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues and Alex Waibel. 2012. Detailed Analysis
of different Strategies for Phrase Table Adaptation
in SMT. In Proceedings of the American Machine
Translation Association (AMTA), San Diego, Cali-
fornia, October.
Jan Niehues and Alex Waibel. 2013. An MT Error-
driven Discriminative Word Lexicon using Sentence
Structure Features. In Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013), Sofia, Bul-
garia.
Jan Niehues, Mohammed Mediani, Teresa Herrmann,
Michael Heck, Christian Herff, and Alex Waibel.
2010. The KIT Translation system for IWSLT 2010.
In Marcello Federico, Ian Lane, Michael Paul, and
Franc?ois Yvon, editors, Proceedings of the seventh
International Workshop on Spoken Language Trans-
lation (IWSLT), pages 93?98.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
the ninth conference on European chapter of the As-
sociation for Computational Linguistics, EACL ?99,
pages 71?76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German, Columbus, Ohio.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, Great Britain.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP, Denver,
Colorado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
108
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185?192,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joint WMT 2013 Submission of the QUAERO Project
?Stephan Peitz, ?Saab Mansour, ?Matthias Huck, ?Markus Freitag, ?Hermann Ney,
?Eunah Cho, ?Teresa Herrmann, ?Mohammed Mediani, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Quoc Khanh Do,
?Bianka Buschbeck, ?Tonio Wandmacher
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint submis-
sion of the QUAERO project for the
German?English translation task of the
ACL 2013 Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013).
The submission was a system combina-
tion of the output of four different transla-
tion systems provided by RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy (KIT), LIMSI-CNRS and SYSTRAN
Software, Inc. The translations were
joined using the RWTH?s system com-
bination approach. Experimental results
show improvements of up to 1.2 points in
BLEU and 1.2 points in TER compared to
the best single translation.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in ma-
chine translation is mainly assigned to the four
groups participating in this joint submission. The
aim of this submission was to show the quality of
a joint translation by combining the knowledge of
the four project partners. Each group develop and
maintain their own different machine translation
system. These single systems differ not only in
their general approach, but also in the preprocess-
ing of training and test data. To take advantage
of these differences of each translation system, we
combined all hypotheses of the different systems,
using the RWTH system combination approach.
This paper is structured as follows. First, the
different engines of all four groups are introduced.
In Section 3, the RWTH Aachen system combina-
tion approach is presented. Experiments with dif-
ferent system selections for system combination
are described in Section 4. This paper is concluded
in Section 5.
2 Translation Systems
For WMT 2013, each QUAERO partner trained
their systems on the parallel Europarl (EPPS),
News Commentary (NC) corpora and the web-
crawled corpus. All single systems were tuned on
the newstest2009 and newstest2010 development
set. The newstest2011 development set was used
to tune the system combination parameters. Fi-
nally, on newstest2012 the results of the different
system combination settings are compared. In this
Section, all four different translation engines are
presented.
2.1 RWTH Aachen Single System
For the WMT 2013 evaluation, RWTH utilized a
phrase-based decoder based on (Wuebker et al,
2012) which is part of RWTH?s open-source SMT
toolkit Jane 2.1 1. GIZA++ (Och and Ney, 2003)
was employed to train a word alignment, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
After phrase pair extraction from the word-
aligned parallel corpus, the translation probabil-
ities are estimated by relative frequencies. The
standard feature set alo includes an n-gram lan-
guage model, phrase-level IBM-1 and word-,
phrase- and distortion-penalties, which are com-
bined in log-linear fashion. Furthermore, we used
an additional reordering model as described in
(Galley and Manning, 2008). By this model six
1http://www-i6.informatik.rwth-aachen.
de/jane/
185
additional feature are added to the log-linear com-
bination. The model weights are optimized with
standard Mert (Och, 2003a) on 200-best lists. The
optimization criterion is BLEU.
2.1.1 Preprocessing
In order to reduce the source vocabulary size trans-
lation, the German text was preprocessed by split-
ting German compound words with the frequency-
based method described in (Koehn and Knight,
2003). To further reduce translation complexity
for the phrase-based approach, we performed the
long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.2 Translation Model
We applied filtering and weighting for domain-
adaptation similarly to (Mansour et al, 2011) and
(Mansour and Ney, 2012). For filtering the bilin-
gual data, a combination of LM and IBM Model
1 scores was used. In addition, we performed
weighted phrase extraction by using a combined
LM and IBM Model 1 weight.
2.1.3 Language Model
During decoding a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl,
the 109 French-English, UN and LDC Gigaword
Fourth Edition corpora.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
The training data was preprocessed prior to the
training. Symbols such as quotes, dashes and
apostrophes are normalized. Then the first words
of each sentence are smart-cased. For the Ger-
man part of the training corpus, the hunspell2 lex-
icon was used, in order to learn a mapping from
old German spelling to new German writing rules.
Compound-splitting was also performed as de-
scribed in Koehn and Knight (2003). We also re-
moved very long sentences, empty lines, and sen-
tences which show big mismatch on the length.
2.2.2 Filtering
The web-crawled corpus was filtered using an
SVM classifier as described in (Mediani et al,
2011). The lexica used in this filtering task were
obtained from Giza alignments trained on the
2http://hunspell.sourceforge.net/
cleaner corpora, EPPS and NC. Assuming that this
corpus is very noisy, we biased our classifier more
towards precision than recall. This was realized
by giving higher number of false examples (80%
of the training data).
This filtering technique ruled out more than
38% of the corpus (the unfiltered corpus contains
around 2.4M pairs, 0.9M of which were rejected
in the filtering task).
2.2.3 System Overview
The in-house phrase-based decoder (Vogel, 2003)
is used to perform decoding. Optimization with
regard to the BLEU score is done using Minimum
Error Rate Training (MERT) as described in Venu-
gopal et al (2005).
2.2.4 Reordering Model
We applied part-of-speech (POS) based reordering
using probabilistic continuous (Rottmann and Vo-
gel, 2007) and discontinuous (Niehues and Kolss,
2009) rules. This was learned using POS tags gen-
erated by the TreeTagger (Schmid, 1994) for short
and long range reorderings respectively.
In addition to this POS-based reordering, we
also used tree-based reordering rules. Syntactic
parse trees of the whole training corpus and the
word alignment between source and target lan-
guage are used to learn rules on how to reorder the
constituents in a German source sentence to make
it match the English target sentence word order
better (Herrmann et al, 2013). The training corpus
was parsed by the Stanford parser (Rafferty and
Manning, 2008). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are en-
coded in a word lattice which is used as input to
the decoder.
Moreover, our reordering model was extended
so that it could include the features of lexicalized
reordering model. The reordering probabilities for
each phrase pair are stored as well as the origi-
nal position of each word in the lattice. During
the decoding, the reordering origin of the words
is checked along with its probability added as an
additional score.
2.2.5 Translation Models
The translation model uses the parallel data of
EPPS, NC, and the filtered web-crawled data. As
word alignment, we used the Discriminative Word
Alignment (DWA) as shown in (Niehues and Vo-
186
gel, 2008). The phrase pairs were extracted using
different source word order suggested by the POS-
based reordering models presented previously as
described in (Niehues et al, 2009).
In order to extend the context of source lan-
guage words, we applied a bilingual language
model (Niehues et al, 2011). A Discriminative
Word Lexicon (DWL) introduced in (Mauser et
al., 2009) was extended so that it could take the
source context also into the account. For this,
we used a bag-of-ngrams instead of representing
the source sentence as a bag-of-words. Filtering
based on counts was then applied to the features
for higher order n-grams. In addition to this, the
training examples were created differently so that
we only used the words that occur in the n-best list
but not in the reference as negative example.
2.2.6 Language Models
We build separate language models and combined
them prior to decoding. As word-token based
language models, one language model is built on
EPPS, NC, and giga corpus, while another one is
built using crawled data. We combined the LMs
linearly by minimizing the perplexity on the de-
velopment data. As a bilingual language model we
used the EPPS, NC, and the web-crawled data and
combined them. Furthermore, we use a 5-gram
cluster-based language model with 1,000 word
clusters, which was trained on the EPPS and NC
corpus. The word clusters were created using the
MKCLS algorithm.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine transla-
tion system based on bilingual n-gram3. In this
approach, the translation model relies on a spe-
cific decomposition of the joint probability of a
sentence pair using the n-gram assumption: a sen-
tence pair is decomposed into a sequence of bilin-
gual units called tuples, defining a joint segmen-
tation of the source and target. In the approach of
(Marin?o et al, 2006), this segmentation is a by-
product of source reordering which ultimately de-
rives from initial word and phrase alignments.
2.3.2 An overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using
3http://ncode.limsi.fr/
a n-gram model of (source,target) pairs (Casacu-
berta and Vidal, 2004). Training this model re-
quires to reorder source sentences so as to match
the target word order. This is performed by
a stochastic finite-state reordering model, which
uses part-of-speech information4 to generalize re-
ordering patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized re-
ordering models (Tillmann, 2004) aiming at pre-
dicting the orientation of the next translation unit;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which compensate for the system prefer-
ence for short translations. The four lexicon mod-
els are similar to the ones use in a standard phrase
based system: two scores correspond to the rel-
ative frequencies of the tuples and two lexical
weights estimated from the automatically gener-
ated word alignments. The weights associated to
feature functions are optimally combined using a
discriminative training framework (Och, 2003b).
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the
tuple extraction process. The resulting reordering
hypotheses are passed to the decoder in the form
of word lattices (Crego and Mario, 2006).
2.3.3 Continuous space translation models
One critical issue with standard n-gram translation
models is that the elementary units are bilingual
pairs, which means that the underlying vocabu-
lary can be quite large, even for small translation
tasks. Unfortunately, the parallel data available to
train these models are typically order of magni-
tudes smaller than the corresponding monolingual
corpora used to train target language models. It is
very likely then, that such models should face se-
vere estimation problems. In such setting, using
neural network language model techniques seem
all the more appropriate. For this study, we fol-
low the recommendations of Le et al (2012), who
propose to factor the joint probability of a sen-
tence pair by decomposing tuples in two (source
and target) parts, and further each part in words.
This yields a word factored translation model that
4Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
187
can be estimated in a continuous space using the
SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the com-
putational cost of computing n-gram probabilities.
The solution used here was to resort to a two pass
approach: the first pass uses a conventional back-
off n-gram model to produce a k-best list; in the
second pass, the k-best list is reordered using the
probabilities of m-gram SOUL translation models.
In the following experiments, we used a fixed con-
text size for SOUL of m= 10, and used k = 300.
2.3.4 Corpora and data pre-processing
All the parallel data allowed in the constrained
task are pooled together to create a single par-
allel corpus. This corpus is word-aligned using
MGIZA++5 with default settings. For the English
monolingual training data, we used the same setup
as last year6 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we also took advantage of our in-
house text processing tools for the tokenization
and detokenization steps (Dchelotte et al, 2008)
and our system is built in ?true-case?. As Ger-
man is morphologically more complex than En-
glish, the default policy which consists in treat-
ing each word form independently is plagued with
data sparsity, which is detrimental both at training
and decoding time. Thus, the German side was
normalized using a specific pre-processing scheme
(described in (Allauzen et al, 2010; Durgar El-
Kahlout and Yvon, 2010)), which notably aims at
reducing the lexical redundancy by (i) normalizing
the orthography, (ii) neutralizing most inflections
and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
In the past few years, SYSTRAN has been focus-
ing on the introduction of statistical approaches
to its rule-based backbone, leading to Hybrid Ma-
chine Translation.
The technique of Statistical Post-Editing
(Dugast et al, 2007) is used to automatically edit
the output of the rule-based system. A Statistical
Post-Editing (SPE) module is generated from a
bilingual corpus. It is basically a translation mod-
ule by itself, however it is trained on rule-based
5http://geek.kyloo.net/software
6The fifth edition of the English Gigaword
(LDC2011T07) was not used.
translations and reference data. It applies correc-
tions and adaptations learned from a phrase-based
5-gram language model. Using this two-step
process will implicitly keep long distance re-
lations and other constraints determined by the
rule-based system while significantly improving
phrasal fluency. It has the advantage that quality
improvements can be achieved with very little
but targeted bilingual data, thus significantly
reducing training time and increasing translation
performance.
The basic setup of the SPE component is identi-
cal to the one described in (Dugast et al, 2007).
A statistical translation model is trained on the
rule-based translation of the source and the target
side of the parallel corpus. Language models are
trained on each target half of the parallel corpora
and also on additional in-domain corpora. More-
over, the following measures - limiting unwanted
statistical effects - were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is sig-
nificantly reduced. In addition, entity trans-
lation is handled more reliably by the rule-
based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the
reference translation) is used to produce an
additional parallel corpus (whose target is
identical to the source). This was added to the
parallel text in order to improve word align-
ment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side
are also discarded.
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained on 2M
phrases from the news/europarl and Common-
Crawl corpora, provided as training data for WMT
2013. Weights for these separate models were
tuned by the Mert algorithm provided in the Moses
toolkit (Koehn et al, 2007), using the provided
news development set.
188
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
3 RWTH Aachen System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses gener-
ated with different translation engines. First, a
word to word alignment for the given single sys-
tem hypotheses is produced. In a second step a
confusion network is constructed. Then, the hy-
pothesis with the highest probability is extracted
from this confusion network. For the alignment
procedure, each of the given single systems gen-
erates one confusion network with its own as pri-
mary system. To this primary system all other hy-
potheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1. The final network for one source sen-
tence is the union of all confusion networks gen-
erated from the different primary systems. That
allows the system combination to select the word
order from different system outputs.
Before performing system combination, each
translation output was normalized by tokenization
and lowercasing. The output of the combination
was then truecased based on the original truecased
output.
The model weights of the system combination
are optimized with standard Mert (Och, 2003a)
on 100-best lists. We add one voting feature for
each single system to the log-linear framework of
the system combination. The voting feature fires
for each word the single system agrees on. More-
over, a word penalty, a language model trained on
the input hypotheses, a binary feature which pe-
nalizes word deletions in the confusion network
and a primary feature which marks the system
which provides the word order are combined in
this log-linear model. The optimization criterion
is 4BLEU-TER.
4 Experimental Results
In this year?s experiments, we tried to improve the
result of the system combination further by com-
bining single systems tuned on different develop-
Table 1: Comparison of single systems tuned on
newstest2009 and newstest2010. The results are
reported on newstest2012.
single systems tuned on newstest2012
newstest BLEU TER
KIT 2009 24.6 58.4
2010 24.6 58.6
LIMSI 2009 22.5 61.5
2010 22.6 59.8
SYSTRAN 2009 20.9 63.3
2010 21.2 62.2
RWTH 2009 23.7 60.8
2010 24.4 58.8
ment sets. The idea is to achieve a more stable
performance in terms of translation quality, if the
single systems are not optimized on the same data
set. In Table 1, the results of each provided single
system tuned on newstest2009 and newstest2010
are shown. For RWTH, LIMSI and SYSTRAN,
it seems that the performance of the single system
depends on the chosen tuning set. However, the
translation quality of the single systems provided
by KIT is stable.
As initial approach and for the final submis-
sion, we grouped single systems with dissimilar
approaches. Thus, KIT (phrase-based SMT) and
SYSTRAN (rule-based MT) tuned their system on
newstest2010, while RWTH (phrase-based SMT)
and LIMSI (n-gram) optimized on newstest2009.
To compare the impact of this approach, all pos-
sible combinations were checked (Table 2). How-
ever, it seems that the translation quality can not be
improved by this approach. For the test set (new-
stest2012), BLEU is steady around 25.6 points.
Even if the single system with lowest BLEU are
combined (KIT 2010, LIMSI 2009, SYSTRAN
2010, RWTH 2009), the translation quality in
terms of BLEU is comparable with the combina-
tion of the best single systems (KIT 2009, LIMSI
2010, SYSTRAN 2010, RWTH 2010). However,
we could gain 1.0 point in TER.
Due to the fact, that for the final submission the
initial grouping was available only, we kept this
189
Table 2: Comparison of different system combination settings. For each possible combination of systems
tuned on different tuning sets, a system combination was set up, re-tuned on newstest2011 and evaluated
on newstest2012. The setting used for further experiments is set in boldface.
single systems system combinations
KIT LIMSI SYSTRAN RWTH newstest2011 newstest2012
tuned on newstest BLEU TER BLEU TER
2009 2009 2009 2009 24.6 58.0 25.6 56.8
2010 2010 2010 2010 24.2 58.1 25.6 57.7
2010 2009 2009 2009 24.5 57.9 25.7 57.4
2009 2010 2009 2009 24.4 58.3 25.7 57.0
2009 2009 2010 2009 24.5 57.9 25.6 57.0
2009 2009 2009 2010 24.5 58.0 25.6 56.8
2009 2010 2010 2010 24.1 57.5 25.4 56.4
2010 2009 2010 2010 24.3 57.6 25.6 56.9
2010 2010 2009 2010 24.2 58.0 25.6 57.3
2010 2010 2010 2009 24.3 57.9 25.5 57.6
2010 2010 2009 2009 24.4 58.1 25.6 57.5
2009 2009 2010 2010 24.4 57.8 25.5 56.6
2009 2010 2010 2009 24.4 58.2 25.5 57.0
2009 2010 2009 2010 24.2 57.8 25.5 56.8
2010 2009 2009 2010 24.4 57.9 25.6 57.4
2010 2009 2010 2009 24.4 57.7 25.6 57.4
Table 3: Results of the final submission (bold-
face) compared with best single system on new-
stest2012.
newstest2011 newstest2012
BLEU TER BLEU TER
best single 23.2 60.9 24.6 58.4
system comb. 24.4 57.7 25.6 57.4
+ IBM-1 24.6 58.1 25.6 57.6
+ bigLM 24.6 57.9 25.8 57.2
combination. To improve this baseline further, two
additional models were added. We applied lexi-
cal smoothing (IBM-1) and an additional language
model (bigLM) trained on the English side of the
parallel data and the News shuffle corpus. The re-
sults are presented in Table 3.
The baseline was slightly improved by 0.2
points in BLEU and TER. Note, this system com-
bination was the final submission.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, the partners of the QUAERO
project (Karlsruhe Institute of Technology, RWTH
Aachen University, LIMSI-CNRS and SYSTRAN
Software, Inc.) provided a joint submission. By
joining the output of four different translation sys-
tems with RWTH?s system combination, we re-
ported an improvement of up to 1.2 points in
BLEU and TER.
Combining systems optimized on different tun-
ing sets does not seem to improve the translation
quality. However, by adding additional model, the
baseline was slightly improved.
All in all, we conclude that the variability in
terms of BLEU does not influence the final result.
It seems that using different approaches of MT in
a system combination is more important (Freitag
et al, 2012).
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
190
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Josep M. Crego and Jose? B. Mario. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on systran?s rule-based trans-
lation system. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
?07, pages 220?223, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Daniel Dchelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Markus Freitag, Stephan Peitz, Matthias Huck, Her-
mann Ney, Teresa Herrmann, Jan Niehues, Alex
Waibel, Alexandre Allauzen, Gilles Adda, Bianka
Buschbeck, Josep Maria Crego, and Jean Senellart.
2012. Joint wmt 2012 submission of the quaero
project. In NAACL 2012 Seventh Workshop on Sta-
tistical Machine Translation, pages 322?329, Mon-
treal, Canada, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 847?855, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantine, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
pages 177?180, Prague, Czech Republic, June.
Alon Lavie and Abhaya Agarwal. 2007. ME-
TEOR: An Automatic Metric for MT Evaluation
with High Levels of Correlation with Human Judg-
ments. pages 228?231, Prague, Czech Republic,
June.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Saab Mansour and Hermann Ney. 2012. A sim-
ple and effective weighted phrase extraction for ma-
chine translation adaptation. In International Work-
shop on Spoken Language Translation, pages 193?
200, Hong Kong, December.
Sab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA,
December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
191
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation Systems for IWSLT
2011. In Proceedings of the Eighth Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
Translation System for the EACL-WMT 2009. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003a. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sap-
poro, Japan, July.
Franz Josef Och. 2003b. Minimum error rate training
in statistical machine translation. In ACL ?03: Proc.
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Evelyne Tzoukermann and SusanEditors Arm-
strong, editors, Proceedings of the ACL SIGDAT-
Workshop, pages 47?50. Kluwer Academic Publish-
ers.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Int. Conf. on Spo-
ken Language Processing, volume 2, pages 901?
904, Denver, Colorado, USA.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
192
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105?113,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
EU-BRIDGE MT: Combined Machine Translation
?
Markus Freitag,
?
Stephan Peitz,
?
Joern Wuebker,
?
Hermann Ney,
?
Matthias Huck,
?
Rico Sennrich,
?
Nadir Durrani,
?
Maria Nadejde,
?
Philip Williams,
?
Philipp Koehn,
?
Teresa Herrmann,
?
Eunah Cho,
?
Alex Waibel
?
RWTH Aachen University, Aachen, Germany
?
University of Edinburgh, Edinburgh, Scotland
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de
?
{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk
?
v1rsennr@staffmail.ed.ac.uk
?
maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk
?
{teresa.herrmann,eunah.cho,alex.waibel}@kit.edu
Abstract
This paper describes one of the col-
laborative efforts within EU-BRIDGE to
further advance the state of the art in
machine translation between two Euro-
pean language pairs, German?English
and English?German. Three research
institutes involved in the EU-BRIDGE
project combined their individual machine
translation systems and participated with a
joint setup in the shared translation task of
the evaluation campaign at the ACL 2014
Eighth Workshop on Statistical Machine
Translation (WMT 2014).
We combined up to nine different machine
translation engines via system combina-
tion. RWTH Aachen University, the Uni-
versity of Edinburgh, and Karlsruhe In-
stitute of Technology developed several
individual systems which serve as sys-
tem combination input. We devoted spe-
cial attention to building syntax-based sys-
tems and combining them with the phrase-
based ones. The joint setups yield em-
pirical gains of up to 1.6 points in BLEU
and 1.0 points in TER on the WMT news-
test2013 test set compared to the best sin-
gle systems.
1 Introduction
EU-BRIDGE
1
is a European research project
which is aimed at developing innovative speech
translation technology. This paper describes a
1
http://www.eu-bridge.eu
joint WMT submission of three EU-BRIDGE
project partners. RWTH Aachen University
(RWTH), the University of Edinburgh (UEDIN)
and Karlsruhe Institute of Technology (KIT) all
provided several individual systems which were
combined by means of the RWTH Aachen system
combination approach (Freitag et al., 2014). As
distinguished from our EU-BRIDGE joint submis-
sion to the IWSLT 2013 evaluation campaign (Fre-
itag et al., 2013), we particularly focused on trans-
lation of news text (instead of talks) for WMT. Be-
sides, we put an emphasis on engineering syntax-
based systems in order to combine them with our
more established phrase-based engines. We built
combined system setups for translation from Ger-
man to English as well as from English to Ger-
man. This paper gives some insight into the tech-
nology behind the system combination framework
and the combined engines which have been used
to produce the joint EU-BRIDGE submission to
the WMT 2014 translation task.
The remainder of the paper is structured as fol-
lows: We first describe the individual systems by
RWTH Aachen University (Section 2), the Uni-
versity of Edinburgh (Section 3), and Karlsruhe
Institute of Technology (Section 4). We then
present the techniques for machine translation sys-
tem combination in Section 5. Experimental re-
sults are given in Section 6. We finally conclude
the paper with Section 7.
2 RWTH Aachen University
RWTH (Peitz et al., 2014) employs both the
phrase-based (RWTH scss) and the hierarchical
(RWTH hiero) decoder implemented in RWTH?s
publicly available translation toolkit Jane (Vilar
105
et al., 2010; Wuebker et al., 2012). The model
weights of all systems have been tuned with stan-
dard Minimum Error Rate Training (Och, 2003)
on a concatenation of the newstest2011 and news-
test2012 sets. RWTH used BLEU as optimiza-
tion objective. Both for language model estima-
tion and querying at decoding, the KenLM toolkit
(Heafield et al., 2013) is used. All RWTH sys-
tems include the standard set of models provided
by Jane. Both systems have been augmented with
a hierarchical orientation model (Galley and Man-
ning, 2008; Huck et al., 2013) and a cluster lan-
guage model (Wuebker et al., 2013). The phrase-
based system (RWTH scss) has been further im-
proved by maximum expected BLEU training sim-
ilar to (He and Deng, 2012). The latter has been
performed on a selection from the News Commen-
tary, Europarl and Common Crawl corpora based
on language and translation model cross-entropies
(Mansour et al., 2011).
3 University of Edinburgh
UEDIN contributed phrase-based and syntax-
based systems to both the German?English and
the English?German joint submission.
3.1 Phrase-based Systems
UEDIN?s phrase-based systems (Durrani et al.,
2014) have been trained using the Moses toolkit
(Koehn et al., 2007), replicating the settings de-
scribed in (Durrani et al., 2013b). The features
include: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA
++
align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, a lexically-driven 5-gram
operation sequence model (OSM) (Durrani et al.,
2013a), msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al., 2012), a distortion limit of 6, a maxi-
mum phrase length of 5, 100-best translation op-
tions, Minimum Bayes Risk decoding (Kumar and
Byrne, 2004), cube pruning (Huang and Chiang,
2007), with a stack size of 1000 during tuning and
5000 during testing and the no-reordering-over-
punctuation heuristic. UEDIN uses POS and mor-
phological target sequence models built on the in-
domain subset of the parallel corpus using Kneser-
Ney smoothed 7-gram models as additional factors
in phrase translation models (Koehn and Hoang,
2007). UEDIN has furthermore built OSM mod-
els over POS and morph sequences following
Durrani et al. (2013c). The English?German
system additionally comprises a target-side LM
over automatically built word classes (Birch et
al., 2013). UEDIN has applied syntactic pre-
reordering (Collins et al., 2005) and compound
splitting (Koehn and Knight, 2003) of the source
side for the German?English system. The sys-
tems have been tuned on a very large tuning set
consisting of the test sets from 2008-2012, with
a total of 13,071 sentences. UEDIN used news-
test2013 as held-out test set. On top of UEDIN
phrase-based 1 system, UEDIN phrase-based 2
augments word classes as additional factor and
learns an interpolated target sequence model over
cluster IDs. Furthermore, it learns OSM models
over POS, morph and word classes.
3.2 Syntax-based Systems
UEDIN?s syntax-based systems (Williams et al.,
2014) follow the GHKM syntax approach as pro-
posed by Galley, Hopkins, Knight, and Marcu
(Galley et al., 2004). The open source Moses
implementation has been employed to extract
GHKM rules (Williams and Koehn, 2012). Com-
posed rules (Galley et al., 2006) are extracted in
addition to minimal rules, but only up to the fol-
lowing limits: at most twenty tree nodes per rule,
a maximum depth of five, and a maximum size of
five. Singleton hierarchical rules are dropped.
The features for the syntax-based systems com-
prise Good-Turing-smoothed phrase translation
probabilities, lexical translation probabilities in
both directions, word and phrase penalty, a rule
rareness penalty, a monolingual PCFG probability,
and a 5-gram language model. UEDIN has used
the SRILM toolkit (Stolcke, 2002) to train the lan-
guage model and relies on KenLM for language
model scoring during decoding. Model weights
are optimized to maximize BLEU. 2000 sentences
from the newstest2008-2012 sets have been se-
lected as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and each contain less than 30 words
for more rapid tuning. Decoding for the syntax-
based systems is carried out with cube pruning
using Moses? hierarchical decoder (Hoang et al.,
2009).
UEDIN?s German?English syntax-based setup
is a string-to-tree system with compound splitting
106
on the German source-language side and syntactic
annotation from the Berkeley Parser (Petrov et al.,
2006) on the English target-language side.
For English?German, UEDIN has trained var-
ious string-to-tree GHKM syntax systems which
differ with respect to the syntactic annotation. A
tree-to-string system and a string-to-string system
(with rules that are not syntactically decorated)
have been trained as well. The English?German
UEDIN GHKM system names in Table 3 denote:
UEDIN GHKM S2T (ParZu): A string-to-tree
system trained with target-side syntactic an-
notation obtained with ParZu (Sennrich et
al., 2013). It uses a modified syntactic label
set, target-side compound splitting, and addi-
tional syntactic constraints.
UEDIN GHKM S2T (BitPar): A string-to-tree
system trained with target-side syntactic
annotation obtained with BitPar (Schmid,
2004).
UEDIN GHKM S2T (Stanford): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Stan-
ford Parser (Rafferty and Manning, 2008a).
UEDIN GHKM S2T (Berkeley): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Berke-
ley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
UEDIN GHKM T2S (Berkeley): A tree-to-
string system trained with source-side syn-
tactic annotation obtained with the English
Berkeley Parser (Petrov et al., 2006).
UEDIN GHKM S2S (Berkeley): A string-to-
string system. The extraction is GHKM-
based with syntactic target-side annotation
from the German Berkeley Parser, but we
strip off the syntactic labels. The final gram-
mar contains rules with a single generic non-
terminal instead of syntactic ones, plus rules
that have been added from plain phrase-based
extraction (Huck et al., 2014).
4 Karlsruhe Institute of Technology
The KIT translations (Herrmann et al., 2014) are
generated by an in-house phrase-based transla-
tions system (Vogel, 2003). The provided News
Commentary, Europarl, and Common Crawl par-
allel corpora are used for training the translation
model. The monolingual part of those parallel
corpora, the News Shuffle corpus for both direc-
tions and additionally the Gigaword corpus for
German?English are used as monolingual train-
ing data for the different language models. Opti-
mization is done with Minimum Error Rate Train-
ing as described in (Venugopal et al., 2005), using
newstest2012 and newstest2013 as development
and test data respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side of the corpus for
German?English translation before training. In
order to improve the quality of the web-crawled
Common Crawl corpus, noisy sentence pairs are
filtered out using an SVM classifier as described
by Mediani et al. (2011).
The word alignment for German?English is
generated using the GIZA
++
toolkit (Och and Ney,
2003). For English?German, KIT uses discrimi-
native word alignment (Niehues and Vogel, 2008).
Phrase extraction and scoring is done using the
Moses toolkit (Koehn et al., 2007). Phrase pair
probabilities are computed using modified Kneser-
Ney smoothing as in (Foster et al., 2006).
In both systems KIT applies short-range re-
orderings (Rottmann and Vogel, 2007) and long-
range reorderings (Niehues and Kolss, 2009)
based on POS tags (Schmid, 1994) to perform
source sentence reordering according to the target
language word order. The long-range reordering
rules are applied to the training corpus to create
reordering lattices to extract the phrases for the
translation model. In addition, a tree-based re-
ordering model (Herrmann et al., 2013) trained
on syntactic parse trees (Rafferty and Manning,
2008b; Klein and Manning, 2003) as well as a lex-
icalized reordering model (Koehn et al., 2005) are
applied.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) and use modified Kneser-
Ney smoothing. Both systems utilize a lan-
guage model based on automatically learned
word classes using the MKCLS algorithm (Och,
1999). The English?German system comprises
language models based on fine-grained part-of-
speech tags (Schmid and Laws, 2008). In addi-
tion, a bilingual language model (Niehues et al.,
2011) is used as well as a discriminative word lex-
icon (Mauser et al., 2009) using source context to
guide the word choices in the target sentence.
107
In total, the English?German system uses the
following language models: two 4-gram word-
based language models trained on the parallel data
and the filtered Common Crawl data separately,
two 5-gram POS-based language models trained
on the same data as the word-based language mod-
els, and a 4-gram cluster-based language model
trained on 1,000 MKCLS word classes.
The German?English system uses a 4-gram
word-based language model trained on all mono-
lingual data and an additional language model
trained on automatically selected data (Moore and
Lewis, 2010). Again, a 4-gram cluster-based
language model trained on 1000 MKCLS word
classes is applied.
5 System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses which
are outputs of different translation engines. The
consensus translations can be better in terms of
translation quality than any of the individual hy-
potheses. To combine the engines of the project
partners for the EU-BRIDGE joint setups, we ap-
ply a system combination implementation that has
been developed at RWTH Aachen University.
The implementation of RWTH?s approach to
machine translation system combination is de-
scribed in (Freitag et al., 2014). This approach
includes an enhanced alignment and reordering
framework. Alignments between the system out-
puts are learned using METEOR (Banerjee and
Lavie, 2005). A confusion network is then built
using one of the hypotheses as ?primary? hypoth-
esis. We do not make a hard decision on which
of the hypotheses to use for that, but instead com-
bine all possible confusion networks into a single
lattice. Majority voting on the generated lattice
is performed using the prior probabilities for each
system as well as other statistical models, e.g. a
special n-gram language model which is learned
on the input hypotheses. Scaling factors of the
models are optimized using the Minimum Error
Rate Training algorithm. The translation with the
best total score within the lattice is selected as con-
sensus translation.
6 Results
In this section, we present our experimental results
on the two translation tasks, German?English
and English?German. The weights of the in-
dividual system engines have been optimized on
different test sets which partially or fully include
newstest2011 or newstest2012. System combina-
tion weights are either optimized on newstest2011
or newstest2012. We kept newstest2013 as an un-
seen test set which has not been used for tuning
the system combination or any of the individual
systems.
6.1 German?English
The automatic scores of all individual systems
as well as of our final system combination sub-
mission are given in Table 1. KIT, UEDIN and
RWTH are each providing one individual phrase-
based system output. RWTH (hiero) and UEDIN
(GHKM) are providing additional systems based
on the hierarchical translation model and a string-
to-tree syntax model. The pairwise difference
of the single system performances is up to 1.3
points in BLEU and 2.5 points in TER. For
German?English, our system combination pa-
rameters are optimized on newstest2012. System
combination gives us a gain of 1.6 points in BLEU
and 1.0 points in TER for newstest2013 compared
to the best single system.
In Table 2 the pairwise BLEU scores for all in-
dividual systems as well as for the system combi-
nation output are given. The pairwise BLEU score
of both RWTH systems (taking one as hypothesis
and the other one as reference) is the highest for all
pairs of individual system outputs. A high BLEU
score means similar hypotheses. The syntax-based
system of UEDIN and RWTH scss differ mostly,
which can be observed from the fact of the low-
est pairwise BLEU score. Furthermore, we can
see that better performing individual systems have
higher BLEU scores when evaluating against the
system combination output.
In Figure 1 system combination output is com-
pared to the best single system KIT. We distribute
the sentence-level BLEU scores of all sentences of
newstest2013. To allow for sentence-wise evalu-
ation, all bi-, tri-, and four-gram counts are ini-
tialized with 1 instead of 0. Many sentences have
been improved by system combination. Neverthe-
less, some sentences fall off in quality compared
to the individual system output of KIT.
6.2 English?German
The results of all English?German system setups
are given in Table 3. For the English?German
translation task, only UEDIN and KIT are con-
108
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
KIT 25.0 57.6 25.2 57.4 27.5 54.4
UEDIN 23.9 59.2 24.7 58.3 27.4 55.0
RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0
RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9
UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9
syscom 25.6 57.1 26.4 56.5 29.1 53.4
Table 1: Results for the German?English translation task. The system combination is tuned on news-
test2012, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly better than the best single system
with p < 0.05.
KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscom
KIT 59.07 57.60 57.91 55.62 77.68
UEDIN 59.17 56.96 57.84 59.89 72.89
RWTH scss 57.64 56.90 64.94 53.10 71.16
RWTH hiero 57.98 57.80 64.97 55.73 70.87
UEDIN S2T 55.75 59.95 53.19 55.82 65.35
syscom 77.76 72.83 71.17 70.85 65.24
Table 2: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as hypothesis and the other one as reference.)
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
UEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7
UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3
UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8
UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9
UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2
UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8
UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8
UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3
KIT 17.1 67.0 17.8 64.8 20.2 62.2
syscom 18.4 65.0 18.7 63.4 21.3 60.6
Table 3: Results for the English?German translation task. The system combination is tuned on news-
test2011, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than
the best single system with p< 0.05. Italic font indicates system combination results that are significantly
better than the best single system with p < 0.1.
tributing individual systems. KIT is providing a
phrase-based system output, UEDIN is providing
two phrase-based system outputs and six syntax-
based ones (GHKM). For English?German, our
system combination parameters are optimized on
newstest2011. Combining all nine different sys-
tem outputs yields an improvement of 0.5 points
in BLEU and 1.7 points in TER over the best sin-
gle system performance.
In Table 4 the cross BLEU scores for all
English?German systems are given. The individ-
ual system of KIT and the syntax-based ParZu sys-
tem of UEDIN have the lowest BLEU score when
scored against each other. Both approaches are
quite different and both are coming from differ-
ent institutes. In contrast, both phrase-based sys-
tems pbt 1 and pbt 2 from UEDIN are very sim-
ilar and hence have a high pairwise BLEU score.
109
pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscom
pbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12
pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75
ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39
BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08
Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51
S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81
T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13
S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46
KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33
syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27
Table 4: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as reference and the other one as hypothesis.)
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 1: Sentence distribution for the
German?English newstest2013 test set compar-
ing system combination output against the best
individual system.
As for the German?English translation direction,
the best performing individual system outputs are
also having the highest BLEU scores when evalu-
ated against the final system combination output.
In Figure 2 system combination output is com-
pared to the best single system pbt 2. We distribute
the sentence-level BLEU scores of all sentences
of newstest2013. Many sentences have been im-
proved by system combination. But there is still
room for improvement as some sentences are still
better in terms of sentence-level BLEU in the indi-
vidual best system pbt 2.
7 Conclusion
We achieved significantly better translation perfor-
mance with gains of up to +1.6 points in BLEU
and -1.0 points in TER by combining up to nine
different machine translation systems. Three dif-
ferent research institutes (RWTH Aachen Univer-
sity, University of Edinburgh, Karlsruhe Institute
of Technology) provided machine translation en-
gines based on different approaches like phrase-
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 2: Sentence distribution for the
English?German newstest2013 test set compar-
ing system combination output against the best
individual system.
based, hierarchical phrase-based, and syntax-
based. For English?German, we included six
different syntax-based systems, which were com-
bined to our final combined translation. The au-
tomatic scores of all submitted system outputs for
the actual 2014 evaluation set are presented on the
WMT submission page.
2
Our joint submission is
the best submission in terms of BLEU and TER for
both translation directions German?English and
English?German without adding any new data.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658.
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1 148717.
2
http://matrix.statmt.org/
110
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 65?72, Ann Arbor, MI, USA, June.
Alexandra Birch, Nadir Durrani, and Philipp Koehn.
2013. Edinburgh SLT and MT System Description
for the IWSLT 2013 Evaluation. In Proceedings
of the 10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg, Ger-
many, December.
Maximilian Bisani and Hermann Ney. 2004. Bootstrap
Estimates for Confidence Intervals in ASR Perfor-
mance Evaluation. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
volume 1, pages 409?412, Montr?eal, Canada, May.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Ma-
chine Translation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Arbor,
Michigan, June.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013a. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013b. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richard Farkas. 2013c. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburgh?s Phrase-based
Machine Translation Systems for WMT-14. In Pro-
ceedings of the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, MD, USA,
June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In EMNLP, pages 53?61.
M. Freitag, S. Peitz, J. Wuebker, H. Ney, N. Dur-
rani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues,
M. Mediani, T. Herrmann, A. Waibel, N. Bertoldi,
M. Cettolo, and M. Federico. 2013. EU-BRIDGE
MT: Text Translation of Talks in the EU-BRIDGE
Project. In International Workshop on Spoken Lan-
guage Translation, Heidelberg, Germany, Decem-
ber.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open Source Machine Translation Sys-
tem Combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, HI, USA, Octo-
ber.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of the Human Language Technology Conf.
/ North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273?280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st International Conf. on Computa-
tional Linguistics and 44th Annual Meeting of the
Assoc. for Computational Linguistics, pages 961?
968, Sydney, Australia, July.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 292?301, Jeju, Republic of Korea,
July.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, UK, July.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Atlanta, GA, USA, June.
111
Teresa Herrmann, Mohammed Mediani, Eunah Cho,
Thanh-Le Ha, Jan Niehues, Isabel Slawik, Yuqi
Zhang, and Alex Waibel. 2014. The Karlsruhe In-
stitute of Technology Translation Systems for the
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. pages 152?159, Tokyo, Japan, December.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A Phrase Orientation Model
for Hierarchical Machine Translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452?463, Sofia, Bulgaria, August.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Augmenting String-to-Tree and Tree-to-
String Translation with Non-Syntactic Phrases. In
Proceedings of the ACL 2014 Ninth Workshop on
Statistical Machine Translation, Baltimore, MD,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In EMNLP-CoNLL, pages 868?876,
Prague, Czech Republic, June.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, pages 177?180,
Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In Proc. Human Language Technol-
ogy Conf. / North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 169?176, Boston, MA, USA,
May.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, CA, USA, December.
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 210?217, Singapore, Au-
gust.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA, USA.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of Third ACL Workshop on Statisti-
cal Machine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Stephan Peitz, Joern Wuebker, Markus Freitag, and
Hermann Ney. 2014. The RWTH Aachen German-
English Machine Translation System for WMT
2014. In Proceedings of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation, Baltimore,
MD, USA, June.
112
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 433?440, Sydney,
Australia, July.
Anna N. Rafferty and Christopher D. Manning. 2008a.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Anna N. Rafferty and Christopher D. Manning. 2008b.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, UK.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, USA, Septem-
ber.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation (WMT), pages 388?394,
Montr?eal, Canada, June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh?s Syntax-Based Systems at
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statisti-
cal Machine Translation. In COLING ?12: The 24th
Int. Conf. on Computational Linguistics, pages 483?
491, Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving Statistical Machine
Translation with Word Class Models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, WA, USA, Oc-
tober.
113
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130?135,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The Karlsruhe Institute of Technology Translation Systems
for the WMT 2014
Teresa Herrmann, Mohammed Mediani, Eunah Cho, Thanh-Le Ha,
Jan Niehues, Isabel Slawik, Yuqi Zhang and Alex Waibel
Institute for Anthropomatics and Robotics
KIT - Karlsruhe Institute of Technology
firstname.lastname@kit.edu
Abstract
In this paper, we present the KIT
systems participating in the Shared
Translation Task translating between
English?German and English?French.
All translations are generated using
phrase-based translation systems, using
different kinds of word-based, part-of-
speech-based and cluster-based language
models trained on the provided data.
Additional models include bilingual lan-
guage models, reordering models based
on part-of-speech tags and syntactic parse
trees, as well as a lexicalized reordering
model. In order to make use of noisy
web-crawled data, we apply filtering
and data selection methods for language
modeling. A discriminative word lexicon
using source context information proved
beneficial for all translation directions.
1 Introduction
We describe the KIT systems for the Shared Trans-
lation Task of the ACL 2014 Ninth Workshop on
Statistical Machine Translation. We participated
in the English?German and English?French
translation directions, using a phrase-based de-
coder with lattice input.
The paper is organized as follows: the next sec-
tion describes the data used for each translation
direction. Section 3 gives a detailed description of
our systems including all the models. The trans-
lation results for all directions are presented after-
wards and we close with a conclusion.
2 Data
We utilize the provided EPPS, NC and Common
Crawl parallel corpora for English?German and
German?English, plus Giga for English?French
and French?English. The monolingual part
of those parallel corpora, the News Shuffle
corpus for all four directions and additionally
the Gigaword corpus for English?French and
German?English are used as monolingual train-
ing data for the different language models. For
optimizing the system parameters, newstest2012
and newstest2013 are used as development and
test data respectively.
3 System Description
Before training we perform a common preprocess-
ing of the raw data, which includes removing long
sentences and sentences with a length mismatch
exceeding a certain threshold. Afterwards, we nor-
malize special symbols, dates, and numbers. Then
we perform smart-casing of the first letter of every
sentence. Compound splitting (Koehn and Knight,
2003) is performed on the source side of the cor-
pus for German?English translation. In order to
improve the quality of the web-crawled Common
Crawl corpus, we filter out noisy sentence pairs us-
ing an SVM classifier for all four translation tasks
as described in Mediani et al. (2011).
Unless stated otherwise, we use 4-gram lan-
guage models (LM) with modified Kneser-Ney
smoothing, trained with the SRILM toolkit (Stol-
cke, 2002). All translations are generated by
an in-house phrase-based translation system (Vo-
gel, 2003), and we use Minimum Error Rate
Training (MERT) as described in Venugopal et
al. (2005) for optimization. The word align-
ment of the parallel corpora is generated using
the GIZA++ Toolkit (Och and Ney, 2003) for
both directions. Afterwards, the alignments are
combined using the grow-diag-final-and heuris-
tic. For English?German, we use discrimi-
native word alignment trained on hand-aligned
data as described in Niehues and Vogel (2008).
The phrase table (PT) is built using the Moses
toolkit (Koehn et al., 2007). The phrase scoring
for the small data sets (German?English) is also
130
done by the Moses toolkit, whereas the bigger sets
(French?English) are scored by our in-house par-
allel phrase scorer (Mediani et al., 2012a). The
phrase pair probabilities are computed using mod-
ified Kneser-Ney smoothing as described in Foster
et al. (2006).
Since German is a highly inflected language,
we try to alleviate the out-of-vocabulary prob-
lem through quasi-morphological operations that
change the lexical entry of a known word form to
an unknown word form as described in Niehues
and Waibel (2011).
3.1 Word Reordering Models
We apply automatically learned reordering rules
based on part-of-speech (POS) sequences and syn-
tactic parse tree constituents to perform source
sentence reordering according to the target lan-
guage word order. The rules are learned
from a parallel corpus with POS tags (Schmid,
1994) for the source side and a word align-
ment to learn reordering rules that cover short
range (Rottmann and Vogel, 2007) and long
range reorderings (Niehues and Kolss, 2009).
In addition, we apply a tree-based reordering
model (Herrmann et al., 2013) to better address
the differences in word order between German and
English. Here, a word alignment and syntactic
parse trees (Rafferty and Manning, 2008; Klein
and Manning, 2003) for the source side of the
training corpus are required to learn rules on how
to reorder the constituents in the source sentence.
The POS-based and tree-based reordering rules
are applied to each input sentence before transla-
tion. The resulting reordered sentence variants as
well as the original sentence are encoded in a re-
ordering lattice. The lattice, which also includes
the original position of each word, is used as input
to the decoder.
In order to acquire phrase pairs matching the
reordered sentence variants, we perform lattice
phrase extraction (LPE) on the training corpus
where phrase are extracted from the reordered
word lattices instead of the original sentences.
In addition, we use a lexicalized reordering
model (Koehn et al., 2005) which stores reorder-
ing probabilities for each phrase pair. During
decoding the lexicalized reordering model deter-
mines the reordering orientation of each phrase
pair at the phrase boundaries. The probability for
the respective orientation with respect to the orig-
inal position of the words is included as an addi-
tional score in the log-linear model of the transla-
tion system.
3.2 Adaptation
In the French?English and English?French sys-
tems, we perform adaptation for translation mod-
els as well as for language models. The EPPS and
NC corpora are used as in-domain data for the di-
rection English?French, while NC corpus is the
in-domain data for French?English.
Two phrase tables are built: one is the out-
of-domain phrase table, which is trained on all
corpora; the other is the in-domain phrase table,
which is trained on in-domain data. We adapt the
translation model by using the scores from the two
phrase tables with the backoff approach described
in Niehues and Waibel (2012). This results in a
phrase table with six scores, the four scores from
the general phrase table as well as the two condi-
tional probabilities from the in-domain phrase ta-
ble. In addition, we take the union of the candidate
phrase pairs collected from both phrase tables A
detailed description of the union method can be
found in Mediani et al. (2012b).
The language model is adapted by log-linearly
combining the general language model and an in-
domain language model. We train a separate lan-
guage model using only the in-domain data. Then
it is used as an additional language model during
decoding. Optimal weights are set during tuning
by MERT.
3.3 Special Language Models
In addition to word-based language models, we
use different types of non-word language models
for each of the systems. With the help of a bilin-
gual language model (Niehues et al., 2011) we
are able to increase the bilingual context between
source and target words beyond phrase bound-
aries. This language model is trained on bilin-
gual tokens created from a target word and all its
aligned source words. The tokens are ordered ac-
cording to the target language word order.
Furthermore, we use language models based
on fine-grained part-of-speech tags (Schmid and
Laws, 2008) as well as word classes to allevi-
ate the sparsity problem for surface words. The
word classes are automatically learned by clus-
tering the words of the corpus using the MKCLS
algorithm (Och, 1999). These n-gram language
models are trained on the target language corpus,
131
where the words have been replaced either by their
corresponding POS tag or cluster ID. During de-
coding, these language models are used as addi-
tional models in the log-linear combination.
The data selection language model is trained
on data automatically selected using cross-entropy
differences between development sets from pre-
vious WMT workshops and the noisy crawled
data (Moore and Lewis, 2010). We selected the
top 10M sentences to train this language model.
3.4 Discriminative Word Lexicon
A discriminative word lexicon (DWL) models the
probability of a target word appearing in the trans-
lation given the words of the source sentence.
DWLs were first introduced by Mauser et al.
(2009). For every target word, they train a maxi-
mum entropy model to determine whether this tar-
get word should be in the translated sentence or
not using one feature per source word.
We use two simplifications of this model that
have shown beneficial to translation quality and
training time in the past (Mediani et al., 2011).
Firstly, we calculate the score for every phrase pair
before translating. Secondly, we restrict the nega-
tive training examples to words that occur within
matching phrase pairs.
In this evaluation, we extended the DWL
with n-gram source context features proposed
by Niehues and Waibel (2013). Instead of rep-
resenting the source sentence as a bag-of-words,
we model it as a bag-of-n-grams. This allows us
to include information about source word order in
the model. We used one feature per n-gram up to
the order of three and applied count filtering for
bigrams and trigrams.
4 Results
This section presents the participating systems
used for the submissions in the four translation
directions of the evaluation. We describe the in-
dividual components that form part of each of
the systems and report the translation qualities
achieved during system development. The scores
are reported in case-sensitive BLEU (Papineni et
al., 2002).
4.1 English-French
The development of our English?French system
is shown in Table 1.
It is noteworthy that, for this direction, we chose
to tune on a subset of 1,000 pairs from news-
test2012, due to the long time the whole set takes
to be decoded. In a preliminary set of experiments
(not reported here), we found no significant differ-
ences between tuning on the small or the big devel-
opment sets. The translation model of the baseline
system is trained on the whole parallel data after
filtering (EPPS, NC, Common Crawl, Giga). The
same data was also used for language modeling.
We also use POS-based reordering.
The biggest improvement was due to using two
additional language models. One consists of a log-
linear interpolation of individual language models
trained on the target side of the parallel data, the
News shuffle, Gigaword and NC corpora. In ad-
dition, an in-domain language model trained only
on NC data is used. This improves the score by
more than 1.4 points. Adaptation of the translation
model towards a smaller model trained on EPPS
and NC brings an additional 0.3 points.
Another 0.3 BLEU points could be gained by
using other special language models: a bilingual
language model together with a 4-gram cluster
language model (trained on all monolingual data
using the MKCLS tool and 500 clusters). Incor-
porating a lexicalized reordering model into the
system had a very noticeable effect on test namely
more than half a BLEU point.
Finally, using a discriminative word lexicon
with source context has a very small positive ef-
fect on the test score, however more than 0.3 on
dev. This final configuration was the basis of our
submitted official translation.
System Dev Test
Baseline 15.63 27.61
+ Big LMs 16.56 29.02
+ PT Adaptation 16.77 29.32
+ Bilingual + Cluster LM 16.87 29.64
+ Lexicalized Reordering 16.92 30.17
+ Source DWL 17.28 30.19
Table 1: Experiments for English?French
4.2 French-English
Several experiments were conducted for the
French?English translation system. They are
summarized in Table 2.
The baseline system is essentially a phrase-
based translation system with some preprocess-
132
ing steps on the source side and utilizing the
short-range POS-based reordering on all parallel
data and fine-grained monolingual corpora such as
EPPS and NC.
Adapting the translation model using a small in-
domain phrase table trained on NC data only helps
us gain more than 0.4 BLEU points.
Using non-word language models including a
bilingual language model and a 4-gram 50-cluster
language model trained on the whole parallel data
attains 0.24 BLEU points on the test set.
Lexicalized reordering improves our system on
the development set by 0.3 BLEU points but has
less effect on the test set with a minor improve-
ment of around 0.1 BLEU points.
We achieve our best system, which is used for
the evaluation, by adding a DWL with source con-
text yielding 31.54 BLEU points on the test set.
System Dev Test
Baseline 30.16 30.70
+ LM Adaptation 30.58 30.94
+ PT Adaptation 30.69 31.14
+ Bilingual + Cluster LM 30.85 31.38
+ Lexicalized Reordering 31.14 31.46
+ Source DWL 31.19 31.54
Table 2: Experiments for French?English
4.3 English-German
Table 3 presents how the English-German transla-
tion system is improved step by step.
In the baseline system, we used parallel data
which consists of the EPPS and NC corpora. The
phrase table is built using discriminative word
alignment. For word reordering, we use word lat-
tices with long range reordering rules. Five lan-
guage models are used in the baseline system; two
word-based language models, a bilingual language
model, and two 9-gram POS-based language mod-
els. The two word-based language models use 4-
gram context and are trained on the parallel data
and the filtered Common Crawl data separately,
while the bilingual language model is built only
on the Common Crawl corpus. The two POS-
based language models are also based on the paral-
lel data and the filtered crawled data, respectively.
When using a 9-gram cluster language model,
we get a slight improvement. The cluster is trained
with 1,000 classes using EPPS, NC, and Common
Crawl data.
We use the filtered crawled data in addition to
the parallel data in order to build the phrase table;
this gave us 1 BLEU point of improvement.
The system is improved by 0.1 BLEU points
when we use lattice phrase extraction along with
lexicalized reordering rules.
Tree-based reordering rules improved the sys-
tem performance further by another 0.1 BLEU
points.
By reducing the context of the two POS-based
language models from 9-grams to 5-grams and
shortening the context of the language model
trained on word classes to 4-grams, the score on
the development set hardly changes but we can see
a slightly improvement for the test case.
Finally, we use the DWL with source context
and build a big bilingual language model using
both the crawled and parallel data. By doing so,
we improved the translation performance by an-
other 0.3 BLEU points. This system was used for
the translation of the official test set.
System Dev Test
Baseline 16.64 18.60
+ Cluster LM 16.76 18.66
+ Common Crawl Data 17.27 19.66
+ LPE + Lexicalized Reordering 17.45 19.75
+ Tree Rules 17.53 19.85
+ Shorter n-grams 17.55 19.92
+ Source DWL + Big BiLM 17.82 20.21
Table 3: Experiments for English?German
4.4 German-English
Table 4 shows the development steps of the
German-English translation system.
For the baseline system, the training data of the
translation model consists of EPPS, NC and the
filtered parallel crawled data. The phrase table
is built using GIZA++ word alignment and lattice
phrase extraction. All language models are trained
with SRILM and scored in the decoding process
with KenLM (Heafield, 2011). We use word lat-
tices generated by short and long range reordering
rules as input to the decoder. In addition, a bilin-
gual language model and a target language model
trained on word clusters with 1,000 classes are in-
cluded in the system.
Enhancing the word reordering with tree-based
reordering rules and a lexicalized reordering
133
model improved the system performance by 0.6
BLEU points.
Adding a language model trained on selected
data from the monolingual corpora gave another
small improvement.
The DWL with source context increased the
score on the test set by another 0.5 BLEU points
and applying morphological operations to un-
known words reduced the out-of-vocabulary rate,
even though no improvement in BLEU can be ob-
served. This system was used to generate the
translation submitted to the evaluation.
System Dev Test
Baseline 24.40 26.34
+ Tree Rules 24.71 26.86
+ Lexicalized Reordering 24.89 26.93
+ LM Data Selection 24.96 27.03
+ Source DWL 25.32 27.53
+ Morphological Operations - 27.53
Table 4: Experiments for German?English
5 Conclusion
In this paper, we have described the systems
developed for our participation in the Shared
Translation Task of the WMT 2014 evaluation
for English?German and English?French. All
translations were generated using a phrase-based
translation system which was extended by addi-
tional models such as bilingual and fine-grained
part-of-speech language models. Discriminative
word lexica with source context proved beneficial
in all four language directions.
For English-French translation using a smaller
development set performed reasonably well and
reduced development time. The most noticeable
gain comes from log-linear interpolation of multi-
ple language models.
Due to the large amounts and diversity of
the data available for French-English, adapta-
tion methods and non-word language models con-
tribute the major improvements to the system.
For English-German translation, the crawled
data and a DWL using source context to guide
word choice brought most of the improvements.
Enhanced word reordering models, namely
tree-based reordering rules and a lexicalized re-
ordering model as well as the source-side fea-
tures for the discriminative word lexicon helped
improve the system performance for German-
English translation.
In average we achieved an improvement of over
1.5 BLEU over the respective baselines for all our
systems.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
?
287658.
References
George F. Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proceedings of the 2006 Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP), Sydney, Australia.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
Edinburgh, Scotland, United Kingdom.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL 2003), Sapporo, Japan.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of the Eleventh Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2003), Budapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the Second International Workshop
on Spoken Language Translation (IWSLT 2005),
Pittsburgh, PA, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL
2007), Demonstration Session, Prague, Czech Re-
public.
134
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models.
In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Suntec, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight International
Workshop on Spoken Language Translation (IWSLT
2011), San Francisco, CA, USA.
Mohammed Mediani, Jan Niehues, and Alex Waibel.
2012a. Parallel Phrase Scoring for Extra-large Cor-
pora. In The Prague Bulletin of Mathematical Lin-
guistics, number 98.
Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan
Niehues, Eunach Cho, Teresa Herrmann, Rainer
K?argel, and Alexander Waibel. 2012b. The KIT
Translation Systems for IWSLT 2012. In Proceed-
ings of the Ninth International Workshop on Spoken
Language Translation (IWSLT 2012), Hong Kong,
HK.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In Proceedings
of the ACL 2010 Conference Short Papers, Uppsala,
Sweden.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation (WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation (WMT 2008), Columbus, OH,
USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In
Proceedings of the Eight International Workshop on
Spoken Language Translation (IWSLT 2008), San
Francisco, CA, USA.
J. Niehues and A. Waibel. 2012. Detailed Analysis of
Different Strategies for Phrase Table Adaptation in
SMT. In Proceedings of the Tenth Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA 2012), San Diego, CA, USA.
J. Niehues and A. Waibel. 2013. An MT Error-Driven
Discriminative Word Lexicon using Sentence Struc-
ture Features. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, Scotland, United King-
dom.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for Deter-
mining Bilingual Word Classes. In Proceedings of
the Ninth Conference of the European Chapter of the
Association for Computational Linguistics (EACL
1999), Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176 (W0109-022), IBM Research Divi-
sion, T. J. Watson Research Center.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German, Columbus, OH,
USA.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of the
11th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI
2007), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In In-
ternational Conference on Computational Linguis-
tics (COLING 2008), Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, United Kingdom.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, USA.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, Ann Arbor, Michigan, USA.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
135
