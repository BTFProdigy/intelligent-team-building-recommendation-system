Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 571?578, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Cost-Benefit Analysis of Hybrid Phone-Manner Representations for ASR
Eric Fosler-Lussier
Department of Computer Science and Engineering
Department of Linguistics
Ohio State University
Columbus, OH 43210
fosler@cse.ohio-state.edu
C. Anton Rytting
Department of Linguistics
Ohio State University
Columbus, OH 43210
rytting@ling.ohio-state.edu
Abstract
In the past decade, several researchers
have started reinvestigating the use of
sub-phonetic models for lexical represen-
tations within automatic speech recogni-
tion systems. Lest history repeat itself,
it may be instructive to mine the further
past for models of lexical representations
in the lexical access literature. In this
work, we re-evaluate the model of Briscoe
(1989), in which a hybrid strategy of lex-
ical representation between phones and
manner classes is promoted. While many
of Briscoe?s assumptions do not match up
with current ASR processing models, we
show that his conclusions are essentially
correct, and that reconsidering this struc-
ture for ASR lexica is an appropriate av-
enue for future ASR research.
1 Introduction
Almost every state-of-the-art large vocabulary au-
tomatic speech recognition (ASR) system requires
the sharing of sub-word units in order to achieve
the desired vocabulary coverage. Traditionally,
these sub-word units are determined by the phones
or phonemes of a language (depending on desired
detail of representation). However, phonetic (or
phonemic) representation has its pitfalls (cf. (Os-
tendorf, 1999)). Among the problems cited in
the literature are that (1) segments are often dif-
ficult for machines to recognize from the acoustic
cues alone, because the acoustic cues to a particu-
lar phoneme are multi-faceted, and (2) the intended
words and phrases are not always recoverable even
from correctly recognized segments, because speak-
ers themselves will also fail to articulate words with
the dictionary-listed phonemes. The first of these
problems refers to the discriminability of phonemes
within an inventory; the second to the reliability of
(actual) phone sequences mapping to the canonical
phonemic representations of words. This is partic-
ularly true in conversational speech (such as that
found in the Switchboard corpus), where pragmatic
context and conversational conventions assist human
comprehension (but not current ASR systems).
A common approach for handling pronunciation
variation is to introduce alternative entries into the
lexicon. However, phones that are perceived as non-
canonical (for example, when an /eh/ is heard as
an /ih/ by linguistic transcribers) often are closer
in acoustic space to the Gaussian means of the
canonical phones, rather than the perceived phones
(Sarac?lar et al, 2000). This insight suggests that
acoustic models need to be cognizant of potential
pronunciation changes. Thus the lexical and acous-
tic models should work hand in hand.
Another way to model this type of pronunciation
variation is to find the commonalities that the canon-
ical and perceived phone share in terms of a sub-
phonetic representation. In the past decade, a signif-
icant community in acoustic-phonetic ASR research
has been turning to distinctive features (Jakobson et
al., 1952) for building ASR lexica. While an ex-
haustive description of these approaches is beyond
the scope of this paper, estimates of phonological
feature probabilities have been combined to obtain
phone probabilities (Kirchhoff, 1998), or incorpo-
rated into ?feature bundles? that allow representa-
571
tion of phonological processes (Finke et al, 1999).
More recent work has integrated phonological
features into graphical models (Livescu et al, 2003)
and landmark based systems (Juneja and Espy-
Wilson, 2004). The common thread among this
research is the notion that acoustic models should
be sensitive to sub-phonetic information. With this
trend in phonological representation research, it is
time to re-examine some older hypotheses about lex-
ical access and speech processing in order to gain
some insight in this current featural renaissance.
Sub-phonetic ASR research is also driven by the
fact that deviations from canonical pronunciation
and from correct perception of phones is far from
random; indeed, there have been a number of stud-
ies demonstrating that both of these variations have
defined, modelable trends. Deviations from canon-
ical pronunciation can be described by phonologi-
cal rules, and errors in perception also tend to con-
form to phonological patterns. By and large, con-
fusions occur (at least in humans) between phones
with phonological features in common (e.g., (Miller
and Nicely, 1955)). In particular, three features
(voicing, manner, and place) have been postulated as
relatively invariant (see e.g., (Stevens, 1981), quoted
in (Church, 1987)). It follows from this phonetic de-
tection based on the most reliable features may han-
dle highly variable speech more robustly than sys-
tems which demand full identity over all the features
for a given phone or phone sequence.
Consequently, a number of researchers have pre-
viously suggested using certain broad classes of seg-
ments, rather than full phonemic identification, for a
first pass on recognition. For instance, Shipman and
Zue (1982), working on large-vocabulary isolated
word recognition, used both two-way consonant-
vowel distinctions and a six-way distinction based
on manner in order to divide their 20,000-word dic-
tionary into ?cohorts? or groups of words. They
found that this partial specification of segments re-
duced the search space of word candidates signifi-
cantly. Carlson et al (1985) found similar results
for English and four other languages.
2 A suggested compromise: a hybrid
phone-manner representation
Briscoe (1989) extended this broad-class approach
to address the problem of lexical access on con-
nected speech. However, Briscoe argues against the
use of broad, manner-based classes at all times. He
argues that manner cues provide no particular ad-
vantage for stressed syllables, but that all cues are
sufficiently reliable in stressed syllables to justify
a full segmental analysis. Working with a 30,000-
word lexicon, Briscoe shows that the manner-based
broad classes for weak (reduced) syllables, together
with full identification of strong (unreduced) sylla-
bles constrained the set of possible candidates satis-
factorily. Unfortunately, he only provides results for
one sentence from his corpus.
This approach proposes to adjust the granularity
of recognition dynamically, depending on the stress
level of the current syllable. The details of how this
would be managed are left somewhat vague. As it
stands, it would seem to depend crucially on first de-
tecting the stress of each frame, so as to determine
which alphabet of symbols to apply to incoming in-
put. Alternatively, it could recognize the broad class
as a first pass, and then refine this into a full phone-
mic analysis for stressed syllables in a second pass,
at the cost of multiplying passes through the speech
data. It is not possible in this system to recover from
the miscategorization of stress.
One possible remedy is to bypass a hard decision
on stress and run both a manner-based broad-class
detector and a traditional phonemic system in paral-
lel. These then may be combined according to the
probability of lexical stress, such that those frames
judged less likely to be stressed weight the broad-
class analysis more heavily, and those judged more
likely to be stressed weight the narrow phonemic
analysis more heavily. Its advantage is that a full
phonemic analysis is recoverable for each frame and
phone, but those in weak syllables (and hence less
likely to be accurate) weigh in less heavily.
Briscoe?s analysis is in terms of lexical access ac-
tivations: taking a cue from the lexical access com-
munity, he assumes that any ?partially activated?
word (e.g., ?boat? and ?both? being active after pro-
cessing ?bo?) will contribute linearly to the process-
ing time in ASR. However, most large-vocabulary
ASR systems today use a tree-based lexicon where
common phonetic prefixes of words are processed
only once, thus invalidating this conjecture. Briscoe
experimented with several triggers for starting a new
word ? at every phone, at the beginnings of sylla-
bles, at the beginnings of syllables with unreduced
vowels, and at the beginnings of word boundaries.
572
Category Example (Vietnamese) Cohorts
# of Words 6247
Stress pattern only
(lower bound) 0001 84
Identify phones
in stressed syllables 0001:m iy z 4709
+CV pattern of
unstressed syllables 0001:CV:VC:CV:m iy z 5609
+manner pattern of
unstressed syllables 0001:FV:CS:NV:m iy z 6076
Phonetic prons.
(upper bound) v iy . eh t . n aa . m iy z 6152
Table 1: Cohorts for varied lexical representations
However, the latter three require oracle information
as to where word or syllable boundaries can occur. A
more appropriate measure commensurate with cur-
rent ASR practice would be to only allow words to
start where a previous word hypothesis ends.
In the remainder of the paper, we seek to validate
(or invalidate) Briscoe?s claim that a hybrid phonetic
and feature model is appropriate for ASR process-
ing. In the 15 years since Briscoe?s paper, the ASR
community has developed large phonetically tran-
scribed corpora and more advanced computational
tools (such as the AT&T Finite-State Toolkit (Mohri
et al, 2001)) that we can apply to this problem.
3 Experiment 1: Effective Partitioning by
Manner-based Broad Classes
Our first experiment explores various types of broad
classes to determine the effects of these encodings
on cohort size within a sample 6,000 word dictio-
nary.1 Here we use the lexical stress-marked dictio-
nary provided with the TIMIT database (Garofolo
et al, 1993), which was syllabified using the NIST
Tsylb2 syllabifier (Fisher, 1996).
Rather than calculate cohort size directly, we cal-
culate the number of cohorts into which our dictio-
nary is partitioned, a measure which Carlson et al
(1985) showed to correlate well with expected co-
hort size. (Note that this is an inverse correlation.)
This describes the static discriminability of the lexi-
con: systems that have words with the same lexical
representation will not be able to discriminate be-
tween these two words acoustically and must rely on
the language model to discriminate between them.
1
?Cohort size? is used here (as with Shipman and Zue
(1982)) to mean the number of distinct vocabulary items that
match a particular broad-class encoding. It is not intended to
imply a particular theory of lexical access.
Before proceeding, it may useful to set upper and
lower bounds for this exercise (Table 1). An obvi-
ous upper bound is the full phonemic disambigua-
tion of every word. Of the 6247 words in the dictio-
nary, 6152 unique pronunciations are found (a few
cohorts consisting of sets of homophones). A con-
venient lower bound is the lexical stress pattern of
the word, devoid of any segmental information: e.g.,
?unidirectional? has its stress on the 4th of 6 sylla-
bles; hence, 000100 is its lexical-stress profile. 84
unique lexical-stress profiles exist in the dictionary.
Between these two bounds, three variant broad-
class partitions were explored for isolated word
recognition. All three use the lower-bound stress
profile as a starting place, combined with full phone-
mic information for the syllable with primary stress.
The first, with no additional segmental information,
produces 4709 distinct cohorts. The second adds a
consonant-vowel (CV) profile for the unstressed syl-
lables, which boosts the number of distinct cohorts
to 5609. The final partition replaces the CV pro-
file with a six-class manner-based broad-class par-
tition (Nasals, Stops, Fricatives, Glides, Liquids,
and Vowels). Including a manner-class representa-
tion for unstressed vowels increases the number of
cohorts to 6076, which is very close to the upper
bound. Thus, there is not much loss of lexical dis-
criminability when using this type of representation.
3.1 Caveats
Now, for this scheme to be maximally useful for
recognition, several conditions must obtain. First,
we have assumed that we can reliably detect lex-
ically stressed syllables within the speech signal.
Waibel (Waibel, 1988) has shown that stress cor-
relates with various acoustic cues such as spectral
change. As a side experiment, we have shown
that very basic methods provide encouraging re-
sults (only sketched here due to space constraints).
We re-annotated TIMIT with lexical stress mark-
ings, where all frames of each stressed syllable (in-
cluding onset and coda consonants, not just the nu-
cleus) were marked as stressed. A multi-layer per-
ceptron with 100 hidden units was trained to pre-
dict P (Stress|Acoustics) with a nine-frame context
window. No additional phonetic information be-
sides the binary label stressed/unstressed was used
in training. Frame-by-frame results on the TIMIT
test set were 75% accurate (chance: 52%), and when
573
MLP output was greater than 0.9, a precision of 89%
was obtained (recall: 20%). While far from perfect,
this result strongly suggests that even very simple
methods can predict lexical stress fairly reasonably.
A second assumption in the above analysis was
that words occur in isolation. It is clear that in con-
nected speech, there are a larger number of poten-
tial lexical confusions. A third assumption is that
those features we are relying upon in our partitions
(namely, all features within stressed syllables, and
manner of articulation for unstressed syllables) are
perfectly reliable and discriminable. In the next two
sections, we relax these assumptions by applying ex-
tensions of this method to connected speech.
4 Experiment 2: What does a hybrid
representation buy you?
As Experiment 1 shows, the hybrid phone/feature
representation does not drastically decrease the dis-
criminability of the (albeit small) lexicon. It is also
possible that such a representation reduces pronun-
ciation variation, by allowing the canonical repre-
sentation to more closely match actual pronuncia-
tions. For example, we have demonstrated that for
common ASR corpora (Switchboard and TIMIT),
segments in unstressed syllables were much more
likely to deviate from their canonical lexical rep-
resentation (Fosler-Lussier et al, 1999). If phones
that deviate from canonical still keep the same man-
ner class, then a dictionary built with Briscoe-esque
representations should more closely match the ac-
tual pronunciations of words in running speech (as
transcribed by a phonetician).
4.1 Method
In order to test this theory, we used phonetic data
from (Fosler-Lussier et al, 1999) in which the
ICSI phonetic transcripts of the Switchboard corpus
(Greenberg et al, 1996; NIST, 1992) were aligned to
a syllabified version of the Pronlex dictionary (Lin-
guistic Data Consortium (LDC), 1996), which has
71014 entries for 66293 words. In this alignment,
for every canonical phone given by the lexicon, there
were zero or more corresponding realized phones.
From these data we extracted the canonical and real-
ized pronunciation of each word token, for a total of
38,527 tokens. Generally, high-frequency function
words show the most variation, so they may benefit
most from a manner-based representation.
Lexicon type Strict Matching
matching w/ deletion
1) Phonetic units 37.0% 50.1%
2) Manner-based function words 50.2% 69.6%
3) + Manner for unstressed syls 53.4% 74.6%
4) + Manner for secondary stress 55.7% 77.9%
5) Manner for all syls 60.7% 85.2%
Table 2: Percent of words pronounced canonically
for phonetic and hybrid lexical representations
Given these word pronunciation data, we can ex-
amine how many word tokens have transcriptions
that match their dictionary-listed pronunciations,
given the broad-class mappings for various sets of
syllables. We built lexica and mapped phonetic tran-
scriptions according to five different criteria:
1. Every segment is phone based (no classes).
2. Function words use manner-based classes.
3. Unstressed syllables and function words use
manner only.
4. Secondary stressed syllables also use manner.
(Primary stressed syllables are phone based.)
5. Every segment uses manner-based classes.
We noted in the data (as others have done) that a
large proportion of the pronunciation variation was
due to phone deletion (29% of words) ? which
would not be handled by the manner-based lexicon.
However, it is likely that not every phone deletion
leads to an ASR error (as attested by the fact that
state-of-the-art Switchboard ASR error rates are typ-
ically less than 29%). Often there is enough residual
phonetic evidence of the deleted phone, or enough
phonetic evidence in other parts of the word, to rec-
ognize a word correctly despite the deletion. Thus,
we decided to use a two-part strategy in calculating
canonical pronunciation (Table 2). The first column,
?strict matching?, allows no insertions or deletions
when comparing the canonical and realized pronun-
ciation. ?Matching with deletion? reports the ideal
situation where phone deletions were perfectly re-
coverable in their canonical form. Including and ig-
noring deletions provides upper and lower bounds
on the true lexical access results. (Insertions are rel-
atively rare and not anticipated to affect the results
significantly, and hence are not examined.)
4.2 Results and Discussion
In Table 2, we see that a standard ASR lexicon ap-
proach (strict matching 1), does not match the tran-
574
scribed data very well, with only 37% of words
pronounced according to the dictionary. The strict
matching hybrid scenario on line 3 most closely re-
sembles Briscoe?s experiment, and shows a marked
improvement in matching the dictionary and real-
ized pronunciations; comparing the two, we see that
using manner-based broad classes reduces mismatch
by 25% of the total error (from 63% error to 47%),
most of which comes from improved modeling of
function words (line 2). Whether this gain in repre-
sentation is worthwhile will depend of course on the
cost in terms of the increased hypothesis space.
By allowing for perfect deletion recovery (which
will of necessity entail another large expansion of
the hypothesis space), a somewhat more optimistic
is obtained. Comparing the ?matching with dele-
tion? columns of lines 1 and 3, we see that a little
over half of the non-deletion pronunciation variation
is due to manner changes in unstressed syllables.
Again, a good chunk of this is in function words.
By moving to manner class for stressed syllables as
well would bring the hypothetical error from 25% to
15%, but at the cost of a huge explosion in the hy-
pothesis space (as Briscoe rightly points out and as
discussed in the next section).
One interesting implication of this data is that
over all types of segments (stressed and unstressed),
roughly three-quarters of word pronunciation vari-
ants differ from the canonical only in terms of
within-manner variation and phonetic deletion.
The moral of this story is that manner-based broad
classes may be a useful type of back off from truly
reduced and variable syllables (particularly func-
tion words), but the full benefit of such a maneuver
would only be realized after a reasonable solution
for recovering large-scale deletions is found. This
may come from predicting with increased specificity
where deletions are likely to occur (e.g., complex
codas), and what reduced realizations (e.g., of func-
tion words) are most common.
5 Experiment 3: What is the cost of a
hybrid representation?
Briscoe measured the cost of hybrid representation
in terms of the number of lexical activations that
a partially-completed word creates (see Section 2).
Yet Briscoe?s methodology has several shortcom-
ings when applied to today?s ASR technology; a
summary of the arguments presented above are: (1)
Tree-based lexica now share processing for words
with identical prefixes. (2) New words are acti-
vated only when other word hypotheses end. (3) We
now have a large amount of phonetically transcribed,
spontaneous speech. (4) Perfect stress detection is
not really achievable.
Given criticism 1, a better measure of potential
processing requirements is to generate a lattice of
hypothesized words and count the number of arcs in
the lattice. This lattice can be constructed in such
a way that criticism 2 is satisfied. In the next sec-
tion, we present a finite state machine formalism for
generating such a lattice.
We apply this technique to the phonetic transcrip-
tion of the Switchboard corpus (thus alleviating crit-
icism 3). However, this introduces several problems.
As Experiment 2 shows, many words have pronun-
ciations that do not appear in the dictionary. Thus,
we must find a way to alleviate the mismatch be-
tween the phonetic transcription and the dictionary
in a way that is plausible for ASR processing.
We can address criticism 4 by creating phone-
based and manner-based transcriptions that will run
in parallel; thus, the lattice generator would be
free to choose whichever representation allows the
matching to a dictionary word.
5.1 Method
In this experiment we consider a finite-state trans-
ducer model of the strategy described above. This
corresponds not to the ASR system as a whole, but
rather to the pronunciation model of a traditional
system. We assume that the pronunciation as given
by the transcriber is correct, but we model the trans-
formation of realized phones into canonical dictio-
nary pronunciations. Since we are only investigating
the combined acoustic-phonetic-lexical representa-
tion, we have left out the re-weighting and prun-
ing of hypotheses due to integration of a language
model, discourse model, or any other constraints.
Specifically, this model consists of three finite
state transducers composed. The first FSM, R,
encodes the representation of the realized phonetic
transcription of the spoken corpus. In order to match
this to dictionary pronunciations, we train a confu-
sion matrix on all realized/canonical phone pairs, to
obtain P (dictionary phone|transcribed phone);
these confusion probabilities are encoded as a finite
state transducer C. Thus, C is derived by computing
575
the strength of all correspondences between the
phonetic transcription of what was actually said
at the phone level and the canonical pronuncia-
tion of the corresponding words. This confusion
matrix consists of three parts, corresponding to
substitutions, insertions, and deletions.
1. Pairwise substitutions are counted to yield a
standard confusion matrix.
2. Where two or more realized phones correspond
to a single canonical phone (a rare occurrence,
as in e.g., really /r iy l iy/ ? [r ih ax l iy]), each
realized phone is allowed (independently) to be
either deleted or substituted with its pairwise
confusions from (1).
3. Deleted phones are assumed to be potentially
recoverable (as in Experiment 2), so both an
epsilon transition and the canonical pronunci-
ation are preserved in the confusion matrix.
In each of these confusion matrices, we have al-
ways preserved the pathway from each realized ut-
terance to its canonical representation for the whole
corpus. So for this seen corpus, it is always possi-
ble in theory to recover the canonical representation,
such that the right answer is always one of the pos-
sible hypotheses. While this may seem a bit strange,
here we can only overestimate the potential hypoth-
esis space (by adding the correct string and by as-
suming that deletions are recoverable); the point of
this exercise is to see the number of total hypotheses
(the search space) generated under such a system.
The third transducer, D, is the ASR dictionary that
we wish to test. Thus, composingR?C?Dwill give
the graph of all potential complete hypotheses in this
space. Figure 1 shows a pruned hypothesis graph for
the phrase ?it?s really sad? (the full hypothesis graph
has 12216 arcs).
5.2 Results and Discussion
By choosing different sub-word representations, we
can test Briscoe?s contention that backing off to
manner-based broad classes for certain (e.g., un-
stressed) syllables will reduce the search space
and/or facilitate recovery of the intended word
string. When a phone is substituted with a manner
class, we construct C so that the generated confu-
sions are over manner classes rather than phones.
0
1
at
it
out
2
it?s
sh
3
sri
re
rea
rhee
4
rio
5
real
reel
rial
6
really
riely
<PHDEL>
a
are
i
oh
uh
are
or
our
we
were
yeah
you
7
was
with
a
are
i
oh
uh
sh
8
sad
ad
add
Figure 1: Pruned hypothesis graph for It?s really sad
Figure 2 shows how the number of hypotheses per
word changes as a function of the number of words
in the hypothesis. Note that if the relationship were
linear, we would expect to see a flat line. The figure
demonstrates that that Briscoe?s conclusions were
correct, given the assumption that one can accu-
rately detect lexical stress (as illustrated by the line
with circles on 2). Across all utterances, the average
number of hypotheses per word for the hybrid dictio-
nary was 510 (roughly 1/3 of the phone-based aver-
age of 1429). However, when one allows for the fact
that stress detection is not perfect, one sees an in-
crease in the amount of necessary computation: the
non-ideal hybrid dictionary has an average of 3322
0 2 4 6 8 10 12 14 16 18 20
0
0.5
1
1.5
2
2.5
x 104 Average number of hypotheses per word by number of words in utterance
Number of words in transcript
Av
er
ag
e 
nu
m
be
r o
f h
yp
ot
he
se
s 
pe
r w
or
d
Phone?based lexicon
Phone + manner lexicon, idealized syllable stress
Phone + manner lexicon, non?idealized syllable stress
Manner?based lexicon
Figure 2: Average number of hypotheses per word
as a function of number of words in utterance
576
hypotheses per word (2.3 times the phone-based av-
erage). Yet this is much lower than the potential
growth of the hypothesis space given with manner-
only dictionaries. This dictionary generated a hy-
pothesis space 12 times as large as a phone based
dictionary (17186 hypotheses/word average); more-
over, the curve grows significantly as a function of
the number of words, so longer utterances will take
disproportionately more space. Thus, Briscoe?s hy-
pothesis that purely manner-based decoding is too
expensive seems to be confirmed.
6 Integration into ASR
This paper has investigated hybrid representations
along computational phonology lines, but we have
also trained an ASR system with a hybrid lexicon for
the Wall Street Journal (WSJ0) corpus. Space does
not permit a full explanation of the experiment here
(for more details, see (Fosler-Lussier et al, 2005)),
but we include the results from this experiment as
evidence of the validity of the approach.
In this experiment, we trained phonetic and
manner-based acoustic models for all segments us-
ing the flat-start recipe of the HTK recognizer
(Young et al, 2002). After a number of itera-
tions of EM-training, we constructed a hybrid set of
acoustic models and lexicon in which phones in un-
stressed syllables were replaced with manner classes
(Hybrid-all). We also derived a lexicon in which the
recognizer could choose whether a manner or pho-
netic representation was appropriate for unstressed
segments (Hybrid-choice). During evaluation, we
found that the Hybrid-choice lexicon degraded only
slightly over a phone-based lexicon (9.9% word er-
ror vs. 9.1%), and in fact improved recognition
in mild (10dB SNR) additive car noise (13.0% vs.
15.4%). The Hybrid-all was worse on clean speech
(13.1% WER) but statistically the same as phone-
based on noisy speech (15.8%). While not conclu-
sive, this suggests that hybrid models may provide
an interesting avenue for robustness research.
7 Conclusion
Our studies verify to some degree Briscoe?s claim
that a hybrid representation for lexical modeling,
with stressed syllables receiving full phonetic rep-
resentation and unstressed syllables represented by
manner classes, can improve ASR processing. How-
ever, our analysis shows that the argument for this
hypothesis plays out along very different lines than
in Briscoe?s study. A hybrid phone-manner lexi-
con can theoretically benefit ASR because (a) the
discriminative power of the lexicon is not reduced
greatly, (b) such a representation is a much better
model of the types of pronunciation variation seen
in spontaneous speech corpora such as Switchboard,
and (c) the theoretical average hypothesis space in-
creases only by a little over a factor of 2. This
last fact is contrary to Briscoe?s finding that the
search space would be reduced because it incorpo-
rates more realistic assumptions about the detection
of stressed versus unstressed syllables.
These experiments were designed primarily to in-
vestigate the validity of Briscoe?s claims, and thus
we attempted to remain true to his model. However,
it is clear that our analysis can be extended in sev-
eral ways. We have begun experimenting with prun-
ing the hypothesis graph to remove unlikely arcs ?
this would give a more accurate model of the ASR
processing that would occur. However, this only
makes sense if language model constraints are in-
tegrated into the processing, since some word se-
quences in the graph would be discarded as unlikely.
This analysis could also benefit from a more accu-
rate model of the ASR system?s transformation be-
tween realized phones and lexical representations.
This could be achieved by comparing the Gaussian
acoustic model distributions in an HMM system or
sampling the acoustic model?s space (McAllaster et
al., 1998). Both of these extensions will be consid-
ered in future work.
The results clearly indicate that further investiga-
tion and development of a hybrid lexical strategy in
an ASR system is worthwhile, particularly for spon-
taneous speech corpora where the problem of pro-
nunciation variation is most rampant.
Acknowledgments
The authors would like to thank Keith Johnson,
Monica Rajamanohar, and Yu Wang for discussion
of this work. This work was funded in part by NSF
grant ITR-0427413; the opinions and conclusions
expressed in this work are those of the authors and
not of any funding agency.
577
References
E. J. Briscoe. 1989. Lexical access in connected speech
recognition. In Proc. 27th Annual Meeting of the As-
sociation for Computational Linguistics, pages 84?90.
R. Carlson, K. Elenius, B. Granstro?m, and H. Hunni-
cutt. 1985. Phonetic and orthographic properties of
the basic vocabulary of five european languages. In
STL-QPSR 1/1985, pages 63?94, Stockholm. Speech
Transmission Laboratory, Dept. of Speech Communi-
cation, Royal Institute of Technology.
K. W. Church. 1987. Phonological Parsing in Speech
Recognition. Kluwer, Dordrecht.
M. Finke, J. Fritsch, and D. Koll. 1999. Modeling and
efficient decoding of large vocabulary conversational
speech. In Conversational Speech Recognition Work-
shop: DARPA Hub-5E Evaluation.
W. Fisher, 1996. The tsylb2 Program: Algorithm De-
scription. NIST. Part of the tsylb2-1.1 package.
E. Fosler-Lussier, S. Greenberg, and N. Morgan. 1999.
Incorporating contextual phonetics into automatic
speech recognition. In Int?l Congress of Phonetic Sci-
ences, San Francisco, California.
E. Fosler-Lussier, C. A. Rytting, and S. Srinivasan. 2005.
Phonetic ignorance is bliss: Investigating the effects of
phonetic information reduction on asr performance. In
Proc. Interspeech, Lisbon, Portugal.
J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, and
N. Dahlgren. 1993. DARPA TIMIT acoustic-phonetic
continuous speech corpus. Technical Report NISTIR
4930, NIST, Gaithersburg, MD.
S. Greenberg, J. Hollenbach, and D. Ellis. 1996. Insights
into spoken language gleaned from phonetic transcrip-
tion of the switchboard corpus. In Proc. 4th Int?l Con-
ference on Spoken Language Processing. Philadel-
phia, PA.
R. Jakobson, G. Fant, and M. Halle. 1952. Preliminar-
ies to speech analysis. Technical Report 13, Acoustics
Laboratory, Massachusetts Instutite of Technology.
A. Juneja and C. Espy-Wilson. 2004. Significance of in-
variant acoustic cues in a probabilistic framework for
landmark-based speech recognition. In From Sound to
Sense: Fifty+ Years of Discoveries in Speech Commu-
nication, Cambridge, MA. MIT.
K. Kirchhoff. 1998. Combining articulatory and acoustic
information for speech recognition in noisy and rever-
berant environments. In Proc. 5th Int?l Conference on
Spoken Language Processing, Sydney.
Linguistic Data Consortium (LDC). 1996. The PRON-
LEX pronunciation dictionary. Available from the
LDC, ldc@unagi.cis.upenn.edu. Part of the COMLEX
distribution.
K. Livescu, J. Glass, and J. Bilmes. 2003. Hidden
feature models for speech recognition using dynamic
bayesian networks. In Proc. 8th European Conference
on Speech Communication and Technology, Geneva,
Switzerland.
D. McAllaster, L. Gillick, F. Scattone, and M. New-
man. 1998. Fabricating conversational speech data
with acoustic models: A program to examine model-
data mismatch. In Proc. 5th Int?l Conference on Spo-
ken Language Processing, pages 1847?1850, Sydney,
Australia.
G. Miller and P. Nicely. 1955. Analysis of some per-
ceptual confusions among some english consonants.
Journal of Acoustical Society of America, 27:338?52.
M. Mohri, F. Pereira, and M. Riley, 2001.
AT&T FSM LibraryTM ? General-Purpose
Finite-State Machine Software Tools. AT&T,
Florham Park, New Jersey. Available at
http://www.research.att.com/sw/tools/fsm.
NIST. 1992. Switchboard Corpus: Recorded telephone
conversations. National Institute of Standards and
Technology Speech Disc 9-1 to 9-25.
M. Ostendorf. 1999. Moving beyound the ?beads-on-
a-string? model of speech. In 1999 IEEE Workshop
on Automatic Speech Recognition and Understanding,
Keystone, Colorado.
M. Sarac?lar, H. Nock, and S. Khudanpur. 2000. Pronun-
ciation modeling by sharing Gaussian densities across
phonetic models. Computer Speech and Language,
14:137?160.
D. W. Shipman and V. W. Zue. 1982. Properties of large
lexicons: Implications for advanced isolated word
recognition systems. In Proc. Int?l Conference on
Acoustics, Speech, and Signal Processing, volume 82,
pages 546?549, Paris, France.
K. Stevens. 1981. Invariant acoustic correlates of pho-
netic features. Journal of Acoustical Society of Amer-
ica, 69 suppl. 1:S31.
A. Waibel. 1988. Prosody and Speech Recognition.
Morgan Kaufmann, San Mateo, California.
S. Young, G. Evermann, T. Hain, D. Kershaw,
G. Moore, J. Odell, D. Ollason, D. Povey,
V. Valtchev, and P. Woodland, 2002. The HTK
Book. Cambridge Unveristy Engineering Department.
http://htk.eng.cam.ac.uk.
578
Greek Word Segmentation Using Minimal Information 
 
 
C. Anton Rytting 
Department of Linguistics 
The Ohio State University 
Columbus, Ohio 43210 
rytting@ling.ohio-state.edu 
 
 
Abstract 
Several computational simulations have been 
proposed for how children solve the word 
segmentation problem, but most have been 
tested only on a limited number of languages, 
often only English.  In order to extend the 
cross-linguistic dimension of word segmenta-
tion research, a finite-state framework for test-
ing various models of word segmentation is 
sketched, and a very simple cue is tested in 
this framework.  Data is taken from Modern 
Greek, a language with phonological patterns 
distinct from English.  A small-scale simula-
tion shows using this cue performs signifi-
cantly better than chance. The utility and 
flexibility of the finite-state approach is con-
firmed; suggestions for improvement are 
noted and directions for future work outlined. 
1 Introduction 
A substantial portion of research in first-language ac-
quisition focuses on the ?word segmentation prob-
lem??how children learn to extract words (or word 
candidates) from a continuous speech signal prior to 
having acquired a substantial vocabulary. Note that the 
hardware and software constraints on the human learner 
are very different from those faced by a speech recogni-
tion system, and hence strategies appropriate for one 
may be irrelevant or disastrously inappropriate for the 
other. 
While a number of robust strategies have been pro-
posed and tested for English and a few other languages 
(discussed below), it is not clear whether or how these 
apply to other languages.  For example, the Metrical 
Segmentation Strategy (e.g., Cutler & Norris 1988) 
turns out to be very robust for English, but is not neces-
sarily applicable to other languages, simply because not 
all languages share English?s predilection for strong 
word-initial syllables (though language-appropriate 
variants of the strategy (stress-based for English) have 
been proposed, e.g., using the syllable in French (Cutler 
& Mehler, 1993) and the mora in Japanese (Otake, 
Hatano, Cutler, & Mehler, 1993)). 
Some more generic strategies (e.g., the Possible 
Word Constraint: see e.g., Norris et al 1997, 2001) have 
been proposed and tested, primarily on English, but also 
on typologically distinct languages such as Sesotho 
(Cutler, Demuth, & McQueen, 2002).  Nevertheless, 
rigorous testing in a larger sample of languages seems 
advisable before making strong claims of universal ap-
plicability.  One interesting strategy explored in e.g., 
(Aslin et al, 1996) is the use of context around (and 
particularly before) utterance boundaries to predict word 
boundaries.  The applicability of this cue is discussed 
for both English and Turkish; a simulation on English 
data is reported.  One goal of the research presented 
here is to further explore that strategy on a different data 
set, taken from a language with phonological patterns 
quite different from English or Turkish. 
The work presented here is intended as a small part 
of a more general line of research, whose purpose is 
twofold: on the one hand I wish to understand the nature 
of the cues present in Modern Greek, on the other I wish 
to establish a framework for orderly comparison of 
word segmentation algorithms across the desired broad 
range of languages. 
1.1 Infant Studies 
At least four types of information in the speech signal 
have been identified as likely cues for infants: (1) super-
segmental cues (e.g., stress) which begins to play a role 
in (English-learning) 7.5 month-olds (Jusczyk, Houston, 
et al, 1999); (2) sub-segmental cues such as co-
articulation and allophonic alternations, which infants 
begin using between 7.5 and 10.5 months of age (Jusc-
zyk, Hohne, et al, 1999); (3) segmental cues, such as 
wordlikeness and phonotactic constraints, which seem 
to be available by 9 months of age (e.g., Jusczyk, Luce, 
et al, 1994; Mattys and Jusczyk, 2001), and (4) statisti-
cal cues from recurrent patterns e.g., of syllables, evi-
dent in English-learning 8-month-olds on an artificial 
micro-language of 4 words (Saffran et al 1996).1 
1.2 Computational Models  
While the infant studies discussed above focus primarily 
on the properties of particular cues, computational stud-
ies of word-segmentation must also choose between 
various implementations, which further complicates 
comparisons.  In addition, several models (e.g., 
Batchelder, 2002; Brent?s MLDP-1, 1999a; Davis, 
2000; de Marcken, 1996; Olivier, 1968) simultaneously 
address the question of vocabulary acquisition, using 
previously learned word-candidates to bootstrap later 
segmentations.  While these models are highly interest-
ing both from their view of the long-term process of 
language acquisition and their high success rate, they 
are hard to relate to the infant studies discussed above.  
Hence, it is beyond the scope of this paper to discuss 
them at length.2 
Rather, this paper focuses on models that do not ac-
cumulate a stored vocabulary, but rely on either on sta-
tistics derived from utterance boundaries (typically 
generalized over feature matrices, as in (Aslin et al, 
1996; Christiansen et al, 1998)) or from the degree of 
predictability of the next syllable (e.g., Saffran et al, 
1996) or segment (Christiansen et al, 1998).  The intui-
tion here, first articulated by Harris (1954) is that word 
boundaries will be marked by a spike in unpredictability 
of the following phoneme.  Christiansen et al (1998) 
also test the contribution of stress and phonemic infor-
mation in addition to that of utterance boundaries, and 
show that while stress contributes in certain circum-
stances, it is not as crucial as featural information near 
utterance boundaries.  
The general line of research herein proposed focuses 
on the same cues as (Christiansen et al, 1998) begin-
ning (in the work reported here) with segmental prob-
ability distributions at utterance boundaries.  This first 
step corresponds most closely with (Aslin et al, 1996), 
where utterance boundaries were treated as a cue on 
their own.  Aslin and his colleagues propose that ?even 
the most minimal assumption about what an infant can 
recognize as a word boundary--namely, the pause after 
an utterance--is sufficient, in principle, for the learning 
the word boundaries within an utterance? (p. 133).  In 
                                                          
1 Full mention of all the studies done is not possible 
here; for a fuller review see e.g., (Johnson & Jusczyk, 2001). 
2 For useful reviews of various computational models, 
see (Brent, 1999a,b). 
that study, however, a considerable amount of context 
before such an utterance was given, namely 18-bit fea-
ture vectors of one, two, or three phonemes immediately 
preceding the final utterance boundary.  For a model 
with 30 hidden units, the following results for boundary 
detection are reported (as estimated from their bar 
graph, fig.8.8, p. 132), accompanied by the claim that 
only with two- and three-phoneme sequences is their 
system capable of learning boundary locations:  
 
 Hits  False 
Alarms 
Precision 
(H/(H+FA)) 
3 phones 62% 22% 74% 
2 phones 53% 23% 70% 
1 phone 45% 44% 51% 
Random  5% 15% 25% 
Table 1. Results reported in Aslin et al (1996, 
Fig. 8.8, p. 132). 
 
They further claim that feature vectors are necessary for 
learning: a string of three phonemes (where each pho-
neme is represented as an atomistic unit) is not suffi-
cient information, although no comparative figures are 
listed for this condition. 
This study may be seen as a replication of (Aslin et 
al., 1996); however, it differs in several crucial re-
spects?not with an eye toward improving upon their 
results, but rather on examining further their definition 
of ?minimal necessary cues.?  First, instead of training 
the transitional probabilities indirectly with connection-
ist networks, the probabilities are encoded directly 
within a finite-state framework.  Secondly, actual phone 
identities (rather than feature bundles) are used as sym-
bols.  Finally, information about a single segment is 
used.  While this very austere use of minimal informa-
tion is surely inadequate to the full task of segmentation, 
it nevertheless serves to demonstrate the gains even a 
very small amount of information can give.  Any evi-
dence of better-than-chance results would suggest that, 
for Modern Greek at least, even more minimal cues are 
possible than those Aslin et al (1996) propose. 
The results of this study may be taken as a rough 
approximation of how predictable word boundaries are 
from (unigram) segmental information alone in the sub-
set of Modern Greek experienced by young children.  
These findings may provide an additional baseline for 
measuring and comparing the relative contributions of 
other cues such as stress as a word segmentation cue.  
2 Constructing a Finite-State Model  
2.1 Data   
The Greek CHILDES corpus (Stephany, 1995) is a da-
tabase of conversations between children and caretak-
ers, broadly transcribed, currently with no notations for 
lexical stress. Audio tapes exist, but are currently un-
available for general use (Stephany, p.c.).  However, the 
transcriptions themselves give an indication at the pho-
nemic level of the sort of input Greek children are likely 
to have in learning their language.  In order to preserve 
adequate unseen data for future simulations and experi-
ments, only a small subset of the total Greek CHILDES 
corpus was used.  
As in other studies, only adult input was used for 
training and testing. In addition, non-segmental infor-
mation such as punctuation, dysfluencies, parenthetical 
references to real-world objects, etc. were removed.  
Word boundaries are represented by the symbol #, ut-
terance boundaries by $, following Brent (1999a).  Each 
line of the file was assumed to be an independent utter-
ance.  Spaces were assumed to represent word bounda-
ries without comment or correction; however, it is worth 
noting that the transcribers sometimes departed from 
standard orthographic practice with respect to certain 
types of word-clitic combinations.  The text also con-
tains a significant number of unrealized final vowels 
(apocopy), such as [in] for /ine/ 'is'.  Such variation was 
not regularized, but treated as part of the learning task.  
The training corpus contains 367 utterance tokens 
with a total of 1066 word tokens (319 types).  Whereas 
the average number of words per utterance (2.9) is com-
parable to the Korman (1984) corpus used by 
Christiansen et al (3.0), utterances and words were 
slightly longer in terms of phonemes (12.8 and 4.4 pho-
nemes respectively, compared to 9.0 and 3.0).  (Statis-
tics on the corpus used in (Aslin et al, 1996) were not 
provided.) 
The test corpus consists of utterances by adults to 
the same child as in the training corpus.  Utterances 
with dysfluencies, missing words, or other irregularities 
were discarded; the remaining utterances include 273 
utterance tokens with a total of 699 words (229 types). 
2.2 Model Design 
This model differs from incremental models such as 
(Brent 1999a) in that it pre-compiles statistics for the 
candidate word-final phonemes off-line, over the entire 
corpus.  These probabilities are thus static.  While this 
difference is not intended as a strong theoretical claim, 
it reflects the fact that even before infants seem to be 
learning the word segmentation process, they have al-
ready been exposed to a large amount of linguistic ma-
terial.  The information gleaned from the corpus is 
represented in three separate (but composible) finite-
state machines: 
 
(1) Like most models in the literature, this model as-
sumes (for sake of convenience and simplicity) that the 
child hears the correct sequence of the actual segments 
produced within an utterance. Hence, the model does 
not take into account the possibility of mishearing a 
segment, as that would add undue complication at this 
stage.  This assumption translates into the finite-state 
domain as a simple acceptor (or equivalently, an iden-
tity transducer) over the segment sequence for a given 
utterance.3  
 
(2) An optional source of knowledge used is the number 
of words in a given utterance.  This is naturally a strong 
assumption to make; it is included primarily to provide 
comparisons with baselines used by Brent (1999a) and 
Christiansen et al (1998), which provide pseudo-
random baselines that make reference either to number 
of boundaries directly or information concerning aver-
age word length.  Results are given both with and with-
out this constraint. 
 
(3) The main item under examination is naturally the 
relative likelihood of breaking the word after a given 
segment S.   
 
The third information source was tested in three vari-
ants.  The first one is of course the approximation sug-
gested by Aslin et al (1996), that P(#|S) may be 
approximated by using P($|S), the probability of an ut-
terance-break given the segment. This approximation 
yields the ranking e>s>o>u>i>a>m>n, with /e/ most 
likely to end an utterance.  This information source was 
compared to two related alternatives, which were used 
as upper and lower bounds to measure the effectiveness 
of the utterance-boundary approximation of word 
boundaries.  As an upper bound (3U), the true value for 
P(#|S) is used, corresponding to training on labeled data, 
or a store of already-learned vocabulary.4  The lower 
bound (3L) consists of the seven final segments 
{a,e,i,o,u,n,s}, but the frequency ranking replaced by an 
equi-probable assumption.  In a sense, this is equivalent 
to a grammar book listing the possible final segments of 
Greek without regard to their actual likelihood.  Finally, 
these three variants are compared with a random walk 
for which no information is used, but boundaries are 
inserted completely by chance.  Each of these three 
types of knowledge was modeled by means of a finite 
state machine, using the AT&T finite-state tools.5   
                                                          
3 While modeling the mishearing of segments is beyond 
the scope of this study, a weighted transducer could in prin-
ciple represent a segmental confusion matrix in a modular 
way and augment the current identity transducer.  For 
further discussion of issues in using ?unsanitized data,? 
(Sundaram, 2003) may be helpful. 
4   The resulting ranking, o>i>e>s>a>u>n>j>m>p, is 
rather different than the one above, reflecting the frequency 
of masculine and feminine articles /o/ and /i/, which are 
never utterance-final. 
5  FSM Library Version 3.7, freely available from 
http://www.research.att.com/sw/tools/fsm/ 
 
(1) Segments: Linear FSA (trivially equivalent to an 
identity transducer). 
 
(2) Number of words: Unweighted FSA. 
 
(3U) Upper bound: Weighted FST, with weights corre-
sponding to -Log(P(#|S)) for a word boundary 
and -Log(1-P(#|S)) for an arc with no word boundary. 
 
(3) Utterance-Boundary Probabilities: Same as (3U), 
with weights corresponding to -Log(P($|S)) for a word 
boundary and -Log(1-P($|S)) for no word boundary.  In 
the condition where (2) was not used, a weight of -1.7 
(determined empirically on the training data) was added 
to the word-boundary arc, to offset the tendency of 
P($|S) to underestimate P(#|S). 
 
(3L) Unweighted (or equally weighted) version of (3).  
In the condition where (2) was not used, a weight of 
(-0.5) was added to the arc that adds boundaries, which 
caused the FST to insert word boundaries after every 
instance of a vowel, /n/, or /s/.  
3 Results  
Six different conditions were tested, corresponding to 
the three variants FSTs (3), (3U), and (3L), both with and 
without the exact-word constraint in FSM (2).  Each of 
these were composed (separately) with the ?segment 
identity? acceptor (1) for a given utterance. The output-
projection of the best path from each resulting FST was 
converted back into text and compared to the text of the 
original utterance.  Scores for both boundaries and 
words are reported (where a word is counted as cor-
rectly segmented only if both its left and right bounda-
ries are correctly placed).  In the case where several 
best-paths of equal cost exist, the average scores for 
precision and recall are counted. 
The results with and without the number of words 
known are shown in Tables 2 and 3, following.  In both 
cases, the precision scores patterned as expected. The 
upper bound condition (representing a supervised case, 
where statistics on the word boundaries are available for 
the training data) proved the most accurate on the test 
data.  This suggests (as has been confirmed for English 
in such studies as Brent 1999a) that the learning of pat-
terns over already-acquired vocabulary has perhaps the 
largest effect in the acquisition of new vocabulary.  
The utterance-based approximation, corresponding 
most closely to (Aslin et al, 1996), seems to be slightly 
better overall than the lower bound.  Without the num-
ber of words known, (3) has an F-score of 20.2 for 
words and 70.2 for boundaries, whereas (3L) has F-
scores of only 17.0 (word) and 68.0 (boundaries), 
though this difference may not be significant.  This dif-
ference was less than expected, given preliminary ex-
amination of the training data; it may be that once the 
set of allowable word-final phonemes is observed, the 
relative probabilities of those phonemes is not as use-
fully learned from utterance boundaries.  However, the 
lower bound (corresponding to purely symbolic knowl-
edge of the allowable word-final segments) is signifi-
cantly better than the random walk, suggesting that any 
knowledge, no matter how rudimentary, begins to make 
a difference. 
 
Table 2: Test Results with Constraint (2) 
 
Words Word Boundaries  
Precision Recall Precision Recall 
Upper 
bound 
219/720 
(30.4%) 
219/699 
(31.3%) 
737/993 
(74.2%) 
737/972 
(75.8%) 
Utt-
prob. 
159/860 
(18.5%) 
159/699 
(22.7%) 
739/1133 
(65.2%) 
739/972 
(76.0%) 
Lower 
bound 
195/1599 
(12.2%) 
195/699 
(27.9%) 
967/1872 
(51.7%) 
967/972 
(99.5%) 
Random 
Walk 
39/1569 
(2.5%) 
39/699 
(5.6%) 
767/1842 
(41.6%) 
767/972 
(78.9%) 
Table 3: Test Results without Constraint (2) 
4 Discussion 
4.1 Comparisons with Aslin et al (1996) 
Obviously, the cues of preceding and following seg-
ments are in and of itself insufficient to predict a word 
boundary with any reasonable degree of accuracy, just 
as Christiansen et al (1998) found that no one cue was 
sufficient for English.  However, a few comparisons 
with Aslin?s et al (1996) data in Table 1 may be useful, 
although they should be interpreted cautiously given the 
differences in the training and testing corpora between 
their study and this one.  Their results for the single-
phoneme condition have nearly equal hits and false 
alarms?a precision of about 51%.  They apparently do 
not consider this sufficient evidence of learning, al-
though it is significantly better than their random base-
line.  Similarly, the worst non-random condition 
Words Word Boundaries  
Precision Recall Preci-
sion 
Recall 
Upper 
bound   
277/699 
(39.6%) 
277/699 
(39.6%) 
751/972 
(77.3%)  
751/972 
(77.3%)  
Utt-
prob.  
226/699 
(32.4%) 
226/699 
(32.4%) 
721/972 
(74.2%)  
721/972 
(74.2%)  
Lower 
bound  
221/699 
(31.6%) 
221/699 
(31.6%) 
708/972 
(72.9%)  
708/972 
(72.9%)  
Random 
Walk 
119/699 
(17.0%) 
119/699 
(17.0%) 
639/972 
(65.7%) 
639/972 
(65.7%) 
reported here (lower bound without constraint (2)) also 
has a precision of 51%.  This, too, is difficult to call 
?learning,? as it represents the heuristic of always in-
serting a word boundary any time there could be one.  
The only fact that has been learned is which segments 
cannot be (excepting foreign loan-words) word-final.   
However, if the criterion for learning (or at least 
satisfactory performance) is hits exceeding false alarms, 
then the utterance-boundary statistical heuristic, with 
739 hits and only 396 false alarms, is nearly as accurate 
as Table 1?s two-phoneme condition.  While further 
information (whether phonological features, longer 
strings of phonemes, or some other cue) is needed to 
reach the 74% accuracy of Table 1?s three-phoneme 
condition, it seems that even these very basic cues come 
closer to Aslin?s et al (1996) results than might be sup-
posed.  Importantly, the same general trend was shown--
that utterance-final information translates into word-
boundary information not only for English, but for other 
languages such as Modern Greek as well. 
A number of further directions are possible under 
this framework, including:  
 
(1) Using transitional probability (P(Sk+1| Sk)) and mu-
tual information measures over two adjacent segments 
as cues to the likelihood of word boundaries between 
those two segments, as suggested in e.g., (Brent, 1999a).  
 
(2) Developing more plausible models for approximat-
ing word-length distributions from utterance-length in-
formation, distances between stressed vowels, pause 
information, and other salient cues available to children.  
 
(3) Incorporating stress cues (as potentially signaling 
both beginnings and approaching ends of content 
words) both alone and in combination with segmental 
cues.  
 
Preliminary work on each of these avenues is currently 
underway.  While some of these heuristics may require 
the use of other techniques in addition to finite-state 
techniques, the general finite-state framework is ex-
pected to prove useful as an organizing tool for compar-
ing various cues in a simple, rational, and transparent 
way.  
References 
Aslin, Richard N., Woodward, Julide Z., LaMendola, 
Nicholas P., & Bever, Thomas G. 1996. Models of 
word segmentation in fluent maternal speech to in-
fants. In James L. Morgan & Katherine Demuth , edi-
tors, Signal to syntax, pages 117-134. Mahwah, NJ: 
Lawrence Erlbaum Associates.  
Batchelder, Elanor Olds  2002.  Bootstrapping the lexi-
con: A computational model of infant speech seg-
mentation. Cognition 83:167-206.  
Brent, Michael R. 1999a. An efficient, probabilistically 
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34:71-105.  
Brent, Michael R. 1999b. Speech segmentation and 
word discovery: a computational perspective. Trends 
in Cognitive Sciences, 3(8):294-301.  
Christiansen, Morton H., Allen, Joseph, & Seidenberg, 
Mark S. 1998. Learning to segment speech using 
multiple cues: A connectionist model. Language and 
Cognitive Processes, 13(2/3):221-268.  
Davis, Matt H. (2000) Lexical segmentation in spoken 
word recognition. Unpublished PhD thesis, Birkbeck 
College, University of London.  Available: 
http://www.mrc-
cbu.cam.ac.uk/personal/matt.davis/thesis/index.html  
de Marcken, Carl G. 1996b. Unsupervised language 
acquisition. PhD dissertation, MIT, Cambridge, MA. 
Available: http://xxx.lanl.gov/abs/cmp- lg/9611002  
Harris, Zelig S. 1954. Distributional structure. Word, 
10:146-162.  
Johnson, Elizabeth K., & Jusczyk, Peter W. 2001. Word 
segmentation by 8- month-olds: when speech cues 
count more than statistics. Journal of Memory and 
Language, 44 (4), 548 567.  
Joseph, Brian.  2001.  ?Word? in Modern Greek. In 
R.M.W. Dixon & A. Aikhenvald (eds.) Proceedings 
of the International Workshop on the Status of 
?Word?. Cambridge: Cambridge University Press 
(2001). 
Jusczyk, Peter W., & Aslin, R. N. 1995. Infant's detec-
tion of sound patterns of words in fluent speech. 
Cognitive Psychology, 29:1-23.  
Jusczyk, Peter W., Hohne, E. A., & Bauman, A. 1999. 
Infants' sensitivity to allophonic cues for word seg-
mentation. Perception & Psychophysics, 61:1465- 
1476.  
Jusczyk, Peter W., Houston, Derek, & Newsome, Mary. 
1999. The beginnings of word segmentation in Eng-
lish-learning infants. Cognitive Psychology, 39:159- 
207.  
Jusczyk, Peter W., Luce, Paul A., & Charles-Luce, Jan 
1994. Infants' sensitivity to phonotactic patterns in 
the native language. Journal of Memory and Lan-
guage, 33:630-645.  
Korman, Myron. 1984. Adaptive aspects of maternal 
vocalizations in differing contexts at ten weeks. First 
Language, 5:44-45.  
Mattys, Sven L., Jusczyk, Peter W., Luce, Paul A., & 
Morgan, James L. 1999. Word segmentation in in-
fants: How phonotactics and prosody combine. Cog-
nitive Psychology, 38:465-494.  
Mattys, Sven L. and Jusczyk, Peter W.  2001.  Phono-
tactic cues for segmentation of fluent speech by in-
fants. Cognition 78:91-121.  
Olivier, D. C. 1968. Stochastic grammars and language 
acquisition mechanisms. PhD dissertation, Harvard 
University, Cambridge, MA.  
Saffran, Jenny R., Aslin, Richard N., & Newport, Elissa 
L. 1996. Statistical cues in language acquisition: 
word segmentation by infants. In G.W. Cottrell, edi-
tor, Proceedings of the 18th Annual Conference of 
the Cognitive Science Society. pages 376-380. Hills-
dale, NJ: Lawrence Erlbaum Associates.  
Stephany, U. 1995. The acquisition of Greek. In D. I. 
Slobin, editor, The crosslinguistic study of language 
acquisition. Vol. 4.  
Sundaram, Ramasubramanian. 2003.  Effects of Tran-
scription Errors on Supervised Learning in Speech 
Recognition.  Unpublished Masters Thesis. Missis-
sippi State University, Mississippi State, MS.  
http://www.isip.msstate.edu/publications/books/msst
ate_theses/2003/transcription_errors/. 
Segment Predictability as a Cue in Word Segmentation: Application to 
Modern Greek 
C. Anton Rytting 
Department of Linguistics 
The Ohio State University 
Columbus, Ohio, U.S.A. 43201 
rytting@ling.ohio-state.edu 
 
Abstract 
Several computational simulations of how 
children solve the word segmentation problem 
have been proposed, but most have been 
applied only to a limited number of languages. 
One model with some experimental support 
uses distributional statistics of sound sequence 
predictability (Saffran et al 1996).  However, 
the experimental design does not fully specify 
how predictability is best measured or 
modeled in a simulation.  Saffran et al (1996) 
assume transitional probability, but Brent 
(1999a) claims mutual information (MI) is 
more appropriate.  Both assume predictability 
is measured locally, relative to neighboring 
segment-pairs. 
This paper replicates Brent?s (1999a) mutual-
information model on a corpus of child-
directed speech in Modern Greek, and 
introduces a variant model using a global 
threshold. Brent?s finding regarding the 
superiority of MI is confirmed; the relative 
performance of local comparisons and global 
thresholds depends on the evaluation metric. 
1 Introduction 
A substantial portion of research in child 
language acquisition focuses on the word 
segmentation problem?how children learn to 
extract words (or word candidates) from a 
continuous speech signal prior to having acquired a 
substantial vocabulary.  While a number of robust 
strategies have been proposed and tested for 
infants learning English and a few other languages 
(discussed in Section 1.1), it is not clear whether or 
how these apply to all or most languages. In 
addition, experiments on infants often leave 
undetermined many details of how particular cues 
are actually used.  Computational simulations of 
word segmentation have also focused mainly on 
data from English corpora, and should also be 
extended to cover a broader range of the corpora 
available. 
The line of research proposed here is twofold: on 
the one hand we wish to understand the nature of 
the cues present in Modern Greek, on the other we 
wish to establish a framework for orderly 
comparison of word segmentation algorithms 
across the desired broad range of languages.  
Finite-state techniques, used by e.g., Belz (1998) in 
modeling phonotactic constraints and syllable 
within various languages, provide one 
straightforward way to formulate some of these 
comparisons, and may be useful in future testing of 
multiple cues. 
Previous research (Rytting, 2004) examined the 
role of utterance-boundary information in Modern 
Greek, implementing a variant of Aslin and 
colleagues? (1996) model within a finite-state 
framework.  The present paper examines more 
closely the proposed cue of segment predictability.  
These two studies lay the groundwork for 
examining the relative worth of various cues, 
separately and as an ensemble. 
1.1 Infant Studies  
Studies of English-learning infants find the 
earliest evidence for word segmentation and 
acquisition between 6 and 7.5 months (Jusczyk and 
Aslin, 1995) although many of the relevant cues 
and strategies seem not to be learned until much 
later. 
Several types of information in the speech signal 
have been identified as likely cues for infants, 
including lexical stress, co-articulation, and 
phonotactic constraints (see e.g., Johnson & 
Jusczyk, 2001 for a review).  In addition, certain 
heuristics using statistical patterns over (strings of) 
segments have also been shown to be helpful in the 
absence of other cues.   
One of these (mentioned above) is extrapolation 
from the segmental context near utterance 
boundaries to predict word boundaries (Aslin et al, 
1996).  Another proposed heuristic utilizes the 
relative predictability of the following segment or 
syllable. For example, Saffran et al (1996) have 
confirmed the usefulness of distributional cues for 
8-month-olds on artificially designed micro-
                                                                  Barcelona, July 2004
                                              Association for Computations Linguistics
                       ACL Special Interest Group on Computational Phonology (SIGPHON)
                                                    Proceedings of the Workshop of the
languages?albeit with English-learning infants 
only. 
The exact details of how infants use these cues 
are unknown, since the patterns in their stimuli fit 
several distinct models (see Section 1.2).  Only 
further research will tell how and to what degree 
these strategies are actually useful in the context of 
natural language-learning settings?particularly for 
a broad range of languages.  However, what is not 
in doubt is that infants are sensitive to the cues in 
question, and that this sensitivity begins well 
before the infant has acquired a large vocabulary. 
1.2 Implementations and Ambiguities 
While the infant studies discussed above focus 
primarily on the properties of particular cues, 
computational studies of word-segmentation must 
also choose between various implementations, 
which further complicates comparisons.  Several 
models (e.g., Batchelder, 2002; Brent?s (1999a) 
MBDP-1 model; Davis, 2000; de Marcken, 1996; 
Olivier, 1968) simultaneously address the question 
of vocabulary acquisition, using previously learned 
word-candidates to bootstrap later segmentations. 
(It is beyond the scope of this paper to discuss 
these in detail; see Brent 1999a,b for a review.) 
Other models do not accumulate a stored 
vocabulary, but instead rely on the degree of 
predictability of the next syllable (e.g., Saffran et 
al., 1996) or segment (e.g., Christiansen et al, 
1998).  The intuition here, first articulated by 
Harris (1954), is that word boundaries are marked 
by a spike in unpredictability of the following 
phoneme.  The results from Saffran et al (1996) 
show that English-learning infants do respond to 
areas of unpredictability; however, it is not clear 
from the experiment how this unpredictability is 
best measured.  Two specific ambiguities in 
measuring (un)predictability are examined here. 
Brent (1999a) points out one type of ambiguity, 
namely that Saffran and colleagues? (1996) results 
can be modeled as favoring word-breaks at points 
of either low transitional probability or low mutual 
information.  Brent reports results for models 
relying on each of these measures. It should be 
noted that these models are not the main focus of 
his paper, but provided for illustrative purposes;  
nevertheless, these models provide the best 
comparison to Saffran and colleagues? experiment, 
and may be regarded as an implementation of the 
same. 
Brent (1999a) compares these two models in 
terms of word tokens correctly segmented (see 
Section 3 for exact criteria), reporting 
approximately 40% precision and 45% recall for 
transitional probability (TP) and 50% precision and 
53% recall for mutual information (MI) on the first 
1000 utterances of his corpus (with improvements 
given larger corpora).  Indeed, their performance 
on word tokens is surpassed only by Brent?s main 
model (MBDP-1), which seems to have about 73% 
precision and 67% recall for the same range.1 
Another question which Saffran et al (1996) 
leave unanswered is whether the segmentation 
depends on local or global comparisons of 
predictability. Saffran et al assume implicitly, and 
Brent (1999a) explicitly, that the proper 
comparison is local?in Brent, dependent solely on 
the adjacent pairs of segments.  However, 
predictability measures for segmental bigrams 
(whether TP or MI) may be compared in any 
number of ways.  One straightforward alternative 
to the local comparison is to compare the 
predictability measures compare to some global 
threshold.  Indeed, Aslin et al (1996) and 
Christiansen et al (1998) simply assumed the 
mean activation level as a global activation 
threshold within their neural network framework.2 
1.3 Global and Local Comparisons 
The global comparison, taken on its own, seems 
a rather simplistic and inflexible heuristic: for any 
pair of phonemes xy, either a word boundary is 
always hypothesized between x and y, or it never 
is.  Clearly, there are many cases where x and y 
sometimes straddle a word boundary and 
sometimes do not.  The heuristic also takes no 
account of lengths of possible words.  However, 
the local comparison may take length into account 
too much, disallowing words of certain lengths.  In 
order to see that, we must examine Brent?s (1999a) 
suggested implementation of Saffran et al (1996) 
more closely. 
In the local comparison, given some string 
?wxyz?, in order for a word boundary to be 
inserted between x and y, the predictability 
measure for xy must be lower than both that of wx 
and of yz.  It follows that neither wx nor yz can 
have word boundaries between them, since they 
cannot simultaneously have a lower predictability 
measure than xy.  This means that, within an 
utterance, word boundaries must have at least two 
segments between them, so this heuristic will not 
correctly segment utterance-internal one-phoneme 
                                                     
1 The specific percentages are not reported in the text, 
but have been read off his graph.  Brent does not report 
precision or recall for utterance boundaries; those 
percentages would undoubtedly be higher. 
2 These methodologies did not ignore local 
information, but encoded it within the feature vector.  
However, Rytting (2004) showed that this extra context, 
while certainly helpful, is not strictly necessary in the 
Greek corpus under question.  A context of just one 
phoneme yielded better-than-chance results. 
words.3  Granted, only a few one-phoneme word 
types exist in either English or Greek (or other 
languages).  However, these words are often 
function words and so are less likely to appear at 
edges of utterances (e.g., ends of utterances for 
articles and prepositions; beginnings for postposed 
elements).  Neither Brent?s (1999a) 
implementation of Saffran?s et al (1996) heuristic 
nor Aslin?s et al (1996) utterance-boundary 
heuristic can explain how these might be learned. 
Brent (1999a) himself points out another length-
related limitation?namely, the relative difficulty 
that the ?local comparison? heuristic has in 
segmenting learning longer words.  The bigram MI 
frequencies may be most strongly influenced by?
and thus as an aggregate largely encode?the most 
frequent, shorter words.  Longer words cannot be 
memorized in this representation (although 
common ends of words such as prefixes and 
suffixes might be).   
In order to test for this, Brent proposes that 
precision for word types (which he calls ?lexicon 
precision?) be measured as well as for word 
tokens. While the word-token metric emphasizes 
the correct segmentation of frequent words, the 
word-type metric does not share this bias.  Brent 
defines this metric as follows:  ?After each block 
[of 500 utterances], each word type that the 
algorithm produced was labeled a true positive if 
that word type had occurred anywhere in the 
portion of the corpus processed so far; otherwise it 
is labeled a false positive.?  Measured this way, MI 
yields a word type precision of only about 27%; 
transitional probability yields a precision of 
approximately 24% for the first 1000 utterances, 
compared to 42% for MBDP-1.  He does not 
measure word type recall. 
This same limitation in finding longer, less 
frequent types may apply to comparisons against a 
global threshold as well.  This is also in need of 
testing.  It seems that both global and local 
comparisons, used on their own as sole or decisive 
heuristics, may have serious limitations.  It is not 
clear a priori which limitation is most serious; 
hence both comparisons are tested here. 
2 Constructing a Finite-State Model  
2.1 Outline of current research  
While in its general approach the study reported 
here replicates the mutual-information and 
transitional-probability models in Brent (1999a), it 
                                                     
3 At the edges of utterances, this restriction will not 
apply, since word boundaries are automatically inserted 
at utterance boundaries, while still allowing the 
possibility of a boundary insertion at the next position. 
differs slightly in the details of their use.  First, 
whereas Brent dynamically updated his measures 
over a single corpus, and thus blurred the line 
between training and testing data, our model pre-
compiles statistics for each distinct bigram-type 
offline, over a separate training corpus.4  Secondly, 
we compare the use of a global threshold 
(described in more detail in Section 2.3, below) to 
Brent?s (1999a) use of the local context (as 
described in Section 1.3 above).   
Like (Brent, 1999a), but unlike Saffran et al 
(1996), our model focuses on pairs of segments, 
not on pairs of syllables.  While Modern Greek 
syllabic structure is not as complicated as 
English?s, it is still more complicated than the CV 
structure assumed in Saffran et al (1996); hence, 
access to syllabification cannot be assumed.5 
2.2 Corpus Data 
In addition to the technical differences discussed 
above, this replication breaks new ground in terms 
of the language from which the training and test 
corpora are drawn.  Modern Greek differs from 
English in having only five vowels, generally 
simpler syllable structures, and a substantial 
amount of inflectional morphology, particularly at 
the ends of words.  It also contains not only 
preposed function words (e.g., determiners) but 
postposed ones as well, such as the possessive 
pronoun, which cannot appear utterance-initially.  
For an in-depth discussion of Modern Greek, see 
(Holton et al, 1997).   While it is not anticipated 
that Modern Greek will be substantially more 
challenging to segment than English, the choice 
does serve as an additional check on current 
assumptions. 
The Stephany corpus (Stephany, 1995) is a 
database of conversations between children and 
caretakers, broadly transcribed, currently with no 
notations for lexical stress, included as part of the 
CHILDES database (MacWhinney, 2000).  In 
order to preserve adequate unseen data for future 
simulations and experiments, and also to use data 
most closely approximating children of a very 
                                                     
4 While this difference is not intended as a strong 
theoretical claim, it can be seen as reflecting the fact 
that even before infants seem to begin the word 
segmentation process, they have already been exposed 
to a substantial amount of linguistic material.  However, 
it is not anticipated to affect the general pattern of 
results.   
5 Furthermore, if Brent?s ?local comparison? 
implementation were based on syllables to more closely 
coincide with Saffran?s et al (1996) experiment (not 
something Brent ever suggests), it would fail to detect 
any one-syllable words, clearly problematic for both 
Greek and English, and many languages besides. 
young age, files from the youngest child only were 
used in this study. However, since the heuristics 
and cues used are very simple compared to 
vocabulary-learning models such as Brent?s 
MDLP-1, it is anticipated that they will require 
relatively little context, and so the small size of the 
training and testing corpora will not adversely 
effect the results to a great degree. 
As in other studies, only adult input was used for 
training and testing. In addition, non-segmental 
information such as punctuation, dysfluencies, 
parenthetical references to real-world objects, etc. 
were removed.  Spaces were taken to represent 
word boundaries without comment or correction; 
however, it is worth noting that the transcribers 
sometimes departed from standard orthographic 
practice when transcribing certain types of word-
clitic combinations.  The text also contains a 
significant number of unrealized vowels, such as 
[ap] for /apo/ ?from?, or [in] or even [n] for /ine/ 
?is?.  Such variation was not regularized, but 
treated as part of the learning task. 
The training corpus contains 367 utterance 
tokens with a total of 1066 word tokens (319 
types).  Whereas the average number of words per 
utterance (2.9) is almost identical to that in the 
Korman (1984) corpus used by Christiansen et al 
(1998), utterances and words were slightly longer 
in terms of phonemes (12.8 and 4.4 phonemes 
respectively, compared to 9.0 and 3.0 in Korman). 
The test corpus consists of 373 utterance tokens 
with a total of 980 words (306 types).  All 
utterances were uttered by adults to the same child 
as in the training corpus.  As with the training 
corpus, dysfluencies, missing words, or other 
irregularities were removed; the word boundaries 
were kept as given by the annotators, even when 
this disagreed with standard orthographic word 
breaks.  
2.3 Model Design 
Used as a solitary cue (as it is in the tests run 
here), comparison against a global threshold may 
be implemented within the same framework as 
Brent?s (1999) TP and MI heuristics.  However, it 
may be implemented within a finite-state 
framework as well, with equivalent behavior.  This 
section will describe how the ?global comparison? 
heuristic is modeled within a finite-state 
framework. 
While such an implementation is not technically 
necessary here, one advantage of the finite-state 
framework is the compositionality of finite state 
machines, which allows for later composition of 
this approach with other heuristics depending on 
other cues, analogous to Christiansen et al (1998).  
Since the finite-state framework selects the best 
path over the whole utterance, it also allows for 
optimization over a sequence of decisions, rather 
than optimizing each local decision separately.6 
Unlike Belz (1998), where the actual FSM 
structure (including classes of phonemes that could 
be group onto one arc) was learned, here the 
structure of each FSM is determined in advance.  
Only the weight on each arc is derived from data.  
No attempt is made to combine phonemes to 
produce more minimal FSMs; each phoneme (and 
phoneme-pair) is modeled separately. 
Like Brent (1999a) and indeed most models in 
the literature, this model assumes (for sake of 
convenience and simplicity) that the child hears 
each segment produced within an utterance without 
error. This assumption translates into the finite-
state domain as a simple acceptor (or equivalently, 
an identity transducer) over the segment sequence 
for a given utterance.7 
Word boundaries are inserted by means of a 
transducer that computes the cost of word 
boundary insertion from the predictability scores.  
In the MI model, the cost of inserting a word 
boundary is proportional to the mutual 
information.  For ease in modeling, this was 
represented with a finite state transducer with two 
paths between every pair of phonemes (x,y), with 
zero-counts modeled with a maximum weight of 
99.  The direct path, representing a path with no 
word boundary inserted, costs ?MI(x,y), which is 
positive for bigrams of low predictability (negative 
MI), where word boundaries are more likely.  The 
other path, representing a word boundary insertion, 
carries the cost of the global threshold, in this case 
arbitrarily set to zero (although it could be 
optimized with held-out data).  A small subset of 
the resulting FST, representing the connections 
over the alphabet {ab} is illustrated in Figure 1, 
below: 
                                                     
6 See Rabiner (1989) for a discussion of choosing 
optimization criteria. It is worth noting that this 
distinction does not come into play in the one-cue 
model reported here, as all decisions are modeled as 
independent of one another.  However, it is expected to 
take on some importance in models combining multiple 
cues, such as those proposed in Section 4 of this paper. 
7 While modeling the mishearing of segments would 
be more realistic and highly interesting, it is beyond the 
scope of this study.  However, a weighted transducer 
representing a segmental confusion matrix could in 
principle replace the current identity transducer, without 
disrupting the general framework of the model. 
 
Figure 1: The MI model over the alphabet {ab} 
The best (least-cost) path over this subset model 
inserts boundaries between two adjacent a?s and 
two adjacent b?s, but not between ab or ba; thus 
the (non-Greek) string ?ababaabbaaa? would be 
segmented ?ababa#ab#ba#a#a? by the FSM. 
The FSM for transitional probability has the 
same structure as that of MI, but with different 
weights on each path. For each pair of phonemes 
xy, the cost for the direct path from x to y is 
?log(P(y|x)).  The global threshold cost of 
inserting a word boundary was set (again, 
arbitrarily) as the negative log of the mean of all 
TP values.  In the two-phoneme subset (shown in 
Figure 2), the only change is that the direct 
pathway from a to b is now more expensive than 
the threshold path, so the best path over the FSM 
will insert word boundaries between a and b as 
well.  Hence our example string ?ababaabbaaa? 
would be segmented ?a#ba#ba#a#b#ba#a#a? by 
the FSM.  (The stranded ?word? #b# would of 
course be an error, but this problem does not arise 
in actual Greek input, since two adjacent b?s, like 
all geminate consonants, are ruled out by Greek 
phonotactics.) 
 
Figure 2: The TP model over the alphabet {ab} 
During testing each FST model was composed 
(separately) with the segment identity transducer 
for the utterance under consideration. A short 
sample section of such a composition, with the best 
path in bold, is shown in Figure 3.  
 
 
Figure 3: A section of the composition of the MI 
model and an utterance acceptor 
The output projection of the best path from the 
resulting FST was converted back into text and 
compared to the text of the original utterance.  
These compositions, best-path projections, and 
conversions were performed using the AT&T finite 
state toolkit (Mohri et al, 1998).8   
2.4 A Concrete Example 
Take, for example, an utterance from the test 
corpus /tora#Telis#na#aniksume#afto/ ?now you 
want us to open this.?  The mutual information and 
transitional probability figures for this utterance 
are given in Table 1. 
 
Context Predictability 
Left Right MI TP 
# t 0.000 3.219 
t o ?1.661 0.781 
o r ?1.018 2.350 
r a ?0.800 1.113 
a T 1.824 6.375 
T e ?0.744 1.540 
e l ?0.225 3.059 
l i ?0.903 1.197 
i s ?0.491 2.382 
s n 1.555 4.317 
n a ?0.300 1.613 
a a 2.516 4.429 
a n ?0.339 2.424 
n i ?0.071 2.029 
i k ?0.337 2.633 
k s ?0.444 2.428 
s u ?0.172 3.219 
u m ?1.387 2.413 
m e ?1.230 1.055 
e a 1.095 3.008 
a f ?1.473 2.525 
f t ?2.068 0.484 
t o ?1.661 0.781 
o # 0.000 3.219 
Table 1: MI and TP values for bigrams in the test 
utterance /tora#Telis#na#aniksume#afto/.  Values 
above threshold are bold; local maxima italicized. 
In this example, the correct boundaries fall 
between the pairs (a,T), (s,n), (a,a), and (e,a).  Both 
the mutual information and the transitional 
probability for the first three of these pairs are 
above the global mean, so word boundaries are 
posited under both global models.9  (Since each of 
these is also a local maximum, the local models 
also posit boundaries between these three pairs.)  
The pair (e,a) is above threshold for MI but not for 
                                                     
8 FSM Library Version 3.7, freely available from 
http://www.research.att.com/sw/tools/fsm/ 
9 Since all values are given in terms of negative MI 
and negative log probability, high values for both 
measures indicate relatively improbable pairings. 
TP, so the global TP model fails to posit a 
boundary here.  Finally, the two local models posit 
a number of spurious boundaries at the other local 
maxima, shown by the italic numbers in the table.  
The resulting predictions for each model are: 
 
Global MI: #tora#Telis#na#aniksume#afto# 
Global TP: #tora#Telis#na#aniksumeafto# 
Local MI: #tora#Te#lis#na#an#iks#ume#afto# 
Local TP: #to#ra#Te#lis#na#ani#ks#ume#afto# 
3 Results 
The four model variants (global MI, global TP, 
local MI, and local TP) were each evaluated on 
three metrics: word boundaries, word tokens, and 
word types.  Note that the first metric reported, 
simple boundary placement, considers only 
utterance-internal word-boundaries, rather than 
including those word boundaries which are 
detected ?for free? by virtue of being utterance-
boundaries also.  This boundary measure may be 
more conservative than that reported by other 
authors, but is easily convertible into other metrics.   
The second metric, the percentage of word 
tokens detected, is the same as Brent (1999a).  In 
order for a word to be counted as correctly found, 
three conditions must be met: (a) the word?s 
beginning (left boundary) is correctly detected, (b) 
the word?s ending (right boundary) is correctly 
detected, and (c) these two are consecutive (i.e., no 
false boundaries are posited within the word). 
The last metric (word type) is slightly more 
conservative than Brent?s (1999a) in that the word 
type must have been actually spoken in the same 
utterance (not the same block of 500 utterances) in 
which it was detected to count as a match.  This 
lessens the possibility that a mismatch that happens 
to be segmentally identical to an actual word (but 
whose semantic context may not be conducive to 
learning its correct meaning) is counted as a match.  
However, this situation is presumably rather rare. 
Tables 2 and 3 present the results over the test 
set for both the global and the local comparisons of 
the predictability statistics proposed by Saffran et 
al. (1996) and Brent (1999a). 
 
Table 2: Global Comparison: FST best paths 
with bigrams compared to a global threshold only 
Local 
Comparison 
Bound-
aries 
Word 
Tokens 
Word 
Types 
Precision 42.0% 31.5% 20.1%
Recall 62.6% 41.1% 27.8%MI
F-Score 50.3% 35.7% 23.4%
Precision 41.5% 28.0% 20.2%
Recall 74.1% 41.6% 22.9%TP
F-Score 53.2% 33.5% 21.4%
Table 3: Local Comparison: Replication of Brent 
(1999a); each bigram compared to both neighbors 
4 Conclusion 
4.1 Comparing the Four Variants 
The findings here confirm Brent?s (1999a) 
contention that mutual information is a better 
measure of predictability than is transitional 
probability?at least for the task of identifying 
words, not just boundaries.  This is particularly 
true in the global comparison.  Transitional 
probability finds more word boundaries in the 
?local comparison? model, but this does not carry 
over to the task of pulling out the word themselves, 
which is arguably the infant?s main concern.  This 
result should be kept in mind when interpreting or 
replicating (Saffran et al, 1996) or similar studies. 
While Brent?s ?local comparison? heuristic was 
unable to pull out one-phoneme-long words, as 
predicted above, this did not adversely affect it as 
much as anticipated.  On the contrary, both the 
local and global comparison heuristics tended to 
postulate too many word boundaries, as Brent had 
observed.  This is not necessarily a bad thing for 
infants, for several reasons.   
First, infants may have a preference for finding 
short words, since these will presumably be easier 
to remember and learn, particularly if the child?s 
phonetic memory is limited.  Second, it is probably 
easier to reject a hypothesized word (for example, 
on failing to find a consistent semantic cue for it) 
than to obtain a word not correctly segmented; 
hence false positives are less of a problem than 
false negatives for the child.  Third and most 
importantly, this cue is not likely to operate on its 
own, but rather as one among many contributing 
cues.  Other cues may act as filters on the 
boundaries suggested by this cue.  One example of 
this is the distribution of segments before utterance 
edges, as used by e.g., Aslin et al (1996) and 
Christiansen et al (1998) which indicate the set of 
possible word-final segments in the language. 
Global 
Comparison 
Bound-
aries 
Word 
Tokens 
Word 
Types 
Precision 43.9% 30.8% 22.3%
Recall 54.4% 35.3% 29.7%MI 
F-Score 48.6% 32.9% 25.5%
Precision 40.4% 28.4% 20.0%
Recall 41.7% 29.0% 28.4%TP 
F-Score 41.0% 28.7% 23.5%
However, as far as these results go, the word 
type metric shows that the finite-state model using 
a global threshold suffered slightly less from this 
problem than the local comparison model.  For the 
MI variants, both recall and precision for word 
type were about 2% higher on the global threshold 
variant.  For transitional probability, the precision 
of the local and global models was roughly equal, 
but recall for the global comparison model was 
5.5% higher.  Not only were the global models 
better at pulling out a variety of words, but they 
also managed to learn longer ones (especially the 
global TP variant), including a few four-syllable 
words.  The local model learned no four-syllable 
words, and relatively few three-syllable words. 
The mixed nature of these results suggests that 
evaluation depends fairly crucially on what 
performance metric needs to be optimized.  This 
demands stronger prior hypotheses regarding the 
process and needed input of a vocabulary-
acquiring child.  However, it cannot be blindly 
assumed that children are selecting low points over 
as short a window as Brent?s (1999a) MI and TP 
models suggest.  Quite possibly the best model 
would involve either a hybrid of local and global 
comparisons, or a longer window, or even a 
?gradient? window where far neighbors count less 
than near ones in a computed average.   
However, further speculation on point this of 
less importance than considering how this cue 
interacts with others known experimentally to be 
salient to infants.  Christiansen et al (1998) and 
Johnson and Jusczyk (2001) have already began 
simulating and testing these interactions in 
English.  However, more work needs to be done to 
understand better the nature of these interactions 
cross-linguistically. 
4.2 Further Research  
As mentioned above, one obvious area for future 
research is the interaction between predictability 
cues like MI and utterance-final information; this 
is one of the cue combinations explored in 
Christiansen et al (1998) in English.  Previous 
research (Rytting, 2004) examined the role of 
utterance-final information in Greek, and found 
that this cue performs better than chance on its 
own.  However, it seems that utterance-final 
information would be more useful as a filter on the 
heuristics explored here to restrain them from 
oversegmenting the utterance. Since nearly all 
Greek words end in /a/, /e/, /i/, /o/, /u/, /n/, or /s/, 
just restricting word boundaries to positions after 
these seven phonemes boosts boundary precision 
considerably with little effect on recall.10 
                                                     
10 Naturally, in unrestricted speech the characteristics 
Preliminary testing suggests that this filter boosts 
both precision and recall at the word level.  
However, a model that incorporates the likelihoods 
of word boundaries after each of these final 
segments, properly weighted, may be even more 
helpful than this simple, unweighted filter. 
Another fruitful direction is the exploration of 
prosodic information such as lexical stress.  With 
the exception of a certain class of clitic groups, 
Greek words have at most one stress.  Hence, at 
least one word boundary must occur between two 
stressed vowels.  Relations between stress and the 
beginnings and endings of words, while not 
predicted to be as robust a cue as in English (see 
e.g., Cutler, 1996), should also provide useful 
information, both alone and in combination with 
segmental cues. 
Finally, the relationship between these more 
?static? cues and the cues that emerge as 
vocabulary begins to be acquired (as in Brent?s 
main MBDP-1 model and others discussed above) 
seems not to have received much attention in the 
literature.  As vocabulary is learned, it can help 
bootstrap these cues by augmenting heuristic cues 
with actual probabilities derived from its parses.  
Hence, the combination of e.g., MLDP-1 and these 
heuristics may prove more powerful than either 
approach alone. 
5 Acknowledgements 
This material is based upon work supported 
under a National Science Foundation Graduate 
Research Fellowship.  Sincere thanks go to the 
NSF for their financial support, and to Chris Brew, 
Eric Fosler-Lussier, Brian Joseph, members of the 
computational linguistics and phonology 
discussion groups at the Ohio State University, and 
to anonymous reviewers of previous versions of 
this paper for their helpful comments and 
encouragement. 
References  
Richard N. Aslin, Julide Z. Woodward, Nicholas P. 
LaMendola, and Thomas G. Bever. 1996. 
Models of word segmentation in fluent maternal 
speech to infants. In Signal to syntax, James L. 
Morgan and Katherine Demuth, ed., pages 117-
                                                                                   
of word boundaries diverge from those of utterance 
boundaries.  For example, final vowels may delete from 
utterance-medial words; instances such as [in] for /ine/ 
?is? and [ap] for /apo/ ?from? were already mentioned.  
However, if we assume that the canonical forms of these 
words occur frequently enough to be acquired normally, 
then knowledge of these canonical forms may assist the 
acquisition of variant forms (as well as the phrasal 
phonological processes that give rise to them) later on. 
134, Lawrence Erlbaum Associates, Mahwah, 
New Jersey. 
Elanor Olds Batchelder. 2002.  Bootstrapping the 
lexicon: A computational model of infant speech 
segmentation. Cognition, 83:167-206. 
Anja Belz. 1998.  An Approach to the Automatic 
Acquisition of Phonotactic Constraints.  In 
Proceedings of SIGPHON ?98: The Computation 
of Phonological Constraints, T. Mark Ellison, 
ed., pages 35-44 
Michael R. Brent. 1999a. An efficient, 
probabilistically sound algorithm for 
segmentation and word discovery. Machine 
Learning, 34:71-105.  
Michael R. Brent. 1999b. Speech segmentation and 
word discovery: A computational perspective. 
Trends in Cognitive Sciences, 3(8):294-301.  
Morton H. Christiansen, Joseph Allen, and Mark S. 
Seidenberg. 1998. Learning to segment speech 
using multiple cues: A connectionist model. 
Language and Cognitive Processes, 13(2/3):221-
268.  
Anne Cutler. 1996. Prosody and the word 
boundary problem.  In Signal to syntax, James L. 
Morgan and Katherine Demuth, ed., pages 87-
100, Lawrence Erlbaum Associates, Mahwah, 
New Jersey. 
Matt H. Davis. 2000. Lexical segmentation in 
spoken word recognition. Unpublished PhD 
thesis, Birkbeck College, University of London.  
Available: http://www.mrc-
cbu.cam.ac.uk/personal/matt.davis/thesis/index.h
tml  
Carl G. de Marcken. 1996. Unsupervised language 
acquisition. PhD dissertation, MIT, Cambridge, 
MA. Available: http://xxx.lanl.gov/abs/cmp-
lg/9611002  
Zelig S. Harris. 1954. Distributional structure. 
Word, 10:146-162.  
David Holton, Peter Mackridge and Irene 
Philippaki-Warburton. 1997.  Greek:  A 
Comprehensive Grammar of the Modern 
Language, Routledge, London and New York. 
Elizabeth K. Johnson and Peter W. Jusczyk. 2001. 
Word segmentation by 8-month-olds: when 
speech cues count more than statistics. Journal 
of Memory and Language, 44 (4), 548 567.  
Myron Korman. 1984. Adaptive aspects of 
maternal vocalizations in differing contexts at 
ten weeks. First Language, 5:44-45.  
Brian MacWhinney. 2000. The CHILDES project: 
Tools for analyzing talk. Third Edition. 
Lawrence Erlbaum Associates, Mahwah, New 
Jersey. 
Mehryar Mohri, Fernando C. N. Pereira, and 
Michael Riley. 1998.  A Rational Design for a 
Weighted Finite-State Transducer Library. 
Lecture Notes in Computer Science, 1436. 
D. C. Olivier. 1968. Stochastic grammars and 
language acquisition mechanisms. PhD 
dissertation, Harvard University, Cambridge, 
Massachusetts.  
Lawrence R. Rabiner. 1989. A tutorial on hidden 
Markov models and selected applications in 
speech recognition. Proceedings of the IEEE, 
77:2, pages 257-285. 
C. Anton Rytting. 2004.  Greek word segmentation 
using minimal information.  In Proceedings of 
the Student Research Workshop at HLT/NAACL 
2004, pages 207-212, Association for 
Computational Linguistics, Boston, Massa-
chusetts.  Available: http://acl.ldc.upenn.edu/hlt-
naacl2004/studws/pdf/sw-8.pdf 
Jenny R. Saffran, Richard N. Aslin and Elissa L. 
Newport. 1996. Statistical learning by 8-month-
old infants. Science, 274, 1926-1928. 
Ursula Stephany. 1995. The acquisition of Greek. 
In The crosslinguistic study of language 
acquisition. D. I. Slobin, ed., Vol. 4. 
 
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 109?115,
Baltimore, Maryland USA, June 26, 2014. c?2014 Association for Computational Linguistics
ArCADE: An Arabic Corpus of Auditory Dictation Errors
C. Anton Rytting
Paul Rodrigues
Tim Buckwalter
Valerie Novak
Aric Bills
University of Maryland
7005 52nd Avenue
College Park, MD 20742
{crytting,prr,tbuckwal,
vnovak,abills}@umd.edu
Noah H. Silbert
Communication
Sciences & Disorders
University of Cincinnati
2600 Clifton Avenue
Cincinnati, Ohio
silbernh
@ucmail.uc.edu
Mohini Madgavkar
Independent Researcher
6120 Dhaka Pl. 20189-6120
Dhaka, Bangladesh
mohini.madgavkar
@gmail.com
Abstract
We present a new corpus of word-level lis-
tening errors collected from 62 native En-
glish speakers learning Arabic designed to
inform models of spell checking for this
learner population. While we use the cor-
pus to assist in automated detection and
correction of auditory errors in electronic
dictionary lookup, the corpus can also be
used as a phonological error layer, to be
combined with a composition error layer
in a more complex spell-checking system
for non-native speakers. The corpus may
be useful to instructors of Arabic as a sec-
ond language, and researchers who study
second language phonology and listening
perception.
1 Introduction
Learner corpora have received attention as an im-
portant resource both for guiding teachers in cur-
riculum development (Nesselhauf, 2004) and for
providing training and evaluation material the de-
velopment of tools for computer-assisted language
learning (CALL). One of the most commonly used
technologies in CALL is spell correction. Spell
correction is used for providing automated feed-
back to language learners (cf. Warschauer and
Ware, 2006), automatic assessment (Bestgen and
Granger, 2011), and in providing cleaner input
to downstream natural language processing (NLP)
tools, thereby improving their performance (e.g.
Nagata et al., 2011). However, off-the-shelf spell
correctors developed for native speakers of the tar-
get language are of only limited use for repairing
language learners? spelling errors, since their error
patterns are different (e.g. Hovermale, 2011; Mit-
ton and Okada, 2007; Okada, 2005).
Most learner corpora (and spell correctors) are
understandably focused on learner-written texts.
Thus, they allow a greater understanding (and im-
provement) of learners? writing skills. However,
another important aspect of language learning is
listening comprehension (cf. Field, 2008; Prince,
2012). A better understanding of listening errors
can guide teachers and curriculum development
just as written production errors do. Listening er-
ror data may also be helpful for improving tech-
nologies for listening training tools, by helping
prioritize the most critical pairs of phonemes for
discrimination, and pointing out the most trouble-
some contexts for phoneme discrimination.
Finally, spell correction specifically designed
to correct listening errors may aid listening com-
prehension and vocabulary acquisition. If learn-
ers are unable to hear, recall and record accu-
rately what they heard, they will be less able to
search dictionaries or the Web for more informa-
tion on new vocabulary items they otherwise could
have learned from listening exercises. While data-
driven spelling correction on popular search en-
gines may catch some non-native errors, native er-
rors are likely to ?drown out? any non-native errors
they conflict with due to larger numbers of native
users of these search engines. On the other hand, if
themost common listening and transcription errors
are automatically corrected within a search tool,
learners will have greater success in finding the
new vocabulary items they may have misheard in
speech.
Learner corpora focused on written production
may not have enough samples of phonologically-
based errors to aid in developing such tools, and
109
even in a large corpus, word avoidance strategies
and other biases would make the source unreli-
able for estimating relative magnitudes of listening
problems accurately. It may be more effective to
target listening errors directly, through other tasks
such as listening dictation.
2 Related Work
Tools for language learning and maintenance, and
learner corpora fromwhich to build them, typically
focus on language pairs for which there is a large
market. Learner corpora for native English learn-
ers of low resource languages such as Arabic have
been until recently comparatively rare, and often
too small to be of practical use for the develop-
ment of educational technology. In the past few
years, however, a number of learner corpora for
Arabic have become available, including a corpus
of 19 non-native (mostlyMalaysian) students at Al
Al-Bayt University (Abu al-Rub, 2007); the Ara-
bic InterlanguageDatabase (ARIDA;Abuhakema
et al., 2008, 2009); the Arabic Learners Writ-
ten Corpus from the University of Arizona Center
for Educational Resources in Culture, Language,
and Literacy (CERCLL; Farwaneh and Tamimi,
2012);1 and the Arabic Learner Corpus v1 (Alfaifi
and Atwell, 2013).2
These corpora are all derived from learner writ-
ing samples, such as essays, and as such they con-
tainmany different types of errors, including errors
in morphology, syntax, and word choice. Spelling
errors are also observed, but relatively rarely, and
the relevance of these spelling errors to listening
competence is unclear. Hence, while they are
likely to be useful for many applications in teach-
ing Arabic writing, their usefulness for other pur-
poses, such as examining listening skills and the
effects of learner phonology on spelling, is limited.
Corpora or datasets focused on speaking and lis-
tening skills in Arabic are rarer. One such corpus,
the West Point Arabic Speech Corpus, available
from the LDC, contains one hour of non-native
(learner) speech (LaRocca and Chouairi, 2002)
Sethy et al. (2005) describe a corpus of elicited
Arabic speech, but because none of the partici-
pants had prior exposure to Arabic, its use for un-
1Available from http://l2arabiccorpus.cercll.
arizona.edu/?q=homepage.
2As of February 2014, a second version, with about 130K
words from non-native speakers, is available from http:
//www.arabiclearnercorpus.com/. It also has a small
(three hour) speech component.
derstanding learner Arabic is limited. While there
have been a few studies of Arabic listening skills
(e.g. Huthaily, 2008; Faircloth, 2013), their cov-
erage was not sufficiently broad to make reuse of
their data likely to inform such purposes as the de-
velopment of phoneme discrimination training or
other CALL technology.
3 Motivation
We present here the Arabic Corpus of Auditory
Dictation Errors (ArCADE) version 1, a corpus
of Arabic words as transcribed by 62 native En-
glish speakers learning Arabic. This corpus fills
the current gap in non-native spelling error cor-
pora, and particularly for spelling errors due to lis-
tening difficulties. Unlike error corpora collected
from non-native Arabic writing samples, it is de-
signed to elicit spelling errors arising from percep-
tual errors; it provides more naturalistic data than
is typical in phoneme identification or confusion
studies.
A principal purpose for creating the corpus was
to aid in the development and evaluation of tools
for detecting and correcting listening errors to aid
in dictionary lookup of words learners encountered
in spoken language (cf. Rytting et al., 2010). As
such, it serves as a complementary dataset for the
dictionary search engine?s query logs, since in this
case the intended target of each transcription is
known (rather than having to be inferred, in the
case of query logs). We list three other potential
uses for this corpus in Section 5.
4 Corpus Design and Creation
The ArCADE corpus was created through an
elicitation experiment, similar in structure to an
American-style spelling test. The principal differ-
ence (other than the language) is that in this case,
the participants are expected to be unfamiliar with
thewords, and thus forced to rely onwhat they hear
in the moment, rather than their lexical knowledge.
We selected words from a commonly-used dictio-
nary of Modern Standard Arabic such that the set
of words would contain a complete set of non-glide
consonants in various phonetic contexts.
4.1 Selection of Stimulus Words
Since the corpus was originally collected for a
study focused on the perception of consonants
within the context of real Arabic words, the stim-
ulus set was designed with three purposes in
110
mind: coverage of target sounds, exclusion of ba-
sic words, and brevity (so that participants could
complete the task in one sitting).
In order to differentiate consonants that are rela-
tively unpredictable (and thus test listening ability)
from consonants whose value could be predicted
from non-acoustic cues (such as prior knowledge
of morphological structure), the corpus is anno-
tated for target consonants vs. non-target conso-
nants. A target consonant is defined as a consonant
that should not be predictable (assuming the word
is unknown to the listener) except by the acoustic
cues alone. Glides /w/ and /j/ were not targeted
in the study because orthographic ambiguities be-
tween glides and vowels would complicate the er-
ror analysis.
Each Arabic consonant other than the glides oc-
curs as a target consonant in the stimulus set in six
consonant/vowel/word-boundary contexts: C_V,
V_C, V_V, #_V, V_#, and C_#.3 (The contexts
#_C and C_C are phonotactically illegal in Mod-
ern Standard Arabic.)
Consonants that were judged morphologically
predictable within a word were considered non-
target consonants. These included: (1) non-root
consonants, when Semitic roots were known to the
researchers; (2) consonants participating in a redu-
plicative pattern such as /tamtam/ and /zalzala/;
and (3) Consonants found in doubled (R2=R3)
roots if the two consonants surfaced separately
(e.g., in broken plurals such as /?asnan/).
We excluded words from our stimulus set if
we anticipated that an intermediate Arabic student
would already be familiar with them or would eas-
ily be able to guess their spellings. Items found
in vocabulary lists associated with two commonly-
used introductory textbooks (Al-Kitaab and Alif-
Baa) were excluded (Brustad et al., 2004a,b).
Loanwords from Western languages were also ex-
cluded, as were well-known place names (e.g.,
/?iskotlanda/ = ?Scotland?). Words found only in
colloquial dialects and terms that might be offen-
sive or otherwise distracting (as judged by native
speaker of Arabic) were removed, as well.
In order to keep the stimulus set as short as pos-
sible while maintaining coverage of the full set of
target stimuli consonants in each targeted context,
we chose words with multiple target consonants
whenever possible. The final set of 261words con-
3C = consonant, V = vowel, # = word boundary, and ?_?
(underscore) = location of target consonant.
tained 649 instances of target consonants: one in-
stance of each geminate consonant and between 17
and 50 instances of each singleton consonant (at
least two instances for each of the six contexts),
with a few exceptions.4 Although glides and vow-
els were not specifically targeted, 6 instances of
/w/, 10 instances of /j/, and at least 12 instances of
each of the monophthong vowels (/a/, /i/, /u/, /a:/,
/i:/, /u:/) occur in the stimulus set.
4.2 Recording of the Stimuli
The audio data used in the dictation was recorded
in a sound-proof boothwith a unidirectionalmicro-
phone (Earthworks SR30/HC) equippedwith a pop
filter, and saved as WAV files (stereo, 44.1kHz,
32-bit) with Adobe Audition. The stimuli were
spoken at a medium-fast rate. The audio files were
segmented and normalized with respect to peak
amplitude with Matlab.
The nativeArabic speaker in the audio recording
is of Egyptian and Levantine background, but was
instructed to speak with a neutral (?BBC Arabic?)
accent.
4.3 Participants and Methodology
Seventy-five participants were recruited from six
universities. To be eligible, participants had to be
18 years of age or older, native speakers of En-
glish, and have no known history of speech lan-
guage pathology or hearing loss. Participants were
required to have completed at least two semesters
of university level Arabic courses in order to en-
sure that they were able to correctly write the Ara-
bic characters and to transcribe Arabic speech.
Heritage speakers of Arabic and non-English dom-
inant bilinguals were excluded from the study. The
corpus contains responses from 62 participants.
The mean duration of Arabic study completed was
5.6 semesters (median 4).
Before beginning the experiment, participants
were asked to fill out a biographical questionnaire.
This included questions about language exposure
during childhood and languages studied in a class-
room setting. There were additional questions
about time spent outside of the United States to
ascertain possible exposure to languages not ad-
dressed in previous questions.
4These exceptions include only one instance of a phone
rather than two for the following contexts: (1) /h/ in the con-
text C_#, (2) /f/ in the context V_#, and (3) /z/ in the context
#_V. One geminate consonant, /x:/, was inadvertently omitted
from the stimulus set.
111
Participants wrote their responses to the 261
stimulus words on a response sheet that contained
numbered boxes. They were asked to use Arabic
orthography with full diacritics and short vowels
(fatha, damma, kasra, shadda and sukun). The
shadda (gemination) mark was required in order
to analyze the participants? perception of geminate
consonants; the other diacritics were included so as
to not single out shadda for special attention (since
participants were na?ve to the purpose of the study)
and also to increase the value of the resulting error
corpus for later analysis of short vowels.
4.4 Presentation of the Stumuli
The proctors who ran the experiment supplied an
iPod Touch tablet to each participant, pre-loaded
with a custom stimuli presentation application.
In this custom iPod application, 261 Arabic
words were randomized into 9 stimulus sets. Each
stimulus set was preceded by four practice items
which were not scored; thus each participant saw
265 items. Each touch screen tablet was initialized
by the testers to deliver a specific stimulus set. A
button on the touch screen allowed the participants
to begin the experiment. After a few seconds? de-
lay, the first word was played. A stimulus num-
ber identifying the word appeared in a large font
to aid the participants in recording the word on pa-
per. Participants were given 15 seconds to write
their response, before the tablet automatically ad-
vanced to the next word. Participants were not able
to replay a word.
The participants used noise-canceling head-
phones (Audio-Technica ATH-ANC7 or ATH-
ANC7B) for listening to the audio stimuli. The
experiment was performed in a quiet classroom.
4.5 Data Coding
The participants? handwritten responses were
typed in as they were written, using Arabic Uni-
code characters. Any diacritics (short vowels or
gemination) written by the participants were pre-
served. An automatic post-process was used to en-
sure that the gemination mark was ordered prop-
erly with respect to an adjacent short vowel mark.
The corpus consists of twomain sections: ortho-
graphic and phonemic. The orthographic section is
very simple: each stimulus word is given in its tar-
get orthography (with diacritics) and in each par-
ticipant?s corresponding orthographic transcrip-
tion (including diacritics if the participant provided
them as instructed). The phonemic section is more
elaborate, containing additional fields designed for
a phone level analysis of target consonants. Its
construction is described in further detail below.
Both the orthographic response and the canon-
ical (reference) spelling were automatically con-
verted to a phonemic representation. This conver-
sion normalizes certain orthographic distinctions,
such as various spellings for word-final vowels.
This phonemic representation of the response for
each stimulus item was then compared with the
phonemic representation of the item?s canonical
pronunciation, and each phoneme of the response
was aligned automatically with the most probable
phoneme (or set of equally plausible phonemes)
in the canonical phonemic representation of the
auditory stimulus. This alignment was done via
dynamic programming with a weighted Leven-
shtein edit distance metric. Specifically, weights
were used to favor the alignment of vowels and
glides with each other rather than with non-glide
consonants (since the scope of our original study
was non-glide consonants). Thus substitutions be-
tween short vowels, long vowels, and glides are
given preference over other confusions. This is in-
tended to reduce the ambiguity of the alignments
and to ensure that non-glide consonants are aligned
with non-glide consonants when possible, without
introducing any bias in the non-glide consonants
alignments. When one unique alignment had the
lowest cost, it was used as the alignment for that
item. In some cases, multiple alignments were tied
for minimal cost. In this case, all alignments were
used and assigned equal probability.
Once the least-cost alignment(s) were found be-
tween a response string and the reference string for
an item, the target consonants within the reference
string were then each paired with the correspond-
ing phonemes in the response, and an error cate-
gory (<substitution>, <deletion>, or <match> for
no error) was assigned. In the case of geminate
phonemes, two subtypes of <substitution>were in-
troduced: <gemination> and <degemination>.
Where an entire word had no response, ?NA?
was used to indicate that no edit operation can be
assigned. (A total of 112 items were missing).
Note that insertions were not marked, because
only the 649 instances of target consonants were
analyzed for the phonemic portion of the corpus,
and no other material in each stimulus word (in-
cluding any possible insertion points for additional
material) were annotated for errors. Insertions can
112
be recovered from the orthographic portion of the
corpus.
The coding method described above yielded a
set of 41,121 target consonant records of partici-
pants? responses to target consonants (not count-
ing the 112 non-response items), including 29,634
matches (72.1%) and 11,487 errors (27.9%). At
the word level, there are 16,217 words, of which
8321 (48.2%) contain at least one error in a tar-
geted consonant, and 5969 (37.1%) are spelled
perfectly (excluding diacritics).
5 Potential Uses of the Corpus
In addition to the uses described in Section  3, we
believe the data could be used for several other
uses, such as examining linguistic correlates of
proficiency, developing phonemic training, and in-
vestigating non-native Arabic handwriting.
One potential use of the corpus is to analyze the
errors by individual learners to determine which
sounds are confused only by relatively beginning
learners (after two semesters) and which are con-
fused by beginning and experienced learners alike.
While hard measures of proficiency are not avail-
able for the participants, the language question-
naire includes time of study and self-report mea-
sures of proficiency. To the extent to which these
proxies are reliable, the corpus may lead to the de-
velopment of hypotheses which can be tested in
more targeted studies.
Since the corpus allows quantitative evidence
for the relative difficulty of particular sound pairs
in particular contexts, it may guide the prioritiza-
tion of foci for phonemic discrimination training
and other listening exercises. At the most basic
level, a teacher can take our original audio stimuli
and use them as dictation exercises for beginning
students (who may not be ready for sentence or
paragraph level dictation). It may also form the ba-
sis for automated phonemic discrimination train-
ing, such as Michael et al. (2013). Cf. Bradlow
(2008) for a review.
Since the participants handwrote their re-
sponses, the corpus contains, as a byproduct, a
set of 16,329 words in non-native handwriting and
their digital transcriptions. As Alfaifi and Atwell
(2013) note, this could be used as a corpus of non-
native handwriting for training or evaluating OCR
on L2 Arabic script. If corresponding native tran-
scriptions of the same (or similar) strings were ob-
tained, the corpus could also be used to differenti-
ate native from non-native handwriting (cf. Farooq
et al., 2006; Ramaiah et al., 2013).
6 Limitations and future work
The corpus as it currently stands has some limita-
tions worth noting. First, there is no control set
of native Arabic listeners to provide a comparison
point for distinguishing non-native perceptual er-
rors from acoustic errors that even native speakers
are subject to. Second, the survey does not con-
tain proficiency ratings (except self-report) for the
participants, making direct correlation of particu-
lar confusion patterns with proficiency level more
difficult.
Statistical analysis of the participants? accuracy
at distinguishing Arabic consonants is currently
underway (Silbert et al., in preparation). An inves-
tigation of the utility of the corpus for training and
evaluating spelling correction for L1 English late
learners of Arabic, including the effects of training
corpus size on accuracy, is also in progress.
7 Conclusion
The Arabic Corpus of Auditory Dictation Errors
(ArCADE) version 1 provides a corpus of word-
level transcriptions of Arabic speech by native En-
glish speakers learning Arabic, ideal for the anal-
ysis of within-word listening errors, as well as the
development and evaluation of NLP tools that seek
to aid either in developing listening skill or in com-
pensating for typical non-native deficits in listen-
ing. Since most learner corpora only include writ-
ten composition or spoken production from stu-
dents, this corpus fills a gap in the resources avail-
able for the study of Arabic as a second language.
The corpus, along with the original audio
stimuli and participants? handwriting samples,
is available at http://www.casl.umd.edu/
datasets/cade/arcade/index.html.
Acknowledgments
This material is based on work supported, in whole
or in part, with funding from the United States
Government. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the University of Maryland,
College Park and/or any agency or entity of the
United States Government.
113
References
Muhammad Abu al-Rub. 2007. ???????? ??????? ?????
.?????? ???????? ??????? ????? ?????? ??? ??????? ?????? ???
Tahl??l al-akht??? al-kit?b?yah ?ala mustaw?
al-iml?? lad? muta?allim? al-lughah al-?arab?yah
al-n?ti?q?na bi-ghayrih? [Analysis of written
spelling errors among non-native speak-
ing learners of Arabic]. ????????? ?????? ???????
.??????????? Dir?s?t, al-?Ul?m al-Ins?n?yah wa-al-
Ijtim???yah [Humanities and Social Sciences],
34(2). http://journals.ju.edu.jo/
DirasatHum/article/view/1911/1898.
Ghazi Abuhakema, Anna Feldman, and Eileen
Fitzpatrick. 2008. Annotating an Arabic learner
corpus for error. In Proceedings of the Interna-
tional Conference on Language Resources and
Evaluation (LREC 2008). Marrakech, Morocco.
Ghazi Abuhakema, Anna Feldman, and Eileen
Fitzpatrick. 2009. ARIDA: An Arabic inter-
language database and its applications: A pilot
study. Journal of the National Council of Less
Commonly Taught Languages (NCOLCTL),
7:161?184.
Abdullah Alfaifi and Eric Atwell. 2013. Potential
uses of the Arabic Learner Corpus. In Leeds
Language, Linguistics, and Translation PGR
Conference 2013. University of Leeds, Leeds,
UK.
Yves Bestgen and Sylvaine Granger. 2011. Cat-
egorising spelling errors to assess L2 writ-
ing. International Journal of Continuing En-
gineering Education and Life-Long Learning,
21(2/3):235?252.
Ann Bradlow. 2008. Training non-native language
sound patterns. In Phonology and Second Lan-
guage Acquisition, Benjamins, Amsterdam and
Philadelphia, pages 287?308.
Kristin Brustad, Mahmoud Al-Batal, and Abbas
Al-Tonsi. 2004a. Al-Kitaab fii Ta?allum al-
?Arabiyya, volume 1. Georgetown University
Press, Washington, DC, 1st edition.
Kristin Brustad, Mahmoud Al-Batal, and Abbas
Al-Tonsi. 2004b. Alif Baa: Introduction to Ara-
bic Letters and Sounds. Georgetown University
Press, Washington, DC, 2nd edition.
Laura Rose Faircloth. 2013. The L2 Perception
of Phonemic Distinctions in Arabic by English
Speakers. BA Thesis, The College of William
and Mary. https://digitalarchive.wm.
edu/bitstream/handle/10288/18160/
FairclothLauraRose2013Thesis.pdf?
sequence=1.
Faisal Farooq, Liana Lorigo, and Venu Govin-
daraju. 2006. On the accent in handwrit-
ing of individuals. In Tenth Interna-
tional Workshop on Frontiers in Hand-
writing Recognition. La Baule, France.
http://hal.inria.fr/docs/00/11/26/
30/PDF/cr103741695994.pdf.
Samira Farwaneh and Mohammed Tamimi. 2012.
Arabic learners written corpus: A resource for
research and learning. Available from the
University of Arizona Center for Educational
Resources in Culture, Language, and Literacy
web site. http://l2arabiccorpus.cercll.
arizona.edu/?q=homepage.
John Field. 2008. Listening in the Language Class-
room. Cambridge University Press, Cambridge,
UK.
DJ Hovermale. 2011. Erron: A Phrase-Based
Machine Traslation Approach to Customized
Spelling Correction. Ph.D. thesis, The Ohio
State University.
Khaled Yahya Huthaily. 2008. Second Lan-
guage Instruction with Phonological Knowl-
edge: Teaching Arabic to Speakers of English.
Ph.D. thesis, The University of Montana.
Col. Stephen A. LaRocca and Rajaa Chouairi.
2002. West Point Arabic speech corpus. Tech-
nical report, LDC, Philadelphia.
Erica B. Michael, Greg Colflesh, Valerie Karuzis,
Michael Key, Svetlana Cook, Noah H. Silbert,
Christopher Green, Evelyn Browne, C. Anton
Rytting, Eric Pelzl, and Michael Bunting. 2013.
Perceptual training for second language speech
perception: Validation study to assess the ef-
ficacy of a new training regimen (TTO 2013).
Technical report, University of Maryland Cen-
ter for Advanced Study of Language, College
Park, MD.
RogerMitton and Takeshi Okada. 2007. The adap-
tation of an English spellchecker for Japanese
writers. Birbeck ePrints, London. http://
eprints.bbk.ac.uk/archive/00000592.
Ryo Nagata, Edward Whittaker, and Vera Shein-
man. 2011. Creating a manually error-tagged
and shallow-parsed learner corpus. In Proceed-
ings of the 49th Annual Meeting of the Asso-
114
ciation for Computational Linguistics. Associ-
ation for Computational Linguistics, Portland,
OR, pages 1210?1219.
Nadja Nesselhauf. 2004. Learner corpora and their
potential in language teaching. In How to Use
Corpora in Language Teaching, Benjamins,
Amsterdam and Philadelphia, pages 125?152.
Takeshi Okada. 2005. Spelling errors made by
Japanese EFL writers: with reference to errors
occurring at the word-initial and word-final po-
sitions. In Vivian Cook and Benedetta Bassetti,
editors, Second language writing systems, Mul-
tilingual Matters, Clevedon, UK, pages 164?
183.
Peter Prince. 2012. Writing it down: Issues re-
lating to the use of restitution tasks in listening
comprehension. TESOL Journal, 3(1):65?86.
Chetan Ramaiah, Arti Shivram, and Venu
Govindaraju. 2013. A Baysian framework
for modeling accents in handwriting. In
12th International Conference on Docu-
ment Analysis and Recognition (ICDAR).
http://ieeexplore.ieee.org/xpls/abs_
all.jsp?arnumber=6628752.
C. Anton Rytting, Paul Rodrigues, Tim Buckwal-
ter, DavidM. Zajic, Bridget Hirsch, Jeff Carnes,
Nathanael Lynn, Sarah Wayland, Chris Taylor,
Jason White, Charles Blake, Evelyn Browne,
Corey Miller, and Tristan Purvis. 2010. Error
correction for Arabic dictionary lookup. In Sev-
enth International Conference on Language Re-
sources and Evaluation (LREC 2010). Valletta,
Malta.
Abhinav Sethy, Shrikanth Narayanan, Nicolaus
Mote, and W. Lewis Johnson. 2005. Modeling
and automating detection of errors inArabic lan-
guage learner speech. In INTERSPEECH-2005.
pages 177?180.
Noah H. Silbert, C. Anton Rytting, Paul Ro-
drigues, Tim Buckwalter, Valerie Novak, Mo-
hiniMadgavkar, Katharine Burk, andAric Bills.
in preparation. Similarity and bias in non-native
Arabic consonant perception.
Mark Warschauer and Paige Ware. 2006. Auto-
mated writing evaluation: Defining the class-
room research agenda. Language Teaching Re-
search, 10(2):157?180.
115
