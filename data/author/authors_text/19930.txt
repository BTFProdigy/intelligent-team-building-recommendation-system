Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2140?2150, Dublin, Ireland, August 23-29 2014.
Left-corner Transitions on Dependency Parsing
Hiroshi Noji and Yusuke Miyao
Department of Informatics
The Graduate University for Advanced Studies
National Institute of Informatics, Tokyo, Japan
{noji,yusuke}@nii.ac.jp
Abstract
We propose a transition system for dependency parsing with a left-corner parsing strategy. Unlike
parsers with conventional transition systems, such as arc-standard or arc-eager, a parser with our
system correctly predicts the processing difficulties people have, such as of center-embedding.
We characterize our transition system by comparing its oracle behaviors with those of other tran-
sition systems on treebanks of 18 typologically diverse languages. A crosslinguistical analysis
confirms the universality of the claim that a parser with our system requires less memory for
parsing naturally occurring sentences.
1 Introduction
It is sometimes argued that transition-based dependency parsing is appealing not only from an engi-
neering perspective due to its efficiency, but also from a scientific perspective: These parsers process a
sentence incrementally similar to a human parser, which have motivated several studies concerning their
cognitive plausibility (Nivre, 2004; Boston and Hale, 2007; Boston et al., 2008). A cognitively plausible
dependency parser is attractive for many reasons, one of the most important being that dependency tree-
banks are available in many languages, so it is suitable for crosslinguistical studies of human language
processing (Keller, 2010). However, current transition systems based on shift-reduce actions fully or
partially employ a bottom-up strategy
1
, which is problematic from a psycholinguistical point of view:
Bottom-up or top-down strategies are known to fail in predicting the difficulty for certain sentences, such
as center-embedding, which people have troubles in comprehending (Abney and Johnson, 1991).
We propose a transition system for dependency parsing with a left-corner strategy. For constituency
parsing, unlike other strategies, the arc-eager left-corner strategy is known to correctly predict processing
difficulties people have (Abney and Johnson, 1991). To the best of our knowledge, however, the idea of
left-corner strategy has not been introduced in the dependency parsing literature. We define the memory
cost for a transition system as the number of unconnected subtrees on a stack. Under this condition, the
proposed system incurs non-constant memory cost only when encountering center-embedded structures.
After developing the transition system, we characterize it by looking into the following question: Is
it true that naturally occurring sentences can be parsed on this system with a lower memory overhead?
This should be true under the assumptions that 1) people avoid generating a sentence that causes diffi-
culty for them, and 2) center-embedding is a kind of such structure. Specifically, we focus on analyzing
the oracle transitions of the system, i.e., parser actions to recover the gold dependency tree for a sen-
tence. In English, it is known that left-corner transformed treebank sentences can be parsed with less
memory (Schuler et al., 2010), but our focus in this paper is on the language universality of the claim in
a crosslingual setting. Two different but relevant motivations exist for this analysis. The first is to an-
swer the following scientific question: Is the claim that people tend to avoid generating center-embedded
sentences language universal? This is unclear since the observation that a center-embedded sentence is
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The top-down parser of Hayashi et al. (2012) is an exception, but its processing is not incremental.
2140
difficult to comprehend is from psycholinguistic studies mainly on English. The second motivation is
to verify whether a parser with the developed system can be viable for crosslinguistical study of human
language processing. There is evidence that a human parser cannot store elements of a small constant
number, such as three or four (Cowan, 2001). If our system confirms to such a severe constraint, we may
claim its cognitive plausibility across languages. We will pursue these questions using the multilingual
dependency treebanks from the CoNLL-X and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et
al., 2007).
In short, our contributions of this paper can be sketched as follows:
1. We formulate a transition system for dependency parsing with a left-corner strategy.
2. We characterize our transition system with its memory cost by simulating oracle transitions along
with other transition systems on the CoNLL multilingual treebanks. This is the first empirical study
of required memory for left-corner parsing in a crosslinguistical setting.
2 Memory cost of Parsing Algorithms
In this work, we focus on the memory cost for dependency parsing transition systems. While there
have been many studies concerning the memory cost for an algorithm in constituency parsing (Abney
and Johnson, 1991; Resnik, 1992), the same kind of study is rare in dependency parsing. This section
discusses the memory cost for the current dependency parsing transition systems. Before that, we first
review the known results in constituency parsing regarding memory cost.
2.1 Center-Embedding and the Left-Corner Strategy
10X
12d6X
8c2X
4b1a
3
5
7
9
11
13
2X
5X
9X
12d8c
10
13
4b
6
11
1a
3
7
2X
9X
12d5X
7c4b
6
8
10 13
1a
3
11
The structures on the right side are called left-branching,
right-branching, and center-embedding, respectively. Peo-
ple have difficulty when parsing center-embedded struc-
tures, while no difficulty with right-branching or left-
branching structures. An example of a center-embedded sentence is the rat [the cat [the dog chased]
bit] ate the cheese, which is difficult, but if we rewrite it as the cheese was eaten [by the rat [that bit the
cat [that chased the dog]]], which is a kind of right-branching structure, the parse becomes easier.
Abney and Johnson (1991) showed that top-down or bottom-up strategies
2
fail to predict this result.
For example, for the right-branching structure, a bottom-up strategy requires O(n) memory, since it
must first construct a subtree of c and d, but the center-embedded structure requires less memory. The
arc-eager left-corner strategy correctly predicts the difficulty of a center-embedded structure, which is
characterized by the following order of recognitions of nodes and arcs:
1. A node is enumerated when the subtree of its first child has been enumerated.
2. An arc is enumerated when two nodes it connects have been enumerated.
The numbers on the trees above indicate the order of recognition for this strategy. We can see that it
requires a constant memory for both right-branching and left-branching structures. For example, for
the right-branching structure, it reaches 7 after reading b, which means that a and b are connected by a
subtree. On the other hand, for the center-embedded structure, it reaches 6 after reading b, but a and b
cannot be connected at this point, requiring extra memory.
2.2 Transition-based Dependency Parsing
Next, we summarize the issues with current transition systems for dependency parsing with regards to
their memory cost. A transition-based dependency parser processes a sentence on a transition system,
which is defined as a set of configurations and a set of transitions between configurations (Nivre, 2008).
Each configuration has a stack preserving constructed subtrees on which we define the memory cost as a
function for each system.
2
We should distinguish between two types of characterizations of parsing: strategy and algorithm. A parsing strategy is
an abstract notion that defines ?a way of enumerating the nodes and arcs of parse trees? (Abney and Johnson, 1991), while
a parsing algorithm defines the implementation of that strategy, typically with push-down automata (Johnson-Laird, 1983;
Resnik, 1992). A parsing strategy is useful for characterizing the properties of each parser, and we concentrate on the strategy
for exposition of constituency parsing. For dependency parsing, we mainly discuss the algorithm, i.e., the transition system.
2141
a b c
(a)
a b c
(b)
a b c
(c)
X
X
cb
a
(d)
Figure 1: (a)?(c): Right-branching dependency
trees for three words; (d): the corresponding CNF.
Arc-standard Arc-eager Left-corner
left-branching O(1) O(1) O(1)
right-branching O(n) O(1 ? n) O(1)
center-embedding O(n) O(1 ? n) O(n)
Table 1: Order of the memory cost for each struc-
ture for each transition system. O(1 ? n) means
that it processes some structures with a constant
cost but some with a non-constant cost.
Arc-Standard We define the memory cost as the number of elements on the stack since all stack
elements are disjoint. In this setting, we can see that the arc-standard system has a problem for a right-
branching structure, such as a
y
b
y
c
y
? ? ? , in which the system first pushes all words on the stack before
connecting each pair of words, requiring O(n) memory. Nivre (2004) discussed the problem with this
system in greater detail, observing that its stack size grows when processing structures that become right-
branching when converted to the Chomsky normal form (CNF) of a context-free grammar (CFG). Figure
1 lists those dependency structures for three words, for which the system must construct a subtree of b
and c before connecting a to either, requiring extra memory. This is because the system builds a tree
bottom-up: each token collects all dependents before being attached to its head. In fact, the arc-standard
system is essentially equivalent to the push-down automaton of a CFG in the CNF with a bottom-up
strategy (Nivre, 2004), so it has the same property as the bottom-up parser for a CFG.
Arc-Eager In the arc-eager system, the stack contains sequences of tokens comprising connected com-
ponents, so we can define the memory cost as the number of connected components on the stack. With
this definition, we can partially resolve the problem with the arc-standard system. The arc-eager system
does not incur any cost for processing the structure in Figure1(a) and a
y
b
y
c
y
? ? ? since it can connect
all tokens on the stack (Nivre, 2004). Because its construction is no longer pure bottom-up, it is difficult
to formally characterize the cost based on the type of tree structure. However, this transition system can-
not correctly predict difficulties with center-embedding because the cost never increases as long as all
dependency arcs are left-to-right, e.g., a sentence a
y
b
y
c d becomes center-embedding when converted
to a CNF, but it does not incur any cost for it. Note that this system still incurs cost for some right-
branching structures, such as in Figures 1(b?c), and some center-embedded structures. Therefore, for the
arc-eager system, it is complicated to discuss the required order of memory cost. We summarize these
results in Table 1. Our goal is to develop an algorithm with the properties of the last column, requiring
non-constant memory for only center-embedded structures.
Other systems All systems where stack elements cannot be connected have the same problem as the
arc-standard system because of their bottom-up constructions, including the hybrid system of Kuhlmann
et al. (2011). Kitagawa and Tanaka-Ishii (2010) and Sartorio et al. (2013) present an interesting variant,
which attaches a node to another node that may not be the head of a subtree on the stack. We can use
the same reasoning for the arc-eager system for these systems: they sometimes do not incur costs for
center-embedded structures, while they incur a non-constant cost for some right-branching structures.
3 Left-corner Dependency Parsing
We now discuss the construction of our transition system with the left-corner strategy. Resnik (1992)
proposed a push-down recognizer for a CFG. In the following, we instead characterize his algorithm by
inference rules, which are more intuitive and helpful to adapt the idea for dependency parsing.
Prediction:
B
?????? A?B C
A
CB
Composition:
A
CB D
???????? C?D E
A
C
ED
B
Prediction and Composition There are two
characteristic operations in the push-down recog-
nizer of Resnik (1992): prediction and composi-
tion. We show inference rules of these operations
on the right side:
Prediction is used to predict the parent node and
the sibling of a recognized subtree when the sub-
2142
SHIFT (?, j|?,A) 7? (?|?j?, ?, A)
INSERT (?|??
?
1
|i|x(?)?), j|?,A) 7? (?|??
?
1
|i|j?, ?, A ? {(i, j)} ? {?
k??
(j, k))
LEFT-PRED (?|??
11
, ? ? ? ?, ?, A) 7? (?|?x(?
11
)?, ?, A)
RIGHT-PRED (?|??
11
, ? ? ? ?, ?, A) 7? (?|??
11
, x(?)?, ?, A)
LEFT-COMP (?|??
?
2
|x(?)?|??
11
, ? ? ? ?, ?, A) 7? (?|??
?
2
|x(? ? {?
11
})?, ?, A)
RIGHT-COMP (?|??
?
2
|x(?)?|??
11
, ? ? ? ?, ?, A) 7? (?|??
?
2
|?
11
|x(?)?, ?, A ? {?
k??
(?
11
, k)})
Figure 2: Actions of the left-corner transition system.
LEFT-PRED: RIGHT-PRED:
a
x
a
a
X(x)
xa
a
a
x
a
X(a)
xa
LEFT-COMP: RIGHT-COMP:
a
b
x
b
x
a
a
X(b)
xb
X(b)
X(x)
xa
b
a
b
x
b
a
x
a
X(b)
xb
X(b)
X(a)
xa
b
Figure 3: Correspondences of reduce actions between dependency and CFG. Nonterminal X(t) means
that its lexical head is t. We only show minimal example subtrees for simplicity. However, a can have
an arbitrary number of children, so can b or x, as long as x is on a right spine and has no right children.
tree is complete and its parent node is not yet recognized. Composition composes two subtrees by first
predicting the parent and the sibling of a recognized subtree then immediately connecting trees by iden-
tifying the same node on two trees (C, in this case). This is used when the parent node of a completed
subtree has already been predicted as a part of another tree in a top-down fashion.
Dummy Node We now turn to the discussion of dependency parsing. The key characteristic of our
transition system is the introduction of a dummy node on a subtree, which is needed to represent a
subtree containing some predicted structures as in constituency subtrees for Resnik?s recognizer. To get
an intuition of the parser actions, we present a simulation of transitions for the sentence in Figure 1(b),
of which current systems fail to predict its difficulty. Our system first shifts a then conducts a kind of
prediction operation, resulting in a subtree
x
a
, where x is a dummy node. This means that we predict
that a will become a left dependent of an incoming word. Next, it shifts b to the stack then conducts a
composition operation to obtain a tree
x
a
b
. It finally inserts c to the position of x, recovering the tree.
Transition system As in many other transition systems, a configuration for our system is a tuple c =
(?, ?,A), where ? is a stack, and we use a vertical bar to signify an append operation, e.g., ? = ?
?
|?
1
denoting ?
1
is the top most element of the stack ?, and ? is an input buffer consisting of token indexes
not processed yet. ? = j|?
?
means j is a first element of ?, and A ? V
w
? V
w
is a set of arcs given V
w
,
a set of token indexes for a sentence w.
Stack:
w
2
w
1
x
w
3
w
5
w
4
w
6
w
7
? = [?2, x({3, 5})?, ?6, 7?]
? = [8, 9, ? ? ? , n]
A = {(2, 1), (5, 4), (6, 7)}
Each element of a stack is a list representing a right
spine of a subtree, as in Kitagawa and Tanaka-Ishii
(2010) and Sartorio et al. (2013). A right spine ?
i
=
??
i1
, ?
i2
, ? ? ? , ?
ik
? consists of all nodes in a descending
path from the head of ?
i
, i.e., ?
i1
, taking the rightmost
child at each step. We also write ?
i
= ?
?
i
|?
ik
meaning that ?
ik
is the right most node of spine ?
i
. Each
element of ?
i
is an index of a token in a sentence, or a dummy node x(?), where ? is a set of the left
dependents of x. The figure above depicts an example of the configuration, where the i-th word in a
sentence is written as w
i
on the stack.
In the following, we say a right spine ?
i
is complete if it does not contain any dummy nodes, while
?
i
containing a dummy node is referred to as incomplete. Our transition system uses six actions, two of
2143
which are shift actions and four are reduce actions. All actions are defined in Figure 2.
Shift Actions There are two kinds of shift actions: SHIFT and INSERT
3
. SHIFT moves a token from
the top of the buffer to the stack. INSERT replaces a dummy node on the top of the stack with a token
from the top of the buffer. This adds arcs from/to tokens connected to the dummy node. Note that this
action can be performed for a configuration where x(?) is the top of ?
1
or ? is empty, in which case
arcs (i, j) or ?
k??
(j, k) are not added. Resnik (1992) does not define this action, but instead uses a
verification operation (Rule 9). One can view our INSERT action as a composition of two actions: SHIFT
and a verification. We note that after these shift actions, the top element of the stack must be complete.
Reduce Actions Reduce actions create new arcs for subtrees on the stack. LEFT-PRED and RIGHT-
PRED correspond to the predictions of the CFG counterpart. Figure 3 describes these transitions for
minimal subtrees. LEFT-PRED assigns a dummy node x as the head of a (this corresponds to ?
11
), while
RIGHT-PRED creates x as a new right dependent. When we convert the resulting tree into a CNF, we can
see that the difference between these two operations lies in the predicted parent node of a: LEFT-PRED
predicts a nonterminal X(x), i.e., it predicts that the head of this subtree is the head of the predicting
sibling node, while RIGHT-PRED predicts that the head is a. Note that different from CFG rules, we do
not have to predict the actual sibling node; rather, we can abstract this predicted node as a dummy node
x. A similar correspondence holds between the composition actions: RIGHT-COMP and LEFT-COMP.
We note that to obtain a valid tree, shift and reduce actions must be performed alternatively. We
can prove this as follows: Let c = (?|?
2
|?
1
, ?, A). Since reduce actions turn an incomplete ?
1
into a
complete subtree, we cannot perform two consecutive reduce actions. Shift actions make ?
1
complete.
After a shift action, we cannot perform INSERT since it requires ?
i
to be incomplete; if we perform
SHIFT, the top two elements on the stack become complete, but we cannot connect these two trees since
the only way to connect two trees on the stack is composition, but this requires ?
2
to be incomplete.
Defining Oracle The oracle for a transition system is a function that returns a correct action given the
current configuration and a set of gold arcs. It is typically used for training a parser (Nivre, 2008), but
we define it to analyze the behavior of our system on treebank sentences.
First, we show that our system has the spurious ambiguity, and discuss its implications. Consider a
sentence a
x
b
y
c, which can be parsed with two different action sequences as follows:
1. SHIFT? LEFT-PRED? INSERT? RIGHT-PRED? INSERT
2. SHIFT? LEFT-PRED? SHIFT? RIGHT-COMP? INSERT
The former INSERTs b at step 3, then RIGHT-PREDs to wait for a right dependent (c). The latter, on the
other hand, SHIFTs b at step 3, then RIGHT-COMPs to combine two subtrees (a
x
x and b) to obtain a
tree a
x
b
y
x. These ambiguities between action sequences and the resulting tree are referred to as the
spurious ambiguity. Next, we analyze the underlying differences between these two operations. We
argue that the difference lies in the form of the recognized constituency tree: The former RIGHT-PREDs
at step 4, which means that it recognizes a constituency of the form ((a b) c), while the latter recognizes
(a (b c)) due to its RIGHT-COMP operation. Therefore, the spurious ambiguity of our system is caused by
the ambiguity of converting a dependency tree to a constituency tree. Recently, some transition systems
have exploited similar ambiguities using dynamic oracles (Goldberg and Nivre, 2013; Sartorio et al.,
2013; Honnibal et al., 2013). The same type of analysis might be possible for our system, but we leave
it for future work; here we only present a static oracle and discuss its properties.
Since our system performs shift and reduce actions interchangeably, we need two functions to define
the oracle. Let c = (?|?
2
|?
1
, ?, A). The next shift action is determined as follows:
? INSERT: if ?
1
= ??
?
1
|i|x(?)? and (i, j) ? A
g
and j has no dependents in ? (if i exists) or ?k ?
?; (j, k) ? A
g
(otherwise).
? SHIFT: otherwise.
The next reduce action is determined as follows:
3
We use small caps to refer to a specific action, e.g., SHIFT, while ?shift? refers to an action type.
2144
? LEFT-COMP: if ?
2
= ??
?
2
|i|x(?)?, ?
1
= ??
11
, ? ? ? ?, ?
11
has no dependents in ?, and ?
11
can be a
left dependent of x: i?s next dependent is the head of ?
11
(if i exists) or k ? ? and ?
11
share the
same head (otherwise).
? RIGHT-COMP: if ?
2
= ??
?
2
|i|x(?)?, ?
1
= ??
11
, ? ? ? ?, ?
11
has one more dependent in ?, and ?
11
can
be insertable at the position of x: (i, ?
11
) ? A
g
or ?k ? ?; (?
11
, k) ? A
g
.
? RIGHT-PRED: if ?
1
= ??
11
, ? ? ? ? and ?
11
has one more dependent in ?.
? LEFT-PRED: otherwise.
Each condition checks whether we can obtain the gold dependency arcs after the transition. This oracle
follows the strategy of ?compose or insert when possible?. As we saw in the example, sometimes INSERT
and SHIFT both can be valid to recover the gold arcs; however, we always select INSERT. Sometimes the
same ambiguity occurs between LEFT-COMP and LEFT-PRED or RIGHT-COMP and RIGHT-PRED, but
we always prefer composition.
As we saw above, the spurious ambiguity of our system occurs when the conversion from a depen-
dency tree to a constituency tree is not deterministic. This oracle has the property that its recognized
constituency tree corresponds to the one that can be obtained by constructing all left-arcs first given a de-
pendency tree. For example, in the above example for a
x
b
y
c, we select action sequences 1 to recognize
a constituency of ((a b) c). We can prove this property by showing that the algorithm always collects all
left-arcs for a head before any right-arcs; Not doing INSERT or composition when possible means that
we create a right-arc for a head when the left-arcs are not yet completed. We can also verify that this
algorithm can parse all no-center-embedded sentences in a CNF converted in this manner with the stack
depth never exceeding three, requiring non-constant memory for only center-embedded structures.
4 Memory Cost Analysis
To characterize our transition system, we compare it to other systems by observing the incurred memory
cost during running oracle transitions for sentences on a set of typologically diverse languages. For this
analysis, we aim to verify the language universality of the claim: naturally occurring sentences should
be parsed with a left-corner parser with less required memory.
Settings We collect 18 treebanks from the CoNLL-X and 2007 shared tasks (Buchholz and Marsi,
2006; Nivre et al., 2007). Some languages were covered by both shared tasks; we use only 2007 data.
We remove sentences with non-projective arcs (Nivre, 2008) or without any root nodes. We follow the
common practice adding a dummy root token to each sentence. This token is placed at the end of each
sentence, as in Ballesteros and Nivre (2013), since it does not change the cost on sentences with one root
token on all systems.
We compare three transition systems: arc-standard, arc-eager, and left-corner. For each system, we
perform oracle transitions for all sentences and languages, measuring the memory cost for each configu-
ration defined as follows. For the arc-standard and left-corner systems, we use the number of elements on
the stack. This arc-standard system uses the original formulation of Nivre (2003), connecting two items
on the stack at the reduce action. For the arc-eager system, we use the number of connected components.
The system can create a subtree at the beginning of a buffer, in which case we add 1 to the cost.
We run a static oracle for each system. For the left-corner system, we implemented the algorithm
presented in Section 3. For the arc-standard and arc-eager systems, we implemented an oracle preferring
reduce actions over shift, which can minimizes the memory cost.
Memory costs for general sentences For each language, we count the number of configurations for
each memory cost during performing oracles on all sentences. In Figure 4, we show the cumulative
frequencies of configurations having each memory cost (see solid lines in the figure). These lines can
answer the question: What memory cost is required to cover X% of configurations when recovering all
gold trees? Note that comparing absolute values are not meaningful since the minimal cost to construct
an arc is different for each system, e.g., the arc-standard system requires at least two items on the stack,
2145
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Arabic
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Basque
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Bulgarian
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Catalan
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Chinese
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Czech
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Danish
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Dutch
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
English
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Greek
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Hungarian
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Italian
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Japanese
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Portuguese
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Slovene
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Spanish
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Swedish
1 2 3 4 5 6
7
8 9 10
50
60
70
80
90
100
Turkish
Arc-standard
Arc-standard (random)
Arc-eager
Arc-eager (random)
Left-corner
Left-corner (random)
Figure 4: Crosslinguistic comparison of cumulative frequency of memory cost when processing all sen-
tences for each system. For example, in Arabic, for the arc-eager system, around 90% of all config-
urations incurred memory cost of ? 5. Dotted lines (random) are results on the sentences, which are
randomly reordered while preserving the graph structure and projectivity.
while the arc-eager system can create a right arc if the stack contains one element. Instead, we focus on
the universality of each system?s behavior for different languages.
As we discussed in section 2.2, the arc-standard system can process only left-branching structures with
a constant memory, which are typical in head-final languages such as Japanese or Turkish, and we can
2146
see this tendency. The system behaves poorly in many other languages.
The arc-eager and left-corner systems behave similarly for many languages, but we can see that there
are some languages for which the left-corner system behaves similarly to other languages, while the arc-
eager system requires larger cost; Arabic, Hungarian, or Japanese, for example. In fact, except Arabic,
the left-corner system reaches 98% of configurations with a memory cost of? 3, which indicates that the
property of the left-corner system requiring less memory is more universal than that of other systems.
Comparison to randomized sentences One might wonder that the results above come from the nature
of left-corner parsing reducing the stack size, not from the bias in language avoiding center-embedded
structures. To partially answer this question, we conduct another experiment comparing oracle transitions
on original treebank sentences and on wrong sentences. We create these wrong sentences by using the
method from Gildea and Temperley (2007). We reorder words in each sentence by first extracting a
directed graph then randomly reordering the children of each node while preserving projectivity. The
dotted lines in Figure 4 denotes the results of randomized sentences for each system.
There are notable differences in required memory between original and random sentences for many
languages. This result indicates that our system can parse with less memory for only naturally occurring
sentences. For Chinese and Hungarian, the differences are subtle. However, the differences are also
small for the other systems, which implies that these corpora have some biases on graphs reducing the
differences.
5 Related Work and Discussion
To the best of our knowledge, parsing with a left-corner strategy has only been studied for constituency.
Roark (2001) proposed a top-down parser for a CFG with a left-corner grammar transform (Johnson,
1998), which is essentially the same as left-corner parsing but enables several extensions in a unified
framework. Roark et al. (2009) studied the psychological plausibility of Roark?s parser, observing that it
fits well to human reading time data. Another model with a left-corner strategy is Schuler et al. (2010):
they observed that the transformed grammar of English requires only limited memory, proposing a finite
state approximation with a hierarchical hidden Markov model. This parser was later extended by van
Schijndel and Schuler (2013), which defined a left-corner parser for constituency with shift and reduce
actions. In fact, they used the same kind of actions as our transition system: shift, insert, predict, and
composition. Though they did not mentioned explicitly, we showed how to construct a left-corner parsing
algorithm with these actions by decomposing the push-down recognizer of Resnik (1992). These are
examples of broad-coverage parsing models with cognitively plausibility, which has recently received
considerable attention in interdisciplinary research on psycholinguistics and computational linguistics
(Schuler et al., 2010; Keller, 2010; Demberg et al., 2013).
Differently from previous models, our target is dependency. A dependency-based cognitively plausible
model is attractive, especially from a crosslinguistical viewpoint. Keller (2010) argued that current
models only work for English, or German in few exceptions, and the importance of crosslinguistically
valid models of human language processing. There has been some attempts to use a transition system for
studying human language processing (Boston and Hale, 2007; Boston et al., 2008), so it is interesting to
compare automatic parsing behaviors with various transition systems to human processing.
We introduced a dummy node for representing a subtree with an unknown head or dependent. Re-
cently, Menzel and colleagues (Beuck and Menzel, 2013; Kohn and Menzel, 2014) have also studied
dependency parsing with a dummy node. While conceptually similar, the aim of introducing a dummy
node is different between our approach and theirs: We need a dummy node to represent a subtree cor-
responding to that in Resnik?s algorithm, while they introduced it to confirm that every dependency tree
on a sentence prefix is fully connected. This difference leads to a technical difference; a subtree of their
parser can contain more than one dummy node, while we restrict each subtree to containing only one
dummy node on a right spine.
Our experiments in section 4 can be considered as a study on functional biases existing in language or
language evolution (Jaeger and Tily, 2011). In computational linguistics, Gildea and Temperley (2007;
2010) examined the bias on general sentences called dependency length minimization (DLM), which
2147
argues that grammar should favor the dependency structures that reduce the sum of dependency arc
lengths. They reordered English and German treebank sentences with various criteria: original, random
with projectivity, and optimal that minimizes the sum of dependency lengths. They observed that the
word order of English fits very well to the optimal ordering, while German does not. We examined the
universality of the bias to reduce memory cost for left-corner parsing. Although we cannot compare the
incurred cost with the optimal reordered sentences, our results on original sentences, in which there are
few configurations requiring the stack depth? 4, suggest the bias to avoid center-embedded structures is
language universal. It will be interesting to analyze in more detail the relationships between DLM and the
bias of our system since the two biases are not independent, e.g., center-embed structures typically appear
with longer dependencies. Are there languages that do not hold DLM while requiring less memory, or
vice versa? For these analyses, we might have to take care of the grammar construction, e.g., there
are several definitions for coordination structures for dependency grammars (Popel et al., 2013). The
functional views discussed above might shed some light on the desired construction for these cases.
6 Conclusion
We have pointed out that the memory cost on current transition systems for dependency parsing do not
coincide with observations in people, proposing a system with a left-corner strategy. Our crosslinguistical
analysis confirms the universality of the claim that people avoid generating center-embedded sentences,
which also suggests that it is worthy for crosslinguistical studies of human language processing.
As a next stage, we are seeking to train a parser model as in other transition systems with a discrimi-
native framework such as a structured perceptron (Zhang and Nivre, 2011; Huang and Sagae, 2010). A
parser with our transition system might also be attractive for the problem of grammar induction, where
recovering dependency trees are a central problem (Klein and Manning, 2004), and where some linguis-
tic biases have been exploited, such as reducibility (Mare?cek and
?
Zabokrtsk?y, 2012) or acoustic cues
(Pate and Goldwater, 2013). Recently, Cohen et al. (2011) showed how to interpret shift-reduce actions
as a generative model; combining their idea and our transition system might enable the model to exploit
memory biases that exist in natural sentences.
Finally, dependency grammars are suitable for treating non-projective structures. Extensions for tran-
sition systems have been proposed to handle non-projective structures with additional actions (Attardi,
2006; Nivre, 2009). Although our system cannot handle non-projective structures, a similar extension
might be possible, which would enable a left-corner analysis for non-projective structures.
Acknowledgements
We thank Pontus Stenetorp and anonymous reviewers for their valuable feedbacks on a preliminary
version of this paper.
References
Steven Abney and Mark Johnson. 1991. Memory requirements and local ambiguities of parsing strategies. Journal
of Psycholinguistic Research, 20(3):233?250.
Giuseppe Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Proceedings
of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 166?170, New York
City, June. Association for Computational Linguistics.
Miguel Ballesteros and Joakim Nivre. 2013. Going to the roots of dependency parsing. Computational Linguis-
tics, 39(1):5?13.
Niels Beuck and Wolfgang Menzel. 2013. Structural prediction in incremental dependency parsing. In Alexander
Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 7816 of Lecture Notes in
Computer Science, pages 245?257. Springer Berlin Heidelberg.
Marisa Ferrara Boston and John T. Hale. 2007. Garden-pathing in a statistical dependency parser. In Proceedings
of the Midwest Computational Linguistics Colloquium, West Lafayette, IN. Midwest Computational Linguistics
Colloquium.
2148
Marisa Ferrara Boston, John T. Hale, Umesh Patil, Reinhold Kliegl, and Shravan Vasishth. 2008. Parsing costs as
predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus. Journal of Eye Movement
Research, 2(1):1?12.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings
of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149?164, New York
City, June. Association for Computational Linguistics.
Shay B. Cohen, Carlos G?omez-Rodr??guez, and Giorgio Satta. 2011. Exact inference for generative probabilistic
non-projective dependency parsing. In Proceedings of the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1234?1245, Edinburgh, Scotland, UK., July. Association for Computational
Linguistics.
Nelson Cowan. 2001. The magical number 4 in short-term memory: A reconsideration of mental storage capacity.
Behavioral and Brain Sciences, 24(1):87?114.
Vera Demberg, Frank Keller, and Alexander Koller. 2013. Incremental, predictive parsing with psycholinguisti-
cally motivated tree-adjoining grammar. Computational Linguistics, 39(4):1025?1066.
Daniel Gildea and David Temperley. 2007. Optimizing grammars for minimum dependency length. In Proceed-
ings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 184?191, Prague, Czech
Republic, June. Association for Computational Linguistics.
Daniel Gildea and David Temperley. 2010. Do grammars minimize dependency length? Cognitive Science,
34(2):286?310.
Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. TACL,
1:403?414.
Katsuhiko Hayashi, Taro Watanabe, Masayuki Asahara, and Yuji Matsumoto. 2012. Head-driven transition-
based parsing with top-down prediction. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 657?665, Jeju Island, Korea, July. Association for
Computational Linguistics.
Matthew Honnibal, Yoav Goldberg, and Mark Johnson. 2013. A non-monotonic arc-eager transition system
for dependency parsing. In Proceedings of the Seventeenth Conference on Computational Natural Language
Learning, pages 163?172, Sofia, Bulgaria, August. Association for Computational Linguistics.
Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077?1086, Uppsala,
Sweden, July. Association for Computational Linguistics.
T.Florian Jaeger and Harry Tily. 2011. On language utility: Processing complexity and communicative efficiency.
Wiley Interdisciplinary Reviews: Cognitive Science, 2(3):323?335.
P. N. Johnson-Laird. 1983. Mental models: towards a cognitive science of language, inference, and consciousness.
Harvard University Press, Cambridge, MA, USA.
Mark Johnson. 1998. Finite-state approximation of constraint-based grammars using left-corner grammar trans-
forms. In Christian Boitet and Pete Whitelock, editors, COLING-ACL, pages 619?623. Morgan Kaufmann
Publishers / ACL.
Frank Keller. 2010. Cognitively plausible models of human language processing. In Proceedings of the ACL 2010
Conference Short Papers, pages 60?67, Uppsala, Sweden, July. Association for Computational Linguistics.
Kotaro Kitagawa and Kumiko Tanaka-Ishii. 2010. Tree-based deterministic dependency parsing ? an application
to nivre?s method ?. In Proceedings of the ACL 2010 Conference Short Papers, pages 189?193, Uppsala,
Sweden, July. Association for Computational Linguistics.
Dan Klein and Christopher Manning. 2004. Corpus-based induction of syntactic structure: Models of depen-
dency and constituency. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 478?485, Barcelona, Spain, July.
Arne Kohn and Wolfgang Menzel. 2014. Incremental predictive parsing with turboparser. In Proceedings of the
ACL 2014 Conference Short Papers, Baltimore, USA, June. Association for Computational Linguistics.
2149
Marco Kuhlmann, Carlos G?omez-Rodr??guez, and Giorgio Satta. 2011. Dynamic programming algorithms for
transition-based dependency parsers. In Proceedings of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies, pages 673?682, Portland, Oregon, USA, June. Association
for Computational Linguistics.
David Mare?cek and Zden?ek
?
Zabokrtsk?y. 2012. Exploiting reducibility in unsupervised dependency parsing. In
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 297?307, Jeju Island, Korea, July. Association for Computational
Linguistics.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007, pages 915?932.
Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT), pages 149?160.
Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Frank Keller, Stephen Clark,
Matthew Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bring-
ing Engineering and Cognition Together, pages 50?57, Barcelona, Spain, July. Association for Computational
Linguistics.
Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics,
34(4):513?553.
Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 351?359, Suntec, Singapore, August. Association for Computational
Linguistics.
John K. Pate and Sharon Goldwater. 2013. Unsupervised dependency parsing with acoustic cues. TACL, 1:63?74.
Martin Popel, David Mare?cek, Jan
?
St?ep?anek, Daniel Zeman, and Zden?ek
?
Zabokrtsk?y. 2013. Coordination struc-
tures in dependency treebanks. In ACL, pages 517?527.
Philip Resnik. 1992. Left-corner parsing and psychological plausibility. In COLING, pages 191?197.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and Christophe Pallier. 2009. Deriving lexical and syntactic
expectation-based measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings
of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324?333, Singapore,
August. Association for Computational Linguistics.
Brian Edward Roark. 2001. Robust Probabilistic Predictive Syntactic Processing: Motivations, Models, and
Applications. Ph.D. thesis, Providence, RI, USA. AAI3006783.
Francesco Sartorio, Giorgio Satta, and Joakim Nivre. 2013. A transition-based dependency parser using a dynamic
parsing strategy. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 135?144, Sofia, Bulgaria, August. Association for Computational Linguistics.
William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2010. Broad-coverage parsing using
human-like memory constraints. Computational Linguistics, 36(1):1?30.
Marten van Schijndel and William Schuler. 2013. An analysis of frequency- and memory-based processing costs.
In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 95?105, Atlanta, Georgia, June. Association for Computa-
tional Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-
nologies, pages 188?193, Portland, Oregon, USA, June. Association for Computational Linguistics.
2150
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1180?1190,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improvements to the Bayesian Topic N -gram Models
Hiroshi Noji??
noji@nii.ac.jp
Daichi Mochihashi??
daichi@ism.ac.jp
?Graduate University for Advanced Studies
?National Institute of Informatics, Tokyo, Japan
?The Institute of Statistical Mathematics, Tokyo, Japan
Yusuke Miyao??
yusuke@nii.ac.jp
Abstract
One of the language phenomena that n-gram
language model fails to capture is the topic in-
formation of a given situation. We advance the
previous study of the Bayesian topic language
model by Wallach (2006) in two directions:
one, investigating new priors to alleviate the
sparseness problem caused by dividing all n-
grams into exclusive topics, and two, develop-
ing a novel Gibbs sampler that enables moving
multiple n-grams across different documents
to another topic. Our blocked sampler can
efficiently search for higher probability space
even with higher order n-grams. In terms of
modeling assumption, we found it is effective
to assign a topic to only some parts of a docu-
ment.
1 Introduction
N -gram language model is still ubiquitous in NLP,
but due to its simplicity it fails to capture some im-
portant aspects of language, such as difference of
word usage in different situations, sentence level
syntactic correctness, and so on. Toward language
model that can consider such a more global con-
text, many extensions have been proposed from
lexical pattern adaptation, e.g., adding cache (Je-
linek et al, 1991) or topic information (Gildea and
Hofmann, 1999; Wallach, 2006), to grammaticality
aware models (Pauls and Klein, 2012).
Topic language models are important for use in
e.g., unsupervised language model adaptation: we
want a language model that can adapt to the do-
main or topic of the current situation (e.g., a doc-
ument in SMT or a conversation in ASR) automat-
ically and select the appropriate words using both
topic and syntactic context. Wallach (2006) is one
such model, which generate each word based on lo-
cal context and global topic information to capture
the difference of lexical usage among different top-
ics.
However, Wallach?s experiments were limited to
bigrams, a toy setting for language models, and ex-
periments with higher-order n-grams have not yet
been sufficiently studied, which we investigate in
this paper. In particular, we point out the two funda-
mental problems caused when extending Wallach?s
model to a higher-order: sparseness caused by di-
viding all n-grams into exclusive topics, and local
minima caused by the deep hierarchy of the model.
On resolving these problems, we make several con-
tributions to both computational linguistics and ma-
chine learning.
To address the first problem, we investigate incor-
porating a global language model for ease of sparse-
ness, along with some priors on a suffix tree to cap-
ture the difference of topicality for each context,
which include an unsupervised extension of the dou-
bly hierarchical Pitman-Yor language model (Wood
and Teh, 2009), a Bayesian generative model for su-
pervised language model adaptation. For the sec-
ond inference problem, we develop a novel blocked
Gibbs sampler. When the number of topics is K
and vocabulary size is V , n-gram topic model has
O(KV n) parameters, which grow exponentially to
n, making the local minima problem even more se-
vere. Our sampler resolves this problem by moving
many customers in the hierarchical Chinese restau-
rant process at a time.
We evaluate various models by incremental cal-
culation of test document perplexity on 3 types of
corpora having different size and diversity. By com-
bining the proposed prior and the sampling method,
our Bayesian model achieve much higher accura-
cies than the naive extension of Wallach (2006) and
shows results competitive with the unigram rescal-
ing (Gildea and Hofmann, 1999), which require
1180
huge computational cost at prediction, with much
faster prediction time.
2 Basic Models
All models presented in this paper are based on the
Bayesian n-gram language model, the hierarchical
Pitman-Yor process language model (HPYLM). In
the following, we first introduce the HPYLM, and
then discuss the topic model extension of Wallach
(2006) with HPYLM.
2.1 HPYLM
Let us first define some notations. W is a vocabulary
set, V = |W | is the size of that set, and u, v, w ?W
represent the word type.
The HPYLM is a Bayesian treatment of the n-
gram language model. The generative story starts
with the unigram word distribution G?, which is
a V -dimensional multinomial where G?(w) repre-
sents the probability of word w. The model first
generates this distribution from the PYP as G? ?
PYP(a, b,G0), where G0 is a V -dimensional uni-
form distribution (G0(u) = 1V ;?u ? W ) and
acts as a prior for G? and a, b are hyperparameters
called discount and concentration, respectively. It
then generates all bigram distributions {Gu}u?W as
Gu ? PYP(a, b,G?). Given this distributions, it
successively generates 3-gram distributions Guv ?
PYP(a, b,Gu) for all (u, v) ? W 2 pairs, which
encode a natural assumption that contexts having
common suffix have similar word distributions. For
example, two contexts ?he is? and ?she is?, which
share the suffix ?is?, are generated from the same
(bigram) distribution Gis, so they would have simi-
lar word distributions. This process continues until
the context length reaches n ? 1 where n is a pre-
specified n-gram order (if n = 3, the above example
is a complete process). We often generalize this pro-
cess using two contexts h and h? as
Gh ? PYP(a, b,Gh?), (1)
where h = ah?, in which a is a leftmost word of h.
We are interested in the posterior word distribu-
tion following a context h. Our training corpus w
is a collection of n-grams, from which we can cal-
culate the posterior p(w|h,w), which is often ex-
plained with the Chinese restaurant process (CRP):
p(w|h,w) = chw ? athwch? + b
+
ath? + b
ch? + b
p(w|h?,w),
(2)
where chw is an observed count of n-gram hw called
customers, while thw is a hidden variable called ta-
bles. ch? and th? represents marginal counts: ch? =?
w chw and th? =
?
w thw. This form is very
similar to the well-known Kneser-Ney smoothing,
and actually the Kneser-Ney can be understood as a
heuristic approximation of the HPYLM. This char-
acteristic enables us to build the state-of-the-art lan-
guage model into a more complex generative model.
2.2 Wallach (2006) with HPYLM
Wallach (2006) is a generative model for a docu-
ment collection that combines the topic model with
a Bayesian n-gram language model. The latent
Dirichlet alocation (LDA) (Blei et al, 2003) is the
most basic topic model, which generates each word
in a document based on a unigram word distribution
defined by a topic allocated to that word. The bi-
gram topic model of Wallach (2006) simply replaces
this unigram word distribution (a multinomial) for
each topic with a bigram word distribution 1. In
other words, ordinary LDA generates word condi-
tioning only on the latent topic, whereas the bigram
topic model generates conditioning on both the la-
tent topic and the previous word, as in the bigram
language model. Extending this model with a higher
order n-gram is trivial; all we have to do is to replace
the bigram language model for each topic with an n-
gram language model.
The formal description of the generative story of
this n-gram topic model is as follows. First, for
each topic k ? 1, ? ? ? ,K, where K is the num-
ber of topics, the model generates an n-gram lan-
guage model Gkh.2 These n-gram models are gen-
erated by the PYP, so Gkh ? PYP(a, b,Gkh?) holds.
The model then generate a document collection. For
each document j ? 1, ? ? ? , D, it generates a K-
1This is the model called prior 2 in Wallach (2006); it con-
sistently outperformed the other prior. Wallach used the Dirich-
let language model as each topic, but we only explore the model
with HPYLM because its superiority to the Dirichlet language
model has been well studied (Teh, 2006b).
2We sometimes denote Gkh to represent a language model of
topic k, not a specific multinomial for some context h, depend-
ing on the context.
1181
dimensional topic distribution ?j by a Dirichlet dis-
tribution Dir(?) where ? = (?1, ?2, ? ? ? , ?K) is a
prior. Finally, for each word position i ? 1, ? ? ? , Nj
where Nj is the number of words in document j, i-
th word?s topic assignment zji is chosen according
to ?j , then a word type wji is generated from Gzjihji
where hji is the last n? 1 words preceding wji. We
can summarize this process as follows:
1. Generate topics:
For each h ? ?, {W}, ? ? ? , {W}n?1:
For each k ? 1, ? ? ? ,K:
Gkh ? PYP(a, b,Gkh?)
2. Generate corpora:
For each document j ? 1, ? ? ?D:
?j ? Dir(?)
For each word position i ? 1, ? ? ? , Nj :
zji ? ?j
wji ? Gzjihji
3 Extended Models
One serious drawback of the n-gram topic model
presented in the previous section is sparseness. At
inference, as in LDA, we assign each n-gram a topic,
resulting in an exclusive clustering of n-grams in
the corpora. Roughly speaking, when the number
of topics is K and the number of all n-grams in the
training corpus is N , a language model of topic k,
Gkh is learned using only about O(N/K) instances
of the n-grams assigned the topic k, making each
Gkh much sparser and unreliable distribution.
One way to alleviate this problem is to place an-
other n-gram model, say G0h, which is shared with
all topic-specific n-gram models {Gkh}Kk=1. How-
ever, what is the best way to use this special distribu-
tion? We explore two different approaches to incor-
porate this distribution in the model presented in the
previous section. In one model, the HIERARCHICAL
model, G0h is used as a prior for all other n-gram
models, where G0h exploits global statistics across
all topics {Gkh}. In the other model, the SWITCH-
ING model, no statistics are shared across G0h and
{Gkh}, but some words are directly generated from
G0h regardless of the topic distribution.
3.1 HIERARCHICAL Model
Informally, what we want to do is to establish hier-
archies among the global G0h and other topics {Gkh}.
In Bayesian formalism, we can explain this using an
???
???
???
???
???
???
Figure 1: Variable dependencies of the HIERARCHICAL
model. {u, v} are word types, k is a topic and each Gkh
is a multinomial word distribution. For example, G2uv
represents a word distribution following the context uv
in topic 2.
abstract distribution F as Gkh ? F(G0h). The prob-
lem here is making the appropriate choice for the
distribution F . Each topic word distribution already
has hierarchies among n? 1-gram and n-gram con-
texts as Gkh ? PYP(a, b,Gkh?). A natural solution
to this problem is the doubly hierarchical Pitman-
Yor process (DHPYP) proposed in Wood and Teh
(2009). Using this distribution, the new generative
process of Gkh is
Gkh ? PYP(a, b, ?Gkh? + (1? ?)G0h), (3)
where ? is a new hyperparameter that determines
mixture weight. The dependencies among G0h and
{Gkh} are shown in Figure 1. Note that the genera-
tive process of G0h is the same as the HPYLM (1).
Let us clarify the DHPYP usage differences be-
tween our model and the previous work of Wood and
Teh (2009). A key difference is the problem setting:
Wood and Teh (2009) is aimed at the supervised
adaptation of a language model for a specific do-
main, whereas our goal is unsupervised adaptation.
In Wood and Teh (2009), each Gkh for k ? 1, 2, ? ? ?
corresponds to a language model of a specific do-
main and the training corpus for each k is pre-
specified and fixed. For ease of data sparseness of
domain-specific corpora, latent model G0h exploits
shared statistics amongGkh for k = 1, 2, ? ? ? . In con-
trast, with our model, each Gkh is a topic, so it must
perform the clustering of n-grams in addition to ex-
1182
ploiting the latent G0h. This makes inference harder
and requires more careful design of ?.
Modeling of ? We can better understand the role
of ? in (3) by considering the posterior predictive
form corresponds to (2), which is written as
p(w|h, k,w) = c
k
hw ? atkhw
ckh? + b
+
atkh? + b
ckh? + b
q(w|h, k,w),
(4)
q(w|h, k,w) = ?p(w|h?, k,w) + (1? ?)p(w|h, 0,w),
where c, t with superscript k corresponds to the
count existing in topic k. This shows us that ? de-
termines the back-off behavior: which probability
we should take into account: the shorter context of
the same topic Gkh? or the full context of the global
model G0h. Wood and Teh (2009) shares this vari-
able across all contexts of the same length, for each
k, but this assumption may not be the best. For ex-
ample, after the context ?in order?, we can predict
the word ?to? or ?that?, and this tendency is unaf-
fected by the topic. We call this property of context
the topicality and say that ?in order? has weak topi-
cality. Therefore, we place ? as a distinct value for
each context h, which we share across all topics. We
designate this ? determined by h ?h in the follow-
ing. Moreover, similar contexts may have similar
values of ?h. For example, the two contexts ?of the?
and ?in the?, which share the suffix ?the?, both have
a strong topicality3. We encode this assumption by
placing hierarchical Beta distributions on the suffix
tree across all topics:
?h ? Beta(??h? , ?(1? ?h?)) = DP(?, ?h?), (5)
where DP is the hierarchical Dirichlet process (Teh
et al, 2006), which has only two atoms in {0,1} and
? is a concentration parameter. As in HPYLM, we
place a uniform prior ?0 = 1/2 on the base distribu-
tion of the top node (?? ? DP(?, ?0)).
Having generated the topic component of the
model, the corpus generating process is the same as
the previous model because we only change the gen-
erating process of Gkh for k = 1, ? ? ? ,K.
3These words can be used very differently depending on the
context. For example, in a teen story, ?in the room? or ?in the
school? seems more dominant than ?in the corpora? or ?in the
topic?, which is likely to appear in this paper.
3.2 SWITCHING Model
Our second extension also exploits the globalG0h, al-
beit differently than the HIERARCHICAL model. In
this model, the relationship of G0h to the other {Gkh}
is flat, not hierarchical: G0h is a special topic that can
generate a word. The model first generates each lan-
guage model of k = 0, 1, 2, ? ? ? ,K independently
as Gkh ? PYP(a, b,Gkh?). When generating a word,
it first determines whether to use global model G0h
or topic model {Gkh}Kk=1. Here, we use the ?h in-
troduced above in a similar way: the probability of
selecting k = 0 for the next word is determined by
the previous context. This assumption seems natu-
ral; we expect theG0h to mainly generate common n-
grams, and the topicality of each context determines
how common that n-gram might be. The complete
generative process of this model is written as fol-
lows:
1. Generate topics:
For each h ? ?, {V }, ? ? ? , {V }n?1:
?h ? DP(?, ??h)
For each k ? 0, ? ? ? ,K:
Gkh ? PYP(a, b,Gkh?)
2. Generate corpora:
For each document j ? 1, ? ? ?D:
?j ? Dir(?)
For each word position i ? 1, ? ? ? , Nj :
lji ? Bern(?hji)
If lji = 0: zji = 0
If lji = 1: zji ? ?j
wji ? Gzjihji
The difference between the two models is their
usage of the global model G0h. For a better under-
standing of this, we provide a comparison of their
graphical models in Figure 2.
4 Inference
For posterior inference, we use the collapsed Gibbs
sampler. In our models, all the latent variables are
{Gkh, ?h, ?j , z,?}, where z is the set of topic assign-
ments and ? = {a, b, ?,?} are hyperparameters,
which are treated later. We collapse all multinomials
in the model, i.e., {Gkh, ?h, ?j}, in which Gkh and ?h
are replaced with the Chinese restaurant process of
PYP and DP respectively. Given the training corpus
w, the target posterior distribution is p(z,S|w,?),
where S is the set of seating arrangements of all
restaurants. To distinguish the two types of restau-
rant, in the following, we refer the restaurant to indi-
1183
(a) HIERARCHICAL (b) SWITCHING
Figure 2: Graphical model representations of our two models in the case of a 3-gram model. Edges that only exist in
one model are colored.
cate the collapsed state of Gkh (PYP), while we refer
the restaurant of ?h to indicates the collapsed state
of ?h (DP). We present two different types of sam-
pler: a token-based sampler and a table-based sam-
pler. For both samplers, we first explain in the case
of our basic model (Section 2.2), and later discuss
some notes on our extended models.
4.1 Token-based Sampler
The token-based sampler is almost identical to
the collapsed sampler of the LDA (Griffiths and
Steyvers, 2004). At each iteration, we consider the
following conditional distribution of zji given all
other topic assignments z?ji and S?ji, which is the
set of seating arrangements with a customer corre-
sponds to wji removed, as
p(zji|z?ji,S?ji) ? p(zji|z?ji)p(wji|zji, hji,S?ji),
(6)
where p(wji|zji, hji,S?ji) =
ckhw ? atkhw
ckh? + b
+
atkh? + b
ckh? + b
p(wji|zji, hji,S?ji) (7)
is a predictive word probability under the topic zji,
and
p(zji|z?ji) =
n?jijk + ?k
Nj ? 1 +
?
k? ?k?
, (8)
where n?jijk is the number of words that is assigned
topic k in document j excluding wji, which is the
same as the LDA. Given the sampled topic zji, we
update the language model of topic zji, by adding
customer wji to the restaurant specified by zji and
context hji. See Teh (2006a) for details of these cus-
tomer operations.
HIERARCHICAL Adding customer operation is
slightly changed: When a new table is added to a
restaurant, we must track the label l ? {0, 1} indi-
cating the parent restaurant of that table, and add the
customer corresponding to l to the restaurant of ?h.
See Wood and Teh (2009) for details of this opera-
tion.
SWITCHING We replace p(zji|z?ji) with
p(zji|z?ji) =
?
?
?
p(lji = 0|hji) (zji = 0)
p(lji = 1|hji) ?
n?jijk +?k
?
k 6=0 n
?ji
jk +
?
k? ?k?
(zji 6= 0),
(9)
where p(lji|hji) is a predictive of lji given by the
CRP of ?hji . We need not assign a label to a new
table, but rather we always add a customer to the
restaurant of ?h according to whether the sampled
topic is 0 or not.
4.2 Table-based Sampler
One problem with the token-based sampler is that
the seating arrangement of the internal restaurant
would never be changed unless a new table is cre-
ated (or an old table is removed) in its child restau-
rant. This probability is very low, particularly in
the restaurants of shallow depth (e.g., unigram or
1184
vConstruct a block
Move the block to the sampled topic
: customer
: table
Figure 3: Transition of the state of restaurants in the
table-based sampler when the number of topics is 2.
{u, v, w} are word types. Each box represents a restau-
rant where the type in the upper-right corner indicates the
context. In this case, we can change the topic of the three
3-grams (vvw, vvw, uvw) in some documents from 1 to
2 at the same time.
bigram restaurants) because these restaurants have
a larger number of customers and tables than those
of deep depth, leading to get stack in undesirable
local minima. For example, imagine a table in
the restaurant of context ?hidden? (depth is 2) and
some topic, served ?unit?. This table is connected
to tables in its child restaurants corresponding to
some 3-grams (e.g., ?of hidden unit? or ?train hid-
den unit?), whereas similar n-grams, such as those
of ?of hidden units? or ?train hidden units? might
be gathered in another topic, but collecting these n-
grams into the same topic might be difficult under
the token-based sampler. The table-based sampler
moves those different n-grams having common suf-
fixes jointly into another topic.
Figure 3 shows a transition of state by the table-
based sampler and Algorithm 4.2 depicts a high-
level description of one iteration. First, we select
a table in a restaurant, which is shown with a dotted
line in the figure. Next, we descend the tree to col-
lect the tables connected to the selected table, which
are pointed by arrows. Because this connection can-
not be preserved in common data structures for a
restaurant described in Teh (2006a) or Blunsom et
al. (2009), we select the child tables randomly. This
is correct because customers in CRP are exchange-
Algorithm 1 Table-based sampler
for all table in all restaurants do
Remove a customer from the parent restaurant.
Construct a block of seating arrangement S by de-
scending the tree recursively.
Sample topic assignment zS ? p(zS |S,S?S , z?S).
Move S to sampled topic, and add a customer to the
parent restaurant of the first selected table.
end for
able, so we can restore the parent-child relations ar-
bitrarily. We continue this process recursively until
reaching the leaf nodes, obtaining a block of seat-
ing arrangement S. After calculating the conditional
distribution, we sample new topic assignment for
this block. Finally, we move this block to the sam-
pled topic, which potentially changes the topic of
many words across different documents, which are
connected to customers in a block at leaf nodes (this
connection is also arbitrary).
Conditional distribution Let zS be the block of
topic assignments connected to S and zS be a vari-
able indicating the topic assignment. Thanks to the
exchangeability of all customers and tables in one
restaurant (Teh, 2006a), we can imagine that cus-
tomers and tables in S have been added to the restau-
rants last. We are interested in the following condi-
tional distribution: (conditioning ? is omitted)
p(zS = k?|S,S?S , z?S) ? p(S|S?S , k?)p(zS = k?|z?S),
where p(S|S?S , k?) is a product of customers? ac-
tions moving to another topic, which can be decom-
posed as:
p(S|S?S , k?) = p(w|k?, h)
?
s?S
p(s|k?) (10)
p(s|k?) =
?ts?1
i=0 (b+a(t
k?(?s)
hsw
+i))
?csi
j=1(j?a)
(b+ck
?(?s)
hsw?
)cs?
(11)
?
?ts?1
i=0 (b+a(t
k?(?s)
hsw
+i))
(b+ck
?(?s)
hsw?
)cs?
. (12)
Let us define some notations used above. Each
s ? S is a part of seating arrangements in a restau-
rant, there being ts tables, i-th of which with csi
customers, with hs as the corresponding context. A
restaurant of context h and topic k has tkhw tables
served dish w, i-th of which with ckhwi customers.
Superscripts ?s indicate excluding the contribution
1185
of customers in s, and xn = x(x+1) ? ? ? (x+n?1)
is the ascending factorial. In (10) p(w|k?, h) is the
parent distribution of the first selected table, and
the other p(s|k?) is the seating arrangement of cus-
tomers. The likelihood for changing topic assign-
ments across documents must also be considered,
which is p(zS = k?|z?S) and decomposed as:
p(zS = k?|z?S) =
?
j
(n?S
jk?
+?k? )
nj(S)
(N?Sj +
?
k ?k)
nj(S)
, (13)
where nj(S) is the number of word tokens con-
nected with S in document j.
HIERARCHICAL We skip tables on restaurants of
k = 0, because these tables are all from other topics
and we cannot construct a block. The effects of ?
can be ignored because these are shared by all topics.
SWITCHING In the SWITCHING, p(zS = k?|z?S)
cannot be calculated in a closed form because
p(lji|hji) in (9) would be changed dynamically
when adding customers. This problem is the same
one addressed by Blunsom and Cohn (2011), and we
follow the same approximation in which, when we
calculate the probability, we fractionally add tables
and customers recursively.
4.3 Inference of Hyperparameters
We also place a prior on each hyperparameter and
sample value from the posterior distribution for ev-
ery iteration. As in Teh (2006a), we set different
values of a and b for each depth of PYP, but share
across all topics and sample values with an auxiliary
variable method. We also set different value of ? for
each depth, on which we place Gamma(1, 1). We
make the topic prior ? asymmetric: ? = ??0;? ?
Gamma(1, 1),?0 ? Dir(1).
5 Related Work
HMM-LDA (Griffiths et al, 2005) is a composite
model of HMM and LDA that assumes the words
in a document are generated by HMM, where only
one state has a document-specific topic distribution.
Our SWITCHING model can be understood as a lex-
ical extension of HMM-LDA. It models the topical-
ity by context-specific binary random variables, not
by hidden states. Other n-gram topic models have
focused mainly on information retrieval. Wang et
min. training set test set
Corpus appear # types # docs # tokens # docs # tokens
Brown 4 19,759 470 1,157,225 30 70,795
NIPS 4 22,705 1500 5,088,786 50 167,730
BNC 10 33,071 6,162 12,783,130 100 202,994
Table 1: Corpus statistics after the pre-processing: We
replace words appearing less than min.appear times in
training + test documents, or appearing only in a test set
with an unknown token. All numbers are replaced with
#, while punctuations are remained.
al. (2007) is a topic model on automatically seg-
mented chunks. Lindsey et al (2012) extended this
model with the hierarchical Pitman-Yor prior. They
also used switching variables, but for a different pur-
pose: to determine the segmenting points. They treat
these variables completely independently, while our
model employs a hierarchical prior to share statisti-
cal strength among similar contexts.
Our primary interest is language model adapta-
tion, which has been studied mainly in the area of
speech processing. Conventionally, this adaptation
has relied on a heuristic combination of two sep-
arately trained models: an n-gram model p(w|h)
and a topic model p(w|d). The unigram rescal-
ing, which is a product model of these two mod-
els, perform better than more simpler models such
as linear interpolation (Gildea and Hofmann, 1999).
There are also some extensions to this method (Tam
and Schultz, 2009; Huang and Renals, 2008), but
these methods have one major drawback: at predic-
tion, the rescaling-based method requires normaliza-
tion across vocabulary at each word, which prohibits
use on applications requiring dynamic (incremental)
adaptation, e.g., settings where we have to update
the topic distribution as new inputs come in. Tam
and Schultz (2005) studied on this incremental set-
tings, but they employ an interpolation. The practi-
cal interest here is whether our Bayesian models can
rival the rescaling-based method in terms of predic-
tion power. We evaluate this in the next section.
6 Experiments
6.1 Settings
We test the effectiveness of presented models and
the blocked sampling method on unsupervised lan-
guage model adaptation settings. Specifically we
1186
0 2 4 6 8time (hr.)7.3e+06
7.5e+067.7e+067.9e+06
8.1e+06
negativel
og-likeliho
od
(a) Brown
0 8 16 24 32time (hr.)2.9e+07
3.1e+073.3e+073.5e+07
3.7e+07
negativel
og-likeliho
od
(b) NIPS
0 15 30 45 60time (hr.)8.0e+07
8.3e+078.6e+078.9e+07
9.2e+07
negativel
og-likeliho
od 3-gram Hpytmtoken3-gram Hpytm4-gram Hpytmtoken4-gram Hpytm
(c) BNC
10 50 100# topics205210
215220225230
235240245
testperpl
exity
(d) Brown
10 50 100# topics100105
110115120
125
testperpl
exity
(e) NIPS
10 50 100# topics130140
150160170
180190
testperpl
exity
HpylmHpytmtokenHpytmRescalingSwitchingHierarchical
(f) BNC
Figure 4: (a)?(c): Comparison of negative log-likelihoods at training of HPYTM (K = 50). Lower is better. HPYTM
is trained on both token- and table-based samplers, while HPYTMtoken is trained only on the token-based sampler.
(d)?(f): Test perplexity of various 3-gram models as a function of number of topics on each corpus.
concentrate on the dynamic adaptation: We update
the posterior of language model given previously ob-
served contexts, which might be decoded transcripts
at that point in ASR or MT.
We use three corpora: the Brown, BNC and NIPS.
The Brown and BNC are balanced corpora that con-
sist of documents of several genres from news to
romance. The Brown corpus comprises 15 cate-
gories. We selected two documents from each cate-
gory for the test set, and use other 470 documents for
the training set. For the NIPS, we randomly select
1,500 papers for training and 50 papers for testing.
For BNC, we first randomly selected 400 documents
from a written corpus and then split each document
into smaller documents every 100 sentences, leading
to 6,262 documents, from which we randomly se-
lected 100 documents for testing, and other are used
for training. See Table 1 for the pre-processing of
unknown types and the resulting corpus statistics.
For comparison, besides our proposed HIERAR-
CHICAL and SWITCHING models, we prepare vari-
ous models for baseline. HPYLM is a n-gram lan-
guage model without any topics. We call the model
without the global G0h introduced in Section 2.2
HPYTM. To see the effect of the table-based sam-
pler, we also prepare HPYTMtoken, which is trained
only on the token-based sampler. RESCALING is
the unigram rescaling. This is a product model of
an n-gram model p(w|h) and a topic model p(w|d),
where we learn each model separately and then com-
bine them by:
p(w|h, d) ?
(p(w|d)
p(w)
)?
p(w|h). (14)
We set ? in (14) to 0.7, which we tuned with the
Brown corpus.
6.2 Effects of Table-based Sampler
We first evaluate the effects of our blocked sam-
pler at training. For simplicity, we concentrate on
the HPYTM with K = 50. Table 4(a)?(c) shows
negative likelihoods of the model during training.
On all corpora, the model with the table-based sam-
pler reached the higher probability space with much
faster speed on both 3-gram and 4-gram models.
1187
6.3 Perplexity Results
Training For burn-in, we ran the sampler as fol-
lows: For HPYLM, we ran 100 Gibbs iterations. For
RESCALING, we ran 900 iterations on LDA and 100
iterations on HPYLM. For all other models, we ran
500 iterations of the Gibbs; HPYTMtoken is trained
only on the token-based sampler, while for other
models, the table-based sampler is performed after
the token-based sampler.
Evaluation We have to adapt to the topic dis-
tribution of unseen documents incrementally. Al-
though previous works have employed incremental
EM (Gildea and Hofmann, 1999; Tam and Schultz,
2005) because their inference is EM/VB-based, we
use the left-to-right method (Wallach et al, 2009),
which is a kind of particle filter updating the poste-
rior topic distribution of a test document. We set the
number of particles to 10 and resampled each parti-
cle every 10 words for all experiments. To get the
final perplexity, after burn-in, we sampled 10 sam-
ples every 10 iterations of Gibbs, calculated a test
perplexity for each sample, and averaged the results.
Comparison of 3-grams Figure 4(d)?(f) shows
perplexities when varying the number of top-
ics. Generally, compared to the HPYTMtoken, the
HPYTM got much perplexity gains, which again
confirm the effectiveness of our blocked sampler.
Both our proposed models, the HIERARCHICAL and
the SWITCHING, got better performances than the
HPYTM, which does not place the global model
G0h. Our SWITCHING model consistently performed
the best. The HIERARCHICAL performed somewhat
worse than the RESCALING when K become large,
but the SWITCHING outperformed that.
Comparison of 4-grams and beyond We sum-
marize the results with higher order n-grams in Ta-
ble 2, where we also show the time for prediction.
We fixed the number of topics K = 100 because
we saw that all models but HPYTMtoken performed
best at K = 100 when n = 3. Generally, the
results are consistent with those of n = 3. The
models with n = ? indicate a model extension
using the Bayesian variable-order language model
(Mochihashi and Sumita, 2008), which can naturally
be integrated with our generative models. By this
extension, we can prune unnecessary nodes stochas-
NIPS BNC
Model n PPL time PPL time
HPYLM 4 117.2 59 169.2 74
HPYLM ? 117.9 61 173.1 59
RESCALING 4 101.4 19009 130.3 36323
HPYTM 4 107.0 1004 133.1 980
HPYTM ? 107.2 1346 133.6 1232
HIERARCHICAL 4 106.3 1038 129.0 993
HIERARCHICAL ? 105.7 1337 129.3 1001
SWITCHING 4 100.0 1059 125.5 991
SWITCHING ? 100.4 1369 125.7 1006
Table 2: Comparison of perplexity and the time require
for prediction (in seconds). The number of topics is fixed
to 100 on all topic-based models.
tically during training. We can see that this ?-
gram did not hurt performances, but the sampled
model get much more compact; in BNC, the number
of nodes of the SWITCHING with 4-gram is about
7.9M, while the one with ?-gram is about 3.9M.
Note that our models require no explicit normaliza-
tion, thereby drastically reducing the time for pre-
diction compared to the RESCALING. This differ-
ence is especially remarkable when the vocabulary
size becomes large.
We can see that our SWITCHING performed con-
sistently better than the HIERARCHICAL. One rea-
son for this result might be the mismatch of pre-
diction of the topic distribution in the HIERARCHI-
CAL. The HIERARCHICAL must allocate some (not
global) topics to every word in a document, so even
the words to which the SWITCHING might allocate
the global topic (mainly function words; see below)
must be allocated to some other topics, causing a
mismatch of allocations of topic.
6.4 Qualitative Results
To observe the behavior in which the SWITCHING
allocates some words to the global topic, in Figure
5, we show the posterior of allocating the topic 0
or not at each word in a part of the NIPS training
corpus. We can see that the model elegantly identi-
fied content and function words, learning the topic
distribution appropriately using only semantic con-
texts. These same results in the HIERARCHICAL are
presented in Table 3, where we show some relations
between ?h and context h. Contexts that might be
likely to precede nouns have a higher value of ?h,
1188
there has been much recent work on measuring image statistics
and on learning probability distributions on images . we observe
that the mapping from images to statistics is many-to-one and
show it can be quantified by a phase space factor .
Figure 5: The posterior for assigning topic 0 or not in
NIPS by the ?-gram SWITCHING. Darker words indi-
cate a higher probability of not being assigned topic 0.
?h h
0.0?0.1 in spite, were unable, a sort, on behalf, . regardless
0.5?0.6 assumed it, rand mines, plans was, other excersises
0.9?1.0 that the, the existing, the new, their own, and spatial
Table 3: Some contexts h for various values of ?h in-
duced by the 3-gram HIERARCHICAL in BNC.
while prefixes of idioms have a lower value. The?-
gram extension gives us the posterior of n-gram or-
der p(n|h), which can be used to calculate the proba-
bility of a word ordering composing a phrase in topic
k as p(w, n|k, h) ? p(n|h)p(w|k, n, h). In Table
4, we show some higher probability topic-specific
phrases from the model trained on the NIPS.
7 Conclusion
We have presented modeling and algorithmic con-
tributions to the existing Bayesian n-gram topic
model. We explored two different priors to incor-
porate a global model, and found the effectiveness
of the flat structured model. We developed a novel
blocked Gibbs move for these types of models to ac-
celerate inference. We believe that this Gibbs op-
eration can be incorporated with other models hav-
ing a similar hierarchical structure. Empirically, we
demonstrate that by a careful model design and effi-
cient inference, a well-defined Bayesian model can
rival the conventional heuristics.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part of
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865?874,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
0 46
according to ? support vectors
? ( # ) in high dimentional
? section # as decision function
techniques such as set of # observations
? ( b ) original data set
83 89
the hierarchical mixtures ? linear discriminant
the rbf units images per class
the gating networks multi-class classification
grown hme ? decision boundaries
the modular architecture references per class
Table 4: Topical phrases from NIPS induced by the ?-
gram SWITCHING model. ? is a symbol for the beginning
of a sentence and # represents a number.
Phil Blunsom, Trevor Cohn, Sharon Goldwater, and Mark
Johnson. 2009. A note on the implementation of hi-
erarchical dirichlet processes. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
337?340, Suntec, Singapore, August. Association for
Computational Linguistics.
Daniel Gildea and Thomas Hofmann. 1999. Topic-based
language models using em. In In Proceedings of EU-
ROSPEECH, pages 2167?2170.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101(Suppl 1):5228?5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In In Advances in Neural Information Pro-
cessing Systems 17, pages 537?544. MIT Press.
Songfang Huang and Steve Renals. 2008. Unsupervised
language model adaptation based on topic and role in-
formation in multiparty meetings. In in Proc. Inter-
speech08, pages 833?836.
F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. 1991.
A dynamic language model for speech recognition. In
Proceedings of the workshop on Speech and Natural
Language, HLT ?91, pages 293?295, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Robert Lindsey, William Headden, and Michael Stipice-
vic. 2012. A phrase-discovering topic model using hi-
erarchical pitman-yor processes. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 214?222, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Daichi Mochihashi and Eiichiro Sumita. 2008. The infi-
nite markov model. In J.C. Platt, D. Koller, Y. Singer,
and S. Roweis, editors, Advances in Neural Informa-
tion Processing Systems 20, pages 1017?1024. MIT
Press, Cambridge, MA.
1189
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers - Volume 1, pages
959?968. Association for Computational Linguistics.
Yik-Cheung Tam and Tanja Schultz. 2005. Dynamic lan-
guage model adaptation using variational bayes infer-
ence. In INTERSPEECH, pages 5?8.
Yik-Cheung Tam and Tanja Schultz. 2009. Correlated
bigram lsa for unsupervised language model adapta-
tion. In D. Koller, D. Schuurmans, Y. Bengio, and
L. Bottou, editors, Advances in Neural Information
Processing Systems 21, pages 1633?1640.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Yee Whye Teh. 2006a. A Bayesian Interpretation of
Interpolated Kneser-Ney. NUS School of Computing
Technical Report TRA2/06.
Yee Whye Teh. 2006b. A hierarchical bayesian language
model based on pitman-yor processes. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 985?
992, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In Proceedings of the 26th Annual In-
ternational Conference on Machine Learning, ICML
?09, pages 1105?1112, New York, NY, USA. ACM.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of the 23rd international
conference on Machine learning, ICML ?06, pages
977?984.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In Proceedings
of the 2007 Seventh IEEE International Conference on
Data Mining, ICDM ?07, pages 697?702, Washington,
DC, USA. IEEE Computer Society.
Frank Wood and Yee Whye Teh. 2009. A hierarchi-
cal nonparametric Bayesian approach to statistical lan-
guage model domain adaptation. In Proceedings of the
International Conference on Artificial Intelligence and
Statistics, volume 12.
1190
